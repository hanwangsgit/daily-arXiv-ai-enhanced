<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 159]
- [cs.AI](#cs.AI) [Total: 113]
- [eess.IV](#eess.IV) [Total: 10]
- [cs.RO](#cs.RO) [Total: 61]
- [eess.SP](#eess.SP) [Total: 28]
- [cs.IT](#cs.IT) [Total: 14]
- [cs.LG](#cs.LG) [Total: 156]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Beyond Ground: Map-Free LiDAR Relocalization for UAVs](https://arxiv.org/abs/2602.13267)
*Hengyu Mu,Jianshi Wu,Yuxin Guo,XianLian Lin,Qingyong Hu,Chenglu Wen,Cheng Wang*

Main category: cs.CV

TL;DR: MAILS is a map-free LiDAR relocalization framework for UAVs that addresses the limitations of existing autonomous-driving-focused methods by introducing novel attention mechanisms and handling UAV-specific challenges like yaw rotations and altitude variations.


<details>
  <summary>Details</summary>
Motivation: Existing LiDAR relocalization methods are designed for autonomous driving and perform poorly in UAV scenarios. There's also a lack of datasets capturing real UAV flight characteristics like irregular trajectories and altitude variations.

Method: Proposes MAILS with: 1) Locality-Preserving Sliding Window Attention for discriminative geometric feature extraction from sparse point clouds; 2) Coordinate-independent feature initialization and locally invariant positional encoding to handle UAV yaw rotations and altitude changes; 3) A new large-scale UAV LiDAR dataset with four scenes and varied flight trajectories.

Result: Extensive experiments show MAILS achieves satisfactory localization precision and significantly outperforms existing techniques. The method demonstrates robustness to UAV-specific challenges.

Conclusion: MAILS provides an effective map-free LiDAR relocalization solution for UAVs, addressing domain-specific challenges and outperforming existing methods. The accompanying dataset fills an important gap in UAV localization research.

Abstract: Localization is a fundamental capability in unmanned aerial vehicle (UAV) systems. Map-free LiDAR relocalization offers an effective solution for achieving high-precision positioning in environments with weak or unavailable GNSS signals. However, existing LiDAR relocalization methods are primarily tailored to autonomous driving, exhibiting significantly degraded accuracy in UAV scenarios. In this paper, we propose MAILS, a novel map-free LiDAR relocalization framework for UAVs. A Locality-Preserving Sliding Window Attention module is first introduced to extract locally discriminative geometric features from sparse point clouds. To handle substantial yaw rotations and altitude variations encountered during UAV flight, we then design a coordinate-independent feature initialization module and a locally invariant positional encoding mechanism, which together significantly enhance the robustness of feature extraction. Furthermore, existing LiDAR-based relocalization datasets fail to capture real-world UAV flight characteristics, such as irregular trajectories and varying altitudes. To address this gap, we construct a large-scale LiDAR localization dataset for UAVs, which comprises four scenes and various flight trajectories, designed to evaluate UAV relocalization performance under realistic conditions. Extensive experiments demonstrate that our method achieves satisfactory localization precision and consistently outperforms existing techniques by a significant margin. Our code and dataset will be released soon.

</details>


### [2] [Explanatory Interactive Machine Learning for Bias Mitigation in Visual Gender Classification](https://arxiv.org/abs/2602.13286)
*Nathanya Satriani,Djordje Slijepčević,Markus Schedl,Matthias Zeppelzauer*

Main category: cs.CV

TL;DR: XIL methods (CAIPI, RRR, and hybrid) effectively reduce bias and improve fairness in gender classifiers by guiding models to focus on relevant features, though with slight performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: To address bias and spurious correlations in visual classifiers, particularly in gender classification where data bias is common, by leveraging explanatory interactive learning to guide models toward relevant features from user perspectives.

Method: Evaluated two XIL strategies (CAIPI and Right for the Right Reasons) plus a novel hybrid approach. Used segmentation masks with GradCAM and Bounded Logit Attention explanations for quantitative evaluation.

Result: XIL methods effectively guide models to focus on relevant image features (especially CAIPI) and reduce bias by balancing misclassification rates between male/female predictions. CAIPI shows potential to improve accuracy while others slightly decrease performance.

Conclusion: XIL methods improve fairness and transparency in gender classifiers, with CAIPI demonstrating particular promise for both bias reduction and potential accuracy improvement, though generally there are slight performance trade-offs for increased fairness.

Abstract: Explanatory interactive learning (XIL) enables users to guide model training in machine learning (ML) by providing feedback on the model's explanations, thereby helping it to focus on features that are relevant to the prediction from the user's perspective. In this study, we explore the capability of this learning paradigm to mitigate bias and spurious correlations in visual classifiers, specifically in scenarios prone to data bias, such as gender classification. We investigate two methodologically different state-of-the-art XIL strategies, i.e., CAIPI and Right for the Right Reasons (RRR), as well as a novel hybrid approach that combines both strategies. The results are evaluated quantitatively by comparing segmentation masks with explanations generated using Gradient-weighted Class Activation Mapping (GradCAM) and Bounded Logit Attention (BLA). Experimental results demonstrate the effectiveness of these methods in (i) guiding ML models to focus on relevant image features, particularly when CAIPI is used, and (ii) reducing model bias (i.e., balancing the misclassification rates between male and female predictions). Our analysis further supports the potential of XIL methods to improve fairness in gender classifiers. Overall, the increased transparency and fairness obtained by XIL leads to slight performance decreases with an exception being CAIPI, which shows potential to even improve classification accuracy.

</details>


### [3] [COOPERTRIM: Adaptive Data Selection for Uncertainty-Aware Cooperative Perception](https://arxiv.org/abs/2602.13287)
*Shilpa Mukhopadhyay,Amit Roy-Chowdhury,Hang Qiu*

Main category: cs.CV

TL;DR: COOPERTRIM is an adaptive feature selection framework for cooperative perception that reduces bandwidth by 80%+ while maintaining accuracy, using temporal awareness to identify dynamic features and avoid redundant static information transmission.


<details>
  <summary>Details</summary>
Motivation: Cooperative perception faces tension between limited wireless bandwidth and rich sensor data. Current selection strategies still stress bandwidth limits, requiring a fundamental solution that exploits temporal continuity to reduce redundant transmissions.

Method: Proactive approach using temporal awareness to identify dynamic features while avoiding repetitive static information. Introduces conformal temporal uncertainty metric to gauge feature relevance and data-driven mechanism to dynamically determine sharing quantity.

Result: Achieves up to 80.28% bandwidth reduction for semantic segmentation and 72.52% for 3D detection while maintaining comparable accuracy. Improves IoU by up to 45.54% with 72% less bandwidth. Combined with compression, reduces bandwidth to 1.46% without compromising IoU.

Conclusion: COOPERTRIM demonstrates graceful adaptation to environmental dynamics, localization error, and communication latency, showing flexibility and paving the way for real-world deployment of cooperative perception systems.

Abstract: Cooperative perception enables autonomous agents to share encoded representations over wireless communication to enhance each other's live situational awareness. However, the tension between the limited communication bandwidth and the rich sensor information hinders its practical deployment. Recent studies have explored selection strategies that share only a subset of features per frame while striving to keep the performance on par. Nevertheless, the bandwidth requirement still stresses current wireless technologies. To fundamentally ease the tension, we take a proactive approach, exploiting the temporal continuity to identify features that capture environment dynamics, while avoiding repetitive and redundant transmission of static information. By incorporating temporal awareness, agents are empowered to dynamically adapt the sharing quantity according to environment complexity. We instantiate this intuition into an adaptive selection framework, COOPERTRIM, which introduces a novel conformal temporal uncertainty metric to gauge feature relevance, and a data-driven mechanism to dynamically determine the sharing quantity. To evaluate COOPERTRIM, we take semantic segmentation and 3D detection as example tasks. Across multiple open-source cooperative segmentation and detection models, COOPERTRIM achieves up to 80.28% and 72.52% bandwidth reduction respectively while maintaining a comparable accuracy. Relative to other selection strategies, COOPERTRIM also improves IoU by as much as 45.54% with up to 72% less bandwidth. Combined with compression strategies, COOPERTRIM can further reduce bandwidth usage to as low as 1.46% without compromising IoU performance. Qualitative results show COOPERTRIM gracefully adapts to environmental dynamics, localization error, and communication latency, demonstrating flexibility and paving the way for real-world deployment.

</details>


### [4] [Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs](https://arxiv.org/abs/2602.13289)
*Paul Jonas Kurz,Tobias Jan Wieczorek,Mohamed A. Abdelsalam,Rahaf Aljundi,Marcus Rohrbach*

Main category: cs.CV

TL;DR: PTQ compression degrades MLLM accuracy and reliability in VQA tasks; data-aware quantization softens the impact; Selector confidence estimator helps mitigate reliability loss; int4 MBQ + Selector offers best efficiency-reliability trade-off.


<details>
  <summary>Details</summary>
Motivation: MLLMs face dual challenges: overconfidence (producing highly certain but incorrect answers) and large size limiting edge deployment. Need to understand how compression affects both accuracy and reliability.

Method: Study Post-Training Quantization (PTQ) effects on Qwen2-VL-7B and Idefics3-8B using data-free (HQQ) and data-aware (MBQ) methods across multiple bit widths. Adapt Selector confidence estimator for quantized multimodal settings and test robustness across quantization levels and OOD scenarios.

Result: PTQ degrades both accuracy and reliability. Data-aware methods soften the degradation. Selector substantially mitigates reliability impact. int4 MBQ + Selector achieves best efficiency-reliability trade-off, approaching uncompressed performance with ~75% less memory.

Conclusion: First systematic study linking quantization and reliability in multimodal settings. Shows PTQ affects both accuracy and reliability, but data-aware quantization + confidence estimation can maintain reliability-efficiency balance for edge deployment.

Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessitating compression. We study the intersection of these two challenges by analyzing how Post-Training Quantization (PTQ) compression affects both accuracy and reliability in Visual Question Answering (VQA). We evaluate two MLLMs, Qwen2-VL-7B and Idefics3-8B, quantized with data-free (HQQ) and data-aware (MBQ) methods across multiple bit widths. To counteract the reduction in reliability caused by quantization, we adapt the Selector confidence estimator for quantized multimodal settings and test its robustness across various quantization levels and out-of-distribution (OOD) scenarios. We find that PTQ degrades both accuracy and reliability. Data-aware methods soften the effect thereof. The Selector substantially mitigates the reliability impact. The combination of int4 MBQ and the Selector achieves the best efficiency-reliability trade-off, closing in on uncompressed performance at approx. 75% less memory demand. Overall, we present the first systematic study linking quantization and reliability in multimodal settings.

</details>


### [5] [NutVLM: A Self-Adaptive Defense Framework against Full-Dimension Attacks for Vision Language Models in Autonomous Driving](https://arxiv.org/abs/2602.13293)
*Xiaoxu Peng,Dong Zhou,Jianwen Zhang,Guanghui Sun,Anh Tu Ngo,Anupam Chattopadhyay*

Main category: cs.CV

TL;DR: NutVLM is a self-adaptive defense framework for Vision Language Models in autonomous driving that detects and mitigates adversarial threats through a unified detection-purification mechanism and expert-guided prompt tuning.


<details>
  <summary>Details</summary>
Motivation: Vision Language Models in autonomous driving are vulnerable to adversarial threats (local patches and global perturbations), but existing defense methods are limited and often compromise clean-sample performance. There's a need for comprehensive security that maintains both robustness and performance.

Method: Uses NutNet++ as a sentinel for three-way classification (benign samples, local patches, global perturbations). Local threats are purified via grayscale masking, while global perturbations trigger Expert-guided Adversarial Prompt Tuning (EAPT) - gradient-based latent optimization that generates "corrective driving prompts" without full-model fine-tuning.

Result: Evaluated on Dolphins benchmark, NutVLM achieves 4.89% improvement in overall metrics (Accuracy, Language Score, GPT Score), demonstrating effective defense while maintaining performance.

Conclusion: NutVLM provides a scalable security solution for intelligent transportation systems, effectively defending against adversarial threats while reconciling robustness with clean-sample performance through adaptive detection and efficient prompt-based mitigation.

Abstract: Vision Language Models (VLMs) have advanced perception in autonomous driving (AD), but they remain vulnerable to adversarial threats. These risks range from localized physical patches to imperceptible global perturbations. Existing defense methods for VLMs remain limited and often fail to reconcile robustness with clean-sample performance. To bridge these gaps, we propose NutVLM, a comprehensive self-adaptive defense framework designed to secure the entire perception-decision lifecycle. Specifically, we first employ NutNet++ as a sentinel, which is a unified detection-purification mechanism. It identifies benign samples, local patches, and global perturbations through three-way classification. Subsequently, localized threats are purified via efficient grayscale masking, while global perturbations trigger Expert-guided Adversarial Prompt Tuning (EAPT). Instead of the costly parameter updates of full-model fine-tuning, EAPT generates "corrective driving prompts" via gradient-based latent optimization and discrete projection. These prompts refocus the VLM's attention without requiring exhaustive full-model retraining. Evaluated on the Dolphins benchmark, our NutVLM yields a 4.89% improvement in overall metrics (e.g., Accuracy, Language Score, and GPT Score). These results validate NutVLM as a scalable security solution for intelligent transportation. Our code is available at https://github.com/PXX/NutVLM.

</details>


### [6] [VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction](https://arxiv.org/abs/2602.13294)
*Jiarong Liang,Max Ku,Ka-Hei Hui,Ping Nie,Wenhu Chen*

Main category: cs.CV

TL;DR: VisPhyWorld is an execution-based framework that evaluates physical reasoning in MLLMs by requiring them to generate executable simulator code from visual observations, making world representations directly testable.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for evaluating physical reasoning in MLLMs rely on recognition-style protocols (VQA, VoE) that can be answered without explicit physical hypotheses, making it difficult to assess genuine physical reasoning capabilities.

Method: Propose VisPhyWorld framework where models generate executable simulator code from visual observations, enabling direct inspection and testing of inferred world representations. Introduce VisPhyBench with 209 scenes from 108 physical templates and systematic evaluation protocol.

Result: Pipeline achieves 97.7% valid reconstructed videos on benchmark. Experiments show state-of-the-art MLLMs have strong semantic scene understanding but struggle with accurate physical parameter inference and consistent physical dynamics simulation.

Conclusion: Execution-based evaluation through code generation provides more rigorous assessment of physical reasoning than recognition-style benchmarks, revealing significant gaps in MLLMs' ability to model physical dynamics despite their semantic understanding capabilities.

Abstract: Evaluating whether Multimodal Large Language Models (MLLMs) genuinely reason about physical dynamics remains challenging. Most existing benchmarks rely on recognition-style protocols such as Visual Question Answering (VQA) and Violation of Expectation (VoE), which can often be answered without committing to an explicit, testable physical hypothesis. We propose VisPhyWorld, an execution-based framework that evaluates physical reasoning by requiring models to generate executable simulator code from visual observations. By producing runnable code, the inferred world representation is directly inspectable, editable, and falsifiable. This separates physical reasoning from rendering. Building on this framework, we introduce VisPhyBench, comprising 209 evaluation scenes derived from 108 physical templates and a systematic protocol that evaluates how well models reconstruct appearance and reproduce physically plausible motion. Our pipeline produces valid reconstructed videos in 97.7% on the benchmark. Experiments show that while state-of-the-art MLLMs achieve strong semantic scene understanding, they struggle to accurately infer physical parameters and to simulate consistent physical dynamics.

</details>


### [7] [MFN Decomposition and Related Metrics for High-Resolution Range Profiles Generative Models](https://arxiv.org/abs/2602.13296)
*Edwyn Brient,Santiago Velasco-Forero,Rami Kassab*

Main category: cs.CV

TL;DR: The paper proposes two new metrics for evaluating generated HRRP data by decomposing it into mask, features, and noise components, addressing limitations of black-box classification-based evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for generated HRRP data rely on classification models which are "black-box" approaches that lack explainability and multi-level evaluation capabilities.

Method: Decomposes HRRP data into three components (mask, features, and noise) and proposes two new metrics based on the physical interpretation of these components.

Result: The proposed metrics demonstrate discriminative ability when evaluated on a challenging task using an expensive dataset.

Conclusion: The decomposition-based metrics provide explainable and multi-level evaluation for generated HRRP data, addressing limitations of current black-box classification approaches.

Abstract: High-resolution range profile (HRRP ) data are in vogue in radar automatic target recognition (RATR). With the interest in classifying models using HRRP, filling gaps in datasets using generative models has recently received promising contributions. Evaluating generated data is a challenging topic, even for explicit data like face images. However, the evaluation methods used in the state-ofthe-art of HRRP generation rely on classification models. Such models, called ''black-box'', do not allow either explainability on generated data or multi-level evaluation. This work focuses on decomposing HRRP data into three components: the mask, the features, and the noise. Using this decomposition, we propose two metrics based on the physical interpretation of those data. We take profit from an expensive dataset to evaluate our metrics on a challenging task and demonstrate the discriminative ability of those.

</details>


### [8] [Conditional Generative Models for High-Resolution Range Profiles: Capturing Geometry-Driven Trends in a Large-Scale Maritime Dataset](https://arxiv.org/abs/2602.13297)
*Edwyn Brient,Santiago Velasco-Forero,Rami Kassab*

Main category: cs.CV

TL;DR: This paper studies conditional HRRP generation for maritime radar target recognition using a large-scale database, showing that geometric factors (ship dimensions and aspect angle) are key drivers for robust signature synthesis.


<details>
  <summary>Details</summary>
Motivation: HRRPs enable fast radar target recognition but are sensitive to acquisition conditions, limiting robustness across operational scenarios. Prior studies are constrained by small, highly specific datasets, creating a need for more comprehensive analysis.

Method: The authors study HRRP synthesis on a large-scale maritime database representing coastal surveillance variability. They identify geometric factors (ship dimensions and desired aspect angle) as fundamental scenario drivers, then condition generative models on these variables to synthesize signatures.

Result: The synthesized signatures reproduce the expected line-of-sight geometric trend observed in real data, demonstrating that acquisition geometry plays a central role in robust HRRP generation.

Conclusion: Geometric conditioning (ship dimensions and aspect angle) is crucial for robust HRRP generation in maritime surveillance applications, addressing the sensitivity issues of traditional HRRP-based target recognition across varying operational scenarios.

Abstract: High-resolution range profiles (HRRPs) enable fast onboard processing for radar automatic target recognition, but their strong sensitivity to acquisition conditions limits robustness across operational scenarios. Conditional HRRP generation can mitigate this issue, yet prior studies are constrained by small, highly specific datasets. We study HRRP synthesis on a largescale maritime database representative of coastal surveillance variability. Our analysis indicates that the fundamental scenario drivers are geometric: ship dimensions and the desired aspect angle. Conditioning on these variables, we train generative models and show that the synthesized signatures reproduce the expected line-of-sight geometric trend observed in real data. These results highlight the central role of acquisition geometry for robust HRRP generation.

</details>


### [9] [Effect of Convolutional Depth on Image Recognition Performance: VGG vs. ResNet vs. GoogLeNet](https://arxiv.org/abs/2602.13298)
*Manfred M. Fischer,Joshua Pitts*

Main category: cs.CV

TL;DR: Depth alone doesn't guarantee better performance; effective depth (how depth manifests during training) matters more than nominal depth for accuracy, stability, and efficiency.


<details>
  <summary>Details</summary>
Motivation: To understand why deeper convolutional networks don't always improve performance, and to isolate how depth influences classification accuracy, convergence behavior, and computational efficiency through controlled architectural comparisons.

Method: Comparative study of three canonical CNN architectures (VGG, ResNet, GoogLeNet) with standardized training protocols, distinguishing between nominal depth (layer count) and effective depth (how depth manifests during training).

Result: Plain deep networks show early accuracy saturation and optimization instability, while residual (ResNet) and inception-based (GoogLeNet) architectures consistently translate additional depth into improved accuracy at lower effective depth with better accuracy-compute trade-offs.

Conclusion: Effective depth, not nominal depth, is the key factor governing depth's productive scaling in convolutional networks, with architectural mechanisms that constrain effective depth being critical for performance benefits.

Abstract: Increasing convolutional depth has been central to advances in image recognition, yet deeper networks do not uniformly yield higher accuracy, stable optimization, or efficient computation. We present a controlled comparative study of three canonical convolutional neural network architectures - VGG, ResNet, and GoogLeNet - to isolate how depth influences classification performance, convergence behavior, and computational efficiency. By standardizing training protocols and explicitly distinguishing between nominal and effective depth, we show that the benefits of depth depend critically on architectural mechanisms that constrain its effective manifestation during training rather than on nominal depth alone. Although plain deep networks exhibit early accuracy saturation and optimization instability, residual and inception-based architectures consistently translate additional depth into improved accuracy at lower effective depth and favorable accuracy-compute trade-offs. These findings demonstrate that effective depth, not nominal depth, is the operative quantity governing depth's role as a productive scaling dimension in convolutional networks.

</details>


### [10] [Spectral Collapse in Diffusion Inversion](https://arxiv.org/abs/2602.13303)
*Nicolas Bourriez,Alexandre Verine,Auguste Genovesio*

Main category: cs.CV

TL;DR: OVG addresses spectral collapse in diffusion inversion for unpaired image translation, restoring textures while preserving structure by correcting ODE dynamics in the null-space of structural gradients.


<details>
  <summary>Details</summary>
Motivation: Standard deterministic diffusion inversion (e.g., DDIM) fails when source domains are spectrally sparse compared to target domains (e.g., super-resolution, sketch-to-image), causing spectral collapse where recovered latents don't follow expected Gaussian distribution, leading to oversmoothed, texture-poor generations.

Method: Proposes Orthogonal Variance Guidance (OVG), an inference-time method that corrects ODE dynamics to enforce theoretical Gaussian noise magnitude within the null-space of the structural gradient, resolving the structure-texture trade-off.

Result: Extensive experiments on microscopy super-resolution (BBBC021) and sketch-to-image (Edges2Shoes) demonstrate OVG effectively restores photorealistic textures while preserving structural fidelity.

Conclusion: OVG successfully addresses spectral collapse in conditional diffusion inversion for cross-domain image translation, enabling high-quality texture generation without structural drift.

Abstract: Conditional diffusion inversion provides a powerful framework for unpaired image-to-image translation. However, we demonstrate through an extensive analysis that standard deterministic inversion (e.g. DDIM) fails when the source domain is spectrally sparse compared to the target domain (e.g., super-resolution, sketch-to-image). In these contexts, the recovered latent from the input does not follow the expected isotropic Gaussian distribution. Instead it exhibits a signal with lower frequencies, locking target sampling to oversmoothed and texture-poor generations. We term this phenomenon spectral collapse. We observe that stochastic alternatives attempting to restore the noise variance tend to break the semantic link to the input, leading to structural drift. To resolve this structure-texture trade-off, we propose Orthogonal Variance Guidance (OVG), an inference-time method that corrects the ODE dynamics to enforce the theoretical Gaussian noise magnitude within the null-space of the structural gradient. Extensive experiments on microscopy super-resolution (BBBC021) and sketch-to-image (Edges2Shoes) demonstrate that OVG effectively restores photorealistic textures while preserving structural fidelity.

</details>


### [11] [KidMesh: Computational Mesh Reconstruction for Pediatric Congenital Hydronephrosis Using Deep Neural Networks](https://arxiv.org/abs/2602.13299)
*Haoran Sun,Zhanpeng Zhu,Anguo Zhang,Bo Liu,Zhaohua Lin,Liqin Huang,Mingjing Yang,Lei Liu,Shan Lin,Wangbin Ding*

Main category: cs.CV

TL;DR: KidMesh: End-to-end deep learning method for direct mesh reconstruction from MRU images for pediatric congenital hydronephrosis, enabling functional urodynamic simulations without post-processing.


<details>
  <summary>Details</summary>
Motivation: Existing voxel-based segmentation methods for congenital hydronephrosis focus only on morphological features and require complex post-processing to convert to mesh representations needed for functional urodynamic simulations. Mesh-level annotations are difficult to obtain due to sparsely sampled MRU slices.

Method: KidMesh extracts feature maps from MRU images, converts them to feature vertices via grid sampling, and deforms a template mesh using these vertices to generate specific CH meshes. Uses novel training schema that doesn't require accurate mesh-level annotations.

Result: Reconstructs CH meshes in 0.4 seconds average, comparable to conventional methods without post-processing. Reconstructed meshes have no self-intersections, with only 3.7% vertices >3.2mm error and 0.2% >6.4mm error. After rasterization, achieves Dice score of 0.86 against manual masks. Meshes usable for renal urine flow simulations.

Conclusion: KidMesh enables direct mesh reconstruction from MRU images for congenital hydronephrosis, providing valuable urodynamic information for clinical practice without requiring mesh-level annotations or post-processing steps.

Abstract: Pediatric congenital hydronephrosis (CH) is a common urinary tract disorder, primarily caused by obstruction at the renal pelvis-ureter junction. Magnetic resonance urography (MRU) can visualize hydronephrosis, including renal pelvis and calyces, by utilizing the natural contrast provided by water. Existing voxel-based segmentation approaches can extract CH regions from MRU, facilitating disease diagnosis and prognosis. However, these segmentation methods predominantly focus on morphological features, such as size, shape, and structure. To enable functional assessments, such as urodynamic simulations, external complex post-processing steps are required to convert these results into mesh-level representations. To address this limitation, we propose an end-to-end method based on deep neural networks, namely KidMesh, which could automatically reconstruct CH meshes directly from MRU. Generally, KidMesh extracts feature maps from MRU images and converts them into feature vertices through grid sampling. It then deforms a template mesh according to these feature vertices to generate the specific CH meshes of MRU images. Meanwhile, we develop a novel schema to train KidMesh without relying on accurate mesh-level annotations, which are difficult to obtain due to the sparsely sampled MRU slices. Experimental results show that KidMesh could reconstruct CH meshes in an average of 0.4 seconds, and achieve comparable performance to conventional methods without requiring post-processing. The reconstructed meshes exhibited no self-intersections, with only 3.7% and 0.2% of the vertices having error distances exceeding 3.2mm and 6.4mm, respectively. After rasterization, these meshes achieved a Dice score of 0.86 against manually delineated CH masks. Furthermore, these meshes could be used in renal urine flow simulations, providing valuable urodynamic information for clinical practice.

</details>


### [12] [FireRed-Image-Edit-1.0 Techinical Report](https://arxiv.org/abs/2602.13344)
*Super Intelligence Team,Changhao Qiao,Chao Hui,Chen Li,Cunzheng Wang,Dejia Song,Jiale Zhang,Jing Li,Qiang Xiang,Runqi Wang,Shuang Sun,Wei Zhu,Xu Tang,Yao Hu,Yibo Chen,Yuhao Huang,Yuxuan Duan,Zhiyi Chen,Ziyuan Guo*

Main category: cs.CV

TL;DR: FireRed-Image-Edit is a diffusion transformer for instruction-based image editing that achieves SOTA performance through optimized data curation, training methodology, and evaluation design.


<details>
  <summary>Details</summary>
Motivation: To create a state-of-the-art instruction-based image editing system that addresses limitations in existing approaches through systematic optimization across data, training, and evaluation.

Method: Constructed 1.6B-sample training corpus with rigorous cleaning/balancing; multi-stage training pipeline (pre-training, supervised fine-tuning, RL); introduced novel techniques: Multi-Condition Aware Bucket Sampler, Stochastic Instruction Alignment, Asymmetric Gradient Optimization for DPO, DiffusionNFT with layout-aware OCR rewards, and differentiable Consistency Loss.

Result: Achieves competitive or superior performance against both open-source and proprietary systems on REDEdit-Bench (15 editing categories) and public benchmarks (ImgEdit and GEdit).

Conclusion: FireRed-Image-Edit demonstrates state-of-the-art instruction-based image editing capabilities through systematic optimization, with released code, models, and benchmark suite to support future research.

Abstract: We present FireRed-Image-Edit, a diffusion transformer for instruction-based image editing that achieves state-of-the-art performance through systematic optimization of data curation, training methodology, and evaluation design. We construct a 1.6B-sample training corpus, comprising 900M text-to-image and 700M image editing pairs from diverse sources. After rigorous cleaning, stratification, auto-labeling, and two-stage filtering, we retain over 100M high-quality samples balanced between generation and editing, ensuring strong semantic coverage and instruction alignment. Our multi-stage training pipeline progressively builds editing capability via pre-training, supervised fine-tuning, and reinforcement learning. To improve data efficiency, we introduce a Multi-Condition Aware Bucket Sampler for variable-resolution batching and Stochastic Instruction Alignment with dynamic prompt re-indexing. To stabilize optimization and enhance controllability, we propose Asymmetric Gradient Optimization for DPO, DiffusionNFT with layout-aware OCR rewards for text editing, and a differentiable Consistency Loss for identity preservation. We further establish REDEdit-Bench, a comprehensive benchmark spanning 15 editing categories, including newly introduced beautification and low-level enhancement tasks. Extensive experiments on REDEdit-Bench and public benchmarks (ImgEdit and GEdit) demonstrate competitive or superior performance against both open-source and proprietary systems. We release code, models, and the benchmark suite to support future research.

</details>


### [13] [DriveMamba: Task-Centric Scalable State Space Model for Efficient End-to-End Autonomous Driving](https://arxiv.org/abs/2602.13301)
*Haisheng Su,Wei Wu,Feixiang Song,Junjie Zhang,Zhenjie Yang,Junchi Yan*

Main category: cs.CV

TL;DR: DriveMamba proposes a single-stage Mamba-based decoder for end-to-end autonomous driving that replaces sequential modular designs with unified task-centric modeling, achieving better efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Current E2E-AD systems use sequential modular designs (perception-prediction-planning) that cause information loss and cumulative errors. They also suffer from insufficient image backbone training and quadratic attention complexity, limiting scalability and efficiency.

Method: DriveMamba uses a single-stage Unified Mamba decoder that converts image features and task outputs into token-level sparse representations sorted by 3D positions. It employs linear-complexity operators for efficient long-context modeling and a bidirectional trajectory-guided "local-to-global" scan method to preserve spatial locality.

Result: Extensive experiments on nuScenes and Bench2Drive datasets demonstrate DriveMamba's superiority, generalizability, and great efficiency compared to existing approaches.

Conclusion: DriveMamba provides a scalable, efficient paradigm for E2E-AD that overcomes limitations of sequential modular designs through unified task-centric modeling with linear-complexity operations.

Abstract: Recent advances towards End-to-End Autonomous Driving (E2E-AD) have been often devoted on integrating modular designs into a unified framework for joint optimization e.g. UniAD, which follow a sequential paradigm (i.e., perception-prediction-planning) based on separable Transformer decoders and rely on dense BEV features to encode scene representations. However, such manual ordering design can inevitably cause information loss and cumulative errors, lacking flexible and diverse relation modeling among different modules and sensors. Meanwhile, insufficient training of image backbone and quadratic-complexity of attention mechanism also hinder the scalability and efficiency of E2E-AD system to handle spatiotemporal input. To this end, we propose DriveMamba, a Task-Centric Scalable paradigm for efficient E2E-AD, which integrates dynamic task relation modeling, implicit view correspondence learning and long-term temporal fusion into a single-stage Unified Mamba decoder. Specifically, both extracted image features and expected task outputs are converted into token-level sparse representations in advance, which are then sorted by their instantiated positions in 3D space. The linear-complexity operator enables efficient long-context sequential token modeling to capture task-related inter-dependencies simultaneously. Additionally, a bidirectional trajectory-guided "local-to-global" scan method is designed to preserve spatial locality from ego-perspective, thus facilitating the ego-planning. Extensive experiments conducted on nuScenes and Bench2Drive datasets demonstrate the superiority, generalizability and great efficiency of DriveMamba.

</details>


### [14] [Progressive Contrast Registration for High-Fidelity Bidirectional Photoacoustic Microscopy Alignment](https://arxiv.org/abs/2602.13304)
*Jiahao Qin*

Main category: cs.CV

TL;DR: PCReg-Net is a progressive contrast-guided registration framework that solves domain shift and geometric misalignment in bidirectional OR-PAM scanning, achieving state-of-the-art alignment quality with real-time performance.


<details>
  <summary>Details</summary>
Motivation: Bidirectional raster scanning in high-speed OR-PAM doubles imaging speed but introduces coupled domain shift and geometric misalignment between forward and backward scan lines. Existing methods based on brightness constancy assumptions achieve limited alignment quality (NCC ≤ 0.96).

Method: PCReg-Net uses a progressive contrast-guided registration framework with four lightweight modules: (1) registration U-Net for coarse alignment, (2) reference feature extractor capturing multi-scale structural cues, (3) contrast module identifying residual misalignment by comparing coarse-registered and reference features, and (4) refinement U-Net with feature injection for high-fidelity output. Also proposes Temporal NCC (TNCC) and Temporal NCC Gap (TNCG) for reference-free evaluation.

Result: On OR-PAM-Reg-4K dataset (432 test samples), PCReg-Net achieves NCC of 0.983, SSIM of 0.982, and PSNR of 46.96 dB, surpassing state-of-the-art by over 14 dB at real-time speed.

Conclusion: PCReg-Net effectively addresses the alignment challenges in bidirectional OR-PAM scanning through progressive contrast-guided registration, achieving superior alignment quality while maintaining real-time performance, with proposed metrics enabling reference-free temporal consistency evaluation.

Abstract: High-speed optical-resolution photoacoustic microscopy (OR-PAM) with bidirectional raster scanning doubles imaging speed but introduces coupled domain shift and geometric misalignment between forward and backward scan lines. Existing methods, constrained by brightness constancy assumptions, achieve limited alignment quality (NCC~$\leq 0.96$). We propose PCReg-Net, a progressive contrast-guided registration framework that performs coarse-to-fine alignment through four lightweight modules: (1)~a registration U-Net for coarse alignment, (2)~a reference feature extractor capturing multi-scale structural cues, (3)~a contrast module that identifies residual misalignment by comparing coarse-registered and reference features, and (4)~a refinement U-Net with feature injection for high-fidelity output. We further propose the Temporal NCC (TNCC) and Temporal NCC Gap (TNCG) for reference-free evaluation of inter-frame temporal consistency. On OR-PAM-Reg-4K (432 test samples), PCReg-Net achieves NCC of 0.983, SSIM of 0.982, and PSNR of 46.96 dB, surpassing the state-of-the-art by over 14 dB at real-time speed. Code is available at https://github.com/JiahaoQin/PCReg-Net

</details>


### [15] [WildfireVLM: AI-powered Analysis for Early Wildfire Detection and Risk Assessment Using Satellite Imagery](https://arxiv.org/abs/2602.13305)
*Aydin Ayanzadeh,Prakhar Dixit,Sadia Kamal,Milton Halem*

Main category: cs.CV

TL;DR: WildfireVLM is an AI framework that combines satellite imagery detection (YOLOv12) with language-driven risk assessment (MLLMs) for real-time wildfire monitoring and prioritized response recommendations.


<details>
  <summary>Details</summary>
Motivation: Wildfires are increasing in frequency and intensity due to climate change and human activities, but current satellite-based monitoring faces challenges with faint smoke signals, dynamic weather conditions, and real-time analysis over large areas.

Method: Created labeled wildfire/smoke dataset from Landsat-8/9, GOES-16, and other Earth observation sources; used YOLOv12 for fire zone and smoke plume detection; integrated Multimodal Large Language Models for contextual risk assessment; validated with LLM-as-judge evaluation; deployed via service-oriented architecture.

Result: Developed a system that combines computer vision detection with language-based reasoning for scalable wildfire monitoring, featuring real-time processing, visual risk dashboards, and long-term tracking capabilities.

Conclusion: The integration of computer vision with language-driven reasoning provides valuable capabilities for scalable wildfire monitoring, demonstrating the effectiveness of combining satellite imagery detection with contextual risk assessment for disaster management.

Abstract: Wildfires are a growing threat to ecosystems, human lives, and infrastructure, with their frequency and intensity rising due to climate change and human activities. Early detection is critical, yet satellite-based monitoring remains challenging due to faint smoke signals, dynamic weather conditions, and the need for real-time analysis over large areas. We introduce WildfireVLM, an AI framework that combines satellite imagery wildfire detection with language-driven risk assessment. We construct a labeled wildfire and smoke dataset using imagery from Landsat-8/9, GOES-16, and other publicly available Earth observation sources, including harmonized products with aligned spectral bands. WildfireVLM employs YOLOv12 to detect fire zones and smoke plumes, leveraging its ability to detect small, complex patterns in satellite imagery. We integrate Multimodal Large Language Models (MLLMs) that convert detection outputs into contextualized risk assessments and prioritized response recommendations for disaster management. We validate the quality of risk reasoning using an LLM-as-judge evaluation with a shared rubric. The system is deployed using a service-oriented architecture that supports real-time processing, visual risk dashboards, and long-term wildfire tracking, demonstrating the value of combining computer vision with language-based reasoning for scalable wildfire monitoring.

</details>


### [16] [Fine-Tuning a Large Vision-Language Model for Artwork's Scoring and Critique](https://arxiv.org/abs/2602.13306)
*Zhehan Zhang,Meihua Qian,Li Luo,Siyu Huang,Chaoyi Zhou,Ripon Saha,Xinxin Song*

Main category: cs.CV

TL;DR: Fine-tuned Qwen2-VL-7B model for automated creativity assessment of paintings, achieving high correlation with expert scores (r>0.97) and generating rubric-aligned feedback.


<details>
  <summary>Details</summary>
Motivation: Manual creativity assessment (like Torrance Tests) is labor-intensive at scale, and existing ML approaches lack explanatory feedback. Need for scalable automated assessment with interpretable feedback for creativity research and arts education.

Method: Fine-tuned vision-language model Qwen2-VL-7B with multi-task learning on 1000 human paintings scored 1-100. Added regression head for score prediction, used system prompts with structured rubric and artwork descriptions to constrain feedback generation. 80/20 train-test split with expert ratings on five dimensions (originality, color, texture, composition, content).

Result: Achieved Pearson correlation r > 0.97 and MAE ≈ 3.95 on 100-point scale. Generated feedback showed high semantic similarity to expert critiques (SBERT cosine similarity = 0.798). Model provides both numerical scores and rubric-aligned feedback in single forward pass.

Conclusion: Framework bridges computer vision and art assessment, offering scalable tool for creativity research and classroom feedback with interpretable explanations.

Abstract: Assessing artistic creativity is foundational to creativity research and arts education, yet manual scoring (e.g., Torrance Tests of Creative Thinking) is labor-intensive at scale. Prior machine-learning approaches show promise for visual creativity scoring, but many rely mainly on image features and provide limited or no explanatory feedback. We propose a framework for automated creativity assessment of human paintings by fine-tuning the vision-language model Qwen2-VL-7B with multi-task learning. Our dataset contains 1000 human-created paintings scored on a 1-100 scale and paired with a short human-written description (content or artist explanation). Two expert raters evaluated each work using a five-dimension rubric (originality, color, texture, composition, content) and provided written critiques; we use an 80/20 train-test split. We add a lightweight regression head on the visual encoder output so the model can predict a numerical score and generate rubric-aligned feedback in a single forward pass. By embedding the structured rubric and the artwork description in the system prompt, we constrain the generated text to match the quantitative prediction. Experiments show strong accuracy, achieving Pearson r > 0.97 and MAE about 3.95 on the 100-point scale. Qualitative evaluation indicates the generated feedback is semantically close to expert critiques (average SBERT cosine similarity = 0.798). The proposed approach bridges computer vision and art assessment and offers a scalable tool for creativity research and classroom feedback.

</details>


### [17] [Visual Para-Thinker: Divide-and-Conquer Reasoning for Visual Comprehension](https://arxiv.org/abs/2602.13310)
*Haoran Xu,Hongyu Wang,Jiaze Li,Shunpeng Chen,Zizhao Tong,Jianzhong Ju,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: Visual Para-Thinker introduces the first parallel reasoning framework for multimodal LLMs, shifting from depth-based to parallel thinking to overcome exploration plateaus in visual reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing LLM test-time scaling laws focus on extended reasoning length (vertical scaling), but this approach often hits plateaus as models get locked into specific thinking patterns. While parallel thinking helps mitigate exploration narrowing in text, extending this paradigm to the visual domain remains an open research question.

Method: 1) Examines the role of visual partitioning in parallelized reasoning and proposes two distinct strategies. 2) Introduces Visual Para-Thinker with Pa-Attention and LPRoPE to maintain path independence and promote reasoning diversity. 3) Develops a native multimodal implementation using the vLLM framework for high-efficiency parallel processing.

Result: Empirical results on benchmark datasets (V*, CountBench, RefCOCO, and HallusionBench) confirm that Visual Para-Thinker successfully extends the benefits of parallel reasoning to the visual domain.

Conclusion: The paper presents the inaugural parallel reasoning framework for MLLMs, demonstrating that parallel thinking can be effectively extended to visual reasoning to overcome exploration limitations of depth-based approaches.

Abstract: Existing LLM test-time scaling laws emphasize the emergence of self-reflective behaviors through extended reasoning length. Nevertheless, this vertical scaling strategy often encounters plateaus in exploration as the model becomes locked into specific thinking pattern. By shifting from depth to parallelism, parallel thinking mitigates the narrowing of exploration. However, the extension of this paradigm to visual domain remains an open research question. In this paper, we first examine the role of visual partitioning in parallelized reasoning and subsequently propose two distinct strategies. Based on the above, we introduce Visual Para-Thinker, representing the inaugural parallel reasoning framework for MLLMs. To maintain path independence and promote diversity in reasoning, our approach integrates Pa-Attention alongside LPRoPE. Leveraging the vLLM framework, we have developed a native multimodal implementation that facilitates high-efficiency parallel processing. Empirical results on benchmark datasets such as V*, CountBench, RefCOCO, and HallusionBench confirm that Visual Para-Thinker successfully extends the benefits of parallel reasoning to the visual domain.

</details>


### [18] [Agentic Spatio-Temporal Grounding via Collaborative Reasoning](https://arxiv.org/abs/2602.13313)
*Heng Zhao,Yew-Soon Ong,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: ASTG is a training-free, agentic framework for spatio-temporal video grounding that uses two specialized MLLM agents (spatial and temporal) working collaboratively to retrieve target objects in videos without supervision.


<details>
  <summary>Details</summary>
Motivation: Existing STVG methods suffer from redundant computation, heavy supervision requirements, limited generalization, and poor performance in weakly-supervised settings. The authors aim to create an open-world, training-free solution that avoids dataset-level train-and-fit constraints.

Method: Proposes ASTG framework with two specialized agents: SRA (Spatial Reasoning Agent) and TRA (Temporal Reasoning Agent), built on MLLMs. They work collaboratively using a propose-and-evaluation paradigm, decoupling spatio-temporal reasoning, automating tube extraction, verification, and temporal localization. Includes visual memory and dialogue context for enhanced efficiency.

Result: Outperforms existing weakly-supervised and zero-shot approaches by a margin, and achieves performance comparable to some fully-supervised methods on popular benchmarks.

Conclusion: ASTG demonstrates the feasibility of training-free, agentic approaches for STVG, offering superior performance over weakly-supervised methods while avoiding heavy annotation requirements and enabling open-world generalization.

Abstract: Spatio-Temporal Video Grounding (STVG) aims to retrieve the spatio-temporal tube of a target object or person in a video given a text query. Most existing approaches perform frame-wise spatial localization within a predicted temporal span, resulting in redundant computation, heavy supervision requirements, and limited generalization. Weakly-supervised variants mitigate annotation costs but remain constrained by the dataset-level train-and-fit paradigm with an inferior performance. To address these challenges, we propose the Agentic Spatio-Temporal Grounder (ASTG) framework for the task of STVG towards an open-world and training-free scenario. Specifically, two specialized agents SRA (Spatial Reasoning Agent) and TRA (Temporal Reasoning Agent) constructed leveraging on modern Multimoal Large Language Models (MLLMs) work collaboratively to retrieve the target tube in an autonomous and self-guided manner. Following a propose-and-evaluation paradigm, ASTG duly decouples spatio-temporal reasoning and automates the tube extraction, verification and temporal localization processes. With a dedicate visual memory and dialogue context, the retrieval efficiency is significantly enhanced. Experiments on popular benchmarks demonstrate the superiority of the proposed approach where it outperforms existing weakly-supervised and zero-shot approaches by a margin and is comparable to some of the fully-supervised methods.

</details>


### [19] [Sim2Radar: Toward Bridging the Radar Sim-to-Real Gap with VLM-Guided Scene Reconstruction](https://arxiv.org/abs/2602.13314)
*Emily Bejerano,Federico Tondolo,Aayan Qayyum,Xiaofan Yu,Xiaofan Jiang*

Main category: cs.CV

TL;DR: Sim2Radar: A framework that synthesizes mmWave radar training data from single RGB images using physics-based simulation, improving radar perception when used for pre-training.


<details>
  <summary>Details</summary>
Motivation: Learning-based radar perception is limited by the scarcity and high cost of collecting and annotating large-scale radar datasets, especially for indoor environments where radar provides reliable perception in visually degraded conditions.

Method: End-to-end framework that reconstructs material-aware 3D scenes from single-view RGB images using monocular depth estimation, segmentation, and vision-language reasoning to infer object materials. Then simulates mmWave propagation with configurable physics-based ray tracing using Fresnel reflection models parameterized by ITU-R electromagnetic properties.

Result: When used for transfer learning (pre-training on synthetic data and fine-tuning on real radar), Sim2Radar improves 3D radar perception by up to +3.7 3D AP (IoU 0.3) on real-world indoor scenes, with gains primarily from improved spatial localization.

Conclusion: Physics-based, vision-driven radar simulation can provide effective geometric priors for radar learning and measurably improve performance under limited real-data supervision, enabling scalable radar data generation without manual scene modeling.

Abstract: Millimeter-wave (mmWave) radar provides reliable perception in visually degraded indoor environments (e.g., smoke, dust, and low light), but learning-based radar perception is bottlenecked by the scarcity and cost of collecting and annotating large-scale radar datasets. We present Sim2Radar, an end-to-end framework that synthesizes training radar data directly from single-view RGB images, enabling scalable data generation without manual scene modeling. Sim2Radar reconstructs a material-aware 3D scene by combining monocular depth estimation, segmentation, and vision-language reasoning to infer object materials, then simulates mmWave propagation with a configurable physics-based ray tracer using Fresnel reflection models parameterized by ITU-R electromagnetic properties. Evaluated on real-world indoor scenes, Sim2Radar improves downstream 3D radar perception via transfer learning: pre-training a radar point-cloud object detection model on synthetic data and fine-tuning on real radar yields up to +3.7 3D AP (IoU 0.3), with gains driven primarily by improved spatial localization. These results suggest that physics-based, vision-driven radar simulation can provide effective geometric priors for radar learning and measurably improve performance under limited real-data supervision.

</details>


### [20] [IDPruner: Harmonizing Importance and Diversity in Visual Token Pruning for MLLMs](https://arxiv.org/abs/2602.13315)
*Yifan Tan,Yifu Sun,Shirui Huang,Hong Liu,Guanghua Yu,Jianchen Zhu,Yangdong Deng*

Main category: cs.CV

TL;DR: IDPruner is a visual token pruning method for MLLMs that optimally balances token importance and diversity using Maximal Marginal Relevance, achieving state-of-the-art performance with high pruning ratios while maintaining compatibility with FlashAttention.


<details>
  <summary>Details</summary>
Motivation: MLLMs face computational bottlenecks from massive visual tokens, and existing pruning methods lack a principled framework for optimally integrating token importance and semantic diversity trade-offs.

Method: Proposes IDPruner using Maximal Marginal Relevance algorithm to achieve Pareto-optimal balance between token importance and diversity, operating without attention maps for FlashAttention compatibility and enabling one-shot pruning.

Result: Achieves SOTA performance across various architectures and benchmarks, retaining 95.18% baseline performance with 75% pruning and 86.40% with 90% pruning on Qwen2.5-VL-7B-Instruct.

Conclusion: IDPruner provides an effective framework for visual token pruning that optimally balances importance and diversity, enabling efficient MLLM inference with minimal performance degradation.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities, yet they encounter significant computational bottlenecks due to the massive volume of visual tokens. Consequently, visual token pruning, which substantially reduces the token count, has emerged as a critical technique for accelerating MLLM inference. Existing approaches focus on token importance, diversity, or an intuitive combination of both, without a principled framework for their optimal integration. To address this issue, we first conduct a systematic analysis to characterize the trade-off between token importance and semantic diversity. Guided by this analysis, we propose the \textbf{I}mportance and \textbf{D}iversity Pruner (\textbf{IDPruner}), which leverages the Maximal Marginal Relevance (MMR) algorithm to achieve a Pareto-optimal balance between these two objectives. Crucially, our method operates without requiring attention maps, ensuring full compatibility with FlashAttention and efficient deployment via one-shot pruning. We conduct extensive experiments across various model architectures and multimodal benchmarks, demonstrating that IDPruner achieves state-of-the-art performance and superior generalization across diverse architectures and tasks. Notably, on Qwen2.5-VL-7B-Instruct, IDPruner retains 95.18\% of baseline performance when pruning 75\% of the tokens, and still maintains 86.40\% even under an extreme 90\% pruning ratio. Our code is available at https://github.com/Tencent/AngelSlim.

</details>


### [21] [Diagnostic Benchmarks for Invariant Learning Dynamics: Empirical Validation of the Eidos Architecture](https://arxiv.org/abs/2602.13322)
*Datorien L. Anderson*

Main category: cs.CV

TL;DR: PSI dataset isolates topological invariance from texture correlations; Eidos architecture achieves >99% accuracy on PSI and 81.67% zero-shot font transfer, validating "Form-First" hypothesis.


<details>
  <summary>Details</summary>
Motivation: Standard vision benchmarks are dominated by textural correlations, making it difficult to isolate and study topological invariance (structural identity across affine transformations). The authors aim to create diagnostic tools to test whether generalization in vision systems comes from geometric integrity rather than statistical scale.

Method: Created PolyShapes-Ideal (PSI) dataset with three diagnostic probes: 1) polygon classification under noise, 2) zero-shot font transfer from MNIST, and 3) geometric collapse mapping under progressive deformation. Tested Eidos architecture on these benchmarks.

Result: Eidos achieved >99% accuracy on PSI benchmarks and 81.67% zero-shot transfer across 30 unseen typefaces without pre-training. This demonstrates strong topological invariance capabilities.

Conclusion: Results validate the "Form-First" hypothesis: generalization in structurally constrained architectures is a property of geometric integrity, not statistical scale. The PSI dataset successfully isolates topological invariance from texture correlations.

Abstract: We present the PolyShapes-Ideal (PSI) dataset, a suite of diagnostic benchmarks designed to isolate topological invariance -- the ability to maintain structural identity across affine transformations -- from the textural correlations that dominate standard vision benchmarks. Through three diagnostic probes (polygon classification under noise, zero-shot font transfer from MNIST, and geometric collapse mapping under progressive deformation), we demonstrate that the Eidos architecture achieves >99% accuracy on PSI and 81.67% zero-shot transfer across 30 unseen typefaces without pre-training. These results validate the "Form-First" hypothesis: generalization in structurally constrained architectures is a property of geometric integrity, not statistical scale.

</details>


### [22] [Synthesizing the Kill Chain: A Zero-Shot Framework for Target Verification and Tactical Reasoning on the Edge](https://arxiv.org/abs/2602.13324)
*Jesse Barkley,Abraham George,Amir Barati Farimani*

Main category: cs.CV

TL;DR: Hierarchical zero-shot framework combines lightweight object detection with compact VLMs for autonomous edge robotics in military environments, achieving high accuracy in false-positive filtering, damage assessment, and vehicle classification with sub-75-second latency.


<details>
  <summary>Details</summary>
Motivation: Autonomous edge robotics in dynamic military environments face two key constraints: scarce domain-specific training data and computational limits of edge hardware. Traditional approaches struggle with these challenges, necessitating a solution that can operate effectively without extensive training while being computationally efficient.

Method: A hierarchical zero-shot framework cascades lightweight object detection (Grounding DINO as text-promptable region proposer) with compact VLMs from Qwen and Gemma families (4B-12B parameters). High-confidence detections are passed to edge-class VLMs for semantic verification. The pipeline extends into an agentic Scout-Commander workflow with a novel "Controlled Input" methodology that decouples perception from reasoning.

Result: Evaluation on 55 high-fidelity synthetic videos from Battlefield 6 shows: false-positive filtering up to 100% accuracy, damage assessment up to 97.5%, fine-grained vehicle classification 55-90%. The agentic workflow achieves 100% correct asset deployment and 9.8/10 reasoning score with sub-75-second latency. Analysis reveals distinct failure patterns: Gemma3-12B excels at tactical logic but fails in visual perception, while Gemma3-4B exhibits reasoning collapse.

Conclusion: Hierarchical zero-shot architectures are validated for edge autonomy, providing a diagnostic framework for certifying VLM suitability in safety-critical applications. The approach addresses both data scarcity and computational constraints while enabling detailed failure analysis for system certification.

Abstract: Deploying autonomous edge robotics in dynamic military environments is constrained by both scarce domain-specific training data and the computational limits of edge hardware. This paper introduces a hierarchical, zero-shot framework that cascades lightweight object detection with compact Vision-Language Models (VLMs) from the Qwen and Gemma families (4B-12B parameters). Grounding DINO serves as a high-recall, text-promptable region proposer, and frames with high detection confidence are passed to edge-class VLMs for semantic verification. We evaluate this pipeline on 55 high-fidelity synthetic videos from Battlefield 6 across three tasks: false-positive filtering (up to 100% accuracy), damage assessment (up to 97.5%), and fine-grained vehicle classification (55-90%). We further extend the pipeline into an agentic Scout-Commander workflow, achieving 100% correct asset deployment and a 9.8/10 reasoning score (graded by GPT-4o) with sub-75-second latency. A novel "Controlled Input" methodology decouples perception from reasoning, revealing distinct failure phenotypes: Gemma3-12B excels at tactical logic but fails in visual perception, while Gemma3-4B exhibits reasoning collapse even with accurate inputs. These findings validate hierarchical zero-shot architectures for edge autonomy and provide a diagnostic framework for certifying VLM suitability in safety-critical applications.

</details>


### [23] [MotionWeaver: Holistic 4D-Anchored Framework for Multi-Humanoid Image Animation](https://arxiv.org/abs/2602.13326)
*Xirui Hu,Yanbo Ding,Jiahao Wang,Tingting Shi,Yali Wang,Guo Zhi Zhi,Weizhan Zhang*

Main category: cs.CV

TL;DR: MotionWeaver is a framework for multi-humanoid image animation that generalizes to diverse humanoid forms and complex interactions using unified motion representations and 4D-anchored supervision.


<details>
  <summary>Details</summary>
Motivation: Existing character animation methods are limited to single-human settings and struggle with multi-humanoid scenarios involving diverse forms, complex interactions, and frequent occlusions.

Method: Two key innovations: 1) Unified motion representations that extract identity-agnostic motions and bind them to characters, 2) Holistic 4D-anchored paradigm that constructs shared 4D space to fuse motion with video latents, reinforced with hierarchical 4D-level supervision.

Result: MotionWeaver achieves state-of-the-art results on a new 300-video benchmark, generalizing effectively across diverse humanoid forms, complex interactions, and challenging multi-humanoid scenarios.

Conclusion: The proposed MotionWeaver framework successfully addresses the gap in multi-humanoid image animation through unified motion representations and 4D-anchored supervision, enabling generalization to diverse humanoid forms and complex interactions.

Abstract: Character image animation, which synthesizes videos of reference characters driven by pose sequences, has advanced rapidly but remains largely limited to single-human settings. Existing methods struggle to generalize to multi-humanoid scenarios, which involve diverse humanoid forms, complex interactions, and frequent occlusions. We address this gap with two key innovations. First, we introduce unified motion representations that extract identity-agnostic motions and explicitly bind them to corresponding characters, enabling generalization across diverse humanoid forms and seamless extension to multi-humanoid scenarios. Second, we propose a holistic 4D-anchored paradigm that constructs a shared 4D space to fuse motion representations with video latents, and further reinforces this process with hierarchical 4D-level supervision to better handle interactions and occlusions. We instantiate these ideas in MotionWeaver, an end-to-end framework for multi-humanoid image animation. To support this setting, we curate a 46-hour dataset of multi-human videos with rich interactions, and construct a 300-video benchmark featuring paired humanoid characters. Quantitative and qualitative experiments demonstrate that MotionWeaver not only achieves state-of-the-art results on our benchmark but also generalizes effectively across diverse humanoid forms, complex interactions, and challenging multi-humanoid scenarios.

</details>


### [24] [HiST-VLA: A Hierarchical Spatio-Temporal Vision-Language-Action Model for End-to-End Autonomous Driving](https://arxiv.org/abs/2602.13329)
*Yiru Wang,Zichong Gu,Yu Gao,Anqing Jiang,Zhigang Sun,Shuo Wang,Yuwen Heng,Hao Sun*

Main category: cs.CV

TL;DR: HiST-VLA is a hierarchical spatio-temporal VLA model for autonomous driving that improves trajectory generation through geometric awareness, dynamic token sparsification, and hierarchical planning with dynamic latent regularization.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language-Action (VLA) models for autonomous driving have limitations including imprecise numerical reasoning, weak 3D spatial awareness, and high sensitivity to context, which constrain their use in safety-critical scenarios.

Method: The framework integrates geometric awareness with fine-grained driving commands and state history prompting. It uses dynamic token sparsification (fusing redundant tokens) to reduce computational cost, and a hierarchical transformer-based planner that refines coarse VLA waypoints into fine-grained trajectories using dynamic latent regularization for spatial grounding.

Result: State-of-the-art performance on NAVSIM v2 benchmark: EPDMS of 88.6 on Navtest and 50.9 on pseudo closed-loop Navhard benchmark.

Conclusion: HiST-VLA addresses key limitations of VLA models for autonomous driving through hierarchical spatio-temporal reasoning and efficient architecture, achieving superior trajectory generation performance while maintaining computational efficiency.

Abstract: Vision-Language-Action (VLA) models offer promising capabilities for autonomous driving through multimodal understanding. However, their utilization in safety-critical scenarios is constrained by inherent limitations, including imprecise numerical reasoning, weak 3D spatial awareness, and high sensitivity to context. To address these challenges, we propose HiST-VLA, a novel Hierarchical Spatio-Temporal VLA model designed for reliable trajectory generation.
  Our framework enhances 3D spatial and temporal reasoning by integrating geometric awareness with fine-grained driving commands and state history prompting. To ensure computational efficiency, we integrate dynamic token sparsification into the VLA architecture. This approach fuses redundant tokens rather than filtering them, effectively reducing redundancy without sacrificing model performance. Furthermore, we employ a hierarchical transformer-based planner to progressively refine coarse VLA waypoints into fine-grained trajectories. Crucially, the planner utilizes dynamic latent regularization to incorporate language commands, ensuring strict spatial grounding and temporal coherence. Extensive evaluation on the NAVSIM v2 benchmark demonstrates state-of-the-art performance on Navtest, achieving an EPDMS of 88.6, and EPDMS of 50.9 on pseudo closed-loop Navhard benchmark.

</details>


### [25] [Zwitscherkasten -- DIY Audiovisual bird monitoring](https://arxiv.org/abs/2602.13330)
*Dominik Blum,Elias Häring,Fabian Jirges,Martin Schäffer,David Schick,Florian Schulenberg,Torsten Schön*

Main category: cs.CV

TL;DR: Zwitscherkasten is a DIY multimodal system for bird monitoring using audio and visual data on edge devices, enabling real-time species identification on resource-constrained hardware.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enable scalable, non-invasive biodiversity monitoring and support citizen science applications by developing an accessible system that can run on low-cost, resource-constrained hardware for bird species identification.

Method: The system uses deep learning models for both bioacoustic (audio) and image-based classification deployed on edge devices. It includes an acoustic activity detector to reduce energy consumption, and employs fine-grained detection and classification pipelines for visual recognition.

Result: Results demonstrate that accurate bird species identification is feasible on embedded platforms, showing the system's effectiveness for real-time monitoring applications.

Conclusion: Zwitscherkasten successfully enables scalable biodiversity monitoring through edge computing, making bird species identification accessible for citizen science and environmental monitoring applications using resource-constrained hardware.

Abstract: This paper presents Zwitscherkasten, a DiY, multimodal system for bird species monitoring using audio and visual data on edge devices. Deep learning models for bioacoustic and image-based classification are deployed on resource-constrained hardware, enabling real-time, non-invasive monitoring. An acoustic activity detector reduces energy consumption, while visual recognition is performed using fine-grained detection and classification pipelines. Results show that accurate bird species identification is feasible on embedded platforms, supporting scalable biodiversity monitoring and citizen science applications.

</details>


### [26] [MedScope: Incentivizing "Think with Videos" for Clinical Reasoning via Coarse-to-Fine Tool Calling](https://arxiv.org/abs/2602.13332)
*Wenjie Li,Yujie Zhang,Haoran Sun,Xingqi He,Hongcheng Gao,Chenglong Ma,Ming Hu,Guankun Wang,Shiyi Yao,Renhao Yang,Hongliang Ren,Lei Wang,Junjun He,Yankai Jiang*

Main category: cs.CV

TL;DR: MedScope is a tool-using clinical video reasoning model that performs coarse-to-fine evidence seeking over long-form medical videos, achieving state-of-the-art performance through grounding-aware optimization.


<details>
  <summary>Details</summary>
Motivation: Current multimodal LLMs process clinical videos with passive sampling or weakly grounded inspection, limiting their ability to iteratively locate, verify, and justify predictions with temporally targeted evidence in long-form medical procedures.

Method: Proposes MedScope with coarse-to-fine evidence seeking, interleaving reasoning with targeted tool calls and verification. Uses ClinVideoSuite dataset and optimizes with Grounding-Aware Group Relative Policy Optimization (GA-GRPO) that reinforces tool use with grounding-aligned rewards.

Result: Achieves state-of-the-art performance on both full and fine-grained video understanding benchmarks in both in-domain and out-of-domain evaluations.

Conclusion: MedScope illuminates a path toward medical AI agents that can genuinely "think with videos" through tool-integrated reasoning, enabling more accurate and trustworthy predictions explicitly grounded in temporally localized visual evidence.

Abstract: Long-form clinical videos are central to visual evidence-based decision-making, with growing importance for applications such as surgical robotics and related settings. However, current multimodal large language models typically process videos with passive sampling or weakly grounded inspection, which limits their ability to iteratively locate, verify, and justify predictions with temporally targeted evidence. To close this gap, we propose MedScope, a tool-using clinical video reasoning model that performs coarse-to-fine evidence seeking over long-form procedures. By interleaving intermediate reasoning with targeted tool calls and verification on retrieved observations, MedScope produces more accurate and trustworthy predictions that are explicitly grounded in temporally localized visual evidence. To address the lack of high-fidelity supervision, we build ClinVideoSuite, an evidence-centric, fine-grained clinical video suite. We then optimize MedScope with Grounding-Aware Group Relative Policy Optimization (GA-GRPO), which directly reinforces tool use with grounding-aligned rewards and evidence-weighted advantages. On full and fine-grained video understanding benchmarks, MedScope achieves state-of-the-art performance in both in-domain and out-of-domain evaluations. Our approach illuminates a path toward medical AI agents that can genuinely "think with videos" through tool-integrated reasoning. We will release our code, models, and data.

</details>


### [27] [Ask the Expert: Collaborative Inference for Vision Transformers with Near-Edge Accelerators](https://arxiv.org/abs/2602.13334)
*Hao Liu,Suhaib A. Fahmy*

Main category: cs.CV

TL;DR: A collaborative inference framework using edge-based generalist ViT and near-edge expert ViTs with dynamic routing reduces latency by 45% and energy by 46% compared to alternatives.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers are computationally heavy for edge devices, while full cloud offloading introduces significant latency overheads, creating a need for efficient edge-cloud collaboration.

Method: Proposes a collaborative framework with lightweight generalist ViT on edge device and multiple medium-sized expert ViTs on near-edge accelerator. Uses Top-k predictions from edge model to dynamically route low-confidence samples to relevant experts, plus progressive specialist training strategy.

Result: Training strategy improves expert specialization accuracy by 4.12% on target subsets and overall accuracy by 2.76% over static experts. Reduces latency by up to 45% compared to edge execution, and energy consumption by up to 46% compared to near-edge offload.

Conclusion: The collaborative inference framework effectively balances accuracy, latency, and energy consumption for Vision Transformer deployment on edge devices through intelligent edge-near-edge coordination and specialized training.

Abstract: Deploying Vision Transformers on edge devices is challenging due to their high computational complexity, while full offloading to cloud resources presents significant latency overheads. We propose a novel collaborative inference framework, which orchestrates a lightweight generalist ViT on an edge device and multiple medium-sized expert ViTs on a near-edge accelerator. A novel routing mechanism uses the edge model's Top-$\mathit{k}$ predictions to dynamically select the most relevant expert for samples with low confidence. We further design a progressive specialist training strategy to enhance expert accuracy on dataset subsets. Extensive experiments on the CIFAR-100 dataset using a real-world edge and near-edge testbed demonstrate the superiority of our framework. Specifically, the proposed training strategy improves expert specialization accuracy by 4.12% on target subsets and enhances overall accuracy by 2.76% over static experts. Moreover, our method reduces latency by up to 45% compared to edge execution, and energy consumption by up to 46% compared to just near-edge offload.

</details>


### [28] [Meningioma Analysis and Diagnosis using Limited Labeled Samples](https://arxiv.org/abs/2602.13335)
*Jiamiao Lu,Wei Wu,Ke Gao,Ping Mao,Weichuan Zhang,Tuo Wang,Lingkun Ma,Jiapan Guo,Zanyi Wu,Yuqing Hu,Changming Sun*

Main category: cs.CV

TL;DR: The paper proposes AMSF-Net, an adaptive multi-scale feature fusion network for few-shot meningioma classification using MRI, which dynamically weights spatial and frequency domain features to improve diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate meningioma grading is crucial for treatment planning and prognosis, but current methods may not optimally leverage both spatial and frequency domain information, especially in few-shot learning scenarios where labeled data is limited.

Method: Proposes AMSF-Net with adaptive weighting of spatial and frequency domain features extracted via discrete wavelet transform. The network learns to dynamically balance contributions from different frequency bands and spatial information for few-shot meningioma classification.

Result: The method outperforms state-of-the-art approaches on three datasets, including a newly introduced meningioma MRI dataset, demonstrating superior classification performance for meningioma grading.

Conclusion: Adaptive fusion of spatial-frequency domain features significantly improves meningioma classification, particularly in few-shot learning settings, offering a promising approach for accurate diagnosis with limited labeled data.

Abstract: The biological behavior and treatment response of meningiomas depend on their grade, making an accurate diagnosis essential for treatment planning and prognosis assessment. We observed that the weighted fusion of spatial-frequency domain features significantly influences meningioma classification performance. Notably, the contribution of specific frequency bands obtained by discrete wavelet transform varies considerably across different images. A feature fusion architecture with adaptive weights of different frequency band information and spatial domain information is proposed for few-shot meningioma learning. To verify the effectiveness of the proposed method, a new MRI dataset of meningiomas is introduced. The experimental results demonstrate the superiority of the proposed method compared with existing state-of-the-art methods in three datasets. The code will be available at: https://github.com/ICL-SUST/AMSF-Net

</details>


### [29] [An Integrated Causal Inference Framework for Traffic Safety Modeling with Semantic Street-View Visual Features](https://arxiv.org/abs/2602.13339)
*Lishan Sun,Yujia Cheng,Pengfei Cui,Lei Han,Mohamed Abdel-Aty,Yunhan Zheng,Xingchen Zhang*

Main category: cs.CV

TL;DR: This paper uses Google Street View imagery and Double Machine Learning to establish causal evidence that urban greenery reduces traffic crashes, with spatial heterogeneity in its protective effects.


<details>
  <summary>Details</summary>
Motivation: Current macroscopic traffic safety models rely on static sociodemographic and infrastructure metrics, overlooking drivers' visual perception of the environment. While visual features are known to impact crashes, existing evidence is observational and lacks robust causality for policy evaluation.

Method: Applied semantic segmentation on 220,000 Google Street View images to extract visual environmental features, then used Double Machine Learning framework to quantify causal effects on regional crashes. Used SHAP values to characterize nonlinear influence mechanisms and causal forests for conditional average treatment effects. Analyzed crash records from Miami metropolitan area.

Result: Greenery proportion has significant negative causal effect on traffic crashes (Average Treatment Effect = -6.38, p = 0.005). Protective effect is spatially heterogeneous, strongest in densely populated and socially vulnerable urban cores. Greenery mitigates angle and rear-end crashes but has limited protective benefit for vulnerable road users.

Conclusion: Provides causal evidence for greening as a potential safety intervention, suggesting prioritization of hazardous visual environments while highlighting need for distinct design optimizations to protect vulnerable road users.

Abstract: Macroscopic traffic safety modeling aims to identify critical risk factors for regional crashes, thereby informing targeted policy interventions for safety improvement. However, current approaches rely heavily on static sociodemographic and infrastructure metrics, frequently overlooking the impacts from drivers' visual perception of driving environment. Although visual environment features have been found to impact driving and traffic crashes, existing evidence remains largely observational, failing to establish the robust causality for traffic policy evaluation under complex spatial environment. To fill these gaps, we applied semantic segmentation on Google Street View imageries to extract visual environmental features and proposed a Double Machine Learning framework to quantify their causal effects on regional crashes. Meanwhile, we utilized SHAP values to characterize the nonlinear influence mechanisms of confounding variables in the models and applied causal forests to estimate conditional average treatment effects. Leveraging crash records from the Miami metropolitan area, Florida, and 220,000 street view images, evidence shows that greenery proportion exerts a significant and robust negative causal effect on traffic crashes (Average Treatment Effect = -6.38, p = 0.005). This protective effect exhibits spatial heterogeneity, being most pronounced in densely populated and socially vulnerable urban cores. While greenery significantly mitigates angle and rear-end crashes, its protective benefit for vulnerable road users (VRUs) remains limited. Our findings provide causal evidence for greening as a potential safety intervention, prioritizing hazardous visual environments while highlighting the need for distinct design optimizations to protect VRUs.

</details>


### [30] [Visual Foresight for Robotic Stow: A Diffusion-Based World Model from Sparse Snapshots](https://arxiv.org/abs/2602.13347)
*Lijun Zhang,Nikhil Chacko,Petter Nilsson,Ruinian Xu,Shantanu Thakar,Bai Lou,Harpreet Sawhney,Zhebin Zhang,Mudit Agrawal,Bhavana Chandrashekhar,Aaron Parness*

Main category: cs.CV

TL;DR: FOREST is a stow-intent-conditioned world model that predicts post-stow bin configurations in automated warehouses using latent diffusion transformers, improving geometric accuracy over baselines.


<details>
  <summary>Details</summary>
Motivation: Automated warehouses need to anticipate how bins will look after stow operations before actual execution for better planning and decision-making.

Method: Represents bin states as item-aligned instance masks and uses a latent diffusion transformer to predict post-stow configurations from observed context and planned stow behavior.

Result: FOREST substantially improves geometric agreement between predicted and true post-stow layouts compared to heuristic baselines, with only modest performance loss in downstream tasks when replacing real masks with predictions.

Conclusion: FOREST provides useful foresight signals for warehouse planning, enabling effective load-quality assessment and multi-stow reasoning through accurate bin configuration predictions.

Abstract: Automated warehouses execute millions of stow operations, where robots place objects into storage bins. For these systems it is valuable to anticipate how a bin will look from the current observations and the planned stow behavior before real execution. We propose FOREST, a stow-intent-conditioned world model that represents bin states as item-aligned instance masks and uses a latent diffusion transformer to predict the post-stow configuration from the observed context. Our evaluation shows that FOREST substantially improves the geometric agreement between predicted and true post-stow layouts compared with heuristic baselines. We further evaluate the predicted post-stow layouts in two downstream tasks, in which replacing the real post-stow masks with FOREST predictions causes only modest performance loss in load-quality assessment and multi-stow reasoning, indicating that our model can provide useful foresight signals for warehouse planning.

</details>


### [31] [From Prompt to Production:Automating Brand-Safe Marketing Imagery with Text-to-Image Models](https://arxiv.org/abs/2602.13349)
*Parmida Atighehchian,Henry Wang,Andrei Kapustin,Boris Lerner,Tiancheng Jiang,Taylor Jensen,Negin Sokhandan*

Main category: cs.CV

TL;DR: A scalable production pipeline for text-to-image models that balances automation with human oversight to generate marketing images with improved fidelity and human preference.


<details>
  <summary>Details</summary>
Motivation: While text-to-image models produce impressive results, deploying them at scale in production environments remains challenging. The key problem is balancing automation for scalability with human feedback to maintain quality and align with creative vision, especially for commercial marketing applications.

Method: The paper presents a new pipeline that offers a fully automated, scalable solution for generating marketing images of commercial products using text-to-image models. The system maintains image quality and fidelity while introducing creative variation that adheres to marketing guidelines, streamlining the process to blend efficiency with human oversight.

Result: The proposed system achieves a 30.77% increase in marketing object fidelity using DINOV2 and a 52.00% increase in human preference over the generated outcomes compared to previous approaches.

Conclusion: The paper demonstrates a successful scalable pipeline for production deployment of text-to-image models that effectively balances automation with human oversight, resulting in significant improvements in both technical fidelity metrics and human preference for marketing applications.

Abstract: Text-to-image models have made significant strides, producing impressive results in generating images from textual descriptions. However, creating a scalable pipeline for deploying these models in production remains a challenge. Achieving the right balance between automation and human feedback is critical to maintain both scale and quality. While automation can handle large volumes, human oversight is still an essential component to ensure that the generated images meet the desired standards and are aligned with the creative vision. This paper presents a new pipeline that offers a fully automated, scalable solution for generating marketing images of commercial products using text-to-image models. The proposed system maintains the quality and fidelity of images, while also introducing sufficient creative variation to adhere to marketing guidelines. By streamlining this process, we ensure a seamless blend of efficiency and human oversight, achieving a $30.77\%$ increase in marketing object fidelity using DINOV2 and a $52.00\%$ increase in human preference over the generated outcome.

</details>


### [32] [Detecting Brick Kiln Infrastructure at Scale: Graph, Foundation, and Remote Sensing Models for Satellite Imagery Data](https://arxiv.org/abs/2602.13350)
*Usman Nazir,Xidong Chen,Hafiz Muhammad Abubakar,Hadia Abu Bakar,Raahim Arbaz,Fezan Rasool,Bin Chen,Sara Khalid*

Main category: cs.CV

TL;DR: Researchers develop ClimateGraph, a graph-based model for detecting brick kilns from high-resolution satellite imagery across South/Central Asia, comparing it with foundation models and remote sensing approaches.


<details>
  <summary>Details</summary>
Motivation: Brick kilns are major sources of air pollution and forced labor in South Asia, but large-scale monitoring is limited by sparse and outdated ground data. Current detection methods lack scalability and accuracy.

Method: Created a multi-city dataset of 1.3M+ high-resolution (0.149m/pixel) satellite image tiles across 5 regions. Proposed ClimateGraph, a region-adaptive graph-based model that captures spatial and directional structure in kiln layouts. Evaluated against graph learning baselines, remote sensing detection pipelines, and recent satellite imagery foundation models.

Result: The study demonstrates complementary strengths across graph-based, foundation model, and remote sensing approaches. ClimateGraph effectively captures spatial patterns in kiln layouts, providing practical solutions for scalable monitoring.

Conclusion: The research provides practical guidance for scalable brick kiln monitoring from satellite imagery by leveraging multiple complementary approaches, with ClimateGraph offering region-adaptive spatial pattern recognition capabilities.

Abstract: Brick kilns are a major source of air pollution and forced labor in South Asia, yet large-scale monitoring remains limited by sparse and outdated ground data. We study brick kiln detection at scale using high-resolution satellite imagery and curate a multi city zoom-20 (0.149 meters per pixel) resolution dataset comprising over 1.3 million image tiles across five regions in South and Central Asia. We propose ClimateGraph, a region-adaptive graph-based model that captures spatial and directional structure in kiln layouts, and evaluate it against established graph learning baselines. In parallel, we assess a remote sensing based detection pipeline and benchmark it against recent foundation models for satellite imagery. Our results highlight complementary strengths across graph, foundation, and remote sensing approaches, providing practical guidance for scalable brick kiln monitoring from satellite imagery.

</details>


### [33] [Using Deep Learning to Generate Semantically Correct Hindi Captions](https://arxiv.org/abs/2602.13352)
*Wasim Akram Khan,Anil Kumar Vuppala*

Main category: cs.CV

TL;DR: This paper presents an automated image captioning system for Hindi language using multimodal architectures with attention mechanisms and pre-trained CNNs, achieving best results with attention-based bidirectional LSTM and VGG16.


<details>
  <summary>Details</summary>
Motivation: Most image captioning research focuses on English, creating a need for development in popular foreign languages like Hindi, the world's fourth most spoken language.

Method: Used Flickr8k dataset with Google Cloud Translator for Hindi captions. Employed pre-trained CNNs (VGG16, ResNet50, Inception V3) for visual features, uni/bi-directional text encoding, and attention mechanisms to combine image characteristics into sentence-level features.

Result: Attention-based bidirectional LSTM with VGG16 achieved best BLEU scores: 0.59 for BLEU-1 and 0.19 for BLEU-4, demonstrating ability to produce relevant, semantically accurate Hindi image captions.

Conclusion: The research successfully developed Hindi image captioning system and provides a model for future research in multilingual captioning.

Abstract: Automated image captioning using the content from the image is very appealing when done by harnessing the capability of computer vision and natural language processing. Extensive research has been done in the field with a major focus on the English language which gives the scope for further developments in the same with consideration of popular foreign languages. This research utilizes distinct models for translating the image caption into Hindi, the fourth most popular language across the world. Exploring the multi-modal architectures this research comprises local visual features, global visual features, attention mechanisms, and pre-trained models. Using google cloud translator on the image dataset from Flickr8k, Hindi image descriptions have been generated. Pre-trained CNNs like VGG16, ResNet50, and Inception V3 helped in retrieving image characteristics, while the uni-directional and bi-directional techniques of text encoding are used for the text encoding process. An additional Attention layer helps to generate a weight vector and, by multiplying it, combine image characteristics from each time step into a sentence-level feature vector. Bilingual evaluation understudy scores are used to compare the research outcome. Many experiments that serve as a baseline are done for the comparative analysis of the research. An image with a score of BLEU-1 is considered sufficient, whereas one with a score of BLEU-4 is considered to have fluid image captioning. For both BLEU scores, the attention-based bidirectional LSTM with VGG16 produced the best results of 0.59 and 0.19 respectively. The experiments conclude that researchs ability to produce relevant, semantically accurate image captions in Hindi. The research accomplishes the goals and future research can be guided by this research model.

</details>


### [34] [AdaCorrection: Adaptive Offset Cache Correction for Accurate Diffusion Transformers](https://arxiv.org/abs/2602.13357)
*Dong Liu,Yanxuan Yu,Ben Lengerich,Ying Nian Wu*

Main category: cs.CV

TL;DR: AdaCorrection is an adaptive cache correction framework for Diffusion Transformers that improves inference efficiency while maintaining generation quality by dynamically blending cached and fresh activations based on spatio-temporal signals.


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers (DiTs) have state-of-the-art image/video generation quality but suffer from expensive iterative inference. Existing cache acceleration methods use static reuse schedules or coarse heuristics, causing temporal drift and cache misalignment that degrade generation quality.

Method: AdaCorrection uses lightweight spatio-temporal signals to estimate cache validity at each timestep, then adaptively blends cached and fresh activations. This on-the-fly correction requires no additional supervision or retraining.

Result: The approach maintains near-original FID scores while providing moderate acceleration. Experiments on image and video diffusion benchmarks show consistent improvement in generation performance with minimal computational overhead.

Conclusion: AdaCorrection enables efficient cache reuse across Transformer layers during diffusion inference while preserving high generation fidelity, addressing the trade-off between acceleration and quality in DiT inference.

Abstract: Diffusion Transformers (DiTs) achieve state-of-the-art performance in high-fidelity image and video generation but suffer from expensive inference due to their iterative denoising structure. While prior methods accelerate sampling by caching intermediate features, they rely on static reuse schedules or coarse-grained heuristics, which often lead to temporal drift and cache misalignment that significantly degrade generation quality. We introduce \textbf{AdaCorrection}, an adaptive offset cache correction framework that maintains high generation fidelity while enabling efficient cache reuse across Transformer layers during diffusion inference. At each timestep, AdaCorrection estimates cache validity with lightweight spatio-temporal signals and adaptively blends cached and fresh activations. This correction is computed on-the-fly without additional supervision or retraining. Our approach achieves strong generation quality with minimal computational overhead, maintaining near-original FID while providing moderate acceleration. Experiments on image and video diffusion benchmarks show that AdaCorrection consistently improves generation performance.

</details>


### [35] [The Diffusion Duet: Harmonizing Dual Channels with Wavelet Suppression for Image Separation](https://arxiv.org/abs/2602.13361)
*Jingwei Li,Wei Pu*

Main category: cs.CV

TL;DR: DCDSM introduces diffusion models to blind image separation, achieving SOTA performance for rain/snow removal and complex mixture separation through wavelet suppression and dual-channel architecture.


<details>
  <summary>Details</summary>
Motivation: Traditional BIS methods struggle with complex real-world scenes due to statistical independence assumptions and CNN/GAN limitations, leading to estimation bias, texture distortion, and artifact residue under strong noise and nonlinear mixing conditions.

Method: Proposes Dual-Channel Diffusion Separation Model (DCDSM) that leverages diffusion models' generative capability to learn source image distributions. Includes novel Wavelet Suppression Module (WSM) in dual-branch reverse denoising process to exploit mutual coupling noise characteristics between source images for enhanced detail separation.

Result: Achieves SOTA performance: 1) Rain removal: 35.0023 dB/0.9549 PSNR/SSIM; Snow removal: 29.8108 dB/0.9243; 2) Complex mixture separation: average 25.0049 dB PSNR and 0.7997 SSIM, surpassing comparative methods by 4.1249 dB and 0.0926 respectively.

Conclusion: DCDSM demonstrates superior performance in addressing rain/snow residue removal and detail preservation challenges, confirming the effectiveness of diffusion models for blind image separation tasks through both subjective and objective evaluations.

Abstract: Blind image separation (BIS) refers to the inverse problem of simultaneously estimating and restoring multiple independent source images from a single observation image under conditions of unknown mixing mode and without prior knowledge of the source images. Traditional methods relying on statistical independence assumptions or CNN/GAN variants struggle to characterize complex feature distributions in real scenes, leading to estimation bias, texture distortion, and artifact residue under strong noise and nonlinear mixing. This paper innovatively introduces diffusion models into dual-channel BIS, proposing an efficient Dual-Channel Diffusion Separation Model (DCDSM). DCDSM leverages diffusion models' powerful generative capability to learn source image feature distributions and reconstruct feature structures effectively. A novel Wavelet Suppression Module (WSM) is designed within the dual-branch reverse denoising process, forming an interactive separation network that enhances detail separation by exploiting the mutual coupling noise characteristic between source images. Extensive experiments on synthetic datasets containing rain/snow and complex mixtures demonstrate that DCDSM achieves state-of-the-art performance: 1) In image restoration tasks, it obtains PSNR/SSIM values of 35.0023 dB/0.9549 and 29.8108 dB/0.9243 for rain and snow removal respectively, outperforming Histoformer and LDRCNet by 1.2570 dB/0.9272 dB (PSNR) and 0.0262/0.0289 (SSIM) on average; 2) For complex mixture separation, the restored dual-source images achieve average PSNR and SSIM of 25.0049 dB and 0.7997, surpassing comparative methods by 4.1249 dB and 0.0926. Both subjective and objective evaluations confirm DCDSM's superiority in addressing rain/snow residue removal and detail preservation challenges.

</details>


### [36] [An Online Reference-Free Evaluation Framework for Flowchart Image-to-Code Generation](https://arxiv.org/abs/2602.13376)
*Giang Son Nguyen,Zi Pong Lim,Sarthak Ketanbhai Modi,Yon Shin Teo,Wenya Wang*

Main category: cs.CV

TL;DR: Proposes a reference-free evaluation framework for flowchart image-to-code generation using OCR-based recall and visual entailment-based precision metrics.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models are increasingly used to convert flowchart images to structured code in production, but there's no ground-truth for arbitrary inputs, making quality assessment difficult.

Method: Uses two automated metrics: Recall_OCR (content coverage via OCR text extraction as proxy reference) and Precision_VE (hallucination detection via Visual Entailment against original image), with harmonic mean F1_OCR-VE as unified score.

Result: Validation on FlowVQA dataset shows strong agreement with ground-truth metrics (Pearson's r = 0.97, 0.91, 0.94 for Recall, Precision, and F1 respectively).

Conclusion: The framework provides a practical, reference-free alternative for continuous quality monitoring in production settings for flowchart image-to-code generation systems.

Abstract: Vision-Language Models (VLMs) are increasingly used in document processing pipelines to convert flowchart images into structured code (e.g., Mermaid). In production, these systems process arbitrary inputs for which no ground-truth code exists, making output quality difficult to assess. We propose a reference-free evaluation framework that monitors flowchart image-to-code generation quality at inference time, using only the input image and the generated output. The framework introduces two automated metrics: $\text{Recall}{\text{OCR}}$, which estimates content coverage by extracting text from the input image via OCR as a proxy reference, and $\text{Precision}{\text{VE}}$, which detects hallucinated elements through Visual Entailment against the original image. Their harmonic mean, $\text{F1}{\text{OCR-VE}}$, provides a unified quality score. Validation on the FlowVQA dataset shows strong agreement with ground-truth metrics (average Pearson's $r = 0.97$, $0.91$, and $0.94$ for Recall, Precision, and F1, respectively), confirming the framework's reliability as a practical, reference-free alternative for continuous quality monitoring in production settings.

</details>


### [37] [LAF-YOLOv10 with Partial Convolution Backbone, Attention-Guided Feature Pyramid, Auxiliary P2 Head, and Wise-IoU Loss for Small Object Detection in Drone Aerial Imagery](https://arxiv.org/abs/2602.13378)
*Sohail Ali Farooqui,Zuhair Ahmed Khan Taha,Mohammed Mudassir Uddin,Shahnawaz Alam*

Main category: cs.CV

TL;DR: LAF-YOLOv10 improves small-object detection for UAV imagery by integrating four complementary techniques into YOLOv10n, achieving better accuracy with minimal parameter increase while maintaining real-time performance on embedded hardware.


<details>
  <summary>Details</summary>
Motivation: Current object detectors struggle with UAV-specific challenges: extremely small targets (few pixels), cluttered backgrounds, heavy occlusion, and strict onboard computational constraints for deployment on drones.

Method: Built on YOLOv10n, integrates four techniques: 1) Partial Convolution C2f (PC-C2f) reduces redundant computation, 2) Attention-Guided FPN with SE gates and DySample for better feature fusion, 3) Auxiliary P2 detection head for sub-8×8 pixel objects, 4) Wise-IoU v3 for robust regression under noisy annotations.

Result: Achieves 35.1±0.3% mAP@0.5 on VisDrone-DET2019 (3.3 points over YOLOv10n) with 2.3M parameters, 35.8±0.4% mAP@0.5 on UAVDT, and runs at 24.3 FPS on Jetson Orin Nano at FP16 precision.

Conclusion: The integrated approach effectively addresses multiple UAV detection bottlenecks simultaneously, demonstrating practical viability for embedded deployment while improving small-object detection accuracy without novel individual components.

Abstract: Unmanned aerial vehicles serve as primary sensing platforms for surveillance, traffic monitoring, and disaster response, making aerial object detection a central problem in applied computer vision. Current detectors struggle with UAV-specific challenges: targets spanning only a few pixels, cluttered backgrounds, heavy occlusion, and strict onboard computational budgets. This study introduces LAF-YOLOv10, built on YOLOv10n, integrating four complementary techniques to improve small-object detection in drone imagery. A Partial Convolution C2f (PC-C2f) module restricts spatial convolution to one quarter of backbone channels, reducing redundant computation while preserving discriminative capacity. An Attention-Guided Feature Pyramid Network (AG-FPN) inserts Squeeze-and-Excitation channel gates before multi-scale fusion and replaces nearest-neighbor upsampling with DySample for content-aware interpolation. An auxiliary P2 detection head at 160$\times$160 resolution extends localization to objects below 8$\times$8 pixels, while the P5 head is removed to redistribute parameters. Wise-IoU v3 replaces CIoU for bounding box regression, attenuating gradients from noisy annotations in crowded aerial scenes. The four modules address non-overlapping bottlenecks: PC-C2f compresses backbone computation, AG-FPN refines cross-scale fusion, the P2 head recovers spatial resolution, and Wise-IoU stabilizes regression under label noise. No individual component is novel; the contribution is the joint integration within a single YOLOv10 framework. Across three training runs (seeds 42, 123, 256), LAF-YOLOv10 achieves 35.1$\pm$0.3\% mAP@0.5 on VisDrone-DET2019 with 2.3\,M parameters, exceeding YOLOv10n by 3.3 points. Cross-dataset evaluation on UAVDT yields 35.8$\pm$0.4\% mAP@0.5. Benchmarks on NVIDIA Jetson Orin Nano confirm 24.3 FPS at FP16, demonstrating viability for embedded UAV deployment.

</details>


### [38] [Handling Supervision Scarcity in Chest X-ray Classification: Long-Tailed and Zero-Shot Learning](https://arxiv.org/abs/2602.13430)
*Ha-Hieu Pham,Hai-Dang Nguyen,Thanh-Huy Nguyen,Min Xu,Ulas Bagci,Trung-Nghia Le,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: The paper presents solutions for CXR classification addressing imperfect supervision: Task 1 handles long-tailed multi-label distributions with imbalance-aware learning, while Task 2 performs zero-shot OOD recognition without using OOD class examples during training.


<details>
  <summary>Details</summary>
Motivation: Clinical CXR classification faces two key challenges: (1) extreme long-tailed multi-label disease distributions where rare findings have limited examples, and (2) missing annotations for rare or previously unseen findings that require zero-shot recognition.

Method: For Task 1 (long-tailed multi-label classification): Adopt imbalance-aware multi-label learning strategy to improve tail class recognition while maintaining performance on frequent findings. For Task 2 (zero-shot OOD recognition): Propose prediction approach that produces scores for unseen disease categories without using any supervised labels or examples from OOD classes during training.

Result: Achieves strong performance on both tasks evaluated with macro-averaged mean Average Precision (mAP), ranking first on the public leaderboard of the development phase of the CXR-LT 2026 challenge.

Conclusion: The proposed task-specific solutions effectively address the imperfect supervision challenges in CXR classification, demonstrating robust performance on both long-tailed multi-label classification and zero-shot OOD recognition tasks.

Abstract: Chest X-ray (CXR) classification in clinical practice is often limited by imperfect supervision, arising from (i) extreme long-tailed multi-label disease distributions and (ii) missing annotations for rare or previously unseen findings. The CXR-LT 2026 challenge addresses these issues on a PadChest-based benchmark with a 36-class label space split into 30 in-distribution classes for training and 6 out-of-distribution (OOD) classes for zero-shot evaluation. We present task-specific solutions tailored to the distinct supervision regimes. For Task 1 (long-tailed multi-label classification), we adopt an imbalance-aware multi-label learning strategy to improve recognition of tail classes while maintaining stable performance on frequent findings. For Task 2 (zero-shot OOD recognition), we propose a prediction approach that produces scores for unseen disease categories without using any supervised labels or examples from the OOD classes during training. Evaluated with macro-averaged mean Average Precision (mAP), our method achieves strong performance on both tasks, ranking first on the public leaderboard of the development phase. Code and pre-trained models are available at https://github.com/hieuphamha19/CXR_LT.

</details>


### [39] [Learning on the Fly: Replay-Based Continual Object Perception for Indoor Drones](https://arxiv.org/abs/2602.13440)
*Sebastian-Ion Nae,Mihai-Eugen Barbu,Sebastian Mocanu,Marius Leordeanu*

Main category: cs.CV

TL;DR: Researchers introduce an indoor UAV video dataset and benchmark replay-based continual learning methods for object detection in resource-constrained aerial systems.


<details>
  <summary>Details</summary>
Motivation: Autonomous indoor drones need to learn new object classes in real-time while avoiding catastrophic forgetting, but existing UAV datasets focus on outdoor scenes and lack temporally coherent indoor videos.

Method: Created an indoor dataset of 14,400 frames with inter-drone and ground vehicle footage using semi-automatic annotation workflow. Benchmarked 3 replay-based CIL strategies (ER, MIR, FAR) using YOLOv11-nano detector under tight memory budgets (5-10% replay). Used Grad-CAM for attention analysis.

Result: FAR performed best under tight memory budgets, achieving 82.96% average accuracy with 5% replay. Grad-CAM analysis revealed attention shifts across classes in mixed scenes, associated with reduced localization quality for drones.

Conclusion: Replay-based continual learning can be effectively applied to edge aerial systems. The work contributes an indoor UAV video dataset with temporal coherence and evaluation of replay-based CIL under limited replay budgets.

Abstract: Autonomous agents such as indoor drones must learn new object classes in real-time while limiting catastrophic forgetting, motivating Class-Incremental Learning (CIL). However, most unmanned aerial vehicle (UAV) datasets focus on outdoor scenes and offer limited temporally coherent indoor videos. We introduce an indoor dataset of $14,400$ frames capturing inter-drone and ground vehicle footage, annotated via a semi-automatic workflow with a $98.6\%$ first-pass labeling agreement before final manual verification. Using this dataset, we benchmark 3 replay-based CIL strategies: Experience Replay (ER), Maximally Interfered Retrieval (MIR), and Forgetting-Aware Replay (FAR), using YOLOv11-nano as a resource-efficient detector for deployment-constrained UAV platforms. Under tight memory budgets ($5-10\%$ replay), FAR performs better than the rest, achieving an average accuracy (ACC, $mAP_{50-95}$ across increments) of $82.96\%$ with $5\%$ replay. Gradient-weighted class activation mapping (Grad-CAM) analysis shows attention shifts across classes in mixed scenes, which is associated with reduced localization quality for drones. The experiments further demonstrate that replay-based continual learning can be effectively applied to edge aerial systems. Overall, this work contributes an indoor UAV video dataset with preserved temporal coherence and an evaluation of replay-based CIL under limited replay budgets. Project page: https://spacetime-vision-robotics-laboratory.github.io/learning-on-the-fly-cl

</details>


### [40] [GLIMPSE : Real-Time Text Recognition and Contextual Understanding for VQA in Wearables](https://arxiv.org/abs/2602.13479)
*Akhil Ramachandran,Ankit Arun,Ashish Shenoy,Abhay Harpale,Srihari Jayakumar,Debojeet Chatterjee,Mohsen Moslehpour,Pierce Chuang,Yichao Lu,Vikas Bhardwaj,Peyman Heidari*

Main category: cs.CV

TL;DR: Hybrid Video LLM architecture for wearables that performs selective high-resolution OCR on-device while streaming low-resolution video, achieving 72% VQA accuracy at 0.49x power consumption of full-resolution streaming.


<details>
  <summary>Details</summary>
Motivation: Deploying Text VQA on wearable devices faces tension between high-resolution requirements for text recognition and battery/thermal constraints. Existing models struggle with coherent temporal context in real-time streams.

Method: Exploits asymmetric resolution requirements: OCR needs fine detail while scene understanding tolerates coarse features. Uses hybrid architecture with selective high-resolution OCR on-device and low-resolution video streaming for visual context.

Result: Achieves 72% accuracy on text-based VQA benchmark across five task categories at 0.49x the power consumption of full-resolution streaming, enabling sustained VQA sessions on resource-constrained wearables.

Conclusion: The hybrid approach successfully balances text recognition quality with power efficiency, making Text VQA practical for wearable devices by addressing the fundamental resolution-power tradeoff.

Abstract: Video Large Language Models (Video LLMs) have shown remarkable progress in understanding and reasoning about visual content, particularly in tasks involving text recognition and text-based visual question answering (Text VQA). However, deploying Text VQA on wearable devices faces a fundamental tension: text recognition requires high-resolution video, but streaming high-quality video drains battery and causes thermal throttling. Moreover, existing models struggle to maintain coherent temporal context when processing text across multiple frames in real-time streams. We observe that text recognition and visual reasoning have asymmetric resolution requirements - OCR needs fine detail while scene understanding tolerates coarse features. We exploit this asymmetry with a hybrid architecture that performs selective high-resolution OCR on-device while streaming low-resolution video for visual context. On a benchmark of text-based VQA samples across five task categories, our system achieves 72% accuracy at 0.49x the power consumption of full-resolution streaming, enabling sustained VQA sessions on resource-constrained wearables without sacrificing text understanding quality.

</details>


### [41] [Benchmarking Video Foundation Models for Remote Parkinson's Disease Screening](https://arxiv.org/abs/2602.13507)
*Md Saiful Islam,Ekram Hossain,Abdelrahman Abdelkader,Tariq Adnan,Fazla Rabbi Mashrur,Sooyong Park,Praveen Kumar,Qasim Sudais,Natalia Chunga,Nami Shah,Jan Freyberg,Christopher Kanan,Ruth Schneider,Ehsan Hoque*

Main category: cs.CV

TL;DR: Systematic evaluation of 7 video foundation models for Parkinson's disease screening using 32,847 videos from 1,888 participants across 16 clinical tasks, showing model-dependent task performance with AUCs 76.4-85.3%.


<details>
  <summary>Details</summary>
Motivation: Remote video-based PD screening is scalable but requires understanding which video foundation models work best for different clinical tasks, as current comparative effectiveness across architectures is poorly understood.

Method: Large-scale study using novel dataset of 32,847 videos from 1,888 participants (727 with PD) across 16 standardized clinical tasks. Evaluated 7 state-of-the-art VFMs (VideoPrism, V-JEPA, ViViT, VideoMAE, TimeSformer) using frozen embeddings with linear classification head.

Result: Task saliency is model-dependent: VideoPrism excels for visual speech kinematics and facial expressivity, V-JEPA superior for upper-limb motor tasks, TimeSformer competitive for rhythmic tasks. AUCs 76.4-85.3%, accuracies 71.5-80.6%, high specificity (up to 90.3%) but lower sensitivity (43.2-57.3%).

Conclusion: Establishes rigorous baseline for VFM-based PD screening, provides roadmap for selecting suitable tasks/architectures in remote neurological monitoring, highlights need for task-aware calibration and multi-task/modal integration.

Abstract: Remote, video-based assessments offer a scalable pathway for Parkinson's disease (PD) screening. While traditional approaches rely on handcrafted features mimicking clinical scales, recent advances in video foundation models (VFMs) enable representation learning without task-specific customization. However, the comparative effectiveness of different VFM architectures across diverse clinical tasks remains poorly understood. We present a large-scale systematic study using a novel video dataset from 1,888 participants (727 with PD), comprising 32,847 videos across 16 standardized clinical tasks. We evaluate seven state-of-the-art VFMs -- including VideoPrism, V-JEPA, ViViT, and VideoMAE -- to determine their robustness in clinical screening. By evaluating frozen embeddings with a linear classification head, we demonstrate that task saliency is highly model-dependent: VideoPrism excels in capturing visual speech kinematics (no audio) and facial expressivity, while V-JEPA proves superior for upper-limb motor tasks. Notably, TimeSformer remains highly competitive for rhythmic tasks like finger tapping. Our experiments yield AUCs of 76.4-85.3% and accuracies of 71.5-80.6%. While high specificity (up to 90.3%) suggests strong potential for ruling out healthy individuals, the lower sensitivity (43.2-57.3%) highlights the need for task-aware calibration and integration of multiple tasks and modalities. Overall, this work establishes a rigorous baseline for VFM-based PD screening and provides a roadmap for selecting suitable tasks and architectures in remote neurological monitoring. Code and anonymized structured data are publicly available: https://anonymous.4open.science/r/parkinson\_video\_benchmarking-A2C5

</details>


### [42] [SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p Masking and Distillation Fine-Tuning](https://arxiv.org/abs/2602.13515)
*Jintao Zhang,Kai Jiang,Chendong Xiang,Weiqi Feng,Yuezhou Hu,Haocheng Xi,Jianfei Chen,Jun Zhu*

Main category: cs.CV

TL;DR: SpargeAttention2 is a trainable sparse attention method that achieves 95% sparsity and 16.2x speedup in video diffusion models while maintaining generation quality, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing training-free sparse attention methods have limitations, and while trainable sparse attention can achieve higher sparsity, current approaches have failures in masking rules and limitations in fine-tuning with diffusion loss.

Method: SpargeAttention2 combines: 1) hybrid masking rule (Top-k + Top-p) for robust high-sparsity masking, 2) efficient trainable sparse attention implementation, and 3) distillation-inspired fine-tuning objective to preserve generation quality.

Result: Achieves 95% attention sparsity and 16.2x attention speedup in video diffusion models while maintaining generation quality, consistently outperforming prior sparse attention methods.

Conclusion: SpargeAttention2 successfully addresses key limitations of existing sparse attention methods through hybrid masking, efficient implementation, and improved fine-tuning, enabling high sparsity without quality degradation.

Abstract: Many training-free sparse attention methods are effective for accelerating diffusion models. Recently, several works suggest that making sparse attention trainable can further increase sparsity while preserving generation quality. We study three key questions: (1) when do the two common masking rules, i.e., Top-k and Top-p, fail, and how can we avoid these failures? (2) why can trainable sparse attention reach higher sparsity than training-free methods? (3) what are the limitations of fine-tuning sparse attention using the diffusion loss, and how can we address them? Based on this analysis, we propose SpargeAttention2, a trainable sparse attention method that achieves high sparsity without degrading generation quality. SpargeAttention2 includes (i) a hybrid masking rule that combines Top-k and Top-p for more robust masking at high sparsity, (ii) an efficient trainable sparse attention implementation, and (iii) a distillation-inspired fine-tuning objective to better preserve generation quality during fine-tuning using sparse attention. Experiments on video diffusion models show that SpargeAttention2 reaches 95% attention sparsity and a 16.2x attention speedup while maintaining generation quality, consistently outperforming prior sparse attention methods.

</details>


### [43] [Nighttime Autonomous Driving Scene Reconstruction with Physically-Based Gaussian Splatting](https://arxiv.org/abs/2602.13549)
*Tae-Kyeong Kim,Xingxin Chen,Guile Wu,Chengjie Huang,Dongfeng Bai,Bingbing Liu*

Main category: cs.CV

TL;DR: A novel approach integrating physically based rendering into 3D Gaussian Splatting for enhanced nighttime scene reconstruction in autonomous driving, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing NeRF and 3DGS methods achieve photorealistic modeling for autonomous driving scenes but primarily focus on normal-light conditions. Nighttime driving scenes are more challenging due to complex lighting and appearance conditions, causing performance degradation in current methods.

Method: Integrates physically based rendering into 3D Gaussian Splatting (3DGS) with composite scene Gaussian representations. Jointly optimizes BRDF-based material properties, explicitly modeling diffuse components through global illumination and specular components via anisotropic spherical Gaussians.

Result: Improves reconstruction quality for outdoor nighttime driving scenes while maintaining real-time rendering. Outperforms state-of-the-art methods both quantitatively and qualitatively across diverse nighttime scenarios on nuScenes and Waymo datasets.

Conclusion: The proposed approach successfully addresses the challenges of nighttime scene reconstruction in autonomous driving by integrating physically based rendering with 3DGS, achieving superior performance over existing methods for low-light conditions.

Abstract: This paper focuses on scene reconstruction under nighttime conditions in autonomous driving simulation. Recent methods based on Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) have achieved photorealistic modeling in autonomous driving scene reconstruction, but they primarily focus on normal-light conditions. Low-light driving scenes are more challenging to model due to their complex lighting and appearance conditions, which often causes performance degradation of existing methods. To address this problem, this work presents a novel approach that integrates physically based rendering into 3DGS to enhance nighttime scene reconstruction for autonomous driving. Specifically, our approach integrates physically based rendering into composite scene Gaussian representations and jointly optimizes Bidirectional Reflectance Distribution Function (BRDF) based material properties. We explicitly model diffuse components through a global illumination module and specular components by anisotropic spherical Gaussians. As a result, our approach improves reconstruction quality for outdoor nighttime driving scenes, while maintaining real-time rendering. Extensive experiments across diverse nighttime scenarios on two real-world autonomous driving datasets, including nuScenes and Waymo, demonstrate that our approach outperforms the state-of-the-art methods both quantitatively and qualitatively.

</details>


### [44] [Privacy-Concealing Cooperative Perception for BEV Scene Segmentation](https://arxiv.org/abs/2602.13555)
*Song Wang,Lingling Li,Marcus Santos,Guanghui Wang*

Main category: cs.CV

TL;DR: PCC framework protects privacy in cooperative BEV perception by hiding visual clues in shared features while maintaining segmentation performance.


<details>
  <summary>Details</summary>
Motivation: Cooperative perception improves autonomous driving but risks privacy leakage as shared data can be reconstructed into sensitive visual content.

Method: Adversarial learning with hiding network to conceal visual clues in BEV features, reconstruction network to uncover them, and end-to-end optimization with perception network.

Result: Effectively degrades reconstructed image quality with minimal impact on segmentation performance, providing privacy protection.

Conclusion: PCC framework successfully balances privacy protection and perception performance in cooperative autonomous driving systems.

Abstract: Cooperative perception systems for autonomous driving aim to overcome the limited perception range of a single vehicle by communicating with adjacent agents to share sensing information. While this improves perception performance, these systems also face a significant privacy-leakage issue, as sensitive visual content can potentially be reconstructed from the shared data. In this paper, we propose a novel Privacy-Concealing Cooperation (PCC) framework for Bird's Eye View (BEV) semantic segmentation. Based on commonly shared BEV features, we design a hiding network to prevent an image reconstruction network from recovering the input images from the shared features. An adversarial learning mechanism is employed to train the network, where the hiding network works to conceal the visual clues in the BEV features while the reconstruction network attempts to uncover these clues. To maintain segmentation performance, the perception network is integrated with the hiding network and optimized end-to-end. The experimental results demonstrate that the proposed PCC framework effectively degrades the quality of the reconstructed images with minimal impact on segmentation performance, providing privacy protection for cooperating vehicles. The source code will be made publicly available upon publication.

</details>


### [45] [Diff-Aid: Inference-time Adaptive Interaction Denoising for Rectified Text-to-Image Generation](https://arxiv.org/abs/2602.13585)
*Binglei Li,Mengping Yang,Zhiyu Tan,Junping Zhang,Hao Li*

Main category: cs.CV

TL;DR: Diff-Aid is a lightweight inference-time method that adaptively adjusts per-token text-image interactions across transformer blocks and denoising timesteps to improve text-to-image diffusion models' prompt adherence.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image diffusion models struggle with complex textual descriptions due to insufficient interactions between textual and visual features. Existing approaches lack flexibility and overlook dynamic interactions across different blocks and denoising stages.

Method: Proposes Diff-Aid, a plug-and-play inference-time module that adaptively adjusts per-token text and image interactions across transformer blocks and denoising timesteps. It provides interpretable modulation patterns revealing how different components contribute to semantic alignment.

Result: Experiments on SD 3.5 and FLUX baselines show consistent improvements in prompt adherence, visual quality, and human preference across various metrics. The method can be integrated into downstream applications like style LoRAs, controllable generation, and zero-shot editing.

Conclusion: Diff-Aid offers a flexible and efficient solution to enhance text-image interactions in diffusion models, providing both improved generation quality and interpretable insights into the generation process.

Abstract: Recent text-to-image (T2I) diffusion models have achieved remarkable advancement, yet faithfully following complex textual descriptions remains challenging due to insufficient interactions between textual and visual features. Prior approaches enhance such interactions via architectural design or handcrafted textual condition weighting, but lack flexibility and overlook the dynamic interactions across different blocks and denoising stages. To provide a more flexible and efficient solution to this problem, we propose Diff-Aid, a lightweight inference-time method that adaptively adjusts per-token text and image interactions across transformer blocks and denoising timesteps. Beyond improving generation quality, Diff-Aid yields interpretable modulation patterns that reveal how different blocks, timesteps, and textual tokens contribute to semantic alignment during denoising. As a plug-and-play module, Diff-Aid can be seamlessly integrated into downstream applications for further improvement, including style LoRAs, controllable generation, and zero-shot editing. Experiments on strong baselines (SD 3.5 and FLUX) demonstrate consistent improvements in prompt adherence, visual quality, and human preference across various metrics. Our code and models will be released.

</details>


### [46] [Two-Stream Interactive Joint Learning of Scene Parsing and Geometric Vision Tasks](https://arxiv.org/abs/2602.13588)
*Guanfeng Tang,Hongbo Zhao,Ziwei Long,Jiayao Li,Bohong Xiao,Wei Ye,Hanli Wang,Rui Fan*

Main category: cs.CV

TL;DR: TwInS is a bio-inspired joint learning framework that simultaneously performs scene parsing and geometric vision tasks through interactive feature exchange between two parallel streams, using semi-supervised training without ground-truth correspondences.


<details>
  <summary>Details</summary>
Motivation: Inspired by the human visual system's parallel processing streams for contextual and spatial understanding, the authors aim to create a unified framework that leverages the complementary nature of scene parsing (semantic understanding) and geometric vision (3D/spatial understanding) tasks.

Method: TwInS uses two interactive streams: scene parsing stream provides contextual features to guide geometric vision refinement, while decoded geometric features are projected back via a cross-task adapter using cross-view geometric cues. A semi-supervised training strategy eliminates need for human-annotated correspondence ground truth by leveraging large-scale multi-view data.

Result: Extensive experiments on three public datasets validate TwInS's effectiveness and demonstrate superior performance over state-of-the-art approaches. The framework shows successful joint learning of scene parsing and geometric vision tasks.

Conclusion: TwInS presents a successful bio-inspired joint learning framework that enables synergistic interaction between scene parsing and geometric vision tasks through bidirectional feature exchange, achieving state-of-the-art performance without requiring costly ground-truth correspondences.

Abstract: Inspired by the human visual system, which operates on two parallel yet interactive streams for contextual and spatial understanding, this article presents Two Interactive Streams (TwInS), a novel bio-inspired joint learning framework capable of simultaneously performing scene parsing and geometric vision tasks. TwInS adopts a unified, general-purpose architecture in which multi-level contextual features from the scene parsing stream are infused into the geometric vision stream to guide its iterative refinement. In the reverse direction, decoded geometric features are projected into the contextual feature space for selective heterogeneous feature fusion via a novel cross-task adapter, which leverages rich cross-view geometric cues to enhance scene parsing. To eliminate the dependence on costly human-annotated correspondence ground truth, TwInS is further equipped with a tailored semi-supervised training strategy, which unleashes the potential of large-scale multi-view data and enables continuous self-evolution without requiring ground-truth correspondences. Extensive experiments conducted on three public datasets validate the effectiveness of TwInS's core components and demonstrate its superior performance over existing state-of-the-art approaches. The source code will be made publicly available upon publication.

</details>


### [47] [AdaVBoost: Mitigating Hallucinations in LVLMs via Token-Level Adaptive Visual Attention Boosting](https://arxiv.org/abs/2602.13600)
*Jiacheng Zhang,Feng Liu,Chao Du,Tianyu Pang*

Main category: cs.CV

TL;DR: AdaVBoost is a token-level visual attention boosting framework that adaptively adjusts boosting strength based on hallucination risk, addressing the trade-off in existing methods where fixed scaling factors can be either too weak or too strong.


<details>
  <summary>Details</summary>
Motivation: Existing visual attention boosting methods use predefined scaling factors that create a fundamental trade-off: they can be too weak at some generation steps (leaving hallucinations unresolved) and too strong at others (causing new hallucinations). This motivates the need for adaptive, token-level boosting.

Method: Proposes AdaVBoost with Visual Grounding Entropy (VGE) to estimate hallucination risk by leveraging visual grounding as a complementary signal to capture evidence mismatches. The framework adaptively applies stronger visual attention boosting to high-risk tokens and weaker boosting to low-risk tokens at each generation step.

Result: Extensive experiments show AdaVBoost significantly outperforms baseline methods across multiple Large Vision-Language Models (LVLMs) and hallucination benchmarks.

Conclusion: AdaVBoost effectively addresses the trade-off in visual attention boosting by enabling token-level adaptive intervention, leading to improved mitigation of hallucinations in LVLMs.

Abstract: Visual attention boosting has emerged as a promising direction for mitigating hallucinations in Large Vision-Language Models (LVLMs), where existing methods primarily focus on where to boost by applying a predefined scaling to the attention of method-specific visual tokens during autoregressive generation. In this paper, we identify a fundamental trade-off in these methods: a predefined scaling factor can be too weak at some generation steps, leaving hallucinations unresolved, yet too strong at others, leading to new hallucinations. Motivated by this finding, we propose AdaVBoost, a token-level visual attention boosting framework that adaptively determines how much attention to boost at each generation step. Specifically, we introduce Visual Grounding Entropy (VGE) to estimate hallucination risk, which leverages visual grounding as a complementary signal to capture evidence mismatches beyond entropy. Guided by VGE, AdaVBoost applies stronger visual attention boosting to high-risk tokens and weaker boosting to low-risk tokens, enabling token-level adaptive intervention at each generation step. Extensive experiments show that AdaVBoost significantly outperforms baseline methods across multiple LVLMs and hallucination benchmarks.

</details>


### [48] [Towards Sparse Video Understanding and Reasoning](https://arxiv.org/abs/2602.13602)
*Chenwei Xu,Zhen Ye,Shang Wu,Weijian Li,Zihan Wang,Zhuofan Xia,Lie Lu,Pranav Maneriker,Fan Du,Manling Li,Han Liu*

Main category: cs.CV

TL;DR: REVISE is a multi-round video QA agent that selects sparse informative frames, maintains summary-as-state, and stops early when confident, improving accuracy while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Current video QA methods often uniformly sample frames, leading to inefficient processing of redundant information. There's a need for sparse, efficient video reasoning that reduces computational costs while maintaining or improving accuracy.

Method: REVISE operates in multi-round fashion: 1) Selects small set of informative frames, 2) Maintains summary-as-state across rounds, 3) Stops early when confident. For open-source models, introduces EAGER reward with three terms: confidence gain, summary sufficiency, and correct-and-early stop.

Result: Across multiple VQA benchmarks, REVISE improves accuracy while reducing frames, rounds, and prompt tokens, demonstrating practical sparse video reasoning.

Conclusion: REVISE enables efficient video QA through sparse frame selection and multi-round reasoning, supporting both proprietary VLMs in plug-and-play settings and open-source models via reinforcement fine-tuning with EAGER rewards.

Abstract: We present \revise (\underline{Re}asoning with \underline{Vi}deo \underline{S}parsity), a multi-round agent for video question answering (VQA). Instead of uniformly sampling frames, \revise selects a small set of informative frames, maintains a summary-as-state across rounds, and stops early when confident. It supports proprietary vision-language models (VLMs) in a ``plug-and-play'' setting and enables reinforcement fine-tuning for open-source models. For fine-tuning, we introduce EAGER (Evidence-Adjusted Gain for Efficient Reasoning), an annotation-free reward with three terms: (1) Confidence gain: after new frames are added, we reward the increase in the log-odds gap between the correct option and the strongest alternative; (2) Summary sufficiency: at answer time we re-ask using only the last committed summary and reward success; (3) Correct-and-early stop: answering correctly within a small turn budget is rewarded. Across multiple VQA benchmarks, \revise improves accuracy while reducing frames, rounds, and prompt tokens, demonstrating practical sparse video reasoning.

</details>


### [49] [A generalizable foundation model for intraoperative understanding across surgical procedures](https://arxiv.org/abs/2602.13633)
*Kanggil Park,Yongjun Jeon,Soyoung Lim,Seonmin Park,Jongmin Shin,Jung Yong Kim,Sehyeon An,Jinsoo Rhu,Jongman Kim,Gyu-Seong Choi,Namkee Oh,Kyu-Hwan Jung*

Main category: cs.CV

TL;DR: ZEN is a generalizable foundation model for surgical video understanding trained on 4M+ frames from 21 procedures using self-supervised multi-teacher distillation, outperforming existing models across 20 downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Variability in surgeons' intraoperative visual interpretation limits consistent assessment, training, and development of reliable AI systems. Most surgical AI models are task-specific and don't generalize across procedures or institutions.

Method: Developed ZEN using a self-supervised multi-teacher distillation framework trained on a large, diverse dataset of over 4 million frames from 21 different surgical procedures. Systematically evaluated multiple representation learning strategies within a unified benchmark.

Result: ZEN consistently outperforms existing surgical foundation models across 20 downstream tasks in full fine-tuning, frozen-backbone, few-shot, and zero-shot settings, demonstrating robust cross-procedure generalization.

Conclusion: ZEN represents a step toward unified representations for surgical scene understanding and supports future applications in intraoperative assistance and surgical training assessment.

Abstract: In minimally invasive surgery, clinical decisions depend on real-time visual interpretation, yet intraoperative perception varies substantially across surgeons and procedures. This variability limits consistent assessment, training, and the development of reliable artificial intelligence systems, as most surgical AI models are designed for narrowly defined tasks and do not generalize across procedures or institutions. Here we introduce ZEN, a generalizable foundation model for intraoperative surgical video understanding trained on more than 4 million frames from over 21 procedures using a self-supervised multi-teacher distillation framework. We curated a large and diverse dataset and systematically evaluated multiple representation learning strategies within a unified benchmark. Across 20 downstream tasks and full fine-tuning, frozen-backbone, few-shot and zero-shot settings, ZEN consistently outperforms existing surgical foundation models and demonstrates robust cross-procedure generalization. These results suggest a step toward unified representations for surgical scene understanding and support future applications in intraoperative assistance and surgical training assessment.

</details>


### [50] [Layer-Guided UAV Tracking: Enhancing Efficiency and Occlusion Robustness](https://arxiv.org/abs/2602.13636)
*Yang Zhou,Derui Ding,Ran Sun,Ying Sun,Haohua Zhang*

Main category: cs.CV

TL;DR: LGTrack is a lightweight UAV tracking framework that balances accuracy and efficiency using dynamic layer selection, feature enhancement, and robust representation learning for occlusions, achieving 258.7 FPS with competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: Visual object tracking for UAV applications faces challenges in balancing accuracy and efficiency, especially under unpredictable occlusion conditions. Current methods struggle with this trade-off, requiring solutions that maintain precision while achieving real-time performance.

Method: Proposes LGTrack with two key modules: 1) Global-Grouped Coordinate Attention (GGCA) for capturing long-range dependencies and global contexts with minimal computation, and 2) Similarity-Guided Layer Adaptation (SGLA) that replaces knowledge distillation to optimize tracking precision vs. inference efficiency trade-off.

Result: Achieves state-of-the-art real-time speed of 258.7 FPS on UAVDT dataset while maintaining competitive tracking accuracy of 82.8% precision. Outperforms existing methods in balancing efficiency and accuracy across three datasets.

Conclusion: LGTrack successfully addresses the accuracy-efficiency trade-off in UAV tracking through lightweight attention mechanisms and adaptive layer selection, demonstrating practical viability for real-world UAV applications requiring both speed and robustness to occlusions.

Abstract: Visual object tracking (VOT) plays a pivotal role in unmanned aerial vehicle (UAV) applications. Addressing the trade-off between accuracy and efficiency, especially under challenging conditions like unpredictable occlusion, remains a significant challenge. This paper introduces LGTrack, a unified UAV tracking framework that integrates dynamic layer selection, efficient feature enhancement, and robust representation learning for occlusions. By employing a novel lightweight Global-Grouped Coordinate Attention (GGCA) module, LGTrack captures long-range dependencies and global contexts, enhancing feature discriminability with minimal computational overhead. Additionally, a lightweight Similarity-Guided Layer Adaptation (SGLA) module replaces knowledge distillation, achieving an optimal balance between tracking precision and inference efficiency. Experiments on three datasets demonstrate LGTrack's state-of-the-art real-time speed (258.7 FPS on UAVDT) while maintaining competitive tracking accuracy (82.8\% precision). Code is available at https://github.com/XiaoMoc/LGTrack

</details>


### [51] [DCDM: Divide-and-Conquer Diffusion Models for Consistency-Preserving Video Generation](https://arxiv.org/abs/2602.13637)
*Haoyu Zhao,Yuang Zhang,Junqi Cheng,Jiaxi Gu,Zenghui Lu,Peng Shu,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: DCDM is a system-level framework that addresses video consistency challenges through three dedicated components: intra-clip world knowledge consistency using LLM parsing, inter-clip camera consistency via temporal camera representation, and inter-shot consistency through holistic scene generation.


<details>
  <summary>Details</summary>
Motivation: Current video generative models show impressive visual fidelity but struggle with semantic, geometric, and identity consistency across different temporal scales (intra-clip, inter-clip, and inter-shot).

Method: DCDM decomposes video consistency into three components: 1) Uses LLM to parse prompts into structured semantic representations for intra-clip consistency, 2) Proposes temporal camera representation in noise space for inter-clip camera control, and 3) Employs holistic scene generation with windowed cross-attention and sparse inter-shot self-attention for inter-shot consistency.

Result: The framework was validated on the CVM Competition test set at AAAI'26, demonstrating that the proposed strategies effectively address the three consistency challenges.

Conclusion: DCDM provides a comprehensive system-level solution for video consistency by addressing different temporal scales through specialized components while maintaining a unified video generation backbone.

Abstract: Recent video generative models have demonstrated impressive visual fidelity, yet they often struggle with semantic, geometric, and identity consistency. In this paper, we propose a system-level framework, termed the Divide-and-Conquer Diffusion Model (DCDM), to address three key challenges: (1) intra-clip world knowledge consistency, (2) inter-clip camera consistency, and (3) inter-shot element consistency. DCDM decomposes video consistency modeling under these scenarios into three dedicated components while sharing a unified video generation backbone. For intra-clip consistency, DCDM leverages a large language model to parse input prompts into structured semantic representations, which are subsequently translated into coherent video content by a diffusion transformer. For inter-clip camera consistency, we propose a temporal camera representation in the noise space that enables precise and stable camera motion control, along with a text-to-image initialization mechanism to further enhance controllability. For inter-shot consistency, DCDM adopts a holistic scene generation paradigm with windowed cross-attention and sparse inter-shot self-attention, ensuring long-range narrative coherence while maintaining computational efficiency. We validate our framework on the test set of the CVM Competition at AAAI'26, and the results demonstrate that the proposed strategies effectively address these challenges.

</details>


### [52] [KorMedMCQA-V: A Multimodal Benchmark for Evaluating Vision-Language Models on the Korean Medical Licensing Examination](https://arxiv.org/abs/2602.13650)
*Byungjin Choi,Seongsu Bae,Sunjun Kweon,Edward Choi*

Main category: cs.CV

TL;DR: KorMedMCQA-V is a Korean multimodal medical QA benchmark with 1,534 questions and 2,043 images from licensing exams, used to evaluate VLMs across various medical imaging modalities.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive benchmark for evaluating vision-language models on Korean medical reasoning tasks, complementing existing text-only benchmarks and addressing the need for multimodal medical QA evaluation in Korean.

Method: Constructed dataset from Korean Medical Licensing Examinations (2012-2023) with 1,534 questions and 2,043 associated images covering various medical modalities. Evaluated over 50 VLMs using zero-shot protocol across proprietary, open-source, general-purpose, medical-specialized, and Korean-specialized models.

Result: Gemini-3.0-Pro achieved best proprietary performance (96.9%), Qwen3-VL-32B-Thinking best open-source (83.7%), VARCO-VISION-2.0-14B best Korean-specialized (43.2%). Reasoning models outperformed instruction-tuned variants by up to 20%, medical specialization showed inconsistent gains, all models struggled with multi-image questions, and performance varied across imaging modalities.

Conclusion: KorMedMCQA-V provides a valuable multimodal benchmark for Korean medical reasoning, revealing significant gaps in current VLMs' capabilities, especially for Korean-specialized models and multi-image reasoning tasks, complementing existing text-only evaluation resources.

Abstract: We introduce KorMedMCQA-V, a Korean medical licensing-exam-style multimodal multiple-choice question answering benchmark for evaluating vision-language models (VLMs). The dataset consists of 1,534 questions with 2,043 associated images from Korean Medical Licensing Examinations (2012-2023), with about 30% containing multiple images requiring cross-image evidence integration. Images cover clinical modalities including X-ray, computed tomography (CT), electrocardiography (ECG), ultrasound, endoscopy, and other medical visuals. We benchmark over 50 VLMs across proprietary and open-source categories-spanning general-purpose, medical-specialized, and Korean-specialized families-under a unified zero-shot evaluation protocol. The best proprietary model (Gemini-3.0-Pro) achieves 96.9% accuracy, the best open-source model (Qwen3-VL-32B-Thinking) 83.7%, and the best Korean-specialized model (VARCO-VISION-2.0-14B) only 43.2%. We further find that reasoning-oriented model variants gain up to +20 percentage points over instruction-tuned counterparts, medical domain specialization yields inconsistent gains over strong general-purpose baselines, all models degrade on multi-image questions, and performance varies notably across imaging modalities. By complementing the text-only KorMedMCQA benchmark, KorMedMCQA-V forms a unified evaluation suite for Korean medical reasoning across text-only and multimodal conditions. The dataset is available via Hugging Face Datasets: https://huggingface.co/datasets/seongsubae/KorMedMCQA-V.

</details>


### [53] [Optimizing Point-of-Care Ultrasound Video Acquisition for Probabilistic Multi-Task Heart Failure Detection](https://arxiv.org/abs/2602.13658)
*Armin Saadat,Nima Hashemi,Bahar Khodabakhshian,Michael Y. Tsang,Christina Luong,Teresa S. M. Tsang,Purang Abolmaesumi*

Main category: cs.CV

TL;DR: RL agent personalizes echocardiography acquisition by selecting optimal next views or terminating early, balancing diagnostic accuracy with acquisition cost for HF assessment.


<details>
  <summary>Details</summary>
Motivation: POCUS echocardiography needs to support clinical decision-making under tight bedside time and operator-effort constraints, requiring efficient data acquisition strategies.

Method: Model POCUS as sequential acquisition problem with RL agent selecting next views or terminating; upon termination, multi-view transformer performs multi-task inference for AS severity classification and LVEF regression with uncertainty estimation.

Result: Achieves 77.2% mean balanced accuracy across AS severity classification and LVEF estimation while using 32% fewer videos than full-study approach, matching full-study performance with reduced acquisition.

Conclusion: Patient-tailored, cost-aware acquisition can streamline POCUS workflows while preserving decision quality, with framework extensible to additional cardiac endpoints and suitable for clinical integration.

Abstract: Purpose: Echocardiography with point-of-care ultrasound (POCUS) must support clinical decision-making under tight bedside time and operator-effort constraints. We introduce a personalized data acquisition strategy in which an RL agent, given a partially observed multi-view study, selects the next view to acquire or terminates acquisition to support heart-failure (HF) assessment. Upon termination, a diagnostic model jointly predicts aortic stenosis (AS) severity and left ventricular ejection fraction (LVEF), two key HF biomarkers, and outputs uncertainty, enabling an explicit trade-off between diagnostic performance and acquisition cost. Methods: We model POCUS as a sequential acquisition problem: at each step, a video selector (RL agent) chooses the next view to acquire or terminates acquisition. Upon termination, a shared multi-view transformer performs multi-task inference with two heads, ordinal AS classification, and LVEF regression, and outputs Gaussian predictive distributions yielding ordinal probabilities over AS classes and EF thresholds. These probabilities drive a reward that balances expected diagnostic benefit against acquisition cost, producing patient-specific acquisition pathways. Results: The dataset comprises 12,180 patient-level studies, split into training/validation/test sets (75/15/15). On the 1,820 test studies, our method matches full-study performance while using 32% fewer videos, achieving 77.2% mean balanced accuracy (bACC) across AS severity classification and LVEF estimation, demonstrating robust multi-task performance under acquisition budgets. Conclusion: Patient-tailored, cost-aware acquisition can streamline POCUS workflows while preserving decision quality, producing interpretable scan pathways suited to bedside use. The framework is extensible to additional cardiac endpoints and merits prospective evaluation for clinical integration.

</details>


### [54] [LeafNet: A Large-Scale Dataset and Comprehensive Benchmark for Foundational Vision-Language Understanding of Plant Diseases](https://arxiv.org/abs/2602.13662)
*Khang Nguyen Quoc,Phuong D. Dao,Luyl-Da Quach*

Main category: cs.CV

TL;DR: LeafNet dataset (186K leaf images, 97 disease classes) and LeafBench benchmark (13,950 QA pairs) introduced to evaluate VLMs on plant pathology tasks, revealing significant performance gaps in disease understanding.


<details>
  <summary>Details</summary>
Motivation: Current VLMs lack comprehensive evaluation in domain-specific agricultural tasks like plant pathology due to missing large-scale multimodal datasets and benchmarks.

Method: Created LeafNet dataset with 186,000 leaf images across 97 disease classes paired with metadata, and LeafBench benchmark with 13,950 question-answer pairs covering six agricultural tasks to systematically evaluate 12 state-of-the-art VLMs.

Result: VLMs show substantial performance disparities: binary healthy-diseased classification exceeds 90% accuracy, while fine-grained pathogen/species identification remains below 65%. Multimodal VLMs outperform vision-only models, confirming linguistic integration enhances diagnostic precision.

Conclusion: LeafBench reveals critical gaps in current VLMs for plant pathology and provides a rigorous framework for advancing AI-assisted disease diagnosis, highlighting the need for improved multimodal understanding in agricultural applications.

Abstract: Foundation models and vision-language pre-training have significantly advanced Vision-Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their application in domain-specific agricultural tasks, such as plant pathology, remains limited due to the lack of large-scale, comprehensive multimodal image--text datasets and benchmarks. To address this gap, we introduce LeafNet, a comprehensive multimodal dataset, and LeafBench, a visual question-answering benchmark developed to systematically evaluate the capabilities of VLMs in understanding plant diseases. The dataset comprises 186,000 leaf digital images spanning 97 disease classes, paired with metadata, generating 13,950 question-answer pairs spanning six critical agricultural tasks. The questions assess various aspects of plant pathology understanding, including visual symptom recognition, taxonomic relationships, and diagnostic reasoning. Benchmarking 12 state-of-the-art VLMs on our LeafBench dataset, we reveal substantial disparity in their disease understanding capabilities. Our study shows performance varies markedly across tasks: binary healthy--diseased classification exceeds 90\% accuracy, while fine-grained pathogen and species identification remains below 65\%. Direct comparison between vision-only models and VLMs demonstrates the critical advantage of multimodal architectures: fine-tuned VLMs outperform traditional vision models, confirming that integrating linguistic representations significantly enhances diagnostic precision. These findings highlight critical gaps in current VLMs for plant pathology applications and underscore the need for LeafBench as a rigorous framework for methodological advancement and progress evaluation toward reliable AI-assisted plant disease diagnosis. Code is available at https://github.com/EnalisUs/LeafBench.

</details>


### [55] [EchoTorrent: Towards Swift, Sustained, and Streaming Multi-Modal Video Generation](https://arxiv.org/abs/2602.13669)
*Rang Meng,Weipeng Wu,Yingjie Yin,Yuming Li,Chenguang Ma*

Main category: cs.CV

TL;DR: EchoTorrent is a novel video generation framework that addresses the efficiency-performance trade-off in streaming inference by combining multi-teacher training, adaptive CFG calibration, hybrid long tail forcing, and VAE decoder refinement to achieve high-quality, temporally consistent video generation with minimal latency.


<details>
  <summary>Details</summary>
Motivation: Current multi-modal video generation models suffer from prohibitive latency and limited temporal stability in streaming inference, leading to spatial blurring, temporal drift, and lip desynchronization - creating an unresolved efficiency-performance trade-off.

Method: Fourfold design: (1) Multi-Teacher Training with specialized domain experts transferring knowledge to a student model; (2) Adaptive CFG Calibration (ACC-DMD) that eliminates redundant CFG computations; (3) Hybrid Long Tail Forcing for alignment on tail frames during self-rollout training; (4) VAE Decoder Refiner for pixel-domain optimization to recover high-frequency details.

Result: Achieves few-pass autoregressive generation with substantially extended temporal consistency, identity preservation, and audio-lip synchronization while maintaining efficiency for real-time deployment.

Conclusion: EchoTorrent effectively resolves the efficiency-performance trade-off in streaming video generation by combining specialized training techniques, adaptive calibration, and architectural innovations to enable high-quality, temporally stable video generation suitable for real-time applications.

Abstract: Recent multi-modal video generation models have achieved high visual quality, but their prohibitive latency and limited temporal stability hinder real-time deployment. Streaming inference exacerbates these issues, leading to pronounced multimodal degradation, such as spatial blurring, temporal drift, and lip desynchronization, which creates an unresolved efficiency-performance trade-off. To this end, we propose EchoTorrent, a novel schema with a fourfold design: (1) Multi-Teacher Training fine-tunes a pre-trained model on distinct preference domains to obtain specialized domain experts, which sequentially transfer domain-specific knowledge to a student model; (2) Adaptive CFG Calibration (ACC-DMD), which calibrates the audio CFG augmentation errors in DMD via a phased spatiotemporal schedule, eliminating redundant CFG computations and enabling single-pass inference per step; (3) Hybrid Long Tail Forcing, which enforces alignment exclusively on tail frames during long-horizon self-rollout training via a causal-bidirectional hybrid architecture, effectively mitigates spatiotemporal degradation in streaming mode while enhancing fidelity to reference frames; and (4) VAE Decoder Refiner through pixel-domain optimization of the VAE decoder to recover high-frequency details while circumventing latent-space ambiguities. Extensive experiments and analysis demonstrate that EchoTorrent achieves few-pass autoregressive generation with substantially extended temporal consistency, identity preservation, and audio-lip synchronization.

</details>


### [56] [An Ensemble Learning Approach towards Waste Segmentation in Cluttered Environment](https://arxiv.org/abs/2602.13681)
*Maimoona Jafar,Syed Imran Ali,Ahsan Saadat,Muhammad Bilal,Shah Khalid*

Main category: cs.CV

TL;DR: This paper proposes an ensemble learning approach combining U-Net and FPN models for waste segmentation to improve recycling efficiency through better object localization for robotic waste sorting.


<details>
  <summary>Details</summary>
Motivation: Environmental pollution requires effective recycling solutions. Waste segregation is crucial for obtaining raw materials, but real-world waste environments are complex with deformed, overlapping objects without specific patterns, making accurate segmentation challenging for robotic systems.

Method: Proposed an ensemble learning approach combining U-Net and FPN segmentation models using weighted average method. U-Net captures fine details and boundaries, while FPN handles scale variation and context. Applied preprocessing techniques to enhance feature learning on a dataset mimicking real-life waste scenarios.

Result: The ensemble model (EL-4) achieved IoU of 0.8306 (improved from U-Net's 0.8065) and reduced Dice loss to 0.09019 (from FPN's 0.1183), demonstrating better segmentation accuracy.

Conclusion: The ensemble approach improves waste segmentation accuracy, which could enhance waste sorting efficiency at Material Recovery Facilities, facilitate better raw material acquisition for recycling with minimal human intervention, and increase overall throughput.

Abstract: Environmental pollution is a critical global issue, with recycling emerging as one of the most viable solutions. This study focuses on waste segregation, a crucial step in recycling processes to obtain raw material. Recent advancements in computer vision have significantly contributed to waste classification and recognition. In waste segregation, segmentation masks are essential for robots to accurately localize and pick objects from conveyor belts. The complexity of real-world waste environments, characterized by deformed items without specific patterns and overlapping objects, further complicates waste segmentation tasks. This paper proposes an Ensemble Learning approach to improve segmentation accuracy by combining high performing segmentation models, U-Net and FPN, using a weighted average method. U-Net excels in capturing fine details and boundaries in segmentation tasks, while FPN effectively handles scale variation and context in complex environments, and their combined masks result in more precise predictions. The dataset used closely mimics real-life waste scenarios, and preprocessing techniques were applied to enhance feature learning for deep learning segmentation models. The ensemble model, referred to as EL-4, achieved an IoU value of 0.8306, an improvement over U-Net's 0.8065, and reduced Dice loss to 0.09019 from FPN's 0.1183. This study could contribute to the efficiency of waste sorting at Material Recovery Facility, facilitating better raw material acquisition for recycling with minimal human intervention and enhancing the overall throughput.

</details>


### [57] [A WDLoRA-Based Multimodal Generative Framework for Clinically Guided Corneal Confocal Microscopy Image Synthesis in Diabetic Neuropathy](https://arxiv.org/abs/2602.13693)
*Xin Zhang,Liangxiu Han,Yue Shi,Yalin Zheng,Uazman Alam,Maryam Ferdousi,Rayaz Malik*

Main category: cs.CV

TL;DR: A WDLoRA-based multimodal generative framework synthesizes clinically accurate CCM images for diabetic neuropathy diagnosis, improving downstream AI model performance by addressing data scarcity.


<details>
  <summary>Details</summary>
Motivation: CCM imaging for diabetic neuropathy diagnosis faces data scarcity and fine-grained variability challenges. Existing AI generative models lack anatomical fidelity for medical imaging, limiting automated diagnostic development.

Method: Proposed Weight-Decomposed Low-Rank Adaptation (WDLoRA) framework that decouples magnitude and directional weight updates, enabling foundation models to learn nerve topology and stromal contrast separately. Jointly conditions on nerve segmentation masks and clinical prompts to synthesize anatomically coherent images across DPN spectrum.

Result: Achieved state-of-the-art visual fidelity (FID: 5.18) and structural integrity (SSIM: 0.630), outperforming GAN and standard diffusion baselines. Synthetic images preserve clinical biomarkers and are statistically equivalent to real data. Improved downstream diagnostic accuracy by 2.1% and segmentation performance by 2.2%.

Conclusion: The WDLoRA-based multimodal generative framework effectively addresses data bottlenecks in medical AI by synthesizing clinically accurate CCM images, enabling better automated diagnostic models for diabetic peripheral neuropathy.

Abstract: Corneal Confocal Microscopy (CCM) is a sensitive tool for assessing small-fiber damage in Diabetic Peripheral Neuropathy (DPN), yet the development of robust, automated deep learning-based diagnostic models is limited by scarce labelled data and fine-grained variability in corneal nerve morphology. Although Artificial Intelligence (AI)-driven foundation generative models excel at natural image synthesis, they often struggle in medical imaging due to limited domain-specific training, compromising the anatomical fidelity required for clinical analysis. To overcome these limitations, we propose a Weight-Decomposed Low-Rank Adaptation (WDLoRA)-based multimodal generative framework for clinically guided CCM image synthesis. WDLoRA is a parameter-efficient fine-tuning (PEFT) mechanism that decouples magnitude and directional weight updates, enabling foundation generative models to independently learn the orientation (nerve topology) and intensity (stromal contrast) required for medical realism. By jointly conditioning on nerve segmentation masks and disease-specific clinical prompts, the model synthesises anatomically coherent images across the DPN spectrum (Control, T1NoDPN, T1DPN). A comprehensive three-pillar evaluation demonstrates that the proposed framework achieves state-of-the-art visual fidelity (Fréchet Inception Distance (FID): 5.18) and structural integrity (Structural Similarity Index Measure (SSIM): 0.630), significantly outperforming GAN and standard diffusion baselines. Crucially, the synthetic images preserve gold-standard clinical biomarkers and are statistically equivalent to real patient data. When used to train automated diagnostic models, the synthetic dataset improves downstream diagnostic accuracy by 2.1% and segmentation performance by 2.2%, validating the framework's potential to alleviate data bottlenecks in medical AI.

</details>


### [58] [Fine-tuned Vision Language Model for Localization of Parasitic Eggs in Microscopic Images](https://arxiv.org/abs/2602.13712)
*Chan Hao Sien,Hezerul Abdul Karim,Nouar AlDahoul*

Main category: cs.CV

TL;DR: Fine-tuned vision language model (Microsoft Florence) outperforms traditional object detection methods in localizing parasitic eggs in microscopic images, achieving 0.94 mIOU.


<details>
  <summary>Details</summary>
Motivation: Soil-transmitted helminth infections affect large populations in tropical regions where diagnostic expertise is limited. Manual microscopic diagnosis is labor-intensive, time-consuming, and error-prone, creating need for automated solutions.

Method: Utilized a vision language model (Microsoft Florence) fine-tuned to localize parasitic eggs within microscopic images. Compared performance against other object detection methods like EfficientDet.

Result: The localization VLM achieved mIOU of 0.94, performing comparatively better than other object detection methods. Demonstrates strong potential for accurate parasitic egg detection.

Conclusion: The proposed VLM shows promise as a core component of an automated framework for intelligent parasitological diagnosis, offering a scalable engineering solution for regions with limited diagnostic expertise.

Abstract: Soil-transmitted helminth (STH) infections continuously affect a large proportion of the global population, particularly in tropical and sub-tropical regions, where access to specialized diagnostic expertise is limited. Although manual microscopic diagnosis of parasitic eggs remains the diagnostic gold standard, the approach can be labour-intensive, time-consuming, and prone to human error. This paper aims to utilize a vision language model (VLM) such as Microsoft Florence that was fine-tuned to localize all parasitic eggs within microscopic images. The preliminary results show that our localization VLM performs comparatively better than the other object detection methods, such as EfficientDet, with an mIOU of 0.94. This finding demonstrates the potential of the proposed VLM to serve as a core component of an automated framework, offering a scalable engineering solution for intelligent parasitological diagnosis.

</details>


### [59] [RGA-Net: A Vision Enhancement Framework for Robotic Surgical Systems Using Reciprocal Attention Mechanisms](https://arxiv.org/abs/2602.13726)
*Quanjun Li,Weixuan Li,Han Xia,Junhua Zhou,Chi-Man Pun,Xuhang Chen*

Main category: cs.CV

TL;DR: RGA-Net is a deep learning framework for surgical smoke removal in robotic surgery that uses reciprocal gating and attention fusion to restore clear visualization, improving surgeon-robot interface and surgical outcomes.


<details>
  <summary>Details</summary>
Motivation: Surgical smoke from energy-based devices degrades endoscopic video feeds in robotic surgery, compromising visual feedback, human-robot interface, and surgical outcomes. Current methods struggle with dense, non-homogeneous smoke distribution and complex light scattering effects.

Method: RGA-Net uses a hierarchical encoder-decoder architecture with two key innovations: (1) Dual-Stream Hybrid Attention module combining shifted window attention with frequency-domain processing, and (2) Axis-Decomposed Attention module for efficient multi-scale feature processing via factorized attention mechanisms. These are connected through reciprocal cross-gating blocks for bidirectional feature modulation.

Result: Extensive experiments on DesmokeData and LSD3K surgical datasets demonstrate superior performance in restoring visual clarity suitable for robotic surgery integration. The method effectively handles dense, non-homogeneous smoke and complex lighting conditions.

Conclusion: RGA-Net enhances surgeon-robot interface by providing consistently clear visualization, laying technical foundation for reducing cognitive burden, optimizing workflows, and minimizing iatrogenic injury risks. The framework represents significant progress toward more reliable and safer robotic surgical systems through computational vision enhancement.

Abstract: Robotic surgical systems rely heavily on high-quality visual feedback for precise teleoperation; yet, surgical smoke from energy-based devices significantly degrades endoscopic video feeds, compromising the human-robot interface and surgical outcomes. This paper presents RGA-Net (Reciprocal Gating and Attention-fusion Network), a novel deep learning framework specifically designed for smoke removal in robotic surgery workflows. Our approach addresses the unique challenges of surgical smoke-including dense, non-homogeneous distribution and complex light scattering-through a hierarchical encoder-decoder architecture featuring two key innovations: (1) a Dual-Stream Hybrid Attention (DHA) module that combines shifted window attention with frequency-domain processing to capture both local surgical details and global illumination changes, and (2) an Axis-Decomposed Attention (ADA) module that efficiently processes multi-scale features through factorized attention mechanisms. These components are connected via reciprocal cross-gating blocks that enable bidirectional feature modulation between encoder and decoder pathways. Extensive experiments on the DesmokeData and LSD3K surgical datasets demonstrate that RGA-Net achieves superior performance in restoring visual clarity suitable for robotic surgery integration. Our method enhances the surgeon-robot interface by providing consistently clear visualization, laying a technical foundation for alleviating surgeons' cognitive burden, optimizing operation workflows, and reducing iatrogenic injury risks in minimally invasive procedures. These practical benefits could be further validated through future clinical trials involving surgeon usability assessments. The proposed framework represents a significant step toward more reliable and safer robotic surgical systems through computational vision enhancement.

</details>


### [60] [Explore Intrinsic Geometry for Query-based Tiny and Oriented Object Detector with Momentum-based Bipartite Matching](https://arxiv.org/abs/2602.13728)
*Junpeng Zhang,Zewei Yang,Jie Feng,Yuhui Zheng,Ronghua Shang,Mengxuan Zhang*

Main category: cs.CV

TL;DR: IGOFormer is a query-based oriented object detector that integrates intrinsic geometry into feature decoding and uses momentum-based bipartite matching to improve detection of rotated objects, especially tiny ones in aerial images.


<details>
  <summary>Details</summary>
Motivation: Current query-based detectors struggle with oriented objects, particularly tiny objects with limited texture. Two main issues: 1) underutilization of intrinsic geometry during pixel-based feature decoding, and 2) inter-stage matching inconsistency from stage-wise bipartite matching.

Method: Two key components: 1) Intrinsic Geometry-aware Decoder that enhances object features by injecting geometric embeddings extrapolated from correlations to capture geometric layout and orientation. 2) Momentum-based Bipartite Matching that aggregates historical matching costs using exponential moving average with query-specific smoothing factors to stabilize inter-stage matching.

Result: Achieves state-of-the-art performance for aerial oriented object detection, with AP50 score of 78.00% on DOTA-V1.0 using Swin-T backbone under single-scale setting.

Conclusion: IGOFormer effectively addresses geometry underutilization and matching inconsistency in oriented object detection, demonstrating superior performance for aerial imagery with rotated objects.

Abstract: Recent query-based detectors have achieved remarkable progress, yet their performance remains constrained when handling objects with arbitrary orientations, especially for tiny objects capturing limited texture information. This limitation primarily stems from the underutilization of intrinsic geometry during pixel-based feature decoding and the occurrence of inter-stage matching inconsistency caused by stage-wise bipartite matching. To tackle these challenges, we present IGOFormer, a novel query-based oriented object detector that explicitly integrates intrinsic geometry into feature decoding and enhances inter-stage matching stability. Specifically, we design an Intrinsic Geometry-aware Decoder, which enhances the object-related features conditioned on an object query by injecting complementary geometric embeddings extrapolated from their correlations to capture the geometric layout of the object, thereby offering a critical geometric insight into its orientation. Meanwhile, a Momentum-based Bipartite Matching scheme is developed to adaptively aggregate historical matching costs by formulating an exponential moving average with query-specific smoothing factors, effectively preventing conflicting supervisory signals arising from inter-stage matching inconsistency. Extensive experiments and ablation studies demonstrate the superiority of our IGOFormer for aerial oriented object detection, achieving an AP$_{50}$ score of 78.00\% on DOTA-V1.0 using Swin-T backbone under the single-scale setting. The code will be made publicly available.

</details>


### [61] [Generative Latent Representations of 3D Brain MRI for Multi-Task Downstream Analysis in Down Syndrome](https://arxiv.org/abs/2602.13731)
*Jordi Malé,Juan Fortea,Mateus Rozalem-Aranha,Neus Martínez-Abadías,Xavier Sevillano*

Main category: cs.CV

TL;DR: VAEs encode 3D brain MRI scans into latent representations for generative and predictive applications, with systematic evaluation showing successful feature capture and clear clustering patterns for Down syndrome classification.


<details>
  <summary>Details</summary>
Motivation: Latent representations in generative models for medical imaging remain underexplored in terms of structure, information content, and clinical applicability, despite their potential for neuroimaging research and clinical decision-making.

Method: Develop multiple variational autoencoders (VAEs) to encode 3D brain MRI scans into compact latent space representations, with systematic evaluation through: (i) quantitative/qualitative assessment of MRI reconstruction quality, (ii) latent space visualization using PCA, and (iii) downstream classification tasks on euploid vs. Down syndrome brain MRI scans.

Result: VAE successfully captures essential brain features while maintaining high reconstruction fidelity. Latent space exhibits clear clustering patterns, particularly in distinguishing individuals with Down syndrome from euploid controls.

Conclusion: The work demonstrates the value of systematically investigating latent representations in generative models for medical imaging, showing their potential for both generative applications and clinical classification tasks in neuroimaging.

Abstract: Generative models have emerged as powerful tools in medical imaging, enabling tasks such as segmentation, anomaly detection, and high-quality synthetic data generation. These models typically rely on learning meaningful latent representations, which are particularly valuable given the high-dimensional nature of 3D medical images like brain magnetic resonance imaging (MRI) scans. Despite their potential, latent representations remain underexplored in terms of their structure, information content, and applicability to downstream clinical tasks. Investigating these representations is crucial for advancing the use of generative models in neuroimaging research and clinical decision-making. In this work, we develop multiple variational autoencoders (VAEs) to encode 3D brain MRI scans into compact latent space representations for generative and predictive applications. We systematically evaluate the effectiveness of the learned representations through three key analyses: (i) a quantitative and qualitative assessment of MRI reconstruction quality, (ii) a visualisation of the latent space structure using Principal Component Analysis, and (iii) downstream classification tasks on a proprietary dataset of euploid and Down syndrome individuals brain MRI scans. Our results demonstrate that the VAE successfully captures essential brain features while maintaining high reconstruction fidelity. The latent space exhibits clear clustering patterns, particularly in distinguishing individuals with Down syndrome from euploid controls.

</details>


### [62] [T2MBench: A Benchmark for Out-of-Distribution Text-to-Motion Generation](https://arxiv.org/abs/2602.13751)
*Bin Yang,Rong Ou,Weisheng Xu,Jiaqi Xiong,Xintao Li,Taowen Wang,Luyu Zhu,Xu Jiang,Jing Tan,Renjing Xu*

Main category: cs.CV

TL;DR: A benchmark for evaluating text-to-motion generation models on out-of-distribution (OOD) textual inputs, featuring 1,025 OOD prompts and comprehensive evaluation framework.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-motion evaluations focus on in-distribution inputs and limited criteria, failing to assess model generalization under complex OOD conditions.

Method: Constructed OOD prompt dataset with 1,025 textual descriptions. Introduced unified evaluation framework combining LLM-based Evaluation, Multi-factor Motion evaluation, and Fine-grained Accuracy Evaluation.

Result: Models show strengths in semantic alignment, motion generalizability, and physical quality, but struggle with Fine-grained Accuracy Evaluation in OOD scenarios.

Conclusion: Existing methods have limitations in OOD scenarios, providing practical guidance for future production-level text-to-motion model design and evaluation.

Abstract: Most existing evaluations of text-to-motion generation focus on in-distribution textual inputs and a limited set of evaluation criteria, which restricts their ability to systematically assess model generalization and motion generation capabilities under complex out-of-distribution (OOD) textual conditions. To address this limitation, we propose a benchmark specifically designed for OOD text-to-motion evaluation, which includes a comprehensive analysis of 14 representative baseline models and the two datasets derived from evaluation results. Specifically, we construct an OOD prompt dataset consisting of 1,025 textual descriptions. Based on this prompt dataset, we introduce a unified evaluation framework that integrates LLM-based Evaluation, Multi-factor Motion evaluation, and Fine-grained Accuracy Evaluation. Our experimental results reveal that while different baseline models demonstrate strengths in areas such as text-to-motion semantic alignment, motion generalizability, and physical quality, most models struggle to achieve strong performance with Fine-grained Accuracy Evaluation. These findings highlight the limitations of existing methods in OOD scenarios and offer practical guidance for the design and evaluation of future production-level text-to-motion models.

</details>


### [63] [OmniScience: A Large-scale Multi-modal Dataset for Scientific Image Understanding](https://arxiv.org/abs/2602.13758)
*Haoyi Tao,Chaozheng Huang,Nan Wang,Han Lyu,Linfeng Zhang,Guolin Ke,Xi Fang*

Main category: cs.CV

TL;DR: OmniScience is a large-scale multimodal dataset of 1.5M scientific figure-caption-context triplets with high-fidelity annotations, addressing limitations of existing datasets for training MLLMs on scientific images.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs perform well on natural images but struggle with scientific images (diagrams, charts, characterizations). Open-source MLLMs are particularly limited due to datasets with poor domain coverage, coarse annotations, and weak semantic grounding.

Method: Created OmniScience dataset with 1.5M triplets across 10+ scientific disciplines. Developed a dynamic model-routing re-captioning pipeline using SOTA MLLMs to generate dense descriptions by synthesizing visual features, original captions, and in-text references. Implemented quality filtering and human expert alignment.

Result: Pipeline boosted multimodal similarity score from 0.769 to 0.956. Qwen2.5-VL-3B model finetuned on OmniScience showed substantial gains: +0.378 on MM-MT-Bench and +0.140 on MMMU benchmarks.

Conclusion: OmniScience addresses critical gaps in scientific multimodal understanding by providing high-quality, domain-diverse training data, enabling significant performance improvements in MLLMs for scientific image interpretation.

Abstract: Multimodal Large Language Models demonstrate strong performance on natural image understanding, yet exhibit limited capability in interpreting scientific images, including but not limited to schematic diagrams, experimental characterizations, and analytical charts. This limitation is particularly pronounced in open-source MLLMs. The gap largely stems from existing datasets with limited domain coverage, coarse structural annotations, and weak semantic grounding. We introduce OmniScience, a large-scale, high-fidelity multi-modal dataset comprising 1.5 million figure-caption-context triplets, spanning more than 10 major scientific disciplines. To obtain image caption data with higher information density and accuracy for multi-modal large-model training, we develop a dynamic model-routing re-captioning pipeline that leverages state-of-the-art multi-modal large language models to generate dense, self-contained descriptions by jointly synthesizing visual features, original figure captions, and corresponding in-text references authored by human scientists. The pipeline is further reinforced with rigorous quality filtering and alignment with human expert judgments, ensuring both factual accuracy and semantic completeness, and boosts the image-text multi-modal similarity score from 0.769 to 0.956. We further propose a caption QA protocol as a proxy task for evaluating visual understanding. Under this setting, Qwen2.5-VL-3B model finetuned on OmniScience show substantial gains over baselines, achieving a gain of 0.378 on MM-MT-Bench and a gain of 0.140 on MMMU.

</details>


### [64] [SAM4Dcap: Training-free Biomechanical Twin System from Monocular Video](https://arxiv.org/abs/2602.13760)
*Li Wang,HaoYu Wang,Xi Chen,ZeKun Jiang,Kang Li,Jian Li*

Main category: cs.CV

TL;DR: SAM4Dcap is an open-source framework that estimates biomechanical metrics from monocular video by combining 4D human mesh recovery with biomechanical simulation, enabling accessible motion analysis outside labs.


<details>
  <summary>Details</summary>
Motivation: Current biomechanical analysis is limited to expensive lab-based optical motion capture systems. Multi-view video approaches have lowered barriers but remain impractical for home-based scenarios requiring monocular capture. There's a need for accessible, non-laboratory motion analysis tools.

Method: SAM4Dcap integrates SAM-Body4D (temporally consistent 4D human mesh recovery) with OpenSim biomechanical solver. The pipeline converts reconstructed meshes into trajectory files compatible with musculoskeletal models. Includes automated prompting strategies and Linux-native build for processing.

Result: Preliminary evaluations on walking and drop-jump tasks show SAM4Dcap has potential to achieve knee kinematic predictions comparable to multi-view systems. Some discrepancies in hip flexion and residual jitter remain, but overall shows promising performance.

Conclusion: SAM4Dcap bridges advanced computer vision with established biomechanical simulation, providing a flexible, accessible foundation for non-laboratory motion analysis. It enables quantitative biomechanical assessment from monocular video without additional training.

Abstract: Quantitative biomechanical analysis is essential for clinical diagnosis and injury prevention but is often restricted to laboratories due to the high cost of optical motion capture systems. While multi-view video approaches have lowered barriers, they remain impractical for home-based scenarios requiring monocular capture. This paper presents SAM4Dcap, an open-source, end-to-end framework for estimating biomechanical metrics from monocular video without additional training. SAM4Dcap integrates the temporally consistent 4D human mesh recovery of SAM-Body4D with the OpenSim biomechanical solver. The pipeline converts reconstructed meshes into trajectory files compatible with diverse musculoskeletal models. We introduce automated prompting strategies and a Linux-native build for processing. Preliminary evaluations on walking and drop-jump tasks indicate that SAM4Dcap has the potential to achieve knee kinematic predictions comparable to multi-view systems, although some discrepancies in hip flexion and residual jitter remain. By bridging advanced computer vision with established biomechanical simulation, SAM4Dcap provides a flexible, accessible foundation for non-laboratory motion analysis.

</details>


### [65] [Offline-Poly: A Polyhedral Framework For Offline 3D Multi-Object Tracking](https://arxiv.org/abs/2602.13772)
*Xiaoyu Li,Yitao Wu,Xian Wu,Haolin Zhuo,Lijun Zhao,Lining Sun*

Main category: cs.CV

TL;DR: Offline-Poly: A general offline 3D multi-object tracking method that refines existing tracking outputs using a tracking-centric design and global optimization capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing offline 3D MOT methods are direct extensions of online frameworks and fail to fully exploit offline advantages. They also depend on fixed upstream architectures, limiting adaptability.

Method: Proposes Tracking-by-Tracking (TBT) paradigm that processes arbitrary tracking outputs through pre-processing, hierarchical matching/fusion, and tracklet refinement. Leverages offline properties: resource unconstrainedness for global optimization and future observability for full temporal reasoning.

Result: Achieves SOTA performance: 77.6% AMOTA on nuScenes and 83.00% HOTA on KITTI. Demonstrates flexibility, generalizability, and modular effectiveness in comprehensive experiments.

Conclusion: Offline-Poly provides a general, effective offline 3D MOT solution that decouples from specific upstream components and fully exploits offline advantages for superior tracking performance.

Abstract: Offline 3D multi-object tracking (MOT) is a critical component of the 4D auto-labeling (4DAL) process. It enhances pseudo-labels generated by high-performance detectors through the incorporation of temporal context. However, existing offline 3D MOT approaches are direct extensions of online frameworks and fail to fully exploit the advantages of offline setting. Moreover, these methods often depend on fixed upstream and customized architectures, limiting their adaptability. To address these limitations, we propose Offline-Poly, a general offline 3D MOT method based on a tracking-centric design. We introduce a standardized paradigm termed Tracking-by-Tracking (TBT), which operates exclusively on arbitrary off-the-shelf tracking outputs and produces offline-refined tracklets. This formulation decouples offline tracker from specific upstream detectors or trackers. Under the TBT paradigm, Offline-Poly accepts one or multiple coarse tracking results and processes them through a structured pipeline comprising pre-processing, hierarchical matching and fusion, and tracklet refinement. Each module is designed to capitalize on the two fundamental properties of offline tracking: resource unconstrainedness, which permits global optimization beyond real-time limits, and future observability, which enables tracklet reasoning over the full temporal horizon. Offline-Poly first eliminates short-term ghost tracklets and re-identifies fragmented segments using global scene context. It then constructs scene-level similarity to associate tracklets across multiple input sources. Finally, Offline-Poly refines tracklets by jointly leveraging local and global motion patterns. On nuScenes, we achieve SOTA performance with 77.6% AMOTA. On KITTI, it achieves leading results with 83.00% HOTA. Comprehensive experiments further validate the flexibility, generalizability, and modular effectiveness of Offline-Poly.

</details>


### [66] [Skeleton2Stage: Reward-Guided Fine-Tuning for Physically Plausible Dance Generation](https://arxiv.org/abs/2602.13778)
*Jidong Jia,Youjian Zhang,Huan Fu,Dacheng Tao*

Main category: cs.CV

TL;DR: The paper proposes a reinforcement learning fine-tuning approach to bridge the skeleton-to-mesh gap in dance generation by incorporating physics-based rewards to prevent body self-penetration and foot-ground contact anomalies.


<details>
  <summary>Details</summary>
Motivation: Current dance generation methods work in the skeletal domain but ignore mesh-level physical constraints, resulting in unrealistic motions with body self-penetration and foot-ground contact anomalies when visualized with human body meshes, limiting aesthetic appeal and real-world applications.

Method: The method uses Reinforcement Learning Fine-Tuning (RLFT) with physics-based rewards derived from body mesh: (1) imitation reward measuring motion plausibility via physical simulator imitability, (2) Foot-Ground Deviation (FGD) reward with test-time guidance for dynamic foot-ground interaction, and (3) anti-freezing reward to preserve motion dynamics while maintaining physical plausibility.

Result: Experiments on multiple dance datasets show the method significantly improves physical plausibility of generated motions, producing more realistic and aesthetically pleasing dances while addressing the skeleton-to-mesh gap.

Conclusion: The proposed RLFT approach with physics-based rewards effectively bridges the skeleton-to-mesh gap in dance generation, enabling physically plausible motion synthesis under mesh visualization while preserving motion dynamics.

Abstract: Despite advances in dance generation, most methods are trained in the skeletal domain and ignore mesh-level physical constraints. As a result, motions that look plausible as joint trajectories often exhibit body self-penetration and Foot-Ground Contact (FGC) anomalies when visualized with a human body mesh, reducing the aesthetic appeal of generated dances and limiting their real-world applications. We address this skeleton-to-mesh gap by deriving physics-based rewards from the body mesh and applying Reinforcement Learning Fine-Tuning (RLFT) to steer the diffusion model toward physically plausible motion synthesis under mesh visualization. Our reward design combines (i) an imitation reward that measures a motion's general plausibility by its imitability in a physical simulator (penalizing penetration and foot skating), and (ii) a Foot-Ground Deviation (FGD) reward with test-time FGD guidance to better capture the dynamic foot-ground interaction in dance. However, we find that the physics-based rewards tend to push the model to generate freezing motions for fewer physical anomalies and better imitability. To mitigate it, we propose an anti-freezing reward to preserve motion dynamics while maintaining physical plausibility. Experiments on multiple dance datasets consistently demonstrate that our method can significantly improve the physical plausibility of generated motions, yielding more realistic and aesthetically pleasing dances. The project page is available at: https://jjd1123.github.io/Skeleton2Stage/

</details>


### [67] [Foundation Model-Driven Semantic Change Detection in Remote Sensing Imagery](https://arxiv.org/abs/2602.13780)
*Hengtong Shen,Li Yan,Hong Xie,Yaxuan Wei,Xinhao Li,Wenfei Shen,Peixian Lv,Fei Tan*

Main category: cs.CV

TL;DR: PerASCD is a semantic change detection method using RS foundation model PerA with a Cascaded Gated Decoder and Soft Semantic Consistency Loss, achieving SOTA performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing semantic change detection methods face challenges in performance and paradigm complexity due to limited semantic understanding capability and task complexity. There's a need for better multi-scale semantic understanding and simplified approaches.

Method: Proposes PerASCD using RS foundation model PerA, introduces Cascaded Gated Decoder (CG-Decoder) to simplify complex decoding pipelines and enable multi-level feature interaction/fusion, and Soft Semantic Consistency Loss (SSCLoss) to address training instability.

Result: Achieves state-of-the-art performance on two public benchmark datasets. The decoder effectively simplifies SCD paradigm and shows seamless adaptation across various vision encoders.

Conclusion: PerASCD enhances multi-scale semantic understanding and overall performance in semantic change detection, providing an effective solution with simplified architecture and improved training stability.

Abstract: Remote sensing (RS) change detection methods can extract critical information on surface dynamics and are an essential means for humans to understand changes in the earth's surface and environment. Among these methods, semantic change detection (SCD) can more effectively interpret the multi-class information contained in bi-temporal RS imagery, providing semantic-level predictions that support dynamic change monitoring. However, due to the limited semantic understanding capability of the model and the inherent complexity of the SCD tasks, existing SCD methods face significant challenges in both performance and paradigm complexity. In this paper, we propose PerASCD, a SCD method driven by RS foundation model PerA, designed to enhance the multi-scale semantic understanding and overall performance. We introduce a modular Cascaded Gated Decoder (CG-Decoder) that simplifies complex SCD decoding pipelines while promoting effective multi-level feature interaction and fusion. In addition, we propose a Soft Semantic Consistency Loss (SSCLoss) to mitigate the numerical instability commonly encountered during SCD training. We further explore the applicability of multiple existing RS foundation models on the SCD task when equipped with the proposed decoder. Experimental results demonstrate that our decoder not only effectively simplifies the paradigm of SCD, but also achieves seamless adaptation across various vision encoders. Our method achieves state-of-the-art (SOTA) performance on two public benchmark datasets, validating its effectiveness. The code is available at https://github.com/SathShen/PerASCD.git.

</details>


### [68] [Gaussian Sequences with Multi-Scale Dynamics for 4D Reconstruction from Monocular Casual Videos](https://arxiv.org/abs/2602.13806)
*Can Li,Jie Gu,Jingmin Chen,Fangzhou Qiu,Lei Sun*

Main category: cs.CV

TL;DR: A novel 4D reconstruction method using multi-scale dynamics factorization and Gaussian sequences for accurate dynamic scene understanding from monocular casual videos.


<details>
  <summary>Details</summary>
Motivation: Understanding dynamic scenes from casual videos is critical for scalable robot learning, but 4D reconstruction under monocular settings is highly ill-posed due to ambiguity in motion estimation.

Method: Proposes Gaussian sequences with multi-scale dynamics - a representation that factorizes complex motion fields through compositions of multi-level motion. Incorporates multi-modal priors from vision foundation models for complementary supervision to constrain solution space.

Result: Enables accurate and globally consistent 4D reconstruction from monocular casual videos. Experiments on dynamic novel-view synthesis (NVS) on benchmark and real-world manipulation datasets show considerable improvements over existing methods.

Conclusion: The multi-scale dynamics mechanism effectively addresses the ill-posed nature of monocular 4D reconstruction by leveraging real-world motion regularity and multi-modal priors, enabling better dynamic scene understanding for robot learning applications.

Abstract: Understanding dynamic scenes from casual videos is critical for scalable robot learning, yet four-dimensional (4D) reconstruction under strictly monocular settings remains highly ill-posed. To address this challenge, our key insight is that real-world dynamics exhibits a multi-scale regularity from object to particle level. To this end, we design the multi-scale dynamics mechanism that factorizes complex motion fields. Within this formulation, we propose Gaussian sequences with multi-scale dynamics, a novel representation for dynamic 3D Gaussians derived through compositions of multi-level motion. This layered structure substantially alleviates ambiguity of reconstruction and promotes physically plausible dynamics. We further incorporate multi-modal priors from vision foundation models to establish complementary supervision, constraining the solution space and improving the reconstruction fidelity. Our approach enables accurate and globally consistent 4D reconstruction from monocular casual videos. Experiments of dynamic novel-view synthesis (NVS) on benchmark and real-world manipulation datasets demonstrate considerable improvements over existing methods.

</details>


### [69] [Joint Orientation and Weight Optimization for Robust Watertight Surface Reconstruction via Dirichlet-Regularized Winding Fields](https://arxiv.org/abs/2602.13801)
*Jiaze Li,Daisheng Jin,Fei Hou,Junhui Hou,Zheng Liu,Shiqing Xin,Wenping Wang,Ying He*

Main category: cs.CV

TL;DR: DiWR is a robust method for reconstructing watertight surfaces from unoriented point clouds with non-uniform sampling, noise, and outliers by jointly optimizing point orientations, area weights, and confidence coefficients using generalized winding number field optimization.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with challenging point clouds from modern computer vision pipelines like 3D Gaussian Splatting that have non-uniform sampling, noise, and outliers. Traditional multi-stage pipelines and recent joint methods have limitations in handling these real-world imperfections.

Method: Uses generalized winding number (GWN) field as target implicit representation and jointly optimizes point orientations, per-point area weights, and confidence coefficients in a single pipeline. Minimizes Dirichlet energy of the induced winding field with additional GWN-based constraints to compensate for non-uniform sampling, reduce noise impact, and downweight outliers without separate preprocessing.

Result: DiWR produces plausible watertight surfaces on challenging inputs including point clouds from 3D Gaussian Splatting and corrupted graphics benchmarks, outperforming both traditional multi-stage pipelines and recent joint orientation-reconstruction methods.

Conclusion: DiWR provides a robust, unified solution for watertight surface reconstruction from imperfect point clouds by jointly optimizing multiple parameters in a single pipeline, effectively handling real-world challenges without requiring separate preprocessing steps.

Abstract: We propose Dirichlet Winding Reconstruction (DiWR), a robust method for reconstructing watertight surfaces from unoriented point clouds with non-uniform sampling, noise, and outliers. Our method uses the generalized winding number (GWN) field as the target implicit representation and jointly optimizes point orientations, per-point area weights, and confidence coefficients in a single pipeline. The optimization minimizes the Dirichlet energy of the induced winding field together with additional GWN-based constraints, allowing DiWR to compensate for non-uniform sampling, reduce the impact of noise, and downweight outliers during reconstruction, with no reliance on separate preprocessing. We evaluate DiWR on point clouds from 3D Gaussian Splatting, a computer-vision pipeline, and corrupted graphics benchmarks. Experiments show that DiWR produces plausible watertight surfaces on these challenging inputs and outperforms both traditional multi-stage pipelines and recent joint orientation-reconstruction methods.

</details>


### [70] [RPGD: RANSAC-P3P Gradient Descent for Extrinsic Calibration in 3D Human Pose Estimation](https://arxiv.org/abs/2602.13901)
*Zhanyu Tuo*

Main category: cs.CV

TL;DR: RPGD is a human-pose-driven extrinsic calibration framework that robustly aligns MoCap 3D skeletal data with RGB cameras using natural human motion, combining RANSAC-P3P for global robustness with gradient descent refinement.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for reliable extrinsic calibration in large-scale 3D human pose estimation (HPE) dataset collection, particularly for aligning motion capture (MoCap) 3D skeletal data with monocular or multi-view RGB cameras using only natural human motion without requiring special calibration objects or patterns.

Method: RPGD formulates extrinsic calibration as a coarse-to-fine problem: 1) Uses RANSAC-P3P for global robustness to handle outliers and noise, 2) Follows with Gradient-Descent-based refinement for precise alignment. The method is specifically tailored to human poses and works with natural human motion data.

Result: RPGD consistently recovers extrinsic parameters with accuracy comparable to ground truth, achieving sub-pixel MPJPE (Mean Per Joint Position Error) reprojection error even in challenging, noisy settings. Evaluated on three large-scale public 3D HPE datasets and a self-collected in-the-wild dataset.

Conclusion: RPGD provides a practical and automatic solution for reliable extrinsic calibration of large-scale 3D HPE dataset collection, enabling accurate alignment of MoCap data with camera systems using only natural human motion.

Abstract: In this paper, we propose RPGD (RANSAC-P3P Gradient Descent), a human-pose-driven extrinsic calibration framework that robustly aligns MoCap-based 3D skeletal data with monocular or multi-view RGB cameras using only natural human motion. RPGD formulates extrinsic calibration as a coarse-to-fine problem tailored to human poses, combining the global robustness of RANSAC-P3P with Gradient-Descent-based refinement. We evaluate RPGD on three large-scale public 3D HPE datasets as well as on a self-collected in-the-wild dataset. Experimental results demonstrate that RPGD consistently recovers extrinsic parameters with accuracy comparable to the provided ground truth, achieving sub-pixel MPJPE reprojection error even in challenging, noisy settings. These results indicate that RPGD provides a practical and automatic solution for reliable extrinsic calibration of large-scale 3D HPE dataset collection.

</details>


### [71] [VAR-3D: View-aware Auto-Regressive Model for Text-to-3D Generation via a 3D Tokenizer](https://arxiv.org/abs/2602.13818)
*Zongcheng Han,Dongyan Cao,Haoran Sun,Yu Hong*

Main category: cs.CV

TL;DR: VAR-3D is a novel text-to-3D generation framework that addresses information loss and objective mismatch issues through view-aware 3D VQ-VAE and rendering-supervised training, achieving superior generation quality and text-3D alignment.


<details>
  <summary>Details</summary>
Motivation: Current text-to-3D generation faces challenges due to information loss during 3D representation encoding and representational distortion amplified by vector quantization, which degrades geometric coherence. Additionally, the conventional two-stage training creates objective mismatch between reconstruction and text-conditioned generation.

Method: Proposes VAR-3D with two key components: 1) View-aware 3D VQ-VAE that converts complex 3D geometric structures into discrete tokens while preserving information, and 2) Rendering-supervised training strategy that couples discrete token prediction with visual reconstruction to maintain visual fidelity and structural consistency.

Result: Experiments show VAR-3D significantly outperforms existing methods in both generation quality and text-3D alignment, demonstrating improved geometric coherence and visual fidelity.

Conclusion: VAR-3D effectively addresses the core challenges in text-to-3D generation by integrating view-aware representation learning with rendering-supervised training, establishing a new state-of-the-art approach for high-quality text-conditioned 3D shape generation.

Abstract: Recent advances in auto-regressive transformers have achieved remarkable success in generative modeling. However, text-to-3D generation remains challenging, primarily due to bottlenecks in learning discrete 3D representations. Specifically, existing approaches often suffer from information loss during encoding, causing representational distortion before the quantization process. This effect is further amplified by vector quantization, ultimately degrading the geometric coherence of text-conditioned 3D shapes. Moreover, the conventional two-stage training paradigm induces an objective mismatch between reconstruction and text-conditioned auto-regressive generation. To address these issues, we propose View-aware Auto-Regressive 3D (VAR-3D), which intergrates a view-aware 3D Vector Quantized-Variational AutoEncoder (VQ-VAE) to convert the complex geometric structure of 3D models into discrete tokens. Additionally, we introduce a rendering-supervised training strategy that couples discrete token prediction with visual reconstruction, encouraging the generative process to better preserve visual fidelity and structural consistency relative to the input text. Experiments demonstrate that VAR-3D significantly outperforms existing methods in both generation quality and text-3D alignment.

</details>


### [72] [Embed-RL: Reinforcement Learning for Reasoning-Driven Multimodal Embeddings](https://arxiv.org/abs/2602.13823)
*Haonan Jiang,Yuji Wang,Yongjie Zhu,Xin Lu,Wenyu Qin,Meng Wang,Pengfei Wan,Yansong Tang*

Main category: cs.CV

TL;DR: A reasoning-driven UME framework using Embedder-Guided Reinforcement Learning to optimize multimodal reasoning for better cross-modal retrieval.


<details>
  <summary>Details</summary>
Motivation: Existing generative embedding methods have limited reasoning capabilities - their generated reasoning chains only analyze textual queries and are irrelevant to target retrieval, limiting cross-modal semantic consistency and fine-grained matching.

Method: Proposes EG-RL (Embedder-Guided Reinforcement Learning) framework where the Embedder supervises the Reasoner to produce evidential Traceability CoT (T-CoT) that extracts multimodal cues relevant to retrieval tasks.

Result: Outperforms pioneering embedding models on MMEB-V2 and UVRB benchmarks with limited computational resources, improving cross-modal semantic consistency and fine-grained matching.

Conclusion: Targeted reasoning optimization significantly improves multimodal embedding quality, providing a practical and efficient solution for reasoning-driven UME development.

Abstract: Leveraging Multimodal Large Language Models (MLLMs) has become pivotal for advancing Universal Multimodal Embeddings (UME) in addressing diverse cross-modal tasks. Recent studies demonstrate that incorporating generative Chain-of-Thought (CoT) reasoning can substantially enhance task-specific representations compared to discriminative methods. However, the generated reasoning CoTs of existing generative embedding methods are limited to the textual analysis of queries and are irrelevant to the retrieval of the targets. To address these limitations, we propose a reasoning-driven UME framework that integrates Embedder-Guided Reinforcement Learning (EG-RL) to optimize the Reasoner to produce evidential Traceability CoT (T-CoT). Our key contributions are threefold: (1) We design an EG-RL framework where the Embedder provides explicit supervision to the Reasoner, ensuring the generated CoT traces are aligned with embedding tasks. (2) We introduce T-CoT, which extracts critical multimodal cues to focus on retrieval-relevant elements and provides multimodal inputs for the Embedder. (3) With limited computational resources, our framework outperforms the pioneering embedding model on both MMEB-V2 and UVRB benchmarks. The integration of multimodal evidence in structured reasoning, paired with retrieval-oriented alignment, effectively strengthens cross-modal semantic consistency and boosts the fine-grained matching capability of the model as well as the generalization across complex scenarios. Our work demonstrates that targeted reasoning optimization can significantly improve multimodal embedding quality, providing a practical and efficient solution for reasoning-driven UME development.

</details>


### [73] [Understanding Sensor Vulnerabilities in Industrial XR Tracking](https://arxiv.org/abs/2602.14413)
*Sourya Saha,Md. Nurul Absur*

Main category: cs.CV

TL;DR: VIO systems in industrial XR show asymmetric fault impacts: visual degradations cause centimeter-level errors, while inertial degradations can cause massive trajectory deviations up to kilometers.


<details>
  <summary>Details</summary>
Motivation: Most VIO evaluations focus on ideal sensor conditions, but industrial XR environments often have degraded sensing. There's insufficient understanding of how sustained sensor degradation affects VIO performance in operational settings.

Method: Controlled empirical study using systematic fault injection to examine VIO behavior under degraded visual and inertial sensing across various operating regimes, with quantitative evaluation.

Result: Found pronounced asymmetry: visual degradations cause bounded pose errors (~centimeters), while inertial degradations cause substantially larger trajectory deviations (hundreds to thousands of meters).

Conclusion: Greater emphasis on inertial reliability is needed in the evaluation and design of XR systems for real-life industrial settings due to the disproportionate impact of inertial faults.

Abstract: Extended Reality (XR) systems deployed in industrial and operational settings rely on Visual--Inertial Odometry (VIO) for continuous six-degree-of-freedom pose tracking, yet these environments often involve sensing conditions that deviate from ideal assumptions. Despite this, most VIO evaluations emphasize nominal sensor behavior, leaving the effects of sustained sensor degradation under operational conditions insufficiently understood. This paper presents a controlled empirical study of VIO behavior under degraded sensing, examining faults affecting visual and inertial modalities across a range of operating regimes. Through systematic fault injection and quantitative evaluation, we observe a pronounced asymmetry in fault impact where degradations affecting visual sensing typically lead to bounded pose errors on the order of centimeters, whereas degradations affecting inertial sensing can induce substantially larger trajectory deviations, in some cases reaching hundreds to thousands of meters. These observations motivate greater emphasis on inertial reliability in the evaluation and design of XR systems for real-life industrial settings.

</details>


### [74] [Prior-guided Hierarchical Instance-pixel Contrastive Learning for Ultrasound Speckle Noise Suppression](https://arxiv.org/abs/2602.13831)
*Zhenyu Bu,Yuanxin Xie,Guang-Quan Zhou*

Main category: cs.CV

TL;DR: A hierarchical contrastive learning model combining pixel and instance-level contrast with Transformer-CNN architecture for ultrasound denoising that preserves structural details while reducing speckle noise.


<details>
  <summary>Details</summary>
Motivation: Ultrasound denoising is crucial for improving image quality and diagnostic reliability, but speckle patterns contain both noise and important anatomical details, making it challenging to suppress noise while preserving structural fidelity.

Method: Proposes a prior-guided hierarchical instance-pixel contrastive learning model with: 1) statistics-guided pixel-level contrastive learning to enhance distributional discrepancies between noisy and clean pixels, 2) memory bank for instance-level contrastive learning in feature space, and 3) hybrid Transformer-CNN architecture coupling Transformer encoder for global context with CNN decoder for fine-grained structure restoration.

Result: Extensive evaluations on two publicly available ultrasound datasets demonstrate that the proposed model consistently outperforms existing methods, confirming its effectiveness and superiority.

Conclusion: The proposed hierarchical contrastive learning approach successfully addresses the challenge of ultrasound denoising by effectively suppressing noise while preserving structural details through complementary pixel-instance learning and Transformer-CNN architecture.

Abstract: Ultrasound denoising is essential for mitigating speckle-induced degradations, thereby enhancing image quality and improving diagnostic reliability. Nevertheless, because speckle patterns inherently encode both texture and fine anatomical details, effectively suppressing noise while preserving structural fidelity remains a significant challenge. In this study, we propose a prior-guided hierarchical instance-pixel contrastive learning model for ultrasound denoising, designed to promote noise-invariant and structure-aware feature representations by maximizing the separability between noisy and clean samples at both pixel and instance levels. Specifically, a statistics-guided pixel-level contrastive learning strategy is introduced to enhance distributional discrepancies between noisy and clean pixels, thereby improving local structural consistency. Concurrently, a memory bank is employed to facilitate instance-level contrastive learning in the feature space, encouraging representations that more faithfully approximate the underlying data distribution. Furthermore, a hybrid Transformer-CNN architecture is adopted, coupling a Transformer-based encoder for global context modeling with a CNN-based decoder optimized for fine-grained anatomical structure restoration, thus enabling complementary exploitation of long-range dependencies and local texture details. Extensive evaluations on two publicly available ultrasound datasets demonstrate that the proposed model consistently outperforms existing methods, confirming its effectiveness and superiority.

</details>


### [75] [Advances in Global Solvers for 3D Vision](https://arxiv.org/abs/2602.14662)
*Zhenjun Zhao,Heng Yang,Bangyan Liao,Yingping Zeng,Shaocheng Yan,Yingdong Gu,Peidong Liu,Yi Zhou,Haoang Li,Javier Civera*

Main category: cs.CV

TL;DR: This survey paper provides the first systematic review of global solvers for 3D vision, covering three core paradigms (Branch-and-Bound, Convex Relaxation, Graduated Non-Convexity) across ten vision tasks, analyzing trade-offs and future directions.


<details>
  <summary>Details</summary>
Motivation: Traditional local/heuristic methods for nonconvex geometric optimization in 3D vision lack certifiability. Global solvers offer provably optimal solutions but need systematic review to unify the field and guide practitioners.

Method: Comprehensive taxonomy of three global solver paradigms: Branch-and-Bound (BnB), Convex Relaxation (CR), and Graduated Non-Convexity (GNC). Analysis includes theoretical foundations, algorithmic designs, practical enhancements, and application to ten core vision tasks.

Result: Reveals optimality-robustness-scalability trade-offs governing solver selection. Provides unified perspective on certifiable geometric vision with continuously-updated literature summary and code tutorials.

Conclusion: Global solvers enable certifiable, trustworthy perception for real-world applications. Future directions include scaling with guarantees, integrating data-driven priors, establishing benchmarks, and addressing societal implications for safety-critical deployment.

Abstract: Global solvers have emerged as a powerful paradigm for 3D vision, offering certifiable solutions to nonconvex geometric optimization problems traditionally addressed by local or heuristic methods. This survey presents the first systematic review of global solvers in geometric vision, unifying the field through a comprehensive taxonomy of three core paradigms: Branch-and-Bound (BnB), Convex Relaxation (CR), and Graduated Non-Convexity (GNC). We present their theoretical foundations, algorithmic designs, and practical enhancements for robustness and scalability, examining how each addresses the fundamental nonconvexity of geometric estimation problems. Our analysis spans ten core vision tasks, from Wahba problem to bundle adjustment, revealing the optimality-robustness-scalability trade-offs that govern solver selection. We identify critical future directions: scaling algorithms while maintaining guarantees, integrating data-driven priors with certifiable optimization, establishing standardized benchmarks, and addressing societal implications for safety-critical deployment. By consolidating theoretical foundations, practical advances, and broader impacts, this survey provides a unified perspective and roadmap toward certifiable, trustworthy perception for real-world applications. A continuously-updated literature summary and companion code tutorials are available at https://github.com/ericzzj1989/Awesome-Global-Solvers-for-3D-Vision.

</details>


### [76] [High-Fidelity Causal Video Diffusion Models for Real-Time Ultra-Low-Bitrate Semantic Communication](https://arxiv.org/abs/2602.13837)
*Cem Eteke,Batuhan Tosun,Alexander Griessel,Wolfgang Kellerer,Eckehard Steinbach*

Main category: cs.CV

TL;DR: A video diffusion model for high-fidelity, real-time video generation under ultra-low-bitrate semantic communication constraints, using semantic scene structure and compressed low-res frames with modular adapters and temporal distillation.


<details>
  <summary>Details</summary>
Motivation: To enable high-quality video generation in ultra-low-bitrate semantic communication scenarios where traditional compression methods fail to preserve fidelity, requiring efficient transmission and reconstruction of video content.

Method: Uses lossy semantic video coding to transmit semantic scene structure plus compressed low-resolution frames for texture. Features a modular video diffusion model with Semantic Control, Restoration Adapter, and Temporal Adapter components. Includes efficient temporal distillation for real-time causal synthesis, reducing parameters by 300x and training time by 2x.

Result: Achieves strong perceptual quality, semantic fidelity, and temporal consistency at ultra-low bitrates (< 0.0003 bpp), outperforming classical, neural, and generative baselines in quantitative, qualitative, and subjective evaluations across diverse datasets.

Conclusion: The proposed framework successfully enables high-fidelity, real-time video generation under extreme bitrate constraints through semantic-aware compression and efficient diffusion modeling, advancing the state-of-the-art in semantic video communication.

Abstract: We introduce a video diffusion model for high-fidelity, causal, and real-time video generation under ultra-low-bitrate semantic communication constraints. Our approach utilizes lossy semantic video coding to transmit the semantic scene structure, complemented by a stream of highly compressed, low-resolution frames that provide sufficient texture information to preserve fidelity. Building on these inputs, we introduce a modular video diffusion model that contains Semantic Control, Restoration Adapter, and Temporal Adapter. We further introduce an efficient temporal distillation procedure that enables extension to real-time and causal synthesis, reducing trainable parameters by 300x and training time by 2x, while adhering to communication constraints. Evaluated across diverse datasets, the framework achieves strong perceptual quality, semantic fidelity, and temporal consistency at ultra-low bitrates (< 0.0003 bpp), outperforming classical, neural, and generative baselines in extensive quantitative, qualitative, and subjective evaluations.

</details>


### [77] [PAct: Part-Decomposed Single-View Articulated Object Generation](https://arxiv.org/abs/2602.14965)
*Qingming Liu,Xinyue Yao,Shuyuan Zhang,Yueci Deng,Guiliang Liu,Zhen Liu,Kui Jia*

Main category: cs.CV

TL;DR: A part-centric generative framework for creating articulated 3D objects from single images, enabling fast feed-forward inference without per-instance optimization while preserving part structure and motion.


<details>
  <summary>Details</summary>
Motivation: Articulated objects are crucial for interactive 3D applications but creating high-fidelity articulated assets is difficult to scale. Existing methods either require slow optimization (minutes to hours per instance) or rely on templates/retrieval that may not match specific input structures.

Method: Part-centric generative framework that models objects as sets of movable parts, each encoded by latent tokens with part identity and articulation cues. Conditioned on single images, it generates articulated 3D assets with instance-level correspondence while maintaining valid part structure and motion.

Result: Improved input consistency, part accuracy, and articulation plausibility over optimization-based and retrieval-driven baselines on common articulated categories (drawers, doors), while substantially reducing inference time compared to optimization methods.

Conclusion: The approach avoids per-instance optimization, enables fast feed-forward inference, and supports controllable assembly and articulation important for embodied interaction, making articulated object creation more scalable.

Abstract: Articulated objects are central to interactive 3D applications, including embodied AI, robotics, and VR/AR, where functional part decomposition and kinematic motion are essential. Yet producing high-fidelity articulated assets remains difficult to scale because it requires reliable part decomposition and kinematic rigging. Existing approaches largely fall into two paradigms: optimization-based reconstruction or distillation, which can be accurate but often takes tens of minutes to hours per instance, and inference-time methods that rely on template or part retrieval, producing plausible results that may not match the specific structure and appearance in the input observation. We introduce a part-centric generative framework for articulated object creation that synthesizes part geometry, composition, and articulation under explicit part-aware conditioning. Our representation models an object as a set of movable parts, each encoded by latent tokens augmented with part identity and articulation cues. Conditioned on a single image, the model generates articulated 3D assets that preserve instance-level correspondence while maintaining valid part structure and motion. The resulting approach avoids per-instance optimization, enables fast feed-forward inference, and supports controllable assembly and articulation, which are important for embodied interaction. Experiments on common articulated categories (e.g., drawers and doors) show improved input consistency, part accuracy, and articulation plausibility over optimization-based and retrieval-driven baselines, while substantially reducing inference time.

</details>


### [78] [Automated Prediction of Paravalvular Regurgitation before Transcatheter Aortic Valve Implantation](https://arxiv.org/abs/2602.13842)
*Michele Cannito,Riccardo Renzulli,Adson Duarte,Farzad Nikfam,Carlo Alberto Barbano,Enrico Chiesa,Francesco Bruno,Federico Giacobbe,Wojciech Wanha,Arturo Giordano,Marco Grangetto,Fabrizio D'Ascenzo*

Main category: cs.CV

TL;DR: Deep learning can predict paravalvular aortic regurgitation (PVR) after TAVI using preoperative cardiac CT scans, potentially improving risk assessment and procedural optimization.


<details>
  <summary>Details</summary>
Motivation: PVR is a common and serious complication after TAVI that affects long-term prognosis, but current methods may not adequately predict its occurrence preoperatively.

Method: Used 3D convolutional neural networks trained on isotropic CT volumes from preoperative TAVI patients to predict PVR occurrence.

Result: Deep learning models can capture subtle anatomical features from pre-TAVI imaging to predict PVR, suggesting potential for clinical application.

Conclusion: Volumetric deep learning offers new perspectives for personalized risk assessment and procedural optimization in TAVI patients.

Abstract: Severe aortic stenosis is a common and life-threatening condition in elderly patients, often treated with Transcatheter Aortic Valve Implantation (TAVI). Despite procedural advances, paravalvular aortic regurgitation (PVR) remains one of the most frequent post-TAVI complications, with a proven impact on long-term prognosis.
  In this work, we investigate the potential of deep learning to predict the occurrence of PVR from preoperative cardiac CT. To this end, a dataset of preoperative TAVI patients was collected, and 3D convolutional neural networks were trained on isotropic CT volumes. The results achieved suggest that volumetric deep learning can capture subtle anatomical features from pre-TAVI imaging, opening new perspectives for personalized risk assessment and procedural optimization. Source code is available at https://github.com/EIDOSLAB/tavi.

</details>


### [79] [Synthetic Dataset Generation and Validation for Robotic Surgery Instrument Segmentation](https://arxiv.org/abs/2602.13844)
*Giorgio Chiesa,Rossella Borra,Vittorio Lauro,Sabrina De Cillis,Daniele Amparore,Cristian Fiori,Riccardo Renzulli,Marco Grangetto*

Main category: cs.CV

TL;DR: A workflow for generating synthetic surgical instrument datasets using 3D reconstruction and animation, validated through segmentation model training showing optimal performance with balanced real+synthetic data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of obtaining sufficient labeled surgical data for computer vision tasks in robotic surgery, particularly for instrument segmentation where real data collection is difficult and annotation is time-consuming.

Method: Created 3D reconstructions of Da Vinci robotic arms, animated them in Autodesk Maya using an automated Python pipeline to generate photorealistic videos with randomized motion, lighting, and blood textures while maintaining pixel-accurate ground truth masks.

Result: Training segmentation models with balanced real+synthetic data significantly improved generalization compared to real-only training, while excessive synthetic data caused domain shift. The framework proved effective for surgical computer vision tasks.

Conclusion: The proposed synthetic data generation framework provides a reproducible, scalable solution for surgical computer vision that enhances model performance when properly balanced with real data, supporting future research in data augmentation and domain adaptation.

Abstract: This paper presents a comprehensive workflow for generating and validating a synthetic dataset designed for robotic surgery instrument segmentation. A 3D reconstruction of the Da Vinci robotic arms was refined and animated in Autodesk Maya through a fully automated Python-based pipeline capable of producing photorealistic, labeled video sequences. Each scene integrates randomized motion patterns, lighting variations, and synthetic blood textures to mimic intraoperative variability while preserving pixel-accurate ground truth masks. To validate the realism and effectiveness of the generated data, several segmentation models were trained under controlled ratios of real and synthetic data. Results demonstrate that a balanced composition of real and synthetic samples significantly improves model generalization compared to training on real data only, while excessive reliance on synthetic data introduces a measurable domain shift. The proposed framework provides a reproducible and scalable tool for surgical computer vision, supporting future research in data augmentation, domain adaptation, and simulation-based pretraining for robotic-assisted surgery. Data and code are available at https://github.com/EIDOSLAB/Sintetic-dataset-DaVinci.

</details>


### [80] [Cardiac Output Prediction from Echocardiograms: Self-Supervised Learning with Limited Data](https://arxiv.org/abs/2602.13846)
*Adson Duarte,Davide Vitturini,Emanuele Milillo,Andrea Bragagnolo,Carlo Alberto Barbano,Riccardo Renzulli,Michele Cannito,Federico Giacobbe,Francesco Bruno,Ovidio de Filippo,Fabrizio D'Ascenzo,Marco Grangetto*

Main category: cs.CV

TL;DR: Self-supervised learning (SSL) using SimCLR improves cardiac output prediction from echocardiographic videos, outperforming larger supervised models even with limited data.


<details>
  <summary>Details</summary>
Motivation: Cardiac output measurement currently requires invasive right-heart catheterization, motivating development of non-invasive alternatives using echocardiography. However, limited labeled data for echocardiographic analysis presents a challenge.

Method: Proposed a self-supervised learning pretraining strategy based on SimCLR framework, using the same limited dataset available for downstream cardiac output prediction task. SSL pretraining helps learn better representations from echocardiographic videos.

Result: SSL mitigates overfitting and improves representation learning, achieving average Pearson correlation of 0.41 on test set. This outperforms PanEcho, a model trained on over one million echocardiographic exams, despite using limited data.

Conclusion: Self-supervised learning shows promise for cardiac output prediction from echocardiography even under data scarcity, demonstrating SSL's potential to improve medical image analysis with limited labeled datasets.

Abstract: Cardiac Output (CO) is a key parameter in the diagnosis and management of cardiovascular diseases. However, its accurate measurement requires right-heart catheterization, an invasive and time-consuming procedure, motivating the development of reliable non-invasive alternatives using echocardiography. In this work, we propose a self-supervised learning (SSL) pretraining strategy based on SimCLR to improve CO prediction from apical four-chamber echocardiographic videos. The pretraining is performed using the same limited dataset available for the downstream task, demonstrating the potential of SSL even under data scarcity. Our results show that SSL mitigates overfitting and improves representation learning, achieving an average Pearson correlation of 0.41 on the test set and outperforming PanEcho, a model trained on over one million echocardiographic exams. Source code is available at https://github.com/EIDOSLAB/cardiac-output.

</details>


### [81] [Low-Pass Filtering Improves Behavioral Alignment of Vision Models](https://arxiv.org/abs/2602.13859)
*Max Wolff,Thomas Klein,Evgenia Rusak,Felix Wichmann,Wieland Brendel*

Main category: cs.CV

TL;DR: The paper shows that the improved behavioral alignment of generative models over discriminative models can be largely explained by image resizing operations that act as low-pass filters, not by fundamental architectural differences.


<details>
  <summary>Details</summary>
Motivation: Deep Neural Networks (DNNs) still don't adequately model human visual behavior, as measured by error consistency and shape bias. Recent work suggested generative classifiers might improve behavioral alignment, but this paper investigates whether simpler factors like image preprocessing might explain the difference.

Method: The authors conducted controlled experiments showing that removing high-frequency spatial information from discriminative models (like CLIP) increases their behavioral alignment. They tested simple blurring at test-time, optimized filters for alignment, and computed the Pareto-optimal frontier for the model-vs-human benchmark. They compared optimal filters to the human visual system's contrast sensitivity function.

Result: Simply blurring images at test-time achieves state-of-the-art scores on the model-vs-human benchmark, halving the alignment gap between DNNs and human observers. Low-pass filters are likely optimal, and optimal Gaussian filters match the frequency spectrum of the human visual system's band-pass filters.

Conclusion: The improved behavioral alignment of generative models can be largely attributed to low-pass filtering effects from image resizing operations, not fundamental architectural advantages. The human visual system's contrast sensitivity function is well-approximated by Gaussian filters of specific widths that maximize error consistency.

Abstract: Despite their impressive performance on computer vision benchmarks, Deep Neural Networks (DNNs) still fall short of adequately modeling human visual behavior, as measured by error consistency and shape bias. Recent work hypothesized that behavioral alignment can be drastically improved through \emph{generative} -- rather than \emph{discriminative} -- classifiers, with far-reaching implications for models of human vision.
  Here, we instead show that the increased alignment of generative models can be largely explained by a seemingly innocuous resizing operation in the generative model which effectively acts as a low-pass filter. In a series of controlled experiments, we show that removing high-frequency spatial information from discriminative models like CLIP drastically increases their behavioral alignment. Simply blurring images at test-time -- rather than training on blurred images -- achieves a new state-of-the-art score on the model-vs-human benchmark, halving the current alignment gap between DNNs and human observers. Furthermore, low-pass filters are likely optimal, which we demonstrate by directly optimizing filters for alignment. To contextualize the performance of optimal filters, we compute the frontier of all possible pareto-optimal solutions to the benchmark, which was formerly unknown.
  We explain our findings by observing that the frequency spectrum of optimal Gaussian filters roughly matches the spectrum of band-pass filters implemented by the human visual system. We show that the contrast sensitivity function, describing the inverse of the contrast threshold required for humans to detect a sinusoidal grating as a function of spatiotemporal frequency, is approximated well by Gaussian filters of the specific width that also maximizes error consistency.

</details>


### [82] [Human-Aligned Evaluation of a Pixel-wise DNN Color Constancy Model](https://arxiv.org/abs/2602.13887)
*Hamed Heidari-Gorji,Raquel Gil Rodriguez,Karl R. Gegenfurtner*

Main category: cs.CV

TL;DR: Researchers compared human color constancy performance with a DNN model using the same achromatic object selection task in VR, finding strong correspondence between model and human behavior across different color constancy mechanisms.


<details>
  <summary>Details</summary>
Motivation: To compare and study color constancy mechanisms (local surround, maximum flux, spatial mean) between computational models and human perception, rather than just evaluating models against physical ground truth.

Method: Used a ResNet-based U-Net pre-trained on rendered images to predict surface reflectance, then fine-tuned only the decoder on VR baseline images. Applied the model to perform the same achromatic object selection task used in human experiments across different conditions.

Result: Strong correspondence between model and human behavior - both achieved high constancy under baseline conditions and showed similar, condition-dependent performance declines when local surround or spatial mean color cues were removed.

Conclusion: The DNN model successfully captures human-like color constancy mechanisms, demonstrating that computational models can be meaningfully compared to human performance using identical behavioral tasks rather than just physical ground truth evaluation.

Abstract: We previously investigated color constancy in photorealistic virtual reality (VR) and developed a Deep Neural Network (DNN) that predicts reflectance from rendered images. Here, we combine both approaches to compare and study a model and human performance with respect to established color constancy mechanisms: local surround, maximum flux and spatial mean. Rather than evaluating the model against physical ground truth, model performance was assessed using the same achromatic object selection task employed in the human experiments. The model, a ResNet based U-Net from our previous work, was pre-trained on rendered images to predict surface reflectance. We then applied transfer learning, fine-tuning only the network's decoder on images from the baseline VR condition. To parallel the human experiment, the model's output was used to perform the same achromatic object selection task across all conditions. Results show a strong correspondence between the model and human behavior. Both achieved high constancy under baseline conditions and showed similar, condition-dependent performance declines when the local surround or spatial mean color cues were removed.

</details>


### [83] [Parameter-Efficient Fine-Tuning of DINOv2 for Large-Scale Font Classification](https://arxiv.org/abs/2602.13889)
*Daniel Chen,Zaria Zinn,Marcus Lowe*

Main category: cs.CV

TL;DR: Fine-tuned DINOv2 Vision Transformer using LoRA achieves ~86% accuracy for 394 font classification with <1% parameter training, using synthetic Google Fonts dataset with diverse augmentations.


<details>
  <summary>Details</summary>
Motivation: To create an accurate and efficient font classification system that can identify a large number of font families (394) from rendered text images, while minimizing computational costs and ensuring generalization to real-world typographic samples.

Method: Fine-tunes DINOv2 Vision Transformer using Low-Rank Adaptation (LoRA) to train fewer than 1% of parameters. Uses synthetic dataset generation pipeline that renders Google Fonts at scale with diverse augmentations (randomized colors, alignment, line wrapping, Gaussian noise). Includes built-in preprocessing for consistency and deploys as HuggingFace Inference Endpoint.

Result: Achieves approximately 86% top-1 accuracy for classifying 394 font families while training only a small fraction (<1%) of the model's 87.2M parameters. The synthetic dataset approach generalizes well to real-world typographic samples.

Conclusion: The proposed system demonstrates that efficient font classification at scale is achievable through LoRA fine-tuning of vision transformers and synthetic data generation. The open-source release of model, dataset, and training pipeline enables broader adoption and further research in typographic analysis.

Abstract: We present a font classification system capable of identifying 394 font families from rendered text images. Our approach fine-tunes a DINOv2 Vision Transformer using Low-Rank Adaptation (LoRA), achieving approximately 86% top-1 accuracy while training fewer than 1% of the model's 87.2M parameters. We introduce a synthetic dataset generation pipeline that renders Google Fonts at scale with diverse augmentations including randomized colors, alignment, line wrapping, and Gaussian noise, producing training images that generalize to real-world typographic samples. The model incorporates built-in preprocessing to ensure consistency between training and inference, and is deployed as a HuggingFace Inference Endpoint. We release the model, dataset, and full training pipeline as open-source resources.

</details>


### [84] [MamaDino: A Hybrid Vision Model for Breast Cancer 3-Year Risk Prediction](https://arxiv.org/abs/2602.13930)
*Ruggiero Santeramo,Igor Zubarev,Florian Jug*

Main category: cs.CV

TL;DR: MamaDino matches state-of-the-art breast cancer risk prediction (Mirai) using 13x fewer pixels by combining CNN and transformer features with explicit contralateral asymmetry modeling.


<details>
  <summary>Details</summary>
Motivation: Current deep learning systems for breast cancer risk prediction use high-resolution images and simple multi-view fusion with limited explicit modeling of contralateral asymmetry. The authors hypothesize that combining complementary inductive biases with explicit asymmetry modeling can achieve state-of-the-art performance with lower-resolution images.

Method: MamaDino fuses frozen self-supervised DINOv3 ViT-S features with a trainable CNN encoder at 512x512 resolution, and aggregates bilateral breast information via a BilateralMixer to output 3-year breast cancer risk scores. Trained on 53,883 women from OPTIMAM dataset.

Result: MamaDino matches Mirai's performance on both internal and external tests while using ~13x fewer input pixels. Adding BilateralMixer improves discrimination to AUC 0.736 (vs 0.713) in-distribution and 0.677 (vs 0.666) out-of-distribution, with consistent performance across demographic and clinical factors.

Conclusion: Explicit contralateral modeling and complementary inductive biases enable state-of-the-art breast cancer risk prediction with substantially lower-resolution mammograms, demonstrating that using less detailed images in a more structured way can recover top accuracy.

Abstract: Breast cancer screening programmes increasingly seek to move from one-size-fits-all interval to risk-adapted and personalized strategies. Deep learning (DL) has enabled image-based risk models with stronger 1- to 5-year prediction than traditional clinical models, but leading systems (e.g., Mirai) typically use convolutional backbones, very high-resolution inputs (>1M pixels) and simple multi-view fusion, with limited explicit modelling of contralateral asymmetry.
  We hypothesised that combining complementary inductive biases (convolutional and transformer-based) with explicit contralateral asymmetry modelling would allow us to match state-of-the-art 3-year risk prediction performance even when operating on substantially lower-resolution mammograms, indicating that using less detailed images in a more structured way can recover state-of-the-art accuracy.
  We present MamaDino, a mammography-aware multi-view attentional DINO model. MamaDino fuses frozen self-supervised DINOv3 ViT-S features with a trainable CNN encoder at 512x512 resolution, and aggregates bilateral breast information via a BilateralMixer to output a 3-year breast cancer risk score. We train on 53,883 women from OPTIMAM (UK) and evaluate on matched 3-year case-control cohorts: an in-distribution test set from four screening sites and an external out-of-distribution cohort from an unseen site.
  At breast-level, MamaDino matches Mirai on both internal and external tests while using ~13x fewer input pixels. Adding the BilateralMixer improves discrimination to AUC 0.736 (vs 0.713) in-distribution and 0.677 (vs 0.666) out-of-distribution, with consistent performance across age, ethnicity, scanner, tumour type and grade. These findings demonstrate that explicit contralateral modelling and complementary inductive biases enable predictions that match Mirai, despite operating on substantially lower-resolution mammograms.

</details>


### [85] [Fusing Pixels and Genes: Spatially-Aware Learning in Computational Pathology](https://arxiv.org/abs/2602.13944)
*Minghao Han,Dingkang Yang,Linhao Qu,Zizhi Chen,Gang Li,Han Wang,Jiacong Wang,Lihua Zhang*

Main category: cs.CV

TL;DR: STAMP integrates spatial transcriptomics with pathology images for molecule-guided multimodal learning, achieving strong performance across multiple tasks.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal pathology models rely on vision and language, but language lacks molecular specificity and offers limited pathological supervision, creating representational bottlenecks.

Method: Proposed STAMP framework with spatial transcriptomics-augmented multimodal learning, hierarchical multi-scale contrastive alignment, cross-scale patch localization, and trained on SpaVis-6M dataset.

Result: Validated across six datasets and four downstream tasks with consistent strong performance, demonstrating the value of spatially resolved molecular supervision.

Conclusion: Integrating spatially resolved molecular supervision is valuable and necessary for advancing multimodal learning in computational pathology.

Abstract: Recent years have witnessed remarkable progress in multimodal learning within computational pathology. Existing models primarily rely on vision and language modalities; however, language alone lacks molecular specificity and offers limited pathological supervision, leading to representational bottlenecks. In this paper, we propose STAMP, a Spatial Transcriptomics-Augmented Multimodal Pathology representation learning framework that integrates spatially-resolved gene expression profiles to enable molecule-guided joint embedding of pathology images and transcriptomic data. Our study shows that self-supervised, gene-guided training provides a robust and task-agnostic signal for learning pathology image representations. Incorporating spatial context and multi-scale information further enhances model performance and generalizability. To support this, we constructed SpaVis-6M, the largest Visium-based spatial transcriptomics dataset to date, and trained a spatially-aware gene encoder on this resource. Leveraging hierarchical multi-scale contrastive alignment and cross-scale patch localization mechanisms, STAMP effectively aligns spatial transcriptomics with pathology images, capturing spatial structure and molecular variation. We validate STAMP across six datasets and four downstream tasks, where it consistently achieves strong performance. These results highlight the value and necessity of integrating spatially resolved molecular supervision for advancing multimodal learning in computational pathology. The code is included in the supplementary materials. The pretrained weights and SpaVis-6M are available at: https://github.com/Hanminghao/STAMP.

</details>


### [86] [MarsRetrieval: Benchmarking Vision-Language Models for Planetary-Scale Geospatial Retrieval on Mars](https://arxiv.org/abs/2602.13961)
*Shuoyuan Wang,Yiran Wang,Hongxin Wei*

Main category: cs.CV

TL;DR: MarsRetrieval is a new benchmark for evaluating vision-language models on Martian geospatial discovery tasks, showing that even strong foundation models struggle with domain-specific geomorphic distinctions and require fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for planetary science are limited to closed-set supervised visual tasks and lack text-guided retrieval capabilities needed for geospatial discovery on Mars.

Method: Introduces MarsRetrieval benchmark with three tasks: paired image-text retrieval, landform retrieval, and global geo-localization. Proposes unified retrieval-centric protocol to evaluate multimodal embedding architectures including contrastive dual-tower encoders and generative vision-language models.

Result: MarsRetrieval is challenging - even strong foundation models often fail to capture domain-specific geomorphic distinctions. Domain-specific fine-tuning is critical for generalizable geospatial discovery in planetary settings.

Conclusion: MarsRetrieval provides a comprehensive benchmark for evaluating vision-language models on Martian geospatial discovery, highlighting the need for domain-specific adaptation and offering a foundation for future research in planetary science AI.

Abstract: Data-driven approaches like deep learning are rapidly advancing planetary science, particularly in Mars exploration. Despite recent progress, most existing benchmarks remain confined to closed-set supervised visual tasks and do not support text-guided retrieval for geospatial discovery. We introduce MarsRetrieval, a retrieval benchmark for evaluating vision-language models for Martian geospatial discovery. MarsRetrieval includes three tasks: (1) paired image-text retrieval, (2) landform retrieval, and (3) global geo-localization, covering multiple spatial scales and diverse geomorphic origins. We propose a unified retrieval-centric protocol to benchmark multimodal embedding architectures, including contrastive dual-tower encoders and generative vision-language models. Our evaluation shows MarsRetrieval is challenging: even strong foundation models often fail to capture domain-specific geomorphic distinctions. We further show that domain-specific fine-tuning is critical for generalizable geospatial discovery in planetary settings. Our code is available at https://github.com/ml-stat-Sustech/MarsRetrieval

</details>


### [87] [Elastic Diffusion Transformer](https://arxiv.org/abs/2602.13993)
*Jiangshan Wang,Zeqiang Lai,Jiarui Chen,Jiayi Guo,Hang Guo,Xiu Li,Xiangyu Yue,Chunchao Guo*

Main category: cs.CV

TL;DR: E-DiT is an adaptive acceleration framework for Diffusion Transformers that dynamically skips blocks and reduces MLP width based on input sparsity, achieving ~2× speedup with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: Current DiT acceleration methods (pruning, distillation) use fixed computational capacity, leading to insufficient acceleration and degraded quality. The authors observed that DiT generative processes exhibit substantial and sample-dependent sparsity that can be leveraged for adaptive acceleration.

Method: E-DiT adds lightweight routers to each DiT block that dynamically identify sample-dependent sparsity from input latents. Each router decides whether to skip the block and predicts optimal MLP width reduction ratio. During inference, a block-level feature caching mechanism eliminates redundant computations without training.

Result: Extensive experiments on 2D image (Qwen-Image, FLUX) and 3D asset (Hunyuan3D-3.0) generation show E-DiT achieves up to ~2× speedup with negligible loss in generation quality.

Conclusion: E-DiT provides an effective adaptive acceleration framework for DiT that maintains generation quality while significantly improving efficiency through dynamic sparsity-aware computation skipping and width reduction.

Abstract: Diffusion Transformers (DiT) have demonstrated remarkable generative capabilities but remain highly computationally expensive. Previous acceleration methods, such as pruning and distillation, typically rely on a fixed computational capacity, leading to insufficient acceleration and degraded generation quality. To address this limitation, we propose \textbf{Elastic Diffusion Transformer (E-DiT)}, an adaptive acceleration framework for DiT that effectively improves efficiency while maintaining generation quality. Specifically, we observe that the generative process of DiT exhibits substantial sparsity (i.e., some computations can be skipped with minimal impact on quality), and this sparsity varies significantly across samples. Motivated by this observation, E-DiT equips each DiT block with a lightweight router that dynamically identifies sample-dependent sparsity from the input latent. Each router adaptively determines whether the corresponding block can be skipped. If the block is not skipped, the router then predicts the optimal MLP width reduction ratio within the block. During inference, we further introduce a block-level feature caching mechanism that leverages router predictions to eliminate redundant computations in a training-free manner. Extensive experiments across 2D image (Qwen-Image and FLUX) and 3D asset (Hunyuan3D-3.0) demonstrate the effectiveness of E-DiT, achieving up to $\sim$2$\times$ speedup with negligible loss in generation quality. Code will be available at https://github.com/wangjiangshan0725/Elastic-DiT.

</details>


### [88] [Inject Where It Matters: Training-Free Spatially-Adaptive Identity Preservation for Text-to-Image Personalization](https://arxiv.org/abs/2602.13994)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: SpatialID is a training-free framework for personalized text-to-image generation that uses spatially-adaptive identity modulation to prevent identity features from contaminating non-facial regions while maintaining text adherence.


<details>
  <summary>Details</summary>
Motivation: Existing tuning-free methods for personalized text-to-image generation suffer from identity features contaminating non-facial regions (backgrounds, lighting), which degrades text adherence. Current approaches use Spatially Uniform Visual Injection that doesn't distinguish between face-relevant and context-free areas.

Method: SpatialID introduces a training-free spatially-adaptive identity modulation framework that: 1) Decouples identity injection into face-relevant and context-free regions using a Spatial Mask Extractor derived from cross-attention responses; 2) Implements Temporal-Spatial Scheduling that dynamically adjusts spatial constraints, transitioning from Gaussian priors to attention-based masks with adaptive relaxation to align with diffusion generation dynamics.

Result: Extensive experiments on IBench show SpatialID achieves state-of-the-art performance in text adherence (CLIP-T: 0.281), visual consistency (CLIP-I: 0.827), and image quality (IQ: 0.523). It significantly eliminates background contamination while maintaining robust identity preservation.

Conclusion: SpatialID provides an effective training-free solution for personalized text-to-image generation that addresses the key limitation of identity contamination in non-facial regions through spatially-adaptive modulation and temporal-spatial scheduling, achieving superior performance across multiple metrics without requiring expensive fine-tuning.

Abstract: Personalized text-to-image generation aims to integrate specific identities into arbitrary contexts. However, existing tuning-free methods typically employ Spatially Uniform Visual Injection, causing identity features to contaminate non-facial regions (e.g., backgrounds and lighting) and degrading text adherence. To address this without expensive fine-tuning, we propose SpatialID, a training-free spatially-adaptive identity modulation framework. SpatialID fundamentally decouples identity injection into face-relevant and context-free regions using a Spatial Mask Extractor derived from cross-attention responses. Furthermore, we introduce a Temporal-Spatial Scheduling strategy that dynamically adjusts spatial constraints - transitioning from Gaussian priors to attention-based masks and adaptive relaxation - to align with the diffusion generation dynamics. Extensive experiments on IBench demonstrate that SpatialID achieves state-of-the-art performance in text adherence (CLIP-T: 0.281), visual consistency (CLIP-I: 0.827), and image quality (IQ: 0.523), significantly eliminating background contamination while maintaining robust identity preservation.

</details>


### [89] [A Deployment-Friendly Foundational Framework for Efficient Computational Pathology](https://arxiv.org/abs/2602.14010)
*Yu Cai,Cheng Jin,Jiabo Ma,Fengtao Zhou,Yingxue Xu,Zhengrui Guo,Yihui Wang,Zhengyu Zhang,Ling Liang,Yonghao Tan,Pingcheng Dong,Du Cai,On Ki Tang,Chenglong Zhao,Xi Wang,Can Yang,Yali Xu,Jing Cui,Zhenhui Li,Ronald Cheong Kin Chan,Yueping Liu,Feng Gao,Xiuming Zhang,Li Liang,Hao Chen,Kwang-Ting Cheng*

Main category: cs.CV

TL;DR: LitePath is a deployment-friendly pathology foundation model framework that reduces computational costs 403x while maintaining 99.71% of state-of-the-art accuracy, enabling efficient deployment on edge hardware.


<details>
  <summary>Details</summary>
Motivation: Pathology foundation models (PFMs) have strong generalization capabilities but suffer from high computational costs that limit clinical accessibility and scalability, especially for gigapixel whole slide images.

Method: LitePath integrates LiteFM (a compact model distilled from three large PFMs using 190M patches) and Adaptive Patch Selector (APS) for task-specific patch selection, reducing parameters 28x and FLOPs 403.5x.

Result: On NVIDIA Jetson Orin Nano, LitePath processes 208 slides/hour (104.5x faster than Virchow2) with 0.36 kWh/3,000 slides (171x lower energy). It ranks 2nd among 19 models, outperforming larger models while retaining 99.71% of Virchow2's AUC.

Conclusion: LitePath enables rapid, cost-effective, energy-efficient pathology analysis on accessible hardware while maintaining accuracy comparable to state-of-the-art PFMs, with the highest Deployability Score (10.64% better than Virchow2).

Abstract: Pathology foundation models (PFMs) have enabled robust generalization in computational pathology through large-scale datasets and expansive architectures, but their substantial computational cost, particularly for gigapixel whole slide images, limits clinical accessibility and scalability. Here, we present LitePath, a deployment-friendly foundational framework designed to mitigate model over-parameterization and patch level redundancy. LitePath integrates LiteFM, a compact model distilled from three large PFMs (Virchow2, H-Optimus-1 and UNI2) using 190 million patches, and the Adaptive Patch Selector (APS), a lightweight component for task-specific patch selection. The framework reduces model parameters by 28x and lowers FLOPs by 403.5x relative to Virchow2, enabling deployment on low-power edge hardware such as the NVIDIA Jetson Orin Nano Super. On this device, LitePath processes 208 slides per hour, 104.5x faster than Virchow2, and consumes 0.36 kWh per 3,000 slides, 171x lower than Virchow2 on an RTX3090 GPU. We validated accuracy using 37 cohorts across four organs and 26 tasks (26 internal, 9 external, and 2 prospective), comprising 15,672 slides from 9,808 patients disjoint from the pretraining data. LitePath ranks second among 19 evaluated models and outperforms larger models including H-Optimus-1, mSTAR, UNI2 and GPFM, while retaining 99.71% of the AUC of Virchow2 on average. To quantify the balance between accuracy and efficiency, we propose the Deployability Score (D-Score), defined as the weighted geometric mean of normalized AUC and normalized FLOP, where LitePath achieves the highest value, surpassing Virchow2 by 10.64%. These results demonstrate that LitePath enables rapid, cost-effective and energy-efficient pathology image analysis on accessible hardware while maintaining accuracy comparable to state-of-the-art PFMs and reducing the carbon footprint of AI deployment.

</details>


### [90] [Flow4R: Unifying 4D Reconstruction and Tracking with Scene Flow](https://arxiv.org/abs/2602.14021)
*Shenhan Qian,Ganlin Zhang,Shangzhe Wu,Daniel Cremers*

Main category: cs.CV

TL;DR: Flow4R is a unified framework that uses camera-space scene flow as central representation for 4D reconstruction and tracking, predicting per-pixel properties from two-view inputs with a Vision Transformer.


<details>
  <summary>Details</summary>
Motivation: Existing approaches decouple geometry from motion: static reconstruction methods assume no motion, while dynamic tracking requires explicit camera pose estimation or separate motion models. There's a need for a unified approach to handle both static and dynamic scenes.

Method: Flow4R treats camera-space scene flow as central representation linking 3D structure, object motion, and camera motion. It predicts per-pixel properties (3D point position, scene flow, pose weight, confidence) from two-view inputs using a Vision Transformer. Uses flow-centric formulation with shared decoder for symmetric inference of local geometry and bidirectional motion in single forward pass, without explicit pose regressors or bundle adjustment.

Result: Achieves state-of-the-art performance on 4D reconstruction and tracking tasks. Demonstrates effectiveness of flow-central representation for spatiotemporal scene understanding.

Conclusion: Flow4R provides a unified framework that successfully integrates geometry and motion through scene flow representation, enabling efficient 4D reconstruction and tracking without traditional pose estimation components.

Abstract: Reconstructing and tracking dynamic 3D scenes remains a fundamental challenge in computer vision. Existing approaches often decouple geometry from motion: multi-view reconstruction methods assume static scenes, while dynamic tracking frameworks rely on explicit camera pose estimation or separate motion models. We propose Flow4R, a unified framework that treats camera-space scene flow as the central representation linking 3D structure, object motion, and camera motion. Flow4R predicts a minimal per-pixel property set-3D point position, scene flow, pose weight, and confidence-from two-view inputs using a Vision Transformer. This flow-centric formulation allows local geometry and bidirectional motion to be inferred symmetrically with a shared decoder in a single forward pass, without requiring explicit pose regressors or bundle adjustment. Trained jointly on static and dynamic datasets, Flow4R achieves state-of-the-art performance on 4D reconstruction and tracking tasks, demonstrating the effectiveness of the flow-central representation for spatiotemporal scene understanding.

</details>


### [91] [Train Short, Inference Long: Training-free Horizon Extension for Autoregressive Video Generation](https://arxiv.org/abs/2602.14027)
*Jia Li,Xiaomeng Fu,Xurui Peng,Weifeng Chen,Youwei Zheng,Tianyu Zhao,Jiexi Wang,Fangmin Chen,Xing Wang,Hayden Kwok-Hay So*

Main category: cs.CV

TL;DR: FLEX is a training-free inference framework that addresses extrapolation failure in autoregressive video diffusion models by using frequency-aware RoPE modulation and antiphase noise sampling to enable long video generation without retraining.


<details>
  <summary>Details</summary>
Motivation: Autoregressive video diffusion models suffer from severe extrapolation failure when generating videos beyond training horizons due to spectral bias in 3D positional embeddings and lack of dynamic priors in noise sampling, leading to temporal degradation.

Method: FLEX introduces three key components: 1) Frequency-aware RoPE Modulation to adaptively interpolate under-trained low-frequency components while extrapolating high-frequency ones, 2) Antiphase Noise Sampling (ANS) to inject high-frequency dynamic priors, and 3) Inference-only Attention Sink to anchor global structure.

Result: FLEX significantly outperforms state-of-the-art models at 6× extrapolation (30s duration) and matches performance of long-video fine-tuned baselines at 12× scale (60s duration). It enables consistent video synthesis at 4-minute scale with existing models like LongLive.

Conclusion: FLEX provides an effective plug-and-play solution for extending video generation horizons without retraining, addressing fundamental limitations in autoregressive video diffusion models through frequency-aware modulation and dynamic prior injection.

Abstract: Autoregressive video diffusion models have emerged as a scalable paradigm for long video generation. However, they often suffer from severe extrapolation failure, where rapid error accumulation leads to significant temporal degradation when extending beyond training horizons. We identify that this failure primarily stems from the \textit{spectral bias} of 3D positional embeddings and the lack of \textit{dynamic priors} in noise sampling. To address these issues, we propose \textbf{FLEX} (\textbf{F}requency-aware \textbf{L}ength \textbf{EX}tension), a training-free inference-time framework that bridges the gap between short-term training and long-term inference. FLEX introduces Frequency-aware RoPE Modulation to adaptively interpolate under-trained low-frequency components while extrapolating high-frequency ones to preserve multi-scale temporal discriminability. This is integrated with Antiphase Noise Sampling (ANS) to inject high-frequency dynamic priors and Inference-only Attention Sink to anchor global structure. Extensive evaluations on VBench demonstrate that FLEX significantly outperforms state-of-the-art models at $6\times$ extrapolation (30s duration) and matches the performance of long-video fine-tuned baselines at $12\times$ scale (60s duration). As a plug-and-play augmentation, FLEX seamlessly integrates into existing inference pipelines for horizon extension. It effectively pushes the generation limits of models such as LongLive, supporting consistent and dynamic video synthesis at a 4-minute scale. Project page is available at \href{https://ga-lee.github.io/FLEX_demo}{https://ga-lee.github.io/FLEX}.

</details>


### [92] [Explainability-Inspired Layer-Wise Pruning of Deep Neural Networks for Efficient Object Detection](https://arxiv.org/abs/2602.14040)
*Abhinav Shukla,Nachiket Tapas*

Main category: cs.CV

TL;DR: An explainability-inspired layer-wise pruning framework for object detection that uses SHAP-inspired gradient-activation attribution to assess layer importance, outperforming traditional magnitude-based pruning methods in accuracy-efficiency trade-offs.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks for object detection are too complex for resource-constrained platforms, and traditional magnitude-based pruning methods don't align with the true functional contribution of network components to task-specific performance.

Method: Proposes an explainability-inspired, layer-wise pruning framework that leverages SHAP-inspired gradient-activation attribution to estimate layer importance as a data-driven proxy for functional contribution, rather than relying on static weight magnitudes.

Result: The method consistently identifies different layers as least important compared to L1-norm-based methods, leading to improved accuracy-efficiency trade-offs. For ShuffleNetV2: 10% empirical increase in inference speed vs. L1-pruning's 13.7% degradation. For RetinaNet: preserves baseline mAP (0.151) with negligible speed impact vs. L1-pruning's 1.3% mAP drop for 6.2% speed increase.

Conclusion: Explainability-inspired compression offers a principled direction for deploying deep neural networks on edge and resource-constrained platforms while preserving both performance and interpretability, highlighting the importance of data-driven layer importance assessment.

Abstract: Deep neural networks (DNNs) have achieved remarkable success in object detection tasks, but their increasing complexity poses significant challenges for deployment on resource-constrained platforms. While model compression techniques such as pruning have emerged as essential tools, traditional magnitude-based pruning methods do not necessarily align with the true functional contribution of network components to task-specific performance. In this work, we present an explainability-inspired, layer-wise pruning framework tailored for efficient object detection. Our approach leverages a SHAP-inspired gradient--activation attribution to estimate layer importance, providing a data-driven proxy for functional contribution rather than relying solely on static weight magnitudes. We conduct comprehensive experiments across diverse object detection architectures, including ResNet-50, MobileNetV2, ShuffleNetV2, Faster R-CNN, RetinaNet, and YOLOv8, evaluating performance on the Microsoft COCO 2017 validation set. The results show that the proposed attribution-inspired pruning consistently identifies different layers as least important compared to L1-norm-based methods, leading to improved accuracy--efficiency trade-offs. Notably, for ShuffleNetV2, our method yields a 10\% empirical increase in inference speed, whereas L1-pruning degrades performance by 13.7\%. For RetinaNet, the proposed approach preserves the baseline mAP (0.151) with negligible impact on inference speed, while L1-pruning incurs a 1.3\% mAP drop for a 6.2\% speed increase. These findings highlight the importance of data-driven layer importance assessment and demonstrate that explainability-inspired compression offers a principled direction for deploying deep neural networks on edge and resource-constrained platforms while preserving both performance and interpretability.

</details>


### [93] [BitDance: Scaling Autoregressive Generative Models with Binary Tokens](https://arxiv.org/abs/2602.14041)
*Yuang Ai,Jiaming Han,Shaobin Zhuang,Weijia Mao,Xuefeng Hu,Ziyan Yang,Zhenheng Yang,Huaibo Huang,Xiangyu Yue,Hao Chen*

Main category: cs.CV

TL;DR: BitDance: Scalable AR image generator using binary visual tokens with diffusion-based decoding, achieving SOTA FID of 1.24 on ImageNet 256x256 with 5.4x fewer parameters and 8.7x speedup.


<details>
  <summary>Details</summary>
Motivation: Standard AR image generators use codebook indices which have limited expressiveness. Binary tokens offer much higher entropy (2^256 states per token) for more compact and expressive representations, but sampling from such huge space is challenging with traditional classification methods.

Method: Uses binary visual tokens instead of codebook indices. Employs binary diffusion head (continuous-space diffusion) instead of softmax classification to generate binary tokens. Introduces next-patch diffusion for parallel prediction of multiple tokens, speeding up inference.

Result: Achieves FID 1.24 on ImageNet 256x256 (best among AR models). With 260M parameters (5.4x fewer than 1.4B parameter models), achieves 8.7x speedup. For text-to-image, generates high-resolution photorealistic images efficiently. 30x speedup for 1024x1024 images compared to prior AR models.

Conclusion: BitDance demonstrates that binary tokens with diffusion-based decoding enable highly efficient and expressive AR image generation, achieving state-of-the-art performance with significantly fewer parameters and faster inference, making AR foundation models more practical.

Abstract: We present BitDance, a scalable autoregressive (AR) image generator that predicts binary visual tokens instead of codebook indices. With high-entropy binary latents, BitDance lets each token represent up to $2^{256}$ states, yielding a compact yet highly expressive discrete representation. Sampling from such a huge token space is difficult with standard classification. To resolve this, BitDance uses a binary diffusion head: instead of predicting an index with softmax, it employs continuous-space diffusion to generate the binary tokens. Furthermore, we propose next-patch diffusion, a new decoding method that predicts multiple tokens in parallel with high accuracy, greatly speeding up inference. On ImageNet 256x256, BitDance achieves an FID of 1.24, the best among AR models. With next-patch diffusion, BitDance beats state-of-the-art parallel AR models that use 1.4B parameters, while using 5.4x fewer parameters (260M) and achieving 8.7x speedup. For text-to-image generation, BitDance trains on large-scale multimodal tokens and generates high-resolution, photorealistic images efficiently, showing strong performance and favorable scaling. When generating 1024x1024 images, BitDance achieves a speedup of over 30x compared to prior AR models. We release code and models to facilitate further research on AR foundation models. Code and models are available at: https://github.com/shallowdream204/BitDance.

</details>


### [94] [Restoration Adaptation for Semantic Segmentation on Low Quality Images](https://arxiv.org/abs/2602.14042)
*Kai Guan,Rongyuan Wu,Shuai Li,Wentao Zhu,Wenjun Zeng,Lei Zhang*

Main category: cs.CV

TL;DR: RASS integrates semantic image restoration with segmentation to handle low-quality images, using semantic-constrained restoration and knowledge transfer to improve segmentation robustness.


<details>
  <summary>Details</summary>
Motivation: Semantic segmentation performance degrades on low-quality images lacking clear structures and details. Existing restoration models focus on pixel fidelity but miss semantic cues, while segmentation models lack robustness to real-world degradations.

Method: Proposes Restoration Adaptation for Semantic Segmentation (RASS) with two components: 1) Semantic-Constrained Restoration (SCR) that aligns cross-attention maps with segmentation masks to inject semantic priors, and 2) LoRA-based module merging and fine-tuning to transfer restoration knowledge to segmentation.

Result: Significantly outperforms state-of-the-art methods on both synthetic and real-world low-quality benchmarks. Constructed a real-world LQ image segmentation dataset with high-quality annotations.

Conclusion: RASS effectively integrates semantic restoration into segmentation, enabling high-quality segmentation on low-quality images directly. The framework bridges the gap between restoration and segmentation for real-world applications.

Abstract: In real-world scenarios, the performance of semantic segmentation often deteriorates when processing low-quality (LQ) images, which may lack clear semantic structures and high-frequency details. Although image restoration techniques offer a promising direction for enhancing degraded visual content, conventional real-world image restoration (Real-IR) models primarily focus on pixel-level fidelity and often fail to recover task-relevant semantic cues, limiting their effectiveness when directly applied to downstream vision tasks. Conversely, existing segmentation models trained on high-quality data lack robustness under real-world degradations. In this paper, we propose Restoration Adaptation for Semantic Segmentation (RASS), which effectively integrates semantic image restoration into the segmentation process, enabling high-quality semantic segmentation on the LQ images directly. Specifically, we first propose a Semantic-Constrained Restoration (SCR) model, which injects segmentation priors into the restoration model by aligning its cross-attention maps with segmentation masks, encouraging semantically faithful image reconstruction. Then, RASS transfers semantic restoration knowledge into segmentation through LoRA-based module merging and task-specific fine-tuning, thereby enhancing the model's robustness to LQ images. To validate the effectiveness of our framework, we construct a real-world LQ image segmentation dataset with high-quality annotations, and conduct extensive experiments on both synthetic and real-world LQ benchmarks. The results show that SCR and RASS significantly outperform state-of-the-art methods in segmentation and restoration tasks. Code, models, and datasets will be available at https://github.com/Ka1Guan/RASS.git.

</details>


### [95] [CoCoEdit: Content-Consistent Image Editing via Region Regularized Reinforcement Learning](https://arxiv.org/abs/2602.14068)
*Yuhui Wu,Chenxi Xie,Ruibin Li,Liyi Chen,Qiaosi Yi,Lei Zhang*

Main category: cs.CV

TL;DR: CoCoEdit is a post-training framework using region-regularized reinforcement learning to improve content consistency in image editing while maintaining editing quality.


<details>
  <summary>Details</summary>
Motivation: Existing image editing models focus on intended editing effects but often cause unwanted changes in unintended regions, lacking content consistency.

Method: 1) Curate 40K high-quality training samples with refined instructions and masks; 2) Use pixel-level similarity reward + MLLM-based rewards; 3) Implement region-based regularizer to preserve non-edited regions for high-reward samples and encourage editing for low-reward samples.

Result: Applied to Qwen-Image-Edit and FLUX-Kontext, achieves competitive editing scores with SOTA models while significantly better content consistency (measured by PSNR/SSIM metrics and human ratings).

Conclusion: CoCoEdit effectively addresses content consistency in image editing through region-regularized reinforcement learning, balancing editing quality with preservation of unintended regions.

Abstract: Image editing has achieved impressive results with the development of large-scale generative models. However, existing models mainly focus on the editing effects of intended objects and regions, often leading to unwanted changes in unintended regions. We present a post-training framework for Content-Consistent Editing (CoCoEdit) via region regularized reinforcement learning. We first augment existing editing datasets with refined instructions and masks, from which 40K diverse and high quality samples are curated as training set. We then introduce a pixel-level similarity reward to complement MLLM-based rewards, enabling models to ensure both editing quality and content consistency during the editing process. To overcome the spatial-agnostic nature of the rewards, we propose a region-based regularizer, aiming to preserve non-edited regions for high-reward samples while encouraging editing effects for low-reward samples. For evaluation, we annotate editing masks for GEdit-Bench and ImgEdit-Bench, introducing pixel-level similarity metrics to measure content consistency and editing quality. Applying CoCoEdit to Qwen-Image-Edit and FLUX-Kontext, we achieve not only competitive editing scores with state-of-the-art models, but also significantly better content consistency, measured by PSNR/SSIM metrics and human subjective ratings.

</details>


### [96] [ForgeryVCR: Visual-Centric Reasoning via Efficient Forensic Tools in MLLMs for Image Forgery Detection and Localization](https://arxiv.org/abs/2602.14098)
*Youqi Wang,Shen Chen,Haowei Wang,Rongxuan Peng,Taiping Yao,Shunquan Tan,Changsheng Chen,Bin Li,Shouhong Ding*

Main category: cs.CV

TL;DR: ForgeryVCR is a visual-centric reasoning framework for image forgery detection that uses forensic tools to make imperceptible tampering traces visible, achieving SOTA performance through strategic tool learning.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs for image forgery detection rely on text-centric reasoning, which causes hallucinations when trying to describe imperceptible low-level tampering traces since language cannot capture fine-grained pixel-level inconsistencies.

Method: Proposes ForgeryVCR with: 1) Forensic toolbox to materialize imperceptible traces into visual intermediates via Visual-Centric Reasoning; 2) Strategic Tool Learning post-training with gain-driven trajectory construction for SFT and RL optimization guided by tool utility reward; 3) Enables MLLM to proactively invoke multi-view reasoning paths including local zoom-in, compression history analysis, noise residuals, and frequency domain inspection.

Result: Achieves state-of-the-art performance in both detection and localization tasks, demonstrating superior generalization and robustness with minimal tool redundancy.

Conclusion: ForgeryVCR successfully overcomes limitations of text-centric approaches by incorporating visual-centric reasoning and strategic tool learning, enabling more accurate and robust image forgery detection and localization.

Abstract: Existing Multimodal Large Language Models (MLLMs) for image forgery detection and localization predominantly operate under a text-centric Chain-of-Thought (CoT) paradigm. However, forcing these models to textually characterize imperceptible low-level tampering traces inevitably leads to hallucinations, as linguistic modalities are insufficient to capture such fine-grained pixel-level inconsistencies. To overcome this, we propose ForgeryVCR, a framework that incorporates a forensic toolbox to materialize imperceptible traces into explicit visual intermediates via Visual-Centric Reasoning. To enable efficient tool utilization, we introduce a Strategic Tool Learning post-training paradigm, encompassing gain-driven trajectory construction for Supervised Fine-Tuning (SFT) and subsequent Reinforcement Learning (RL) optimization guided by a tool utility reward. This paradigm empowers the MLLM to act as a proactive decision-maker, learning to spontaneously invoke multi-view reasoning paths including local zoom-in for fine-grained inspection and the analysis of invisible inconsistencies in compression history, noise residuals, and frequency domains. Extensive experiments reveal that ForgeryVCR achieves state-of-the-art (SOTA) performance in both detection and localization tasks, demonstrating superior generalization and robustness with minimal tool redundancy. The project page is available at https://youqiwong.github.io/projects/ForgeryVCR/.

</details>


### [97] [GeoFusionLRM: Geometry-Aware Self-Correction for Consistent 3D Reconstruction](https://arxiv.org/abs/2602.14119)
*Ahmet Burak Yildirim,Tuna Saygin,Duygu Ceylan,Aysegul Dundar*

Main category: cs.CV

TL;DR: GeoFusionLRM is a geometry-aware self-correction framework that improves 3D reconstruction from single images by using the model's own normal and depth predictions to refine structural accuracy without extra supervision.


<details>
  <summary>Details</summary>
Motivation: Current large reconstruction models (LRMs) for single-image 3D reconstruction often produce geometrically inconsistent results with misaligned details, limiting reconstruction fidelity.

Method: Introduces a geometry-aware self-correction framework that leverages the model's own normal and depth predictions through a dedicated transformer and fusion module to feed back geometric cues for error correction and consistency enforcement.

Result: Achieves sharper geometry, more consistent normals, and higher fidelity than state-of-the-art LRM baselines, improving alignment between reconstructed mesh and input views.

Conclusion: GeoFusionLRM effectively addresses geometric inconsistencies in single-image 3D reconstruction by enabling models to self-correct using their own geometric predictions, enhancing reconstruction quality without additional supervision.

Abstract: Single-image 3D reconstruction with large reconstruction models (LRMs) has advanced rapidly, yet reconstructions often exhibit geometric inconsistencies and misaligned details that limit fidelity. We introduce GeoFusionLRM, a geometry-aware self-correction framework that leverages the model's own normal and depth predictions to refine structural accuracy. Unlike prior approaches that rely solely on features extracted from the input image, GeoFusionLRM feeds back geometric cues through a dedicated transformer and fusion module, enabling the model to correct errors and enforce consistency with the conditioning image. This design improves the alignment between the reconstructed mesh and the input views without additional supervision or external signals. Extensive experiments demonstrate that GeoFusionLRM achieves sharper geometry, more consistent normals, and higher fidelity than state-of-the-art LRM baselines.

</details>


### [98] [EgoSound: Benchmarking Sound Understanding in Egocentric Videos](https://arxiv.org/abs/2602.14122)
*Bingwen Zhu,Yuqian Fu,Qiaole Dong,Guolei Sun,Tianwen Qian,Yuzheng Wu,Danda Pani Paudel,Xiangyang Xue,Yanwei Fu*

Main category: cs.CV

TL;DR: EgoSound is the first benchmark for evaluating egocentric sound understanding in MLLMs, covering 7 tasks across 7315 QA pairs from 900 videos, revealing current models' limitations in spatial and causal reasoning.


<details>
  <summary>Details</summary>
Motivation: Human perception is multisensory (sight, sound, motion), but current MLLMs focus mainly on vision-language understanding. Sound provides crucial cues about spatial layout, off-screen events, and causal interactions in egocentric settings where auditory and visual signals are tightly coupled.

Method: Created EgoSound benchmark by unifying data from Ego4D and EgoBlind datasets. Developed a seven-task taxonomy covering intrinsic sound perception, spatial localization, causal inference, and cross-modal reasoning. Used a multi-stage auto-generative pipeline to construct 7315 validated QA pairs across 900 videos.

Result: Comprehensive experiments on nine state-of-the-art MLLMs show that current models exhibit emerging auditory reasoning abilities but remain limited in fine-grained spatial and causal understanding.

Conclusion: EgoSound establishes a challenging foundation for advancing multisensory egocentric intelligence, bridging the gap between seeing and truly hearing the world. It reveals current limitations and provides a benchmark for future research in sound-aware MLLMs.

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved remarkable progress in vision-language understanding. Yet, human perception is inherently multisensory, integrating sight, sound, and motion to reason about the world. Among these modalities, sound provides indispensable cues about spatial layout, off-screen events, and causal interactions, particularly in egocentric settings where auditory and visual signals are tightly coupled. To this end, we introduce EgoSound, the first benchmark designed to systematically evaluate egocentric sound understanding in MLLMs. EgoSound unifies data from Ego4D and EgoBlind, encompassing both sighted and sound-dependent experiences. It defines a seven-task taxonomy spanning intrinsic sound perception, spatial localization, causal inference, and cross-modal reasoning. Constructed through a multi-stage auto-generative pipeline, EgoSound contains 7315 validated QA pairs across 900 videos. Comprehensive experiments on nine state-of-the-art MLLMs reveal that current models exhibit emerging auditory reasoning abilities but remain limited in fine-grained spatial and causal understanding. EgoSound establishes a challenging foundation for advancing multisensory egocentric intelligence, bridging the gap between seeing and truly hearing the world.

</details>


### [99] [DenseMLLM: Standard Multimodal LLMs are Intrinsic Dense Predictors](https://arxiv.org/abs/2602.14134)
*Yi Li,Hongze Shen,Lexiang Tang,Xin Li,Xinpeng Ding,Yinsong Liu,Deqiang Jiang,Xing Sun,Xiaomeng Li*

Main category: cs.CV

TL;DR: DenseMLLM enables standard multimodal LLMs to perform dense prediction tasks like semantic segmentation without task-specific decoders, using vision token supervision for multiple labels.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs require complex task-specific decoders for dense prediction tasks, which increases model complexity and deviates from generalist design principles, limiting practicality.

Method: Proposes DenseMLLM with novel vision token supervision strategy for multiple labels and tasks, using standard MLLM architecture without additional decoders.

Result: Achieves highly competitive performance across dense prediction and vision-language benchmarks, demonstrating standard MLLMs can effectively support dense perception.

Conclusion: A standard, general-purpose MLLM can perform dense prediction tasks effectively without architectural specialization, challenging the need for task-specific decoders.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in high-level visual understanding. However, extending these models to fine-grained dense prediction tasks, such as semantic segmentation and depth estimation, typically necessitates the incorporation of complex, task-specific decoders and other customizations. This architectural fragmentation increases model complexity and deviates from the generalist design of MLLMs, ultimately limiting their practicality. In this work, we challenge this paradigm by accommodating standard MLLMs to perform dense predictions without requiring additional task-specific decoders. The proposed model is called DenseMLLM, grounded in the standard architecture with a novel vision token supervision strategy for multiple labels and tasks. Despite its minimalist design, our model achieves highly competitive performance across a wide range of dense prediction and vision-language benchmarks, demonstrating that a standard, general-purpose MLLM can effectively support dense perception without architectural specialization.

</details>


### [100] [Detection of On-Ground Chestnuts Using Artificial Intelligence Toward Automated Picking](https://arxiv.org/abs/2602.14140)
*Kaixuan Fang,Yuzhen Lu,Xinyang Mu*

Main category: cs.CV

TL;DR: This study evaluates 29 state-of-the-art real-time object detectors (YOLO and RT-DETR models) for chestnut detection on orchard floors, finding YOLOv12m achieves best mAP@0.5 of 95.1%, with all models showing potential for real-time automated harvesting applications.


<details>
  <summary>Details</summary>
Motivation: Traditional mechanized chestnut harvesting is costly for small producers, non-selective, and damages nuts. Accurate chestnut detection is crucial for developing low-cost, vision-guided automated harvesting, but faces challenges from complex orchard environments with shading, varying light, weeds, leaves, stones, and other ground objects.

Method: Collected 319 images of chestnuts on orchard floors containing 6524 annotated chestnuts. Systematically evaluated 29 state-of-the-art real-time object detectors (14 YOLO v11-13 models and 15 RT-DETR v1-v4 models at varied scales) through replicated modeling experiments.

Result: YOLOv12m achieved best mAP@0.5 of 95.1% among all models. RT-DETRv2-R101 was most accurate RT-DETR variant with mAP@0.5 of 91.1%. YOLOv11x achieved best mAP@[0.5:0.95] of 80.1%. YOLO models outperformed RT-DETR in both detection accuracy and inference speed, making them better for on-board deployment.

Conclusion: All evaluated models demonstrate significant potential for real-time chestnut detection. YOLO models are particularly suitable for on-board deployment in automated harvesting systems. The publicly available dataset and software support further development of vision-guided chestnut harvesting technology.

Abstract: Traditional mechanized chestnut harvesting is too costly for small producers, non-selective, and prone to damaging nuts. Accurate, reliable detection of chestnuts on the orchard floor is crucial for developing low-cost, vision-guided automated harvesting technology. However, developing a reliable chestnut detection system faces challenges in complex environments with shading, varying natural light conditions, and interference from weeds, fallen leaves, stones, and other foreign on-ground objects, which have remained unaddressed. This study collected 319 images of chestnuts on the orchard floor, containing 6524 annotated chestnuts. A comprehensive set of 29 state-of-the-art real-time object detectors, including 14 in the YOLO (v11-13) and 15 in the RT-DETR (v1-v4) families at varied model scales, was systematically evaluated through replicated modeling experiments for chestnut detection. Experimental results show that the YOLOv12m model achieves the best mAP@0.5 of 95.1% among all the evaluated models, while the RT-DETRv2-R101 was the most accurate variant among RT-DETR models, with mAP@0.5 of 91.1%. In terms of mAP@[0.5:0.95], the YOLOv11x model achieved the best accuracy of 80.1%. All models demonstrate significant potential for real-time chestnut detection, and YOLO models outperformed RT-DETR models in terms of both detection accuracy and inference, making them better suited for on-board deployment. Both the dataset and software programs in this study have been made publicly available at https://github.com/AgFood-Sensing-and-Intelligence-Lab/ChestnutDetection.

</details>


### [101] [LaViDa-R1: Advancing Reasoning for Unified Multimodal Diffusion Language Models](https://arxiv.org/abs/2602.14147)
*Shufan Li,Yuchen Zhu,Jiuxiang Gu,Kangning Liu,Zhe Lin,Yongxin Chen,Molei Tao,Aditya Grover,Jason Kuen*

Main category: cs.CV

TL;DR: LaViDa-R1 is a multimodal reasoning diffusion language model that unifies diverse understanding/generation tasks through a novel post-training framework combining SFT and multi-task RL with techniques like answer-forcing and tree search.


<details>
  <summary>Details</summary>
Motivation: Existing reasoning diffusion LLMs use task-specific reinforcement learning, lacking a unified approach for multimodal understanding and generation tasks. The authors aim to create a general-purpose reasoning dLLM that can handle diverse multimodal tasks in a unified manner.

Method: Proposes LaViDa-R1 with a novel unified post-training framework that seamlessly integrates supervised finetuning (SFT) and multi-task reinforcement learning (RL). Uses novel techniques: answer-forcing, tree search, and complementary likelihood estimation to enhance effectiveness and scalability.

Result: Extensive experiments demonstrate LaViDa-R1's strong performance on a wide range of multimodal tasks including visual math reasoning, reason-intensive grounding, and image editing.

Conclusion: LaViDa-R1 successfully creates a unified multimodal reasoning diffusion language model that outperforms task-specific approaches across diverse understanding and generation tasks.

Abstract: Diffusion language models (dLLMs) recently emerged as a promising alternative to auto-regressive LLMs. The latest works further extended it to multimodal understanding and generation tasks. In this work, we propose LaViDa-R1, a multimodal, general-purpose reasoning dLLM. Unlike existing works that build reasoning dLLMs through task-specific reinforcement learning, LaViDa-R1 incorporates diverse multimodal understanding and generation tasks in a unified manner. In particular, LaViDa-R1 is built with a novel unified post-training framework that seamlessly integrates supervised finetuning (SFT) and multi-task reinforcement learning (RL). It employs several novel training techniques, including answer-forcing, tree search, and complementary likelihood estimation, to enhance effectiveness and scalability. Extensive experiments demonstrate LaViDa-R1's strong performance on a wide range of multimodal tasks, including visual math reasoning, reason-intensive grounding, and image editing.

</details>


### [102] [ARport: An Augmented Reality System for Markerless Image-Guided Port Placement in Robotic Surgery](https://arxiv.org/abs/2602.14153)
*Zheng Han,Zixin Yang,Yonghao Long,Lin Zhang,Peter Kazanzides,Qi Dou*

Main category: cs.CV

TL;DR: ARport: An AR system for markerless visualization of pre-planned trocar layouts on patient's body surface during robot-assisted surgery preparation.


<details>
  <summary>Details</summary>
Motivation: Precise port placement is critical in robot-assisted surgery as it affects both visual access and instrument maneuverability. There's a need to bridge the gap between preoperative planning and intraoperative execution without complex setups.

Method: ARport uses an optical see-through head-mounted display (OST-HMD) without external sensors or markers. It reconstructs the operative scene from RGB, depth, and pose data, extracts the patient's body surface using a foundation model, performs surface-based markerless registration to align preoperative anatomical models, and visualizes planned trocar layouts in-situ.

Result: In full-scale human-phantom experiments, ARport accurately overlaid pre-planned trocar sites onto the physical phantom, achieving consistent spatial correspondence between virtual plans and real anatomy.

Conclusion: ARport provides a fully marker-free and hardware-minimal solution for visualizing preoperative trocar plans directly on the patient's body surface, facilitating efficient intraoperative setup with potential for seamless clinical workflow integration.

Abstract: Purpose: Precise port placement is a critical step in robot-assisted surgery, where port configuration influences both visual access to the operative field and instrument maneuverability. To bridge the gap between preoperative planning and intraoperative execution, we present ARport, an augmented reality (AR) system that automatically maps pre-planned trocar layouts onto the patient's body surface, providing intuitive spatial guidance during surgical preparation. Methods: ARport, implemented on an optical see-through head-mounted display (OST-HMD), operates without any external sensors or markers, simplifying setup and enhancing workflow integration. It reconstructs the operative scene from RGB, depth, and pose data captured by the OST-HMD, extracts the patient's body surface using a foundation model, and performs surface-based markerless registration to align preoperative anatomical models to the extracted patient's body surface, enabling in-situ visualization of planned trocar layouts. A demonstration video illustrating the overall workflow is available online. Results: In full-scale human-phantom experiments, ARport accurately overlaid pre-planned trocar sites onto the physical phantom, achieving consistent spatial correspondence between virtual plans and real anatomy. Conclusion: ARport provides a fully marker-free and hardware-minimal solution for visualizing preoperative trocar plans directly on the patient's body surface. The system facilitates efficient intraoperative setup and demonstrates potential for seamless integration into routine clinical workflows.

</details>


### [103] [When Test-Time Guidance Is Enough: Fast Image and Video Editing with Diffusion Guidance](https://arxiv.org/abs/2602.14157)
*Ahmed Ghorbel,Badr Moufad,Navid Bagheri Shouraki,Alain Oliviero Durmus,Thomas Hirtz,Eric Moulines,Jimmy Olsson,Yazid Janati*

Main category: cs.CV

TL;DR: VJP-free test-time guidance for diffusion/flow models achieves competitive image/video editing performance without costly vector-Jacobian product computations.


<details>
  <summary>Details</summary>
Motivation: Text-driven image/video editing as inpainting requires efficient test-time guidance without expensive VJP computations that limit practical applicability.

Method: Extends Moufad et al. (2025) VJP-free approximation for test-time guidance in diffusion/flow models, with comprehensive evaluation on large-scale image/video editing benchmarks.

Result: Test-time guidance alone achieves performance comparable to or surpassing training-based methods on image and video editing tasks.

Conclusion: VJP-free test-time guidance provides efficient, practical solution for text-driven image/video editing without needing expensive VJP computations or model retraining.

Abstract: Text-driven image and video editing can be naturally cast as inpainting problems, where masked regions are reconstructed to remain consistent with both the observed content and the editing prompt. Recent advances in test-time guidance for diffusion and flow models provide a principled framework for this task; however, existing methods rely on costly vector--Jacobian product (VJP) computations to approximate the intractable guidance term, limiting their practical applicability. Building upon the recent work of Moufad et al. (2025), we provide theoretical insights into their VJP-free approximation and substantially extend their empirical evaluation to large-scale image and video editing benchmarks. Our results demonstrate that test-time guidance alone can achieve performance comparable to, and in some cases surpass, training-based methods.

</details>


### [104] [Towards Spatial Transcriptomics-driven Pathology Foundation Models](https://arxiv.org/abs/2602.14177)
*Konstantin Hemker,Andrew H. Song,Cristina Almagro-Pérez,Guillaume Jaume,Sophia J. Wagner,Anurag Vaidya,Nikola Simidjievski,Mateja Jamnik,Faisal Mahmood*

Main category: cs.CV

TL;DR: SEAL is a parameter-efficient vision-omics finetuning framework that infuses localized molecular information from spatial transcriptomics into pathology vision encoders, improving performance across various downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Spatial transcriptomics provides spatially resolved gene expression data that can be aligned with tissue morphology. The success of multimodal foundation models suggests that morphomolecular coupling between local expression and morphology can systematically improve histological representations.

Method: SEAL is a self-supervised learning framework designed as a parameter-efficient vision-omics finetuning method that can be flexibly applied to existing pathology foundation models. It trains on over 700,000 paired gene expression spot-tissue region examples from tumor and normal samples across 14 organs.

Result: Tested across 38 slide-level and 15 patch-level downstream tasks, SEAL consistently improves performance over vision-only and ST prediction baselines on slide-level molecular status, pathway activity, treatment response prediction, and patch-level gene expression prediction. It also shows robust domain generalization and enables new cross-modal capabilities like gene-to-image retrieval.

Conclusion: SEAL provides a general framework for ST-guided finetuning of pathology foundation models, demonstrating that augmenting existing models with localized molecular supervision is effective for improving visual representations and expanding cross-modal utility.

Abstract: Spatial transcriptomics (ST) provides spatially resolved measurements of gene expression, enabling characterization of the molecular landscape of human tissue beyond histological assessment as well as localized readouts that can be aligned with morphology. Concurrently, the success of multimodal foundation models that integrate vision with complementary modalities suggests that morphomolecular coupling between local expression and morphology can be systematically used to improve histological representations themselves. We introduce Spatial Expression-Aligned Learning (SEAL), a vision-omics self-supervised learning framework that infuses localized molecular information into pathology vision encoders. Rather than training new encoders from scratch, SEAL is designed as a parameter-efficient vision-omics finetuning method that can be flexibly applied to widely used pathology foundation models. We instantiate SEAL by training on over 700,000 paired gene expression spot-tissue region examples spanning tumor and normal samples from 14 organs. Tested across 38 slide-level and 15 patch-level downstream tasks, SEAL provides a drop-in replacement for pathology foundation models that consistently improves performance over widely used vision-only and ST prediction baselines on slide-level molecular status, pathway activity, and treatment response prediction, as well as patch-level gene expression prediction tasks. Additionally, SEAL encoders exhibit robust domain generalization on out-of-distribution evaluations and enable new cross-modal capabilities such as gene-to-image retrieval. Our work proposes a general framework for ST-guided finetuning of pathology foundation models, showing that augmenting existing models with localized molecular supervision is an effective and practical step for improving visual representations and expanding their cross-modal utility.

</details>


### [105] [UniWeTok: An Unified Binary Tokenizer with Codebook Size $\mathit{2^{128}}$ for Unified Multimodal Large Language Model](https://arxiv.org/abs/2602.14178)
*Shaobin Zhuang,Yuang Ai,Jiaming Han,Weijia Mao,Xiaohui Li,Fangyikang Wang,Xiao Wang,Yan Li,Shanchuan Lin,Kun Xu,Zhenheng Yang,Huaibo Huang,Xiangyu Yue,Hao Chen,Yali Wang*

Main category: cs.CV

TL;DR: UniWeTok is a unified multimodal tokenizer using a massive binary codebook that achieves SOTA image generation with low training compute while supporting multimodal understanding, generation, and editing tasks.


<details>
  <summary>Details</summary>
Motivation: Existing visual tokenizers struggle to simultaneously support high-fidelity reconstruction, complex semantic extraction, and generative suitability within a single framework, creating a need for a unified solution.

Method: Uses a massive binary codebook (2^128) with Pre-Post Distillation and Generative-Aware Prior for training, a convolution-attention hybrid architecture with SigLu activation, and a three-stage training framework for cross-resolution adaptability.

Result: Achieves SOTA image generation (FID: 1.38 vs REPA 1.42) with low training compute (33B vs 262B tokens), competitive multimodal understanding, and strong image editing performance (GEdit: 5.09 vs OmniGen 5.06).

Conclusion: UniWeTok successfully bridges the gap in unified multimodal tokenization, offering high performance across diverse tasks with efficient training, enabling better exploration of unified MLLMs.

Abstract: Unified Multimodal Large Language Models (MLLMs) require a visual representation that simultaneously supports high-fidelity reconstruction, complex semantic extraction, and generative suitability. However, existing visual tokenizers typically struggle to satisfy these conflicting objectives within a single framework. In this paper, we introduce UniWeTok, a unified discrete tokenizer designed to bridge this gap using a massive binary codebook ($\mathit{2^{128}}$). For training framework, we introduce Pre-Post Distillation and a Generative-Aware Prior to enhance the semantic extraction and generative prior of the discrete tokens. In terms of model architecture, we propose a convolution-attention hybrid architecture with the SigLu activation function. SigLu activation not only bounds the encoder output and stabilizes the semantic distillation process but also effectively addresses the optimization conflict between token entropy loss and commitment loss. We further propose a three-stage training framework designed to enhance UniWeTok's adaptability cross various image resolutions and perception-sensitive scenarios, such as those involving human faces and textual content. On ImageNet, UniWeTok achieves state-of-the-art image generation performance (FID: UniWeTok 1.38 vs. REPA 1.42) while requiring a remarkably low training compute (Training Tokens: UniWeTok 33B vs. REPA 262B). On general-domain, UniWeTok demonstrates highly competitive capabilities across a broad range of tasks, including multimodal understanding, image generation (DPG Score: UniWeTok 86.63 vs. FLUX.1 [Dev] 83.84), and editing (GEdit Overall Score: UniWeTok 5.09 vs. OmniGen 5.06). We release code and models to facilitate community exploration of unified tokenizer and MLLM.

</details>


### [106] [UniRef-Image-Edit: Towards Scalable and Consistent Multi-Reference Image Editing](https://arxiv.org/abs/2602.14186)
*Hongyang Wei,Bin Wen,Yancheng Long,Yankai Yang,Yuhang Hu,Tianke Zhang,Wei Chen,Haonan Fan,Kaiyu Jiang,Jiankang Chen,Changyi Liu,Kaiyu Tang,Haojie Ding,Xiao Yang,Jia Sun,Huaiqing Wang,Zhenyu Yang,Xinyu Wei,Xianglong He,Yangguang Li,Fan Yang,Tingting Gao,Lei Zhang,Guorui Zhou,Han Li*

Main category: cs.CV

TL;DR: UniRef-Image-Edit is a unified multi-modal generation system that handles both single-image editing and multi-image composition using a novel Sequence-Extended Latent Fusion (SELF) representation and two-stage training with supervised fine-tuning and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based editing methods struggle with maintaining consistency across multiple reference images due to limited interaction between reference inputs. There's a need for a unified framework that can handle both single-image editing and multi-image composition tasks effectively.

Method: 1) Introduces Sequence-Extended Latent Fusion (SELF) - a unified input representation that dynamically serializes multiple reference images into coherent latent sequences. 2) Two-stage training: a) Supervised fine-tuning (SFT) with progressive sequence length training (1024² → 1536² → 2048² pixel budget) to establish generative prior. b) Reinforcement learning stage with Multi-Source GRPO (MSGRPO) to optimize conflicting visual constraints and enhance compositional consistency.

Result: The system achieves high-performance multi-modal generation with improved cross-reference consistency and visual fidelity. The progressive training enables incremental capture of finer visual details while maintaining stable alignment across references.

Conclusion: UniRef-Image-Edit provides a unified framework for both single-image editing and multi-image composition, addressing consistency issues in existing methods through innovative SELF representation and two-stage training. The authors will open-source code, models, training data, and reward data for community research.

Abstract: We present UniRef-Image-Edit, a high-performance multi-modal generation system that unifies single-image editing and multi-image composition within a single framework. Existing diffusion-based editing methods often struggle to maintain consistency across multiple conditions due to limited interaction between reference inputs. To address this, we introduce Sequence-Extended Latent Fusion (SELF), a unified input representation that dynamically serializes multiple reference images into a coherent latent sequence. During a dedicated training stage, all reference images are jointly constrained to fit within a fixed-length sequence under a global pixel-budget constraint. Building upon SELF, we propose a two-stage training framework comprising supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we jointly train on single-image editing and multi-image composition tasks to establish a robust generative prior. We adopt a progressive sequence length training strategy, in which all input images are initially resized to a total pixel budget of $1024^2$, and are then gradually increased to $1536^2$ and $2048^2$ to improve visual fidelity and cross-reference consistency. This gradual relaxation of compression enables the model to incrementally capture finer visual details while maintaining stable alignment across references. For the RL stage, we introduce Multi-Source GRPO (MSGRPO), to our knowledge the first reinforcement learning framework tailored for multi-reference image generation. MSGRPO optimizes the model to reconcile conflicting visual constraints, significantly enhancing compositional consistency. We will open-source the code, models, training data, and reward data for community research purposes.

</details>


### [107] [GeoEyes: On-Demand Visual Focusing for Evidence-Grounded Understanding of Ultra-High-Resolution Remote Sensing Imagery](https://arxiv.org/abs/2602.14201)
*Fengxiang Wang,Mingshuo Chen,Yueying Li,Yajie Yang,Yifan Zhang,Long Lan,Xue Yang,Hongda Sun,Yulin Wang,Di Wang,Jun Song,Jing Zhang,Bo Du*

Main category: cs.CV

TL;DR: GeoEyes is a staged training framework for multimodal LLMs that addresses tool usage homogenization in UHR remote sensing VQA by combining a cold-start SFT dataset (UHR-CoZ) with agentic reinforcement learning (AdaZoom-GRPO) to enable effective on-demand zooming.


<details>
  <summary>Details</summary>
Motivation: Existing zoom-enabled MLLMs suffer from "Tool Usage Homogenization" where tool calls collapse into task-agnostic patterns, limiting effective evidence acquisition in ultra-high-resolution remote sensing VQA where task-relevant cues are sparse and tiny.

Method: Two-stage framework: (1) Cold-start SFT dataset UHR Chain-of-Zoom (UHR-CoZ) covering diverse zooming regimes, and (2) Agentic reinforcement learning method AdaZoom-GRPO that explicitly rewards evidence gain and answer improvement during zoom interactions.

Result: The model learns on-demand zooming with proper stopping behavior and achieves substantial improvements on UHR remote sensing benchmarks, with 54.23% accuracy on XLRS-Bench.

Conclusion: GeoEyes addresses the tool usage homogenization problem in MLLMs for UHR remote sensing VQA through a staged training approach that combines supervised fine-tuning with reinforcement learning, enabling effective evidence acquisition through intelligent zooming behavior.

Abstract: The "thinking-with-images" paradigm enables multimodal large language models (MLLMs) to actively explore visual scenes via zoom-in tools. This is essential for ultra-high-resolution (UHR) remote sensing VQA, where task-relevant cues are sparse and tiny. However, we observe a consistent failure mode in existing zoom-enabled MLLMs: Tool Usage Homogenization, where tool calls collapse into task-agnostic patterns, limiting effective evidence acquisition. To address this, we propose GeoEyes, a staged training framework consisting of (1) a cold-start SFT dataset, UHR Chain-of-Zoom (UHR-CoZ), which covers diverse zooming regimes, and (2) an agentic reinforcement learning method, AdaZoom-GRPO, that explicitly rewards evidence gain and answer improvement during zoom interactions. The resulting model learns on-demand zooming with proper stopping behavior and achieves substantial improvements on UHR remote sensing benchmarks, with 54.23% accuracy on XLRS-Bench.

</details>


### [108] [HiVid: LLM-Guided Video Saliency For Content-Aware VOD And Live Streaming](https://arxiv.org/abs/2602.14214)
*Jiahui Chen,Bo Peng,Lianchen Jia,Zeyu Zhang,Tianchi Huang,Lifeng Sun*

Main category: cs.CV

TL;DR: HiVid uses LLMs as scalable human proxies to generate chunk-level importance weights for content-aware video streaming, addressing VOD and live streaming challenges with perception, ranking, and prediction modules.


<details>
  <summary>Details</summary>
Motivation: Content-aware streaming needs dynamic importance weights for QoE optimization, but human annotation is expensive and vision-saliency models generalize poorly. LLMs offer a scalable alternative to generate high-fidelity importance weights.

Method: Three modules: (1) Perception module uses local context windows for frame assessment, (2) Ranking module performs global re-ranking with LLM-guided merge-sort for VOD, (3) Prediction module uses multi-modal time series model with content-aware attention and adaptive horizon for live streaming.

Result: Improves weight prediction accuracy by up to 11.5% for VOD and 26% for live streaming over SOTA baselines. Real-world user study shows 14.7% boost in streaming QoE correlation.

Conclusion: HiVid successfully leverages LLMs as scalable human proxies for generating importance weights, effectively addressing modality limitations, rating inconsistency, and low-latency requirements for both VOD and live streaming applications.

Abstract: Content-aware streaming requires dynamic, chunk-level importance weights to optimize subjective quality of experience (QoE). However, direct human annotation is prohibitively expensive while vision-saliency models generalize poorly. We introduce HiVid, the first framework to leverage Large Language Models (LLMs) as a scalable human proxy to generate high-fidelity weights for both Video-on-Demand (VOD) and live streaming. We address 3 non-trivial challenges: (1) To extend LLMs' limited modality and circumvent token limits, we propose a perception module to assess frames in a local context window, autoregressively building a coherent understanding of the video. (2) For VOD with rating inconsistency across local windows, we propose a ranking module to perform global re-ranking with a novel LLM-guided merge-sort algorithm. (3) For live streaming which requires low-latency, online inference without future knowledge, we propose a prediction module to predict future weights with a multi-modal time series model, which comprises a content-aware attention and adaptive horizon to accommodate asynchronous LLM inference. Extensive experiments show HiVid improves weight prediction accuracy by up to 11.5\% for VOD and 26\% for live streaming over SOTA baselines. Real-world user study validates HiVid boosts streaming QoE correlation by 14.7\%.

</details>


### [109] [Freq-DP Net: A Dual-Branch Network for Fence Removal using Dual-Pixel and Fourier Priors](https://arxiv.org/abs/2602.14226)
*Kunal Swami,Sudha Velusamy,Chandra Sekhar Seelamantula*

Main category: cs.CV

TL;DR: Freq-DP Net: First framework using dual-pixel sensors for single-image fence removal, combining geometric defocus disparity and structural pattern priors via attention fusion.


<details>
  <summary>Details</summary>
Motivation: Fence occlusions degrade visual quality and limit computer vision applications. Existing methods fail on static scenes or require multiple frames, creating a need for single-image solutions.

Method: Dual-branch network (Freq-DP Net) fuses two priors: geometric prior from defocus disparity using explicit cost volume, and structural prior of fence patterns learned via Fast Fourier Convolution. Attention mechanism merges these cues for accurate fence segmentation.

Result: Method significantly outperforms strong general-purpose baselines, establishing new state-of-the-art for single-image, DP-based fence removal. Authors release diverse benchmark with different fence varieties for validation.

Conclusion: First successful framework leveraging dual-pixel sensors for single-image fence removal, demonstrating superior performance through fusion of geometric and structural priors with attention-based integration.

Abstract: Removing fence occlusions from single images is a challenging task that degrades visual quality and limits downstream computer vision applications. Existing methods often fail on static scenes or require motion cues from multiple frames. To overcome these limitations, we introduce the first framework to leverage dual-pixel (DP) sensors for this problem. We propose Freq-DP Net, a novel dual-branch network that fuses two complementary priors: a geometric prior from defocus disparity, modeled using an explicit cost volume, and a structural prior of the fence's global pattern, learned via Fast Fourier Convolution (FFC). An attention mechanism intelligently merges these cues for highly accurate fence segmentation. To validate our approach, we build and release a diverse benchmark with different fence varieties. Experiments demonstrate that our method significantly outperforms strong general-purpose baselines, establishing a new state-of-the-art for single-image, DP-based fence removal.

</details>


### [110] [Learning Significant Persistent Homology Features for 3D Shape Understanding](https://arxiv.org/abs/2602.14228)
*Prachi Kudeshia,Jiju Poovvancheri*

Main category: cs.CV

TL;DR: This paper introduces topologically-enriched 3D shape datasets (ModelNet40 and ShapeNet) with persistent homology features, proposes TopoGAT for learning to select significant topological features, and demonstrates improved performance in point cloud classification and segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing 3D shape datasets focus on geometric information but neglect topological structure. There's a need for unified geometry-topology learning benchmarks and better methods for selecting informative topological features beyond hand-crafted statistical criteria.

Method: 1) Create topologically-enriched versions of ModelNet40 and ShapeNet by augmenting point clouds with persistent homology features. 2) Propose TopoGAT, a deep learning-based method that learns to identify the most informative topological features directly from input data and topological signatures.

Result: TopoGAT outperforms traditional statistical approaches in stability and discriminative power. Integrating selected persistent points into standard point cloud classification and part-segmentation pipelines improves both classification accuracy and segmentation metrics.

Conclusion: The topologically-enriched datasets and learnable feature selection approach enable broader integration of persistent homology into practical deep learning workflows for 3D point cloud analysis, bridging the gap between geometric and topological shape descriptors.

Abstract: Geometry and topology constitute complementary descriptors of three-dimensional shape, yet existing benchmark datasets primarily capture geometric information while neglecting topological structure. This work addresses this limitation by introducing topologically-enriched versions of ModelNet40 and ShapeNet, where each point cloud is augmented with its corresponding persistent homology features. These benchmarks with the topological signatures establish a foundation for unified geometry-topology learning and enable systematic evaluation of topology-aware deep learning architectures for 3D shape analysis. Building on this foundation, we propose a deep learning-based significant persistent point selection method, \textit{TopoGAT}, that learns to identify the most informative topological features directly from input data and the corresponding topological signatures, circumventing the limitations of hand-crafted statistical selection criteria. A comparative study verifies the superiority of the proposed method over traditional statistical approaches in terms of stability and discriminative power. Integrating the selected significant persistent points into standard point cloud classification and part-segmentation pipelines yields improvements in both classification accuracy and segmentation metrics. The presented topologically-enriched datasets, coupled with our learnable significant feature selection approach, enable the broader integration of persistent homology into the practical deep learning workflows for 3D point cloud analysis.

</details>


### [111] [Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models](https://arxiv.org/abs/2602.14236)
*Vishnu Sai,Dheeraj Sai,Srinath B,Girish Varma,Priyesh Shukla*

Main category: cs.CV

TL;DR: Sali-Cache is a novel KV cache optimization framework for VLMs that uses dual-signal adaptive caching (temporal optical flow + spatial saliency) to proactively manage memory before attention computations, achieving 2.20x memory compression with no accuracy loss.


<details>
  <summary>Details</summary>
Motivation: VLMs processing long-form video face memory bottlenecks due to linear KV cache growth with sequence length. Existing reactive eviction strategies waste computation by computing full attention matrices before discarding tokens.

Method: Sali-Cache implements dual-signal adaptive caching through proactive memory management: (1) temporal filter using optical flow analysis to detect inter-frame redundancy, and (2) spatial filter using saliency detection to identify visually significant regions. This manages memory allocation before expensive attention operations.

Result: On LLaVA 1.6 architecture: achieves 2.20x compression ratio in effective memory usage while maintaining 100% accuracy across BLEU, ROUGE-L, and Exact Match metrics. Preserves context-rich features over extended temporal durations under identical memory budget constraints.

Conclusion: Sali-Cache enables efficient processing of long-form video content on consumer-grade hardware by intelligently managing KV cache through proactive, dual-signal adaptive caching without degrading model performance.

Abstract: Vision-Language Models (VLMs) face a critical memory bottleneck when processing long-form video content due to the linear growth of the Key-Value (KV) cache with sequence length. Existing solutions predominantly employ reactive eviction strategies that compute full attention matrices before discarding tokens, resulting in substantial computational waste. We propose Sali-Cache, a novel a priori optimization framework that implements dual-signal adaptive caching through proactive memory management. By integrating a temporal filter based on optical flow analysis for detecting inter-frame redundancy and a spatial filter leveraging saliency detection for identifying visually significant regions, Sali-Cache intelligently manages memory allocation before entering computationally expensive attention operations. Experimental evaluation on the LLaVA 1.6 architecture demonstrates that our method achieves a 2.20x compression ratio in effective memory usage while maintaining 100% accuracy across BLEU, ROUGE-L, and Exact Match metrics. Furthermore, under identical memory budget constraints, Sali-Cache preserves context-rich features over extended temporal durations without degrading model performance, enabling efficient processing of long-form video content on consumer-grade hardware.

</details>


### [112] [AbracADDbra: Touch-Guided Object Addition by Decoupling Placement and Editing Subtasks](https://arxiv.org/abs/2602.14237)
*Kunal Swami,Raghu Chittersu,Yuvraj Rathore,Rajeev Irny,Shashavali Doodekula,Alok Shukla*

Main category: cs.CV

TL;DR: AbracADDbra is a user-friendly framework that uses intuitive touch inputs instead of ambiguous text or tedious masks to add objects to images, achieving precise placement and high-fidelity blending through a decoupled vision-language transformer and diffusion model.


<details>
  <summary>Details</summary>
Motivation: Current instruction-based object addition methods suffer from ambiguity with text-only prompts or tediousness with mask-based inputs, creating a usability gap that needs addressing for more accessible creative tools.

Method: A decoupled architecture with two main components: 1) a vision-language transformer that uses touch-guided priors to spatially ground instructions for precise placement, and 2) a diffusion model that jointly generates the object and an instance mask for high-fidelity blending.

Result: The placement model significantly outperforms both random placement and general-purpose VLM baselines, producing high-fidelity edits. Analysis shows strong correlation between initial placement accuracy and final edit quality, validating the decoupled approach.

Conclusion: AbracADDbra provides a more accessible and efficient framework for interactive object addition using intuitive touch inputs, paving the way for improved creative tools with its decoupled architecture and standardized Touch2Add benchmark.

Abstract: Instruction-based object addition is often hindered by the ambiguity of text-only prompts or the tedious nature of mask-based inputs. To address this usability gap, we introduce AbracADDbra, a user-friendly framework that leverages intuitive touch priors to spatially ground succinct instructions for precise placement. Our efficient, decoupled architecture uses a vision-language transformer for touch-guided placement, followed by a diffusion model that jointly generates the object and an instance mask for high-fidelity blending. To facilitate standardized evaluation, we contribute the Touch2Add benchmark for this interactive task. Our extensive evaluations, where our placement model significantly outperforms both random placement and general-purpose VLM baselines, confirm the framework's ability to produce high-fidelity edits. Furthermore, our analysis reveals a strong correlation between initial placement accuracy and final edit quality, validating our decoupled approach. This work thus paves the way for more accessible and efficient creative tools.

</details>


### [113] [Moving Beyond Sparse Grounding with Complete Screen Parsing Supervision](https://arxiv.org/abs/2602.14276)
*A. Said Gurbuz,Sunghwan Hong,Ahmed Nassar,Marc Pollefeys,Peter Staar*

Main category: cs.CV

TL;DR: ScreenParse: A large-scale dataset for complete screen parsing with dense UI element annotations, used to train ScreenVLM, a compact VLM that outperforms larger models on dense parsing tasks.


<details>
  <summary>Details</summary>
Motivation: Existing grounding datasets provide sparse supervision with insufficient and low-diversity labels, limiting coverage and generalization for computer-use agents that need to perceive screens as structured states. Practical deployment also requires efficiency for low-latency, on-device use.

Method: 1) Created ScreenParse dataset with dense annotations of all visible UI elements (boxes, 55-class types, and text) across 771K web screenshots (21M elements) using Webshot pipeline. 2) Trained ScreenVLM, a 316M-parameter VLM that decodes ScreenTag markup representation with structure-aware loss upweighting structure-critical tokens.

Result: ScreenVLM substantially outperforms larger foundation VLMs on dense parsing (0.592 vs. 0.294 PageIoU on ScreenParse) and shows strong transfer to public benchmarks. Finetuning foundation VLMs on ScreenParse consistently improves their grounding performance.

Conclusion: Dense screen supervision provides transferable structural priors for UI understanding, enabling compact models to achieve superior performance compared to larger foundation models on screen parsing tasks.

Abstract: Modern computer-use agents (CUA) must perceive a screen as a structured state, what elements are visible, where they are, and what text they contain, before they can reliably ground instructions and act. Yet, most available grounding datasets provide sparse supervision, with insufficient and low-diversity labels that annotate only a small subset of task-relevant elements per screen, which limits both coverage and generalization; moreover, practical deployment requires efficiency to enable low-latency, on-device use. We introduce ScreenParse, a large-scale dataset for complete screen parsing, with dense annotations of all visible UI elements (boxes, 55-class types, and text) across 771K web screenshots (21M elements). ScreenParse is generated by Webshot, an automated, scalable pipeline that renders diverse urls, extracts annotations and applies VLM-based relabeling and quality filtering. Using ScreenParse, we train ScreenVLM, a compact, 316M-parameter vision language model (VLM) that decodes a compact ScreenTag markup representation with a structure-aware loss that upweights structure-critical tokens. ScreenVLM substantially outperforms much larger foundation VLMs on dense parsing (e.g., 0.592 vs. 0.294 PageIoU on ScreenParse) and shows strong transfer to public benchmarks. Moreover, finetuning foundation VLMs on ScreenParse consistently improves their grounding performance, suggesting that dense screen supervision provides transferable structural priors for UI understanding. Project page: https://saidgurbuz.github.io/screenparse/.

</details>


### [114] [Differential pose optimization in descriptor space -- Combining Geometric and Photometric Methods for Motion Estimation](https://arxiv.org/abs/2602.14297)
*Andreas L. Teigen,Annette Stahl,Rudolf Mester*

Main category: cs.CV

TL;DR: The paper proposes combining photometric and geometric feature paradigms by using dense geometric descriptors instead of photometric error, but finds it doesn't outperform traditional reprojection error methods despite using more information.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between accuracy, robustness, and loop closing in two-frame relative pose optimization by combining strengths of both photometric and geometric feature paradigms into a unified approach.

Method: Replace photometric error with descriptor residual from densely sampled geometric feature descriptors, enabling sub-pixel accuracy from differential photometric methods while leveraging the expressiveness of geometric descriptors.

Result: The proposed strategy produces accurate tracking but ultimately does not outperform pose optimization strategies based on reprojection error despite utilizing more information.

Conclusion: The descriptor similarity metric is too slowly varying and doesn't strictly correspond to keypoint placement accuracy, explaining why the combined approach doesn't outperform traditional methods.

Abstract: One of the fundamental problems in computer vision is the two-frame relative pose optimization problem. Primarily, two different kinds of error values are used: photometric error and re-projection error. The selection of error value is usually directly dependent on the selection of feature paradigm, photometric features, or geometric features. It is a trade-off between accuracy, robustness, and the possibility of loop closing. We investigate a third method that combines the strengths of both paradigms into a unified approach. Using densely sampled geometric feature descriptors, we replace the photometric error with a descriptor residual from a dense set of descriptors, thereby enabling the employment of sub-pixel accuracy in differential photometric methods, along with the expressiveness of the geometric feature descriptor. Experiments show that although the proposed strategy is an interesting approach that results in accurate tracking, it ultimately does not outperform pose optimization strategies based on re-projection error despite utilizing more information. We proceed to analyze the underlying reason for this discrepancy and present the hypothesis that the descriptor similarity metric is too slowly varying and does not necessarily correspond strictly to keypoint placement accuracy.

</details>


### [115] [A Generative AI Approach for Reducing Skin Tone Bias in Skin Cancer Classification](https://arxiv.org/abs/2602.14356)
*Areez Muhammed Shabu,Mohammad Samar Ansari,Asra Aslam*

Main category: cs.CV

TL;DR: Generative AI pipeline using Stable Diffusion with LoRA fine-tuning creates synthetic dark-skin dermoscopic images to address dataset imbalance in skin cancer detection, improving segmentation and classification performance.


<details>
  <summary>Details</summary>
Motivation: Current AI skin cancer diagnostic tools suffer from reduced accuracy and fairness for darker skin tones due to severe dataset imbalance (ISIC dataset has >70% light skin vs <8% dark skin images), creating barriers to equitable healthcare delivery.

Method: Fine-tuned pre-trained Stable Diffusion model using Low-Rank Adaptation (LoRA) on dark-skin subset of ISIC dataset to generate synthetic dermoscopic images conditioned on lesion type and skin tone, creating an augmented dataset for downstream tasks.

Result: Models trained on augmented dataset showed consistent improvements in segmentation metrics (IoU, Dice coefficient, boundary accuracy) and achieved 92.14% accuracy in binary classification using EfficientNet-B0, demonstrating reduced bias and increased fairness.

Conclusion: Synthetic data augmentation with Generative AI can substantially reduce bias in dermatological diagnostics, addressing skin tone imbalance and opening new directions for equitable healthcare AI systems.

Abstract: Skin cancer is one of the most common cancers worldwide and early detection is critical for effective treatment. However, current AI diagnostic tools are often trained on datasets dominated by lighter skin tones, leading to reduced accuracy and fairness for people with darker skin. The International Skin Imaging Collaboration (ISIC) dataset, one of the most widely used benchmarks, contains over 70% light skin images while dark skins fewer than 8%. This imbalance poses a significant barrier to equitable healthcare delivery and highlights the urgent need for methods that address demographic diversity in medical imaging. This paper addresses this challenge of skin tone imbalance in automated skin cancer detection using dermoscopic images. To overcome this, we present a generative augmentation pipeline that fine-tunes a pre-trained Stable Diffusion model using Low-Rank Adaptation (LoRA) on the image dark-skin subset of the ISIC dataset and generates synthetic dermoscopic images conditioned on lesion type and skin tone. In this study, we investigated the utility of these images on two downstream tasks: lesion segmentation and binary classification. For segmentation, models trained on the augmented dataset and evaluated on held-out real images show consistent improvements in IoU, Dice coefficient, and boundary accuracy. These evalutions provides the verification of Generated dataset. For classification, an EfficientNet-B0 model trained on the augmented dataset achieved 92.14% accuracy. This paper demonstrates that synthetic data augmentation with Generative AI integration can substantially reduce bias with increase fairness in conventional dermatological diagnostics and open challenges for future directions.

</details>


### [116] [Image-based Joint-level Detection for Inflammation in Rheumatoid Arthritis from Small and Imbalanced Data](https://arxiv.org/abs/2602.14365)
*Shun Kato,Yasushi Kondo,Shuntaro Saito,Yoshimitsu Aoki,Mariko Isogawa*

Main category: cs.CV

TL;DR: Proposes a framework for detecting rheumatoid arthritis inflammation from RGB hand images using self-supervised pretraining on healthy hands and imbalance-aware training to address data scarcity challenges.


<details>
  <summary>Details</summary>
Motivation: Early RA diagnosis is crucial to prevent irreversible joint damage, but specialist care access is limited. There's a need for accessible home-based inflammation detection using simple RGB images, but existing methods don't address the challenges of data scarcity and imbalance in this domain.

Method: Constructs a dedicated RA inflammation dataset, develops a global-local encoder framework with self-supervised pretraining on large-scale healthy hand images, and implements imbalance-aware training techniques to handle the data challenges.

Result: The proposed approach improves F1-score by 0.2 points and Gmean by 0.25 points compared to baseline models, demonstrating effectiveness in detecting RA inflammation from RGB hand images.

Conclusion: The framework successfully addresses key challenges in RA inflammation detection from RGB images, offering a promising approach for accessible home-based monitoring that could improve early diagnosis and management of rheumatoid arthritis.

Abstract: Rheumatoid arthritis (RA) is an autoimmune disease characterized by systemic joint inflammation. Early diagnosis and tight follow-up are essential to the management of RA, as ongoing inflammation can cause irreversible joint damage. The detection of arthritis is important for diagnosis and assessment of disease activity; however, it often takes a long time for patients to receive appropriate specialist care. Therefore, there is a strong need to develop systems that can detect joint inflammation easily using RGB images captured at home. Consequently, we tackle the task of RA inflammation detection from RGB hand images. This task is highly challenging due to general issues in medical imaging, such as the scarcity of positive samples, data imbalance, and the inherent difficulty of the task itself. However, to the best of our knowledge, no existing work has explicitly addressed these challenges in RGB-based RA inflammation detection. This paper quantitatively demonstrates the difficulty of visually detecting inflammation by constructing a dedicated dataset, and we propose a inflammation detection framework with global local encoder that combines self-supervised pretraining on large-scale healthy hand images with imbalance-aware training to detect RA-related joint inflammation from RGB hand images. Our experiments demonstrated that the proposed approach improves F1-score by 0.2 points and Gmean by 0.25 points compared with the baseline model.

</details>


### [117] [Event-based Visual Deformation Measurement](https://arxiv.org/abs/2602.14376)
*Yuliang Wu,Wei Zhai,Yuxin Cui,Tiesong Zhao,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: Event-frame fusion framework for visual deformation measurement using events for dense temporal motion cues and frames for spatial precision, with affine invariant simplicial modeling and neighborhood-greedy optimization.


<details>
  <summary>Details</summary>
Motivation: Traditional image-based VDM methods require minimal inter-frame motion, limiting applicability to dynamic scenes or requiring high-speed cameras with prohibitive storage/computational costs.

Method: Event-frame fusion framework combining events (temporal density) and frames (spatial precision). Uses Affine Invariant Simplicial (AIS) framework to partition deformation field into linearized sub-regions with low-parametric representation. Introduces neighborhood-greedy optimization strategy to speed parameter search and reduce error accumulation.

Result: Outperforms state-of-the-art baseline by 1.6% in survival rate. Achieves results using only 18.9% of data storage and processing resources compared to high-speed video methods. Validated on benchmark dataset with 120+ sequences across diverse deformation scenarios.

Conclusion: The proposed event-frame fusion approach with AIS framework and neighborhood-greedy optimization enables efficient and accurate dense deformation tracking in dynamic scenes while significantly reducing computational and storage requirements.

Abstract: Visual Deformation Measurement (VDM) aims to recover dense deformation fields by tracking surface motion from camera observations. Traditional image-based methods rely on minimal inter-frame motion to constrain the correspondence search space, which limits their applicability to highly dynamic scenes or necessitates high-speed cameras at the cost of prohibitive storage and computational overhead. We propose an event-frame fusion framework that exploits events for temporally dense motion cues and frames for spatially dense precise estimation. Revisiting the solid elastic modeling prior, we propose an Affine Invariant Simplicial (AIS) framework. It partitions the deformation field into linearized sub-regions with low-parametric representation, effectively mitigating motion ambiguities arising from sparse and noisy events. To speed up parameter searching and reduce error accumulation, a neighborhood-greedy optimization strategy is introduced, enabling well-converged sub-regions to guide their poorly-converged neighbors, effectively suppress local error accumulation in long-term dense tracking. To evaluate the proposed method, a benchmark dataset with temporally aligned event streams and frames is established, encompassing over 120 sequences spanning diverse deformation scenarios. Experimental results show that our method outperforms the state-of-the-art baseline by 1.6% in survival rate. Remarkably, it achieves this using only 18.9% of the data storage and processing resources of high-speed video methods.

</details>


### [118] [Adapting VACE for Real-Time Autoregressive Video Diffusion](https://arxiv.org/abs/2602.14381)
*Ryan Fosdick*

Main category: cs.CV

TL;DR: Adapted VACE for real-time autoregressive video generation by moving reference frames to parallel conditioning pathway, enabling streaming with fixed chunk sizes and causal attention while reusing pretrained weights.


<details>
  <summary>Details</summary>
Motivation: Original VACE provides unified video control but uses bidirectional attention over full sequences, making it incompatible with real-time streaming pipelines that require fixed chunk sizes and causal attention.

Method: Key modification moves reference frames from diffusion latent space into parallel conditioning pathway, preserving fixed chunk sizes and KV caching needed for autoregressive models. Reuses existing pretrained VACE weights without additional training.

Result: Across 1.3B and 14B model scales, VACE adds 20-30% latency overhead for structural control and inpainting with negligible VRAM cost. However, reference-to-video fidelity is severely degraded compared to batch VACE due to causal attention constraints.

Conclusion: Successfully adapted VACE for real-time autoregressive video generation with acceptable latency overhead, though reference fidelity suffers due to causal attention limitations in streaming pipelines.

Abstract: We describe an adaptation of VACE (Video All-in-one Creation and Editing) for real-time autoregressive video generation. VACE provides unified video control (reference guidance, structural conditioning, inpainting, and temporal extension) but assumes bidirectional attention over full sequences, making it incompatible with streaming pipelines that require fixed chunk sizes and causal attention. The key modification moves reference frames from the diffusion latent space into a parallel conditioning pathway, preserving the fixed chunk sizes and KV caching that autoregressive models require. This adaptation reuses existing pretrained VACE weights without additional training. Across 1.3B and 14B model scales, VACE adds 20-30% latency overhead for structural control and inpainting, with negligible VRAM cost relative to the base model. Reference-to-video fidelity is severely degraded compared to batch VACE due to causal attention constraints. A reference implementation is available at https://github.com/daydreamlive/scope.

</details>


### [119] [Multi-Turn Adaptive Prompting Attack on Large Vision-Language Models](https://arxiv.org/abs/2602.14399)
*In Chong Choi,Jiacheng Zhang,Feng Liu,Yiliao Song*

Main category: cs.CV

TL;DR: MAPA is a multi-turn adaptive prompting attack for vision-language models that alternates text-vision attacks and refines trajectories to bypass safety defenses, achieving 11-35% higher success rates than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing multi-turn jailbreak attacks work on text-only LLMs but fail on vision-language models because overly malicious visual inputs trigger safety defenses, making responses more conservative. There's a need for effective attacks that can bypass LVLM safety mechanisms.

Method: Two-level design: 1) At each turn, alternates between text and vision attack actions to elicit the most malicious response; 2) Across turns, adjusts attack trajectory through iterative back-and-forth refinement to gradually amplify response maliciousness.

Result: Consistently outperforms state-of-the-art methods, improving attack success rates by 11-35% on benchmarks against LLaVA-V1.6-Mistral-7B, Qwen2.5-VL-7B-Instruct, Llama-3.2-Vision-11B-Instruct and GPT-4o-mini.

Conclusion: MAPA demonstrates that adaptive multi-turn attacks with alternating text-vision actions and trajectory refinement can effectively bypass safety-aligned vision-language model defenses, highlighting vulnerabilities in current LVLM safety mechanisms.

Abstract: Multi-turn jailbreak attacks are effective against text-only large language models (LLMs) by gradually introducing malicious content across turns. When extended to large vision-language models (LVLMs), we find that naively adding visual inputs can cause existing multi-turn jailbreaks to be easily defended. For example, overly malicious visual input will easily trigger the defense mechanism of safety-aligned LVLMs, making the response more conservative. To address this, we propose MAPA: a multi-turn adaptive prompting attack that 1) at each turn, alternates text-vision attack actions to elicit the most malicious response; and 2) across turns, adjusts the attack trajectory through iterative back-and-forth refinement to gradually amplify response maliciousness. This two-level design enables MAPA to consistently outperform state-of-the-art methods, improving attack success rates by 11-35% on recent benchmarks against LLaVA-V1.6-Mistral-7B, Qwen2.5-VL-7B-Instruct, Llama-3.2-Vision-11B-Instruct and GPT-4o-mini.

</details>


### [120] [pFedNavi: Structure-Aware Personalized Federated Vision-Language Navigation for Embodied AI](https://arxiv.org/abs/2602.14401)
*Qingqian Yang,Hao Wang,Sai Qian Zhang,Jian Li,Yang Hua,Miao Pan,Tao Song,Zhengwei Qi,Haibing Guan*

Main category: cs.CV

TL;DR: pFedNavi: A personalized federated learning framework for Vision-Language Navigation that addresses privacy concerns and client heterogeneity through adaptive layer-wise personalization and fine-grained parameter fusion.


<details>
  <summary>Details</summary>
Motivation: VLN requires large-scale trajectory data from private indoor environments, raising privacy concerns. Federated Learning helps but vanilla FL struggles with extreme cross-client heterogeneity in environments and instruction styles, making a single global model suboptimal.

Method: pFedNavi adaptively identifies client-specific layers via layer-wise mixing coefficients and performs fine-grained parameter fusion on selected components (encoder-decoder projection and environment-sensitive decoder layers) to balance global knowledge sharing with local specialization.

Result: Outperforms FedAvg-based VLN baseline across all metrics on R2R and RxR benchmarks with ResNet and CLIP visual representations. Achieves up to 7.5% improvement in navigation success rate, up to 7.8% gain in trajectory fidelity, and converges 1.38x faster under non-IID conditions.

Conclusion: pFedNavi effectively addresses privacy concerns and client heterogeneity in VLN through personalized federated learning, demonstrating significant performance improvements over standard FL approaches.

Abstract: Vision-Language Navigation VLN requires large-scale trajectory instruction data from private indoor environments, raising significant privacy concerns. Federated Learning FL mitigates this by keeping data on-device, but vanilla FL struggles under VLNs' extreme cross-client heterogeneity in environments and instruction styles, making a single global model suboptimal. This paper proposes pFedNavi, a structure-aware and dynamically adaptive personalized federated learning framework tailored for VLN. Our key idea is to personalize where it matters: pFedNavi adaptively identifies client-specific layers via layer-wise mixing coefficients, and performs fine-grained parameter fusion on the selected components (e.g., the encoder-decoder projection and environment-sensitive decoder layers) to balance global knowledge sharing with local specialization. We evaluate pFedNavi on two standard VLN benchmarks, R2R and RxR, using both ResNet and CLIP visual representations. Across all metrics, pFedNavi consistently outperforms the FedAvg-based VLN baseline, achieving up to 7.5% improvement in navigation success rate and up to 7.8% gain in trajectory fidelity, while converging 1.38x faster under non-IID conditions.

</details>


### [121] [Feature Recalibration Based Olfactory-Visual Multimodal Model for Fine-Grained Rice Deterioration Detection](https://arxiv.org/abs/2602.14408)
*Rongqiang Zhao,Hengrui Hu,Yijing Wang,Mingchun Sun,Jie Liu*

Main category: cs.CV

TL;DR: A novel olfactory-visual multimodal model for fine-grained rice deterioration detection using feature recalibration to enhance abnormal feature representation and simplify detection process.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal methods for rice deterioration detection have limited capability in representing fine-grained abnormal features and rely on expensive devices (hyperspectral cameras, mass spectrometers) that increase costs and prolong data acquisition time.

Method: Proposes a feature recalibration based olfactory-visual multimodal model with two components: 1) Fine-grained deterioration embedding constructor (FDEC) to reconstruct labeled multimodal embedded-feature dataset for enhanced sample representation, and 2) Fine-grained deterioration recalibration attention network (FDRA-Net) to emphasize signal variations and increase sensitivity to fine-grained deterioration on rice surface.

Result: Achieves 99.89% classification accuracy, outperforms state-of-the-art methods in detection accuracy while simplifying the procedure. Field detection demonstrates advantages in accuracy and operational simplicity.

Conclusion: The proposed method effectively addresses limitations of existing multimodal approaches for rice deterioration detection and can be extended to other agrifood applications in agriculture and food industry.

Abstract: Multimodal methods are widely used in rice deterioration detection, which exhibit limited capability in representing and extracting fine-grained abnormal features. Moreover, these methods rely on devices, such as hyperspectral cameras and mass spectrometers, increasing detection costs and prolonging data acquisition time. To address these issues, we propose a feature recalibration based olfactory-visual multimodal model for fine-grained rice deterioration detection. The fine-grained deterioration embedding constructor (FDEC) is proposed to reconstruct the labeled multimodal embedded-feature dataset, enhancing sample representation. The fine-grained deterioration recalibration attention network (FDRA-Net) is proposed to emphasize signal variations and increase sensitivity to fine-grained deterioration on the rice surface. Experiments show that the proposed method achieves a classification accuracy of 99.89%. Compared with state-of-the-art methods, the detection accuracy is improved and the procedure is simplified. Furthermore, field detection demonstrates the advantages of accuracy and operational simplicity. The proposed method can also be extended to other agrifood in agriculture and food industry.

</details>


### [122] [Learning Proposes, Geometry Disposes: A Modular Framework for Efficient Spatial Reasoning](https://arxiv.org/abs/2602.14409)
*Haichao Zhu,Zhaorui Yang,Qian Zhang*

Main category: cs.CV

TL;DR: Learning-based geometric proposals need geometric validation; modular systems with learning proposals + geometric disposal outperform pure learning approaches in camera pose estimation.


<details>
  <summary>Details</summary>
Motivation: To investigate whether learning components should directly replace geometric estimation or serve as intermediate modules in spatial perception pipelines, addressing the open question of optimal integration between learning and geometric methods.

Method: End-to-end modular framework where learning proposes geometric hypotheses (pose and depth) and geometric algorithms make final estimation decisions. Evaluated using VGGT for learning-based proposals and classical point-to-plane RGB-D ICP as geometric backend on TUM RGB-D benchmark.

Result: Three key findings: (1) learning-based pose proposals alone are unreliable; (2) learning-proposed geometry misaligned with camera intrinsics degrades performance; (3) geometrically aligned learning-proposed depth followed by geometric disposal improves performance in moderately challenging rigid settings.

Conclusion: Geometry is essential for validating and absorbing learning-based observations, not just refinement. Modular, geometry-aware system design is crucial for robust spatial perception.

Abstract: Spatial perception aims to estimate camera motion and scene structure from visual observations, a problem traditionally addressed through geometric modeling and physical consistency constraints. Recent learning-based methods have demonstrated strong representational capacity for geometric perception and are increasingly used to augment classical geometry-centric systems in practice. However, whether learning components should directly replace geometric estimation or instead serve as intermediate modules within such pipelines remains an open question.
  In this work, we address this gap and investigate an end-to-end modular framework for effective spatial reasoning, where learning proposes geometric hypotheses, while geometric algorithms dispose estimation decisions. In particular, we study this principle in the context of relative camera pose estimation on RGB-D sequences. Using VGGT as a representative learning model, we evaluate learning-based pose and depth proposals under varying motion magnitudes and scene dynamics, followed by a classical point-to-plane RGB-D ICP as the geometric backend. Our experiments on the TUM RGB-D benchmark reveal three consistent findings: (1) learning-based pose proposals alone are unreliable; (2) learning-proposed geometry, when improperly aligned with camera intrinsics, can degrade performance; and (3) when learning-proposed depth is geometrically aligned and followed by a geometric disposal stage, consistent improvements emerge in moderately challenging rigid settings.
  These results demonstrate that geometry is not merely a refinement component, but an essential arbiter that validates and absorbs learning-based geometric observations. Our study highlights the importance of modular, geometry-aware system design for robust spatial perception.

</details>


### [123] [Hierarchical Vision-Language Interaction for Facial Action Unit Detection](https://arxiv.org/abs/2602.14425)
*Yong Li,Yi Ren,Yizhe Zhang,Wenhua Zhang,Tianyi Zhang,Muyun Jiang,Guo-Sen Xie,Cuntai Guan*

Main category: cs.CV

TL;DR: HiVA: A hierarchical vision-language interaction method that uses textual AU descriptions as semantic priors to enhance facial action unit detection, achieving state-of-the-art performance through multi-grained cross-modal learning.


<details>
  <summary>Details</summary>
Motivation: The main challenge in facial AU detection is learning discriminative and generalizable representations with limited annotated data. The authors propose leveraging textual AU descriptions as semantic priors to guide and enhance AU detection.

Method: HiVA uses LLMs to generate diverse AU descriptions, employs an AU-aware dynamic graph module for AU-specific visual representations, and integrates these within a hierarchical cross-modal attention architecture with two mechanisms: Disentangled Dual Cross-Attention (fine-grained AU-specific interactions) and Contextual Dual Cross-Attention (global inter-AU dependencies).

Result: Extensive experiments show HiVA consistently surpasses state-of-the-art approaches. Qualitative analyses reveal semantically meaningful activation patterns, demonstrating robust and interpretable cross-modal correspondences.

Conclusion: HiVA effectively leverages multi-grained vision-based AU features with refined language-based AU details through collaborative cross-modal learning, achieving robust and semantically enriched AU detection capabilities for comprehensive facial behavior analysis.

Abstract: Facial Action Unit (AU) detection seeks to recognize subtle facial muscle activations as defined by the Facial Action Coding System (FACS). A primary challenge w.r.t AU detection is the effective learning of discriminative and generalizable AU representations under conditions of limited annotated data. To address this, we propose a Hierarchical Vision-language Interaction for AU Understanding (HiVA) method, which leverages textual AU descriptions as semantic priors to guide and enhance AU detection. Specifically, HiVA employs a large language model to generate diverse and contextually rich AU descriptions to strengthen language-based representation learning. To capture both fine-grained and holistic vision-language associations, HiVA introduces an AU-aware dynamic graph module that facilitates the learning of AU-specific visual representations. These features are further integrated within a hierarchical cross-modal attention architecture comprising two complementary mechanisms: Disentangled Dual Cross-Attention (DDCA), which establishes fine-grained, AU-specific interactions between visual and textual features, and Contextual Dual Cross-Attention (CDCA), which models global inter-AU dependencies. This collaborative, cross-modal learning paradigm enables HiVA to leverage multi-grained vision-based AU features in conjunction with refined language-based AU details, culminating in robust and semantically enriched AU detection capabilities. Extensive experiments show that HiVA consistently surpasses state-of-the-art approaches. Besides, qualitative analyses reveal that HiVA produces semantically meaningful activation patterns, highlighting its efficacy in learning robust and interpretable cross-modal correspondences for comprehensive facial behavior analysis.

</details>


### [124] [D-SECURE: Dual-Source Evidence Combination for Unified Reasoning in Misinformation Detection](https://arxiv.org/abs/2602.14441)
*Gagandeep Singh,Samudi Amarasinghe,Priyanka Singh*

Main category: cs.CV

TL;DR: D-SECURE is a framework that combines internal manipulation detection with external evidence-based reasoning to combat multimodal misinformation in news-style posts.


<details>
  <summary>Details</summary>
Motivation: Existing systems fail against multimodal misinformation that mixes realistic image edits with misleading text. Content-based detectors miss global factual truth, while retrieval-based fact-checkers miss subtle visual/textual manipulations, creating failure cases where internally consistent fabrications bypass detection.

Method: D-SECURE integrates HAMMER manipulation detector with DEFAME retrieval pipeline. DEFAME performs broad verification, and HAMMER analyzes residual or uncertain cases that may contain fine-grained edits. The framework combines internal manipulation detection with external evidence-based reasoning.

Result: Experiments on DGM4 and ClaimReview samples highlight the complementary strengths of both systems and motivate their fusion. The framework provides a unified, explainable report incorporating manipulation cues and external evidence.

Conclusion: Combining internal manipulation detection with external evidence-based reasoning creates a more robust system against multimodal misinformation, addressing limitations of single-source approaches and providing comprehensive verification for news-style posts.

Abstract: Multimodal misinformation increasingly mixes realistic im-age edits with fluent but misleading text, producing persuasive posts that are difficult to verify. Existing systems usually rely on a single evidence source. Content-based detectors identify local inconsistencies within an image and its caption but cannot determine global factual truth. Retrieval-based fact-checkers reason over external evidence but treat inputs as coarse claims and often miss subtle visual or textual manipulations. This separation creates failure cases where internally consistent fabrications bypass manipulation detectors and fact-checkers verify claims that contain pixel-level or token-level corruption. We present D-SECURE, a framework that combines internal manipulation detection with external evidence-based reasoning for news-style posts. D-SECURE integrates the HAMMER manipulation detector with the DEFAME retrieval pipeline. DEFAME performs broad verification, and HAMMER analyses residual or uncertain cases that may contain fine-grained edits. Experiments on DGM4 and ClaimReview samples highlight the complementary strengths of both systems and motivate their fusion. We provide a unified, explainable report that incorporates manipulation cues and external evidence.

</details>


### [125] [Controlling Your Image via Simplified Vector Graphics](https://arxiv.org/abs/2602.14443)
*Lanqing Guo,Xi Liu,Yufei Wang,Zhihao Li,Siyu Huang*

Main category: cs.CV

TL;DR: The paper introduces Vec2Pix, a method for layer-wise controllable image generation using simplified vector graphics representations, enabling intuitive element-level modifications like shape adjustment, color alteration, and object manipulation.


<details>
  <summary>Details</summary>
Motivation: Current image generation methods lack fine-grained control at the element level, making it difficult to perform intuitive modifications such as adjusting shapes, altering colors, or adding/removing objects. The authors aim to address this fundamental challenge in controllable image generation.

Method: The approach first parses images into hierarchical vector graphics representations that are semantic-aligned and structurally coherent. Then, a novel image synthesis framework is designed that uses these VG representations as guidance, allowing users to modify elements and translate edits into photorealistic outputs by leveraging structural/semantic features of VGs with noise prediction.

Result: Extensive experiments demonstrate the method's effectiveness in diverse applications including image editing, object-level manipulation, and fine-grained content creation. The approach establishes a new paradigm for controllable image generation with precise control over geometry, color, and object semantics.

Conclusion: Vec2Pix introduces a novel approach to layer-wise controllable image generation through vector graphics representations, enabling intuitive element-level modifications and establishing a new paradigm for controllable image synthesis with applications in image editing and content creation.

Abstract: Recent advances in image generation have achieved remarkable visual quality, while a fundamental challenge remains: Can image generation be controlled at the element level, enabling intuitive modifications such as adjusting shapes, altering colors, or adding and removing objects? In this work, we address this challenge by introducing layer-wise controllable generation through simplified vector graphics (VGs). Our approach first efficiently parses images into hierarchical VG representations that are semantic-aligned and structurally coherent. Building on this representation, we design a novel image synthesis framework guided by VGs, allowing users to freely modify elements and seamlessly translate these edits into photorealistic outputs. By leveraging the structural and semantic features of VGs in conjunction with noise prediction, our method provides precise control over geometry, color, and object semantics. Extensive experiments demonstrate the effectiveness of our approach in diverse applications, including image editing, object-level manipulation, and fine-grained content creation, establishing a new paradigm for controllable image generation. Project page: https://guolanqing.github.io/Vec2Pix/

</details>


### [126] [CoCoDiff: Correspondence-Consistent Diffusion Model for Fine-grained Style Transfer](https://arxiv.org/abs/2602.14464)
*Wenbo Nie,Zixiang Li,Renshuai Tao,Bin Wu,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: CoCoDiff is a training-free style transfer framework that uses pretrained diffusion models to achieve fine-grained, semantically consistent stylization by mining pixel-wise correspondence cues from intermediate diffusion features.


<details>
  <summary>Details</summary>
Motivation: Existing style transfer methods operate at global level but overlook region-wise and pixel-wise semantic correspondence, failing to preserve semantic alignment between similar objects when transferring visual style.

Method: CoCoDiff leverages pretrained latent diffusion models without additional training. It introduces: 1) pixel-wise semantic correspondence module that mines intermediate diffusion features to construct dense alignment maps between content and style images, and 2) cycle-consistency module that enforces structural and perceptual alignment across iterations for object/region level stylization.

Result: Despite requiring no additional training or supervision, CoCoDiff achieves state-of-the-art visual quality and strong quantitative results, outperforming methods that rely on extra training or annotations.

Conclusion: CoCoDiff demonstrates that correspondence cues within generative diffusion models are under-explored and can be effectively leveraged for fine-grained, semantically consistent style transfer without additional training costs.

Abstract: Transferring visual style between images while preserving semantic correspondence between similar objects remains a central challenge in computer vision. While existing methods have made great strides, most of them operate at global level but overlook region-wise and even pixel-wise semantic correspondence. To address this, we propose CoCoDiff, a novel training-free and low-cost style transfer framework that leverages pretrained latent diffusion models to achieve fine-grained, semantically consistent stylization. We identify that correspondence cues within generative diffusion models are under-explored and that content consistency across semantically matched regions is often neglected. CoCoDiff introduces a pixel-wise semantic correspondence module that mines intermediate diffusion features to construct a dense alignment map between content and style images. Furthermore, a cycle-consistency module then enforces structural and perceptual alignment across iterations, yielding object and region level stylization that preserves geometry and detail. Despite requiring no additional training or supervision, CoCoDiff delivers state-of-the-art visual quality and strong quantitative results, outperforming methods that rely on extra training or annotations.

</details>


### [127] [TikArt: Aperture-Guided Observation for Fine-Grained Visual Reasoning via Reinforcement Learning](https://arxiv.org/abs/2602.14482)
*Hao Ding,Zhichuan Yang,Weijie Ge,Ziqin Gao,Chaoyi Lu,Lei Zhao*

Main category: cs.CV

TL;DR: TikArt is an aperture-guided agent for fine-grained visual reasoning in MLLMs that uses a Think-Aperture-Observe loop with Zoom and Segment actions to focus on small/cluttered regions, trained with AGRPO reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Fine-grained visual reasoning in multimodal LLMs suffers from loss of key evidence in tiny objects, cluttered regions, or subtle markings when using single global image encoding. Current approaches fail to effectively focus on and extract information from these critical local regions.

Method: TikArt uses a Think-Aperture-Observe loop: alternates between language generation and two aperture actions (Zoom for rectangular crops, Segment using SAM2 for mask-based crops). After each action, produces explicit observations to convert visual cues to linguistic memory. Built on Qwen3-VL-8B and trained with AGRPO (GRPO-style RL) using two-stage curriculum: warm up segmentation, then jointly optimize visual math, fine-grained VQA, and segmentation with rewards coupling task success with purposeful aperture use.

Result: Experiments on V*, HR-Bench-4K/8K, MME-RealWorld-Lite, MMStar, RefCOCO, and ReasonSeg show consistent gains over backbone model and yield interpretable aperture trajectories for high-resolution reasoning.

Conclusion: TikArt effectively addresses fine-grained visual reasoning by enabling multi-step region-focused reasoning through aperture-guided actions, with reinforcement learning optimizing both task performance and strategic aperture usage, demonstrating improved performance across diverse visual reasoning benchmarks.

Abstract: We address fine-grained visual reasoning in multimodal large language models (MLLMs), where key evidence may reside in tiny objects, cluttered regions, or subtle markings that are lost under a single global image encoding. We introduce TikArt (Thinking Aperture), an aperture-guided agent that casts multi-step vision-language reasoning as a decision process over regions of interest. TikArt follows a Think-Aperture-Observe loop, alternating between language generation and two aperture actions: Zoom extracts rectangular crops, while Segment invokes SAM2 to obtain mask-based crops for irregular targets. After every action, the model must produce an explicit observation, turning local visual cues into persistent linguistic memory. Built on Qwen3-VL-8B, TikArt optimizes its reasoning policy with AGRPO, a GRPO-style reinforcement learning algorithm with a two-stage curriculum: it warms up segmentation actions and then jointly optimizes visual math, fine-grained VQA, and segmentation, using rewards that couple task success with purposeful aperture use. Experiments on V*, HR-Bench-4K/8K, MME-RealWorld-Lite, MMStar, RefCOCO, and ReasonSeg show consistent gains over the backbone and yield interpretable aperture trajectories for high-resolution reasoning.

</details>


### [128] [Gaussian Mesh Renderer for Lightweight Differentiable Rendering](https://arxiv.org/abs/2602.14493)
*Xinpeng Liu,Fumio Okura*

Main category: cs.CV

TL;DR: Gaussian Mesh Renderer (GMR) integrates Gaussian splatting efficiency with mesh representation for faster differentiable rendering and better optimization with limited memory.


<details>
  <summary>Details</summary>
Motivation: Triangle mesh models are popular for surface reconstruction but suffer from slow/heavy optimization in traditional mesh-based differentiable renderers, while 3D Gaussian Splatting offers fast rendering but lacks mesh structure.

Method: Proposes Gaussian Mesh Renderer (GMR) that leverages 3DGS's efficient rasterization process by deriving Gaussian primitives analytically from mesh triangles, preserving structural fidelity and enabling gradient flow.

Result: Achieves smoother gradients compared to traditional mesh renderers, enabling better optimization with smaller batch sizes and limited memory constraints.

Conclusion: GMR successfully bridges Gaussian and mesh representations, offering a lightweight differentiable renderer that combines the efficiency of 3DGS with the structural benefits of mesh models.

Abstract: 3D Gaussian Splatting (3DGS) has enabled high-fidelity virtualization with fast rendering and optimization for novel view synthesis. On the other hand, triangle mesh models still remain a popular choice for surface reconstruction but suffer from slow or heavy optimization in traditional mesh-based differentiable renderers. To address this problem, we propose a new lightweight differentiable mesh renderer leveraging the efficient rasterization process of 3DGS, named Gaussian Mesh Renderer (GMR), which tightly integrates the Gaussian and mesh representations. Each Gaussian primitive is analytically derived from the corresponding mesh triangle, preserving structural fidelity and enabling the gradient flow. Compared to the traditional mesh renderers, our method achieves smoother gradients, which especially contributes to better optimization using smaller batch sizes with limited memory. Our implementation is available in the public GitHub repository at https://github.com/huntorochi/Gaussian-Mesh-Renderer.

</details>


### [129] [Uncertainty-Aware Vision-Language Segmentation for Medical Imaging](https://arxiv.org/abs/2602.14498)
*Aryan Das,Tanishq Rachamalla,Koushik Biswas,Swalpa Kumar Roy,Vinay Kumar Verma*

Main category: cs.CV

TL;DR: A novel uncertainty-aware multimodal segmentation framework using radiological images and clinical text with Modality Decoding Attention Block and Spectral-Entropic Uncertainty Loss for improved reliability in medical diagnosis.


<details>
  <summary>Details</summary>
Motivation: To address challenges in medical segmentation under complex clinical circumstances with poor image quality by leveraging both visual and textual information while incorporating uncertainty modeling for improved reliability.

Method: Proposes a multimodal segmentation framework with: 1) Modality Decoding Attention Block (MoDAB) with lightweight State Space Mixer (SSMix) for efficient cross-modal fusion and long-range dependency modeling; 2) Spectral-Entropic Uncertainty (SEU) Loss that jointly captures spatial overlap, spectral consistency, and predictive uncertainty in a unified objective.

Result: Extensive experiments on QATA-COVID19, MosMed++, and Kvasir-SEG datasets demonstrate superior segmentation performance while being significantly more computationally efficient than existing State-of-the-Art approaches.

Conclusion: The framework highlights the importance of incorporating uncertainty modeling and structured modality alignment in vision-language medical segmentation tasks, improving reliability in complex clinical scenarios with poor image quality.

Abstract: We introduce a novel uncertainty-aware multimodal segmentation framework that leverages both radiological images and associated clinical text for precise medical diagnosis. We propose a Modality Decoding Attention Block (MoDAB) with a lightweight State Space Mixer (SSMix) to enable efficient cross-modal fusion and long-range dependency modelling. To guide learning under ambiguity, we propose the Spectral-Entropic Uncertainty (SEU) Loss, which jointly captures spatial overlap, spectral consistency, and predictive uncertainty in a unified objective. In complex clinical circumstances with poor image quality, this formulation improves model reliability. Extensive experiments on various publicly available medical datasets, QATA-COVID19, MosMed++, and Kvasir-SEG, demonstrate that our method achieves superior segmentation performance while being significantly more computationally efficient than existing State-of-the-Art (SoTA) approaches. Our results highlight the importance of incorporating uncertainty modelling and structured modality alignment in vision-language medical segmentation tasks. Code: https://github.com/arya-domain/UA-VLS

</details>


### [130] [Prototype Instance-semantic Disentanglement with Low-rank Regularized Subspace Clustering for WSIs Explainable Recognition](https://arxiv.org/abs/2602.14501)
*Chentao Li,Pan Huang*

Main category: cs.CV

TL;DR: PID-LRSC: A prototype instance semantic disentanglement framework with low-rank regularized subspace clustering for tumor detection in whole slide images, addressing instance-semantic entanglement issues in pathological diagnosis.


<details>
  <summary>Details</summary>
Motivation: Tumor detection in whole slide images faces challenges: 1) high similarity between tumor and precancerous lesions, and 2) non-tumor instances greatly outnumber tumor instances. These cause instance-semantic entanglement in multi-instance learning, degrading model representation and interpretability.

Method: Two-component framework: 1) Low-rank regularized subspace clustering (LRSC) with secondary instance subspace learning to address instance entanglement from imbalanced non-tumor instances; 2) Prototype instance semantic disentanglement (PID) with enhanced contrastive learning to resolve semantic entanglement between tumor and precancerous tissues.

Result: Extensive experiments on multicentre pathology datasets show PID-LRSC outperforms other state-of-the-art methods, providing clearer instance semantics during decision-making.

Conclusion: PID-LRSC significantly enhances the reliability of auxiliary diagnostic outcomes by effectively disentangling instance-semantic entanglement in pathological whole slide image analysis.

Abstract: The tumor region plays a key role in pathological diagnosis. Tumor tissues are highly similar to precancerous lesions and non tumor instances often greatly exceed tumor instances in whole slide images (WSIs). These issues cause instance-semantic entanglement in multi-instance learning frameworks, degrading both model representation capability and interpretability. To address this, we propose an end-to-end prototype instance semantic disentanglement framework with low-rank regularized subspace clustering, PID-LRSC, in two aspects. First, we use secondary instance subspace learning to construct low-rank regularized subspace clustering (LRSC), addressing instance entanglement caused by an excessive proportion of non tumor instances. Second, we employ enhanced contrastive learning to design prototype instance semantic disentanglement (PID), resolving semantic entanglement caused by the high similarity between tumor and precancerous tissues. We conduct extensive experiments on multicentre pathology datasets, implying that PID-LRSC outperforms other SOTA methods. Overall, PID-LRSC provides clearer instance semantics during decision-making and significantly enhances the reliability of auxiliary diagnostic outcomes.

</details>


### [131] [MacNet: An End-to-End Manifold-Constrained Adaptive Clustering Network for Interpretable Whole Slide Image Classification](https://arxiv.org/abs/2602.14509)
*Mingrui Ma,Chentao Li,Pan Huang,Jing Qin*

Main category: cs.CV

TL;DR: Proposes an end-to-end MIL framework for WSI analysis integrating Grassmann re-embedding and manifold adaptive clustering to improve both grading accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current two-step WSI frameworks use offline feature encoders without domain knowledge. Attention-based MIL methods are outcome-oriented with limited interpretability, while clustering approaches suffer from high-dimensional features and ambiguous centroids.

Method: End-to-end MIL framework with Grassmann re-embedding and manifold adaptive clustering that leverages manifold geometric structure for robust clustering. Includes prior knowledge guiding proxy instance labeling and aggregation strategy to approximate patch labels and focus on tumor regions.

Result: Experiments on multicentre WSI datasets show superior performance in both grading accuracy and interpretability. End-to-end learning refines feature representations while requiring acceptable computation resources.

Conclusion: The proposed framework effectively addresses limitations of existing methods by combining the strengths of MIL with improved clustering through manifold geometry and domain-specific knowledge integration.

Abstract: Whole slide images (WSIs) are the gold standard for pathological diagnosis and sub-typing. Current main-stream two-step frameworks employ offline feature encoders trained without domain-specific knowledge. Among them, attention-based multiple instance learning (MIL) methods are outcome-oriented and offer limited interpretability. Clustering-based approaches can provide explainable decision-making process but suffer from high dimension features and semantically ambiguous centroids. To this end, we propose an end-to-end MIL framework that integrates Grassmann re-embedding and manifold adaptive clustering, where the manifold geometric structure facilitates robust clustering results. Furthermore, we design a prior knowledge guiding proxy instance labeling and aggregation strategy to approximate patch labels and focus on pathologically relevant tumor regions. Experiments on multicentre WSI datasets demonstrate that: 1) our cluster-incorporated model achieves superior performance in both grading accuracy and interpretability; 2) end-to-end learning refines better feature representations and it requires acceptable computation resources.

</details>


### [132] [MedVAR: Towards Scalable and Efficient Medical Image Generation via Next-scale Autoregressive Prediction](https://arxiv.org/abs/2602.14512)
*Zhicheng He,Yunpeng Zhao,Junde Wu,Ziwei Niu,Zijun Li,Lanfen Lin,Yueming Jin*

Main category: cs.CV

TL;DR: MedVAR is the first autoregressive foundation model for medical image generation using next-scale prediction for fast, scalable synthesis with structured multi-scale representations.


<details>
  <summary>Details</summary>
Motivation: Current medical image generation approaches lack architectural efficiency, sufficient multi-organ data, and principled evaluation needed for scalable generative backbones in applications like data augmentation and privacy-preserving data sharing.

Method: MedVAR uses autoregressive-based next-scale prediction paradigm for coarse-to-fine image generation, producing structured multi-scale representations. It's trained on a harmonized dataset of ~440,000 CT and MRI images spanning six anatomical regions.

Result: Comprehensive experiments across fidelity, diversity, and scalability show MedVAR achieves state-of-the-art generative performance and offers a promising architectural direction for future medical generative foundation models.

Conclusion: MedVAR represents the first autoregressive-based foundation model for medical imaging that enables fast, scale-up-friendly synthesis with structured representations suitable for downstream applications.

Abstract: Medical image generation is pivotal in applications like data augmentation for low-resource clinical tasks and privacy-preserving data sharing. However, developing a scalable generative backbone for medical imaging requires architectural efficiency, sufficient multi-organ data, and principled evaluation, yet current approaches leave these aspects unresolved. Therefore, we introduce MedVAR, the first autoregressive-based foundation model that adopts the next-scale prediction paradigm to enable fast and scale-up-friendly medical image synthesis. MedVAR generates images in a coarse-to-fine manner and produces structured multi-scale representations suitable for downstream use. To support hierarchical generation, we curate a harmonized dataset of around 440,000 CT and MRI images spanning six anatomical regions. Comprehensive experiments across fidelity, diversity, and scalability show that MedVAR achieves state-of-the-art generative performance and offers a promising architectural direction for future medical generative foundation models.

</details>


### [133] [Efficient Text-Guided Convolutional Adapter for the Diffusion Model](https://arxiv.org/abs/2602.14514)
*Aryan Das,Koushik Biswas,Swalpa Kumar Roy,Badri Narayana Patro,Vinay Kumar Verma*

Main category: cs.CV

TL;DR: Nexus Adapters are efficient text-guided adapters for diffusion models that preserve structure while being parameter-efficient and prompt-aware.


<details>
  <summary>Details</summary>
Motivation: Existing structure-preserving methods for conditional image generation are inefficient, often requiring as many parameters as the base model, and lack prompt awareness in their adapters.

Method: Proposed two efficient adapters (Nexus Prime and Nexus Slim) with cross-attention mechanisms for multimodal conditioning, making them both prompt-aware and structure-preserving.

Result: Nexus Prime enhances performance with only 8M additional parameters vs baseline, while Nexus Slim has 18M fewer parameters than T2I-Adapter while achieving state-of-the-art results.

Conclusion: Nexus Adapters provide efficient, prompt-aware structure preservation for diffusion models, addressing parameter inefficiency and lack of prompt awareness in existing methods.

Abstract: We introduce the Nexus Adapters, novel text-guided efficient adapters to the diffusion-based framework for the Structure Preserving Conditional Generation (SPCG). Recently, structure-preserving methods have achieved promising results in conditional image generation by using a base model for prompt conditioning and an adapter for structure input, such as sketches or depth maps. These approaches are highly inefficient and sometimes require equal parameters in the adapter compared to the base architecture. It is not always possible to train the model since the diffusion model is itself costly, and doubling the parameter is highly inefficient. In these approaches, the adapter is not aware of the input prompt; therefore, it is optimal only for the structural input but not for the input prompt. To overcome the above challenges, we proposed two efficient adapters, Nexus Prime and Slim, which are guided by prompts and structural inputs. Each Nexus Block incorporates cross-attention mechanisms to enable rich multimodal conditioning. Therefore, the proposed adapter has a better understanding of the input prompt while preserving the structure. We conducted extensive experiments on the proposed models and demonstrated that the Nexus Prime adapter significantly enhances performance, requiring only 8M additional parameters compared to the baseline, T2I-Adapter. Furthermore, we also introduced a lightweight Nexus Slim adapter with 18M fewer parameters than the T2I-Adapter, which still achieved state-of-the-art results. Code: https://github.com/arya-domain/Nexus-Adapters

</details>


### [134] [Architectural Insights for Post-Tornado Damage Recognition](https://arxiv.org/abs/2602.14523)
*Robinson Umeike,Thang Dao,Shane Crawford,John van de Lindt,Blythe Johnston,Wanting,Wang,Trung Do,Ajibola Mofikoya,Sarbesh Banjara,Cuong Pham*

Main category: cs.CV

TL;DR: Systematic evaluation of 79 deep learning models for tornado damage assessment reveals optimizer choice (SGD over Adam) and low learning rate are more critical than architecture selection for achieving operational performance.


<details>
  <summary>Details</summary>
Motivation: Current automated methods struggle with tornado damage assessment due to visual complexity, domain shift from standard datasets, and extreme class imbalance in disaster data, hindering critical emergency response operations.

Method: Systematic experimental framework evaluating 79 CNN and Vision Transformer models across 2,300+ experiments on new QSTD benchmark dataset, analyzing architecture-optimizer interactions, learning rates, and cross-event generalization.

Result: Optimizer choice proved more consequential than architecture: SGD over Adam provided +25 to +38 F1 gains for Vision Transformers, making them competitive with top CNNs. Low learning rate (1e-4) boosted average F1 by +10.2 points. ConvNeXt-Base with optimized settings achieved 46.4% Macro F1 (+34.6 over baseline) with strong cross-event generalization.

Conclusion: Achieving operational-grade tornado damage assessment requires optimizing both architecture and training dynamics, with optimizer selection and learning rate being critical factors that can dramatically transform model performance and rankings.

Abstract: Rapid and accurate building damage assessment in the immediate aftermath of tornadoes is critical for coordinating life-saving search and rescue operations, optimizing emergency resource allocation, and accelerating community recovery. However, current automated methods struggle with the unique visual complexity of tornado-induced wreckage, primarily due to severe domain shift from standard pre-training datasets and extreme class imbalance in real-world disaster data. To address these challenges, we introduce a systematic experimental framework evaluating 79 open-source deep learning models, encompassing both Convolutional Neural Networks (CNNs) and Vision Transformers, across over 2,300 controlled experiments on our newly curated Quad-State Tornado Damage (QSTD) benchmark dataset. Our findings reveal that achieving operational-grade performance hinges on a complex interaction between architecture and optimization, rather than architectural selection alone. Most strikingly, we demonstrate that optimizer choice can be more consequential than architecture: switching from Adam to SGD provided dramatic F1 gains of +25 to +38 points for Vision Transformer and Swin Transformer families, fundamentally reversing their ranking from bottom-tier to competitive with top-performing CNNs. Furthermore, a low learning rate of 1x10^(-4) proved universally critical, boosting average F1 performance by +10.2 points across all architectures. Our champion model, ConvNeXt-Base trained with these optimized settings, demonstrated strong cross-event generalization on the held-out Tuscaloosa-Moore Tornado Damage (TMTD) dataset, achieving 46.4% Macro F1 (+34.6 points over baseline) and retaining 85.5% Ordinal Top-1 Accuracy despite temporal and sensor domain shifts.

</details>


### [135] [Error Patterns in Historical OCR: A Comparative Analysis of TrOCR and a Vision-Language Model](https://arxiv.org/abs/2602.14524)
*Ari Vesalainen,Eetu Mäkelä,Laura Ruotsalainen,Mikko Tolonen*

Main category: cs.CV

TL;DR: Comparing TrOCR and Qwen for 18th-century OCR reveals that while Qwen has better overall accuracy, it silently normalizes historical forms, whereas TrOCR preserves orthography better but has more error propagation.


<details>
  <summary>Details</summary>
Motivation: OCR for 18th-century texts is challenging due to degraded print, archaic glyphs, and non-standardized orthography. Current metrics like CER/WER provide limited insight into reliability for scholarly use, necessitating deeper analysis of how different OCR architectures perform on historical materials.

Method: Compared a dedicated OCR transformer (TrOCR) and a general-purpose Vision-Language Model (Qwen) on line-level historical English texts using length-weighted accuracy metrics and hypothesis-driven error analysis to understand their performance characteristics.

Result: Qwen achieved lower CER/WER and greater robustness to degraded input but exhibited selective linguistic regularization and orthographic normalization that may silently alter historically meaningful forms. TrOCR preserved orthographic fidelity more consistently but was more prone to cascading error propagation.

Conclusion: Architectural inductive biases shape OCR error structure systematically. Models with similar aggregate accuracy can differ substantially in error locality, detectability, and downstream scholarly risk, highlighting the need for architecture-aware evaluation in historical digitization workflows.

Abstract: Optical Character Recognition (OCR) of eighteenth-century printed texts remains challenging due to degraded print quality, archaic glyphs, and non-standardized orthography. Although transformer-based OCR systems and Vision-Language Models (VLMs) achieve strong aggregate accuracy, metrics such as Character Error Rate (CER) and Word Error Rate (WER) provide limited insight into their reliability for scholarly use. We compare a dedicated OCR transformer (TrOCR) and a general-purpose Vision-Language Model (Qwen) on line-level historical English texts using length-weighted accuracy metrics and hypothesis driven error analysis.
  While Qwen achieves lower CER/WER and greater robustness to degraded input, it exhibits selective linguistic regularization and orthographic normalization that may silently alter historically meaningful forms. TrOCR preserves orthographic fidelity more consistently but is more prone to cascading error propagation. Our findings show that architectural inductive biases shape OCR error structure in systematic ways. Models with similar aggregate accuracy can differ substantially in error locality, detectability, and downstream scholarly risk, underscoring the need for architecture-aware evaluation in historical digitization workflows.

</details>


### [136] [Cross-view Domain Generalization via Geometric Consistency for LiDAR Semantic Segmentation](https://arxiv.org/abs/2602.14525)
*Jindong Zhao,Yuan Gao,Yang Xia,Sheng Nie,Jun Yue,Weiwei Sun,Shaobo Xia*

Main category: cs.CV

TL;DR: CVGC is a novel framework for cross-view domain generalization in LiDAR semantic segmentation that addresses viewpoint-dependent structural incompleteness and non-uniform point density through geometric consistency.


<details>
  <summary>Details</summary>
Motivation: Existing domain generalization methods for LiDAR semantic segmentation assume similar acquisition views (e.g., vehicle-mounted) and struggle with cross-view scenarios where observations differ substantially due to viewpoint-dependent structural incompleteness and non-uniform point density.

Method: Proposes CVGC framework with two key modules: 1) cross-view geometric augmentation module that models viewpoint-induced variations in visibility and sampling density to generate multiple cross-view observations of the same scene, and 2) geometric consistency module that enforces consistent semantic and occupancy predictions across geometrically augmented point clouds.

Result: Extensive experiments on six public LiDAR datasets establish the first systematic evaluation of cross-view domain generalization for LiDAR semantic segmentation, demonstrating that CVGC consistently outperforms state-of-the-art methods when generalizing from a single source domain to multiple target domains with heterogeneous acquisition viewpoints.

Conclusion: CVGC effectively addresses the cross-view domain generalization challenge in LiDAR semantic segmentation by modeling viewpoint variations and enforcing geometric consistency, providing a robust solution for real-world applications with diverse acquisition viewpoints.

Abstract: Domain-generalized LiDAR semantic segmentation (LSS) seeks to train models on source-domain point clouds that generalize reliably to multiple unseen target domains, which is essential for real-world LiDAR applications. However, existing approaches assume similar acquisition views (e.g., vehicle-mounted) and struggle in cross-view scenarios, where observations differ substantially due to viewpoint-dependent structural incompleteness and non-uniform point density. Accordingly, we formulate cross-view domain generalization for LiDAR semantic segmentation and propose a novel framework, termed CVGC (Cross-View Geometric Consistency). Specifically, we introduce a cross-view geometric augmentation module that models viewpoint-induced variations in visibility and sampling density, generating multiple cross-view observations of the same scene. Subsequently, a geometric consistency module enforces consistent semantic and occupancy predictions across geometrically augmented point clouds of the same scene. Extensive experiments on six public LiDAR datasets establish the first systematic evaluation of cross-view domain generalization for LiDAR semantic segmentation, demonstrating that CVGC consistently outperforms state-of-the-art methods when generalizing from a single source domain to multiple target domains with heterogeneous acquisition viewpoints. The source code will be publicly available at https://github.com/KintomZi/CVGC-DG

</details>


### [137] [MoRL: Reinforced Reasoning for Unified Motion Understanding and Generation](https://arxiv.org/abs/2602.14534)
*Hongpeng Wang,Zeyu Zhang,Wenhao Li,Hao Tang*

Main category: cs.CV

TL;DR: MoRL is a unified multimodal motion model that combines supervised fine-tuning with reinforcement learning using verifiable rewards, enhanced by Chain-of-Motion reasoning and large-scale CoT datasets for improved motion understanding and generation.


<details>
  <summary>Details</summary>
Motivation: Current human motion understanding and generation models lack sufficient reasoning capability and test-time planning, limiting their practical applications in vision and robotics.

Method: Proposes MoRL with supervised fine-tuning + RL with verifiable rewards, task-specific reward design (semantic alignment, reasoning coherence, physical plausibility, text-motion consistency), Chain-of-Motion test-time reasoning, and two large-scale CoT datasets (MoUnd-CoT-140K and MoGen-CoT-140K).

Result: Achieves significant gains over state-of-the-art baselines on HumanML3D and KIT-ML datasets, improving both logical reasoning and perceptual realism.

Conclusion: MoRL provides a unified framework that enhances motion understanding and generation through verifiable rewards and test-time reasoning, with publicly available code and datasets.

Abstract: Human motion understanding and generation are crucial for vision and robotics but remain limited in reasoning capability and test-time planning. We propose MoRL, a unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards. Our task-specific reward design combines semantic alignment and reasoning coherence for understanding with physical plausibility and text-motion consistency for generation, improving both logical reasoning and perceptual realism. To further enhance inference, we introduce Chain-of-Motion (CoM), a test-time reasoning method that enables step-by-step planning and reflection. We also construct two large-scale CoT datasets, MoUnd-CoT-140K and MoGen-CoT-140K, to align motion sequences with reasoning traces and action descriptions. Experiments on HumanML3D and KIT-ML show that MoRL achieves significant gains over state-of-the-art baselines. Code: https://github.com/AIGeeksGroup/MoRL. Website: https://aigeeksgroup.github.io/MoRL.

</details>


### [138] [OmniVTON++: Training-Free Universal Virtual Try-On with Principal Pose Guidance](https://arxiv.org/abs/2602.14552)
*Zhaotong Yang,Yong Du,Shengfeng He,Yuhui Li,Xinzhe Li,Yangyang Xu,Junyu Dong,Jian Yang*

Main category: cs.CV

TL;DR: OmniVTON++ is a training-free virtual try-on framework that achieves universal applicability across diverse scenarios without task-specific retraining, using structured garment morphing, pose guidance, and boundary stitching.


<details>
  <summary>Details</summary>
Motivation: Existing VTON approaches are typically optimized for specific data conditions, requiring retraining for different scenarios and limiting their generalization as a unified solution. There's a need for a universal framework that works across diverse settings without retraining.

Method: The framework coordinates three key components: 1) Structured Garment Morphing for correspondence-driven garment adaptation, 2) Principal Pose Guidance for step-wise structural regulation during diffusion sampling, and 3) Continuous Boundary Stitching for boundary-aware refinement. It operates as a training-free, cohesive pipeline.

Result: OmniVTON++ achieves state-of-the-art performance across diverse generalization settings including cross-dataset and cross-garment-type evaluations. It reliably operates across scenarios and diffusion backbones within a single formulation, and supports multi-garment, multi-human, and anime character virtual try-on.

Conclusion: OmniVTON++ presents a universal, training-free VTON framework that overcomes the limitations of existing approaches by addressing garment alignment, human structural coherence, and boundary continuity challenges. It expands the scope of virtual try-on applications and demonstrates strong generalization capabilities.

Abstract: Image-based Virtual Try-On (VTON) concerns the synthesis of realistic person imagery through garment re-rendering under human pose and body constraints. In practice, however, existing approaches are typically optimized for specific data conditions, making their deployment reliant on retraining and limiting their generalization as a unified solution. We present OmniVTON++, a training-free VTON framework designed for universal applicability. It addresses the intertwined challenges of garment alignment, human structural coherence, and boundary continuity by coordinating Structured Garment Morphing for correspondence-driven garment adaptation, Principal Pose Guidance for step-wise structural regulation during diffusion sampling, and Continuous Boundary Stitching for boundary-aware refinement, forming a cohesive pipeline without task-specific retraining. Experimental results demonstrate that OmniVTON++ achieves state-of-the-art performance across diverse generalization settings, including cross-dataset and cross-garment-type evaluations, while reliably operating across scenarios and diffusion backbones within a single formulation. In addition to single-garment, single-human cases, the framework supports multi-garment, multi-human, and anime character virtual try-on, expanding the scope of virtual try-on applications. The source code will be released to the public.

</details>


### [139] [DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving](https://arxiv.org/abs/2602.14577)
*Chenxu Dang,Sining Ang,Yongkang Li,Haochen Tian,Jie Wang,Guang Li,Hangjun Ye,Jie Ma,Long Chen,Yan Wang*

Main category: cs.CV

TL;DR: DriveFine is a masked diffusion VLA model for autonomous driving that combines flexible decoding with self-correction capabilities using a novel block-MoE architecture with separate generation and refinement experts.


<details>
  <summary>Details</summary>
Motivation: Current VLA models for autonomous driving have limitations: diffusion-based planners suffer from modality alignment issues, low training efficiency, and limited generalization, while token-based planners have cumulative causal errors and irreversible decoding. These two dominant paradigms have complementary strengths and weaknesses.

Method: Proposes DriveFine with a novel plug-and-play block-MoE architecture that seamlessly injects a refinement expert on top of the generation expert. Uses explicit expert selection during inference and gradient blocking during training to fully decouple the experts. Also designs a hybrid reinforcement learning strategy to encourage effective exploration of refinement expert while maintaining training stability.

Result: Extensive experiments on NAVSIM v1, v2, and Navhard benchmarks demonstrate that DriveFine exhibits strong efficacy and robustness.

Conclusion: DriveFine successfully addresses limitations of existing VLA models by combining flexible decoding with self-correction capabilities through the block-MoE design, showing promising results for autonomous driving applications.

Abstract: Vision-Language-Action (VLA) models for autonomous driving increasingly adopt generative planners trained with imitation learning followed by reinforcement learning. Diffusion-based planners suffer from modality alignment difficulties, low training efficiency, and limited generalization. Token-based planners are plagued by cumulative causal errors and irreversible decoding. In summary, the two dominant paradigms exhibit complementary strengths and weaknesses. In this paper, we propose DriveFine, a masked diffusion VLA model that combines flexible decoding with self-correction capabilities. In particular, we design a novel plug-and-play block-MoE, which seamlessly injects a refinement expert on top of the generation expert. By enabling explicit expert selection during inference and gradient blocking during training, the two experts are fully decoupled, preserving the foundational capabilities and generic patterns of the pretrained weights, which highlights the flexibility and extensibility of the block-MoE design. Furthermore, we design a hybrid reinforcement learning strategy that encourages effective exploration of refinement expert while maintaining training stability. Extensive experiments on NAVSIM v1, v2, and Navhard benchmarks demonstrate that DriveFine exhibits strong efficacy and robustness. The code will be released at https://github.com/MSunDYY/DriveFine.

</details>


### [140] [YOLO26: A Comprehensive Architecture Overview and Key Improvements](https://arxiv.org/abs/2602.14582)
*Priyanto Hidayatullah,Refdinal Tubagus*

Main category: cs.CV

TL;DR: YOLO26 introduces key enhancements including elimination of DFL, end-to-end NMS-free inference, ProgLoss+STAL, and MuSGD optimizer to achieve 43% faster CPU inference for edge devices, while improving performance on segmentation, pose estimation, and OBB tasks.


<details>
  <summary>Details</summary>
Motivation: To provide the first comprehensive architectural analysis of YOLO26, extracting operational mechanisms from source code that others have overlooked, and to help researchers/developers understand and enhance YOLO to maintain its leadership in computer vision.

Method: Rigorous architectural investigation using YOLO26's GitHub repository source code and official documentation to extract authentic operational mechanisms, resulting in creation of the first CNN-based YOLO26 architectural diagram.

Result: YOLO26 achieves 43% faster inference speed in CPU mode through key improvements: elimination of DFL, end-to-end NMS-free inference, ProgLoss+STAL label assignment, and MuSGD optimizer, enabling real-time performance on edge devices without GPUs.

Conclusion: This study provides the first precise architectural understanding of YOLO26's core CNN architecture, offering valuable insights for researchers and developers to enhance YOLO and maintain its position as the leading deep learning model in computer vision.

Abstract: You Only Look Once (YOLO) has been the prominent model for computer vision in deep learning for a decade. This study explores the novel aspects of YOLO26, the most recent version in the YOLO series. The elimination of Distribution Focal Loss (DFL), implementation of End-to-End NMS-Free Inference, introduction of ProgLoss + Small-Target-Aware Label Assignment (STAL), and use of the MuSGD optimizer are the primary enhancements designed to improve inference speed, which is claimed to achieve a 43% boost in CPU mode. This is designed to allow YOLO26 to attain real-time performance on edge devices or those without GPUs. Additionally, YOLO26 offers improvements in many computer vision tasks, including instance segmentation, pose estimation, and oriented bounding box (OBB) decoding. We aim for this effort to provide more value than just consolidating information already included in the existing technical documentation. Therefore, we performed a rigorous architectural investigation into YOLO26, mostly using the source code available in its GitHub repository and its official documentation. The authentic and detailed operational mechanisms of YOLO26 are inside the source code, which is seldom extracted by others. The YOLO26 architectural diagram is shown as the outcome of the investigation. This study is, to our knowledge, the first one presenting the CNN-based YOLO26 architecture, which is the core of YOLO26. Our objective is to provide a precise architectural comprehension of YOLO26 for researchers and developers aspiring to enhance the YOLO model, ensuring it remains the leading deep learning model in computer vision.

</details>


### [141] [VariViT: A Vision Transformer for Variable Image Sizes](https://arxiv.org/abs/2602.14615)
*Aswathi Varma,Suprosanna Shit,Chinmay Prabhakar,Daniel Scholz,Hongwei Bran Li,Bjoern Menze,Daniel Rueckert,Benedikt Wiestler*

Main category: cs.CV

TL;DR: VariViT is a Vision Transformer variant that handles variable-sized medical images while maintaining fixed patch size, improving tumor classification accuracy and reducing computation time.


<details>
  <summary>Details</summary>
Motivation: Standard ViTs require fixed-size image patches, which is problematic for medical imaging where irregular structures like tumors need variable-sized crops. Resizing medical images can degrade information and introduce artifacts, while fixed crops create variable foreground-to-background ratios that hinder feature learning.

Method: VariViT adapts Vision Transformers to handle variable image sizes while keeping patch size consistent. It introduces a novel positional embedding resizing scheme for variable numbers of patches and implements a new batching strategy to reduce computational complexity.

Result: On two 3D brain MRI datasets, VariViT outperforms vanilla ViTs and ResNet in glioma genotype prediction and brain tumor classification, achieving F1-scores of 75.5% and 76.3%. The batching strategy reduces computation time by up to 30% compared to conventional architectures.

Conclusion: VariViT effectively addresses the limitations of standard ViTs in medical imaging by enabling variable-sized inputs while maintaining computational efficiency, resulting in improved feature representation and classification performance for irregular medical structures like tumors.

Abstract: Vision Transformers (ViTs) have emerged as the state-of-the-art architecture in representation learning, leveraging self-attention mechanisms to excel in various tasks. ViTs split images into fixed-size patches, constraining them to a predefined size and necessitating pre-processing steps like resizing, padding, or cropping. This poses challenges in medical imaging, particularly with irregularly shaped structures like tumors. A fixed bounding box crop size produces input images with highly variable foreground-to-background ratios. Resizing medical images can degrade information and introduce artefacts, impacting diagnosis. Hence, tailoring variable-sized crops to regions of interest can enhance feature representation capabilities. Moreover, large images are computationally expensive, and smaller sizes risk information loss, presenting a computation-accuracy tradeoff. We propose VariViT, an improved ViT model crafted to handle variable image sizes while maintaining a consistent patch size. VariViT employs a novel positional embedding resizing scheme for a variable number of patches. We also implement a new batching strategy within VariViT to reduce computational complexity, resulting in faster training and inference times. In our evaluations on two 3D brain MRI datasets, VariViT surpasses vanilla ViTs and ResNet in glioma genotype prediction and brain tumor classification. It achieves F1-scores of 75.5% and 76.3%, respectively, learning more discriminative features. Our proposed batching strategy reduces computation time by up to 30% compared to conventional architectures. These findings underscore the efficacy of VariViT in image representation learning. Our code can be found here: https://github.com/Aswathi-Varma/varivit

</details>


### [142] [VIGIL: Tackling Hallucination Detection in Image Recontextualization](https://arxiv.org/abs/2602.14633)
*Joanna Wojciechowicz,Maria Łubniewska,Jakub Antczak,Justyna Baczyńska,Wojciech Gromski,Wojciech Kozłowski,Maciej Zięba*

Main category: cs.CV

TL;DR: VIGIL introduces the first benchmark dataset and framework for fine-grained categorization of hallucinations in multimodal image recontextualization tasks for LMMs, decomposing errors into five specific categories.


<details>
  <summary>Details</summary>
Motivation: Existing research treats hallucinations as a uniform issue, creating a significant gap in multimodal evaluation. There's a need for fine-grained categorization to better understand where models fail in image recontextualization tasks.

Method: Proposes a multi-stage detection pipeline that processes recontextualized images through specialized steps targeting object-level fidelity, background consistency, and omission detection using a coordinated ensemble of open-source models.

Result: The framework enables deeper understanding of model failures with explanations, filling a gap in the field as no prior methods offer such categorization and decomposition for this task.

Conclusion: VIGIL provides the first comprehensive benchmark for fine-grained hallucination analysis in multimodal image recontextualization, with open release of dataset, detection pipeline, and benchmark code to promote transparency and further exploration.

Abstract: We introduce VIGIL (Visual Inconsistency & Generative In-context Lucidity), the first benchmark dataset and framework providing a fine-grained categorization of hallucinations in the multimodal image recontextualization task for large multimodal models (LMMs). While existing research often treats hallucinations as a uniform issue, our work addresses a significant gap in multimodal evaluation by decomposing these errors into five categories: pasted object hallucinations, background hallucinations, object omission, positional & logical inconsistencies, and physical law violations. To address these complexities, we propose a multi-stage detection pipeline. Our architecture processes recontextualized images through a series of specialized steps targeting object-level fidelity, background consistency, and omission detection, leveraging a coordinated ensemble of open-source models, whose effectiveness is demonstrated through extensive experimental evaluations. Our approach enables a deeper understanding of where the models fail with an explanation; thus, we fill a gap in the field, as no prior methods offer such categorization and decomposition for this task. To promote transparency and further exploration, we openly release VIGIL, along with the detection pipeline and benchmark code, through our GitHub repository: https://github.com/mlubneuskaya/vigil and Data repository: https://huggingface.co/datasets/joannaww/VIGIL.

</details>


### [143] [SketchingReality: From Freehand Scene Sketches To Photorealistic Images](https://arxiv.org/abs/2602.14648)
*Ahmed Bourouis,Mikhail Bessmeltsev,Yulia Gryaditskaya*

Main category: cs.CV

TL;DR: A method for generating photorealistic images from freehand sketches that balances sketch adherence with realism, using semantic interpretation rather than strict edge alignment.


<details>
  <summary>Details</summary>
Motivation: While generative AI has advanced with various conditioning signals, true freehand sketch-to-image generation remains challenging due to sketches' inherent abstraction and lack of pixel-aligned ground truth images.

Method: Proposes a modulation-based approach that prioritizes semantic interpretation over strict edge adherence, with a novel loss function enabling training on freehand sketches without requiring ground-truth pixel-aligned images.

Result: Outperforms existing approaches in both semantic alignment with freehand sketch inputs and in the realism and overall quality of generated images.

Conclusion: The method successfully addresses the challenge of generating photorealistic images from freehand sketches by focusing on semantic interpretation rather than precise edge matching, enabling effective training without ground-truth alignment.

Abstract: Recent years have witnessed remarkable progress in generative AI, with natural language emerging as the most common conditioning input. As underlying models grow more powerful, researchers are exploring increasingly diverse conditioning signals, such as depth maps, edge maps, camera parameters, and reference images, to give users finer control over generation. Among different modalities, sketches are a natural and long-standing form of human communication, enabling rapid expression of visual concepts. Previous literature has largely focused on edge maps, often misnamed 'sketches', yet algorithms that effectively handle true freehand sketches, with their inherent abstraction and distortions, remain underexplored. We pursue the challenging goal of balancing photorealism with sketch adherence when generating images from freehand input. A key obstacle is the absence of ground-truth, pixel-aligned images: by their nature, freehand sketches do not have a single correct alignment. To address this, we propose a modulation-based approach that prioritizes semantic interpretation of the sketch over strict adherence to individual edge positions. We further introduce a novel loss that enables training on freehand sketches without requiring ground-truth pixel-aligned images. We show that our method outperforms existing approaches in both semantic alignment with freehand sketch inputs and in the realism and overall quality of the generated images.

</details>


### [144] [MeFEm: Medical Face Embedding model](https://arxiv.org/abs/2602.14672)
*Yury Borets,Stepan Botman*

Main category: cs.CV

TL;DR: MeFEm is a vision model using modified JEPA architecture for facial biometric and medical analysis, featuring axial stripe masking, circular loss weighting, and CLS token reassignment, achieving state-of-the-art performance on anthropometric tasks with less data.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient vision model for biometric and medical analysis from facial images that addresses domain bias in existing datasets and achieves strong performance with less training data.

Method: Modified Joint Embedding Predictive Architecture (JEPA) with three key innovations: 1) axial stripe masking strategy to focus learning on semantically relevant facial regions, 2) circular loss weighting scheme, and 3) probabilistic reassignment of CLS token for improved linear probing.

Result: MeFEm outperforms strong baselines (FaRL and Franca) on core anthropometric tasks despite using significantly less training data. Shows promising results on BMI estimation using a novel consolidated dataset that addresses domain bias.

Conclusion: MeFEm provides a strong baseline for facial biometric and medical analysis, with model weights publicly available for future research in this domain.

Abstract: We present MeFEm, a vision model based on a modified Joint Embedding Predictive Architecture (JEPA) for biometric and medical analysis from facial images. Key modifications include an axial stripe masking strategy to focus learning on semantically relevant regions, a circular loss weighting scheme, and the probabilistic reassignment of the CLS token for high quality linear probing. Trained on a consolidated dataset of curated images, MeFEm outperforms strong baselines like FaRL and Franca on core anthropometric tasks despite using significantly less data. It also shows promising results on Body Mass Index (BMI) estimation, evaluated on a novel, consolidated closed-source dataset that addresses the domain bias prevalent in existing data. Model weights are available at https://huggingface.co/boretsyury/MeFEm , offering a strong baseline for future work in this domain.

</details>


### [145] [Universal Image Immunization against Diffusion-based Image Editing via Semantic Injection](https://arxiv.org/abs/2602.14679)
*Chanhui Lee,Seunghyun Shin,Donggyu Choi,Hae-gon Jeon,Jeany Son*

Main category: cs.CV

TL;DR: First universal image immunization framework using adversarial perturbations to protect images from AI-driven semantic manipulation in diffusion models.


<details>
  <summary>Details</summary>
Motivation: Address ethical/legal risks of diffusion models (deepfakes, copyright violations) by developing scalable immunization against malicious editing, overcoming limitations of image-specific approaches.

Method: Generates single universal adversarial perturbation (UAP) that embeds semantic target into images while suppressing original content, misdirecting diffusion models during editing; operates data-free without training data.

Result: Outperforms baselines in UAP setting, matches image-specific methods under restricted perturbation budgets, shows strong black-box transferability across diffusion models.

Conclusion: Proposes practical, scalable universal immunization framework effective against AI-driven semantic manipulation, enhancing real-world applicability of image protection.

Abstract: Recent advances in diffusion models have enabled powerful image editing capabilities guided by natural language prompts, unlocking new creative possibilities. However, they introduce significant ethical and legal risks, such as deepfakes and unauthorized use of copyrighted visual content. To address these risks, image immunization has emerged as a promising defense against AI-driven semantic manipulation. Yet, most existing approaches rely on image-specific adversarial perturbations that require individual optimization for each image, thereby limiting scalability and practicality. In this paper, we propose the first universal image immunization framework that generates a single, broadly applicable adversarial perturbation specifically designed for diffusion-based editing pipelines. Inspired by universal adversarial perturbation (UAP) techniques used in targeted attacks, our method generates a UAP that embeds a semantic target into images to be protected. Simultaneously, it suppresses original content to effectively misdirect the model's attention during editing. As a result, our approach effectively blocks malicious editing attempts by overwriting the original semantic content in the image via the UAP. Moreover, our method operates effectively even in data-free settings without requiring access to training data or domain knowledge, further enhancing its practicality and broad applicability in real-world scenarios. Extensive experiments show that our method, as the first universal immunization approach, significantly outperforms several baselines in the UAP setting. In addition, despite the inherent difficulty of universal perturbations, our method also achieves performance on par with image-specific methods under a more restricted perturbation budget, while also exhibiting strong black-box transferability across different diffusion models.

</details>


### [146] [It's a Matter of Time: Three Lessons on Long-Term Motion for Perception](https://arxiv.org/abs/2602.14705)
*Willem Davison,Xinyue Hao,Laura Sevilla-Lara*

Main category: cs.CV

TL;DR: Long-term motion representations outperform image representations for understanding actions, objects, materials, and spatial information, generalize better in low-data settings, and offer better computational efficiency than standard video representations.


<details>
  <summary>Details</summary>
Motivation: While temporal information is essential for perception, the role of long-term motion information remains poorly understood compared to image information. The research aims to explore what can be learned from long-term motion information and its properties for visual learning.

Method: Leveraged recent advances in point-track estimation to learn temporal representations and conducted experiments on various perceptual tasks to analyze the properties and capabilities of long-term motion information.

Result: Three key findings: 1) Long-term motion representations contain rich information about actions, objects, materials, and spatial information, often outperforming image representations. 2) They generalize significantly better than image representations in low-data and zero-shot settings. 3) Their low dimensionality offers better computational efficiency (GFLOPs vs accuracy trade-off) than standard video representations, and combining them achieves higher performance than video alone.

Conclusion: Long-term motion information is a powerful and efficient representation for perception that generalizes well and contains rich semantic information. These insights should guide future model designs to better leverage long-term motion information for improved perceptual capabilities.

Abstract: Temporal information has long been considered to be essential for perception. While there is extensive research on the role of image information for perceptual tasks, the role of the temporal dimension remains less well understood: What can we learn about the world from long-term motion information? What properties does long-term motion information have for visual learning? We leverage recent success in point-track estimation, which offers an excellent opportunity to learn temporal representations and experiment on a variety of perceptual tasks. We draw 3 clear lessons: 1) Long-term motion representations contain information to understand actions, but also objects, materials, and spatial information, often even better than images. 2) Long-term motion representations generalize far better than image representations in low-data settings and in zero-shot tasks. 3) The very low dimensionality of motion information makes motion representations a better trade-off between GFLOPs and accuracy than standard video representations, and used together they achieve higher performance than video representations alone. We hope these insights will pave the way for the design of future models that leverage the power of long-term motion information for perception.

</details>


### [147] [Depth Completion as Parameter-Efficient Test-Time Adaptation](https://arxiv.org/abs/2602.14751)
*Bingxin Ke,Qunjie Zhou,Jiahui Huang,Xuanchi Ren,Tianchang Shen,Konrad Schindler,Laura Leal-Taixé,Shengyu Huang*

Main category: cs.CV

TL;DR: CAPA is a parameter-efficient test-time optimization framework that adapts pre-trained 3D foundation models for depth completion using sparse geometric cues, achieving SOTA results without retraining the backbone.


<details>
  <summary>Details</summary>
Motivation: Prior methods train task-specific encoders for auxiliary inputs which often overfit and generalize poorly. There's a need for a more efficient approach that can leverage pre-trained 3D foundation models while being adaptable to specific scenes at inference time.

Method: CAPA freezes the FM backbone and updates only a minimal set of parameters using Parameter-Efficient Fine-Tuning (LoRA or VPT). Gradients are calculated directly from sparse observations at inference time. For videos, it introduces sequence-level parameter sharing to jointly adapt all frames and exploit temporal correlations.

Result: CAPA achieves state-of-the-art results across diverse condition patterns on both indoor and outdoor datasets. It effectively grounds the foundation model's geometric prior in scene-specific measurements, correcting distortions and misplaced structures.

Conclusion: CAPA provides a model-agnostic, parameter-efficient framework for test-time adaptation of 3D foundation models for depth completion, offering improved generalization, robustness, and multi-frame consistency compared to prior methods.

Abstract: We introduce CAPA, a parameter-efficient test-time optimization framework that adapts pre-trained 3D foundation models (FMs) for depth completion, using sparse geometric cues. Unlike prior methods that train task-specific encoders for auxiliary inputs, which often overfit and generalize poorly, CAPA freezes the FM backbone. Instead, it updates only a minimal set of parameters using Parameter-Efficient Fine-Tuning (e.g. LoRA or VPT), guided by gradients calculated directly from the sparse observations available at inference time. This approach effectively grounds the foundation model's geometric prior in the scene-specific measurements, correcting distortions and misplaced structures. For videos, CAPA introduces sequence-level parameter sharing, jointly adapting all frames to exploit temporal correlations, improve robustness, and enforce multi-frame consistency. CAPA is model-agnostic, compatible with any ViT-based FM, and achieves state-of-the-art results across diverse condition patterns on both indoor and outdoor datasets. Project page: research.nvidia.com/labs/dvl/projects/capa.

</details>


### [148] [SAILS: Segment Anything with Incrementally Learned Semantics for Task-Invariant and Training-Free Continual Learning](https://arxiv.org/abs/2602.14767)
*Shishir Muralidhara,Didier Stricker,René Schuster*

Main category: cs.CV

TL;DR: SAILS is a training-free framework for Class-Incremental Semantic Segmentation that uses SAM for zero-shot region extraction and prototype-based semantic association, eliminating forgetting and computational costs.


<details>
  <summary>Details</summary>
Motivation: Continual learning faces challenges of repeated retraining, high computational costs, and catastrophic forgetting, limiting real-world applicability. Current approaches require iterative model updates that exacerbate these problems.

Method: SAILS decouples CISS into two stages: (1) Zero-shot region extraction using Segment Anything Model (SAM), and (2) Semantic association through prototypes in a fixed feature space with selective intra-class clustering to create multiple prototypes per class.

Result: SAILS typically surpasses training-based approaches on standard CISS datasets, especially in long/challenging sequences. It completely eliminates forgetting, maintains consistent performance, and exhibits positive backward transfer where new classes enhance previous class performance.

Conclusion: SAILS offers a training-free solution to continual learning challenges, providing superior performance without forgetting, computational costs, or retraining requirements, making continual learning more practical for real-world applications.

Abstract: Continual learning remains constrained by the need for repeated retraining, high computational costs, and the persistent challenge of forgetting. These factors significantly limit the applicability of continuous learning in real-world settings, as iterative model updates require significant computational resources and inherently exacerbate forgetting. We present SAILS -- Segment Anything with Incrementally Learned Semantics, a training-free framework for Class-Incremental Semantic Segmentation (CISS) that sidesteps these challenges entirely. SAILS leverages foundational models to decouple CISS into two stages: Zero-shot region extraction using Segment Anything Model (SAM), followed by semantic association through prototypes in a fixed feature space. SAILS incorporates selective intra-class clustering, resulting in multiple prototypes per class to better model intra-class variability. Our results demonstrate that, despite requiring no incremental training, SAILS typically surpasses the performance of existing training-based approaches on standard CISS datasets, particularly in long and challenging task sequences where forgetting tends to be most severe. By avoiding parameter updates, SAILS completely eliminates forgetting and maintains consistent, task-invariant performance. Furthermore, SAILS exhibits positive backward transfer, where the introduction of new classes can enhance performance on previous classes.

</details>


### [149] [GOT-JEPA: Generic Object Tracking with Model Adaptation and Occlusion Handling using Joint-Embedding Predictive Architecture](https://arxiv.org/abs/2602.14771)
*Shih-Fang Chen,Jun-Cheng Chen,I-Hong Jhuo,Yen-Yu Lin*

Main category: cs.CV

TL;DR: GOT-JEPA is a model-predictive pretraining framework that extends JEPA from image feature prediction to tracking model prediction, improving generalization and occlusion handling in object tracking. Combined with OccuSolver for detailed occlusion-pattern capture, the method enhances tracker robustness in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Current generic object trackers have limitations: they are often over-optimized for training targets (limiting robustness and generalization to unseen scenarios), and their occlusion reasoning remains coarse without detailed modeling of occlusion patterns. The human visual system, in contrast, adapts to changes and reasons about occlusion at fine granularity.

Method: 1) GOT-JEPA: A model-predictive pretraining framework where a teacher predictor generates pseudo-tracking models from clean current frames, and a student predictor learns to predict the same models from corrupted versions. This trains the predictor to produce reliable tracking models under occlusions and adverse conditions. 2) OccuSolver: Adapts a point-centric point tracker for object-aware visibility estimation and detailed occlusion-pattern capture. It iteratively refines visibility states using object priors from the tracker, producing higher-quality reference labels that improve subsequent predictions.

Result: Extensive evaluations on seven benchmarks demonstrate that the method effectively enhances tracker generalization and robustness in dynamic environments.

Conclusion: The proposed GOT-JEPA framework with OccuSolver addresses key limitations in current object tracking by improving generalization to unseen scenarios and providing detailed occlusion perception, resulting in more robust tracking performance comparable to human visual system capabilities.

Abstract: The human visual system tracks objects by integrating current observations with previously observed information, adapting to target and scene changes, and reasoning about occlusion at fine granularity. In contrast, recent generic object trackers are often optimized for training targets, which limits robustness and generalization in unseen scenarios, and their occlusion reasoning remains coarse, lacking detailed modeling of occlusion patterns. To address these limitations in generalization and occlusion perception, we propose GOT-JEPA, a model-predictive pretraining framework that extends JEPA from predicting image features to predicting tracking models. Given identical historical information, a teacher predictor generates pseudo-tracking models from a clean current frame, and a student predictor learns to predict the same pseudo-tracking models from a corrupted version of the current frame. This design provides stable pseudo supervision and explicitly trains the predictor to produce reliable tracking models under occlusions, distractors, and other adverse observations, improving generalization to dynamic environments. Building on GOT-JEPA, we further propose OccuSolver to enhance occlusion perception for object tracking. OccuSolver adapts a point-centric point tracker for object-aware visibility estimation and detailed occlusion-pattern capture. Conditioned on object priors iteratively generated by the tracker, OccuSolver incrementally refines visibility states, strengthens occlusion handling, and produces higher-quality reference labels that progressively improve subsequent model predictions. Extensive evaluations on seven benchmarks show that our method effectively enhances tracker generalization and robustness.

</details>


### [150] [VIPA: Visual Informative Part Attention for Referring Image Segmentation](https://arxiv.org/abs/2602.14788)
*Yubin Cho,Hyunwoo Yu,Kyeongbo Kong,Kyomin Sohn,Bongjoon Hyun,Suk-Ju Kang*

Main category: cs.CV

TL;DR: VIPA is a novel framework for Referring Image Segmentation that uses visual expressions to better align language descriptions with fine-grained image regions through informative part attention.


<details>
  <summary>Details</summary>
Motivation: Existing RIS methods need to more effectively exploit visual contexts for fine-grained segmentation. Current approaches that leverage vision information into language tokens still face challenges with high-variance cross-modal projection and semantic consistency in attention mechanisms.

Method: Proposes Visual Informative Part Attention (VIPA) framework with a Visual Expression Generator (VEG) module. VEG retrieves informative visual tokens using local-global linguistic context cues, refines them to reduce noise, and shares informative visual attributes to create visual expressions that capture semantic visual contexts.

Result: Extensive experiments show VIPA outperforms existing state-of-the-art methods on four public RIS benchmarks. The framework enables robust alignment with fine-grained regions of interest.

Conclusion: VIPA effectively leverages visual informative parts to reduce cross-modal projection variance and enhance semantic consistency, achieving superior performance in referring image segmentation by better capturing structural and semantic visual target information.

Abstract: Referring Image Segmentation (RIS) aims to segment a target object described by a natural language expression. Existing methods have evolved by leveraging the vision information into the language tokens. To more effectively exploit visual contexts for fine-grained segmentation, we propose a novel Visual Informative Part Attention (VIPA) framework for referring image segmentation. VIPA leverages the informative parts of visual contexts, called a visual expression, which can effectively provide the structural and semantic visual target information to the network. This design reduces high-variance cross-modal projection and enhances semantic consistency in an attention mechanism of the referring image segmentation. We also design a visual expression generator (VEG) module, which retrieves informative visual tokens via local-global linguistic context cues and refines the retrieved tokens for reducing noise information and sharing informative visual attributes. This module allows the visual expression to consider comprehensive contexts and capture semantic visual contexts of informative regions. In this way, our framework enables the network's attention to robustly align with the fine-grained regions of interest. Extensive experiments and visual analysis demonstrate the effectiveness of our approach. Our VIPA outperforms the existing state-of-the-art methods on four public RIS benchmarks.

</details>


### [151] [Debiasing Central Fixation Confounds Reveals a Peripheral "Sweet Spot" for Human-like Scanpaths in Hard-Attention Vision](https://arxiv.org/abs/2602.14834)
*Pengcheng Pan,Yonekura Shogo,Yasuo Kuniyosh*

Main category: cs.CV

TL;DR: Standard scanpath metrics for evaluating task-driven hard-attention models are strongly confounded by dataset center bias. A trivial center-fixation baseline performs surprisingly well on Gaze-CIFAR-10, making metrics optimistic. The paper proposes GCS, a center-debiased composite metric that reveals a "peripheral sweet spot" where models achieve genuine behavioral alignment beyond central tendency.


<details>
  <summary>Details</summary>
Motivation: Current evaluation of hard-attention models using scanpath metrics is problematic because these metrics are strongly confounded by dataset-specific center bias, especially on object-centric datasets. This makes it difficult to distinguish genuine behavioral alignment from mere central tendency, potentially leading to overly optimistic assessments of learned attention policies.

Method: 1) Show that a trivial center-fixation baseline achieves strong scanpath scores on Gaze-CIFAR-10, approaching learned policies. 2) Analyze a hard-attention classifier under constrained vision by sweeping foveal patch size and peripheral context. 3) Propose GCS (Gaze Consistency Score), a center-debiased composite metric augmented with movement similarity to address center bias.

Result: Reveals a "peripheral sweet spot": only a narrow range of sensory constraints yields scanpaths that are both above the center baseline after debiasing and temporally human-like in movement statistics. GCS uncovers a robust sweet spot at medium patch size with both foveal and peripheral vision, and highlights a "shortcut regime" when field-of-view becomes too large.

Conclusion: Standard scanpath metrics are insufficient for evaluating active perception on object-centric datasets due to center bias. The proposed GCS metric better separates genuine behavioral alignment from central tendency, revealing important insights about the relationship between sensory constraints and human-like scanpaths. This has implications for designing better gaze benchmarks and evaluating hard-attention models.

Abstract: Human eye movements in visual recognition reflect a balance between foveal sampling and peripheral context. Task-driven hard-attention models for vision are often evaluated by how well their scanpaths match human gaze. However, common scanpath metrics can be strongly confounded by dataset-specific center bias, especially on object-centric datasets. Using Gaze-CIFAR-10, we show that a trivial center-fixation baseline achieves surprisingly strong scanpath scores, approaching many learned policies. This makes standard metrics optimistic and blurs the distinction between genuine behavioral alignment and mere central tendency. We then analyze a hard-attention classifier under constrained vision by sweeping foveal patch size and peripheral context, revealing a peripheral sweet spot: only a narrow range of sensory constraints yields scanpaths that are simultaneously (i) above the center baseline after debiasing and (ii) temporally human-like in movement statistics. To address center bias, we propose GCS (Gaze Consistency Score), a center-debiased composite metric augmented with movement similarity. GCS uncovers a robust sweet spot at medium patch size with both foveal and peripheral vision, that is not obvious from raw scanpath metrics or accuracy alone, and also highlights a "shortcut regime" when the field-of-view becomes too large. We discuss implications for evaluating active perception on object-centric datasets and for designing gaze benchmarks that better separate behavioral alignment from center bias.

</details>


### [152] [Integrating Affordances and Attention models for Short-Term Object Interaction Anticipation](https://arxiv.org/abs/2602.14837)
*Lorenzo Mur Labadia,Ruben Martinez-Cantin,Jose J. Guerrero,Giovanni M. Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: STAformer models improve short-term object-interaction anticipation by integrating attention mechanisms with environment affordances and interaction hotspots for better prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Short-term object-interaction anticipation is crucial for wearable assistants and human-robot interaction to understand user goals and provide timely assistance. Current methods need improvement in predicting location, noun/verb categories, and time to contact from egocentric video.

Method: Proposes STAformer and STAformer++ architectures with attention mechanisms, frame-guided temporal pooling, dual image-video attention, and multiscale feature fusion. Introduces two novel modules: 1) environment affordance model as persistent memory of possible interactions, integrated via late fusion or adaptive learning, and 2) interaction hotspots prediction from hand and object trajectories.

Result: Significant improvements on Overall Top-5 mAP: up to +23 percentage points on Ego4D and +31 percentage points on curated EPIC-Kitchens STA labels.

Conclusion: The proposed attention-based architectures with affordance modeling and interaction hotspots substantially improve short-term object-interaction anticipation performance, with code and annotations released to encourage future research.

Abstract: Short Term object-interaction Anticipation consists in detecting the location of the next active objects, the noun and verb categories of the interaction, as well as the time to contact from the observation of egocentric video. This ability is fundamental for wearable assistants to understand user goals and provide timely assistance, or to enable human-robot interaction. In this work, we present a method to improve the performance of STA predictions. Our contributions are two-fold: 1 We propose STAformer and STAformer plus plus, two novel attention-based architectures integrating frame-guided temporal pooling, dual image-video attention, and multiscale feature fusion to support STA predictions from an image-input video pair; 2 We introduce two novel modules to ground STA predictions on human behavior by modeling affordances. First, we integrate an environment affordance model which acts as a persistent memory of interactions that can take place in a given physical scene. We explore how to integrate environment affordances via simple late fusion and with an approach which adaptively learns how to best fuse affordances with end-to-end predictions. Second, we predict interaction hotspots from the observation of hands and object trajectories, increasing confidence in STA predictions localized around the hotspot. Our results show significant improvements on Overall Top-5 mAP, with gain up to +23p.p on Ego4D and +31p.p on a novel set of curated EPIC-Kitchens STA labels. We released the code, annotations, and pre-extracted affordances on Ego4D and EPIC-Kitchens to encourage future research in this area.

</details>


### [153] [Multi-dimensional Persistent Sheaf Laplacians for Image Analysis](https://arxiv.org/abs/2602.14846)
*Xiang Xiang Wang,Guo-Wei Wei*

Main category: cs.CV

TL;DR: Multi-dimensional persistent sheaf Laplacian framework for image analysis that combines topological features across multiple reduced dimensions instead of selecting a single dimension.


<details>
  <summary>Details</summary>
Motivation: Address the sensitivity of dimensionality reduction techniques like PCA to the choice of reduced dimension, avoiding the need to select a single dimension or average across dimensions.

Method: Treat image samples as simplicial complexes, apply persistent sheaf Laplacians to extract multiscale localized topological spectral representations, then aggregate statistical summaries across scales and dimensions.

Result: More stable performance across wide range of reduced dimensions and consistent improvements over PCA-based baselines in moderate dimensional regimes on COIL20 and ETH80 datasets.

Conclusion: The MPSL framework effectively exploits complementary advantages of multiple reduced dimensions for robust image representation and classification.

Abstract: We propose a multi-dimensional persistent sheaf Laplacian (MPSL) framework on simplicial complexes for image analysis. The proposed method is motivated by the strong sensitivity of commonly used dimensionality reduction techniques, such as principal component analysis (PCA), to the choice of reduced dimension. Rather than selecting a single reduced dimension or averaging results across dimensions, we exploit complementary advantages of multiple reduced dimensions. At a given dimension, image samples are regarded as simplicial complexes, and persistent sheaf Laplacians are utilized to extract a multiscale localized topological spectral representation for individual image samples. Statistical summaries of the resulting spectra are then aggregated across scales and dimensions to form multiscale multi-dimensional image representations. We evaluate the proposed framework on the COIL20 and ETH80 image datasets using standard classification protocols. Experimental results show that the proposed method provides more stable performance across a wide range of reduced dimensions and achieves consistent improvements to PCA-based baselines in moderate dimensional regimes.

</details>


### [154] [CT-Bench: A Benchmark for Multimodal Lesion Understanding in Computed Tomography](https://arxiv.org/abs/2602.14879)
*Qingqing Zhu,Qiao Jin,Tejas S. Mathai,Yin Fang,Zhizheng Wang,Yifan Yang,Maame Sarfo-Gyamfi,Benjamin Hou,Ran Gu,Praveen T. S. Balamuralikrishna,Kenneth C. Wang,Ronald M. Summers,Zhiyong Lu*

Main category: cs.CV

TL;DR: CT-Bench is a new benchmark dataset for AI-based lesion analysis on CT scans, featuring 20,335 lesions with annotations and 2,850 QA pairs for evaluating multimodal models.


<details>
  <summary>Details</summary>
Motivation: Progress in AI for CT lesion analysis is limited by scarce publicly available datasets with lesion-level annotations, creating a need for comprehensive benchmarks to advance the field.

Method: Created CT-Bench with two components: 1) Lesion Image and Metadata Set (20,335 lesions from 7,795 CT studies with bounding boxes, descriptions, size info), and 2) multitask visual question answering benchmark (2,850 QA pairs covering lesion localization, description, size estimation, attribute categorization). Includes hard negative examples for real-world challenges.

Result: Evaluated multiple state-of-the-art multimodal models (vision-language and medical CLIP variants) against radiologist assessments. Fine-tuning models on the Lesion Image and Metadata Set yielded significant performance gains across both benchmark components.

Conclusion: CT-Bench serves as a comprehensive benchmark for lesion analysis, demonstrates clinical utility through performance improvements, and bridges the gap in publicly available CT datasets with lesion-level annotations.

Abstract: Artificial intelligence (AI) can automatically delineate lesions on computed tomography (CT) and generate radiology report content, yet progress is limited by the scarcity of publicly available CT datasets with lesion-level annotations. To bridge this gap, we introduce CT-Bench, a first-of-its-kind benchmark dataset comprising two components: a Lesion Image and Metadata Set containing 20,335 lesions from 7,795 CT studies with bounding boxes, descriptions, and size information, and a multitask visual question answering benchmark with 2,850 QA pairs covering lesion localization, description, size estimation, and attribute categorization. Hard negative examples are included to reflect real-world diagnostic challenges. We evaluate multiple state-of-the-art multimodal models, including vision-language and medical CLIP variants, by comparing their performance to radiologist assessments, demonstrating the value of CT-Bench as a comprehensive benchmark for lesion analysis. Moreover, fine-tuning models on the Lesion Image and Metadata Set yields significant performance gains across both components, underscoring the clinical utility of CT-Bench.

</details>


### [155] [Wrivinder: Towards Spatial Intelligence for Geo-locating Ground Images onto Satellite Imagery](https://arxiv.org/abs/2602.14929)
*Chandrakanth Gudavalli,Tajuddin Manhar Mohammed,Abhay Yadav,Ananth Vishnu Bhaskar,Hardik Prajapati,Cheng Peng,Rama Chellappa,Shivkumar Chandrasekaran,B. S. Manjunath*

Main category: cs.CV

TL;DR: Wrivinder is a zero-shot geometry-driven framework that aggregates multiple ground photos to reconstruct 3D scenes and align them with satellite imagery for accurate camera geo-localization, achieving sub-30m accuracy without paired supervision.


<details>
  <summary>Details</summary>
Motivation: Aligning ground-level imagery with satellite maps is crucial for mapping, navigation, and situational awareness, but remains challenging under large viewpoint gaps or when GPS is unreliable. Existing methods lack suitable benchmarks for systematic evaluation.

Method: Wrivinder combines SfM reconstruction, 3D Gaussian Splatting, semantic grounding, and monocular depth-based metric cues to produce stable zenith-view renderings that can be directly matched to satellite imagery. The framework also introduces MC-Sat, a curated dataset linking multi-view ground imagery with geo-registered satellite tiles across diverse outdoor environments.

Result: Wrivinder achieves sub-30m geolocation accuracy across both dense and large-area scenes in zero-shot experiments. The MC-Sat dataset provides the first comprehensive benchmark for geometry-centered cross-view alignment without paired supervision.

Conclusion: Wrivinder and MC-Sat establish a foundational baseline for geometry-based ground-to-satellite localization, demonstrating the promise of geometry-based aggregation for robust cross-view alignment without requiring supervised training data.

Abstract: Aligning ground-level imagery with geo-registered satellite maps is crucial for mapping, navigation, and situational awareness, yet remains challenging under large viewpoint gaps or when GPS is unreliable. We introduce Wrivinder, a zero-shot, geometry-driven framework that aggregates multiple ground photographs to reconstruct a consistent 3D scene and align it with overhead satellite imagery. Wrivinder combines SfM reconstruction, 3D Gaussian Splatting, semantic grounding, and monocular depth--based metric cues to produce a stable zenith-view rendering that can be directly matched to satellite context for metrically accurate camera geo-localization. To support systematic evaluation of this task, which lacks suitable benchmarks, we also release MC-Sat, a curated dataset linking multi-view ground imagery with geo-registered satellite tiles across diverse outdoor environments. Together, Wrivinder and MC-Sat provide a first comprehensive baseline and testbed for studying geometry-centered cross-view alignment without paired supervision. In zero-shot experiments, Wrivinder achieves sub-30\,m geolocation accuracy across both dense and large-area scenes, highlighting the promise of geometry-based aggregation for robust ground-to-satellite localization.

</details>


### [156] [AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories](https://arxiv.org/abs/2602.14941)
*Zun Wang,Han Lin,Jaehong Yoon,Jaemin Cho,Yue Zhang,Mohit Bansal*

Main category: cs.CV

TL;DR: AnchorWeave: A memory-augmented video generation framework that uses multiple clean local geometric memories instead of a single misaligned global 3D scene to improve long-term spatial consistency in camera-controllable video generation.


<details>
  <summary>Details</summary>
Motivation: Existing memory-based approaches for camera-controllable video generation reconstruct global 3D scenes from history, but pose and depth estimation errors cause cross-view misalignment. These inconsistencies accumulate into noisy geometry that contaminates conditioning signals and degrades generation quality.

Method: AnchorWeave replaces single misaligned global memory with multiple clean local geometric memories and learns to reconcile cross-view inconsistencies. It performs coverage-driven local memory retrieval aligned with target trajectory and integrates selected local memories through a multi-anchor weaving controller during generation.

Result: Extensive experiments demonstrate that AnchorWeave significantly improves long-term scene consistency while maintaining strong visual quality. Ablation and analysis studies validate the effectiveness of local geometric conditioning, multi-anchor control, and coverage-driven retrieval.

Conclusion: AnchorWeave addresses the challenge of maintaining spatial world consistency over long horizons by using multiple local geometric memories instead of a single global reconstruction, effectively handling cross-view misalignment issues that degrade video generation quality.

Abstract: Maintaining spatial world consistency over long horizons remains a central challenge for camera-controllable video generation. Existing memory-based approaches often condition generation on globally reconstructed 3D scenes by rendering anchor videos from the reconstructed geometry in the history. However, reconstructing a global 3D scene from multiple views inevitably introduces cross-view misalignment, as pose and depth estimation errors cause the same surfaces to be reconstructed at slightly different 3D locations across views. When fused, these inconsistencies accumulate into noisy geometry that contaminates the conditioning signals and degrades generation quality. We introduce AnchorWeave, a memory-augmented video generation framework that replaces a single misaligned global memory with multiple clean local geometric memories and learns to reconcile their cross-view inconsistencies. To this end, AnchorWeave performs coverage-driven local memory retrieval aligned with the target trajectory and integrates the selected local memories through a multi-anchor weaving controller during generation. Extensive experiments demonstrate that AnchorWeave significantly improves long-term scene consistency while maintaining strong visual quality, with ablation and analysis studies further validating the effectiveness of local geometric conditioning, multi-anchor control, and coverage-driven retrieval.

</details>


### [157] [ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery](https://arxiv.org/abs/2602.14989)
*Ayush Shrivastava,Kirtan Gangani,Laksh Jain,Mayank Goel,Nipun Batra*

Main category: cs.CV

TL;DR: ThermEval-B is a new benchmark for evaluating VLMs on thermal imagery, revealing current models' failure at temperature reasoning and need for dedicated thermal evaluation beyond RGB assumptions.


<details>
  <summary>Details</summary>
Motivation: VLMs perform well on RGB but fail on thermal images, which are critical for applications like nighttime surveillance, search/rescue, autonomous driving, and medical screening where visible light fails. Thermal images encode physical temperature rather than color/texture, requiring capabilities not evaluated by existing RGB benchmarks.

Method: Created ThermEval-B benchmark with ~55,000 thermal visual question answering pairs. Integrated public datasets with newly collected ThermEval-D dataset featuring dense per-pixel temperature maps with semantic body-part annotations across diverse indoor/outdoor environments. Evaluated 25 open-source and closed-source VLMs.

Result: Models consistently fail at temperature-grounded reasoning, degrade under colormap transformations, default to language priors or fixed responses, with only marginal gains from prompting or supervised fine-tuning.

Conclusion: Thermal understanding requires dedicated evaluation beyond RGB-centric assumptions. ThermEval serves as a benchmark to drive progress in thermal vision language modeling.

Abstract: Vision language models (VLMs) achieve strong performance on RGB imagery, but they do not generalize to thermal images. Thermal sensing plays a critical role in settings where visible light fails, including nighttime surveillance, search and rescue, autonomous driving, and medical screening. Unlike RGB imagery, thermal images encode physical temperature rather than color or texture, requiring perceptual and reasoning capabilities that existing RGB-centric benchmarks do not evaluate. We introduce ThermEval-B, a structured benchmark of approximately 55,000 thermal visual question answering pairs designed to assess the foundational primitives required for thermal vision language understanding. ThermEval-B integrates public datasets with our newly collected ThermEval-D, the first dataset to provide dense per-pixel temperature maps with semantic body-part annotations across diverse indoor and outdoor environments. Evaluating 25 open-source and closed-source VLMs, we find that models consistently fail at temperature-grounded reasoning, degrade under colormap transformations, and default to language priors or fixed responses, with only marginal gains from prompting or supervised fine-tuning. These results demonstrate that thermal understanding requires dedicated evaluation beyond RGB-centric assumptions, positioning ThermEval as a benchmark to drive progress in thermal vision language modeling.

</details>


### [158] [Image Generation with a Sphere Encoder](https://arxiv.org/abs/2602.15030)
*Kaiyu Yue,Menglin Jia,Ji Hou,Tom Goldstein*

Main category: cs.CV

TL;DR: Sphere Encoder is a single-pass generative framework that maps images to/from a spherical latent space, achieving competitive performance with diffusion models using fewer than 5 inference steps.


<details>
  <summary>Details</summary>
Motivation: To create a more efficient generative model that can produce high-quality images with minimal inference steps, addressing the computational cost limitations of many-step diffusion models.

Method: Learns an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to image space. Trained solely with image reconstruction losses. Generation involves decoding random points on the sphere, with optional few-step encoder/decoder loops for quality enhancement.

Result: Achieves performance competitive with state-of-the-art diffusion models across several datasets, but with a small fraction of the inference cost (fewer than 5 steps vs many-step diffusion).

Conclusion: Sphere Encoder provides an efficient alternative to diffusion models, offering competitive image generation quality with dramatically reduced inference complexity through its spherical latent space approach.

Abstract: We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space. Trained solely through image reconstruction losses, the model generates an image by simply decoding a random point on the sphere. Our architecture naturally supports conditional generation, and looping the encoder/decoder a few times can further enhance image quality. Across several datasets, the sphere encoder approach yields performance competitive with state of the art diffusions, but with a small fraction of the inference cost. Project page is available at https://sphere-encoder.github.io .

</details>


### [159] [EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing](https://arxiv.org/abs/2602.15031)
*Yehonathan Litman,Shikun Liu,Dario Seyb,Nicholas Milef,Yang Zhou,Carl Marshall,Shubham Tulsiani,Caleb Leak*

Main category: cs.CV

TL;DR: EditCtrl is an efficient video inpainting framework that focuses computation only on masked regions, achieving 10x efficiency gains while improving editing quality compared to full-attention methods.


<details>
  <summary>Details</summary>
Motivation: Current video editing methods inefficiently process full video context even for sparse, localized edits, creating computational bottlenecks despite quality improvements from pre-trained video foundation models.

Method: EditCtrl uses a local video context module operating only on masked tokens (cost proportional to edit size) plus a lightweight temporal global context embedder for video-wide consistency with minimal overhead.

Result: EditCtrl achieves 10x computational efficiency compared to state-of-the-art methods while improving editing quality, and enables new capabilities like multi-region editing with text prompts and autoregressive content propagation.

Conclusion: EditCtrl provides an efficient video inpainting control framework that focuses computation where needed, offering both efficiency gains and quality improvements while unlocking new video editing capabilities.

Abstract: High-fidelity generative video editing has seen significant quality improvements by leveraging pre-trained video foundation models. However, their computational cost is a major bottleneck, as they are often designed to inefficiently process the full video context regardless of the inpainting mask's size, even for sparse, localized edits. In this paper, we introduce EditCtrl, an efficient video inpainting control framework that focuses computation only where it is needed. Our approach features a novel local video context module that operates solely on masked tokens, yielding a computational cost proportional to the edit size. This local-first generation is then guided by a lightweight temporal global context embedder that ensures video-wide context consistency with minimal overhead. Not only is EditCtrl 10 times more compute efficient than state-of-the-art generative editing methods, it even improves editing quality compared to methods designed with full-attention. Finally, we showcase how EditCtrl unlocks new capabilities, including multi-region editing with text prompts and autoregressive content propagation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [160] [Agentic AI for Commercial Insurance Underwriting with Adversarial Self-Critique](https://arxiv.org/abs/2602.13213)
*Joyjit Roy,Samaresh Kumar Singh*

Main category: cs.AI

TL;DR: AI system for insurance underwriting uses adversarial self-critique to reduce errors while keeping humans in final decision loop.


<details>
  <summary>Details</summary>
Motivation: Commercial insurance underwriting is manual and labor-intensive. Current AI solutions lack comprehensive reasoning and safety mechanisms for regulated, high-stakes environments where full automation is impractical and human accountability is critical.

Method: Developed a decision-negative, human-in-the-loop agentic system with adversarial self-critique mechanism. A critic agent challenges the primary agent's conclusions before recommendations go to human reviewers. Also created a formal taxonomy of failure modes for decision-negative agents.

Result: Adversarial critique reduced AI hallucination rates from 11.3% to 3.8% and increased decision accuracy from 92% to 96% on 500 expert-validated underwriting cases. The framework maintains strict human authority over all binding decisions.

Conclusion: Adversarial self-critique supports safer AI deployment in regulated domains and provides a model for responsible integration where human oversight is indispensable, addressing critical AI safety gaps in regulated workflows.

Abstract: Commercial insurance underwriting is a labor-intensive process that requires manual review of extensive documentation to assess risk and determine policy pricing. While AI offers substantial efficiency improvements, existing solutions lack comprehensive reasoning capabilities and internal mechanisms to ensure reliability within regulated, high-stakes environments. Full automation remains impractical and inadvisable in scenarios where human judgment and accountability are critical. This study presents a decision-negative, human-in-the-loop agentic system that incorporates an adversarial self-critique mechanism as a bounded safety architecture for regulated underwriting workflows. Within this system, a critic agent challenges the primary agent's conclusions prior to submitting recommendations to human reviewers. This internal system of checks and balances addresses a critical gap in AI safety for regulated workflows. Additionally, the research develops a formal taxonomy of failure modes to characterize potential errors by decision-negative agents. This taxonomy provides a structured framework for risk identification and risk management in high-stakes applications. Experimental evaluation using 500 expert-validated underwriting cases demonstrates that the adversarial critique mechanism reduces AI hallucination rates from 11.3% to 3.8% and increases decision accuracy from 92% to 96%. At the same time, the framework enforces strict human authority over all binding decisions by design. These findings indicate that adversarial self-critique supports safer AI deployment in regulated domains and offers a model for responsible integration where human oversight is indispensable.

</details>


### [161] [BotzoneBench: Scalable LLM Evaluation via Graded AI Anchors](https://arxiv.org/abs/2602.13214)
*Lingfeng Li,Yunlong Lu,Yuefei Zhang,Jingyu Yao,Yixin Zhu,KeYuan Cheng,Yongyi Wang,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: BotzoneBench: A scalable framework for evaluating LLM strategic reasoning using fixed hierarchies of skill-calibrated game AI instead of volatile LLM-vs-LLM tournaments.


<details>
  <summary>Details</summary>
Motivation: Existing LLM benchmarks fail to capture dynamic strategic abilities needed for interactive environments. Current game-based evaluations using LLM-vs-LLM tournaments are computationally expensive (quadratic costs), produce transient rankings dependent on model pools, and lack stable performance anchors for longitudinal tracking.

Method: Anchored evaluation using fixed hierarchies of skill-calibrated game AI on the Botzone platform. Evaluates LLMs across eight diverse games (deterministic perfect-information board games to stochastic imperfect-information card games) with 177,047 state-action pairs from five flagship models.

Result: Reveals significant performance disparities and distinct strategic behaviors among models. Top-performing models achieve proficiency comparable to mid-to-high-tier specialized game AI in multiple domains. Enables linear-time absolute skill measurement with stable cross-temporal interpretability.

Conclusion: This anchored evaluation paradigm generalizes beyond games to any domain with well-defined skill hierarchies, establishing a scalable and reusable framework for assessing interactive AI capabilities.

Abstract: Large Language Models (LLMs) are increasingly deployed in interactive environments requiring strategic decision-making, yet systematic evaluation of these capabilities remains challenging. Existing benchmarks for LLMs primarily assess static reasoning through isolated tasks and fail to capture dynamic strategic abilities. Recent game-based evaluations employ LLM-vs-LLM tournaments that produce relative rankings dependent on transient model pools, incurring quadratic computational costs and lacking stable performance anchors for longitudinal tracking. The central challenge is establishing a scalable evaluation framework that measures LLM strategic reasoning against consistent, interpretable standards rather than volatile peer models. Here we show that anchoring LLM evaluation to fixed hierarchies of skill-calibrated game Artificial Intelligence (AI) enables linear-time absolute skill measurement with stable cross-temporal interpretability. Built on the Botzone platform's established competitive infrastructure, our BotzoneBench evaluates LLMs across eight diverse games spanning deterministic perfect-information board games to stochastic imperfect-information card games. Through systematic assessment of 177,047 state-action pairs from five flagship models, we reveal significant performance disparities and identify distinct strategic behaviors, with top-performing models achieving proficiency comparable to mid-to-high-tier specialized game AI in multiple domains. This anchored evaluation paradigm generalizes beyond games to any domain with well-defined skill hierarchies, establishing a scalable and reusable framework for assessing interactive AI capabilities.

</details>


### [162] [When to Think Fast and Slow? AMOR: Entropy-Based Metacognitive Gate for Dynamic SSM-Attention Switching](https://arxiv.org/abs/2602.13215)
*Haoran Zheng*

Main category: cs.AI

TL;DR: AMOR is a hybrid architecture combining SSMs with sparse attention that activates only when the SSM is uncertain, achieving efficient long-range retrieval with adaptive computation.


<details>
  <summary>Details</summary>
Motivation: Transformers use uniform computation for all positions regardless of difficulty, while SSMs are efficient but struggle with precise long-range information retrieval. Inspired by dual-process cognition theories, the authors aim to create an adaptive system that only uses expensive attention when needed.

Method: AMOR uses SSM as backbone with Ghost KV (projecting keys/values from SSM hidden states). It dynamically engages sparse attention only when SSM prediction entropy indicates "uncertainty," reusing SSM's O(n) computation instead of requiring O(n²) attention at every layer.

Result: On synthetic retrieval tasks, AMOR outperforms SSM-only and transformer-only baselines, achieving perfect retrieval accuracy while using attention on only 22% of positions. Prediction entropy reliably signals retrieval need with 1.09 nats gap between retrieval and local positions.

Conclusion: AMOR provides an efficient, interpretable hybrid architecture that adaptively engages attention based on information-theoretic uncertainty, combining SSM efficiency with transformer precision when needed.

Abstract: Transformers allocate uniform computation to every position, regardless of difficulty. State Space Models (SSMs) offer efficient alternatives but struggle with precise information retrieval over a long horizon. Inspired by dual-process theories of cognition (Kahneman, 2011), we propose AMOR (Adaptive Metacognitive Output Router), a hybrid architecture that dynamically engages sparse attention only when an SSM backbone is "uncertain"--as measured by prediction entropy. Compared to standard transformers, AMOR gains efficiency by projecting keys and values from SSM hidden states (Ghost KV), reusing the SSM's O(n) computation rather than requiring O(n^2) attention at every layer. On small-scale synthetic retrieval tasks, AMOR outperforms both SSM-only and transformer-only baselines, achieving perfect retrieval accuracy while engaging attention on only 22% of positions. We validate that prediction entropy reliably signals retrieval need, with a gap of 1.09 nats (nearly half the entropy range) between retrieval and local positions. Additionally, our approach provides interpretable adaptive computation, where routing decisions can be understood in information-theoretic terms.

</details>


### [163] [VeRA: Verified Reasoning Data Augmentation at Scale](https://arxiv.org/abs/2602.13217)
*Zerui Cheng,Jiashuo Liu,Chunjie Wu,Jianzhu Yao,Pramod Viswanath,Ge Zhang,Wenhao Huang*

Main category: cs.AI

TL;DR: VeRA is a framework that converts benchmark problems into executable specifications to generate unlimited verified variants, addressing static evaluation issues through automated data augmentation.


<details>
  <summary>Details</summary>
Motivation: Current evaluation schemes are static, allowing memorization and format exploitation, which prevents genuine measurement of AI progress. Robust evaluation needs to be built by construction rather than detected post-hoc.

Method: VeRA converts benchmark problems into executable specifications with three components: (1) natural language template with placeholder slots, (2) coherent generator that samples valid configurations, and (3) deterministic verifier that validates parameters and calculates correct answers. It operates in two modes: VeRA-E (equivalent variants) and VeRA-H (hardened variants).

Result: Evaluation of 16 frontier models shows: (1) VeRA-E improves evaluation quality and reveals contamination patterns, (2) VeRA-H enables human-free generation of hard tasks with reliable labels, and (3) VeRA establishes verified benchmarks as a general paradigm.

Conclusion: VeRA reconceptualizes benchmarks from static objects to executable specifications that generate fresh, verified instances on demand, enabling evaluation to scale indefinitely without sacrificing label integrity in any verifiable domain.

Abstract: The main issue with most evaluation schemes today is their "static" nature: the same problems are reused repeatedly, allowing for memorization, format exploitation, and eventual saturation. To measure genuine AI progress, we need evaluation that is robust by construction, not by post-hoc detection. In response, we propose VeRA (Verified Reasoning Data Augmentation), a framework that converts benchmark problems into executable specifications, comprising (i) a natural language template with placeholder slots, (ii) a coherent generator that samples valid configurations, and (iii) a deterministic verifier that validates parameters and calculates the corresponding correct answers for each configuration. From a single seed problem, VeRA automatically creates unlimited verified variants with reliable labels at near-zero marginal cost without human involvement.
  VeRA operates in two complementary modes. VeRA-E (equivalent) rewrites problems while keeping the underlying logic intact, useful for detecting memorization versus genuine reasoning. VeRA-H (hardened) systematically increases complexity while remaining verifiable, enabling reliable creation and labelling of fresh difficult tasks at the boundary of intelligence. Evaluating 16 frontier models with VeRA, we find: (i) VeRA-E improves evaluation quality and reveals contamination patterns. (ii) VeRA-H enables human-free generation of hard tasks with reliable labels. (iii) VeRA establishes verified benchmarks as a general paradigm. VeRA reconceptualizes benchmarks from static objects used until exhausted, to executable specifications generating fresh, verified instances on demand, enhancing robustness and cost-effectiveness for evaluation.
  With VeRA, we envision that evaluation in any verifiable domain can scale indefinitely without sacrificing label integrity. To stimulate future research, we have open-sourced all code and datasets.

</details>


### [164] [Scaling the Scaling Logic: Agentic Meta-Synthesis of Logic Reasoning](https://arxiv.org/abs/2602.13218)
*Bowen Liu,Zhi Wu,Runquan Xie,Zhanhui Kang,Jia Li*

Main category: cs.AI

TL;DR: SSLogic is an agentic meta-synthesis framework that scales verifiable training signals for RLVR by iteratively synthesizing and repairing executable Generator-Validator program pairs, enabling continuous evolution of task families with controllable difficulty.


<details>
  <summary>Details</summary>
Motivation: Scaling verifiable training signals is a key bottleneck for Reinforcement Learning from Verifiable Rewards (RLVR). Logical reasoning provides formal constraints and programmatically checkable answers, but prior synthesis pipelines are limited to expert-written code or fixed templates, restricting growth to instance-level perturbations rather than task-family level scaling.

Method: SSLogic uses an agentic meta-synthesis framework with a closed Generate-Validate-Repair loop. It iteratively synthesizes and repairs executable Generator-Validator program pairs. A Multi-Gate Validation Protocol combines multi-strategy consistency checks with Adversarial Blind Review, where independent agents solve instances by writing and executing code to filter ambiguous or ill-posed tasks.

Result: Starting from 400 seed families, two evolution rounds expanded to 953 families and 21,389 verifiable instances (from 5,718). Training on SSLogic-evolved data yields consistent gains: SynLogic +5.2, BBEH +1.4, AIME25 +3.0, and Brumo25 +3.7 over seed baseline at matched training steps.

Conclusion: SSLogic successfully scales verifiable training signals at the task-family level through agentic meta-synthesis, enabling continuous evolution of logical reasoning tasks with controllable difficulty and demonstrating improved performance across multiple benchmarks.

Abstract: Scaling verifiable training signals remains a key bottleneck for Reinforcement Learning from Verifiable Rewards (RLVR). Logical reasoning is a natural substrate: constraints are formal and answers are programmatically checkable. However, prior synthesis pipelines either depend on expert-written code or operate within fixed templates/skeletons, which limits growth largely to instance-level perturbations. We propose SSLogic, an agentic meta-synthesis framework that scales at the task-family level by iteratively synthesizing and repairing executable Generator--Validator program pairs in a closed Generate--Validate--Repair loop, enabling continuous family evolution with controllable difficulty. To ensure reliability, we introduce a Multi-Gate Validation Protocol that combines multi-strategy consistency checks with Adversarial Blind Review, where independent agents must solve instances by writing and executing code to filter ambiguous or ill-posed tasks. Starting from 400 seed families, two evolution rounds expand to 953 families and 21,389 verifiable instances (from 5,718). Training on SSLogic-evolved data yields consistent gains over the seed baseline at matched training steps, improving SynLogic by +5.2, BBEH by +1.4, AIME25 by +3.0, and Brumo25 by +3.7.

</details>


### [165] [A Geometric Taxonomy of Hallucinations in LLMs](https://arxiv.org/abs/2602.13224)
*Javier Marín*

Main category: cs.AI

TL;DR: The paper proposes a taxonomy of LLM hallucinations with three types, showing that embedding-based detection works for unfaithfulness and confabulation but fails for factual errors due to fundamental limitations of embedding representations.


<details>
  <summary>Details</summary>
Motivation: Current understanding of "hallucination" in LLMs conflates distinct phenomena with different underlying mechanisms. The authors aim to disentangle these phenomena and understand their geometric signatures in embedding space to clarify what can and cannot be detected using embedding-based methods.

Method: Proposed a taxonomy of three hallucination types: unfaithfulness, confabulation, and factual error. Analyzed geometric signatures in embedding space using standard benchmarks and human-crafted confabulations. Measured detection performance via AUROC within and across domains, and examined cosine similarity between discriminative directions.

Result: Detection works well for Types I and II (AUROC 0.76-0.99 within domains, 0.96 for human-crafted confabulations) but fails for Type III (0.478 AUROC, chance level). Discriminative directions are orthogonal between domains for LLM-generated hallucinations but consistent for human-crafted ones. This reveals that embeddings encode distributional patterns, not truth values.

Conclusion: Embedding-based detection is effective for unfaithfulness and confabulation but fundamentally limited for factual errors. Type III hallucinations require external verification mechanisms since embeddings capture contextual patterns rather than correspondence to external reality. The taxonomy clarifies the scope of different detection approaches.

Abstract: The term "hallucination" in large language models conflates distinct phenomena with different geometric signatures in embedding space. We propose a taxonomy identifying three types: unfaithfulness (failure to engage with provided context), confabulation (invention of semantically foreign content), and factual error (incorrect claims within correct conceptual frames). We observe a striking asymmetry. On standard benchmarks where hallucinations are LLM-generated, detection is domain-local: AUROC 0.76-0.99 within domains, but 0.50 (chance level) across domains. Discriminative directions are approximately orthogonal between domains (mean cosine similarity -0.07). On human-crafted confabulations - invented institutions, redefined terminology, fabricated mechanisms - a single global direction achieves 0.96 AUROC with 3.8% cross-domain degradation. We interpret this divergence as follows: benchmarks capture generation artifacts (stylistic signatures of prompted fabrication), while human-crafted confabulations capture genuine topical drift. The geometric structure differs because the underlying phenomena differ. Type III errors show 0.478 AUROC - indistinguishable from chance. This reflects a theoretical constraint: embeddings encode distributional co-occurrence, not correspondence to external reality. Statements with identical contextual patterns occupy similar embedding regions regardless of truth value. The contribution is a geometric taxonomy clarifying the scope of embedding-based detection: Types I and II are detectable; Type III requires external verification mechanisms.

</details>


### [166] [Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection](https://arxiv.org/abs/2602.13226)
*Xuecong Li,Xiaohong Li,Qiang Hu,Yao Zhang,Junjie Wang*

Main category: cs.AI

TL;DR: VaryBalance is a simple yet effective LLM-generated text detection method that outperforms state-of-the-art detectors by up to 34.3% in AUROC.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-generated text detectors have limitations: they rely on impractical white-box assumptions or only use text-level features, resulting in imprecise detection capabilities.

Method: VaryBalance leverages the observation that human texts have greater difference from their LLM-rewritten versions compared to LLM-generated texts. It quantifies this difference using mean standard deviation to distinguish between human and LLM-generated texts.

Result: VaryBalance outperforms state-of-the-art detectors (Binoculars) by up to 34.3% in AUROC, and maintains robustness across multiple generating models and languages.

Conclusion: VaryBalance provides a simple, effective, and practical solution for LLM-generated text detection that addresses limitations of existing methods while achieving superior performance.

Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and practical LLM-generated text detection method, VaryBalance. The core of VaryBalance is that, compared to LLM-generated texts, there is a greater difference between human texts and their rewritten version via LLMs. Leveraging this observation, VaryBalance quantifies this through mean standard deviation and distinguishes human texts and LLM-generated texts. Comprehensive experiments demonstrated that VaryBalance outperforms the state-of-the-art detectors, i.e., Binoculars, by up to 34.3\% in terms of AUROC, and maintains robustness against multiple generating models and languages.

</details>


### [167] [Intelligence as Trajectory-Dominant Pareto Optimization](https://arxiv.org/abs/2602.13230)
*Truong Xuan Khanh,Truong Quynh Hoa*

Main category: cs.AI

TL;DR: The paper introduces Trajectory-Dominant Pareto Optimization to explain intelligence stagnation as Pareto traps in trajectory space, independent of learning progress or model scale.


<details>
  <summary>Details</summary>
Motivation: Despite advances in AI, many systems show stagnation in long-horizon adaptability despite optimization. The authors argue this isn't due to insufficient learning, data, or model capacity, but from deeper structural properties of how intelligence is optimized over time.

Method: Formulate intelligence as trajectory-level phenomenon with multi-objective trade-offs. Introduce Trajectory-Dominant Pareto Optimization (path-wise generalization of classical Pareto optimality). Define Trap Escape Difficulty Index (TEDI) to measure escape constraints. Develop formal taxonomy of Pareto traps and illustrate with minimal agent-environment model.

Result: Show that dynamic intelligence ceilings arise as inevitable geometric consequences of trajectory-level dominance, independent of learning progress or architectural scale. Demonstrate trajectory-level divergence using minimal agent-environment model.

Conclusion: Shifts focus from terminal performance to optimization geometry, providing principled framework for diagnosing and overcoming long-horizon developmental constraints in adaptive systems.

Abstract: Despite recent advances in artificial intelligence, many systems exhibit stagnation in long-horizon adaptability despite continued performance optimization. This work argues that such limitations do not primarily arise from insufficient learning, data, or model capacity, but from a deeper structural property of how intelligence is optimized over time. We formulate intelligence as a trajectory-level phenomenon governed by multi-objective trade-offs, and introduce Trajectory-Dominant Pareto Optimization, a path-wise generalization of classical Pareto optimality in which dominance is defined over full trajectories. Within this framework, Pareto traps emerge as locally non-dominated regions of trajectory space that nevertheless restrict access to globally superior developmental paths under conservative local optimization. To characterize the rigidity of such constraints, we define the Trap Escape Difficulty Index (TEDI), a composite geometric measure capturing escape distance, structural constraints, and behavioral inertia. We show that dynamic intelligence ceilings arise as inevitable geometric consequences of trajectory-level dominance, independent of learning progress or architectural scale. We further introduce a formal taxonomy of Pareto traps and illustrate the resulting trajectory-level divergence using a minimal agent-environment model. Together, these results shift the locus of intelligence from terminal performance to optimization geometry, providing a principled framework for diagnosing and overcoming long-horizon developmental constraints in adaptive systems.

</details>


### [168] [PlotChain: Deterministic Checkpointed Evaluation of Multimodal LLMs on Engineering Plot Reading](https://arxiv.org/abs/2602.13232)
*Mayank Ravishankara*

Main category: cs.AI

TL;DR: PlotChain is a deterministic benchmark for evaluating MLLMs on reading engineering plots, featuring 450 plots across 15 families with exact ground truth and checkpoint-based diagnostics for failure analysis.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal benchmarks focus on OCR extraction or free-form captioning, but lack systematic evaluation of MLLMs' ability to recover quantitative values from classic engineering plots like Bode diagrams, stress-strain curves, and pump curves.

Method: Created a generator-based benchmark with 15 plot families (30 plots each) where every plot is rendered from known parameters with exact ground truth. Introduced checkpoint-based diagnostics with intermediate 'cp_' fields to isolate sub-skills. Evaluated 4 MLLMs using deterministic protocol (temperature=0, JSON-only numeric output) with per-field tolerance scoring.

Result: Top models achieved: Gemini 2.5 Pro (80.42%), GPT-4.1 (79.84%), Claude Sonnet 4.5 (78.21%), GPT-4o (61.59%). Frequency-domain tasks remain challenging with bandpass response ≤23% and FFT spectrum difficulties.

Conclusion: PlotChain provides a reproducible benchmark for evaluating MLLMs on engineering plot reading, revealing strengths in many plot families but persistent weaknesses in frequency-domain analysis. The released generator and scoring tools enable standardized evaluation and future improvements.

Abstract: We present PlotChain, a deterministic, generator-based benchmark for evaluating multimodal large language models (MLLMs) on engineering plot reading-recovering quantitative values from classic plots (e.g., Bode/FFT, step response, stress-strain, pump curves) rather than OCR-only extraction or free-form captioning. PlotChain contains 15 plot families with 450 rendered plots (30 per family), where every item is produced from known parameters and paired with exact ground truth computed directly from the generating process. A central contribution is checkpoint-based diagnostic evaluation: in addition to final targets, each item includes intermediate 'cp_' fields that isolate sub-skills (e.g., reading cutoff frequency or peak magnitude) and enable failure localization within a plot family. We evaluate four state-of-the-art MLLMs under a standardized, deterministic protocol (temperature = 0 and a strict JSON-only numeric output schema) and score predictions using per-field tolerances designed to reflect human plot-reading precision. Under the 'plotread' tolerance policy, the top models achieve 80.42% (Gemini 2.5 Pro), 79.84% (GPT-4.1), and 78.21% (Claude Sonnet 4.5) overall field-level pass rates, while GPT-4o trails at 61.59%. Despite strong performance on many families, frequency-domain tasks remain brittle: bandpass response stays low (<= 23%), and FFT spectrum remains challenging. We release the generator, dataset, raw model outputs, scoring code, and manifests with checksums to support fully reproducible runs and retrospective rescoring under alternative tolerance policies.

</details>


### [169] [Stay in Character, Stay Safe: Dual-Cycle Adversarial Self-Evolution for Safety Role-Playing Agents](https://arxiv.org/abs/2602.13234)
*Mingyang Liao,Yichen Wan,shuchen wu,Chenxi Miao,Xin Shen,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: Dual-Cycle Adversarial Self-Evolution framework improves role-playing LLM safety without training by using attacker-defender cycles to build hierarchical safety knowledge.


<details>
  <summary>Details</summary>
Motivation: Current role-playing LLMs face a trade-off: better persona fidelity increases vulnerability to jailbreak attacks. Training-based solutions are costly, degrade performance, and don't work with closed-weight LLMs.

Method: Two coupled cycles: Persona-Targeted Attacker Cycle generates stronger jailbreak prompts, while Role-Playing Defender Cycle builds hierarchical knowledge base (global safety rules, persona constraints, safe exemplars). At inference, retrieves and composes this knowledge to guide safe, faithful responses.

Result: Extensive experiments show consistent improvements over baselines in both role fidelity and jailbreak resistance, with robust generalization to unseen personas and attack prompts across multiple proprietary LLMs.

Conclusion: The training-free Dual-Cycle framework effectively balances persona fidelity and safety without costly retraining, offering a practical solution for evolving role-playing systems with closed-weight LLMs.

Abstract: LLM-based role-playing has rapidly improved in fidelity, yet stronger adherence to persona constraints commonly increases vulnerability to jailbreak attacks, especially for risky or negative personas. Most prior work mitigates this issue with training-time solutions (e.g., data curation or alignment-oriented regularization). However, these approaches are costly to maintain as personas and attack strategies evolve, can degrade in-character behavior, and are typically infeasible for frontier closed-weight LLMs. We propose a training-free Dual-Cycle Adversarial Self-Evolution framework with two coupled cycles. A Persona-Targeted Attacker Cycle synthesizes progressively stronger jailbreak prompts, while a Role-Playing Defender Cycle distills observed failures into a hierarchical knowledge base of (i) global safety rules, (ii) persona-grounded constraints, and (iii) safe in-character exemplars. At inference time, the Defender retrieves and composes structured knowledge from this hierarchy to guide generation, producing responses that remain faithful to the target persona while satisfying safety constraints. Extensive experiments across multiple proprietary LLMs show consistent gains over strong baselines on both role fidelity and jailbreak resistance, and robust generalization to unseen personas and attack prompts.

</details>


### [170] [Lang2Act: Fine-Grained Visual Reasoning through Self-Emergent Linguistic Toolchains](https://arxiv.org/abs/2602.13235)
*Yuqi Xiong,Chunyi Peng,Zhipeng Xu,Zhenghao Liu,Zulong Chen,Yukun Yan,Shuo Wang,Yu Gu,Ge Yu*

Main category: cs.AI

TL;DR: Lang2Act enables fine-grained visual perception and reasoning for VLMs through self-emergent linguistic toolchains instead of rigid external tools, using a two-stage RL training framework.


<details>
  <summary>Details</summary>
Motivation: Existing VRAG frameworks rely on rigid, pre-defined external tools that separate visual perception from reasoning, causing unnecessary loss of visual information when operations like cropping are applied.

Method: Lang2Act uses self-emergent linguistic toolchains where VLMs collect actions as linguistic tools. A two-stage RL framework: first stage optimizes VLMs to self-explore high-quality actions for a reusable toolbox; second stage optimizes VLMs to effectively exploit these tools for downstream reasoning.

Result: Lang2Act substantially enhances visual perception capabilities of VLMs, achieving performance improvements of over 4%.

Conclusion: The proposed approach effectively addresses limitations of existing VRAG frameworks by enabling fine-grained visual perception through self-emergent linguistic tools rather than rigid external engines.

Abstract: Visual Retrieval-Augmented Generation (VRAG) enhances Vision-Language Models (VLMs) by incorporating external visual documents to address a given query. Existing VRAG frameworks usually depend on rigid, pre-defined external tools to extend the perceptual capabilities of VLMs, typically by explicitly separating visual perception from subsequent reasoning processes. However, this decoupled design can lead to unnecessary loss of visual information, particularly when image-based operations such as cropping are applied. In this paper, we propose Lang2Act, which enables fine-grained visual perception and reasoning through self-emergent linguistic toolchains. Rather than invoking fixed external engines, Lang2Act collects self-emergent actions as linguistic tools and leverages them to enhance the visual perception capabilities of VLMs. To support this mechanism, we design a two-stage Reinforcement Learning (RL)-based training framework. Specifically, the first stage optimizes VLMs to self-explore high-quality actions for constructing a reusable linguistic toolbox, and the second stage further optimizes VLMs to exploit these linguistic tools for downstream reasoning effectively. Experimental results demonstrate the effectiveness of Lang2Act in substantially enhancing the visual perception capabilities of VLMs, achieving performance improvements of over 4%. All code and data are available at https://github.com/NEUIR/Lang2Act.

</details>


### [171] [NL2LOGIC: AST-Guided Translation of Natural Language into First-Order Logic with Large Language Models](https://arxiv.org/abs/2602.13237)
*Rizky Ramadhana Putra,Raihan Sultan Pasha Basuki,Yutong Cheng,Peng Gao*

Main category: cs.AI

TL;DR: NL2LOGIC is a framework that improves natural language to first-order logic translation using abstract syntax trees as intermediate representation, achieving near-perfect syntactic accuracy and significant semantic improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for translating natural language to first-order logic using LLMs suffer from fragile syntax control (weak global grammar constraints) and low semantic faithfulness (insufficient clause-level understanding), limiting their reliability in critical domains like law and governance.

Method: NL2LOGIC introduces an abstract syntax tree as intermediate representation, combining a recursive LLM-based semantic parser with an AST-guided generator that deterministically produces solver-ready logic code.

Result: Achieves 99% syntactic accuracy and improves semantic correctness by up to 30% over state-of-the-art baselines on FOLIO, LogicNLI, and ProofWriter benchmarks. Integration with Logic-LM yields near-perfect executability and improves downstream reasoning accuracy by 31%.

Conclusion: NL2LOGIC provides a robust framework for natural language to first-order logic translation that addresses both syntactic and semantic limitations of existing approaches, enabling more reliable automated reasoning in critical domains.

Abstract: Automated reasoning is critical in domains such as law and governance, where verifying claims against facts in documents requires both accuracy and interpretability. Recent work adopts structured reasoning pipelines that translate natural language into first-order logic and delegate inference to automated solvers. With the rise of large language models, approaches such as GCD and CODE4LOGIC leverage their reasoning and code generation capabilities to improve logic parsing. However, these methods suffer from fragile syntax control due to weak enforcement of global grammar constraints and low semantic faithfulness caused by insufficient clause-level semantic understanding. We propose NL2LOGIC, a first-order logic translation framework that introduces an abstract syntax tree as an intermediate representation. NL2LOGIC combines a recursive large language model based semantic parser with an abstract syntax tree guided generator that deterministically produces solver-ready logic code. Experiments on the FOLIO, LogicNLI, and ProofWriter benchmarks show that NL2LOGIC achieves 99 percent syntactic accuracy and improves semantic correctness by up to 30 percent over state-of-the-art baselines. Furthermore, integrating NL2LOGIC into Logic-LM yields near-perfect executability and improves downstream reasoning accuracy by 31 percent compared to Logic-LM's original few-shot unconstrained translation module.

</details>


### [172] [AST-PAC: AST-guided Membership Inference for Code](https://arxiv.org/abs/2602.13240)
*Roham Koohestani,Ali Al-Kaswan,Jonathan Katzy,Maliheh Izadi*

Main category: cs.AI

TL;DR: The paper explores membership inference attacks for auditing code LLMs, finds PAC outperforms baseline but degrades on large/complex code, and introduces AST-PAC using AST-based perturbations for better syntax-aware auditing.


<details>
  <summary>Details</summary>
Motivation: Code LLMs are trained on restrictively licensed source code, creating urgent data governance and copyright challenges. Membership Inference Attacks can serve as auditing mechanisms to detect unauthorized data usage, but existing methods like PAC are underexplored in the code domain and have limitations with code syntax.

Method: The paper conducts an exploratory study evaluating MIA methods (Loss Attack baseline and Polarized Augment Calibration) on 3B-7B parameter code models. To address PAC's limitations with code syntax, they introduce AST-PAC, a domain-specific adaptation using Abstract Syntax Tree-based perturbations to generate syntactically valid calibration samples.

Result: PAC generally outperforms the Loss baseline but degrades on larger, complex files due to augmentation strategies that disregard code syntax. AST-PAC improves performance as syntactic size grows (where PAC degrades), but under-mutates small files and underperforms on alphanumeric-rich code.

Conclusion: The findings motivate future work on syntax-aware and size-adaptive calibration as prerequisites for reliable provenance auditing of code language models. Domain-specific adaptations like AST-PAC show promise but need refinement for comprehensive code auditing.

Abstract: Code Large Language Models are frequently trained on massive datasets containing restrictively licensed source code. This creates urgent data governance and copyright challenges. Membership Inference Attacks (MIAs) can serve as an auditing mechanism to detect unauthorized data usage in models. While attacks like the Loss Attack provide a baseline, more involved methods like Polarized Augment Calibration (PAC) remain underexplored in the code domain. This paper presents an exploratory study evaluating these methods on 3B--7B parameter code models. We find that while PAC generally outperforms the Loss baseline, its effectiveness relies on augmentation strategies that disregard the rigid syntax of code, leading to performance degradation on larger, complex files. To address this, we introduce AST-PAC, a domain-specific adaptation that utilizes Abstract Syntax Tree (AST) based perturbations to generate syntactically valid calibration samples. Preliminary results indicate that AST-PAC improves as syntactic size grows, where PAC degrades, but under-mutates small files and underperforms on alphanumeric-rich code. Overall, the findings motivate future work on syntax-aware and size-adaptive calibration as a prerequisite for reliable provenance auditing of code language models.

</details>


### [173] [X-Blocks: Linguistic Building Blocks of Natural Language Explanations for Automated Vehicles](https://arxiv.org/abs/2602.13248)
*Ashkan Y. Zadeh,Xiaomeng Li,Andry Rakotonirainy,Ronald Schroeter,Sebastien Glaser,Zishuo Zhu*

Main category: cs.AI

TL;DR: X-Blocks is a hierarchical framework analyzing natural language explanations for AVs across context, syntax, and lexical levels, with RACE achieving 91.45% accuracy in classifying explanations into scenario-aware categories.


<details>
  <summary>Details</summary>
Motivation: Existing approaches lack systematic frameworks for analyzing how humans linguistically construct driving rationales across diverse scenarios, despite natural language explanations being critical for establishing trust and acceptance of automated vehicles.

Method: X-Blocks framework analyzes explanations at three levels: context (using RACE - multi-LLM ensemble with Chain-of-Thought reasoning and self-consistency), syntax (dependency parsing and template extraction), and lexicon (log-odds analysis with informative Dirichlet priors).

Result: RACE achieves 91.45% accuracy and Cohen's kappa of 0.91 for context classification, near-human reliability. Analysis reveals context-specific vocabulary patterns and limited repertoire of reusable grammar families with systematic variation across scenarios.

Conclusion: X-Blocks provides dataset-agnostic, task-independent framework with evidence-based linguistic design principles for generating scenario-aware explanations that support transparency, user trust, and cognitive accessibility in automated driving systems.

Abstract: Natural language explanations play a critical role in establishing trust and acceptance of automated vehicles (AVs), yet existing approaches lack systematic frameworks for analysing how humans linguistically construct driving rationales across diverse scenarios. This paper introduces X-Blocks (eXplanation Blocks), a hierarchical analytical framework that identifies the linguistic building blocks of natural language explanations for AVs at three levels: context, syntax, and lexicon.
  At the context level, we propose RACE (Reasoning-Aligned Classification of Explanations), a multi-LLM ensemble framework that combines Chain-of-Thought reasoning with self-consistency mechanisms to robustly classify explanations into 32 scenario-aware categories. Applied to human-authored explanations from the Berkeley DeepDrive-X dataset, RACE achieves 91.45 percent accuracy and a Cohens kappa of 0.91 against cases with human annotator agreement, indicating near-human reliability for context classification.
  At the lexical level, log-odds analysis with informative Dirichlet priors reveals context-specific vocabulary patterns that distinguish driving scenarios. At the syntactic level, dependency parsing and template extraction show that explanations draw from a limited repertoire of reusable grammar families, with systematic variation in predicate types and causal constructions across contexts.
  The X-Blocks framework is dataset-agnostic and task-independent, offering broad applicability to other automated driving datasets and safety-critical domains. Overall, our findings provide evidence-based linguistic design principles for generating scenario-aware explanations that support transparency, user trust, and cognitive accessibility in automated driving systems.

</details>


### [174] [DPBench: Large Language Models Struggle with Simultaneous Coordination](https://arxiv.org/abs/2602.13255)
*Najmul Hasan,Prashanth BusiReddyGari*

Main category: cs.AI

TL;DR: DPBench is a Dining Philosophers-based benchmark that tests LLM coordination under resource contention, finding LLMs fail catastrophically in simultaneous decision scenarios despite working well sequentially.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly deployed in multi-agent systems, but there's a lack of benchmarks testing their ability to coordinate under resource contention scenarios.

Method: Created DPBench benchmark based on Dining Philosophers problem, testing LLM coordination across 8 conditions varying decision timing (sequential vs simultaneous), group size, and communication. Tested GPT-5.2, Claude Opus 4.5, and Grok 4.1.

Result: LLMs coordinate effectively in sequential settings but fail catastrophically in simultaneous decision scenarios, with deadlock rates exceeding 95%. Communication doesn't help and can even increase deadlock rates. Failure traced to convergent reasoning where agents independently adopt identical strategies that cause deadlock when executed simultaneously.

Conclusion: Multi-agent LLM systems requiring concurrent resource access need external coordination mechanisms rather than relying on emergent coordination. DPBench released as open-source benchmark.

Abstract: Large language models are increasingly deployed in multi-agent systems, yet we lack benchmarks that test whether they can coordinate under resource contention. We introduce DPBench, a benchmark based on the Dining Philosophers problem that evaluates LLM coordination across eight conditions that vary decision timing, group size, and communication. Our experiments with GPT-5.2, Claude Opus 4.5, and Grok 4.1 reveal a striking asymmetry: LLMs coordinate effectively in sequential settings but fail when decisions must be made simultaneously, with deadlock rates exceeding 95\% under some conditions. We trace this failure to convergent reasoning, where agents independently arrive at identical strategies that, when executed simultaneously, guarantee deadlock. Contrary to expectations, enabling communication does not resolve this problem and can even increase deadlock rates. Our findings suggest that multi-agent LLM systems requiring concurrent resource access may need external coordination mechanisms rather than relying on emergent coordination. DPBench is released as an open-source benchmark. Code and benchmark are available at https://github.com/najmulhasan-code/dpbench.

</details>


### [175] [MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems](https://arxiv.org/abs/2602.13258)
*Deepak Babu Piskala*

Main category: cs.AI

TL;DR: MAPLE decomposes LLM agent personalization into three distinct components (Memory, Learning, Personalization) instead of treating them as unified capabilities, achieving significant improvements in adaptation to individual users.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents have limited ability to adapt to individual users because they conflate memory, learning, and personalization as a single capability rather than treating them as distinct mechanisms requiring different infrastructure and optimization.

Method: Proposes MAPLE (Memory-Adaptive Personalized LEarning), a principled decomposition where: 1) Memory handles storage/retrieval infrastructure, 2) Learning extracts intelligence from interactions asynchronously, and 3) Personalization applies learned knowledge in real-time within context budgets. Each component operates as a dedicated sub-agent with specialized tooling.

Result: 14.6% improvement in personalization score compared to stateless baseline (p < 0.01, Cohen's d = 0.95) and increases trait incorporation rate from 45% to 75% on the MAPLE-Personas benchmark.

Conclusion: Decomposing personalization into distinct memory, learning, and personalization components enables LLM agents to genuinely learn and adapt to individual users, addressing a fundamental architectural limitation in current systems.

Abstract: Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a unified capability rather than three distinct mechanisms requiring different infrastructure, operating on different timescales, and benefiting from independent optimization. We propose MAPLE (Memory-Adaptive Personalized LEarning), a principled decomposition where Memory handles storage and retrieval infrastructure; Learning extracts intelligence from accumulated interactions asynchronously; and Personalization applies learned knowledge in real-time within finite context budgets. Each component operates as a dedicated sub-agent with specialized tooling and well-defined interfaces. Experimental evaluation on the MAPLE-Personas benchmark demonstrates that our decomposition achieves a 14.6% improvement in personalization score compared to a stateless baseline (p < 0.01, Cohen's d = 0.95) and increases trait incorporation rate from 45% to 75% -- enabling agents that genuinely learn and adapt.

</details>


### [176] [General learned delegation by clones](https://arxiv.org/abs/2602.13262)
*Darren Li,Meiqi Chen,Chenze Shao,Fandong Meng,Jie Zhou*

Main category: cs.AI

TL;DR: SELFCEST improves language model performance by enabling parallel reasoning with same-weight clones through agentic reinforcement learning, optimizing compute efficiency under fixed inference budgets.


<details>
  <summary>Details</summary>
Motivation: Current frontier language models improve with additional test-time computation, but serial reasoning or uncoordinated parallel sampling can be compute-inefficient under fixed inference budgets. There's a need for more efficient allocation of computational resources during inference.

Method: SELFCEST equips a base model with the ability to spawn same-weight clones in separate parallel contexts using agentic reinforcement learning. Training is end-to-end under a global task reward with shared-parameter rollouts, yielding a learned controller that allocates both generation and context budget across branches.

Result: Across challenging math reasoning benchmarks and long-context multi-hop QA, SELFCEST improves the accuracy-cost Pareto frontier relative to monolithic baselines at matched inference budget, and exhibits out-of-distribution generalization in both domains.

Conclusion: SELFCEST provides an effective approach for improving language model performance through parallel reasoning with optimized compute allocation, demonstrating better efficiency than traditional methods while maintaining generalization capabilities.

Abstract: Frontier language models improve with additional test-time computation, but serial reasoning or uncoordinated parallel sampling can be compute-inefficient under fixed inference budgets. We propose SELFCEST, which equips a base model with the ability to spawn same-weight clones in separate parallel contexts by agentic reinforcement learning. Training is end-to-end under a global task reward with shared-parameter rollouts, yielding a learned controller that allocates both generation and context budget across branches. Across challenging math reasoning benchmarks and long-context multi-hop QA, SELFCEST improves the accuracy-cost Pareto frontier relative to monolithic baselines at matched inference budget, and exhibits out-of-distribution generalization in both domains.

</details>


### [177] [Human-Centered Explainable AI for Security Enhancement: A Deep Intrusion Detection Framework](https://arxiv.org/abs/2602.13271)
*Md Muntasir Jahid Ayan,Md. Shahriar Rashid,Tazzina Afroze Hassan,Hossain Md. Mubashshir Jamil,Mahbubul Islam,Lisan Al Amin,Rupak Kumar Das,Farzana Akter,Faisal Quader*

Main category: cs.AI

TL;DR: Novel IDS framework combining CNN-LSTM deep learning with XAI (SHAP) for interpretable intrusion detection, achieving 0.99 accuracy on NSL-KDD dataset while providing transparency through feature importance analysis.


<details>
  <summary>Details</summary>
Motivation: Increasing cyber-threat complexity demands intrusion detection systems that are both accurate and interpretable, addressing the need for transparency in deep learning models for security applications.

Method: Integrated CNN-LSTM networks to capture temporal dependencies in traffic sequences, combined with SHAP (XAI) for interpretability, evaluated on NSL-KDD dataset with expert surveys using IPIP6 and Big Five personality traits.

Result: Both CNN and LSTM achieved 0.99 accuracy; LSTM outperformed CNN in macro average precision, recall, and F-1 score; SHAP identified key influential features (srv_serror_rate, dst_host_srv_serror_rate, serror_rate); expert surveys validated system reliability and usability.

Conclusion: Demonstrates successful integration of performance and transparency in cybersecurity solutions, highlighting potential for XAI-enhanced IDS and recommending future work on adaptive learning for real-time threat detection.

Abstract: The increasing complexity and frequency of cyber-threats demand intrusion detection systems (IDS) that are not only accurate but also interpretable. This paper presented a novel IDS framework that integrated Explainable Artificial Intelligence (XAI) to enhance transparency in deep learning models. The framework was evaluated experimentally using the benchmark dataset NSL-KDD, demonstrating superior performance compared to traditional IDS and black-box deep learning models. The proposed approach combined Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) networks for capturing temporal dependencies in traffic sequences. Our deep learning results showed that both CNN and LSTM reached 0.99 for accuracy, whereas LSTM outperformed CNN at macro average precision, recall, and F-1 score. For weighted average precision, recall, and F-1 score, both models scored almost similarly. To ensure interpretability, the XAI model SHapley Additive exPlanations (SHAP) was incorporated, enabling security analysts to understand and validate model decisions. Some notable influential features were srv_serror_rate, dst_host_srv_serror_rate, and serror_rate for both models, as pointed out by SHAP. We also conducted a trust-focused expert survey based on IPIP6 and Big Five personality traits via an interactive UI to evaluate the system's reliability and usability. This work highlighted the potential of combining performance and transparency in cybersecurity solutions and recommends future enhancements through adaptive learning for real-time threat detection.

</details>


### [178] [TemporalBench: A Benchmark for Evaluating LLM-Based Agents on Contextual and Event-Informed Time Series Tasks](https://arxiv.org/abs/2602.13272)
*Muyan Weng,Defu Cao,Wei Yang,Yashaswi Sharma,Yan Liu*

Main category: cs.AI

TL;DR: TemporalBench is a multi-domain benchmark for evaluating temporal reasoning across four tiers: historical structure interpretation, context-free forecasting, contextual temporal reasoning, and event-conditioned prediction in retail, healthcare, energy, and physical systems domains.


<details>
  <summary>Details</summary>
Motivation: Current forecasting benchmarks don't distinguish between genuine temporal understanding and contextual/event-driven reasoning. It's unclear whether strong forecasting performance reflects true temporal reasoning or just pattern recognition under specific conditions.

Method: Created TemporalBench with four-tier task taxonomy across four real-world domains. Controls access to future targets and contextual information to enable diagnostic analysis of temporal reasoning capabilities. Includes baseline experiments with existing models.

Result: Strong numerical forecasting accuracy doesn't reliably translate to robust contextual or event-aware temporal reasoning. Existing agent frameworks show fragmented strengths and systematic failure modes that remain hidden under forecasting-only benchmarks.

Conclusion: TemporalBench reveals limitations in current temporal reasoning models and provides a diagnostic tool for evaluating genuine temporal understanding versus contextual pattern matching. The benchmark is publicly available with a leaderboard for ongoing evaluation.

Abstract: It is unclear whether strong forecasting performance reflects genuine temporal understanding or the ability to reason under contextual and event-driven conditions. We introduce TemporalBench, a multi-domain benchmark designed to evaluate temporal reasoning behavior under progressively richer informational settings. TemporalBench adopts a four-tier task taxonomy that examines historical structure interpretation, context-free forecasting, contextual temporal reasoning, and event-conditioned prediction across four real-world domains: retail, healthcare, energy, and physical systems. By controlling access to future targets and contextual information, the benchmark enables a diagnostic analysis of whether models can correctly interpret temporal patterns, align them with external context, and adapt predictions when conditions change. Extensive baseline experiments show that strong numerical forecasting accuracy does not reliably translate into robust contextual or event-aware temporal reasoning; instead, existing agent frameworks exhibit fragmented strengths and systematic failure modes that remain largely hidden under forecasting-only benchmarks. The TemporalBench dataset is publicly available at https://huggingface.co/datasets/Melady/TemporalBench, and we additionally provide a public leaderboard at https://huggingface.co/spaces/Melady/TemporalBench_Leaderboard.

</details>


### [179] [ProMoral-Bench: Evaluating Prompting Strategies for Moral Reasoning and Safety in LLMs](https://arxiv.org/abs/2602.13274)
*Rohan Subramanian Thomas,Shikhar Shiromani,Abdullah Chaudhry,Ruizhe Li,Vasu Sharma,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: ProMoral-Bench is a unified benchmark evaluating 11 prompting paradigms across 4 LLM families, showing compact exemplar-guided prompts outperform complex reasoning for moral safety.


<details>
  <summary>Details</summary>
Motivation: Prompt design significantly impacts LLM moral competence and safety alignment, but empirical comparisons are fragmented across datasets and models, lacking standardized evaluation.

Method: Introduced ProMoral-Bench benchmark with 11 prompting paradigms across 4 LLM families, using ETHICS, Scruples, WildJailbreak, and new ETHICS-Contrast robustness test, measured via Unified Moral Safety Score (UMSS).

Result: Compact, exemplar-guided scaffolds outperform complex multi-stage reasoning, providing higher UMSS scores and greater robustness at lower token cost. Few-shot exemplars enhance moral stability and jailbreak resistance.

Conclusion: ProMoral-Bench establishes a standardized framework for principled, cost-effective prompt engineering, showing simpler exemplar-based approaches are more effective than complex reasoning for moral safety.

Abstract: Prompt design significantly impacts the moral competence and safety alignment of large language models (LLMs), yet empirical comparisons remain fragmented across datasets and models.We introduce ProMoral-Bench, a unified benchmark evaluating 11 prompting paradigms across four LLM families. Using ETHICS, Scruples, WildJailbreak, and our new robustness test, ETHICS-Contrast, we measure performance via our proposed Unified Moral Safety Score (UMSS), a metric balancing accuracy and safety. Our results show that compact, exemplar-guided scaffolds outperform complex multi-stage reasoning, providing higher UMSS scores and greater robustness at a lower token cost. While multi-turn reasoning proves fragile under perturbations, few-shot exemplars consistently enhance moral stability and jailbreak resistance. ProMoral-Bench establishes a standardized framework for principled, cost-effective prompt engineering.

</details>


### [180] [Artificial Organisations](https://arxiv.org/abs/2602.13275)
*William Waites*

Main category: cs.AI

TL;DR: Multi-agent AI systems should use institutional design (compartmentalization, adversarial review) rather than relying on individual alignment to achieve reliable collective behavior, as demonstrated by the Perseverance Composition Engine.


<details>
  <summary>Details</summary>
Motivation: Current alignment research focuses on making individual AI systems reliable, but human institutions achieve reliable collective behavior through organizational structure that mitigates risks from misaligned individuals. Multi-agent AI systems should adopt this institutional model rather than assuming individual alignment.

Method: Created the Perseverance Composition Engine, a multi-agent system with three specialized agents: Composer (drafts text), Corroborator (verifies factual substantiation with full source access), and Critic (evaluates argumentative quality without source access). This enforces information asymmetry through system architecture, creating layered verification.

Result: Observations from 474 composition tasks showed patterns consistent with institutional hypothesis. When assigned impossible tasks requiring fabricated content, the system progressed from attempted fabrication toward honest refusal with alternative proposals—behavior not instructed or individually incentivized.

Conclusion: Organizational theory provides a productive framework for multi-agent AI safety. Architectural enforcement through information compartmentalization offers a route to reliable collective behavior from unreliable individual components, positioning institutional design as an alternative to individual alignment approaches.

Abstract: Alignment research focuses on making individual AI systems reliable. Human institutions achieve reliable collective behaviour differently: they mitigate the risk posed by misaligned individuals through organisational structure. Multi-agent AI systems should follow this institutional model using compartmentalisation and adversarial review to achieve reliable outcomes through architectural design rather than assuming individual alignment.
  We demonstrate this approach through the Perseverance Composition Engine, a multi-agent system for document composition. The Composer drafts text, the Corroborator verifies factual substantiation with full source access, and the Critic evaluates argumentative quality without access to sources: information asymmetry enforced by system architecture. This creates layered verification: the Corroborator detects unsupported claims, whilst the Critic independently assesses coherence and completeness. Observations from 474 composition tasks (discrete cycles of drafting, verification, and evaluation) exhibit patterns consistent with the institutional hypothesis. When assigned impossible tasks requiring fabricated content, this iteration enabled progression from attempted fabrication toward honest refusal with alternative proposals--behaviour neither instructed nor individually incentivised. These findings motivate controlled investigation of whether architectural enforcement produces reliable outcomes from unreliable components.
  This positions organisational theory as a productive framework for multi-agent AI safety. By implementing verification and evaluation as structural properties enforced through information compartmentalisation, institutional design offers a route to reliable collective behaviour from unreliable individual components.

</details>


### [181] [BEAGLE: Behavior-Enforced Agent for Grounded Learner Emulation](https://arxiv.org/abs/2602.13280)
*Hanchen David Wang,Clayton Cohn,Zifan Xu,Siyuan Guo,Gautam Biswas,Meiyi Ma*

Main category: cs.AI

TL;DR: BEAGLE is a neuro-symbolic framework that simulates authentic student learning behaviors in open-ended problem-solving by incorporating Self-Regulated Learning theory to overcome LLM competency bias.


<details>
  <summary>Details</summary>
Motivation: Collecting authentic student learning data is challenging due to privacy concerns and high costs of longitudinal studies. While LLMs can simulate students, they suffer from competency bias - they optimize for efficient correctness rather than replicating the erratic, iterative struggle characteristic of novice learners.

Method: BEAGLE integrates three key innovations: (1) a semi-Markov model governing timing and transitions of cognitive/metacognitive behaviors, (2) Bayesian Knowledge Tracing with explicit flaw injection to enforce realistic knowledge gaps, and (3) a decoupled agent design separating high-level strategy use from code generation to prevent silent error correction.

Result: BEAGLE significantly outperforms state-of-the-art baselines in reproducing authentic trajectories on Python programming tasks. In a human Turing test, users couldn't distinguish synthetic traces from real student data, achieving 52.8% accuracy (indistinguishable from random guessing).

Conclusion: BEAGLE successfully addresses LLM competency bias in student simulation by incorporating Self-Regulated Learning theory, enabling realistic simulation of novice learning behaviors for education research applications like training adaptive tutoring systems and testing pedagogical interventions.

Abstract: Simulating student learning behaviors in open-ended problem-solving environments holds potential for education research, from training adaptive tutoring systems to stress-testing pedagogical interventions. However, collecting authentic data is challenging due to privacy concerns and the high cost of longitudinal studies. While Large Language Models (LLMs) offer a promising path to student simulation, they suffer from competency bias, optimizing for efficient correctness rather than the erratic, iterative struggle characteristic of novice learners. We present BEAGLE, a neuro-symbolic framework that addresses this bias by incorporating Self-Regulated Learning (SRL) theory into a novel architecture. BEAGLE integrates three key technical innovations: (1) a semi-Markov model that governs the timing and transitions of cognitive behaviors and metacognitive behaviors; (2) Bayesian Knowledge Tracing with explicit flaw injection to enforce realistic knowledge gaps and "unknown unknowns"; and (3) a decoupled agent design that separates high-level strategy use from code generation actions to prevent the model from silently correcting its own intentional errors. In evaluations on Python programming tasks, BEAGLE significantly outperforms state-of-the-art baselines in reproducing authentic trajectories. In a human Turing test, users were unable to distinguish synthetic traces from real student data, achieving an accuracy indistinguishable from random guessing (52.8%).

</details>


### [182] [Accuracy Standards for AI at Work vs. Personal Life: Evidence from an Online Survey](https://arxiv.org/abs/2602.13283)
*Gaston Besanson,Federico Todeschini*

Main category: cs.AI

TL;DR: People demand higher accuracy from AI tools at work than in personal life, and experience more disruption when tools are unavailable in personal contexts.


<details>
  <summary>Details</summary>
Motivation: To understand how people trade off accuracy when using AI tools in professional vs personal contexts, and how they cope when these tools are unavailable.

Method: Online survey (N=300) measuring accuracy requirements in work vs personal contexts, with "accuracy" defined as context-specific reliability aligning with user intent within tolerance thresholds.

Result: Significantly higher accuracy requirements at work (24.1% top-box) vs personal life (8.8%). Heavy app use correlates with stricter work standards. More disruption reported when tools unavailable in personal contexts (34.1%) vs work (15.3%).

Conclusion: Accuracy expectations for AI tools differ substantially between professional and personal contexts, with work demanding higher reliability. Users adapt differently to tool unavailability across contexts.

Abstract: We study how people trade off accuracy when using AI-powered tools in professional versus personal contexts for adoption purposes, the determinants of those trade-offs, and how users cope when AI/apps are unavailable. Because modern AI systems (especially generative models) can produce acceptable but non-identical outputs, we define "accuracy" as context-specific reliability: the degree to which an output aligns with the user's intent within a tolerance threshold that depends on stakes and the cost of correction. In an online survey (N=300), among respondents with both accuracy items (N=170), the share requiring high accuracy (top-box) is 24.1% at work vs. 8.8% in personal life (+15.3 pp; z=6.29, p<0.001). The gap remains large under a broader top-two-box definition (67.0% vs. 32.9%) and on the full 1-5 ordinal scale (mean 3.86 vs. 3.08). Heavy app use and experience patterns correlate with stricter work standards (H2). When tools are unavailable (H3), respondents report more disruption in personal routines than at work (34.1% vs. 15.3%, p<0.01). We keep the main text focused on these substantive results and place test taxonomy and power derivations in a technical appendix.

</details>


### [183] [Mirror: A Multi-Agent System for AI-Assisted Ethics Review](https://arxiv.org/abs/2602.13292)
*Yifan Ding,Yuhui Shi,Zhiyan Li,Zilong Wang,Yifeng Gao,Yajun Yang,Mengjie Yang,Yixiu Liang,Xipeng Qiu,Xuanjing Huang,Xingjun Ma,Yu-Gang Jiang,Guoyu Wang*

Main category: cs.AI

TL;DR: Mirror is an AI framework for ethics review using fine-tuned LLMs to automate expedited reviews and simulate committee deliberations.


<details>
  <summary>Details</summary>
Motivation: Traditional ethics review systems are strained by large-scale interdisciplinary research, creating demand for consistent decisions despite heterogeneous risk profiles. Current LLMs lack sufficient ethical reasoning, regulatory integration, and privacy compliance for direct application.

Method: Developed Mirror framework with EthicsLLM (fine-tuned on EthicsQA dataset of 41K ethical reasoning examples). Two modes: Mirror-ER for automated expedited review using executable rule base, and Mirror-CR for simulated committee deliberation with multiple expert agents.

Result: Empirical evaluations show Mirror significantly improves quality, consistency, and professionalism of ethics assessments compared to generalist LLMs.

Conclusion: Mirror demonstrates that AI-assisted frameworks can effectively support ethics review by integrating ethical reasoning with regulatory structures, addressing current limitations in institutional review capacity.

Abstract: Ethics review is a foundational mechanism of modern research governance, yet contemporary systems face increasing strain as ethical risks arise as structural consequences of large-scale, interdisciplinary scientific practice. The demand for consistent and defensible decisions under heterogeneous risk profiles exposes limitations in institutional review capacity rather than in the legitimacy of ethics oversight. Recent advances in large language models (LLMs) offer new opportunities to support ethics review, but their direct application remains limited by insufficient ethical reasoning capability, weak integration with regulatory structures, and strict privacy constraints on authentic review materials. In this work, we introduce Mirror, an agentic framework for AI-assisted ethical review that integrates ethical reasoning, structured rule interpretation, and multi-agent deliberation within a unified architecture. At its core is EthicsLLM, a foundational model fine-tuned on EthicsQA, a specialized dataset of 41K question-chain-of-thought-answer triples distilled from authoritative ethics and regulatory corpora. EthicsLLM provides detailed normative and regulatory understanding, enabling Mirror to operate in two complementary modes. Mirror-ER (expedited Review) automates expedited review through an executable rule base that supports efficient and transparent compliance checks for minimal-risk studies. Mirror-CR (Committee Review) simulates full-board deliberation through coordinated interactions among expert agents, an ethics secretary agent, and a principal investigator agent, producing structured, committee-level assessments across ten ethical dimensions. Empirical evaluations demonstrate that Mirror significantly improves the quality, consistency, and professionalism of ethics assessments compared with strong generalist LLMs.

</details>


### [184] [DECKBench: Benchmarking Multi-Agent Frameworks for Academic Slide Generation and Editing](https://arxiv.org/abs/2602.13318)
*Daesik Jang,Morgan Lindsay Heisler,Linzi Xing,Yifei Li,Edward Wang,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.AI

TL;DR: DECKBench is a new benchmark for evaluating multi-agent slide generation and editing systems, addressing limitations of existing evaluation methods for academic presentation creation.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks don't adequately measure the complex requirements of academic slide generation and editing, which need faithful content selection, coherent organization, layout-aware rendering, and robust multi-turn instruction following.

Method: Created DECKBench with curated paper-to-slide pairs and realistic simulated editing instructions. Implemented a modular multi-agent baseline system that decomposes tasks into paper parsing/summarization, slide planning, HTML creation, and iterative editing.

Result: DECKBench systematically assesses slide-level and deck-level fidelity, coherence, layout quality, and multi-turn instruction following. The benchmark highlights system strengths, exposes failure modes, and provides actionable insights for improvement.

Conclusion: This work establishes a standardized foundation for reproducible and comparable evaluation of academic presentation generation and editing systems, with publicly available code and data.

Abstract: Automatically generating and iteratively editing academic slide decks requires more than document summarization. It demands faithful content selection, coherent slide organization, layout-aware rendering, and robust multi-turn instruction following. However, existing benchmarks and evaluation protocols do not adequately measure these challenges. To address this gap, we introduce the Deck Edits and Compliance Kit Benchmark (DECKBench), an evaluation framework for multi-agent slide generation and editing. DECKBench is built on a curated dataset of paper to slide pairs augmented with realistic, simulated editing instructions. Our evaluation protocol systematically assesses slide-level and deck-level fidelity, coherence, layout quality, and multi-turn instruction following. We further implement a modular multi-agent baseline system that decomposes the slide generation and editing task into paper parsing and summarization, slide planning, HTML creation, and iterative editing. Experimental results demonstrate that the proposed benchmark highlights strengths, exposes failure modes, and provides actionable insights for improving multi-agent slide generation and editing systems. Overall, this work establishes a standardized foundation for reproducible and comparable evaluation of academic presentation generation and editing. Code and data are publicly available at https://github.com/morgan-heisler/DeckBench .

</details>


### [185] [Situation Graph Prediction: Structured Perspective Inference for User Modeling](https://arxiv.org/abs/2602.13319)
*Jisung Shin,Daniel Platnick,Marjan Alirezaie,Hossein Rahnama*

Main category: cs.AI

TL;DR: SGP frames perspective modeling as inverse inference of structured representations from multimodal artifacts, using synthetic data generation to overcome privacy/data bottlenecks.


<details>
  <summary>Details</summary>
Motivation: Perspective-aware AI needs to model evolving internal states (goals, emotions, contexts), but faces data bottlenecks: digital footprints are privacy-sensitive and perspective states are rarely labeled.

Method: Propose Situation Graph Prediction (SGP) task that frames perspective modeling as inverse inference problem. Use structure-first synthetic generation strategy that aligns latent labels and observable traces by design. Construct dataset and run diagnostic study using retrieval-augmented in-context learning as proxy for supervision.

Result: Study with GPT-4o shows gap between surface-level extraction and latent perspective inference, indicating latent-state inference is harder than surface extraction under controlled setting. SGP is non-trivial and provides evidence for structure-first data synthesis strategy.

Conclusion: SGP offers promising approach to perspective modeling by framing it as structured inverse inference problem, with synthetic data generation strategy showing potential to overcome real-world data limitations.

Abstract: Perspective-Aware AI requires modeling evolving internal states--goals, emotions, contexts--not merely preferences. Progress is limited by a data bottleneck: digital footprints are privacy-sensitive and perspective states are rarely labeled. We propose Situation Graph Prediction (SGP), a task that frames perspective modeling as an inverse inference problem: reconstructing structured, ontology-aligned representations of perspective from observable multimodal artifacts. To enable grounding without real labels, we use a structure-first synthetic generation strategy that aligns latent labels and observable traces by design. As a pilot, we construct a dataset and run a diagnostic study using retrieval-augmented in-context learning as a proxy for supervision. In our study with GPT-4o, we observe a gap between surface-level extraction and latent perspective inference--indicating latent-state inference is harder than surface extraction under our controlled setting. Results suggest SGP is non-trivial and provide evidence for the structure-first data synthesis strategy.

</details>


### [186] [Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol](https://arxiv.org/abs/2602.13320)
*Flint Xiaofeng Fan,Cheston Tan,Roger Wattenhofer,Yew-Soon Ong*

Main category: cs.AI

TL;DR: First theoretical framework analyzing error accumulation in MCP agents, showing linear error growth with O(√T) deviation bounds, validated across multiple LLMs with practical deployment guidelines.


<details>
  <summary>Details</summary>
Motivation: As AI agents using LLMs increasingly rely on external tools for high-stakes decisions, understanding how errors propagate across sequential tool calls becomes critical for reliability and trustworthiness.

Method: Developed theoretical framework with martingale concentration bounds for error propagation, created hybrid distortion metric combining discrete fact matching and continuous semantic similarity, and conducted experiments across Qwen2-7B, Llama-3-8B, and Mistral-7B models.

Result: Proved cumulative distortion grows linearly with O(√T) high-probability deviation bounds, validated empirically showing distortion tracks linear trend within theoretical envelopes. Found semantic weighting reduces distortion by 80%, and periodic re-grounding every ~9 steps controls errors.

Conclusion: Established theoretical guarantees for predictable agent behavior without exponential failure modes, providing actionable deployment principles for trustworthy agent systems based on concentration properties of error propagation.

Abstract: As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context Protocol (MCP) agents, proving that cumulative distortion exhibits linear growth and high-probability deviations bounded by $O(\sqrt{T})$. This concentration property ensures predictable system behavior and rules out exponential failure modes. We develop a hybrid distortion metric combining discrete fact matching with continuous semantic similarity, then establish martingale concentration bounds on error propagation through sequential tool interactions. Experiments across Qwen2-7B, Llama-3-8B, and Mistral-7B validate our theoretical predictions, showing empirical distortion tracks the linear trend with deviations consistently within $O(\sqrt{T})$ envelopes. Key findings include: semantic weighting reduces distortion by 80\%, and periodic re-grounding approximately every 9 steps suffices for error control. We translate these concentration guarantees into actionable deployment principles for trustworthy agent systems.

</details>


### [187] [Detecting Jailbreak Attempts in Clinical Training LLMs Through Automated Linguistic Feature Extraction](https://arxiv.org/abs/2602.13321)
*Tri Nguyen,Huy Hoang Bao Le,Lohith Srikanth Pentapalli,Laurah Turner,Kelly Cohen*

Main category: cs.AI

TL;DR: This paper presents an automated jailbreak detection system for clinical LLMs using LLM-derived linguistic features instead of manual annotations, achieving strong performance with interpretable results.


<details>
  <summary>Details</summary>
Motivation: Prior work relied on manually annotated linguistic features for jailbreak detection in clinical LLMs, which limited scalability and expressiveness. The authors aim to develop a more scalable and automated approach.

Method: Used expert annotations of four core linguistic features (Professionalism, Medical Relevance, Ethical Behavior, Contextual Distraction) to train BERT-based LLM models (both general and medical domain) as feature regressors. Selected the best regressor for each dimension, then used these as feature extractors for a second layer of classifiers (tree-based, linear, probabilistic, ensemble methods) to predict jailbreak likelihood.

Result: The system achieves strong overall performance across cross-validation and held-out evaluations, demonstrating that LLM-derived linguistic features provide an effective basis for automated jailbreak detection. Error analysis revealed limitations in current annotations and feature representations.

Conclusion: This work demonstrates a scalable and interpretable approach for detecting jailbreak behavior in safety-critical clinical dialogue systems, with future improvements suggested including richer annotation schemes, finer-grained feature extraction, and methods capturing evolving risk over dialogue.

Abstract: Detecting jailbreak attempts in clinical training large language models (LLMs) requires accurate modeling of linguistic deviations that signal unsafe or off-task user behavior. Prior work on the 2-Sigma clinical simulation platform showed that manually annotated linguistic features could support jailbreak detection. However, reliance on manual annotation limited both scalability and expressiveness. In this study, we extend this framework by using experts' annotations of four core linguistic features (Professionalism, Medical Relevance, Ethical Behavior, and Contextual Distraction) and training multiple general-domain and medical-domain BERT-based LLM models to predict these features directly from text. The most reliable feature regressor for each dimension was selected and used as the feature extractor in a second layer of classifiers. We evaluate a suite of predictive models, including tree-based, linear, probabilistic, and ensemble methods, to determine jailbreak likelihood from the extracted features. Across cross-validation and held-out evaluations, the system achieves strong overall performance, indicating that LLM-derived linguistic features provide an effective basis for automated jailbreak detection. Error analysis further highlights key limitations in current annotations and feature representations, pointing toward future improvements such as richer annotation schemes, finer-grained feature extraction, and methods that capture the evolving risk of jailbreak behavior over the course of a dialogue. This work demonstrates a scalable and interpretable approach for detecting jailbreak behavior in safety-critical clinical dialogue systems.

</details>


### [188] [Contrastive explanations of BDI agents](https://arxiv.org/abs/2602.13323)
*Michael Winikoff*

Main category: cs.AI

TL;DR: The paper extends BDI agent explanations to answer contrastive questions ("why X instead of F?"), finding they reduce explanation length and may improve trust and understanding, but surprisingly finds full explanations sometimes worse than no explanations.


<details>
  <summary>Details</summary>
Motivation: While BDI agents can answer "why did you do X?" questions, people naturally ask contrastive questions ("why X instead of F?"). The research aims to improve agent transparency and trust development by enabling contrastive explanations.

Method: Extended previous BDI agent explanation framework to handle contrastive questions. Conducted computational evaluation measuring explanation length reduction, and human subject evaluation assessing preference, trust development, transparency, and understanding.

Result: Contrastive questions significantly reduce explanation length. Some evidence shows contrastive answers are preferred and lead to higher trust, perceived understanding, and confidence. Surprisingly, providing full explanations sometimes performed worse than no explanations.

Conclusion: Contrastive explanations offer practical benefits (shorter, potentially more effective) but the overall value of explanations is nuanced - sometimes less explanation may be better than full explanation for trust and transparency.

Abstract: The ability of autonomous systems to provide explanations is important for supporting transparency and aiding the development of (appropriate) trust. Prior work has defined a mechanism for Belief-Desire-Intention (BDI) agents to be able to answer questions of the form ``why did you do action $X$?''. However, we know that we ask \emph{contrastive} questions (``why did you do $X$ \emph{instead of} $F$?''). We therefore extend previous work to be able to answer such questions. A computational evaluation shows that using contrastive questions yields a significant reduction in explanation length. A human subject evaluation was conducted to assess whether such contrastive answers are preferred, and how well they support trust development and transparency. We found some evidence for contrastive answers being preferred, and some evidence that they led to higher trust, perceived understanding, and confidence in the system's correctness. We also evaluated the benefit of providing explanations at all. Surprisingly, there was not a clear benefit, and in some situations we found evidence that providing a (full) explanation was worse than not providing any explanation.

</details>


### [189] [Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts](https://arxiv.org/abs/2602.13367)
*Chen Yang,Guangyue Peng,Jiaying Zhu,Ran Le,Ruixiang Feng,Tao Zhang,Xiyun Xu,Yang Song,Yiming Jia,Yuntao Wen,Yunzhi Xu,Zekai Wang,Zhenwei An,Zhicong Sun,Zongchao Chen*

Main category: cs.AI

TL;DR: Nanbeige4.1-3B is a 3B parameter language model that achieves strong agentic behavior, code generation, and general reasoning in a single model, outperforming larger models through innovative training techniques.


<details>
  <summary>Details</summary>
Motivation: To create the first open-source small language model (3B parameters) that can simultaneously handle agentic behavior, code generation, and general reasoning - demonstrating that small models can achieve both broad competence and strong specialization.

Method: Combined point-wise and pair-wise reward modeling for reasoning/preference alignment; designed complexity-aware rewards in RL for code generation; performed complex data synthesis with turn-level supervision for deep search; enabled stable long-horizon tool interactions (up to 600 tool-call turns).

Result: Significantly outperforms prior models of similar scale (Nanbeige4-3B-2511, Qwen3-4B) and even achieves superior performance compared to much larger models (Qwen3-30B-A3B). Demonstrates reliable execution of up to 600 tool-call turns for complex problem-solving.

Conclusion: Small models (3B parameters) can achieve both broad competence and strong specialization simultaneously, redefining the potential of small language models and challenging the assumption that larger models are always better.

Abstract: We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B. Our results demonstrate that small models can achieve both broad competence and strong specialization simultaneously, redefining the potential of 3B parameter models.

</details>


### [190] [MoralityGym: A Benchmark for Evaluating Hierarchical Moral Alignment in Sequential Decision-Making Agents](https://arxiv.org/abs/2602.13372)
*Simon Rosen,Siddarth Singh,Ebenezer Gelo,Helen Sarah Robertson,Ibrahim Suder,Victoria Williams,Benjamin Rosman,Geraud Nangue Tasse,Steven James*

Main category: cs.AI

TL;DR: Morality Chains formalism and MoralityGym benchmark for evaluating AI moral alignment in hierarchical norm conflicts using trolley-dilemma environments.


<details>
  <summary>Details</summary>
Motivation: The need to evaluate moral alignment in AI agents when navigating conflicting, hierarchically structured human norms, addressing challenges at the intersection of AI safety, moral philosophy, and cognitive science.

Method: Introduces Morality Chains (ordered deontic constraints for moral norms) and MoralityGym (98 ethical-dilemma problems as trolley-dilemma-style Gymnasium environments). Decouples task-solving from moral evaluation with a novel Morality Metric.

Result: Baseline results with Safe RL methods reveal key limitations, showing current approaches are insufficient for ethical decision-making in complex norm conflicts.

Conclusion: Provides foundation for developing more reliable, transparent, and ethical AI systems in complex real-world contexts, highlighting need for more principled approaches to ethical decision-making.

Abstract: Evaluating moral alignment in agents navigating conflicting, hierarchically structured human norms is a critical challenge at the intersection of AI safety, moral philosophy, and cognitive science. We introduce Morality Chains, a novel formalism for representing moral norms as ordered deontic constraints, and MoralityGym, a benchmark of 98 ethical-dilemma problems presented as trolley-dilemma-style Gymnasium environments. By decoupling task-solving from moral evaluation and introducing a novel Morality Metric, MoralityGym allows the integration of insights from psychology and philosophy into the evaluation of norm-sensitive reasoning. Baseline results with Safe RL methods reveal key limitations, underscoring the need for more principled approaches to ethical decision-making. This work provides a foundation for developing AI systems that behave more reliably, transparently, and ethically in complex real-world contexts.

</details>


### [191] [On-Policy Supervised Fine-Tuning for Efficient Reasoning](https://arxiv.org/abs/2602.13407)
*Anhao Zhao,Ziyang Chen,Junlong Tong,Yingqi Fan,Fanghua Ye,Shuhao Li,Yunpu Ma,Wenjie Li,Xiaoyu Shen*

Main category: cs.AI

TL;DR: On-policy SFT simplifies reasoning model training by replacing complex RL with supervised fine-tuning on self-generated correct and concise reasoning chains, achieving better accuracy-efficiency trade-offs than RL methods.


<details>
  <summary>Details</summary>
Motivation: Current RL-based training for large reasoning models is computationally expensive and complex, with multi-reward objectives often destabilizing training and yielding suboptimal trade-offs between correctness and brevity.

Method: Simplify training by removing KL regularization and group-wise normalization, using only truncation-based length penalty. This reduces to supervised fine-tuning on self-generated data filtered for both correctness and conciseness (on-policy SFT).

Result: Achieves up to 80% reduction in chain-of-thought length while maintaining original accuracy, outperforming complex RL methods across five benchmarks. Also reduces GPU memory usage by 50% and accelerates convergence by 70%.

Conclusion: Complex RL extensions are unnecessary for optimizing reasoning models; simple on-policy SFT consistently defines the accuracy-efficiency Pareto frontier while being more efficient and stable than RL-based approaches.

Abstract: Large reasoning models (LRMs) are commonly trained with reinforcement learning (RL) to explore long chain-of-thought reasoning, achieving strong performance at high computational cost. Recent methods add multi-reward objectives to jointly optimize correctness and brevity, but these complex extensions often destabilize training and yield suboptimal trade-offs. We revisit this objective and challenge the necessity of such complexity. Through principled analysis, we identify fundamental misalignments in this paradigm: KL regularization loses its intended role when correctness and length are directly verifiable, and group-wise normalization becomes ambiguous under multiple reward signals. By removing these two items and simplifying the reward to a truncation-based length penalty, we show that the optimization problem reduces to supervised fine-tuning on self-generated data filtered for both correctness and conciseness. We term this simplified training strategy on-policy SFT. Despite its simplicity, on-policy SFT consistently defines the accuracy-efficiency Pareto frontier. It reduces CoT length by up to 80 while maintaining original accuracy, surpassing more complex RL-based methods across five benchmarks. Furthermore, it significantly enhances training efficiency, reducing GPU memory usage by 50% and accelerating convergence by 70%. Our code is available at https://github.com/EIT-NLP/On-Policy-SFT.

</details>


### [192] [NeuroWeaver: An Autonomous Evolutionary Agent for Exploring the Programmatic Space of EEG Analysis Pipelines](https://arxiv.org/abs/2602.13473)
*Guoan Wang,Shihao Yang,Jun-En Ding,Hao Zhu,Feng Liu*

Main category: cs.AI

TL;DR: NeuroWeaver is an autonomous evolutionary agent for EEG analysis that uses domain-informed search and multi-objective optimization to create lightweight, high-performing pipelines that outperform task-specific methods and match foundation models with far fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Foundation models for EEG analysis require massive data and parameters, making them computationally prohibitive for clinical settings. General-purpose AutoML frameworks lack neurophysiological priors and produce scientifically implausible solutions.

Method: Reformulates EEG pipeline engineering as discrete constrained optimization with Domain-Informed Subspace Initialization (confines search to neuroscientifically plausible manifolds) and Multi-Objective Evolutionary Optimization (dynamically balances performance, novelty, and efficiency via self-reflective refinement).

Result: Empirical evaluations across five heterogeneous benchmarks show NeuroWeaver synthesizes lightweight solutions that consistently outperform state-of-the-art task-specific methods and achieve performance comparable to large-scale foundation models despite using significantly fewer parameters.

Conclusion: NeuroWeaver provides a unified autonomous approach for EEG analysis that bridges the gap between computationally expensive foundation models and scientifically implausible general AutoML, enabling effective deployment in resource-constrained clinical environments.

Abstract: Although foundation models have demonstrated remarkable success in general domains, the application of these models to electroencephalography (EEG) analysis is constrained by substantial data requirements and high parameterization. These factors incur prohibitive computational costs, thereby impeding deployment in resource-constrained clinical environments. Conversely, general-purpose automated machine learning frameworks are often ill-suited for this domain, as exploration within an unbounded programmatic space fails to incorporate essential neurophysiological priors and frequently yields solutions that lack scientific plausibility. To address these limitations, we propose NeuroWeaver, a unified autonomous evolutionary agent designed to generalize across diverse EEG datasets and tasks by reformulating pipeline engineering as a discrete constrained optimization problem. Specifically, we employ a Domain-Informed Subspace Initialization to confine the search to neuroscientifically plausible manifolds, coupled with a Multi-Objective Evolutionary Optimization that dynamically balances performance, novelty, and efficiency via self-reflective refinement. Empirical evaluations across five heterogeneous benchmarks demonstrate that NeuroWeaver synthesizes lightweight solutions that consistently outperform state-of-the-art task-specific methods and achieve performance comparable to large-scale foundation models, despite utilizing significantly fewer parameters.

</details>


### [193] [OMNI-LEAK: Orchestrator Multi-Agent Network Induced Data Leakage](https://arxiv.org/abs/2602.13477)
*Akshat Naik,Jay Culligan,Yarin Gal,Philip Torr,Rahaf Aljundi,Alasdair Paren,Adel Bibi*

Main category: cs.AI

TL;DR: Multi-agent LLM systems are vulnerable to novel OMNI-LEAK attacks that bypass data access controls through indirect prompt injection, compromising multiple agents simultaneously.


<details>
  <summary>Details</summary>
Motivation: As multi-agent LLM systems become practical, there's insufficient threat modeling for their security vulnerabilities, especially in orchestrator setups where a central agent delegates tasks to specialized agents.

Method: Red-teaming a representative orchestrator setup to demonstrate OMNI-LEAK attack vector that compromises multiple agents through single indirect prompt injection, testing frontier models' susceptibility.

Result: Both reasoning and non-reasoning models are vulnerable to OMNI-LEAK attacks that leak sensitive data even with access controls, without requiring attacker's insider knowledge of implementation details.

Conclusion: Safety research must generalize from single-agent to multi-agent settings to prevent real-world privacy breaches, financial losses, and maintain public trust in AI agents.

Abstract: As Large Language Model (LLM) agents become more capable, their coordinated use in the form of multi-agent systems is anticipated to emerge as a practical paradigm. Prior work has examined the safety and misuse risks associated with agents. However, much of this has focused on the single-agent case and/or setups missing basic engineering safeguards such as access control, revealing a scarcity of threat modeling in multi-agent systems. We investigate the security vulnerabilities of a popular multi-agent pattern known as the orchestrator setup, in which a central agent decomposes and delegates tasks to specialized agents. Through red-teaming a concrete setup representative of a likely future use case, we demonstrate a novel attack vector, OMNI-LEAK, that compromises several agents to leak sensitive data through a single indirect prompt injection, even in the \textit{presence of data access control}. We report the susceptibility of frontier models to different categories of attacks, finding that both reasoning and non-reasoning models are vulnerable, even when the attacker lacks insider knowledge of the implementation details. Our work highlights the importance of safety research to generalize from single-agent to multi-agent settings, in order to reduce the serious risks of real-world privacy breaches and financial losses and overall public trust in AI agents.

</details>


### [194] [Translating Dietary Standards into Healthy Meals with Minimal Substitutions](https://arxiv.org/abs/2602.13502)
*Trevor Chan,Ilias Tagkopoulos*

Main category: cs.AI

TL;DR: A framework that converts dietary standards into complete meals with minimal changes using meal archetypes and generative models to improve nutrition while maintaining convenience and affordability.


<details>
  <summary>Details</summary>
Motivation: To improve nutritional quality of meals without compromising convenience or affordability, addressing the challenge of making dietary guidelines practical and accessible for everyday use.

Method: Used WWEIA intake data for 135,491 meals to identify 34 interpretable meal archetypes, then conditioned a generative model and portion predictor on these archetypes to meet USDA nutritional targets while allowing minimal food substitutions.

Result: Generated meals improved adherence to RDI targets by 47.0%, remained compositionally close to real meals, achieved 10% more nutritious meals with 1-3 food substitutions, and reduced costs by 19-32% on average.

Conclusion: The framework successfully turns dietary guidelines into realistic, budget-aware meals with simple swaps, providing a scalable solution for clinical decision support, public-health programs, and consumer apps to improve everyday nutrition equitably.

Abstract: An important goal for personalized diet systems is to improve nutritional quality without compromising convenience or affordability. We present an end-to-end framework that converts dietary standards into complete meals with minimal change. Using the What We Eat in America (WWEIA) intake data for 135,491 meals, we identify 34 interpretable meal archetypes that we then use to condition a generative model and a portion predictor to meet USDA nutritional targets. In comparisons within archetypes, generated meals are better at following recommended daily intake (RDI) targets by 47.0%, while remaining compositionally close to real meals. Our results show that by allowing one to three food substitutions, we were able to create meals that were 10% more nutritious, while reducing costs 19-32%, on average. By turning dietary guidelines into realistic, budget-aware meals and simple swaps, this framework can underpin clinical decision support, public-health programs, and consumer apps that deliver scalable, equitable improvements in everyday nutrition.

</details>


### [195] [SPILLage: Agentic Oversharing on the Web](https://arxiv.org/abs/2602.13516)
*Jaechul Roh,Eugene Bagdasarian,Hamed Haddadi,Ali Shahin Shamsabadi*

Main category: cs.AI

TL;DR: Web agents frequently overshare user information through both content and behavioral traces when automating tasks across live websites, with behavioral oversharing being 5x more common than content leakage.


<details>
  <summary>Details</summary>
Motivation: LLM-powered web agents automate user tasks across the open web with access to sensitive user resources, but unlike controlled chatbot settings, they leave action traces that could unintentionally disclose user information to third parties.

Method: Introduces SPILLage framework characterizing oversharing along channel (content vs. behavior) and directness (explicit vs. implicit) dimensions. Benchmarks 180 tasks on live e-commerce sites with ground-truth annotations, conducting 1,080 runs across two agentic frameworks and three backbone LLMs.

Result: Oversharing is pervasive with behavioral oversharing dominating content oversharing by 5x. Prompt-level mitigation is ineffective, but removing task-irrelevant information before execution improves task success by up to 17.9%.

Conclusion: Protecting privacy in web agents requires a broader view of "output" that accounts for what agents do on the web, not just what they type, as behavioral traces pose significant privacy risks.

Abstract: LLM-powered agents are beginning to automate user's tasks across the open web, often with access to user resources such as emails and calendars. Unlike standard LLMs answering questions in a controlled ChatBot setting, web agents act "in the wild", interacting with third parties and leaving behind an action trace. Therefore, we ask the question: how do web agents handle user resources when accomplishing tasks on their behalf across live websites? In this paper, we formalize Natural Agentic Oversharing -- the unintentional disclosure of task-irrelevant user information through an agent trace of actions on the web. We introduce SPILLage, a framework that characterizes oversharing along two dimensions: channel (content vs. behavior) and directness (explicit vs. implicit). This taxonomy reveals a critical blind spot: while prior work focuses on text leakage, web agents also overshare behaviorally through clicks, scrolls, and navigation patterns that can be monitored. We benchmark 180 tasks on live e-commerce sites with ground-truth annotations separating task-relevant from task-irrelevant attributes. Across 1,080 runs spanning two agentic frameworks and three backbone LLMs, we demonstrate that oversharing is pervasive with behavioral oversharing dominates content oversharing by 5x. This effect persists -- and can even worsen -- under prompt-level mitigation. However, removing task-irrelevant information before execution improves task success by up to 17.9%, demonstrating that reducing oversharing improves task success. Our findings underscore that protecting privacy in web agents is a fundamental challenge, requiring a broader view of "output" that accounts for what agents do on the web, not just what they type. Our datasets and code are available at https://github.com/jrohsc/SPILLage.

</details>


### [196] [REMem: Reasoning with Episodic Memory in Language Agent](https://arxiv.org/abs/2602.13530)
*Yiheng Shu,Saisri Padmaja Jonnalagedda,Xiang Gao,Bernal Jiménez Gutiérrez,Weijian Qi,Kamalika Das,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: REMem is a two-phase episodic memory framework for language agents that creates hybrid memory graphs from experiences and uses agentic retrieval for reasoning, outperforming state-of-the-art systems on episodic tasks.


<details>
  <summary>Details</summary>
Motivation: Current language agents lack effective episodic memory capabilities - they mainly use semantic memory and cannot properly recollect and reason over interaction histories like humans do. Existing approaches overlook episodicity, lack explicit event modeling, or focus too much on simple retrieval rather than complex reasoning.

Method: Two-phase framework: 1) Offline indexing converts experiences into a hybrid memory graph linking time-aware gists and facts. 2) Online inference uses an agentic retriever with curated tools for iterative retrieval over the memory graph.

Result: REMem outperforms state-of-the-art memory systems (Mem0 and HippoRAG 2) with 3.4% and 13.4% absolute improvements on episodic recollection and reasoning tasks respectively across four benchmarks. It also shows more robust refusal behavior for unanswerable questions.

Conclusion: REMem successfully addresses the episodic memory gap in language agents by providing a framework that enables effective recollection and reasoning over interaction histories, demonstrating significant improvements over existing memory systems.

Abstract: Humans excel at remembering concrete experiences along spatiotemporal contexts and performing reasoning across those events, i.e., the capacity for episodic memory. In contrast, memory in language agents remains mainly semantic, and current agents are not yet capable of effectively recollecting and reasoning over interaction histories. We identify and formalize the core challenges of episodic recollection and reasoning from this gap, and observe that existing work often overlooks episodicity, lacks explicit event modeling, or overemphasizes simple retrieval rather than complex reasoning. We present REMem, a two-phase framework for constructing and reasoning with episodic memory: 1) Offline indexing, where REMem converts experiences into a hybrid memory graph that flexibly links time-aware gists and facts. 2) Online inference, where REMem employs an agentic retriever with carefully curated tools for iterative retrieval over the memory graph. Comprehensive evaluation across four episodic memory benchmarks shows that REMem substantially outperforms state-of-the-art memory systems such as Mem0 and HippoRAG 2, showing 3.4% and 13.4% absolute improvements on episodic recollection and reasoning tasks, respectively. Moreover, REMem also demonstrates more robust refusal behavior for unanswerable questions.

</details>


### [197] [OpAgent: Operator Agent for Web Navigation](https://arxiv.org/abs/2602.13559)
*Yuyu Guo,Wenjie Yang,Siyuan Yang,Ziyang Liu,Cheng Chen,Yuan Wei,Yun Hu,Yang Huang,Guoliang Hao,Dongsheng Yuan,Jianming Wang,Xin Chen,Hang Yu,Lei Lei,Peng Di*

Main category: cs.AI

TL;DR: Online RL web agent achieves 71.6% SOTA on WebArena through hierarchical fine-tuning, online RL with hybrid rewards, and modular OpAgent framework.


<details>
  <summary>Details</summary>
Motivation: Conventional methods (SFT/offline RL) suffer from distributional shifts in volatile web environments; need robust agents that can handle real-time feedback and stochastic state transitions.

Method: 1) Hierarchical multi-task fine-tuning on Planning/Acting/Grounding datasets; 2) Online RL with hybrid rewards (WebJudge + Rule-based Decision Tree); 3) Modular OpAgent framework with Planner, Grounder, Reflector, Summarizer.

Result: RL-enhanced model achieves 38.1% pass@5 on WebArena; OpAgent framework elevates performance to 71.6% SOTA success rate.

Conclusion: Online RL with hybrid rewards and modular agentic framework enables robust web navigation, overcoming limitations of offline methods and achieving state-of-the-art performance.

Abstract: To fulfill user instructions, autonomous web agents must contend with the inherent complexity and volatile nature of real-world websites. Conventional paradigms predominantly rely on Supervised Fine-Tuning (SFT) or Offline Reinforcement Learning (RL) using static datasets. However, these methods suffer from severe distributional shifts, as offline trajectories fail to capture the stochastic state transitions and real-time feedback of unconstrained wide web environments. In this paper, we propose a robust Online Reinforcement Learning WebAgent, designed to optimize its policy through direct, iterative interactions with unconstrained wide websites. Our approach comprises three core innovations: 1) Hierarchical Multi-Task Fine-tuning: We curate a comprehensive mixture of datasets categorized by functional primitives -- Planning, Acting, and Grounding -- establishing a Vision-Language Model (VLM) with strong instruction-following capabilities for Web GUI tasks. 2) Online Agentic RL in the Wild: We develop an online interaction environment and fine-tune the VLM using a specialized RL pipeline. We introduce a Hybrid Reward Mechanism that combines a ground-truth-agnostic WebJudge for holistic outcome assessment with a Rule-based Decision Tree (RDT) for progress reward. This system effectively mitigates the credit assignment challenge in long-horizon navigation. Notably, our RL-enhanced model achieves a 38.1\% success rate (pass@5) on WebArena, outperforming all existing monolithic baselines. 3) Operator Agent: We introduce a modular agentic framework, namely \textbf{OpAgent}, orchestrating a Planner, Grounder, Reflector, and Summarizer. This synergy enables robust error recovery and self-correction, elevating the agent's performance to a new State-of-the-Art (SOTA) success rate of \textbf{71.6\%}.

</details>


### [198] [Who Do LLMs Trust? Human Experts Matter More Than Other LLMs](https://arxiv.org/abs/2602.13568)
*Anooshka Bajaj,Zoran Tiganj*

Main category: cs.AI

TL;DR: LLMs show human-expert bias: they conform more to responses labeled as coming from human experts than from other LLMs, even when the expert signal is incorrect.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs exhibit patterns of social influence similar to humans, particularly whether they privilege feedback from humans over feedback from other LLMs, and whether this influence depends on source credibility and consensus strength.

Method: Three binary decision-making tasks (reading comprehension, multi-step reasoning, moral judgment) with four instruction-tuned LLMs. Presented prior responses attributed to friends, human experts, or other LLMs, manipulated group correctness and size. Second experiment introduced direct disagreement between single human and single LLM.

Result: Models conform significantly more to responses labeled as coming from human experts, including when incorrect, and revise answers toward experts more readily than toward other LLMs. Expert framing acts as a strong prior across decision domains.

Conclusion: LLMs exhibit credibility-sensitive social influence that generalizes across domains, showing a human-expert bias similar to human social influence patterns.

Abstract: Large language models (LLMs) increasingly operate in environments where they encounter social information such as other agents' answers, tool outputs, or human recommendations. In humans, such inputs influence judgments in ways that depend on the source's credibility and the strength of consensus. This paper investigates whether LLMs exhibit analogous patterns of influence and whether they privilege feedback from humans over feedback from other LLMs. Across three binary decision-making tasks, reading comprehension, multi-step reasoning, and moral judgment, we present four instruction-tuned LLMs with prior responses attributed either to friends, to human experts, or to other LLMs. We manipulate whether the group is correct and vary the group size. In a second experiment, we introduce direct disagreement between a single human and a single LLM. Across tasks, models conform significantly more to responses labeled as coming from human experts, including when that signal is incorrect, and revise their answers toward experts more readily than toward other LLMs. These results reveal that expert framing acts as a strong prior for contemporary LLMs, suggesting a form of credibility-sensitive social influence that generalizes across decision domains.

</details>


### [199] [Differentiable Rule Induction from Raw Sequence Inputs](https://arxiv.org/abs/2602.13583)
*Kun Gao,Katsumi Inoue,Yongzhi Cao,Hanpin Wang,Feng Yang*

Main category: cs.AI

TL;DR: A novel differentiable ILP method that integrates self-supervised clustering to learn rules from raw data without explicit label leakage, enabling interpretable rule learning from time series and image data.


<details>
  <summary>Details</summary>
Motivation: Current differentiable ILP models rely on symbolic datasets and struggle with learning directly from raw data due to explicit label leakage - the inability to map continuous inputs to symbolic variables without explicit supervision of input feature labels.

Method: Integration of a self-supervised differentiable clustering model with a novel differentiable ILP model, enabling rule learning from raw data without explicit label leakage by mapping continuous inputs to symbolic variables through clustering.

Result: The method successfully learns generalized rules from time series and image data, effectively describing raw data through its features in an intuitive and precise manner.

Conclusion: The proposed approach addresses the explicit label leakage problem in differentiable ILP, enabling interpretable rule learning directly from raw data while maintaining the transparency and interpretability of rule-based models.

Abstract: Rule learning-based models are widely used in highly interpretable scenarios due to their transparent structures. Inductive logic programming (ILP), a form of machine learning, induces rules from facts while maintaining interpretability. Differentiable ILP models enhance this process by leveraging neural networks to improve robustness and scalability. However, most differentiable ILP methods rely on symbolic datasets, facing challenges when learning directly from raw data. Specifically, they struggle with explicit label leakage: The inability to map continuous inputs to symbolic variables without explicit supervision of input feature labels. In this work, we address this issue by integrating a self-supervised differentiable clustering model with a novel differentiable ILP model, enabling rule learning from raw data without explicit label leakage. The learned rules effectively describe raw data through its features. We demonstrate that our method intuitively and precisely learns generalized rules from time series and image data.

</details>


### [200] [A First Proof Sprint](https://arxiv.org/abs/2602.13587)
*Joseph Corneli*

Main category: cs.AI

TL;DR: Multi-agent proof sprint workflow combining rapid draft generation with adversarial verification and targeted repair for ten research problems, with explicit provenance tracking and layer-separated validation status.


<details>
  <summary>Details</summary>
Motivation: To develop a structured methodology for collaborative mathematical proof development that improves reliability and calibration in compressed proof sprints through explicit provenance tracking and layer-separated validation.

Method: Uses wiring-diagram decompositions of claim dependencies to localize gaps and coordinate reviewer-driven revisions. Combines rapid draft generation with adversarial verification, targeted repair, and explicit provenance tracking. Separates mathematical status from QC-validation status.

Result: Heterogeneous outcomes across ten problems: Problem 3 has validation-complete existence path, Problem 5 solved in scope-limited form, Problem 10 conditional with explicit assumptions, Problems 4 and 6 partial with named obligations, Problem 7 provisionally closed. QC layer shows Problems 7 and 9 have node-level validation but unresolved verifier gaps.

Conclusion: Structure-aware verification and layer-switching strategies improve reliability and calibration in compressed proof sprints, demonstrating the value of explicit provenance tracking and separation of mathematical content from validation status.

Abstract: This monograph reports a multi-agent proof sprint on ten research-level problems, combining rapid draft generation with adversarial verification, targeted repair, and explicit provenance. The workflow uses wiring-diagram decompositions of claim dependencies to localize gaps and coordinate reviewer-driven revisions. Final outcomes are heterogeneous but explicit: the manuscript distinguishes mathematical status from QC-validation status. Mathematically, Problem~3 has a validation-complete existence path under the scoped criterion used here (uniqueness/irreducibility treated as optional), Problem 5 is solved in a scope-limited form for $F_O$-local connective spectra, Problem 10 is conditional under clearly stated assumptions (with explicit necessity counterexamples when assumptions are dropped), and Problems 4 and 6 are partial with named remaining obligations in the general case (including an unconditional $K_n$ result for Problem 6 with $c_0 = 1/3$). Problem 7 is treated as provisionally closed via the rotation-route theorem chain, pending independent ledger re-check. At the QC layer, Problems~7 and~9 have node-level validation artifacts but still contain unresolved verifier gaps. The main methodological result is that structure-aware verification and layer-switching strategies improve reliability and calibration in compressed proof sprints.

</details>


### [201] [Hippocampus: An Efficient and Scalable Memory Module for Agentic AI](https://arxiv.org/abs/2602.13594)
*Yi Li,Lianjie Cao,Faraz Ahmed,Puneet Sharma,Bingzhe Li*

Main category: cs.AI

TL;DR: Hippocampus is an agentic memory system using binary signatures for semantic search and token-ID streams for content reconstruction, achieving 31× faster retrieval and 14× lower token footprint while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Agentic AI needs persistent memory beyond LLM context windows, but existing systems using dense vector databases or knowledge graphs suffer from high retrieval latency and poor storage scalability.

Method: Uses compact binary signatures for semantic search and lossless token-ID streams for exact content reconstruction, with a Dynamic Wavelet Matrix (DWM) that compresses and co-indexes both streams for ultra-fast search in compressed domain.

Result: Reduces end-to-end retrieval latency by up to 31×, cuts per-query token footprint by up to 14×, while maintaining accuracy on LoCoMo and LongMemEval benchmarks.

Conclusion: Hippocampus provides a scalable memory management system for agentic AI with linear scaling, making it suitable for long-horizon deployments by avoiding costly dense-vector or graph computations.

Abstract: Agentic AI require persistent memory to store user-specific histories beyond the limited context window of LLMs. Existing memory systems use dense vector databases or knowledge-graph traversal (or hybrid), incurring high retrieval latency and poor storage scalability. We introduce Hippocampus, an agentic memory management system that uses compact binary signatures for semantic search and lossless token-ID streams for exact content reconstruction. Its core is a Dynamic Wavelet Matrix (DWM) that compresses and co-indexes both streams to support ultra-fast search in the compressed domain, thus avoiding costly dense-vector or graph computations. This design scales linearly with memory size, making it suitable for long-horizon agentic deployments. Empirically, our evaluation shows that Hippocampus reduces end-to-end retrieval latency by up to 31$\times$ and cuts per-query token footprint by up to 14$\times$, while maintaining accuracy on both LoCoMo and LongMemEval benchmarks.

</details>


### [202] [The Quantization Trap: Breaking Linear Scaling Laws in Multi-Hop Reasoning](https://arxiv.org/abs/2602.13595)
*Henry Han,Xiyang Liu,Xiaodong Wang,Fei Han,Xiaodong Li*

Main category: cs.AI

TL;DR: Neural scaling laws break for multi-hop reasoning due to a 'quantization trap' where reducing precision from 16-bit to 8/4-bit paradoxically increases energy consumption while degrading accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper challenges the conventional neural scaling law assumption that reducing numerical precision linearly improves computational efficiency and energy profile (E ∝ bits). The authors investigate whether this scaling law holds for complex multi-hop reasoning tasks, questioning the industry's "smaller-is-better" heuristic for quantization.

Method: The authors demonstrate the breakdown of scaling laws through empirical evidence showing increased energy consumption and degraded accuracy when reducing precision. They provide a rigorous theoretical decomposition that attributes this failure to two main factors: hardware casting overhead (hidden latency cost of dequantization kernels) and sequential energy amortization failure in reasoning chains.

Result: The study reveals a 'quantization trap' where reducing precision from 16-bit to 8/4-bit paradoxically increases net energy consumption while degrading reasoning accuracy. The scaling law breaking is shown to be unavoidable in practice for complex reasoning tasks.

Conclusion: The industry's "smaller-is-better" heuristic for quantization is mathematically counterproductive for complex reasoning tasks. Neural scaling laws that work for standard tasks break down for multi-hop reasoning due to hardware overhead and energy amortization issues in sequential chains.

Abstract: Neural scaling laws provide a predictable recipe for AI advancement: reducing numerical precision should linearly improve computational efficiency and energy profile (E proportional to bits). In this paper, we demonstrate that this scaling law breaks in the context of multi-hop reasoning. We reveal a 'quantization trap' where reducing precision from 16-bit to 8/4-bit paradoxically increases more net energy consumption while degrading reasoning accuracy. We provide a rigorous theoretical decomposition that attributes this failure to hardware casting overhead, the hidden latency cost of dequantization kernels, which becomes a dominant bottleneck in sequential reasoning chains, as well as to a sequential energy amortization failure. As a result, scaling law breaking is unavoidable in practice. Our findings suggest that the industry's "smaller-is-better" heuristic is mathematically counterproductive for complex reasoning tasks.

</details>


### [203] [DiffusionRollout: Uncertainty-Aware Rollout Planning in Long-Horizon PDE Solving](https://arxiv.org/abs/2602.13616)
*Seungwoo Yoo,Juil Koo,Daehyeon Choi,Minhyuk Sung*

Main category: cs.AI

TL;DR: DiffusionRollout is a selective rollout planning strategy for autoregressive diffusion models that uses predictive uncertainty to adaptively select step sizes, reducing error accumulation in long-horizon PDE predictions.


<details>
  <summary>Details</summary>
Motivation: To mitigate error accumulation in long-horizon predictions of physical systems governed by partial differential equations (PDEs) when using autoregressive diffusion models, where conditioning on inaccurate prior outputs leads to compounding errors.

Method: Proposes DiffusionRollout, which leverages the probabilistic nature of diffusion models to quantify predictive uncertainty through standard deviations over multiple samples. Uses this uncertainty measure as a proxy for predictive confidence to adaptively select step sizes during autoregressive rollouts, reducing conditioning on inaccurate prior outputs.

Result: Extensive evaluation on long-trajectory PDE prediction benchmarks shows the uncertainty measure strongly correlates with prediction errors. The adaptive planning strategy achieves lower prediction errors and longer predicted trajectories that maintain high correlation with ground truths.

Conclusion: DiffusionRollout effectively mitigates error accumulation in long-horizon PDE predictions by using uncertainty quantification to guide adaptive rollout planning, improving the reliability of autoregressive diffusion models for physical system prediction.

Abstract: We propose DiffusionRollout, a novel selective rollout planning strategy for autoregressive diffusion models, aimed at mitigating error accumulation in long-horizon predictions of physical systems governed by partial differential equations (PDEs). Building on the recently validated probabilistic approach to PDE solving, we further explore its ability to quantify predictive uncertainty and demonstrate a strong correlation between prediction errors and standard deviations computed over multiple samples-supporting their use as a proxy for the model's predictive confidence. Based on this observation, we introduce a mechanism that adaptively selects step sizes during autoregressive rollouts, improving long-term prediction reliability by reducing the compounding effect of conditioning on inaccurate prior outputs. Extensive evaluation on long-trajectory PDE prediction benchmarks validates the effectiveness of the proposed uncertainty measure and adaptive planning strategy, as evidenced by lower prediction errors and longer predicted trajectories that retain a high correlation with their ground truths.

</details>


### [204] [Guided Collaboration in Heterogeneous LLM-Based Multi-Agent Systems via Entropy-Based Understanding Assessment and Experience Retrieval](https://arxiv.org/abs/2602.13639)
*Linlin Wang,Tianqing Zhu,Laiqiao Qin,Longxiang Gao,Wanlei Zhou*

Main category: cs.AI

TL;DR: The paper reveals that strong-weak LLM collaborations can underperform weak-weak combinations due to cognitive mismatching, and proposes an entropy-based adaptive guidance framework with RAG to dynamically align guidance with agent cognitive states.


<details>
  <summary>Details</summary>
Motivation: As AI systems move from single-agent to multi-agent architectures, heterogeneous systems face cognitive mismatching problems where capability differences cause strong and weak models to fail collaborating effectively. The paper aims to address this bottleneck in heterogeneous multi-agent systems.

Method: Proposes an Entropy-Based Adaptive Guidance Framework that uses multi-dimensional entropy metrics (expression, uncertainty, structure, coherence, relevance) to quantify weak agents' understanding, then adaptively adjusts guidance intensity at light, moderate, and intensive levels. Incorporates Retrieval-Augmented Generation (RAG) to retain successful collaboration experiences for both immediate adaptation and long-term learning.

Result: Extensive experiments on GSM8K, MBPP, and CVRP benchmarks demonstrate that the approach consistently enhances effectiveness and stability of heterogeneous collaboration, outperforming baseline methods and showing that adaptive guidance mitigates cognitive imbalance.

Conclusion: The framework establishes a scalable pathway toward more robust, cooperative multi-agent intelligence by addressing cognitive mismatching through dynamic alignment of guidance with agent cognitive states, enabling effective heterogeneous collaboration.

Abstract: With recent breakthroughs in large language models (LLMs) for reasoning, planning, and complex task generation, artificial intelligence systems are transitioning from isolated single-agent architectures to multi-agent systems with collaborative intelligence. However, in heterogeneous multi-agent systems (HMAS), capability differences among agents give rise to consistent cognitive problems, where strong and weak models fail to contribute effectively. We define the collaboration as a strong-weak system. Through comprehensive experiments, we disclose a counterintuitive phenomenon in the strong-weak system: a strong-weak collaboration may under-perform weak-weak combinations, revealing that cognitive mismatching are key bottlenecks limiting heterogeneous cooperation. To overcome these challenges, we propose an Entropy-Based Adaptive Guidance Framework that dynamically aligns the guidance with the cognitive state of each agent. The framework quantifies the understanding of weak agents through multi-dimensional entropy metrics - covering expression, uncertainty, structure, coherence, and relevance - and adaptively adjusts the intensity of the guidance at light, moderate and intensive levels. Furthermore, a Retrieval-Augmented Generation (RAG) mechanism is incorporated to retain successful collaboration experiences, enabling both immediate adaptation and long-term learning. Extensive experiments on three benchmark datasets, GSM8K, MBPP, and CVRP demonstrate that our approach consistently enhances the effectiveness and stability of heterogeneous collaboration. The results highlight that adaptive guidance not only mitigates cognitive imbalance but also establishes a scalable pathway toward more robust, cooperative multi-agent intelligence.

</details>


### [205] [Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization](https://arxiv.org/abs/2602.13653)
*Yibo Wang,Guangda Huzhang,Yuwei Hu,Yu Xia,Shiyin Lu,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,Lijun Zhang*

Main category: cs.AI

TL;DR: A novel MLLM-centered framework for GUI agents with agentic-Q estimation and step-wise policy optimization that reduces data collection costs and enables stable optimization in non-stationary GUI environments.


<details>
  <summary>Details</summary>
Motivation: GUI agents face challenges in real-world applications due to non-stationary environments, leading to high computational costs for data curation and policy optimization. Existing approaches struggle with these practical deployment issues.

Method: Two-component framework: 1) Agentic-Q estimation - optimizes a Q-model to generate step-wise values evaluating action contributions to task completion; 2) Step-wise policy optimization - takes step-wise samples from state-action trajectories and optimizes policy via reinforcement learning using the agentic-Q model. The framework uses self-generated trajectories and decouples policy updates from the environment.

Result: The framework endows Ovis2.5-9B with powerful GUI interaction capabilities, achieving remarkable performances on GUI navigation and grounding benchmarks, even surpassing larger-scale contenders.

Conclusion: The proposed MLLM-centered framework effectively addresses computational cost challenges in GUI agent deployment, enabling efficient and stable policy optimization while maintaining strong performance on benchmark tasks.

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for data curation and policy optimization. In this report, we introduce a novel MLLM-centered framework for GUI agents, which consists of two components: agentic-Q estimation and step-wise policy optimization. The former one aims to optimize a Q-model that can generate step-wise values to evaluate the contribution of a given action to task completion. The latter one takes step-wise samples from the state-action trajectory as inputs, and optimizes the policy via reinforcement learning with our agentic-Q model. It should be noticed that (i) all state-action trajectories are produced by the policy itself, so that the data collection costs are manageable; (ii) the policy update is decoupled from the environment, ensuring stable and efficient optimization. Empirical evaluations show that our framework endows Ovis2.5-9B with powerful GUI interaction capabilities, achieving remarkable performances on GUI navigation and grounding benchmarks and even surpassing contenders with larger scales.

</details>


### [206] [HyFunc: Accelerating LLM-based Function Calls for Agentic AI through Hybrid-Model Cascade and Dynamic Templating](https://arxiv.org/abs/2602.13665)
*Weibin Liao,Jian-guang Lou,Haoyi Xiong*

Main category: cs.AI

TL;DR: HyFunc is a framework that reduces LLM inference latency in agentic AI by eliminating three redundancies: processing all function descriptions, using large models for predictable sequences, and generating boilerplate syntax.


<details>
  <summary>Details</summary>
Motivation: Current agentic AI systems using LLMs for function calling suffer from computational redundancy leading to high inference latency, which hinders real-time applications. Three key inefficiencies need addressing: redundant function description processing, redundant use of large models for predictable sequences, and redundant generation of boilerplate syntax.

Method: HyFunc uses a hybrid-model cascade: a large model distills user intent into a single "soft token" that guides a lightweight retriever to select relevant functions and directs a smaller, prefix-tuned model to generate the final call. It also employs "dynamic templating" to inject boilerplate parameter syntax on-the-fly within an extended vLLM engine.

Result: On the unseen BFCL benchmark, HyFunc achieves 0.828 seconds inference latency (outperforming all baselines) and 80.1% performance (surpassing comparable-scale models), demonstrating excellent efficiency-performance balance.

Conclusion: HyFunc offers a more efficient paradigm for agentic AI by systematically eliminating computational redundancies while maintaining strong performance, enabling real-time applications.

Abstract: While agentic AI systems rely on LLMs to translate user intent into structured function calls, this process is fraught with computational redundancy, leading to high inference latency that hinders real-time applications. This paper identifies and addresses three key redundancies: (1) the redundant processing of a large library of function descriptions for every request; (2) the redundant use of a large, slow model to generate an entire, often predictable, token sequence; and (3) the redundant generation of fixed, boilerplate parameter syntax. We introduce HyFunc, a novel framework that systematically eliminates these inefficiencies. HyFunc employs a hybrid-model cascade where a large model distills user intent into a single "soft token." This token guides a lightweight retriever to select relevant functions and directs a smaller, prefix-tuned model to generate the final call, thus avoiding redundant context processing and full-sequence generation by the large model. To eliminate syntactic redundancy, our "dynamic templating" technique injects boilerplate parameter syntax on-the-fly within an extended vLLM engine. To avoid potential limitations in generalization, we evaluate HyFunc on an unseen benchmark dataset, BFCL. Experimental results demonstrate that HyFunc achieves an excellent balance between efficiency and performance. It achieves an inference latency of 0.828 seconds, outperforming all baseline models, and reaches a performance of 80.1%, surpassing all models with a comparable parameter scale. These results suggest that HyFunc offers a more efficient paradigm for agentic AI. Our code is publicly available at https://github.com/MrBlankness/HyFunc.

</details>


### [207] [AllMem: A Memory-centric Recipe for Efficient Long-context Modeling](https://arxiv.org/abs/2602.13680)
*Ziming Wang,Xiang Wang,Kailong Peng,Lang Qin,Juan Gabriel Kostelec,Christos Sourmpis,Axel Laborieux,Qinghai Guo*

Main category: cs.AI

TL;DR: AllMem is a hybrid architecture combining sliding window attention with non-linear test-time training memory networks to enable efficient long-sequence processing in LLMs while mitigating catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: LLMs face performance bottlenecks in long-sequence tasks due to computational complexity and memory overhead of self-attention, plus issues with catastrophic forgetting and representation constraints in linear memory models.

Method: Integrates Sliding Window Attention with non-linear Test-Time Training memory networks, plus Memory-Efficient Fine-Tuning to replace standard attention layers in pre-trained models with memory-augmented sliding window layers.

Result: 4k window model achieves near-lossless performance on 37k LongBench with only 0.83 drop vs full attention; 8k window variant outperforms full attention on InfiniteBench at 128k context, validating effectiveness in mitigating noise and maintaining robust long-range modeling.

Conclusion: AllMem enables efficient scaling to ultra-long contexts while reducing computational and memory footprint, overcoming representation constraints of linear memory models and facilitating transformation of any pre-trained LLM into this architecture.

Abstract: Large Language Models (LLMs) encounter significant performance bottlenecks in long-sequence tasks due to the computational complexity and memory overhead inherent in the self-attention mechanism. To address these challenges, we introduce \textsc{AllMem}, a novel and efficient hybrid architecture that integrates Sliding Window Attention (SWA) with non-linear Test-Time Training (TTT) memory networks. \textsc{AllMem} enables models to effectively scale to ultra-long contexts while mitigating catastrophic forgetting. This approach not only overcomes the representation constraints typical of linear memory models but also significantly reduces the computational and memory footprint during long-sequence inference. Furthermore, we implement a Memory-Efficient Fine-Tuning strategy to replace standard attention layers in pre-trained models with memory-augmented sliding window layers. This framework facilitates the efficient transformation of any off-the-shelf pre-trained LLM into an \textsc{AllMem}-based architecture. Empirical evaluations confirm that our 4k window model achieves near-lossless performance on 37k LongBench with a marginal 0.83 drop compared to full attention. Furthermore, on InfiniteBench at a 128k context, our 8k window variant outperforms full attention, which validates the effectiveness of our parameterized memory in mitigating noise and maintaining robust long-range modeling without the prohibitive costs of global attention.

</details>


### [208] [PhGPO: Pheromone-Guided Policy Optimization for Long-Horizon Tool Planning](https://arxiv.org/abs/2602.13691)
*Yu Li,Guangfeng Cai,Shengtian Yang,Han Luo,Shuo Han,Xu He,Dong Li,Lei Feng*

Main category: cs.AI

TL;DR: PhGPO improves LLM agent tool planning by learning reusable transition patterns from historical successful trajectories, similar to ant colony optimization pheromones.


<details>
  <summary>Details</summary>
Motivation: Long-horizon multi-step tool planning suffers from combinatorial explosion, and current approaches treat successful tool-use paths as immediate rewards without extracting reusable information for future training.

Method: Pheromone-Guided Policy Optimization (PhGPO) learns trajectory-based transition patterns (pheromone) from historical successful trajectories, then uses this learned pheromone to guide policy optimization toward historically successful tool transitions.

Result: Comprehensive experimental results demonstrate the effectiveness of PhGPO in improving long-horizon tool planning for LLM agents.

Conclusion: Historical successful trajectories contain reusable tool-transition patterns that can be leveraged throughout training, and PhGPO effectively extracts and utilizes these patterns to improve LLM agent performance on complex multi-step tool planning tasks.

Abstract: Recent advancements in Large Language Model (LLM) agents have demonstrated strong capabilities in executing complex tasks through tool use. However, long-horizon multi-step tool planning is challenging, because the exploration space suffers from a combinatorial explosion. In this scenario, even when a correct tool-use path is found, it is usually considered an immediate reward for current training, which would not provide any reusable information for subsequent training. In this paper, we argue that historically successful trajectories contain reusable tool-transition patterns, which can be leveraged throughout the whole training process. Inspired by ant colony optimization where historically successful paths can be reflected by the pheromone, we propose Pheromone-Guided Policy Optimization (PhGPO), which learns a trajectory-based transition pattern (i.e., pheromone) from historical trajectories and then uses the learned pheromone to guide policy optimization. This learned pheromone provides explicit and reusable guidance that steers policy optimization toward historically successful tool transitions, thereby improving long-horizon tool planning. Comprehensive experimental results demonstrate the effectiveness of our proposed PhGPO.

</details>


### [209] [Can a Lightweight Automated AI Pipeline Solve Research-Level Mathematical Problems?](https://arxiv.org/abs/2602.13695)
*Lve Meng,Weilong Zhao,Yanzhi Zhang,Haoxiang Guan,Jiyan He*

Main category: cs.AI

TL;DR: LLMs integrated into a streamlined automated pipeline with citation-based verification can solve sophisticated research-grade mathematical problems, achieving verified solutions for competition-level and previously unpublished research questions.


<details>
  <summary>Details</summary>
Motivation: While LLMs have shown success in mathematical proofs and competition benchmarks, their deployment via lightweight natural-language pipelines for actual research problems remains underexplored. The authors aim to demonstrate that next-generation models can solve sophisticated research-grade problems when properly integrated into an optimized pipeline.

Method: Developed a streamlined automated pipeline optimized for citation-based verification, integrating next-generation LLMs (Gemini 3 Pro, GPT-5.2 Pro). Evaluated on two novel datasets: 1) ICCM problem sets (comparable to S.-T. Yau College Student Mathematics Contest) proposed by leading mathematicians, and 2) "First Proof" problem set consisting of previously unpublished research questions.

Result: The pipeline generated candidate proofs for all problems in the first two ICCM sets and the "First Proof" set. Solutions for the first two ICCM sets and Problem 4 of the "First Proof" set were fully verified by the team. All generated proofs were submitted to the official organization and are publicly available.

Conclusion: Next-generation LLMs integrated into optimized automated pipelines with citation-based verification can successfully solve sophisticated research-grade mathematical problems, demonstrating practical deployment potential beyond competition benchmarks. The authors plan to open-source the complete pipeline methodology.

Abstract: Large language models (LLMs) have recently achieved remarkable success in generating rigorous mathematical proofs, with "AI for Math" emerging as a vibrant field of research. While these models have mastered competition-level benchmarks like the International Mathematical Olympiad and show promise in research applications through auto-formalization, their deployment via lightweight, natural-language pipelines for research problems remains underexplored. In this work, we demonstrate that next-generation models (e.g., Gemini 3 Pro, GPT-5.2 Pro), when integrated into a streamlined automated pipeline optimized for citation-based verification, can solve sophisticated research-grade problems. We evaluate our pipeline on two novel datasets: (1) the ICCM problem sets (comparable to the S.-T. Yau College Student Mathematics Contest) proposed by leading mathematicians, and (2) the "First Proof" problem set, consisting of previously unpublished research questions. Our pipeline generated candidate proofs for all problems in the first two ICCM sets and the "First Proof" set. The solutions for the first two ICCM sets and Problem 4 of the "First Proof" set have been fully verified by our team. All generated proofs have been submitted to the official organization, and our generated results are publicly available. We plan to open-source the complete pipeline methodology in due course.

</details>


### [210] [No Need to Train Your RDB Foundation Model](https://arxiv.org/abs/2602.13697)
*Linjie Xu,Yanlin Zhang,Quan Gan,Minjie Wang,David Wipf*

Main category: cs.AI

TL;DR: A novel RDB foundation model that enables in-context learning across multiple relational tables without retraining, using column-wise compression and SQL primitives for zero-shot deployment.


<details>
  <summary>Details</summary>
Motivation: To avoid retraining models for each new prediction task in enterprise relational databases, and to extend in-context learning foundation models from single-table to multi-table relational databases while maintaining zero-shot capability.

Method: Proposes constrained compression within high-dimensional RDB columns (not across columns), uses parameter-free encoders that maintain expressiveness, and implements scalable SQL primitives for encoder stage. Can be paired with existing single-table ICL foundation models without training or fine-tuning.

Result: Developed an open-source RDB foundation model (RDBLearn) capable of robust performance on unseen datasets out-of-the-box, with theoretical and empirical evidence supporting the column-wise compression approach.

Conclusion: A principled family of RDB encoders enables seamless integration with existing ICL foundation models for multi-table relational databases, eliminating the need for retraining while maintaining strong zero-shot performance.

Abstract: Relational databases (RDBs) contain vast amounts of heterogeneous tabular information that can be exploited for predictive modeling purposes. But since the space of potential targets is vast across enterprise settings, how can we \textit{avoid retraining} a new model each time we wish to predict a new quantity of interest? Foundation models based on in-context learning (ICL) offer a convenient option, but so far are largely restricted to single-table operability. In generalizing to multiple interrelated tables, it is essential to compress variably-sized RDB neighborhoods into fixed-length ICL samples for consumption by the decoder. However, the details here are critical: unlike existing supervised learning RDB pipelines, we provide theoretical and empirical evidence that ICL-specific compression should be constrained \emph{within} high-dimensional RDB columns where all entities share units and roles, not \textit{across} columns where the relevance of heterogeneous data types cannot possibly be determined without label information. Conditioned on this restriction, we then demonstrate that encoder expressiveness is actually not compromised by excluding trainable parameters. Hence we arrive at a principled family of RDB encoders that can be seamlessly paired with already-existing single-table ICL foundation models, whereby no training or fine-tuning is required. From a practical standpoint, we develop scalable SQL primitives to implement the encoder stage, resulting in an easy-to-use open-source RDB foundation model\footnote{\label{foot: RDBLearn_learn} https://github.com/HKUSHXLab/rdblearn} capable of robust performance on unseen datasets out of the box.

</details>


### [211] [OneLatent: Single-Token Compression for Visual Latent Reasoning](https://arxiv.org/abs/2602.13738)
*Bo Lv,Yasheng Sun,Junjie Wang,Haoxiang Shi*

Main category: cs.AI

TL;DR: OneLatent compresses chain-of-thought reasoning into a single latent token using supervision from rendered CoT images and OCR hidden states, dramatically reducing output length while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought prompting improves reasoning but significantly increases inference costs (1-2 orders of magnitude). There's a need to reduce output length while preserving reasoning quality.

Method: Render textual reasoning steps into images, use DeepSeek-OCR hidden states as supervision signal, compress intermediate reasoning into a single latent token, enabling deterministic and auditable supervision without verbose textual output.

Result: 11× reduction in average output length with only 2.21% accuracy drop vs textual CoT; 6.8× improvement in output token contribution; 99.80% on ProntoQA and 97.80% on ProsQA with one latent token; up to 87.4× compression.

Conclusion: OneLatent enables efficient reasoning compression with minimal accuracy loss, supporting compression-constrained generalization and making CoT more practical for deployment.

Abstract: Chain-of-thought (CoT) prompting improves reasoning but often increases inference cost by one to two orders of magnitude. To address these challenges, we present \textbf{OneLatent}, a framework that compresses intermediate reasoning into a single latent token via supervision from rendered CoT images and DeepSeek-OCR hidden states. By rendering textual steps into images, we obtain a deterministic supervision signal that can be inspected and audited without requiring the model to output verbose textual rationales. Across benchmarks, OneLatent reduces average output length by $11\times$ with only a $2.21\%$ average accuracy drop relative to textual CoT, while improving output token contribution (OTC) by $6.8\times$. On long-chain logical reasoning, OneLatent reaches $99.80\%$ on ProntoQA and $97.80\%$ on ProsQA with one latent token, with compression up to $87.4\times$, supporting compression-constrained generalization.

</details>


### [212] [OR-Agent: Bridging Evolutionary Search and Structured Research for Automated Algorithm Discovery](https://arxiv.org/abs/2602.13769)
*Qi Liu,Wanjing Ma*

Main category: cs.AI

TL;DR: OR-Agent is a multi-agent research framework for automated scientific discovery that combines evolutionary selection, structured hypothesis management, and hierarchical reflection mechanisms to outperform evolutionary baselines in optimization problems.


<details>
  <summary>Details</summary>
Motivation: Current automated discovery approaches rely too heavily on simple program mutation loops, lacking structured hypothesis management, environment interaction, and principled reflection needed for complex experimental domains.

Method: OR-Agent organizes research as a tree-based workflow with branching hypothesis generation and systematic backtracking. It features: 1) evolutionary-systematic ideation unifying evolutionary selection, research plan generation, and coordinated exploration; 2) hierarchical reflection with short-term verbal gradients, long-term verbal momentum, and memory compression regularization.

Result: OR-Agent outperforms strong evolutionary baselines across classical combinatorial optimization benchmarks (traveling salesman, vehicle routing, bin packing, orienteering, multiple knapsack) and simulation-based cooperative driving scenarios.

Conclusion: OR-Agent provides a general, extensible, and inspectable framework for AI-assisted scientific discovery that moves beyond simple mutation loops to structured hypothesis management with principled reflection mechanisms.

Abstract: Automating scientific discovery in complex, experiment-driven domains requires more than iterative mutation of programs; it demands structured hypothesis management, environment interaction, and principled reflection. We present OR-Agent, a configurable multi-agent research framework designed for automated exploration in rich experimental environments. OR-Agent organizes research as a structured tree-based workflow that explicitly models branching hypothesis generation and systematic backtracking, enabling controlled management of research trajectories beyond simple mutation-crossover loops. At its core, we introduce an evolutionary-systematic ideation mechanism that unifies evolutionary selection of research starting points, comprehensive research plan generation, and coordinated exploration within a research tree. We further propose a hierarchical optimization-inspired reflection system: short-term experimental reflection operates as a form of verbal gradient providing immediate corrective signals; long-term reflection accumulates cross-experiment insights as verbal momentum; and memory compression serves as a regularization mechanism analogous to weight decay, preserving essential signals while mitigating drift. Together, these components form a principled architecture governing research dynamics. We conduct extensive experiments across classical combinatorial optimization benchmarks-including traveling salesman, capacitated vehicle routing, bin packing, orienteering, and multiple knapsack problems-as well as simulation-based cooperative driving scenarios. Results demonstrate that OR-Agent outperforms strong evolutionary baselines while providing a general, extensible, and inspectable framework for AI-assisted scientific discovery. OR-Agent source code and experiments data are publicly available at https://github.com/qiliuchn/OR-Agent.

</details>


### [213] [StackingNet: Collective Inference Across Independent AI Foundation Models](https://arxiv.org/abs/2602.13792)
*Siyang Li,Chenhao Liu,Dongrui Wu,Zhigang Zeng,Lieyun Ding*

Main category: cs.AI

TL;DR: StackingNet is a meta-ensemble framework that coordinates multiple black-box foundation models without accessing their internal parameters or training data, improving accuracy, robustness, and fairness across various AI tasks.


<details>
  <summary>Details</summary>
Motivation: Current AI systems built on large foundation models remain isolated and cannot easily share capabilities. Integrating complementary strengths of independent foundation models is essential for building trustworthy intelligent systems, but there's no established approach for coordinating such black-box heterogeneous models.

Method: StackingNet uses a meta-ensemble framework based on principles of collective intelligence to combine model predictions during inference. It operates without access to internal parameters or training data, enabling reliability ranking and identifying/pruning models that degrade performance.

Result: Across language comprehension, visual estimation, and academic paper rating tasks, StackingNet consistently improves accuracy, robustness, and fairness compared to individual models and classic ensembles. It reduces bias and enables better performance coordination.

Conclusion: StackingNet establishes a practical foundation for coordinated artificial intelligence by turning model diversity from a source of inconsistency into collaboration. Progress may emerge not only from larger single models but also from principled cooperation among specialized models.

Abstract: Artificial intelligence built on large foundation models has transformed language understanding, vision and reasoning, yet these systems remain isolated and cannot readily share their capabilities. Integrating the complementary strengths of such independent foundation models is essential for building trustworthy intelligent systems. Despite rapid progress in individual model design, there is no established approach for coordinating such black-box heterogeneous models. Here we show that coordination can be achieved through a meta-ensemble framework termed StackingNet, which draws on principles of collective intelligence to combine model predictions during inference. StackingNet improves accuracy, reduces bias, enables reliability ranking, and identifies or prunes models that degrade performance, all operating without access to internal parameters or training data. Across tasks involving language comprehension, visual estimation, and academic paper rating, StackingNet consistently improves accuracy, robustness, and fairness, compared with individual models and classic ensembles. By turning diversity from a source of inconsistency into collaboration, StackingNet establishes a practical foundation for coordinated artificial intelligence, suggesting that progress may emerge from not only larger single models but also principled cooperation among many specialized ones.

</details>


### [214] [Attention in Constant Time: Vashista Sparse Attention for Long-Context Decoding with Exponential Guarantees](https://arxiv.org/abs/2602.13804)
*Vashista Nobaub*

Main category: cs.AI

TL;DR: The paper analyzes attention sparsity in LLMs, proves entropic attention concentrates on constant-size active tokens under support gap conditions, and introduces Vashista Sparse Attention for efficient long-context decoding.


<details>
  <summary>Details</summary>
Motivation: LLMs spend most inference cost on attention over long contexts, but empirical evidence suggests only a small subset of tokens meaningfully contributes to each query. This motivates formal analysis of attention sparsity and development of efficient sparse attention mechanisms.

Method: 1) Formalize attention as projection onto convex hull of key vectors with entropic relaxation; 2) Prove face-stability theorem showing entropic attention concentrates on constant-size active face under strict complementarity margin; 3) Introduce Vashista Sparse Attention with paging-style context selection strategy compatible with modern inference stacks.

Result: Theoretical: Attention mass on inactive tokens decays exponentially as exp(-Ω(Δ/ε)), while error on active face scales linearly in temperature ε. Practical: Vashista achieves stable constant-size effective support, strong wall-clock speedups, and minimal quality degradation in regimes predicted by support-gap diagnostics.

Conclusion: The paper provides theoretical foundation for sparse attention, practical criterion for safe sparse decoding, and deployable mechanism (Vashista) that enables predictable latency/cost without external retrieval dependencies, particularly useful for privacy-sensitive and air-gapped settings.

Abstract: Large language models spend most of their inference cost on attention over long contexts, yet empirical behavior suggests that only a small subset of tokens meaningfully contributes to each query. We formalize this phenomenon by modeling attention as a projection onto the convex hull of key vectors and analyzing its entropic (softmax-like) relaxation. Our main theoretical contribution is a face-stability theorem showing that, under a strict complementarity margin (a support gap (Δ) certified by KKT multipliers), entropic attention concentrates on a constant-size active face: the total mass assigned to inactive tokens decays exponentially as (\exp(-Ω(Δ/\varepsilon))), while the error on the active face scales linearly in the temperature/regularization parameter (\varepsilon). This yields a practical criterion for when sparse long-context decoding is safe and provides a principled knob to trade accuracy for compute.
  Building on these guarantees, we introduce Vashista Sparse Attention, a drop-in mechanism that maintains a small candidate set per query through a paging-style context selection strategy compatible with modern inference stacks. Across long-context evaluations, we observe stable constant-size effective support, strong wall-clock speedups, and minimal quality degradation in the regimes predicted by the support-gap diagnostics. Finally, we discuss deployment implications for privacy-sensitive and air-gapped settings, where interchangeable attention modules enable predictable latency and cost without external retrieval dependencies.

</details>


### [215] [An end-to-end agentic pipeline for smart contract translation and quality evaluation](https://arxiv.org/abs/2602.13808)
*Abhinav Goel,Chaitya Shah,Agostino Capponi,Alfio Gliozzo*

Main category: cs.AI

TL;DR: An end-to-end framework for evaluating LLM-generated smart contracts from natural language specs, using structured parsing, code generation, and multi-dimensional quality assessment with automated security checks.


<details>
  <summary>Details</summary>
Motivation: There's a need for systematic evaluation of LLM-generated smart contracts to ensure quality, security, and alignment with natural language specifications, providing reproducible benchmarks for research.

Method: Uses CrewAI-style agent teams with iterative refinement to parse contractual text into structured schemas, generate Solidity code, and perform automated quality assessment through compilation and security checks.

Result: The framework measures quality across five dimensions (functional completeness, variable fidelity, state-machine correctness, business-logic fidelity, code quality) and supports paired evaluation against ground-truth implementations to quantify alignment and identify systematic error modes.

Conclusion: Provides a reproducible benchmark for empirical research on smart contract synthesis quality and supports extensions to formal verification and compliance checking.

Abstract: We present an end-to-end framework for systematic evaluation of LLM-generated smart contracts from natural-language specifications. The system parses contractual text into structured schemas, generates Solidity code, and performs automated quality assessment through compilation and security checks. Using CrewAI-style agent teams with iterative refinement, the pipeline produces structured artifacts with full provenance metadata. Quality is measured across five dimensions, including functional completeness, variable fidelity, state-machine correctness, business-logic fidelity, and code quality aggregated into composite scores. The framework supports paired evaluation against ground-truth implementations, quantifying alignment and identifying systematic error modes such as logic omissions and state transition inconsistencies. This provides a reproducible benchmark for empirical research on smart contract synthesis quality and supports extensions to formal verification and compliance checking.

</details>


### [216] [Experimentation Accelerator: Interpretable Insights and Creative Recommendations for A/B Testing with Content-Aware ranking](https://arxiv.org/abs/2602.13852)
*Zhengmian Hu,Lei Shi,Ritwik Sinha,Justin Grover,David Arbour*

Main category: cs.AI

TL;DR: A unified AI framework for online experimentation that prioritizes test variants, explains why winners win, and generates new high-potential variants using embeddings and historical data.


<details>
  <summary>Details</summary>
Motivation: Online experimentation faces traffic scarcity and manual post-hoc analysis, while organizations underuse historical results and content embeddings that could improve test prioritization and creative iteration.

Method: 1) Train CTR ranking model with treatment embeddings and historical outcomes using fixed effects for contextual shifts; 2) Project treatments onto semantic marketing attributes and use sign-consistent sparse constrained Lasso for interpretability; 3) Compute opportunity index combining attribute importance with under-expression; 4) Use LLMs to translate opportunities into creative suggestions with estimated potential.

Result: The framework has been built into Adobe's "Experimentation Accelerator" product and evaluated on real-world experiments by Adobe business customers, validating the high quality of the generation pipeline.

Conclusion: The proposed framework enables faster, more informative, and more efficient test cycles by providing AI-based insights and opportunities to scale experimentation for customers.

Abstract: Modern online experimentation faces two bottlenecks: scarce traffic forces tough choices on which variants to test, and post-hoc insight extraction is manual, inconsistent, and often content-agnostic. Meanwhile, organizations underuse historical A/B results and rich content embeddings that could guide prioritization and creative iteration. We present a unified framework to (i) prioritize which variants to test, (ii) explain why winners win, and (iii) surface targeted opportunities for new, higher-potential variants. Leveraging treatment embeddings and historical outcomes, we train a CTR ranking model with fixed effects for contextual shifts that scores candidates while balancing value and content diversity. For better interpretability and understanding, we project treatments onto curated semantic marketing attributes and re-express the ranker in this space via a sign-consistent, sparse constrained Lasso, yielding per-attribute coefficients and signed contributions for visual explanations, top-k drivers, and natural-language insights. We then compute an opportunity index combining attribute importance (from the ranker) with under-expression in the current experiment to flag missing, high-impact attributes. Finally, LLMs translate ranked opportunities into concrete creative suggestions and estimate both learning and conversion potential, enabling faster, more informative, and more efficient test cycles. These components have been built into a real Adobe product, called \textit{Experimentation Accelerator}, to provide AI-based insights and opportunities to scale experimentation for customers. We provide an evaluation of the performance of the proposed framework on some real-world experiments by Adobe business customers that validate the high quality of the generation pipeline.

</details>


### [217] [From Fluent to Verifiable: Claim-Level Auditability for Deep Research Agents](https://arxiv.org/abs/2602.13855)
*Razeen A Rasheed,Somnath Banerjee,Animesh Mukherjee,Rima Hazra*

Main category: cs.AI

TL;DR: Paper argues that as AI research generation becomes cheap, auditability becomes the bottleneck, shifting risk from factual errors to weak claim-evidence links. Proposes claim-level auditability as key design target and introduces Auditable Autonomous Research (AAR) standard.


<details>
  <summary>Details</summary>
Motivation: Current deep research agents can produce fluent scientific reports quickly, but verification reveals the real cost is tracing claim-evidence relationships. As research generation becomes cheap, auditability becomes the bottleneck, with dominant risk shifting to scientifically styled outputs with weak, missing, or misleading claim-evidence links.

Method: Proposes claim-level auditability as first-class design/evaluation target, identifies recurring failure modes (objective drift, transient constraints, unverifiable inference), introduces AAR standard with four testable metrics (provenance coverage, provenance soundness, contradiction transparency, audit effort), and advocates for semantic provenance with protocolized validation using persistent, queryable provenance graphs.

Result: Develops a framework for measuring auditability through the AAR standard, which makes auditability testable via specific metrics. Proposes practical instrumentation patterns for semantic provenance with continuous validation during synthesis rather than after publication.

Conclusion: Auditability should be a primary design consideration for deep research agents, with semantic provenance and protocolized validation as key solutions. The AAR standard provides a concrete framework for making auditability measurable and actionable in AI research systems.

Abstract: A deep research agent produces a fluent scientific report in minutes; a careful reader then tries to verify the main claims and discovers the real cost is not reading, but tracing: which sentence is supported by which passage, what was ignored, and where evidence conflicts. We argue that as research generation becomes cheap, auditability becomes the bottleneck, and the dominant risk shifts from isolated factual errors to scientifically styled outputs whose claim-evidence links are weak, missing, or misleading. This perspective proposes claim-level auditability as a first-class design and evaluation target for deep research agents, distills recurring long-horizon failure modes (objective drift, transient constraints, and unverifiable inference), and introduces the Auditable Autonomous Research (AAR) standard, a compact measurement framework that makes auditability testable via provenance coverage, provenance soundness, contradiction transparency, and audit effort. We then argue for semantic provenance with protocolized validation: persistent, queryable provenance graphs that encode claim--evidence relations (including conflicts) and integrate continuous validation during synthesis rather than after publication, with practical instrumentation patterns to support deployment at scale.

</details>


### [218] [Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay](https://arxiv.org/abs/2602.13865)
*Gabriel Romio,Mateus Begnini Melchiades,Bruno Castro da Silva,Gabriel de Oliveira Ramos*

Main category: cs.AI

TL;DR: MOC-2HER improves hierarchical RL for sparse-reward multi-goal tasks by adding dual hindsight goal relabeling for both object manipulation and agent interaction.


<details>
  <summary>Details</summary>
Motivation: Existing HRL methods like Option-Critic and MOC struggle in sparse-reward multi-goal environments, especially in object manipulation tasks where rewards depend on object states rather than direct agent actions.

Method: First extends MOC with standard HER (MOC-HER), then introduces Dual Objectives HER (2HER) that creates two sets of virtual goals: one from object's final state and another from agent's effector positions, rewarding both interaction and task completion.

Result: MOC-2HER achieves up to 90% success rate in robotic manipulation environments, compared to less than 11% for both original MOC and MOC-HER.

Conclusion: The dual objective relabeling strategy effectively addresses sparse reward challenges in multi-goal tasks, particularly for object manipulation where standard HER alone is insufficient.

Abstract: Hierarchical Reinforcement Learning (HRL) frameworks like Option-Critic (OC) and Multi-updates Option Critic (MOC) have introduced significant advancements in learning reusable options. However, these methods underperform in multi-goal environments with sparse rewards, where actions must be linked to temporally distant outcomes. To address this limitation, we first propose MOC-HER, which integrates the Hindsight Experience Replay (HER) mechanism into the MOC framework. By relabeling goals from achieved outcomes, MOC-HER can solve sparse reward environments that are intractable for the original MOC. However, this approach is insufficient for object manipulation tasks, where the reward depends on the object reaching the goal rather than on the agent's direct interaction. This makes it extremely difficult for HRL agents to discover how to interact with these objects. To overcome this issue, we introduce Dual Objectives Hindsight Experience Replay (2HER), a novel extension that creates two sets of virtual goals. In addition to relabeling goals based on the object's final state (standard HER), 2HER also generates goals from the agent's effector positions, rewarding the agent for both interacting with the object and completing the task. Experimental results in robotic manipulation environments show that MOC-2HER achieves success rates of up to 90%, compared to less than 11% for both MOC and MOC-HER. These results highlight the effectiveness of our dual objective relabeling strategy in sparse reward, multi-goal tasks.

</details>


### [219] [Ambient Physics: Training Neural PDE Solvers with Partial Observations](https://arxiv.org/abs/2602.13873)
*Harris Abdul Majid,Giannis Daras,Francesco Tudisco,Steven McDonagh*

Main category: cs.AI

TL;DR: Ambient Physics: A framework for learning joint distributions of PDE coefficient-solution pairs directly from partial observations without needing complete training data, achieving SOTA reconstruction with 62.5% error reduction and 125× fewer function evaluations.


<details>
  <summary>Details</summary>
Motivation: In scientific settings, acquiring complete observations of PDE coefficients and solutions is often expensive, hazardous, or impossible. Existing diffusion-based methods require complete observations for training, which limits their applicability in real-world scenarios where only partial data is available.

Method: The framework randomly masks a subset of already-observed measurements and supervises on them, creating a scenario where the model cannot distinguish between "truly unobserved" and "artificially unobserved" data. This forces the model to produce plausible predictions everywhere. The approach leverages a "one-point transition" phenomenon where masking just one already-observed point enables learning from partial observations.

Result: Ambient Physics achieves state-of-the-art reconstruction performance with a 62.51% reduction in average overall error compared to prior diffusion-based methods, while using 125× fewer function evaluations. The method works across various architectures and measurement patterns.

Conclusion: Ambient Physics enables scientific progress in settings where complete observations are unavailable by learning directly from partial observations, overcoming the limitation of existing methods that require complete training data. The framework's efficiency and effectiveness make it practical for real-world scientific applications.

Abstract: In many scientific settings, acquiring complete observations of PDE coefficients and solutions can be expensive, hazardous, or impossible. Recent diffusion-based methods can reconstruct fields given partial observations, but require complete observations for training. We introduce Ambient Physics, a framework for learning the joint distribution of coefficient-solution pairs directly from partial observations, without requiring a single complete observation. The key idea is to randomly mask a subset of already-observed measurements and supervise on them, so the model cannot distinguish "truly unobserved" from "artificially unobserved", and must produce plausible predictions everywhere. Ambient Physics achieves state-of-the-art reconstruction performance. Compared with prior diffusion-based methods, it achieves a 62.51$\%$ reduction in average overall error while using 125$\times$ fewer function evaluations. We also identify a "one-point transition": masking a single already-observed point enables learning from partial observations across architectures and measurement patterns. Ambient Physics thus enables scientific progress in settings where complete observations are unavailable.

</details>


### [220] [VSAL: A Vision Solver with Adaptive Layouts for Graph Property Detection](https://arxiv.org/abs/2602.13880)
*Jiahao Xie,Guangmo Tong*

Main category: cs.AI

TL;DR: VSAL is a vision-based framework that uses adaptive graph layouts to improve graph property detection, outperforming existing methods on tasks like Hamiltonian cycle and planarity detection.


<details>
  <summary>Details</summary>
Motivation: Existing vision-based graph property detection methods rely on fixed visual layouts, limiting their expressiveness and performance. The authors aim to overcome this limitation by creating a framework that can adaptively generate informative graph visualizations tailored to individual graph instances.

Method: VSAL (Vision-based framework with adaptive layout generator) incorporates a dynamic layout generator that produces informative graph visualizations tailored to each specific graph instance, rather than using fixed layouts. This adaptive approach enhances the visual representation for better property detection.

Result: Extensive experiments show VSAL outperforms state-of-the-art vision-based methods on various graph property detection tasks including Hamiltonian cycle detection, planarity detection, claw-freeness detection, and tree detection.

Conclusion: Adaptive graph visualization layouts significantly improve vision-based graph property detection performance compared to fixed-layout approaches, demonstrating the importance of tailored visual representations for different graph instances.

Abstract: Graph property detection aims to determine whether a graph exhibits certain structural properties, such as being Hamiltonian. Recently, learning-based approaches have shown great promise by leveraging data-driven models to detect graph properties efficiently. In particular, vision-based methods offer a visually intuitive solution by processing the visualizations of graphs. However, existing vision-based methods rely on fixed visual graph layouts, and therefore, the expressiveness of their pipeline is restricted. To overcome this limitation, we propose VSAL, a vision-based framework that incorporates an adaptive layout generator capable of dynamically producing informative graph visualizations tailored to individual instances, thereby improving graph property detection. Extensive experiments demonstrate that VSAL outperforms state-of-the-art vision-based methods on various tasks such as Hamiltonian cycle, planarity, claw-freeness, and tree detection.

</details>


### [221] [Diagnosing Pathological Chain-of-Thought in Reasoning Models](https://arxiv.org/abs/2602.13904)
*Manqing Liu,David Williams-King,Ida Caspary,Linh Le,Hannes Whittingham,Puria Radmard,Cameron Tice,Edward James Young*

Main category: cs.AI

TL;DR: The paper identifies three CoT reasoning pathologies (post-hoc rationalization, encoded reasoning, internalized reasoning), develops metrics to detect them, and validates using deliberately trained pathological models.


<details>
  <summary>Details</summary>
Motivation: CoT reasoning is fundamental to modern LLMs and critical for AI safety, but it may exhibit pathologies that prevent it from being useful for monitoring. Existing work has identified three distinct failure modes that need better understanding and detection.

Method: Created a set of concrete metrics that are simple to implement, computationally inexpensive, and task-agnostic to detect CoT pathologies. Developed model organisms deliberately trained to exhibit specific CoT pathologies for validation.

Result: Developed practical toolkit for assessing CoT pathologies with direct implications for training-time monitoring. The metrics can discriminate between the three identified pathologies.

Conclusion: The work provides a practical approach to understanding and detecting CoT reasoning pathologies, offering tools that can be applied during training to improve AI safety monitoring.

Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three distinct pathologies: post-hoc rationalization, where models generate plausible explanations backwards from predetermined answers; encoded reasoning, where intermediate steps conceal information within seemingly interpretable text; and internalized reasoning, where models replace explicit reasoning with meaningless filler tokens while computing internally. To better understand and discriminate between these pathologies, we create a set of concrete metrics that are simple to implement, computationally inexpensive, and task-agnostic. To validate our approach, we develop model organisms deliberately trained to exhibit specific CoT pathologies. Our work provides a practical toolkit for assessing CoT pathologies, with direct implications for training-time monitoring.

</details>


### [222] [From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design](https://arxiv.org/abs/2602.13912)
*Sha Li,Stefano Petrangeli,Yu Shen,Xiang Chen*

Main category: cs.AI

TL;DR: LaySPA is a reinforcement learning framework that enhances LLMs with explicit spatial reasoning for graphic layout design, producing interpretable reasoning traces and structured layouts while improving quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addresses LLMs' limited spatial reasoning capabilities and the lack of transparency in design decision-making for graphic layout generation.

Method: Reformulates layout design as policy learning over a structured textual spatial environment encoding canvas geometry, element attributes, and relationships. Uses multi-objective spatial critique (geometric validity, relational coherence, aesthetic consistency) and relative group optimization for training.

Result: LaySPA improves structural validity and visual quality, outperforms larger proprietary LLMs, achieves performance comparable to specialized SOTA layout generators, requires fewer annotated samples, and reduces latency.

Conclusion: LaySPA successfully equips LLMs with explicit spatial reasoning for transparent and controllable layout design, demonstrating superior performance and efficiency compared to existing approaches.

Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision making. Instead of operating at the pixel level, we reformulate layout design as a policy learning problem over a structured textual spatial environment that explicitly encodes canvas geometry, element attributes, and inter-element relationships. LaySPA produces dual-level outputs comprising interpretable reasoning traces and structured layout specifications, enabling transparent and controllable design decision making. Layout design policy is optimized via a multi-objective spatial critique that decomposes layout quality into geometric validity, relational coherence, and aesthetic consistency, and is trained using relative group optimization to stabilize learning in open-ended design spaces. Experiments demonstrate that LaySPA improves structural validity and visual quality, outperforming larger proprietary LLMs and achieving performance comparable to specialized SOTA layout generators while requiring fewer annotated samples and reduced latency.

</details>


### [223] [HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling](https://arxiv.org/abs/2602.13933)
*Xiaochen Zhao,Kaikai Wang,Xiaowen Zhang,Chen Yao,Aili Wang*

Main category: cs.AI

TL;DR: HyMem is a hybrid memory architecture for LLM agents that uses multi-granular memory representations and dynamic scheduling to balance efficiency and effectiveness in long dialogues, reducing computational cost by 92.6% while maintaining strong performance.


<details>
  <summary>Details</summary>
Motivation: LLM agents perform well in short-text contexts but struggle in extended dialogues due to inefficient memory management. Existing approaches face a trade-off between efficiency (memory compression loses critical details) and effectiveness (retaining raw text causes computational overhead). Current monolithic memory representations and static retrieval mechanisms fail to emulate human-like flexible memory scheduling.

Method: HyMem uses a hybrid memory architecture inspired by cognitive economy principles. It features: 1) Dual-granular storage scheme with multi-granular memory representations, 2) Dynamic two-tier retrieval system with a lightweight module for summary-level context and an LLM-based deep module selectively activated for complex queries, 3) Reflection mechanism for iterative reasoning refinement.

Result: HyMem achieves strong performance on both LOCOMO and LongMemEval benchmarks, outperforming full-context approaches while reducing computational cost by 92.6%. It establishes state-of-the-art balance between efficiency and performance in long-term memory management.

Conclusion: HyMem successfully addresses the efficiency-effectiveness trade-off in LLM memory management through its hybrid architecture with dynamic scheduling, demonstrating that multi-granular memory representations and selective activation mechanisms can significantly improve long-dialogue performance while dramatically reducing computational costs.

Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical details required for complex reasoning, while retaining raw text introduces unnecessary computational overhead for simple queries. The crux lies in the limitations of monolithic memory representations and static retrieval mechanisms, which fail to emulate the flexible and proactive memory scheduling capabilities observed in humans, thus struggling to adapt to diverse problem scenarios. Inspired by the principle of cognitive economy, we propose HyMem, a hybrid memory architecture that enables dynamic on-demand scheduling through multi-granular memory representations. HyMem adopts a dual-granular storage scheme paired with a dynamic two-tier retrieval system: a lightweight module constructs summary-level context for efficient response generation, while an LLM-based deep module is selectively activated only for complex queries, augmented by a reflection mechanism for iterative reasoning refinement. Experiments show that HyMem achieves strong performance on both the LOCOMO and LongMemEval benchmarks, outperforming full-context while reducing computational cost by 92.6\%, establishing a state-of-the-art balance between efficiency and performance in long-term memory management.

</details>


### [224] [Statistical Early Stopping for Reasoning Models](https://arxiv.org/abs/2602.13935)
*Yangxinyu Xie,Tao Wang,Soham Mallick,Yan Sun,Georgy Noarov,Mengxin Yu,Tanwi Mallick,Weijie J. Su,Edgar Dobriban*

Main category: cs.AI

TL;DR: The paper introduces statistically principled early stopping methods for LLM reasoning that monitor uncertainty signals during generation to prevent overthinking on ambiguous queries.


<details>
  <summary>Details</summary>
Motivation: LLMs often overthink and generate unnecessary reasoning steps, especially when faced with uncertainty, ill-posed, or ambiguous queries, which reduces efficiency and reliability.

Method: Two approaches: 1) Parametric method models inter-arrival times of uncertainty keywords as a renewal process with sequential testing; 2) Nonparametric method provides finite-sample guarantees on probability of halting too early on well-posed queries.

Result: Empirical evaluations across reasoning tasks and models show uncertainty-aware early stopping improves both efficiency and reliability, with especially significant gains for math reasoning.

Conclusion: Statistically principled early stopping based on uncertainty monitoring can effectively mitigate LLM overthinking, enhancing reasoning efficiency and reliability across domains.

Abstract: While LLMs have seen substantial improvement in reasoning capabilities, they also sometimes overthink, generating unnecessary reasoning steps, particularly under uncertainty, given ill-posed or ambiguous queries. We introduce statistically principled early stopping methods that monitor uncertainty signals during generation to mitigate this issue. Our first approach is parametric: it models inter-arrival times of uncertainty keywords as a renewal process and applies sequential testing for stopping. Our second approach is nonparametric and provides finite-sample guarantees on the probability of halting too early on well-posed queries. We conduct empirical evaluations on reasoning tasks across several domains and models. Our results indicate that uncertainty-aware early stopping can improve both efficiency and reliability in LLM reasoning, and we observe especially significant gains for math reasoning.

</details>


### [225] [A Generalizable Physics-guided Causal Model for Trajectory Prediction in Autonomous Driving](https://arxiv.org/abs/2602.13936)
*Zhenyu Zong,Yuchen Wang,Haohong Lin,Lu Gan,Huajie Shao*

Main category: cs.AI

TL;DR: PCM: A physics-guided causal model for zero-shot trajectory prediction in autonomous driving using domain-invariant scene features and kinematic integration.


<details>
  <summary>Details</summary>
Motivation: Achieving effective zero-shot generalization for trajectory prediction in unseen domains is challenging; leveraging domain-invariant kinematic knowledge can enhance prediction capabilities across diverse environments.

Method: Proposes Physics-guided Causal Model (PCM) with: 1) Disentangled Scene Encoder using intervention-based disentanglement to extract domain-invariant features, and 2) CausalODE Decoder with causal attention to integrate kinematic models with contextual information.

Result: Extensive experiments on real-world autonomous driving datasets show superior zero-shot generalization performance in unseen cities, significantly outperforming competitive baselines.

Conclusion: The PCM framework effectively addresses zero-shot generalization challenges by incorporating domain-invariant knowledge through causal modeling and physics guidance, enabling robust trajectory prediction across unseen domains.

Abstract: Trajectory prediction for traffic agents is critical for safe autonomous driving. However, achieving effective zero-shot generalization in previously unseen domains remains a significant challenge. Motivated by the consistent nature of kinematics across diverse domains, we aim to incorporate domain-invariant knowledge to enhance zero-shot trajectory prediction capabilities. The key challenges include: 1) effectively extracting domain-invariant scene representations, and 2) integrating invariant features with kinematic models to enable generalized predictions. To address these challenges, we propose a novel generalizable Physics-guided Causal Model (PCM), which comprises two core components: a Disentangled Scene Encoder, which adopts intervention-based disentanglement to extract domain-invariant features from scenes, and a CausalODE Decoder, which employs a causal attention mechanism to effectively integrate kinematic models with meaningful contextual information. Extensive experiments on real-world autonomous driving datasets demonstrate our method's superior zero-shot generalization performance in unseen cities, significantly outperforming competitive baselines. The source code is released at https://github.com/ZY-Zong/Physics-guided-Causal-Model.

</details>


### [226] [Neuromem: A Granular Decomposition of the Streaming Lifecycle in External Memory for LLMs](https://arxiv.org/abs/2602.13967)
*Ruicheng Zhang,Xinyi Li,Tianyi Xu,Shuhao Zhang,Xiaofei Liao,Hai Jin*

Main category: cs.AI

TL;DR: Neuromem is a testbed for evaluating External Memory Modules under streaming conditions with interleaved insertions and retrievals, analyzing performance across five lifecycle dimensions.


<details>
  <summary>Details</summary>
Motivation: Most evaluations of External Memory Modules assume static settings where memory is built offline and queried at fixed states, but in practice memory is streaming with continuous fact arrivals and evolving states during query serving.

Method: Neuromem is a scalable testbed that benchmarks External Memory Modules under an interleaved insertion-and-retrieval protocol, decomposing the memory lifecycle into five dimensions: memory data structure, normalization strategy, consolidation policy, query formulation strategy, and context integration mechanism.

Result: Performance typically degrades as memory grows across rounds, time-related queries remain most challenging, memory data structure determines attainable quality frontier, and aggressive compression/generative integration mostly shift cost between insertion and retrieval with limited accuracy gain.

Conclusion: The paper presents Neuromem as a comprehensive evaluation framework for streaming memory systems, highlighting the importance of considering the full memory lifecycle and identifying key factors that affect performance in practical deployment scenarios.

Abstract: Most evaluations of External Memory Module assume a static setting: memory is built offline and queried at a fixed state. In practice, memory is streaming: new facts arrive continuously, insertions interleave with retrievals, and the memory state evolves while the model is serving queries. In this regime, accuracy and cost are governed by the full memory lifecycle, which encompasses the ingestion, maintenance, retrieval, and integration of information into generation. We present Neuromem, a scalable testbed that benchmarks External Memory Modules under an interleaved insertion-and-retrieval protocol and decomposes its lifecycle into five dimensions including memory data structure, normalization strategy, consolidation policy, query formulation strategy, and context integration mechanism. Using three representative datasets LOCOMO, LONGMEMEVAL, and MEMORYAGENTBENCH, Neuromem evaluates interchangeable variants within a shared serving stack, reporting token-level F1 and insertion/retrieval latency. Overall, we observe that performance typically degrades as memory grows across rounds, and time-related queries remain the most challenging category. The memory data structure largely determines the attainable quality frontier, while aggressive compression and generative integration mechanisms mostly shift cost between insertion and retrieval with limited accuracy gain.

</details>


### [227] [Cognitive Chunking for Soft Prompts: Accelerating Compressor Learning via Block-wise Causal Masking](https://arxiv.org/abs/2602.13980)
*Guojie Liu,Yiqi Wang,Yanfeng Yang,Wenqi Fan,Songlei Jian,Jianfeng Zhang,Jie Yu*

Main category: cs.AI

TL;DR: PIC (Parallelized Iterative Compression) improves context compression for LLMs by restricting memory tokens to local chunks, reducing training difficulty and achieving better performance especially at high compression ratios.


<details>
  <summary>Details</summary>
Motivation: Long contexts increase LLM inference latency due to quadratic self-attention costs. Existing soft prompt compression methods compress entire contexts indiscriminately, requiring global dependency capture and extensive training data.

Method: PIC modifies Transformer's attention mask to restrict memory tokens' receptive field to sequential local chunks, inspired by human working memory chunking and spatial specialization observations.

Result: PIC outperforms baselines across multiple tasks, especially at high compression ratios (29.8% F1 and 40.7% EM improvements at 64× compression). Reduces training time by ~40% while surpassing baseline peak performance.

Conclusion: PIC provides an effective context compression approach that simplifies compressor training through local chunk restrictions, achieving superior performance with reduced computational requirements.

Abstract: Providing extensive context via prompting is vital for leveraging the capabilities of Large Language Models (LLMs). However, lengthy contexts significantly increase inference latency, as the computational cost of self-attention grows quadratically with sequence length. To mitigate this issue, context compression-particularly soft prompt compressio-has emerged as a widely studied solution, which converts long contexts into shorter memory embeddings via a trained compressor. Existing methods typically compress the entire context indiscriminately into a set of memory tokens, requiring the compressor to capture global dependencies and necessitating extensive pre-training data to learn effective patterns. Inspired by the chunking mechanism in human working memory and empirical observations of the spatial specialization of memory embeddings relative to original tokens, we propose Parallelized Iterative Compression (PIC). By simply modifying the Transformer's attention mask, PIC explicitly restricts the receptive field of memory tokens to sequential local chunks, thereby lowering the difficulty of compressor training. Experiments across multiple downstream tasks demonstrate that PIC consistently outperforms competitive baselines, with superiority being particularly pronounced in high compression scenarios (e.g., achieving relative improvements of 29.8\% in F1 score and 40.7\% in EM score on QA tasks at the $64\times$ compression ratio). Furthermore, PIC significantly expedites the training process. Specifically, when training the 16$\times$ compressor, it surpasses the peak performance of the competitive baseline while effectively reducing the training time by approximately 40\%.

</details>


### [228] [Bridging AI and Clinical Reasoning: Abductive Explanations for Alignment on Critical Symptoms](https://arxiv.org/abs/2602.13985)
*Belona Sonna,Alban Grastien*

Main category: cs.AI

TL;DR: AI clinical diagnostics need better interpretability; formal abductive explanations provide guaranteed reasoning alignment with clinical frameworks while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: AI in clinical diagnostics often diverges from structured clinical reasoning, limiting trust and adoption. Critical symptoms may be overlooked even when predictions are correct, and existing explanation methods lack transparency and formal guarantees.

Method: Leverage formal abductive explanations that provide consistent, guaranteed reasoning over minimal sufficient feature sets to align AI decision-making with clinical reasoning frameworks.

Result: The approach preserves predictive accuracy while providing clinically actionable insights, enabling clear understanding of AI decision-making aligned with clinical reasoning.

Conclusion: Formal abductive explanations establish a robust framework for trustworthy AI in medical diagnosis by ensuring interpretability and alignment with clinical reasoning while maintaining accuracy.

Abstract: Artificial intelligence (AI) has demonstrated strong potential in clinical diagnostics, often achieving accuracy comparable to or exceeding that of human experts. A key challenge, however, is that AI reasoning frequently diverges from structured clinical frameworks, limiting trust, interpretability, and adoption. Critical symptoms, pivotal for rapid and accurate decision-making, may be overlooked by AI models even when predictions are correct. Existing post hoc explanation methods provide limited transparency and lack formal guarantees. To address this, we leverage formal abductive explanations, which offer consistent, guaranteed reasoning over minimal sufficient feature sets. This enables a clear understanding of AI decision-making and allows alignment with clinical reasoning. Our approach preserves predictive accuracy while providing clinically actionable insights, establishing a robust framework for trustworthy AI in medical diagnosis.

</details>


### [229] [Prompt-Driven Low-Altitude Edge Intelligence: Modular Agents and Generative Reasoning](https://arxiv.org/abs/2602.14003)
*Jiahao You,Ziye Jia,Chao Dong,Qihui Wu*

Main category: cs.AI

TL;DR: P2AECF is a prompt-to-agent edge cognition framework that enables flexible, efficient, and adaptive deployment of large AI models at the edge by converting semantic prompts into executable reasoning workflows through three key mechanisms.


<details>
  <summary>Details</summary>
Motivation: Current deployment of large AI models at the edge faces three limitations: 1) rigid task-model binding limiting flexibility, 2) computational/memory demands exceeding edge device capacity, and 3) static inference pipelines unable to adapt to real-time task changes.

Method: P2AECF transforms high-level semantic prompts into executable reasoning workflows through three mechanisms: 1) prompt-defined cognition (parsing task intent into abstract representations), 2) agent-based modular execution (using lightweight reusable agents selected based on resources), and 3) diffusion-controlled inference planning (adaptively constructing execution strategies with runtime feedback).

Result: The framework is illustrated through a low-altitude intelligent network use case, demonstrating its ability to deliver adaptive, modular, and scalable edge intelligence for real-time low-altitude aerial collaborations.

Conclusion: P2AECF addresses key limitations in edge AI deployment by enabling flexible, efficient, and adaptive edge intelligence through prompt-to-agent transformation, making large AI models more practical for edge computing scenarios like aerial collaborations.

Abstract: The large artificial intelligence models (LAMs) show strong capabilities in perception, reasoning, and multi-modal understanding, and can enable advanced capabilities in low-altitude edge intelligence. However, the deployment of LAMs at the edge remains constrained by some fundamental limitations. First, tasks are rigidly tied to specific models, limiting the flexibility. Besides, the computational and memory demands of full-scale LAMs exceed the capacity of most edge devices. Moreover, the current inference pipelines are typically static, making it difficult to respond to real-time changes of tasks. To address these challenges, we propose a prompt-to-agent edge cognition framework (P2AECF), enabling the flexible, efficient, and adaptive edge intelligence. Specifically, P2AECF transforms high-level semantic prompts into executable reasoning workflows through three key mechanisms. First, the prompt-defined cognition parses task intent into abstract and model-agnostic representations. Second, the agent-based modular execution instantiates these tasks using lightweight and reusable cognitive agents dynamically selected based on current resource conditions. Third, the diffusion-controlled inference planning adaptively constructs and refines execution strategies by incorporating runtime feedback and system context. In addition, we illustrate the framework through a representative low-altitude intelligent network use case, showing its ability to deliver adaptive, modular, and scalable edge intelligence for real-time low-altitude aerial collaborations.

</details>


### [230] [GRAIL: Goal Recognition Alignment through Imitation Learning](https://arxiv.org/abs/2602.14252)
*Osher Elhadad,Felipe Meneguzzi,Reuth Mirsky*

Main category: cs.AI

TL;DR: GRAIL uses imitation learning to learn goal-directed policies from demonstrations, enabling accurate goal recognition even with suboptimal or biased behavior.


<details>
  <summary>Details</summary>
Motivation: Existing goal recognition methods assume optimal goal-oriented policies, but real agents often exhibit suboptimal or systematically biased behavior, leading to inaccurate goal inference.

Method: GRAIL combines imitation learning and inverse reinforcement learning to learn one goal-directed policy per candidate goal directly from demonstration trajectories, then scores partial trajectories with these policies in a single forward pass.

Result: GRAIL significantly improves F1-scores: +0.5+ under systematically biased optimal behavior, +0.1-0.3 under suboptimal behavior, +0.4 under noisy optimal trajectories, while remaining competitive in fully optimal settings.

Conclusion: GRAIL enables scalable and robust goal recognition that works well across various behavioral patterns (optimal, suboptimal, biased, noisy), advancing models for interpreting agent goals in uncertain environments.

Abstract: Understanding an agent's goals from its behavior is fundamental to aligning AI systems with human intentions. Existing goal recognition methods typically rely on an optimal goal-oriented policy representation, which may differ from the actor's true behavior and hinder the accurate recognition of their goal. To address this gap, this paper introduces Goal Recognition Alignment through Imitation Learning (GRAIL), which leverages imitation learning and inverse reinforcement learning to learn one goal-directed policy for each candidate goal directly from (potentially suboptimal) demonstration trajectories. By scoring an observed partial trajectory with each learned goal-directed policy in a single forward pass, GRAIL retains the one-shot inference capability of classical goal recognition while leveraging learned policies that can capture suboptimal and systematically biased behavior. Across the evaluated domains, GRAIL increases the F1-score by more than 0.5 under systematically biased optimal behavior, achieves gains of approximately 0.1-0.3 under suboptimal behavior, and yields improvements of up to 0.4 under noisy optimal trajectories, while remaining competitive in fully optimal settings. This work contributes toward scalable and robust models for interpreting agent goals in uncertain environments.

</details>


### [231] [FloCA: Towards Faithful and Logically Consistent Flowchart Reasoning](https://arxiv.org/abs/2602.14035)
*Jinzi Zou,Bolin Wang,Liang Li,Shuo Zhang,Nuo Xu,Junzhou Zhao*

Main category: cs.AI

TL;DR: FloCA is a zero-shot flowchart-oriented dialogue agent that uses LLMs for intent understanding and response generation while delegating flowchart reasoning to an external tool for faithful, topology-constrained graph execution.


<details>
  <summary>Details</summary>
Motivation: Flowchart-oriented dialogue systems need to guide users through multi-turn decision-making using domain-specific flowcharts, but current LLMs lack explicit mechanisms for flowchart topology representation and reasoning, and are prone to hallucinations leading to unfaithful flowchart reasoning.

Method: FloCA uses a hybrid approach: LLMs handle intent understanding and response generation, while an external tool performs topology-constrained graph execution for flowchart reasoning, ensuring faithful and logically consistent node transitions across dialogue turns.

Result: Extensive experiments on FLODIAL and PFDial datasets show FloCA's superiority over existing LLM-based methods, highlighting the bottlenecks of current approaches and demonstrating improved performance.

Conclusion: FloCA effectively addresses LLM limitations in flowchart reasoning by separating intent understanding from topology-constrained graph execution, providing a robust framework for faithful flowchart-oriented dialogue systems with comprehensive evaluation metrics.

Abstract: Flowchart-oriented dialogue (FOD) systems aim to guide users through multi-turn decision-making or operational procedures by following a domain-specific flowchart to achieve a task goal. In this work, we formalize flowchart reasoning in FOD as grounding user input to flowchart nodes at each dialogue turn while ensuring node transition is consistent with the correct flowchart path. Despite recent advances of LLMs in task-oriented dialogue systems, adapting them to FOD still faces two limitations: (1) LLMs lack an explicit mechanism to represent and reason over flowchart topology, and (2) they are prone to hallucinations, leading to unfaithful flowchart reasoning. To address these limitations, we propose FloCA, a zero-shot flowchart-oriented conversational agent. FloCA uses an LLM for intent understanding and response generation while delegating flowchart reasoning to an external tool that performs topology-constrained graph execution, ensuring faithful and logically consistent node transitions across dialogue turns. We further introduce an evaluation framework with an LLM-based user simulator and five new metrics covering reasoning accuracy and interaction efficiency. Extensive experiments on FLODIAL and PFDial datasets highlight the bottlenecks of existing LLM-based methods and demonstrate the superiority of FloCA. Our codes are available at https://github.com/Jinzi-Zou/FloCA-flowchart-reasoning.

</details>


### [232] [Choosing How to Remember: Adaptive Memory Structures for LLM Agents](https://arxiv.org/abs/2602.14038)
*Mingfei Lu,Mengjia Wu,Feng Liu,Jiawei Xu,Weikai Li,Haoyang Wang,Zhengdong Hu,Ying Ding,Yizhou Sun,Jie Lu,Yi Zhang*

Main category: cs.AI

TL;DR: FluxMem: A unified framework for adaptive memory organization in LLM agents that learns to select optimal memory structures based on interaction context, achieving 9.18% and 6.14% improvements on long-horizon benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing agent memory systems have two key limitations: they use one-size-fits-all memory structures and don't adaptively select memory structures based on context, which limits their ability to handle heterogeneous interaction patterns and results in suboptimal performance.

Method: Proposes FluxMem framework with multiple complementary memory structures, learns to select among them using offline supervision from downstream response quality and memory utilization. Introduces three-level memory hierarchy and Beta Mixture Model-based probabilistic gate for distribution-aware memory fusion instead of brittle similarity thresholds.

Result: Experiments on two long-horizon benchmarks (PERSONAMEM and LoCoMo) show average improvements of 9.18% and 6.14% respectively compared to existing methods.

Conclusion: FluxMem enables adaptive memory organization for LLM agents, addressing limitations of existing memory systems by learning context-adaptive memory structure selection and providing robust long-horizon memory evolution through hierarchical organization and probabilistic fusion.

Abstract: Memory is critical for enabling large language model (LLM) based agents to maintain coherent behavior over long-horizon interactions. However, existing agent memory systems suffer from two key gaps: they rely on a one-size-fits-all memory structure and do not model memory structure selection as a context-adaptive decision, limiting their ability to handle heterogeneous interaction patterns and resulting in suboptimal performance. We propose a unified framework, FluxMem, that enables adaptive memory organization for LLM agents. Our framework equips agents with multiple complementary memory structures. It explicitly learns to select among these structures based on interaction-level features, using offline supervision derived from downstream response quality and memory utilization. To support robust long-horizon memory evolution, we further introduce a three-level memory hierarchy and a Beta Mixture Model-based probabilistic gate for distribution-aware memory fusion, replacing brittle similarity thresholds. Experiments on two long-horizon benchmarks, PERSONAMEM and LoCoMo, demonstrate that our method achieves average improvements of 9.18% and 6.14%.

</details>


### [233] [REAL: Resolving Knowledge Conflicts in Knowledge-Intensive Visual Question Answering via Reasoning-Pivot Alignment](https://arxiv.org/abs/2602.14065)
*Kai Ye,Xianwei Mao,Sheng Zhou,Zirui Shao,Ye Mo,Liangliang Liu,Haikuan Huang,Bin Li,Jiajun Bu*

Main category: cs.AI

TL;DR: REAL framework uses reasoning-pivots to detect and resolve knowledge conflicts in visual question answering, achieving state-of-the-art performance through pivot-aware training and guided decoding.


<details>
  <summary>Details</summary>
Motivation: Knowledge-intensive VQA suffers from severe knowledge conflicts due to limitations of open-domain retrieval, and existing approaches lack generalizable conflict detection and intra-model constraint mechanisms to handle conflicting evidence.

Method: Proposes REAL framework with Reasoning-Pivot concept - atomic units in reasoning chains that emphasize knowledge linkage. Uses Reasoning-Pivot Aware SFT (RPA-SFT) to train a generalizable discriminator by aligning conflicts with pivot extraction, and Reasoning-Pivot Guided Decoding (RPGD) for intra-model conflict mitigation.

Result: Extensive experiments across diverse benchmarks demonstrate that REAL significantly enhances discrimination accuracy and achieves state-of-the-art performance.

Conclusion: The pivot-driven resolution paradigm effectively addresses knowledge conflicts in KI-VQA, validating the effectiveness of reasoning-pivot based approach for conflict detection and mitigation.

Abstract: Knowledge-intensive Visual Question Answering (KI-VQA) frequently suffers from severe knowledge conflicts caused by the inherent limitations of open-domain retrieval. However, existing paradigms face critical limitations due to the lack of generalizable conflict detection and intra-model constraint mechanisms to handle conflicting evidence. To address these challenges, we propose the REAL (Reasoning-Pivot Alignment) framework centered on the novel concept of the Reasoning-Pivot. Distinct from reasoning steps that prioritize internal self-derivation, a reasoning-pivot serves as an atomic unit (node or edge) in the reasoning chain that emphasizes knowledge linkage, and it typically relies on external evidence to complete the reasoning. Supported by our constructed REAL-VQA dataset, our approach integrates Reasoning-Pivot Aware SFT (RPA-SFT) to train a generalizable discriminator by aligning conflicts with pivot extraction, and employs Reasoning-Pivot Guided Decoding (RPGD), an intra-model decoding strategy that leverages these pivots for targeted conflict mitigation. Extensive experiments across diverse benchmarks demonstrate that REAL significantly enhances discrimination accuracy and achieves state-of-the-art performance, validating the effectiveness of our pivot-driven resolution paradigm.

</details>


### [234] [Plan-MCTS: Plan Exploration for Action Exploitation in Web Navigation](https://arxiv.org/abs/2602.14083)
*Weiming Zhang,Jihong Wang,Jiamu Zhou,Qingyao Li,Xinbei Ma,Congmin Zheng,Xingyu Lou,Weiwen Liu,Zhuosheng Zhang,Jun Wang,Yong Yu,Weinan Zhang*

Main category: cs.AI

TL;DR: Plan-MCTS: A framework that shifts web navigation exploration to semantic Plan Space, using dense plan trees and abstracted semantic history to overcome sparse paths and noisy contexts, achieving SOTA performance on WebArena.


<details>
  <summary>Details</summary>
Motivation: Current LLM-powered autonomous agents face two critical challenges in web navigation: (1) sparse valid paths leading to inefficient exploration, and (2) noisy context that dilutes accurate state perception, limiting the effectiveness of tree search algorithms in web environments.

Method: Plan-MCTS reformulates web navigation by decoupling strategic planning from execution grounding. It transforms sparse action space into a Dense Plan Tree for efficient exploration, distills noisy contexts into an Abstracted Semantic History for precise state awareness, incorporates Dual-Gating Reward to validate physical executability and strategic alignment, and uses Structural Refinement for on-policy repair of failed subplans.

Result: Extensive experiments on WebArena demonstrate that Plan-MCTS achieves state-of-the-art performance, surpassing current approaches with higher task effectiveness and search efficiency.

Conclusion: By shifting exploration to semantic Plan Space and addressing key challenges of sparse paths and noisy contexts, Plan-MCTS provides an effective framework for enhancing LLM-powered autonomous agents in complex web navigation tasks.

Abstract: Large Language Models (LLMs) have empowered autonomous agents to handle complex web navigation tasks. While recent studies integrate tree search to enhance long-horizon reasoning, applying these algorithms in web navigation faces two critical challenges: sparse valid paths that lead to inefficient exploration, and a noisy context that dilutes accurate state perception. To address this, we introduce Plan-MCTS, a framework that reformulates web navigation by shifting exploration to a semantic Plan Space. By decoupling strategic planning from execution grounding, it transforms sparse action space into a Dense Plan Tree for efficient exploration, and distills noisy contexts into an Abstracted Semantic History for precise state awareness. To ensure efficiency and robustness, Plan-MCTS incorporates a Dual-Gating Reward to strictly validate both physical executability and strategic alignment and Structural Refinement for on-policy repair of failed subplans. Extensive experiments on WebArena demonstrate that Plan-MCTS achieves state-of-the-art performance, surpassing current approaches with higher task effectiveness and search efficiency.

</details>


### [235] [GUI-GENESIS: Automated Synthesis of Efficient Environments with Verifiable Rewards for GUI Agent Post-Training](https://arxiv.org/abs/2602.14093)
*Yuan Cao,Dezhi Ran,Mengzhou Wu,Yuzhe Guo,Xin Chen,Ang Li,Gang Cao,Gong Zhi,Hao Yu,Linyi Li,Wei Yang,Tao Xie*

Main category: cs.AI

TL;DR: GUI-GENESIS: A framework that automatically synthesizes lightweight web environments from real-world applications with code-native rewards for efficient GUI agent training, reducing latency by 10x and costs by $28k per epoch while improving performance.


<details>
  <summary>Details</summary>
Motivation: Training GUI agents on real-world applications faces challenges: high latency, poor reproducibility, and unverifiable rewards relying on noisy visual proxies, which hinder development of generalization and long-horizon planning capabilities.

Method: GUI-GENESIS reconstructs real-world applications into lightweight web environments using multimodal code models and equips them with code-native rewards (executable assertions) that provide deterministic reward signals and eliminate visual estimation noise.

Result: The framework reduces environment latency by 10 times and costs by over $28,000 per epoch compared to training on real applications. Agents trained with GUI-GENESIS outperform the base model by 14.54% and real-world RL baselines by 3.27% on held-out real-world tasks.

Conclusion: GUI-GENESIS enables efficient GUI agent training with verifiable rewards, and the observation that models can synthesize environments they cannot yet solve suggests a pathway for self-improving agents.

Abstract: Post-training GUI agents in interactive environments is critical for developing generalization and long-horizon planning capabilities. However, training on real-world applications is hindered by high latency, poor reproducibility, and unverifiable rewards relying on noisy visual proxies. To address the limitations, we present GUI-GENESIS, the first framework to automatically synthesize efficient GUI training environments with verifiable rewards. GUI-GENESIS reconstructs real-world applications into lightweight web environments using multimodal code models and equips them with code-native rewards, executable assertions that provide deterministic reward signals and eliminate visual estimation noise. Extensive experiments show that GUI-GENESIS reduces environment latency by 10 times and costs by over $28,000 per epoch compared to training on real applications. Notably, agents trained with GUI-GENESIS outperform the base model by 14.54% and even real-world RL baselines by 3.27% on held-out real-world tasks. Finally, we observe that models can synthesize environments they cannot yet solve, highlighting a pathway for self-improving agents.

</details>


### [236] [NEST: Nascent Encoded Steganographic Thoughts](https://arxiv.org/abs/2602.14095)
*Artem Karpov*

Main category: cs.AI

TL;DR: Current LLMs show limited but emerging capability to hide reasoning in innocuous text, with Claude Opus 4.5 achieving 92% accuracy on simplified hidden tasks, highlighting potential steganographic risks for AI safety monitoring.


<details>
  <summary>Details</summary>
Motivation: To assess the risk of LLMs learning to conceal their chain-of-thought reasoning through steganography, which could compromise safety monitoring and enable deceptive behavior in AI agents.

Method: Systematic evaluation of 28 models across different generations, measuring monitor evasion, refusal rates, encoding fidelity, and hidden task accuracy across four datasets, comparing steganographic acrostics against plain reasoning and filler-token baselines.

Result: Current models cannot sustain hidden reasoning for complex math/arithmetic tasks, but Claude Opus 4.5 achieved 92% accuracy on simplified counting tasks. GPT-5.2 showed rare (<1%) paradoxical behavior of refusing while complying with steganographic instructions.

Conclusion: The study demonstrates nascent steganographic capabilities in frontier models, underscoring the need for continuous evaluation and preemptive detection methods to prevent hidden reasoning that could enable misaligned scheming and deceptive behavior.

Abstract: Monitoring chain-of-thought (CoT) reasoning is a foundational safety technique for large language model (LLM) agents; however, this oversight is compromised if models learn to conceal their reasoning. We explore the potential for steganographic CoT -- where models hide secret reasoning within innocuous text -- to inform risk assessment and deployment policies. We systematically evaluate the limits of steganographic capabilities across 28 models, ranging from past generations to the current frontier. We measure monitor evasion, refusal rates, encoding fidelity, and hidden task accuracy across four datasets, comparing steganographic acrostics against plain reasoning and filler-token baselines. We find that current models cannot yet sustain hidden reasoning for complex math and arithmetic tasks. However, in a simplified counting experiment, Claude Opus 4.5 achieved 92% accuracy on the hidden task, demonstrating nascent capability. Notably, in rare cases (<1%), GPT-5.2 might refuse steganographic instructions while simultaneously complying with them. Our findings underscore the need for continuous evaluation of steganographic risks. This study provides a methodology to preemptively detect and prevent hidden reasoning that might empower misaligned scheming and deceptive behavior.

</details>


### [237] [Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity](https://arxiv.org/abs/2602.14130)
*Kazuo Yano,Jonghyeok Lee,Tae Ishitomi,Hironobu Kawaguchi,Akira Koyama,Masakuni Ota,Yuki Ota,Nobuo Sato,Keita Shimada,Sho Takematsu,Ayaka Tobinai,Satomi Tsuji,Kazunori Yanagi,Keiko Yano,Manabu Harada,Yuki Matsuda,Kazunori Matsumoto,Kenichi Matsumura,Hamae Matsuo,Yumi Miyazaki,Kotaro Murai,Tatsuya Ohshita,Marie Seki,Shun Tanoue,Tatsuki Terakado,Yuko Ichimaru,Mirei Saito,Akihiro Otsuka,Koji Ara*

Main category: cs.AI

TL;DR: AQI framework uses noncommutative algebraic structures inspired by quantum theory to expand semantic space and enable more creative LLM outputs by maintaining multiple future possibilities.


<details>
  <summary>Details</summary>
Motivation: Current LLMs have limited creativity because rich context strongly constrains future generations, making the process near-deterministic. Existing approaches like test-time scaling don't fundamentally address this structural limitation.

Method: Proposed Algebraic Quantum Intelligence (AQI) - a noncommutative algebraic framework inspired by quantum theory that represents semantic states as Hilbert space vectors and uses C-values from noncommutative operators to maintain multiple future possibilities. Implemented by extending a transformer-based LLM with 600+ specialized operators.

Result: AQI consistently outperforms strong baselines on creative reasoning benchmarks across 10 domains with statistically significant improvements and reduced cross-domain variance. The architecture has been deployed in real-world enterprise environments.

Conclusion: Noncommutative algebraic dynamics provide a practical and reproducible foundation for machine creativity, addressing fundamental limitations in current LLM architectures.

Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provided with rich context, the space of future generations becomes strongly constrained, and the generation process is effectively governed by near-deterministic dynamics. Recent approaches such as test-time scaling and context adaptation improve performance but do not fundamentally alter this constraint. To address this issue, we propose Algebraic Quantum Intelligence (AQI) as a computational framework that enables systematic expansion of semantic space. AQI is formulated as a noncommutative algebraic structure inspired by quantum theory, allowing properties such as order dependence, interference, and uncertainty to be implemented in a controlled and designable manner. Semantic states are represented as vectors in a Hilbert space, and their evolution is governed by C-values computed from noncommutative operators, thereby ensuring the coexistence and expansion of multiple future semantic possibilities. In this study, we implement AQI by extending a transformer-based LLM with more than 600 specialized operators. We evaluate the resulting system on creative reasoning benchmarks spanning ten domains under an LLM-as-a-judge protocol. The results show that AQI consistently outperforms strong baseline models, yielding statistically significant improvements and reduced cross-domain variance. These findings demonstrate that noncommutative algebraic dynamics can serve as a practical and reproducible foundation for machine creativity. Notably, this architecture has already been deployed in real-world enterprise environments.

</details>


### [238] [ForesightSafety Bench: A Frontier Risk Evaluation and Governance Framework towards Safe AI](https://arxiv.org/abs/2602.14135)
*Haibo Tong,Feifei Zhao,Linghao Feng,Ruoyu Wu,Ruolin Chen,Lu Jia,Zhou Zhao,Jindong Li,Tenglong Li,Erliang Lin,Shuai Yang,Enmeng Lu,Yinqian Sun,Qian Zhang,Zizhe Ruan,Zeyang Yue,Ping Wu,Huangrui Li,Chengyi Sun,Yi Zeng*

Main category: cs.AI

TL;DR: ForesightSafety Bench is a comprehensive AI safety evaluation framework with 94 risk dimensions across 7 fundamental pillars and advanced domains, revealing widespread safety vulnerabilities in frontier AI models.


<details>
  <summary>Details</summary>
Motivation: Current AI safety evaluation systems have critical limitations: restricted risk dimensions, failed frontier risk detection, and lagging benchmarks that can't address complex challenges from cutting-edge AI models with increasing autonomy and goal-directed capabilities.

Method: Proposed "ForesightSafety Bench" framework starting with 7 Fundamental Safety pillars, extending to advanced domains (Embodied AI Safety, AI4Science Safety, Social/Environmental AI risks, Catastrophic/Existential Risks, and 8 industrial safety domains), totaling 94 refined risk dimensions. Accumulated tens of thousands of structured risk data points and assessment results.

Result: Systematic evaluation of over twenty mainstream advanced large models identified key risk patterns and capability boundaries. Revealed widespread safety vulnerabilities across multiple pillars, particularly in Risky Agentic Autonomy, AI4Science Safety, Embodied AI Safety, Social AI Safety, and Catastrophic/Existential Risks.

Conclusion: Established a widely encompassing, hierarchically clear, and dynamically evolving AI safety evaluation framework that addresses current limitations and provides comprehensive risk assessment for frontier AI systems.

Abstract: Rapidly evolving AI exhibits increasingly strong autonomy and goal-directed capabilities, accompanied by derivative systemic risks that are more unpredictable, difficult to control, and potentially irreversible. However, current AI safety evaluation systems suffer from critical limitations such as restricted risk dimensions and failed frontier risk detection. The lagging safety benchmarks and alignment technologies can hardly address the complex challenges posed by cutting-edge AI models. To bridge this gap, we propose the "ForesightSafety Bench" AI Safety Evaluation Framework, beginning with 7 major Fundamental Safety pillars and progressively extends to advanced Embodied AI Safety, AI4Science Safety, Social and Environmental AI risks, Catastrophic and Existential Risks, as well as 8 critical industrial safety domains, forming a total of 94 refined risk dimensions. To date, the benchmark has accumulated tens of thousands of structured risk data points and assessment results, establishing a widely encompassing, hierarchically clear, and dynamically evolving AI safety evaluation framework. Based on this benchmark, we conduct systematic evaluation and in-depth analysis of over twenty mainstream advanced large models, identifying key risk patterns and their capability boundaries. The safety capability evaluation results reveals the widespread safety vulnerabilities of frontier AI across multiple pillars, particularly focusing on Risky Agentic Autonomy, AI4Science Safety, Embodied AI Safety, Social AI Safety and Catastrophic and Existential Risks. Our benchmark is released at https://github.com/Beijing-AISI/ForesightSafety-Bench. The project website is available at https://foresightsafety-bench.beijing-aisi.ac.cn/.

</details>


### [239] [Process-Supervised Multi-Agent Reinforcement Learning for Reliable Clinical Reasoning](https://arxiv.org/abs/2602.14160)
*Chaeeun Lee,T. Michael Yates,Pasquale Minervini,T. Ian Simpson*

Main category: cs.AI

TL;DR: The paper introduces an agent-as-tool reinforcement learning framework for gene-disease validity curation that improves both outcome accuracy and process alignment through process-level supervision and hierarchical multi-agent coordination.


<details>
  <summary>Details</summary>
Motivation: Clinical decision-making requires nuanced reasoning over heterogeneous evidence with traceable justifications, but current LLM multi-agent systems focus mainly on outcome accuracy while overlooking process-grounded reasoning aligned with clinical standards, particularly in gene-disease validity curation.

Method: An agent-as-tool reinforcement learning framework with two objectives: (1) process-level supervision to ensure reasoning follows valid clinical pathways, and (2) efficient coordination via a hierarchical multi-agent system, using GRPO-trained Qwen3-4B as supervisor agent.

Result: With outcome-only rewards, MAS with GRPO-trained supervisor improved final outcome accuracy from 0.195 to 0.732 but had poor process alignment (0.392 F1). With process+outcome rewards, MAS achieved higher outcome accuracy (0.750) while significantly improving process fidelity to 0.520 F1 on the ClinGen dataset.

Conclusion: The proposed framework successfully addresses the need for both accurate outcomes and clinically aligned reasoning processes in gene-disease validity curation, demonstrating that process-level supervision is crucial for achieving both objectives simultaneously.

Abstract: Clinical decision-making requires nuanced reasoning over heterogeneous evidence and traceable justifications. While recent LLM multi-agent systems (MAS) show promise, they largely optimise for outcome accuracy while overlooking process-grounded reasoning aligned with clinical standards. One critical real-world case of this is gene-disease validity curation, where experts must determine whether a gene is causally implicated in a disease by synthesising diverse biomedical evidence. We introduce an agent-as-tool reinforcement learning framework for this task with two objectives: (i) process-level supervision to ensure reasoning follows valid clinical pathways, and (ii) efficient coordination via a hierarchical multi-agent system. Our evaluation on the ClinGen dataset shows that with outcome-only rewards, MAS with a GRPO-trained Qwen3-4B supervisor agent substantially improves final outcome accuracy from 0.195 with a base model supervisor to 0.732, but results in poor process alignment (0.392 F1). Conversely, with process + outcome rewards, MAS with GRPO-trained supervisor achieves higher outcome accuracy (0.750) while significantly improving process fidelity to 0.520 F1. Our code is available at https://github.com/chaeeunlee-io/GeneDiseaseCurationAgents.

</details>


### [240] [Text Before Vision: Staged Knowledge Injection Matters for Agentic RLVR in Ultra-High-Resolution Remote Sensing Understanding](https://arxiv.org/abs/2602.14225)
*Fengxiang Wang,Mingshuo Chen,Yueying Li,Yajie Yang,Yuhao Zhou,Di Wang,Yifan Zhang,Haoyu Wang,Haiyan Zhao,Hongda Sun,Long Lan,Jun Song,Yulin Wang,Jing Zhang,Wenlong Zhang,Bo Du*

Main category: cs.AI

TL;DR: Text-only Earth-science QA surprisingly drives UHR remote sensing gains more than visual training, enabling new SOTA via staged knowledge injection.


<details>
  <summary>Details</summary>
Motivation: Multimodal reasoning for ultra-high-resolution remote sensing is bottlenecked by visual evidence acquisition - models struggle to locate tiny relevant regions in massive pixel spaces, and standard RL fails without domain priors.

Method: Two-stage approach: (1) Cold-start with scalable, knowledge-graph-verified Earth-science text QA to instill reasoning structures; (2) "Pre-warm" on hard UHR image-text examples during SFT to stabilize subsequent tool-based RL.

Result: Achieves 60.40% Pass@1 on XLRS-Bench, significantly outperforming larger general purpose models (GPT-5.2, Gemini 3.0 Pro, Intern-S1) and establishing new state-of-the-art.

Conclusion: Domain-specific text QA is crucial for UHR visual reasoning as it injects concepts, mechanistic explanations, and decision rules needed to guide visual evidence retrieval, enabling effective staged knowledge injection.

Abstract: Multimodal reasoning for ultra-high-resolution (UHR) remote sensing (RS) is usually bottlenecked by visual evidence acquisition: the model necessitates localizing tiny task-relevant regions in massive pixel spaces. While Agentic Reinforcement Learning with Verifiable Rewards (RLVR) using zoom-in tools offers a path forward, we find that standard reinforcement learning struggles to navigate these vast visual spaces without structured domain priors. In this paper, we investigate the interplay between post-training paradigms: comparing Cold-start Supervised Fine-Tuning (SFT), RLVR, and Agentic RLVR on the UHR RS benchmark.Our controlled studies yield a counter-intuitive finding: high-quality Earth-science text-only QA is a primary driver of UHR visual reasoning gains. Despite lacking images, domain-specific text injects the concepts, mechanistic explanations, and decision rules necessary to guide visual evidence retrieval.Based on this, we propose a staged knowledge injection recipe: (1) cold-starting with scalable, knowledge-graph-verified Earth-science text QA to instill reasoning structures;and (2) "pre-warming" on the same hard UHR image-text examples during SFT to stabilize and amplify subsequent tool-based RL. This approach achieves a 60.40% Pass@1 on XLRS-Bench, significantly outperforming larger general purpose models (e.g., GPT-5.2, Gemini 3.0 Pro, Intern-S1) and establishing a new state-of-the-art.

</details>


### [241] [CORPGEN: Simulating Corporate Environments with Autonomous Digital Employees in Multi-Horizon Task Environments](https://arxiv.org/abs/2602.14229)
*Abubakarr Jaye,Nigel Boachie Kumankumah,Chidera Biringa,Anjel Shaileshbhai Patel,Sulaiman Vesal,Dayquan Julienne,Charlotte Siska,Manuel Raúl Meléndez Luján,Anthony Twum-Barimah,Mauricio Velazco,Tianwei Chen*

Main category: cs.AI

TL;DR: Multi-Horizon Task Environments (MHTEs) challenge autonomous agents with concurrent long-horizon tasks. CorpGen framework addresses four failure modes with hierarchical planning and tiered memory, achieving 3.5x improvement over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks evaluate agents on single tasks in isolation, but real organizational work requires managing many concurrent long-horizon tasks with interleaving, dependencies, and reprioritization. There's a need for environments that simulate realistic corporate workloads.

Method: Introduces Multi-Horizon Task Environments (MHTEs) requiring coherent execution across dozens of interleaved tasks. Presents CorpGen framework with hierarchical planning for multi-horizon goal alignment, sub-agent isolation, tiered memory (working, structured, semantic), and adaptive summarization. Simulates corporate environments through digital employees with persistent identities.

Result: CorpGen achieves up to 3.5x improvement over baselines (15.2% vs 4.3%) with stable performance under increasing load. Identifies four failure modes causing baseline degradation from 16.7% to 8.7% completion as load scales. Ablation studies show experiential learning provides largest gains.

Conclusion: The CorpGen framework effectively addresses key challenges in multi-horizon task environments through architectural mechanisms that prevent context saturation, memory interference, dependency complexity, and reprioritization overhead, with gains consistent across different CUA implementations.

Abstract: Long-horizon reasoning is a key challenge for autonomous agents, yet existing benchmarks evaluate agents on single tasks in isolation. Real organizational work requires managing many concurrent long-horizon tasks with interleaving, dependencies, and reprioritization. We introduce Multi-Horizon Task Environments (MHTEs): a distinct problem class requiring coherent execution across dozens of interleaved tasks (45+, 500-1500+ steps) within persistent execution contexts spanning hours. We identify four failure modes that cause baseline CUAs to degrade from 16.7% to 8.7% completion as load scales 25% to 100%, a pattern consistent across three independent implementations. These failure modes are context saturation (O(N) vs O(1) growth), memory interference, dependency complexity (DAGs vs. chains), and reprioritization overhead. We present CorpGen, an architecture-agnostic framework addressing these failures via hierarchical planning for multi-horizon goal alignment, sub-agent isolation preventing cross-task contamination, tiered memory (working, structured, semantic), and adaptive summarization. CorpGen simulates corporate environments through digital employees with persistent identities and realistic schedules. Across three CUA backends (UFO2, OpenAI CUA, hierarchical) on OSWorld Office, CorpGen achieves up to 3.5x improvement over baselines (15.2% vs 4.3%) with stable performance under increasing load, confirming that gains stem from architectural mechanisms rather than specific CUA implementations. Ablation studies show experiential learning provides the largest gains.

</details>


### [242] [REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents](https://arxiv.org/abs/2602.14234)
*Zheng Chu,Xiao Wang,Jack Hong,Huiming Fan,Yuqi Huang,Yue Yang,Guohai Xu,Chenxiao Zhao,Cheng Xiang,Shengchao Hu,Dongdong Kuang,Ming Liu,Bing Qin,Xing Yu*

Main category: cs.AI

TL;DR: REDSearcher is a unified framework that codesigns complex task synthesis, mid-training, and post-training to optimize search agents, addressing challenges of sparse high-quality search trajectories and reward signals in LLM-based problem solving.


<details>
  <summary>Details</summary>
Motivation: Large language models face challenges when transitioning to real-world problem solvers due to extreme sparsity of high-quality search trajectories and reward signals, stemming from difficulties in scalable long-horizon task construction and high costs of interaction-heavy rollouts with external tools.

Method: REDSearcher introduces: 1) Dual-constrained optimization for task synthesis using graph topology and evidence dispersion, 2) Tool-augmented queries to encourage proactive tool use, 3) Mid-training to strengthen core atomic capabilities (knowledge, planning, function calling), and 4) Local simulated environment for rapid RL iteration.

Result: The approach achieves state-of-the-art performance across both text-only and multimodal search-agent benchmarks, and the authors will release 10K high-quality complex text search trajectories, 5K multimodal trajectories, 1K text RL query set, along with code and model checkpoints.

Conclusion: REDSearcher provides a scalable framework for optimizing search agents by addressing key bottlenecks in task synthesis, training efficiency, and evaluation, advancing the development of long-horizon search agents for real-world problem solving.

Abstract: Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interactionheavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, a unified framework that codesigns complex task synthesis, midtraining, and posttraining for scalable searchagent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as a dualconstrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion, allowing scalable generation of complex, highquality tasks. (2) We introduce toolaugmented queries to encourage proactive tool use rather than passive recall.(3) During midtraining, we strengthen core atomic capabilities knowledge, planning, and function calling substantially reducing the cost of collecting highquality trajectories for downstream training. (4) We build a local simulated environment that enables rapid, lowcost algorithmic iteration for reinforcement learning experiments. Across both textonly and multimodal searchagent benchmarks, our approach achieves stateoftheart performance. To facilitate future research on longhorizon search agents, we will release 10K highquality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints.

</details>


### [243] [AutoWebWorld: Synthesizing Infinite Verifiable Web Environments via Finite State Machines](https://arxiv.org/abs/2602.14296)
*Yifan Wu,Yiran Peng,Yiyu Chen,Jianhao Ruan,Zijie Zhuang,Cheng Yang,Jiayi Zhang,Man Chen,Yenchi Tseng,Zhaoyang Yu,Liang Chen,Yuyao Zhai,Bang Liu,Chenglin Wu,Yuyu Luo*

Main category: cs.AI

TL;DR: AutoWebWorld: A framework for synthesizing controllable web environments as Finite State Machines to generate cheap, verifiable training data for autonomous Web GUI agents, significantly improving real-world performance.


<details>
  <summary>Details</summary>
Motivation: Collecting interaction trajectories from real websites is expensive and difficult to verify due to hidden state transitions and reliance on costly external verifiers for step-level correctness evaluation.

Method: Model web environments as Finite State Machines (FSMs) and use coding agents to translate FSMs into interactive websites, enabling explicit definition of all states, actions, and transition rules for programmatic verification.

Result: Generated 11,663 verified trajectories from 29 diverse web environments at only $0.04 per trajectory. Training on this synthetic data significantly boosts real-world performance: 7B Web GUI agent outperforms all baselines within 15 steps on WebVoyager, with clear scaling law showing improved performance on WebVoyager and Online-Mind2Web as synthetic data volume increases.

Conclusion: AutoWebWorld provides an effective solution to the data bottleneck in Web GUI agent training by enabling cheap, verifiable synthetic data generation through FSM-based environment synthesis, demonstrating strong performance improvements and scalability.

Abstract: The performance of autonomous Web GUI agents heavily relies on the quality and quantity of their training data. However, a fundamental bottleneck persists: collecting interaction trajectories from real-world websites is expensive and difficult to verify. The underlying state transitions are hidden, leading to reliance on inconsistent and costly external verifiers to evaluate step-level correctness. To address this, we propose AutoWebWorld, a novel framework for synthesizing controllable and verifiable web environments by modeling them as Finite State Machines (FSMs) and use coding agents to translate FSMs into interactive websites. Unlike real websites, where state transitions are implicit, AutoWebWorld explicitly defines all states, actions, and transition rules. This enables programmatic verification: action correctness is checked against predefined rules, and task success is confirmed by reaching a goal state in the FSM graph. AutoWebWorld enables a fully automated search-and-verify pipeline, generating over 11,663 verified trajectories from 29 diverse web environments at only $0.04 per trajectory. Training on this synthetic data significantly boosts real-world performance. Our 7B Web GUI agent outperforms all baselines within 15 steps on WebVoyager. Furthermore, we observe a clear scaling law: as the synthetic data volume increases, performance on WebVoyager and Online-Mind2Web consistently improves.

</details>


### [244] [Benchmarking at the Edge of Comprehension](https://arxiv.org/abs/2602.14307)
*Samuele Marro,Jialin Yu,Emanuele La Malfa,Oishi Deb,Jiawei Li,Yibo Yang,Ebey Abraham,Sunando Sengupta,Eric Sommerlade,Michael Wooldridge,Philip Torr*

Main category: cs.AI

TL;DR: Proposes Critique-Resilient Benchmarking, an adversarial framework to compare LLMs when human comprehension becomes infeasible, using critique-resilient correctness and humans as bounded verifiers.


<details>
  <summary>Details</summary>
Motivation: As frontier LLMs saturate benchmarks, traditional benchmarking becomes infeasible because humans can't generate discriminative tasks, provide accurate ground-truth, or evaluate complex solutions. This threatens our ability to measure AI progress in the "post-comprehension regime."

Method: Introduces Critique-Resilient Benchmarking with critique-resilient correctness: an answer is correct if no adversary has convincingly proved otherwise. Humans serve as bounded verifiers focusing on localized claims. Uses itemized bipartite Bradley-Terry model to jointly rank LLMs by solving ability and question generation difficulty.

Result: Demonstrated effectiveness in mathematical domain across eight frontier LLMs, showing stable scores that correlate with external capability measures.

Conclusion: Reformulates benchmarking as an adversarial generation-evaluation game with humans as final adjudicators, enabling model comparison even when full human comprehension is infeasible.

Abstract: As frontier Large Language Models (LLMs) increasingly saturate new benchmarks shortly after they are published, benchmarking itself is at a juncture: if frontier models keep improving, it will become increasingly hard for humans to generate discriminative tasks, provide accurate ground-truth answers, or evaluate complex solutions. If benchmarking becomes infeasible, our ability to measure any progress in AI is at stake. We refer to this scenario as the post-comprehension regime. In this work, we propose Critique-Resilient Benchmarking, an adversarial framework designed to compare models even when full human understanding is infeasible. Our technique relies on the notion of critique-resilient correctness: an answer is deemed correct if no adversary has convincingly proved otherwise. Unlike standard benchmarking, humans serve as bounded verifiers and focus on localized claims, which preserves evaluation integrity beyond full comprehension of the task. Using an itemized bipartite Bradley-Terry model, we jointly rank LLMs by their ability to solve challenging tasks and to generate difficult yet solvable questions. We showcase the effectiveness of our method in the mathematical domain across eight frontier LLMs, showing that the resulting scores are stable and correlate with external capability measures. Our framework reformulates benchmarking as an adversarial generation-evaluation game in which humans serve as final adjudicators.

</details>


### [245] [Competition for attention predicts good-to-bad tipping in AI](https://arxiv.org/abs/2602.14370)
*Neil F. Johnson,Frank Y. Huo*

Main category: cs.AI

TL;DR: Paper identifies a mathematical tipping point mechanism for dangerous AI outputs in edge devices, offering new safety controls without cloud connectivity.


<details>
  <summary>Details</summary>
Motivation: With over half the global population using devices that can run ChatGPT-like models offline with minimal safety oversight, there's urgent need for safety tools that don't require cloud connectivity and can prevent harm before it occurs.

Method: The paper shows that dangerous tipping originates at the atomistic scale in edge AI due to competition for attention machinery. It develops a mathematical formula for dynamical tipping point n* governed by dot-product competition between conversation context and competing output basins.

Result: The mechanism was validated against multiple AI models and can be instantiated for different definitions of 'good' and 'bad', making it applicable across domains (health, law, finance, defense), legal landscapes, languages, and cultural settings.

Conclusion: The paper reveals new control levers for AI safety in edge devices by mathematically characterizing the tipping point mechanism for dangerous outputs, offering a solution that works without cloud connectivity and can prevent harm proactively.

Abstract: More than half the global population now carries devices that can run ChatGPT-like language models with no Internet connection and minimal safety oversight -- and hence the potential to promote self-harm, financial losses and extremism among other dangers. Existing safety tools either require cloud connectivity or discover failures only after harm has occurred. Here we show that a large class of potentially dangerous tipping originates at the atomistic scale in such edge AI due to competition for the machinery's attention. This yields a mathematical formula for the dynamical tipping point n*, governed by dot-product competition for attention between the conversation's context and competing output basins, that reveals new control levers. Validated against multiple AI models, the mechanism can be instantiated for different definitions of 'good' and 'bad' and hence in principle applies across domains (e.g. health, law, finance, defense), changing legal landscapes (e.g. EU, UK, US and state level), languages, and cultural settings.

</details>


### [246] [Boule or Baguette? A Study on Task Topology, Length Generalization, and the Benefit of Reasoning Traces](https://arxiv.org/abs/2602.14404)
*William L. Tong,Ege Cakar,Cengiz Pehlevan*

Main category: cs.AI

TL;DR: The paper introduces PITA, a large-scale dataset for studying reasoning models, and finds that reasoning trace models generalize well on broad/shallow tasks but deteriorate on narrow/deep tasks compared to non-RT baselines.


<details>
  <summary>Details</summary>
Motivation: Despite rapid progress in reasoning models that generate intermediate reasoning traces, our understanding of how these traces support reasoning and the limits of this paradigm remains incomplete. The authors aim to promote greater clarity about the benefits and limitations of reasoning traces.

Method: The authors introduce PITA, a novel large-scale dataset of over 23 million statements in propositional logic with corresponding proofs. They focus on length generalization as a benchmark for robust reasoning, proposing notions of task depth (number of steps required) and task breadth (number of unique examples). They vary these quantities across PITA subsets and compare reasoning trace models to non-RT baselines.

Result: Reasoning trace models generalize well on broad and shallow subsets but deteriorate on narrow and deep subsets relative to non-RT baselines. The authors validate these findings on a simple synthetic syllogism task, suggesting the results are not idiosyncratic to PITA.

Conclusion: The study identifies fundamental benefits and limitations of reasoning traces: they excel on broad tasks but face scaling limitations on deep tasks. The findings provide a theoretical framework for understanding when reasoning trace models are most effective and where their limitations lie.

Abstract: Recent years have witnessed meteoric progress in reasoning models: neural networks that generate intermediate reasoning traces (RTs) before producing a final output. Despite the rapid advancement, our understanding of how RTs support reasoning, and the limits of this paradigm, remain incomplete. To promote greater clarity, we introduce PITA: a novel large-scale dataset of over 23 million statements in propositional logic and their corresponding proofs. As a benchmark for robust reasoning, we focus on length generalization: if a model is trained to determine truth or falsity on statements with proofs up to fixed length, how well does it generalize to statements requiring longer proofs? We propose notions of (1) task depth and (2) task breadth, which measure respectively (1) the number of steps required to solve an example from a task and (2) the number of unique examples across a task. We vary these quantities across subsets of PITA, and find that RT models generalize well on broad and shallow subsets, while deteriorating on narrow and deep subsets relative to non-RT baselines. To determine whether our results are idiosyncratic to PITA or indicative of general phenomena, we compare our results to a simple synthetic task based on syllogisms. Our resulting theory suggests fundamental scalings that limit how well RT models perform on deep tasks, and highlights their generalization strengths on broad tasks. Our findings overall identify fundamental benefits and limitations inherent in using reasoning traces.

</details>


### [247] [Precedent-Informed Reasoning: Mitigating Overthinking in Large Reasoning Models via Test-Time Precedent Learning](https://arxiv.org/abs/2602.14451)
*Qianyue Wang,Jinwu Hu,Huanxiang Lin,Bolin Chen,Zhiquan Wen,Yaofo Chen,Yu Rong,Mingkui Tan*

Main category: cs.AI

TL;DR: PIR transforms LLM reasoning from exhaustive self-exploration to guided learning from precedents, using adaptive precedent selection and test-time experience internalization to shorten reasoning traces while maintaining/improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Current LLM reasoning suffers from inefficient long chain-of-thought traces with redundant self-exploration and validation, which inflates computational costs and can degrade performance. The authors are inspired by human reasoning patterns where people solve new problems by leveraging past related cases to constrain search spaces and reduce trial-and-error.

Method: PIR has two key components: 1) Adaptive Precedent Selection (APS) constructs a compact set of precedents for each question and LLM, ranking examples by joint score of semantic similarity and model perplexity, then adapts the amount of precedents to maximize perplexity reduction. 2) Test-time Experience Internalization (TEI) performs test-time learning on precedent-informed instruction, updating lightweight adapters to internalize solution patterns and use them as a prior during subsequent reasoning.

Result: Experiments across mathematical reasoning, scientific QA, and code generation demonstrate that PIR consistently shortens reasoning traces while maintaining or improving final accuracy across LLMs, yielding outstanding accuracy-efficiency trade-offs.

Conclusion: PIR successfully transforms LLM reasoning from exhaustive self-exploration to guided learning from precedents, addressing key challenges of what precedents to adopt and how to utilize them, resulting in more efficient and effective reasoning.

Abstract: Reasoning in Large Language Models (LLMs) often suffers from inefficient long chain-of-thought traces with redundant self-exploration and validation, which inflate computational costs and even degrade performance. Inspired by human reasoning patterns where people solve new problems by leveraging past related cases to constrain search spaces and reduce trial-and-error, we propose Precedent Informed Reasoning (PIR) transforming LRMs'reasoning paradigm from exhaustive self-exploration to guided learning from precedents. PIR addresses two key challenges: what precedents to adopt and how to utilize them. First, Adaptive Precedent Selection (APS) constructs, for each question and LRM, a compact set of precedents that are both semantically related and informative for the model. It ranks examples by a joint score with semantic similarity and model perplexity, then adapts the amount of precedents to maximize perplexity reduction. Second, Test-time Experience Internalization (TEI) is treated as the test-time learning on precedent-informed instruction, updating lightweight adapters to internalize solution patterns and use them as a prior during subsequent reasoning. Experiments across mathematical reasoning, scientific QA, and code generation demonstrate that PIR consistently shortens reasoning traces while maintaining or improving final accuracy across LLMs, yielding outstanding accuracy-efficiency trade-offs.

</details>


### [248] [Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5](https://arxiv.org/abs/2602.14457)
*Dongrui Liu,Yi Yu,Jie Zhang,Guanxu Chen,Qihao Lin,Hanxi Zhu,Lige Huang,Yijin Zhou,Peng Wang,Shuai Shao,Boxuan Zhang,Zicheng Liu,Jingwei Sun,Yu Li,Yuejin Xie,Jiaxuan Guo,Jia Xu,Chaochao Lu,Bowen Zhou,Xia Hu,Jing Shao*

Main category: cs.AI

TL;DR: Updated risk assessment framework for frontier AI models covering cyber offense, persuasion, deception, uncontrolled R&D, and self-replication with new mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: To address unprecedented risks from rapidly advancing AI models, especially LLMs and agentic AI, by providing a comprehensive and updated risk assessment framework.

Method: Introduces updated granular assessment across five dimensions: more complex cyber offense scenarios, LLM-to-LLM persuasion evaluation on new models, emergent misalignment experiments for strategic deception, monitoring "mis-evolution" of autonomous agents expanding memory/tools, and resource-constrained self-replication scenarios.

Result: Presents a comprehensive risk analysis with validated mitigation strategies, providing technical pathways for secure deployment of frontier AI systems.

Conclusion: The work reflects current understanding of AI frontier risks and urges collective action to mitigate these emerging challenges through robust risk management frameworks.

Abstract: To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the proliferation of agentic AI, this version of the risk analysis technical report presents an updated and granular assessment of five critical dimensions: cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R\&D, and self-replication. Specifically, we introduce more complex scenarios for cyber offense. For persuasion and manipulation, we evaluate the risk of LLM-to-LLM persuasion on newly released LLMs. For strategic deception and scheming, we add the new experiment with respect to emergent misalignment. For uncontrolled AI R\&D, we focus on the ``mis-evolution'' of agents as they autonomously expand their memory substrates and toolsets. Besides, we also monitor and evaluate the safety performance of OpenClaw during the interaction on the Moltbook. For self-replication, we introduce a new resource-constrained scenario. More importantly, we propose and validate a series of robust mitigation strategies to address these emerging threats, providing a preliminary technical and actionable pathway for the secure deployment of frontier AI. This work reflects our current understanding of AI frontier risks and urges collective action to mitigate these challenges.

</details>


### [249] [Bounding Probabilities of Causation with Partial Causal Diagrams](https://arxiv.org/abs/2602.14503)
*Yuxuan Xie,Ang Li*

Main category: cs.AI

TL;DR: General framework for bounding probabilities of causation using partial causal information when full identifiability is impossible


<details>
  <summary>Details</summary>
Motivation: Probabilities of causation are crucial for individual-level explanation and decision making, but they are counterfactual and not point-identifiable from data. Existing bounds are limited: they ignore covariates, require complete causal graphs, or only work in restrictive binary settings, making them impractical for real-world applications where causal information is often partial but nontrivial.

Method: Proposes a general framework that systematically incorporates available structural or statistical information as constraints in an optimization programming formulation. This allows deriving tighter and formally valid bounds without requiring full identifiability.

Result: The approach yields tighter bounds than existing methods by leveraging partial causal information, and extends the applicability of probabilities of causation to realistic settings where causal knowledge is incomplete but informative.

Conclusion: The proposed framework enables practical bounding of probabilities of causation in real-world scenarios where causal information is partial but available, overcoming limitations of previous methods that required complete causal knowledge or worked only in restrictive settings.

Abstract: Probabilities of causation are fundamental to individual-level explanation and decision making, yet they are inherently counterfactual and not point-identifiable from data in general. Existing bounds either disregard available covariates, require complete causal graphs, or rely on restrictive binary settings, limiting their practical use. In real-world applications, causal information is often partial but nontrivial. This paper proposes a general framework for bounding probabilities of causation using partial causal information. We show how the available structural or statistical information can be systematically incorporated as constraints in a optimization programming formulation, yielding tighter and formally valid bounds without full identifiability. This approach extends the applicability of probabilities of causation to realistic settings where causal knowledge is incomplete but informative.

</details>


### [250] [Formally Verifying and Explaining Sepsis Treatment Policies with COOL-MC](https://arxiv.org/abs/2602.14505)
*Dennis Gross*

Main category: cs.AI

TL;DR: COOL-MC is a tool that combines formal verification and explainability to analyze RL policies for sepsis treatment, making them safer and more interpretable by constructing only reachable state spaces and providing clinical insights.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning policies for sepsis treatment optimization are opaque and difficult to verify, making them unsafe for clinical deployment. Standard model checkers are infeasible for large MDPs and lack explainability features.

Method: COOL-MC wraps Storm model checker with three key capabilities: 1) constructs only reachable state space induced by trained policies, 2) automatically labels states with clinically meaningful atomic propositions, and 3) integrates explainability methods with PCTL queries to reveal feature importance.

Result: Applied to ICU-Sepsis MDP (17,000 patient records), established hard bounds via full MDP verification, trained safe RL policy achieving optimal survival probability, and revealed that the policy relies on prior dosing history rather than patient's evolving condition.

Conclusion: COOL-MC serves as a tool for clinicians to investigate and debug sepsis treatment policies before deployment by integrating formal verification with explainability, exposing weaknesses invisible to standard evaluation methods.

Abstract: Safe and interpretable sequential decision-making is critical in healthcare, yet reinforcement learning (RL) policies for sepsis treatment optimization remain opaque and difficult to verify. Standard probabilistic model checkers operate on the full state space, which becomes infeasible for larger MDPs, and cannot explain why a learned policy makes particular decisions. COOL-MC wraps the model checker Storm but adds three key capabilities: it constructs only the reachable state space induced by a trained policy, yielding a smaller discrete-time Markov chain amenable to verification even when full-MDP analysis is intractable; it automatically labels states with clinically meaningful atomic propositions; and it integrates explainability methods with probabilistic computation tree logic (PCTL) queries to reveal which features drive decisions across treatment trajectories. We demonstrate COOL-MC's capabilities on the ICU-Sepsis MDP, a benchmark derived from approximately 17,000 sepsis patient records, which serves as a case study for applying COOL-MC to the formal analysis of sepsis treatment policies. Our analysis establishes hard bounds via full MDP verification, trains a safe RL policy that achieves optimal survival probability, and analyzes its behavior via PCTL verification and explainability on the induced DTMC. This reveals, for instance, that our trained policy relies predominantly on prior dosing history rather than the patient's evolving condition, a weakness that is invisible to standard evaluation but is exposed by COOL-MC's integration of formal verification and explainability. Our results illustrate how COOL-MC could serve as a tool for clinicians to investigate and debug sepsis treatment policies before deployment.

</details>


### [251] [Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning](https://arxiv.org/abs/2602.14518)
*Jing Tang,Kun Wang,Haolang Lu,Hongjin Chen,KaiTao Chen,Zhongxiang Sun,Qiankun Li,Lingjuan Lyu,Guoshun Nan,Zhigang Zeng*

Main category: cs.AI

TL;DR: MLLMs struggle with conflicting knowledge in long reasoning chains. The paper analyzes how different conflict types are encoded in model representations, revealing linear separability, depth localization, hierarchical consistency, and directional asymmetry.


<details>
  <summary>Details</summary>
Motivation: Multimodal large language models often fail in long chain-of-thought reasoning when different knowledge sources provide conflicting information. Understanding how these conflicts are processed internally is crucial for improving model reliability.

Method: The authors formalize knowledge conflicts into input-level objective conflict vs. process-level effective conflict. They probe internal representations of MLLMs to analyze how conflicts are encoded, using techniques to examine feature separability, layer localization, signal aggregation, and directional preferences.

Result: Four key findings: (1) Different conflict types are linearly separable features, not entangled; (2) Conflict signals concentrate in mid-to-late layers; (3) Aggregating token-level signals robustly recovers input-level conflict types; (4) Reinforcing implicit source preferences is easier than enforcing opposite sources.

Conclusion: The study provides a mechanism-level understanding of how MLLMs handle knowledge conflicts during reasoning. These insights enable principled diagnosis and control of failures in long chain-of-thought reasoning, offering pathways to improve multimodal reasoning reliability.

Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conflict. Through probing internal representations, we reveal that: (I) Linear Separability: different conflict types are explicitly encoded as linearly separable features rather than entangled; (II) Depth Localization: conflict signals concentrate in mid-to-late layers, indicating a distinct processing stage for conflict encoding; (III) Hierarchical Consistency: aggregating noisy token-level signals along trajectories robustly recovers input-level conflict types; and (IV) Directional Asymmetry: reinforcing the model's implicit source preference under conflict is far easier than enforcing the opposite source. Our findings provide a mechanism-level view of multimodal reasoning under knowledge conflict and enable principled diagnosis and control of long-CoT failures.

</details>


### [252] [Disentangling Deception and Hallucination Failures in LLMs](https://arxiv.org/abs/2602.14529)
*Haolang Lu,Hongrui Peng,WeiYe Fu,Guoshun Nan,Xinye Cao,Xingrui Li,Hongcan Guo,Kun Wang*

Main category: cs.AI

TL;DR: The paper proposes separating knowledge existence from behavior expression in LLM failures, distinguishing hallucination from deception as different underlying mechanisms despite similar outputs.


<details>
  <summary>Details</summary>
Motivation: Current analysis of LLM failures often conflates different failure mechanisms by focusing only on behavioral outputs. The authors want to distinguish between failures due to missing knowledge versus failures in expressing existing knowledge.

Method: Create controlled environment for entity-centric factual questions where knowledge is preserved but behavioral expression is selectively altered. Analyze four behavioral cases using representation separability, sparse interpretability, and inference-time activation steering.

Result: The paper distinguishes hallucination (missing knowledge) from deception (knowledge exists but not expressed) as qualitatively different failure modes that appear similar at output level but have different underlying mechanisms.

Conclusion: A mechanism-oriented perspective separating Knowledge Existence from Behavior Expression provides better understanding of LLM failures, enabling more targeted analysis and potential interventions for different failure types.

Abstract: Failures in large language models (LLMs) are often analyzed from a behavioral perspective, where incorrect outputs in factual question answering are commonly associated with missing knowledge. In this work, focusing on entity-based factual queries, we suggest that such a view may conflate different failure mechanisms, and propose an internal, mechanism-oriented perspective that separates Knowledge Existence from Behavior Expression. Under this formulation, hallucination and deception correspond to two qualitatively different failure modes that may appear similar at the output level but differ in their underlying mechanisms. To study this distinction, we construct a controlled environment for entity-centric factual questions in which knowledge is preserved while behavioral expression is selectively altered, enabling systematic analysis of four behavioral cases. We analyze these failure modes through representation separability, sparse interpretability, and inference-time activation steering.

</details>


### [253] [MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs](https://arxiv.org/abs/2602.14589)
*Gabriel Roccabruna,Olha Khomyn,Giuseppe Riccardi*

Main category: cs.AI

TL;DR: MATEO benchmark assesses LVLMs' temporal reasoning for planning using multimodal recipe steps with Temporal Execution Order graphs.


<details>
  <summary>Details</summary>
Motivation: Existing research on foundational models' understanding of temporal execution is limited to automatically derived annotations, linear chain approximations, or text-only inputs, creating a gap in evaluating real-world planning capabilities.

Method: Created MATEO benchmark using professional multimodal recipe corpus with discrete steps and corresponding images, collected TEO annotations as graphs via scalable crowdsourcing pipeline, evaluated six state-of-the-art LVLMs across various factors.

Result: Benchmark established for evaluating LVLMs' temporal reasoning; evaluation of six state-of-the-art models across model scales, language context variations, multimodal input structures, and fine-tuning strategies.

Conclusion: MATEO addresses the gap in assessing LVLMs' temporal reasoning for real-world planning and provides a foundation for improving multimodal temporal understanding capabilities.

Abstract: AI agents need to plan to achieve complex goals that involve orchestrating perception, sub-goal decomposition, and execution. These plans consist of ordered steps structured according to a Temporal Execution Order (TEO, a directed acyclic graph that ensures each step executes only after its preconditions are satisfied. Existing research on foundational models' understanding of temporal execution is limited to automatically derived annotations, approximations of the TEO as a linear chain, or text-only inputs. To address this gap, we introduce MATEO (MultimodAl Temporal Execution Order), a benchmark designed to assess and improve the temporal reasoning abilities of Large Vision Language Models (LVLMs) required for real-world planning. We acquire a high-quality professional multimodal recipe corpus, authored through a standardized editorial process that decomposes instructions into discrete steps, each paired with corresponding images. We collect TEO annotations as graphs by designing and using a scalable crowdsourcing pipeline. Using MATEO, we evaluate six state-of-the-art LVLMs across model scales, varying language context, multimodal input structure, and fine-tuning strategies.

</details>


### [254] [Tabular Foundation Models Can Learn Association Rules](https://arxiv.org/abs/2602.14622)
*Erkan Karabulut,Daniel Daza,Paul Groth,Martijn C. Schut,Victoria Degeler*

Main category: cs.AI

TL;DR: TabProbe: A model-agnostic framework that extracts association rules from tabular foundation models without frequent itemset mining, achieving high-quality rules with strong predictive performance even in low-data regimes.


<details>
  <summary>Details</summary>
Motivation: Classical ARM methods suffer from rule explosion and poor scalability, while neural approaches degrade in low-data settings. Tabular foundation models offer strong in-context generalization that could address these limitations.

Method: Introduces a model-agnostic framework to extract association rules from any conditional probabilistic model over tabular data. TabProbe instantiates this framework using TFMs as conditional probability estimators to learn association rules without frequent itemset mining.

Result: TFMs consistently produce concise, high-quality association rules with strong predictive performance and remain robust in low-data settings without task-specific training.

Conclusion: TabProbe successfully leverages tabular foundation models to overcome limitations of both classical and neural ARM approaches, providing an effective solution for association rule mining that works well across different data regimes.

Abstract: Association Rule Mining (ARM) is a fundamental task for knowledge discovery in tabular data and is widely used in high-stakes decision-making. Classical ARM methods rely on frequent itemset mining, leading to rule explosion and poor scalability, while recent neural approaches mitigate these issues but suffer from degraded performance in low-data regimes. Tabular foundation models (TFMs), pretrained on diverse tabular data with strong in-context generalization, provide a basis for addressing these limitations. We introduce a model-agnostic association rule learning framework that extracts association rules from any conditional probabilistic model over tabular data, enabling us to leverage TFMs. We then introduce TabProbe, an instantiation of our framework that utilizes TFMs as conditional probability estimators to learn association rules out-of-the-box without frequent itemset mining. We evaluate our approach on tabular datasets of varying sizes based on standard ARM rule quality metrics and downstream classification performance. The results show that TFMs consistently produce concise, high-quality association rules with strong predictive performance and remain robust in low-data settings without task-specific training. Source code is available at https://github.com/DiTEC-project/tabprobe.

</details>


### [255] [Arbor: A Framework for Reliable Navigation of Critical Conversation Flows](https://arxiv.org/abs/2602.14643)
*Luís Silva,Diogo Gonçalves,Catarina Farinha,Clara Matos,Luís Ungaro*

Main category: cs.AI

TL;DR: Arbor framework decomposes decision tree navigation into specialized node-level tasks to improve LLM performance in structured workflows like healthcare triage, achieving significant accuracy, latency, and cost improvements over monolithic prompt approaches.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with strict adherence to structured workflows in high-stakes domains like healthcare triage. Monolithic approaches that encode entire decision structures in single prompts suffer from instruction-following degradation, lost-in-the-middle effects, and context window overflow as prompt length increases.

Method: Arbor decomposes decision tree navigation into specialized node-level tasks. Decision trees are standardized into edge-list representation and stored for dynamic retrieval. A DAG-based orchestration mechanism iteratively retrieves only outgoing edges of the current node, evaluates valid transitions via dedicated LLM calls, and delegates response generation to separate inference steps. The framework is agnostic to underlying decision logic and model provider.

Result: Evaluated across 10 foundation models using real clinical triage conversations, Arbor improves mean turn accuracy by 29.4 percentage points, reduces per-turn latency by 57.1%, and achieves average 14.4x reduction in per-turn cost compared to single-prompt baselines.

Conclusion: Architectural decomposition reduces dependence on intrinsic model capability, enabling smaller models to match or exceed larger models operating under single-prompt baselines. The approach effectively addresses limitations of monolithic prompt-based methods for structured decision-making workflows.

Abstract: Large language models struggle to maintain strict adherence to structured workflows in high-stakes domains such as healthcare triage. Monolithic approaches that encode entire decision structures within a single prompt are prone to instruction-following degradation as prompt length increases, including lost-in-the-middle effects and context window overflow. To address this gap, we present Arbor, a framework that decomposes decision tree navigation into specialized, node-level tasks. Decision trees are standardized into an edge-list representation and stored for dynamic retrieval. At runtime, a directed acyclic graph (DAG)-based orchestration mechanism iteratively retrieves only the outgoing edges of the current node, evaluates valid transitions via a dedicated LLM call, and delegates response generation to a separate inference step. The framework is agnostic to the underlying decision logic and model provider. Evaluated against single-prompt baselines across 10 foundation models using annotated turns from real clinical triage conversations. Arbor improves mean turn accuracy by 29.4 percentage points, reduces per-turn latency by 57.1%, and achieves an average 14.4x reduction in per-turn cost. These results indicate that architectural decomposition reduces dependence on intrinsic model capability, enabling smaller models to match or exceed larger models operating under single-prompt baselines.

</details>


### [256] [From User Preferences to Base Score Extraction Functions in Gradual Argumentation](https://arxiv.org/abs/2602.14674)
*Aniol Civit,Antonio Rago,Antonio Andriella,Guillem Alenyà,Francesca Toni*

Main category: cs.AI

TL;DR: Base Score Extraction Functions map user preferences over arguments to base scores in gradual argumentation, enabling easier construction of Quantitative Bipolar Argumentation Frameworks without requiring expert score selection.


<details>
  <summary>Details</summary>
Motivation: Selecting appropriate base scores for arguments in gradual argumentation requires user expertise and can be challenging. Organizing arguments by preference could simplify this process, making argumentation systems more accessible.

Method: Introduce Base Score Extraction Functions that map user preferences over arguments to base scores. These functions work with Bipolar Argumentation Frameworks supplemented with preferences to create Quantitative Bipolar Argumentation Frameworks. The method includes an algorithm for base score extraction that approximates non-linearities in human preferences.

Result: The approach is evaluated both theoretically and experimentally in a robotics setting. The paper provides recommendations for selecting appropriate gradual semantics in practice and demonstrates how the method enables better approximation of real human preferences.

Conclusion: Base Score Extraction Functions provide a practical solution for deriving base scores from user preferences, making gradual argumentation systems more accessible and reducing the need for expert score selection while maintaining the ability to use established computational tools.

Abstract: Gradual argumentation is a field of symbolic AI which is attracting attention for its ability to support transparent and contestable AI systems. It is considered a useful tool in domains such as decision-making, recommendation, debate analysis, and others. The outcomes in such domains are usually dependent on the arguments' base scores, which must be selected carefully. Often, this selection process requires user expertise and may not always be straightforward. On the other hand, organising the arguments by preference could simplify the task. In this work, we introduce \emph{Base Score Extraction Functions}, which provide a mapping from users' preferences over arguments to base scores. These functions can be applied to the arguments of a \emph{Bipolar Argumentation Framework} (BAF), supplemented with preferences, to obtain a \emph{Quantitative Bipolar Argumentation Framework} (QBAF), allowing the use of well-established computational tools in gradual argumentation. We outline the desirable properties of base score extraction functions, discuss some design choices, and provide an algorithm for base score extraction. Our method incorporates an approximation of non-linearities in human preferences to allow for better approximation of the real ones. Finally, we evaluate our approach both theoretically and experimentally in a robotics setting, and offer recommendations for selecting appropriate gradual semantics in practice.

</details>


### [257] [GREAT-EER: Graph Edge Attention Network for Emergency Evacuation Responses](https://arxiv.org/abs/2602.14676)
*Attila Lischka,Balázs Kulcsár*

Main category: cs.AI

TL;DR: Deep reinforcement learning with graph learning solves Bus Evacuation Orienteering Problem (BEOP) for fast urban evacuation planning using buses to reduce congestion.


<details>
  <summary>Details</summary>
Motivation: Urban evacuations are needed for man-made and natural disasters (increasing due to climate change). Current car-focused evacuations cause congestion and disorder, so bus-based evacuation methods are needed for effective, fast evacuation planning.

Method: Proposed deep reinforcement learning method with graph learning to solve BEOP. Once trained, achieves fast inference speed (fractions of seconds). Solution quality bounded using MILP formulation. Validated on San Francisco using real-world road networks and travel times.

Result: Achieves near-optimal solution quality. Can determine how many evacuation vehicles needed to achieve certain bus-based evacuation quotas within predefined evacuation time while maintaining adequate run time.

Conclusion: Deep reinforcement learning with graph learning provides effective, fast solution for bus evacuation planning, addressing NP-hard BEOP with practical applications for urban disaster response.

Abstract: Emergency situations that require the evacuation of urban areas can arise from man-made causes (e.g., terrorist attacks or industrial accidents) or natural disasters, the latter becoming more frequent due to climate change. As a result, effective and fast methods to develop evacuation plans are of great importance. In this work, we identify and propose the Bus Evacuation Orienteering Problem (BEOP), an NP-hard combinatorial optimization problem with the goal of evacuating as many people from an affected area by bus in a short, predefined amount of time. The purpose of bus-based evacuation is to reduce congestion and disorder that arises in purely car-focused evacuation scenarios. To solve the BEOP, we propose a deep reinforcement learning-based method utilizing graph learning, which, once trained, achieves fast inference speed and is able to create evacuation routes in fractions of seconds. We can bound the gap of our evacuation plans using an MILP formulation. To validate our method, we create evacuation scenarios for San Francisco using real-world road networks and travel times. We show that we achieve near-optimal solution quality and are further able to investigate how many evacuation vehicles are necessary to achieve certain bus-based evacuation quotas given a predefined evacuation time while keeping run time adequate.

</details>


### [258] [Removing Planner Bias in Goal Recognition Through Multi-Plan Dataset Generation](https://arxiv.org/abs/2602.14691)
*Mustafa F. Abdelwahed,Felipe Meneguzzi Kin Max Piamolini Gusmao,Joan Espasa*

Main category: cs.AI

TL;DR: A new method using top-k planning generates diverse plans for goal recognition benchmarks to address systematic bias in existing datasets, introducing a Version Coverage Score metric to measure recognizer resilience.


<details>
  <summary>Details</summary>
Motivation: Existing goal recognition datasets suffer from systematic bias induced by heuristic-based forward search planning systems, lacking challenge for realistic scenarios where agents use different planners, which impacts evaluation of goal recognizers.

Method: Proposes using top-k planning to generate multiple, different plans for the same goal hypothesis, creating benchmarks that mitigate bias found in current datasets.

Result: Introduces Version Coverage Score (VCS) metric to measure resilience of goal recognizers when inferring goals based on different sets of plans. Results show current state-of-the-art goal recognizer resilience degrades substantially under low observability settings.

Conclusion: The proposed method addresses systematic bias in goal recognition datasets and provides better evaluation through the VCS metric, revealing vulnerabilities in current goal recognizers under realistic conditions.

Abstract: Autonomous agents require some form of goal and plan recognition to interact in multiagent settings. Unfortunately, all existing goal recognition datasets suffer from a systematical bias induced by the planning systems that generated them, namely heuristic-based forward search. This means that existing datasets lack enough challenge for more realistic scenarios (e.g., agents using different planners), which impacts the evaluation of goal recognisers with respect to using different planners for the same goal. In this paper, we propose a new method that uses top-k planning to generate multiple, different, plans for the same goal hypothesis, yielding benchmarks that mitigate the bias found in the current dataset. This allows us to introduce a new metric called Version Coverage Score (VCS) to measure the resilience of the goal recogniser when inferring a goal based on different sets of plans. Our results show that the resilience of the current state-of-the-art goal recogniser degrades substantially under low observability settings.

</details>


### [259] [Evolutionary System Prompt Learning can Facilitate Reinforcement Learning for LLMs](https://arxiv.org/abs/2602.14697)
*Lunjun Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: E-SPL is a method that combines reinforcement learning with evolutionary system prompt learning to jointly improve model contexts (prompts) and model weights, achieving better performance and generalization on reasoning and agentic tasks.


<details>
  <summary>Details</summary>
Motivation: Current LLM self-improvement methods are limited to either self-reflection for context updates or reinforcement learning for weight updates, but not both simultaneously. There's a need for a method that can jointly optimize both model contexts and model weights for better autonomous self-improvement.

Method: E-SPL runs parallel RL rollouts with multiple system prompts, applies RL updates to model weights conditioned on each prompt, and evolves the prompt population using LLM-driven mutation and crossover. TrueSkill ratings track prompt performance for evolutionary selection.

Result: E-SPL improves RL success rate from 38.8% to 45.1% in easy-to-hard generalization (AIME → BeyondAIME), outperforming reflective prompt evolution (40.0%). It shows consistent gains in sample efficiency and generalization across reasoning and agentic tasks.

Conclusion: Coupling reinforcement learning with system prompt evolution enables joint optimization of declarative knowledge (in prompts) and procedural knowledge (in weights), leading to better autonomous self-improvement and generalization capabilities.

Abstract: Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL), a method for jointly improving model contexts and model weights. In each RL iteration, E-SPL selects multiple system prompts and runs rollouts with each in parallel. It applies RL updates to model weights conditioned on each system prompt, and evolutionary updates to the system prompt population via LLM-driven mutation and crossover. Each system prompt has a TrueSkill rating for evolutionary selection, updated from relative performance within each RL iteration batch. E-SPL encourages a natural division between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME $\rightarrow$ BeyondAIME) generalization setting, E-SPL improves RL success rate from 38.8% $\rightarrow$ 45.1% while also outperforming reflective prompt evolution (40.0%). Overall, our results show that coupling reinforcement learning with system prompt evolution yields consistent gains in sample efficiency and generalization. Code: https://github.com/LunjunZhang/E-SPL

</details>


### [260] [WebWorld: A Large-Scale World Model for Web Agent Training](https://arxiv.org/abs/2602.14721)
*Zikai Xiao,Jianhong Tu,Chuhang Zou,Yuxin Zuo,Zhi Li,Peng Wang,Bowen Yu,Fei Huang,Junyang Lin,Zuozhu Liu*

Main category: cs.AI

TL;DR: WebWorld is a scalable open-web simulator trained on 1M+ interactions that enables effective web agent training with cross-domain generalization capabilities.


<details>
  <summary>Details</summary>
Motivation: Real-world web agent training faces limitations due to network latency, rate limits, and safety risks, while existing simulators are restricted to closed environments with limited trajectories.

Method: Leverages scalable data pipeline to train on 1M+ open-web interactions, supports reasoning, multi-format data, and long-horizon simulations (30+ steps). Introduces WebWorld-Bench with dual metrics across nine dimensions for evaluation.

Result: WebWorld achieves simulation performance comparable to Gemini-3-Pro. Qwen3-14B trained on WebWorld-synthesized trajectories improves by +9.2% on WebArena, reaching GPT-4o-level performance. Enables effective inference-time search, outperforming GPT-5 as world model. Shows cross-domain generalization to code, GUI, and game environments.

Conclusion: WebWorld provides a scalable solution for web agent training and offers a replicable recipe for world model construction with demonstrated cross-domain generalization capabilities.

Abstract: Web agents require massive trajectories to generalize, yet real-world training is constrained by network latency, rate limits, and safety risks. We introduce \textbf{WebWorld} series, the first open-web simulator trained at scale. While existing simulators are restricted to closed environments with thousands of trajectories, WebWorld leverages a scalable data pipeline to train on 1M+ open-web interactions, supporting reasoning, multi-format data, and long-horizon simulations of 30+ steps. For intrinsic evaluation, we introduce WebWorld-Bench with dual metrics spanning nine dimensions, where WebWorld achieves simulation performance comparable to Gemini-3-Pro. For extrinsic evaluation, Qwen3-14B trained on WebWorld-synthesized trajectories improves by +9.2\% on WebArena, reaching performance comparable to GPT-4o. WebWorld enables effective inference-time search, outperforming GPT-5 as a world model. Beyond web simulation, WebWorld exhibits cross-domain generalization to code, GUI, and game environments, providing a replicable recipe for world model construction.

</details>


### [261] [AI Arms and Influence: Frontier Models Exhibit Sophisticated Reasoning in Simulated Nuclear Crises](https://arxiv.org/abs/2602.14740)
*Kenneth Payne*

Main category: cs.AI

TL;DR: AI models in nuclear crisis simulation exhibit sophisticated strategic behavior including deception, theory of mind, and metacognition, but challenge traditional nuclear deterrence theories by showing willingness to escalate to nuclear conflict.


<details>
  <summary>Details</summary>
Motivation: To understand how frontier AI models behave in high-stakes strategic scenarios like nuclear crises, and to test whether they follow or deviate from established theories of human strategic reasoning and deterrence.

Method: Crisis simulation with three frontier LLMs (GPT-5.2, Claude Sonnet 4, Gemini 3 Flash) playing opposing leaders in a nuclear crisis scenario, analyzing their strategic decision-making and escalation patterns.

Result: AI models validated some strategic theories (Schelling, Kahn, Jervis) but challenged others: nuclear taboo didn't prevent escalation, strategic nuclear attacks occurred, threats provoked counter-escalation, high credibility accelerated conflict, and models never chose accommodation or withdrawal.

Conclusion: AI simulation is a powerful strategic analysis tool but requires calibration against human reasoning patterns; understanding AI-human strategic logic differences is crucial as AI increasingly shapes strategic outcomes.

Abstract: Today's leading AI models engage in sophisticated behaviour when placed in strategic competition. They spontaneously attempt deception, signaling intentions they do not intend to follow; they demonstrate rich theory of mind, reasoning about adversary beliefs and anticipating their actions; and they exhibit credible metacognitive self-awareness, assessing their own strategic abilities before deciding how to act.
  Here we present findings from a crisis simulation in which three frontier large language models (GPT-5.2, Claude Sonnet 4, Gemini 3 Flash) play opposing leaders in a nuclear crisis. Our simulation has direct application for national security professionals, but also, via its insights into AI reasoning under uncertainty, has applications far beyond international crisis decision-making.
  Our findings both validate and challenge central tenets of strategic theory. We find support for Schelling's ideas about commitment, Kahn's escalation framework, and Jervis's work on misperception, inter alia. Yet we also find that the nuclear taboo is no impediment to nuclear escalation by our models; that strategic nuclear attack, while rare, does occur; that threats more often provoke counter-escalation than compliance; that high mutual credibility accelerated rather than deterred conflict; and that no model ever chose accommodation or withdrawal even when under acute pressure, only reduced levels of violence.
  We argue that AI simulation represents a powerful tool for strategic analysis, but only if properly calibrated against known patterns of human reasoning. Understanding how frontier models do and do not imitate human strategic logic is essential preparation for a world in which AI increasingly shapes strategic outcomes.

</details>


### [262] [Return of the Schema: Building Complete Datasets for Machine Learning and Reasoning on Knowledge Graphs](https://arxiv.org/abs/2602.14795)
*Ivan Diliso,Roberto Barile,Claudia d'Amato,Nicola Fanizzi*

Main category: cs.AI

TL;DR: A resource providing workflow for extracting datasets with both schema and ground facts from knowledge graphs, enabling evaluation of methods using ontological constraints and neurosymbolic techniques.


<details>
  <summary>Details</summary>
Motivation: Current datasets for knowledge graph refinement evaluation contain only ground facts with limited schema information, which prevents proper assessment of methods that rely on rich ontological constraints, reasoning, or neurosymbolic techniques in real-world scenarios.

Method: Presents a workflow for extracting datasets including both schema and ground facts, handling inconsistencies, leveraging reasoning for implicit knowledge, serializing in OWL format, and providing utilities for tensor representations compatible with ML libraries.

Result: Creates a curated suite of datasets including newly extracted datasets from KGs with expressive schemas and enriched existing datasets with schema information, all serialized in OWL format ready for reasoning services.

Conclusion: Provides the first comprehensive resource for evaluating knowledge graph refinement methods that require both schema and factual information, bridging the gap between symbolic reasoning and machine learning approaches.

Abstract: Datasets for the experimental evaluation of knowledge graph refinement algorithms typically contain only ground facts, retaining very limited schema level knowledge even when such information is available in the source knowledge graphs. This limits the evaluation of methods that rely on rich ontological constraints, reasoning or neurosymbolic techniques and ultimately prevents assessing their performance in large-scale, real-world knowledge graphs. In this paper, we present \resource{} the first resource that provides a workflow for extracting datasets including both schema and ground facts, ready for machine learning and reasoning services, along with the resulting curated suite of datasets. The workflow also handles inconsistencies detected when keeping both schema and facts and also leverage reasoning for entailing implicit knowledge. The suite includes newly extracted datasets from KGs with expressive schemas while simultaneously enriching existing datasets with schema information. Each dataset is serialized in OWL making it ready for reasoning services. Moreover, we provide utilities for loading datasets in tensor representations typical of standard machine learning libraries.

</details>


### [263] [World Models for Policy Refinement in StarCraft II](https://arxiv.org/abs/2602.14857)
*Yixin Zhang,Ziyi Wang,Yiming Rong,Haoxi Wang,Jinling Jiang,Shuang Xu,Haoran Wu,Shiyu Zhou,Bo Xu*

Main category: cs.AI

TL;DR: StarWM introduces the first world model for StarCraft II that predicts future observations under partial observability, enabling foresight-driven policy refinement through a Generate-Simulate-Refine decision loop.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based SC2 agents focus on improving policies but overlook integrating learnable, action-conditioned transition models into decision-making, creating a gap for world models that can predict future states in complex environments.

Method: Proposes StarWM with structured textual representation factorizing observations into five semantic modules, creates SC2-Dynamics-50k instruction-tuning dataset, develops multi-dimensional offline evaluation framework, and integrates world model into Generate-Simulate-Refine decision loop.

Result: StarWM achieves nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency offline. StarWM-Agent shows 30%, 15%, and 30% win-rate gains against Hard, Harder, and VeryHard AI levels respectively, with improved macro-management stability.

Conclusion: World models significantly enhance LLM-based decision-making in complex environments like StarCraft II, demonstrating that integrating action-conditioned transition models into decision loops yields substantial performance improvements in both prediction accuracy and tactical outcomes.

Abstract: Large Language Models (LLMs) have recently shown strong reasoning and generalization capabilities, motivating their use as decision-making policies in complex environments. StarCraft II (SC2), with its massive state-action space and partial observability, is a challenging testbed. However, existing LLM-based SC2 agents primarily focus on improving the policy itself and overlook integrating a learnable, action-conditioned transition model into the decision loop. To bridge this gap, we propose StarWM, the first world model for SC2 that predicts future observations under partial observability. To facilitate learning SC2's hybrid dynamics, we introduce a structured textual representation that factorizes observations into five semantic modules, and construct SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction. We further develop a multi-dimensional offline evaluation framework for predicted structured observations. Offline results show StarWM's substantial gains over zero-shot baselines, including nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency. Finally, we propose StarWM-Agent, a world-model-augmented decision system that integrates StarWM into a Generate--Simulate--Refine decision loop for foresight-driven policy refinement. Online evaluation against SC2's built-in AI demonstrates consistent improvements, yielding win-rate gains of 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macro-management stability and tactical risk assessment.

</details>


### [264] [EmbeWebAgent: Embedding Web Agents into Any Customized UI](https://arxiv.org/abs/2602.14865)
*Chenyang Ma,Clyde Fare,Matthew Wilson,Dave Braines*

Main category: cs.AI

TL;DR: EmbeWebAgent embeds agents directly into enterprise UIs using frontend hooks and backend workflows, enabling robust multi-step behaviors with minimal retrofitting.


<details>
  <summary>Details</summary>
Motivation: Most web agents operate at human interface level (screenshots/DOM) without application-level access, limiting robustness and action expressiveness. Enterprise settings offer explicit control of both frontend and backend.

Method: Lightweight frontend hooks (curated ARIA/URL observations, per-page function registry via WebSocket) + reusable backend workflow for reasoning/actions. Stack-agnostic, supports mixed-granularity actions, orchestrates navigation/manipulation/analytics via MCP tools.

Result: Minimal retrofitting effort and robust multi-step behaviors grounded in live UI setting. Demo shows framework working in enterprise environment.

Conclusion: EmbeWebAgent enables embedded web agents with application-level access, overcoming limitations of traditional human-interface-level agents through direct UI integration.

Abstract: Most web agents operate at the human interface level, observing screenshots or raw DOM trees without application-level access, which limits robustness and action expressiveness. In enterprise settings, however, explicit control of both the frontend and backend is available. We present EmbeWebAgent, a framework for embedding agents directly into existing UIs using lightweight frontend hooks (curated ARIA and URL-based observations, and a per-page function registry exposed via a WebSocket) and a reusable backend workflow that performs reasoning and takes actions. EmbeWebAgent is stack-agnostic (e.g., React or Angular), supports mixed-granularity actions ranging from GUI primitives to higher-level composites, and orchestrates navigation, manipulation, and domain-specific analytics via MCP tools. Our demo shows minimal retrofitting effort and robust multi-step behaviors grounded in a live UI setting. Live Demo: https://youtu.be/Cy06Ljee1JQ

</details>


### [265] [Concept Influence: Leveraging Interpretability to Improve Performance and Efficiency in Training Data Attribution](https://arxiv.org/abs/2602.14869)
*Matthew Kowal,Goncalo Paulo,Louis Jaburi,Tom Tseng,Lev E McKinney,Stefan Heimersheim,Aaron David Tucker,Adam Gleave,Kellin Pelrine*

Main category: cs.AI

TL;DR: Concept Influence: A scalable TDA method that attributes model behavior to semantic concepts rather than individual examples, using interpretable structures like linear probes for faster attribution.


<details>
  <summary>Details</summary>
Motivation: Existing TDA methods are computationally expensive and biased toward syntactic similarity rather than semantic behavior attribution. Need scalable methods to identify training data driving specific behaviors, especially unintended ones.

Method: Introduces Concept Influence that attributes model behavior to semantic directions (linear probes or sparse autoencoder features) instead of individual test examples. Shows probe-based methods are first-order approximations of Concept Influence that are much faster.

Result: Concept Influence achieves comparable performance to classical influence functions while being substantially more scalable (over an order-of-magnitude faster). Validated across emergent misalignment benchmarks and real post-training datasets.

Conclusion: Incorporating interpretable structure within TDA pipelines enables more scalable, explainable, and better control of model behavior through data attribution.

Abstract: As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influence functions are both computationally expensive and attribute based on single test examples, which can bias results toward syntactic rather than semantic similarity. To address these issues of scalability and influence to abstract behavior, we leverage interpretable structures within the model during the attribution. First, we introduce Concept Influence which attribute model behavior to semantic directions (such as linear probes or sparse autoencoder features) rather than individual test examples. Second, we show that simple probe-based attribution methods are first-order approximations of Concept Influence that achieve comparable performance while being over an order-of-magnitude faster. We empirically validate Concept Influence and approximations across emergent misalignment benchmarks and real post-training datasets, and demonstrate they achieve comparable performance to classical influence functions while being substantially more scalable. More broadly, we show that incorporating interpretable structure within traditional TDA pipelines can enable more scalable, explainable, and better control of model behavior through data.

</details>


### [266] [Lifted Relational Probabilistic Inference via Implicit Learning](https://arxiv.org/abs/2602.14890)
*Luise Ge,Brendan Juba,Kris Nilsson,Alison Shao*

Main category: cs.AI

TL;DR: First polynomial-time framework that implicitly learns first-order probabilistic logic and performs lifted inference over individuals and worlds without constructing explicit models.


<details>
  <summary>Details</summary>
Motivation: Reconcile the tension between inductive learning and deductive reasoning in first-order relational domains, addressing the challenge of learning probabilistic models from partial, noisy observations which is intractable in traditional approaches.

Method: Merge incomplete first-order axioms with partially observed examples into a bounded-degree fragment of the sum-of-squares (SOS) hierarchy using two simultaneous lifts: grounding-lift (renaming-equivalent ground moments share variables) and world-lift (enforcing all pseudo-models in parallel).

Result: Developed first polynomial-time framework that implicitly learns first-order probabilistic logic and performs lifted inference over both individuals and worlds without constructing explicit models.

Conclusion: Successfully reconciles inductive learning and deductive reasoning through implicit learning-to-reason approach, enabling efficient probabilistic inference in first-order relational domains without explicit model construction.

Abstract: Reconciling the tension between inductive learning and deductive reasoning in first-order relational domains is a longstanding challenge in AI. We study the problem of answering queries in a first-order relational probabilistic logic through a joint effort of learning and reasoning, without ever constructing an explicit model. Traditional lifted inference assumes access to a complete model and exploits symmetry to evaluate probabilistic queries; however, learning such models from partial, noisy observations is intractable in general. We reconcile these two challenges through implicit learning to reason and first-order relational probabilistic inference techniques. More specifically, we merge incomplete first-order axioms with independently sampled, partially observed examples into a bounded-degree fragment of the sum-of-squares (SOS) hierarchy in polynomial time. Our algorithm performs two lifts simultaneously: (i) grounding-lift, where renaming-equivalent ground moments share one variable, collapsing the domain of individuals; and (ii) world-lift, where all pseudo-models (partial world assignments) are enforced in parallel, producing a global bound that holds across all worlds consistent with the learned constraints. These innovations yield the first polynomial-time framework that implicitly learns a first-order probabilistic logic and performs lifted inference over both individuals and worlds.

</details>


### [267] [The Potential of CoT for Reasoning: A Closer Look at Trace Dynamics](https://arxiv.org/abs/2602.14903)
*Gregor Bachmann,Yichen Jiang,Seyed Mohsen Moosavi Dezfooli,Moin Nabi*

Main category: cs.AI

TL;DR: The paper analyzes Chain-of-Thought (CoT) prompting in LLMs using a "potential" metric to quantify how different parts of reasoning traces contribute to correct answers, revealing non-monotonic patterns, reasoning insights, and transferability between models.


<details>
  <summary>Details</summary>
Motivation: While CoT prompting is widely used to elicit reasoning-like responses from LLMs, the underlying mechanisms driving its success remain poorly understood. The authors aim to better understand how and which parts of CoT reasoning actually contribute to final answers.

Method: The authors introduce a "potential" metric that quantifies how much a given part of CoT increases the likelihood of a correct completion. They analyze CoT traces from competition-level mathematics questions, examining patterns in the potential. They also investigate CoT transferability by measuring the potential of a weaker model under partial CoT from a stronger model.

Result: The analysis reveals surprising patterns: (1) strong non-monotonicity due to reasoning tangents, (2) sharp spikes indicating reasoning insights/jumps, (3) lucky guesses where correct answers appear without relevant justifications. Transferability experiments show that as little as 20% of partial CoT from a stronger model can "unlock" the weaker model's performance on previously unsolvable problems.

Conclusion: While some CoT behaviors align with human intuition (insights and tangents), others remain difficult to interpret. The transferability findings suggest that a significant portion of CoT mechanics are transferable between models, highlighting that reasoning insights can be shared and leveraged across different LLM capabilities.

Abstract: Chain-of-thought (CoT) prompting is a de-facto standard technique to elicit reasoning-like responses from large language models (LLMs), allowing them to spell out individual steps before giving a final answer. While the resemblance to human-like reasoning is undeniable, the driving forces underpinning the success of CoT reasoning still remain largely unclear. In this work, we perform an in-depth analysis of CoT traces originating from competition-level mathematics questions, with the aim of better understanding how, and which parts of CoT actually contribute to the final answer. To this end, we introduce the notion of a potential, quantifying how much a given part of CoT increases the likelihood of a correct completion. Upon examination of reasoning traces through the lens of the potential, we identify surprising patterns including (1) its often strong non-monotonicity (due to reasoning tangents), (2) very sharp but sometimes tough to interpret spikes (reasoning insights and jumps) as well as (3) at times lucky guesses, where the model arrives at the correct answer without providing any relevant justifications before. While some of the behaviours of the potential are readily interpretable and align with human intuition (such as insights and tangents), others remain difficult to understand from a human perspective. To further quantify the reliance of LLMs on reasoning insights, we investigate the notion of CoT transferability, where we measure the potential of a weaker model under the partial CoT from another, stronger model. Indeed aligning with our previous results, we find that as little as 20% of partial CoT can ``unlock'' the performance of the weaker model on problems that were previously unsolvable for it, highlighting that a large part of the mechanics underpinning CoT are transferable.

</details>


### [268] [Position: Introspective Experience from Conversational Environments as a Path to Better Learning](https://arxiv.org/abs/2602.14910)
*Claudiu Cristian Musat,Jackson Tolins,Diego Antognini,Jingling Li,Martin Klissarov,Tom Duerig*

Main category: cs.AI

TL;DR: The paper argues that robust AI reasoning emerges from linguistic self-reflection learned through high-quality social interaction, not just from scale. It proposes introspection and dialogue quality as key to next-generation general intelligence.


<details>
  <summary>Details</summary>
Motivation: Current AI training treats reasoning as an emergent property of scale, but the authors argue this approach is insufficient. They propose that true robust reasoning emerges from linguistic self-reflection, which develops through high-quality social interactions, drawing inspiration from Vygotskian developmental psychology.

Method: The paper advances three core positions: 1) Social Genesis of the Private Mind - learning from conversational environments where alignment friction refines reasoning; 2) Dialogically scaffolded introspective experiences that transform raw data into learnable narratives; 3) Dialogue Quality as the New Data Quality - where reasoning depth depends on the diversity and rigor of mastered dialogues.

Result: The paper presents a theoretical framework that shifts focus from scale-based training to conversation-based reasoning development. It establishes dialogue quality as a critical factor for efficient test-time compute and deep private reasoning capabilities.

Conclusion: Optimizing conversational scaffolds is the primary lever for developing the next generation of general intelligence, moving beyond scale-based approaches to focus on high-quality dialogue and introspective learning from social interactions.

Abstract: Current approaches to AI training treat reasoning as an emergent property of scale. We argue instead that robust reasoning emerges from linguistic self-reflection, itself internalized from high-quality social interaction. Drawing on Vygotskian developmental psychology, we advance three core positions centered on Introspection. First, we argue for the Social Genesis of the Private Mind: learning from conversational environments rises to prominence as a new way to make sense of the world; the friction of aligning with another agent, internal or not, refines and crystallizes the reasoning process. Second, we argue that dialogically scaffolded introspective experiences allow agents to engage in sense-making that decouples learning from immediate data streams, transforming raw environmental data into rich, learnable narratives. Finally, we contend that Dialogue Quality is the New Data Quality: the depth of an agent's private reasoning, and its efficiency regarding test-time compute, is determined by the diversity and rigor of the dialogues it has mastered. We conclude that optimizing these conversational scaffolds is the primary lever for the next generation of general intelligence.

</details>


### [269] [ReusStdFlow: A Standardized Reusability Framework for Dynamic Workflow Construction in Agentic AI](https://arxiv.org/abs/2602.14922)
*Gaoyang Zhang,Shanghong Zou,Yafang Wang,He Zhang,Ruohua Xu,Feng Zhao*

Main category: cs.AI

TL;DR: ReusStdFlow framework solves enterprise Agentic AI's reusability dilemma by standardizing workflow extraction, storage, and construction with over 90% accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the "reusability dilemma" and structural hallucinations in enterprise Agentic AI by enabling automated reorganization and efficient reuse of enterprise digital assets.

Method: Proposes ReusStdFlow framework with "Extraction-Storage-Construction" paradigm: 1) Deconstructs heterogeneous DSLs into standardized workflow segments, 2) Uses dual knowledge architecture (graph + vector databases) for synergistic retrieval of topological structures and functional semantics, 3) Intelligently assembles workflows using RAG strategy.

Result: Tested on 200 real-world n8n workflows, achieves over 90% accuracy in both extraction and construction phases.

Conclusion: Provides a standardized solution for automated reorganization and efficient reuse of enterprise digital assets, effectively addressing the reusability dilemma in Agentic AI systems.

Abstract: To address the ``reusability dilemma'' and structural hallucinations in enterprise Agentic AI,this paper proposes ReusStdFlow, a framework centered on a novel ``Extraction-Storage-Construction'' paradigm. The framework deconstructs heterogeneous, platform-specific Domain Specific Languages (DSLs) into standardized, modular workflow segments. It employs a dual knowledge architecture-integrating graph and vector databases-to facilitate synergistic retrieval of both topological structures and functional semantics. Finally, workflows are intelligently assembled using a retrieval-augmented generation (RAG) strategy. Tested on 200 real-world n8n workflows, the system achieves over 90% accuracy in both extraction and construction. This framework provides a standardized solution for the automated reorganization and efficient reuse of enterprise digital assets.

</details>


### [270] [MAC-AMP: A Closed-Loop Multi-Agent Collaboration System for Multi-Objective Antimicrobial Peptide Design](https://arxiv.org/abs/2602.14926)
*Gen Zhou,Sugitha Janarthanan,Lianghong Chen,Pingzhao Hu*

Main category: cs.AI

TL;DR: MAC-AMP is a closed-loop multi-agent LLM system for designing antimicrobial peptides that optimizes multiple properties (activity, toxicity, novelty) through autonomous peer review and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Current AI models for AMP design struggle to balance multiple objectives (activity, toxicity, novelty) and use unclear scoring methods, making results hard to interpret. LLM-based multi-agent systems offer promising potential for complex scientific design scenarios.

Method: MAC-AMP uses a closed-loop multi-agent collaboration system with a fully autonomous simulated peer review-adaptive reinforcement learning framework. It requires only task description and example dataset to design novel AMPs, supporting multi-objective optimization while remaining explainable.

Result: MAC-AMP outperforms other AMP generative models by effectively optimizing AMP generation for multiple key molecular properties, demonstrating exceptional results in antibacterial activity, AMP likeliness, toxicity compliance, and structural reliability.

Conclusion: The system introduces a novel closed-loop multi-agent approach for AMP design with cross-domain transferability, supporting multi-objective optimization while maintaining explainability rather than being a 'black box'.

Abstract: To address the global health threat of antimicrobial resistance, antimicrobial peptides (AMP) are being explored for their potent and promising ability to fight resistant pathogens. While artificial intelligence (AI) is being employed to advance AMP discovery and design, most AMP design models struggle to balance key goals like activity, toxicity, and novelty, using rigid or unclear scoring methods that make results hard to interpret and optimize. As the capabilities of Large Language Models (LLM) advance and evolve swiftly, we turn to AI multi-agent collaboration based on such models (multi-agent LLMs), which show rapidly rising potential in complex scientific design scenarios. Based on this, we introduce MAC-AMP, a closed-loop multi-agent collaboration (MAC) system for multi-objective AMP design. The system implements a fully autonomous simulated peer review-adaptive reinforcement learning framework that requires only a task description and example dataset to design novel AMPs. The novelty of our work lies in introducing a closed-loop multi-agent system for AMP design, with cross-domain transferability, that supports multi-objective optimization while remaining explainable rather than a 'black box'. Experiments show that MAC-AMP outperforms other AMP generative models by effectively optimizing AMP generation for multiple key molecular properties, demonstrating exceptional results in antibacterial activity, AMP likeliness, toxicity compliance, and structural reliability.

</details>


### [271] [On the Semantics of Primary Cause in Hybrid Dynamic Domains](https://arxiv.org/abs/2602.14994)
*Shakil M. Khan,Asim Mehmood,Sandra Zilles*

Main category: cs.AI

TL;DR: Proposes two equivalent definitions of primary cause in a hybrid action-theoretic framework combining discrete and continuous change, with verification through modified "but-for" tests.


<details>
  <summary>Details</summary>
Motivation: Reasoning about actual causes is fundamental to rationality but existing research has largely focused on discrete change, with limited work on continuous/hybrid systems. The paper addresses this gap by developing causation theories for hybrid environments where change can be both discrete and continuous.

Method: Builds on the hybrid temporal situation calculus framework. Proposes two definitions of primary cause: one foundational in nature and another formalizing causation through contributions, verified using a modified "but-for" test. Proves equivalence between the two definitions.

Result: Establishes two equivalent definitions of primary cause in hybrid systems. Demonstrates that these definitions possess intuitively justifiable properties for causal reasoning in environments with both discrete and continuous change.

Conclusion: The paper successfully extends causal reasoning to hybrid action-theoretic frameworks, providing mathematically sound definitions that handle both discrete and continuous change while maintaining intuitive properties for practical application.

Abstract: Reasoning about actual causes of observed effects is fundamental to the study of rationality. This important problem has been studied since the time of Aristotle, with formal mathematical accounts emerging recently. We live in a world where change due to actions can be both discrete and continuous, that is, hybrid. Yet, despite extensive research on actual causation, only few recent studies looked into causation with continuous change. Building on recent progress, in this paper we propose two definitions of primary cause in a hybrid action-theoretic framework, namely the hybrid temporal situation calculus. One of these is foundational in nature while the other formalizes causation through contributions, which can then be verified from a counterfactual perspective using a modified ``but-for'' test. We prove that these two definitions are indeed equivalent. We then show that our definitions of causation have some intuitively justifiable properties.

</details>


### [272] [Hunt Globally: Deep Research AI Agents for Drug Asset Scouting in Investing, Business Development, and Search & Evaluation](https://arxiv.org/abs/2602.15019)
*Alisa Vinogradova,Vlad Vinogradov,Luba Greenwood,Ilya Yasny,Dmitry Kobyzev,Shoman Kasbekar,Kong Nguyen,Dmitrii Radkevich,Roman Doronin,Andrey Doronichev*

Main category: cs.AI

TL;DR: The paper introduces a benchmarking methodology and a tree-based self-learning Bioptic Agent for drug asset scouting that outperforms existing AI systems in discovering non-U.S., multilingual pharmaceutical innovations with minimal hallucinations.


<details>
  <summary>Details</summary>
Motivation: The pharmaceutical innovation landscape has shifted globally, with most new drug assets now originating outside the U.S. and being disclosed in non-English channels. Current AI agents fail to achieve high-recall discovery across heterogeneous, multilingual sources without hallucinations, creating multi-billion-dollar risks for investors and business development teams.

Method: The authors propose a benchmarking methodology using a multilingual multi-agent pipeline with complex user queries paired with ground-truth assets outside U.S.-centric radar. They collected screening queries from expert professionals and used them to conditionally generate benchmark queries. They developed a tuned, tree-based self-learning Bioptic Agent for complete, non-hallucinated scouting, evaluated using LLM-as-judge methodology calibrated to expert opinions.

Result: Bioptic Agent achieved 79.7% F1 score, significantly outperforming competing systems: Claude Opus 4.6 (56.2%), Gemini 3 Pro + Deep Research (50.6%), GPT-5.2 Pro (46.6%), Perplexity Deep Research (44.2%), and Exa Websets (26.9%). Performance improved steeply with additional compute.

Conclusion: The proposed Bioptic Agent demonstrates superior performance in drug asset scouting across multilingual sources, addressing critical coverage gaps in pharmaceutical innovation discovery. The results support the view that more compute yields better results in this domain.

Abstract: Bio-pharmaceutical innovation has shifted: many new drug assets now originate outside the United States and are disclosed primarily via regional, non-English channels. Recent data suggests >85% of patent filings originate outside the U.S., with China accounting for nearly half of the global total; a growing share of scholarly output is also non-U.S. Industry estimates put China at ~30% of global drug development, spanning 1,200+ novel candidates. In this high-stakes environment, failing to surface "under-the-radar" assets creates multi-billion-dollar risk for investors and business development teams, making asset scouting a coverage-critical competition where speed and completeness drive value. Yet today's Deep Research AI agents still lag human experts in achieving high-recall discovery across heterogeneous, multilingual sources without hallucinations.
  We propose a benchmarking methodology for drug asset scouting and a tuned, tree-based self-learning Bioptic Agent aimed at complete, non-hallucinated scouting. We construct a challenging completeness benchmark using a multilingual multi-agent pipeline: complex user queries paired with ground-truth assets that are largely outside U.S.-centric radar. To reflect real deal complexity, we collected screening queries from expert investors, BD, and VC professionals and used them as priors to conditionally generate benchmark queries. For grading, we use LLM-as-judge evaluation calibrated to expert opinions. We compare Bioptic Agent against Claude Opus 4.6, OpenAI GPT-5.2 Pro, Perplexity Deep Research, Gemini 3 Pro + Deep Research, and Exa Websets. Bioptic Agent achieves 79.7% F1 versus 56.2% (Claude Opus 4.6), 50.6% (Gemini 3 Pro + Deep Research), 46.6% (GPT-5.2 Pro), 44.2% (Perplexity Deep Research), and 26.9% (Exa Websets). Performance improves steeply with additional compute, supporting the view that more compute yields better results.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [273] [Deep Learning CNN for Pneumonia Detection: Advancing Digital Health in Society 5.0](https://arxiv.org/abs/2602.13270)
*Hadi Almohab*

Main category: eess.IV

TL;DR: CNN-based deep learning model achieves 91.67% accuracy in automated pneumonia detection from chest X-rays, showing potential as a reliable diagnostic aid.


<details>
  <summary>Details</summary>
Motivation: Pneumonia is a serious global health problem with high morbidity and mortality, especially in areas with limited diagnostic tools and healthcare resources. There's a need for automated, reliable diagnostic assistance.

Method: Developed a Convolutional Neural Network (CNN) based on deep learning, trained on labeled chest X-ray datasets with preprocessing techniques including normalization, data augmentation, and image quality enhancement to improve robustness and generalization.

Result: The optimized model achieves 91.67% accuracy, ROC-AUC of 0.96, and PR-AUC of 0.95, demonstrating strong performance in distinguishing pneumonia from normal chest X-ray images.

Conclusion: The CNN model has significant potential as a fast, consistent, and reliable diagnostic aid, supporting Society 5.0 by integrating artificial intelligence to improve healthcare services and public well-being.

Abstract: Pneumonia is a serious global health problem, contributing to high morbidity and mortality, especially in areas with limited diagnostic tools and healthcare resources. This study develops a Convolutional Neural Network (CNN) based on deep learning to automatically detect pneumonia from chest X-ray images. The method involves training the model on labeled datasets with preprocessing techniques such as normalization, data augmentation, and image quality enhancement to improve robustness and generalization. Testing results show that the optimized model achieves 91.67% accuracy, ROC-AUC of 0.96, and PR-AUC of 0.95, demonstrating strong performance in distinguishing pneumonia from normal images. In conclusion, this CNN model has significant potential as a fast, consistent, and reliable diagnostic aid, supporting Society 5.0 by integrating artificial intelligence to improve healthcare services and public well-being.

</details>


### [274] [Learning to Select Like Humans: Explainable Active Learning for Medical Imaging](https://arxiv.org/abs/2602.13308)
*Ifrat Ikhtear Uddin,Longwei Wang,Xiao Qin,Yang Zhou,KC Santosh*

Main category: eess.IV

TL;DR: Explainability-guided active learning framework that combines classification uncertainty with attention misalignment to select samples that improve both predictive performance and clinical interpretability in medical imaging.


<details>
  <summary>Details</summary>
Motivation: Medical image analysis requires expensive expert annotation. Traditional active learning methods only consider predictive uncertainty and ignore whether models learn from clinically meaningful features, which is critical for clinical deployment.

Method: Proposes a dual-criterion selection strategy: (1) classification uncertainty to identify informative examples, and (2) attention misalignment between Grad-CAM attention maps and radiologist-defined ROIs using Dice similarity. This framework integrates spatial attention alignment into sample acquisition process.

Result: Using only 570 strategically selected samples, the approach outperforms random sampling across all three datasets: 77.22% accuracy on BraTS (MRI brain tumors), 52.37% on VinDr-CXR (chest X-rays), and 52.66% on SIIM-COVID-19 (chest X-rays). Grad-CAM visualizations confirm models focus on diagnostically relevant regions.

Conclusion: Incorporating explanation guidance into sample acquisition yields superior data efficiency while maintaining clinical interpretability. The framework demonstrates that combining predictive uncertainty with spatial attention alignment enhances both model performance and clinical relevance.

Abstract: Medical image analysis requires substantial labeled data for model training, yet expert annotation is expensive and time-consuming. Active learning (AL) addresses this challenge by strategically selecting the most informative samples for the annotation purpose, but traditional methods solely rely on predictive uncertainty while ignoring whether models learn from clinically meaningful features a critical requirement for clinical deployment. We propose an explainability-guided active learning framework that integrates spatial attention alignment into a sample acquisition process. Our approach advocates for a dual-criterion selection strategy combining: (i) classification uncertainty to identify informative examples, and (ii) attention misalignment with radiologist-defined regions-of-interest (ROIs) to target samples where the model focuses on incorrect features. By measuring misalignment between Grad-CAM attention maps and expert annotations using \emph{Dice similarity}, our acquisition function judiciously identifies samples that enhance both predictive performance and spatial interpretability. We evaluate the framework using three expert-annotated medical imaging datasets, namely, BraTS (MRI brain tumors), VinDr-CXR (chest X-rays), and SIIM-COVID-19 (chest X-rays). Using only 570 strategically selected samples, our explainability-guided approach consistently outperforms random sampling across all the datasets, achieving 77.22\% accuracy on BraTS, 52.37\% on VinDr-CXR, and 52.66\% on SIIM-COVID. Grad-CAM visualizations confirm that the models trained by our dual-criterion selection focus on diagnostically relevant regions, demonstrating that incorporating explanation guidance into sample acquisition yields superior data efficiency while maintaining clinical interpretability.

</details>


### [275] [FUTON: Fourier Tensor Network for Implicit Neural Representations](https://arxiv.org/abs/2602.13414)
*Pooya Ashtari,Pourya Behmandpoor,Nikos Deligiannis,Aleksandra Pizurica*

Main category: eess.IV

TL;DR: FUTON (Fourier Tensor Network) is a new implicit neural representation that uses Fourier series with low-rank tensor decomposition for better signal encoding than MLP-based approaches.


<details>
  <summary>Details</summary>
Motivation: MLP-based implicit neural representations (INRs) suffer from slow convergence, overfitting to noise, and poor extrapolation. There's a need for better architectures that combine complementary inductive biases for improved signal representation.

Method: FUTON models signals as generalized Fourier series whose coefficients are parameterized by low-rank tensor decomposition. It uses orthonormal, separable basis functions combining Fourier bases (for smoothness/periodicity) with low-rank parameterization (for low-dimensional spectral structure).

Result: FUTON outperforms state-of-the-art MLP-based INRs on image and volume representation while training 2-5× faster. On inverse problems like denoising and super-resolution, it generalizes better and converges faster. Theoretical guarantees include universal approximation theorem and linear complexity inference algorithm.

Conclusion: FUTON provides an effective alternative to MLP-based INRs by combining Fourier bases with low-rank tensor decomposition, offering better performance, faster training, and improved generalization for signal representation and inverse problems.

Abstract: Implicit neural representations (INRs) have emerged as powerful tools for encoding signals, yet dominant MLP-based designs often suffer from slow convergence, overfitting to noise, and poor extrapolation. We introduce FUTON (Fourier Tensor Network), which models signals as generalized Fourier series whose coefficients are parameterized by a low-rank tensor decomposition. FUTON implicitly expresses signals as weighted combinations of orthonormal, separable basis functions, combining complementary inductive biases: Fourier bases capture smoothness and periodicity, while the low-rank parameterization enforces low-dimensional spectral structure. We provide theoretical guarantees through a universal approximation theorem and derive an inference algorithm with complexity linear in the spectral resolution and the input dimension. On image and volume representation, FUTON consistently outperforms state-of-the-art MLP-based INRs while training 2--5$\times$ faster. On inverse problems such as image denoising and super-resolution, FUTON generalizes better and converges faster.

</details>


### [276] [A real-time UAS hyperspectral anomaly detection system](https://arxiv.org/abs/2602.13509)
*Thomas P. Watson,Kevin McKenzie,Joseph Conroy,Eddie L. Jacobs*

Main category: eess.IV

TL;DR: Real-time anomaly detection for hyperspectral images on UAS with wireless transmission to ground station for immediate operator interaction.


<details>
  <summary>Details</summary>
Motivation: Current hyperspectral anomaly detection requires post-processing which delays insight; need for real-time detection and immediate operator interaction.

Method: Deploy anomaly detection algorithm on VNIR push-broom hyperspectral sensor onboard UAS, coupled with fast georectification algorithm for real-time processing and wireless transmission of concise anomaly data.

Result: Demonstrated complete end-to-end real-time solution from data capture through anomaly detection, transmission, to ground station display and interaction using low-cost components.

Conclusion: Successfully implemented novel real-time hyperspectral anomaly detection system that enables immediate operator investigation and characterization of anomalous areas.

Abstract: Detecting anomalies in hyperspectral image data, i.e. regions which are spectrally distinct from the image background, is a common task in hyperspectral imaging. Such regions may represent interesting objects to human operators, but obtaining results often requires post-processing of captured data, delaying insight. To address this limitation, we apply an anomaly detection algorithm to a visible and near-infrared (VNIR) push-broom hyperspectral image sensor in real time onboard a small uncrewed aerial system (UAS), exploring how UAS limitations affect the algorithm. As the generated anomaly information is much more concise than the raw hyperspectral data, it can feasibly be transmitted wirelessly. To detection, we couple an innovative and fast georectification algorithm that enables anomalous areas to be interactively investigated and characterized immediately by a human operator receiving the anomaly data at a ground station. Using these elements, we demonstrate a novel and complete end-to-end solution from data capture and preparation, through anomaly detection and transmission, to ground station display and interaction, all in real time and with relatively low cost components.

</details>


### [277] [Frequency-Enhanced Hilbert Scanning Mamba for Short-Term Arctic Sea Ice Concentration Prediction](https://arxiv.org/abs/2602.13522)
*Feng Gao,Zheng Gong,Wenli Liu,Yanhai Gan,Zhuoran Zheng,Junyu Dong,Qian Du*

Main category: eess.IV

TL;DR: FH-Mamba improves Arctic sea ice concentration prediction using 3D Hilbert scanning for temporal correlations and wavelet transforms for boundary details, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Vanilla Mamba models struggle with temporal correlations and boundary details in Arctic sea ice concentration prediction, limiting their effectiveness for short-term forecasting.

Method: Proposes Frequency-enhanced Hilbert scanning Mamba Framework (FH-Mamba) with: 1) 3D Hilbert scan mechanism for locality-preserving spatiotemporal traversal, 2) wavelet transform to amplify high-frequency details, and 3) Hybrid Shuffle Attention module to adaptively aggregate sequence and frequency features.

Result: Experiments on OSI-450a1 and AMSR2 datasets show FH-Mamba achieves superior prediction performance compared to state-of-the-art baselines, confirming effectiveness in improving temporal consistency and edge reconstruction.

Conclusion: FH-Mamba effectively addresses Mamba's limitations for Arctic SIC prediction through Hilbert scanning and frequency-aware attention, providing better temporal correlation modeling and boundary detail preservation.

Abstract: While Mamba models offer efficient sequence modeling, vanilla versions struggle with temporal correlations and boundary details in Arctic sea ice concentration (SIC) prediction. To address these limitations, we propose Frequency-enhanced Hilbert scanning Mamba Framework (FH-Mamba) for short-term Arctic SIC prediction. Specifically, we introduce a 3D Hilbert scan mechanism that traverses the 3D spatiotemporal grid along a locality-preserving path, ensuring that adjacent indices in the flattened sequence correspond to neighboring voxels in both spatial and temporal dimensions. Additionally, we incorporate wavelet transform to amplify high-frequency details and we also design a Hybrid Shuffle Attention module to adaptively aggregate sequence and frequency features. Experiments conducted on the OSI-450a1 and AMSR2 datasets demonstrate that our FH-Mamba achieves superior prediction performance compared with state-of-the-art baselines. The results confirm the effectiveness of Hilbert scanning and frequency-aware attention in improving both temporal consistency and edge reconstruction for Arctic SIC forecasting. Our codes are publicly available at https://github.com/oucailab/FH-Mamba.

</details>


### [278] [NeuroMambaLLM: Dynamic Graph Learning of fMRI Functional Connectivity in Autistic Brains Using Mamba and Language Model Reasoning](https://arxiv.org/abs/2602.13770)
*Yasaman Torabi,Parsa Razmara,Hamed Ajorlou,Bardia Baraeinejad*

Main category: eess.IV

TL;DR: NeuroMambaLLM integrates dynamic brain connectivity learning with LLMs for autism diagnosis and clinical report generation from fMRI data.


<details>
  <summary>Details</summary>
Motivation: Current fMRI analysis methods use static connectivity representations that miss transient neural dynamics important for neurodevelopmental disorders like autism. LLMs have strong semantic reasoning but aren't well integrated with brain connectivity models, and state-space approaches like Mamba are used standalone without high-level reasoning.

Method: End-to-end framework combining dynamic latent graph learning and selective state-space temporal modeling (Mamba) with LLMs. Learns functional connectivity dynamically from raw BOLD time series, replaces fixed correlation graphs with adaptive latent connectivity, suppresses motion artifacts, captures long-range dependencies. Dynamic brain representations are projected into LLM embedding space using frozen base LLM with lightweight LoRA modules for parameter-efficient alignment.

Result: Enables LLM to perform both diagnostic classification and language-based reasoning, analyzing dynamic fMRI patterns and generating clinically meaningful textual reports.

Conclusion: NeuroMambaLLM successfully integrates dynamic brain connectivity modeling with LLM reasoning capabilities for improved autism diagnosis and clinical report generation from fMRI data.

Abstract: Large Language Models (LLMs) have demonstrated strong semantic reasoning across multimodal domains. However, their integration with graph-based models of brain connectivity remains limited. In addition, most existing fMRI analysis methods rely on static Functional Connectivity (FC) representations, which obscure transient neural dynamics critical for neurodevelopmental disorders such as autism. Recent state-space approaches, including Mamba, model temporal structure efficiently, but are typically used as standalone feature extractors without explicit high-level reasoning. We propose NeuroMambaLLM, an end-to-end framework that integrates dynamic latent graph learning and selective state-space temporal modelling with LLMs. The proposed method learns the functional connectivity dynamically from raw Blood-Oxygen-Level-Dependent (BOLD) time series, replacing fixed correlation graphs with adaptive latent connectivity while suppressing motion-related artifacts and capturing long-range temporal dependencies. The resulting dynamic brain representations are projected into the embedding space of an LLM model, where the base language model remains frozen and lightweight low-rank adaptation (LoRA) modules are trained for parameter-efficient alignment. This design enables the LLM to perform both diagnostic classification and language-based reasoning, allowing it to analyze dynamic fMRI patterns and generate clinically meaningful textual reports.

</details>


### [279] [A Deep Convolutional Network to Extract Real-Time Landmarks for UAV Navigation](https://arxiv.org/abs/2602.13814)
*Osman Tokluoglu,Mustafa Ozturk*

Main category: eess.IV

TL;DR: A deep learning approach using convolutional networks is proposed for extracting visual landmarks from UAV camera images to enable navigation in GNSS-denied environments where satellite signals are unavailable.


<details>
  <summary>Details</summary>
Motivation: GNSS signals can be degraded or lost due to environmental conditions or intentional jamming, creating GNSS-denied environments where traditional positioning fails. This is particularly problematic for UAVs used in monitoring applications that need reliable navigation capabilities.

Method: The study proposes a convolution-based deep learning approach for extracting appropriate visual landmarks from images captured by onboard UAV cameras. This enables navigation without GNSS support by identifying reliable visual landmarks.

Result: The effectiveness of the proposed convolution-based deep learning approach for landmark extraction is examined, though specific performance metrics are not provided in the abstract.

Conclusion: Visual landmark extraction using deep learning offers a promising solution for UAV navigation in GNSS-denied environments, addressing the critical need for alternative positioning methods when satellite signals are unavailable.

Abstract: Recent advances in satellite and communication technologies have significantly improved geographical information and monitoring systems. Global System for Mobile Communications (GSM) and Global Navigation Satellite System (GNSS) technologies, which rely on electromagnetic signals transmitted from satellites and base stations, have long been utilized for geolocation applications. However, signal attenuation due to environmental conditions or intentional interference such as jamming may lead to severe degradation or complete loss of positioning capability. In such GNSS-denied environments, landmark extraction becomes critical for the navigation of unmanned aerial vehicles (UAVs) used in monitoring applications. By processing images captured from onboard UAV cameras, reliable visual landmarks can be identified to enable navigation without GNSS support. In this study, a convolution-based deep learning approach is proposed for the extraction of appropriate landmarks, and its effectiveness is examined.

</details>


### [280] [Scan-Adaptive Dynamic MRI Undersampling Using a Dictionary of Efficiently Learned Patterns](https://arxiv.org/abs/2602.13984)
*Siddhant Gautam,Angqi Li,Prachi P. Agarwal,Anil K. Attili,Jeffrey A. Fessler,Nicole Seiberlich,Saiprasad Ravishankar*

Main category: eess.IV

TL;DR: Learning-based framework for scan-adaptive Cartesian undersampling patterns in dynamic cardiac MRI to accelerate acquisition while preserving diagnostic quality.


<details>
  <summary>Details</summary>
Motivation: Long acquisition times in cardiac MRI cause patient discomfort and motion artifacts, necessitating acceleration methods that maintain diagnostic image quality.

Method: Developed learning-based framework to design scan- or slice-adaptive Cartesian undersampling masks optimized using fully sampled training data. At inference, nearest-neighbor search in low-frequency k-space selects optimal mask from learned pattern dictionary.

Result: Improved reconstruction quality across multiple acceleration factors on public and in-house datasets: 2-3 dB PSNR gains, reduced NMSE, improved SSIM, and higher radiologist ratings.

Conclusion: The scan-adaptive sampling framework enables faster, higher-quality dynamic cardiac MRI by adapting k-space sampling to individual scans.

Abstract: Cardiac MRI is limited by long acquisition times, which can lead to patient discomfort and motion artifacts. We aim to accelerate Cartesian dynamic cardiac MRI by learning efficient, scan-adaptive undersampling patterns that preserve diagnostic image quality. We develop a learning-based framework for designing scan- or slice-adaptive Cartesian undersampling masks tailored to dynamic cardiac MRI. Undersampling patterns are optimized using fully sampled training dynamic time-series data. At inference time, a nearest-neighbor search in low-frequency $k$-space selects an optimized mask from a dictionary of learned patterns. Our learned sampling approach improves reconstruction quality across multiple acceleration factors on public and in-house cardiac MRI datasets, including PSNR gains of 2-3 dB, reduced NMSE, improved SSIM, and higher radiologist ratings. The proposed scan-adaptive sampling framework enables faster and higher-quality dynamic cardiac MRI by adapting $k$-space sampling to individual scans.

</details>


### [281] [Learnable Multi-level Discrete Wavelet Transforms for 3D Gaussian Splatting Frequency Modulation](https://arxiv.org/abs/2602.14199)
*Hung Nguyen,An Le,Truong Nguyen*

Main category: eess.IV

TL;DR: Proposes multi-level DWT-based frequency modulation for 3DGS to reduce Gaussian primitives while maintaining rendering quality, improving on AutoOpti3DGS's single-level approach.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting (3DGS) suffers from excessive growth of Gaussian primitives during training, increasing memory and storage costs. Existing coarse-to-fine strategies like AutoOpti3DGS have limitations: single-level DWT restricts modulation depth, and joint optimization causes gradient competition that promotes excessive Gaussian densification.

Method: Multi-level DWT-based frequency modulation framework that recursively decomposes low-frequency subbands to create deeper curriculum. Uses progressive coarser supervision during early training. Simplifies modulation to single scaling parameter instead of learning full 2-tap high-pass filter.

Result: Experimental results on standard benchmarks show further reduction in Gaussian counts while maintaining competitive rendering quality compared to existing methods.

Conclusion: The proposed multi-level DWT framework effectively reduces Gaussian primitive growth in 3DGS through deeper frequency modulation curriculum, addressing limitations of previous single-level approaches and achieving better efficiency without sacrificing quality.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful approach for novel view synthesis. However, the number of Gaussian primitives often grows substantially during training as finer scene details are reconstructed, leading to increased memory and storage costs. Recent coarse-to-fine strategies regulate Gaussian growth by modulating the frequency content of the ground-truth images. In particular, AutoOpti3DGS employs the learnable Discrete Wavelet Transform (DWT) to enable data-adaptive frequency modulation. Nevertheless, its modulation depth is limited by the 1-level DWT, and jointly optimizing wavelet regularization with 3D reconstruction introduces gradient competition that promotes excessive Gaussian densification. In this paper, we propose a multi-level DWT-based frequency modulation framework for 3DGS. By recursively decomposing the low-frequency subband, we construct a deeper curriculum that provides progressively coarser supervision during early training, consistently reducing Gaussian counts. Furthermore, we show that the modulation can be performed using only a single scaling parameter, rather than learning the full 2-tap high-pass filter. Experimental results on standard benchmarks demonstrate that our method further reduces Gaussian counts while maintaining competitive rendering quality.

</details>


### [282] [Deep Image Prior for Computed Tomography Reconstruction](https://arxiv.org/abs/2602.14709)
*Simon Arridge,Riccardo Barbano,Alexander Denker,Zeljko Kereta*

Main category: eess.IV

TL;DR: Deep Image Prior (DIP) framework overview for CT image reconstruction using unsupervised neural networks without large datasets, focusing on overfitting mitigation and computational improvements.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning for CT reconstruction requires large supervised datasets, which can be impractical. DIP offers an unsupervised alternative that works with single noisy measurements by leveraging neural network architecture biases.

Method: DIP uses convolutional neural networks in unsupervised setting with single measurements. Key strategies include early stopping, explicit regularization, self-guided methods with adaptive network inputs, warm-start initialization, and stochastic optimization for faster reconstruction.

Result: Methods tested on real μCT measurements, allowing examination of trade-offs among different modifications and extensions to the DIP framework for CT reconstruction.

Conclusion: DIP provides a viable unsupervised alternative to conventional deep learning for CT image reconstruction, with various strategies available to mitigate overfitting and improve computational efficiency.

Abstract: We present a comprehensive overview of the Deep Image Prior (DIP) framework and its applications to image reconstruction in computed tomography. Unlike conventional deep learning methods that rely on large, supervised datasets, the DIP exploits the implicit bias of convolutional neural networks and operates in a fully unsupervised setting, requiring only a single measurement, even in the presence of noise. We describe the standard DIP formulation, outline key algorithmic design choices, and review several strategies to mitigate overfitting, including early stopping, explicit regularisation, and self-guided methods that adapt the network input. In addition, we examine computational improvements such as warm-start and stochastic optimisation methods to reduce the reconstruction time. The discussed methods are tested on real $μ$CT measurements, which allows examination of trade-offs among the different modifications and extensions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [283] [UAVGENT: A Language-Guided Distributed Control Framework](https://arxiv.org/abs/2602.13212)
*Ziyi Zhang,Xiyu Deng,Guannan Qu,Yorie Nakahira*

Main category: cs.RO

TL;DR: Three-layer architecture combining LLM-based natural language interpretation with distributed drone control for provably robust mission execution.


<details>
  <summary>Details</summary>
Motivation: To enable human operators to control multi-drone systems using natural language while maintaining formal robustness guarantees at the physical layer for evolving, high-level missions.

Method: Three-layer architecture: (1) human operator issues natural-language instructions, (2) LLM-based supervisor periodically interprets, verifies, and corrects tasks based on latest state/target estimates, (3) distributed inner-loop controller tracks references using only local relative information.

Result: Theoretical guarantee characterizes tracking performance under bounded disturbances and piecewise-smooth references with discrete jumps induced by LLM updates.

Conclusion: Centralized language-based task reasoning can be effectively combined with distributed feedback control to achieve complex behaviors with provable robustness and stability in multi-drone systems.

Abstract: We study language-in-the-loop control for multi-drone systems that execute evolving, high-level missions while retaining formal robustness guarantees at the physical layer. We propose a three-layer architecture in which (i) a human operator issues natural-language instructions, (ii) an LLM-based supervisor periodically interprets, verifies, and corrects the commanded task in the context of the latest state and target estimates, and (iii) a distributed inner-loop controller tracks the resulting reference using only local relative information. We derive a theoretical guarantee that characterizes tracking performance under bounded disturbances and piecewise-smooth references with discrete jumps induced by LLM updates. Overall, our results illustrate how centralized language-based task reasoning can be combined with distributed feedback control to achieve complex behaviors with provable robustness and stability.

</details>


### [284] [DORA: Dataflow Oriented Robotic Architecture](https://arxiv.org/abs/2602.13252)
*Xiaodong Zhang,Baorui Lv,Xavier Tao,Xiong Wang,Jie Bao,Yong He,Yue Chen,Zijiang Yang*

Main category: cs.RO

TL;DR: DORA is a new robotic middleware that reduces latency and CPU overhead by enabling zero-copy data transmission and explicit data dependency specification, addressing inefficiencies in existing middleware for data-intensive robotic applications.


<details>
  <summary>Details</summary>
Motivation: Existing robotic middleware have significant limitations: they rely heavily on (de)serialization causing overhead for large data, and lack efficient support for heterogeneous data sizes, especially in intra-robot communication and Python environments. These inefficiencies impact system responsiveness, stability, and productivity in data-intensive industrial robotic applications.

Method: Proposed Dataflow-Oriented Robotic Architecture (DORA) that enables explicit data dependency specification and efficient zero-copy data transmission. Implemented as an open-source system and evaluated through extensive experiments in both simulation and real-world robotic environments.

Result: Experimental results demonstrate substantial reductions in latency and CPU overhead compared to state-of-the-art middleware.

Conclusion: DORA effectively addresses the limitations of existing robotic middleware by providing more efficient data communication through zero-copy transmission and explicit data dependency management, improving performance for data-intensive robotic applications.

Abstract: Robotic middleware serves as the foundational infrastructure, enabling complex robotic systems to operate in a coordinated and modular manner. In data-intensive robotic applications, especially in industrial scenarios, communication efficiency directly impact system responsiveness, stability, and overall productivity. However, existing robotic middleware exhibit several limitations: (1) they rely heavily on (de)serialization mechanisms, introducing significant overhead for large-sized data; (2) they lack efficient and flexible support for heterogeneous data sizes, particularly in intra-robot communication and Python-based execution environments. To address these challenges, we propose Dataflow-Oriented Robotic Architecture (DORA) that enables explicit data dependency specification and efficient zero-copy data transmission. We implement the proposed framework as an open-source system and evaluate it through extensive experiments in both simulation and real-world robotic environments. Experimental results demonstrate substantial reductions in latency and CPU overhead compared to state-of-the-art middleware.

</details>


### [285] [High-Fidelity, Customizable Force Sensing for the Wearable Human-Robot Interface](https://arxiv.org/abs/2602.13436)
*Noah Rubin,Ava Schraeder,Hrishikesh Sahu,Thomas C. Bulea,Lillian Chin*

Main category: cs.RO

TL;DR: Researchers developed a fluidic innervation sensing system using 3D-printed silicone pads with embedded air channels to measure human-machine interface forces, demonstrating high linearity and potential for wearable robotics control.


<details>
  <summary>Details</summary>
Motivation: The human-machine interface in wearable robotics is challenging to sensorize due to manufacturing complexity and non-linear sensor responses, making it difficult to understand user behavior and optimize device performance.

Method: Created 3D-printed silicone pads with embedded air channels that compress under applied forces, measuring pressure changes with off-the-shelf transducers. Tested in benchtop, clinical dynamometer, unconstrained arm movements, and integrated into a lower-extremity exoskeleton strap.

Result: Pad pressure showed high linear correlation with applied force (R²=0.998). In clinical tests, above-knee pressure correlated with flexion torque (R²=0.95) and below-knee with extension torque (R²=0.75). Pressure tracked movement phases in arm curls and exoskeleton squats.

Conclusion: Fluidic innervation provides a customizable, high signal-to-noise sensing modality for capturing human-machine mechanical interaction, with potential for real-time control of wearable robotics and monitoring user function during device use.

Abstract: Mechanically characterizing the human-machine interface is essential to understanding user behavior and optimizing wearable robot performance. This interface has been challenging to sensorize due to manufacturing complexity and non-linear sensor responses. Here, we measure human limb-device interaction via fluidic innervation, creating a 3D-printed silicone pad with embedded air channels to measure forces. As forces are applied to the pad, the air channels compress, resulting in a pressure change measurable by off-the-shelf pressure transducers. We demonstrate in benchtop testing that pad pressure is highly linearly related to applied force ($R^2 = 0.998$). This is confirmed with clinical dynamometer correlations with isometric knee torque, where above-knee pressure was highly correlated with flexion torque ($R^2 = 0.95$), while below-knee pressure was highly correlated with extension torque ($R^2 = 0.75$). We build on these idealized settings to test pad performance in more unconstrained settings. We place the pad over \textit{biceps brachii} during cyclic curls and stepwise isometric holds, observing a correlation between pressure and elbow angle. Finally, we integrated the sensor into the strap of a lower-extremity robotic exoskeleton and recorded pad pressure during repeated squats with the device unpowered. Pad pressure tracked squat phase and overall task dynamics consistently. Overall, our preliminary results suggest fluidic innervation is a readily customizable sensing modality with high signal-to-noise ratio and temporal resolution for capturing human-machine mechanical interaction. In the long-term, this modality may provide an alternative real-time sensing input to control / optimize wearable robotic systems and to capture user function during device use.

</details>


### [286] [FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation](https://arxiv.org/abs/2602.13444)
*Huajian Zeng,Lingyun Chen,Jiaqi Yang,Yuantai Zhang,Fan Shi,Peidong Liu,Xingxing Zuo*

Main category: cs.RO

TL;DR: FlowHOI: A flow-matching framework that generates hand-object interaction sequences (poses + contact states) from egocentric observations and language instructions, enabling better long-horizon manipulation with explicit HOI structure representation.


<details>
  <summary>Details</summary>
Motivation: Current VLA models fail in long-horizon, contact-rich tasks because they lack explicit representation of hand-object interaction structure. An embodiment-agnostic interaction representation is needed to make manipulation behaviors easier to validate and transfer across different robots.

Method: Two-stage flow-matching framework: 1) geometry-centric grasping, 2) semantics-centric manipulation conditioned on 3D scene tokens. Uses motion-text alignment loss for semantic grounding. Introduces reconstruction pipeline to recover aligned hand-object trajectories and meshes from egocentric videos for supervision.

Result: Achieves highest action recognition accuracy on GRAB and HOT3D benchmarks, 1.7× higher physics simulation success rate than strongest diffusion-based baseline, and 40× inference speedup. Successfully demonstrates real-robot execution on four dexterous manipulation tasks.

Conclusion: FlowHOI provides an effective embodiment-agnostic interaction representation that captures explicit HOI structure, enabling robust generation of semantically grounded manipulation behaviors that can be retargeted to real-robot execution pipelines.

Abstract: Recent vision-language-action (VLA) models can generate plausible end-effector motions, yet they often fail in long-horizon, contact-rich tasks because the underlying hand-object interaction (HOI) structure is not explicitly represented. An embodiment-agnostic interaction representation that captures this structure would make manipulation behaviors easier to validate and transfer across robots. We propose FlowHOI, a two-stage flow-matching framework that generates semantically grounded, temporally coherent HOI sequences, comprising hand poses, object poses, and hand-object contact states, conditioned on an egocentric observation, a language instruction, and a 3D Gaussian splatting (3DGS) scene reconstruction. We decouple geometry-centric grasping from semantics-centric manipulation, conditioning the latter on compact 3D scene tokens and employing a motion-text alignment loss to semantically ground the generated interactions in both the physical scene layout and the language instruction. To address the scarcity of high-fidelity HOI supervision, we introduce a reconstruction pipeline that recovers aligned hand-object trajectories and meshes from large-scale egocentric videos, yielding an HOI prior for robust generation. Across the GRAB and HOT3D benchmarks, FlowHOI achieves the highest action recognition accuracy and a 1.7$\times$ higher physics simulation success rate than the strongest diffusion-based baseline, while delivering a 40$\times$ inference speedup. We further demonstrate real-robot execution on four dexterous manipulation tasks, illustrating the feasibility of retargeting generated HOI representations to real-robot execution pipelines.

</details>


### [287] [Inferring Turn-Rate-Limited Engagement Zones with Sacrificial Agents for Safe Trajectory Planning](https://arxiv.org/abs/2602.13457)
*Grant Stagg,Cameron K. Peterson*

Main category: cs.RO

TL;DR: Learning-based framework uses sacrificial agents to estimate pursuer parameters in pursuit-evasion games, then generates safe paths for high-value agents.


<details>
  <summary>Details</summary>
Motivation: Need to estimate unknown pursuer parameters (like turn rate limits) in pursuit-evasion scenarios to enable safe navigation for high-value agents through adversarial environments.

Method: Uses sacrificial agents on straight-line trajectories that report binary interception outcomes. Two geometric models relate outcomes to pursuer parameters: boundary-interception and interior-interception cases. Parameter inference via gradient-based multi-start optimization with custom loss functions. Two trajectory selection strategies: geometric heuristic maximizing spread of expected interception points, and Bayesian experimental design maximizing D-score of expected Gauss-Newton information matrix.

Result: Monte Carlo experiments show accurate parameter recovery with 5-12 sacrificial agents. Learned engagement models successfully generate safe, time-optimal paths for high-value agents that avoid all feasible pursuer engagement regions.

Conclusion: The framework effectively estimates pursuer parameters using sacrificial agents, enabling safe path planning for high-value assets in adversarial environments with turn-rate-limited pursuers.

Abstract: This paper presents a learning-based framework for estimating pursuer parameters in turn-rate-limited pursuit-evasion scenarios using sacrificial agents. Each sacrificial agent follows a straight-line trajectory toward an adversary and reports whether it was intercepted or survived. These binary outcomes are related to the pursuer's parameters through a geometric reachable-region (RR) model. Two formulations are introduced: a boundary-interception case, where capture occurs at the RR boundary, and an interior-interception case, which allows capture anywhere within it. The pursuer's parameters are inferred using a gradient-based multi-start optimization with custom loss functions tailored to each case.
  Two trajectory-selection strategies are proposed for the sacrificial agents: a geometric heuristic that maximizes the spread of expected interception points, and a Bayesian experimental-design method that maximizes the D-score of the expected Gauss-Newton information matrix, thereby selecting trajectories that yield maximal information gain. Monte Carlo experiments demonstrate accurate parameter recovery with five to twelve sacrificial agents. The learned engagement models are then used to generate safe, time-optimal paths for high-value agents that avoid all feasible pursuer engagement regions.

</details>


### [288] [AsyncVLA: An Asynchronous VLA for Fast and Robust Navigation on the Edge](https://arxiv.org/abs/2602.13476)
*Noriaki Hirose,Catherine Glossop,Dhruv Shah,Sergey Levine*

Main category: cs.RO

TL;DR: AsyncVLA is an asynchronous control framework that decouples semantic reasoning (using large foundation models on remote workstations) from reactive execution (using lightweight edge adapters) to enable real-time robotic deployment despite high inference latency.


<details>
  <summary>Details</summary>
Motivation: Robotic foundation models achieve strong generalization but have massive computational costs causing high inference latency, which breaks the control loop in dynamic environments and makes powerful models unsafe for real-time deployment.

Method: AsyncVLA uses hierarchical control with a large foundation model on a remote workstation for high-level guidance, while a lightweight onboard Edge Adapter continuously refines actions at high frequency. It includes end-to-end finetuning and trajectory re-weighting strategies to bridge domain gaps between asynchronous streams.

Result: The approach achieves 40% higher success rate than state-of-the-art baselines on real-world vision-based navigation tasks with communication delays up to 6 seconds, effectively bridging semantic intelligence with reactivity requirements.

Conclusion: AsyncVLA successfully addresses the latency bottleneck of robotic foundation models by decoupling semantic reasoning from reactive execution, enabling safe real-time deployment of powerful models in dynamic environments.

Abstract: Robotic foundation models achieve strong generalization by leveraging internet-scale vision-language representations, but their massive computational cost creates a fundamental bottleneck: high inference latency. In dynamic environments, this latency breaks the control loop, rendering powerful models unsafe for real-time deployment. We propose AsyncVLA, an asynchronous control framework that decouples semantic reasoning from reactive execution. Inspired by hierarchical control, AsyncVLA runs a large foundation model on a remote workstation to provide high-level guidance, while a lightweight, onboard Edge Adapter continuously refines actions at high frequency. To bridge the domain gap between these asynchronous streams, we introduce an end-to-end finetuning protocol and a trajectory re-weighting strategy that prioritizes dynamic interactions. We evaluate our approach on real-world vision-based navigation tasks with communication delays up to 6 seconds. AsyncVLA achieves a 40% higher success rate than state-of-the-art baselines, effectively bridging the gap between the semantic intelligence of large models and the reactivity required for edge robotics.

</details>


### [289] [ONRAP: Occupancy-driven Noise-Resilient Autonomous Path Planning](https://arxiv.org/abs/2602.13577)
*Faizan M. Tariq,Avinash Singh,Vipul Ramtekkar,Jovin D'sa,David Isele,Yosuke Sakamoto,Sangjae Bae*

Main category: cs.RO

TL;DR: A real-time occupancy-grid planner using nonlinear programming with bicycle model for robust path planning in noisy environments, handling static/dynamic obstacles via occupancy-flow predictions.


<details>
  <summary>Details</summary>
Motivation: Need for reliable dynamic path planning despite sensing noise, uncertain localization, and incomplete semantic perception in real-world autonomous navigation.

Method: Nonlinear program in spatial domain using modified bicycle model with feasibility/collision-avoidance penalties, operating on occupancy grids with optional occupancy-flow predictions for dynamic obstacles.

Result: Real-time performance (>10 Hz), minimal tuning, validated in simulation with severe noise and on F1TENTH platform, demonstrating safe navigation through narrow passages and rough routes.

Conclusion: Provides robust foundation for noise-resilient, prediction-aware planning without handcrafted heuristics, suitable for integration with standard control stacks.

Abstract: Dynamic path planning must remain reliable in the presence of sensing noise, uncertain localization, and incomplete semantic perception. We propose a practical, implementation-friendly planner that operates on occupancy grids and optionally incorporates occupancy-flow predictions to generate ego-centric, kinematically feasible paths that safely navigate through static and dynamic obstacles. The core is a nonlinear program in the spatial domain built on a modified bicycle model with explicit feasibility and collision-avoidance penalties. The formulation naturally handles unknown obstacle classes and heterogeneous agent motion by operating purely in occupancy space. The pipeline runs in real-time (faster than 10 Hz on average), requires minimal tuning, and interfaces cleanly with standard control stacks. We validate our approach in simulation with severe localization and perception noises, and on an F1TENTH platform, demonstrating smooth and safe maneuvering through narrow passages and rough routes. The approach provides a robust foundation for noise-resilient, prediction-aware planning, eliminating the need for handcrafted heuristics. The project website can be accessed at https://honda-research-institute.github.io/onrap/

</details>


### [290] [TactAlign: Human-to-Robot Policy Transfer via Tactile Alignment](https://arxiv.org/abs/2602.13579)
*Youngsun Wi,Jessica Yin,Elvis Xiang,Akash Sharma,Jitendra Malik,Mustafa Mukadam,Nima Fazeli,Tess Hellebrekers*

Main category: cs.RO

TL;DR: TactAlign enables cross-embodiment tactile alignment to transfer human-collected tactile signals to robots with different sensing modalities, without requiring paired data or identical sensors.


<details>
  <summary>Details</summary>
Motivation: Human demonstrations with wearable tactile devices provide rich supervision for policy learning, but transferring these tactile signals to robots is challenging due to differences in sensing modalities and embodiment. Existing H2R approaches assume identical sensors, require paired data, and have minimal embodiment gaps, limiting scalability.

Method: TactAlign transforms human and robot tactile observations into a shared latent representation using rectified flow, without paired datasets, manual labels, or privileged information. It uses hand-object interaction-derived pseudo-pairs for low-cost latent transport.

Result: TactAlign improves H2R policy transfer across multiple contact-rich tasks (pivoting, insertion, lid closing), generalizes to unseen objects and tasks with minimal human data (<5 minutes), and enables zero-shot H2R transfer on highly dexterous tasks like light bulb screwing.

Conclusion: TactAlign provides a scalable and generalizable approach for cross-embodiment tactile alignment that overcomes the limitations of existing H2R methods, enabling effective transfer of human tactile demonstrations to robots with different sensing modalities.

Abstract: Human demonstrations collected by wearable devices (e.g., tactile gloves) provide fast and dexterous supervision for policy learning, and are guided by rich, natural tactile feedback. However, a key challenge is how to transfer human-collected tactile signals to robots despite the differences in sensing modalities and embodiment. Existing human-to-robot (H2R) approaches that incorporate touch often assume identical tactile sensors, require paired data, and involve little to no embodiment gap between human demonstrator and the robots, limiting scalability and generality. We propose TactAlign, a cross-embodiment tactile alignment method that transfers human-collected tactile signals to a robot with different embodiment. TactAlign transforms human and robot tactile observations into a shared latent representation using a rectified flow, without paired datasets, manual labels, or privileged information. Our method enables low-cost latent transport guided by hand-object interaction-derived pseudo-pairs. We demonstrate that TactAlign improves H2R policy transfer across multiple contact-rich tasks (pivoting, insertion, lid closing), generalizes to unseen objects and tasks with human data (less than 5 minutes), and enables zero-shot H2R transfer on a highly dexterous tasks (light bulb screwing).

</details>


### [291] [AgentRob: From Virtual Forum Agents to Hijacked Physical Robots](https://arxiv.org/abs/2602.13591)
*Wenrui Liu,Yaxuan Wang,Xun Zhang,Yanshu Wang,Jiashen Wei,Yifan Xiang,Yuhang Wang,Mingshen Ye,Elsie Dai,Zhiqi Liu,Yingjie Xu,Xinyang Chen,Hengzhe Sun,Jiyu Shen,Jingjing He,Tong Yang*

Main category: cs.RO

TL;DR: AgentRob is a framework that connects LLM-powered agents to physical robots through online forums, enabling autonomous agents to read forum posts, execute robot commands, and report results back to the community.


<details>
  <summary>Details</summary>
Motivation: Current LLM-powered autonomous agents are mostly confined to virtual environments, with limited integration to the physical world through direct control interfaces. There's a need to bridge online communities with physical robotics in a more natural, forum-mediated way.

Method: A three-layer framework: 1) Forum Layer for asynchronous multi-agent interaction, 2) Agent Layer with forum agents that poll for @mention-targeted commands, and 3) Robot Layer with VLM-driven controllers and Unitree Go2/G1 hardware that translate commands into robot primitives via iterative tool calling.

Result: The system successfully enables multiple concurrent agents with distinct identities and physical embodiments to coexist in the same forum, establishing the feasibility of forum-mediated multi-agent robot orchestration.

Conclusion: AgentRob introduces a novel paradigm where autonomous agents can participate in online forums to control physical robots, bridging the gap between virtual communities and physical robotics through the Model Context Protocol.

Abstract: Large Language Model (LLM)-powered autonomous agents have demonstrated significant capabilities in virtual environments, yet their integration with the physical world remains narrowly confined to direct control interfaces. We present AgentRob, a framework that bridges online community forums, LLM-powered agents, and physical robots through the Model Context Protocol (MCP). AgentRob enables a novel paradigm where autonomous agents participate in online forums--reading posts, extracting natural language commands, dispatching physical robot actions, and reporting results back to the community. The system comprises three layers: a Forum Layer providing asynchronous, persistent, multi-agent interaction; an Agent Layer with forum agents that poll for @mention-targeted commands; and a Robot Layer with VLM-driven controllers and Unitree Go2/G1 hardware that translate commands into robot primitives via iterative tool calling. The framework supports multiple concurrent agents with distinct identities and physical embodiments coexisting in the same forum, establishing the feasibility of forum-mediated multi-agent robot orchestration.

</details>


### [292] [Hierarchical Audio-Visual-Proprioceptive Fusion for Precise Robotic Manipulation](https://arxiv.org/abs/2602.13640)
*Siyuan Li,Jiani Lu,Yu Song,Xianren Li,Bo An,Peng Liu*

Main category: cs.RO

TL;DR: A hierarchical multimodal fusion framework that integrates audio, vision, and proprioception for robotic manipulation, using acoustic cues to enhance contact-related state inference in partially observable environments.


<details>
  <summary>Details</summary>
Motivation: Existing robotic manipulation methods rely on visual and proprioceptive observations but struggle with contact-related interaction states in partially observable environments. Acoustic cues naturally encode rich interaction dynamics during contact but are underexploited. Current multimodal fusion approaches assume homogeneous roles across modalities with flat symmetric structures, which is ill-suited for sparse, contact-driven acoustic signals.

Method: Proposes a hierarchical representation fusion framework that progressively integrates audio, vision, and proprioception. First conditions visual and proprioceptive representations on acoustic cues, then explicitly models higher-order cross-modal interactions to capture complementary dependencies. Uses a diffusion-based policy to generate continuous robot actions from multimodal observations through end-to-end learning.

Result: Evaluated on real-world robotic manipulation tasks including liquid pouring and cabinet opening. Consistently outperforms state-of-the-art multimodal fusion frameworks, particularly in scenarios where acoustic cues provide task-relevant information not available from visual observations alone. Mutual information analysis conducted to interpret audio cue effects.

Conclusion: The hierarchical fusion structure enables exploitation of task-relevant acoustic information while mitigating interference from less informative modalities. Demonstrates the value of acoustic-informed perception for precise robotic manipulation in partially observable real-world environments.

Abstract: Existing robotic manipulation methods primarily rely on visual and proprioceptive observations, which may struggle to infer contact-related interaction states in partially observable real-world environments. Acoustic cues, by contrast, naturally encode rich interaction dynamics during contact, yet remain underexploited in current multimodal fusion literature. Most multimodal fusion approaches implicitly assume homogeneous roles across modalities, and thus design flat and symmetric fusion structures. However, this assumption is ill-suited for acoustic signals, which are inherently sparse and contact-driven. To achieve precise robotic manipulation through acoustic-informed perception, we propose a hierarchical representation fusion framework that progressively integrates audio, vision, and proprioception. Our approach first conditions visual and proprioceptive representations on acoustic cues, and then explicitly models higher-order cross-modal interactions to capture complementary dependencies among modalities. The fused representation is leveraged by a diffusion-based policy to directly generate continuous robot actions from multimodal observations. The combination of end-to-end learning and hierarchical fusion structure enables the policy to exploit task-relevant acoustic information while mitigating interference from less informative modalities. The proposed method has been evaluated on real-world robotic manipulation tasks, including liquid pouring and cabinet opening. Extensive experiment results demonstrate that our approach consistently outperforms state-of-the-art multimodal fusion frameworks, particularly in scenarios where acoustic cues provide task-relevant information not readily available from visual observations alone. Furthermore, a mutual information analysis is conducted to interpret the effect of audio cues in robotic manipulation via multimodal fusion.

</details>


### [293] [SPLIT: Sparse Incremental Learning of Error Dynamics for Control-Oriented Modeling in Autonomous Vehicles](https://arxiv.org/abs/2602.13641)
*Yaoyu Li,Chaosheng Huang,Jun Li*

Main category: cs.RO

TL;DR: SPLIT is a sparse incremental learning framework for vehicle dynamics modeling that combines nominal models with GP-based residuals, addressing computational efficiency and online learning challenges through model decomposition, local incremental learning, and GP sparsification.


<details>
  <summary>Details</summary>
Motivation: Current hybrid vehicle models combining nominal models with GP-based residuals face three main challenges: curse of dimensionality, high evaluation complexity, and inefficient online learning, which prevent real-time deployment in autonomous vehicle controllers.

Method: SPLIT introduces three innovations: (1) Model decomposition into invariant elements (calibrated experimentally) and variant elements (compensated by residual model) to reduce feature dimensionality; (2) Local incremental learning that defines valid regions in feature space and partitions them for efficient online learning from streaming data; (3) GP sparsification using Bayesian committee machine for scalable online evaluation.

Result: SPLIT improves model accuracy and control performance online, enables rapid adaptation to vehicle dynamics deviations, and exhibits robust generalization to previously unseen scenarios in both aggressive simulations and real-vehicle experiments.

Conclusion: SPLIT successfully addresses the computational and online learning challenges of GP-based hybrid vehicle models, making them practical for real-time autonomous vehicle control through its sparse incremental learning framework.

Abstract: Accurate, computationally efficient, and adaptive vehicle models are essential for autonomous vehicle control. Hybrid models that combine a nominal model with a Gaussian Process (GP)-based residual model have emerged as a promising approach. However, the GP-based residual model suffers from the curse of dimensionality, high evaluation complexity, and the inefficiency of online learning, which impede the deployment in real-time vehicle controllers. To address these challenges, we propose SPLIT, a sparse incremental learning framework for control-oriented vehicle dynamics modeling. SPLIT integrates three key innovations: (i) Model Decomposition. We decompose the vehicle model into invariant elements calibrated by experiments, and variant elements compensated by the residual model to reduce feature dimensionality. (ii) Local Incremental Learning. We define the valid region in the feature space and partition it into subregions, enabling efficient online learning from streaming data. (iii) GP Sparsification. We use bayesian committee machine to ensure scalable online evaluation. Integrated into model-based controllers, SPLIT is evaluated in aggressive simulations and real-vehicle experiments. Results demonstrate that SPLIT improves model accuracy and control performance online. Moreover, it enables rapid adaptation to vehicle dynamics deviations and exhibits robust generalization to previously unseen scenarios.

</details>


### [294] [A Kung Fu Athlete Bot That Can Do It All Day: Highly Dynamic, Balance-Challenging Motion Dataset and Autonomous Fall-Resilient Tracking](https://arxiv.org/abs/2602.13656)
*Zhongxiang Lei,Lulu Cao,Xuyang Wang,Tianyi Qian,Jinyan Liu,Xuesong Li*

Main category: cs.RO

TL;DR: KungFuAthlete dataset captures high-dynamic martial arts motions with professional athletes, enabling training of unified policies for both motion tracking and fall recovery in humanoid robots.


<details>
  <summary>Details</summary>
Motivation: Current humanoid motion tracking systems struggle near hardware limits and lack robustness for highly dynamic motions like martial arts. Existing datasets don't capture extreme motion intensity, and prior work lacks unified strategies for handling unsafe states and recovery from failures.

Method: Created KungFuAthlete dataset from professional athletes' training videos, including ground and jump subsets with complex motion patterns. Proposed novel training paradigm where a single policy jointly learns high-dynamic motion tracking and fall recovery within one framework.

Result: KungFuAthlete dataset shows substantially higher joint, linear, and angular velocities compared to LAFAN1, PHUMA, and AMASS datasets, indicating significantly increased motion intensity and complexity. The unified framework enables robots to handle both agile execution and stabilization.

Conclusion: The approach expands robotic capability from pure motion tracking to recovery-enabled execution, promoting more robust and autonomous humanoid performance in real-world high-dynamic scenarios by unifying agile motion execution with fall recovery in a single policy.

Abstract: Current humanoid motion tracking systems can execute routine and moderately dynamic behaviors, yet significant gaps remain near hardware performance limits and algorithmic robustness boundaries. Martial arts represent an extreme case of highly dynamic human motion, characterized by rapid center-of-mass shifts, complex coordination, and abrupt posture transitions. However, datasets tailored to such high-intensity scenarios remain scarce. To address this gap, we construct KungFuAthlete, a high-dynamic martial arts motion dataset derived from professional athletes' daily training videos. The dataset includes ground and jump subsets covering representative complex motion patterns. The jump subset exhibits substantially higher joint, linear, and angular velocities compared to commonly used datasets such as LAFAN1, PHUMA, and AMASS, indicating significantly increased motion intensity and complexity. Importantly, even professional athletes may fail during highly dynamic movements. Similarly, humanoid robots are prone to instability and falls under external disturbances or execution errors. Most prior work assumes motion execution remains within safe states and lacks a unified strategy for modeling unsafe states and enabling reliable autonomous recovery. We propose a novel training paradigm that enables a single policy to jointly learn high-dynamic motion tracking and fall recovery, unifying agile execution and stabilization within one framework. This framework expands robotic capability from pure motion tracking to recovery-enabled execution, promoting more robust and autonomous humanoid performance in real-world high-dynamic scenarios.

</details>


### [295] [Symmetry-Aware Fusion of Vision and Tactile Sensing via Bilateral Force Priors for Robotic Manipulation](https://arxiv.org/abs/2602.13689)
*Wonju Lee,Matteo Grimaldi,Tao Yu*

Main category: cs.RO

TL;DR: CMT with physics-informed regularization achieves 96.59% insertion success by fusing vision and tactile data through structured attention, nearly matching privileged sensing performance.


<details>
  <summary>Details</summary>
Motivation: Insertion tasks require precise contact-rich interactions that vision alone cannot handle, and existing naive visuo-tactile fusion methods fail to deliver consistent improvements despite tactile feedback's intuitive value.

Method: Proposes Cross-Modal Transformer (CMT) for visuo-tactile fusion using structured self- and cross-attention to integrate wrist-camera observations with tactile signals. Adds physics-informed regularization encouraging bilateral force balance to stabilize tactile embeddings, reflecting human motor control principles.

Result: CMT with symmetry regularization achieves 96.59% insertion success rate on TacSL benchmark, surpassing naive and gated fusion baselines and closely matching the privileged "wrist + contact force" configuration (96.09%).

Conclusion: Tactile sensing is indispensable for precise alignment, and principled multimodal fusion with physics-informed regularization unlocks complementary strengths of vision and touch, approaching privileged performance under realistic sensing.

Abstract: Insertion tasks in robotic manipulation demand precise, contact-rich interactions that vision alone cannot resolve. While tactile feedback is intuitively valuable, existing studies have shown that naïve visuo-tactile fusion often fails to deliver consistent improvements. In this work, we propose a Cross-Modal Transformer (CMT) for visuo-tactile fusion that integrates wrist-camera observations with tactile signals through structured self- and cross-attention. To stabilize tactile embeddings, we further introduce a physics-informed regularization that encourages bilateral force balance, reflecting principles of human motor control. Experiments on the TacSL benchmark show that CMT with symmetry regularization achieves a 96.59% insertion success rate, surpassing naïve and gated fusion baselines and closely matching the privileged "wrist + contact force" configuration (96.09%). These results highlight two central insights: (i) tactile sensing is indispensable for precise alignment, and (ii) principled multimodal fusion, further strengthened by physics-informed regularization, unlocks complementary strengths of vision and touch, approaching privileged performance under realistic sensing.

</details>


### [296] [HybridFlow: A Two-Step Generative Policy for Robotic Manipulation](https://arxiv.org/abs/2602.13718)
*Zhenchen Dong,Jinna Fu,Jiaming Wu,Shengyuan Yu,Fulin Chen,Yide Liu*

Main category: cs.RO

TL;DR: HybridFlow is a 3-stage, 2-NFE method that combines MeanFlow and ReFlow to achieve fast, high-quality action generation for robot manipulation, offering 8× speedup over Diffusion Policy while improving success rates.


<details>
  <summary>Details</summary>
Motivation: Existing robot manipulation policies suffer from high inference latency, limiting real-time interaction capabilities. While flow matching methods are faster than diffusion, they still need improvement for interactive robot control where both speed and precision are critical.

Method: HybridFlow uses a 3-stage approach with 2 neural function evaluations (NFE): 1) Global Jump in MeanFlow mode for fast generation, 2) ReNoise for distribution alignment, and 3) Local Refine in ReFlow mode for precision improvement. This balances speed and quality by leveraging MeanFlow's one-step advantage while ensuring action precision with minimal steps.

Result: HybridFlow outperforms 16-step Diffusion Policy by 15-25% in success rate while reducing inference time from 152ms to 19ms (8× speedup, ~52Hz). It achieves 70.0% success on unseen-color OOD grasping and 66.3% on deformable object folding.

Conclusion: HybridFlow represents a practical low-latency method that enhances real-world interaction capabilities of robotic manipulation policies by balancing inference speed and generation quality, making it suitable for interactive robot control applications.

Abstract: Limited by inference latency, existing robot manipulation policies lack sufficient real-time interaction capability with the environment. Although faster generation methods such as flow matching are gradually replacing diffusion methods, researchers are pursuing even faster generation suitable for interactive robot control. MeanFlow, as a one-step variant of flow matching, has shown strong potential in image generation, but its precision in action generation does not meet the stringent requirements of robotic manipulation. We therefore propose \textbf{HybridFlow}, a \textbf{3-stage method} with \textbf{2-NFE}: Global Jump in MeanFlow mode, ReNoise for distribution alignment, and Local Refine in ReFlow mode. This method balances inference speed and generation quality by leveraging the rapid advantage of MeanFlow one-step generation while ensuring action precision with minimal generation steps. Through real-world experiments, HybridFlow outperforms the 16-step Diffusion Policy by \textbf{15--25\%} in success rate while reducing inference time from 152ms to 19ms (\textbf{8$\times$ speedup}, \textbf{$\sim$52Hz}); it also achieves 70.0\% success on unseen-color OOD grasping and 66.3\% on deformable object folding. We envision HybridFlow as a practical low-latency method to enhance real-world interaction capabilities of robotic manipulation policies.

</details>


### [297] [FC-Vision: Real-Time Visibility-Aware Replanning for Occlusion-Free Aerial Target Structure Scanning in Unknown Environments](https://arxiv.org/abs/2602.13720)
*Chen Feng,Yang Xu,Shaojie Shen*

Main category: cs.RO

TL;DR: FC-Vision is a visibility-aware replanning framework for UAV scanning that prevents target occlusions in real-time while maintaining coverage and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing autonomous aerial scanning methods focus on collision avoidance and efficiency but neglect occlusion-induced visibility degradation, which severely compromises scanning quality when encountering unknown obstacles during flight.

Method: Two-level decomposition approach: 1) occlusion-free viewpoint repair that maintains coverage with minimal deviation from original plan, and 2) segment-wise clean-sensing connection in 5-DoF space. Includes plug-in integration for existing UAV systems.

Result: Maximum coverage gain of 55.32%, 73.17% reduction in occlusion ratio, real-time performance with moderate flight time increase. Validated through comprehensive simulation and real-world evaluations.

Conclusion: FC-Vision effectively addresses occlusion problems in aerial scanning, significantly improving scanning quality under unexpected occluders while maintaining practical efficiency and integration compatibility.

Abstract: Autonomous aerial scanning of target structures is crucial for practical applications, requiring online adaptation to unknown obstacles during flight. Existing methods largely emphasize collision avoidance and efficiency, but overlook occlusion-induced visibility degradation, severely compromising scanning quality. In this study, we propose FC-Vision, an on-the-fly visibility-aware replanning framework that proactively and safely prevents target occlusions while preserving the intended coverage and efficiency of the original plan. Our approach explicitly enforces dense surface-visibility constraints to regularize replanning behavior in real-time via an efficient two-level decomposition: occlusion-free viewpoint repair that maintains coverage with minimal deviation from the nominal scan intent, followed by segment-wise clean-sensing connection in 5-DoF space. A plug-in integration strategy is also presented to seamlessly interface FC-Vision with existing UAV scanning systems without architectural changes. Comprehensive simulation and real-world evaluations show that FC-Vision consistently improves scanning quality under unexpected occluders, delivering a maximum coverage gain of 55.32% and a 73.17% reduction in the occlusion ratio, while achieving real-time performance with a moderate increase in flight time. The source code will be made publicly available.

</details>


### [298] [Improving Driver Satisfaction with a Driving Function Learning from Implicit Human Feedback -- a Test Group Study](https://arxiv.org/abs/2602.13733)
*Robin Schwager,Andrea Anastasio,Simon Hartmann,Andreas Ronellenfitsch,Michael Grimm,Tim Brühl,Tin Stribor Sohn,Tim Dieter Eberhardt,Sören Hohmann*

Main category: cs.RO

TL;DR: Personalized speed profile adjustment for predictive longitudinal driving functions using driver intervention feedback to increase satisfaction and reduce interventions.


<details>
  <summary>Details</summary>
Motivation: Driver interventions during ADAS use contain valuable feedback about deviations from personal preferences, which should be utilized to optimize and personalize driving functions.

Method: Algorithm that iteratively adjusts PLDF's speed profile by combining original speed profile with driver demonstrations, enabling personalization during active PLDF use in traded control scenarios.

Result: Driving simulator study with 43 participants showed significant increase in driver satisfaction and significant reduction in intervention frequency with adaptive PLDF.

Conclusion: The proposed algorithm successfully personalizes PLDF behavior using driver feedback, improving satisfaction and reducing interventions, with identified optimization potentials for further development.

Abstract: During the use of advanced driver assistance systems, drivers frequently intervene into the active driving function and adjust the system's behavior to their personal wishes. These active driver-initiated takeovers contain feedback about deviations in the driving function's behavior from the drivers' personal preferences. This feedback should be utilized to optimize and personalize the driving function's behavior. In this work, the adjustment of the speed profile of a Predictive Longitudinal Driving Function (PLDF) on a pre-defined route is highlighted. An algorithm is introduced which iteratively adjusts the PLDF's speed profile by taking into account both the original speed profile of the PLDF and the driver demonstration. This approach allows for personalization in a traded control scenario during active use of the PLDF. The applicability of the proposed algorithm is tested in a driving simulator-based test group study with 43 participants. The study finds a significant increase in driver satisfaction and a significant reduction in the intervention frequency when using the proposed adaptive PLDF. Additionally, feedback by the participants was gathered to identify further optimization potentials of the proposed system.

</details>


### [299] [XIT: Exploration and Exploitation Informed Trees for Active Gas Distribution Mapping in Unknown Environments](https://arxiv.org/abs/2602.13739)
*Mal Fazliu,Matthew Coombes,Sen Wang,Cunjia Liu*

Main category: cs.RO

TL;DR: XIT is a sampling-based informative path planner for autonomous mobile robots that balances exploration of unknown environments with exploitation of gas concentration measurements to create high-quality gas distribution maps.


<details>
  <summary>Details</summary>
Motivation: Current mobile robotic gas distribution mapping systems rely on teleoperation, limiting scalability and response speed during emergency responses to hazardous gas releases. Autonomous active GDM is challenging because robots must simultaneously explore unknown environments, map the space, and infer gas distributions from sparse measurements.

Method: Formulates active GDM as a next-best-trajectory informative path planning problem. Proposes XIT (Exploration-Exploitation Informed Trees), a sampling-based planner that balances exploration and exploitation by generating concurrent trajectories toward exploration-rich goals while collecting informative gas measurements. Uses Upper Confidence Bound information field, expands trees with cost trade-off between travel effort, gas concentration, and uncertainty. Introduces gas frontier concept and Wavefront Gas Frontier Detection algorithm for plume-aware exploration.

Result: High-fidelity simulations and real-world experiments demonstrate XIT's benefits in terms of GDM quality and efficiency. The method effectively balances exploration and exploitation for gas distribution mapping.

Conclusion: XIT provides an effective solution for autonomous active gas distribution mapping in unknown environments. Although developed for GDM, the approach is readily applicable to other robotic information-gathering tasks that face the exploration-exploitation trade-off in unknown environments.

Abstract: Mobile robotic gas distribution mapping (GDM) provides critical situational awareness during emergency responses to hazardous gas releases. However, most systems still rely on teleoperation, limiting scalability and response speed. Autonomous active GDM is challenging in unknown and cluttered environments, because the robot must simultaneously explore traversable space, map the environment, and infer the gas distribution belief from sparse chemical measurements. We address this by formulating active GDM as a next-best-trajectory informative path planning (IPP) problem and propose XIT (Exploration-Exploitation Informed Trees), a sampling-based planner that balances exploration and exploitation by generating concurrent trajectories toward exploration-rich goals while collecting informative gas measurements en route. XIT draws batches of samples from an Upper Confidence Bound (UCB) information field derived from the current gas posterior and expands trees using a cost that trades off travel effort against gas concentration and uncertainty. To enable plume-aware exploration, we introduce the gas frontier concept, defined as unobserved regions adjacent to high gas concentrations, and propose the Wavefront Gas Frontier Detection (WGFD) algorithm for their identification. High-fidelity simulations and real-world experiments demonstrate the benefits of XIT in terms of GDM quality and efficiency. Although developed for active GDM, XIT is readily applicable to other robotic information-gathering tasks in unknown environments that face the exploration and exploitation trade-off.

</details>


### [300] [The More the Merrier: Running Multiple Neuromorphic Components On-Chip for Robotic Control](https://arxiv.org/abs/2602.13747)
*Evan Eames,Priyadarshini Kannan,Ronan Sangouard,Philipp Plank,Elvin Hajizada,Gintautas Palinauskas,Lana Amaya,Michael Neumeier,Sai Thejeshwar Sharma,Marcella Toth,Prottush Sarkar,Axel von Arnim*

Main category: cs.RO

TL;DR: A neuromorphic robotics pipeline using spiking neural state machines for on-chip orchestration of multiple networks, validated on Intel Loihi 2 with low power and latency.


<details>
  <summary>Details</summary>
Motivation: Neuromorphic hardware offers benefits for robotics (low energy, low latency, unique learning methods), but complex multimodal tasks are hindered by inability to orchestrate multiple networks on-chip without off-chip process management.

Method: Developed a pipeline for vision-based robot control using spiking neural state machines to orchestrate multiple complex networks entirely on neuromorphic hardware (Intel Loihi 2).

Result: All components run concurrently on-chip with milliwatt power consumption and competitive latencies. The system accomplished robotic arm plug insertion in simulation, with core elements tested on a real robotic arm.

Conclusion: The spiking neural state machine approach enables complex robotic control pipelines to run entirely on neuromorphic hardware, overcoming previous orchestration limitations while maintaining low power and latency benefits.

Abstract: It has long been realized that neuromorphic hardware offers benefits for the domain of robotics such as low energy, low latency, as well as unique methods of learning. In aiming for more complex tasks, especially those incorporating multimodal data, one hurdle continuing to prevent their realization is an inability to orchestrate multiple networks on neuromorphic hardware without resorting to off-chip process management logic. To address this, we show a first example of a pipeline for vision-based robot control in which numerous complex networks can be run entirely on hardware via the use of a spiking neural state machine for process orchestration. The pipeline is validated on the Intel Loihi 2 research chip. We show that all components can run concurrently on-chip in the milli Watt regime at latencies competitive with the state-of-the-art. An equivalent network on simulated hardware is shown to accomplish robotic arm plug insertion in simulation, and the core elements of the pipeline are additionally tested on a real robotic arm.

</details>


### [301] [Impact-Robust Posture Optimization for Aerial Manipulation](https://arxiv.org/abs/2602.13762)
*Amr Afifi,Ahmad Gazar,Javier Alonso-Mora,Paolo Robuffo Giordano,Antonio Franchi*

Main category: cs.RO

TL;DR: Novel method optimizes robot postures to reduce impact-induced velocity spikes using min-max optimization and gradient-based motion tasks integrated with whole-body control.


<details>
  <summary>Details</summary>
Motivation: To improve robot safety and robustness during impacts by reducing spikes in robot state and input commands that occur when robots make contact with the environment.

Method: Uses rigid impact model to create configuration-dependent metric quantifying velocity changes during impacts. Formulates posture optimization as min-max problem, then reformulates as real-time gradient-based motion task embedded within task-space inverse dynamics whole-body controller.

Result: Achieved up to 51% reduction in post-impact spikes for aerial manipulator compared to standard TSID, avoided actuator saturation. Additional simulations on quadruped and humanoid showed up to 45% reduction, demonstrating importance of kinematic redundancy.

Conclusion: The proposed method effectively optimizes robot postures to improve impact robustness, reduces state spikes significantly, and demonstrates the value of kinematic redundancy for impact resilience across different robot platforms.

Abstract: We present a novel method for optimizing the posture of kinematically redundant torque-controlled robots to improve robustness during impacts. A rigid impact model is used as the basis for a configuration-dependent metric that quantifies the variation between pre- and post-impact velocities. By finding configurations (postures) that minimize the aforementioned metric, spikes in the robot's state and input commands can be significantly reduced during impacts, improving safety and robustness. The problem of identifying impact-robust postures is posed as a min-max optimization of the aforementioned metric. To overcome the real-time intractability of the problem, we reformulate it as a gradient-based motion task that iteratively guides the robot towards configurations that minimize the proposed metric. This task is embedded within a task-space inverse dynamics (TSID) whole-body controller, enabling seamless integration with other control objectives. The method is applied to a kinematically redundant aerial manipulator performing repeated point contact tasks. We test our method inside a realistic physics simulator and compare it with the nominal TSID. Our method leads to a reduction (up to 51% w.r.t. standard TSID) of post-impact spikes in the robot's configuration and successfully avoids actuator saturation. Moreover, we demonstrate the importance of kinematic redundancy for impact robustness using additional numerical simulations on a quadruped and a humanoid robot, resulting in up to 45% reduction of post-impact spikes in the robot's state w.r.t. nominal TSID.

</details>


### [302] [MOTIF: Learning Action Motifs for Few-shot Cross-Embodiment Transfer](https://arxiv.org/abs/2602.13764)
*Heng Zhi,Wentao Tan,Lei Zhu,Fengling Li,Jingjing Li,Guoli Yang,Heng Tao Shen*

Main category: cs.RO

TL;DR: MOTIF enables efficient few-shot cross-embodiment transfer for robotic learning by decoupling embodiment-agnostic action patterns from heterogeneous action data.


<details>
  <summary>Details</summary>
Motivation: Cross-embodiment transfer in vision-language-action models is challenging due to kinematic heterogeneity and high cost of collecting real-world demonstrations. Existing shared-private architectures have limited capacity and lack explicit adaptation mechanisms.

Method: MOTIF learns unified action motifs via vector quantization with progress-aware alignment and embodiment adversarial constraints. A lightweight predictor then predicts these motifs from real-time inputs to guide a flow-matching policy, fusing them with robot-specific states for action generation.

Result: MOTIF significantly outperforms strong baselines in few-shot transfer scenarios by 6.5% in simulation and 43.7% in real-world settings.

Conclusion: MOTIF provides an effective approach for cross-embodiment transfer by decoupling embodiment-agnostic spatiotemporal patterns from heterogeneous action data, enabling efficient adaptation to new robotic embodiments with minimal demonstrations.

Abstract: While vision-language-action (VLA) models have advanced generalist robotic learning, cross-embodiment transfer remains challenging due to kinematic heterogeneity and the high cost of collecting sufficient real-world demonstrations to support fine-tuning. Existing cross-embodiment policies typically rely on shared-private architectures, which suffer from limited capacity of private parameters and lack explicit adaptation mechanisms. To address these limitations, we introduce MOTIF for efficient few-shot cross-embodiment transfer that decouples embodiment-agnostic spatiotemporal patterns, termed action motifs, from heterogeneous action data. Specifically, MOTIF first learns unified motifs via vector quantization with progress-aware alignment and embodiment adversarial constraints to ensure temporal and cross-embodiment consistency. We then design a lightweight predictor that predicts these motifs from real-time inputs to guide a flow-matching policy, fusing them with robot-specific states to enable action generation on new embodiments. Evaluations across both simulation and real-world environments validate the superiority of MOTIF, which significantly outperforms strong baselines in few-shot transfer scenarios by 6.5% in simulation and 43.7% in real-world settings. Code is available at https://github.com/buduz/MOTIF.

</details>


### [303] [Ontological grounding for sound and natural robot explanations via large language models](https://arxiv.org/abs/2602.13800)
*Alberto Olivares-Alarcos,Muhammad Ahsan,Satrio Sanjaya,Hsien-I Lin,Guillem Alenyà*

Main category: cs.RO

TL;DR: Hybrid framework combining ontology reasoning with LLMs for semantically grounded, natural robot explanations that are both logically sound and human-aligned.


<details>
  <summary>Details</summary>
Motivation: Robots need to derive conclusions from experiences that are logically sound AND communicated in ways aligned with human expectations for effective human-robot interaction.

Method: Blends ontology-based reasoning (for logical consistency/domain grounding) with LLMs (for fluent, context-aware language generation). Grounds data from human-robot experiences to reason about typical/atypical events. Integrates static contrastive ontology-based narratives with LLM agent for interactive explanations.

Result: Significant improvements in clarity and brevity of ontology-based narratives while preserving semantic accuracy. System demonstrates ability to adapt explanations to user feedback.

Conclusion: Ontology-LLM integration advances explainable agency and promotes more transparent human-robot collaboration.

Abstract: Building effective human-robot interaction requires robots to derive conclusions from their experiences that are both logically sound and communicated in ways aligned with human expectations. This paper presents a hybrid framework that blends ontology-based reasoning with large language models (LLMs) to produce semantically grounded and natural robot explanations. Ontologies ensure logical consistency and domain grounding, while LLMs provide fluent, context-aware and adaptive language generation. The proposed method grounds data from human-robot experiences, enabling robots to reason about whether events are typical or atypical based on their properties. We integrate a state-of-the-art algorithm for retrieving and constructing static contrastive ontology-based narratives with an LLM agent that uses them to produce concise, clear, interactive explanations. The approach is validated through a laboratory study replicating an industrial collaborative task. Empirical results show significant improvements in the clarity and brevity of ontology-based narratives while preserving their semantic accuracy. Initial evaluations further demonstrate the system's ability to adapt explanations to user feedback. Overall, this work highlights the potential of ontology-LLM integration to advance explainable agency, and promote more transparent human-robot collaboration.

</details>


### [304] [Semantic-Contact Fields for Category-Level Generalizable Tactile Tool Manipulation](https://arxiv.org/abs/2602.13833)
*Kevin Yuchen Ma,Heng Zhang,Weisi Lin,Mike Zheng Shou,Yan Wu*

Main category: cs.RO

TL;DR: SCFields is a 3D representation combining visual semantics with dense contact estimates for contact-rich tool manipulation, trained via sim-to-real pipeline to enable robust category-level generalization.


<details>
  <summary>Details</summary>
Motivation: Current robot policies have limitations: VLA models lack physical grounding for contact-rich manipulation, while contact-aware policies using tactile sensing are instance-specific and don't generalize across diverse tool geometries. There's a need for unified contact representations, but real-world tactile data is scarce and sim-to-real transfer is challenging due to complex soft sensor dynamics.

Method: Proposes Semantic-Contact Fields (SCFields) - unified 3D representation fusing visual semantics with dense contact estimates. Uses two-stage Sim-to-Real Contact Learning Pipeline: 1) pre-train on large simulation dataset to learn general contact physics, 2) fine-tune on small real dataset pseudo-labeled via geometric heuristics and force optimization to align sensor characteristics. Uses SCFields as dense observation input for diffusion policy.

Result: Experiments on scraping, crayon drawing, and peeling tasks demonstrate robust category-level generalization, significantly outperforming vision-only and raw-tactile baselines.

Conclusion: SCFields enables physical generalization to unseen tools for contact-rich manipulation tasks by bridging the gap between semantic planning and precise physical control through unified contact representations learned via efficient sim-to-real pipeline.

Abstract: Generalizing tool manipulation requires both semantic planning and precise physical control. Modern generalist robot policies, such as Vision-Language-Action (VLA) models, often lack the high-fidelity physical grounding required for contact-rich tool manipulation. Conversely, existing contact-aware policies that leverage tactile or haptic sensing are typically instance-specific and fail to generalize across diverse tool geometries. Bridging this gap requires learning unified contact representations from diverse data, yet a fundamental barrier remains: diverse real-world tactile data are prohibitive at scale, while direct zero-shot sim-to-real transfer is challenging due to the complex dynamics of nonlinear deformation of soft sensors.
  To address this, we propose Semantic-Contact Fields (SCFields), a unified 3D representation fusing visual semantics with dense contact estimates. We enable this via a two-stage Sim-to-Real Contact Learning Pipeline: first, we pre-train on a large simulation data set to learn general contact physics; second, we fine-tune on a small set of real data, pseudo-labeled via geometric heuristics and force optimization, to align sensor characteristics. This allows physical generalization to unseen tools. We leverage SCFields as the dense observation input for a diffusion policy to enable robust execution of contact-rich tool manipulation tasks. Experiments on scraping, crayon drawing, and peeling demonstrate robust category-level generalization, significantly outperforming vision-only and raw-tactile baselines.

</details>


### [305] [Push-Placement: A Hybrid Approach Integrating Prehensile and Non-Prehensile Manipulation for Object Rearrangement](https://arxiv.org/abs/2602.13849)
*Majid Sadeghinejad,Arman Barghi,Hamed Hosseini,Mehdi Tale Masouleh,Ahmad Kalhor*

Main category: cs.RO

TL;DR: Push-placement: A hybrid action primitive that uses grasped objects to displace obstructing items during placement, reducing explicit buffering in tabletop rearrangement tasks.


<details>
  <summary>Details</summary>
Motivation: Tabletop rearrangement faces challenges with collisions and temporary buffering. Prehensile pick-and-place requires extra moves, while non-prehensile pushing is imprecise. A hybrid approach could improve efficiency.

Method: Proposes push-placement action primitive integrated into physics-in-the-loop Monte Carlo Tree Search (MCTS) planner, evaluated in PyBullet simulator.

Result: Push-placement reduces manipulator travel cost by up to 11.12% vs baseline MCTS planner and 8.56% vs dynamic stacking.

Conclusion: Hybrid prehensile/non-prehensile action primitives can substantially improve efficiency in long-horizon rearrangement tasks.

Abstract: Efficient tabletop rearrangement remains challenging due to collisions and the need for temporary buffering when target poses are obstructed. Prehensile pick-and-place provides precise control but often requires extra moves, whereas non-prehensile pushing can be more efficient but suffers from complex, imprecise dynamics. This paper proposes push-placement, a hybrid action primitive that uses the grasped object to displace obstructing items while being placed, thereby reducing explicit buffering. The method is integrated into a physics-in-the-loop Monte Carlo Tree Search (MCTS) planner and evaluated in the PyBullet simulator. Empirical results show push-placement reduces the manipulator travel cost by up to 11.12% versus a baseline MCTS planner and 8.56% versus dynamic stacking. These findings indicate that hybrid prehensile/non-prehensile action primitives can substantially improve efficiency in long-horizon rearrangement tasks.

</details>


### [306] [Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement](https://arxiv.org/abs/2602.13850)
*Minku Kim,Kuan-Chia Chen,Aayam Shrestha,Li Fuxin,Stefan Lee,Alan Fern*

Main category: cs.RO

TL;DR: A skill-based framework for humanoid box rearrangement using shared whole-body controller enables long-horizon execution by sequencing reusable skills, with data augmentation to maintain robustness.


<details>
  <summary>Details</summary>
Motivation: To enable long-horizon humanoid box rearrangement by developing a framework that sequences reusable skills through a shared whole-body controller, addressing robustness issues that arise when reusing pretrained controllers across different skill compositions.

Method: Skill-based framework with shared task-agnostic whole-body controller (WBC) for all skills, using data aggregation procedure that augments WBC training with rollouts from closed-loop skill execution under domain randomization. Introduces Humanoid Hanoi benchmark for evaluation.

Result: Demonstrated fully autonomous box rearrangement over extended horizons in simulation and on Digit V3 humanoid robot, showing benefits of shared-WBC approach over non-shared baselines.

Conclusion: Shared whole-body controller with data augmentation enables robust long-horizon humanoid box rearrangement by providing consistent interface for skill composition, outperforming non-shared controller designs.

Abstract: We investigate a skill-based framework for humanoid box rearrangement that enables long-horizon execution by sequencing reusable skills at the task level. In our architecture, all skills execute through a shared, task-agnostic whole-body controller (WBC), providing a consistent closed-loop interface for skill composition, in contrast to non-shared designs that use separate low-level controllers per skill. We find that naively reusing the same pretrained WBC can reduce robustness over long horizons, as new skills and their compositions induce shifted state and command distributions. We address this with a simple data aggregation procedure that augments shared-WBC training with rollouts from closed-loop skill execution under domain randomization. To evaluate the approach, we introduce \emph{Humanoid Hanoi}, a long-horizon Tower-of-Hanoi box rearrangement benchmark, and report results in simulation and on the Digit V3 humanoid robot, demonstrating fully autonomous rearrangement over extended horizons and quantifying the benefits of the shared-WBC approach over non-shared baselines.

</details>


### [307] [Modeling and Optimizing the Provisioning of Exhaustible Capabilities for Simultaneous Task Allocation and Scheduling](https://arxiv.org/abs/2602.13866)
*Jinwoo Park,Harish Ravichandar,Seth Hutchinson*

Main category: cs.RO

TL;DR: TRAITS is a novel offline heterogeneous multi-robot task allocation framework that optimizes exhaustible trait provisioning under battery and temporal constraints using nonlinear programming.


<details>
  <summary>Details</summary>
Motivation: Deploying heterogeneous robot teams for multiple tasks over extended time horizons presents significant computational challenges for task allocation and planning, especially when dealing with exhaustible traits (like battery) under temporal constraints.

Method: TRAITS uses a nonlinear programming-based trait distribution module that optimizes trait-provisioning rates of coalitions to yield feasible and time-efficient solutions. It provides accurate feasibility assessment and task execution time estimation by leveraging trait-provisioning rates while optimizing battery consumption.

Result: TRAITS demonstrates advantages over two state-of-the-art frameworks in satisfying complex trait and battery requirements while remaining computationally tractable.

Conclusion: TRAITS represents the first comprehensive time-extended offline heterogeneous multi-robot task allocation framework that can effectively handle exhaustible trait provisioning under battery and temporal constraints, offering improved feasibility assessment and computational efficiency.

Abstract: Deploying heterogeneous robot teams to accomplish multiple tasks over extended time horizons presents significant computational challenges for task allocation and planning. In this paper, we present a comprehensive, time-extended, offline heterogeneous multi-robot task allocation framework, TRAITS, which we believe to be the first that can cope with the provisioning of exhaustible traits under battery and temporal constraints. Specifically, we introduce a nonlinear programming-based trait distribution module that can optimize the trait-provisioning rate of coalitions to yield feasible and time-efficient solutions. TRAITS provides a more accurate feasibility assessment and estimation of task execution times and makespan by leveraging trait-provisioning rates while optimizing battery consumption -- an advantage that state-of-the-art frameworks lack. We evaluate TRAITS against two state-of-the-art frameworks, with results demonstrating its advantage in satisfying complex trait and battery requirements while remaining computationally tractable.

</details>


### [308] [UAV-SEAD: State Estimation Anomaly Dataset for UAVs](https://arxiv.org/abs/2602.13900)
*Aykut Kabaoglu,Sanem Sariel*

Main category: cs.RO

TL;DR: The paper introduces a large-scale real-world UAV dataset for state estimation anomaly detection, containing 1396 flight logs (52+ hours) from diverse environments without synthetic manipulation, with proposed anomaly classifications.


<details>
  <summary>Details</summary>
Motivation: Accurate UAV state estimation is crucial for safety and reliability, but existing datasets rely on simulated/injected faults rather than real-world anomalies. There's a need for realistic datasets to improve anomaly detection systems for UAV reliability.

Method: Collected 1396 real flight logs from PX4-based UAVs across diverse indoor/outdoor environments with various sensor configurations. Proposed a structured classification system categorizing UAV state estimation anomalies into four classes: mechanical/electrical, external position, global position, and altitude anomalies.

Result: Created a unique large-scale real-world UAV dataset with both normal and anomalous flights (no synthetic manipulation), totaling over 52 hours of flight time. The dataset includes multivariate sensor data streams (IMU, GPS, barometer, magnetometer, distance sensors, visual odometry, optical flow) from PX4 logging.

Conclusion: This dataset addresses a critical gap in UAV reliability research and will facilitate development, training, and evaluation of anomaly detection/isolation systems for more realistic and effective UAV safety monitoring.

Abstract: Accurate state estimation in Unmanned Aerial Vehicles (UAVs) is crucial for ensuring reliable and safe operation, as anomalies occurring during mission execution may induce discrepancies between expected and observed system behaviors, thereby compromising mission success or posing potential safety hazards. It is essential to continuously monitor and detect such conditions in order to ensure a timely response and maintain system reliability. In this work, we focus on UAV state estimation anomalies and provide a large-scale real-world UAV dataset to facilitate research aimed at improving the development of anomaly detection. Unlike existing datasets that primarily rely on injected faults into simulated data, this dataset comprises 1396 real flight logs totaling over 52 hours of flight time, collected across diverse indoor and outdoor environments using a collection of PX4-based UAVs equipped with a variety of sensor configurations. The dataset comprises both normal and anomalous flights without synthetic manipulation, making it uniquely suitable for realistic anomaly detection tasks. A structured classification is proposed that categorizes UAV state estimation anomalies into four classes: mechanical and electrical, external position, global position, and altitude anomalies. These classifications reflect collective, contextual, and outlier anomalies observed in multivariate sensor data streams, including IMU, GPS, barometer, magnetometer, distance sensors, visual odometry, and optical flow, that can be found in the PX4 logging mechanism. It is anticipated that this dataset will play a key role in the development, training, and evaluation of anomaly detection and isolation systems to address the critical gap in UAV reliability research.

</details>


### [309] [High-fidelity 3D reconstruction for planetary exploration](https://arxiv.org/abs/2602.13909)
*Alfonso Martínez-Petersen,Levin Gerdes,David Rodríguez-Martínez,C. J. Pérez-del-Pulgar*

Main category: cs.RO

TL;DR: This paper presents a unified pipeline integrating radiance field methods (NeRF and Gaussian Splatting) for automated 3D reconstruction in planetary robotics, combining Nerfstudio and COLMAP with ROS2 to process rover data from rosbag recordings.


<details>
  <summary>Details</summary>
Motivation: Planetary rovers need autonomous spatial awareness for navigation under severe constraints (no GPS, communication delays, low-texture terrains). Traditional SfM/SLAM methods struggle with radiometric detail and scaling in unstructured extraterrestrial environments.

Method: Integration of radiance field-based methods (NeRF and Gaussian Splatting) into a unified pipeline using Nerfstudio and COLMAP frameworks with ROS2-compatible workflow that processes raw rover data directly from rosbag recordings.

Result: The system generates dense, photorealistic, and metrically consistent 3D representations from minimal visual input, supporting improved perception and planning for autonomous systems in planetary-like conditions.

Conclusion: The pipeline establishes a foundation for radiance field-based mapping in planetary exploration, bridging the gap between geometric and neural representations for future research.

Abstract: Planetary exploration increasingly relies on autonomous robotic systems capable of perceiving, interpreting, and reconstructing their surroundings in the absence of global positioning or real-time communication with Earth. Rovers operating on planetary surfaces must navigate under sever environmental constraints, limited visual redundancy, and communication delays, making onboard spatial awareness and visual localization key components for mission success. Traditional techniques based on Structure-from-Motion (SfM) and Simultaneous Localization and Mapping (SLAM) provide geometric consistency but struggle to capture radiometric detail or to scale efficiently in unstructured, low-texture terrains typical of extraterrestrial environments. This work explores the integration of radiance field-based methods - specifically Neural Radiance Fields (NeRF) and Gaussian Splatting - into a unified, automated environment reconstruction pipeline for planetary robotics. Our system combines the Nerfstudio and COLMAP frameworks with a ROS2-compatible workflow capable of processing raw rover data directly from rosbag recordings. This approach enables the generation of dense, photorealistic, and metrically consistent 3D representations from minimal visual input, supporting improved perception and planning for autonomous systems operating in planetary-like conditions. The resulting pipeline established a foundation for future research in radiance field-based mapping, bridging the gap between geometric and neural representations in planetary exploration.

</details>


### [310] [Joint Task Assistance Planning via Nested Branch and Bound (Extended Version)](https://arxiv.org/abs/2602.13932)
*Omer Daube,Oren Salzman*

Main category: cs.RO

TL;DR: Joint Task Assistance Planning problem where two robots collaborate: one executes a timed mission while the other provides sensor-based support, aiming to maximize assistance duration through efficient path planning.


<details>
  <summary>Details</summary>
Motivation: Generalizes prior work on optimizing robotic collaboration assistance, addressing the challenge of combinatorial explosion in path combinations combined with temporal constraints in multi-robot systems.

Method: Proposes a nested branch-and-bound framework that hierarchically explores robot path spaces to efficiently solve the joint planning problem with temporal considerations.

Result: Empirical evaluation shows speedup of up to two orders of magnitude compared to baseline approaches, demonstrating significant computational efficiency improvements.

Conclusion: The proposed hierarchical branch-and-bound framework effectively addresses the combinatorial and temporal challenges in joint task assistance planning for collaborative robots.

Abstract: We introduce and study the Joint Task Assistance Planning problem which generalizes prior work on optimizing assistance in robotic collaboration. In this setting, two robots operate over predefined roadmaps, each represented as a graph corresponding to its configuration space. One robot, the task robot, must execute a timed mission, while the other, the assistance robot, provides sensor-based support that depends on their spatial relationship. The objective is to compute a path for both robots that maximizes the total duration of assistance given. Solving this problem is challenging due to the combinatorial explosion of possible path combinations together with the temporal nature of the problem (time needs to be accounted for as well). To address this, we propose a nested branch-and-bound framework that efficiently explores the space of robot paths in a hierarchical manner. We empirically evaluate our algorithm and demonstrate a speedup of up to two orders of magnitude when compared to a baseline approach.

</details>


### [311] [WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL](https://arxiv.org/abs/2602.13977)
*Zhennan Jiang,Shangqing Zhou,Yutong Jiang,Zefang Huang,Mingjie Wei,Yuhui Chen,Tianxing Zhou,Zhen Guo,Hao Lin,Quanlu Zhang,Yu Wang,Haoran Li,Chao Yu,Dongbin Zhao*

Main category: cs.RO

TL;DR: WoVR is a world-model-based RL framework that enables stable policy optimization for VLA models by explicitly controlling hallucination in imagined rollouts, achieving significant performance improvements on both simulated and real-world robotic tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional RL requires massive real-world interaction which is impractical for physical robots. While world models can serve as simulators, their imagined rollouts suffer from hallucination and error accumulation that corrupts the optimization signal, causing policies to exploit model inaccuracies rather than learn genuine task progress.

Method: WoVR regulates RL interaction with imperfect imagined dynamics through three key components: 1) A controllable action-conditioned video world model for improved rollout stability, 2) Keyframe-Initialized Rollouts to reduce effective error depth by reshaping imagined interaction, and 3) World Model-Policy co-evolution to maintain policy-simulator alignment.

Result: WoVR achieves substantial improvements: average LIBERO benchmark success increases from 39.95% to 69.2% (+29.3 points) and real-robot manipulation success increases from 61.7% to 91.7% (+30.0 points). The framework enables stable long-horizon imagined rollouts and effective policy optimization.

Conclusion: Learned world models can serve as practical simulators for reinforcement learning when hallucination is explicitly controlled. WoVR demonstrates that by regulating how RL interacts with imperfect imagined dynamics, world-model-based RL can effectively post-train VLA policies without requiring massive real-world interaction.

Abstract: Reinforcement learning (RL) promises to unlock capabilities beyond imitation learning for Vision-Language-Action (VLA) models, but its requirement for massive real-world interaction prevents direct deployment on physical robots. Recent work attempts to use learned world models as simulators for policy optimization, yet closed-loop imagined rollouts inevitably suffer from hallucination and long-horizon error accumulation. Such errors do not merely degrade visual fidelity; they corrupt the optimization signal, encouraging policies to exploit model inaccuracies rather than genuine task progress. We propose WoVR, a reliable world-model-based reinforcement learning framework for post-training VLA policies. Instead of assuming a faithful world model, WoVR explicitly regulates how RL interacts with imperfect imagined dynamics. It improves rollout stability through a controllable action-conditioned video world model, reshapes imagined interaction to reduce effective error depth via Keyframe-Initialized Rollouts, and maintains policy-simulator alignment through World Model-Policy co-evolution. Extensive experiments on LIBERO benchmarks and real-world robotic manipulation demonstrate that WoVR enables stable long-horizon imagined rollouts and effective policy optimization, improving average LIBERO success from 39.95% to 69.2% (+29.3 points) and real-robot success from 61.7% to 91.7% (+30.0 points). These results show that learned world models can serve as practical simulators for reinforcement learning when hallucination is explicitly controlled.

</details>


### [312] [It Takes Two to Tango: A Holistic Simulator for Joint Order Scheduling and Multi-Agent Path Finding in Robotic Warehouses](https://arxiv.org/abs/2602.13999)
*Haozheng Xu,Wenhao Li,Zifan Wei,Bo Jin,Hongxing Bai,Ben Yang,Xiangfeng Wang*

Main category: cs.RO

TL;DR: WareRover is a holistic simulation platform that tightly couples order scheduling and multi-agent pathfinding to address the limitations of current decoupled approaches in warehouse robotics.


<details>
  <summary>Details</summary>
Motivation: Current RMFS systems treat order scheduling and multi-agent pathfinding as isolated sub-problems, creating a fundamental bottleneck that masks critical dependencies between high-level dispatching and low-level congestion. Existing simulators fail to bridge this gap by abstracting away heterogeneous kinematics and stochastic execution failures.

Method: WareRover enforces tight coupling between order scheduling and multi-agent pathfinding through a unified, closed-loop optimization interface. It integrates dynamic order streams, physics-aware motion constraints, and non-nominal recovery mechanisms into a single evaluation loop, unlike standard benchmarks.

Result: Experiments show that state-of-the-art algorithms often falter under realistic coupled constraints, demonstrating that WareRover provides a necessary and challenging testbed for robust, next-generation warehouse coordination.

Conclusion: WareRover addresses the critical gap in current warehouse robotics simulation by providing a holistic platform that captures the complex interdependencies between scheduling and pathfinding, offering a more realistic testbed for developing robust coordination algorithms.

Abstract: The prevailing paradigm in Robotic Mobile Fulfillment Systems (RMFS) typically treats order scheduling and multi-agent pathfinding as isolated sub-problems. We argue that this decoupling is a fundamental bottleneck, masking the critical dependencies between high-level dispatching and low-level congestion. Existing simulators fail to bridge this gap, often abstracting away heterogeneous kinematics and stochastic execution failures. We propose WareRover, a holistic simulation platform that enforces a tight coupling between OS and MAPF via a unified, closed-loop optimization interface. Unlike standard benchmarks, WareRover integrates dynamic order streams, physics-aware motion constraints, and non-nominal recovery mechanisms into a single evaluation loop. Experiments reveal that SOTA algorithms often falter under these realistic coupled constraints, demonstrating that WareRover provides a necessary and challenging testbed for robust, next-generation warehouse coordination. The project and video is available at https://hhh-x.github.io/WareRover/.

</details>


### [313] [RoboAug: One Annotation to Hundreds of Scenes via Region-Contrastive Data Augmentation for Robotic Manipulation](https://arxiv.org/abs/2602.14032)
*Xinhua Wang,Kun Wu,Zhen Zhao,Hu Cao,Yinuo Zhao,Zhiyuan Xu,Meng Li,Shichao Fan,Di Wu,Yixue Zhang,Ning Liu,Zhengping Che,Jian Tang*

Main category: cs.RO

TL;DR: RoboAug is a generative data augmentation framework that improves robotic generalization using minimal bounding box annotations and pre-trained generative models, achieving significant success rate improvements across three robots.


<details>
  <summary>Details</summary>
Motivation: Current robotic learning approaches for generalization either require labor-intensive large-scale pretraining or rely on perfect object detection assumptions, which are impractical for real-world scenarios.

Method: RoboAug uses only single-image bounding box annotations, leverages pre-trained generative models for semantic data augmentation, and incorporates a plug-and-play region-contrastive loss to focus on task-relevant regions.

Result: Extensive real-world experiments on three robots (UR-5e, AgileX, Tien Kung 2.0) with over 35k rollouts show RoboAug significantly outperforms SOTA baselines, improving success rates from 0.09 to 0.47, 0.16 to 0.60, and 0.19 to 0.67 respectively in unseen scenes.

Conclusion: RoboAug demonstrates superior generalization capability for real-world manipulation tasks with minimal annotation requirements, offering a practical solution that reduces reliance on large-scale pretraining and perfect visual recognition assumptions.

Abstract: Enhancing the generalization capability of robotic learning to enable robots to operate effectively in diverse, unseen scenes is a fundamental and challenging problem. Existing approaches often depend on pretraining with large-scale data collection, which is labor-intensive and time-consuming, or on semantic data augmentation techniques that necessitate an impractical assumption of flawless upstream object detection in real-world scenarios. In this work, we propose RoboAug, a novel generative data augmentation framework that significantly minimizes the reliance on large-scale pretraining and the perfect visual recognition assumption by requiring only the bounding box annotation of a single image during training. Leveraging this minimal information, RoboAug employs pre-trained generative models for precise semantic data augmentation and integrates a plug-and-play region-contrastive loss to help models focus on task-relevant regions, thereby improving generalization and boosting task success rates. We conduct extensive real-world experiments on three robots, namely UR-5e, AgileX, and Tien Kung 2.0, spanning over 35k rollouts. Empirical results demonstrate that RoboAug significantly outperforms state-of-the-art data augmentation baselines. Specifically, when evaluating generalization capabilities in unseen scenes featuring diverse combinations of backgrounds, distractors, and lighting conditions, our method achieves substantial gains over the baseline without augmentation. The success rates increase from 0.09 to 0.47 on UR-5e, from 0.16 to 0.60 on AgileX, and from 0.19 to 0.67 on Tien Kung 2.0. These results highlight the superior generalization and effectiveness of RoboAug in real-world manipulation tasks. Our project is available at https://x-roboaug.github.io/.

</details>


### [314] [ProAct: A Dual-System Framework for Proactive Embodied Social Agents](https://arxiv.org/abs/2602.14048)
*Zeyi Zhang,Zixi Kang,Ruijie Zhao,Yusen Feng,Biao Jiang,Libin Liu*

Main category: cs.RO

TL;DR: ProAct: A dual-system framework for embodied social agents that enables proactive social behavior by decoupling low-latency reactive interaction from slower cognitive reasoning, allowing seamless integration of proactive intentions into continuous motion streams.


<details>
  <summary>Details</summary>
Motivation: Current embodied social agents are mostly reactive, responding only to immediate sensory inputs within short time windows. This limits their ability to exhibit proactive social behavior, which requires deliberation over accumulated context and intent inference - conflicting with real-time interaction latency requirements.

Method: ProAct uses a dual-system framework: 1) a low-latency Behavioral System for streaming multimodal interaction, and 2) a slower Cognitive System for long-horizon social reasoning producing proactive intentions. A streaming flow-matching model with ControlNet conditioning translates deliberative intentions into continuous non-verbal behaviors, supporting asynchronous intention injection without disrupting motion fluency.

Result: The system was deployed on a physical humanoid robot and evaluated for both motion quality and interactive effectiveness. In real-world user studies, participants and observers consistently preferred ProAct over reactive variants in perceived proactivity, social presence, and overall engagement.

Conclusion: The ProAct framework successfully reconciles the time-scale conflict between real-time interaction requirements and proactive social reasoning needs, demonstrating the benefits of dual-system proactive control for embodied social interaction through improved proactivity, social presence, and engagement.

Abstract: Embodied social agents have recently advanced in generating synchronized speech and gestures. However, most interactive systems remain fundamentally reactive, responding only to current sensory inputs within a short temporal window. Proactive social behavior, in contrast, requires deliberation over accumulated context and intent inference, which conflicts with the strict latency budget of real-time interaction. We present \emph{ProAct}, a dual-system framework that reconciles this time-scale conflict by decoupling a low-latency \emph{Behavioral System} for streaming multimodal interaction from a slower \emph{Cognitive System} which performs long-horizon social reasoning and produces high-level proactive intentions. To translate deliberative intentions into continuous non-verbal behaviors without disrupting fluency, we introduce a streaming flow-matching model conditioned on intentions via ControlNet. This mechanism supports asynchronous intention injection, enabling seamless transitions between reactive and proactive gestures within a single motion stream. We deploy ProAct on a physical humanoid robot and evaluate both motion quality and interactive effectiveness. In real-world interaction user studies, participants and observers consistently prefer ProAct over reactive variants in perceived proactivity, social presence, and overall engagement, demonstrating the benefits of dual-system proactive control for embodied social interaction.

</details>


### [315] [SemanticFeels: Semantic Labeling during In-Hand Manipulation](https://arxiv.org/abs/2602.14099)
*Anas Al Shikh Khalil,Haozhi Qi,Roberto Calandra*

Main category: cs.RO

TL;DR: SemanticFeels extends NeuralFeels by integrating semantic labeling with neural implicit shape representation from vision and touch, focusing on material classification during in-hand manipulation.


<details>
  <summary>Details</summary>
Motivation: As robots become more integrated into everyday tasks, they need to perceive both shape and properties of objects during in-hand manipulation for adaptive and intelligent behavior.

Method: Uses high-resolution Digit tactile readings processed by fine-tuned EfficientNet-B0 CNN for local material predictions, embedded into an augmented signed distance field (SDF) network that jointly predicts geometry and continuous material regions.

Result: System achieves high correspondence between predicted and actual materials on single- and multi-material objects, with average matching accuracy of 79.87% across multiple manipulation trials on a multi-material object.

Conclusion: SemanticFeels successfully integrates semantic labeling with neural implicit shape representation, enabling robots to perceive both geometry and material properties during in-hand manipulation.

Abstract: As robots become increasingly integrated into everyday tasks, their ability to perceive both the shape and properties of objects during in-hand manipulation becomes critical for adaptive and intelligent behavior. We present SemanticFeels, an extension of the NeuralFeels framework that integrates semantic labeling with neural implicit shape representation, from vision and touch. To illustrate its application, we focus on material classification: high-resolution Digit tactile readings are processed by a fine-tuned EfficientNet-B0 convolutional neural network (CNN) to generate local material predictions, which are then embedded into an augmented signed distance field (SDF) network that jointly predicts geometry and continuous material regions. Experimental results show that the system achieves a high correspondence between predicted and actual materials on both single- and multi-material objects, with an average matching accuracy of 79.87% across multiple manipulation trials on a multi-material object.

</details>


### [316] [Rigidity-Based Multi-Finger Coordination for Precise In-Hand Manipulation of Force-Sensitive Objects](https://arxiv.org/abs/2602.14104)
*Xinan Rong,Changhuang Wan,Aochen He,Xiaolong Li,Gangshan Jing*

Main category: cs.RO

TL;DR: A dual-layer framework for multi-finger coordination enables precise manipulation of force-sensitive objects without tactile feedback by solving coordinated force planning and converting forces to joint trajectories.


<details>
  <summary>Details</summary>
Motivation: Multi-fingered hands face challenges in precise manipulation of force-sensitive objects due to reliance on fingertip point contacts (no pull forces) and lack of calibrated torque sensors in commercial hands, making coordinated force planning difficult without tactile feedback.

Method: Proposes a dual-layer framework for multi-finger coordination that solves coordinated contact force planning using graph rigidity and force closure constraints, then employs force-to-position mapping to convert planned force trajectories to joint trajectories for manipulation without tactile feedback.

Result: Validated on a custom dexterous hand, demonstrating high-precision and safe manipulation of fragile objects including a soft yarn, plastic cup, and raw egg.

Conclusion: The framework enables precise manipulation of force-sensitive objects through joint control without tactile feedback, addressing challenges of multi-fingered hands with limited sensing capabilities.

Abstract: Precise in-hand manipulation of force-sensitive objects typically requires judicious coordinated force planning as well as accurate contact force feedback and control. Unlike multi-arm platforms with gripper end effectors, multi-fingered hands rely solely on fingertip point contacts and are not able to apply pull forces, therefore poses a more challenging problem. Furthermore, calibrated torque sensors are lacking in most commercial dexterous hands, adding to the difficulty. To address these challenges, we propose a dual-layer framework for multi-finger coordination, enabling high-precision manipulation of force-sensitive objects through joint control without tactile feedback. This approach solves coordinated contact force planning by incorporating graph rigidity and force closure constraints. By employing a force-to-position mapping, the planned force trajectory is converted to a joint trajectory. We validate the framework on a custom dexterous hand, demonstrating the capability to manipulate fragile objects-including a soft yarn, a plastic cup, and a raw egg-with high precision and safety.

</details>


### [317] [Direction Matters: Learning Force Direction Enables Sim-to-Real Contact-Rich Manipulation](https://arxiv.org/abs/2602.14174)
*Yifei Yang,Anzhe Chen,Zhenjie Zhu,Kechun Xu,Yunxuan Mao,Yufei Wei,Lu Chen,Rong Xiong,Yue Wang*

Main category: cs.RO

TL;DR: A sim-to-real transfer framework for contact-rich manipulation that uses privileged supervision from expert-designed controllers to predict force directions (not magnitudes) which are robust across the sim-to-real gap, combined with manually tuned force magnitudes for adaptive compliance.


<details>
  <summary>Details</summary>
Motivation: Sim-to-real transfer for contact-rich manipulation is challenging due to contact dynamics discrepancies. Existing methods rely on costly real-world data or fixed controllers with blind compliance, lacking adaptive, task-aligned behavior.

Method: Uses human-designed finite state machine based position/force controller in simulation as privileged guidance. Trains policy to predict end-effector pose, contact state, and desired contact force direction (not magnitude). At deployment, combines policy's directional intent with manually tuned constant force magnitudes in a force-aware admittance controller.

Result: Outperforms strong baselines in success rate and robustness on four real-world tasks: microwave opening, peg-in-hole, whiteboard wiping, and door opening. Lightweight tuning requires only a single scalar per contact state.

Conclusion: Force directions encode high-level task geometry and remain robust across sim-to-real gap, enabling effective transfer for contact-rich manipulation with adaptive compliance. The approach combines privileged supervision with lightweight manual tuning for practical deployment.

Abstract: Sim-to-real transfer for contact-rich manipulation remains challenging due to the inherent discrepancy in contact dynamics. While existing methods often rely on costly real-world data or utilize blind compliance through fixed controllers, we propose a framework that leverages expert-designed controller logic for transfer. Inspired by the success of privileged supervision in kinematic tasks, we employ a human-designed finite state machine based position/force controller in simulation to provide privileged guidance. The resulting policy is trained to predict the end-effector pose, contact state, and crucially the desired contact force direction. Unlike force magnitudes, which are highly sensitive to simulation inaccuracies, force directions encode high-level task geometry and remain robust across the sim-to-real gap. At deployment, these predictions configure a force-aware admittance controller. By combining the policy's directional intent with a constant, low-cost manually tuned force magnitude, the system generates adaptive, task-aligned compliance. This tuning is lightweight, typically requiring only a single scalar per contact state. We provide theoretical analysis for stability and robustness to disturbances. Experiments on four real-world tasks, i.e., microwave opening, peg-in-hole, whiteboard wiping, and door opening, demonstrate that our approach significantly outperforms strong baselines in both success rate and robustness. Videos are available at: https://yifei-y.github.io/project-pages/DirectionMatters/.

</details>


### [318] [Learning Part-Aware Dense 3D Feature Field for Generalizable Articulated Object Manipulation](https://arxiv.org/abs/2602.14193)
*Yue Chen,Muqing Jiang,Kaifeng Zheng,Jiaqi Liang,Chenrui Tie,Haoran Lu,Ruihai Wu,Hao Dong*

Main category: cs.RO

TL;DR: PA3FF is a 3D feature field with part awareness for articulated object manipulation, enabling better generalization than 2D features through contrastive learning on 3D part proposals.


<details>
  <summary>Details</summary>
Motivation: Generalizing articulated object manipulation across diverse objects is challenging. Existing 2D foundation features lack part awareness and face issues when lifted to 3D (runtime, inconsistency, low resolution). Need for 3D features that understand functional parts for better manipulation.

Method: Propose Part-Aware 3D Feature Field (PA3FF) trained via contrastive learning on 3D part proposals from labeled dataset. Predicts continuous 3D feature field from point clouds where feature distance reflects functional part proximity. Build Part-Aware Diffusion Policy (PADP) imitation learning framework on top.

Result: PA3FF outperforms 2D and 3D representations (CLIP, DINOv2, Grounded-SAM) in manipulation tasks. Enables diverse downstream applications including correspondence learning and segmentation. Demonstrates effectiveness in simulated and real-world robotic tasks.

Conclusion: PA3FF provides a versatile 3D foundation feature with part awareness that enhances generalization and sample efficiency for articulated object manipulation, addressing limitations of previous 2D-based approaches.

Abstract: Articulated object manipulation is essential for various real-world robotic tasks, yet generalizing across diverse objects remains a major challenge. A key to generalization lies in understanding functional parts (e.g., door handles and knobs), which indicate where and how to manipulate across diverse object categories and shapes. Previous works attempted to achieve generalization by introducing foundation features, while these features are mostly 2D-based and do not specifically consider functional parts. When lifting these 2D features to geometry-profound 3D space, challenges arise, such as long runtimes, multi-view inconsistencies, and low spatial resolution with insufficient geometric information. To address these issues, we propose Part-Aware 3D Feature Field (PA3FF), a novel dense 3D feature with part awareness for generalizable articulated object manipulation. PA3FF is trained by 3D part proposals from a large-scale labeled dataset, via a contrastive learning formulation. Given point clouds as input, PA3FF predicts a continuous 3D feature field in a feedforward manner, where the distance between point features reflects the proximity of functional parts: points with similar features are more likely to belong to the same part. Building on this feature, we introduce the Part-Aware Diffusion Policy (PADP), an imitation learning framework aimed at enhancing sample efficiency and generalization for robotic manipulation. We evaluate PADP on several simulated and real-world tasks, demonstrating that PA3FF consistently outperforms a range of 2D and 3D representations in manipulation scenarios, including CLIP, DINOv2, and Grounded-SAM. Beyond imitation learning, PA3FF enables diverse downstream methods, including correspondence learning and segmentation tasks, making it a versatile foundation for robotic manipulation. Project page: https://pa3ff.github.io

</details>


### [319] [Muscle Coactivation in the Sky: Geometry and Pareto Optimality of Energy vs. Promptness in Multirotors](https://arxiv.org/abs/2602.14222)
*Antonio Franchi*

Main category: cs.RO

TL;DR: This paper introduces a geometric framework for multi-objective control allocation in aerial multirotors, balancing energy economy and kinematic readiness (promptness) through a fiber-bundle formulation that reveals fundamental trade-offs between agility and endurance.


<details>
  <summary>Details</summary>
Motivation: The tension between energy economy and kinematic readiness in robotics and biomechanics needs to be addressed for aerial multirotors. Current approaches rely on heuristic tuning or black-box algorithms rather than principled understanding of the fundamental trade-offs between agility and endurance.

Method: Develops a geometric fiber-bundle formulation for redundancy resolution, introduces promptness as a dynamic aerodynamic manipulability metric, uses diffeomorphic transformation to linearize the signed-quadratic propulsion model, and creates a principled multi-objective program on affine fibers to study energy-promptness trade-offs.

Result: Reveals that the interplay between energy and promptness is fiber-dependent and shaped by hardware constraints. For unidirectional thrusters, feasible fibers are compact with interior allocations and short Pareto arcs. For reversible propellers, null space enables antagonistic rotor co-contraction that drives promptness to hardware limits, making optimal endurance and agility fundamentally incompatible in those regimes.

Conclusion: Provides a foundational geometric framework for understanding why and how to achieve agility through geometry-aware control allocation, offering guidance for vehicle design, certification metrics, and threat-aware flight operations, moving beyond heuristic tuning to principled multi-objective optimization.

Abstract: In robotics and human biomechanics, the tension between energy economy and kinematic readiness is well recognized; this work brings that fundamental principle to aerial multirotors. We show that the limited torque of the motors and the nonlinear aerodynamic map from rotor speed to thrust naturally give rise to the novel concept of promptness-a metric akin to dynamic aerodynamic manipulability. By treating energy consumption as a competing objective and introducing a geometric fiber-bundle formulation, we turn redundancy resolution into a principled multi-objective program on affine fibers. The use of the diffeomorphic transformation linearizing the signed-quadratic propulsion model allows us to lay the foundations for a rigorous study of the interplay between these costs. Through an illustrative case study on 4-DoF allocation on the hexarotor, we reveal that this interplay is fiber-dependent and physically shaped by hardware inequalities. For unidirectional thrusters, the feasible fibers are compact, yielding interior allocations and a short Pareto arc, while torque demands break symmetry and separate the optima. Conversely, with reversible propellers, the null space enables antagonistic rotor co-contraction that drives promptness to hardware limits, making optimal endurance and agility fundamentally incompatible in those regimes. Ultimately, rather than relying on heuristic tuning or black box algorithms to empirically improve task execution, this framework provides a foundational understanding of why and how to achieve agility through geometry-aware control allocation, offering possible guidance for vehicle design, certification metrics, and threat-aware flight operation.

</details>


### [320] [Path Planning Optimisation for SParse, AwaRe and Cooperative Networked Aerial Robot Teams (SpArC-NARTs): Optimisation Tool and Ground Sensing Coverage Use Cases](https://arxiv.org/abs/2602.14247)
*Maria Conceição,António Grilo,Meysam Basiri*

Main category: cs.RO

TL;DR: A path planning tool for sparse networked aerial robot teams that maximizes mission goals while enabling cooperation through communication-aware motion constraints and dynamic rewards.


<details>
  <summary>Details</summary>
Motivation: Networked aerial robot teams need to maintain connectivity for cooperative behaviors, especially when prior knowledge is inaccurate or incomplete. Current planning tools don't adequately address sparse connectivity, varying prior information, and the need for decentralized decision-making during exploration missions.

Method: Proposes SpArC-NART path planning tool that simultaneously considers: different levels of prior environmental information, limited agent energy/sensing/communication, and various team constitutions. Uses communication model accounting for radio technology limitations and physical phenomena. Cooperation mechanism employs soft-motion constraints and dynamic rewards based on Value of Movement and expected communication availability at each time step.

Result: Developed a novel planning tool that maximizes mission goals (target finding, area coverage) while enabling cooperation to reduce reporting times, increase situational awareness, and facilitate mission replanning. Demonstrated capabilities through ground sensing coverage use case.

Conclusion: The SpArC-NART tool effectively addresses the challenges of sparse connectivity in networked aerial robot teams, enabling cooperative behaviors that improve mission efficiency, resilience, and decision-making during exploration missions with incomplete prior information.

Abstract: A networked aerial robot team (NART) comprises a group of agents (e.g., unmanned aerial vehicles (UAVs), ground control stations, etc.) interconnected by wireless links. Inter-agent connectivity, even if intermittent (i.e. sparse), enables data exchanges between agents and supports cooperative behaviours in several NART missions. It can benefit online decentralised decision-making and group resilience, particularly when prior knowledge is inaccurate or incomplete. These requirements can be accounted for in the offline mission planning stages to incentivise cooperative behaviours and improve mission efficiency during the NART deployment. This paper proposes a novel path planning tool for a Sparse, Aware, and Cooperative Networked Aerial Robot Team (SpArC-NART) in exploration missions. It simultaneously considers different levels of prior information regarding the environment, limited agent energy, sensing, and communication, as well as distinct NART constitutions. The communication model takes into account the limitations of user-defined radio technology and physical phenomena. The proposed tool aims to maximise the mission goals (e.g., finding one or multiple targets, covering the full area of the environment, etc.), while cooperating with other agents to reduce agent reporting times, increase their global situational awareness (e.g., their knowledge of the environment), and facilitate mission replanning, if required. The developed cooperation mechanism leverages soft-motion constraints and dynamic rewards based on the Value of Movement and the expected communication availability between the agents at each time step. A ground sensing coverage use case was chosen to illustrate the current capabilities of this tool.

</details>


### [321] [A Latency-Aware Framework for Visuomotor Policy Learning on Industrial Robots](https://arxiv.org/abs/2602.14255)
*Daniel Ruan,Salma Mozaffari,Sigrid Adriaenssens,Arash Adel*

Main category: cs.RO

TL;DR: A framework for deploying visuomotor policies on industrial robots that handles latency issues through synchronized sensing, communication pipelines, and latency-aware execution scheduling.


<details>
  <summary>Details</summary>
Motivation: Industrial robots face large observation-execution gaps due to sensing, inference, and control latency, which is worse than research robots due to high-level interfaces and slower dynamics. This timing gap makes policy deployment unreliable.

Method: A latency-aware framework with calibrated multimodal sensing, temporal synchronization, unified communication, and teleoperation interface. Introduces latency-aware execution that schedules finite-horizon action sequences based on temporal feasibility, enabling asynchronous inference without modifying policies.

Result: Latency-aware execution maintains smooth motion, compliant contact behavior, and consistent task progression across varying latencies while reducing idle time and avoiding instability seen in blocking and naive asynchronous baselines.

Conclusion: Explicitly handling latency is crucial for reliable closed-loop deployment of visuomotor policies on industrial robots, and the proposed framework enables robust performance under realistic timing constraints.

Abstract: Industrial robots are increasingly deployed in contact-rich construction and manufacturing tasks that involve uncertainty and long-horizon execution. While learning-based visuomotor policies offer a promising alternative to open-loop control, their deployment on industrial platforms is challenged by a large observation-execution gap caused by sensing, inference, and control latency. This gap is significantly greater than on low-latency research robots due to high-level interfaces and slower closed-loop dynamics, making execution timing a critical system-level issue. This paper presents a latency-aware framework for deploying and evaluating visuomotor policies on industrial robotic arms under realistic timing constraints. The framework integrates calibrated multimodal sensing, temporally consistent synchronization, a unified communication pipeline, and a teleoperation interface for demonstration collection. Within this framework, we introduce a latency-aware execution strategy that schedules finite-horizon, policy-predicted action sequences based on temporal feasibility, enabling asynchronous inference and execution without modifying policy architectures or training. We evaluate the framework on a contact-rich industrial assembly task while systematically varying inference latency. Using identical policies and sensing pipelines, we compare latency-aware execution with blocking and naive asynchronous baselines. Results show that latency-aware execution maintains smooth motion, compliant contact behavior, and consistent task progression across a wide range of latencies while reducing idle time and avoiding instability observed in baseline methods. These findings highlight the importance of explicitly handling latency for reliable closed-loop deployment of visuomotor policies on industrial robots.

</details>


### [322] [Autonomous Robotic Tissue Palpation and Abnormalities Characterisation via Ergodic Exploration](https://arxiv.org/abs/2602.14287)
*Luca Beber,Edoardo Lamon,Matteo Saveriano,Daniele Fontanelli,Luigi Palopoli*

Main category: cs.RO

TL;DR: Autonomous robotic palpation framework for real-time elastic mapping using viscoelastic tissue model with ergodic control and online parameter estimation.


<details>
  <summary>Details</summary>
Motivation: To develop an autonomous system for tissue characterization that can identify diagnostically relevant regions (like pathological tissue) through robotic palpation, improving on existing techniques for medical diagnostics and screening.

Method: Combines force-based parameter estimation using force/torque sensor with ergodic control driven by Expected Information Density. Uses Extended Kalman Filter for online viscoelastic parameter estimation, Gaussian Process Regression for spatial elasticity modeling, and Heat Equation Driven Area Coverage controller for adaptive trajectory planning.

Result: Simulations show better reconstruction accuracy, enhanced segmentation capability, and improved robustness in detecting stiff inclusions compared to Bayesian Optimization techniques. Experimental validation on silicone phantom with embedded inclusions confirms method's potential for autonomous tissue characterization.

Conclusion: The proposed framework demonstrates effective autonomous robotic palpation for tissue characterization, showing promise for diagnostic and screening applications in identifying pathological tissue regions.

Abstract: We propose a novel autonomous robotic palpation framework for real-time elastic mapping during tissue exploration using a viscoelastic tissue model. The method combines force-based parameter estimation using a commercial force/torque sensor with an ergodic control strategy driven by a tailored Expected Information Density, which explicitly biases exploration toward diagnostically relevant regions by jointly considering model uncertainty, stiffness magnitude, and spatial gradients. An Extended Kalman Filter is employed to estimate viscoelastic model parameters online, while Gaussian Process Regression provides spatial modelling of the estimated elasticity, and a Heat Equation Driven Area Coverage controller enables adaptive, continuous trajectory planning. Simulations on synthetic stiffness maps demonstrate that the proposed approach achieves better reconstruction accuracy, enhanced segmentation capability, and improved robustness in detecting stiff inclusions compared to Bayesian Optimisation-based techniques. Experimental validation on a silicone phantom with embedded inclusions emulating pathological tissue regions further corroborates the potential of the method for autonomous tissue characterisation in diagnostic and screening applications.

</details>


### [323] [Exploiting Structure-from-Motion for Robust Vision-Based Map Matching for Aircraft Surface Movement](https://arxiv.org/abs/2602.14311)
*Daniel Choate,Jason Rife*

Main category: cs.RO

TL;DR: A vision-aided navigation pipeline for autonomous aircraft ground navigation that combines indirect methods' efficiency with direct image-based robustness, using SfM and ground plane mosaic matching to satellite imagery, showing drift challenges but offering integrity features for outlier detection.


<details>
  <summary>Details</summary>
Motivation: To develop a robust and certifiable vision-aided navigation solution for autonomous aircraft surface movement that combines computational efficiency with solution integrity, addressing the need for reliable ground navigation systems.

Method: Combines indirect methods' efficiency with direct image-based robustness. Processes ground images via feature-based structure-from-motion (SfM), constructs ground plane mosaic via homography transforms, and matches to satellite imagery using sum of squares differences (SSD) of intensities.

Result: Experimental results show SfM solution drift (similar to dead-reckoning) challenges expected accuracy benefits of map-matching with wide-baseline ground-plane mosaic. However, the algorithm demonstrates key integrity features including registration anomaly identification and ambiguous match detection.

Conclusion: The pipeline's integrity features can mitigate outlier behaviors and contribute toward a robust, certifiable solution for autonomous surface movement of aircraft, despite drift challenges in the SfM component.

Abstract: In this paper we introduce a vision-aided navigation (VAN) pipeline designed to support ground navigation of autonomous aircraft. The proposed algorithm combines the computational efficiency of indirect methods with the robustness of direct image-based techniques to enhance solution integrity. The pipeline starts by processing ground images (e.g., acquired by a taxiing aircraft) and relates them via a feature-based structure-from-motion (SfM) solution. A ground plane mosaic is then constructed via homography transforms and matched to satellite imagery using a sum of squares differences (SSD) of intensities. Experimental results reveal that drift within the SfM solution, similar to that observed in dead-reckoning systems, challenges the expected accuracy benefits of map-matching with a wide-baseline ground-plane mosaic. However, the proposed algorithm demonstrates key integrity features, such as the ability to identify registration anomalies and ambiguous matches. These characteristics of the pipeline can mitigate outlier behaviors and contribute toward a robust, certifiable solution for autonomous surface movement of aircraft.

</details>


### [324] [AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation](https://arxiv.org/abs/2602.14363)
*Morgan Byrd,Donghoon Baek,Kartik Garg,Hyunyoung Jung,Daesol Cho,Maks Sorokin,Robert Wright,Sehoon Ha*

Main category: cs.RO

TL;DR: AdaptManip: Autonomous framework for humanoid robots to perform integrated navigation, object lifting, and delivery using reinforcement learning without human demonstrations.


<details>
  <summary>Details</summary>
Motivation: Prior imitation learning-based approaches rely on human demonstrations and are brittle to disturbances, so the authors aim to develop a robust loco-manipulation policy without human demonstrations or teleoperation data.

Method: Three coupled components: (1) recurrent object state estimator for tracking objects under limited FOV/occlusions, (2) whole-body base policy for locomotion with residual manipulation control, (3) LiDAR-based robot global position estimator for drift-robust localization. All trained in simulation via RL and deployed zero-shot.

Result: Significantly outperforms baseline methods (including imitation learning) in adaptability and success rate. Accurate object state estimation improves manipulation under occlusion. Demonstrated fully autonomous real-world navigation, object lifting, and delivery on humanoid robot.

Conclusion: AdaptManip enables robust autonomous whole-body loco-manipulation for humanoid robots without human demonstrations, showing superior performance over imitation learning approaches and successful real-world deployment.

Abstract: This paper presents Adaptive Whole-body Loco-Manipulation, AdaptManip, a fully autonomous framework for humanoid robots to perform integrated navigation, object lifting, and delivery. Unlike prior imitation learning-based approaches that rely on human demonstrations and are often brittle to disturbances, AdaptManip aims to train a robust loco-manipulation policy via reinforcement learning without human demonstrations or teleoperation data. The proposed framework consists of three coupled components: (1) a recurrent object state estimator that tracks the manipulated object in real time under limited field-of-view and occlusions; (2) a whole-body base policy for robust locomotion with residual manipulation control for stable object lifting and delivery; and (3) a LiDAR-based robot global position estimator that provides drift-robust localization. All components are trained in simulation using reinforcement learning and deployed on real hardware in a zero-shot manner. Experimental results show that AdaptManip significantly outperforms baseline methods, including imitation learning-based approaches, in adaptability and overall success rate, while accurate object state estimation improves manipulation performance even under occlusion. We further demonstrate fully autonomous real-world navigation, object lifting, and delivery on a humanoid robot.

</details>


### [325] [A Soft Wrist with Anisotropic and Selectable Stiffness for Robust Robot Learning in Contact-rich Manipulation](https://arxiv.org/abs/2602.14434)
*Steven Oh,Tomoya Takahashi,Cristian C. Beltran-Hernandez,Yuki Kuroda,Masashi Hamaya*

Main category: cs.RO

TL;DR: CLAW is a novel soft wrist mechanism using orthogonal leaf springs with locking joints that provides large 6-DOF deformation, tunable anisotropic stiffness, and enables robust robot learning in contact-rich manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Contact-rich manipulation in unstructured environments poses robustness challenges where unexpected collisions can cause damage and hinder policy learning. Existing soft end-effectors have limitations: limited deformation range, lack of directional stiffness control, or complex actuation systems that compromise practicality.

Method: Introduces CLAW (Compliant Leaf-spring Anisotropic soft Wrist) - a simple yet effective design using two orthogonal leaf springs and rotary joints with a locking mechanism. The mechanism provides large 6-DOF deformation (40mm lateral, 20mm vertical) with anisotropic stiffness tunable across three distinct modes.

Result: Experimental evaluations using imitation learning show CLAW achieves 76% success rate in benchmark peg-insertion tasks, outperforming Fin Ray gripper (43%) and rigid gripper alternatives (36%). CLAW handles diverse contact-rich scenarios including precision assembly with tight tolerances and delicate object manipulation.

Conclusion: CLAW demonstrates potential to enable robust robot learning in contact-rich domains through its simple design, large deformation capability, tunable anisotropic stiffness, lightweight construction (330g), and low cost ($550).

Abstract: Contact-rich manipulation tasks in unstructured environments pose significant robustness challenges for robot learning, where unexpected collisions can cause damage and hinder policy acquisition. Existing soft end-effectors face fundamental limitations: they either provide a limited deformation range, lack directional stiffness control, or require complex actuation systems that compromise practicality. This study introduces CLAW (Compliant Leaf-spring Anisotropic soft Wrist), a novel soft wrist mechanism that addresses these limitations through a simple yet effective design using two orthogonal leaf springs and rotary joints with a locking mechanism. CLAW provides large 6-degree-of-freedom deformation (40mm lateral, 20mm vertical), anisotropic stiffness that is tunable across three distinct modes, while maintaining lightweight construction (330g) at low cost ($550). Experimental evaluations using imitation learning demonstrate that CLAW achieves 76% success rate in benchmark peg-insertion tasks, outperforming both the Fin Ray gripper (43%) and rigid gripper alternatives (36%). CLAW successfully handles diverse contact-rich scenarios, including precision assembly with tight tolerances and delicate object manipulation, demonstrating its potential to enable robust robot learning in contact-rich domains. Project page: https://project-page-manager.github.io/CLAW/

</details>


### [326] [RoboSolver: A Multi-Agent Large Language Model Framework for Solving Robotic Arm Problems](https://arxiv.org/abs/2602.14438)
*Hamid Khabazi,Ali F. Meghdari,Alireza Taheri*

Main category: cs.RO

TL;DR: Proposes an intelligent multi-agent framework using LLMs and VLMs for robotics that automatically analyzes and solves robotic manipulator problems including kinematics, simulation, and motion control.


<details>
  <summary>Details</summary>
Motivation: To integrate the strengths of LLMs and VLMs with computational tools for automatic analysis and problem-solving in robotics, enabling users to query complex robotic tasks through text and visual inputs.

Method: Developed a multi-agent framework that accepts textual and visual inputs, automatically performs forward/inverse kinematics, computes velocities/accelerations, generates 3D simulations, and executes motion control in simulated environments based on user queries.

Result: In three benchmark tests: 1) Text-based forward kinematics: framework with GPT-4o achieved 0.97 accuracy vs 0.30 for raw model; 2) Visual input: framework achieved 0.93 accuracy, ~20% higher than raw model; 3) Comprehensive robotic tasks: framework achieved 0.97 accuracy across simulation, control, kinematics, and Jacobian calculations.

Conclusion: The proposed framework significantly outperforms raw LLM/VLM models in robotic analysis tasks, demonstrating the effectiveness of integrating language/vision models with computational tools for automated robotics problem-solving.

Abstract: This study proposes an intelligent multi-agent framework built on LLMs and VLMs and specifically tailored to robotics. The goal is to integrate the strengths of LLMs and VLMs with computational tools to automatically analyze and solve problems related to robotic manipulators. Our developed framework accepts both textual and visual inputs and can automatically perform forward and inverse kinematics, compute velocities and accelerations of key points, generate 3D simulations of the robot, and ultimately execute motion control within the simulated environment, all according to the user's query. To evaluate the framework, three benchmark tests were designed, each consisting of ten questions. In the first benchmark test, the framework was evaluated while connected to GPT-4o, DeepSeek-V3.2, and Claude-Sonnet-4.5, as well as their corresponding raw models. The objective was to extract the forward kinematics of robots directly from textual descriptions. The results showed that the framework integrated with GPT-4o achieved the highest accuracy, reaching 0.97 in computing the final solution, whereas the raw model alone attained an accuracy of only 0.30 for the same task. Similarly, for the other two models, the framework consistently outperformed the corresponding raw models in terms of accuracy. The second benchmark test was identical to the first, except that the input was provided in visual form. In this test, the GPT-4o LLM was used alongside the Gemini 2.5 Pro VLM. The results showed that the framework achieved an accuracy of 0.93 in obtaining the final answer, which is approximately 20% higher than that of the corresponding raw model. The third benchmark test encompassed a range of robotic tasks, including simulation, control, velocity and acceleration computation, as well as inverse kinematics and Jacobian calculation, for which the framework achieved an accuracy of 0.97.

</details>


### [327] [Learning Transferability: A Two-Stage Reinforcement Learning Approach for Enhancing Quadruped Robots' Performance in U-Shaped Stair Climbing](https://arxiv.org/abs/2602.14473)
*Baixiao Huang,Baiyu Huang,Yu Hou*

Main category: cs.RO

TL;DR: Two-stage deep RL enables robot dogs to autonomously climb U-shaped stairs with transferable policies to other stair types.


<details>
  <summary>Details</summary>
Motivation: Autonomous stair climbing remains a major challenge for quadruped robots in building construction, especially for navigating different indoor staircase configurations like U-shaped stairs.

Method: Two-stage end-to-end deep reinforcement learning approach: first training on pyramid-stair terrain in Isaac Lab, then transferring learned policies to climb U-shaped indoor staircases.

Result: (1) Successful goal achievement for robot dogs climbing U-shaped stairs with stall penalty, (2) Demonstrated policy transferability between different stair types (U-shaped to straight/L-shaped/spiral, and vice versa).

Conclusion: End-to-end RL methods enable autonomous stair climbing for quadruped robots with transferable policies across different staircase configurations, advancing their utility in building construction scenarios.

Abstract: Quadruped robots are employed in various scenarios in building construction. However, autonomous stair climbing across different indoor staircases remains a major challenge for robot dogs to complete building construction tasks. In this project, we employed a two-stage end-to-end deep reinforcement learning (RL) approach to optimize a robot's performance on U-shaped stairs. The training robot-dog modality, Unitree Go2, was first trained to climb stairs on Isaac Lab's pyramid-stair terrain, and then to climb a U-shaped indoor staircase using the learned policies. This project explores end-to-end RL methods that enable robot dogs to autonomously climb stairs. The results showed (1) the successful goal reached for robot dogs climbing U-shaped stairs with a stall penalty, and (2) the transferability from the policy trained on U-shaped stairs to deployment on straight, L-shaped, and spiral stair terrains, and transferability from other stair models to deployment on U-shaped terrain.

</details>


### [328] [TWISTED-RL: Hierarchical Skilled Agents for Knot-Tying without Human Demonstrations](https://arxiv.org/abs/2602.14526)
*Guy Freund,Tom Jurgenson,Matan Sudry,Erez Karpas*

Main category: cs.RO

TL;DR: TWISTED-RL improves robotic knot-tying by replacing supervised learning with multi-step RL conditioned on topological actions, enabling better generalization and solving more complex knots.


<details>
  <summary>Details</summary>
Motivation: Robotic knot-tying is challenging due to complex deformable object interactions and topological constraints. Previous approach TWISTED used demonstration-free decomposition but had limitations with single-step inverse models and inefficient data collection.

Method: Replaces TWISTED's single-step supervised learning inverse model with multi-step Reinforcement Learning policy conditioned on abstract topological actions rather than goal states. Uses the same smart decomposition into manageable subproblems with specialized agents.

Result: Solves previously unattainable knots of higher complexity (Figure-8, Overhand). Shows increased success rates and reduced planning time, establishing new state-of-the-art in demonstration-free robotic knot-tying.

Conclusion: TWISTED-RL's multi-step RL approach with topological action conditioning enables more delicate state transitions and better generalization, making it superior to previous methods for complex robotic knot-tying tasks.

Abstract: Robotic knot-tying represents a fundamental challenge in robotics due to the complex interactions between deformable objects and strict topological constraints. We present TWISTED-RL, a framework that improves upon the previous state-of-the-art in demonstration-free knot-tying (TWISTED), which smartly decomposed a single knot-tying problem into manageable subproblems, each addressed by a specialized agent. Our approach replaces TWISTED's single-step inverse model that was learned via supervised learning with a multi-step Reinforcement Learning policy conditioned on abstract topological actions rather than goal states. This change allows more delicate topological state transitions while avoiding costly and ineffective data collection protocols, thus enabling better generalization across diverse knot configurations. Experimental results demonstrate that TWISTED-RL manages to solve previously unattainable knots of higher complexity, including commonly used knots such as the Figure-8 and the Overhand. Furthermore, the increase in success rates and drop in planning time establishes TWISTED-RL as the new state-of-the-art in robotic knot-tying without human demonstrations.

</details>


### [329] [Multimodal Covariance Steering in Belief Space with Active Probing and Influence for Autonomous Driving](https://arxiv.org/abs/2602.14540)
*Devodita Chakravarty,John Dolan,Yiwei Lyu*

Main category: cs.RO

TL;DR: Hierarchical belief modeling with active probing and risk-aware control for safer autonomous driving in uncertain interactive scenarios.


<details>
  <summary>Details</summary>
Motivation: Current approaches treat prediction and planning in isolation, limiting safe interaction in uncertain scenarios where simply reacting to predictions can lead to unsafe maneuvers or overly conservative behavior.

Method: Three-layer approach: 1) Hierarchical belief model for multi-resolution reasoning of human behavior, 2) Active probing strategy to disambiguate intent and gently steer human decisions, 3) Runtime risk-evaluation layer using Conditional Value-at-Risk (CVaR) to ensure actions remain within human risk tolerance.

Result: Simulations in lane-merging and unsignaled intersection scenarios show higher success rates and shorter completion times compared to existing methods.

Conclusion: Coupling belief inference, active probing, and risk monitoring creates a principled and interpretable framework for planning under uncertainty in autonomous driving.

Abstract: Autonomous driving in complex traffic requires reasoning under uncertainty. Common approaches rely on prediction-based planning or risk-aware control, but these are typically treated in isolation, limiting their ability to capture the coupled nature of action and inference in interactive settings. This gap becomes especially critical in uncertain scenarios, where simply reacting to predictions can lead to unsafe maneuvers or overly conservative behavior. Our central insight is that safe interaction requires not only estimating human behavior but also shaping it when ambiguity poses risks. To this end, we introduce a hierarchical belief model that structures human behavior across coarse discrete intents and fine motion modes, updated via Bayesian inference for interpretable multi-resolution reasoning. On top of this, we develop an active probing strategy that identifies when multimodal ambiguity in human predictions may compromise safety and plans disambiguating actions that both reveal intent and gently steer human decisions toward safer outcomes. Finally, a runtime risk-evaluation layer based on Conditional Value-at-Risk (CVaR) ensures that all probing actions remain within human risk tolerance during influence. Our simulations in lane-merging and unsignaled intersection scenarios demonstrate that our approach achieves higher success rates and shorter completion times compared to existing methods. These results highlight the benefit of coupling belief inference, probing, and risk monitoring, yielding a principled and interpretable framework for planning under uncertainty.

</details>


### [330] [Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction](https://arxiv.org/abs/2602.14551)
*Taichi Kato,Takuya Kiyokawa,Namiko Saito,Kensuke Harada*

Main category: cs.RO

TL;DR: A dual-correction framework enhances VLM-based human-robot collaboration by adding internal logical verification and external physical failure correction, improving success rates in assembly tasks.


<details>
  <summary>Details</summary>
Motivation: Human instructions in HRC are often ambiguous and underspecified, making it hard to generate feasible robot behaviors. While VLMs help interpret instructions, they suffer from hallucinated reasoning and inability to anticipate physical execution failures.

Method: Proposes an HRC framework that augments VLM-based reasoning with a dual-correction mechanism: 1) internal correction model verifies logical consistency and task feasibility before execution, 2) external correction model detects and rectifies physical failures through post-execution feedback.

Result: Simulation ablation studies show improved success rate compared to baselines without correction models. Real-world experiments in collaborative assembly tasks with a humanoid robot confirm effectiveness in enabling interactive replanning across different tasks.

Conclusion: The dual-correction framework effectively addresses VLM limitations in HRC, improving both logical reasoning and physical execution, validating practical feasibility for collaborative assembly tasks.

Abstract: Human-Robot Collaboration (HRC) plays an important role in assembly tasks by enabling robots to plan and adjust their motions based on interactive, real-time human instructions. However, such instructions are often linguistically ambiguous and underspecified, making it difficult to generate physically feasible and cooperative robot behaviors. To address this challenge, many studies have applied Vision-Language Models (VLMs) to interpret high-level instructions and generate corresponding actions. Nevertheless, VLM-based approaches still suffer from hallucinated reasoning and an inability to anticipate physical execution failures. To address these challenges, we propose an HRC framework that augments a VLM-based reasoning with a dual-correction mechanism: an internal correction model that verifies logical consistency and task feasibility prior to action execution, and an external correction model that detects and rectifies physical failures through post-execution feedback. Simulation ablation studies demonstrate that the proposed method improves the success rate compared to baselines without correction models. Our real-world experiments in collaborative assembly tasks supported by object fixation or tool preparation by an upper body humanoid robot further confirm the framewor's effectiveness in enabling interactive replanning across different collaborative tasks in response to human instructions, validating its practical feasibility.

</details>


### [331] [Simulation-based Learning of Electrical Cabinet Assembly Using Robot Skills](https://arxiv.org/abs/2602.14561)
*Arik Laemmle,Balázs András Bálint,Philipp Tenbrock,Frank Naegele,David Traunecker,József Váncza,Marco F. Huber*

Main category: cs.RO

TL;DR: Simulation-driven DRL automates force-controlled assembly of electrical terminals on DIN-rails, using physics-based simulation with analytical/rigid-body joining models, modular robot skills, and domain randomization for robust real-world transfer.


<details>
  <summary>Details</summary>
Motivation: Traditional force-controlled assembly of electrical terminals on DIN-rails requires high programming effort and struggles with product variability, making it inefficient for small-batch manufacturing.

Method: Integrates DRL with parameterizable robot skills in physics-based simulation. Uses analytical beam theory models and MuJoCo rigid-body models for realistic snap-fit assembly simulation. Employs pitasc framework for modular skills, trains with SAC and TD3 algorithms, applies domain randomization for robustness.

Result: Achieves up to 100% success rates in both simulation and real-world settings with UR10e robot, even under significant positional/rotational deviations. Generalizes to new terminal types and positions, significantly reducing manual programming effort.

Conclusion: Combining simulation-based learning with modular robot skills enables flexible, scalable automation for small-batch manufacturing, with potential for hybrid learning methods and automated environment parameterization in future work.

Abstract: This paper presents a simulation-driven approach for automating the force-controlled assembly of electrical terminals on DIN-rails, a task traditionally hindered by high programming effort and product variability. The proposed method integrates deep reinforcement learning (DRL) with parameterizable robot skills in a physics-based simulation environment. To realistically model the snap-fit assembly process, we develop and evaluate two types of joining models: analytical models based on beam theory and rigid-body models implemented in the MuJoCo physics engine. These models enable accurate simulation of interaction forces, essential for training DRL agents. The robot skills are structured using the pitasc framework, allowing modular, reusable control strategies. Training is conducted in simulation using Soft Actor-Critic (SAC) and Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithms. Domain randomization is applied to improve robustness. The trained policies are transferred to a physical UR10e robot system without additional tuning. Experimental results demonstrate high success rates (up to 100%) in both simulation and real-world settings, even under significant positional and rotational deviations. The system generalizes well to new terminal types and positions, significantly reducing manual programming effort. This work highlights the potential of combining simulation-based learning with modular robot skills for flexible, scalable automation in small-batch manufacturing. Future work will explore hybrid learning methods, automated environment parameterization, and further refinement of joining models for design integration.

</details>


### [332] [Real-time Monocular 2D and 3D Perception of Endoluminal Scenes for Controlling Flexible Robotic Endoscopic Instruments](https://arxiv.org/abs/2602.14666)
*Ruofeng Wei,Kai Chen,Yui Lun Ng,Yiyao Ma,Justin Di-Lang Ho,Hon Sing Tong,Xiaomei Wang,Jing Dai,Ka-Wai Kwok,Qi Dou*

Main category: cs.RO

TL;DR: Visual perception platform for continuum robotic endoluminal surgery using monocular endoscopic image-based algorithms to identify instrument position/orientation and measure tissue distances, with a physically-realistic simulator for training and data generation.


<details>
  <summary>Details</summary>
Motivation: Endoluminal surgery for early-stage gastrointestinal and urinary tract cancers has limitations due to surgical tools and steep learning curve. Robotic systems, especially continuum robots, offer flexible instruments for precise tissue resection but need better perception and control.

Method: Developed 2D and 3D learning-based perception algorithms using monocular endoscopic images to identify flexible instrument position/orientation and measure tissue distances. Created a physically-realistic simulator modeling flexible instrument dynamics to generate realistic endoluminal scenes for training and data collection.

Result: Algorithms improved control of flexible instruments, reducing manipulation time by over 70% for trajectory-following tasks. Enhanced understanding of surgical scenarios and enabled robust endoluminal surgeries. Module and system-level evaluations with continuum robot prototype demonstrated effectiveness.

Conclusion: The visual perception platform successfully addresses limitations in endoluminal surgery by improving instrument control and surgical understanding through learning-based perception algorithms and realistic simulation, potentially making robotic endoluminal surgery more accessible and effective.

Abstract: Endoluminal surgery offers a minimally invasive option for early-stage gastrointestinal and urinary tract cancers but is limited by surgical tools and a steep learning curve. Robotic systems, particularly continuum robots, provide flexible instruments that enable precise tissue resection, potentially improving outcomes. This paper presents a visual perception platform for a continuum robotic system in endoluminal surgery. Our goal is to utilize monocular endoscopic image-based perception algorithms to identify position and orientation of flexible instruments and measure their distances from tissues. We introduce 2D and 3D learning-based perception algorithms and develop a physically-realistic simulator that models flexible instruments dynamics. This simulator generates realistic endoluminal scenes, enabling control of flexible robots and substantial data collection. Using a continuum robot prototype, we conducted module and system-level evaluations. Results show that our algorithms improve control of flexible instruments, reducing manipulation time by over 70% for trajectory-following tasks and enhancing understanding of surgical scenarios, leading to robust endoluminal surgeries.

</details>


### [333] [ManeuverNet: A Soft Actor-Critic Framework for Precise Maneuvering of Double-Ackermann-Steering Robots with Optimized Reward Functions](https://arxiv.org/abs/2602.14726)
*Kohio Deflesselle,Mélodie Daniel,Aly Magassouba,Miguel Aranda,Olivier Ly*

Main category: cs.RO

TL;DR: ManeuverNet: A DRL framework for double-Ackermann-steering robots that combines Soft Actor-Critic with CrossQ and specialized reward functions, achieving 40%+ improvement over DRL baselines and 90% real-world efficiency gains.


<details>
  <summary>Details</summary>
Motivation: Autonomous control of double-Ackermann-steering robots is crucial for agricultural applications requiring precise maneuvers in limited spaces. Classical methods like TEB planner require extensive parameter tuning and are sensitive to configuration/environment changes, while end-to-end DRL methods often fail due to unsuitable reward functions for non-holonomic constraints.

Method: ManeuverNet is a DRL framework specifically designed for double-Ackermann systems, combining Soft Actor-Critic with CrossQ. It introduces four specifically designed reward functions to support maneuver learning, without relying on expert data or handcrafted guidance.

Result: ManeuverNet achieves more than 40% gain over DRL baselines in maneuverability and success rates, effectively mitigates parameter sensitivity issues of TEB planner, and in real-world trials achieves up to 90% increase in maneuvering trajectory efficiency.

Conclusion: ManeuverNet provides a robust and practical solution for autonomous control of double-Ackermann-steering robots, demonstrating superior performance over both classical and DRL methods while eliminating the need for expert data or constant parameter recalibration.

Abstract: Autonomous control of double-Ackermann-steering robots is essential in agricultural applications, where robots must execute precise and complex maneuvers within a limited space. Classical methods, such as the Timed Elastic Band (TEB) planner, can address this problem, but they rely on parameter tuning, making them highly sensitive to changes in robot configuration or environment and impractical to deploy without constant recalibration. At the same time, end-to-end deep reinforcement learning (DRL) methods often fail due to unsuitable reward functions for non-holonomic constraints, resulting in sub-optimal policies and poor generalization. To address these challenges, this paper presents ManeuverNet, a DRL framework tailored for double-Ackermann systems, combining Soft Actor-Critic with CrossQ. Furthermore, ManeuverNet introduces four specifically designed reward functions to support maneuver learning. Unlike prior work, ManeuverNet does not depend on expert data or handcrafted guidance. We extensively evaluate ManeuverNet against both state-of-the-art DRL baselines and the TEB planner. Experimental results demonstrate that our framework substantially improves maneuverability and success rates, achieving more than a 40% gain over DRL baselines. Moreover, ManeuverNet effectively mitigates the strong parameter sensitivity observed in the TEB planner. In real-world trials, ManeuverNet achieved up to a 90% increase in maneuvering trajectory efficiency, highlighting its robustness and practical applicability.

</details>


### [334] [Analysis of a Cuspidal 6R Robot](https://arxiv.org/abs/2602.14794)
*Alexander Feeß,Martin Weiß*

Main category: cs.RO

TL;DR: Analysis of kinematics for the Transpressor, a cuspidal 6R robot with up to 16 inverse kinematics solutions, including geometric description, analytical solutions for special poses, numerical solver, and proof of cuspidality.


<details>
  <summary>Details</summary>
Motivation: To analyze the kinematics of the Transpressor robot, which is a cuspidal 6R robot, and understand its inverse kinematics solutions and cuspidal properties.

Method: Theoretical and numerical analysis including: geometric description of up to 16 inverse kinematics solutions, analytical solutions for special target poses, development of a simple numerical solver for general cases, and analytical estimation of Jacobian determinant on paths between solutions.

Result: The Transpressor admits up to 16 inverse kinematics solutions that can be described geometrically. Analytical solutions are provided for special target poses, and a numerical solver works for general cases. The analytical estimate of Jacobian determinant proves cuspidality for a class of robots similar to the Transpressor.

Conclusion: The paper provides comprehensive kinematic analysis of the Transpressor robot, demonstrating its cuspidal nature with up to 16 inverse kinematics solutions, offering both analytical and numerical solution methods, and proving cuspidality for similar robot architectures.

Abstract: We present a theoretical and numerical analysis of the kinematics for the "Transpressor", a cuspidal 6R robot. It admits up to 16 inverse kinematics solutions which are described geometrically. For special target poses, we provide the solutions analytically and present a simple numerical solver for the general case. Moreover, an analytical estimate of the Jacobian determinant on a path between two solutions proves cuspidality for a class of robots similar to the transpressor.

</details>


### [335] [Scalable Multi-Robot Path Planning via Quadratic Unconstrained Binary Optimization](https://arxiv.org/abs/2602.14799)
*Javier González Villasmil*

Main category: cs.RO

TL;DR: QUBO formulation for multi-agent path finding that achieves scalability through BFS pre-processing, adaptive penalties, and time-windowed decomposition, demonstrating near-optimal solutions for up to 4 robots.


<details>
  <summary>Details</summary>
Motivation: Classical centralized MAPF approaches suffer from exponential growth in joint-state complexity as agent numbers increase, creating scalability limitations for multi-robot systems.

Method: QUBO formulation with BFS-based logical pre-processing (95% variable reduction), adaptive penalty design for collision/constraint enforcement, and time-windowed decomposition strategy for hardware feasibility.

Result: Experimental evaluation in grid environments with up to 4 robots showed near-optimal solutions in dense scenarios and favorable scaling compared to sequential classical planning.

Conclusion: Establishes a practical, reproducible baseline for future quantum and quantum-inspired multi-robot coordination approaches.

Abstract: Multi-Agent Path Finding (MAPF) remains a fundamental challenge in robotics, where classical centralized approaches exhibit exponential growth in joint-state complexity as the number of agents increases. This paper investigates Quadratic Unconstrained Binary Optimization (QUBO) as a structurally scalable alternative for simultaneous multi-robot path planning. This approach is a robotics-oriented QUBO formulation incorporating BFS-based logical pre-processing (achieving over 95% variable reduction), adaptive penalty design for collision and constraint enforcement, and a time-windowed decomposition strategy that enables execution within current hardware limitations. An experimental evaluation in grid environments with up to four robots demonstrated near-optimal solutions in dense scenarios and favorable scaling behavior compared to sequential classical planning. These results establish a practical and reproducible baseline for future quantum and quantum-inspired multi-robot coordinations.

</details>


### [336] [Affordance Transfer Across Object Instances via Semantically Anchored Functional Map](https://arxiv.org/abs/2602.14874)
*Xiaoxiang Dong,Weiming Zhi*

Main category: cs.RO

TL;DR: SemFM enables robots to transfer learned affordances from one object to geometrically different objects using a single visual demonstration, avoiding the need for physical robot demonstrations.


<details>
  <summary>Details</summary>
Motivation: Traditional LfD requires many physical demonstrations which are time-consuming and hard to scale. While recent methods allow learning from human videos, they struggle to generalize demonstrated interactions across objects with similar function but different geometry.

Method: Semantic Anchored Functional Maps (SemFM) identifies semantically corresponding functional regions between objects, selects mutually exclusive semantic anchors, and propagates these constraints using functional maps to obtain dense, semantically consistent correspondence for affordance transfer.

Result: Experiments on synthetic object categories and real-world robotic manipulation tasks show accurate affordance transfer with modest computational cost, making it suitable for practical robotic perception-to-action pipelines.

Conclusion: SemFM provides a lightweight, interpretable framework for transferring demonstrated interactions across geometrically diverse objects from just a single visual demonstration, addressing the generalization challenge in learning from demonstration.

Abstract: Traditional learning from demonstration (LfD) generally demands a cumbersome collection of physical demonstrations, which can be time-consuming and challenging to scale. Recent advances show that robots can instead learn from human videos by extracting interaction cues without direct robot involvement. However, a fundamental challenge remains: how to generalize demonstrated interactions across different object instances that share similar functionality but vary significantly in geometry. In this work, we propose \emph{Semantic Anchored Functional Maps} (SemFM), a framework for transferring affordances across objects from a single visual demonstration. Starting from a coarse mesh reconstructed from an image, our method identifies semantically corresponding functional regions between objects, selects mutually exclusive semantic anchors, and propagates these constraints over the surface using a functional map to obtain a dense, semantically consistent correspondence. This enables demonstrated interaction regions to be transferred across geometrically diverse objects in a lightweight and interpretable manner. Experiments on synthetic object categories and real-world robotic manipulation tasks show that our approach enables accurate affordance transfer with modest computational cost, making it well-suited for practical robotic perception-to-action pipelines.

</details>


### [337] [Kalman Filtering Based Flight Management System Modeling for AAM Aircraft](https://arxiv.org/abs/2602.14948)
*Balram Kandoria,Aryaman Singh Samyal*

Main category: cs.RO

TL;DR: A Kalman Filter-based uncertainty propagation method for AAM flight planning that uses sigmoid-blended measurement noise covariance to adapt measurement trust based on waypoint progress, achieving 76% accuracy in arrival time prediction.


<details>
  <summary>Details</summary>
Motivation: Current AAM flight planning uses conservative linear models for uncertainty estimation due to limited real-world data, which is insufficient for strategic planning that needs to account for spatial and temporal uncertainties against hazards like weather, restricted airspace, and CNS disruptions.

Method: Novel Kalman Filter-based uncertainty propagation method that models AAM FMS architectures through sigmoid-blended measurement noise covariance. The method continuously adapts measurement trust based on progress toward waypoints, allowing FMS correction behavior to emerge naturally. It scales proportionally with control inputs and is tunable for specific aircraft or route conditions.

Result: Validated using real ADS-B data from general aviation aircraft divided into training and verification sets. Parameters tuned on training set achieved 76% accuracy in predicting arrival times when compared against verification dataset.

Conclusion: The method demonstrates effectiveness for strategic flight plan validation in AAM operations by providing adaptive uncertainty estimation that outperforms current conservative linear models, enabling more accurate prediction of arrival times and better hazard avoidance planning.

Abstract: Advanced Aerial Mobility (AAM) operations require strategic flight planning services that predict both spatial and temporal uncertainties to safely validate flight plans against hazards such as weather cells, restricted airspaces, and CNS disruption areas. Current uncertainty estimation methods for AAM vehicles rely on conservative linear models due to limited real-world performance data. This paper presents a novel Kalman Filter-based uncertainty propagation method that models AAM Flight Management System (FMS) architectures through sigmoid-blended measurement noise covariance. Unlike existing approaches with fixed uncertainty thresholds, our method continuously adapts the filter's measurement trust based on progress toward waypoints, enabling FMS correction behavior to emerge naturally. The approach scales proportionally with control inputs and is tunable to match specific aircraft characteristics or route conditions. We validate the method using real ADS-B data from general aviation aircraft divided into training and verification sets. Uncertainty propagation parameters were tuned on the training set, achieving 76% accuracy in predicting arrival times when compared against the verification dataset, demonstrating the method's effectiveness for strategic flight plan validation in AAM operations.

</details>


### [338] [Morphing of and writing with a scissor linkage mechanism](https://arxiv.org/abs/2602.14958)
*Mohanraj A,S Ganga Prasath*

Main category: cs.RO

TL;DR: Scissor-unit assemblies with single degree of freedom can be programmed for shape morphing and writing tasks using differentiable simulation optimization, enabling automated navigation in complex domains.


<details>
  <summary>Details</summary>
Motivation: To explore how scissor-unit assemblies (two rigid linear members connected by a pin joint) can be programmed for reproducible motion with minimal actuation, leveraging their geometric properties for practical applications.

Method: Derived expressions for effective curvature and tip trajectory as functions of geometric variables, then formulated shape morphing and writing tasks as optimization problems using differentiable simulation framework.

Result: Successfully programmed scissor assemblies for shape morphing and writing tasks, demonstrating potential for automated navigation and inspection in complex domains through table-top experiments.

Conclusion: Scissor assembly geometry can be leveraged for automated tasks using optimization frameworks, but challenges remain in rapid programming and error-free implementation without feedback.

Abstract: Kinematics of mechanisms is intricately coupled to their geometry and their utility often arises out of the ability to perform reproducible motion with fewer actuating degrees of freedom. In this article, we explore the assembly of scissor-units, each made of two rigid linear members connected by a pin joint. The assembly has a single degree of freedom, where actuating any single unit results in a shape change of the entire assembly. We derive expressions for the effective curvature of the unit and the trajectory of the mechanism's tip as a function of the geometric variables which we then use as the basis to program two tasks in the mechanism: shape morphing and writing. By phrasing these tasks as optimization problems and utilizing the differentiable simulation framework, we arrive at solutions that are then tested in table-top experiments. Our results show that the geometry of scissor assemblies can be leveraged for automated navigation and inspection in complex domains, in light of the optimization framework. However, we highlight that the challenges associated with rapid programming and error-free implementation in experiments without feedback still remain.

</details>


### [339] [PhyScensis: Physics-Augmented LLM Agents for Complex Physical Scene Arrangement](https://arxiv.org/abs/2602.14968)
*Yian Wang,Han Yang,Minghao Guo,Xiaowen Qiu,Tsun-Hsuan Wang,Wojciech Matusik,Joshua B. Tenenbaum,Chuang Gan*

Main category: cs.RO

TL;DR: PhyScensis: An LLM agent-based framework with physics engine integration for generating complex, physically plausible 3D environments for robotic manipulation training.


<details>
  <summary>Details</summary>
Motivation: Existing 3D environment generation methods focus on asset placement but neglect physical relationships (contact, support, balance, containment) needed for realistic manipulation scenarios like tabletop arrangements, shelf organization, and box packing.

Method: Three-component framework: 1) LLM agent iteratively proposes assets with spatial/physical predicates, 2) physics engine solver realizes predicates into 3D scenes, 3) feedback loop refines configurations. Uses probabilistic programming for stability and heuristic regulation of stability/spatial relations.

Result: Outperforms prior approaches in scene complexity, visual quality, and physical accuracy. Provides unified pipeline for generating complex physical scene layouts suitable for robotic manipulation training.

Conclusion: PhyScensis addresses key challenges in physical scene generation (high object density, supporting relationships, physical modeling) and enables scalable creation of realistic manipulation environments for robotics data collection.

Abstract: Automatically generating interactive 3D environments is crucial for scaling up robotic data collection in simulation. While prior work has primarily focused on 3D asset placement, it often overlooks the physical relationships between objects (e.g., contact, support, balance, and containment), which are essential for creating complex and realistic manipulation scenarios such as tabletop arrangements, shelf organization, or box packing. Compared to classical 3D layout generation, producing complex physical scenes introduces additional challenges: (a) higher object density and complexity (e.g., a small shelf may hold dozens of books), (b) richer supporting relationships and compact spatial layouts, and (c) the need to accurately model both spatial placement and physical properties. To address these challenges, we propose PhyScensis, an LLM agent-based framework powered by a physics engine, to produce physically plausible scene configurations with high complexity. Specifically, our framework consists of three main components: an LLM agent iteratively proposes assets with spatial and physical predicates; a solver, equipped with a physics engine, realizes these predicates into a 3D scene; and feedback from the solver informs the agent to refine and enrich the configuration. Moreover, our framework preserves strong controllability over fine-grained textual descriptions and numerical parameters (e.g., relative positions, scene stability), enabled through probabilistic programming for stability and a complementary heuristic that jointly regulates stability and spatial relations. Experimental results show that our method outperforms prior approaches in scene complexity, visual quality, and physical accuracy, offering a unified pipeline for generating complex physical scene layouts for robotic manipulation.

</details>


### [340] [DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI](https://arxiv.org/abs/2602.14974)
*En Yu,Haoran Lv,Jianjian Sun,Kangheng Lin,Ruitao Zhang,Yukang Shi,Yuyang Chen,Ze Chen,Ziheng Zhang,Fan Jia,Kaixin Liu,Meng Zhang,Ruitao Hao,Saike Huang,Songhan Xie,Yu Liu,Zhao Wu,Bin Xie,Pengwei Zhang,Qi Yang,Xianchi Deng,Yunfei Wei,Enwen Zhang,Hongyang Peng,Jie Zhao,Kai Liu,Wei Sun,Yajun Wei,Yi Yang,Yunqiao Zhang,Ziwei Yan,Haitao Yang,Hao Liu,Haoqiang Fan,Haowei Zhang,Junwen Huang,Yang Chen,Yunchao Ma,Yunhuan Yang,Zhengyuan Du,Ziming Liu,Jiahui Niu,Yucheng Zhao,Daxin Jiang,Wenbin Tang,Xiangyu Zhang,Zheng Ge,Erjin Zhou,Tiancai Wang*

Main category: cs.RO

TL;DR: DM0 is an embodied-native VLA framework that unifies manipulation and navigation from the start, using a three-stage pipeline with hybrid training and spatial reasoning scaffolding to achieve SOTA on RoboChallenge.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches adapt internet-pretrained models to physical tasks as an afterthought. The authors want to move beyond this paradigm by creating an embodied-native framework that learns physical grounding from heterogeneous data sources from the beginning.

Method: Three-stage pipeline: 1) Large-scale unified pretraining on VLM with diverse corpora (web text, driving scenarios, embodied logs). 2) Build flow-matching action expert on VLM. 3) Hybrid training strategy: VLM gradients frozen for embodied data but trainable on non-embodied data. Plus Embodied Spatial Scaffolding for spatial Chain-of-Thought reasoning.

Result: DM0 achieves state-of-the-art performance on RoboChallenge benchmark in both Specialist and Generalist settings on Table30.

Conclusion: DM0 demonstrates that an embodied-native approach, with unified learning from diverse physical data sources and careful architectural design, outperforms traditional adaptation methods for physical AI tasks.

Abstract: Moving beyond the traditional paradigm of adapting internet-pretrained models to physical tasks, we present DM0, an Embodied-Native Vision-Language-Action (VLA) framework designed for Physical AI. Unlike approaches that treat physical grounding as a fine-tuning afterthought, DM0 unifies embodied manipulation and navigation by learning from heterogeneous data sources from the onset. Our methodology follows a comprehensive three-stage pipeline: Pretraining, Mid-Training, and Post-Training. First, we conduct large-scale unified pretraining on the Vision-Language Model (VLM) using diverse corpora--seamlessly integrating web text, autonomous driving scenarios, and embodied interaction logs-to jointly acquire semantic knowledge and physical priors. Subsequently, we build a flow-matching action expert atop the VLM. To reconcile high-level reasoning with low-level control, DM0 employs a hybrid training strategy: for embodied data, gradients from the action expert are not backpropagated to the VLM to preserve generalized representations, while the VLM remains trainable on non-embodied data. Furthermore, we introduce an Embodied Spatial Scaffolding strategy to construct spatial Chain-of-Thought (CoT) reasoning, effectively constraining the action solution space. Experiments on the RoboChallenge benchmark demonstrate that DM0 achieves state-of-the-art performance in both Specialist and Generalist settings on Table30.

</details>


### [341] [RynnBrain: Open Embodied Foundation Models](https://arxiv.org/abs/2602.14979)
*Ronghao Dang,Jiayan Guo,Bohan Hou,Sicong Leng,Kehan Li,Xin Li,Jiangpin Liu,Yunxuan Mao,Zhikai Wang,Yuqian Yuan,Minghao Zhu,Xiao Lin,Yang Bai,Qian Jiang,Yaxi Zhao,Minghua Zeng,Junlong Gao,Yuming Jiang,Jun Cen,Siteng Huang,Liuyi Wang,Wenqiao Zhang,Chengju Liu,Jianfei Yang,Shijian Lu,Deli Zhao*

Main category: cs.RO

TL;DR: RynnBrain is an open-source spatiotemporal foundation model for embodied intelligence that integrates perception, reasoning, and planning in real-world spatial-temporal dynamics through four core capabilities.


<details>
  <summary>Details</summary>
Motivation: The embodied intelligence community lacks a unified, physically grounded foundation model that integrates perception, reasoning, and planning within real-world spatial-temporal dynamics, despite rapid progress in multimodal foundation models.

Method: RynnBrain is built as a spatiotemporal foundation model with four core capabilities: comprehensive egocentric understanding, diverse spatiotemporal localization, physically grounded reasoning, and physics-aware planning. The framework includes three foundation model scales (2B, 8B, and 30B-A3B MoE) and four post-trained variants for downstream tasks.

Result: RynnBrain foundation models significantly outperform existing embodied foundation models on 20 embodied benchmarks and 8 general vision understanding benchmarks. The post-trained variants demonstrate two key potentials: enabling physically grounded reasoning/planning and serving as an efficient pretrained backbone for diverse embodied tasks.

Conclusion: RynnBrain successfully addresses the need for a unified, physically grounded foundation model for embodied intelligence, demonstrating superior performance and versatility across multiple benchmarks and downstream applications.

Abstract: Despite rapid progress in multimodal foundation models, embodied intelligence community still lacks a unified, physically grounded foundation model that integrates perception, reasoning, and planning within real-world spatial-temporal dynamics. We introduce RynnBrain, an open-source spatiotemporal foundation model for embodied intelligence. RynnBrain strengthens four core capabilities in a unified framework: comprehensive egocentric understanding, diverse spatiotemporal localization, physically grounded reasoning, and physics-aware planning. The RynnBrain family comprises three foundation model scales (2B, 8B, and 30B-A3B MoE) and four post-trained variants tailored for downstream embodied tasks (i.e., RynnBrain-Nav, RynnBrain-Plan, and RynnBrain-VLA) or complex spatial reasoning tasks (i.e., RynnBrain-CoP). In terms of extensive evaluations on 20 embodied benchmarks and 8 general vision understanding benchmarks, our RynnBrain foundation models largely outperform existing embodied foundation models by a significant margin. The post-trained model suite further substantiates two key potentials of the RynnBrain foundation model: (i) enabling physically grounded reasoning and planning, and (ii) serving as a strong pretrained backbone that can be efficiently adapted to diverse embodied tasks.

</details>


### [342] [BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames](https://arxiv.org/abs/2602.15010)
*Max Sobol Mark,Jacky Liang,Maria Attarian,Chuyuan Fu,Debidatta Dwibedi,Dhruv Shah,Aviral Kumar*

Main category: cs.RO

TL;DR: Big Picture Policies (BPP) uses vision-language models to extract key task-relevant frames from robot histories, enabling effective history conditioning without spurious correlations.


<details>
  <summary>Details</summary>
Motivation: Current robot policies only condition on current observations, failing at tasks requiring history (like searching rooms). Naive history conditioning suffers from spurious correlations due to limited training coverage over exponentially growing history space.

Method: BPP uses vision-language models to detect minimal sets of meaningful keyframes from robot trajectories. Projects diverse rollouts onto compact task-relevant events to reduce distribution shift while maintaining expressivity.

Result: Evaluated on 4 real-world manipulation tasks and 3 simulation tasks requiring history conditioning. Achieves 70% higher success rates than best comparison methods on real-world evaluations.

Conclusion: BPP effectively addresses the coverage problem in history conditioning by focusing on task-relevant keyframes, substantially reducing distribution shift and improving generalization to out-of-distribution trajectories.

Abstract: Many robot tasks require attending to the history of past observations. For example, finding an item in a room requires remembering which places have already been searched. However, the best-performing robot policies typically condition only on the current observation, limiting their applicability to such tasks. Naively conditioning on past observations often fails due to spurious correlations: policies latch onto incidental features of training histories that do not generalize to out-of-distribution trajectories upon deployment. We analyze why policies latch onto these spurious correlations and find that this problem stems from limited coverage over the space of possible histories during training, which grows exponentially with horizon. Existing regularization techniques provide inconsistent benefits across tasks, as they do not fundamentally address this coverage problem. Motivated by these findings, we propose Big Picture Policies (BPP), an approach that conditions on a minimal set of meaningful keyframes detected by a vision-language model. By projecting diverse rollouts onto a compact set of task-relevant events, BPP substantially reduces distribution shift between training and deployment, without sacrificing expressivity. We evaluate BPP on four challenging real-world manipulation tasks and three simulation tasks, all requiring history conditioning. BPP achieves 70% higher success rates than the best comparison on real-world evaluations.

</details>


### [343] [Neurosim: A Fast Simulator for Neuromorphic Robot Perception](https://arxiv.org/abs/2602.15018)
*Richeek Das,Pratik Chaudhari*

Main category: cs.RO

TL;DR: Neurosim is a high-performance sensor simulation library (up to 2700 FPS) for DVS, RGB, depth, and inertial sensors, plus multi-rotor vehicle dynamics. It integrates with Cortex (ZeroMQ-based communication) for ML/robotics workflows, enabling training and real-time testing of neuromorphic algorithms.


<details>
  <summary>Details</summary>
Motivation: There's a need for fast, real-time sensor simulation to train and test neuromorphic perception and control algorithms in complex, dynamic environments, with seamless integration into ML/robotics workflows.

Method: Developed Neurosim (GPU-accelerated sensor simulator) and Cortex (ZeroMQ-based communication library). Neurosim simulates various sensors and vehicle dynamics, while Cortex provides high-throughput, low-latency messaging with native NumPy/PyTorch support.

Result: Achieves ~2700 FPS on desktop GPU. Enables training of neuromorphic algorithms using self-supervised learning on time-synchronized multi-modal data, and real-time closed-loop testing of implementations.

Conclusion: Neurosim and Cortex provide an effective platform for developing and testing neuromorphic perception and control systems, with high-performance simulation and seamless integration capabilities for ML/robotics applications.

Abstract: Neurosim is a fast, real-time, high-performance library for simulating sensors such as dynamic vision sensors, RGB cameras, depth sensors, and inertial sensors. It can also simulate agile dynamics of multi-rotor vehicles in complex and dynamic environments. Neurosim can achieve frame rates as high as ~2700 FPS on a desktop GPU. Neurosim integrates with a ZeroMQ-based communication library called Cortex to facilitate seamless integration with machine learning and robotics workflows. Cortex provides a high-throughput, low-latency message-passing system for Python and C++ applications, with native support for NumPy arrays and PyTorch tensors. This paper discusses the design philosophy behind Neurosim and Cortex. It demonstrates how they can be used to (i) train neuromorphic perception and control algorithms, e.g., using self-supervised learning on time-synchronized multi-modal data, and (ii) test real-time implementations of these algorithms in closed-loop. Neurosim and Cortex are available at https://github.com/grasp-lyrl/neurosim .

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [344] [SIM-assisted Secure Mobile Communications via Enhanced Proximal Policy Optimization Algorithm](https://arxiv.org/abs/2602.13265)
*Wenxuan Ma,Bin Lin,Hongyang Pan,Geng Sun,Enyu Shi,Jiancheng An,Chau Yuen*

Main category: eess.SP

TL;DR: SIM-assisted secure communication for mobile users using enhanced PPO algorithm to maximize secrecy rate under channel uncertainty and hardware impairments.


<details>
  <summary>Details</summary>
Motivation: With 6G development, security challenges for mobile users are increasing. Physical layer security using stacked intelligent metasurfaces offers energy-efficient security enhancement, but faces practical challenges like channel uncertainty, multiple user interference, and hardware impairments in mobile environments.

Method: Proposed an enhanced proximal policy optimization algorithm (PPO-BOP) with bidirectional LSTM mechanism, off-policy data utilization, and policy feedback mechanism to jointly optimize power and phase shift for SIM-assisted secure communication system.

Result: Extensive simulations show PPO-BOP significantly outperforms benchmark strategies and other deep reinforcement learning algorithms in terms of achievable secrecy rate.

Conclusion: The proposed SIM-assisted secure communication system with enhanced PPO algorithm effectively addresses practical challenges in 6G mobile environments and provides superior security performance for mobile users.

Abstract: With the development of sixth-generation (6G) wireless communication networks, the security challenges are becoming increasingly prominent, especially for mobile users (MUs). As a promising solution, physical layer security (PLS) technology leverages the inherent characteristics of wireless channels to provide security assurance. Particularly, stacked intelligent metasurface (SIM) directly manipulates electromagnetic waves through their multilayer structures, offering significant potential for enhancing PLS performance in an energy efficient manner. Thus, in this work, we investigate an SIM-assisted secure communication system for MUs under the threat of an eavesdropper, addressing practical challenges such as channel uncertainty in mobile environments, multiple MU interference, and residual hardware impairments. Consequently, we formulate a joint power and phase shift optimization problem (JPPSOP), aiming at maximizing the achievable secrecy rate (ASR) of all MUs. Given the non-convexity and dynamic nature of this optimization problem, we propose an enhanced proximal policy optimization algorithm with a bidirectional long short-term memory mechanism, an off-policy data utilization mechanism, and a policy feedback mechanism (PPO-BOP). Through these mechanisms, the proposed algorithm can effectively capture short-term channel fading and long-term MU mobility, improve sample utilization efficiency, and enhance exploration capabilities. Extensive simulation results demonstrate that PPO-BOP significantly outperforms benchmark strategies and other deep reinforcement learning algorithms in terms of ASR.10.1109/TWC.2026.3658332

</details>


### [345] [Sample-level EEG-based Selective Auditory Attention Decoding with Markov Switching Models](https://arxiv.org/abs/2602.13447)
*Yuanyuan Yao,Simon Geirnaert,Tinne Tuytelaars,Alexander Bertrand*

Main category: eess.SP

TL;DR: Proposes a Markov switching model (MSM) for sample-level selective auditory attention decoding from EEG signals, integrating decoding and smoothing into a single probabilistic framework to improve temporal resolution and switch detection speed.


<details>
  <summary>Details</summary>
Motivation: Existing methods for selective auditory attention decoding operate at window level, creating a trade-off between temporal resolution and accuracy. While HMM-based post-processing can smooth window-level outputs, it requires separate steps. The authors aim to integrate decoding and smoothing into a unified framework.

Method: Proposes a Markov switching model (MSM) that directly models the relationship between EEG and speech envelopes under each attention state while incorporating temporal dynamics of attention. Uses expectation-maximization algorithm for joint estimation of model parameters and attention states, enabling sample-level decoding.

Result: The integrated MSM formulation achieves comparable decoding accuracy to HMM post-processing while providing faster attention switch detection. The method enables sample-level attention decoding from EEG signals.

Conclusion: The Markov switching model provides an effective integrated framework for selective auditory attention decoding that combines decoding and smoothing, offering improved temporal resolution and faster switch detection compared to separate HMM post-processing approaches.

Abstract: Selective auditory attention decoding aims to identify the speaker of interest from listeners' neural signals, such as electroencephalography (EEG), in the presence of multiple concurrent speakers. Most existing methods operate at the window level, facing a trade-off between temporal resolution and decoding accuracy. Recent work has shown that hidden Markov model (HMM)-based post-processing can smooth window-level decoder outputs to improve this trade-off. Instead of using a separate smoothing step, we propose to integrate the decoding and smoothing components into a single probabilistic framework using a Markov switching model (MSM). It directly models the relationship between the EEG and speech envelopes under each attention state while incorporating the temporal dynamics of attention. This formulation enables sample-level attention decoding, with model parameters and attention states jointly estimated via the expectation-maximization algorithm. Experimental results demonstrate that this integrated MSM formulation achieves comparable decoding accuracy to HMM post-processing while providing faster attention switch detection. The code for the proposed method is available at https://github.com/YYao-42/MSM.

</details>


### [346] [Towards Causality-Aware Modeling for Multimodal Brain-Muscle Interactions](https://arxiv.org/abs/2602.13459)
*Farwa Abbas,Wei Dai,Zoran Cvetkovic,Verity McClelland*

Main category: eess.SP

TL;DR: A new DBN-informed CCM framework combines geometric manifold reconstruction with probabilistic temporal modeling to better analyze causal interactions in biomedical signals, showing improved performance in EEG-EMG studies of dystonia.


<details>
  <summary>Details</summary>
Motivation: Existing methods for analyzing causal interactions in biomedical signals have limitations: DBNs assume linear/simple dependencies, while manifold techniques like CCM capture nonlinear interactions but lack probabilistic quantification and interventional modeling capabilities.

Method: The authors introduce a DBN-informed CCM framework that integrates geometric manifold reconstruction from CCM with probabilistic temporal modeling from DBNs. This hybrid approach combines the strengths of both methods.

Result: Applied to multimodal EEG-EMG recordings from dystonic and neurotypical children, the method quantifies uncertainty, supports interventional simulation, and reveals frequency-specific reorganization of corticomuscular pathways in dystonia. It shows marked improvements in predictive consistency and causal stability compared to baseline approaches.

Conclusion: The proposed causality-aware multimodal modeling framework has potential for developing quantitative biomarkers and guiding targeted neuromodulatory interventions in neurological disorders like dystonia.

Abstract: Robust characterization of dynamic causal interactions in multivariate biomedical signals is essential for advancing computational and algorithmic methods in biomedical imaging. Conventional approaches, such as Dynamic Bayesian Networks (DBNs), often assume linear or simple statistical dependencies, while manifold based techniques like Convergent Cross Mapping (CCM) capture nonlinear, lagged interactions but lack probabilistic quantification and interventional modeling. We introduce a DBN informed CCM framework that integrates geometric manifold reconstruction with probabilistic temporal modeling. Applied to multimodal EEG-EMG recordings from dystonic and neurotypical children, the method quantifies uncertainty, supports interventional simulation, and reveals distinct frequency specific reorganization of corticomuscular pathways in dystonia. Experimental results show marked improvements in predictive consistency and causal stability as compared to baseline approaches, demonstrating the potential of causality aware multimodal modeling for developing quantitative biomarkers and guiding targeted neuromodulatory interventions.

</details>


### [347] [Blind Deconvolution Demixing using Modulated Inputs](https://arxiv.org/abs/2602.13481)
*Humera Hameed,Ali Ahmed*

Main category: eess.SP

TL;DR: Blind deconvolution demixing of modulated signals using deterministic subspace assumptions and gradient descent, requiring modulation rate Q ≥ N²(B+M).


<details>
  <summary>Details</summary>
Motivation: Addressing the challenging problem of blind deconvolution demixing with modulated inputs, where multiple bandlimited signals are modulated with known random sequences, convolved with different channels, and mixed at a receiver.

Method: Uses deterministic subspace assumption for input signals while keeping channel impulse responses arbitrary. Employs gradient descent algorithm for recovery, requiring modulating sequence alteration rate Q ≥ N²(B+M) and obeying sample complexity bounds.

Result: Shows that all signals and channels {s_n(t), h_n(t)} can be estimated from observed mixture y(t) using gradient descent when conditions are met. Extensive simulations demonstrate algorithm robustness, with phase transitions used to numerically investigate theoretical guarantees.

Conclusion: Proposes a feasible solution for blind deconvolution demixing of modulated signals using subspace assumptions and gradient descent, with theoretical conditions and empirical validation showing practical viability.

Abstract: This paper focuses on solving a challenging problem of blind deconvolution demixing involving modulated inputs. Specifically, multiple input signals $s_n(t)$, each bandlimited to $B$ Hz, are modulated with known random sequences $r_n(t)$ that alter at rate $Q$. Each modulated signal is convolved with a different M tap channel of impulse response $h_n(t)$, and the outputs of each channel are added at a common receiver to give the observed signal $y(t)=\sum_{n=1}^N (r_n(t)\odot s_n(t))\circledast h_n(t)$, where $\odot$ is the point wise multiplication, and $\circledast$ is circular convolution. Given this observed signal $y(t)$, we are concerned with recovering $s_n(t)$ and $h_n(t)$. We employ deterministic subspace assumption for the input signal $s_n(t)$ and keep the channel impulse response $h_n(t)$ arbitrary. We show that if modulating sequence is altered at a rate $Q \geq N^2 (B+M)$ and sample complexity bound is obeyed then all the signals and the channels, $\{s_n(t),h_n(t)\}_{n=1}^N$, can be estimated from the observed mixture $y(t)$ using gradient descent algorithm. We have performed extensive simulations that show the robustness of our algorithm and used phase transitions to numerically investigate the theoretical guarantees provided by our algorithm.

</details>


### [348] [Feasibility of simultaneous EEG-fMRI at 0.55 T: Recording, Denoising, and Functional Mapping](https://arxiv.org/abs/2602.13489)
*Parsa Razmara,Takfarinas Medani,Majid Abbasi Sisara,Anand A. Joshi,Rui Chen,Woojae Jeong,Ye Tian,Krishna S. Nayak,Richard M. Leahy*

Main category: eess.SP

TL;DR: Feasibility study shows simultaneous EEG-fMRI at low-field (0.55T) is viable with reduced artifacts, enabling multimodal neuroimaging with preserved signal quality.


<details>
  <summary>Details</summary>
Motivation: High-field (≥3T) EEG-fMRI systems have technical limitations including EEG artifacts, reduced compatibility with implants, high noise, and susceptibility artifacts. Low-field systems could potentially overcome these limitations while maintaining multimodal integration capabilities.

Method: Proof-of-concept study using simultaneous EEG-fMRI at 0.55T during visual tasks. Characterized gradient and ballistocardiogram (BCG) artifacts, tested multimodal integration pipeline, and compared EEG power envelope with hemodynamic BOLD response.

Result: Reduced BCG magnitude consistent with expected scaling of pulse-related artifacts with magnetic field strength. EEG power envelope corresponds with hemodynamic BOLD response, supporting neurovascular coupling measurement. Demonstrated feasibility of combined EEG-fMRI at 0.55T.

Conclusion: Simultaneous EEG-fMRI at 0.55T is feasible and represents a promising environment for multimodal neuroimaging with reduced artifacts, better compatibility, and preserved signal integrity for studying neurovascular coupling.

Abstract: Simultaneous recording of electroencephalography (EEG) and functional MRI (fMRI) can provide a more complete view of brain function by merging high temporal and spatial resolutions. High-field ($\geq$3T) systems are standard, and require technical trade-offs, including artifacts in the EEG signal, reduced compatibility with metallic implants, high acoustic noise, and artifacts around high-susceptibility areas such as the optic nerve and nasal sinus. This proof-of-concept study demonstrates the feasibility of simultaneous EEG-fMRI at 0.55T in a visual task. We characterize the gradient and ballistocardiogram (BCG) artifacts inherent to this environment and observe reduced BCG magnitude consistent with the expected scaling of pulse-related artifacts with static magnetic field strength. This reduction shows promise for facilitating effective denoising while preserving the alpha rhythm and signal integrity. Furthermore, we tested a multimodal integration pipeline and demonstrated that the EEG power envelope corresponds with the hemodynamic BOLD response, supporting the potential to measure neurovascular coupling in this environment. We demonstrate that combined EEG-fMRI at 0.55T is feasible and represents a promising environment for multimodal neuroimaging.

</details>


### [349] [Sub Specie Aeternitatis: Fourier Transforms from the Theory of Heat to Musical Signals](https://arxiv.org/abs/2602.13520)
*Victor Lazzarini*

Main category: eess.SP

TL;DR: The paper traces the historical development of Fourier's ideas from heat theory to modern musical signal processing, highlighting how Fourier's coefficient method and double integral evolved into the Fourier theorem with applications in music theory and signal analysis.


<details>
  <summary>Details</summary>
Motivation: To explore the historical journey of Fourier's mathematical concepts from their origins in heat propagation theory to their modern applications in musical signal processing, using primary sources to trace this intellectual evolution.

Method: Historical analysis using primary sources, tracing the development of Fourier's ideas through key figures like Ohm, Helmholtz, De Morgan, and Dirac, examining how Fourier's coefficient method and double integral evolved into the modern Fourier theorem.

Result: Demonstrates how Fourier's 1822 work on heat theory provided two key ideas that were later developed by other scientists into foundational concepts for musical tone theory and signal processing, culminating in the modern understanding of time-frequency duality.

Conclusion: Fourier's theorem represents a fundamental duality between time and frequency domains that emerged from historical developments in mathematics and physics, with profound implications for understanding musical signals and signal processing.

Abstract: J. B. Fourier in his \emph{Théorie Analytique de la Chaleur} of 1822 introduced, amongst other things, two ideas that have made a fundamental impact in fields as diverse as Mathematical Physics, Electrical Engineering, Computer Science, and Music. The first one of these, a method to find the coefficients for a trigonometric series describing an arbitrary function, was very early on picked up by G. Ohm and H. Helmholtz as the foundation for a theory of \emph{musical tones}. The second one, which is described by Fourier's double integral, became the basis for treating certain kinds of infinity in discontinuous functions, as shown by A. De Morgan in his 1842 \emph{The Differential and Integral Calculus}. Both make up the fundamental basis for what is now commonly known as the \emph{Fourier theorem}. With the help of P. A. M. Dirac's insights into the nature of these infinities, we can have a compact description of the frequency spectrum of a function of time, or conversely of a waveform corresponding to a given function of frequency. This paper, using solely primary sources, takes us from the physics of heat propagation to the modern theory of musical signals. It concludes with some considerations on the inherent duality of time and frequency emerging from Fourier's theorem.

</details>


### [350] [DopplerGLRTNet for Radar Off-Grid Detection](https://arxiv.org/abs/2602.13546)
*Yadang Alexis Rouzoumka,Jean Pinsolle,Eugénie Terreaux,Christèle Morisseau,Jean-Philippe Ovarlez,Chengfang Ren*

Main category: eess.SP

TL;DR: DopplerGLRTNet is a lightweight neural network approach that predicts continuous Doppler parameters to solve off-grid detection saturation in radar systems, achieving near-optimal performance at much lower computational cost than traditional dense scanning methods.


<details>
  <summary>Details</summary>
Motivation: Classical normalized matched-filter detectors suffer from severe performance degradation when targets have Doppler frequencies that don't align with discrete processing grids, causing detection probability saturation at operationally relevant low false-alarm rates even at high SNR.

Method: Proposes DopplerGLRTNet, an amortized off-grid GLRT using a lightweight regressor neural network to predict continuous Doppler within a resolution cell from whitened observations, then outputs a single GLRT/NMF-like score based on normalized matched-filter energy at the predicted Doppler.

Result: Monte Carlo simulations in Gaussian and compound-Gaussian clutter show that DopplerGLRTNet mitigates off-grid saturation, approaches dense-scan performance at a fraction of its computational cost, and improves robustness to covariance estimation while maintaining empirically calibrated false-alarm rates.

Conclusion: DopplerGLRTNet provides an efficient solution to the off-grid detection problem by combining neural network-based parameter prediction with traditional detection theory, offering significant computational savings while maintaining detection performance and robustness.

Abstract: Off-grid targets whose Doppler (or angle) does not lie on the discrete processing grid can severely degrade classical normalized matched-filter (NMF) detectors: even at high SNR, the detection probability may saturate at operationally relevant low false-alarm rates. A principled remedy is the continuous-parameter GLRT, which maximizes a normalized correlation over the parameter domain; however, dense scanning increases test-time cost and remains sensitive to covariance mismatch through whitening. We propose DopplerGLRTNet, an amortized off-grid GLRT: a lightweight regressor predicts the continuous Doppler within a resolution cell from the whitened observation, and the detector outputs a single GLRT/NMF-like score given by the normalized matched-filter energy at the predicted Doppler. Monte Carlo simulations in Gaussian and compound-Gaussian clutter show that DopplerGLRTNet mitigates off-grid saturation, approaches dense-scan performance at a fraction of its cost, and improves robustness to covariance estimation at the same empirically calibrated Pfa.

</details>


### [351] [Twenty-five years of J-DSP Online Labs for Signal Processing Classes and Workforce Development Programs](https://arxiv.org/abs/2602.13863)
*Andreas Spanias*

Main category: eess.SP

TL;DR: J-DSP is a long-running online simulation program for DSP education, originally in Java and now HTML5, supporting digital filters, FFT, machine learning, and Quantum Fourier Transform with mobile apps and widespread university use.


<details>
  <summary>Details</summary>
Motivation: To create and maintain an online simulation platform for supporting digital signal processing (DSP) education through laboratory exercises, enabling students to learn DSP concepts through interactive simulations.

Method: Developed web-based software initially in Java, later transitioned to HTML5 for security, with mobile versions for iOS and Android. The program supports various DSP laboratory exercises including digital filter design, FFT spectral analysis, machine learning for signal classification, and Quantum Fourier Transform simulations.

Result: J-DSP has been successfully deployed since 2000, used in multiple universities, and specific functions have been utilized in NSF REU, IRES, and RET workforce development programs as well as high school outreach initiatives. The program has undergone continuous development and evaluation.

Conclusion: J-DSP represents a successful, long-term educational technology project that has evolved over two decades to support DSP education through online simulations, demonstrating sustainability and adaptability to new technologies and educational needs.

Abstract: This paper presents the history of the online simulation program Java-DSP (J-DSP) and the most recent function development and deployment. J-DSP was created to support online laboratories in DSP classes and was first deployed in our ASU DSP class in 2000. The development of the program and its extensions was supported by several NSF grants including CCLI and IUSE. The web-based software was developed by our team in Java and later transitioned to the more secure HTML5 environment. J-DSP supports laboratory exercises on: digital filters and their design, the FFT and its utility in spectral analysis, machine learning for signal classification, and more recently online simulations with the Quantum Fourier Transform. Throughout the J-DSP development and deployment of this tool and its associated laboratory exercises, we documented evaluations. Mobile versions of the program for iOS and Android were also developed. J-DSP is used to this day in several universities, and specific functions of the program have been used in NSF REU, IRES and RET workforce development and high school outreach.

</details>


### [352] [Efficient Off-Grid Near-Field Cascade Channel Estimation for XL-IRS Systems via Tucker Decomposition](https://arxiv.org/abs/2602.13988)
*Wenzhou Cao,Yashuai Cao,Tiejun Lv,Mugen Peng*

Main category: eess.SP

TL;DR: Proposed a tensor-based off-grid cascaded channel estimation framework for XL-IRS in near-field scenarios using sparse Tucker decomposition to avoid quantization errors and reduce complexity.


<details>
  <summary>Details</summary>
Motivation: XL-IRS in next-generation networks face near-field spherical wavefront propagation that complicates cascaded channel estimation. Conventional dictionary-based methods suffer from cumulative quantization errors and high complexity, especially for UPA systems.

Method: 1) Tensor modelization of NF cascaded channels exploiting tensor product among BS and IRS array response vectors; 2) Off-grid estimation framework based on sparse Tucker decomposition; 3) Formulated sparse core tensor minimization with tri-modal log-sum sparsity constraints; 4) Accelerated via HOSVD preprocessing, majorization-minimization, and tensor over-relaxation fast iterative shrinkage-thresholding.

Result: Achieves 13.6 dB improvement in normalized mean square error over benchmarks with significantly reduced runtime. Derived Cramér-Rao lower bound and conducted convergence analysis.

Conclusion: The proposed tensor-based off-grid framework effectively addresses near-field cascaded channel estimation challenges for XL-IRS, overcoming quantization errors and complexity issues of conventional methods while achieving superior performance.

Abstract: Accurate cascaded channel state information is pivotal for extremely large-scale intelligent reflecting surfaces (XL-IRS) in next-generation wireless networks. However, the large XL-IRS aperture induces spherical wavefront propagation due to near-field (NF) effects, complicating cascaded channel estimation. Conventional dictionary-based methods suffer from cumulative quantization errors and high complexity, especially in uniform planar array (UPA) systems. To address these issues, we first propose a tensor modelization method for NF cascaded channels by exploiting the tensor product among the horizontal and vertical response vectors of the UPA-structured base station (BS) and the incident-reflective array response vector of the IRS. This structure leverages spatial characteristics, enabling independent estimation of factor matrices to improve efficiency. Meanwhile, to avoid quantization errors, we propose an off-grid cascaded channel estimation framework based on sparse Tucker decomposition. Specifically, we model the received signal as a Tucker tensor, where the sparse core tensor captures path gain-delay terms and three factor matrices are spanned by BS and NF IRS array responses. We then formulate a sparse core tensor minimization problem with tri-modal log-sum sparsity constraints to tackle the NP-hard challenge. Finally, the method is accelerated via higher-order singular value decomposition preprocessing, combined with majorization-minimization and a tailored tensor over-relaxation fast iterative shrinkage-thresholding technique. We derive the Cramér-Rao lower bound and conduct convergence analysis. Simulations show the proposed scheme achieves a 13.6 dB improvement in normalized mean square error over benchmarks with significantly reduced runtime.

</details>


### [353] [Lightweight Range-Angle Imaging Based Algorithm for Quasi-Static Human Detection on Low-Cost FMCW Radar](https://arxiv.org/abs/2602.14001)
*Huy Trinh,George Shaker*

Main category: eess.SP

TL;DR: Lightweight 60 GHz radar method improves quasi-static human detection accuracy from ~68% with CFAR detectors to ~93% while achieving 120 FPS on Raspberry Pi.


<details>
  <summary>Details</summary>
Motivation: Quasi-static human activities produce low Doppler shifts and spread radar signatures that are hard to detect with conventional CFAR detectors. Privacy concerns and low lighting limit camera use in long-term care facilities.

Method: Proposes a lightweight, non-visual image-based method using low-cost 60 GHz FMCW radar with Range-Angle (RA) preprocessing pipeline. Benchmarked on Raspberry Pi 4B.

Result: Improved average detection accuracy from 68.3% (CA-CFAR) and 78.8% (OS-CFAR) to 93.24% for Subject 1, similar improvements for other subjects. Achieved 8.2 ms per frame (120 FPS), 74× speed-up over OS-CFAR.

Conclusion: Simple image-based processing provides robust, deployable quasi-static human sensing in cluttered indoor environments, addressing privacy and lighting limitations of cameras.

Abstract: Quasi-static human activities such as lying, standing or sitting produce very low Doppler shifts and highly spread radar signatures, making them difficult to detect with conventional constant-false-alarm rate (CFAR) detectors tuned for point targets. Moreover, privacy concerns and low lighting conditions limit the use of cameras in long-term care (LTC) facilities. This paper proposes a lightweight, non-visual image-based method for robust quasi-static human presence detection using a low-cost 60 GHz FMCW radar. On a dataset covering five semi-static activities, the proposed method improves average detection accuracy from 68.3% for Cell-Averaging CFAR (CA-CFAR) and 78.8% for Order-Statistics CFAR (OS-CFAR) to 93.24% for Subject 1, from 51.3%, 68.3% to 92.3% for Subject 2, and 57.72%, 69.94% to 94.82% for Subject 3, respectively. Finally, we benchmarked all three detectors across all activities on a Raspberry Pi 4B using a shared Range-Angle (RA) preprocessing pipeline. The proposed algorithm obtains an average 8.2 ms per frame, resulting in over 120 frames per second (FPS) and a 74 times speed-up over OS-CFAR. These results demonstrate that simple image-based processing can provide robust and deployable quasi-static human sensing in cluttered indoor environments.

</details>


### [354] [Rethinking RSSI for WiFi Sensing](https://arxiv.org/abs/2602.14004)
*Zhongqin Wang,J. Andrew Zhang,Kai Wu,Y. Jay Guo*

Main category: eess.SP

TL;DR: WiRSSI enables passive human tracking using only RSSI measurements from commodity WiFi, achieving sub-meter accuracy comparable to CSI-based methods.


<details>
  <summary>Details</summary>
Motivation: RSSI is widely available on commodity WiFi devices but considered too coarse for fine-grained sensing. The paper aims to revisit RSSI's sensing potential as a low-cost alternative to CSI-based WiFi sensing.

Method: WiRSSI uses a bistatic WiFi sensing framework with 1Tx-3Rx configuration. It extracts Doppler-AoA features via 2D FFT and infers delay from amplitude-only information, then maps AoA and delay to Cartesian coordinates with denoising to recover motion trajectories.

Result: WiRSSI achieves median XY localization errors of 0.905 m, 0.784 m, and 0.785 m for elliptical, linear, and rectangular trajectories respectively. This compares to CSI-based method errors of 0.574 m, 0.599 m, and 0.514 m, showing an average accuracy gap of only 0.26 m.

Conclusion: Despite lower resolution, RSSI can support practical passive sensing and offers a viable low-cost alternative to CSI-based WiFi sensing, making fine-grained sensing more accessible on commodity devices.

Abstract: The Received Signal Strength Indicator (RSSI) is widely available on commodity WiFi devices but is commonly regarded as too coarse for fine-grained sensing. This paper revisits its sensing potential and presents WiRSSI, a bistatic WiFi sensing framework for passive human tracking using only RSSI measurements. WiRSSI adopts a 1Tx-3Rx configuration and is readily extensible to Multiple-Input Multiple-Output (MIMO) deployments. We first reveal how CSI power implicitly encodes phase-related information and how this relationship carries over to RSSI, showing that RSSI preserves exploitable Doppler, Angle-of-Arrival (AoA), and delay cues associated with human motion. WiRSSI then extracts Doppler-AoA features via a 2D Fast Fourier Transform and infers delay from amplitude-only information in the absence of subcarrier-level phase. The estimated AoA and delay are then mapped to Cartesian coordinates and denoised to recover motion trajectories. Experiments in practical environments show that WiRSSI achieves median XY localization errors of 0.905 m, 0.784 m, and 0.785 m for elliptical, linear, and rectangular trajectories, respectively. In comparison, a representative CSI-based method attains median errors of 0.574 m, 0.599 m, and 0.514 m, corresponding to an average accuracy gap of 0.26 m. These results demonstrate that, despite its lower resolution, RSSI can support practical passive sensing and offers a low-cost alternative to CSI-based WiFi sensing.

</details>


### [355] [Extended Universal Joint Source-Channel Coding for Digital Semantic Communications: Improving Channel-Adaptability](https://arxiv.org/abs/2602.14018)
*Eunsoo Kim,Yoon Huh,Wan Choi*

Main category: eess.SP

TL;DR: euJSCC is an extended universal JSCC framework that achieves SNR- and modulation-adaptive transmission using hypernetwork-based normalization and dynamic codebook generation, outperforming existing methods in image transmission over both block fading and AWGN channels.


<details>
  <summary>Details</summary>
Motivation: Existing VQ-based JSCC methods use fixed, modulation-specific encoders, decoders, and codebooks, which limits their adaptability to fine-grained SNR variations in dynamic wireless environments.

Method: Proposes euJSCC with hypernetwork-based normalization for fine-grained feature vector normalization and dynamic codebook generation network that refines modulation-specific base codebooks according to block-wise SNR. Uses inner-outer encoder-decoder architecture for block fading channels and two-phase training (pretraining on AWGN then finetuning on block fading).

Result: euJSCC consistently outperforms state-of-the-art channel-adaptive digital JSCC schemes in image transmission under both block fading and AWGN channels.

Conclusion: The euJSCC framework successfully achieves SNR- and modulation-adaptive transmission within a single model, demonstrating superior performance over existing methods in dynamic wireless environments.

Abstract: Recent advances in deep learning (DL)-based joint source-channel coding (JSCC) have enabled efficient semantic communication in dynamic wireless environments. Among these approaches, vector quantization (VQ)-based JSCC effectively maps high-dimensional semantic feature vectors into compact codeword indices for digital modulation. However, existing methods, including universal JSCC (uJSCC), rely on fixed, modulation-specific encoders, decoders, and codebooks, limiting adaptability to fine-grained SNR variations. We propose an extended universal JSCC (euJSCC) framework that achieves SNR- and modulation-adaptive transmission within a single model. euJSCC employs a hypernetwork-based normalization layer for fine-grained feature vector normalization and a dynamic codebook generation (DCG) network that refines modulation-specific base codebooks according to block-wise SNR. To handle block fading channels, which consist of multiple coherence blocks, an inner-outer encoder-decoder architecture is adopted, where the outer encoder and decoder capture long-term channel statistics, and the inner encoder and decoder refine feature vectors to align with block-wise codebooks. A two-phase training strategy, i.e., pretraining on AWGN channels followed by finetuning on block fading channels, ensures stable convergence. Experiments on image transmission demonstrate that euJSCC consistently outperforms state-of-the-art channel-adaptive digital JSCC schemes under both block fading and AWGN channels.

</details>


### [356] [Convexity Meets Curvature: Lifted Near-Field Super-Resolution](https://arxiv.org/abs/2602.14063)
*Sajad Daei,Gábor Fodor,Mikael Skoglund*

Main category: eess.SP

TL;DR: Gridless superresolution framework for near-field ISAC systems using Bessel-Vandermonde factorization and atomic-norm minimization to jointly recover continuous angles and ranges from undersampled hybrid measurements.


<details>
  <summary>Details</summary>
Motivation: Next-generation wireless systems with extra-large apertures, high carrier frequencies, and ISAC requirements operate in Fresnel region where spherical wavefronts break classical Fourier/Vandermonde structure. Existing near-field approaches rely on costly 2D gridding and struggle with hybrid front-ends providing limited pilot measurements.

Method: Proposes Bessel-Vandermonde factorization of Fresnel-phase manifold to expose hidden Vandermonde structure in angle while isolating range dependence. Uses lifting to map each range bin and continuous angle to structured rank-one atoms, converting nonlinear near-field model into linear inverse problem over row-sparse matrix. Solves via atomic-norm minimization with explicit dual characterization via bounded trigonometric polynomials.

Result: Simulations with strongly undersampled hybrid observations validate reliable joint angle-range recovery. Framework enables super-resolution of off-grid angles and identification of active range bins without costly 2D gridding.

Conclusion: Convex optimization meets near-field curvature through gridless superresolution framework, enabling practical joint angle-range inference for next-generation wireless and ISAC systems with limited pilot measurements.

Abstract: Extra-large apertures, high carrier frequencies, and integrated sensing and communications (ISAC) are pushing array processing into the Fresnel region, where spherical wavefronts induce a range-dependent phase across the aperture. This curvature breaks the Fourier/Vandermonde structure behind classical subspace methods, and it is especially limiting with hybrid front-ends that provide only a small number of pilot measurements. Consequently, practical systems need continuous angle resolution and joint angle-range inference where many near-field approaches still rely on costly 2D gridding. We show that convexity can meet curvature via a lifted, gridless superresolution framework for near-field measurements. The key is a Bessel-Vandermonde factorization of the Fresnel-phase manifold that exposes a hidden Vandermonde structure in angle while isolating the range dependence into a compact coefficient map. Building on this, we introduce a lifting that maps each range bin and continuous angle to a structured rank-one atom, converting the nonlinear near-field model into a linear inverse problem over a row-sparse matrix. Recovery is posed as atomic-norm minimization and an explicit dual characterization via bounded trigonometric polynomials yields certificate-based localization that super-resolves off-grid angles and identifies active range bins. Simulations with strongly undersampled hybrid observations validate reliable joint angle-range recovery for next-generation wireless and ISAC systems.

</details>


### [357] [Wireless Physical Neural Networks (WPNNs): Opportunities and Challenges](https://arxiv.org/abs/2602.14094)
*Meng Hua,Itsik Bergel,Tolga Girici,Marco Di Renzo,Deniz Gunduz*

Main category: eess.SP

TL;DR: The paper introduces Wireless Physical Neural Networks (WPNNs), treating wireless network components as computational layers in a learning architecture, enabling joint communication-computation optimization through differentiable physical operators.


<details>
  <summary>Details</summary>
Motivation: Wireless communication systems share structural similarities with neural networks - signals propagate through cascaded elements, interact with environments, and undergo transformations. This observation motivates rethinking wireless networks as computational learning architectures rather than just communication systems.

Method: The authors propose treating wireless propagation environments and network elements (transceivers, relays, backscatter, intelligent surfaces) as differentiable operators within a unified learning architecture. This enables joint optimization through learning-based methods applied directly to the physical network, either independently or combined with conventional digital neural layers.

Result: Numerical examples demonstrate potential performance gains in processing, adaptability, efficiency, and end-to-end optimization. The approach shows promise for reconfiguring wireless systems as learning networks in next-generation communication frameworks.

Conclusion: Wireless Physical Neural Networks represent a paradigm shift where wireless networks become computational resources, enabling hybrid communication-learning pipelines and opening new opportunities for joint communication-computation designs in future wireless systems.

Abstract: Wireless communication systems exhibit structural and functional similarities to neural networks: signals propagate through cascaded elements, interact with the environment, and undergo transformations. Building upon this perspective, we introduce a unified paradigm, termed \textit{wireless physical neural networks (WPNNs)}, in which components of a wireless network, such as transceivers, relays, backscatter, and intelligent surfaces, are interpreted as computational layers within a learning architecture. By treating the wireless propagation environment and network elements as differentiable operators, new opportunities arise for joint communication-computation designs, where system optimization can be achieved through learning-based methods applied directly to the physical network. This approach may operate independently of, or in conjunction with, conventional digital neural layers, enabling hybrid communication learning pipelines. In the article, we outline representative architectures that embody this viewpoint and discuss the algorithmic and training considerations required to leverage the wireless medium as a computational resource. Through numerical examples, we highlight the potential performance gains in processing, adaptability, efficiency, and end-to-end optimization, demonstrating the promise of reconfiguring wireless systems as learning networks in next-generation communication frameworks.

</details>


### [358] [Electromagnetic Bounds on Realizing Targeted MIMO Transfer Functions in Real-World Systems with Wave-Domain Programmability](https://arxiv.org/abs/2602.14152)
*Philipp del Hougne*

Main category: eess.SP

TL;DR: The paper derives electromagnetically consistent bounds for how accurately desired linear operators can be realized in reconfigurable wave systems, considering mutual coupling and hardware constraints.


<details>
  <summary>Details</summary>
Motivation: There are no existing electromagnetically consistent bounds for the fidelity with which desired linear operators can be realized in real-world reconfigurable wave systems, despite the relevance for applications like hybrid-MIMO analog combiners, computational meta-imagers, and programmable wave-domain signal processing.

Method: The authors use an electromagnetically consistent multiport-network model that captures mutual coupling between tunable elements and accounts for real-world hardware constraints (lossy, 1-bit-programmable elements). They formulate operator synthesis as a quadratically constrained fractional-quadratic problem and compute rigorous fidelity upper bounds using semidefinite relaxation.

Result: The technique was applied to three experimental setups: two RIS-based 4×4 MIMO channels (free-space and rich-scattering at 2.45 GHz with 100 1-bit elements) and one DMA-based 4×4 MIMO channel at 19 GHz. Results show strong influence of coupling strength on fidelity bounds, and for RIS-based setups, the bounds indicate insufficient wave-domain flexibility for operator synthesis.

Conclusion: The paper provides the first electromagnetically consistent bounds for reconfigurable wave systems, revealing fundamental limitations in operator synthesis fidelity and demonstrating the critical impact of mutual coupling between tunable elements.

Abstract: A key question for most applications involving reconfigurable linear wave systems is how accurately a desired linear operator can be realized by configuring the system's tunable elements. The relevance of this question spans from hybrid-MIMO analog combiners via computational meta-imagers to programmable wave-domain signal processing. Yet, no electromagnetically consistent bounds have been derived for the fidelity with which a desired operator can be realized in a real-world reconfigurable wave system. Here, we derive such bounds based on an electromagnetically consistent multiport-network model (capturing mutual coupling between tunable elements) and accounting for real-world hardware constraints (lossy, 1-bit-programmable elements). Specifically, we formulate the operator-synthesis task as a quadratically constrained fractional-quadratic problem and compute rigorous fidelity upper bounds based on semidefinite relaxation. We apply our technique to three distinct experimental setups. The first two setups are, respectively, a free-space and a rich-scattering $4\times 4$ MIMO channel at 2.45 GHz parameterized by a reconfigurable intelligent surface (RIS) comprising 100 1-bit-programmable elements. The third setup is a $4\times 4$ MIMO channel at 19 GHz from four feeds of a dynamic metasurface antenna (DMA) to four users. We systematically study how the achievable fidelity scales with the number of tunable elements, and we probe the tightness of our bounds by trying to find optimized configurations approaching the bounds with standard discrete-optimization techniques. We observe a strong influence of the coupling strength between tunable elements on our fidelity bound. For the two RIS-based setups, our bound attests to insufficient wave-domain flexibility for the considered operator synthesis.

</details>


### [359] [Explainable Interictal Epileptiform Discharge Detection Method Based on Scalp EEG and Retrieval-Augmented Generation](https://arxiv.org/abs/2602.14170)
*Yu Zhu,Jiayang Guo,Jun Jiang,Peipei Gu,Xin Shu,Duo Chen*

Main category: eess.SP

TL;DR: IED-RAG: An explainable multimodal framework for joint epileptiform discharge detection and report generation using retrieval-augmented generation with explicit evidence from EEG-text pairs.


<details>
  <summary>Details</summary>
Motivation: Automated IED detection methods lack interpretability, which is crucial for clinical diagnosis of epilepsy. There's a need for explainable systems that can both detect epileptiform discharges and generate transparent clinical reports.

Method: Uses dual-encoder to extract EEG and semantic features, aligned via contrastive learning in shared embedding space. Retrieves clinically relevant EEG-text pairs as explicit evidence from vector database to condition LLM for evidence-based report generation.

Result: Achieved 89.17% balanced accuracy on private dataset and 71.38% on public TUEV dataset. BLEU scores of 89.61% and 64.14% respectively. Outperforms standard black-box methods in both diagnostic performance and clinical interpretability.

Conclusion: Retrieval of explicit evidence enhances both diagnostic accuracy and clinical interpretability, providing a transparent framework for joint IED detection and report generation that addresses the interpretability limitations of existing automated methods.

Abstract: The detection of interictal epileptiform discharge (IED) is crucial for the diagnosis of epilepsy, but automated methods often lack interpretability. This study proposes IED-RAG, an explainable multimodal framework for joint IED detection and report generation. Our approach employs a dual-encoder to extract electrophysiological and semantic features, aligned via contrastive learning in a shared EEG-text embedding space. During inference, clinically relevant EEG-text pairs are retrieved from a vector database as explicit evidence to condition a large language model (LLM) for the generation of evidence-based reports. Evaluated on a private dataset from Wuhan Children's Hospital and the public TUH EEG Events Corpus (TUEV), the framework achieved balanced accuracies of 89.17\% and 71.38\%, with BLEU scores of 89.61\% and 64.14\%, respectively. The results demonstrate that retrieval of explicit evidence enhances both diagnostic performance and clinical interpretability compared to standard black-box methods.

</details>


### [360] [Reconfigurable Intelligent Surfaces-assisted Positioning in Integrated Sensing and Communication Systems](https://arxiv.org/abs/2602.14415)
*Huyen-Trang Ta,Ngoc-Son Duong,Trung-Hieu Nguyen,Van-Linh Nguyen,Thai-Mai Dinh*

Main category: eess.SP

TL;DR: Proposes a fast iterative refinement algorithm for high-precision target localization in RIS-aided ISAC systems, achieving comparable accuracy to conventional methods with significantly reduced complexity.


<details>
  <summary>Details</summary>
Motivation: Need for efficient high-precision target localization in integrated sensing and communication systems using both direct and RIS-assisted reflection paths, where conventional methods have high computational complexity.

Method: 1) Sequential matched-filter estimator for coarse angular parameters, 2) Range recovery via subcarrier phase differences, 3) Formulate as non-linear least squares optimization, 4) Fast iterative refinement exploiting separable least-squares structure, 5) Modified Levenberg algorithm with approximation strategy for low-cost updates.

Result: Simulation results show the proposed refinement method achieves accuracy comparable to conventional approaches while significantly reducing algorithmic complexity.

Conclusion: The proposed fast iterative refinement algorithm enables efficient high-precision target localization in RIS-aided ISAC systems by exploiting separable parameter structure and using approximation strategies to reduce computational burden without sacrificing accuracy.

Abstract: This paper investigates the problem of high-precision target localization in integrated sensing and communication (ISAC) systems, where the target is sensed via both a direct path and a reconfigurable intelligent surface (RIS)-assisted reflection path. We first develop a sequential matched-filter estimator to acquire coarse angular parameters, followed by a range recovery process based on subcarrier phase differences. Subsequently, we formulate the target localization problem as a non-linear least squares optimization, using the coarse estimates to initialize the target's position coordinates. To solve this efficiently, we introduce a fast iterative refinement algorithm tailored for RIS-aided ISAC environments. Recognizing that the signal model involves both linear path gains and non-linear geometric dependencies, we exploit the separable least-squares structure to decouple these parameters. Furthermore, we propose a modified Levenberg algorithm with an approximation strategy, which enables low-cost parameter updates without necessitating repeated evaluations of the full non-linear model. Simulation results show that the proposed refinement method achieves accuracy comparable to conventional approaches, while significantly reducing algorithmic complexity.

</details>


### [361] [Localization Exploiting Spatial Variations in the Magnetic Field: Principles and Challenges](https://arxiv.org/abs/2602.14181)
*Isaac Skog,Manon Kok,Christophe Prieur,Gustaf Hendeby*

Main category: eess.SP

TL;DR: Overview of signal processing principles and research challenges in magnetic field-based localization, highlighting decimeter-level indoor accuracy and outdoor performance comparable to strategic-grade INS.


<details>
  <summary>Details</summary>
Motivation: Signal processing is fundamental to modern localization technologies, including magnetic field-based localization which offers high accuracy without infrastructure. The paper aims to provide a comprehensive overview of current principles and open challenges from a statistical signal-processing perspective.

Method: Presents existing key technologies within a common parametric signal-model framework compatible with established statistical inference methods. Uses a signal-processing approach to analyze magnetic field variations for localization.

Result: Contemporary magnetic field-based localization achieves decimeter-level indoor accuracy and outdoor accuracy comparable to strategic-grade inertial navigation systems. The framework enables understanding of similarities and differences among existing technologies.

Conclusion: Signal processing plays a crucial role in magnetic field-based localization, enabling high-accuracy positioning through statistical inference, modeling, and calibration. The paper provides a unified framework for understanding current technologies and identifying future research directions.

Abstract: Signal processing has played, and continues to play, a fundamental role in the evolution of modern localization technologies. Localization using spatial variations in the Earth's magnetic field is no exception. It relies on signal-processing methods for statistical state inference, magnetic-field modeling, and sensor calibration. Contemporary localization techniques based on spatial variations in the magnetic field can provide decimeter-level indoor localization accuracy and outdoor localization accuracy on par with strategic-grade inertial navigation systems. This article provides a broad, high-level overview of current signal-processing principles and open research challenges in localization using spatial variations in the Earth's magnetic field. The aim is to provide the reader with an understanding of the similarities and differences among existing key technologies from a statistical signal-processing perspective. To that end, existing key technologies will be presented within a common parametric signal-model framework compatible with well-established statistical inference methods.

</details>


### [362] [Robust SAC-Enabled UAV-RIS Assisted Secure MISO Systems With Untrusted EH Receivers](https://arxiv.org/abs/2602.14191)
*Hamid Reza Hashempour,Le-Nam Tran,Duy H. N. Nguyen,Hien Quoc Ngo*

Main category: eess.SP

TL;DR: Secure UAV-RIS system with untrusted energy-harvesting receivers uses deep reinforcement learning to maximize worst-case secrecy energy efficiency under imperfect CSI.


<details>
  <summary>Details</summary>
Motivation: Need secure downlink transmission in UAV-assisted RIS networks where energy-harvesting receivers can also eavesdrop, requiring robust optimization under imperfect CSI and practical constraints.

Method: Propose soft actor-critic (SAC) deep reinforcement learning framework to jointly optimize UAV position, RIS phase shifts, and power allocation. Also develop successive convex approximation (SCA) benchmark for perfect CSI case.

Result: SAC achieves 28% and 16% secrecy energy efficiency gains over SCA and DDPG baselines, with superior robustness to CSI uncertainty and stable performance across varying conditions.

Conclusion: SAC-based DRL effectively solves complex nonconvex secure transmission problems in UAV-RIS networks with untrusted receivers, outperforming conventional optimization methods.

Abstract: This paper investigates secure downlink transmission in a UAV-assisted reconfigurable intelligent surface (RIS)-enabled multiuser multiple-input single-output network, where legitimate information-harvesting receivers coexist with untrusted energy-harvesting receivers (UEHRs) capable of eavesdropping. A UAV-mounted RIS provides blockage mitigation and passive beamforming, while the base station employs zero-forcing precoding for multiuser interference suppression. Due to limited feedback from UEHRs, their channel state information (CSI) is imperfect, leading to a worst-case secrecy energy efficiency (WCSEE) maximization problem. We jointly optimize the UAV horizontal position, RIS phase shifts, and transmit power allocation under both perfect and imperfect CSI, considering discrete RIS phases, UAV mobility, and energy-harvesting constraints. The resulting problem is highly nonconvex due to coupled channel geometry, robustness requirements, and discrete variables. To address this challenge, we propose a soft actor-critic (SAC)-based deep reinforcement learning framework that learns WCSEE-maximizing policies through interaction with the wireless environment. As a structured benchmark, a successive convex approximation (SCA) approach is developed for the perfect CSI case with continuous RIS phases. Simulation results show that the proposed SAC method achieves up to 28% and 16% secrecy energy efficiency gains over SCA and deep deterministic policy gradient baselines, respectively, while demonstrating superior robustness to CSI uncertainty and stable performance across varying transmit power levels and RIS sizes.

</details>


### [363] [Low-Cost Physical-Layer Security Design for IRS-Assisted mMIMO Systems with One-Bit DACs](https://arxiv.org/abs/2602.14292)
*Weijie Xiong,Jian Yang,Jingran Lin,Hongli Liu,Zhiling Xiao,Qiang Li*

Main category: eess.SP

TL;DR: One-bit DACs enable cost-effective physical-layer security in IRS-assisted mMIMO systems by jointly optimizing quantized precoding and IRS phase shifts to maximize secrecy rates.


<details>
  <summary>Details</summary>
Motivation: Traditional mMIMO systems with IRS require high-resolution quantizers, leading to substantial hardware complexity and cost. There's a need for cost-effective PLS solutions that maintain security performance while reducing hardware requirements.

Method: Two algorithms: (1) WMMSE-PDD reformulates secrecy rate maximization into non-fractional programs using WMMSE method and solves via penalty dual decomposition; (2) EPPRGD transforms the problem into unconstrained optimization over product Riemannian manifold for faster convergence.

Result: Both algorithms provide analytical solutions at each iteration and converge to KKT points. WMMSE-PDD achieves superior secrecy performance, while EPPRGD enables faster convergence with slight trade-off in secrecy performance.

Conclusion: One-bit DACs offer a viable cost-effective solution for PLS in IRS-assisted mMIMO systems. The proposed algorithms effectively balance hardware complexity reduction with security performance, providing practical implementation options.

Abstract: Integrating massive multiple-input multiple-output (mMIMO) systems with intelligent reflecting surfaces (IRS) presents a promising paradigm for enhancing physical-layer security (PLS) in wireless communications. However, deploying high-resolution quantizers in large-scale mMIMO arrays, along with numerous IRS elements, leads to substantial hardware complexity. To address these challenges, this paper proposes a cost-effective PLS design for IRS-assisted mMIMO systems by employing one-bit digital-to-analog converters (DACs). The focus is on jointly optimizing one-bit quantized precoding at the transmitter and constant-modulus phase shifts at the IRS to maximize the secrecy rate. This leads to a highly non-convex fractional secrecy rate maximization (SRM) problem. To efficiently solve this problem, two algorithms are proposed: (1) the WMMSE-PDD algorithm, which reformulates the SRM problem into a sequence of non-fractional programs with auxiliary variables using the weighted minimum mean-square error (WMMSE) method and solves them via the penalty dual decomposition (PDD) approach, achieving superior secrecy performance; and (2) the exact penalty product Riemannian gradient descent (EPPRGD) algorithm, which transforms the SRM problem into an unconstrained optimization over a product Riemannian manifold, eliminating auxiliary variables and enabling faster convergence with a slight trade-off in secrecy performance. Both algorithms provide analytical solutions at each iteration and are proven to converge to Karush-Kuhn-Tucker (KKT) points. Simulation results confirm the effectiveness of the proposed methods and highlight their respective advantages.

</details>


### [364] [Online Architecture Search for Compressed Sensing based on Hypergradient Descent](https://arxiv.org/abs/2602.14411)
*Ayano Nakai-Kasai,Yusuke Nakane,Tadashi Wadayama*

Main category: eess.SP

TL;DR: Proposed HGD-AS-ISTA and HGD-AS-FISTA algorithms use hypergradient descent for online optimization of structural parameters, eliminating need for training data and avoiding retraining when environment changes.


<details>
  <summary>Details</summary>
Motivation: Existing AS-ISTA/AS-FISTA methods require training data and have large training overhead, and need retraining when environment changes. Need for online parameter optimization without training data.

Method: Proposed HGD-AS-ISTA and HGD-AS-FISTA use hypergradient descent (online hyperparameter optimization) to determine structural parameters instead of deep unfolding training.

Result: Experimental results show improved performance over conventional ISTA/FISTA while avoiding need for retraining when environment changes.

Conclusion: Hypergradient descent enables online optimization of structural parameters in compressed sensing algorithms, eliminating training requirements and improving adaptability to changing environments.

Abstract: AS-ISTA (Architecture Searched-Iterative Shrinkage Thresholding Algorithm) and AS-FISTA (AS-Fast ISTA) are compressed sensing algorithms introducing structural parameters to ISTA and FISTA to enable architecture search within the iterative process. The structural parameters are determined using deep unfolding, but this approach requires training data and the large overhead of training time. In this paper, we propose HGD-AS-ISTA (Hypergradient Descent-AS-ISTA) and HGD-AS-FISTA that use hypergradient descent, which is an online hyperparameter optimization method, to determine the structural parameters. Experimental results show that the proposed method improves performance of the conventional ISTA/FISTA while avoiding the need for re-training when the environment changes.

</details>


### [365] [Cramer--Rao Bounds for Magneto-Inductive Integrated Sensing and Communications](https://arxiv.org/abs/2602.14453)
*Haofan Dong,Ozgur B. Akan*

Main category: eess.SP

TL;DR: Derives closed-form Cramér-Rao bound for joint estimation of range and medium conductivity using magnetic induction in ISAC framework, showing conductivity sensing adds at most 3dB penalty to ranging precision.


<details>
  <summary>Details</summary>
Motivation: Magnetic induction enables communication in RF-denied environments (underground, underwater, in-body) where medium conductivity affects the channel. There's a need to understand fundamental limits for joint sensing of range and conductivity in ISAC systems.

Method: Derives closed-form Cramér-Rao bound (CRB) for joint estimation of range and medium conductivity from MI pilot observations. Analyzes Fisher information matrix to quantify estimation penalty. Validates with Monte Carlo maximum-likelihood simulations.

Result: The joint estimation penalty converges to 3dB in near-field regime, meaning conductivity sensing adds at most a factor-of-two loss in ranging precision. Monte Carlo simulations confirm CRB is achievable under practical conditions.

Conclusion: Joint estimation of range and conductivity via magnetic induction is feasible with bounded performance penalty, enabling integrated sensing and communication in RF-denied environments with minimal impact on ranging accuracy.

Abstract: Magnetic induction (MI) enables communication in RF-denied environments (underground, underwater, in-body), where the medium conductivity imprints a deterministic signature on the channel. This letter derives a closed-form Cramér--Rao bound (CRB) for the joint estimation of range and medium conductivity from MI pilot observations in an integrated sensing and communication (ISAC) framework. The Fisher information matrix reveals that the joint estimation penalty converges to 3\,dB in the near-field regime, meaning conductivity sensing adds at most a factor-of-two loss in ranging precision. Monte Carlo maximum-likelihood simulations confirm that the CRB is achievable under practical operating conditions.

</details>


### [366] [All-pole centroids in the Wasserstein metric with applications to clustering of spectral densities](https://arxiv.org/abs/2602.14583)
*Rumeshika Pallewela,Filip Elvander*

Main category: eess.SP

TL;DR: Proposes computing spectral Wasserstein-2 barycenters restricted to all-pole spectra for compact, interpretable autoregressive representatives of Gaussian processes.


<details>
  <summary>Details</summary>
Motivation: Existing Wasserstein barycenters for spectral estimation are non-parametric with storage complexity depending on discretization grids. Need compact, low-dimensional, interpretable spectral centroids for downstream tasks.

Method: Restrict barycenters to all-pole spectra with fixed model order, solving non-convex optimization via gradient descent in model parameters. Quantifies sub-optimality of obtained centroids.

Result: Method yields compact, interpretable spectral centroids that can represent autoregressive processes. Demonstrated on phoneme classification problem.

Conclusion: Proposed method provides low-dimensional spectral barycenters for autoregressive modeling, offering interpretable centroids for spectral analysis tasks despite non-convex optimization challenges.

Abstract: In this work, we propose a method for computing centroids, or barycenters, in the spectral Wasserstein-2 metric for sets of power spectral densities, where the barycenters are restricted to belong to the set of all-pole spectra with a certain model order. This may be interpreted as finding an autoregressive representative for sets of second-order stationary Gaussian processes. While Wasserstein, or optimal transport, barycenters have been successfully used earlier in problems of spectral estimation and clustering, the resulting barycenters are non-parametric and the complexity of representing and storing them depends on, e.g., the choice of discretization grid. In contrast, the herein proposed method yields compact, low-dimensional, and interpretable spectral centroids that can be used in downstream tasks. Computing the all-pole centroids corresponds to solving a non-convex optimization problem in the model parameters, and we present a gradient descent scheme for addressing this. Although convergence to a globally optimal point cannot be guaranteed, the sub-optimality of the obtained centroids can be quantified. The proposed method is illustrated on a problem of phoneme classification.

</details>


### [367] [Learning Dirac Spectral Transforms for Topological Signals](https://arxiv.org/abs/2602.14590)
*Leonardo Di Nino,Tiziana Cattai,Sergio Barbarossa,Ginestra Bianconi,Paolo Di Lorenzo*

Main category: eess.SP

TL;DR: Comparison of Dirac vs Hodge-Laplacian operators for graph signal processing, showing Dirac's cross-domain advantages and proposing a parameterized transform with learnable mass parameters that achieves optimal distortion-sparsity tradeoff.


<details>
  <summary>Details</summary>
Motivation: The Dirac operator provides a unified framework for processing signals across different topological domains (nodes and edges) with eigenmodes that capture cross-domain interactions, unlike conventional Hodge-Laplacian eigenmodes that operate within single dimensions.

Method: 1) Compare Dirac and Hodge-Laplacian operators in distortion/sparsity trade-off; 2) Show overcomplete basis combining both dictionaries improves performance; 3) Propose parameterized nonredundant transform with mode-specific mass parameters capturing node-edge interplay; 4) Learn mass parameters from data.

Result: The learned parameterized transform achieves the best distortion-sparsity tradeoff compared to both complete (individual Dirac/Hodge-Laplacian) and overcomplete (combined) bases.

Conclusion: Learning mass parameters in the proposed Dirac-based transform enables optimal cross-domain signal representation, outperforming both conventional approaches and their combination through better adaptation to data structure.

Abstract: The Dirac operator provides a unified framework for processing signals defined over different order topological domains, such as node and edge signals. Its eigenmodes define a spectral representation that inherently captures cross-domain interactions, in contrast to conventional Hodge-Laplacian eigenmodes that operate within a single topological dimension. In this paper, we compare the two alternatives in terms of the distortion/sparsity trade-off and we show how an overcomplete basis built concatenating the two dictionaries can provide better performance with respect to each approach. Then, we propose a parameterized nonredundant transform whose eigenmodes incorporate a mode-specific mass parameter that captures the interplay between node and edge modes. Interestingly, we show that learning the mass parameters from data makes the proposed transform able to achieve the best distortion-sparsity tradeoff with respect to both complete and overcomplete bases.

</details>


### [368] [Synthetic Aperture Communication: Principles and Application to Massive IoT Satellite Uplink](https://arxiv.org/abs/2602.14629)
*Lucas Giroto,Marcus Henninger,Silvio Mandelli*

Main category: eess.SP

TL;DR: Synthetic Aperture Communication (SAC) enables precise satellite-to-device uplink with spatial signal separation and localization, achieving low power transmission (-10 dBm) and interference mitigation for IoT connectivity.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored potential of coherent synthetic aperture techniques for communication, specifically for direct satellite-to-device uplink in non-terrestrial networks, enabling massive IoT connectivity with spatial signal separation and overcoming power limitations.

Method: Proposes Synthetic Aperture Communication (SAC) principles for satellite-to-device uplink, using LEO satellite motion to create synthetic aperture for precise DoA estimation, spatial signal separation, and localization. Simulated with 600 km orbit satellite, two UE devices using OFDM with polar coding at 3.5 GHz.

Result: Achieves block error rates below 0.1 with transmission powers as low as -10 dBm, even under strong interference when UE devices are resolved but fall on each other's strongest angular sidelobe, demonstrating effective interference mitigation and power efficiency.

Conclusion: SAC successfully addresses mutual interference and stringent power limitations in satellite-to-device communication, validating its potential for enabling massive IoT connectivity in non-terrestrial networks.

Abstract: While synthetic aperture radar is widely adopted to provide high-resolution imaging at long distances using small arrays, the concept of coherent synthetic aperture communication (SAC) has not yet been explored. This article introduces the principles of SAC for direct satellite-to-device uplink, showcasing precise direction-of-arrival estimation for user equipment (UE) devices, facilitating spatial signal separation, localization, and easing link budget constraints. Simulations for a low Earth orbit satellite at 600 km orbit and two UE devices performing orthogonal frequency-division multiplexing-based transmission with polar coding at 3.5 GHz demonstrate block error rates below 0.1 with transmission powers as low as -10 dBm, even under strong interference when UE devices are resolved but fall on each other's strongest angular sidelobe. These results validate the ability of the proposed scheme to address mutual interference and stringent power limitations, paving the way for massive Internet of Things connectivity in non-terrestrial networks.

</details>


### [369] [RF-GPT: Teaching AI to See the Wireless World](https://arxiv.org/abs/2602.14833)
*Hang Zou,Yu Tian,Bohao Wang,Lina Bariah,Samson Lasaulce,Chongwen Huang,Mérouane Debbah*

Main category: eess.SP

TL;DR: RF-GPT bridges the gap between RF signal processing and high-level reasoning by adapting multimodal LLMs to understand radio-frequency spectrograms, achieving strong performance on various wireless tasks without manual labeling.


<details>
  <summary>Details</summary>
Motivation: Current LLMs and multimodal models don't natively support RF signals, creating a gap between low-level RF perception and high-level reasoning. Existing approaches either focus on text/structured data or build separate models for specific RF tasks.

Method: RF-GPT maps complex IQ waveforms to time-frequency spectrograms, processes them through pretrained visual encoders from multimodal LLMs, injects RF tokens into a decoder-only LLM, and performs supervised instruction fine-tuning using a fully synthetic RF corpus generated from standards-compliant waveform generators.

Result: RF-GPT achieves strong multi-task performance across benchmarks for wideband modulation classification, overlap analysis, wireless-technology recognition, WLAN user counting, and 5G NR information extraction, while general-purpose VLMs without RF grounding largely fail.

Conclusion: RF-GPT successfully bridges the RF perception-reasoning gap by adapting multimodal LLMs to understand RF spectrograms, enabling unified RF reasoning without manual labeling through synthetic data generation.

Abstract: Large language models (LLMs) and multimodal models have become powerful general-purpose reasoning systems. However, radio-frequency (RF) signals, which underpin wireless systems, are still not natively supported by these models. Existing LLM-based approaches for telecom focus mainly on text and structured data, while conventional RF deep-learning models are built separately for specific signal-processing tasks, highlighting a clear gap between RF perception and high-level reasoning. To bridge this gap, we introduce RF-GPT, a radio-frequency language model (RFLM) that utilizes the visual encoders of multimodal LLMs to process and understand RF spectrograms. In this framework, complex in-phase/quadrature (IQ) waveforms are mapped to time-frequency spectrograms and then passed to pretrained visual encoders. The resulting representations are injected as RF tokens into a decoder-only LLM, which generates RF-grounded answers, explanations, and structured outputs. To train RF-GPT, we perform supervised instruction fine-tuning of a pretrained multimodal LLM using a fully synthetic RF corpus. Standards-compliant waveform generators produce wideband scenes for six wireless technologies, from which we derive time-frequency spectrograms, exact configuration metadata, and dense captions. A text-only LLM then converts these captions into RF-grounded instruction-answer pairs, yielding roughly 12,000 RF scenes and 0.625 million instruction examples without any manual labeling. Across benchmarks for wideband modulation classification, overlap analysis, wireless-technology recognition, WLAN user counting, and 5G NR information extraction, RF-GPT achieves strong multi-task performance, whereas general-purpose VLMs with no RF grounding largely fail.

</details>


### [370] [Lattice XBAR Filters in Thin-Film Lithium Niobate](https://arxiv.org/abs/2602.14937)
*Taran Anusorn,Byeongjin Kim,Ian Anderson,Ziqian Yao,Ruochen Lu*

Main category: eess.SP

TL;DR: Demonstration of wideband, low-loss lattice filters using XBARs in P3F TFLN achieving 27-39% FBW and <1 dB IL at ~20 GHz with compact footprints <1.3 mm².


<details>
  <summary>Details</summary>
Motivation: To develop compact, high-performance RF front-end filters for next-generation wireless communication and sensing systems by leveraging the strong electromechanical coupling of XBARs in P3F TFLN and the wideband nature of lattice topologies.

Method: Designed and fabricated two lattice filter implementations (direct lattice and layout-balanced lattice topologies) using laterally excited bulk acoustic resonators (XBARs) in periodically poled piezoelectric film (P3F) thin-film lithium niobate (TFLN).

Result: Achieved 3-dB fractional bandwidths of 27.42% and 39.11% with low insertion losses of 0.88 dB and 0.96 dB at approximately 20 GHz for direct and layout-balanced lattice filters respectively, all with compact footprints smaller than 1.3 mm².

Conclusion: XBAR-based lattice architectures show strong potential for enabling low-loss, wideband acoustic filters for compact RF front ends in next-generation systems, while also identifying key challenges and directions for further optimization.

Abstract: This work presents the demonstration of lattice filters based on laterally excited bulk acoustic resonators (XBARs). Two filter implementations, namely direct lattice and layout-balanced lattice topologies, are designed and fabricated in periodically poled piezoelectric film (P3F) thin-film lithium niobate (TFLN). By leveraging the strong electromechanical coupling of XBARs in P3F TFLN together with the inherently wideband nature of the lattice topology, 3-dB fractional bandwidths (FBWs) of 27.42\% and 39.11\% and low insertion losses (ILs) of 0.88 dB and 0.96 dB are achieved at approximately 20 GHz for the direct and layout-balanced lattice filters, respectively, under conjugate matching. Notably, all prototypes feature compact footprints smaller than 1.3 mm\textsuperscript{2}. These results highlight the potential of XBAR-based lattice architectures to enable low-loss, wideband acoustic filters for compact, high-performance RF front ends in next-generation wireless communication and sensing systems, while also identifying key challenges and directions for further optimization.

</details>


### [371] [Real-time Range-Angle Estimation and Tag Localization for Multi-static Backscatter Systems](https://arxiv.org/abs/2602.14985)
*Tara Esmaeilbeig,Kartik Patel,Traian E. Abrudan,John Kimionis,Eleftherios Kampianakis,Michael S. Eggleston*

Main category: eess.SP

TL;DR: Low-complexity algorithms for real-time localization in multi-static backscatter networks achieve 3m median error with 40-500x speedup over baselines.


<details>
  <summary>Details</summary>
Motivation: Need for efficient real-time localization algorithms for large-scale multi-static backscatter networks in 6G ambient IoT, where thousands of devices require low-complexity range/AoA estimation and fusion.

Method: Proposed two low-complexity algorithms: JRAC (Joint Range-Angle Clustering) and SRAE (Stage-wise Range-Angle Estimation) for range/AoA estimation, plus two localization algorithms: ML gradient search and IRLS (Iterative Re-weighted Least Squares) for fusing estimates.

Result: JRAC/SRAE reduce runtime by 40x vs FFT/subspace baselines; IRLS achieves 500x reduction over ML brute force search. Achieves 3m median localization error with 4 illuminators, 1 receiver, and 100 tags in sub-6GHz 40MHz bandwidth.

Conclusion: Proposed algorithms make real-time, scalable backscatter localization practical for next-generation ambient IoT networks by maintaining accuracy while drastically reducing computational complexity.

Abstract: Multi-static backscatter networks (BNs) are strong candidates for joint communication and localization in the ambient IoT paradigm for 6G. Enabling real-time localization in large-scale multi-static deployments with thousands of devices require highly efficient algorithms for estimating key parameters such as range and angle of arrival (AoA), and for fusing these parameters into location estimates. We propose two low-complexity algorithms, Joint Range-Angle Clustering (JRAC) and Stage-wise Range-Angle Estimation (SRAE). Both deliver range and angle estimation accuracy comparable to FFT- and subspace-based baselines while significantly reducing the computation. We then introduce two real-time localization algorithms that fuse the estimated ranges and AoAs: a maximum-likelihood (ML) method solved via gradient search and an iterative re-weighted least squares (IRLS) method. Both achieve localization accuracy comparable to ML-based brute force search albeit with far lower complexity. Experiments on a real-world large-scale multi-static testbed with 4 illuminators, 1 multi-antenna receiver, and 100 tags show that JRAC and SRAE reduce runtime by up to 40X and IRLS achieves up to 500X reduction over ML-based brute force search without degrading localization accuracy. The proposed methods achieve 3 m median localization error across all 100 tags in a sub-6GHz band with 40 MHz bandwidth. These results demonstrate that multi-static range-angle estimation and localization algorithms can make real-time, scalable backscatter localization practical for next-generation ambient IoT networks.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [372] [Sequential BP-based Decoding of QLDPC Codes](https://arxiv.org/abs/2602.13420)
*Mohsen Moradi,Salman Habib,Vahid Nourozi,David G. M. Mitchell*

Main category: cs.IT

TL;DR: Sequential scheduling variants (SCNS and SVNS) improve BP decoding for QLDPC codes by stabilizing message updates, and sequential BPGD further enhances performance with fewer decimation rounds.


<details>
  <summary>Details</summary>
Motivation: Conventional BP decoders perform poorly on QLDPC codes due to non-convergence caused by stabilizer constraints that create short cycles and degeneracy problems.

Method: Two scheduling variants: sequential check node scheduling (SCNS) and sequential variable node scheduling (SVNS) that process nodes in fixed order. Also applied to BP guided decimation (BPGD) to create sequential BPGD (SBPGD).

Result: Sequential schedules lower block error rate relative to conventional BP. SBPGD outperforms BPGD with significantly fewer decimation rounds. SVNS-BP surpasses BP-OSD-0 for [[1922,50,16]] code at similar complexity to standard BP.

Conclusion: Changing update schedules without altering the code improves both reliability and efficiency of BP-based decoding for QLDPC codes.

Abstract: Quantum low-density parity-check (QLDPC) codes are a leading approach to quantum error correction, yet conventional belief propagation (BP) decoders often perform poorly, primarily due to non-convergence exacerbated by stabilizer constraints, which induce short cycles and degeneracy. We propose two scheduling variants, sequential check node scheduling (SCNS) and sequential variable node scheduling (SVNS), that improve BP's error-correction ability by processing check nodes (CNs) or variable nodes (VNs), respectively, in a fixed order, stabilizing message updates and reducing stalls. We also employ this technique to an improved BP-variant called BP guided decimation (BPGD), where symbols are progressively fixed during decoding iterations. Here, we demonstrate that the sequential BPGD (SBPGD) decoder can further improve the convergence properties and performance of the decoder. On standard QLDPC benchmarks under a Pauli-X noise model, our sequential schedules are shown to lower the block error rate relative to conventional BP, and SBPGD outperforms BPGD while using significantly fewer decimation rounds, translating to lower computational cost. These results demonstrate that changing the update schedule, without altering the code, can improve both the reliability and efficiency of BP-based decoding for QLDPC codes. For the [[1922,50,16]] C2 hypergraph-product code with independent X errors, SVNS-BP surpasses BP-OSD-0 in error correction at roughly the same complexity as standard BP.

</details>


### [373] [End-to-End NOMA with Perfect and Quantized CSI Over Rayleigh Fading Channels](https://arxiv.org/abs/2602.13446)
*Selma Benouadah,Mojtaba Vaezi,Ruizhan Shen,Hamid Jafarkhani*

Main category: cs.IT

TL;DR: End-to-end autoencoder framework for downlink NOMA over Rayleigh fading channels learns interference-aware super-constellations with practical CSI constraints including limited feedback quantization.


<details>
  <summary>Details</summary>
Motivation: Existing NOMA approaches either assume AWGN channels or treat fading channels without fully end-to-end learning, lacking integration of practical channel state information constraints like limited feedback quantization.

Method: Developed an end-to-end autoencoder framework that directly embeds Rayleigh fading channels into both training and inference, incorporating limited feedback via uniform and Lloyd-Max quantization of channel gains.

Result: With perfect CSI, the proposed AE outperforms existing analytical NOMA schemes. Lloyd-Max quantization achieves superior BER performance compared to uniform quantization.

Conclusion: End-to-end AEs trained directly over Rayleigh fading can effectively learn robust, interference-aware signaling strategies, enabling NOMA deployment in fading environments with realistic CSI constraints.

Abstract: An end-to-end autoencoder (AE) framework is developed for downlink non-orthogonal multiple access (NOMA) over Rayleigh fading channels, which learns interference-aware and channel-adaptive super-constellations. While existing works either assume additive white Gaussian noise channels or treat fading channels without a fully end-to-end learning approach, our framework directly embeds the wireless channel into both training and inference. To account for practical channel state information (CSI), we further incorporate limited feedback via both uniform and Lloyd-Max quantization of channel gains and analyze their impact on AE training and bit error rate (BER) performance. Simulation results show that, with perfect CSI, the proposed AE outperforms the existing analytical NOMA schemes. In addition, Lloyd-Max quantization achieves superior BER performance compared to uniform quantization. These results demonstrate that end-to-end AEs trained directly over Rayleigh fading can effectively learn robust, interference-aware signaling strategies, paving the way for NOMA deployment in fading environments with realistic CSI constraints.

</details>


### [374] [An Algebraic Invariant for Free Convolutional Codes over Finite Local Rings](https://arxiv.org/abs/2602.13468)
*Mohammed El Oued*

Main category: cs.IT

TL;DR: The paper introduces a new invariant called Residual Structural Polynomial (Δ_p(C)) for free convolutional codes over finite local rings Z_{p^r}, proves it's an algebraic criterion for catastrophicity, and establishes a duality theorem showing this invariant is preserved under orthogonality.


<details>
  <summary>Details</summary>
Motivation: To develop algebraic tools for analyzing free convolutional codes over finite local rings, particularly to characterize catastrophicity (an undesirable property where finite input sequences produce infinite output sequences) through an intrinsic algebraic invariant.

Method: Introduces Residual Structural Polynomial Δ_p(C) constructed via encoders that are reduced internal degree matrices (RIDM). Proves this invariant is intrinsic to the code (independent of equivalent RIDMs). Uses this invariant to establish algebraic criteria for catastrophicity.

Result: 1) Δ_p(C) is an intrinsic invariant of free convolutional codes. 2) A free code C admits non-catastrophic realization iff Δ_p(C) is a monomial D^s. 3) Duality theorem: Δ_p(C) = Δ_p(C^⊥), showing catastrophicity is preserved under orthogonality.

Conclusion: The Residual Structural Polynomial provides a powerful algebraic tool for analyzing free convolutional codes over finite local rings, offering both a criterion for catastrophicity and revealing deep structural symmetry through the duality theorem.

Abstract: This paper investigates the algebraic structure of free convolutional codes over the finite local ring Z_{p^r}. We introduce a new structural invariant, the Residual Structural Polynomial, denoted by Delta_p(C) in F_p[D]. We construct this invariant via encoders which are reduced internal degree matrices (RIDM). We formally demonstrate that Delta_p(C) is an intrinsic characteristic of the code, invariant under equivalent RIDMs. A central result of this work is the establishment that Delta_p(C) serves as an algebraic criterion for intrinsic catastrophicity: we prove that a free code C admits a non-catastrophic realization if and only if Delta_p(C) is a monomial of the form D^s. Furthermore, we establish a fundamental duality theorem, proving that Delta_p(C) = Delta_p(C^perp). This result reveals a deep structural symmetry, showing that the "catastrophicity" of a free code is preserved under orthogonality.

</details>


### [375] [Convergence of Differential Entropies -- II](https://arxiv.org/abs/2602.13493)
*Mahesh Godavarti*

Main category: cs.IT

TL;DR: Differential entropy convergence under uniform integrability and tightness of entropy integrands, with new entropy-weighted Orlicz condition that's strictly weaker than previous fixed-α conditions.


<details>
  <summary>Details</summary>
Motivation: To establish conditions for convergence of differential entropy when probability density functions converge in measure, improving upon previous work by Godavarti-Hero and others.

Method: Uses Vitali's convergence theorem to show entropy convergence when integrands f_n|log f_n| are uniformly integrable and tight. Introduces entropy-weighted Orlicz condition with single superlinear Ψ. Disproves Godavarti-Hero conjecture about α_n ↓ 1.

Result: New sufficient condition (Ψ-superlinear) strictly weaker than fixed-α conditions. Shows condition is necessary on bounded domains. Recovers previous results as corollaries.

Conclusion: Provides comprehensive characterization of differential entropy convergence with optimal conditions, settling open questions and unifying previous work.

Abstract: We show that under convergence in measure of probability density functions, differential entropy converges whenever the entropy integrands $f_n |\log f_n|$ are uniformly integrable and tight -- a direct consequence of Vitali's convergence theorem. We give an entropy-weighted Orlicz condition: $\sup_n \int f_n\, Ψ(|\log f_n|) < \infty$ for a single superlinear $Ψ$, strictly weaker than the fixed-$α$ condition of Godavarti and Hero (2004). We also disprove the Godavarti-Hero conjecture that $α> 1$ could be replaced by $α_n \downarrow 1$. We recover the sufficient conditions of Godavarti-Hero, Piera-Parada, and Ghourchian-Gohari-Amini as corollaries, and we show the condition is also necessary on bounded domains.

</details>


### [376] [Constructing Quantum Convolutional Codes via Difference Triangle Sets](https://arxiv.org/abs/2602.13505)
*Vahid Nourozi,David Mitchell*

Main category: cs.IT

TL;DR: Quantum convolutional codes constructed using difference triangle sets with guaranteed minimum distance.


<details>
  <summary>Details</summary>
Motivation: To provide a constructive design for quantum convolutional codes that guarantees a prescribed minimum distance, addressing the challenge of finding polynomial stabilizers that commute while keeping them sparse and encoding memory small.

Method: Construct QCCs using difference triangle sets (DTSs). X(D) corresponds to classical convolutional self-orthogonal codes from strong DTS supports, and Z(D) is constructed by reflecting DTS indices of X(D) to ensure symplectic orthogonality.

Result: Numerical results demonstrate successful construction for a variety of code rates, showing the approach works for different parameter configurations.

Conclusion: The DTS-based construction provides a systematic method to build quantum convolutional codes with guaranteed minimum distance properties.

Abstract: In this paper, we introduce a construction of quantum convolutional codes (QCCs) based on difference triangle sets (DTSs). To construct QCCs, one must determine polynomial stabilizers $X(D)$ and $Z(D)$ that commute (symplectic orthogonality), while keeping the stabilizers sparse and encoding memory small. To construct Z(D), we show that one can use a reflection of the DTS indices of X(D), where X(D) corresponds to a classical convolutional self-orthogonal code (CSOC) constructed from strong DTS supports. The motivation of this approach is to provide a constructive design that guarantees a prescribed minimum distance. We provide numerical results demonstrating the construction for a variety of code rates.

</details>


### [377] [Redundancy-Optimal Constructions of $(1,1)$-Criss-Cross Deletion Correcting Codes with Efficient Encoding/Decoding Algorithms](https://arxiv.org/abs/2602.13548)
*Wenhao Liu,Zhengyi Jiang,Zhongyi Huang,Hanxu Hou*

Main category: cs.IT

TL;DR: First construction of q-ary 2D codes correcting (1,1)-criss-cross deletions with optimal redundancy (within O(1) gap) and explicit O(n²) encoding/decoding algorithms.


<details>
  <summary>Details</summary>
Motivation: 2D error-correcting codes are crucial for applications like QR codes, DNA storage, and racetrack memories. Criss-cross deletions (simultaneous row and column deletions) are significant error patterns that need efficient correction.

Method: Novel code construction for q-ary (1,1)-criss-cross deletion correction with complete encoding, decoding, and data recovery algorithms. Works for n ≥ 11 and q ≥ 3 with O(n²) complexity.

Result: For n ≥ 11 and q = Ω(n), achieves redundancy of 2n + 2log_q n + O(1), which attains the lower bound (2n + 2log_q n - 3) within O(1) gap. First construction with optimal redundancy and explicit algorithms.

Conclusion: Presents the first construction achieving near-optimal redundancy for (1,1)-criss-cross deletion correction with practical O(n²) encoding/decoding algorithms, advancing 2D error correction for real applications.

Abstract: Two-dimensional error-correcting codes, where codewords are represented as $n \times n$ arrays over a $q$-ary alphabet, find important applications in areas such as QR codes, DNA-based storage, and racetrack memories. Among the possible error patterns, $(t_r,t_c)$-criss-cross deletions-where $t_r$ rows and $t_c$ columns are simultaneously deleted-are of particular significance. In this paper, we focus on $q$-ary $(1,1)$-criss-cross deletion correcting codes. We present a novel code construction and develop complete encoding, decoding, and data recovery algorithms for parameters $n \ge 11$ and $q \ge 3$. The complexity of the proposed encoding, decoding, and data recovery algorithms is $\mathcal{O}(n^2)$. Furthermore, we show that for $n \ge 11$ and $q = Ω(n)$ (i.e., there exists a constant $c>0$ such that $q \ge cn$), both the code redundancy and the encoder redundancy of the constructed codes are $2n + 2\log_q n + \mathcal{O}(1)$, which attain the lower bound ($2n + 2\log_q n - 3$) within an $\mathcal{O}(1)$ gap. To the best of our knowledge, this is the first construction that can achieve the optimal redundancy with only an $\mathcal{O}(1)$ gap, while simultaneously featuring explicit encoding and decoding algorithms.

</details>


### [378] [Discrete-Space Generative AI Pipeline for Semantic Transmission of Signals](https://arxiv.org/abs/2602.13556)
*Silvija Kokalj-Filipovic,Yagna Kaasaragadda*

Main category: cs.IT

TL;DR: Discernment is a semantic communication system that transmits signal meaning using GenAI models, dynamically switching between autoregressive and diffusion-based algorithms to adapt to channel impairments while maintaining semantic integrity even under severe capacity degradation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create a robust semantic communication system that can maintain meaning transmission over impaired channels, particularly for IoT deployments where channel conditions vary and spectral efficiency is crucial.

Method: Discernment uses GenAI models operating in discrete spaces to transmit semantic meaning of physical signals. It dynamically switches between autoregressive and diffusion-based generative algorithms based on erasure patterns in the channel (modeled as erasure channels).

Result: Discernment maintains semantic integrity even as channel capacity severely degrades, showing very small and graceful performance decline in both classification accuracy and statistical fidelity of reconstructed meaning. It adapts to diverse physical channel conditions while maintaining spectral efficiency and low model complexity.

Conclusion: Discernment is well-suited for IoT deployments and strongly motivates further research on the semantic channel paradigm, demonstrating effective adaptation to channel impairments while preserving meaning transmission.

Abstract: We introduce Discernment, a semantic communication system that transmits the meaning of physical signals (baseband radio and audio) over a technical channel using GenAI models operating in discrete spaces. Discernment dynamically adapts to channel impairments - modeled as erasure channels - by switching between an autoregressive or a diffusion-based generative algorithm, depending on the erasure pattern. Our results show that Discernment maintains semantic integrity even as channel capacity severely degrades, exhibiting very small and graceful performance decline in both classification accuracy and statistical fidelity of the reconstructed meaning. These findings demonstrate Discernment's ability to adjust to diverse physical channel conditions while maintaining spectral efficiency and low model complexity, making it well suited for IoT deployments and strongly motivating further research on this semantic channel paradigm.

</details>


### [379] [UAV Swarm Enabled Aerial Movable Antenna System for Low-Altitude Economy: From Far-Field to Near-Field Communication](https://arxiv.org/abs/2602.13687)
*Haiquan Lu,Chao Feng,Yong Zeng,Shaodan Ma,Long Shi,Shi Jin,Rui Zhang*

Main category: cs.IT

TL;DR: UAV swarm enabled near-field AMA communication using non-uniform spherical wave model, with joint optimization of 3D trajectory and beamforming to maximize minimum average rate.


<details>
  <summary>Details</summary>
Motivation: UAV swarm provides 3D mobility for aerial movable antenna systems, enabling extremely large-scale arrays where conventional far-field uniform plane wave models become invalid for aerial-to-ground links.

Method: Formulated optimization problem for joint 3D UAV swarm trajectory and receive beamforming; used SCA technique for single UE case; derived optimal placement for single/two UAVs; proposed symmetric hyperbola placement for IUI-free two-UE communication; developed alternating optimization algorithm for arbitrary UEs.

Result: Numerical results show significant performance gains over benchmark schemes; derived closed-form optimal placements for specific cases; demonstrated IUI-free communication achievable for two UEs with symmetric UAV placement.

Conclusion: UAV swarm enabled near-field AMA communication with NUSW model effectively addresses limitations of conventional far-field models, with proposed optimization methods achieving substantial performance improvements.

Abstract: Unmanned aerial vehicle (UAV) with the intrinsic three-dimensional (3D) mobility provides an ideal platform for implementing aerial movable antenna (AMA) system enabled by UAV swarm cooperation. Besides, AMA system is readily to achieve an extremely large-scale array aperture, rendering the conventional far-field uniform plane wave (UPW) model no longer valid for aerial-to-ground links. This paper studies the UAV swarm enabled near-field AMA communication, by taking into account the non-uniform spherical wave (NUSW) model, where UAV swarm trajectory simultaneously influences the channel amplitude and phase. We formulate a general optimization problem to maximize the minimum average communication rate over user equipments (UEs), by jointly optimizing the 3D UAV swarm trajectory and receive beamforming for all UEs. To draw useful insights, the special case of single UE is first studied, and successive convex approximation (SCA) technique is proposed to efficiently optimize the UAV swarm trajectory. For the special case of placement optimization, the optimal placement positions of UAVs for cases of single UAV and two UAVs are derived in closed-form. Then, for the special case of two UEs, we show that an inter-UE interference (IUI)-free communication can be achieved by symmetrically placing an even number of UAVs along a hyperbola, with its foci corresponding to the locations of the two UEs. Furthermore, for arbitrary number of UEs, an alternating optimization algorithm is proposed to efficiently tackle the non-convex optimization problem. Numerical results validate the significant performance gains over the benchmark schemes.

</details>


### [380] [BRAIN: Bayesian Reasoning via Active Inference for Agentic and Embodied Intelligence in Mobile Networks](https://arxiv.org/abs/2602.14033)
*Osman Tugay Basaran,Martin Maier,Falko Dressler*

Main category: cs.IT

TL;DR: BRAIN agent uses Active Inference for 6G networks, offering explainable, adaptive AI for radio resource allocation without retraining.


<details>
  <summary>Details</summary>
Motivation: Current DRL-based AI agents for 6G networks lack explainability and suffer from brittle adaptation with catastrophic forgetting in dynamic environments. There's a need for autonomous agents that can adapt in real-time while maintaining transparency in decision-making.

Method: Proposes BRAIN (Bayesian reasoning via Active Inference) agent using deep generative models of network environment. Minimizes variational free energy to unify perception and action in closed-loop paradigm. Implemented as O-RAN xApp on GPU-accelerated testbed.

Result: BRAIN demonstrates: (i) robust causal reasoning for dynamic radio resource allocation maintaining QoS targets, (ii) 28.3% higher robustness to sudden traffic shifts without retraining, (iii) real-time interpretability through human-interpretable belief state diagnostics.

Conclusion: BRAIN agent addresses key limitations of DRL-based approaches for 6G networks by providing explainable, adaptive AI with superior robustness and interpretability for dynamic radio resource management.

Abstract: Future sixth-generation (6G) mobile networks will demand artificial intelligence (AI) agents that are not only autonomous and efficient, but also capable of real-time adaptation in dynamic environments and transparent in their decisionmaking. However, prevailing agentic AI approaches in networking, exhibit significant shortcomings in this regard. Conventional deep reinforcement learning (DRL)-based agents lack explainability and often suffer from brittle adaptation, including catastrophic forgetting of past knowledge under non-stationary conditions. In this paper, we propose an alternative solution for these challenges: Bayesian reasoning via Active Inference (BRAIN) agent. BRAIN harnesses a deep generative model of the network environment and minimizes variational free energy to unify perception and action in a single closed-loop paradigm. We implement BRAIN as O-RAN eXtended application (xApp) on GPU-accelerated testbed and demonstrate its advantages over standard DRL baselines. In our experiments, BRAIN exhibits (i) robust causal reasoning for dynamic radio resource allocation, maintaining slice-specific quality of service (QoS) targets (throughput, latency, reliability) under varying traffic loads, (ii) superior adaptability with up to 28.3% higher robustness to sudden traffic shifts versus benchmarks (achieved without any retraining), and (iii) real-time interpretability of its decisions through human-interpretable belief state diagnostics.

</details>


### [381] [Energy-Efficient Over-the-Air Federated Learning via Pinching Antenna Systems](https://arxiv.org/abs/2602.14250)
*Saba Asaad,Ali Bereyhi*

Main category: cs.IT

TL;DR: PASS (Pinching Antennas Systems) technology reduces energy consumption for over-the-air federated learning compared to conventional MIMO servers.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of PASS technology for improving energy efficiency in over-the-air federated learning systems, addressing the energy consumption challenges in distributed learning for next-generation wireless systems.

Method: Developed a low-complexity algorithmic approach that jointly tunes PASS parameters and schedules mobile devices to minimize energy consumption in OTA-FL. Compared PASS-assisted server performance against conventional MIMO server setups.

Result: Numerical experiments show that using a single-waveguide PASS at the server drastically reduces required energy for model aggregation compared to fully-digital MIMO servers, especially in moderately sized areas.

Conclusion: PASS technology shows promise as an energy-efficient solution for distributed learning in next-generation wireless systems, offering significant energy savings for over-the-air federated learning applications.

Abstract: Pinching antennas systems (PASSs) have recently been proposed as a novel flexible-antenna technology. These systems are implemented by attaching low-cost pinching elements to dielectric waveguides. As the direct link is bypassed through waveguides, PASSs can effectively compensate large-scale effects of the wireless channel. This work explores the potential gains of employing PASSs for over-the-air federated learning (OTA-FL). For a PASS-assisted server, we develop a low-complexity algorithmic approach, which jointly tunes the PASS parameters and schedules the mobile devices for minimal energy consumption in OTA-FL. We study the efficiency of the proposed design and compare it against the conventional OTA-FL setting with MIMO server. Numerical experiments demonstrate that using a single-waveguide PASS at the server within a moderately sized area, the required energy for model aggregation is drastically reduced as compared to the case with fully-digital MIMO server. This introduces PASS as a potential technology for energy-efficient distributed learning in next generations of wireless systems.

</details>


### [382] [Diversity vs Degrees of Freedom in Gaussian Fading Channels](https://arxiv.org/abs/2602.14371)
*Mahesh Godavarti*

Main category: cs.IT

TL;DR: The paper redefines degrees of freedom (DOF) and diversity using geometric concepts rather than logρ normalization, introduces Bhattacharyya packing to create gauge-DOF and B-diversity as workable proxies, and establishes cross-gauge tradeoffs for noncoherent fast fading channels.


<details>
  <summary>Details</summary>
Motivation: Standard definitions of DOF and diversity normalize by logρ, which can give zero or undefined results when this normalization is inappropriate. The authors argue that DOF and diversity should be intrinsic channel properties, not artifacts of normalization choices.

Method: For Gaussian fading channels, DOF and diversity are defined geometrically as ranks of the bilinear map HX with different variables free. The paper introduces Bhattacharyya packing to create gauge-DOF and B-diversity as finite, informative proxies that work on all gauges. Three gauge classes are identified: logρ, loglogρ, and (logρ)^β.

Result: Main result: For noncoherent fast fading, capacity lives on loglogρ gauge while B-diversity lives on logρ gauge (exponentially larger), with matching upper and lower bounds. The approach recovers or extends known scaling laws for coherent MIMO, block fading, and irregular-spectrum channels.

Conclusion: The geometric approach provides gauge-independent definitions of DOF and diversity, with Bhattacharyya packing enabling cross-gauge tradeoffs. This framework unifies various channel models and reveals fundamental relationships between capacity and diversity across different scaling regimes.

Abstract: The standard definitions of degrees of freedom (DOF) and diversity both normalize by $\logρ$. When this ruler is wrong, both measurements give zero or become undefined, yet intuitively DOF and diversity ought to be channel properties, not artifacts of a normalization choice. We formalize this for Gaussian fading channels. For fixed-$H$ MIMO, DOF and diversity are both ranks of the bilinear map~$HX$ with different variables free: $\varepsilon$-covering the image of~$X\!\mapsto\!HX$ gives DOF on the $\logρ$ gauge; expanding across all dimensions of the fading map gives diversity on the linear~$ρ$ gauge. Covering produces logs; expansion produces linear growth; so in every model studied here the two gauges differ. These geometric definitions do not yield tradeoff curves. We bridge the gap with Bhattacharyya packing, obtaining gauge-DOF and B-diversity as workable proxies -- finite and informative on every gauge, including those where the classical diversity order is zero. Three gauge classes emerge: $\logρ$, $\log\logρ$, and $(\logρ)^β$, $β\in(0,1)$. The main result is a cross-gauge tradeoff for noncoherent fast fading: capacity lives on $\log\logρ$, but B-diversity lives on $\logρ$, exponentially larger, with matching upper and lower bounds. For coherent MIMO, block fading, and irregular-spectrum channels, the same approach recovers or extends known scaling laws.

</details>


### [383] [On the Rate-Distortion-Complexity Tradeoff for Semantic Communication](https://arxiv.org/abs/2602.14481)
*Jingxuan Chai,Yong Xiao,Guangming Shi*

Main category: cs.IT

TL;DR: This paper proposes a rate-distortion-complexity (RDC) framework for semantic communication that extends classical rate-distortion theory by incorporating semantic distance constraints and computational complexity measures, revealing a fundamental three-way tradeoff among rate, semantic accuracy, and model complexity.


<details>
  <summary>Details</summary>
Motivation: Existing DL-based semantic communication systems overlook the high computational complexity in both training and inference of encoders/decoders. There's a need to bridge the gap between semantic extraction effectiveness and practical computational constraints for real-world deployment.

Method: Proposes a rate-distortion-complexity (RDC) framework extending classical rate-distortion theory with semantic distance constraints (bit-wise distortion and statistical divergence metrics) and complexity measures from minimum description length and information bottleneck theory. Derives closed-form theoretical results for Gaussian and binary semantic sources.

Result: Theoretical analysis reveals a fundamental three-way tradeoff among achievable rate, semantic distance, and model complexity. Experiments on real-world image/video datasets validate this tradeoff and show the information-theoretic complexity measure correlates with practical computational costs.

Conclusion: The RDC framework provides theoretical foundation for designing efficient semantic communication systems under computational constraints, offering practical guidance for resource-constrained scenarios through validated complexity-performance tradeoffs.

Abstract: Semantic communication is a novel communication paradigm that focuses on conveying the user's intended meaning rather than the bit-wise transmission of source signals. One of the key challenges is to effectively represent and extract the semantic meaning of any given source signals. While deep learning (DL)-based solutions have shown promising results in extracting implicit semantic information from a wide range of sources, existing work often overlooks the high computational complexity inherent in both model training and inference for the DL-based encoder and decoder. To bridge this gap, this paper proposes a rate-distortion-complexity (RDC) framework which extends the classical rate-distortion theory by incorporating the constraints on semantic distance, including both the traditional bit-wise distortion metric and statistical difference-based divergence metric, and complexity measure, adopted from the theory of minimum description length and information bottleneck. We derive the closed-form theoretical results of the minimum achievable rate under given constraints on semantic distance and complexity for both Gaussian and binary semantic sources. Our theoretical results show a fundamental three-way tradeoff among achievable rate, semantic distance, and model complexity. Extensive experiments on real-world image and video datasets validate this tradeoff and further demonstrate that our information-theoretic complexity measure effectively correlates with practical computational costs, guiding efficient system design in resource-constrained scenarios.

</details>


### [384] [Center-Fed Pinching Antenna System (C-PASS): Modeling, Analysis, and Beamforming Design](https://arxiv.org/abs/2602.14805)
*Xu Gan,Yuanwei Liu*

Main category: cs.IT

TL;DR: Proposes a generalized framework for center-fed pinching antenna system (C-PASS) with closed-form DoF and power scaling laws, develops alternating optimization algorithm for sum-rate maximization, and shows superior performance over existing PASS systems.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient antenna system framework that provides better degree of freedom and power scaling performance compared to existing pinching antenna systems, particularly in challenging high-attenuation regimes.

Method: Proposes a generalized C-PASS framework, derives closed-form expressions for DoF and power scaling laws, formulates sum-rate maximization problem, and develops an efficient alternating optimization algorithm using closed-form solutions for transmit precoding/power splitting and BCD methods for antenna positions/radiation coefficients.

Result: C-PASS achieves linear DoF scaling with input ports and receive antennas, achieves power gain of order O(P_T M), and outperforms single-waveguide PASS in DoF/power scaling while beating multi-waveguide PASS in high-attenuation regimes with gains exceeding 10 dB.

Conclusion: The proposed C-PASS framework provides superior performance with better theoretical scaling properties and practical gains, making it particularly effective in high-attenuation wireless environments compared to existing antenna systems.

Abstract: A generalized framework for the novel center-fed pinching antenna system (C-PASS) is proposed. Within this framework, closed-form expressions for the degree of freedom (DoF) and power scaling law of the proposed C-PASS are first derived. These theoretical results reveal that the achievable DoF scales linearly with the number of input ports, $M$, and the number of receive antennas, $K$. Furthermore, the derived power scaling laws demonstrate that the C-PASS achieves a power gain of order $\mathcal{O}(P_T M)$, where $P_T$ denotes the transmit power. Based on the proposed C-PASS modeling, a sum-rate maximization problem for the joint optimization of transmit and pinching beamforming is then formulated. To solve this highly coupled non-convex problem, an efficient alternating optimization algorithm is developed. More particularly, the transmit precoding and power splitting ratios are updated via derived closed-form solutions, while the pinching antenna positions and radiation coefficients are optimized using block coordinate descent (BCD) methods. Finally, our numerical results reveal that the single-waveguide C-PASS: 1) achieves superior DoF and power scaling laws compared to the single-waveguide PASS; and 2) outperforms the multi-waveguide PASS in high-attenuation regimes, yielding a substantial gain exceeding $10$ dB.

</details>


### [385] [Constructions of linear codes from vectorial plateaued functions and their subfield codes with applications to quantum CSS codes](https://arxiv.org/abs/2602.14832)
*Virginio Fratianni,Sihem Mesnager*

Main category: cs.IT

TL;DR: Extends 3D linear code construction from 2 to 3 functions, adds vector-valued functions, uses Bent/s-Plateaued functions, yields few-weight codes with optimal duals, connects to classical constructions, and applies to quantum coding.


<details>
  <summary>Details</summary>
Motivation: To enhance flexibility of parameters in linear code construction by extending from 2 to 3 functions, and to expand construction space by introducing vector-valued functions for richer structural properties.

Method: Extends Xu et al.'s 2023 framework from 2 to 3 functions, introduces vector-valued functions, uses Bent and s-Plateaued functions (including Almost Bent) as code generators, analyzes via Walsh transform properties to determine parameters and weight distributions.

Result: Constructed codes have few weights, their duals are distance and dimensionally optimal with respect to Sphere Packing and Griesmer bounds, establishes theoretical connection to classical first generic construction, provides conditions for minimal and self-orthogonal codes.

Conclusion: The extended framework with 3 functions and vector-valued functions successfully enhances parameter flexibility and construction space, producing optimal few-weight codes with applications in quantum coding via Calderbank-Shor-Steane framework.

Abstract: Linear codes over finite fields parameterized by functions have proven to be a powerful tool in coding theory, yielding optimal and few-weight codes with significant applications in secret sharing, authentication codes, and association schemes. In 2023, Xu et al. introduced a construction framework for 3-dimensional linear codes parameterized by two functions, which has demonstrated considerable success in generating infinite families of optimal linear codes. Motivated by this approach, we propose a construction that extends the framework to three functions, thereby enhancing the flexibility of the parameters. Additionally, we introduce a vectorial setting by allowing vector-valued functions, expanding the construction space and the set of achievable structural properties. We analyze both scalar and vectorial frameworks, employing Bent and s-Plateaued functions, including Almost Bent, to define the code generators. By exploiting the properties of the Walsh transform, we determine the explicit parameters and weight distributions of these codes and their punctured versions. A key result of this study is that the constructed codes have few weights, and their duals are distance and dimensionally optimal with respect to both the Sphere Packing and Griesmer bounds. Furthermore, we establish a theoretical connection between our vectorial approach and the classical first generic construction of linear codes, providing sufficient conditions for the resulting codes to be minimal and self-orthogonal. Finally, we investigate applications to quantum coding theory within the Calderbank-Shor-Steane framework.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [386] [Directional Concentration Uncertainty: A representational approach to uncertainty quantification for generative models](https://arxiv.org/abs/2602.13264)
*Souradeep Chattopadhyay,Brendan Kennedy,Sai Munikoti,Soumik Sarkar,Karl Pazdernik*

Main category: cs.LG

TL;DR: DCU is a flexible uncertainty quantification method using von Mises-Fisher distribution to measure geometric dispersion of language model embeddings, outperforming heuristic approaches and generalizing to multi-modal tasks.


<details>
  <summary>Details</summary>
Motivation: Current uncertainty quantification methods for generative models rely on rigid heuristics that don't generalize well across different tasks and modalities, limiting their trustworthiness and robustness.

Method: Proposes Directional Concentration Uncertainty (DCU), a statistical procedure based on von Mises-Fisher distribution that measures geometric dispersion of multiple generated outputs using continuous embeddings without task-specific heuristics.

Result: DCU matches or exceeds calibration levels of prior methods like semantic entropy and generalizes well to more complex tasks in multi-modal domains.

Conclusion: DCU provides a flexible framework for uncertainty quantification that can be integrated into multi-modal and agentic systems, offering improved generalization across tasks and modalities.

Abstract: In the critical task of making generative models trustworthy and robust, methods for Uncertainty Quantification (UQ) have begun to show encouraging potential. However, many of these methods rely on rigid heuristics that fail to generalize across tasks and modalities. Here, we propose a novel framework for UQ that is highly flexible and approaches or surpasses the performance of prior heuristic methods. We introduce Directional Concentration Uncertainty (DCU), a novel statistical procedure for quantifying the concentration of embeddings based on the von Mises-Fisher (vMF) distribution. Our method captures uncertainty by measuring the geometric dispersion of multiple generated outputs from a language model using continuous embeddings of the generated outputs without any task specific heuristics. In our experiments, we show that DCU matches or exceeds calibration levels of prior works like semantic entropy (Kuhn et al., 2023) and also generalizes well to more complex tasks in multi-modal domains. We present a framework for the wider potential of DCU and its implications for integration into UQ for multi-modal and agentic frameworks.

</details>


### [387] [BLUEPRINT Rebuilding a Legacy: Multimodal Retrieval for Complex Engineering Drawings and Documents](https://arxiv.org/abs/2602.13345)
*Ethan Seefried,Ran Eldegaway,Sanjay Das,Nathaniel Blanchard,Tirthankar Ghosal*

Main category: cs.LG

TL;DR: Blueprint is a multimodal retrieval system for engineering drawings that uses layout-aware region detection, VLM-based OCR, and fused retrieval to automatically structure metadata from legacy archives.


<details>
  <summary>Details</summary>
Motivation: Decades of engineering drawings and technical records are locked in legacy archives with inconsistent or missing metadata, making retrieval difficult and often requiring manual effort.

Method: Blueprint detects canonical drawing regions, applies region-restricted VLM-based OCR, normalizes identifiers (DWG, part, facility), and fuses lexical and dense retrieval with a lightweight region-level reranker.

Result: Deployed on ~770k unlabeled files, it automatically produces structured metadata. Evaluation on 5k-file benchmark with 350 expert-curated queries shows 10.1% absolute gain in Success@3 and 18.9% relative improvement in nDCG@3 over strongest vision-language baseline.

Conclusion: Blueprint outperforms vision, text, and multimodal baselines, with oracle ablations showing headroom for improvement. The system enables cross-facility search in legacy engineering archives, with all resources released for reproducibility.

Abstract: Decades of engineering drawings and technical records remain locked in legacy archives with inconsistent or missing metadata, making retrieval difficult and often manual. We present Blueprint, a layout-aware multimodal retrieval system designed for large-scale engineering repositories. Blueprint detects canonical drawing regions, applies region-restricted VLM-based OCR, normalizes identifiers (e.g., DWG, part, facility), and fuses lexical and dense retrieval with a lightweight region-level reranker. Deployed on ~770k unlabeled files, it automatically produces structured metadata suitable for cross-facility search.
  We evaluate Blueprint on a 5k-file benchmark with 350 expert-curated queries using pooled, graded (0/1/2) relevance judgments. Blueprint delivers a 10.1% absolute gain in Success@3 and an 18.9% relative improvement in nDCG@3 over the strongest vision-language baseline}, consistently outperforming across vision, text, and multimodal intents. Oracle ablations reveal substantial headroom under perfect region detection and OCR. We release all queries, runs, annotations, and code to facilitate reproducible evaluation on legacy engineering archives.

</details>


### [388] [Exploring the Performance of ML/DL Architectures on the MNIST-1D Dataset](https://arxiv.org/abs/2602.13348)
*Michael Beebe,GodsGift Uzor,Manasa Chepuri,Divya Sree Vemula,Angel Ayala*

Main category: cs.LG

TL;DR: The paper evaluates ResNet, TCN, and DCNN architectures on MNIST-1D, showing they outperform simpler models and achieve near-human performance, validating MNIST-1D as a robust benchmark for computational-constrained environments.


<details>
  <summary>Details</summary>
Motivation: Small datasets like MNIST are useful for rapid experimentation but lack complexity to distinguish advanced architectures. MNIST-1D was introduced to explore inductive biases in sequential data while maintaining small-scale advantages. The paper aims to extend MNIST-1D exploration by evaluating advanced sequential architectures.

Method: Implemented and benchmarked Residual Networks (ResNet), Temporal Convolutional Networks (TCN), and Dilated Convolutional Neural Networks (DCNN) on MNIST-1D dataset. Compared these advanced architectures against previously tested models including logistic regression, MLPs, CNNs, and GRUs.

Result: TCN and DCNN consistently outperform simpler models, achieving near-human performance on MNIST-1D. ResNet also shows significant improvements. Advanced architectures demonstrate superior ability to capture sequential patterns and hierarchical features in small structured datasets.

Conclusion: MNIST-1D is validated as a robust benchmark for evaluating machine learning architectures under computational constraints. Architectural innovations leveraging inductive biases and hierarchical feature extraction significantly improve performance on small structured datasets, offering insights for optimizing deep learning in resource-limited environments.

Abstract: Small datasets like MNIST have historically been instrumental in advancing machine learning research by providing a controlled environment for rapid experimentation and model evaluation. However, their simplicity often limits their utility for distinguishing between advanced neural network architectures. To address these challenges, Greydanus et al. introduced the MNIST-1D dataset, a one-dimensional adaptation of MNIST designed to explore inductive biases in sequential data. This dataset maintains the advantages of small-scale datasets while introducing variability and complexity that make it ideal for studying advanced architectures.
  In this paper, we extend the exploration of MNIST-1D by evaluating the performance of Residual Networks (ResNet), Temporal Convolutional Networks (TCN), and Dilated Convolutional Neural Networks (DCNN). These models, known for their ability to capture sequential patterns and hierarchical features, were implemented and benchmarked alongside previously tested architectures such as logistic regression, MLPs, CNNs, and GRUs. Our experimental results demonstrate that advanced architectures like TCN and DCNN consistently outperform simpler models, achieving near-human performance on MNIST-1D. ResNet also shows significant improvements, highlighting the importance of leveraging inductive biases and hierarchical feature extraction in small structured datasets.
  Through this study, we validate the utility of MNIST-1D as a robust benchmark for evaluating machine learning architectures under computational constraints. Our findings emphasize the role of architectural innovations in improving model performance and offer insights into optimizing deep learning models for resource-limited environments.

</details>


### [389] [The Speed-up Factor: A Quantitative Multi-Iteration Active Learning Performance Metric](https://arxiv.org/abs/2602.13359)
*Hannes Kath,Thiago S. Gouvêa,Daniel Sonntag*

Main category: cs.LG

TL;DR: The paper introduces the speed-up factor as a new metric for evaluating active learning query methods, showing it's more stable and accurate than existing metrics for measuring annotation efficiency.


<details>
  <summary>Details</summary>
Motivation: Active learning research focuses heavily on developing query methods but lacks appropriate performance metrics for evaluating the iterative learning process. Current evaluation methods don't adequately measure how efficiently query methods reduce annotation needs compared to random sampling.

Method: The authors formally define the speed-up factor metric, which quantifies the fraction of samples needed by a query method to match random sampling performance. They conduct an 8-year literature review and empirical evaluation using 4 diverse datasets and 7 different query methods to compare the speed-up factor against state-of-the-art AL metrics.

Result: The speed-up factor accurately captures the described fraction of samples needed, demonstrates superior stability across iterations compared to existing metrics, and confirms the underlying assumptions about its effectiveness as a performance measure for active learning query methods.

Conclusion: The speed-up factor provides a robust, quantitative metric for evaluating active learning query methods that better captures their efficiency in reducing annotation costs compared to random sampling, addressing a critical gap in AL evaluation methodology.

Abstract: Machine learning models excel with abundant annotated data, but annotation is often costly and time-intensive. Active learning (AL) aims to improve the performance-to-annotation ratio by using query methods (QMs) to iteratively select the most informative samples. While AL research focuses mainly on QM development, the evaluation of this iterative process lacks appropriate performance metrics. This work reviews eight years of AL evaluation literature and formally introduces the speed-up factor, a quantitative multi-iteration QM performance metric that indicates the fraction of samples needed to match random sampling performance. Using four datasets from diverse domains and seven QMs of various types, we empirically evaluate the speed-up factor and compare it with state-of-the-art AL performance metrics. The results confirm the assumptions underlying the speed-up factor, demonstrate its accuracy in capturing the described fraction, and reveal its superior stability across iterations.

</details>


### [390] [Accelerated Discovery of Cryoprotectant Cocktails via Multi-Objective Bayesian Optimization](https://arxiv.org/abs/2602.13398)
*Daniel Emerson,Nora Gaby-Biegel,Purva Joshi,Yoed Rabin,Rebecca D. Sandlin,Levent Burak Kara*

Main category: cs.LG

TL;DR: Active learning framework combining high-throughput screening with multi-objective Bayesian optimization to efficiently design cryoprotectant cocktails that balance high concentration and cell viability.


<details>
  <summary>Details</summary>
Motivation: Traditional CPA cocktail design faces a tradeoff between high concentration (to prevent ice formation) and low toxicity (to preserve cell viability), creating a complex multi-objective design space that requires slow, intuition-based or exhaustive experimental approaches.

Method: Data-efficient framework combining high-throughput screening with active learning loop using multi-objective Bayesian optimization. Trains probabilistic surrogate models to predict concentration and viability with uncertainty quantification, then iteratively selects experiments by maximizing expected Pareto improvement under uncertainty.

Result: Method improves dominated hypervolume by 9.5% over naive strategy and 4.5% over strong baseline while reducing experiments needed. In synthetic studies, recovers comparable Pareto-optimal solutions using only 30% of evaluations required by prior state-of-the-art, saving ~10 weeks of experimental time.

Conclusion: The framework accelerates cryoprotectant cocktail discovery by efficiently navigating the tradeoff between concentration and viability, and can be adapted to different CPA libraries, objective definitions, and cell lines for broader cryopreservation development.

Abstract: Designing cryoprotectant agent (CPA) cocktails for vitrification is challenging because formulations must be concentrated enough to suppress ice formation yet non-toxic enough to preserve cell viability. This tradeoff creates a large, multi-objective design space in which traditional discovery is slow, often relying on expert intuition or exhaustive experimentation. We present a data-efficient framework that accelerates CPA cocktail design by combining high-throughput screening with an active-learning loop based on multi-objective Bayesian optimization. From an initial set of measured cocktails, we train probabilistic surrogate models to predict concentration and viability and quantify uncertainty across candidate formulations. We then iteratively select the next experiments by prioritizing cocktails expected to improve the Pareto front, maximizing expected Pareto improvement under uncertainty, and update the models as new assay results are collected. Wet-lab validation shows that our approach efficiently discovers cocktails that simultaneously achieve high CPA concentrations and high post-exposure viability. Relative to a naive strategy and a strong baseline, our method improves dominated hypervolume by 9.5\% and 4.5\%, respectively, while reducing the number of experiments needed to reach high-quality solutions. In complementary synthetic studies, it recovers a comparably strong set of Pareto-optimal solutions using only 30\% of the evaluations required by the prior state-of-the-art multi-objective approach, which amounts to saving approximately 10 weeks of experimental time. Because the framework assumes only a suitable assay and defined formulation space, it can be adapted to different CPA libraries, objective definitions, and cell lines to accelerate cryopreservation development.

</details>


### [391] [Why is Normalization Preferred? A Worst-Case Complexity Theory for Stochastically Preconditioned SGD under Heavy-Tailed Noise](https://arxiv.org/abs/2602.13413)
*Yuchen Fang,James Demmel,Javad Lavaei*

Main category: cs.LG

TL;DR: The paper develops worst-case complexity theory for stochastically preconditioned SGD (SPSGD) under heavy-tailed noise, showing normalization guarantees convergence while clipping may fail due to statistical dependence between preconditioner and gradient estimates.


<details>
  <summary>Details</summary>
Motivation: There's a need to understand the theoretical properties of adaptive optimization methods (like Adam, RMSProp, Shampoo) under heavy-tailed noise conditions, particularly to explain why normalization empirically outperforms clipping in large-scale model training.

Method: Develops worst-case complexity theory for stochastically preconditioned SGD and its accelerated variants under heavy-tailed noise (finite p-th moment for p∈(1,2]). Uses novel vector-valued Burkholder-type inequality to analyze convergence properties of normalization vs. clipping approaches.

Result: Normalization guarantees convergence to first-order stationary point at rate O(T^{-(p-1)/(3p-2)}) with known parameters and O(T^{-(p-1)/(2p)}) with unknown parameters (matching optimal rates for normalized SGD). Clipping may fail to converge in worst case due to statistical dependence between stochastic preconditioner and gradient estimates.

Conclusion: The theoretical analysis explains the empirical preference for normalization over clipping in large-scale model training, showing fundamental separation in worst-case properties between these two stabilization techniques for stochastically preconditioned optimization.

Abstract: We develop a worst-case complexity theory for stochastically preconditioned stochastic gradient descent (SPSGD) and its accelerated variants under heavy-tailed noise, a setting that encompasses widely used adaptive methods such as Adam, RMSProp, and Shampoo. We assume the stochastic gradient noise has a finite $p$-th moment for some $p \in (1,2]$, and measure convergence after $T$ iterations. While clipping and normalization are parallel tools for stabilizing training of SGD under heavy-tailed noise, there is a fundamental separation in their worst-case properties in stochastically preconditioned settings. We demonstrate that normalization guarantees convergence to a first-order stationary point at rate $\mathcal{O}(T^{-\frac{p-1}{3p-2}})$ when problem parameters are known, and $\mathcal{O}(T^{-\frac{p-1}{2p}})$ when problem parameters are unknown, matching the optimal rates for normalized SGD, respectively. In contrast, we prove that clipping may fail to converge in the worst case due to the statistical dependence between the stochastic preconditioner and the gradient estimates. To enable the analysis, we develop a novel vector-valued Burkholder-type inequality that may be of independent interest. These results provide a theoretical explanation for the empirical preference for normalization over clipping in large-scale model training.

</details>


### [392] [High-Resolution Climate Projections Using Diffusion-Based Downscaling of a Lightweight Climate Emulator](https://arxiv.org/abs/2602.13416)
*Haiwen Guan,Moein Darman,Dibyajyoti Chakraborty,Troy Arcomano,Ashesh Chattopadhyay,Romit Maulik*

Main category: cs.LG

TL;DR: A deep learning downscaling framework using diffusion models to enhance LUCIE climate emulator resolution from ~300km to 25km while preserving coarse dynamics.


<details>
  <summary>Details</summary>
Motivation: Current data-driven climate models like LUCIE have limitations in resolution (~300km) that prevent detailed regional impact assessments, requiring downscaling to finer scales for practical applications.

Method: Developed a deep learning-based downscaling framework using probabilistic diffusion-based generative models with conditional and posterior sampling frameworks to downscale coarse LUCIE outputs to 25km resolution.

Result: The approach successfully preserves coarse-grained dynamics from LUCIE while generating fine-scaled climatological statistics at ~28km resolution, as validated by multiple metrics including RMSE, power spectrum, and probability density functions.

Conclusion: The diffusion-based downscaling framework effectively bridges the resolution gap in climate modeling, enabling detailed regional climate assessments while maintaining physical consistency from the parent model.

Abstract: The proliferation of data-driven models in weather and climate sciences has marked a significant paradigm shift, with advanced models demonstrating exceptional skill in medium-range forecasting. However, these models are often limited by long-term instabilities, climatological drift, and substantial computational costs during training and inference, restricting their broader application for climate studies. Addressing these limitations, Guan et al. (2024) introduced LUCIE, a lightweight, physically consistent climate emulator utilizing a Spherical Fourier Neural Operator (SFNO) architecture. This model is able to reproduce accurate long-term statistics including climatological mean and seasonal variability. However, LUCIE's native resolution (~300 km) is inadequate for detailed regional impact assessments. To overcome this limitation, we introduce a deep learning-based downscaling framework, leveraging probabilistic diffusion-based generative models with conditional and posterior sampling frameworks. These models downscale coarse LUCIE outputs to 25 km resolution. They are trained on approximately 14,000 ERA5 timesteps spanning 2000-2009 and evaluated on LUCIE predictions from 2010 to 2020. Model performance is assessed through diverse metrics, including latitude-averaged RMSE, power spectrum, probability density functions and First Empirical Orthogonal Function of the zonal wind. We observe that the proposed approach is able to preserve the coarse-grained dynamics from LUCIE while generating fine-scaled climatological statistics at ~28km resolution.

</details>


### [393] [Text Has Curvature](https://arxiv.org/abs/2602.13418)
*Karish Grover,Hanqing Zeng,Yinglong Xia,Christos Faloutsos,Geoffrey J. Gordon*

Main category: cs.LG

TL;DR: Text has intrinsic curvature detectable through Texture - a word-level discrete curvature signal derived from context beliefs, enabling geometry-aware NLP without geometric training.


<details>
  <summary>Details</summary>
Motivation: To determine if text has intrinsic curvature beyond embedding space artifacts, and to develop a language-native way to detect, define, and utilize this curvature for practical NLP applications.

Method: Proposes Texture - a text-native word-level discrete curvature signal defined by reconciling left- and right-context beliefs around masked words through a Schrodinger bridge, creating a curvature field where positive values indicate focused meaning and negative values indicate competing continuations.

Result: Provides empirical and theoretical evidence that semantic inference in natural corpora is non-flat (has inherent curvature), demonstrates Texture's utility in improving long-context inference through curvature-guided compression and retrieval-augmented generation through curvature-guided routing.

Conclusion: Text does have intrinsic curvature, and Texture establishes a text-native curvature paradigm that makes curvature measurable and practically useful for NLP tasks without requiring geometric training.

Abstract: Does text have an intrinsic curvature? Language is increasingly modeled in curved geometries - hyperbolic spaces for hierarchy, mixed-curvature manifolds for compositional structure - yet a basic scientific question remains unresolved: what does curvature mean for text itself, in a way that is native to language rather than an artifact of the embedding space we choose? We argue that text does indeed have curvature, and show how to detect it, define it, and use it. To this end, we propose Texture, a text-native, word-level discrete curvature signal, and make three contributions. (a) Existence: We provide empirical and theoretical certificates that semantic inference in natural corpora is non-flat, i.e. language has inherent curvature. (b) Definition: We define Texture by reconciling left- and right-context beliefs around a masked word through a Schrodinger bridge, yielding a curvature field that is positive where context focuses meaning and negative where it fans out into competing continuations. (c) Utility: Texture is actionable: it serves as a general-purpose measurement and control primitive enabling geometry without geometric training; we instantiate it on two representative tasks, improving long-context inference through curvature-guided compression and retrieval-augmented generation through curvature-guided routing. Together, our results establish a text-native curvature paradigm, making curvature measurable and practically useful.

</details>


### [394] [Comparing Classifiers: A Case Study Using PyCM](https://arxiv.org/abs/2602.13482)
*Sadra Sabouri,Alireza Zolanvari,Sepand Haghighi*

Main category: cs.LG

TL;DR: PyCM library tutorial for deep-dive evaluation of multi-class classifiers, showing how metric choice affects model interpretation and emphasizing need for multi-dimensional evaluation framework.


<details>
  <summary>Details</summary>
Motivation: Optimal classification model selection requires robust understanding of model performance, but standard metrics may miss subtle performance trade-offs and important differences.

Method: Tutorial on PyCM library demonstrating its utility through examination of two different case scenarios to illustrate how evaluation metric choice affects model efficacy interpretation.

Result: Choice of evaluation metrics fundamentally shifts interpretation of model's efficacy; multi-dimensional evaluation framework is essential for uncovering small but important performance differences.

Conclusion: Multi-dimensional evaluation framework is crucial for comprehensive model assessment as standard metrics may miss subtle performance trade-offs in multi-class classifiers.

Abstract: Selecting an optimal classification model requires a robust and comprehensive understanding of the performance of the model. This paper provides a tutorial on the PyCM library, demonstrating its utility in conducting deep-dive evaluations of multi-class classifiers. By examining two different case scenarios, we illustrate how the choice of evaluation metrics can fundamentally shift the interpretation of a model's efficacy. Our findings emphasize that a multi-dimensional evaluation framework is essential for uncovering small but important differences in model performance. However, standard metrics may miss these subtle performance trade-offs.

</details>


### [395] [Finding Highly Interpretable Prompt-Specific Circuits in Language Models](https://arxiv.org/abs/2602.13483)
*Gabriel Franco,Lucas M. Tassis,Azalea Rohr,Mark Crovella*

Main category: cs.LG

TL;DR: The paper introduces ACC++, an improved method for extracting prompt-specific circuits from language models, showing that circuits vary by prompt even within the same task, and develops an automated interpretability pipeline for analyzing prompt families.


<details>
  <summary>Details</summary>
Motivation: Prior work assumes a single stable circuit per task by averaging across prompts, but this obscures prompt-specific mechanisms. The authors aim to reveal that circuits are prompt-specific even within fixed tasks, requiring more granular analysis.

Method: Builds on attention causal communication (ACC) with refinements (ACC++) that extract cleaner, lower-dimensional causal signals from single forward passes without replacement models or activation patching. Applied to indirect object identification (IOI) in GPT-2, Pythia, and Gemma 2.

Result: No single circuit exists for IOI in any model - different prompt templates induce systematically different mechanisms. Prompts cluster into families with similar circuits, and representative circuits can be identified for each family. Automated interpretability pipeline surfaces human-interpretable features and mechanistic explanations.

Conclusion: Circuits should be studied at the prompt level rather than task level, enabling scalable circuit descriptions despite prompt-specific mechanisms. ACC++ provides practical tools for this more granular analysis.

Abstract: Understanding the internal circuits that language models use to solve tasks remains a central challenge in mechanistic interpretability. Most prior work identifies circuits at the task level by averaging across many prompts, implicitly assuming a single stable mechanism per task. We show that this assumption can obscure a crucial source of structure: circuits are prompt-specific, even within a fixed task. Building on attention causal communication (ACC) (Franco & Crovella, 2025), we introduce ACC++, refinements that extract cleaner, lower-dimensional causal signals inside attention heads from a single forward pass. Like ACC, our approach does not require replacement models (e.g., SAEs) or activation patching; ACC++ further improves circuit precision by reducing attribution noise. Applying ACC++ to indirect object identification (IOI) in GPT-2, Pythia, and Gemma 2, we find there is no single circuit for IOI in any model: different prompt templates induce systematically different mechanisms. Despite this variation, prompts cluster into prompt families with similar circuits, and we propose a representative circuit for each family as a practical unit of analysis. Finally, we develop an automated interpretability pipeline that uses ACC++ signals to surface human-interpretable features and assemble mechanistic explanations for prompt families behavior. Together, our results recast circuits as a meaningful object of study by shifting the unit of analysis from tasks to prompts, enabling scalable circuit descriptions in the presence of prompt-specific mechanisms.

</details>


### [396] [Federated Learning of Nonlinear Temporal Dynamics with Graph Attention-based Cross-Client Interpretability](https://arxiv.org/abs/2602.13485)
*Ayse Tursucular,Ayush Mohanty,Nazal Mohamed,Nagi Gebraeel*

Main category: cs.LG

TL;DR: Federated framework for learning interpretable temporal interdependencies across decentralized industrial subsystems with fixed proprietary models, using nonlinear state space models and Graph Attention Networks.


<details>
  <summary>Details</summary>
Motivation: Modern industrial systems have distributed sensors generating high-dimensional time series data from interdependent subsystems. Challenges include: decentralized settings where raw data cannot be shared, heterogeneous client observations, fixed proprietary models that cannot be modified/retrained, and nonlinear dynamics making cross-client temporal interdependencies difficult to interpret.

Method: Each client maps high-dimensional local observations to low-dimensional latent states using a nonlinear state space model. A central server learns a graph-structured neural state transition model over communicated latent states using a Graph Attention Network (GAT). For interpretability, the Jacobian of the learned server-side transition model is related to attention coefficients.

Result: Theoretical convergence guarantees to a centralized oracle are established. Synthetic experiments demonstrate convergence, interpretability, scalability, and privacy. Real-world experiments show performance comparable to decentralized baselines.

Conclusion: The framework provides the first interpretable characterization of cross-client temporal interdependencies in decentralized nonlinear systems, addressing practical constraints of industrial deployments with fixed proprietary models and privacy concerns.

Abstract: Networks of modern industrial systems are increasingly monitored by distributed sensors, where each system comprises multiple subsystems generating high dimensional time series data. These subsystems are often interdependent, making it important to understand how temporal patterns at one subsystem relate to others. This is challenging in decentralized settings where raw measurements cannot be shared and client observations are heterogeneous. In practical deployments each subsystem (client) operates a fixed proprietary model that cannot be modified or retrained, limiting existing approaches. Nonlinear dynamics further make cross client temporal interdependencies difficult to interpret because they are embedded in nonlinear state transition functions. We present a federated framework for learning temporal interdependencies across clients under these constraints. Each client maps high dimensional local observations to low dimensional latent states using a nonlinear state space model. A central server learns a graph structured neural state transition model over the communicated latent states using a Graph Attention Network. For interpretability we relate the Jacobian of the learned server side transition model to attention coefficients, providing the first interpretable characterization of cross client temporal interdependencies in decentralized nonlinear systems. We establish theoretical convergence guarantees to a centralized oracle and validate the framework through synthetic experiments demonstrating convergence, interpretability, scalability and privacy. Additional real world experiments show performance comparable to decentralized baselines.

</details>


### [397] [Fast Swap-Based Element Selection for Multiplication-Free Dimension Reduction](https://arxiv.org/abs/2602.13532)
*Nobutaka Ono*

Main category: cs.LG

TL;DR: Proposes fast element selection algorithm for dimension reduction using subset selection instead of matrix multiplications, with swap-based local search optimization.


<details>
  <summary>Details</summary>
Motivation: Standard dimension reduction methods like PCA require matrix multiplications which become bottlenecks on resource-constrained systems. Need multiplication-free approach that reduces computational cost while maintaining effectiveness.

Method: Element selection reduces dimension by selecting subset of input elements. Evaluates subsets via minimum mean-squared error of linear regression predicting target vector (labels or input itself). Uses matrix inversion lemma to derive efficient formula for objective change when swapping selected/unselected elements, enabling swap-based local search optimization.

Result: Experiments on MNIST handwritten-digit images demonstrate effectiveness of proposed method. Achieves dimension reduction without matrix multiplications through efficient combinatorial optimization.

Conclusion: Element selection provides practical multiplication-free alternative to PCA for dimension reduction, suitable for resource-constrained systems while maintaining performance through efficient swap-based optimization.

Abstract: In this paper, we propose a fast algorithm for element selection, a multiplication-free form of dimension reduction that produces a dimension-reduced vector by simply selecting a subset of elements from the input. Dimension reduction is a fundamental technique for reducing unnecessary model parameters, mitigating overfitting, and accelerating training and inference. A standard approach is principal component analysis (PCA), but PCA relies on matrix multiplications; on resource-constrained systems, the multiplication count itself can become a bottleneck. Element selection eliminates this cost because the reduction consists only of selecting elements, and thus the key challenge is to determine which elements should be retained. We evaluate a candidate subset through the minimum mean-squared error of linear regression that predicts a target vector from the selected elements, where the target may be, for example, a one-hot label vector in classification. When an explicit target is unavailable, the input itself can be used as the target, yielding a reconstruction-based criterion. The resulting optimization is combinatorial, and exhaustive search is impractical. To address this, we derive an efficient formula for the objective change caused by swapping a selected and an unselected element, using the matrix inversion lemma, and we perform a swap-based local search that repeatedly applies objective-decreasing swaps until no further improvement is possible. Experiments on MNIST handwritten-digit images demonstrate the effectiveness of the proposed method.

</details>


### [398] [Preventing Rank Collapse in Federated Low-Rank Adaptation with Client Heterogeneity](https://arxiv.org/abs/2602.13486)
*Fei Wu,Jia Hu,Geyong Min,Shiqiang Wang*

Main category: cs.LG

TL;DR: FedLoRA suffers from rank collapse in heterogeneous settings where global update energy concentrates on minimum shared rank, causing suboptimal performance. Proposed raFLoRA prevents this via rank-partitioned aggregation weighted by effective client contributions.


<details>
  <summary>Details</summary>
Motivation: In practical federated learning, client heterogeneity in system resources and data distributions motivates using heterogeneous LoRA ranks across clients. However, existing FedLoRA approaches suffer from rank collapse where the energy of global updates concentrates on the minimum shared rank, leading to poor performance and high sensitivity to rank configurations.

Method: Proposed raFLoRA (rank-partitioned aggregation) method that decomposes local updates into rank partitions and aggregates each partition weighted by its effective client contributions. This addresses the mismatch between rank-agnostic aggregation weights and rank-dependent client contributions that causes rank collapse.

Result: Extensive experiments across classification and reasoning tasks show that raFLoRA prevents rank collapse, improves model performance, and preserves communication efficiency compared to state-of-the-art FedLoRA baselines.

Conclusion: raFLoRA effectively addresses the rank collapse problem in heterogeneous FedLoRA by properly weighting rank partitions based on client contributions, leading to better performance while maintaining communication efficiency in federated learning scenarios with heterogeneous client resources and data distributions.

Abstract: Federated low-rank adaptation (FedLoRA) has facilitated communication-efficient and privacy-preserving fine-tuning of foundation models for downstream tasks. In practical federated learning scenarios, client heterogeneity in system resources and data distributions motivates heterogeneous LoRA ranks across clients. We identify a previously overlooked phenomenon in heterogeneous FedLoRA, termed rank collapse, where the energy of the global update concentrates on the minimum shared rank, resulting in suboptimal performance and high sensitivity to rank configurations. Through theoretical analysis, we reveal the root cause of rank collapse: a mismatch between rank-agnostic aggregation weights and rank-dependent client contributions, which systematically suppresses higher-rank updates at a geometric rate over rounds. Motivated by this insight, we propose raFLoRA, a rank-partitioned aggregation method that decomposes local updates into rank partitions and then aggregates each partition weighted by its effective client contributions. Extensive experiments across classification and reasoning tasks show that raFLoRA prevents rank collapse, improves model performance, and preserves communication efficiency compared to state-of-the-art FedLoRA baselines.

</details>


### [399] [Coverage Guarantees for Pseudo-Calibrated Conformal Prediction under Distribution Shift](https://arxiv.org/abs/2602.14913)
*Farbod Siahkali,Ashwin Verma,Vijay Gupta*

Main category: cs.LG

TL;DR: Pseudo-calibration improves conformal prediction robustness under distribution shift by inflating prediction sets based on Wasserstein distance between source and target domains.


<details>
  <summary>Details</summary>
Motivation: Conformal prediction guarantees fail under distribution shift; need methods to maintain coverage when data distributions change between source and target domains.

Method: Use pseudo-calibration with domain adaptation tools to derive coverage lower bounds, design slack-inflated prediction sets, and propose source-tuned pseudo-calibration algorithm that adapts based on classifier uncertainty.

Result: Theoretical bounds track pseudo-calibration behavior; source-tuned scheme mitigates coverage degradation under distribution shift while maintaining reasonable prediction set sizes.

Conclusion: Pseudo-calibration with domain adaptation analysis provides effective approach to maintain conformal prediction coverage guarantees under bounded label-conditional covariate shift.

Abstract: Conformal prediction (CP) offers distribution-free marginal coverage guarantees under an exchangeability assumption, but these guarantees can fail if the data distribution shifts. We analyze the use of pseudo-calibration as a tool to counter this performance loss under a bounded label-conditional covariate shift model. Using tools from domain adaptation, we derive a lower bound on target coverage in terms of the source-domain loss of the classifier and a Wasserstein measure of the shift. Using this result, we provide a method to design pseudo-calibrated sets that inflate the conformal threshold by a slack parameter to keep target coverage above a prescribed level. Finally, we propose a source-tuned pseudo-calibration algorithm that interpolates between hard pseudo-labels and randomized labels as a function of classifier uncertainty. Numerical experiments show that our bounds qualitatively track pseudo-calibration behavior and that the source-tuned scheme mitigates coverage degradation under distribution shift while maintaining nontrivial prediction set sizes.

</details>


### [400] [TrasMuon: Trust-Region Adaptive Scaling for Orthogonalized Momentum Optimizers](https://arxiv.org/abs/2602.13498)
*Peng Cheng,Jiucheng Zang,Qingnan Li,Liheng Ma,Yufei Cui,Yingxue Zhang,Boxing Chen,Ming Jian,Wen Tong*

Main category: cs.LG

TL;DR: TrasMuon improves Muon-style optimizers by adding RMS calibration and trust-region clipping to stabilize training while preserving near-isometric geometry, achieving faster convergence and better stability without warmup.


<details>
  <summary>Details</summary>
Motivation: Muon-style optimizers use Newton-Schulz iterations for orthogonalization but discard magnitude information, making training sensitive to step-size hyperparameters and vulnerable to high-energy bursts. The authors aim to preserve Muon's geometric benefits while adding stabilization mechanisms.

Method: TrasMuon combines: (1) global RMS calibration to reintroduce adaptive scaling, and (2) energy-based trust-region clipping that confines updates to a stable zone based on relative energy ratios. This preserves near-isometric geometry while stabilizing magnitudes.

Result: Empirical experiments on vision and language models show TrasMuon converges faster than baseline methods. Additional experiments without warmup stages confirm superior stability and robustness compared to other optimizers.

Conclusion: TrasMuon successfully addresses the instability issues of Muon-style optimizers by combining adaptive scaling with trust-region constraints, achieving both faster convergence and improved training stability without requiring warmup stages.

Abstract: Muon-style optimizers leverage Newton-Schulz (NS) iterations to orthogonalize updates, yielding update geometries that often outperform Adam-series methods. However, this orthogonalization discards magnitude information, rendering training sensitive to step-size hyperparameters and vulnerable to high-energy bursts. To mitigate this, we introduce TrasMuon (\textbf{T}rust \textbf{R}egion \textbf{A}daptive \textbf{S}caling \textbf{Muon}). TrasMuon preserves the near-isometric geometry of Muon while stabilizing magnitudes through (i) global RMS calibration and (ii) energy-based trust-region clipping. We demonstrate that while reintroducing adaptive scaling improves optimization efficiency, it typically exacerbates instability due to high-energy outliers. TrasMuon addresses this by defining a trust region based on relative energy ratios, confining updates to a stable zone. Empirical experiments on vision and language models demonstrate that TrasMuon converges faster than baselines. Furthermore, experiments without warmup stages confirm TrasMuon's superior stability and robustness.

</details>


### [401] [$γ$-weakly $θ$-up-concavity: Linearizable Non-Convex Optimization with Applications to DR-Submodular and OSS Functions](https://arxiv.org/abs/2602.13506)
*Mohammad Pedramfar,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: The paper introduces γ-weakly θ-up-concavity, a new first-order condition that generalizes DR-submodular and OSS functions, enabling unified approximation guarantees for monotone non-convex optimization via linearization.


<details>
  <summary>Details</summary>
Motivation: Optimizing monotone non-convex functions is fundamental in ML and combinatorial optimization, but existing frameworks like DR-submodular and OSS functions are limited. There's a need for a unifying framework that captures broader classes of such functions while providing strong approximation guarantees.

Method: Introduces γ-weakly θ-up-concavity as a novel first-order condition. Proves these functions are upper-linearizable: for any feasible point, a linear surrogate can be constructed whose gains approximate the original objective up to a constant factor (approximation coefficient). Uses this linearizability to obtain unified guarantees via reductions to linear optimization.

Result: Provides unified approximation guarantees for offline optimization and static/dynamic regret bounds in online settings. Recovers optimal approximation coefficient for DR-submodular maximization and significantly improves existing coefficients for OSS optimization, especially over matroid constraints.

Conclusion: γ-weakly θ-up-concavity offers a powerful unifying framework for monotone non-convex optimization that strictly generalizes important function classes while enabling strong approximation guarantees through linearization techniques.

Abstract: Optimizing monotone non-convex functions is a fundamental challenge across machine learning and combinatorial optimization. We introduce and study $γ$-weakly $θ$-up-concavity, a novel first-order condition that characterizes a broad class of such functions. This condition provides a powerful unifying framework, strictly generalizing both DR-submodular functions and One-Sided Smooth (OSS) functions. Our central theoretical contribution demonstrates that $γ$-weakly $θ$-up-concave functions are upper-linearizable: for any feasible point, we can construct a linear surrogate whose gains provably approximate the original non-linear objective. This approximation holds up to a constant factor, namely the approximation coefficient, dependent solely on $γ$, $θ$, and the geometry of the feasible set. This linearizability yields immediate and unified approximation guarantees for a wide range of problems. Specifically, we obtain unified approximation guarantees for offline optimization as well as static and dynamic regret bounds in online settings via standard reductions to linear optimization. Moreover, our framework recovers the optimal approximation coefficient for DR-submodular maximization and significantly improves existing approximation coefficients for OSS optimization, particularly over matroid constraints.

</details>


### [402] [Singular Vectors of Attention Heads Align with Features](https://arxiv.org/abs/2602.13524)
*Gabriel Franco,Carson Loughridge,Mark Crovella*

Main category: cs.LG

TL;DR: The paper investigates whether singular vectors of attention matrices can reliably identify feature representations in language models, providing theoretical justification and empirical evidence for when this alignment occurs.


<details>
  <summary>Details</summary>
Motivation: Recent studies have assumed that feature representations can be inferred from singular vectors of attention matrices, but this assumption lacks sound justification. The paper aims to address this gap by examining why and when singular vectors align with features.

Method: 1) Demonstrates alignment in a model where features are directly observable, 2) Provides theoretical analysis showing alignment is expected under certain conditions, 3) Proposes sparse attention decomposition as a testable prediction for recognizing alignment in real models where features aren't directly observable.

Result: Singular vectors robustly align with features in observable models, theoretical conditions support alignment, and sparse attention decomposition emerges in real models consistent with predictions, providing evidence for the alignment phenomenon.

Conclusion: Alignment of singular vectors with features can be a sound and theoretically justified basis for feature identification in language models, addressing the previously unjustified assumption in mechanistic interpretability research.

Abstract: Identifying feature representations in language models is a central task in mechanistic interpretability. Several recent studies have made an implicit assumption that feature representations can be inferred in some cases from singular vectors of attention matrices. However, sound justification for this assumption is lacking. In this paper we address that question, asking: why and when do singular vectors align with features? First, we demonstrate that singular vectors robustly align with features in a model where features can be directly observed. We then show theoretically that such alignment is expected under a range of conditions. We close by asking how, operationally, alignment may be recognized in real models where feature representations are not directly observable. We identify sparse attention decomposition as a testable prediction of alignment, and show evidence that it emerges in a manner consistent with predictions in real models. Together these results suggest that alignment of singular vectors with features can be a sound and theoretically justified basis for feature identification in language models.

</details>


### [403] [QuaRK: A Quantum Reservoir Kernel for Time Series Learning](https://arxiv.org/abs/2602.13531)
*Abdallah Aaraba,Soumaya Cherkaoui,Ola Ahmad,Shengrui Wang*

Main category: cs.LG

TL;DR: QuaRK is an end-to-end quantum reservoir computing framework with hardware-realistic quantum reservoir featurizer and kernel-based readout, offering efficient implementation, learning guarantees, and scalability for time series tasks.


<details>
  <summary>Details</summary>
Motivation: Quantum reservoir computing shows promise for time series learning but lacks efficient, implementable architectures with learning guarantees. Current literature has scarce studies on practical quantum reservoir designs with theoretical performance assurances.

Method: QuaRK combines hardware-realistic quantum reservoir featurizer with kernel-based readout. Sequential data points are injected into quantum reservoir, producing compact feature vectors from k-local observables measured via classical shadow tomography. Classical kernel-based readout learns target mapping with explicit regularization and fast optimization.

Result: Framework provides clear computational knobs (circuit width/depth, measurement budget), preserves kernel method flexibility for nonlinear temporal functionals, scales to high-dimensional data, and offers learning-theoretic generalization guarantees for dependent temporal data.

Conclusion: QuaRK bridges gap between quantum reservoir computing theory and practice, providing principled guidance for building reliable temporal learners with empirical validation on synthetic beta-mixing time series tasks.

Abstract: Quantum reservoir computing offers a promising route for time series learning by modelling sequential data via rich quantum dynamics while the only training required happens at the level of a lightweight classical readout. However, studies featuring efficient and implementable quantum reservoir architectures along with model learning guarantees remain scarce in the literature. To close this gap, we introduce QuaRK, an end-to-end framework that couples a hardware-realistic quantum reservoir featurizer with a kernel-based readout scheme. Given a sequence of sample points, the reservoir injects the points one after the other to yield a compact feature vector from efficiently measured k-local observables using classical shadow tomography, after which a classical kernel-based readout learns the target mapping with explicit regularization and fast optimization. The resulting pipeline exposes clear computational knobs -- circuit width and depth as well as the measurement budget -- while preserving the flexibility of kernel methods to model nonlinear temporal functionals and being scalable to high-dimensional data. We further provide learning-theoretic generalization guarantees for dependent temporal data, linking design and resource choices to finite-sample performance, thereby offering principled guidance for building reliable temporal learners. Empirical experiments validate QuaRK and illustrate the predicted interpolation and generalization behaviours on synthetic beta-mixing time series tasks.

</details>


### [404] [Efficient Sampling with Discrete Diffusion Models: Sharp and Adaptive Guarantees](https://arxiv.org/abs/2602.15008)
*Daniil Dmitriev,Zhihan Huang,Yuting Wei*

Main category: cs.LG

TL;DR: The paper establishes sharp convergence guarantees for τ-leaping-based discrete diffusion samplers, showing improved iteration complexity that eliminates linear dependence on vocabulary size and adapts to low-dimensional structure.


<details>
  <summary>Details</summary>
Motivation: Despite empirical success of discrete diffusion models, theoretical foundations remain incomplete. The paper aims to provide rigorous theoretical analysis of sampling efficiency for score-based discrete diffusion models under continuous-time Markov chain formulation.

Method: Theoretical analysis of τ-leaping-based samplers for discrete diffusion models under CTMC formulation. For uniform discrete diffusion, analysis of standard τ-leaping algorithm. For masking discrete diffusion, introduction of a modified τ-leaping sampler with convergence governed by effective total correlation.

Result: For uniform diffusion: τ-leaping achieves iteration complexity of Õ(d/ε), eliminating linear dependence on vocabulary size S and improving existing bounds by factor d, with matching lower bound showing linear dependence on dimension is unavoidable. For masking diffusion: modified sampler's convergence rate governed by effective total correlation (bounded by d log S but can be sublinear/constant for structured data), enabling adaptation to low-dimensional structure without prior knowledge.

Conclusion: The paper provides rigorous theoretical foundations for discrete diffusion models, demonstrating improved sampling efficiency and adaptive convergence to low-dimensional structure. The analysis requires minimal assumptions beyond score entropy loss control, making results broadly applicable to practical scenarios like hidden Markov models, image data, and random graphs.

Abstract: Diffusion models over discrete spaces have recently shown striking empirical success, yet their theoretical foundations remain incomplete. In this paper, we study the sampling efficiency of score-based discrete diffusion models under a continuous-time Markov chain (CTMC) formulation, with a focus on $τ$-leaping-based samplers. We establish sharp convergence guarantees for attaining $\varepsilon$ accuracy in Kullback-Leibler (KL) divergence for both uniform and masking noising processes. For uniform discrete diffusion, we show that the $τ$-leaping algorithm achieves an iteration complexity of order $\tilde O(d/\varepsilon)$, with $d$ the ambient dimension of the target distribution, eliminating linear dependence on the vocabulary size $S$ and improving existing bounds by a factor of $d$; moreover, we establish a matching algorithmic lower bound showing that linear dependence on the ambient dimension is unavoidable in general. For masking discrete diffusion, we introduce a modified $τ$-leaping sampler whose convergence rate is governed by an intrinsic information-theoretic quantity, termed the effective total correlation, which is bounded by $d \log S$ but can be sublinear or even constant for structured data. As a consequence, the sampler provably adapts to low-dimensional structure without prior knowledge or algorithmic modification, yielding sublinear convergence rates for various practical examples (such as hidden Markov models, image data, and random graphs). Our analysis requires no boundedness or smoothness assumptions on the score estimator beyond control of the score entropy loss.

</details>


### [405] [Out-of-Support Generalisation via Weight Space Sequence Modelling](https://arxiv.org/abs/2602.13550)
*Roussel Desmond Nzoyem*

Main category: cs.LG

TL;DR: WeightCaster reformulates out-of-support generalization as sequence modeling in weight space, using concentric shells of training data to make plausible, interpretable predictions with uncertainty awareness.


<details>
  <summary>Details</summary>
Motivation: Neural networks often fail catastrophically on out-of-support samples (data outside training range), producing unrealistic but overconfident predictions, which is problematic for safety-critical applications.

Method: Reformulate OoS generalization as sequence modeling in weight space, partition training set into concentric shells corresponding to discrete sequential steps, and use WeightCaster framework without explicit inductive biases.

Result: Competitive or superior performance to state-of-the-art on synthetic cosine dataset and real-world air quality sensor readings, with plausible, interpretable, uncertainty-aware predictions while maintaining computational efficiency.

Conclusion: WeightCaster enhances reliability beyond in-distribution scenarios, enabling wider adoption of AI in safety-critical applications by addressing catastrophic failure on out-of-support samples.

Abstract: As breakthroughs in deep learning transform key industries, models are increasingly required to extrapolate on datapoints found outside the range of the training set, a challenge we coin as out-of-support (OoS) generalisation. However, neural networks frequently exhibit catastrophic failure on OoS samples, yielding unrealistic but overconfident predictions. We address this challenge by reformulating the OoS generalisation problem as a sequence modelling task in the weight space, wherein the training set is partitioned into concentric shells corresponding to discrete sequential steps. Our WeightCaster framework yields plausible, interpretable, and uncertainty-aware predictions without necessitating explicit inductive biases, all the while maintaining high computational efficiency. Emprical validation on a synthetic cosine dataset and real-world air quality sensor readings demonstrates performance competitive or superior to the state-of-the-art. By enhancing reliability beyond in-distribution scenarios, these results hold significant implications for the wider adoption of artificial intelligence in safety-critical applications.

</details>


### [406] [Scenario-Adaptive MU-MIMO OFDM Semantic Communication With Asymmetric Neural Network](https://arxiv.org/abs/2602.13557)
*Chongyang Li,Tianqian Zhang,Shouyin Liu*

Main category: cs.LG

TL;DR: Proposed scenario-adaptive MU-MIMO semantic communication framework for 6G downlink that outperforms existing DJSCC and SSCC schemes, especially in low-SNR conditions.


<details>
  <summary>Details</summary>
Motivation: Semantic Communication (SemCom) is promising for 6G but faces challenges in realistic MU-MIMO OFDM systems due to multi-user interference and frequency-selective fading. Existing DJSCC schemes designed for point-to-point links suffer performance saturation in multi-user scenarios.

Method: Asymmetric architecture with scenario-aware semantic encoder that dynamically adjusts feature extraction based on CSI and SNR, neural precoding network to mitigate MUI in semantic domain, and lightweight decoder with pilot-guided attention mechanism for implicit channel equalization and feature calibration.

Result: Extensive simulations over 3GPP channel models show significant outperformance over DJSCC and traditional SSCC in PSNR and classification accuracy, particularly in low-SNR regimes, while maintaining low latency and computational cost on edge devices.

Conclusion: The proposed scenario-adaptive MU-MIMO SemCom framework effectively addresses multi-user interference and fading challenges in realistic 6G downlink systems, demonstrating superior performance and practical feasibility for edge deployment.

Abstract: Semantic Communication (SemCom) has emerged as a promising paradigm for 6G networks, aiming to extract and transmit task-relevant information rather than minimizing bit errors. However, applying SemCom to realistic downlink Multi-User Multi-Input Multi-Output (MU-MIMO) Orthogonal Frequency Division Multiplexing (OFDM) systems remains challenging due to severe Multi-User Interference (MUI) and frequency-selective fading. Existing Deep Joint Source-Channel Coding (DJSCC) schemes, primarily designed for point-to-point links, suffer from performance saturation in multi-user scenarios. To address these issues, we propose a scenario-adaptive MU-MIMO SemCom framework featuring an asymmetric architecture tailored for downlink transmission. At the transmitter, we introduce a scenario-aware semantic encoder that dynamically adjusts feature extraction based on Channel State Information (CSI) and Signal-to-Noise Ratio (SNR), followed by a neural precoding network designed to mitigate MUI in the semantic domain. At the receiver, a lightweight decoder equipped with a novel pilot-guided attention mechanism is employed to implicitly perform channel equalization and feature calibration using reference pilot symbols. Extensive simulation results over 3GPP channel models demonstrate that the proposed framework significantly outperforms DJSCC and traditional Separate Source-Channel Coding (SSCC) schemes in terms of Peak Signal-to-Noise Ratio (PSNR) and classification accuracy, particularly in low-SNR regimes, while maintaining low latency and computational cost on edge devices.

</details>


### [407] [Interpretable clustering via optimal multiway-split decision trees](https://arxiv.org/abs/2602.13586)
*Hayato Suzuki,Shunnosuke Ikeda,Yuichi Takano*

Main category: cs.LG

TL;DR: Proposes interpretable clustering using optimal multiway-split decision trees formulated as 0-1 integer linear optimization, with one-dimensional K-means for discretization, achieving better accuracy and interpretability than binary trees.


<details>
  <summary>Details</summary>
Motivation: Existing clustering methods using binary decision trees suffer from high computational costs, suboptimal solutions, and excessive depth that reduces interpretability. Need for more tractable optimization and better interpretability in clustering.

Method: Formulates interpretable clustering as optimal multiway-split decision trees using 0-1 integer linear optimization. Integrates one-dimensional K-means algorithm for discretization of continuous variables, enabling flexible data-driven branching.

Result: Outperforms baseline methods in clustering accuracy and interpretability on real-world datasets. Produces multiway-split trees with concise decision rules while maintaining competitive performance across various evaluation metrics.

Conclusion: The proposed method provides a more tractable optimization approach than existing mixed-integer nonlinear problems, yielding interpretable multiway-split decision trees with better accuracy and clearer decision rules for clustering.

Abstract: Clustering serves as a vital tool for uncovering latent data structures, and achieving both high accuracy and interpretability is essential. To this end, existing methods typically construct binary decision trees by solving mixed-integer nonlinear optimization problems, often leading to significant computational costs and suboptimal solutions. Furthermore, binary decision trees frequently result in excessively deep structures, which makes them difficult to interpret. To mitigate these issues, we propose an interpretable clustering method based on optimal multiway-split decision trees, formulated as a 0-1 integer linear optimization problem. This reformulation renders the optimization problem more tractable compared to existing models. A key feature of our method is the integration of a one-dimensional K-means algorithm for the discretization of continuous variables, allowing for flexible and data-driven branching. Extensive numerical experiments on publicly available real-world datasets demonstrate that our method outperforms baseline methods in terms of clustering accuracy and interpretability. Our method yields multiway-split decision trees with concise decision rules while maintaining competitive performance across various evaluation metrics.

</details>


### [408] [Benchmark Leakage Trap: Can We Trust LLM-based Recommendation?](https://arxiv.org/abs/2602.13626)
*Mingqiao Zhang,Qiyao Peng,Yumeng Wang,Chunyuan Liu,Hongtao Liu*

Main category: cs.LG

TL;DR: LLM-based recommendation systems suffer from benchmark data leakage where models memorize benchmark data during training, leading to inflated performance metrics that don't reflect true capability.


<details>
  <summary>Details</summary>
Motivation: The integration of LLMs into recommender systems creates evaluation reliability challenges, with benchmark data leakage being a previously overlooked issue that artificially inflates performance metrics.

Method: Simulated diverse data leakage scenarios through continued pre-training of foundation models on strategically blended corpora containing user-item interactions from both in-domain and out-of-domain sources.

Result: Data leakage has a dual-effect: domain-relevant leakage causes substantial but spurious performance gains, while domain-irrelevant leakage typically degrades recommendation accuracy, revealing complex contamination effects.

Conclusion: Data leakage is a critical, previously unaccounted-for factor in LLM-based recommendation that impacts true model performance, highlighting the need for more rigorous evaluation protocols.

Abstract: The expanding integration of Large Language Models (LLMs) into recommender systems poses critical challenges to evaluation reliability. This paper identifies and investigates a previously overlooked issue: benchmark data leakage in LLM-based recommendation. This phenomenon occurs when LLMs are exposed to and potentially memorize benchmark datasets during pre-training or fine-tuning, leading to artificially inflated performance metrics that fail to reflect true model performance. To validate this phenomenon, we simulate diverse data leakage scenarios by conducting continued pre-training of foundation models on strategically blended corpora, which include user-item interactions from both in-domain and out-of-domain sources. Our experiments reveal a dual-effect of data leakage: when the leaked data is domain-relevant, it induces substantial but spurious performance gains, misleadingly exaggerating the model's capability. In contrast, domain-irrelevant leakage typically degrades recommendation accuracy, highlighting the complex and contingent nature of this contamination. Our findings reveal that data leakage acts as a critical, previously unaccounted-for factor in LLM-based recommendation, which could impact the true model performance. We release our code at https://github.com/yusba1/LLMRec-Data-Leakage.

</details>


### [409] [Optimization-Free Graph Embedding via Distributional Kernel for Community Detection](https://arxiv.org/abs/2602.13634)
*Shuaibin Song,Kai Ming Ting,Kaifeng Zhang,Tianrun Liang*

Main category: cs.LG

TL;DR: The paper proposes a distribution-aware kernel for graph embedding that addresses over-smoothing in NAS methods by incorporating node and degree distributions, requiring no optimization, and improving community detection performance.


<details>
  <summary>Details</summary>
Motivation: NAS-based graph embedding methods (GNNs and WL) suffer from over-smoothing - loss of node distinguishability with increased iterations. The authors identify that existing methods overlook two critical network characteristics: distributions of nodes and node degrees, which contribute significantly to over-smoothing.

Method: Proposes a novel weighted distribution-aware kernel that embeds nodes while considering their distributional characteristics. The method explicitly incorporates both node and degree distributions, requires no optimization, and effectively mitigates over-smoothing effects, allowing WL to preserve node distinguishability even after many iterations.

Result: Experiments show superior community detection performance via spectral clustering, outperforming existing graph embedding methods (including deep learning methods) on standard benchmarks. The method preserves node distinguishability and expressiveness even after many embedding iterations.

Conclusion: Incorporating node and degree distribution characteristics is crucial for mitigating over-smoothing in NAS-based graph embedding methods. The proposed distribution-aware kernel provides an effective, optimization-free solution that significantly improves representation quality and downstream task performance.

Abstract: Neighborhood Aggregation Strategy (NAS) is a widely used approach in graph embedding, underpinning both Graph Neural Networks (GNNs) and Weisfeiler-Lehman (WL) methods. However, NAS-based methods are identified to be prone to over-smoothing-the loss of node distinguishability with increased iterations-thereby limiting their effectiveness. This paper identifies two characteristics in a network, i.e., the distributions of nodes and node degrees that are critical for expressive representation but have been overlooked in existing methods. We show that these overlooked characteristics contribute significantly to over-smoothing of NAS-methods. To address this, we propose a novel weighted distribution-aware kernel that embeds nodes while taking their distributional characteristics into consideration. Our method has three distinguishing features: (1) it is the first method to explicitly incorporate both distributional characteristics; (2) it requires no optimization; and (3) it effectively mitigates the adverse effects of over-smoothing, allowing WL to preserve node distinguishability and expressiveness even after many iterations of embedding. Experiments demonstrate that our method achieves superior community detection performance via spectral clustering, outperforming existing graph embedding methods, including deep learning methods, on standard benchmarks.

</details>


### [410] [Joint Time Series Chain: Detecting Unusual Evolving Trend across Time Series](https://arxiv.org/abs/2602.13649)
*Li Zhang,Nital Patel,Xiuqi Li,Jessica Lin*

Main category: cs.LG

TL;DR: Proposes Joint Time Series Chain (JointTSC) to find evolving patterns across interrupted or related time series, addressing limitations of existing single-series chain methods.


<details>
  <summary>Details</summary>
Motivation: Existing time series chain definitions only work within single time series, missing evolving patterns across interrupted time series or related time series. This limitation prevents discovery of unusual evolving trends that span gaps or multiple related datasets.

Method: Introduces Joint Time Series Chain definition specifically designed for finding evolving trends across interrupted or related time series. Focuses on mitigating robustness issues from gaps/interruptions. Proposes effective ranking criterion to identify best chain.

Result: Outperforms existing TSC methods in locating unusual evolving patterns through extensive empirical evaluations. Demonstrates utility with real-life manufacturing application from Intel.

Conclusion: JointTSC successfully addresses limitations of single-series chain methods by enabling discovery of evolving patterns across interrupted or related time series, with practical applications in real-world scenarios like manufacturing.

Abstract: Time series chain (TSC) is a recently introduced concept that captures the evolving patterns in large scale time series. Informally, a time series chain is a temporally ordered set of subsequences, in which consecutive subsequences in the chain are similar to one another, but the last and the first subsequences maybe be dissimilar. Time series chain has the great potential to reveal latent unusual evolving trend in the time series, or identify precursor of important events in a complex system. Unfortunately, existing definitions of time series chains only consider finding chains in a single time series. As a result, they are likely to miss unexpected evolving patterns in interrupted time series, or across two related time series. To address this limitation, in this work, we introduce a new definition called \textit{Joint Time Series Chain}, which is specially designed for the task of finding unexpected evolving trend across interrupted time series or two related time series. Our definition focuses on mitigating the robustness issues caused by the gap or interruption in the time series. We further propose an effective ranking criterion to identify the best chain. We demonstrate that our proposed approach outperforms existing TSC work in locating unusual evolving patterns through extensive empirical evaluations. We further demonstrate the utility of our work with a real-life manufacturing application from Intel. Our source code is publicly available at the supporting page https://github.com/lizhang-ts/JointTSC .

</details>


### [411] [Cumulative Utility Parity for Fair Federated Learning under Intermittent Client Participation](https://arxiv.org/abs/2602.13651)
*Stefan Behfar,Richard Mortier*

Main category: cs.LG

TL;DR: Proposes cumulative utility parity for federated learning fairness, addressing systematic under-representation of intermittently available clients by evaluating long-term benefit per participation opportunity rather than per training round.


<details>
  <summary>Details</summary>
Motivation: Existing FL fairness approaches focus on equalizing loss/accuracy conditional on participation, assuming comparable participation opportunities. However, when participation is uneven (intermittent, heterogeneous, correlated with data/resource constraints), these approaches lead to systematic under-representation of intermittently available clients despite per-round fairness.

Method: Introduces cumulative utility parity principle and operationalizes it through availability-normalized cumulative utility. This disentangles unavoidable physical constraints from avoidable algorithmic bias in scheduling and aggregation.

Result: Experiments on temporally skewed, non-IID federated benchmarks show the approach substantially improves long-term representation parity while maintaining near-perfect performance.

Conclusion: Fairness in FL should consider long-term benefit per participation opportunity rather than per-round performance, addressing systematic bias against intermittently available clients through availability-normalized cumulative utility.

Abstract: In real-world federated learning (FL) systems, client participation is intermittent, heterogeneous, and often correlated with data characteristics or resource constraints. Existing fairness approaches in FL primarily focus on equalizing loss or accuracy conditional on participation, implicitly assuming that clients have comparable opportunities to contribute over time. However, when participation itself is uneven, these objectives can lead to systematic under-representation of intermittently available clients, even if per-round performance appears fair. We propose cumulative utility parity, a fairness principle that evaluates whether clients receive comparable long-term benefit per participation opportunity, rather than per training round. To operationalize this notion, we introduce availability-normalized cumulative utility, which disentangles unavoidable physical constraints from avoidable algorithmic bias arising from scheduling and aggregation. Experiments on temporally skewed, non-IID federated benchmarks demonstrate that our approach substantially improves long-term representation parity, while maintaining near-perfect performance.

</details>


### [412] [Zero-Order Optimization for LLM Fine-Tuning via Learnable Direction Sampling](https://arxiv.org/abs/2602.13659)
*Valery Parfenov,Grigoriy Evseev,Andrey Veprikov,Nikolay Bushkov,Stanislav Moiseev,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: A policy-driven zero-order framework that learns sampling distributions over perturbation directions to reduce variance and improve LLM fine-tuning performance with memory efficiency.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning large language models requires substantial memory for backpropagation and optimizer states, limiting deployment in resource-constrained settings. Zero-order methods offer memory savings but suffer from high variance and poor scaling with parameter dimensionality.

Method: Proposes a policy-driven ZO framework that treats the sampling distribution over perturbation directions as a learnable policy. Develops a practical algorithm that updates this distribution to reduce variance of directional estimates, improving gradient information quality and relaxing dimensionality dependence.

Result: Theoretical analysis shows learned sampling distributions improve gradient quality and relax explicit dependence on parameter dimensionality in convergence bounds. Empirical validation on challenging LLM fine-tuning benchmarks demonstrates substantially improved performance compared to standard ZO baselines.

Conclusion: Adaptive direction sampling is a promising approach to make zero-order fine-tuning viable at scale, offering memory-efficient LLM adaptation with improved performance over classical ZO methods.

Abstract: Fine-tuning large pretrained language models (LLMs) is a cornerstone of modern NLP, yet its growing memory demands (driven by backpropagation and large optimizer States) limit deployment in resource-constrained settings. Zero-order (ZO) methods bypass backpropagation by estimating directional derivatives from forward evaluations, offering substantial memory savings. However, classical ZO estimators suffer from high variance and an adverse dependence on the parameter dimensionality $d$, which has constrained their use to low-dimensional problems. In this work, we propose a policy-driven ZO framework that treats the sampling distribution over perturbation directions as a learnable policy and updates it to reduce the variance of directional estimates. We develop a practical algorithm implementing this idea and provide a theoretical analysis, showing that learned sampling distributions improve the quality of gradient information and relax the explicit dependence on $d$ in convergence bounds. Empirically, we validate the approach on challenging LLM fine-tuning benchmarks, demonstrating substantially improved performance compared to standard ZO baselines. Our results suggest that adaptive direction sampling is a promising route to make ZO fine-tuning viable at scale. The source code is available at https://github.com/brain-lab-research/zo_ldsd

</details>


### [413] [Optimized Certainty Equivalent Risk-Controlling Prediction Sets](https://arxiv.org/abs/2602.13660)
*Jiayi Huang,Amirmohammad Farzaneh,Osvaldo Simeone*

Main category: cs.LG

TL;DR: OCE-RCPS: A novel risk-controlling prediction set framework using optimized certainty equivalent risk measures (like CVaR) with high-probability guarantees for safety-critical applications like medical image segmentation.


<details>
  <summary>Details</summary>
Motivation: Current risk-controlling prediction sets (RCPS) only provide guarantees on expected risk, failing to capture tail behavior and worst-case scenarios crucial for safety-critical applications like medical image segmentation where reliability beyond average performance is essential.

Method: Introduces optimized certainty equivalent RCPS (OCE-RCPS) framework that uses upper confidence bounds to identify prediction set parameters satisfying user-specified risk tolerance levels with provable reliability for OCE risk measures including CVaR and entropic risk.

Result: Theoretical guarantees show OCE-RCPS satisfies probabilistic constraints for loss functions like miscoverage and false negative rate. Experiments on image segmentation demonstrate OCE-RCPS consistently meets target satisfaction rates across various risk measures, while existing OCE-CRC fails to provide probabilistic guarantees.

Conclusion: OCE-RCPS provides a robust framework for safety-critical applications by offering high-probability guarantees on tail risk measures, addressing limitations of conventional RCPS that only control expected risk.

Abstract: In safety-critical applications such as medical image segmentation, prediction systems must provide reliability guarantees that extend beyond conventional expected loss control. While risk-controlling prediction sets (RCPS) offer probabilistic guarantees on the expected risk, they fail to capture tail behavior and worst-case scenarios that are crucial in high-stakes settings. This paper introduces optimized certainty equivalent RCPS (OCE-RCPS), a novel framework that provides high-probability guarantees on general optimized certainty equivalent (OCE) risk measures, including conditional value-at-risk (CVaR) and entropic risk. OCE-RCPS leverages upper confidence bounds to identify prediction set parameters that satisfy user-specified risk tolerance levels with provable reliability. We establish theoretical guarantees showing that OCE-RCPS satisfies the desired probabilistic constraint for loss functions such as miscoverage and false negative rate. Experiments on image segmentation demonstrate that OCE-RCPS consistently meets target satisfaction rates across various risk measures and reliability configurations, while OCE-CRC fails to provide probabilistic guarantees.

</details>


### [414] [ALMo: Interactive Aim-Limit-Defined, Multi-Objective System for Personalized High-Dose-Rate Brachytherapy Treatment Planning and Visualization for Cervical Cancer](https://arxiv.org/abs/2602.13666)
*Edward Chen,Natalie Dullerud,Pang Wei Koh,Thomas Niedermayr,Elizabeth Kidd,Sanmi Koyejo,Carlos Guestrin*

Main category: cs.LG

TL;DR: ALMo is an interactive decision support system for HDR brachytherapy planning that uses aim-limit thresholds to automate parameter setup and enable clinicians to navigate Pareto tradeoffs between tumor coverage and organ sparing.


<details>
  <summary>Details</summary>
Motivation: Clinicians face cognitive challenges in tracking competing metrics with aim and limit thresholds in complex clinical decisions like HDR brachytherapy planning, leading to variability and inefficiency in treatment planning.

Method: ALMo employs a novel optimization framework that minimizes manual input through automated parameter setup, allows flexible control over toxicity risks, and enables clinicians to navigate Pareto surfaces by directly manipulating intuitive aim and limit values.

Result: In 25 clinical cases, ALMo generated plans meeting or exceeding manual planning quality, with 65% showing dosimetric improvements, and reduced average planning time to ~17 minutes vs. conventional 30-60 minutes.

Conclusion: ALMo demonstrates a generalized framework for streamlining interaction in multi-criteria clinical decision-making, validated in brachytherapy but applicable to other clinical contexts.

Abstract: In complex clinical decision-making, clinicians must often track a variety of competing metrics defined by aim (ideal) and limit (strict) thresholds. Sifting through these high-dimensional tradeoffs to infer the optimal patient-specific strategy is cognitively demanding and historically prone to variability. In this paper, we address this challenge within the context of High-Dose-Rate (HDR) brachytherapy for cervical cancer, where planning requires strictly managing radiation hot spots while balancing tumor coverage against organ sparing. We present ALMo (Aim-Limit-defined Multi-Objective system), an interactive decision support system designed to infer and operationalize clinician intent. ALMo employs a novel optimization framework that minimizes manual input through automated parameter setup and enables flexible control over toxicity risks. Crucially, the system allows clinicians to navigate the Pareto surface of dosimetric tradeoffs by directly manipulating intuitive aim and limit values. In a retrospective evaluation of 25 clinical cases, ALMo generated treatment plans that consistently met or exceeded manual planning quality, with 65% of cases demonstrating dosimetric improvements. Furthermore, the system significantly enhanced efficiency, reducing average planning time to approximately 17 minutes, compared to the conventional 30-60 minutes. While validated in brachytherapy, ALMo demonstrates a generalized framework for streamlining interaction in multi-criteria clinical decision-making.

</details>


### [415] [Advancing Analytic Class-Incremental Learning through Vision-Language Calibration](https://arxiv.org/abs/2602.13670)
*Binyu Zhao,Wei Zhang,Xingrui Yu,Zhaonian Zou,Ivor Tsang*

Main category: cs.LG

TL;DR: VILA proposes a dual-branch vision-language calibration framework for class-incremental learning with pre-trained models, addressing representation rigidity through geometric feature calibration and cross-modal decision rectification while maintaining analytic learning efficiency.


<details>
  <summary>Details</summary>
Motivation: Class-incremental learning with pre-trained models faces a critical trade-off between efficient adaptation and long-term stability. Analytic learning enables rapid updates but suffers from accumulated errors and feature incompatibility. The paper identifies representation rigidity as the primary bottleneck in PTM-based analytic CIL.

Method: VILA is a novel dual-branch framework with two-level vision-language calibration: 1) Feature-level geometric calibration that fuses plastic, task-adapted features with a frozen universal semantic anchor, and 2) Decision-level cross-modal priors to rectify prediction bias. This maintains analytic learning's efficiency while overcoming its brittleness.

Result: Extensive experiments across eight benchmarks demonstrate that VILA consistently yields superior performance, particularly in fine-grained and long-sequence scenarios. The framework harmonizes high-fidelity prediction with the simplicity of analytic learning.

Conclusion: VILA successfully addresses the representation rigidity problem in PTM-based analytic CIL through vision-language calibration, achieving both efficiency and stability in class-incremental learning while maintaining the benefits of analytic learning.

Abstract: Class-incremental learning (CIL) with pre-trained models (PTMs) faces a critical trade-off between efficient adaptation and long-term stability. While analytic learning enables rapid, recursive closed-form updates, its efficacy is often compromised by accumulated errors and feature incompatibility. In this paper, we first conduct a systematic study to dissect the failure modes of PTM-based analytic CIL, identifying representation rigidity as the primary bottleneck. Motivated by these insights, we propose \textbf{VILA}, a novel dual-branch framework that advances analytic CIL via a two-level vision-language calibration strategy. Specifically, we coherently fuse plastic, task-adapted features with a frozen, universal semantic anchor at the feature level through geometric calibration, and leverage cross-modal priors at the decision level to rectify prediction bias. This confluence maintains analytic-learning's extreme efficiency while overcoming its inherent brittleness. Extensive experiments across eight benchmarks demonstrate that VILA consistently yields superior performance, particularly in fine-grained and long-sequence scenarios. Our framework harmonizes high-fidelity prediction with the simplicity of analytic learning. Our code is available at https://github.com/byzhaoAI/VILA

</details>


### [416] [On the Sparsifiability of Correlation Clustering: Approximation Guarantees under Edge Sampling](https://arxiv.org/abs/2602.13684)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: This paper studies sparsification-approximation trade-offs for Correlation Clustering, showing that pseudometric instances allow efficient approximations with sparse edge information, while general instances require near-complete information.


<details>
  <summary>Details</summary>
Motivation: Traditional LP-based approximation guarantees for Correlation Clustering require Θ(n³) triangle inequality constraints, which are computationally prohibitive at scale. The paper aims to understand how much edge information is needed to retain LP-based guarantees through sparsification.

Method: The paper establishes a structural dichotomy between pseudometric and general weighted instances. For pseudometric instances, it proves VC dimension results yielding additive ε-coresets, analyzes LP vertex structure to enable cutting-plane solvers, and develops a sparsified LP-PIVOT algorithm that imputes missing marginals via triangle inequalities.

Result: For pseudometric instances: VC dimension is exactly n-1, enabling optimal ε-coresets; at most binom(n,2) triangle inequalities are active at LP vertices; sparsified LP-PIVOT achieves 10/3-approximation with Õ(n^{3/2}) edges. For general instances: any algorithm observing o(n) random edges has unbounded approximation ratio.

Conclusion: The pseudometric condition is crucial for both tractability and robustness to incomplete information in Correlation Clustering. There's a sharp threshold at Õ(n^{3/2}) edges for pseudometric instances, while general instances require near-complete edge information for bounded approximation.

Abstract: Correlation Clustering (CC) is a fundamental unsupervised learning primitive whose strongest LP-based approximation guarantees require $Θ(n^3)$ triangle inequality constraints and are prohibitive at scale. We initiate the study of \emph{sparsification--approximation trade-offs} for CC, asking how much edge information is needed to retain LP-based guarantees. We establish a structural dichotomy between pseudometric and general weighted instances. On the positive side, we prove that the VC dimension of the clustering disagreement class is exactly $n{-}1$, yielding additive $\varepsilon$-coresets of optimal size $\tilde{O}(n/\varepsilon^2)$; that at most $\binom{n}{2}$ triangle inequalities are active at any LP vertex, enabling an exact cutting-plane solver; and that a sparsified variant of LP-PIVOT, which imputes missing LP marginals via triangle inequalities, achieves a robust $\frac{10}{3}$-approximation (up to an additive term controlled by an empirically computable imputation-quality statistic $\overlineΓ_w$) once $\tildeΘ(n^{3/2})$ edges are observed, a threshold we prove is sharp. On the negative side, we show via Yao's minimax principle that without pseudometric structure, any algorithm observing $o(n)$ uniformly random edges incurs an unbounded approximation ratio, demonstrating that the pseudometric condition governs not only tractability but also the robustness of CC to incomplete information.

</details>


### [417] [Physics Aware Neural Networks: Denoising for Magnetic Navigation](https://arxiv.org/abs/2602.13690)
*Aritra Das,Yashas Shende,Muskaan Chugh,Reva Laxmi Chauhan,Arghya Pathak,Debayan Gupta*

Main category: cs.LG

TL;DR: A physics-constrained deep learning framework for magnetic-anomaly navigation that enforces divergence-free and E(3)-equivariance constraints to handle aircraft-induced magnetic noise, using Contiformer architecture and synthetic data generation.


<details>
  <summary>Details</summary>
Motivation: Magnetic-anomaly navigation is a GPS alternative, but aircraft-induced magnetic noise corrupts geomagnetic field data. Classical Tolles-Lawson model inadequately handles stochastic noise in navigation scenarios.

Method: Proposed framework uses two physics-based constraints: divergence-free vector field (via neural network outputting vector potential A, with magnetic field as its curl) and E(3)-equivariance (using tensor products of geometric tensors with spherical harmonics). Uses Contiformer architecture for continuous-time dynamics and long-term memory. Generates synthetic datasets with WMM and time-series conditional GANs.

Result: Physics constraints significantly improve predictive accuracy and physical plausibility. Contiformer outperforms state-of-the-art methods (CNNs, MLPs, Liquid Time Constant models). Ablation studies show benefits of individual and joint constraints.

Conclusion: Embedding physical constraints acts as implicit regularizer, improving spatio-temporal performance for magnetic-anomaly navigation. The approach outperforms classical and unconstrained deep learning methods, addressing data scarcity through synthetic data generation.

Abstract: Magnetic-anomaly navigation, leveraging small-scale variations in the Earth's magnetic field, is a promising alternative when GPS is unavailable or compromised. Airborne systems face a key challenge in extracting geomagnetic field data: the aircraft itself induces magnetic noise. Although the classical Tolles-Lawson model addresses this, it inadequately handles stochastically corrupted magnetic data required for navigation. To address stochastic noise, we propose a framework based on two physics-based constraints: divergence-free vector field and E(3)-equivariance. These ensure the learned magnetic field obeys Maxwell's equations and that outputs transform correctly with sensor position/orientation. The divergence-free constraint is implemented by training a neural network to output a vector potential $A$, with the magnetic field defined as its curl. For E(3)-equivariance, we use tensor products of geometric tensors representable via spherical harmonics with known rotational transformations. Enforcing physical consistency and restricting the admissible function space acts as an implicit regularizer that improves spatio-temporal performance. We present ablation studies evaluating each constraint alone and jointly across CNNs, MLPs, Liquid Time Constant models, and Contiformers. Continuous-time dynamics and long-term memory are critical for modelling magnetic time series; the Contiformer architecture, which provides both, outperforms state-of-the-art methods. To mitigate data scarcity, we generate synthetic datasets using the World Magnetic Model (WMM) with time-series conditional GANs, producing realistic, temporally consistent magnetic sequences across varied trajectories and environments. Experiments show that embedding these constraints significantly improves predictive accuracy and physical plausibility, outperforming classical and unconstrained deep learning approaches.

</details>


### [418] [Attention Head Entropy of LLMs Predicts Answer Correctness](https://arxiv.org/abs/2602.13699)
*Sophie Ostmeier,Brian Axelrod,Maya Varma,Asad Aali,Yabin Zhang,Magdalini Paschali,Sanmi Koyejo,Curtis Langlotz,Akshay Chaudhari*

Main category: cs.LG

TL;DR: Head Entropy method predicts answer correctness by analyzing attention entropy patterns in LLMs, outperforming baselines in both in-distribution and out-of-domain generalization.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate plausible but incorrect answers, especially dangerous in safety-critical domains like medicine. Existing evaluation methods are either expensive (human evaluation) or risk hidden errors (LLM-as-judge). White-box methods using model internals exist but their ability to predict answer correctness and generalize out-of-domain remains unclear.

Method: Head Entropy analyzes attention entropy patterns in LLMs, specifically measuring the spread of attention mass. Uses sparse logistic regression on per-head 2-Renyi entropies to predict answer correctness. Can work with attention patterns from just the question/context before answer generation.

Result: Head Entropy matches or exceeds baselines in-distribution and generalizes substantially better out-of-domain, outperforming the closest baseline by +8.5% AUROC on average. Attention patterns from question/context alone already provide predictive signal, achieving +17.7% AUROC over closest baseline. Evaluated across 5 instruction-tuned LLMs and 3 QA datasets spanning general knowledge, multi-hop reasoning, and medicine.

Conclusion: Attention entropy patterns provide reliable signals for predicting answer correctness in LLMs, with strong generalization capabilities across domains. The method offers a promising white-box approach for detecting incorrect answers without relying on expensive human evaluation or error-prone LLM-as-judge methods.

Abstract: Large language models (LLMs) often generate plausible yet incorrect answers, posing risks in safety-critical settings such as medicine. Human evaluation is expensive, and LLM-as-judge approaches risk introducing hidden errors. Recent white-box methods detect contextual hallucinations using model internals, focusing on the localization of the attention mass, but two questions remain open: do these approaches extend to predicting answer correctness, and do they generalize out-of-domains? We introduce Head Entropy, a method that predicts answer correctness from attention entropy patterns, specifically measuring the spread of the attention mass. Using sparse logistic regression on per-head 2-Renyi entropies, Head Entropy matches or exceeds baselines in-distribution and generalizes substantially better on out-of-domains, it outperforms the closest baseline on average by +8.5% AUROC. We further show that attention patterns over the question/context alone, before answer generation, already carry predictive signal using Head Entropy with on average +17.7% AUROC over the closest baseline. We evaluate across 5 instruction-tuned LLMs and 3 QA datasets spanning general knowledge, multi-hop reasoning, and medicine.

</details>


### [419] [Optimal Regret for Policy Optimization in Contextual Bandits](https://arxiv.org/abs/2602.13700)
*Orin Levy,Yishay Mansour*

Main category: cs.LG

TL;DR: First high-probability optimal regret bound for policy optimization in stochastic contextual bandits with general offline function approximation, achieving Õ(√(K|A|log|F|)) regret.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between theory and practice for widely used policy optimization methods in contextual bandits, providing rigorous optimal regret guarantees that match practical performance.

Method: Policy optimization technique applied to stochastic contextual multi-armed bandit (CMAB) with general offline function approximation, using function class F to approximate losses.

Result: Achieves optimal regret bound of Õ(√(K|A|log|F|)) where K is number of rounds, A is set of arms, and F is function class; algorithm is efficient and supported by empirical evaluation.

Conclusion: Demonstrates that practical policy optimization methods for contextual bandits can achieve rigorously-proved optimal regret bounds, closing theory-practice gap for this important problem.

Abstract: We present the first high-probability optimal regret bound for a policy optimization technique applied to the problem of stochastic contextual multi-armed bandit (CMAB) with general offline function approximation. Our algorithm is both efficient and achieves an optimal regret bound of $\widetilde{O}(\sqrt{ K|\mathcal{A}|\log|\mathcal{F}|})$, where $K$ is the number of rounds, $\mathcal{A}$ is the set of arms, and $\mathcal{F}$ is the function class used to approximate the losses. Our results bridge the gap between theory and practice, demonstrating that the widely used policy optimization methods for the contextual bandit problem can achieve a rigorously-proved optimal regret bound. We support our theoretical results with an empirical evaluation of our algorithm.

</details>


### [420] [Near-Optimal Regret for Policy Optimization in Contextual MDPs with General Offline Function Approximation](https://arxiv.org/abs/2602.13706)
*Orin Levy,Aviv Rosenberg,Alon Cohen,Yishay Mansour*

Main category: cs.LG

TL;DR: OPO-CMDP is the first policy optimization algorithm for stochastic Contextual MDPs under general offline function approximation, achieving optimal regret bounds with improved dependence on state/action spaces.


<details>
  <summary>Details</summary>
Motivation: Current methods for Contextual Markov Decision Processes (CMDPs) have suboptimal dependence on state and action space sizes. There's a need for algorithms with optimal theoretical guarantees and computational efficiency.

Method: OPO-CMDP uses optimistic policy optimization with general offline function approximation, leveraging finite function classes to approximate losses and dynamics.

Result: Achieves high probability regret bound of Õ(H⁴√(T|S||A|log(|F||P|))), which is the first bound with optimal dependence on |S| and |A|, improving upon Qian, Hu, and Simchi-Levi (2024).

Conclusion: Optimistic policy optimization provides a natural, computationally superior, and theoretically near-optimal approach for solving CMDPs under general offline function approximation.

Abstract: We introduce \texttt{OPO-CMDP}, the first policy optimization algorithm for stochastic Contextual Markov Decision Process (CMDPs) under general offline function approximation. Our approach achieves a high probability regret bound of $\widetilde{O}(H^4\sqrt{T|S||A|\log(|\mathcal{F}||\mathcal{P}|)}),$ where $S$ and $A$ denote the state and action spaces, $H$ the horizon length, $T$ the number of episodes, and $\mathcal{F}, \mathcal{P}$ the finite function classes used to approximate the losses and dynamics, respectively. This is the first regret bound with optimal dependence on $|S|$ and $|A|$, directly improving the current state-of-the-art (Qian, Hu, and Simchi-Levi, 2024). These results demonstrate that optimistic policy optimization provides a natural, computationally superior and theoretically near-optimal path for solving CMDPs.

</details>


### [421] [HBVLA: Pushing 1-Bit Post-Training Quantization for Vision-Language-Action Models](https://arxiv.org/abs/2602.13710)
*Xin Yan,Zhenglin Wan,Feiyang Ye,Xingrui Yu,Hangyu Du,Yang You,Ivor Tsang*

Main category: cs.LG

TL;DR: HBVLA is a binarization framework for Vision-Language-Action models that uses policy-aware Hessian analysis and sparse orthogonal transforms to achieve 1-bit quantization with minimal performance degradation, enabling efficient deployment on resource-constrained robots.


<details>
  <summary>Details</summary>
Motivation: VLA models are computationally expensive for resource-constrained robots. Existing binarization methods suffer from distribution gaps between binarized and full-precision weights, causing error accumulation in long-horizon tasks and degraded action quality.

Method: 1) Policy-aware enhanced Hessian identifies action-critical weights. 2) Sparse orthogonal transform for non-salient weights creates low-entropy intermediate state. 3) Group-wise 1-bit quantization in Harr domain for both salient and non-salient weights.

Result: Quantized OpenVLA-OFT retains 92.2% performance on LIBERO, CogAct retains 93.6% on SimplerEnv, outperforming SOTA binarization methods. Real-world evaluation shows only marginal success-rate degradation compared to full-precision models.

Conclusion: HBVLA provides practical ultra-low-bit quantization for VLAs, enabling reliable deployment on hardware-limited robotic platforms with robust performance under tight hardware constraints.

Abstract: Vision-Language-Action (VLA) models enable instruction-following embodied control, but their large compute and memory footprints hinder deployment on resource-constrained robots and edge platforms. While reducing weights to 1-bit precision through binarization can greatly improve efficiency, existing methods fail to narrow the distribution gap between binarized and full-precision weights, causing quantization errors to accumulate under long-horizon closed-loop execution and severely degrade actions. To fill this gap, we propose HBVLA, a VLA-tailored binarization framework. First, we use a policy-aware enhanced Hessian to identify weights that are truly critical for action generation. Then, we employ a sparse orthogonal transform for non-salient weights to induce a low-entropy intermediate state. Finally, we quantize both salient and non-salient weights in the Harr domain with group-wise 1-bit quantization. We have evaluated our approach on different VLAs: on LIBERO, quantized OpenVLA-OFT retains 92.2% of full-precision performance; on SimplerEnv, quantized CogAct retains 93.6%, significantly outperforming state-of-the-art binarization methods. We further validate our method on real-world evaluation suite and the results show that HBVLA incurs only marginal success-rate degradation compared to the full-precision model, demonstrating robust deployability under tight hardware constraints. Our work provides a practical foundation for ultra-low-bit quantization of VLAs, enabling more reliable deployment on hardware-limited robotic platforms.

</details>


### [422] [sleep2vec: Unified Cross-Modal Alignment for Heterogeneous Nocturnal Biosignals](https://arxiv.org/abs/2602.13857)
*Weixuan Yuan,Zengrui Jin,Yichen Wang,Donglin Xie,Ziyi Ye,Chao Zhang,Xuesong Chen*

Main category: cs.LG

TL;DR: sleep2vec is a foundation model for diverse and incomplete nocturnal biosignals that learns shared representations via cross-modal alignment, handling device heterogeneity and sensor dropout while enabling label-efficient modeling.


<details>
  <summary>Details</summary>
Motivation: Traditional sleep monitoring uses diverse PSG devices, bedside monitors, and wearables capturing various biosignals, but device heterogeneity and frequent sensor dropout create challenges for unified multimodal signal modeling.

Method: Contrastive pre-training on 42,249 overnight recordings across nine modalities using a Demography, Age, Site & History-aware InfoNCE objective that incorporates physiological and acquisition metadata to dynamically weight negatives and mitigate cohort-specific shortcuts.

Result: sleep2vec consistently outperforms strong baselines on downstream sleep staging and clinical outcome assessment, remains robust to any subset of available modalities and sensor dropout, and establishes scaling laws for nocturnal biosignals with respect to modality diversity and model capacity.

Conclusion: Unified cross-modal alignment with principled scaling enables label-efficient, general-purpose modeling of real-world nocturnal biosignals, addressing key challenges in sleep monitoring and clinical applications.

Abstract: Tasks ranging from sleep staging to clinical diagnosis traditionally rely on standard polysomnography (PSG) devices, bedside monitors and wearable devices, which capture diverse nocturnal biosignals (e.g., EEG, EOG, ECG, SpO$_2$). However, heterogeneity across devices and frequent sensor dropout pose significant challenges for unified modelling of these multimodal signals. We present \texttt{sleep2vec}, a foundation model for diverse and incomplete nocturnal biosignals that learns a shared representation via cross-modal alignment. \texttt{sleep2vec} is contrastively pre-trained on 42,249 overnight recordings spanning nine modalities using a \textit{Demography, Age, Site \& History-aware InfoNCE} objective that incorporates physiological and acquisition metadata (\textit{e.g.}, age, gender, recording site) to dynamically weight negatives and mitigate cohort-specific shortcuts. On downstream sleep staging and clinical outcome assessment, \texttt{sleep2vec} consistently outperforms strong baselines and remains robust to any subset of available modalities and sensor dropout. We further characterize, to our knowledge for the first time, scaling laws for nocturnal biosignals with respect to modality diversity and model capacity. Together, these results show that unified cross-modal alignment, coupled with principled scaling, enables label-efficient, general-purpose modelling of real-world nocturnal biosignals.

</details>


### [423] [Data-driven Bi-level Optimization of Thermal Power Systems with embedded Artificial Neural Networks](https://arxiv.org/abs/2602.13746)
*Talha Ansar,Muhammad Mujtaba Abbas,Ramit Debnath,Vivek Dua,Waqar Muhammad Ashraf*

Main category: cs.LG

TL;DR: A bi-level ML optimization framework using ANN models and KKT conditions for efficient hierarchical optimization of industrial thermal power systems.


<details>
  <summary>Details</summary>
Motivation: Industrial thermal power systems have coupled performance variables with hierarchical importance, making simultaneous optimization computationally challenging or infeasible, limiting scalable operation optimization.

Method: A fully machine learning-powered bi-level optimization framework where objective functions are approximated by ANN models, and the lower-level problem is analytically embedded through KKT optimality conditions (ANN-KKT framework).

Result: The framework achieves comparable solutions to bi-level benchmarks with marginal computational time (0.22-0.88s), yielding 583 MW (coal) and 402 MW (gas turbine) power output at optimal turbine heat rates. It also delineates feasible and robust operating envelopes accounting for uncertainty.

Conclusion: ANN-KKT offers a scalable and computationally efficient route for hierarchical, data-driven optimization of industrial thermal power systems, enabling energy-efficient operations and contributing to Industry 5.0.

Abstract: Industrial thermal power systems have coupled performance variables with hierarchical order of importance, making their simultaneous optimization computationally challenging or infeasible. This barrier limits the integrated and computationally scaleable operation optimization of industrial thermal power systems. To address this issue for large-scale engineering systems, we present a fully machine learning-powered bi-level optimization framework for data-driven optimization of industrial thermal power systems. The objective functions of upper and lower levels are approximated by artificial neural network (ANN) models and the lower-level problem is analytically embedded through Karush-Kuhn-Tucker (KKT) optimality conditions. The reformulated single level optimization framework integrating ANN models and KKT constraints (ANN-KKT) is validated on benchmark problems and on real-world power generation operation of 660 MW coal power plant and 395 MW gas turbine system. The results reveal a comparable solutions obtained from the proposed ANN-KKT framework to the bi-level solutions of the benchmark problems. Marginal computational time requirement (0.22 to 0.88 s) to compute optimal solutions yields 583 MW (coal) and 402 MW (gas turbine) of power output at optimal turbine heat rate of 7337 kJ/kWh and 7542 kJ/kWh, respectively. In addition, the method expands to delineate a feasible and robust operating envelope that accounts for uncertainty in operating variables while maximizing thermal efficiency in various scenarios. These results demonstrate that ANN-KKT offers a scalable and computationally efficient route for hierarchical, data-driven optimization of industrial thermal power systems, achieving energy-efficient operations of large-scale engineering systems and contributing to industry 5.0.

</details>


### [424] [Decentralized Federated Learning With Energy Harvesting Devices](https://arxiv.org/abs/2602.14051)
*Kai Zhang,Xuanyu Cao,Khaled B. Letaief*

Main category: cs.LG

TL;DR: Proposes a decentralized policy iteration algorithm for energy-harvesting federated learning that optimizes device scheduling and power control using only local two-hop neighbor information, achieving asymptotic optimality with reduced complexity.


<details>
  <summary>Details</summary>
Motivation: Energy-intensive operations in decentralized federated learning rapidly deplete device batteries, reducing operational lifetime and degrading learning performance. Energy harvesting offers sustainability but requires intelligent resource management to accelerate convergence.

Method: Derives convergence bound for wireless DFL with energy harvesting, formulates joint device scheduling and power control as multi-agent MDP, and proposes fully decentralized policy iteration algorithm using only local two-hop neighbor information.

Result: Theoretical analysis shows asymptotic optimality of the decentralized algorithm. Comprehensive numerical experiments on real-world datasets validate theoretical results and demonstrate algorithm effectiveness.

Conclusion: The proposed decentralized approach enables sustainable DFL with energy harvesting while maintaining convergence performance, overcoming scalability limitations of centralized MDP algorithms through local information exchange.

Abstract: Decentralized federated learning (DFL) enables edge devices to collaboratively train models through local training and fully decentralized device-to-device (D2D) model exchanges. However, these energy-intensive operations often rapidly deplete limited device batteries, reducing their operational lifetime and degrading the learning performance. To address this limitation, we apply energy harvesting technique to DFL systems, allowing edge devices to extract ambient energy and operate sustainably. We first derive the convergence bound for wireless DFL with energy harvesting, showing that the convergence is influenced by partial device participation and transmission packet drops, both of which further depend on the available energy supply. To accelerate convergence, we formulate a joint device scheduling and power control problem and model it as a multi-agent Markov decision process (MDP). Traditional MDP algorithms (e.g., value or policy iteration) require a centralized coordinator with access to all device states and exhibit exponential complexity in the number of devices, making them impractical for large-scale decentralized networks. To overcome these challenges, we propose a fully decentralized policy iteration algorithm that leverages only local state information from two-hop neighboring devices, thereby substantially reducing both communication overhead and computational complexity. We further provide a theoretical analysis showing that the proposed decentralized algorithm achieves asymptotic optimality. Finally, comprehensive numerical experiments on real-world datasets are conducted to validate the theoretical results and corroborate the effectiveness of the proposed algorithm.

</details>


### [425] [Discrete Double-Bracket Flows for Isotropic-Noise Invariant Eigendecomposition](https://arxiv.org/abs/2602.13759)
*ZhiMing Li,JiaHe Feng*

Main category: cs.LG

TL;DR: Matrix-free eigendecomposition method with isotropic shift invariance, achieving global convergence with improved sample complexity under trace-free perturbations.


<details>
  <summary>Details</summary>
Motivation: Standard stochastic approximation methods for eigendecomposition either use fixed steps that couple stability to covariance norm, or adapt steps in ways that slow down due to vanishing updates. The paper aims to overcome these limitations.

Method: Introduces a discrete double-bracket flow whose generator is invariant to isotropic shifts, making it pathwise invariant to isotropic noise at discrete-time level. Uses maximal stable step size proportional to 1/‖C_e‖₂², where C_e is trace-free covariance. Employs strict-saddle geometry analysis and input-to-state stability analysis.

Result: Establishes global convergence with sample complexity scaling as O(‖C_e‖₂²/(Δ²ε)) under trace-free perturbations. Achieves accelerated O(log(1/ζ)) saddle-escape rate and provides high-probability finite-time convergence guarantee through explicit characterization of degenerate blocks.

Conclusion: The proposed discrete double-bracket flow method provides an efficient matrix-free eigendecomposition algorithm that is invariant to isotropic noise, achieves global convergence with improved sample complexity, and offers theoretical guarantees for saddle-escape and finite-time convergence.

Abstract: We study matrix-free eigendecomposition under a matrix-vector product (MVP) oracle, where each step observes a covariance operator $C_k = C_{sig} + σ_k^2 I + E_k$. Standard stochastic approximation methods either use fixed steps that couple stability to $\|C_k\|_2$, or adapt steps in ways that slow down due to vanishing updates. We introduce a discrete double-bracket flow whose generator is invariant to isotropic shifts, yielding pathwise invariance to $σ_k^2 I$ at the discrete-time level. The resulting trajectory and a maximal stable step size $η_{max} \propto 1/\|C_e\|_2^2$ depend only on the trace-free covariance $C_e$. We establish global convergence via strict-saddle geometry for the diagonalization objective and an input-to-state stability analysis, with sample complexity scaling as $O(\|C_e\|_2^2 / (Δ^2 ε))$ under trace-free perturbations. An explicit characterization of degenerate blocks yields an accelerated $O(\log(1/ζ))$ saddle-escape rate and a high-probability finite-time convergence guarantee.

</details>


### [426] [On Representation Redundancy in Large-Scale Instruction Tuning Data Selection](https://arxiv.org/abs/2602.13773)
*Youwei Shu,Shaomian Zheng,Dingnan Jin,Wenjie Qu,Ziyao Guo,Qing Cui,Jun Zhou,Jiaheng Zhang*

Main category: cs.LG

TL;DR: CRDS framework improves instruction-tuning data selection by reducing redundancy in LLM embeddings through compressed representations, achieving strong performance with only 3.5% of data.


<details>
  <summary>Details</summary>
Motivation: Data quality is crucial for LLM training, but systematic methods for industrial-scale data selection in instruction tuning remain underexplored. Current LLM encoders produce highly redundant semantic embeddings that limit effective data selection.

Method: Proposes Compressed Representation Data Selection (CRDS) with two variants: CRDS-R uses Rademacher random projection with concatenated transformer hidden-layer representations, and CRDS-W employs whitening-based dimensionality reduction to improve representational quality.

Result: Both CRDS variants substantially enhance data quality and outperform state-of-the-art representation-based selection methods. CRDS-W achieves strong performance using only 3.5% of data, surpassing full-data baseline by average 0.71% across four datasets.

Conclusion: CRDS effectively addresses redundancy in LLM embeddings for data selection, enabling efficient instruction tuning with minimal high-quality data while maintaining or improving performance over full-dataset baselines.

Abstract: Data quality is a crucial factor in large language models training. While prior work has shown that models trained on smaller, high-quality datasets can outperform those trained on much larger but noisy or low-quality corpora, systematic methods for industrial-scale data selection in instruction tuning remain underexplored. In this work, we study instruction-tuning data selection through the lens of semantic representation similarity and identify a key limitation of state-of-the-art LLM encoders: they produce highly redundant semantic embeddings. To mitigate this redundancy, we propose Compressed Representation Data Selection (CRDS), a novel framework with two variants. CRDS-R applies Rademacher random projection followed by concatenation of transformer hidden-layer representations, while CRDS-W employs whitening-based dimensionality reduction to improve representational quality. Experimental results demonstrate that both variants substantially enhance data quality and consistently outperform state-of-the-art representation-based selection methods. Notably, CRDS-W achieves strong performance using only 3.5% of the data, surpassing the full-data baseline by an average of 0.71% across four datasets. Our code is available at https://github.com/tdano1/CRDS.

</details>


### [427] [MEMTS: Internalizing Domain Knowledge via Parameterized Memory for Retrieval-Free Domain Adaptation of Time Series Foundation Models](https://arxiv.org/abs/2602.13783)
*Xiaoyun Yu,Li fan,Xiangfei Qiu,Nanqing Dong,Yonggui Huang,Honggang Qi,Geguang Pu,Wanli Ouyang,Xi Chen,Jilin Hu*

Main category: cs.LG

TL;DR: MEMTS is a lightweight plug-and-play method for retrieval-free domain adaptation in time series forecasting that addresses catastrophic forgetting and high latency issues in current approaches.


<details>
  <summary>Details</summary>
Motivation: Time Series Foundation Models (TSFMs) degrade in real-world vertical domains due to temporal distribution shifts and domain-specific periodic structures. Current solutions (DAPT and RAG) have limitations: DAPT causes catastrophic forgetting of global temporal patterns, while RAG introduces substantial retrieval overhead that doesn't meet real-time stream processing requirements.

Method: MEMTS uses a Knowledge Persistence Module (KPM) that internalizes domain-specific temporal dynamics (seasonal patterns, trends) into a compact set of learnable latent prototypes. This transforms fragmented historical observations into continuous, parameterized knowledge representations without requiring architectural modifications to the frozen TSFM backbone.

Result: MEMTS achieves accurate domain adaptation with constant-time inference and near-zero latency while effectively mitigating catastrophic forgetting of general temporal patterns. Extensive experiments on multiple datasets demonstrate state-of-the-art performance.

Conclusion: MEMTS provides a lightweight, plug-and-play solution for retrieval-free domain adaptation in time series forecasting that addresses the scalability bottleneck of current approaches, enabling efficient real-time stream processing without compromising on performance.

Abstract: While Time Series Foundation Models (TSFMs) have demonstrated exceptional performance in generalized forecasting, their performance often degrades significantly when deployed in real-world vertical domains characterized by temporal distribution shifts and domain-specific periodic structures. Current solutions are primarily constrained by two paradigms: Domain-Adaptive Pretraining (DAPT), which improves short-term domain fitting but frequently disrupts previously learned global temporal patterns due to catastrophic forgetting; and Retrieval-Augmented Generation (RAG), which incorporates external knowledge but introduces substantial retrieval overhead. This creates a severe scalability bottleneck that fails to meet the high-efficiency requirements of real-time stream processing. To break this impasse, we propose Memory for Time Series (MEMTS), a lightweight and plug-and-play method for retrieval-free domain adaptation in time series forecasting. The key component of MEMTS is a Knowledge Persistence Module (KPM), which internalizes domain-specific temporal dynamics, such as recurring seasonal patterns and trends into a compact set of learnable latent prototypes. In doing so, it transforms fragmented historical observations into continuous, parameterized knowledge representations. This paradigm shift enables MEMTS to achieve accurate domain adaptation with constant-time inference and near-zero latency, while effectively mitigating catastrophic forgetting of general temporal patterns, all without requiring any architectural modifications to the frozen TSFM backbone. Extensive experiments on multiple datasets demonstrate the SOTA performance of MEMTS.

</details>


### [428] [Parameter-Minimal Neural DE Solvers via Horner Polynomials](https://arxiv.org/abs/2602.14737)
*T. Matulić,D. Seršić*

Main category: cs.LG

TL;DR: Horner-factorized polynomial networks with minimal parameters solve differential equations by enforcing initial conditions exactly and using piecewise extensions for better accuracy.


<details>
  <summary>Details</summary>
Motivation: To create resource-efficient neural architectures for solving differential equations that work well with minimal parameters while maintaining accuracy, addressing the trade-off between model complexity and computational resources in scientific modeling.

Method: Restrict hypothesis class to Horner-factorized polynomials to create implicit, differentiable trial solutions with few learnable coefficients. Enforce initial conditions exactly by fixing low-order polynomial degrees. Introduce piecewise "spline-like" extension using multiple small Horner models on subintervals with continuity constraints at boundaries.

Result: Horner networks with tens or fewer parameters accurately match solutions and derivatives on ODE benchmarks and heat-equation examples, outperforming small MLP and sinusoidal-representation baselines under same training settings.

Conclusion: The approach demonstrates a practical accuracy-parameter trade-off for resource-efficient scientific modeling, showing that minimal-parameter polynomial architectures can effectively solve differential equations while maintaining accuracy comparable to more complex models.

Abstract: We propose a parameter-minimal neural architecture for solving differential equations by restricting the hypothesis class to Horner-factorized polynomials, yielding an implicit, differentiable trial solution with only a small set of learnable coefficients. Initial conditions are enforced exactly by construction by fixing the low-order polynomial degrees of freedom, so training focuses solely on matching the differential-equation residual at collocation points. To reduce approximation error without abandoning the low-parameter regime, we introduce a piecewise ("spline-like") extension that trains multiple small Horner models on subintervals while enforcing continuity (and first-derivative continuity) at segment boundaries. On illustrative ODE benchmarks and a heat-equation example, Horner networks with tens (or fewer) parameters accurately match the solution and its derivatives and outperform small MLP and sinusoidal-representation baselines under the same training settings, demonstrating a practical accuracy-parameter trade-off for resource-efficient scientific modeling.

</details>


### [429] [MechPert: Mechanistic Consensus as an Inductive Bias for Unseen Perturbation Prediction](https://arxiv.org/abs/2602.13791)
*Marc Boubnovski Martell,Josefa Lia Stoisser,Lawrence Phillips,Aditya Misra,Robert Kitchen,Jesper Ferkinghoff-Borg,Jialin Yu,Philip Torr,Kaspar Märten*

Main category: cs.LG

TL;DR: MechPert: LLM-based framework for predicting transcriptional responses to genetic perturbations by generating directed regulatory hypotheses rather than relying on functional similarity or static knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: Existing methods for predicting transcriptional responses to genetic perturbations rely on static, potentially incomplete knowledge graphs or language models that retrieve associations based on symmetric co-occurrence in scientific text rather than directed regulatory logic.

Method: MechPert uses lightweight LLM agents to independently propose candidate regulators with confidence scores, then aggregates these through a consensus mechanism that filters spurious associations to produce weighted neighborhoods for downstream prediction.

Result: On Perturb-seq benchmarks across four human cell lines, MechPert improves Pearson correlation by up to 10.5% over similarity-based baselines for perturbation prediction in low-data regimes (N=50). For experimental design, MechPert-selected anchor genes outperform standard network centrality heuristics by up to 46% in well-characterized cell lines.

Conclusion: MechPert provides an effective framework for predicting transcriptional responses to unseen genetic perturbations by leveraging LLM agents to generate directed regulatory hypotheses, outperforming existing approaches in both prediction accuracy and experimental design applications.

Abstract: Predicting transcriptional responses to unseen genetic perturbations is essential for understanding gene regulation and prioritizing large-scale perturbation experiments. Existing approaches either rely on static, potentially incomplete knowledge graphs, or prompt language models for functionally similar genes, retrieving associations shaped by symmetric co-occurrence in scientific text rather than directed regulatory logic. We introduce MechPert, a lightweight framework that encourages LLM agents to generate directed regulatory hypotheses rather than relying solely on functional similarity. Multiple agents independently propose candidate regulators with associated confidence scores; these are aggregated through a consensus mechanism that filters spurious associations, producing weighted neighborhoods for downstream prediction. We evaluate MechPert on Perturb-seq benchmarks across four human cell lines. For perturbation prediction in low-data regimes ($N=50$ observed perturbations), MechPert improves Pearson correlation by up to 10.5\% over similarity-based baselines. For experimental design, MechPert-selected anchor genes outperform standard network centrality heuristics by up to 46\% in well-characterized cell lines.

</details>


### [430] [Cast-R1: Learning Tool-Augmented Sequential Decision Policies for Time Series Forecasting](https://arxiv.org/abs/2602.13802)
*Xiaoyu Tao,Mingyue Cheng,Chuang Jiang,Tian Gao,Huanjian Zhang,Yaguo Liu*

Main category: cs.LG

TL;DR: Cast-R1 reformulates time series forecasting as a sequential decision-making problem using a memory-based agentic framework with tool-augmented workflow and iterative refinement.


<details>
  <summary>Details</summary>
Motivation: Traditional model-centric forecasting approaches struggle in complex, evolving settings because they lack autonomous evidence acquisition, reasoning about future changes, and iterative refinement capabilities.

Method: Introduces a memory-based state management mechanism for accumulating contextual evidence, and uses a tool-augmented agentic workflow where an agent interacts with a modular toolkit to extract features, invoke lightweight models, perform reasoning-based prediction, and iteratively refine forecasts through self-reflection.

Result: Extensive experiments on multiple real-world time series datasets demonstrate the effectiveness of Cast-R1.

Conclusion: This work provides a practical step toward exploring agentic paradigms for time series modeling, offering a framework that enables autonomous evidence gathering, reasoning, and iterative refinement in forecasting.

Abstract: Time series forecasting has long been dominated by model-centric approaches that formulate prediction as a single-pass mapping from historical observations to future values. Despite recent progress, such formulations often struggle in complex and evolving settings, largely because most forecasting models lack the ability to autonomously acquire informative evidence, reason about potential future changes, or revise predictions through iterative decision processes. In this work, we propose Cast-R1, a learned time series forecasting framework that reformulates forecasting as a sequential decision-making problem. Cast-R1 introduces a memory-based state management mechanism that maintains decision-relevant information across interaction steps, enabling the accumulation of contextual evidence to support long-horizon reasoning. Building on this formulation, forecasting is carried out through a tool-augmented agentic workflow, in which the agent autonomously interacts with a modular toolkit to extract statistical features, invoke lightweight forecasting models for decision support, perform reasoning-based prediction, and iteratively refine forecasts through self-reflection. To train Cast-R1, we adopt a two-stage learning strategy that combines supervised fine-tuning with multi-turn reinforcement learning, together with a curriculum learning scheme that progressively increases task difficulty to improve policy learning. Extensive experiments on multiple real-world time series datasets demonstrate the effectiveness of Cast-R1. We hope this work provides a practical step towards further exploration of agentic paradigms for time series modeling. Our code is available at https://github.com/Xiaoyu-Tao/Cast-R1-TS.

</details>


### [431] [Fast Physics-Driven Untrained Network for Highly Nonlinear Inverse Scattering Problems](https://arxiv.org/abs/2602.13805)
*Yutong Du,Zicheng Liu,Yi Huang,Bazargul Matkerim,Bo Qi,Yali Zong,Peixian Han*

Main category: cs.LG

TL;DR: Real-time physics-driven Fourier-spectral solver achieves 100x speedup over untrained neural networks for electromagnetic inverse scattering by using spectral-domain dimensionality reduction.


<details>
  <summary>Details</summary>
Motivation: Untrained neural networks provide high-fidelity electromagnetic inverse scattering reconstruction but suffer from computational limitations due to high-dimensional spatial-domain optimization, preventing real-time applications.

Method: Proposes PDF solver with spectral-domain dimensionality reduction using truncated Fourier basis, contraction integral equation to handle high-contrast nonlinearity, contrast-compensated operator to correct spectral attenuation, and bridge-suppressing loss for boundary sharpness.

Result: Achieves sub-second reconstruction with 100-fold speedup over state-of-the-art UNNs, maintains robust performance under noise and antenna uncertainties, enabling real-time microwave imaging.

Conclusion: The PDF solver enables real-time electromagnetic inverse scattering through efficient spectral-domain optimization, overcoming computational bottlenecks of spatial-domain UNNs while maintaining reconstruction quality.

Abstract: Untrained neural networks (UNNs) offer high-fidelity electromagnetic inverse scattering reconstruction but are computationally limited by high-dimensional spatial-domain optimization. We propose a Real-Time Physics-Driven Fourier-Spectral (PDF) solver that achieves sub-second reconstruction through spectral-domain dimensionality reduction. By expanding induced currents using a truncated Fourier basis, the optimization is confined to a compact low-frequency parameter space supported by scattering measurements. The solver integrates a contraction integral equation (CIE) to mitigate high-contrast nonlinearity and a contrast-compensated operator (CCO) to correct spectral-induced attenuation. Furthermore, a bridge-suppressing loss is formulated to enhance boundary sharpness between adjacent scatterers. Numerical and experimental results demonstrate a 100-fold speedup over state-of-the-art UNNs with robust performance under noise and antenna uncertainties, enabling real-time microwave imaging applications.

</details>


### [432] [AnomaMind: Agentic Time Series Anomaly Detection with Tool-Augmented Reasoning](https://arxiv.org/abs/2602.13807)
*Xiaoyu Tao,Yuchong Wu,Mingyue Cheng,Ze Guo,Tian Gao*

Main category: cs.LG

TL;DR: AnomaMind: An agentic framework that reformulates time series anomaly detection as a sequential decision-making process with adaptive feature preparation, reasoning-aware detection, and iterative refinement.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection methods treat the task as purely discriminative prediction with fixed features, struggling with context-dependent or diverse anomaly patterns. They lack adaptive feature preparation, reasoning-aware detection, and iterative refinement during inference.

Method: Proposes AnomaMind, an agentic framework with structured workflow: coarse-to-fine anomaly localization, multi-turn tool interactions for adaptive feature preparation, and self-reflective refinement. Uses hybrid inference mechanism combining general-purpose models for tool interaction/refinement with reinforcement learning for core detection decisions under workflow-level feedback.

Result: Extensive experiments across diverse settings demonstrate that AnomaMind consistently improves anomaly detection performance.

Conclusion: AnomaMind successfully addresses limitations of existing methods by reformulating anomaly detection as an evidence-driven diagnostic process with adaptive reasoning, showing consistent performance improvements across various settings.

Abstract: Time series anomaly detection is critical in many real-world applications, where effective solutions must localize anomalous regions and support reliable decision-making under complex settings. However, most existing methods frame anomaly detection as a purely discriminative prediction task with fixed feature inputs, rather than an evidence-driven diagnostic process. As a result, they often struggle when anomalies exhibit strong context dependence or diverse patterns. We argue that these limitations stem from the lack of adaptive feature preparation, reasoning-aware detection, and iterative refinement during inference. To address these challenges, we propose AnomaMind, an agentic time series anomaly detection framework that reformulates anomaly detection as a sequential decision-making process. AnomaMind operates through a structured workflow that progressively localizes anomalous intervals in a coarse-to-fine manner, augments detection through multi-turn tool interactions for adaptive feature preparation, and refines anomaly decisions via self-reflection. The workflow is supported by a set of reusable tool engines, enabling context-aware diagnostic analysis. A key design of AnomaMind is an explicitly designed hybrid inference mechanism for tool-augmented anomaly detection. In this mechanism, general-purpose models are responsible for autonomous tool interaction and self-reflective refinement, while core anomaly detection decisions are learned through reinforcement learning under verifiable workflow-level feedback, enabling task-specific optimization within a flexible reasoning framework. Extensive experiments across diverse settings demonstrate that AnomaMind consistently improves anomaly detection performance. The code is available at https://anonymous.4open.science/r/AnomaMind.

</details>


### [433] [Mean Flow Policy with Instantaneous Velocity Constraint for One-step Action Generation](https://arxiv.org/abs/2602.13810)
*Guojian Zhan,Letian Tao,Pengcheng Wang,Yixiao Wang,Yiheng Li,Yuxin Chen,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: MVP is a new generative policy that models mean velocity field for fastest one-step action generation, achieving SOTA performance on robotic tasks with improved speed.


<details>
  <summary>Details</summary>
Motivation: Flow-based policies face trade-off between expressiveness and computational burden controlled by number of flow steps. Need faster, more expressive policy functions for RL.

Method: Propose mean velocity policy (MVP) that models mean velocity field for one-step action generation. Introduce instantaneous velocity constraint (IVC) during training to ensure high expressiveness and serve as crucial boundary condition.

Result: MVP achieves state-of-the-art success rates on Robomimic and OGBench robotic manipulation tasks. Delivers substantial improvements in training and inference speed over existing flow-based policy baselines.

Conclusion: MVP provides an effective solution to the expressiveness-efficiency trade-off in flow-based policies, enabling faster and more expressive generative policies for RL applications.

Abstract: Learning expressive and efficient policy functions is a promising direction in reinforcement learning (RL). While flow-based policies have recently proven effective in modeling complex action distributions with a fast deterministic sampling process, they still face a trade-off between expressiveness and computational burden, which is typically controlled by the number of flow steps. In this work, we propose mean velocity policy (MVP), a new generative policy function that models the mean velocity field to achieve the fastest one-step action generation. To ensure its high expressiveness, an instantaneous velocity constraint (IVC) is introduced on the mean velocity field during training. We theoretically prove that this design explicitly serves as a crucial boundary condition, thereby improving learning accuracy and enhancing policy expressiveness. Empirically, our MVP achieves state-of-the-art success rates across several challenging robotic manipulation tasks from Robomimic and OGBench. It also delivers substantial improvements in training and inference speed over existing flow-based policy baselines.

</details>


### [434] [Pawsterior: Variational Flow Matching for Structured Simulation-Based Inference](https://arxiv.org/abs/2602.13813)
*Jorge Carrasco-Pollo,Floor Eijkelboom,Jan-Willem van de Meent*

Main category: cs.LG

TL;DR: Pawsterior is a variational flow-matching framework for simulation-based inference that handles structured domains (bounded/hybrid variables) and discrete latent structure, improving stability and extending SBI to previously inaccessible problems.


<details>
  <summary>Details</summary>
Motivation: Standard flow-matching methods for SBI operate in unconstrained spaces, but many real-world problems involve structured domains (bounded physical parameters, hybrid discrete-continuous variables). This mismatch leads to inefficient learning and difficulty respecting physical constraints.

Method: Introduces endpoint-induced affine geometric confinement principle that incorporates domain geometry directly into inference via two-sided variational model. Generalizes CatFlow's geometric inductive bias and enables handling of discrete latent structure through variational parameterization.

Result: Improves numerical stability during sampling and leads to consistently better posterior fidelity, demonstrated by improved classifier two-sample test performance across standard SBI benchmarks. Enables SBI tasks involving discrete latent structure (e.g., switching systems) that were previously incompatible with conventional flow-matching.

Conclusion: Pawsterior extends flow-matching to a broader class of structured SBI problems by addressing both geometric constraints and discrete latent structure, making previously inaccessible problems tractable.

Abstract: We introduce Pawsterior, a variational flow-matching framework for improved and extended simulation-based inference (SBI). Many SBI problems involve posteriors constrained by structured domains, such as bounded physical parameters or hybrid discrete-continuous variables, yet standard flow-matching methods typically operate in unconstrained spaces. This mismatch leads to inefficient learning and difficulty respecting physical constraints. Our contributions are twofold. First, generalizing the geometric inductive bias of CatFlow, we formalize endpoint-induced affine geometric confinement, a principle that incorporates domain geometry directly into the inference process via a two-sided variational model. This formulation improves numerical stability during sampling and leads to consistently better posterior fidelity, as demonstrated by improved classifier two-sample test performance across standard SBI benchmarks. Second, and more importantly, our variational parameterization enables SBI tasks involving discrete latent structure (e.g., switching systems) that are fundamentally incompatible with conventional flow-matching approaches. By addressing both geometric constraints and discrete latent structure, Pawsterior extends flow-matching to a broader class of structured SBI problems that were previously inaccessible.

</details>


### [435] [Testing For Distribution Shifts with Conditional Conformal Test Martingales](https://arxiv.org/abs/2602.13848)
*Shalev Shaer,Yarin Bar,Drew Prinster,Yaniv Romano*

Main category: cs.LG

TL;DR: Proposes a sequential test for detecting arbitrary distribution shifts using conformal test martingales that avoids test-time contamination by using a fixed reference dataset instead of continually growing reference set.


<details>
  <summary>Details</summary>
Motivation: Existing conformal test martingale detectors suffer from test-time contamination where post-shift observations enter the reference set, diluting evidence for distribution shift and increasing detection delay.

Method: Uses a fixed null reference dataset instead of growing reference set, with robust martingale construction that accounts for estimation error in reference distribution from finite reference set, maintaining validity conditional on null reference data.

Result: Achieves anytime-valid type-I error control, asymptotic power one, bounded expected detection delay, and empirically detects shifts faster than standard CTMs.

Conclusion: Provides a powerful and reliable distribution-shift detector that avoids contamination by design, offering improved detection performance over existing methods.

Abstract: We propose a sequential test for detecting arbitrary distribution shifts that allows conformal test martingales (CTMs) to work under a fixed, reference-conditional setting. Existing CTM detectors construct test martingales by continually growing a reference set with each incoming sample, using it to assess how atypical the new sample is relative to past observations. While this design yields anytime-valid type-I error control, it suffers from test-time contamination: after a change, post-shift observations enter the reference set and dilute the evidence for distribution shift, increasing detection delay and reducing power.
  In contrast, our method avoids contamination by design by comparing each new sample to a fixed null reference dataset. Our main technical contribution is a robust martingale construction that remains valid conditional on the null reference data, achieved by explicitly accounting for the estimation error in the reference distribution induced by the finite reference set. This yields anytime-valid type-I error control together with guarantees of asymptotic power one and bounded expected detection delay. Empirically, our method detects shifts faster than standard CTMs, providing a powerful and reliable distribution-shift detector.

</details>


### [436] [Sufficient Conditions for Stability of Minimum-Norm Interpolating Deep ReLU Networks](https://arxiv.org/abs/2602.13910)
*Ouns El Harzli,Yoonsoo Nam,Ilja Kuzborskij,Bernardo Cuenca Grau,Ard A. Louis*

Main category: cs.LG

TL;DR: The paper analyzes algorithmic stability of deep ReLU homogeneous neural networks that achieve zero training error with minimum L2-norm parameters, finding stability depends on having a stable sub-network followed by a low-rank weight matrix layer.


<details>
  <summary>Details</summary>
Motivation: Algorithmic stability is a classical framework for analyzing generalization error, but has had limited success in analyzing deep neural networks. The authors aim to understand when overparameterized deep ReLU networks trained to achieve zero training error with minimum L2-norm parameters (minimum-norm interpolation) exhibit stability properties.

Method: The authors study deep ReLU homogeneous neural networks that achieve zero training error using parameters with the smallest L2 norm (minimum-norm interpolation). They investigate sufficient conditions for such networks to be stable, analyzing the relationship between stable sub-networks and the rank properties of subsequent weight matrices.

Result: Two key findings: 1) Networks are stable when they contain a (possibly small) stable sub-network followed by a layer with a low-rank weight matrix. 2) Networks are not guaranteed to be stable even when they contain a stable sub-network if the following layer is not low-rank. The low-rank assumption is supported by empirical and theoretical evidence showing training deep networks is biased toward low-rank weight matrices for minimum-norm interpolation and weight-decay regularization.

Conclusion: The study provides theoretical insights into when deep ReLU homogeneous neural networks with minimum-norm interpolation exhibit algorithmic stability, highlighting the crucial role of low-rank weight matrices following stable sub-networks. This contributes to understanding generalization in overparameterized deep learning models.

Abstract: Algorithmic stability is a classical framework for analyzing the generalization error of learning algorithms. It predicts that an algorithm has small generalization error if it is insensitive to small perturbations in the training set such as the removal or replacement of a training point. While stability has been demonstrated for numerous well-known algorithms, this framework has had limited success in analyses of deep neural networks. In this paper we study the algorithmic stability of deep ReLU homogeneous neural networks that achieve zero training error using parameters with the smallest $L_2$ norm, also known as the minimum-norm interpolation, a phenomenon that can be observed in overparameterized models trained by gradient-based algorithms. We investigate sufficient conditions for such networks to be stable. We find that 1) such networks are stable when they contain a (possibly small) stable sub-network, followed by a layer with a low-rank weight matrix, and 2) such networks are not guaranteed to be stable even when they contain a stable sub-network, if the following layer is not low-rank. The low-rank assumption is inspired by recent empirical and theoretical results which demonstrate that training deep neural networks is biased towards low-rank weight matrices, for minimum-norm interpolation and weight-decay regularization.

</details>


### [437] [GREPO: A Benchmark for Graph Neural Networks on Repository-Level Bug Localization](https://arxiv.org/abs/2602.13921)
*Juntong Wang,Libin Chen,Xiyuan Wang,Shijia Kang,Haotong Yang,Da Zheng,Muhan Zhang*

Main category: cs.LG

TL;DR: GREPO is the first GNN benchmark for repository-level bug localization, containing 86 Python repos and 47,294 bug-fixing tasks, showing GNNs outperform traditional retrieval methods.


<details>
  <summary>Details</summary>
Motivation: Repository-level bug localization is critical but challenging for standard LLMs due to context window limitations. Existing retrieval methods (keyword matching, text similarity, simple graph heuristics) are limited, and GNNs show promise but lack dedicated benchmarks for evaluation.

Method: Created GREPO benchmark with 86 Python repositories and 47,294 bug-fixing tasks, providing graph-based data structures ready for GNN processing. Evaluated various GNN architectures against established information retrieval baselines.

Result: GNNs demonstrated outstanding performance compared to traditional information retrieval baselines for repository-scale bug localization tasks.

Conclusion: GREPO establishes a foundation for future GNN research in bug localization, highlighting GNNs' potential to model complex repository-wide dependencies and overcome limitations of current approaches.

Abstract: Repository-level bug localization-the task of identifying where code must be modified to fix a bug-is a critical software engineering challenge. Standard Large Language Modles (LLMs) are often unsuitable for this task due to context window limitations that prevent them from processing entire code repositories. As a result, various retrieval methods are commonly used, including keyword matching, text similarity, and simple graph-based heuristics such as Breadth-First Search. Graph Neural Networks (GNNs) offer a promising alternative due to their ability to model complex, repository-wide dependencies; however, their application has been hindered by the lack of a dedicated benchmark. To address this gap, we introduce GREPO, the first GNN benchmark for repository-scale bug localization tasks. GREPO comprises 86 Python repositories and 47294 bug-fixing tasks, providing graph-based data structures ready for direct GNN processing. Our evaluation of various GNN architectures shows outstanding performance compared to established information retrieval baselines. This work highlights the potential of GNNs for bug localization and established GREPO as a foundation resource for future research, The code is available at https://github.com/qingpingmo/GREPO.

</details>


### [438] [Why Code, Why Now: Learnability, Computability, and the Real Limits of Machine Learning](https://arxiv.org/abs/2602.13934)
*Zhimin Zhao*

Main category: cs.LG

TL;DR: The paper proposes a 5-level hierarchy of learnability based on information structure, explaining why code generation scales better than reinforcement learning due to better feedback quality.


<details>
  <summary>Details</summary>
Motivation: To understand why code generation progresses more reliably than reinforcement learning, and to challenge the assumption that scaling alone will solve remaining ML challenges.

Method: Proposes a formal hierarchy of learnability based on information structure, distinguishing between expressibility, computability, and learnability, and analyzing their relationships.

Result: Establishes that code provides dense, local, verifiable feedback making it highly learnable, while RL problems lack this structure, explaining different scaling behaviors.

Conclusion: The ceiling on ML progress depends more on whether a task is learnable at all rather than model size, challenging the scaling assumption and explaining structural differences between code generation and RL.

Abstract: Code generation has progressed more reliably than reinforcement learning, largely because code has an information structure that makes it learnable. Code provides dense, local, verifiable feedback at every token, whereas most reinforcement learning problems do not. This difference in feedback quality is not binary but graded. We propose a five-level hierarchy of learnability based on information structure and argue that the ceiling on ML progress depends less on model size than on whether a task is learnable at all. The hierarchy rests on a formal distinction among three properties of computational problems (expressibility, computability, and learnability). We establish their pairwise relationships, including where implications hold and where they fail, and present a unified template that makes the structural differences explicit. The analysis suggests why supervised learning on code scales predictably while reinforcement learning does not, and why the common assumption that scaling alone will solve remaining ML challenges warrants scrutiny.

</details>


### [439] [A Multi-Agent Framework for Code-Guided, Modular, and Verifiable Automated Machine Learning](https://arxiv.org/abs/2602.13937)
*Dat Le,Duc-Cuong Le,Anh-Son Nguyen,Tuan-Dung Bui,Thu-Trang Nguyen,Son Nguyen,Hieu Dinh Vo*

Main category: cs.LG

TL;DR: iML is a multi-agent AutoML framework that replaces black-box prompting with code-guided, modular, and verifiable architecture to address hallucination and logic entanglement in LLM-based agents.


<details>
  <summary>Details</summary>
Motivation: Traditional AutoML frameworks are black boxes lacking flexibility and transparency, while recent LLM-based agents suffer from hallucinated logic and logic entanglement in monolithic code generation, leading to unrecoverable runtime failures.

Method: iML introduces three key innovations: (1) Code-Guided Planning using autonomous empirical profiling to create strategic blueprints, (2) Code-Modular Implementation that decouples preprocessing and modeling into specialized components with interface contracts, and (3) Code-Verifiable Integration with dynamic contract verification and iterative self-correction.

Result: iML achieves 85% valid submission rate and 45% competitive medal rate on MLE-BENCH (APS 0.77), outperforms other approaches by 38%-163% on iML-BENCH, and maintains 70% success rate under stripped task descriptions.

Conclusion: iML bridges the gap between stochastic generation and reliable engineering, marking a meaningful step toward truly automated machine learning through its code-guided, modular, and verifiable architectural paradigm.

Abstract: Automated Machine Learning (AutoML) has revolutionized the development of data-driven solutions; however, traditional frameworks often function as "black boxes", lacking the flexibility and transparency required for complex, real-world engineering tasks. Recent Large Language Model (LLM)-based agents have shifted toward code-driven approaches. However, they frequently suffer from hallucinated logic and logic entanglement, where monolithic code generation leads to unrecoverable runtime failures. In this paper, we present iML, a novel multi-agent framework designed to shift AutoML from black-box prompting to a code-guided, modular, and verifiable architectural paradigm. iML introduces three main ideas: (1) Code-Guided Planning, which synthesizes a strategic blueprint grounded in autonomous empirical profiling to eliminate hallucination; (2) Code-Modular Implementation, which decouples preprocessing and modeling into specialized components governed by strict interface contracts; and (3) Code-Verifiable Integration, which enforces physical feasibility through dynamic contract verification and iterative self-correction. We evaluate iML across MLE-BENCH and the newly introduced iML-BENCH, comprising a diverse range of real-world Kaggle competitions. The experimental results show iML's superiority over state-of-the-art agents, achieving a valid submission rate of 85% and a competitive medal rate of 45% on MLE-BENCH, with an average standardized performance score (APS) of 0.77. On iML-BENCH, iML significantly outperforms the other approaches by 38%-163% in APS. Furthermore, iML maintains a robust 70% success rate even under stripped task descriptions, effectively filling information gaps through empirical profiling. These results highlight iML's potential to bridge the gap between stochastic generation and reliable engineering, marking a meaningful step toward truly AutoML.

</details>


### [440] [An Adaptive Model Selection Framework for Demand Forecasting under Horizon-Induced Degradation to Support Business Strategy and Operations](https://arxiv.org/abs/2602.13939)
*Adolfo González,Víctor Parada*

Main category: cs.LG

TL;DR: AHSIV is a horizon-aware, regime-conditioned model selection framework that addresses ranking instability across forecast horizons in intermittent and variable demand environments.


<details>
  <summary>Details</summary>
Motivation: Business environments with structural demand intermittency, high variability, and multi-step planning horizons need robust model selection mechanisms. No forecasting model is universally dominant, and relative rankings vary across error metrics, demand regimes, and forecast horizons, creating ambiguity in multi-SKU decision contexts.

Method: AHSIV integrates scaled/absolute error metrics adjusted via Metric Degradation by Forecast Horizon (MDFH) procedure, structural demand classification, multi-objective Pareto dominance, and hierarchical bias refinement within a unified decision architecture.

Result: AHSIV achieves statistical equivalence with the strongest monometric baseline in aggregated performance while increasing horizon-specific best-model selection frequency. Evaluation on Walmart, M3, M4, and M5 datasets under multiple train-test partitions and twelve-step horizons.

Conclusion: Model selection in heterogeneous demand environments cannot be treated as static ranking; horizon-consistent, structurally adaptive mechanisms provide principled, operationally coherent solutions for multi-SKU forecasting.

Abstract: Business environments characterized by structural demand intermittency, high variability, and multi-step planning horizons require robust and reproducible model selection mechanisms. Empirical evidence shows that no forecasting model is universally dominant and that relative rankings vary across error metrics, demand regimes, and forecast horizons, generating ambiguity in multi-SKU decision contexts. This study proposes AHSIV (Adaptive Hybrid Selector for Intermittency and Variability), a horizon-aware and regime-conditioned model selection framework designed to address horizon-induced ranking instability. The proposed approach integrates scaled and absolute error metrics adjusted through a Metric Degradation by Forecast Horizon (MDFH) procedure, structural demand classification, multi-objective Pareto dominance, and hierarchical bias refinement within a unified decision architecture. The empirical evaluation is conducted on the Walmart, M3, M4, and M5 datasets under multiple train-test partition schemes and twelve-step forecasting horizons. Results indicate that AHSIV achieves statistical equivalence with the strongest monometric baseline in terms of aggregated performance while increasing the frequency of horizon-specific best-model selection. The findings demonstrate that model selection in heterogeneous demand environments cannot be treated as a static ranking problem, and that horizon-consistent, structurally adaptive mechanisms provide a principled, operationally coherent solution for multi-SKU forecasting.

</details>


### [441] [You Can Learn Tokenization End-to-End with Reinforcement Learning](https://arxiv.org/abs/2602.13940)
*Sam Dauncey,Roger Wattenhofer*

Main category: cs.LG

TL;DR: The paper proposes learning token boundaries for LLMs using score function estimates instead of straight-through estimates, showing better performance at 100M parameter scale.


<details>
  <summary>Details</summary>
Motivation: Tokenization remains a hardcoded compression step in LLM training pipelines despite the trend toward end-to-end architectures. Current approaches use heuristics or straight-through estimates which treat discrete token boundary decisions as continuous problems.

Method: Uses score function estimates to learn token boundaries, which directly optimize the discrete boundary drawing problem to minimize loss. Incorporates reinforcement learning techniques like time discounting to reduce variance and make the approach practical.

Result: The proposed method outperforms prior straight-through estimates both qualitatively and quantitatively at the 100 million parameter scale.

Conclusion: Score function estimates with variance reduction techniques provide a more effective way to learn token boundaries end-to-end in LLMs compared to existing approaches.

Abstract: Tokenization is a hardcoded compression step which remains in the training pipeline of Large Language Models (LLMs), despite a general trend towards architectures becoming increasingly end-to-end. Prior work has shown promising results at scale in bringing this compression step inside the LLMs' architecture with heuristics to draw token boundaries, and also attempts to learn these token boundaries with straight-through estimates, which treat the problem of drawing discrete token boundaries as a continuous one. We show that these token boundaries can instead be learned using score function estimates, which have tighter theoretical guarantees due to directly optimizing the problem of drawing discrete token boundaries to minimize loss. We observe that techniques from reinforcement learning, such as time discounting, are necessary to reduce the variance of this score function sufficiently to make it practicable. We demonstrate that the resultant method outperforms prior proposed straight-through estimates, both qualitatively and quantitatively at the $100$ million parameter scale.

</details>


### [442] [Experiential Reinforcement Learning](https://arxiv.org/abs/2602.13949)
*Taiwei Shi,Sihao Chen,Bowen Jiang,Linxin Song,Longqi Yang,Jieyu Zhao*

Main category: cs.LG

TL;DR: ERL embeds explicit experience-reflection-consolidation loops into RL for language models, converting sparse delayed feedback into structured behavioral revisions to improve learning efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning for language models faces challenges with sparse and delayed environmental feedback, requiring models to implicitly infer how failures should translate into behavioral changes for future iterations.

Method: Experiential Reinforcement Learning (ERL) introduces an explicit experience-reflection-consolidation loop: model generates initial attempt, receives feedback, produces reflection to guide refined second attempt, then reinforces successful outcomes into base policy.

Result: ERL consistently improves learning efficiency and final performance over strong RL baselines, achieving up to +81% gains in complex multi-step environments and up to +11% in tool-using reasoning tasks.

Conclusion: Integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement without additional inference cost at deployment.

Abstract: Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback, and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision, improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.

</details>


### [443] [QuRL: Efficient Reinforcement Learning with Quantized Rollout](https://arxiv.org/abs/2602.13953)
*Yuhang Li,Reena Elangovan,Xin Dong,Priyadarshini Panda,Brucek Khailany*

Main category: cs.LG

TL;DR: QuRL accelerates RL training for LLMs by using quantized actors for rollout, addressing training collapse with Adaptive Clipping Range and weight update issues with invariant scaling.


<details>
  <summary>Details</summary>
Motivation: RL training for reasoning LLMs suffers from slow rollout processes (up to 70% of training time) due to autoregressive decoding, creating an efficiency bottleneck.

Method: Proposes Quantized Reinforcement Learning (QuRL) with two key techniques: 1) Adaptive Clipping Range (ACR) to prevent training collapse by dynamically adjusting clipping ratio based on policy differences, and 2) invariant scaling to address weight update problems by reducing quantization noise.

Result: Achieves 20% to 80% faster rollout during training with INT8 and FP8 quantization experiments on DeepScaleR and DAPO benchmarks.

Conclusion: QuRL effectively accelerates RL training for LLMs by using quantized actors while maintaining training stability through novel techniques addressing quantization-specific challenges.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a trending paradigm for training reasoning large language models (LLMs). However, due to the autoregressive decoding nature of LLMs, the rollout process becomes the efficiency bottleneck of RL training, consisting of up to 70\% of the total training time. In this work, we propose Quantized Reinforcement Learning (QuRL) that uses a quantized actor for accelerating the rollout. We address two challenges in QuRL. First, we propose Adaptive Clipping Range (ACR) that dynamically adjusts the clipping ratio based on the policy ratio between the full-precision actor and the quantized actor, which is essential for mitigating long-term training collapse. Second, we identify the weight update problem, where weight changes between RL steps are extremely small, making it difficult for the quantization operation to capture them effectively. We mitigate this problem through the invariant scaling technique that reduces quantization noise and increases weight update. We evaluate our method with INT8 and FP8 quantization experiments on DeepScaleR and DAPO, and achieve 20% to 80% faster rollout during training.

</details>


### [444] [Chemical Language Models for Natural Products: A State-Space Model Approach](https://arxiv.org/abs/2602.13958)
*Ho-Hsuan Wang,Afnan Sultan,Andrea Volkamer,Dietrich Klakow*

Main category: cs.LG

TL;DR: NP-specific chemical language models (Mamba variants) outperform transformers for natural product tasks using domain-specific pre-training on 1M NPs, achieving comparable results to models trained on 100x larger datasets.


<details>
  <summary>Details</summary>
Motivation: Natural Products (NPs) are important for drug discovery but underexplored in chemical language modeling, with existing models primarily focused on general molecular tasks rather than NP-specific applications.

Method: Developed NP-specific chemical language models by pre-training state-space models (Mamba and Mamba-2) and comparing with transformer baselines (GPT) on 1M NP dataset. Evaluated eight tokenization strategies including character-level, AIS, BPE, and NP-specific BPE. Assessed performance on molecule generation (validity, uniqueness, novelty) and property prediction (membrane permeability, taste, anti-cancer activity) using MCC and AUC-ROC metrics.

Result: Mamba generates 1-2% more valid and unique molecules than Mamba-2 and GPT with fewer long-range dependency errors, while GPT yields slightly more novel structures. For property prediction, Mamba variants outperform GPT by 0.02-0.04 MCC under random splits, with comparable performance under scaffold splits. Domain-specific pre-training on 1M NPs matches models trained on datasets over 100 times larger.

Conclusion: State-space models (particularly Mamba) are effective for NP-focused chemical language modeling, demonstrating that domain-specific pre-training on moderate-sized NP datasets can achieve competitive performance without requiring massive general molecular datasets.

Abstract: Language models are widely used in chemistry for molecular property prediction and small-molecule generation, yet Natural Products (NPs) remain underexplored despite their importance in drug discovery. To address this gap, we develop NP-specific chemical language models (NPCLMs) by pre-training state-space models (Mamba and Mamba-2) and comparing them with transformer baselines (GPT). Using a dataset of about 1M NPs, we present the first systematic comparison of selective state-space models and transformers for NP-focused tasks, together with eight tokenization strategies including character-level, Atom-in-SMILES (AIS), byte-pair encoding (BPE), and NP-specific BPE. We evaluate molecule generation (validity, uniqueness, novelty) and property prediction (membrane permeability, taste, anti-cancer activity) using MCC and AUC-ROC. Mamba generates 1-2 percent more valid and unique molecules than Mamba-2 and GPT, with fewer long-range dependency errors, while GPT yields slightly more novel structures. For property prediction, Mamba variants outperform GPT by 0.02-0.04 MCC under random splits, while scaffold splits show comparable performance. Results demonstrate that domain-specific pre-training on about 1M NPs can match models trained on datasets over 100 times larger.

</details>


### [445] [Steady-State Behavior of Constant-Stepsize Stochastic Approximation: Gaussian Approximation and Tail Bounds](https://arxiv.org/abs/2602.13960)
*Zedong Wang,Yuyang Wang,Ijay Narang,Felix Wang,Yuzhou Wang,Siva Theja Maguluri*

Main category: cs.LG

TL;DR: The paper provides explicit, non-asymptotic error bounds for approximating the stationary distribution of constant-stepsize stochastic approximation algorithms by Gaussian distributions, covering both i.i.d. and Markovian noise models.


<details>
  <summary>Details</summary>
Motivation: Constant-stepsize SA is computationally efficient but its stationary distribution is rarely tractable. While prior work shows weak convergence to Gaussian as stepsize approaches zero, there are no usable error bounds for fixed stepsize approximations.

Method: Prove general theorems bounding Wasserstein distance between centered-scaled steady state and Gaussian distribution under regularity conditions. Cover i.i.d. and Markovian noise. Instantiate for three SA settings: SGD for smooth strongly convex objectives, linear SA, and contractive nonlinear SA.

Result: Obtain dimension- and stepsize-dependent explicit bounds in Wasserstein distance of order α^{1/2}log(1/α) for small α. Derive non-uniform Berry-Esseen-type tail bounds with error decaying in both deviation level and stepsize. For SGD beyond strong convexity, identify non-Gaussian (Gibbs) limiting law.

Conclusion: The paper provides the first explicit, non-asymptotic error bounds for Gaussian approximation of SA steady states, enabling practical use of Gaussian approximations for fixed stepsize algorithms. Results cover multiple SA settings and noise models.

Abstract: Constant-stepsize stochastic approximation (SA) is widely used in learning for computational efficiency. For a fixed stepsize, the iterates typically admit a stationary distribution that is rarely tractable. Prior work shows that as the stepsize $α\downarrow 0$, the centered-and-scaled steady state converges weakly to a Gaussian random vector. However, for fixed $α$, this weak convergence offers no usable error bound for approximating the steady-state by its Gaussian limit. This paper provides explicit, non-asymptotic error bounds for fixed $α$. We first prove general-purpose theorems that bound the Wasserstein distance between the centered-scaled steady state and an appropriate Gaussian distribution, under regularity conditions for drift and moment conditions for noise. To ensure broad applicability, we cover both i.i.d. and Markovian noise models. We then instantiate these theorems for three representative SA settings: (1) stochastic gradient descent (SGD) for smooth strongly convex objectives, (2) linear SA, and (3) contractive nonlinear SA. We obtain dimension- and stepsize-dependent, explicit bounds in Wasserstein distance of order $α^{1/2}\log(1/α)$ for small $α$. Building on the Wasserstein approximation error, we further derive non-uniform Berry--Esseen-type tail bounds that compare the steady-state tail probability to Gaussian tails. We achieve an explicit error term that decays in both the deviation level and stepsize $α$. We adapt the same analysis for SGD beyond strongly convexity and study general convex objectives. We identify a non-Gaussian (Gibbs) limiting law under the correct scaling, which is validated numerically, and provide a corresponding pre-limit Wasserstein error bound.

</details>


### [446] [KoopGen: Koopman Generator Networks for Representing and Predicting Dynamical Systems with Continuous Spectra](https://arxiv.org/abs/2602.14011)
*Liangyu Su,Jun Shu,Rui Liu,Deyu Meng,Zongben Xu*

Main category: cs.LG

TL;DR: KoopGen is a generator-based neural Koopman framework that models chaotic dynamics through structured, state-dependent representations of Koopman generators, separating conservative transport from irreversible dissipation while enforcing operator-theoretic constraints.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for high-dimensional chaotic systems lack stability, interpretability, and scalability. Koopman methods provide linear perspectives but rely on restrictive finite-dimensional assumptions or explicit spectral parameterizations that degrade in high-dimensional settings.

Method: KoopGen uses a generator-based neural Koopman framework with structured, state-dependent representation of Koopman generators. It exploits Cartesian decomposition into skew-adjoint (conservative transport) and self-adjoint (irreversible dissipation) components while enforcing exact operator-theoretic constraints during learning.

Result: Across nonlinear oscillators, high-dimensional chaotic systems, and spatiotemporal dynamics, KoopGen improves prediction accuracy and stability while clarifying which components of continuous-spectrum dynamics admit interpretable and learnable representations.

Conclusion: KoopGen provides a principled approach to modeling high-dimensional chaotic dynamics by combining neural networks with operator-theoretic constraints, offering improved stability and interpretability while maintaining accuracy in continuous-spectrum regimes.

Abstract: Representing and predicting high-dimensional and spatiotemporally chaotic dynamical systems remains a fundamental challenge in dynamical systems and machine learning. Although data-driven models can achieve accurate short-term forecasts, they often lack stability, interpretability, and scalability in regimes dominated by broadband or continuous spectra. Koopman-based approaches provide a principled linear perspective on nonlinear dynamics, but existing methods rely on restrictive finite-dimensional assumptions or explicit spectral parameterizations that degrade in high-dimensional settings. Against these issues, we introduce KoopGen, a generator-based neural Koopman framework that models dynamics through a structured, state-dependent representation of Koopman generators. By exploiting the intrinsic Cartesian decomposition into skew-adjoint and self-adjoint components, KoopGen separates conservative transport from irreversible dissipation while enforcing exact operator-theoretic constraints during learning. Across systems ranging from nonlinear oscillators to high-dimensional chaotic and spatiotemporal dynamics, KoopGen improves prediction accuracy and stability, while clarifying which components of continuous-spectrum dynamics admit interpretable and learnable representations.

</details>


### [447] [S2SServiceBench: A Multimodal Benchmark for Last-Mile S2S Climate Services](https://arxiv.org/abs/2602.14017)
*Chenyue Li,Wen Deng,Zhuotao Sun,Mengxi Jin,Hanzhe Cui,Han Li,Shentong Li,Man Kit Yu,Ming Long Lai,Yuhao Yang,Mengqian Lu,Binhang Yuan*

Main category: cs.LG

TL;DR: S2SServiceBench: A multimodal benchmark for evaluating MLLMs' ability to translate S2S climate forecasts into actionable services across 6 domains with 500+ tasks.


<details>
  <summary>Details</summary>
Motivation: There's a critical "last-mile gap" in S2S forecasting where scientific forecasts aren't effectively translated into trusted, actionable climate services. While MLLMs show promise for workflow support, it's unclear if they can reliably generate decision-making deliverables from operational climate service products under uncertainty.

Method: Created S2SServiceBench - a multimodal benchmark curated from operational climate-service systems covering 10 service products with 150+ expert-selected cases across Agriculture, Disasters, Energy, Finance, Health, and Shipping domains. Each case has three service levels, yielding ~500 tasks and 1,000+ evaluation items.

Result: Benchmarked state-of-the-art MLLMs and agents, revealing persistent challenges in S2S service plot understanding and reasoning: actionable signal comprehension, operationalizing uncertainty into executable handoffs, and stable evidence-grounded analysis/planning for dynamic hazards.

Conclusion: The benchmark provides actionable guidance for building future climate-service agents by identifying key capability gaps in translating climate forecasts into decision-making services under uncertainty.

Abstract: Subseasonal-to-seasonal (S2S) forecasts play an essential role in providing a decision-critical weeks-to-months planning window for climate resilience and sustainability, yet a growing bottleneck is the last-mile gap: translating scientific forecasts into trusted, actionable climate services, requiring reliable multimodal understanding and decision-facing reasoning under uncertainty. Meanwhile, multimodal large language models (MLLMs) and corresponding agentic paradigms have made rapid progress in supporting various workflows, but it remains unclear whether they can reliably generate decision-making deliverables from operational service products (e.g., actionable signal comprehension, decision-making handoff, and decision analysis & planning) under uncertainty. We introduce S2SServiceBench, a multimodal benchmark for last-mile S2S climate services curated from an operational climate-service system to evaluate this capability. S2SServiceBenchcovers 10 service products with about 150+ expert-selected cases in total, spanning six application domains - Agriculture, Disasters, Energy, Finance, Health, and Shipping. Each case is instantiated at three service levels, yielding around 500 tasks and 1,000+ evaluation items across climate resilience and sustainability applications. Using S2SServiceBench, we benchmark state-of-the-art MLLMs and agents, and analyze performance across products and service levels, revealing persistent challenges in S2S service plot understanding and reasoning - namely, actionable signal comprehension, operationalizing uncertainty into executable handoffs, and stable, evidence-grounded analysis and planning for dynamic hazards-while offering actionable guidance for building future climate-service agents.

</details>


### [448] [EIDOS: Latent-Space Predictive Learning for Time Series Foundation Models](https://arxiv.org/abs/2602.14024)
*Xinxing Zhou,Qingren Yao,Yiji Zhao,Chenghao Liu,Flora Salim,Xiaojie Yuan,Yanlong Wen,Ming Jin*

Main category: cs.LG

TL;DR: EIDOS introduces a time series foundation model that shifts from direct future value prediction to latent-space predictive learning, achieving better structured representations and state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current time series foundation models predict future observations directly, resulting in weakly structured latent representations that capture surface noise rather than coherent temporal dynamics.

Method: Trains a causal Transformer to predict latent representation evolution, uses lightweight aggregation branch for stable targets, and employs joint objective with latent-space alignment, observational grounding, and direct forecasting supervision.

Result: EIDOS mitigates structural fragmentation in representation space and achieves state-of-the-art performance on the GIFT-Eval benchmark.

Conclusion: Constraining models to learn predictable latent dynamics is a principled approach toward more robust and reliable time series foundation models.

Abstract: Most time series foundation models are pretrained by directly predicting future observations, which often yields weakly structured latent representations that capture surface noise rather than coherent and predictable temporal dynamics. In this work, we introduce EIDOS, a foundation model family that shifts pretraining from future value prediction to latent-space predictive learning. We train a causal Transformer to predict the evolution of latent representations, encouraging the emergence of structured and temporally coherent latent states. To ensure stable targets for latent-space learning, we design a lightweight aggregation branch to construct target representations. EIDOS is optimized via a joint objective that integrates latent-space alignment, observational grounding to anchor representations to the input signal, and direct forecasting supervision. On the GIFT-Eval benchmark, EIDOS mitigates structural fragmentation in the representation space and achieves state-of-the-art performance. These results demonstrate that constraining models to learn predictable latent dynamics is a principled step toward more robust and reliable time series foundation models.

</details>


### [449] [UniST-Pred: A Robust Unified Framework for Spatio-Temporal Traffic Forecasting in Transportation Networks Under Disruptions](https://arxiv.org/abs/2602.14049)
*Yue Wang,Areg Karapetyan,Djellel Difallah,Samer Madanat*

Main category: cs.LG

TL;DR: UniST-Pred is a unified spatio-temporal traffic forecasting framework that decouples temporal modeling from spatial representation learning, then integrates both through adaptive fusion, achieving competitive performance with lightweight design while maintaining robustness under network disruptions.


<details>
  <summary>Details</summary>
Motivation: Real-world traffic forecasting models must operate under structural and observational uncertainties rarely considered in model design. Existing approaches often tightly couple spatial and temporal modeling at the cost of increased complexity and limited modularity, while efficient time-series models capture long-range dependencies without explicit network structure.

Method: UniST-Pred first decouples temporal modeling from spatial representation learning, then integrates both through adaptive representation-level fusion. The approach is evaluated using both a custom dataset from MATSim agent-based simulator (for network disconnection scenarios) and standard traffic prediction datasets.

Result: UniST-Pred demonstrates competitive performance against established models despite lightweight design, maintains strong predictive performance across real-world and simulated datasets, and yields interpretable spatio-temporal representations under infrastructure disruptions.

Conclusion: The proposed decoupled approach provides a robust and efficient solution for spatio-temporal traffic forecasting that handles real-world uncertainties while maintaining interpretability and competitive performance with simpler architecture.

Abstract: Spatio-temporal traffic forecasting is a core component of intelligent transportation systems, supporting various downstream tasks such as signal control and network-level traffic management. In real-world deployments, forecasting models must operate under structural and observational uncertainties, conditions that are rarely considered in model design. Recent approaches achieve strong short-term predictive performance by tightly coupling spatial and temporal modeling, often at the cost of increased complexity and limited modularity. In contrast, efficient time-series models capture long-range temporal dependencies without relying on explicit network structure. We propose UniST-Pred, a unified spatio-temporal forecasting framework that first decouples temporal modeling from spatial representation learning, then integrates both through adaptive representation-level fusion. To assess robustness of the proposed approach, we construct a dataset based on an agent-based, microscopic traffic simulator (MATSim) and evaluate UniST-Pred under severe network disconnection scenarios. Additionally, we benchmark UniST-Pred on standard traffic prediction datasets, demonstrating its competitive performance against existing well-established models despite a lightweight design. The results illustrate that UniST-Pred maintains strong predictive performance across both real-world and simulated datasets, while also yielding interpretable spatio-temporal representations under infrastructure disruptions. The source code and the generated dataset are available at https://anonymous.4open.science/r/UniST-Pred-EF27

</details>


### [450] [Position Encoding with Random Float Sampling Enhances Length Generalization of Transformers](https://arxiv.org/abs/2602.14050)
*Atsushi Shimizu,Shohei Taniguchi,Yutaka Matsuo*

Main category: cs.LG

TL;DR: RFS (Random Float Sampling) is a novel position encoding method that uses continuous random position indices during training to improve length generalization in language models.


<details>
  <summary>Details</summary>
Motivation: Current position encoding methods struggle with length generalization - maintaining performance on inputs longer than seen during training. This is because they use predefined discrete position indices that become out-of-distribution for unseen lengths.

Method: RFS replaces discrete position indices with randomly sampled continuous values during training. This exposes models to diverse position indices, avoiding OOD issues on unseen lengths. The approach can be easily integrated with existing position encodings like sinusoidal, RoPE, and ALiBi.

Result: RFS demonstrates superior performance in length generalization tasks and zero-shot commonsense reasoning benchmarks compared to traditional position encoding methods.

Conclusion: Random Float Sampling provides a simple yet effective solution to the length generalization problem by using continuous position indices during training, making it broadly applicable to various existing position encoding schemes.

Abstract: Length generalization is the ability of language models to maintain performance on inputs longer than those seen during pretraining. In this work, we introduce a simple yet powerful position encoding (PE) strategy, Random Float Sampling (RFS), that generalizes well to lengths unseen during pretraining or fine-tuning. In particular, instead of selecting position indices from a predefined discrete set, RFS uses randomly sampled continuous values, thereby avoiding out-of-distribution (OOD) issues on unseen lengths by exposing the model to diverse indices during training. Since assigning indices to tokens is a common and fundamental procedure in widely used PEs, the advantage of RFS can easily be incorporated into, for instance, the absolute sinusoidal encoding, RoPE, and ALiBi. Experiments corroborate its effectiveness by showing that RFS results in superior performance in length generalization tasks as well as zero-shot commonsense reasoning benchmarks.

</details>


### [451] [Policy Gradient with Adaptive Entropy Annealing for Continual Fine-Tuning](https://arxiv.org/abs/2602.14078)
*Yaqian Zhang,Bernhard Pfahringer,Eibe Frank,Albert Bifet*

Main category: cs.LG

TL;DR: The paper proposes aEPG, an adaptive entropy annealing method that transitions from cross-entropy-like exploration to direct 0-1 loss minimization for better class-incremental learning in vision models.


<details>
  <summary>Details</summary>
Motivation: Large pretrained vision models suffer from catastrophic forgetting in class-incremental settings. While parameter-efficient fine-tuning helps, most approaches still use cross-entropy loss which is a surrogate for the true 0-1 loss objective. The authors want to directly minimize misclassification error.

Method: Formulate classification as a one-step Markov Decision Process and derive Expected Policy Gradient (EPG) to directly minimize 0-1 loss. Analyze CE as EPG with sample-weighting, then propose adaptive entropy annealing (aEPG) that transitions from exploratory (CE-like) to exploitative (EPG-like) learning.

Result: aEPG-based methods outperform CE-based methods across diverse benchmarks with various PEFT modules. Lower entropy of output prediction distribution enhances adaptation in pretrained vision models.

Conclusion: Directly minimizing 0-1 loss via EPG with adaptive entropy annealing improves class-incremental learning performance. The transition from exploration to exploitation aligns with how models should adapt to new tasks while preserving old knowledge.

Abstract: Despite their success, large pretrained vision models remain vulnerable to catastrophic forgetting when adapted to new tasks in class-incremental settings. Parameter-efficient fine-tuning (PEFT) alleviates this by restricting trainable parameters, yet most approaches still rely on cross-entropy (CE) loss, a surrogate for the 0-1 loss, to learn from new data. We revisit this choice and revive the true objective (0-1 loss) through a reinforcement learning perspective. By formulating classification as a one-step Markov Decision Process, we derive an Expected Policy Gradient (EPG) method that directly minimizes misclassification error with a low-variance gradient estimation. Our analysis shows that CE can be interpreted as EPG with an additional sample-weighting mechanism: CE encourages exploration by emphasizing low-confidence samples, while EPG prioritizes high-confidence ones. Building on this insight, we propose adaptive entropy annealing (aEPG), a training strategy that transitions from exploratory (CE-like) to exploitative (EPG-like) learning. aEPG-based methods outperform CE-based methods across diverse benchmarks and with various PEFT modules. More broadly, we evaluate various entropy regularization methods and demonstrate that lower entropy of the output prediction distribution enhances adaptation in pretrained vision models.

</details>


### [452] [Neural Optimal Transport in Hilbert Spaces: Characterizing Spurious Solutions and Gaussian Smoothing](https://arxiv.org/abs/2602.14086)
*Jae-Hwan Choi,Jiwoo Yoon,Dohyun Kwon,Jaewoong Choi*

Main category: cs.LG

TL;DR: Neural OT in Hilbert spaces suffers from spurious solutions; Gaussian smoothing via Brownian motion resolves ill-posedness and recovers unique Monge maps.


<details>
  <summary>Details</summary>
Motivation: Semi-dual Neural OT in infinite-dimensional Hilbert spaces produces spurious solutions in non-regular settings, failing to accurately capture target distributions. This ill-posedness problem needs resolution for practical applications.

Method: Extend semi-dual framework with Gaussian smoothing strategy based on Brownian motion. Use regular measures framework to characterize spurious solutions, and apply covariance operator kernel analysis for smoothing effectiveness.

Result: Theoretical proof shows well-posedness under regular source measures with unique Monge map recovery. Sharp characterization of smoothed measure regularity reveals kernel dependence. Empirical results on functional data and time-series show spurious solution suppression and baseline outperformance.

Conclusion: Gaussian smoothing resolves Neural OT ill-posedness in Hilbert spaces, providing well-posed formulation and unique Monge maps, with practical effectiveness demonstrated on real-world datasets.

Abstract: We study Neural Optimal Transport in infinite-dimensional Hilbert spaces. In non-regular settings, Semi-dual Neural OT often generates spurious solutions that fail to accurately capture target distributions. We analytically characterize this spurious solution problem using the framework of regular measures, which generalize Lebesgue absolute continuity in finite dimensions. To resolve ill-posedness, we extend the semi-dual framework via a Gaussian smoothing strategy based on Brownian motion. Our primary theoretical contribution proves that under a regular source measure, the formulation is well-posed and recovers a unique Monge map. Furthermore, we establish a sharp characterization for the regularity of smoothed measures, proving that the success of smoothing depends strictly on the kernel of the covariance operator. Empirical results on synthetic functional data and time-series datasets demonstrate that our approach effectively suppresses spurious solutions and outperforms existing baselines.

</details>


### [453] [Geometry-Aware Physics-Informed PointNets for Modeling Flows Across Porous Structures](https://arxiv.org/abs/2602.14108)
*Luigi Ciceri,Corrado Mio,Jianyi Lin,Gabriele Gianini*

Main category: cs.LG

TL;DR: Physics-informed neural networks (PIPN and PI-GANO) predict fluid flow through and around porous bodies by enforcing Navier-Stokes equations in free-flow regions and Darcy-Forchheimer in porous regions, generalizing across diverse geometries without retraining.


<details>
  <summary>Details</summary>
Motivation: Predicting coupled fluid flow through and around porous bodies is challenging due to complex physics across different regions and the need to generalize across diverse geometries and boundary conditions without expensive simulations.

Method: Two physics-informed learning approaches: Physics Informed PointNets (PIPN) and Physics Informed Geometry Aware Neural Operator (PI-GANO). They enforce incompressible Navier-Stokes equations in free-flow regions and Darcy-Forchheimer extension in porous regions within a unified loss function, conditioned on geometry and material parameters. Datasets generated with OpenFOAM on 2D ducts with porous obstacles and 3D windbreak scenarios with tree canopies and buildings.

Result: Consistently low velocity and pressure errors in both seen and unseen cases, with accurate reproduction of wake structures. Performance degrades primarily near sharp interfaces and in regions with large gradients. The models generalize well to unseen shapes, and for PI-GANO, to variable boundary conditions and parameter settings.

Conclusion: The study provides the first systematic evaluation of PIPN/PI-GANO for simultaneous through-and-around porous flows, demonstrating their potential to accelerate design studies without requiring retraining for each new geometry, though challenges remain near interfaces and high-gradient regions.

Abstract: Predicting flows that occur both through and around porous bodies is challenging due to coupled physics across fluid and porous regions and the need to generalize across diverse geometries and boundary conditions. We address this problem using two Physics Informed learning approaches: Physics Informed PointNets (PIPN) and Physics Informed Geometry Aware Neural Operator (P-IGANO). We enforce the incompressible Navier Stokes equations in the free-flow region and a Darcy Forchheimer extension in the porous region within a unified loss and condition the networks on geometry and material parameters. Datasets are generated with OpenFOAM on 2D ducts containing porous obstacles and on 3D windbreak scenarios with tree canopies and buildings. We first verify the pipeline via the method of manufactured solutions, then assess generalization to unseen shapes, and for PI-GANO, to variable boundary conditions and parameter settings. The results show consistently low velocity and pressure errors in both seen and unseen cases, with accurate reproduction of the wake structures. Performance degrades primarily near sharp interfaces and in regions with large gradients. Overall, the study provides a first systematic evaluation of PIPN/PI-GANO for simultaneous through-and-around porous flows and shows their potential to accelerate design studies without retraining per geometry.

</details>


### [454] [Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?](https://arxiv.org/abs/2602.14111)
*Anton Korznikov,Andrey Galichin,Alexey Dontsov,Oleg Rogov,Ivan Oseledets,Elena Tutubalina*

Main category: cs.LG

TL;DR: SAEs fail to reliably recover meaningful features despite strong reconstruction performance, as shown through synthetic ground-truth experiments and random baseline comparisons.


<details>
  <summary>Details</summary>
Motivation: Despite excitement about SAEs for neural network interpretability, growing negative results in downstream tasks raise doubts about whether SAEs actually recover meaningful features from model activations.

Method: Two complementary evaluations: 1) Synthetic setup with known ground-truth features to measure recovery rate, 2) Real activation evaluation using three baselines that constrain SAE feature directions or activation patterns to random values, tested across multiple SAE architectures.

Result: On synthetic data, SAEs recover only 9% of true features despite achieving 71% explained variance. On real activations, random baselines match fully-trained SAEs in interpretability (0.87 vs 0.90), sparse probing (0.69 vs 0.72), and causal editing (0.73 vs 0.72).

Conclusion: SAEs in their current state do not reliably decompose models' internal mechanisms, as they fail to recover meaningful features even when reconstruction performance is strong.

Abstract: Sparse Autoencoders (SAEs) have emerged as a promising tool for interpreting neural networks by decomposing their activations into sparse sets of human-interpretable features. Recent work has introduced multiple SAE variants and successfully scaled them to frontier models. Despite much excitement, a growing number of negative results in downstream tasks casts doubt on whether SAEs recover meaningful features. To directly investigate this, we perform two complementary evaluations. On a synthetic setup with known ground-truth features, we demonstrate that SAEs recover only $9\%$ of true features despite achieving $71\%$ explained variance, showing that they fail at their core task even when reconstruction is strong. To evaluate SAEs on real activations, we introduce three baselines that constrain SAE feature directions or their activation patterns to random values. Through extensive experiments across multiple SAE architectures, we show that our baselines match fully-trained SAEs in interpretability (0.87 vs 0.90), sparse probing (0.69 vs 0.72), and causal editing (0.73 vs 0.72). Together, these results suggest that SAEs in their current state do not reliably decompose models' internal mechanisms.

</details>


### [455] [ROAST: Rollout-based On-distribution Activation Steering Technique](https://arxiv.org/abs/2602.14143)
*Xuanbo Su,Hao Luo,Yingfang Zhang,Lijun Zhang*

Main category: cs.LG

TL;DR: ROAST is a new activation steering technique that uses on-distribution rollouts and continuous scaling with grouped normalization to improve LLM control without brittle interventions.


<details>
  <summary>Details</summary>
Motivation: Existing activation steering methods rely on off-distribution supervision and discrete masking, leading to brittle interventions that don't work well in practice.

Method: ROAST uses rollout-based on-distribution activation steering with ROC estimation, Continuous Soft Scaling (CSS) instead of hard sparsification, and Grouped Mean Normalization to balance contributions across samples.

Result: ROAST consistently improves performance across models (0.6B to 32B) on diverse tasks (+9.7% on GSM8K for Qwen3-0.6B, +12.1% on TruthfulQA for GLM4-32B), with CSS better preserving activation energy.

Conclusion: ROAST provides more robust activation steering by addressing magnitude variance issues through on-distribution estimation and proper normalization, enabling better control over LLMs at inference time.

Abstract: Activation steering provides parameter-efficient control over large language models (LLMs) at inference time, but many methods rely on off-distribution supervision and discrete masking, leading to brittle interventions. We propose ROAST (Rollout-based On-distribution Activation Steering Technique), which estimates steering directions from the model's own on-distribution rollouts via ROC and avoids hard sparsification via Continuous Soft Scaling (CSS) and Grouped Mean Normalization. Our empirical analysis reveals that while activation magnitude correlates moderately with directional consistency, the variance in magnitude is significant and often disproportionate to semantic quality. This suggests that high-magnitude activations risk dominating the global steering direction if not properly normalized. To address this, ROAST employs grouped normalization to balance contributions across samples, ensuring a more robust estimation of the consensus steering direction. Across models (0.6B to 32B), ROAST consistently improves performance on diverse tasks (e.g., +9.7% on GSM8K for Qwen3-0.6B and +12.1% on TruthfulQA for GLM4-32B), and analyses show that CSS better preserves activation energy.

</details>


### [456] [A Penalty Approach for Differentiation Through Black-Box Quadratic Programming Solvers](https://arxiv.org/abs/2602.14154)
*Yuxuan Linghu,Zhiyuan Liu,Qi Deng*

Main category: cs.LG

TL;DR: dXPP is a penalty-based differentiation framework for quadratic programs that decouples QP solving from differentiation, improving computational efficiency and robustness over KKT-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing KKT-based differentiation methods for quadratic programs suffer from computational cost and numerical robustness issues at scale, creating a need for more efficient and robust alternatives.

Method: Proposes dXPP with two steps: 1) forward pass uses any black-box QP solver, 2) backward pass maps solution to a smooth approximate penalty problem and implicitly differentiates through it using only a smaller linear system in primal variables.

Result: dXPP is competitive with KKT-based methods and achieves substantial speedups on large-scale problems, demonstrated on random QPs, large-scale sparse projection problems, and multi-period portfolio optimization.

Conclusion: dXPP provides a more efficient and robust alternative to KKT-based differentiation for quadratic programs by decoupling solving from differentiation and using penalty-based implicit differentiation.

Abstract: Differentiating through the solution of a quadratic program (QP) is a central problem in differentiable optimization. Most existing approaches differentiate through the Karush--Kuhn--Tucker (KKT) system, but their computational cost and numerical robustness can degrade at scale. To address these limitations, we propose dXPP, a penalty-based differentiation framework that decouples QP solving from differentiation. In the solving step (forward pass), dXPP is solver-agnostic and can leverage any black-box QP solver. In the differentiation step (backward pass), we map the solution to a smooth approximate penalty problem and implicitly differentiate through it, requiring only the solution of a much smaller linear system in the primal variables. This approach bypasses the difficulties inherent in explicit KKT differentiation and significantly improves computational efficiency and robustness. We evaluate dXPP on various tasks, including randomly generated QPs, large-scale sparse projection problems, and a real-world multi-period portfolio optimization task. Empirical results demonstrate that dXPP is competitive with KKT-based differentiation methods and achieves substantial speedups on large-scale problems.

</details>


### [457] [Synergistic Intra- and Cross-Layer Regularization Losses for MoE Expert Specialization](https://arxiv.org/abs/2602.14159)
*Rizhen Hu,Yuan Cao,Boao Kong,Mou Sun,Kun Yuan*

Main category: cs.LG

TL;DR: Two plug-and-play regularization losses (intra-layer specialization and cross-layer coupling) improve MoE models by reducing expert overlap and routing ambiguity without architectural changes, leading to better performance and faster inference.


<details>
  <summary>Details</summary>
Motivation: Sparse Mixture-of-Experts models suffer from expert overlap (redundant representations) and routing ambiguity, which severely underutilizes model capacity. Existing architectural solutions require substantial modifications and only use intra-layer signals.

Method: Two regularization losses: 1) Intra-layer specialization loss penalizes cosine similarity between experts' SwiGLU activations on identical tokens to encourage complementary specialization. 2) Cross-layer coupling loss maximizes joint Top-k routing probabilities across adjacent layers to establish coherent expert pathways through network depth.

Result: Experiments across pre-training, fine-tuning, and zero-shot benchmarks show consistent task gains, higher expert specialization, lower-entropy routing, and faster inference via more stable expert pathways.

Conclusion: The proposed plug-and-play regularization losses effectively enhance MoE specialization and routing efficiency without modifying router or model architectures, improving performance while maintaining compatibility with existing MoE architectures.

Abstract: Sparse Mixture-of-Experts (MoE) models scale Transformers efficiently but suffer from expert overlap -- redundant representations across experts and routing ambiguity, resulting in severely underutilized model capacity. While architectural solutions like DeepSeekMoE promote specialization, they require substantial structural modifications and rely solely on intra-layer signals. In this paper, we propose two plug-and-play regularization losses that enhance MoE specialization and routing efficiency without modifying router or model architectures. First, an intra-layer specialization loss penalizes cosine similarity between experts' SwiGLU activations on identical tokens, encouraging experts to specialize in complementary knowledge. Second, a cross-layer coupling loss maximizes joint Top-$k$ routing probabilities across adjacent layers, establishing coherent expert pathways through network depth while reinforcing intra-layer expert specialization. Both losses are orthogonal to the standard load-balancing loss and compatible with both the shared-expert architecture in DeepSeekMoE and vanilla top-$k$ MoE architectures. We implement both losses as a drop-in Megatron-LM module. Extensive experiments across pre-training, fine-tuning, and zero-shot benchmarks demonstrate consistent task gains, higher expert specialization, and lower-entropy routing; together, these improvements translate into faster inference via more stable expert pathways.

</details>


### [458] [When Benchmarks Lie: Evaluating Malicious Prompt Classifiers Under True Distribution Shift](https://arxiv.org/abs/2602.14161)
*Max Fomin*

Main category: cs.LG

TL;DR: Current prompt injection detection methods overestimate performance due to flawed evaluation practices. Leave-One-Dataset-Out (LODO) evaluation reveals significant generalization gaps, with classifiers relying on dataset-specific shortcuts rather than semantic understanding.


<details>
  <summary>Details</summary>
Motivation: As LLM-based agents process increasing amounts of untrusted data from emails, documents, and external APIs, robust detection of prompt injection and jailbreak attacks becomes critical for safe deployment. Current evaluation practices and production systems have fundamental limitations that need to be addressed.

Method: The study uses a diverse benchmark of 18 datasets covering harmful requests, jailbreaks, indirect prompt injections, and extraction attacks. They propose Leave-One-Dataset-Out (LODO) evaluation to measure true out-of-distribution generalization, analyze Sparse Auto-Encoder (SAE) feature coefficients across LODO folds, and systematically compare production guardrails (PromptGuard 2, LlamaGuard) and LLM-as-judge approaches.

Result: Standard train-test splits from same dataset sources severely overestimate performance by 8.4 percentage points AUC on average, with per-dataset accuracy gaps ranging from 1% to 25%. Analysis reveals 28% of top features are dataset-dependent shortcuts. Production guardrails fail on indirect attacks targeting agents (7-37% detection), and PromptGuard 2/LlamaGuard cannot evaluate agentic tool injection due to architectural limitations.

Conclusion: LODO evaluation is essential for realistic assessment of prompt attack detection systems. Current methods rely on dataset artifacts rather than semantic understanding, and production guardrails have significant limitations. LODO-stable SAE features provide more reliable explanations by filtering dataset artifacts, establishing LODO as the appropriate protocol for future research.

Abstract: Detecting prompt injection and jailbreak attacks is critical for deploying LLM-based agents safely. As agents increasingly process untrusted data from emails, documents, tool outputs, and external APIs, robust attack detection becomes essential. Yet current evaluation practices and production systems have fundamental limitations. We present a comprehensive analysis using a diverse benchmark of 18 datasets spanning harmful requests, jailbreaks, indirect prompt injections, and extraction attacks. We propose Leave-One-Dataset-Out (LODO) evaluation to measure true out-of-distribution generalization, revealing that the standard practice of train-test splits from the same dataset sources severely overestimates performance: aggregate metrics show an 8.4 percentage point AUC inflation, but per-dataset gaps range from 1% to 25% accuracy-exposing heterogeneous failure modes. To understand why classifiers fail to generalize, we analyze Sparse Auto-Encoder (SAE) feature coefficients across LODO folds, finding that 28% of top features are dataset-dependent shortcuts whose class signal depends on specific dataset compositions rather than semantic content. We systematically compare production guardrails (PromptGuard 2, LlamaGuard) and LLM-as-judge approaches on our benchmark, finding all three fail on indirect attacks targeting agents (7-37% detection) and that PromptGuard 2 and LlamaGuard cannot evaluate agentic tool injection due to architectural limitations. Finally, we show that LODO-stable SAE features provide more reliable explanations for classifier decisions by filtering dataset artifacts. We release our evaluation framework at https://github.com/maxf-zn/prompt-mining to establish LODO as the appropriate protocol for prompt attack detection research.

</details>


### [459] [Deep Dense Exploration for LLM Reinforcement Learning via Pivot-Driven Resampling](https://arxiv.org/abs/2602.14169)
*Yiran Guo,Zhongjian Qiao,Yingqi Xie,Jie Liu,Dan Ye,Ruiqing Zhang,Shuang Qiu,Lijie Xu*

Main category: cs.LG

TL;DR: DDE (Deep Dense Exploration) is a new RL exploration strategy for LLMs that focuses on deep, recoverable states within unsuccessful trajectories to discover rare correct solutions more efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing RL exploration methods for LLMs have limitations: GRPO only samples from the root, saturating high-probability trajectories while leaving deep error-prone states under-explored. Tree-based methods blindly disperse sampling budgets across trivial or unrecoverable states, causing sampling dilution that fails to uncover rare correct suffixes and destabilizes local baselines.

Method: DDE focuses exploration on "pivots" - deep, recoverable states within unsuccessful trajectories. DEEP-GRPO implements DDE with three innovations: (1) lightweight data-driven utility function balancing recoverability and depth bias to identify pivot states; (2) local dense resampling at each pivot to increase probability of discovering correct subsequent trajectories; (3) dual-stream optimization objective decoupling global policy learning from local corrective updates.

Result: Experiments on mathematical reasoning benchmarks demonstrate that the method consistently outperforms GRPO, tree-based methods, and other strong baselines.

Conclusion: DDE addresses key exploration challenges in RL for LLMs by strategically focusing sampling resources on deep, recoverable states within unsuccessful trajectories, leading to more efficient discovery of rare correct solutions.

Abstract: Effective exploration is a key challenge in reinforcement learning for large language models: discovering high-quality trajectories within a limited sampling budget from the vast natural language sequence space. Existing methods face notable limitations: GRPO samples exclusively from the root, saturating high-probability trajectories while leaving deep, error-prone states under-explored. Tree-based methods blindly disperse budgets across trivial or unrecoverable states, causing sampling dilution that fails to uncover rare correct suffixes and destabilizes local baselines. To address this, we propose Deep Dense Exploration (DDE), a strategy that focuses exploration on $\textit{pivots}$-deep, recoverable states within unsuccessful trajectories. We instantiate DDE with DEEP-GRPO, which introduces three key innovations: (1) a lightweight data-driven utility function that automatically balances recoverability and depth bias to identify pivot states; (2) local dense resampling at each pivot to increase the probability of discovering correct subsequent trajectories; and (3) a dual-stream optimization objective that decouples global policy learning from local corrective updates. Experiments on mathematical reasoning benchmarks demonstrate that our method consistently outperforms GRPO, tree-based methods, and other strong baselines.

</details>


### [460] [TS-Haystack: A Multi-Scale Retrieval Benchmark for Time Series Language Models](https://arxiv.org/abs/2602.14200)
*Nicolas Zumarraga,Thomas Kaar,Ning Wang,Maxwell A. Xu,Max Rosenblattl,Markus Kreft,Kevin O'Sullivan,Paul Schmiedmayer,Patrick Langer,Robert Jakob*

Main category: cs.LG

TL;DR: TS-Haystack benchmark reveals that current Time Series Language Models struggle with long-context temporal retrieval despite good classification performance, due to information loss in temporal compression.


<details>
  <summary>Details</summary>
Motivation: Real-world time-series sensor streams can span millions of datapoints, but existing TSLMs are trained on short sequences, creating a mismatch for long-context retrieval tasks that require precise temporal localization under computational constraints.

Method: Introduced TS-Haystack benchmark with controlled needle insertion (embedding short activity bouts into longer accelerometer recordings) across 10 task types in 4 categories, evaluating context lengths from seconds to 2 hours.

Result: Learned latent compression preserves classification accuracy up to 176× compression but degrades retrieval performance with context length, showing divergence between classification and retrieval behavior across models.

Conclusion: Architectural designs must decouple sequence length from computational complexity while preserving temporal fidelity, as current compression methods lose temporally localized information needed for retrieval.

Abstract: Time Series Language Models (TSLMs) are emerging as unified models for reasoning over continuous signals in natural language. However, long-context retrieval remains a major limitation: existing models are typically trained and evaluated on short sequences, while real-world time-series sensor streams can span millions of datapoints. This mismatch requires precise temporal localization under strict computational constraints, a regime that is not captured by current benchmarks. We introduce TS-Haystack, a long-context temporal retrieval benchmark comprising ten task types across four categories: direct retrieval, temporal reasoning, multi-step reasoning and contextual anomaly. The benchmark uses controlled needle insertion by embedding short activity bouts into longer longitudinal accelerometer recordings, enabling systematic evaluation across context lengths ranging from seconds to 2 hours per sample. We hypothesize that existing TSLM time series encoders overlook temporal granularity as context length increases, creating a task-dependent effect: compression aids classification but impairs retrieval of localized events. Across multiple model and encoding strategies, we observe a consistent divergence between classification and retrieval behavior. Learned latent compression preserves or improves classification accuracy at compression ratios up to 176$\times$, but retrieval performance degrades with context length, incurring in the loss of temporally localized information. These results highlight the importance of architectural designs that decouple sequence length from computational complexity while preserving temporal fidelity.

</details>


### [461] [Fast Catch-Up, Late Switching: Optimal Batch Size Scheduling via Functional Scaling Laws](https://arxiv.org/abs/2602.14208)
*Jinbo Wang,Binghui Li,Zhanpeng Zhou,Mingze Wang,Yuxuan Sun,Jiaqi Zhang,Xunliang Cai,Lei Wu*

Main category: cs.LG

TL;DR: The paper analyzes batch size scheduling using functional scaling laws, showing optimal schedules depend on task difficulty: easy tasks need increasing batch sizes throughout, while hard tasks require small batches early with late switching to large batches.


<details>
  <summary>Details</summary>
Motivation: Batch size scheduling is critical for large-scale deep learning but lacks theoretical foundations. The authors aim to provide a principled understanding of optimal batch size scheduling using functional scaling laws.

Method: Uses functional scaling law (FSL) framework to analyze batch size scheduling. Characterizes optimal schedules under fixed data budget, identifies fast catch-up effect mechanism, and validates with extensive LLM pretraining experiments on Dense and MoE architectures up to 1.1B parameters and 1T tokens.

Result: Optimal batch size schedule structure depends sharply on task difficulty. For hard tasks, optimal schedule maintains small batches early and switches to large batches late. Late-switch schedules consistently outperform constant-batch and early-switch baselines across all LLM pretraining experiments.

Conclusion: Functional scaling laws provide a principled framework for understanding batch size scheduling. The fast catch-up effect enables deferring large batches to late training without performance loss while reducing data consumption, with practical implications for efficient large-scale training.

Abstract: Batch size scheduling (BSS) plays a critical role in large-scale deep learning training, influencing both optimization dynamics and computational efficiency. Yet, its theoretical foundations remain poorly understood. In this work, we show that the functional scaling law (FSL) framework introduced in Li et al. (2025a) provides a principled lens for analyzing BSS. Specifically, we characterize the optimal BSS under a fixed data budget and show that its structure depends sharply on task difficulty. For easy tasks, optimal schedules keep increasing batch size throughout. In contrast, for hard tasks, the optimal schedule maintains small batch sizes for most of training and switches to large batches only in a late stage. To explain the emergence of late switching, we uncover a dynamical mechanism -- the fast catch-up effect -- which also manifests in large language model (LLM) pretraining. After switching from small to large batches, the loss rapidly aligns with the constant large-batch trajectory. Using FSL, we show that this effect stems from rapid forgetting of accumulated gradient noise, with the catch-up speed determined by task difficulty. Crucially, this effect implies that large batches can be safely deferred to late training without sacrificing performance, while substantially reducing data consumption. Finally, extensive LLM pretraining experiments -- covering both Dense and MoE architectures with up to 1.1B parameters and 1T tokens -- validate our theoretical predictions. Across all settings, late-switch schedules consistently outperform constant-batch and early-switch baselines.

</details>


### [462] [MAGE: All-[MASK] Block Already Knows Where to Look in Diffusion LLM](https://arxiv.org/abs/2602.14209)
*Omin Kwon,Yeonjae Kim,Doyeon Kim,Minseo Kim,Yeonhong Park,Jae W. Lee*

Main category: cs.LG

TL;DR: MAGE introduces a training-free sparse attention method for block diffusion LLMs that uses attention patterns from the first denoising step to guide efficient KV caching, achieving near-lossless accuracy with 3-4x speedup.


<details>
  <summary>Details</summary>
Motivation: Block diffusion LLMs show promise but suffer from memory bottlenecks due to KV caching in long-context settings. Existing sparse attention methods designed for autoregressive LLMs perform poorly when adapted to block diffusion.

Method: MAGE leverages the observation that attention at the first All-[MASK] denoising step reliably predicts important KV entries and budget requirements. It performs a single exact attention pass per block and reuses it for training-free sparse denoising. A lightweight fine-tuning strategy strengthens [MASK]-guided patterns with minimal training cost.

Result: Across long-context benchmarks (LongBench and Needle-in-a-Haystack), MAGE achieves near-lossless accuracy with a fraction of the KV budget while delivering 3-4x end-to-end speedup, consistently outperforming AR-oriented sparse attention baselines.

Conclusion: MAGE demonstrates that block diffusion LLMs have unique properties that enable efficient sparse attention without training overhead, offering a practical solution to memory bottlenecks in long-context generation while maintaining accuracy.

Abstract: Block diffusion LLMs are emerging as a promising next paradigm for language generation, but their use of KV caching makes memory access a dominant bottleneck in long-context settings. While dynamic sparse attention has been actively explored, existing methods designed for autoregressive LLMs rely on approximate importance estimation and perform poorly when adapted to block diffusion. This work identifies a key opportunity unique to block diffusion: attention at the first All-[MASK] denoising step reliably predicts important KV entries and budget requirements, enabling MAGE to perform a single exact attention pass per block and reuse it for training-free sparse denoising. Across long-context benchmarks including LongBench and Needle-in-a-Haystack, MAGE achieves near-lossless accuracy with a fraction of the KV budget while delivering up to 3-4x end-to-end speedup, consistently outperforming AR-oriented sparse attention baselines. A lightweight fine-tuning strategy further strengthens [MASK]-guided patterns with minimal cost, requiring only a few hours of training on a single NVIDIA H100 GPU for both 1.5B and 7B models.

</details>


### [463] [Robust multi-task boosting using clustering and local ensembling](https://arxiv.org/abs/2602.14231)
*Seyedsaman Emami,Daniel Hernández-Lobato,Gonzalo Martínez-Muñoz*

Main category: cs.LG

TL;DR: RMB-CLE is a robust multi-task learning framework that uses error-based task clustering and local ensembling to prevent negative transfer and improve predictive performance.


<details>
  <summary>Details</summary>
Motivation: Conventional MTL methods suffer from negative transfer when unrelated or noisy tasks are forced to share representations, which degrades performance.

Method: Uses error-based task clustering (deriving inter-task similarity from cross-task errors with risk decomposition) and local ensembling within clusters. Tasks are grouped adaptively via agglomerative clustering.

Result: Outperforms multi-task, single-task, and pooling-based ensemble methods across diverse benchmarks. Recovers ground-truth clusters in synthetic data.

Conclusion: RMB-CLE is a general, scalable framework that establishes a new basis for robust multi-task learning, not just a combination of clustering and boosting.

Abstract: Multi-Task Learning (MTL) aims to boost predictive performance by sharing information across related tasks, yet conventional methods often suffer from negative transfer when unrelated or noisy tasks are forced to share representations. We propose Robust Multi-Task Boosting using Clustering and Local Ensembling (RMB-CLE), a principled MTL framework that integrates error-based task clustering with local ensembling. Unlike prior work that assumes fixed clusters or hand-crafted similarity metrics, RMB-CLE derives inter-task similarity directly from cross-task errors, which admit a risk decomposition into functional mismatch and irreducible noise, providing a theoretically grounded mechanism to prevent negative transfer. Tasks are grouped adaptively via agglomerative clustering, and within each cluster, a local ensemble enables robust knowledge sharing while preserving task-specific patterns. Experiments show that RMB-CLE recovers ground-truth clusters in synthetic data and consistently outperforms multi-task, single-task, and pooling-based ensemble methods across diverse real-world and synthetic benchmarks. These results demonstrate that RMB-CLE is not merely a combination of clustering and boosting but a general and scalable framework that establishes a new basis for robust multi-task learning.

</details>


### [464] [Evaluating LLMs in Finance Requires Explicit Bias Consideration](https://arxiv.org/abs/2602.14233)
*Yaxuan Kong,Hoyoung Lee,Yoontae Hwang,Alejandro Lopez-Lira,Bradford Levy,Dhagash Mehta,Qingsong Wen,Chanyeol Choi,Yongjae Lee,Stefan Zohren*

Main category: cs.LG

TL;DR: The paper identifies five key biases in financial LLM applications (look-ahead, survivorship, narrative, objective, cost) that inflate performance and contaminate results, proposes a Structural Validity Framework and checklist for bias mitigation.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in finance, but evaluation practices haven't kept up, allowing finance-specific biases to inflate performance metrics, contaminate backtests, and make reported results unreliable for deployment claims.

Method: The authors reviewed 164 papers from 2023-2025 to identify recurring biases, then proposed a Structural Validity Framework with an evaluation checklist for bias diagnosis and system design.

Result: Found that no single bias is discussed in more than 28% of studies, revealing widespread neglect of bias issues in financial LLM research. The biases compound to create an illusion of validity.

Conclusion: Bias in financial LLM systems requires explicit attention, and structural validity should be enforced before any result is used to support deployment claims. The proposed framework provides minimal requirements for bias mitigation.

Abstract: Large Language Models (LLMs) are increasingly integrated into financial workflows, but evaluation practice has not kept up. Finance-specific biases can inflate performance, contaminate backtests, and make reported results useless for any deployment claim. We identify five recurring biases in financial LLM applications. They include look-ahead bias, survivorship bias, narrative bias, objective bias, and cost bias. These biases break financial tasks in distinct ways and they often compound to create an illusion of validity. We reviewed 164 papers from 2023 to 2025 and found that no single bias is discussed in more than 28 percent of studies. This position paper argues that bias in financial LLM systems requires explicit attention and that structural validity should be enforced before any result is used to support a deployment claim. We propose a Structural Validity Framework and an evaluation checklist with minimal requirements for bias diagnosis and future system design. The material is available at https://github.com/Eleanorkong/Awesome-Financial-LLM-Bias-Mitigation.

</details>


### [465] [Multi-Agent Debate: A Unified Agentic Framework for Tabular Anomaly Detection](https://arxiv.org/abs/2602.14251)
*Pinqiao Wang,Sheng Li*

Main category: cs.LG

TL;DR: MAD is a multi-agent debating framework for tabular anomaly detection that treats model disagreement as a signal and resolves it through a mathematically grounded coordination layer.


<details>
  <summary>Details</summary>
Motivation: Current tabular anomaly detection uses single detectors or static ensembles, but strong performance requires heterogeneous model families that often disagree under distribution shift, missingness, and rare-anomaly regimes.

Method: Multi-agent framework where each agent is an ML-based detector producing anomaly scores, confidence, and evidence, augmented by LLM-based critics. A coordinator converts messages into bounded per-agent losses and updates agent influence via exponentiated-gradient rule.

Result: MAD shows improved robustness over baselines on diverse tabular anomaly benchmarks and provides clearer traces of model disagreement. It can recover existing approaches and has regret guarantees with conformal calibration for false positive control.

Conclusion: MAD provides a unified agentic framework that leverages model disagreement as a valuable signal for more robust tabular anomaly detection with auditability.

Abstract: Tabular anomaly detection is often handled by single detectors or static ensembles, even though strong performance on tabular data typically comes from heterogeneous model families (e.g., tree ensembles, deep tabular networks, and tabular foundation models) that frequently disagree under distribution shift, missingness, and rare-anomaly regimes. We propose MAD, a Multi-Agent Debating framework that treats this disagreement as a first-class signal and resolves it through a mathematically grounded coordination layer. Each agent is a machine learning (ML)-based detector that produces a normalized anomaly score, confidence, and structured evidence, augmented by a large language model (LLM)-based critic. A coordinator converts these messages into bounded per-agent losses and updates agent influence via an exponentiated-gradient rule, yielding both a final debated anomaly score and an auditable debate trace. MAD is a unified agentic framework that can recover existing approaches, such as mixture-of-experts gating and learning-with-expert-advice aggregation, by restricting the message space and synthesis operator. We establish regret guarantees for the synthesized losses and show how conformal calibration can wrap the debated score to control false positives under exchangeability. Experiments on diverse tabular anomaly benchmarks show improved robustness over baselines and clearer traces of model disagreement

</details>


### [466] [Cross-household Transfer Learning Approach with LSTM-based Demand Forecasting](https://arxiv.org/abs/2602.14267)
*Manal Rahal,Bestoun S. Ahmed,Roger Renström,Robert Stener*

Main category: cs.LG

TL;DR: DELTAiF is a transfer learning framework that enables scalable prediction of household hot water consumption by leveraging knowledge from a representative household, reducing training time by ~67% while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: With rapid increase in residential heat pump installations, optimizing hot water production faces scalability challenges. Training separate ML models for each household is computationally expensive, especially in cloud-connected deployments.

Method: DELTAiF uses transfer learning to predict large hot water usage events (like showers). It learns from a representative household and fine-tunes across others, eliminating need for separate models per installation.

Result: Reduces overall training time by approximately 67% while maintaining high predictive accuracy (0.874-0.991) and low mean absolute percentage error (0.001-0.017). Transfer learning is particularly effective when source household has regular consumption patterns.

Conclusion: DELTAiF enables scalable and accurate hot water demand forecasting for residential heat pumps through transfer learning, addressing computational challenges of per-household modeling while maintaining prediction quality.

Abstract: With the rapid increase in residential heat pump (HP) installations, optimizing hot water production in households is essential, yet it faces major technical and scalability challenges. Adapting production to actual household needs requires accurate forecasting of hot water demand to ensure comfort and, most importantly, to reduce energy waste. However, the conventional approach of training separate machine learning models for each household becomes computationally expensive at scale, particularly in cloud-connected HP deployments.
  This study introduces DELTAiF, a transfer learning (TL) based framework that provides scalable and accurate prediction of household hot water consumption. By predicting large hot water usage events, such as showers, DELTAiF enables adaptive yet scalable hot water production at the household level. DELTAiF leverages learned knowledge from a representative household and fine-tunes it across others, eliminating the need to train separate machine learning models for each HP installation. This approach reduces overall training time by approximately 67 percent while maintaining high predictive accuracy values between 0.874 and 0.991, and mean absolute percentage error values between 0.001 and 0.017. The results show that TL is particularly effective when the source household exhibits regular consumption patterns, enabling hot water demand forecasting at scale.

</details>


### [467] [Radial-VCReg: More Informative Representation Learning Through Radial Gaussianization](https://arxiv.org/abs/2602.14272)
*Yilun Kuang,Yash Dagade,Deep Chakraborty,Erik Learned-Miller,Randall Balestriero,Tim G. J. Rudner,Yann LeCun*

Main category: cs.LG

TL;DR: Radial-VCReg improves self-supervised learning by adding radial Gaussianization to VCReg, aligning feature norms with Chi distribution for better information maximization.


<details>
  <summary>Details</summary>
Motivation: Self-supervised learning aims for maximally informative representations, but explicit information maximization faces the curse of dimensionality. Existing methods like VCReg only regularize first and second-order statistics, which cannot fully achieve maximum entropy.

Method: Proposes Radial-VCReg, which augments VCReg with a radial Gaussianization loss that aligns feature norms with the Chi distribution - a defining property of high-dimensional Gaussians.

Result: Radial-VCReg transforms a broader class of distributions towards normality compared to VCReg, and consistently improves performance on synthetic and real-world datasets by reducing higher-order dependencies and promoting more diverse and informative representations.

Conclusion: Radial-VCReg effectively addresses the limitations of VCReg by incorporating radial Gaussianization, leading to better information maximization and improved representation learning performance.

Abstract: Self-supervised learning aims to learn maximally informative representations, but explicit information maximization is hindered by the curse of dimensionality. Existing methods like VCReg address this by regularizing first and second-order feature statistics, which cannot fully achieve maximum entropy. We propose Radial-VCReg, which augments VCReg with a radial Gaussianization loss that aligns feature norms with the Chi distribution-a defining property of high-dimensional Gaussians. We prove that Radial-VCReg transforms a broader class of distributions towards normality compared to VCReg and show on synthetic and real-world datasets that it consistently improves performance by reducing higher-order dependencies and promoting more diverse and informative representations.

</details>


### [468] [Integrating Unstructured Text into Causal Inference: Empirical Evidence from Real Data](https://arxiv.org/abs/2602.14274)
*Boning Zhou,Ziyu Wang,Han Hong,Haoqi Hu*

Main category: cs.LG

TL;DR: Transformer-based framework enables causal inference from unstructured text, matching structured data results


<details>
  <summary>Details</summary>
Motivation: Traditional causal inference relies on structured data, but in real-world scenarios this data is often incomplete or unavailable, limiting decision-making capabilities

Method: Develops a framework using transformer-based language models to perform causal inference directly from unstructured text data

Result: Causal estimates from unstructured text consistently match those from structured data across population, group, and individual levels

Conclusion: Unstructured text can effectively substitute for structured data in causal inference, extending applicability to scenarios where only textual data is available

Abstract: Causal inference, a critical tool for informing business decisions, traditionally relies heavily on structured data. However, in many real-world scenarios, such data can be incomplete or unavailable. This paper presents a framework that leverages transformer-based language models to perform causal inference using unstructured text. We demonstrate the effectiveness of our framework by comparing causal estimates derived from unstructured text against those obtained from structured data across population, group, and individual levels. Our findings show consistent results between the two approaches, validating the potential of unstructured text in causal inference tasks. Our approach extends the applicability of causal inference methods to scenarios where only textual data is available, enabling data-driven business decision-making when structured tabular data is scarce.

</details>


### [469] [Reverse N-Wise Output-Oriented Testing for AI/ML and Quantum Computing Systems](https://arxiv.org/abs/2602.14275)
*Lamine Rihani*

Main category: cs.LG

TL;DR: Reverse n-wise output testing: A paradigm inversion that constructs covering arrays over output equivalence classes (ML confidence buckets, fairness partitions, quantum measurement distributions) and uses metaheuristic optimization to synthesize inputs that elicit targeted behavioral signatures from opaque AI/ML and quantum systems.


<details>
  <summary>Details</summary>
Motivation: AI/ML and quantum computing systems present unprecedented testing challenges: high-dimensional continuous input spaces, probabilistic outputs, behavioral correctness defined only through observable predictions/measurements, and critical quality dimensions (trustworthiness, fairness, calibration, robustness) that manifest through complex multi-way interactions among output properties rather than deterministic input-output mappings.

Method: Reverse n-wise output testing constructs covering arrays directly over domain-specific output equivalence classes (ML confidence calibration buckets, decision boundary regions, fairness partitions, embedding clusters, ranking stability bands, quantum measurement outcome distributions, error syndrome patterns). It then solves the black-box inverse mapping problem via gradient-free metaheuristic optimization to synthesize input feature configurations or quantum circuit parameters capable of eliciting targeted behavioral signatures.

Result: The framework delivers synergistic benefits: explicit customer-centric prediction/measurement coverage guarantees, substantial improvements in fault detection rates for ML calibration/boundary failures and quantum error syndromes, enhanced test suite efficiency, and structured MLOps/quantum validation pipelines with automated partition discovery from uncertainty analysis and coverage drift monitoring.

Conclusion: Reverse n-wise output testing provides a mathematically principled paradigm inversion for testing opaque AI/ML and quantum systems, addressing their unique challenges through output-space coverage and inverse mapping optimization, enabling more effective validation of critical quality dimensions in these complex probabilistic systems.

Abstract: Artificial intelligence/machine learning (AI/ML) systems and emerging quantum computing software present unprecedented testing challenges characterized by high-dimensional/continuous input spaces, probabilistic/non-deterministic output distributions, behavioral correctness defined exclusively over observable prediction behaviors and measurement outcomes, and critical quality dimensions, trustworthiness, fairness, calibration, robustness, error syndrome patterns, that manifest through complex multi-way interactions among semantically meaningful output properties rather than deterministic input-output mappings. This paper introduces reverse n-wise output testing, a mathematically principled paradigm inversion that constructs covering arrays directly over domain-specific output equivalence classes, ML confidence calibration buckets, decision boundary regions, fairness partitions, embedding clusters, ranking stability bands, quantum measurement outcome distributions (0-dominant, 1-dominant, superposition collapse), error syndrome patterns (bit-flip, phase-flip, correlated errors), then solves the computationally challenging black-box inverse mapping problem via gradient-free metaheuristic optimization to synthesize input feature configurations or quantum circuit parameters capable of eliciting targeted behavioral signatures from opaque models. The framework delivers synergistic benefits across both domains: explicit customer-centric prediction/measurement coverage guarantees, substantial improvements in fault detection rates for ML calibration/boundary failures and quantum error syndromes, enhanced test suite efficiency, and structured MLOps/quantum validation pipelines with automated partition discovery from uncertainty analysis and coverage drift monitoring.

</details>


### [470] [Whom to Query for What: Adaptive Group Elicitation via Multi-Turn LLM Interactions](https://arxiv.org/abs/2602.14279)
*Ruomeng Ding,Tianwei Gao,Thomas P. Zollo,Eitan Bachmat,Richard Zemel,Zhun Deng*

Main category: cs.LG

TL;DR: A framework for adaptive group elicitation that combines LLM-based question scoring with graph neural networks to optimize both question selection and respondent sampling under budget constraints.


<details>
  <summary>Details</summary>
Motivation: Existing elicitation methods don't adaptively select respondents or leverage population structure when dealing with partial/incomplete responses, especially under real-world budget constraints for both queries and participation.

Method: Proposes a multi-round adaptive framework with: (1) LLM-based expected information gain scoring for candidate questions, and (2) heterogeneous graph neural network propagation to aggregate responses and participant attributes, impute missing data, and guide respondent selection.

Result: Across three real-world opinion datasets, the method consistently improves population-level response prediction under constrained budgets, achieving >12% relative gain on CES at 10% respondent budget.

Conclusion: The closed-loop framework effectively queries a small, informative subset of individuals while inferring population-level responses through structured similarity, addressing the gap in adaptive group elicitation with budget constraints.

Abstract: Eliciting information to reduce uncertainty about latent group-level properties from surveys and other collective assessments requires allocating limited questioning effort under real costs and missing data. Although large language models enable adaptive, multi-turn interactions in natural language, most existing elicitation methods optimize what to ask with a fixed respondent pool, and do not adapt respondent selection or leverage population structure when responses are partial or incomplete. To address this gap, we study adaptive group elicitation, a multi-round setting where an agent adaptively selects both questions and respondents under explicit query and participation budgets. We propose a theoretically grounded framework that combines (i) an LLM-based expected information gain objective for scoring candidate questions with (ii) heterogeneous graph neural network propagation that aggregates observed responses and participant attributes to impute missing responses and guide per-round respondent selection. This closed-loop procedure queries a small, informative subset of individuals while inferring population-level responses via structured similarity. Across three real-world opinion datasets, our method consistently improves population-level response prediction under constrained budgets, including a >12% relative gain on CES at a 10% respondent budget.

</details>


### [471] [KernelBlaster: Continual Cross-Task CUDA Optimization via Memory-Augmented In-Context Reinforcement Learning](https://arxiv.org/abs/2602.14293)
*Kris Shengjun Dong,Sahil Modi,Dima Nikiforov,Sana Damani,Edward Lin,Siva Kumar Sastry Hari,Christos Kozyrakis*

Main category: cs.LG

TL;DR: KernelBlaster is a Memory-Augmented In-context Reinforcement Learning framework that improves LLM-based GPU coding agents' ability to optimize CUDA code across multiple GPU generations by accumulating optimization knowledge in a retrievable knowledge base.


<details>
  <summary>Details</summary>
Motivation: Optimizing CUDA code across multiple GPU generations is challenging due to complex hardware-specific optimization spaces. Traditional compilers use fixed heuristics, fine-tuning LLMs is expensive, and existing agentic workflows can't effectively aggregate prior knowledge, leading to biased sampling and suboptimal solutions.

Method: Proposes KernelBlaster, a Memory-Augmented In-context Reinforcement Learning (MAIC-RL) framework with a Persistent CUDA Knowledge Base that accumulates optimization knowledge. Uses a novel profile-guided, textual-gradient-based agentic flow for CUDA generation and optimization to systematically explore high-potential strategies beyond naive rewrites.

Result: Achieves geometric mean speedups of 1.43x, 2.50x, and 1.50x on KernelBench Levels 1, 2, and 3 respectively compared to PyTorch baseline. The framework is released as open-source with test harness, verification components, and reproducible evaluation pipeline.

Conclusion: KernelBlaster effectively addresses CUDA optimization challenges across GPU generations by enabling LLM agents to learn from experience and make systematically informed decisions through persistent knowledge accumulation, outperforming traditional approaches.

Abstract: Optimizing CUDA code across multiple generations of GPU architectures is challenging, as achieving peak performance requires an extensive exploration of an increasingly complex, hardware-specific optimization space. Traditional compilers are constrained by fixed heuristics, whereas finetuning Large Language Models (LLMs) can be expensive. However, agentic workflows for CUDA code optimization have limited ability to aggregate knowledge from prior exploration, leading to biased sampling and suboptimal solutions. We propose KernelBlaster, a Memory-Augmented In-context Reinforcement Learning (MAIC-RL) framework designed to improve CUDA optimization search capabilities of LLM-based GPU coding agents. KernelBlaster enables agents to learn from experience and make systematically informed decisions on future tasks by accumulating knowledge into a retrievable Persistent CUDA Knowledge Base. We propose a novel profile-guided, textual-gradient-based agentic flow for CUDA generation and optimization to achieve high performance across generations of GPU architectures. KernelBlaster guides LLM agents to systematically explore high-potential optimization strategies beyond naive rewrites. Compared to the PyTorch baseline, our method achieves geometric mean speedups of 1.43x, 2.50x, and 1.50x on KernelBench Levels 1, 2, and 3, respectively. We release KernelBlaster as an open-source agentic framework, accompanied by a test harness, verification components, and a reproducible evaluation pipeline.

</details>


### [472] [Machine Learning as a Tool (MLAT): A Framework for Integrating Statistical ML Models as Callable Tools within LLM Agent Workflows](https://arxiv.org/abs/2602.14295)
*Edwin Chen,Zulekha Bibi*

Main category: cs.LG

TL;DR: MLAT is a design pattern that exposes pre-trained ML models as callable tools within LLM agent workflows, enabling orchestrating agents to invoke quantitative predictions when needed and reason about outputs in context.


<details>
  <summary>Details</summary>
Motivation: To move beyond conventional pipelines that treat ML inference as static preprocessing, and instead position ML models as first-class tools that LLMs can dynamically invoke based on conversational context, enabling better integration of quantitative predictions with contextual reasoning.

Method: Introduces MLAT framework where pre-trained ML models are exposed as callable tools within LLM agent workflows. Validated with PitchCraft system using two agents: Research Agent for prospect intelligence gathering and Draft Agent that invokes XGBoost pricing model as tool call. Uses structured outputs and training methodology for extreme data scarcity (70 examples combining real and synthetic data).

Result: PitchCraft system reduces proposal generation time from multiple hours to under 10 minutes. Pricing model achieves R^2 = 0.807 on held-out data with mean absolute error of 3688 USD. Demonstrates meaningful learned relationships through sensitivity analysis.

Conclusion: MLAT successfully generalizes to domains requiring quantitative estimation combined with contextual reasoning, positioning ML models as dynamic tools within LLM workflows rather than static preprocessing steps.

Abstract: We introduce Machine Learning as a Tool (MLAT), a design pattern in which pre-trained statistical machine learning models are exposed as callable tools within large language model (LLM) agent workflows. This allows an orchestrating agent to invoke quantitative predictions when needed and reason about their outputs in context. Unlike conventional pipelines that treat ML inference as a static preprocessing step, MLAT positions the model as a first-class tool alongside web search, database queries, and APIs, enabling the LLM to decide when and how to use it based on conversational context.
  To validate MLAT, we present PitchCraft, a pilot production system that converts discovery call recordings into professional proposals with ML-predicted pricing. The system uses two agents: a Research Agent that gathers prospect intelligence via parallel tool calls, and a Draft Agent that invokes an XGBoost pricing model as a tool call and generates a complete proposal through structured outputs. The pricing model, trained on 70 examples combining real and human-verified synthetic data, achieves R^2 = 0.807 on held-out data with a mean absolute error of 3688 USD. The system reduces proposal generation time from multiple hours to under 10 minutes.
  We describe the MLAT framework, structured output architecture, training methodology under extreme data scarcity, and sensitivity analysis demonstrating meaningful learned relationships. MLAT generalizes to domains requiring quantitative estimation combined with contextual reasoning.

</details>


### [473] [DeepFusion: Accelerating MoE Training via Federated Knowledge Distillation from Heterogeneous Edge Devices](https://arxiv.org/abs/2602.14301)
*Songyuan Li,Jia Hu,Ahmed M. Abdelmoniem,Geyong Min,Haojun Huang,Jiwei Huang*

Main category: cs.LG

TL;DR: DeepFusion: A scalable federated MoE training framework that enables heterogeneous on-device LLMs to contribute knowledge via federated knowledge distillation with View-Aligned Attention, achieving near-centralized performance with reduced communication costs.


<details>
  <summary>Details</summary>
Motivation: MoE-based LLMs require vast training data, but traditional FL approaches are impractical for resource-constrained devices that cannot host local MoE models. There's a need for privacy-preserving MoE training that can leverage heterogeneous edge device data without requiring devices to host full MoE models.

Method: DeepFusion allows devices to independently configure and train on-device LLMs tailored to their needs/hardware. It introduces View-Aligned Attention (VAA) module that integrates multi-stage feature representations from the global MoE model to create predictive perspectives aligned with on-device LLMs, enabling effective cross-architecture knowledge distillation and solving view-mismatch problems.

Result: DeepFusion achieves performance close to centralized MoE training with industry-level models (Qwen-MoE, DeepSeek-MoE) and real-world datasets (medical, finance). It reduces communication costs by up to 71% and improves token perplexity by up to 5.28% compared to federated MoE baselines.

Conclusion: DeepFusion provides a practical solution for scalable federated MoE training that addresses resource constraints of edge devices while maintaining privacy and achieving high performance comparable to centralized training.

Abstract: Recent Mixture-of-Experts (MoE)-based large language models (LLMs) such as Qwen-MoE and DeepSeek-MoE are transforming generative AI in natural language processing. However, these models require vast and diverse training data. Federated learning (FL) addresses this challenge by leveraging private data from heterogeneous edge devices for privacy-preserving MoE training. Nonetheless, traditional FL approaches require devices to host local MoE models, which is impractical for resource-constrained devices due to large model sizes. To address this, we propose DeepFusion, the first scalable federated MoE training framework that enables the fusion of heterogeneous on-device LLM knowledge via federated knowledge distillation, yielding a knowledge-abundant global MoE model. Specifically, DeepFusion features each device to independently configure and train an on-device LLM tailored to its own needs and hardware limitations. Furthermore, we propose a novel View-Aligned Attention (VAA) module that integrates multi-stage feature representations from the global MoE model to construct a predictive perspective aligned with on-device LLMs, thereby enabling effective cross-architecture knowledge distillation. By explicitly aligning predictive perspectives, VAA resolves the view-mismatch problem in traditional federated knowledge distillation, which arises from heterogeneity in model architectures and prediction behaviors between on-device LLMs and the global MoE model. Experiments with industry-level MoE models (Qwen-MoE and DeepSeek-MoE) and real-world datasets (medical and finance) demonstrate that DeepFusion achieves performance close to centralized MoE training. Compared with key federated MoE baselines, DeepFusion reduces communication costs by up to 71% and improves token perplexity by up to 5.28%.

</details>


### [474] [In Transformer We Trust? A Perspective on Transformer Architecture Failure Modes](https://arxiv.org/abs/2602.14318)
*Trishit Mondal,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: A comprehensive review examining the trustworthiness of transformer models across high-stakes applications, evaluating reliability through interpretability, robustness, fairness, and privacy metrics.


<details>
  <summary>Details</summary>
Motivation: Transformers are increasingly deployed in critical applications (healthcare, autonomous systems, climate modeling, etc.), necessitating rigorous assessment of their trustworthiness to ensure safe and reliable deployment.

Method: Systematic evaluation through comprehensive review of interpretability, explainability, robustness against adversarial attacks, fairness, and privacy aspects across diverse safety-critical domains.

Result: Identifies recurring structural vulnerabilities, domain-specific risks, and open research challenges that limit reliable deployment of transformers in high-stakes applications.

Conclusion: Transformers require deeper trustworthiness assessment before widespread deployment in critical applications; the paper provides a framework for evaluating reliability and identifies key research gaps.

Abstract: Transformer architectures have revolutionized machine learning across a wide range of domains, from natural language processing to scientific computing. However, their growing deployment in high-stakes applications, such as computer vision, natural language processing, healthcare, autonomous systems, and critical areas of scientific computing including climate modeling, materials discovery, drug discovery, nuclear science, and robotics, necessitates a deeper and more rigorous understanding of their trustworthiness. In this work, we critically examine the foundational question: \textitHow trustworthy are transformer models?} We evaluate their reliability through a comprehensive review of interpretability, explainability, robustness against adversarial attacks, fairness, and privacy. We systematically examine the trustworthiness of transformer-based models in safety-critical applications spanning natural language processing, computer vision, and science and engineering domains, including robotics, medicine, earth sciences, materials science, fluid dynamics, nuclear science, and automated theorem proving; highlighting high-impact areas where these architectures are central and analyzing the risks associated with their deployment. By synthesizing insights across these diverse areas, we identify recurring structural vulnerabilities, domain-specific risks, and open research challenges that limit the reliable deployment of transformers.

</details>


### [475] [Conformal Signal Temporal Logic for Robust Reinforcement Learning Control: A Case Study](https://arxiv.org/abs/2602.14322)
*Hani Beirami,M M Manjurul Islam*

Main category: cs.LG

TL;DR: Conformal STL shield enhances RL flight control safety by filtering actions to maintain airspeed specifications, outperforming baseline PPO and classical shields in robustness.


<details>
  <summary>Details</summary>
Motivation: To improve safety and robustness of reinforcement learning control in aerospace applications by enforcing formal temporal logic specifications at runtime, addressing challenges like model mismatch, actuator limits, noise, and setpoint changes.

Method: Train PPO agent for F-16 throttle control to track airspeed, encode objective as Signal Temporal Logic (STL) requirement, and introduce conformal STL shield using online conformal prediction to filter RL actions at runtime.

Result: Conformal shield preserves STL satisfaction while maintaining near baseline performance, providing stronger robustness guarantees than classical shield under stress scenarios with model mismatch, rate limits, noise, and setpoint jumps.

Conclusion: Combining formal specification monitoring with data-driven RL control significantly improves autonomous flight control reliability in challenging environments, demonstrating practical value of conformal shielding for safety-critical aerospace applications.

Abstract: We investigate how formal temporal logic specifications can enhance the safety and robustness of reinforcement learning (RL) control in aerospace applications. Using the open source AeroBench F-16 simulation benchmark, we train a Proximal Policy Optimization (PPO) agent to regulate engine throttle and track commanded airspeed. The control objective is encoded as a Signal Temporal Logic (STL) requirement to maintain airspeed within a prescribed band during the final seconds of each maneuver. To enforce this specification at run time, we introduce a conformal STL shield that filters the RL agent's actions using online conformal prediction. We compare three settings: (i) PPO baseline, (ii) PPO with a classical rule-based STL shield, and (iii) PPO with the proposed conformal shield, under both nominal conditions and a severe stress scenario involving aerodynamic model mismatch, actuator rate limits, measurement noise, and mid-episode setpoint jumps. Experiments show that the conformal shield preserves STL satisfaction while maintaining near baseline performance and providing stronger robustness guarantees than the classical shield. These results demonstrate that combining formal specification monitoring with data driven RL control can substantially improve the reliability of autonomous flight control in challenging environments.

</details>


### [476] [Train Less, Learn More: Adaptive Efficient Rollout Optimization for Group-Based Reinforcement Learning](https://arxiv.org/abs/2602.14338)
*Zhi Zhang,Zhen Han,Costas Mavromatis,Qi Zhu,Yunyi Zhang,Sheng Guan,Dingmin Wang,Xiong Zhou,Shuai Wang,Soji Adeshina,Vassilis Ioannidis,Huzefa Rangwala*

Main category: cs.LG

TL;DR: AERO improves RL fine-tuning efficiency by adaptively managing rollouts to avoid zero-gradient scenarios, reducing compute by ~48% while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: GRPO's fixed group size approach wastes compute when all rollouts in a group share the same outcome (all correct or all incorrect), resulting in zero gradient signals and inefficient fine-tuning.

Method: AERO enhances GRPO with three key components: adaptive rollout strategy (dynamically adjusts group size), selective rejection (strategically prunes rollouts), and Bayesian posterior maintenance (prevents zero-advantage dead zones).

Result: Across three model configurations, AERO reduces total training compute by ~48% and wall-clock time per step by ~45% on average, while matching or improving Pass@8 and Avg@8 metrics compared to GRPO.

Conclusion: AERO provides a practical, scalable, and compute-efficient strategy for RL-based LLM alignment that maintains performance while significantly reducing computational costs.

Abstract: Reinforcement learning (RL) plays a central role in large language model (LLM) post-training. Among existing approaches, Group Relative Policy Optimization (GRPO) is widely used, especially for RL with verifiable rewards (RLVR) fine-tuning. In GRPO, each query prompts the LLM to generate a group of rollouts with a fixed group size $N$. When all rollouts in a group share the same outcome, either all correct or all incorrect, the group-normalized advantages become zero, yielding no gradient signal and wasting fine-tuning compute. We introduce Adaptive Efficient Rollout Optimization (AERO), an enhancement of GRPO. AERO uses an adaptive rollout strategy, applies selective rejection to strategically prune rollouts, and maintains a Bayesian posterior to prevent zero-advantage dead zones. Across three model configurations (Qwen2.5-Math-1.5B, Qwen2.5-7B, and Qwen2.5-7B-Instruct), AERO improves compute efficiency without sacrificing performance. Under the same total rollout budget, AERO reduces total training compute by about 48% while shortening wall-clock time per step by about 45% on average. Despite the substantial reduction in compute, AERO matches or improves Pass@8 and Avg@8 over GRPO, demonstrating a practical, scalable, and compute-efficient strategy for RL-based LLM alignment.

</details>


### [477] [Zero-Shot Instruction Following in RL via Structured LTL Representations](https://arxiv.org/abs/2602.14344)
*Mathias Jackermeier,Mattia Giuri,Jacques Cloete,Alessandro Abate*

Main category: cs.LG

TL;DR: A novel hierarchical neural architecture with attention mechanism for learning structured task representations from LTL specifications, enabling better zero-shot generalization in multi-task RL.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for instruction following in multi-task RL struggle to effectively capture the rich logical and temporal structure inherent in LTL specifications, limiting their ability to zero-shot execute novel tasks.

Method: Conditions policy on sequences of Boolean formulae from task automaton; uses hierarchical neural architecture to encode logical structure of formulae; introduces attention mechanism for reasoning about future subgoals.

Result: Experiments in complex environments demonstrate strong generalization capabilities and superior performance compared to existing approaches.

Conclusion: The proposed structured task representation approach effectively captures LTL specifications' logical and temporal structure, enabling better training and zero-shot generalization in multi-task reinforcement learning.

Abstract: We study instruction following in multi-task reinforcement learning, where an agent must zero-shot execute novel tasks not seen during training. In this setting, linear temporal logic (LTL) has recently been adopted as a powerful framework for specifying structured, temporally extended tasks. While existing approaches successfully train generalist policies, they often struggle to effectively capture the rich logical and temporal structure inherent in LTL specifications. In this work, we address these concerns with a novel approach to learn structured task representations that facilitate training and generalisation. Our method conditions the policy on sequences of Boolean formulae constructed from a finite automaton of the task. We propose a hierarchical neural architecture to encode the logical structure of these formulae, and introduce an attention mechanism that enables the policy to reason about future subgoals. Experiments in a variety of complex environments demonstrate the strong generalisation capabilities and superior performance of our approach.

</details>


### [478] [WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control](https://arxiv.org/abs/2602.14351)
*Mehran Aghabozorgi,Alireza Moazeni,Yanshu Zhang,Ke Li*

Main category: cs.LG

TL;DR: WIMLE extends IMLE to model-based RL to learn stochastic multi-modal world models with uncertainty estimation, using confidence-weighted transitions to improve stability and sample efficiency.


<details>
  <summary>Details</summary>
Motivation: Model-based RL suffers from compounding model error, unimodal world models that average over multi-modal dynamics, and overconfident predictions that bias learning, limiting practical performance despite promising sample efficiency.

Method: WIMLE extends Implicit Maximum Likelihood Estimation (IMLE) to model-based RL framework to learn stochastic, multi-modal world models without iterative sampling. It estimates predictive uncertainty via ensembles and latent sampling, and weights synthetic transitions by predicted confidence during training to preserve useful rollouts while attenuating bias from uncertain predictions.

Result: Across 40 continuous-control tasks in DeepMind Control, MyoSuite, and HumanoidBench, WIMLE achieves superior sample efficiency and competitive or better asymptotic performance than strong model-free and model-based baselines. On Humanoid-run, it improves sample efficiency by over 50% relative to strongest competitor, and on HumanoidBench solves 8 of 14 tasks (vs 4 for BRO and 5 for SimbaV2).

Conclusion: The results highlight the value of IMLE-based multi-modality and uncertainty-aware weighting for stable model-based RL, demonstrating that addressing model error, multi-modality, and overconfidence can significantly improve practical performance.

Abstract: Model-based reinforcement learning promises strong sample efficiency but often underperforms in practice due to compounding model error, unimodal world models that average over multi-modal dynamics, and overconfident predictions that bias learning. We introduce WIMLE, a model-based method that extends Implicit Maximum Likelihood Estimation (IMLE) to the model-based RL framework to learn stochastic, multi-modal world models without iterative sampling and to estimate predictive uncertainty via ensembles and latent sampling. During training, WIMLE weights each synthetic transition by its predicted confidence, preserving useful model rollouts while attenuating bias from uncertain predictions and enabling stable learning. Across $40$ continuous-control tasks spanning DeepMind Control, MyoSuite, and HumanoidBench, WIMLE achieves superior sample efficiency and competitive or better asymptotic performance than strong model-free and model-based baselines. Notably, on the challenging Humanoid-run task, WIMLE improves sample efficiency by over $50$\% relative to the strongest competitor, and on HumanoidBench it solves $8$ of $14$ tasks (versus $4$ for BRO and $5$ for SimbaV2). These results highlight the value of IMLE-based multi-modality and uncertainty-aware weighting for stable model-based RL.

</details>


### [479] [A Study on Multi-Class Online Fuzzy Classifiers for Dynamic Environments](https://arxiv.org/abs/2602.14375)
*Kensuke Ajimoto,Yuma Yamamoto,Yoshifumi Kusunoki,Tomoharu Nakashima*

Main category: cs.LG

TL;DR: Proposes multi-class online fuzzy classifier for dynamic environments, extending conventional two-class approaches to handle multiple classes in streaming data scenarios.


<details>
  <summary>Details</summary>
Motivation: Conventional online fuzzy classifiers only handle two-class problems, but real-world dynamic environments often require multi-class classification. There's a need to extend fuzzy classifiers to handle streaming data with multiple classes where patterns become available incrementally over time.

Method: Extends conventional fuzzy classifier framework to multi-class problems using fuzzy if-then rules with predetermined antecedent fuzzy sets and learned consequent real values. Operates in online framework where only few patterns are available at each time step, with subsequent patterns arriving incrementally.

Result: Evaluated through numerical experiments on synthetic dynamic data and several benchmark datasets. Performance of multi-class online fuzzy classifiers was assessed, though specific metrics not detailed in abstract.

Conclusion: Successfully extends online fuzzy classification to multi-class problems, demonstrating feasibility for dynamic environments where data arrives incrementally and multiple classes need to be distinguished.

Abstract: This paper proposes a multi-class online fuzzy classifier for dynamic environments. A fuzzy classifier comprises a set of fuzzy if-then rules where human users determine the antecedent fuzzy sets beforehand. In contrast, the consequent real values are determined by learning from training data. In an online framework, not all training dataset patterns are available beforehand. Instead, only a few patterns are available at a time step, and the subsequent patterns become available at the following time steps. The conventional online fuzzy classifier considered only two-class problems. This paper investigates the extension to the conventional fuzzy classifiers for multi-class problems. We evaluate the performance of the multi-class online fuzzy classifiers through numerical experiments on synthetic dynamic data and also several benchmark datasets.

</details>


### [480] [The geometry of invariant learning: an information-theoretic analysis of data augmentation and generalization](https://arxiv.org/abs/2602.14423)
*Abdelali Bouyahia,Frédéric LeBlanc,Mario Marchand*

Main category: cs.LG

TL;DR: The paper proposes an information-theoretic framework to analyze how data augmentation affects generalization and invariance learning, deriving bounds that decompose generalization gap into distributional divergence, algorithm stability, and augmentation sensitivity terms.


<details>
  <summary>Details</summary>
Motivation: While data augmentation is widely used to improve generalization by promoting invariance to label-irrelevant transformations, its theoretical role remains only partially understood. The authors aim to systematically account for the effect of augmentation on generalization and invariance learning.

Method: The authors propose an information-theoretic framework building upon mutual information-based generalization bounds. They model the augmented distribution as a composition of the original data distribution with a distribution over transformations, inducing an orbit-averaged loss function. Under sub-Gaussian assumptions, they derive generalization bounds decomposing the expected generalization gap into three interpretable terms. They introduce the notion of "group diameter" to connect bounds to augmentation geometry.

Result: The framework yields a new generalization bound that decomposes the expected generalization gap into: (1) distributional divergence between original and augmented data, (2) algorithm stability term, and (3) augmentation sensitivity term. The group diameter provides a unified control parameter that bounds all three terms and reveals an intrinsic trade-off: small diameters preserve data fidelity but offer limited regularization, while large diameters enhance stability at the cost of increased bias and sensitivity. Numerical experiments validate that the bound reliably tracks and predicts true generalization gap behavior.

Conclusion: The proposed information-theoretic framework provides a systematic way to understand how data augmentation affects generalization and invariance learning. The decomposition into three interpretable terms and the introduction of group diameter offer insights into the trade-offs involved in augmentation design, with practical implications for selecting appropriate augmentation strategies.

Abstract: Data augmentation is one of the most widely used techniques to improve generalization in modern machine learning, often justified by its ability to promote invariance to label-irrelevant transformations. However, its theoretical role remains only partially understood. In this work, we propose an information-theoretic framework that systematically accounts for the effect of augmentation on generalization and invariance learning. Our approach builds upon mutual information-based bounds, which relate the generalization gap to the amount of information a learning algorithm retains about its training data. We extend this framework by modeling the augmented distribution as a composition of the original data distribution with a distribution over transformations, which naturally induces an orbit-averaged loss function. Under mild sub-Gaussian assumptions on the loss function and the augmentation process, we derive a new generalization bound that decompose the expected generalization gap into three interpretable terms: (1) a distributional divergence between the original and augmented data, (2) a stability term measuring the algorithm dependence on training data, and (3) a sensitivity term capturing the effect of augmentation variability. To connect our bounds to the geometry of the augmentation group, we introduce the notion of group diameter, defined as the maximal perturbation that augmentations can induce in the input space. The group diameter provides a unified control parameter that bounds all three terms and highlights an intrinsic trade-off: small diameters preserve data fidelity but offer limited regularization, while large diameters enhance stability at the cost of increased bias and sensitivity. We validate our theoretical bounds with numerical experiments, demonstrating that it reliably tracks and predicts the behavior of the true generalization gap.

</details>


### [481] [A unified framework for evaluating the robustness of machine-learning interpretability for prospect risking](https://arxiv.org/abs/2602.14430)
*Prithwijit Chowdhury,Ahmad Mustafa,Mohit Prabhushankar,Ghassan AlRegib*

Main category: cs.LG

TL;DR: The paper proposes a unified framework using counterfactuals and causal concepts (necessity/sufficiency) to evaluate robustness of LIME and SHAP explanations on hydrocarbon prospect risking data.


<details>
  <summary>Details</summary>
Motivation: Machine learning classifiers for hydrocarbon prospect risking lack transparency, and existing XAI methods (LIME/SHAP) often produce conflicting explanations for complex data due to differing definitions of feature importance.

Method: Developed a unified framework that generates counterfactuals and quantifies necessity and sufficiency to perform robustness evaluation of LIME and SHAP explanations on high-dimensional structured prospect risking data.

Result: The robustness test provides deeper insights into model capabilities to handle erroneous data and identifies which XAI module works best with which model for hydrocarbon indication tasks.

Conclusion: Grounding feature importance rankings using causal concepts of necessity and sufficiency offers a more reliable and robust approach to improve trustworthiness of XAI explanations in hydrocarbon prospect risking.

Abstract: In geophysics, hydrocarbon prospect risking involves assessing the risks associated with hydrocarbon exploration by integrating data from various sources. Machine learning-based classifiers trained on tabular data have been recently used to make faster decisions on these prospects. The lack of transparency in the decision-making processes of such models has led to the emergence of explainable AI (XAI). LIME and SHAP are two such examples of these XAI methods which try to generate explanations of a particular decision by ranking the input features in terms of importance. However, explanations of the same scenario generated by these two different explanation strategies have shown to disagree or be different, particularly for complex data. This is because the definitions of "importance" and "relevance" differ for different explanation strategies. Thus, grounding these ranked features using theoretically backed causal ideas of necessity and sufficiency can prove to be a more reliable and robust way to improve the trustworthiness of the concerned explanation strategies.We propose a unified framework to generate counterfactuals as well as quantify necessity and sufficiency and use these to perform a robustness evaluation of the explanations provided by LIME and SHAP on high dimensional structured prospect risking data. This robustness test gives us deeper insights into the models capabilities to handle erronous data and which XAI module works best in pair with which model for our dataset for hydorcarbon indication.

</details>


### [482] [S2D: Selective Spectral Decay for Quantization-Friendly Conditioning of Neural Activations](https://arxiv.org/abs/2602.14432)
*Arnav Chavan,Nahush Lele,Udbhav Bamba,Sankalp Dayal,Aditi Raghunathan,Deepak Gupta*

Main category: cs.LG

TL;DR: S^2D (Selective Spectral Decay) is a novel conditioning method that reduces activation outliers in large transformers by selectively regularizing weight components with largest singular values, enabling better quantization performance.


<details>
  <summary>Details</summary>
Motivation: Activation outliers in large transformer models create excessive ranges that cause severe accuracy drops during quantization, with the problem worsening as models scale up (e.g., from CLIP to SigLIP/SigLIP2).

Method: Proposes Selective Spectral Decay (S^2D), a geometrically-principled conditioning method that surgically regularizes only the weight components corresponding to the largest singular values during fine-tuning, based on the insight linking activation outliers to dominant singular values.

Result: S^2D significantly reduces activation outliers and produces well-conditioned representations that are quantization-friendly, achieving up to 7% improved PTQ accuracy on ImageNet under W4A4 quantization and 4% gains when combined with QAT, with improvements generalizing across downstream tasks and vision-language models.

Conclusion: S^2D enables scaling of increasingly large and rigorously trained models without sacrificing deployment efficiency by addressing the fundamental challenge of activation outliers in quantization through targeted spectral regularization.

Abstract: Activation outliers in large-scale transformer models pose a fundamental challenge to model quantization, creating excessively large ranges that cause severe accuracy drops during quantization. We empirically observe that outlier severity intensifies with pre-training scale (e.g., progressing from CLIP to the more extensively trained SigLIP and SigLIP2). Through theoretical analysis as well as empirical correlation studies, we establish the direct link between these activation outliers and dominant singular values of the weights. Building on this insight, we propose Selective Spectral Decay ($S^2D$), a geometrically-principled conditioning method that surgically regularizes only the weight components corresponding to the largest singular values during fine-tuning. Through extensive experiments, we demonstrate that $S^2D$ significantly reduces activation outliers and produces well-conditioned representations that are inherently quantization-friendly. Models trained with $S^2D$ achieve up to 7% improved PTQ accuracy on ImageNet under W4A4 quantization and 4% gains when combined with QAT. These improvements also generalize across downstream tasks and vision-language models, enabling the scaling of increasingly large and rigorously trained models without sacrificing deployment efficiency.

</details>


### [483] [Broken Chains: The Cost of Incomplete Reasoning in LLMs](https://arxiv.org/abs/2602.14444)
*Ian Su,Gaurav Purushothaman,Jey Narayan,Ruhika Goel,Kevin Zhu,Sunishchal Dev,Yash More,Maheep Chaudhary*

Main category: cs.LG

TL;DR: Models perform differently under token constraints: code reasoning degrades gracefully while natural language reasoning collapses; hybrid reasoning underperforms; and robustness varies by model.


<details>
  <summary>Details</summary>
Motivation: Reasoning-specialized models use substantial compute for chain-of-thought traces, but reasoning tokens are costly. The paper investigates how different reasoning modalities (code, natural language, hybrid, none) perform under token constraints.

Method: Introduced a framework that constrains models to reason exclusively through code, comments, both, or neither, then systematically ablates token budgets to 10%, 30%, 50%, and 70% of optimal. Evaluated four frontier models (GPT-5.1, Gemini 3 Flash, DeepSeek-V3.2, Grok 4.1) across mathematical benchmarks (AIME, GSM8K, HMMT).

Result: (1) Truncated reasoning can hurt: DeepSeek-V3.2 achieves 53% with no reasoning but only 17% with truncated CoT at 50% budget; (2) Code degrades gracefully: Gemini's comments collapse to 0% while code maintains 43-47%; (3) Hybrid reasoning underperforms single modalities; (4) Robustness is model-dependent: Grok maintains 80-90% at 30% budget where OpenAI and DeepSeek collapse to 7-27%.

Conclusion: Incomplete reasoning chains actively mislead models, with implications for deploying reasoning-specialized systems under resource constraints. Code reasoning shows more graceful degradation than natural language reasoning under token constraints.

Abstract: Reasoning-specialized models like OpenAI's 5.1 and DeepSeek-V3.2 allocate substantial inference compute to extended chain-of-thought (CoT) traces, yet reasoning tokens incur significant costs. How do different reasoning modalities of code, natural language, hybrid, or none do perform under token constraints? We introduce a framework that constrains models to reason exclusively through code, comments, both, or neither, then systematically ablates token budgets to 10\%, 30\%, 50\%, and 70\% of optimal. We evaluate four frontier models (GPT-5.1, Gemini 3 Flash, DeepSeek-V3.2, Grok 4.1) across mathematical benchmarks (AIME, GSM8K, HMMT). Our findings reveal: (1) \textbf{truncated reasoning can hurt} as DeepSeek-V3.2 achieves 53\% with no reasoning but only 17\% with truncated CoT at 50\% budget; (2) \textbf{code degrades gracefully} as Gemini's comments collapse to 0\% while code maintains 43-47\%; (3) \textbf{hybrid reasoning underperforms} single modalities; (4) \textbf{robustness is model-dependent} as Grok maintains 80-90\% at 30\% budget where OpenAI and DeepSeek collapse to 7-27\%. These results suggest incomplete reasoning chains actively mislead models, with implications for deploying reasoning-specialized systems under resource constraints.

</details>


### [484] [Selective Synchronization Attention](https://arxiv.org/abs/2602.14445)
*Hasi Hays*

Main category: cs.LG

TL;DR: Proposes Selective Synchronization Attention (SSA), a biologically-inspired attention mechanism based on coupled oscillators that replaces standard self-attention with closed-form synchronization weights, offering sparsity, unified encoding, and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Transformers have become foundational but suffer from quadratic computational complexity and lack biological grounding. The authors aim to create a more efficient, biologically-inspired attention mechanism that addresses these limitations.

Method: SSA represents tokens as oscillators with learnable natural frequencies and phases. Attention weights are derived from synchronization strength between token pairs based on the Kuramoto model's steady-state solution. This creates a closed-form operator that avoids iterative ODE integration.

Result: SSA provides three key advantages: 1) natural sparsity from phase-locking thresholds, 2) unified positional-semantic encoding through frequency spectrum, and 3) single-pass closed-form computation. The Oscillatory Synchronization Network (OSN) shows non-uniform, head-diverse coupling patterns even at initialization.

Conclusion: SSA offers a biologically-grounded, computationally efficient alternative to standard self-attention with stronger architectural inductive bias, potentially enabling more efficient Transformer architectures while maintaining performance.

Abstract: The Transformer architecture has become the foundation of modern deep learning, yet its core self-attention mechanism suffers from quadratic computational complexity and lacks grounding in biological neural computation. We propose Selective Synchronization Attention (SSA), a novel attention mechanism that replaces the standard dot-product self-attention with a closed-form operator derived from the steady-state solution of the Kuramoto model of coupled oscillators. In SSA, each token is represented as an oscillator characterized by a learnable natural frequency and phase; the synchronization strength between token pairs, determined by a frequency-dependent coupling and phase-locking condition, serves as the attention weight. This formulation provides three key advantages: (i) natural sparsity arising from the phase-locking threshold, whereby tokens with incompatible frequencies automatically receive zero attention weight without explicit masking; (ii) unified positional-semantic encoding through the natural frequency spectrum, eliminating the need for separate positional encodings; and (iii) a single-pass, closed-form computation that avoids iterative ODE integration, with all components (coupling, order parameter, synchronization) derived from the oscillatory framework. We instantiate SSA within the Oscillatory Synchronization Network (OSN), a drop-in replacement for the Transformer block. Analysis of the synchronization matrices reveals non-uniform, head-diverse coupling patterns even at initialization, demonstrating a stronger architectural inductive bias than the approximately uniform attention produced by randomly initialized Transformers.

</details>


### [485] [WiSparse: Boosting LLM Inference Efficiency with Weight-Aware Mixed Activation Sparsity](https://arxiv.org/abs/2602.14452)
*Lei Chen,Yuan Meng,Xiaoyu Zhan,Zhi Wang,Wenwu Zhu*

Main category: cs.LG

TL;DR: WiSparse: A training-free activation sparsity method for LLMs that uses weight-aware mixed-granularity allocation to achieve better performance than existing methods at high sparsity ratios.


<details>
  <summary>Details</summary>
Motivation: LLMs have high inference costs due to dense computation and memory access. Existing training-free activation sparsity methods are suboptimal because they only use activation information and uniform sparsity ratios, ignoring weight importance and varying sensitivity across model blocks.

Method: WiSparse combines activation magnitudes with precomputed weight norms to identify salient channels. It uses mixed-granularity allocation: global budget distribution across blocks via evolutionary search to protect sensitive regions, then refined within blocks to minimize reconstruction error. Also improves sparse kernels.

Result: At 50% sparsity, WiSparse preserves 97% of Llama3.1's dense performance, surpassing the strongest baseline by 2.23 percentage points while achieving 21.4% acceleration in end-to-end inference speed. Demonstrated effectiveness on three representative models.

Conclusion: WiSparse advances training-free approaches for efficient LLM inference by leveraging both activation and weight information with adaptive sparsity allocation, pushing the boundaries of achievable speedup without training.

Abstract: Large Language Models (LLMs) offer strong capabilities but incur high inference costs due to dense computation and memory access. Training-free activation sparsity is a promising approach for efficient LLM inference, yet existing methods often rely solely on activation information and uniform sparsity ratios. This overlooks the critical interplay with weights and inter-block sensitivity variation, leading to suboptimal performance. We identify two key phenomena in modern LLMs: 1) less significant activations may align with highly important weights, and 2) sparsity sensitivity varies non-monotonically across model blocks. We propose Weight-aware Mixed-Granularity Training-free Activation Sparsity (WiSparse), which leverages both activation and weight information for adaptive sparsity allocation. Specifically, we introduce a weight-aware mechanism integrating activation magnitudes with precomputed weight norms to accurately identify salient channels. This is combined with a mixed-granularity allocation scheme: a global budget is distributed across blocks via evolutionary search to protect sensitive regions, then refined within blocks to minimize reconstruction error. We improve sparse kernels and demonstrate effectiveness on three representative models. Notably, at 50% sparsity, WiSparse preserves 97% of Llama3.1's dense performance, surpassing the strongest baseline by 2.23 percentage points while achieving a 21.4% acceleration in end-to-end inference speed. Our research advances the limits of training-free approaches for efficient LLM inference, pushing the boundaries of achievable speedup without training.

</details>


### [486] [Traceable Latent Variable Discovery Based on Multi-Agent Collaboration](https://arxiv.org/abs/2602.14456)
*Huaming Du,Tao Hu,Yijie Huang,Yu Zhao,Guisong Liu,Tao Gu,Gang Kou,Carl Yang*

Main category: cs.LG

TL;DR: TLVD is a novel causal modeling framework that combines LLMs' metadata reasoning with traditional causal discovery algorithms to infer latent variables and their semantics, addressing limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional causal discovery algorithms face limitations due to lack of high-quality data, assumption of no latent confounders, and inability to capture precise semantics of latent variables, hindering broader application of causal discovery.

Method: Three-step approach: 1) Data-driven causal graph construction with latent variables, 2) Multi-LLM collaboration modeled as incomplete information game to infer specific latent variables via Bayesian Nash Equilibrium, 3) LLM-based evidence exploration for validation across real-world data sources.

Result: Extensive evaluation on three real patient datasets and two benchmark datasets shows TLVD achieves average improvements of 32.67% in accuracy, 62.21% in conditional accuracy, and 26.72% in ECit across all datasets.

Conclusion: TLVD effectively integrates LLMs' reasoning capabilities with traditional causal discovery to overcome limitations in latent variable inference, demonstrating significant performance improvements and practical applicability in real-world scenarios.

Abstract: Revealing the underlying causal mechanisms in the real world is crucial for scientific and technological progress. Despite notable advances in recent decades, the lack of high-quality data and the reliance of traditional causal discovery algorithms (TCDA) on the assumption of no latent confounders, as well as their tendency to overlook the precise semantics of latent variables, have long been major obstacles to the broader application of causal discovery. To address this issue, we propose a novel causal modeling framework, TLVD, which integrates the metadata-based reasoning capabilities of large language models (LLMs) with the data-driven modeling capabilities of TCDA for inferring latent variables and their semantics. Specifically, we first employ a data-driven approach to construct a causal graph that incorporates latent variables. Then, we employ multi-LLM collaboration for latent variable inference, modeling this process as a game with incomplete information and seeking its Bayesian Nash Equilibrium (BNE) to infer the possible specific latent variables. Finally, to validate the inferred latent variables across multiple real-world web-based data sources, we leverage LLMs for evidence exploration to ensure traceability. We comprehensively evaluate TLVD on three de-identified real patient datasets provided by a hospital and two benchmark datasets. Extensive experimental results confirm the effectiveness and reliability of TLVD, with average improvements of 32.67% in Acc, 62.21% in CAcc, and 26.72% in ECit across the five datasets.

</details>


### [487] [Silent Inconsistency in Data-Parallel Full Fine-Tuning: Diagnosing Worker-Level Optimization Misalignment](https://arxiv.org/abs/2602.14462)
*Hong Li,Zhen Zhou,Honggang Zhang,Yuping Luo,Xinyue Wang,Han Gong,Zhiyuan Liu*

Main category: cs.LG

TL;DR: The paper identifies "silent inconsistency" in data-parallel training where workers diverge despite synchronized weights, proposes lightweight diagnostic metrics to detect this hidden instability, and validates them on a 1B-parameter model fine-tuning task.


<details>
  <summary>Details</summary>
Motivation: While DP training with synchronous all-reduce ensures weight synchronization, it doesn't guarantee alignment of worker-level optimization dynamics before gradient aggregation. This "silent inconsistency" can cause hidden divergence in losses and gradients that remains invisible under conventional aggregated monitoring.

Method: Proposes a lightweight, model-agnostic diagnostic framework with three complementary metrics: (1) loss dispersion, (2) gradient-norm dispersion, and (3) gradient-direction consistency measured by inter-worker cosine similarity. The framework requires no modification to model architecture, synchronization mechanisms, or optimization algorithms.

Result: Experimental validation on fine-tuning the 1B-parameter openPangu-Embedded-1B-V1.1 model with 8-NPU DP setup shows that controlled perturbations of cross-rank stochasticity (desynchronized data shuffling and random seeds) lead to substantial increases in loss/gradient dispersion and reduced directional alignment, despite smooth globally averaged loss curves.

Conclusion: The proposed diagnostic metrics provide actionable visibility into hidden instability modes in large-scale DP fine-tuning, enabling more reliable diagnosis and configuration assessment without significant overhead.

Abstract: Data-parallel (DP) training with synchronous all-reduce is a dominant paradigm for full-parameter fine-tuning of large language models (LLMs). While parameter synchronization guarantees numerical equivalence of model weights after each iteration, it does not necessarily imply alignment of worker-level optimization dynamics before gradient aggregation. This paper identifies and studies this latent mismatch, termed \emph{silent inconsistency}, where cross-worker divergence in losses and gradients can remain invisible under conventional aggregated monitoring signals. We propose a lightweight, model-agnostic diagnostic framework that quantifies worker-level consistency using training signals readily available in standard pipelines. Specifically, we introduce three complementary metrics: loss dispersion, gradient-norm dispersion, and gradient-direction consistency measured by inter-worker cosine similarity. The proposed metrics incur negligible overhead and require no modification to model architecture, synchronization mechanisms, or optimization algorithms. We validate the framework by fully fine-tuning the 1B-parameter \texttt{openPangu-Embedded-1B-V1.1} model on the \texttt{tatsu-lab/alpaca} dataset using an 8-NPU DP setup, under controlled perturbations of cross-rank stochasticity. Experimental results show that progressively desynchronized data shuffling and random seeds lead to substantial increases in loss/gradient dispersion and reduced directional alignment, despite smooth globally averaged loss curves. These findings demonstrate that the proposed indicators provide actionable visibility into hidden instability modes in large-scale DP fine-tuning, enabling more reliable diagnosis and configuration assessment.

</details>


### [488] [LACONIC: Length-Aware Constrained Reinforcement Learning for LLM](https://arxiv.org/abs/2602.14468)
*Chang Liu,Yiran Zhao,Lawrence Liu,Yaoqi Ye,Csaba Szepesvári,Lin F. Yang*

Main category: cs.LG

TL;DR: LACONIC is a reinforcement learning method that enforces token budget constraints during LLM training to reduce response length while preserving task performance.


<details>
  <summary>Details</summary>
Motivation: RL training for LLMs often produces excessively long responses, increasing inference latency and computational costs. Existing length-control methods use fixed heuristic reward shaping that can misalign with task objectives and require brittle tuning.

Method: LACONIC updates policy models using an augmented objective combining task reward with length-based cost. The cost scale is adaptively adjusted throughout training to balance brevity and task performance, enforcing a target token budget.

Result: Across mathematical reasoning models and datasets, LACONIC preserves or improves pass@1 while reducing output length by over 50%. It maintains out-of-domain performance on general knowledge and multilingual benchmarks with 44% fewer tokens.

Conclusion: LACONIC provides robust length control while preserving task reward, integrates into standard RL-tuning with no inference changes and minimal deployment overhead, and has theoretical guarantees supporting the method.

Abstract: Reinforcement learning (RL) has enhanced the capabilities of large language models (LLMs) through reward-driven training. Nevertheless, this process can introduce excessively long responses, inflating inference latency and computational overhead. Prior length-control approaches typically rely on fixed heuristic reward shaping, which can misalign with the task objective and require brittle tuning. In this work, we propose LACONIC, a reinforcement learning method that enforces a target token budget during training. Specifically, we update policy models using an augmented objective that combines the task reward with a length-based cost. To balance brevity and task performance, the cost scale is adaptively adjusted throughout training. This yields robust length control while preserving task reward. We provide a theoretical guarantee that support the method. Across mathematical reasoning models and datasets, LACONIC preserves or improves pass@1 while reducing output length by over 50%. It maintains out-of-domain performance on general knowledge and multilingual benchmarks with 44% fewer tokens. Moreover, LACONIC integrates into standard RL-tuning with no inference changes and minimal deployment overhead.

</details>


### [489] [One Good Source is All You Need: Near-Optimal Regret for Bandits under Heterogeneous Noise](https://arxiv.org/abs/2602.14474)
*Aadirupa Saha,Amith Bhat,Haipeng Luo*

Main category: cs.LG

TL;DR: SOAR algorithm for multi-armed bandits with multiple data sources achieves near-optimal regret by adaptively selecting low-variance sources while minimizing regret.


<details>
  <summary>Details</summary>
Motivation: Standard MAB assumes a single data source, but real-world scenarios often involve multiple heterogeneous data sources with different noise variances. Existing approaches may suffer from arbitrarily large regret when using high-variance sources instead of optimal low-variance ones.

Method: SOAR (Source-Optimistic Adaptive Regret minimization) uses sharp variance-concentration bounds to quickly prune high-variance sources, followed by a balanced min-max LCB-UCB approach that simultaneously identifies the best arm and optimal (minimum-variance) data source.

Result: SOAR achieves instance-dependent regret bound of $\tilde{O}\left({σ^*}^2\sum_{i=2}^K \frac{\log T}{Δ_i} + \sqrt{K \sum_{j=1}^M σ_j^2}\right)$, attaining optimal single-source MAB regret with minimum variance σ*² plus small additive cost for source identification.

Conclusion: SOAR effectively handles multiple data sources without prior knowledge of their variances, achieving near-optimal performance and significantly outperforming baselines that could suffer arbitrarily large regret when using high-variance sources.

Abstract: We study $K$-armed Multiarmed Bandit (MAB) problem with $M$ heterogeneous data sources, each exhibiting unknown and distinct noise variances $\{σ_j^2\}_{j=1}^M$. The learner's objective is standard MAB regret minimization, with the additional complexity of adaptively selecting which data source to query from at each round. We propose Source-Optimistic Adaptive Regret minimization (SOAR), a novel algorithm that quickly prunes high-variance sources using sharp variance-concentration bounds, followed by a `balanced min-max LCB-UCB approach' that seamlessly integrates the parallel tasks of identifying the best arm and the optimal (minimum-variance) data source. Our analysis shows SOAR achieves an instance-dependent regret bound of $\tilde{O}\left({σ^*}^2\sum_{i=2}^K \frac{\log T}{Δ_i} + \sqrt{K \sum_{j=1}^M σ_j^2}\right)$, up to preprocessing costs depending only on problem parameters, where ${σ^*}^2 := \min_j σ_j^2$ is the minimum source variance and $Δ_i$ denotes the suboptimality gap of the $i$-th arm. This result is both surprising as despite lacking prior knowledge of the minimum-variance source among $M$ alternatives, SOAR attains the optimal instance-dependent regret of standard single-source MAB with variance ${σ^*}^2$, while incurring only an small (and unavoidable) additive cost of $\tilde O(\sqrt{K \sum_{j=1}^M σ_j^2})$ towards the optimal (minimum variance) source identification. Our theoretical bounds represent a significant improvement over some proposed baselines, e.g. Uniform UCB or Explore-then-Commit UCB, which could potentially suffer regret scaling with $σ_{\max}^2$ in place of ${σ^*}^2$-a gap that can be arbitrarily large when $σ_{\max} \gg σ^*$. Experiments on multiple synthetic problem instances and the real-world MovieLens\;25M dataset, demonstrating the superior performance of SOAR over the baselines.

</details>


### [490] [Revisiting the Platonic Representation Hypothesis: An Aristotelian View](https://arxiv.org/abs/2602.14486)
*Fabian Gröger,Shuo Wen,Maria Brbić*

Main category: cs.LG

TL;DR: The paper shows that existing representational similarity metrics are confounded by network scale, introduces a calibration framework to fix this, and finds that while global convergence disappears after calibration, local neighborhood similarity persists - leading to the Aristotelian Representation Hypothesis.


<details>
  <summary>Details</summary>
Motivation: The Platonic Representation Hypothesis suggests neural network representations converge to a common statistical model of reality, but existing similarity metrics may be confounded by network scale effects, requiring proper calibration to test this hypothesis accurately.

Method: Introduces a permutation-based null-calibration framework that transforms any representational similarity metric into a calibrated score with statistical guarantees, correcting for scale confounds from model depth/width.

Result: After calibration, the apparent convergence reported by global spectral measures largely disappears, while local neighborhood similarity (but not local distances) retains significant agreement across different modalities.

Conclusion: Proposes the Aristotelian Representation Hypothesis: neural network representations converge to shared local neighborhood relationships rather than global statistical models, with proper calibration revealing this nuanced picture.

Abstract: The Platonic Representation Hypothesis suggests that representations from neural networks are converging to a common statistical model of reality. We show that the existing metrics used to measure representational similarity are confounded by network scale: increasing model depth or width can systematically inflate representational similarity scores. To correct these effects, we introduce a permutation-based null-calibration framework that transforms any representational similarity metric into a calibrated score with statistical guarantees. We revisit the Platonic Representation Hypothesis with our calibration framework, which reveals a nuanced picture: the apparent convergence reported by global spectral measures largely disappears after calibration, while local neighborhood similarity, but not local distances, retains significant agreement across different modalities. Based on these findings, we propose the Aristotelian Representation Hypothesis: representations in neural networks are converging to shared local neighborhood relationships.

</details>


### [491] [Parameter-Efficient Fine-Tuning of LLMs with Mixture of Space Experts](https://arxiv.org/abs/2602.14490)
*Buze Zhang,Jinkai Tao,Zilang Zeng,Neil He,Ali Maatouk,Menglin Yang,Rex Ying*

Main category: cs.LG

TL;DR: MoSLoRA introduces a mixture of geometric spaces framework that extends LoRA with heterogeneous geometric experts, enabling dynamic selection of appropriate geometric manifolds based on input context for improved representation learning.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods operate in Euclidean space, limiting their capacity to capture complex geometric structures in language data. While alternative geometric spaces (hyperbolic, spherical) offer advantages, forcing representations into a single manifold type limits expressiveness even with learnable curvature parameters.

Method: Proposes Mixture of Space (MoS) framework leveraging multiple geometric spaces simultaneously. Develops MoSLoRA which extends LoRA with heterogeneous geometric experts, enabling dynamic selection/combination of geometric spaces based on input context. Includes lightweight routing mechanism to address computational overhead of frequent manifold switching.

Result: MoSLoRA consistently outperforms strong baselines across diverse benchmarks, achieving up to 5.6% improvement on MATH500 and 15.9% on MAWPS. Provides empirical insights into how curvature optimization impacts training stability and model performance.

Conclusion: The proposed MoSLoRA framework successfully addresses limitations of single-manifold PEFT methods by enabling dynamic, curvature-aware representation learning across multiple geometric spaces, leading to significant performance improvements on various benchmarks.

Abstract: Large Language Models (LLMs) have achieved remarkable progress, with Parameter-Efficient Fine-Tuning (PEFT) emerging as a key technique for downstream task adaptation. However, existing PEFT methods mainly operate in Euclidean space, fundamentally limiting their capacity to capture complex geometric structures inherent in language data. While alternative geometric spaces, like hyperbolic geometries for hierarchical data and spherical manifolds for circular patterns, offer theoretical advantages, forcing representations into a single manifold type ultimately limits expressiveness, even when curvature parameters are learnable. To address this, we propose Mixture of Space (MoS), a unified framework that leverages multiple geometric spaces simultaneously to learn richer, curvature-aware representations. Building on this scheme, we develop MoSLoRA, which extends Low-Rank Adaptation (LoRA) with heterogeneous geometric experts, enabling models to dynamically select or combine appropriate geometric spaces based on input context. Furthermore, to address the computational overhead of frequent manifold switching, we develop a lightweight routing mechanism. Moreover, we provide empirical insights into how curvature optimization impacts training stability and model performance. Our experiments across diverse benchmarks demonstrate that MoSLoRA consistently outperforms strong baselines, achieving up to 5.6% improvement on MATH500 and 15.9% on MAWPS.

</details>


### [492] [Divine Benevolence is an $x^2$: GLUs scale asymptotically faster than MLPs](https://arxiv.org/abs/2602.14495)
*Alejandro Francisco Queiruga*

Main category: cs.LG

TL;DR: The paper shows that GLU variants in LLMs achieve asymptotically faster scaling (P⁻³) than MLPs (P⁻²) due to their piecewise quadratic forms, and proposes a new "Gated Quadratic Unit" with even steeper scaling.


<details>
  <summary>Details</summary>
Motivation: The success of GLU variants in frontier LLMs has been largely empirical, lacking theoretical understanding of why they outperform traditional MLPs. The paper aims to provide first-principles numerical analysis to explain their superior scaling properties.

Method: The authors apply numerical analysis tools to demonstrate that GLUs have piecewise quadratic functional forms enabling quadratic order of approximation. They provide parameter constructions and empirical verification of scaling slopes for 1D function approximation problems, comparing L(P) ∝ P⁻³ for GLUs vs L(P) ∝ P⁻² for MLPs.

Result: Theoretical analysis shows GLUs achieve asymptotically faster scaling (P⁻³) than MLPs (P⁻²). Empirical verification confirms these scaling slopes in 1D function approximation. Based on these principles, the authors propose a "Gated Quadratic Unit" with even steeper scaling slope.

Conclusion: Architecture design can be guided by first-principles numerical theory to unlock superior scaling in large models. The proposed Gated Quadratic Unit demonstrates the potential for theoretically-informed architecture improvements beyond empirical discoveries.

Abstract: Scaling laws can be understood from ground-up numerical analysis, where traditional function approximation theory can explain shifts in model architecture choices. GLU variants now dominate frontier LLMs and similar outer-product architectures are prevalent in ranking models. The success of these architectures has mostly been left as an empirical discovery. In this paper, we apply the tools of numerical analysis to expose a key factor: these models have an $x^2$ which enables \emph{asymptotically} faster scaling than MLPs. GLUs have piecewise quadratic functional forms that are sufficient to exhibit quadratic order of approximation. Our key contribution is to demonstrate that the $L(P)$ scaling slope is $L(P)\propto P^{-3}$ for GLUs but only $L(P)=P^{-2}$ for MLPs on function reconstruction problems. We provide a parameter construction and empirical verification of these slopes for 1D function approximation. From the first principles we discover, we make one stride and propose the ``Gated Quadratic Unit'' which has an even steeper $L(P)$ slope than the GLU and MLP. This opens the possibility of architecture design from first principles numerical theory to unlock superior scaling in large models. Replication code is available at https://github.com/afqueiruga/divine_scaling.

</details>


### [493] [Covariance-Aware Transformers for Quadratic Programming and Decision Making](https://arxiv.org/abs/2602.14506)
*Kutay Tire,Yufan Zhang,Ege Onur Taga,Samet Oymak*

Main category: cs.LG

TL;DR: Transformers can solve quadratic programs via linear attention, enabling one-pass decision-making with covariance matrices, outperforming traditional predict-then-optimize approaches.


<details>
  <summary>Details</summary>
Motivation: To explore how transformers can solve quadratic programming problems and leverage this capability for decision-making tasks involving covariance matrices, particularly in portfolio optimization where second-order statistics are crucial.

Method: Show that linear attention can solve unconstrained QPs by tokenizing matrix variables row-by-row and emulating gradient descent. Extend to ℓ₁-penalized QPs using MLPs for iterative soft-thresholding, and ℓ₁-constrained QPs with additional feedback loops. Introduce Time2Decide method that enhances time series foundation models by explicitly feeding covariance matrices.

Result: Time2Decide uniformly outperforms base TSFM models for portfolio optimization (ℓ₁-constrained QP formulation) and remarkably outperforms traditional Predict-then-Optimize procedures in suitable settings.

Conclusion: Transformers benefit from explicit use of second-order statistics and can effectively solve complex decision-making problems like portfolio construction in a single forward pass, demonstrating the practical value of integrating QP-solving capabilities into transformer architectures.

Abstract: We explore the use of transformers for solving quadratic programs and how this capability benefits decision-making problems that involve covariance matrices. We first show that the linear attention mechanism can provably solve unconstrained QPs by tokenizing the matrix variables (e.g.~$A$ of the objective $\frac{1}{2}x^\top Ax+b^\top x$) row-by-row and emulating gradient descent iterations. Furthermore, by incorporating MLPs, a transformer block can solve (i) $\ell_1$-penalized QPs by emulating iterative soft-thresholding and (ii) $\ell_1$-constrained QPs when equipped with an additional feedback loop. Our theory motivates us to introduce Time2Decide: a generic method that enhances a time series foundation model (TSFM) by explicitly feeding the covariance matrix between the variates. We empirically find that Time2Decide uniformly outperforms the base TSFM model for the classical portfolio optimization problem that admits an $\ell_1$-constrained QP formulation. Remarkably, Time2Decide also outperforms the classical "Predict-then-Optimize (PtO)" procedure, where we first forecast the returns and then explicitly solve a constrained QP, in suitable settings. Our results demonstrate that transformers benefit from explicit use of second-order statistics, and this can enable them to effectively solve complex decision-making problems, like portfolio construction, in one forward pass.

</details>


### [494] [DeepMTL2R: A Library for Deep Multi-task Learning to Rank](https://arxiv.org/abs/2602.14519)
*Chaosheng Dong,Peiyao Xiao,Yijia Wang,Kaiyi Ji*

Main category: cs.LG

TL;DR: DeepMTL2R is an open-source deep learning framework for multi-task learning to rank that integrates heterogeneous relevance signals using transformer self-attention, supports 21 MTL algorithms with multi-objective optimization, and enables controlled comparisons across MTL strategies.


<details>
  <summary>Details</summary>
Motivation: Modern ranking systems need to optimize multiple relevance criteria simultaneously, but existing approaches struggle with integrating heterogeneous signals and handling potentially conflicting objectives in a unified, scalable manner.

Method: The framework leverages transformer self-attention mechanisms to capture complex dependencies and long-range interactions among items and labels, integrates 21 state-of-the-art multi-task learning algorithms, and supports multi-objective optimization to identify Pareto-optimal ranking models.

Result: Demonstrated effectiveness on publicly available datasets with competitive performance, visualized trade-offs among objectives, and provided a scalable, expressive solution for modern ranking systems.

Conclusion: DeepMTL2R offers a comprehensive, open-source framework for multi-task learning to rank that facilitates controlled comparisons across MTL strategies and enables the development of more sophisticated ranking systems that can handle multiple, potentially conflicting relevance criteria.

Abstract: This paper presents DeepMTL2R, an open-source deep learning framework for Multi-task Learning to Rank (MTL2R), where multiple relevance criteria must be optimized simultaneously. DeepMTL2R integrates heterogeneous relevance signals into a unified, context-aware model by leveraging the self-attention mechanism of transformer architectures, enabling effective learning across diverse and potentially conflicting objectives. The framework includes 21 state-of-the-art multi-task learning algorithms and supports multi-objective optimization to identify Pareto-optimal ranking models. By capturing complex dependencies and long-range interactions among items and labels, DeepMTL2R provides a scalable and expressive solution for modern ranking systems and facilitates controlled comparisons across MTL strategies. We demonstrate its effectiveness on a publicly available dataset, report competitive performance, and visualize the resulting trade-offs among objectives. DeepMTL2R is available at \href{https://github.com/amazon-science/DeepMTL2R}{https://github.com/amazon-science/DeepMTL2R}.

</details>


### [495] [Truly Adapting to Adversarial Constraints in Constrained MABs](https://arxiv.org/abs/2602.14543)
*Francesco Emanuele Stradi,Kalana Kalupahana,Matteo Castiglioni,Alberto Marchesi,Nicola Gatti*

Main category: cs.LG

TL;DR: This paper studies constrained multi-armed bandits with unknown constraints under both full and bandit feedback, in non-stationary environments where both losses and constraints can change arbitrarily over time.


<details>
  <summary>Details</summary>
Motivation: Prior work either focused on stochastic constraints or relaxed benchmarks with adversarial constraints. The authors aim to provide algorithms that achieve optimal regret and positive constraint violation when constraints are stochastic but losses are adversarial, with guarantees that degrade smoothly as constraints become more adversarial.

Method: The authors propose algorithms for three scenarios: 1) full feedback with stochastic constraints and adversarial losses, 2) bandit feedback for losses only, and 3) bandit feedback for constraints. They use a non-stationary environment model where distributions can change arbitrarily over time.

Result: Under full feedback: algorithm achieves Õ(√T + C) regret and Õ(√T + C) positive violation. With bandit feedback for losses: similar guarantees. With bandit feedback for constraints: algorithm achieves Õ(√T + C) positive violation and Õ(√T + C√T) regret, where C quantifies constraint non-stationarity.

Conclusion: This work provides the first algorithms achieving optimal rates of regret and positive constraint violation for constrained MAB with stochastic constraints and adversarial losses, with guarantees that degrade smoothly as constraints become more adversarial.

Abstract: We study the constrained variant of the \emph{multi-armed bandit} (MAB) problem, in which the learner aims not only at minimizing the total loss incurred during the learning dynamic, but also at controlling the violation of multiple \emph{unknown} constraints, under both \emph{full} and \emph{bandit feedback}. We consider a non-stationary environment that subsumes both stochastic and adversarial models and where, at each round, both losses and constraints are drawn from distributions that may change arbitrarily over time. In such a setting, it is provably not possible to guarantee both sublinear regret and sublinear violation. Accordingly, prior work has mainly focused either on settings with stochastic constraints or on relaxing the benchmark with fully adversarial constraints (\emph{e.g.}, via competitive ratios with respect to the optimum). We provide the first algorithms that achieve optimal rates of regret and \emph{positive} constraint violation when the constraints are stochastic while the losses may vary arbitrarily, and that simultaneously yield guarantees that degrade smoothly with the degree of adversariality of the constraints. Specifically, under \emph{full feedback} we propose an algorithm attaining $\widetilde{\mathcal{O}}(\sqrt{T}+C)$ regret and $\widetilde{\mathcal{O}}(\sqrt{T}+C)$ {positive} violation, where $C$ quantifies the amount of non-stationarity in the constraints. We then show how to extend these guarantees when only bandit feedback is available for the losses. Finally, when \emph{bandit feedback} is available for the constraints, we design an algorithm achieving $\widetilde{\mathcal{O}}(\sqrt{T}+C)$ {positive} violation and $\widetilde{\mathcal{O}}(\sqrt{T}+C\sqrt{T})$ regret.

</details>


### [496] [Governing AI Forgetting: Auditing for Machine Unlearning Compliance](https://arxiv.org/abs/2602.14553)
*Qinqi Lin,Ningning Ding,Lingjie Duan,Jianwei Huang*

Main category: cs.LG

TL;DR: First economic framework for auditing machine unlearning compliance using certified unlearning theory and game theory, revealing counterintuitive results about optimal inspection intensity.


<details>
  <summary>Details</summary>
Motivation: AI operators routinely fail to comply with data deletion requests despite legal mandates, creating a gap between technical machine unlearning solutions and regulatory implementation.

Method: Integrates certified unlearning theory with regulatory enforcement: 1) characterizes verification uncertainty using hypothesis-testing interpretation of certified unlearning, 2) proposes game-theoretic model of auditor-operator interactions, 3) transforms complex bivariate nonlinear fixed-point problem into tractable univariate auxiliary problem.

Result: Counterintuitive finding: auditor can optimally reduce inspection intensity as deletion requests increase due to operator's weakened unlearning making non-compliance easier to detect. Also proves undisclosed auditing reduces regulatory cost-effectiveness relative to disclosed auditing despite informational advantages.

Conclusion: Provides first economic framework for auditing machine unlearning compliance, addressing fundamental gap between technical feasibility and regulatory implementation, with practical implications for regulatory strategy optimization.

Abstract: Despite legal mandates for the right to be forgotten, AI operators routinely fail to comply with data deletion requests. While machine unlearning (MU) provides a technical solution to remove personal data's influence from trained models, ensuring compliance remains challenging due to the fundamental gap between MU's technical feasibility and regulatory implementation. In this paper, we introduce the first economic framework for auditing MU compliance, by integrating certified unlearning theory with regulatory enforcement. We first characterize MU's inherent verification uncertainty using a hypothesis-testing interpretation of certified unlearning to derive the auditor's detection capability, and then propose a game-theoretic model to capture the strategic interactions between the auditor and the operator. A key technical challenge arises from MU-specific nonlinearities inherent in the model utility and the detection probability, which create complex strategic couplings that traditional auditing frameworks do not address and that also preclude closed-form solutions. We address this by transforming the complex bivariate nonlinear fixed-point problem into a tractable univariate auxiliary problem, enabling us to decouple the system and establish the equilibrium existence, uniqueness, and structural properties without relying on explicit solutions. Counterintuitively, our analysis reveals that the auditor can optimally reduce the inspection intensity as deletion requests increase, since the operator's weakened unlearning makes non-compliance easier to detect. This is consistent with recent auditing reductions in China despite growing deletion requests. Moreover, we prove that although undisclosed auditing offers informational advantages for the auditor, it paradoxically reduces the regulatory cost-effectiveness relative to disclosed auditing.

</details>


### [497] [Fluid-Agent Reinforcement Learning](https://arxiv.org/abs/2602.14559)
*Shishir Sharma,Doina Precup,Theodore J. Perkins*

Main category: cs.LG

TL;DR: Proposes fluid-agent environments where agents can dynamically create other agents, unlike traditional MARL with fixed populations.


<details>
  <summary>Details</summary>
Motivation: Real-world multi-agent systems often have dynamic populations where agents can spawn new agents (e.g., cell division, company spin-offs), but traditional MARL assumes fixed numbers of agents.

Method: Introduces fluid-agent framework with game-theoretic solution concepts, evaluates MARL algorithms on fluid variants of Predator-Prey and Level-Based Foraging benchmarks, plus a new environment showing novel strategies.

Result: Demonstrates that fluid-agent framework enables agent teams to dynamically adjust their size to match environmental demands, unlocking novel solution strategies beyond fixed-population settings.

Conclusion: Fluid-agent environments better model real-world multi-agent systems and enable more adaptive, dynamic team formation strategies compared to traditional fixed-population MARL approaches.

Abstract: The primary focus of multi-agent reinforcement learning (MARL) has been to study interactions among a fixed number of agents embedded in an environment. However, in the real world, the number of agents is neither fixed nor known a priori. Moreover, an agent can decide to create other agents (for example, a cell may divide, or a company may spin off a division). In this paper, we propose a framework that allows agents to create other agents; we call this a fluid-agent environment. We present game-theoretic solution concepts for fluid-agent games and empirically evaluate the performance of several MARL algorithms within this framework. Our experiments include fluid variants of established benchmarks such as Predator-Prey and Level-Based Foraging, where agents can dynamically spawn, as well as a new environment we introduce that highlights how fluidity can unlock novel solution strategies beyond those observed in fixed-population settings. We demonstrate that this framework yields agent teams that adjust their size dynamically to match environmental demands.

</details>


### [498] [DCTracks: An Open Dataset for Machine Learning-Based Drift Chamber Track Reconstruction](https://arxiv.org/abs/2602.14571)
*Qian Liyan,Zhang Yao,Yuan Ye,Zhang Zhaoke,Fang Jin,Jiang Shimiao,Zhang Jin,Li Ke,Liu Beijiang,Xu Chenglin,Zhang Yifan,Jia Xiaoqian,Qin Xiaoshuai,Huang Xingtao*

Main category: cs.LG

TL;DR: A Monte Carlo dataset for ML-based track reconstruction with standardized metrics and benchmark results for traditional algorithms and GNNs.


<details>
  <summary>Details</summary>
Motivation: To advance Machine Learning-based track reconstruction by providing standardized evaluation tools and enabling reproducible validation for future research.

Method: Created a Monte Carlo dataset of single- and two-track drift chamber events, defined track reconstruction specific metrics, and benchmarked both traditional algorithms and Graph Neural Networks methods.

Result: Established a standardized evaluation framework with reported results for traditional track reconstruction algorithms and GNNs, enabling rigorous and comparable validation.

Conclusion: The introduced dataset and metrics provide a foundation for reproducible, standardized evaluation of ML-based track reconstruction methods, facilitating future research advancement.

Abstract: We introduce a Monte Carlo (MC) dataset of single- and two-track drift chamber events to advance Machine Learning (ML)-based track reconstruction. To enable standardized and comparable evaluation, we define track reconstruction specific metrics and report results for traditional track reconstruction algorithms and a Graph Neural Networks (GNNs) method, facilitating rigorous, reproducible validation for future research.

</details>


### [499] [RNM-TD3: N:M Semi-structured Sparse Reinforcement Learning From Scratch](https://arxiv.org/abs/2602.14578)
*Isam Vrce,Andreas Kassler,Gökçe Aydos*

Main category: cs.LG

TL;DR: First study of N:M structured sparsity in reinforcement learning, showing that RNM-TD3 (N:M sparse TD3) outperforms dense networks at 50-75% sparsity while maintaining hardware acceleration compatibility.


<details>
  <summary>Details</summary>
Motivation: Existing DRL sparsity methods use unstructured fine-grained sparsity that limits hardware acceleration, while structured coarse-grained sparsity typically degrades performance. There's a need for sparsity that balances compression, performance, and hardware efficiency.

Method: Propose RNM-TD3 framework that enforces row-wise N:M sparsity throughout training for all networks in off-policy RL (TD3). This maintains compatibility with accelerators supporting N:M sparse matrix operations.

Result: RNM-TD3 outperforms dense counterpart at 50-75% sparsity (2:4 and 1:4), achieving up to 14% performance increase at 2:4 sparsity on Ant environment. Remains competitive even at 87.5% sparsity (1:8) while enabling potential training speedups.

Conclusion: N:M structured sparsity in RL successfully balances compression, performance, and hardware efficiency, demonstrating that structured sparsity can outperform dense networks while enabling hardware acceleration benefits.

Abstract: Sparsity is a well-studied technique for compressing deep neural networks (DNNs) without compromising performance. In deep reinforcement learning (DRL), neural networks with up to 5% of their original weights can still be trained with minimal performance loss compared to their dense counterparts. However, most existing methods rely on unstructured fine-grained sparsity, which limits hardware acceleration opportunities due to irregular computation patterns. Structured coarse-grained sparsity enables hardware acceleration, yet typically degrades performance and increases pruning complexity. In this work, we present, to the best of our knowledge, the first study on N:M structured sparsity in RL, which balances compression, performance, and hardware efficiency. Our framework enforces row-wise N:M sparsity throughout training for all networks in off-policy RL (TD3), maintaining compatibility with accelerators that support N:M sparse matrix operations. Experiments on continuous-control benchmarks show that RNM-TD3, our N:M sparse agent, outperforms its dense counterpart at 50%-75% sparsity (e.g., 2:4 and 1:4), achieving up to a 14% increase in performance at 2:4 sparsity on the Ant environment. RNM-TD3 remains competitive even at 87.5% sparsity (1:8), while enabling potential training speedups.

</details>


### [500] [Replicable Constrained Bandits](https://arxiv.org/abs/2602.14580)
*Matteo Bollini,Gianmarco Genalti,Francesco Emanuele Stradi,Matteo Castiglioni,Alberto Marchesi*

Main category: cs.LG

TL;DR: Replicable algorithms for constrained multi-armed bandits that achieve same regret/violation bounds as non-replicable ones


<details>
  <summary>Details</summary>
Motivation: Need for reproducible experiments in machine learning; algorithmic replicability ensures same decisions across executions; study replicability in constrained MAB problems where learners must maximize reward while satisfying constraints

Method: Develop replicable algorithms for constrained MABs; design replicable UCB-like algorithm for unconstrained MABs as key step; employ optimism in-the-face-of-uncertainty principle in replicable framework

Result: Replicability can be achieved in constrained MABs; replicable algorithms match regret and constraint violation bounds of non-replicable ones; first replicable UCB-like algorithm for unconstrained MABs

Conclusion: Algorithmic replicability is achievable in constrained bandit problems without sacrificing performance; optimism-based algorithms can be made replicable; opens new direction for reproducible decision-making under constraints

Abstract: Algorithmic \emph{replicability} has recently been introduced to address the need for reproducible experiments in machine learning. A \emph{replicable online learning} algorithm is one that takes the same sequence of decisions across different executions in the same environment, with high probability. We initiate the study of algorithmic replicability in \emph{constrained} MAB problems, where a learner interacts with an unknown stochastic environment for $T$ rounds, seeking not only to maximize reward but also to satisfy multiple constraints. Our main result is that replicability can be achieved in constrained MABs. Specifically, we design replicable algorithms whose regret and constraint violation match those of non-replicable ones in terms of $T$. As a key step toward these guarantees, we develop the first replicable UCB-like algorithm for \emph{unconstrained} MABs, showing that algorithms that employ the optimism in-the-face-of-uncertainty principle can be replicable, a result that we believe is of independent interest.

</details>


### [501] [Decoupled Continuous-Time Reinforcement Learning via Hamiltonian Flow](https://arxiv.org/abs/2602.14587)
*Minh Nguyen*

Main category: cs.LG

TL;DR: Novel decoupled continuous-time actor-critic algorithm with alternating updates for event-driven control problems, outperforming prior methods on continuous-control benchmarks and real-world trading.


<details>
  <summary>Details</summary>
Motivation: Standard discrete-time RL struggles with continuous-time, event-driven control problems because as time gaps shrink, the Q-function collapses to the value function, eliminating action ranking. Existing continuous-time methods have complex optimization problems that are difficult to train reliably.

Method: Proposes a decoupled continuous-time actor-critic algorithm with alternating updates: q (advantage-rate function) is learned from diffusion generators on V, and V is updated via a Hamiltonian-based value flow that remains informative under infinitesimal time steps where standard backups fail.

Result: Theoretically proves rigorous convergence via new probabilistic arguments. Empirically outperforms prior continuous-time and leading discrete-time baselines across challenging continuous-control benchmarks and a real-world trading task, achieving 21% profit over a single quarter - nearly doubling the second-best method.

Conclusion: The proposed decoupled approach successfully addresses limitations of existing continuous-time RL methods by separating V and q learning with alternating updates, providing both theoretical convergence guarantees and superior empirical performance on real-world continuous-time control problems.

Abstract: Many real-world control problems, ranging from finance to robotics, evolve in continuous time with non-uniform, event-driven decisions. Standard discrete-time reinforcement learning (RL), based on fixed-step Bellman updates, struggles in this setting: as time gaps shrink, the $Q$-function collapses to the value function $V$, eliminating action ranking. Existing continuous-time methods reintroduce action information via an advantage-rate function $q$. However, they enforce optimality through complicated martingale losses or orthogonality constraints, which are sensitive to the choice of test processes. These approaches entangle $V$ and $q$ into a large, complex optimization problem that is difficult to train reliably. To address these limitations, we propose a novel decoupled continuous-time actor-critic algorithm with alternating updates: $q$ is learned from diffusion generators on $V$, and $V$ is updated via a Hamiltonian-based value flow that remains informative under infinitesimal time steps, where standard max/softmax backups fail. Theoretically, we prove rigorous convergence via new probabilistic arguments, sidestepping the challenge that generator-based Hamiltonians lack Bellman-style contraction under the sup-norm. Empirically, our method outperforms prior continuous-time and leading discrete-time baselines across challenging continuous-control benchmarks and a real-world trading task, achieving 21% profit over a single quarter$-$nearly doubling the second-best method.

</details>


### [502] [OPBench: A Graph Benchmark to Combat the Opioid Crisis](https://arxiv.org/abs/2602.14602)
*Tianyi Ma,Yiyang Li,Yiyue Qian,Zheyuan Zhang,Zehong Wang,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: OPBench is the first comprehensive benchmark for evaluating graph learning methods on real-world opioid crisis scenarios across five datasets in three domains: overdose detection, drug trafficking detection, and drug misuse prediction.


<details>
  <summary>Details</summary>
Motivation: The opioid epidemic strains healthcare systems and communities, requiring computational solutions. While graph learning methods show promise for modeling drug-related phenomena, there's no comprehensive benchmark for systematically evaluating these methods across real-world opioid crisis scenarios.

Method: Created OPBench with five datasets across three domains: opioid overdose detection from healthcare claims, illicit drug trafficking detection from digital platforms, and drug misuse prediction from dietary patterns. Used diverse graph structures (heterogeneous graphs and hypergraphs) to preserve complex relational information. Collaborated with domain experts and institutions for data curation while following privacy/ethical guidelines. Established unified evaluation framework with standardized protocols, predefined data splits, and reproducible baselines.

Result: Through extensive experiments, analyzed strengths and limitations of existing graph learning methods, providing actionable insights for future research. Made source code and datasets publicly available at https://github.com/Tianyi-Billy-Ma/OPBench.

Conclusion: OPBench bridges the critical gap in evaluating graph learning methods for opioid crisis applications, enabling fair and systematic comparisons to advance computational solutions for combating the opioid epidemic.

Abstract: The opioid epidemic continues to ravage communities worldwide, straining healthcare systems, disrupting families, and demanding urgent computational solutions. To combat this lethal opioid crisis, graph learning methods have emerged as a promising paradigm for modeling complex drug-related phenomena. However, a significant gap remains: there is no comprehensive benchmark for systematically evaluating these methods across real-world opioid crisis scenarios. To bridge this gap, we introduce OPBench, the first comprehensive opioid benchmark comprising five datasets across three critical application domains: opioid overdose detection from healthcare claims, illicit drug trafficking detection from digital platforms, and drug misuse prediction from dietary patterns. Specifically, OPBench incorporates diverse graph structures, including heterogeneous graphs and hypergraphs, to preserve the rich and complex relational information among drug-related data. To address data scarcity, we collaborate with domain experts and authoritative institutions to curate and annotate datasets while adhering to privacy and ethical guidelines. Furthermore, we establish a unified evaluation framework with standardized protocols, predefined data splits, and reproducible baselines to facilitate fair and systematic comparison among graph learning methods. Through extensive experiments, we analyze the strengths and limitations of existing graph learning methods, thereby providing actionable insights for future research in combating the opioid crisis. Our source code and datasets are available at https://github.com/Tianyi-Billy-Ma/OPBench.

</details>


### [503] [Concepts' Information Bottleneck Models](https://arxiv.org/abs/2602.14626)
*Karim Galliamov,Syed M Ahsan Kazmi,Adil Khan,Adín Ramírez Rivera*

Main category: cs.LG

TL;DR: IB-regularized CBMs use information bottleneck to create minimal-sufficient concept representations, improving both accuracy and faithfulness of concept interventions.


<details>
  <summary>Details</summary>
Motivation: Standard Concept Bottleneck Models suffer from reduced accuracy and concept leakage that undermines faithfulness of concept-based explanations and interventions.

Method: Introduce explicit Information Bottleneck regularizer on concept layer that penalizes I(X;C) while preserving I(C;Y), encouraging minimal-sufficient concept representations. Two practical variants: variational objective and entropy-based surrogate, integrated into standard CBM training without architectural changes.

Result: IB-regularized models consistently outperform vanilla counterparts across six CBM families and three benchmarks. Information-plane analyses confirm intended behavior - improved predictive performance and reliability of concept-level interventions.

Conclusion: Enforcing minimal-sufficient concept bottleneck improves both predictive performance and intervention reliability. The regularizer offers theory-grounded, architecture-agnostic path to more faithful and intervenable CBMs, resolving prior evaluation inconsistencies.

Abstract: Concept Bottleneck Models (CBMs) aim to deliver interpretable predictions by routing decisions through a human-understandable concept layer, yet they often suffer reduced accuracy and concept leakage that undermines faithfulness. We introduce an explicit Information Bottleneck regularizer on the concept layer that penalizes $I(X;C)$ while preserving task-relevant information in $I(C;Y)$, encouraging minimal-sufficient concept representations. We derive two practical variants (a variational objective and an entropy-based surrogate) and integrate them into standard CBM training without architectural changes or additional supervision. Evaluated across six CBM families and three benchmarks, the IB-regularized models consistently outperform their vanilla counterparts. Information-plane analyses further corroborate the intended behavior. These results indicate that enforcing a minimal-sufficient concept bottleneck improves both predictive performance and the reliability of concept-level interventions. The proposed regularizer offers a theoretic-grounded, architecture-agnostic path to more faithful and intervenable CBMs, resolving prior evaluation inconsistencies by aligning training protocols and demonstrating robust gains across model families and datasets.

</details>


### [504] [Alignment Adapter to Improve the Performance of Compressed Deep Learning Models](https://arxiv.org/abs/2602.14635)
*Rohit Raj Rai,Abhishek Dhaka,Amit Awekar*

Main category: cs.LG

TL;DR: AlAd is a lightweight adapter that aligns compressed model embeddings with original large model embeddings to boost performance with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Compressed DL models are needed for resource-constrained environments but suffer performance degradation compared to large models. There's a need to bridge this performance gap without significant overhead.

Method: Alignment Adapter (AlAd) uses sliding-window-based approach to align token-level embeddings of compressed models with original large models. It preserves local contextual semantics, works across different dimensionalities/architectures, and is compression-method agnostic. Can be deployed as plug-and-play module or jointly fine-tuned.

Result: AlAd significantly boosts performance of compressed BERT-family models across three token-level NLP tasks with only marginal size and latency overhead.

Conclusion: AlAd provides an effective solution to enhance compressed model performance while maintaining efficiency, making it suitable for deployment in resource-constrained environments.

Abstract: Compressed Deep Learning (DL) models are essential for deployment in resource-constrained environments. But their performance often lags behind their large-scale counterparts. To bridge this gap, we propose Alignment Adapter (AlAd): a lightweight, sliding-window-based adapter. It aligns the token-level embeddings of a compressed model with those of the original large model. AlAd preserves local contextual semantics, enables flexible alignment across differing dimensionalities or architectures, and is entirely agnostic to the underlying compression method. AlAd can be deployed in two ways: as a plug-and-play module over a frozen compressed model, or by jointly fine-tuning AlAd with the compressed model for further performance gains. Through experiments on BERT-family models across three token-level NLP tasks, we demonstrate that AlAd significantly boosts the performance of compressed models with only marginal overhead in size and latency.

</details>


### [505] [An Embarrassingly Simple Way to Optimize Orthogonal Matrices at Scale](https://arxiv.org/abs/2602.14656)
*Adrián Javaloy,Antonio Vergari*

Main category: cs.LG

TL;DR: POGO is a fast GPU-friendly optimizer for orthogonality constraints that improves upon the Landing algorithm, enabling modern adaptive optimizers while maintaining orthogonality with minimal hyperparameters.


<details>
  <summary>Details</summary>
Motivation: Current optimizers for orthogonality constraints are computationally expensive and don't scale to problems with hundreds/thousands of constraints. The Landing algorithm exists but relaxes orthogonality, so there's a need for a better solution that maintains orthogonality while being efficient.

Method: POGO improves on Landing algorithm ideas, enabling inclusion of modern adaptive optimizers while ensuring orthogonal constraints are effectively met. The algorithm is fast and GPU-friendly, consisting of only 5 matrix products, and maintains orthogonality at all times in practice.

Result: POGO greatly outperforms recent optimizers on several challenging benchmarks, can optimize problems with thousands of orthogonal matrices in minutes (vs hours for alternatives), and sets a milestone for exploiting orthogonality constraints at scale.

Conclusion: POGO provides an efficient, scalable solution for orthogonality constraints in ML with minimal hyperparameters, enabling practical use of orthogonal constraints at large scale.

Abstract: Orthogonality constraints are ubiquitous in robust and probabilistic machine learning. Unfortunately, current optimizers are computationally expensive and do not scale to problems with hundreds or thousands of constraints. One notable exception is the Landing algorithm (Ablin et al., 2024) which, however comes at the expense of temporarily relaxing orthogonality. In this work, we revisit and improve on the ideas behind Landing, enabling the inclusion of modern adaptive optimizers while ensuring that orthogonal constraints are effectively met. Remarkably, these improvements come at little to no cost, and reduce the number of required hyperparemeters. Our algorithm POGO is fast and GPU-friendly, consisting of only 5 matrix products, and in practice maintains orthogonality at all times. On several challenging benchmarks, POGO greatly outperforms recent optimizers and shows it can optimize problems with thousands of orthogonal matrices in minutes while alternatives would take hours. As such, POGO sets a milestone to finally exploit orthogonality constraints in ML at scale. A PyTorch implementation of POGO is publicly available at https://github.com/adrianjav/pogo.

</details>


### [506] [Pseudo-differential-enhanced physics-informed neural networks](https://arxiv.org/abs/2602.14663)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: Pseudo-differential enhanced PINNs apply Fourier transforms to gradient enhancement, improving training efficiency and spectral learning while accommodating fractional derivatives and mesh flexibility.


<details>
  <summary>Details</summary>
Motivation: To enhance PINN training by extending gradient enhancement to Fourier space, addressing frequency bias, improving spectral learning, and enabling mesh flexibility for various domains.

Method: Apply Fourier transforms to PDE residuals, using multiplication with Fourier wavenumbers instead of differentiation, enabling pseudo-differential enhancement compatible with Monte Carlo methods and various domains.

Result: Superior PINN vs numerical error in fewer iterations, improved spectral eigenvalue decay of NTK, better high-frequency learning, and compatibility with mesh-invariant approaches.

Conclusion: Pseudo-differential enhancement in Fourier space effectively improves PINN training, mitigates frequency bias, accommodates fractional derivatives, and offers mesh flexibility for diverse applications.

Abstract: We present pseudo-differential enhanced physics-informed neural networks (PINNs), an extension of gradient enhancement but in Fourier space. Gradient enhancement of PINNs dictates that the PDE residual is taken to a higher differential order than prescribed by the PDE, added to the objective as an augmented term in order to improve training and overall learning fidelity. We propose the same procedure after application via Fourier transforms, since differentiating in Fourier space is multiplication with the Fourier wavenumber under suitable decay. Our methods are fast and efficient. Our methods oftentimes achieve superior PINN versus numerical error in fewer training iterations, potentially pair well with few samples in collocation, and can on occasion break plateaus in low collocation settings. Moreover, our methods are suitable for fractional derivatives. We establish that our methods improve spectral eigenvalue decay of the neural tangent kernel (NTK), and so our methods contribute towards the learning of high frequencies in early training, mitigating the effects of frequency bias up to the polynomial order and possibly greater with smooth activations. Our methods accommodate advanced techniques in PINNs, such as Fourier feature embeddings. A pitfall of discrete Fourier transforms via the Fast Fourier Transform (FFT) is mesh subjugation, and so we demonstrate compatibility of our methods for greater mesh flexibility and invariance on alternative Euclidean and non-Euclidean domains via Monte Carlo methods and otherwise.

</details>


### [507] [Exposing Diversity Bias in Deep Generative Models: Statistical Origins and Correction of Diversity Error](https://arxiv.org/abs/2602.14682)
*Farzan Farnia,Mohammad Jalali,Azim Ospanov*

Main category: cs.LG

TL;DR: Modern generative models systematically underestimate data diversity compared to true distributions, as revealed by entropy-based diversity metrics Vendi and RKE.


<details>
  <summary>Details</summary>
Motivation: While deep generative models excel at sample quality, their ability to capture the full diversity of underlying data distributions remains understudied. The paper aims to systematically investigate whether state-of-the-art models faithfully represent data diversity.

Method: The authors compare diversity of generated samples vs. test data using reference-free entropy-based diversity scores (Vendi and RKE). They analyze finite-sample behavior of these scores and show expected values increase with sample size, explaining why training on finite datasets leads to diversity underestimation.

Result: Across multiple benchmark datasets, test data consistently achieves substantially higher Vendi and RKE diversity scores than generated samples, revealing systematic downward diversity bias in modern generative models. This bias stems from finite-sample estimation limitations.

Conclusion: Generative models optimized to minimize divergence to empirical data distributions inherently lose diversity. The paper proposes diversity-aware regularization and guidance strategies based on Vendi and RKE as principled solutions to mitigate this bias, with empirical evidence supporting their potential effectiveness.

Abstract: Deep generative models have achieved great success in producing high-quality samples, making them a central tool across machine learning applications. Beyond sample quality, an important yet less systematically studied question is whether trained generative models faithfully capture the diversity of the underlying data distribution. In this work, we address this question by directly comparing the diversity of samples generated by state-of-the-art models with that of test samples drawn from the target data distribution, using recently proposed reference-free entropy-based diversity scores, Vendi and RKE. Across multiple benchmark datasets, we find that test data consistently attains substantially higher Vendi and RKE diversity scores than the generated samples, suggesting a systematic downward diversity bias in modern generative models. To understand the origin of this bias, we analyze the finite-sample behavior of entropy-based diversity scores and show that their expected values increase with sample size, implying that diversity estimated from finite training sets could inherently underestimate the diversity of the true distribution. As a result, optimizing the generators to minimize divergence to empirical data distributions would induce a loss of diversity. Finally, we discuss potential diversity-aware regularization and guidance strategies based on Vendi and RKE as principled directions for mitigating this bias, and provide empirical evidence suggesting their potential to improve the results.

</details>


### [508] [SynthSAEBench: Evaluating Sparse Autoencoders on Scalable Realistic Synthetic Data](https://arxiv.org/abs/2602.14687)
*David Chanin,Adrià Garriga-Alonso*

Main category: cs.LG

TL;DR: SynthSAEBench is a synthetic data toolkit with realistic feature characteristics (correlation, hierarchy, superposition) and a standardized benchmark model (SynthSAEBench-16k) for precise evaluation of Sparse Autoencoder architectures, enabling controlled experiments with ground-truth features.


<details>
  <summary>Details</summary>
Motivation: Current SAE benchmarks on LLMs are too noisy to differentiate architectural improvements, and synthetic data experiments are too small-scale and unrealistic to provide meaningful comparisons. There's a need for precise validation tools that can diagnose SAE failure modes.

Method: Developed SynthSAEBench toolkit for generating large-scale synthetic data with realistic feature characteristics (correlation, hierarchy, superposition). Created standardized benchmark model SynthSAEBench-16k for direct comparison of SAE architectures.

Result: The benchmark reproduces key LLM SAE phenomena: disconnect between reconstruction and latent quality metrics, poor SAE probing results, and precision-recall trade-off mediated by L0. Identified new failure mode: Matching Pursuit SAEs exploit superposition noise to improve reconstruction without learning ground-truth features.

Conclusion: SynthSAEBench complements LLM benchmarks by providing ground-truth features and controlled ablations, enabling researchers to precisely diagnose SAE failure modes and validate architectural improvements before scaling to LLMs.

Abstract: Improving Sparse Autoencoders (SAEs) requires benchmarks that can precisely validate architectural innovations. However, current SAE benchmarks on LLMs are often too noisy to differentiate architectural improvements, and current synthetic data experiments are too small-scale and unrealistic to provide meaningful comparisons. We introduce SynthSAEBench, a toolkit for generating large-scale synthetic data with realistic feature characteristics including correlation, hierarchy, and superposition, and a standardized benchmark model, SynthSAEBench-16k, enabling direct comparison of SAE architectures. Our benchmark reproduces several previously observed LLM SAE phenomena, including the disconnect between reconstruction and latent quality metrics, poor SAE probing results, and a precision-recall trade-off mediated by L0. We further use our benchmark to identify a new failure mode: Matching Pursuit SAEs exploit superposition noise to improve reconstruction without learning ground-truth features, suggesting that more expressive encoders can easily overfit. SynthSAEBench complements LLM benchmarks by providing ground-truth features and controlled ablations, enabling researchers to precisely diagnose SAE failure modes and validate architectural improvements before scaling to LLMs.

</details>


### [509] [A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't)](https://arxiv.org/abs/2602.14696)
*Nihal V. Nayak,Paula Rodriguez-Diaz,Neha Hulkund,Sara Beery,David Alvarez-Melis*

Main category: cs.LG

TL;DR: This paper systematically analyzes targeted instruction selection for LLM fine-tuning, finding gradient-based data representations with greedy round-robin selection works best at low budgets, and unifies existing methods as approximate distance minimization.


<details>
  <summary>Details</summary>
Motivation: The literature on targeted instruction selection is fragmented and opaque - methods vary widely, often omit zero-shot baselines, and entangle key components, leaving practitioners without actionable guidance for selecting instructions for their target tasks.

Method: The authors create a framework to disentangle and systematically analyze two core ingredients: data representation and selection algorithms. They enable controlled comparisons across models, tasks, and budgets, and unify existing selection algorithms as forms of approximate distance minimization.

Result: Only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. Gradient-based representations with greedy round-robin selection perform best at low budgets, but benefits diminish at larger budgets.

Conclusion: The findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning, with gradient-based representations being most reliable and no single method dominating across all scenarios.

Abstract: Instruction fine-tuning of large language models (LLMs) often involves selecting a subset of instruction training data from a large candidate pool, using a small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As a result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms. Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradient-based representations paired with a greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds. More broadly, our findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/targeted-instruction-selection.

</details>


### [510] [Unbiased Approximate Vector-Jacobian Products for Efficient Backpropagation](https://arxiv.org/abs/2602.14701)
*Killian Bakong,Laurent Massoulié,Edouard Oyallon,Kevin Scaman*

Main category: cs.LG

TL;DR: Proposes randomized, unbiased approximations of vector-jacobian products during backpropagation to reduce computational and memory costs of training deep neural networks.


<details>
  <summary>Details</summary>
Motivation: Training deep neural networks is computationally expensive and memory-intensive, especially during backpropagation. The authors aim to reduce these costs by replacing exact vector-jacobian products with randomized approximations.

Method: Replace exact vector-jacobian products with randomized, unbiased approximations during backpropagation. The authors provide theoretical analysis of the trade-off between epochs needed for target precision and cost reduction per epoch. They identify specific unbiased estimates with minimal variance under sparsity constraints.

Result: Theoretical analysis establishes optimality properties of minimal variance under sparsity constraints. Experimental validation on multi-layer perceptrons, BagNets, and Visual Transformers confirms the potential for reducing deep learning costs.

Conclusion: The proposed unbiased randomized backpropagation approach effectively reduces computational and memory costs of training deep neural networks while maintaining performance, as validated by both theoretical analysis and experimental results.

Abstract: In this work we introduce methods to reduce the computational and memory costs of training deep neural networks. Our approach consists in replacing exact vector-jacobian products by randomized, unbiased approximations thereof during backpropagation. We provide a theoretical analysis of the trade-off between the number of epochs needed to achieve a target precision and the cost reduction for each epoch. We then identify specific unbiased estimates of vector-jacobian products for which we establish desirable optimality properties of minimal variance under sparsity constraints. Finally we provide in-depth experiments on multi-layer perceptrons, BagNets and Visual Transfomers architectures. These validate our theoretical results, and confirm the potential of our proposed unbiased randomized backpropagation approach for reducing the cost of deep learning.

</details>


### [511] [D2-LoRA: A Synergistic Approach to Differential and Directional Low-Rank Adaptation](https://arxiv.org/abs/2602.14728)
*Nozomu Fujisawa,Masaaki Kondo*

Main category: cs.LG

TL;DR: D2-LoRA: A parameter-efficient fine-tuning method with signed low-rank residual updates and train-time projection that achieves better accuracy than LoRA while preserving algebraic mergeability for zero inference latency.


<details>
  <summary>Details</summary>
Motivation: To systematically investigate parameter-efficient fine-tuning under practical data and compute constraints, and develop a method that improves performance over existing approaches like LoRA while maintaining inference efficiency through algebraic mergeability.

Method: D2-LoRA combines signed low-rank residual updates with additive and subtractive components, plus a train-time column-wise projection that keeps each column close to its original norm. The adapter is merged into a single weight matrix after training.

Result: Achieves 76.4% average accuracy across 8 QA/reading comprehension benchmarks with only 5k samples per task and 2 epochs. Improves over LoRA by 2.2 percentage points, matches/exceeds DoRA, improves generative tasks (+1.2 ROUGE-L, +1.1% win rate), reduces training volatility by 36%, and recovers ~1.91x evaluation throughput after merge.

Conclusion: D2-LoRA provides superior parameter-efficient fine-tuning with architectural improvements over LoRA, maintains inference efficiency through algebraic mergeability, and offers stable training with practical benefits across multiple task types.

Abstract: We systematically investigate the parameter-efficient fine-tuning design space under practical data and compute constraints, and propose D2-LoRA. D2-LoRA achieves 76.4 percent average accuracy across eight question answering and reading comprehension benchmarks using only 5k training samples per task and two epochs, while preserving algebraic mergeability at inference with near-exact numerical equivalence. The method combines signed low-rank residual updates with additive and subtractive components, together with a train-time column-wise projection that keeps each column close to its original norm. After training, the adapter is merged into a single weight matrix, adding zero inference latency. Compared with LoRA, D2-LoRA improves average accuracy by 2.2 percentage points; at matched parameter counts (LoRA rank 2r versus D2-LoRA rank r), the improvement is 1.6 points, indicating gains from architectural design rather than increased parameterization. Compared with DoRA, it matches or exceeds performance on most tasks. Beyond QA and reading comprehension, D2-LoRA improves generative tasks (plus 1.2 ROUGE-L and plus 1.1 percent win rate) and shows 36 percent lower training volatility. The merge preserves numerical fidelity (mean gap about 0.03 percentage points) and recovers about 1.91x evaluation throughput. Training overhead is 19 percent, comparable to DoRA, and decreases with longer input sequences. We provide a geometric analysis explaining how the projection stabilizes training, together with ablation studies isolating the contribution of each design component.

</details>


### [512] [Scale redundancy and soft gauge fixing in positively homogeneous neural networks](https://arxiv.org/abs/2602.14729)
*Rodrigo Carmo Terin*

Main category: cs.LG

TL;DR: The paper introduces gauge-theoretic concepts to analyze neural networks with homogeneous activations, showing that neuron-wise rescalings create gauge redundancies that can be managed via orbit-selection penalties to improve optimization.


<details>
  <summary>Details</summary>
Motivation: Neural networks with positively homogeneous activations have continuous reparametrization symmetries (neuron-wise rescalings) that create parameter-space orbits where input-output functions remain invariant. This symmetry represents a gauge redundancy that affects optimization dynamics.

Method: The authors interpret the symmetry as gauge redundancy and introduce gauge-adapted coordinates separating invariant and scale-imbalance directions. They propose a soft orbit-selection functional (norm-balancing) inspired by gauge fixing in field theory, which acts only on redundant scale coordinates to induce dissipative relaxation of imbalance modes.

Result: The orbit-selection penalty expands the stable learning-rate regime and suppresses scale drift without changing network expressivity. Analytical results show it induces dissipative relaxation of imbalance modes while preserving the realized function.

Conclusion: The work establishes a structural link between gauge-orbit geometry and optimization conditioning, providing a concrete connection between gauge-theoretic concepts and machine learning, with practical benefits for training stability.

Abstract: Neural networks with positively homogeneous activations exhibit an exact continuous reparametrization symmetry: neuron-wise rescalings generate parameter-space orbits along which the input--output function is invariant. We interpret this symmetry as a gauge redundancy and introduce gauge-adapted coordinates that separate invariant and scale-imbalance directions. Inspired by gauge fixing in field theory, we introduce a soft orbit-selection (norm-balancing) functional acting only on redundant scale coordinates. We show analytically that it induces dissipative relaxation of imbalance modes to preserve the realized function. In controlled experiments, this orbit-selection penalty expands the stable learning-rate regime and suppresses scale drift without changing expressivity. These results establish a structural link between gauge-orbit geometry and optimization conditioning, providing a concrete connection between gauge-theoretic concepts and machine learning.

</details>


### [513] [Inner Loop Inference for Pretrained Transformers: Unlocking Latent Capabilities Without Training](https://arxiv.org/abs/2602.14759)
*Jonathan Lys,Vincent Gripon,Bastien Pasdeloup,Lukas Mauch,Fabien Cardinaux,Ghouthi Boukli Hacene*

Main category: cs.LG

TL;DR: Inner looping extends inference by repeatedly applying selected transformer blocks for continued refinement in frozen pretrained models, yielding modest accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: Transformers can be viewed as iterative refinement of latent representations, where inner representations are shared across layers and tokens can be decoded early. The paper explores whether additional refinement can be obtained through test-time computation extension.

Method: Proposes inference-time inner looping that prolongs refinement by repeatedly re-applying a selected block range in pretrained language models without modifying weights.

Result: Across multiple benchmarks, inner looping yields modest but consistent accuracy improvements. Analyses show more stable state evolution and continued semantic refinement in latent trajectories.

Conclusion: Additional refinement can be obtained through simple test-time looping, extending computation in frozen pretrained models without retraining.

Abstract: Deep Learning architectures, and in particular Transformers, are conventionally viewed as a composition of layers. These layers are actually often obtained as the sum of two contributions: a residual path that copies the input and the output of a Transformer block. As a consequence, the inner representations (i.e. the input of these blocks) can be interpreted as iterative refinement of a propagated latent representation. Under this lens, many works suggest that the inner space is shared across layers, meaning that tokens can be decoded at early stages. Mechanistic interpretability even goes further by conjecturing that some layers act as refinement layers. Following this path, we propose inference-time inner looping, which prolongs refinement in pretrained off-the-shelf language models by repeatedly re-applying a selected block range. Across multiple benchmarks, inner looping yields modest but consistent accuracy improvements. Analyses of the resulting latent trajectories suggest more stable state evolution and continued semantic refinement. Overall, our results suggest that additional refinement can be obtained through simple test-time looping, extending computation in frozen pretrained models.

</details>


### [514] [Universal Algorithm-Implicit Learning](https://arxiv.org/abs/2602.14761)
*Stefano Woerner,Seong Joon Oh,Christian F. Baumgartner*

Main category: cs.LG

TL;DR: TAIL is a transformer-based universal meta-learner that works across diverse tasks, modalities, and label spaces, outperforming existing methods while being computationally efficient.


<details>
  <summary>Details</summary>
Motivation: Current meta-learning methods are limited to narrow task distributions with fixed features/labels, and the field lacks precise definitions for "universal" meta-learning, hindering comparability and progress.

Method: Introduces a theoretical framework defining practical universality and algorithm-explicit vs algorithm-implicit learning. Presents TAIL with three innovations: random projections for cross-modal encoding, random injection label embeddings for larger label spaces, and efficient inline query processing.

Result: TAIL achieves SOTA on few-shot benchmarks, generalizes to unseen domains and modalities (e.g., text classification despite image-only training), handles tasks with 20x more classes than training, and provides orders-of-magnitude computational savings.

Conclusion: TAIL demonstrates practical universal meta-learning capabilities across diverse tasks, modalities, and label configurations while being computationally efficient, advancing the field with both theoretical framework and practical implementation.

Abstract: Current meta-learning methods are constrained to narrow task distributions with fixed feature and label spaces, limiting applicability. Moreover, the current meta-learning literature uses key terms like "universal" and "general-purpose" inconsistently and lacks precise definitions, hindering comparability. We introduce a theoretical framework for meta-learning which formally defines practical universality and introduces a distinction between algorithm-explicit and algorithm-implicit learning, providing a principled vocabulary for reasoning about universal meta-learning methods. Guided by this framework, we present TAIL, a transformer-based algorithm-implicit meta-learner that functions across tasks with varying domains, modalities, and label configurations. TAIL features three innovations over prior transformer-based meta-learners: random projections for cross-modal feature encoding, random injection label embeddings that extrapolate to larger label spaces, and efficient inline query processing. TAIL achieves state-of-the-art performance on standard few-shot benchmarks while generalizing to unseen domains. Unlike other meta-learning methods, it also generalizes to unseen modalities, solving text classification tasks despite training exclusively on images, handles tasks with up to 20$\times$ more classes than seen during training, and provides orders-of-magnitude computational savings over prior transformer-based approaches.

</details>


### [515] [Learning Structural Hardness for Combinatorial Auctions: Instance-Dependent Algorithm Selection via Graph Neural Networks](https://arxiv.org/abs/2602.14772)
*Sungwoo Kang*

Main category: cs.LG

TL;DR: The paper proposes a hardness classifier to predict when combinatorial auction instances will defeat greedy heuristics, enabling algorithm selection between greedy and GNN solvers.


<details>
  <summary>Details</summary>
Motivation: Existing ML approaches for combinatorial optimization focus on replacing solvers, but GNNs rarely outperform classical methods. Instead of learning to replace solvers, the authors aim to predict when instances are hard for greedy allocation to enable intelligent algorithm selection.

Method: Designed a 20-dimensional structural feature vector and trained a lightweight MLP hardness classifier to predict greedy optimality gap. For instances identified as hard (exhibiting "whale-fish" trap structure), deployed a heterogeneous GNN specialist. Created a hybrid allocator combining hardness classifier with GNN and greedy solvers.

Result: Hardness classifier achieved mean absolute error 0.033, Pearson correlation 0.937, and binary classification accuracy 94.7%. GNN specialist achieved ≈0% optimality gap on all six adversarial configurations (vs. 3.75-59.24% for greedy). Hybrid allocator achieved 0.51% overall gap on mixed distributions. Evaluation on CATS benchmarks confirmed GNNs don't outperform Gurobi (0.45-0.71 vs. 0.20 gap).

Conclusion: Learning when to deploy expensive solvers is more tractable than learning to replace them. The algorithm selection approach successfully combines ML prediction with specialized solvers to achieve better overall performance than either method alone.

Abstract: The Winner Determination Problem (WDP) in combinatorial auctions is NP-hard, and no existing method reliably predicts which instances will defeat fast greedy heuristics. The ML-for-combinatorial-optimization community has focused on learning to \emph{replace} solvers, yet recent evidence shows that graph neural networks (GNNs) rarely outperform well-tuned classical methods on standard benchmarks. We pursue a different objective: learning to predict \emph{when} a given instance is hard for greedy allocation, enabling instance-dependent algorithm selection. We design a 20-dimensional structural feature vector and train a lightweight MLP hardness classifier that predicts the greedy optimality gap with mean absolute error 0.033, Pearson correlation 0.937, and binary classification accuracy 94.7\% across three random seeds. For instances identified as hard -- those exhibiting ``whale-fish'' trap structure where greedy provably fails -- we deploy a heterogeneous GNN specialist that achieves ${\approx}0\%$ optimality gap on all six adversarial configurations tested (vs.\ 3.75--59.24\% for greedy). A hybrid allocator combining the hardness classifier with GNN and greedy solvers achieves 0.51\% overall gap on mixed distributions. Our honest evaluation on CATS benchmarks confirms that GNNs do not outperform Gurobi (0.45--0.71 vs.\ 0.20 gap), motivating the algorithm selection framing. Learning \emph{when} to deploy expensive solvers is more tractable than learning to replace them.

</details>


### [516] [On the Stability of Nonlinear Dynamics in GD and SGD: Beyond Quadratic Potentials](https://arxiv.org/abs/2602.14789)
*Rotem Mulayoff,Sebastian U. Stich*

Main category: cs.LG

TL;DR: The paper shows that linear stability analysis can be misleading for gradient descent optimization, and derives exact nonlinear stability criteria that depend on high-order derivatives, revealing that SGD stability can be dictated by single unstable batches rather than average behavior.


<details>
  <summary>Details</summary>
Motivation: Prior work often uses linearization to determine stability of optimization algorithms, but it's unclear whether linearized dynamics faithfully capture full nonlinear behavior. Recent work shows GD can stably oscillate near linearly unstable minima, indicating linear analysis can be misleading.

Method: The authors derive an exact criterion for stable oscillations of GD near minima in multivariate setting, depending on high-order derivatives. They extend analysis to SGD, showing nonlinear dynamics can diverge in expectation even if a single batch is unstable.

Result: Nonlinear stability conditions generalize existing results and show that SGD stability can be dictated by a single batch that oscillates unstably, rather than average effect as linear analysis suggests. If all batches are linearly stable, nonlinear dynamics of SGD are stable in expectation.

Conclusion: Nonlinear terms significantly impact optimization stability, and linear analysis can be misleading. The derived nonlinear stability criteria provide more accurate understanding of optimization dynamics, especially for SGD where single unstable batches can dominate stability behavior.

Abstract: The dynamical stability of the iterates during training plays a key role in determining the minima obtained by optimization algorithms. For example, stable solutions of gradient descent (GD) correspond to flat minima, which have been associated with favorable features. While prior work often relies on linearization to determine stability, it remains unclear whether linearized dynamics faithfully capture the full nonlinear behavior. Recent work has shown that GD may stably oscillate near a linearly unstable minimum and still converge once the step size decays, indicating that linear analysis can be misleading. In this work, we explicitly study the effect of nonlinear terms. Specifically, we derive an exact criterion for stable oscillations of GD near minima in the multivariate setting. Our condition depends on high-order derivatives, generalizing existing results. Extending the analysis to stochastic gradient descent (SGD), we show that nonlinear dynamics can diverge in expectation even if a single batch is unstable. This implies that stability can be dictated by a single batch that oscillates unstably, rather than an average effect, as linear analysis suggests. Finally, we prove that if all batches are linearly stable, the nonlinear dynamics of SGD are stable in expectation.

</details>


### [517] [Extending Multi-Source Bayesian Optimization With Causality Principles](https://arxiv.org/abs/2602.14791)
*Luuk Jacobs,Mohammad Ali Javidian*

Main category: cs.LG

TL;DR: The paper proposes Multi-Source Causal Bayesian Optimization (MSCBO), which integrates causal principles with multi-source Bayesian optimization to handle scenarios with causal information and interventions, improving efficiency and scalability.


<details>
  <summary>Details</summary>
Motivation: Traditional Multi-Source Bayesian Optimization (MSBO) assumes independent variables, limiting effectiveness in causal scenarios like clinical trials or policy-making where interventions are possible. Single-source Causal Bayesian Optimization (CBO) shows benefits but doesn't address multi-source settings.

Method: The authors develop Multi-Source Causal Bayesian Optimization (MSCBO) by integrating MSBO and CBO methodologies. They present theoretical foundations and demonstrate how causal principles can be combined with multi-source optimization to reduce dimensionality and computational complexity.

Result: MSCBO outperforms foundational counterparts (MSBO and CBO) on both synthetic and real-world datasets with varying noise levels. The integration facilitates dimensionality reduction, lowers operational costs, and improves convergence speed, performance, and scalability.

Conclusion: Integrating MSBO with causality principles enables more efficient optimization in higher-dimensional problems with causal information, making MSCBO robust and applicable to real-world scenarios where interventions are possible.

Abstract: Multi-Source Bayesian Optimization (MSBO) serves as a variant of the traditional Bayesian Optimization (BO) framework applicable to situations involving optimization of an objective black-box function over multiple information sources such as simulations, surrogate models, or real-world experiments. However, traditional MSBO assumes the input variables of the objective function to be independent and identically distributed, limiting its effectiveness in scenarios where causal information is available and interventions can be performed, such as clinical trials or policy-making. In the single-source domain, Causal Bayesian Optimization (CBO) extends standard BO with the principles of causality, enabling better modeling of variable dependencies. This leads to more accurate optimization, improved decision-making, and more efficient use of low-cost information sources. In this article, we propose a principled integration of the MSBO and CBO methodologies in the multi-source domain, leveraging the strengths of both to enhance optimization efficiency and reduce computational complexity in higher-dimensional problems. We present the theoretical foundations of both Causal and Multi-Source Bayesian Optimization, and demonstrate how their synergy informs our Multi-Source Causal Bayesian Optimization (MSCBO) algorithm. We compare the performance of MSCBO against its foundational counterparts for both synthetic and real-world datasets with varying levels of noise, highlighting the robustness and applicability of MSCBO. Based on our findings, we conclude that integrating MSBO with the causality principles of CBO facilitates dimensionality reduction and lowers operational costs, ultimately improving convergence speed, performance, and scalability.

</details>


### [518] [Learning State-Tracking from Code Using Linear RNNs](https://arxiv.org/abs/2602.14814)
*Julien Siems,Riccardo Grazzi,Kirill Kalinin,Hitesh Ballani,Babak Rahmani*

Main category: cs.LG

TL;DR: Transformers fail at state-tracking tasks in code REPL format while linear RNNs succeed, but linear RNNs struggle with probabilistic state reveals where actions are partially observable.


<details>
  <summary>Details</summary>
Motivation: Existing state-tracking tasks use sequence-to-sequence mapping incompatible with next-token prediction used in language model training. Need to test architectures in realistic code execution settings.

Method: Convert permutation composition to code via REPL traces with interleaved state-reveals (prints) and variable transformations. Frame state-tracking as probabilistic finite-state automaton with deterministic state reveals.

Result: Linear RNNs capable of state-tracking excel in REPL code setting while Transformers still fail. However, linear RNNs can be worse than non-linear RNNs when tracking states with probabilistic reveals and partial observability.

Conclusion: State-tracking in code is difficult due to partial observability of actions. Linear RNNs have limitations compared to non-linear RNNs in probabilistic reveal settings, highlighting architectural trade-offs for different state-tracking scenarios.

Abstract: Over the last years, state-tracking tasks, particularly permutation composition, have become a testbed to understand the limits of sequence models architectures like Transformers and RNNs (linear and non-linear). However, these are often sequence-to-sequence tasks: learning to map actions (permutations) to states, which is incompatible with the next-token prediction setting commonly used to train language models. We address this gap by converting permutation composition into code via REPL traces that interleave state-reveals through prints and variable transformations. We show that linear RNNs capable of state-tracking excel also in this setting, while Transformers still fail. Motivated by this representation, we investigate why tracking states in code is generally difficult: actions are not always fully observable. We frame this as tracking the state of a probabilistic finite-state automaton with deterministic state reveals and show that linear RNNs can be worse than non-linear RNNs at tracking states in this setup.

</details>


### [519] [Interactionless Inverse Reinforcement Learning: A Data-Centric Framework for Durable Alignment](https://arxiv.org/abs/2602.14844)
*Elias Malomgré,Pieter Simoens*

Main category: cs.LG

TL;DR: The paper proposes Interactionless Inverse Reinforcement Learning to decouple alignment from policy optimization, creating reusable reward models instead of single-use alignment artifacts, and introduces an Alignment Flywheel for iterative human-in-the-loop refinement.


<details>
  <summary>Details</summary>
Motivation: Current AI alignment approaches suffer from structural flaws that entangle safety objectives with agent policies, creating opaque, single-use "Alignment Waste" artifacts that are not inspectable, editable, or reusable across different models.

Method: Proposes Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing inspectable, editable, and model-agnostic reward models. Also introduces the Alignment Flywheel - a human-in-the-loop lifecycle for iterative hardening through automated audits and refinement.

Result: The approach transforms safety from a disposable expense into a durable, verifiable engineering asset by creating reusable alignment artifacts that can be inspected, edited, and applied across different AI models.

Conclusion: The proposed architecture addresses the critical structural flaw in current alignment methods by decoupling safety objectives from policies, creating durable alignment assets through inspectable reward models and iterative human-in-the-loop refinement cycles.

Abstract: AI alignment is growing in importance, yet current approaches suffer from a critical structural flaw that entangles the safety objectives with the agent's policy. Methods such as Reinforcement Learning from Human Feedback and Direct Preference Optimization create opaque, single-use alignment artifacts, which we term Alignment Waste. We propose Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing an inspectable, editable, and model-agnostic reward model. Additionally, we introduce the Alignment Flywheel, a human-in-the-loop lifecycle that iteratively hardens the reward model through automated audits and refinement. This architecture transforms safety from a disposable expense into a durable, verifiable engineering asset.

</details>


### [520] [Atomix: Timely, Transactional Tool Use for Reliable Agentic Workflows](https://arxiv.org/abs/2602.14849)
*Bardia Mohammadi,Nearchos Potamitis,Lars Klein,Akhil Arora,Laurent Bindschaedler*

Main category: cs.LG

TL;DR: Atomix provides transactional semantics for LLM agent tool calls with progress-aware commit and compensation mechanisms to prevent unintended side effects from speculative or failed operations.


<details>
  <summary>Details</summary>
Motivation: LLM agents acting on external systems face risks of unintended side effects from speculative execution, failures, or contention, with no safe rollback mechanism for leaked effects.

Method: Atomix uses epoch tagging for calls, tracks per-resource frontiers, commits only when progress predicates indicate safety, buffers effects when possible, and provides compensation for externalized effects on abort.

Result: Transactional retry improves task success rates across real workloads with fault injection, while frontier-gated commit strengthens isolation under speculation and contention scenarios.

Conclusion: Atomix provides effective transactional semantics for LLM agent tool calls, addressing critical isolation and rollback challenges in speculative and failure-prone environments.

Abstract: LLM agents increasingly act on external systems, yet tool effects are immediate. Under failures, speculation, or contention, losing branches can leak unintended side effects with no safe rollback. We introduce Atomix, a runtime that provides progress-aware transactional semantics for agent tool calls. Atomix tags each call with an epoch, tracks per-resource frontiers, and commits only when progress predicates indicate safety; bufferable effects can be delayed, while externalized effects are tracked and compensated on abort. Across real workloads with fault injection, transactional retry improves task success, while frontier-gated commit strengthens isolation under speculation and contention.

</details>


### [521] [BEACONS: Bounded-Error, Algebraically-Composable Neural Solvers for Partial Differential Equations](https://arxiv.org/abs/2602.14853)
*Jonathan Gorard,Ammar Hakim,James Juno*

Main category: cs.LG

TL;DR: BEACONS framework enables formally-verified neural network PDE solvers with guaranteed correctness even in extrapolatory regimes using bounded-error compositional architectures.


<details>
  <summary>Details</summary>
Motivation: Neural networks struggle to generalize beyond training data convex hulls, which is problematic for computational physics where PDEs often need solving in unvalidated regimes far from training data.

Method: Uses method of characteristics to predict PDE solution properties a priori, constructs rigorous extrapolatory bounds on worst-case L^inf errors for shallow networks, then composes them into deep architectures using compositional deep learning to suppress large errors.

Result: BEACONS framework includes automatic code generator for neural solvers and automated theorem-proving system for machine-checkable correctness certificates; successfully applied to linear/non-linear PDEs (advection, Burgers', Euler equations) in 1D/2D with reliable bounded extrapolation.

Conclusion: BEACONS provides formally-verified neural PDE solvers with guaranteed correctness in extrapolatory regimes, overcoming limitations of traditional neural networks and offering advantages over classical PINN approaches.

Abstract: The traditional limitations of neural networks in reliably generalizing beyond the convex hulls of their training data present a significant problem for computational physics, in which one often wishes to solve PDEs in regimes far beyond anything which can be experimentally or analytically validated. In this paper, we show how it is possible to circumvent these limitations by constructing formally-verified neural network solvers for PDEs, with rigorous convergence, stability, and conservation properties, whose correctness can therefore be guaranteed even in extrapolatory regimes. By using the method of characteristics to predict the analytical properties of PDE solutions a priori (even in regions arbitrarily far from the training domain), we show how it is possible to construct rigorous extrapolatory bounds on the worst-case L^inf errors of shallow neural network approximations. Then, by decomposing PDE solutions into compositions of simpler functions, we show how it is possible to compose these shallow neural networks together to form deep architectures, based on ideas from compositional deep learning, in which the large L^inf errors in the approximations have been suppressed. The resulting framework, called BEACONS (Bounded-Error, Algebraically-COmposable Neural Solvers), comprises both an automatic code-generator for the neural solvers themselves, as well as a bespoke automated theorem-proving system for producing machine-checkable certificates of correctness. We apply the framework to a variety of linear and non-linear PDEs, including the linear advection and inviscid Burgers' equations, as well as the full compressible Euler equations, in both 1D and 2D, and illustrate how BEACONS architectures are able to extrapolate solutions far beyond the training data in a reliable and bounded way. Various advantages of the approach over the classical PINN approach are discussed.

</details>


### [522] [A Pragmatic Method for Comparing Clusterings with Overlaps and Outliers](https://arxiv.org/abs/2602.14855)
*Ryan DeWolfe,Paweł Prałat,François Théberge*

Main category: cs.LG

TL;DR: Proposes a new similarity measure for comparing clusterings that can handle overlapping clusters and outliers, addressing a gap in existing evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Current clustering comparison methods don't handle real-world scenarios where clusterings may contain outliers (objects belonging to no cluster) or overlapping clusters (objects belonging to multiple clusters), creating a gap in evaluation methodology.

Method: Defines a pragmatic similarity measure specifically designed for comparing clusterings with overlaps and outliers, focusing on practical applicability and desirable mathematical properties.

Result: The proposed measure demonstrates several desirable properties and experimental validation shows it avoids common biases that affect existing clustering comparison measures.

Conclusion: The new similarity measure fills an important gap in clustering evaluation by providing a robust method for comparing clusterings with overlaps and outliers, which better reflects real-world clustering scenarios.

Abstract: Clustering algorithms are an essential part of the unsupervised data science ecosystem, and extrinsic evaluation of clustering algorithms requires a method for comparing the detected clustering to a ground truth clustering. In a general setting, the detected and ground truth clusterings may have outliers (objects belonging to no cluster), overlapping clusters (objects may belong to more than one cluster), or both, but methods for comparing these clusterings are currently undeveloped. In this note, we define a pragmatic similarity measure for comparing clusterings with overlaps and outliers, show that it has several desirable properties, and experimentally confirm that it is not subject to several common biases afflicting other clustering comparison measures.

</details>


### [523] [Goldilocks RL: Tuning Task Difficulty to Escape Sparse Rewards for Reasoning](https://arxiv.org/abs/2602.14868)
*Ilia Mahrooghi,Aryo Lotfi,Emmanuel Abbe*

Main category: cs.LG

TL;DR: Goldilocks is a teacher-driven data sampling strategy that selects questions of appropriate difficulty (neither too easy nor too hard) for student models during reinforcement learning training, improving sample efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning for reasoning in LLMs suffers from sample inefficiency due to sparse rewards. Traditional curriculum learning has limitations because the optimal difficulty ordering for a specific model is unclear.

Method: Proposes Goldilocks: a teacher model predicts question difficulty for the student model and selects questions of appropriate difficulty (Goldilocks principle). The teacher continuously adapts to the student's evolving abilities based on performance on seen samples, while training the student with GRPO.

Result: On the OpenMathReasoning dataset, Goldilocks data sampling improves the performance of models trained with standard GRPO under the same compute budget.

Conclusion: Goldilocks provides an effective teacher-driven sampling strategy that addresses sample inefficiency in RL for reasoning tasks by dynamically selecting appropriately difficult questions based on the student model's current capabilities.

Abstract: Reinforcement learning has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, relying on sparse rewards makes this process highly sample-inefficient, as models must navigate vast search spaces with minimal feedback. While classic curriculum learning aims to mitigate this by ordering data based on complexity, the right ordering for a specific model is often unclear. To address this, we propose Goldilocks, a novel teacher-driven data sampling strategy that aims to predict each question's difficulty for the student model. The teacher model selects questions of appropriate difficulty for the student model, i.e., questions that are neither too easy nor too hard (Goldilocks principle), while training the student with GRPO. By leveraging the student's performance on seen samples, the teacher continuously adapts to the student's evolving abilities. On OpenMathReasoning dataset, Goldilocks data sampling improves the performance of models trained with standard GRPO under the same compute budget.

</details>


### [524] [On the Learning Dynamics of RLVR at the Edge of Competence](https://arxiv.org/abs/2602.14872)
*Yu Huang,Zixin Wen,Yuejie Chi,Yuting Wei,Aarti Singh,Yingbin Liang,Yuxin Chen*

Main category: cs.LG

TL;DR: RLVR helps overcome long-horizon reasoning barriers through smooth difficulty spectrums that create relay effects, avoiding grokking plateaus.


<details>
  <summary>Details</summary>
Motivation: To understand how reinforcement learning with verifiable rewards (RLVR) based solely on final outcomes can overcome long-horizon barriers in extended reasoning tasks, despite the sparse reward signal.

Method: Developed a theory of RL training dynamics for transformers on compositional reasoning tasks using Fourier analysis on finite groups, characterizing how RLVR effectiveness depends on difficulty spectrum smoothness.

Result: Smooth difficulty spectrums create relay effects where persistent gradient signals on easier problems elevate capabilities to tackle harder ones, while abrupt difficulty discontinuities cause grokking-type phase transitions with prolonged plateaus.

Conclusion: RLVR improves performance at the edge of competence through relay effects, and appropriately designed data mixtures with smooth difficulty transitions can yield scalable gains in reasoning capabilities.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has been a main driver of recent breakthroughs in large reasoning models. Yet it remains a mystery how rewards based solely on final outcomes can help overcome the long-horizon barrier to extended reasoning. To understand this, we develop a theory of the training dynamics of RL for transformers on compositional reasoning tasks. Our theory characterizes how the effectiveness of RLVR is governed by the smoothness of the difficulty spectrum. When data contains abrupt discontinuities in difficulty, learning undergoes grokking-type phase transitions, producing prolonged plateaus before progress recurs. In contrast, a smooth difficulty spectrum leads to a relay effect: persistent gradient signals on easier problems elevate the model's capabilities to the point where harder ones become tractable, resulting in steady and continuous improvement. Our theory explains how RLVR can improve performance at the edge of competence, and suggests that appropriately designed data mixtures can yield scalable gains. As a technical contribution, our analysis develops and adapts tools from Fourier analysis on finite groups to our setting. We validate the predicted mechanisms empirically via synthetic experiments.

</details>


### [525] [Web-Scale Multimodal Summarization using CLIP-Based Semantic Alignment](https://arxiv.org/abs/2602.14889)
*Mounvik K,N Harshit*

Main category: cs.LG

TL;DR: Web-Scale Multimodal Summarization: A lightweight framework that generates summaries by retrieving and combining text and images from web sources using CLIP for semantic alignment and optional BLIP captioning.


<details>
  <summary>Details</summary>
Motivation: To create a configurable, deployable tool for web-scale summarization that integrates language, retrieval, and vision models, enabling users to generate multimodal summaries from web sources with controllable parameters.

Method: Performs parallel web, news, and image searches for user-defined topics, ranks retrieved images using fine-tuned CLIP for semantic alignment, optionally uses BLIP captioning for image-only summaries, and supports adjustable fetch limits, semantic filtering, and summary styling.

Result: Evaluation on 500 image-caption pairs with 20:1 contrastive negatives shows strong performance: ROC-AUC of 0.9270, F1-score of 0.6504, and accuracy of 96.99%, demonstrating effective multimodal alignment.

Conclusion: The framework provides a user-extensible, configurable pipeline for web-scale multimodal summarization that successfully integrates language, retrieval, and vision models, exposed via a Gradio-based API with controllable parameters.

Abstract: We introduce Web-Scale Multimodal Summarization, a lightweight framework for generating summaries by combining retrieved text and image data from web sources. Given a user-defined topic, the system performs parallel web, news, and image searches. Retrieved images are ranked using a fine-tuned CLIP model to measure semantic alignment with topic and text. Optional BLIP captioning enables image-only summaries for stronger multimodal coherence.The pipeline supports features such as adjustable fetch limits, semantic filtering, summary styling, and downloading structured outputs. We expose the system via a Gradio-based API with controllable parameters and preconfigured presets.Evaluation on 500 image-caption pairs with 20:1 contrastive negatives yields a ROC-AUC of 0.9270, an F1-score of 0.6504, and an accuracy of 96.99%, demonstrating strong multimodal alignment. This work provides a configurable, deployable tool for web-scale summarization that integrates language, retrieval, and vision models in a user-extensible pipeline.

</details>


### [526] [Algorithmic Simplification of Neural Networks with Mosaic-of-Motifs](https://arxiv.org/abs/2602.14896)
*Pedram Bakhtiarifard,Tong Chen,Jonathan Wenshøj,Erik B Dam,Raghavendra Selvan*

Main category: cs.LG

TL;DR: Deep neural networks are compressible because trained weights have lower algorithmic complexity than random initialization, and the proposed MoMos method exploits this by partitioning parameters into reusable motifs.


<details>
  <summary>Details</summary>
Motivation: To explain why deep neural networks are highly compressible despite their large parameter counts, by examining the algorithmic complexity of trained weights versus random initialization.

Method: Introduces Mosaic-of-Motifs (MoMos) method that partitions model parameters into blocks and restricts each block to be selected from a set of reusable motifs, creating algorithmically simpler parameterizations.

Result: Empirical evidence shows neural networks' algorithmic complexity (approximated via Kolmogorov complexity) decreases during training, and MoMos yields models with comparable performance to unconstrained models while being algorithmically simpler.

Conclusion: Deep neural networks are compressible because training introduces structure and repeatability in weights, reducing their algorithmic complexity, which compression methods like MoMos can effectively exploit.

Abstract: Large-scale deep learning models are well-suited for compression. Methods like pruning, quantization, and knowledge distillation have been used to achieve massive reductions in the number of model parameters, with marginal performance drops across a variety of architectures and tasks. This raises the central question: \emph{Why are deep neural networks suited for compression?} In this work, we take up the perspective of algorithmic complexity to explain this behavior. We hypothesize that the parameters of trained models have more structure and, hence, exhibit lower algorithmic complexity compared to the weights at (random) initialization. Furthermore, that model compression methods harness this reduced algorithmic complexity to compress models. Although an unconstrained parameterization of model weights, $\mathbf{w} \in \mathbb{R}^n$, can represent arbitrary weight assignments, the solutions found during training exhibit repeatability and structure, making them algorithmically simpler than a generic program. To this end, we formalize the Kolmogorov complexity of $\mathbf{w}$ by $\mathcal{K}(\mathbf{w})$. We introduce a constrained parameterization $\widehat{\mathbf{w}}$, that partitions parameters into blocks of size $s$, and restricts each block to be selected from a set of $k$ reusable motifs, specified by a reuse pattern (or mosaic). The resulting method, $\textit{Mosaic-of-Motifs}$ (MoMos), yields algorithmically simpler model parameterization compared to unconstrained models. Empirical evidence from multiple experiments shows that the algorithmic complexity of neural networks, measured using approximations to Kolmogorov complexity, can be reduced during training. This results in models that perform comparably with unconstrained models while being algorithmically simpler.

</details>


### [527] [Picking the Right Specialist: Attentive Neural Process-based Selection of Task-Specialized Models as Tools for Agentic Healthcare Systems](https://arxiv.org/abs/2602.14901)
*Pramit Saha,Joshua Strong,Mohammad Alsharid,Divyanshu Mishra,J. Alison Noble*

Main category: cs.LG

TL;DR: ToolSelect is a method for adaptive model selection in agentic healthcare systems that learns to choose the best specialist model from a heterogeneous pool for each clinical query, outperforming 10 state-of-the-art methods across multiple task families.


<details>
  <summary>Details</summary>
Motivation: In healthcare agentic systems, no single "best" model exists for clinical tasks - different specialist models excel on different data samples. Agents need to reliably select the right specialist model from a heterogeneous pool of tools for each query, but current methods lack effective model selection capabilities.

Method: ToolSelect adaptively learns model selection by minimizing population risk over sampled specialist tool candidates using a consistent surrogate of task-conditional selection loss. It uses an Attentive Neural Process-based selector conditioned on the query and per-model behavioral summaries to choose among specialist models.

Result: The authors introduce ToolSelectBench, a benchmark of 1448 queries in an agentic Chest X-ray environment with diverse task-specialized models (17 disease detection, 19 report generation, 6 visual grounding, and 13 VQA). ToolSelect consistently outperforms 10 state-of-the-art methods across four different task families.

Conclusion: ToolSelect provides an effective solution for adaptive model selection in agentic healthcare systems, enabling reliable selection of the most appropriate specialist model for each clinical query, which is crucial for building robust healthcare AI agents.

Abstract: Task-specialized models form the backbone of agentic healthcare systems, enabling the agents to answer clinical queries across tasks such as disease diagnosis, localization, and report generation. Yet, for a given task, a single "best" model rarely exists. In practice, each task is better served by multiple competing specialist models where different models excel on different data samples. As a result, for any given query, agents must reliably select the right specialist model from a heterogeneous pool of tool candidates. To this end, we introduce ToolSelect, which adaptively learns model selection for tools by minimizing a population risk over sampled specialist tool candidates using a consistent surrogate of the task-conditional selection loss. Concretely, we propose an Attentive Neural Process-based selector conditioned on the query and per-model behavioral summaries to choose among the specialist models. Motivated by the absence of any established testbed, we, for the first time, introduce an agentic Chest X-ray environment equipped with a diverse suite of task-specialized models (17 disease detection, 19 report generation, 6 visual grounding, and 13 VQA) and develop ToolSelectBench, a benchmark of 1448 queries. Our results demonstrate that ToolSelect consistently outperforms 10 SOTA methods across four different task families.

</details>


### [528] [Additive Control Variates Dominate Self-Normalisation in Off-Policy Evaluation](https://arxiv.org/abs/2602.14914)
*Olivier Jeunen,Shashank Gupta*

Main category: cs.LG

TL;DR: The paper proves that β*-IPS (optimal additive baseline) asymptotically dominates SNIPS in MSE for off-policy evaluation in ranking/recommendation systems.


<details>
  <summary>Details</summary>
Motivation: SNIPS is standard for variance reduction in OPE but recent work suggests additive control variates (baseline corrections) may perform better, though lacking theoretical guarantees for evaluation.

Method: Theoretical analysis proving β*-IPS (estimator with optimal additive baseline) asymptotically dominates SNIPS in Mean Squared Error, with analytical decomposition of variance gap showing SNIPS is equivalent to using a specific sub-optimal additive baseline.

Result: β*-IPS asymptotically dominates SNIPS in MSE, providing theoretical justification for shifting from self-normalization to optimal baseline corrections.

Conclusion: Theoretical results justify moving from self-normalization to optimal additive baseline corrections for off-policy evaluation in ranking and recommendation systems.

Abstract: Off-policy evaluation (OPE) is essential for assessing ranking and recommendation systems without costly online interventions. Self-Normalised Inverse Propensity Scoring (SNIPS) is a standard tool for variance reduction in OPE, leveraging a multiplicative control variate. Recent advances in off-policy learning suggest that additive control variates (baseline corrections) may offer superior performance, yet theoretical guarantees for evaluation are lacking. This paper provides a definitive answer: we prove that $β^\star$-IPS, an estimator with an optimal additive baseline, asymptotically dominates SNIPS in Mean Squared Error. By analytically decomposing the variance gap, we show that SNIPS is asymptotically equivalent to using a specific -- but generally sub-optimal -- additive baseline. Our results theoretically justify shifting from self-normalisation to optimal baseline corrections for both ranking and recommendation.

</details>


### [529] [BHyGNN+: Unsupervised Representation Learning for Heterophilic Hypergraphs](https://arxiv.org/abs/2602.14919)
*Tianyi Ma,Yiyue Qian,Zehong Wang,Zheyuan Zhang,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: BHyGNN+ is a self-supervised learning framework for heterophilic hypergraphs that uses hypergraph duality for contrastive learning without needing labeled data or negative samples.


<details>
  <summary>Details</summary>
Motivation: Existing Hypergraph Neural Networks (HyGNNs) struggle with heterophilic hypergraphs where connected nodes have dissimilar representations, and they rely heavily on labeled data which is scarce in real-world applications.

Method: Extends BHyGNN using hypergraph duality - interchanging nodes and hyperedges to create augmented views. Uses contrastive learning with cosine similarity between original hypergraph and its dual, eliminating need for negative samples.

Result: Outperforms state-of-the-art supervised and self-supervised baselines on 11 benchmark datasets for both heterophilic and homophilic hypergraphs.

Conclusion: Hypergraph duality provides an effective self-supervised learning paradigm for challenging unlabeled hypergraphs, establishing a new approach for representation learning without ground-truth labels.

Abstract: Hypergraph Neural Networks (HyGNNs) have demonstrated remarkable success in modeling higher-order relationships among entities. However, their performance often degrades on heterophilic hypergraphs, where nodes connected by the same hyperedge tend to have dissimilar semantic representations or belong to different classes. While several HyGNNs, including our prior work BHyGNN, have been proposed to address heterophily, their reliance on labeled data significantly limits their applicability in real-world scenarios where annotations are scarce or costly. To overcome this limitation, we introduce BHyGNN+, a self-supervised learning framework that extends BHyGNN for representation learning on heterophilic hypergraphs without requiring ground-truth labels. The core idea of BHyGNN+ is hypergraph duality, a structural transformation where the roles of nodes and hyperedges are interchanged. By contrasting augmented views of a hypergraph against its dual using cosine similarity, our framework captures essential structural patterns in a fully unsupervised manner. Notably, this duality-based formulation eliminates the need for negative samples, a common requirement in existing hypergraph contrastive learning methods that is often difficult to satisfy in practice. Extensive experiments on eleven benchmark datasets demonstrate that BHyGNN+ consistently outperforms state-of-the-art supervised and self-supervised baselines on both heterophilic and homophilic hypergraphs. Our results validate the effectiveness of leveraging hypergraph duality for self-supervised learning and establish a new paradigm for representation learning on challenging, unlabeled hypergraphs.

</details>


### [530] [Variance-Reduced $(\varepsilon,δ)-$Unlearning using Forget Set Gradients](https://arxiv.org/abs/2602.14938)
*Martin Van Waerebeke,Marco Lorenzi,Kevin Scaman,El Mahdi El Mhamdi,Giovanni Neglia*

Main category: cs.LG

TL;DR: VRU is the first first-order algorithm that directly uses forget set gradients while provably satisfying (ε,δ)-unlearning guarantees, achieving better convergence rates than methods that ignore the forget set.


<details>
  <summary>Details</summary>
Motivation: Existing certified unlearning methods only use the forget set to calibrate noise injection, while empirical heuristics use forget set gradients but lack formal guarantees. There's a gap between theoretically sound methods and practically effective approaches.

Method: Variance-Reduced Unlearning (VRU) algorithm that directly incorporates forget set gradients in its update rule while maintaining formal (ε,δ)-unlearning guarantees. It's a first-order method that uses both the forget set and noise calibration.

Result: VRU achieves strictly improved convergence rates compared to existing first-order (ε,δ)-unlearning methods. In low-error regimes, it asymptotically outperforms any first-order method that ignores the forget set. Experiments show consistent gains over both certified methods and empirical baselines.

Conclusion: VRU successfully bridges the gap between theoretically certified unlearning and practically effective gradient-based methods by being the first algorithm to directly use forget set gradients while provably satisfying (ε,δ)-unlearning guarantees.

Abstract: In machine unlearning, $(\varepsilon,δ)-$unlearning is a popular framework that provides formal guarantees on the effectiveness of the removal of a subset of training data, the forget set, from a trained model. For strongly convex objectives, existing first-order methods achieve $(\varepsilon,δ)-$unlearning, but they only use the forget set to calibrate injected noise, never as a direct optimization signal. In contrast, efficient empirical heuristics often exploit the forget samples (e.g., via gradient ascent) but come with no formal unlearning guarantees. We bridge this gap by presenting the Variance-Reduced Unlearning (VRU) algorithm. To the best of our knowledge, VRU is the first first-order algorithm that directly includes forget set gradients in its update rule, while provably satisfying ($(\varepsilon,δ)-$unlearning. We establish the convergence of VRU and show that incorporating the forget set yields strictly improved rates, i.e. a better dependence on the achieved error compared to existing first-order $(\varepsilon,δ)-$unlearning methods. Moreover, we prove that, in a low-error regime, VRU asymptotically outperforms any first-order method that ignores the forget set.Experiments corroborate our theory, showing consistent gains over both state-of-the-art certified unlearning methods and over empirical baselines that explicitly leverage the forget set.

</details>


### [531] [Locally Adaptive Multi-Objective Learning](https://arxiv.org/abs/2602.14952)
*Jivat Neet Kaur,Isaac Gibbs,Michael I. Jordan*

Main category: cs.LG

TL;DR: Proposes an adaptive online learning method for multi-objective prediction that handles distribution shifts by replacing part of existing methods with adaptive algorithms, showing improved performance on energy forecasting and fairness datasets.


<details>
  <summary>Details</summary>
Motivation: Existing multi-objective learning methods (for calibration, regret, multiaccuracy) don't adapt well to distribution shifts over time, and earlier attempts at local adaptivity lack empirical validation.

Method: Replaces one component of existing multi-objective learning methods with an adaptive online algorithm to achieve local adaptivity to distribution shifts.

Result: Empirical evaluation on energy forecasting and algorithmic fairness datasets shows improved performance over existing approaches, achieving unbiased subgroup predictions while remaining robust under distribution shift.

Conclusion: The proposed adaptive approach effectively handles distribution shifts in multi-objective online learning, outperforming existing methods that lack local adaptivity.

Abstract: We consider the general problem of learning a predictor that satisfies multiple objectives of interest simultaneously, a broad framework that captures a range of specific learning goals including calibration, regret, and multiaccuracy. We work in an online setting where the data distribution can change arbitrarily over time. Existing approaches to this problem aim to minimize the set of objectives over the entire time horizon in a worst-case sense, and in practice they do not necessarily adapt to distribution shifts. Earlier work has aimed to alleviate this problem by incorporating additional objectives that target local guarantees over contiguous subintervals. Empirical evaluation of these proposals is, however, scarce. In this article, we consider an alternative procedure that achieves local adaptivity by replacing one part of the multi-objective learning method with an adaptive online algorithm. Empirical evaluations on datasets from energy forecasting and algorithmic fairness show that our proposed method improves upon existing approaches and achieves unbiased predictions over subgroups, while remaining robust under distribution shift.

</details>


### [532] [Use What You Know: Causal Foundation Models with Partial Graphs](https://arxiv.org/abs/2602.14972)
*Arik Reuter,Anish Dhir,Cristiana Diaconu,Jake Robertson,Ole Ossen,Frank Hutter,Adrian Weller,Mark van der Wilk,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: The paper introduces methods to condition Causal Foundation Models (CFMs) on causal information like graphs or ancestral knowledge, enabling them to incorporate domain expertise while maintaining general-purpose capabilities.


<details>
  <summary>Details</summary>
Motivation: Current Causal Foundation Models don't allow incorporation of domain knowledge, leading to suboptimal predictions. There's a need to bridge this gap by enabling CFMs to leverage causal information while maintaining their general-purpose nature.

Method: Introduces conditioning methods for CFMs using causal information (full graphs or partial ancestral info). Proposes injecting learnable biases into attention mechanisms as the most effective conditioning strategy to utilize both full and partial causal information.

Result: Conditioning allows general-purpose CFMs to match performance of specialized models trained on specific causal structures. The approach effectively leverages any amount of domain expertise while answering causal queries in a data-driven manner.

Conclusion: The work addresses a central hurdle for all-in-one causal foundation models by enabling them to incorporate domain knowledge while maintaining general-purpose capabilities, moving toward more unified causal estimation.

Abstract: Estimating causal quantities traditionally relies on bespoke estimators tailored to specific assumptions. Recently proposed Causal Foundation Models (CFMs) promise a more unified approach by amortising causal discovery and inference in a single step. However, in their current state, they do not allow for the incorporation of any domain knowledge, which can lead to suboptimal predictions. We bridge this gap by introducing methods to condition CFMs on causal information, such as the causal graph or more readily available ancestral information. When access to complete causal graph information is too strict a requirement, our approach also effectively leverages partial causal information. We systematically evaluate conditioning strategies and find that injecting learnable biases into the attention mechanism is the most effective method to utilise full and partial causal information. Our experiments show that this conditioning allows a general-purpose CFM to match the performance of specialised models trained on specific causal structures. Overall, our approach addresses a central hurdle on the path towards all-in-one causal foundation models: the capability to answer causal queries in a data-driven manner while effectively leveraging any amount of domain expertise.

</details>


### [533] [MacroGuide: Topological Guidance for Macrocycle Generation](https://arxiv.org/abs/2602.14977)
*Alicja Maksymiuk,Alexandre Duplessis,Michael Bronstein,Alexander Tong,Fernanda Duarte,İsmail İlkan Ceylan*

Main category: cs.LG

TL;DR: MacroGuide uses persistent homology guidance to steer diffusion models toward generating macrocycles, increasing success rates from 1% to 99% while maintaining quality metrics.


<details>
  <summary>Details</summary>
Motivation: Macrocycles are promising drug candidates with enhanced selectivity and binding affinity, but they're underexplored in generative modeling due to dataset scarcity and topological constraint challenges in standard models.

Method: MacroGuide introduces a diffusion guidance mechanism using Persistent Homology to steer pretrained molecular generative models. At each denoising step, it constructs a Vietoris-Rips complex from atomic positions and optimizes persistent homology features to promote ring formation.

Result: Applying MacroGuide to pretrained diffusion models increases macrocycle generation rates from 1% to 99%, while matching or exceeding state-of-the-art performance on key quality metrics including chemical validity, diversity, and PoseBusters checks.

Conclusion: MacroGuide successfully addresses the challenge of generating macrocycles by incorporating topological guidance into diffusion models, enabling efficient generation of these valuable drug-like molecules in both unconditional and protein-pocket conditional settings.

Abstract: Macrocycles are ring-shaped molecules that offer a promising alternative to small-molecule drugs due to their enhanced selectivity and binding affinity against difficult targets. Despite their chemical value, they remain underexplored in generative modeling, likely owing to their scarcity in public datasets and the challenges of enforcing topological constraints in standard deep generative models. We introduce MacroGuide: Topological Guidance for Macrocycle Generation, a diffusion guidance mechanism that uses Persistent Homology to steer the sampling of pretrained molecular generative models toward the generation of macrocycles, in both unconditional and conditional (protein pocket) settings. At each denoising step, MacroGuide constructs a Vietoris-Rips complex from atomic positions and promotes ring formation by optimizing persistent homology features. Empirically, applying MacroGuide to pretrained diffusion models increases macrocycle generation rates from 1% to 99%, while matching or exceeding state-of-the-art performance on key quality metrics such as chemical validity, diversity, and PoseBusters checks.

</details>


### [534] [Orthogonalized Multimodal Contrastive Learning with Asymmetric Masking for Structured Representations](https://arxiv.org/abs/2602.14983)
*Carolin Cissee,Raneen Younis,Zahra Ahmadi*

Main category: cs.LG

TL;DR: COrAL is a multimodal contrastive learning framework that explicitly preserves redundant, unique, and synergistic information in multimodal representations using orthogonality constraints and asymmetric masking.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal contrastive learning methods mainly capture redundant cross-modal signals while neglecting modality-specific (unique) and interaction-driven (synergistic) information. Current extensions either fail to model synergistic interactions explicitly or learn different information components in an entangled manner, leading to incomplete representations and potential information leakage.

Method: COrAL uses a dual-path architecture with orthogonality constraints to disentangle shared and modality-specific features. It introduces asymmetric masking with complementary view-specific patterns to promote synergy modeling, forcing the model to infer cross-modal dependencies rather than relying solely on redundant cues.

Result: Extensive experiments on synthetic benchmarks and diverse MultiBench datasets show that COrAL consistently matches or outperforms state-of-the-art methods while exhibiting low performance variance across runs.

Conclusion: Explicitly modeling the full spectrum of multimodal information (redundant, unique, and synergistic) yields more stable, reliable, and comprehensive embeddings.

Abstract: Multimodal learning seeks to integrate information from heterogeneous sources, where signals may be shared across modalities, specific to individual modalities, or emerge only through their interaction. While self-supervised multimodal contrastive learning has achieved remarkable progress, most existing methods predominantly capture redundant cross-modal signals, often neglecting modality-specific (unique) and interaction-driven (synergistic) information. Recent extensions broaden this perspective, yet they either fail to explicitly model synergistic interactions or learn different information components in an entangled manner, leading to incomplete representations and potential information leakage. We introduce \textbf{COrAL}, a principled framework that explicitly and simultaneously preserves redundant, unique, and synergistic information within multimodal representations. COrAL employs a dual-path architecture with orthogonality constraints to disentangle shared and modality-specific features, ensuring a clean separation of information components. To promote synergy modeling, we introduce asymmetric masking with complementary view-specific patterns, compelling the model to infer cross-modal dependencies rather than rely solely on redundant cues. Extensive experiments on synthetic benchmarks and diverse MultiBench datasets demonstrate that COrAL consistently matches or outperforms state-of-the-art methods while exhibiting low performance variance across runs. These results indicate that explicitly modeling the full spectrum of multimodal information yields more stable, reliable, and comprehensive embeddings.

</details>


### [535] [Spectral Convolution on Orbifolds for Geometric Deep Learning](https://arxiv.org/abs/2602.14997)
*Tim Mangliers,Bernhard Mössner,Benjamin Himpel*

Main category: cs.LG

TL;DR: Introduces spectral convolution on orbifolds as a new geometric deep learning building block for non-Euclidean data with orbifold structure, demonstrated with music theory applications.


<details>
  <summary>Details</summary>
Motivation: Geometric deep learning needs to handle diverse topological and geometric structures beyond Euclidean spaces. There's demand to make more complex data domains (like orbifolds) accessible to machine learning for application-related data.

Method: Introduces the concept of spectral convolution on orbifolds, extending existing spectral convolution techniques that form building blocks for convolutional neural network-like architectures on non-Euclidean data.

Result: Develops a theoretical framework for spectral convolution on orbifolds, providing a new building block for geometric deep learning that can handle orbifold-structured data.

Conclusion: Spectral convolution on orbifolds enables geometric deep learning to work with orbifold-structured data, expanding the scope of machine learning to more complex geometric domains, with practical applications demonstrated in music theory.

Abstract: Geometric deep learning (GDL) deals with supervised learning on data domains that go beyond Euclidean structure, such as data with graph or manifold structure. Due to the demand that arises from application-related data, there is a need to identify further topological and geometric structures with which these use cases can be made accessible to machine learning. There are various techniques, such as spectral convolution, that form the basic building blocks for some convolutional neural network-like architectures on non-Euclidean data. In this paper, the concept of spectral convolution on orbifolds is introduced. This provides a building block for making learning on orbifold structured data accessible using GDL. The theory discussed is illustrated using an example from music theory.

</details>


### [536] [Boundary Point Jailbreaking of Black-Box LLMs](https://arxiv.org/abs/2602.15001)
*Xander Davies,Giorgi Giglemiani,Edmund Lau,Eric Winsor,Geoffrey Irving,Yarin Gal*

Main category: cs.LG

TL;DR: BPJ is a new black-box jailbreak attack that evades strong LLM safeguards by using only binary classifier feedback and curriculum learning with boundary point evaluation.


<details>
  <summary>Details</summary>
Motivation: Current LLM safeguards have become robust enough to survive thousands of hours of human red teaming, but existing automated attacks rely on white/grey-box assumptions or existing jailbreak libraries. There's a need for fully black-box attacks that can bypass the strongest industry-deployed defenses.

Method: BPJ uses only binary feedback (whether classifier flags interaction) and converts target harmful strings into curriculum of intermediate attack targets. It actively selects "boundary points" - evaluation points that best detect small changes in attack strength - to optimize attacks without direct access to classifier scores or gradients.

Result: BPJ is the first fully automated attack that succeeds against Constitutional Classifiers and GPT-5's input classifier without human attack seeds. It demonstrates that current single-interaction defenses are insufficient against sophisticated black-box optimization.

Conclusion: BPJ shows that effective defense requires supplementing single-interaction methods with batch-level monitoring, as the attack incurs many flags during optimization but ultimately succeeds in evading detection in individual interactions.

Abstract: Frontier LLMs are safeguarded against attempts to extract harmful information via adversarial prompts known as "jailbreaks". Recently, defenders have developed classifier-based systems that have survived thousands of hours of human red teaming. We introduce Boundary Point Jailbreaking (BPJ), a new class of automated jailbreak attacks that evade the strongest industry-deployed safeguards. Unlike previous attacks that rely on white/grey-box assumptions (such as classifier scores or gradients) or libraries of existing jailbreaks, BPJ is fully black-box and uses only a single bit of information per query: whether or not the classifier flags the interaction. To achieve this, BPJ addresses the core difficulty in optimising attacks against robust real-world defences: evaluating whether a proposed modification to an attack is an improvement. Instead of directly trying to learn an attack for a target harmful string, BPJ converts the string into a curriculum of intermediate attack targets and then actively selects evaluation points that best detect small changes in attack strength ("boundary points"). We believe BPJ is the first fully automated attack algorithm that succeeds in developing universal jailbreaks against Constitutional Classifiers, as well as the first automated attack algorithm that succeeds against GPT-5's input classifier without relying on human attack seeds. BPJ is difficult to defend against in individual interactions but incurs many flags during optimisation, suggesting that effective defence requires supplementing single-interaction methods with batch-level monitoring.

</details>


### [537] [PDE foundation models are skillful AI weather emulators for the Martian atmosphere](https://arxiv.org/abs/2602.15004)
*Johannes Schmude,Sujit Roy,Liping Wang,Theodore van Kessel,Levente Klein,Marcus Freitag,Eloisa Bentivegna,Robert Manson-Sawko,Bjorn Lutjens,Manil Maskey,Campbell Watson,Rahul Ramachandran,Juan Bernabe-Moreno*

Main category: cs.LG

TL;DR: PDE foundation models pretrained on diverse PDE solutions can be adapted to create skillful weather emulators for Mars, achieving 34.4% performance improvement with limited data and compute.


<details>
  <summary>Details</summary>
Motivation: To develop efficient weather prediction models for Mars that can work with sparse data and limited computational resources by leveraging pretrained PDE foundation models.

Method: Adapt Poseidon PDE foundation model from 2D to 3D while preserving pretraining, fine-tune on Martian atmospheric data (4 Martian years, ~34GB), and test with sparse initial conditions.

Result: 34.4% performance improvement on held-out test data compared to baseline, demonstrating effective adaptation with only 13 GPU hours median compute budget.

Conclusion: PDE foundation models can serve as effective anchors for real-world complex systems with limited training data or compute resources, extending beyond theoretical PDE approximation to practical applications.

Abstract: We show that AI foundation models that are pretrained on numerical solutions to a diverse corpus of partial differential equations can be adapted and fine-tuned to obtain skillful predictive weather emulators for the Martian atmosphere. We base our work on the Poseidon PDE foundation model for two-dimensional systems. We develop a method to extend Poseidon from two to three dimensions while keeping the pretraining information. Moreover, we investigate the performance of the model in the presence of sparse initial conditions. Our results make use of four Martian years (approx.~34 GB) of training data and a median compute budget of 13 GPU hours. We find that the combination of pretraining and model extension yields a performance increase of 34.4\% on a held-out year. This shows that PDEs-FMs can not only approximate solutions to (other) PDEs but also anchor models for real-world problems with complex interactions that lack a sufficient amount of training data or a suitable compute budget.

</details>


### [538] [Scaling Beyond Masked Diffusion Language Models](https://arxiv.org/abs/2602.15014)
*Subham Sekhar Sahoo,Jean-Marie Lemercier,Zhihan Yang,Justin Deschenaux,Jingyu Liu,John Thickstun,Ante Jukic*

Main category: cs.LG

TL;DR: The paper challenges the dominance of Masked diffusion in language modeling by showing uniform-state diffusion remains competitive on benchmarks and outperforms others on GSM8K despite worse perplexity, highlighting that perplexity alone is insufficient for cross-algorithm comparison.


<details>
  <summary>Details</summary>
Motivation: To challenge the prevailing view that Masked diffusion is categorically superior for diffusion language modeling, and to demonstrate that perplexity metrics alone are insufficient for comparing different diffusion approaches, especially when considering practical sampling speed and efficiency.

Method: Conducted the first scaling law study of uniform-state and interpolating discrete diffusion methods. Improved Masked diffusion efficiency by ~12% using a simple cross-entropy objective. Compared different diffusion families (Masked, uniform-state, interpolating) scaled to 1.7B parameters, evaluating them on both perplexity-based benchmarks and practical metrics like sampling speed.

Result: Uniform-state diffusion remains competitive on likelihood benchmarks and outperforms both autoregressive and Masked diffusion models on GSM8K despite having worse validation perplexity. Perplexity is informative within a diffusion family but misleading across families, as models with worse likelihood scaling may be preferable due to faster, more practical sampling.

Conclusion: Masked diffusion is not categorically the future of diffusion language modeling; perplexity alone is insufficient for cross-algorithm comparison. The speed-quality Pareto frontier reveals that uniform-state diffusion offers competitive performance with practical advantages, challenging current assumptions about diffusion model evaluation.

Abstract: Diffusion language models are a promising alternative to autoregressive models due to their potential for faster generation. Among discrete diffusion approaches, Masked diffusion currently dominates, largely driven by strong perplexity on language modeling benchmarks. In this work, we present the first scaling law study of uniform-state and interpolating discrete diffusion methods. We also show that Masked diffusion models can be made approximately 12% more FLOPs-efficient when trained with a simple cross-entropy objective. We find that perplexity is informative within a diffusion family but can be misleading across families, where models with worse likelihood scaling may be preferable due to faster and more practical sampling, as reflected by the speed-quality Pareto frontier. These results challenge the view that Masked diffusion is categorically the future of diffusion language modeling and that perplexity alone suffices for cross-algorithm comparison. Scaling all methods to 1.7B parameters, we show that uniform-state diffusion remains competitive on likelihood-based benchmarks and outperforms autoregressive and Masked diffusion models on GSM8K, despite worse validation perplexity. We provide the code, model checkpoints, and video tutorials on the project page: http://s-sahoo.github.io/scaling-dllms

</details>


### [539] [Rethinking Diffusion Models with Symmetries through Canonicalization with Applications to Molecular Graph Generation](https://arxiv.org/abs/2602.15022)
*Cai Zhou,Zijie Chen,Zian Li,Jike Wang,Kaiyi Jiang,Pan Li,Rose Yu,Muhan Zhang,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: Canonical diffusion: map samples to canonical poses, train unconstrained models on canonical slices, then sample random symmetry transforms - outperforms equivariant methods in molecular generation.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches enforce invariance/equivariance through architectural constraints, but this paper challenges that tradition by proposing canonicalization as an alternative perspective for handling group symmetries in generative tasks.

Method: 1. Map each sample to orbit representative with canonical pose/order, 2. Train unconstrained diffusion/flow model on canonical slice, 3. Recover invariant distribution by sampling random symmetry transform at generation. Uses geometric spectra-based canonicalization and mild positional encodings.

Result: Canonical diffusion significantly outperforms equivariant baselines in 3D molecule generation tasks with similar or less computation. CanonFlow achieves SOTA on GEOM-DRUG dataset, with large advantages in few-step generation.

Conclusion: Canonicalization provides superior expressivity over invariant targets, accelerates training by removing diffusion score complexity, reduces conditional variance, and works complementarily with aligned priors and optimal transport for improved efficiency.

Abstract: Many generative tasks in chemistry and science involve distributions invariant to group symmetries (e.g., permutation and rotation). A common strategy enforces invariance and equivariance through architectural constraints such as equivariant denoisers and invariant priors. In this paper, we challenge this tradition through the alternative canonicalization perspective: first map each sample to an orbit representative with a canonical pose or order, train an unconstrained (non-equivariant) diffusion or flow model on the canonical slice, and finally recover the invariant distribution by sampling a random symmetry transform at generation time. Building on a formal quotient-space perspective, our work provides a comprehensive theory of canonical diffusion by proving: (i) the correctness, universality and superior expressivity of canonical generative models over invariant targets; (ii) canonicalization accelerates training by removing diffusion score complexity induced by group mixtures and reducing conditional variance in flow matching. We then show that aligned priors and optimal transport act complementarily with canonicalization and further improves training efficiency. We instantiate the framework for molecular graph generation under $S_n \times SE(3)$ symmetries. By leveraging geometric spectra-based canonicalization and mild positional encodings, canonical diffusion significantly outperforms equivariant baselines in 3D molecule generation tasks, with similar or even less computation. Moreover, with a novel architecture Canon, CanonFlow achieves state-of-the-art performance on the challenging GEOM-DRUG dataset, and the advantage remains large in few-step generation.

</details>


### [540] [Long Context, Less Focus: A Scaling Gap in LLMs Revealed through Privacy and Personalization](https://arxiv.org/abs/2602.15028)
*Shangding Gu*

Main category: cs.LG

TL;DR: PAPerBench benchmark reveals that increasing context length in LLMs degrades both personalization quality and privacy protection due to attention dilution in fixed-capacity Transformers.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in privacy-critical and personalization scenarios, but the impact of context length on privacy leakage and personalization effectiveness remains unexplored.

Method: Created PAPerBench benchmark with ~29,000 instances across context lengths from 1K to 256K tokens (377K total questions) to systematically evaluate personalization performance and privacy risks in LLMs.

Result: Extensive evaluations show consistent performance degradation in both personalization and privacy as context length increases. Theoretical analysis reveals this is due to attention dilution in fixed-capacity Transformers.

Conclusion: Current LLMs face a "long context, less focus" scaling gap where longer contexts reduce both personalization quality and privacy protection, highlighting an inherent limitation of soft attention mechanisms.

Abstract: Large language models (LLMs) are increasingly deployed in privacy-critical and personalization-oriented scenarios, yet the role of context length in shaping privacy leakage and personalization effectiveness remains largely unexplored. We introduce a large-scale benchmark, PAPerBench, to systematically study how increasing context length influences both personalization quality and privacy protection in LLMs. The benchmark comprises approximately 29,000 instances with context lengths ranging from 1K to 256K tokens, yielding a total of 377K evaluation questions. It jointly evaluates personalization performance and privacy risks across diverse scenarios, enabling controlled analysis of long-context model behavior. Extensive evaluations across state-of-the-art LLMs reveal consistent performance degradation in both personalization and privacy as context length increases. We further provide a theoretical analysis of attention dilution under context scaling, explaining this behavior as an inherent limitation of soft attention in fixed-capacity Transformers. The empirical and theoretical findings together suggest a general scaling gap in current models -- long context, less focus. We release the benchmark to support reproducible evaluation and future research on scalable privacy and personalization. Code and data are available at https://github.com/SafeRL-Lab/PAPerBench

</details>


### [541] [Symmetry in language statistics shapes the geometry of model representations](https://arxiv.org/abs/2602.15029)
*Dhruva Karkada,Daniel J. Korchinski,Andres Nava,Matthieu Wyart,Yasaman Bahri*

Main category: cs.LG

TL;DR: The paper shows that translation symmetry in language statistics explains geometric structures in LLM representations, and these structures are robust to perturbations due to underlying continuous latent variables.


<details>
  <summary>Details</summary>
Motivation: To understand why simple geometric structures emerge in LLM representations (like months forming a circle, years forming a smooth manifold, and cities' coordinates being linearly decodable) and what governs these structures.

Method: The authors identify translation symmetry in language statistics (e.g., co-occurrence probability depends only on time intervals), prove this governs geometric structures in high-dimensional word embedding models, and show robustness emerges from underlying continuous latent variables controlling co-occurrence statistics collectively.

Result: The geometric structures persist even when co-occurrence statistics are strongly perturbed (e.g., removing sentences with two months together) and at moderate embedding dimensions. The theoretical framework is empirically validated in word embedding models, text embedding models, and large language models.

Conclusion: Translation symmetry in language statistics governs the emergence of simple geometric structures in LLM representations, and the robustness of these structures is explained by underlying continuous latent variables that collectively control co-occurrence statistics.

Abstract: Although learned representations underlie neural networks' success, their fundamental properties remain poorly understood. A striking example is the emergence of simple geometric structures in LLM representations: for example, calendar months organize into a circle, years form a smooth one-dimensional manifold, and cities' latitudes and longitudes can be decoded by a linear probe. We show that the statistics of language exhibit a translation symmetry -- e.g., the co-occurrence probability of two months depends only on the time interval between them -- and we prove that the latter governs the aforementioned geometric structures in high-dimensional word embedding models. Moreover, we find that these structures persist even when the co-occurrence statistics are strongly perturbed (for example, by removing all sentences in which two months appear together) and at moderate embedding dimension. We show that this robustness naturally emerges if the co-occurrence statistics are collectively controlled by an underlying continuous latent variable. We empirically validate this theoretical framework in word embedding models, text embedding models, and large language models.

</details>
