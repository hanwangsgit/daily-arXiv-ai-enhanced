{"id": "2510.01636", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.01636", "abs": "https://arxiv.org/abs/2510.01636", "authors": ["Xingyu Zhou", "Le Liang", "Jing Zhang", "Chao-Kai Wen", "Shi Jin"], "title": "Next-Generation AI-Native Wireless Communications: MCMC-Based Receiver Architectures for Unified Processing", "comment": "7 pages, 6 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "The multiple-input multiple-output (MIMO) receiver processing is a key\ntechnology for current and next-generation wireless communications. However, it\nfaces significant challenges related to complexity and scalability as the\nnumber of antennas increases. Artificial intelligence (AI), a cornerstone of\nnext-generation wireless networks, offers considerable potential for addressing\nthese challenges. This paper proposes an AI-driven, universal MIMO receiver\narchitecture based on Markov chain Monte Carlo (MCMC) techniques. Unlike\nexisting AI-based methods that treat receiver processing as a black box, our\nMCMC-based approach functions as a generic Bayesian computing engine applicable\nto various processing tasks, including channel estimation, symbol detection,\nand channel decoding. This method enhances the interpretability, scalability,\nand flexibility of receivers in diverse scenarios. Furthermore, the proposed\napproach integrates these tasks into a unified probabilistic framework, thereby\nenabling overall performance optimization. This unified framework can also be\nseamlessly combined with data-driven learning methods to facilitate the\ndevelopment of fully intelligent communication receivers.", "AI": {"tldr": "AI-driven MIMO receiver using MCMC as a universal Bayesian computing engine for channel estimation, symbol detection, and channel decoding, enhancing interpretability and scalability.", "motivation": "Address complexity and scalability challenges in MIMO receivers as antenna numbers increase, leveraging AI's potential for next-generation wireless networks.", "method": "Markov chain Monte Carlo (MCMC) based approach that serves as a generic Bayesian computing engine, integrating multiple processing tasks into a unified probabilistic framework.", "result": "Enhanced interpretability, scalability, and flexibility of receivers across diverse scenarios, enabling overall performance optimization.", "conclusion": "The proposed unified framework can be combined with data-driven learning to develop fully intelligent communication receivers."}}
{"id": "2510.01750", "categories": ["cs.IT", "math.IT", "math.RA"], "pdf": "https://arxiv.org/pdf/2510.01750", "abs": "https://arxiv.org/abs/2510.01750", "authors": ["Krishna Gopal Benerjee", "Manish K Gupta"], "title": "On Algebraic Approaches for DNA Codes with Multiple Constraints", "comment": "54 pages, draft", "summary": "DNA strings and their properties are widely studied since last 20 years due\nto its applications in DNA computing. In this area, one designs a set of DNA\nstrings (called DNA code) which satisfies certain thermodynamic and\ncombinatorial constraints such as reverse constraint, reverse-complement\nconstraint, $GC$-content constraint and Hamming constraint. However recent\napplications of DNA codes in DNA data storage resulted in many new constraints\non DNA codes such as avoiding tandem repeats constraint (a generalization of\nnon-homopolymer constraint) and avoiding secondary structures constraint.\nTherefore, in this chapter, we introduce DNA codes with recently developed\nconstraints. In particular, we discuss reverse, reverse-complement,\n$GC$-content, Hamming, uncorrelated-correlated, thermodynamic, avoiding tandem\nrepeats and avoiding secondary structures constraints. DNA codes are\nconstructed using various approaches such as algebraic, computational, and\ncombinatorial. In particular, in algebraic approaches, one uses a finite ring\nand a map to construct a DNA code. Most of such approaches does not yield DNA\ncodes with high Hamming distance. In this chapter, we focus on algebraic\nconstructions using maps (usually an isometry on some finite ring) which yields\nDNA codes with high Hamming distance. We focus on non-cyclic DNA codes. We\nbriefly discuss various metrics such as Gau distance, Non-Homopolymer distance\netc. We discuss about algebraic constructions of families of DNA codes that\nsatisfy multiple constraints and/or properties. Further, we also discuss about\nalgebraic bounds on DNA codes with multiple constraints. Finally, we present\nsome open research directions in this area.", "AI": {"tldr": "This chapter introduces DNA codes with recent constraints for DNA data storage applications, focusing on algebraic constructions that yield codes with high Hamming distance.", "motivation": "DNA codes are essential for DNA computing and data storage, requiring specific constraints like avoiding tandem repeats and secondary structures. Traditional algebraic approaches often fail to achieve high Hamming distance, motivating new construction methods.", "method": "The chapter uses algebraic approaches with finite rings and isometries to construct non-cyclic DNA codes. It discusses various metrics (Gau distance, Non-Homopolymer distance) and methods to satisfy multiple constraints simultaneously.", "result": "The chapter presents families of DNA codes that satisfy multiple constraints and properties, along with algebraic bounds for such codes. It focuses on achieving high Hamming distance through improved algebraic constructions.", "conclusion": "The work advances DNA code design for data storage applications and identifies open research directions in developing DNA codes with multiple constraints and improved properties."}}
{"id": "2510.01811", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.01811", "abs": "https://arxiv.org/abs/2510.01811", "authors": ["Silouanos Brazitikos", "Theodoulos Garefalakis", "Eleni Tzanaki"], "title": "List decoding of evaluation codes", "comment": null, "summary": "Polynomial evaluation codes hold a prominent place in coding theory. In this\nwork, we study the problem of list decoding for a general class of polynomial\nevaluation codes, also known as Toric codes, that are defined for any given\nconvex polytope P. Special cases, such as Reed-Solomon and Reed-Muller codes,\nhave been studied extensively. We present a generalization of the\nGuruswami-Sudan algorithm that takes into account the geometry and the\ncombinatorics of P and compute bounds for the decoding radius.", "AI": {"tldr": "Generalization of Guruswami-Sudan algorithm for polynomial evaluation codes (Toric codes) defined by convex polytopes, with bounds on decoding radius.", "motivation": "To extend list decoding techniques from special cases like Reed-Solomon and Reed-Muller codes to the broader class of polynomial evaluation codes defined by convex polytopes.", "method": "Developed a generalized Guruswami-Sudan algorithm that incorporates the geometry and combinatorics of the defining convex polytope P.", "result": "Computed bounds for the decoding radius of these generalized polynomial evaluation codes.", "conclusion": "Successfully extended list decoding capabilities to the general class of Toric codes, providing analytical bounds on their decoding performance."}}
{"id": "2510.01813", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.01813", "abs": "https://arxiv.org/abs/2510.01813", "authors": ["Li Wan", "Huarui Yin", "Wenyi Zhang"], "title": "Parallelism Empowered Guessing Random Additive Noise Decoding", "comment": null, "summary": "Advances in parallel hardware platforms have motivated the development of\nefficient universal decoders capable of meeting stringent throughput and\nlatency requirements. Guessing Random Additive Noise Decoding (GRAND) is a\nrecently proposed decoding paradigm that sequentially tests Error Patterns\n(EPs) until finding a valid codeword. While Soft GRAND (SGRAND) achieves\nmaximum-likelihood (ML) decoding, its inherently sequential nature hinders\nparallelism and results in high decoding latency. In this work, we utilize a\nunified binary tree representation of EPs, termed the EP tree, which enables\ncompact representation, efficient manipulation, and parallel exploration.\nBuilding upon this EP tree representation, we propose a parallel design of\nSGRAND, preserving its ML optimality while significantly reducing decoding\nlatency through pruning strategies and tree-based computation. Furthermore, we\ndevelop a hybrid GRAND algorithm that enhances Ordered Reliability Bits (ORB)\nGRAND with the EP tree representation, thereby achieving ML decoding with\nminimal additional computational cost beyond ORBGRAND while retaining parallel\nefficiency. Numerical experiments demonstrate that parallel SGRAND achieves a\n$3.75\\times$ acceleration compared to serial implementation, while the hybrid\nenhanced method achieves a $4.8\\times$ acceleration, with further gains\nexpected under hardware mapping.", "AI": {"tldr": "This paper proposes parallel implementations of Soft GRAND (SGRAND) decoding using a unified binary tree representation of Error Patterns (EPs), achieving significant latency reduction while maintaining maximum-likelihood optimality.", "motivation": "To overcome the high decoding latency of inherently sequential GRAND decoders and meet stringent throughput requirements enabled by advances in parallel hardware platforms.", "method": "Developed a unified binary tree representation of EPs (EP tree) for compact representation and parallel exploration, then proposed parallel SGRAND design with pruning strategies and tree-based computation, plus a hybrid GRAND algorithm combining EP tree with Ordered Reliability Bits GRAND.", "result": "Parallel SGRAND achieved 3.75\u00d7 acceleration over serial implementation, while the hybrid enhanced method achieved 4.8\u00d7 acceleration, with further gains expected under hardware mapping.", "conclusion": "The EP tree representation enables efficient parallel GRAND decoding while preserving ML optimality, significantly reducing latency and making GRAND more practical for high-throughput applications."}}
{"id": "2510.01361", "categories": ["eess.IV", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.01361", "abs": "https://arxiv.org/abs/2510.01361", "authors": ["Conall Daly", "Darren Ramsook", "Anil Kokaram"], "title": "An Efficient Quality Metric for Video Frame Interpolation Based on Motion-Field Divergence", "comment": "IEEE 17th International Conference on Quality of Multimedia\n  Experience 2025 accepted manuscript, 7 pages", "summary": "Video frame interpolation is a fundamental tool for temporal video\nenhancement, but existing quality metrics struggle to evaluate the perceptual\nimpact of interpolation artefacts effectively. Metrics like PSNR, SSIM and\nLPIPS ignore temporal coherence. State-of-the-art quality metrics tailored\ntowards video frame interpolation, like FloLPIPS, have been developed but\nsuffer from computational inefficiency that limits their practical application.\nWe present $\\text{PSNR}_{\\text{DIV}}$, a novel full-reference quality metric\nthat enhances PSNR through motion divergence weighting, a technique adapted\nfrom archival film restoration where it was developed to detect temporal\ninconsistencies. Our approach highlights singularities in motion fields which\nis then used to weight image errors. Evaluation on the BVI-VFI dataset (180\nsequences across multiple frame rates, resolutions and interpolation methods)\nshows $\\text{PSNR}_{\\text{DIV}}$ achieves statistically significant\nimprovements: +0.09 Pearson Linear Correlation Coefficient over FloLPIPS, while\nbeing 2.5$\\times$ faster and using 4$\\times$ less memory. Performance remains\nconsistent across all content categories and are robust to the motion estimator\nused. The efficiency and accuracy of $\\text{PSNR}_{\\text{DIV}}$ enables fast\nquality evaluation and practical use as a loss function for training neural\nnetworks for video frame interpolation tasks. An implementation of our metric\nis available at www.github.com/conalld/psnr-div.", "AI": {"tldr": "A novel PSNR-based quality metric called PSNR_DIV that uses motion divergence weighting to better evaluate video frame interpolation quality, outperforming existing methods in both accuracy and efficiency.", "motivation": "Existing quality metrics like PSNR, SSIM, and LPIPS ignore temporal coherence in video frame interpolation, while specialized metrics like FloLPIPS are computationally inefficient for practical use.", "method": "Enhances PSNR through motion divergence weighting, a technique adapted from archival film restoration that detects temporal inconsistencies by highlighting singularities in motion fields to weight image errors.", "result": "Evaluation on BVI-VFI dataset shows PSNR_DIV achieves +0.09 Pearson Linear Correlation Coefficient improvement over FloLPIPS, while being 2.5x faster and using 4x less memory. Performance is consistent across content categories and robust to motion estimator choice.", "conclusion": "PSNR_DIV provides efficient and accurate quality evaluation for video frame interpolation, enabling practical use as a loss function for training neural networks in this domain."}}
{"id": "2510.01253", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01253", "abs": "https://arxiv.org/abs/2510.01253", "authors": ["Jianzhang Zhang", "Jialong Zhou", "Chuang Liu"], "title": "OR-Toolformer: Modeling and Solving Operations Research Problems with Tool Augmented Large Language Models", "comment": null, "summary": "Large language models (LLMs) demonstrate strong mathematical reasoning, but\nreliance on closed-source APIs for OR tasks raises privacy concerns, and\ntraining open-source models from scratch incurs high compute costs. We\nintroduce OR-Toolformer, which fine-tunes Llama-3.1-8B-Instruct with a\nsemi-automatic data synthesis pipeline that generates diverse OR problem-answer\npairs and augments the model with external solvers to produce API calls. On\nthree of four standard benchmarks, OR-Toolformer achieves up to 80.1% execution\naccuracy, exceeding size-matched baselines by over 4.3%. In zero-shot\nevaluation on two unseen OR problem types, it attains 54% average accuracy, a\n21 percentage-point improvement over the strongest baseline. These findings\nvalidate the efficacy of tool-augmented fine-tuning LLMs for accurate and\ngeneralizable OR problem modeling and solving.", "AI": {"tldr": "OR-Toolformer fine-tunes Llama-3.1-8B-Instruct with a semi-automatic data synthesis pipeline and external solver integration to address privacy concerns and high compute costs in OR tasks, achieving superior performance on standard benchmarks and strong zero-shot generalization.", "motivation": "Address privacy concerns with closed-source APIs for OR tasks and reduce high compute costs of training open-source models from scratch while leveraging LLMs' mathematical reasoning capabilities.", "method": "Fine-tune Llama-3.1-8B-Instruct using semi-automatic data synthesis pipeline to generate diverse OR problem-answer pairs, and augment the model with external solvers to produce API calls.", "result": "Achieves up to 80.1% execution accuracy on three of four standard benchmarks (exceeding baselines by over 4.3%), and 54% average accuracy on unseen OR problems (21 percentage-point improvement over strongest baseline).", "conclusion": "Tool-augmented fine-tuning of LLMs is effective for accurate and generalizable OR problem modeling and solving."}}
{"id": "2510.01206", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01206", "abs": "https://arxiv.org/abs/2510.01206", "authors": ["Hung Le", "Sherif Abbas", "Minh Hoang Nguyen", "Van Dai Do", "Huu Hiep Nguyen", "Dung Nguyen"], "title": "Accelerating Long-Term Molecular Dynamics with Physics-Informed Time-Series Forecasting", "comment": "16 pages, preprint", "summary": "Efficient molecular dynamics (MD) simulation is vital for understanding\natomic-scale processes in materials science and biophysics. Traditional density\nfunctional theory (DFT) methods are computationally expensive, which limits the\nfeasibility of long-term simulations. We propose a novel approach that\nformulates MD simulation as a time-series forecasting problem, enabling\nadvanced forecasting models to predict atomic trajectories via displacements\nrather than absolute positions. We incorporate a physics-informed loss and\ninference mechanism based on DFT-parametrised pair-wise Morse potential\nfunctions that penalize unphysical atomic proximity to enforce physical\nplausibility. Our method consistently surpasses standard baselines in\nsimulation accuracy across diverse materials. The results highlight the\nimportance of incorporating physics knowledge to enhance the reliability and\nprecision of atomic trajectory forecasting. Remarkably, it enables stable\nmodeling of thousands of MD steps in minutes, offering a scalable alternative\nto costly DFT simulations.", "AI": {"tldr": "A novel approach formulates molecular dynamics simulation as time-series forecasting, using physics-informed constraints to predict atomic trajectories efficiently.", "motivation": "Traditional DFT methods for MD simulation are computationally expensive, limiting long-term simulations. A more efficient alternative is needed.", "method": "Formulates MD simulation as time-series forecasting, predicts atomic trajectories via displacements, incorporates physics-informed loss using DFT-parametrized Morse potential functions to enforce physical plausibility.", "result": "Consistently surpasses standard baselines in simulation accuracy across diverse materials, enables stable modeling of thousands of MD steps in minutes.", "conclusion": "Physics knowledge incorporation enhances reliability and precision of atomic trajectory forecasting, offering scalable alternative to costly DFT simulations."}}
{"id": "2510.01213", "categories": ["eess.SP", "cs.AR", "cs.CV", "cs.HC", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.01213", "abs": "https://arxiv.org/abs/2510.01213", "authors": ["Tao Han", "Ang Li", "Qinyu Chen", "Chang Gao"], "title": "JaneEye: A 12-nm 2K-FPS 18.9-$\u03bc$J/Frame Event-based Eye Tracking Accelerator", "comment": "Accepted to 2026 IEEE 31st Asia and South Pacific Design Automation\n  Conference (ASP-DAC) 2026", "summary": "Eye tracking has become a key technology for gaze-based interactions in\nExtended Reality (XR). However, conventional frame-based eye-tracking systems\noften fall short of XR's stringent requirements for high accuracy, low latency,\nand energy efficiency. Event cameras present a compelling alternative, offering\nultra-high temporal resolution and low power consumption. In this paper, we\npresent JaneEye, an energy-efficient event-based eye-tracking hardware\naccelerator designed specifically for wearable devices, leveraging sparse,\nhigh-temporal-resolution event data. We introduce an ultra-lightweight neural\nnetwork architecture featuring a novel ConvJANET layer, which simplifies the\ntraditional ConvLSTM by retaining only the forget gate, thereby halving\ncomputational complexity without sacrificing temporal modeling capability. Our\nproposed model achieves high accuracy with a pixel error of 2.45 on the 3ET+\ndataset, using only 17.6K parameters, with up to 1250 Hz event frame rate. To\nfurther enhance hardware efficiency, we employ custom linear approximations of\nactivation functions (hardsigmoid and hardtanh) and fixed-point quantization.\nThrough software-hardware co-design, our 12-nm ASIC implementation operates at\n400 MHz, delivering an end-to-end latency of 0.5 ms (equivalent to 2000 Frames\nPer Second (FPS)) at an energy efficiency of 18.9 $\\mu$J/frame. JaneEye sets a\nnew benchmark in low-power, high-performance eye-tracking solutions suitable\nfor integration into next-generation XR wearables.", "AI": {"tldr": "JaneEye is an energy-efficient event-based eye-tracking hardware accelerator for XR wearables that uses a lightweight neural network with novel ConvJANET layer, achieving high accuracy with minimal parameters and low latency.", "motivation": "Conventional frame-based eye-tracking systems fail to meet XR requirements for high accuracy, low latency, and energy efficiency. Event cameras offer ultra-high temporal resolution and low power consumption as an alternative.", "method": "Developed an ultra-lightweight neural network with novel ConvJANET layer that simplifies ConvLSTM by keeping only forget gate, reducing computational complexity by half. Used custom linear approximations of activation functions and fixed-point quantization for hardware efficiency.", "result": "Achieved pixel error of 2.45 on 3ET+ dataset with only 17.6K parameters, up to 1250 Hz event frame rate. ASIC implementation operates at 400 MHz with 0.5 ms latency (2000 FPS) and 18.9 \u03bcJ/frame energy efficiency.", "conclusion": "JaneEye sets a new benchmark for low-power, high-performance eye-tracking solutions suitable for next-generation XR wearables through software-hardware co-design."}}
{"id": "2510.01339", "categories": ["cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01339", "abs": "https://arxiv.org/abs/2510.01339", "authors": ["Alessio Spagnoletti", "Andr\u00e9s Almansa", "Marcelo Pereyra"], "title": "LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration", "comment": "23 pages, 12 figures", "summary": "Computational imaging methods increasingly rely on powerful generative\ndiffusion models to tackle challenging image restoration tasks. In particular,\nstate-of-the-art zero-shot image inverse solvers leverage distilled\ntext-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy\nand perceptual quality with high computational efficiency. However, extending\nthese advances to high-definition video restoration remains a significant\nchallenge, due to the need to recover fine spatial detail while capturing\nsubtle temporal dependencies. Consequently, methods that naively apply\nimage-based LDM priors on a frame-by-frame basis often result in temporally\ninconsistent reconstructions. We address this challenge by leveraging recent\nadvances in Video Consistency Models (VCMs), which distill video latent\ndiffusion models into fast generators that explicitly capture temporal\ncausality. Building on this foundation, we propose LVTINO, the first zero-shot\nor plug-and-play inverse solver for high definition video restoration with\npriors encoded by VCMs. Our conditioning mechanism bypasses the need for\nautomatic differentiation and achieves state-of-the-art video reconstruction\nquality with only a few neural function evaluations, while ensuring strong\nmeasurement consistency and smooth temporal transitions across frames.\nExtensive experiments on a diverse set of video inverse problems show\nsignificant perceptual improvements over current state-of-the-art methods that\napply image LDMs frame by frame, establishing a new benchmark in both\nreconstruction fidelity and computational efficiency.", "AI": {"tldr": "LVTINO is a zero-shot video restoration method that uses Video Consistency Models (VCMs) instead of frame-by-frame image diffusion models to achieve temporally consistent reconstructions with high computational efficiency.", "motivation": "Existing methods that apply image-based latent diffusion models frame-by-frame for video restoration result in temporally inconsistent reconstructions, failing to capture subtle temporal dependencies in high-definition video.", "method": "Leverages Video Consistency Models (VCMs) that distill video latent diffusion models into fast generators capturing temporal causality. Uses a conditioning mechanism that bypasses automatic differentiation and requires only a few neural function evaluations.", "result": "Achieves state-of-the-art video reconstruction quality with strong measurement consistency and smooth temporal transitions. Shows significant perceptual improvements over frame-by-frame image LDM methods across diverse video inverse problems.", "conclusion": "LVTINO establishes a new benchmark in both reconstruction fidelity and computational efficiency for zero-shot video restoration, demonstrating the superiority of VCM-based priors over image-based approaches."}}
{"id": "2510.02020", "categories": ["cs.IT", "math.IT", "11T"], "pdf": "https://arxiv.org/pdf/2510.02020", "abs": "https://arxiv.org/abs/2510.02020", "authors": ["Run Zheng", "Nung-Sing Sze", "Zejun Huang"], "title": "The dimension and Bose distance of some BCH codes of length $\\frac{q^{m}-1}\u03bb$", "comment": null, "summary": "BCH codes are important error correction codes, widely utilized due to their\nrobust algebraic structure, multi-error correcting capability, and efficient\ndecoding algorithms. Despite their practical importance and extensive study,\ntheir parameters, including dimension, minimum distance and Bose distance,\nremain largely unknown in general. This paper addresses this challenge by\ninvestigating the dimension and Bose distance of BCH codes of length $(q^m -\n1)/\\lambda$ over the finite field $\\mathbb{F}_q$, where $\\lambda$ is a positive\ndivisor of $q - 1$. Specifically, for narrow-sense BCH codes of this length\nwith $m \\geq 4$, we derive explicit formulas for their dimension for designed\ndistance $2 \\leq \\delta \\leq (q^{\\lfloor (2m - 1)/3 \\rfloor + 1} - 1)/{\\lambda}\n+ 1$. We also provide explicit formulas for their Bose distance in the range $2\n\\leq \\delta \\leq (q^{\\lfloor (2m - 1)/3 \\rfloor + 1} - 1)/{\\lambda}$. These\nranges for $\\delta$ are notably larger than the previously known results for\nthis class of BCH codes. Furthermore, we extend these findings to determine the\ndimension and Bose distance for certain non-narrow-sense BCH codes of the same\nlength. Applying our results, we identify several BCH codes with good\nparameters.", "AI": {"tldr": "This paper provides explicit formulas for the dimension and Bose distance of narrow-sense BCH codes of length (q^m - 1)/\u03bb over finite fields, with significantly extended ranges for the designed distance \u03b4 compared to previous results.", "motivation": "BCH codes are widely used error correction codes, but their key parameters (dimension, minimum distance, Bose distance) remain largely unknown in general, creating a fundamental challenge in coding theory.", "method": "The authors investigate BCH codes of length (q^m - 1)/\u03bb over F_q, where \u03bb divides q-1. They derive explicit formulas for dimension and Bose distance for narrow-sense codes with m\u22654, covering designed distances \u03b4 up to (q^\u230a(2m-1)/3\u230b+1 - 1)/\u03bb + 1, and extend these results to certain non-narrow-sense codes.", "result": "The paper obtains explicit formulas for dimension and Bose distance with significantly larger ranges for \u03b4 than previously known. Several BCH codes with good parameters are identified through application of these results.", "conclusion": "The research provides substantial progress in understanding BCH code parameters, offering explicit formulas for dimension and Bose distance over extended ranges, and identifies practical codes with good parameters."}}
{"id": "2510.01666", "categories": ["eess.IV", "cs.CV", "q-bio.QM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01666", "abs": "https://arxiv.org/abs/2510.01666", "authors": ["Jianxu Wang", "Ge Wang"], "title": "Median2Median: Zero-shot Suppression of Structured Noise in Images", "comment": "13 pages, 6 figures, not published yet", "summary": "Image denoising is a fundamental problem in computer vision and medical\nimaging. However, real-world images are often degraded by structured noise with\nstrong anisotropic correlations that existing methods struggle to remove. Most\ndata-driven approaches rely on large datasets with high-quality labels and\nstill suffer from limited generalizability, whereas existing zero-shot methods\navoid this limitation but remain effective only for independent and identically\ndistributed (i.i.d.) noise. To address this gap, we propose Median2Median\n(M2M), a zero-shot denoising framework designed for structured noise. M2M\nintroduces a novel sampling strategy that generates pseudo-independent\nsub-image pairs from a single noisy input. This strategy leverages directional\ninterpolation and generalized median filtering to adaptively exclude values\ndistorted by structured artifacts. To further enlarge the effective sampling\nspace and eliminate systematic bias, a randomized assignment strategy is\nemployed, ensuring that the sampled sub-image pairs are suitable for\nNoise2Noise training. In our realistic simulation studies, M2M performs on par\nwith state-of-the-art zero-shot methods under i.i.d. noise, while consistently\noutperforming them under correlated noise. These findings establish M2M as an\nefficient, data-free solution for structured noise suppression and mark the\nfirst step toward effective zero-shot denoising beyond the strict i.i.d.\nassumption.", "AI": {"tldr": "M2M is a zero-shot denoising framework that addresses structured noise using pseudo-independent sub-image pairs generated from a single noisy input, outperforming existing methods on correlated noise while matching performance on i.i.d. noise.", "motivation": "Real-world images often contain structured noise with anisotropic correlations that existing denoising methods struggle with. Data-driven approaches require large labeled datasets and have limited generalizability, while zero-shot methods only work well for i.i.d. noise.", "method": "M2M uses directional interpolation and generalized median filtering to generate pseudo-independent sub-image pairs from a single noisy input. It employs a randomized assignment strategy to eliminate systematic bias and enable Noise2Noise training without clean data.", "result": "In realistic simulations, M2M performs on par with state-of-the-art zero-shot methods under i.i.d. noise and consistently outperforms them under correlated noise conditions.", "conclusion": "M2M provides an efficient, data-free solution for structured noise suppression and represents the first effective zero-shot denoising approach that works beyond the strict i.i.d. assumption."}}
{"id": "2510.01272", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01272", "abs": "https://arxiv.org/abs/2510.01272", "authors": ["Kunal Jha", "Aydan Yuenan Huang", "Eric Ye", "Natasha Jaques", "Max Kleiman-Weiner"], "title": "Modeling Others' Minds as Code", "comment": null, "summary": "Accurate prediction of human behavior is essential for robust and safe\nhuman-AI collaboration. However, existing approaches for modeling people are\noften data-hungry and brittle because they either make unrealistic assumptions\nabout rationality or are too computationally demanding to adapt rapidly. Our\nkey insight is that many everyday social interactions may follow predictable\npatterns; efficient \"scripts\" that minimize cognitive load for actors and\nobservers, e.g., \"wait for the green light, then go.\" We propose modeling these\nroutines as behavioral programs instantiated in computer code rather than\npolicies conditioned on beliefs and desires. We introduce ROTE, a novel\nalgorithm that leverages both large language models (LLMs) for synthesizing a\nhypothesis space of behavioral programs, and probabilistic inference for\nreasoning about uncertainty over that space. We test ROTE in a suite of\ngridworld tasks and a large-scale embodied household simulator. ROTE predicts\nhuman and AI behaviors from sparse observations, outperforming competitive\nbaselines -- including behavior cloning and LLM-based methods -- by as much as\n50% in terms of in-sample accuracy and out-of-sample generalization. By\ntreating action understanding as a program synthesis problem, ROTE opens a path\nfor AI systems to efficiently and effectively predict human behavior in the\nreal-world.", "AI": {"tldr": "ROTE is a novel algorithm that models human behavior as behavioral programs synthesized by LLMs and uses probabilistic inference to predict actions, achieving 50% better accuracy than baselines in gridworld and household simulation tasks.", "motivation": "Existing human behavior modeling approaches are data-hungry and brittle due to unrealistic rationality assumptions or computational demands. Many social interactions follow predictable patterns (scripts) that minimize cognitive load.", "method": "ROTE treats action understanding as program synthesis, using LLMs to generate behavioral programs and probabilistic inference to reason about uncertainty over the hypothesis space of programs.", "result": "ROTE outperforms competitive baselines (behavior cloning and LLM-based methods) by up to 50% in both in-sample accuracy and out-of-sample generalization across gridworld tasks and embodied household simulations.", "conclusion": "By modeling routines as behavioral programs, ROTE enables AI systems to efficiently and effectively predict human behavior in real-world scenarios, opening new paths for robust human-AI collaboration."}}
{"id": "2510.01218", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01218", "abs": "https://arxiv.org/abs/2510.01218", "authors": ["Sergey Troshin", "Wafaa Mohammed", "Yan Meng", "Christof Monz", "Antske Fokkens", "Vlad Niculae"], "title": "Control the Temperature: Selective Sampling for Diverse and High-Quality LLM Outputs", "comment": "Second Conference on Language Modeling, 2025", "summary": "Diversity is an essential metric for evaluating the creativity of outputs\ngenerated by language models. Temperature-based sampling is a common strategy\nto increase diversity. However, for tasks that require high precision, e.g.,\nmathematical reasoning, uncontrolled high temperature sampling, e.g., min-$p$\nor top-$p$, degrades reasoning quality. We demonstrate that the loss of\naccuracy is caused by sampling incorrect continuations in sensitive decoding\npositions. To address this, in this paper, we propose \\textbf{selective\nsampling}, a method that dynamically switches between greedy and\nhigh-temperature sampling based on a sampling risk metric. This risk metric\nestimates the likelihood of output errors when applying high-temperature\nsampling on the current token position. To predict sampling risk, we train a\nlightweight classifier on a small subset of verifiable problems. The trained\nclassifier can be integrated with the base language model with minimal latency\noverhead. Experiments on mathematical reasoning tasks demonstrate that\nselective sampling enhances the quality-diversity trade-off, even in\nhigh-temperature settings.", "AI": {"tldr": "Selective sampling dynamically switches between greedy and high-temperature sampling based on sampling risk to improve diversity while maintaining accuracy in mathematical reasoning tasks.", "motivation": "Temperature-based sampling increases diversity but degrades reasoning quality in precision-required tasks like mathematical reasoning, due to sampling incorrect continuations in sensitive positions.", "method": "Proposes selective sampling with a sampling risk metric that estimates error likelihood. Uses a lightweight classifier trained on verifiable problems to predict risk and switch between greedy and high-temperature sampling.", "result": "Experiments show selective sampling enhances quality-diversity trade-off in mathematical reasoning tasks, even with high-temperature settings.", "conclusion": "Selective sampling effectively addresses the accuracy-diversity trade-off in language models for precision tasks by dynamically controlling sampling based on risk prediction."}}
{"id": "2510.01408", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.01408", "abs": "https://arxiv.org/abs/2510.01408", "authors": ["Jeong Min Kong", "Ian P. Roberts"], "title": "Satellite Assignment Policy Learning for Coexistence in LEO Networks", "comment": null, "summary": "Unlike in terrestrial cellular networks, certain frequency bands for\nlow-earth orbit (LEO) satellite systems have thus far been allocated on a\nnon-exclusive basis. In this context, systems that launch their satellites\nearlier (referred to as primary systems) are given spectrum access priority\nover those that launch later, known as secondary systems. For a secondary\nsystem to function, it is expected to either coordinate with primary systems or\nensure that it does not cause excessive interference to primary ground users.\nReliably meeting this interference constraint requires real-time knowledge of\nthe receive beams of primary users, which in turn depends on the primary\nsatellite-to-primary user associations. However, in practice, primary systems\nhave thus far not publicly disclosed their satellite assignment policies;\ntherefore, it becomes essential for secondary systems to develop methods to\ninfer such policies. Assuming there is limited historical data indicating which\nprimary satellites have served which primary users, we propose an end-to-end\ngraph structure learning-based algorithm for learning highest elevation primary\nsatellite assignment policies, that, upon deployment, can directly map the\nprimary satellite coordinates into assignment decisions for the primary users.\nSimulation results show that our method can outperform the best baseline,\nachieving approximately a 15% improvement in prediction accuracy.", "AI": {"tldr": "The paper proposes a graph structure learning-based algorithm to infer primary satellite assignment policies in LEO satellite systems, achieving 15% better prediction accuracy than baselines.", "motivation": "Secondary LEO satellite systems need to coordinate with primary systems to avoid interference, but primary systems don't disclose their satellite assignment policies publicly, making inference necessary.", "method": "An end-to-end graph structure learning-based algorithm that uses limited historical data to learn highest elevation primary satellite assignment policies and map satellite coordinates to assignment decisions.", "result": "The method outperforms the best baseline, achieving approximately 15% improvement in prediction accuracy.", "conclusion": "The proposed graph learning approach effectively infers primary satellite assignment policies, enabling secondary systems to operate without causing excessive interference to primary users."}}
{"id": "2510.01347", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01347", "abs": "https://arxiv.org/abs/2510.01347", "authors": ["Shuochen Chang"], "title": "Image Generation Based on Image Style Extraction", "comment": null, "summary": "Image generation based on text-to-image generation models is a task with\npractical application scenarios that fine-grained styles cannot be precisely\ndescribed and controlled in natural language, while the guidance information of\nstylized reference images is difficult to be directly aligned with the textual\nconditions of traditional textual guidance generation. This study focuses on\nhow to maximize the generative capability of the pretrained generative model,\nby obtaining fine-grained stylistic representations from a single given\nstylistic reference image, and injecting the stylistic representations into the\ngenerative body without changing the structural framework of the downstream\ngenerative model, so as to achieve fine-grained controlled stylized image\ngeneration. In this study, we propose a three-stage training style\nextraction-based image generation method, which uses a style encoder and a\nstyle projection layer to align the style representations with the textual\nrepresentations to realize fine-grained textual cue-based style guide\ngeneration. In addition, this study constructs the Style30k-captions dataset,\nwhose samples contain a triad of images, style labels, and text descriptions,\nto train the style encoder and style projection layer in this experiment.", "AI": {"tldr": "A method for fine-grained style-controlled image generation that extracts style representations from reference images and aligns them with text conditions without modifying the base generative model.", "motivation": "Text-to-image models struggle with precise fine-grained style control using natural language, and stylized reference images are difficult to align with textual conditions in traditional generation approaches.", "method": "Three-stage training method using a style encoder and style projection layer to extract style representations from reference images and align them with text representations. Uses a custom Style30k-captions dataset containing image-style label-text description triads.", "result": "Enables fine-grained controlled stylized image generation by injecting extracted style representations into the generative model while maintaining its structural framework.", "conclusion": "The proposed approach successfully achieves fine-grained style control in image generation by extracting and aligning style representations with text conditions, maximizing the capabilities of pretrained generative models."}}
{"id": "2510.02022", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.02022", "abs": "https://arxiv.org/abs/2510.02022", "authors": ["Masoud Ghazikor", "Van Ly Nguyen", "Morteza Hashemi"], "title": "Performance Analysis of RIS-Assisted UAV Communication in NOMA Networks", "comment": "IEEE Consumer Communications & Networking Conference (CCNC) 2026", "summary": "This paper investigates the performance of downlink non-orthogonal multiple\naccess (NOMA) communication in unmanned aerial vehicle (UAV) networks enhanced\nby partitionable reconfigurable intelligent surfaces (RISs). We analyze three\ntypes of links between base station (BS) and UAVs: direct, RIS-only indirect,\nand composite links, under both Line-of-Sight (LoS) and Non-LoS (NLoS)\npropagation. The RIS-only indirect link and direct link are modeled using\ndouble Nakagami-m and Nakagami-m fading, respectively, while the composite link\nfollows a combined fading channel model. Closed-form expressions for the\ncumulative distribution function (CDF) of the received signal-to-noise ratio\n(SNR) are derived for all links, enabling tractable outage probability\nanalysis. Then, we formulate a fairness-efficiency bilevel optimization problem\nto minimize the maximum outage probability among UAVs while minimizing the\ntotal number of required RIS reflecting elements. Accordingly, an RIS-assisted\nUAV Outage Minimization (RUOM) algorithm is proposed, which fairly allocates\nthe NOMA power coefficients while minimizing the total number of RIS reflecting\nelements required, subject to NOMA-defined constraints, RIS resource\nlimitations, and maximum allowable outage threshold. Simulation results\nvalidate the analytical models and demonstrate that the proposed RUOM algorithm\nsignificantly improves fairness and efficiency in BS-UAV communication.", "AI": {"tldr": "This paper analyzes downlink NOMA communication in UAV networks enhanced by partitionable RISs, deriving closed-form SNR expressions and proposing an optimization algorithm to minimize outage probability while reducing RIS element requirements.", "motivation": "To improve fairness and efficiency in UAV communication networks by leveraging partitionable RIS technology to enhance signal quality and reduce outage probability in challenging propagation environments.", "method": "Modeled three link types (direct, RIS-only indirect, composite) under LoS/NLoS conditions using Nakagami-m fading models. Derived closed-form CDF expressions for SNR and formulated a bilevel optimization problem solved by the proposed RUOM algorithm.", "result": "Simulation results validate analytical models and show the RUOM algorithm significantly improves fairness and efficiency in BS-UAV communication while minimizing required RIS reflecting elements.", "conclusion": "The proposed RIS-assisted approach with the RUOM algorithm effectively enhances UAV communication performance by optimizing power allocation and RIS resource utilization, achieving better fairness and reduced outage probability."}}
{"id": "2510.01919", "categories": ["eess.IV", "cs.CV", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2510.01919", "abs": "https://arxiv.org/abs/2510.01919", "authors": ["Jhonatan Contreras", "Thomas Bocklitz"], "title": "GFSR-Net: Guided Focus via Segment-Wise Relevance Network for Interpretable Deep Learning in Medical Imaging", "comment": null, "summary": "Deep learning has achieved remarkable success in medical image analysis,\nhowever its adoption in clinical practice is limited by a lack of\ninterpretability. These models often make correct predictions without\nexplaining their reasoning. They may also rely on image regions unrelated to\nthe disease or visual cues, such as annotations, that are not present in\nreal-world conditions. This can reduce trust and increase the risk of\nmisleading diagnoses. We introduce the Guided Focus via Segment-Wise Relevance\nNetwork (GFSR-Net), an approach designed to improve interpretability and\nreliability in medical imaging. GFSR-Net uses a small number of human\nannotations to approximate where a person would focus within an image\nintuitively, without requiring precise boundaries or exhaustive markings,\nmaking the process fast and practical. During training, the model learns to\nalign its focus with these areas, progressively emphasizing features that carry\ndiagnostic meaning. This guidance works across different types of natural and\nmedical images, including chest X-rays, retinal scans, and dermatological\nimages. Our experiments demonstrate that GFSR achieves comparable or superior\naccuracy while producing saliency maps that better reflect human expectations.\nThis reduces the reliance on irrelevant patterns and increases confidence in\nautomated diagnostic tools.", "AI": {"tldr": "GFSR-Net improves interpretability in medical image analysis by using human annotations to guide model focus toward diagnostically relevant regions, achieving comparable accuracy with more reliable saliency maps.", "motivation": "Deep learning models lack interpretability in medical imaging, potentially relying on irrelevant patterns or visual cues, which reduces trust and increases diagnostic risks.", "method": "GFSR-Net uses a small number of human annotations to approximate intuitive focus areas without precise boundaries, training the model to align its focus with diagnostically meaningful regions across various medical images.", "result": "Experiments show GFSR-Net achieves comparable or superior accuracy while producing saliency maps that better reflect human expectations, reducing reliance on irrelevant patterns.", "conclusion": "The approach increases confidence in automated diagnostic tools by improving interpretability and reliability across different medical imaging modalities."}}
{"id": "2510.01293", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01293", "abs": "https://arxiv.org/abs/2510.01293", "authors": ["Zekun Jiang", "Chunming Xu", "Tianhang Zhou"], "title": "Cyber Academia-Chemical Engineering (CA-ChemE): A Living Digital Town for Self-Directed Research Evolution and Emergent Scientific Discovery", "comment": null, "summary": "The rapid advancement of artificial intelligence (AI) has demonstrated\nsubstantial potential in chemical engineering, yet existing AI systems remain\nlimited in interdisciplinary collaboration and exploration of uncharted\nproblems. To address these issues, we present the Cyber Academia-Chemical\nEngineering (CA-ChemE) system, a living digital town that enables self-directed\nresearch evolution and emergent scientific discovery through multi-agent\ncollaboration. By integrating domain-specific knowledge bases, knowledge\nenhancement technologies, and collaboration agents, the system successfully\nconstructs an intelligent ecosystem capable of deep professional reasoning and\nefficient interdisciplinary collaboration. Our findings demonstrate that\nknowledge base-enabled enhancement mechanisms improved dialogue quality scores\nby 10-15% on average across all seven expert agents, fundamentally ensuring\ntechnical judgments are grounded in verifiable scientific evidence. However, we\nobserved a critical bottleneck in cross-domain collaboration efficiency,\nprompting the introduction of a Collaboration Agent (CA) equipped with ontology\nengineering capabilities. CA's intervention achieved 8.5% improvements for\ndistant-domain expert pairs compared to only 0.8% for domain-proximate pairs -\na 10.6-fold difference - unveiling the \"diminished collaborative efficiency\ncaused by knowledge-base gaps\" effect. This study demonstrates how carefully\ndesigned multi-agent architectures can provide a viable pathway toward\nautonomous scientific discovery in chemical engineering.", "AI": {"tldr": "The CA-ChemE system is a multi-agent AI platform that enables autonomous scientific discovery in chemical engineering through knowledge-enhanced agents and collaboration mechanisms, addressing interdisciplinary collaboration bottlenecks.", "motivation": "To overcome limitations in existing AI systems for interdisciplinary collaboration and exploration of uncharted problems in chemical engineering.", "method": "Developed a living digital town with multi-agent collaboration, integrating domain-specific knowledge bases, knowledge enhancement technologies, and collaboration agents with ontology engineering capabilities.", "result": "Knowledge base enhancement improved dialogue quality by 10-15%, while collaboration agent intervention achieved 8.5% improvement for distant-domain expert pairs versus only 0.8% for domain-proximate pairs, revealing a 10.6-fold difference in collaborative efficiency.", "conclusion": "Carefully designed multi-agent architectures provide a viable pathway toward autonomous scientific discovery in chemical engineering by addressing knowledge-base gaps and enabling effective interdisciplinary collaboration."}}
{"id": "2510.01235", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01235", "abs": "https://arxiv.org/abs/2510.01235", "authors": ["Subham Ghosh", "Abhishek Tewari"], "title": "Automated Extraction of Material Properties using LLM-based AI Agents", "comment": null, "summary": "The rapid discovery of materials is constrained by the lack of large,\nmachine-readable datasets that couple performance metrics with structural\ncontext. Existing databases are either small, manually curated, or biased\ntoward first principles results, leaving experimental literature\nunderexploited. We present an agentic, large language model (LLM)-driven\nworkflow that autonomously extracts thermoelectric and structural-properties\nfrom about 10,000 full-text scientific articles. The pipeline integrates\ndynamic token allocation, zeroshot multi-agent extraction, and conditional\ntable parsing to balance accuracy against computational cost. Benchmarking on\n50 curated papers shows that GPT-4.1 achieves the highest accuracy (F1 = 0.91\nfor thermoelectric properties and 0.82 for structural fields), while GPT-4.1\nMini delivers nearly comparable performance (F1 = 0.89 and 0.81) at a fraction\nof the cost, enabling practical large scale deployment. Applying this workflow,\nwe curated 27,822 temperature resolved property records with normalized units,\nspanning figure of merit (ZT), Seebeck coefficient, conductivity, resistivity,\npower factor, and thermal conductivity, together with structural attributes\nsuch as crystal class, space group, and doping strategy. Dataset analysis\nreproduces known thermoelectric trends, such as the superior performance of\nalloys over oxides and the advantage of p-type doping, while also surfacing\nbroader structure-property correlations. To facilitate community access, we\nrelease an interactive web explorer with semantic filters, numeric queries, and\nCSV export. This study delivers the largest LLM-curated thermoelectric dataset\nto date, provides a reproducible and cost-profiled extraction pipeline, and\nestablishes a foundation for scalable, data-driven materials discovery beyond\nthermoelectrics.", "AI": {"tldr": "An LLM-driven workflow autonomously extracts thermoelectric and structural properties from 10,000 scientific articles, creating the largest LLM-curated thermoelectric dataset with 27,822 records and enabling scalable materials discovery.", "motivation": "Materials discovery is constrained by lack of large, machine-readable datasets that couple performance metrics with structural context, with experimental literature being underexploited.", "method": "Agentic LLM-driven workflow with dynamic token allocation, zero-shot multi-agent extraction, and conditional table parsing to balance accuracy and computational cost.", "result": "Created 27,822 temperature-resolved property records with normalized units and structural attributes. GPT-4.1 achieved highest accuracy (F1=0.91 for thermoelectric, 0.82 for structural), while GPT-4.1 Mini delivered comparable performance at lower cost.", "conclusion": "The study delivers the largest LLM-curated thermoelectric dataset, provides reproducible extraction pipeline, and establishes foundation for scalable, data-driven materials discovery beyond thermoelectrics."}}
{"id": "2510.01411", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.01411", "abs": "https://arxiv.org/abs/2510.01411", "authors": ["Hibatallah Alwazani", "Omran Abbas", "Loic Markley", "Anas Chaaban"], "title": "Delay-Augmented Stacked Intelligent Surfaces: Potential, Challenges, and Opportunities", "comment": "7 pages, 3 figures", "summary": "Stacked intelligent surfaces (SIS)s have been proposed recently as an\nenabling technology for Holographic Multiple Input Multiple Output (HMIMO) and\nUltra-massive MIMO (umMIMO) technologies. Their utility can extend beyond\nspatial wave-domain processing of signals if they are enhanced with\nstrategically-tuned symbol-duration level delays to enable temporal processing\nas well. In this work, we introduce the idea of a delay-augmented SIS (DA-SIS).\nWe shed light on the feasibility of realizing delay units in an SIS. Then, we\ndiscuss the relevance of the proposed DA-SIS and present a use case that\nillustrates its potential, wherein the DA-SIS serves as an analog equalizer\nthat aids in eliminating multi-path-induced inter-symbol-interference (ISI). We\nshow how the number of elements affect the equalization process using the bit\nerror rate (BER) as a metric, and demonstrate the potential of the DA-SIS in\nequalization via comparing with digital equalizers as a benchmark. Finally, we\npresent opportunities and future research directions that can be undertaken to\nbring this idea to fruition.", "AI": {"tldr": "Introduces delay-augmented stacked intelligent surfaces (DA-SIS) that combine spatial wave-domain processing with temporal processing through strategically-tuned delays, enabling applications like analog equalization to eliminate inter-symbol interference.", "motivation": "To extend the utility of stacked intelligent surfaces beyond spatial processing by incorporating temporal processing capabilities through delay units, enabling more comprehensive signal processing for Holographic MIMO and Ultra-massive MIMO systems.", "method": "Proposes DA-SIS with strategically-tuned symbol-duration level delays, discusses feasibility of delay units in SIS, and demonstrates application as analog equalizer for ISI elimination. Compares performance with digital equalizers using bit error rate metric.", "result": "Demonstrates the potential of DA-SIS in equalization, showing how the number of elements affects the equalization process and comparing favorably with digital equalizers as benchmark.", "conclusion": "DA-SIS represents a promising technology that combines spatial and temporal processing, with opportunities for future research to bring this concept to practical implementation."}}
{"id": "2510.01362", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01362", "abs": "https://arxiv.org/abs/2510.01362", "authors": ["Shijia Feng", "Michael Wray", "Walterio Mayol-Cuevas"], "title": "EvoStruggle: A Dataset Capturing the Evolution of Struggle across Activities and Skill Levels", "comment": "10 pages", "summary": "The ability to determine when a person struggles during skill acquisition is\ncrucial for both optimizing human learning and enabling the development of\neffective assistive systems. As skills develop, the type and frequency of\nstruggles tend to change, and understanding this evolution is key to\ndetermining the user's current stage of learning. However, existing\nmanipulation datasets have not focused on how struggle evolves over time. In\nthis work, we collect a dataset for struggle determination, featuring 61.68\nhours of video recordings, 2,793 videos, and 5,385 annotated temporal struggle\nsegments collected from 76 participants. The dataset includes 18 tasks grouped\ninto four diverse activities -- tying knots, origami, tangram puzzles, and\nshuffling cards, representing different task variations. In addition,\nparticipants repeated the same task five times to capture their evolution of\nskill. We define the struggle determination problem as a temporal action\nlocalization task, focusing on identifying and precisely localizing struggle\nsegments with start and end times. Experimental results show that Temporal\nAction Localization models can successfully learn to detect struggle cues, even\nwhen evaluated on unseen tasks or activities. The models attain an overall\naverage mAP of 34.56% when generalizing across tasks and 19.24% across\nactivities, indicating that struggle is a transferable concept across various\nskill-based tasks while still posing challenges for further improvement in\nstruggle detection. Our dataset is available at\nhttps://github.com/FELIXFENG2019/EvoStruggle.", "AI": {"tldr": "This paper introduces EvoStruggle, a dataset for temporal struggle determination during skill acquisition, featuring 61.68 hours of video recordings from 76 participants performing 18 tasks across 4 activities, with 5 repetitions to capture skill evolution.", "motivation": "Existing manipulation datasets haven't focused on how struggle evolves over time during skill acquisition, which is crucial for understanding learning stages and developing effective assistive systems.", "method": "Collected a large dataset with 2,793 videos and 5,385 annotated temporal struggle segments from 76 participants performing 18 tasks across 4 activities (tying knots, origami, tangram puzzles, shuffling cards), with 5 repetitions per task. Defined struggle determination as a temporal action localization task.", "result": "Temporal Action Localization models successfully learned to detect struggle cues, achieving 34.56% mAP when generalizing across tasks and 19.24% mAP across activities, showing struggle is a transferable concept across skill-based tasks.", "conclusion": "Struggle is a transferable concept across various skill-based tasks, and temporal action localization models can effectively detect struggle, though there's room for improvement in cross-activity generalization."}}
{"id": "2510.02048", "categories": ["cs.IT", "cs.LG", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.02048", "abs": "https://arxiv.org/abs/2510.02048", "authors": ["Xinyang Li", "Vlad C. Andrei", "Peter J. Gu", "Yiqi Chen", "Ullrich J. M\u00f6nich", "Holger Boche"], "title": "Variational Secret Common Randomness Extraction", "comment": null, "summary": "This paper studies the problem of extracting common randomness (CR) or secret\nkeys from correlated random sources observed by two legitimate parties, Alice\nand Bob, through public discussion in the presence of an eavesdropper, Eve. We\npropose a practical two-stage CR extraction framework. In the first stage, the\nvariational probabilistic quantization (VPQ) step is introduced, where Alice\nand Bob employ probabilistic neural network (NN) encoders to map their\nobservations into discrete, nearly uniform random variables (RVs) with high\nagreement probability while minimizing information leakage to Eve. This is\nrealized through a variational learning objective combined with adversarial\ntraining. In the second stage, a secure sketch using code-offset construction\nreconciles the encoder outputs into identical secret keys, whose secrecy is\nguaranteed by the VPQ objective. As a representative application, we study\nphysical layer key (PLK) generation. Beyond the traditional methods, which rely\non the channel reciprocity principle and require two-way channel probing, thus\nsuffering from large protocol overhead and being unsuitable in high mobility\nscenarios, we propose a sensing-based PLK generation method for integrated\nsensing and communications (ISAC) systems, where paired range-angle (RA) maps\nmeasured at Alice and Bob serve as correlated sources. The idea is verified\nthrough both end-to-end simulations and real-world software-defined radio (SDR)\nmeasurements, including scenarios where Eve has partial knowledge about Bob's\nposition. The results demonstrate the feasibility and convincing performance of\nboth the proposed CR extraction framework and sensing-based PLK generation\nmethod.", "AI": {"tldr": "A practical two-stage framework for extracting common randomness/secret keys from correlated sources using neural network encoders and secure sketches, applied to sensing-based physical layer key generation in ISAC systems.", "motivation": "To develop efficient common randomness extraction methods that minimize information leakage to eavesdroppers, addressing limitations of traditional physical layer key generation methods that suffer from large protocol overhead and poor performance in high mobility scenarios.", "method": "Two-stage framework: 1) Variational probabilistic quantization using neural network encoders to map observations into discrete uniform random variables with high agreement probability, 2) Secure sketch using code-offset construction to reconcile encoder outputs into identical secret keys. Applied to sensing-based PLK generation using paired range-angle maps in ISAC systems.", "result": "Verified through end-to-end simulations and real-world SDR measurements, demonstrating feasibility and convincing performance even when Eve has partial knowledge about Bob's position.", "conclusion": "The proposed CR extraction framework and sensing-based PLK generation method are effective for secure key generation in integrated sensing and communications systems, overcoming limitations of traditional approaches."}}
{"id": "2510.02063", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2510.02063", "abs": "https://arxiv.org/abs/2510.02063", "authors": ["Jinwei Zhang", "Lianrui Zuo", "Yihao Liu", "Hang Zhang", "Samuel W. Remedios", "Bennett A. Landman", "Peter A. Calabresi", "Shiv Saidha", "Scott D. Newsome", "Dzung L. Pham", "Jerry L. Prince", "Ellen M. Mowry", "Aaron Carass"], "title": "MSRepaint: Multiple Sclerosis Repaint with Conditional Denoising Diffusion Implicit Model for Bidirectional Lesion Filling and Synthesis", "comment": null, "summary": "In multiple sclerosis, lesions interfere with automated magnetic resonance\nimaging analyses such as brain parcellation and deformable registration, while\nlesion segmentation models are hindered by the limited availability of\nannotated training data. To address both issues, we propose MSRepaint, a\nunified diffusion-based generative model for bidirectional lesion filling and\nsynthesis that restores anatomical continuity for downstream analyses and\naugments segmentation through realistic data generation. MSRepaint conditions\non spatial lesion masks for voxel-level control, incorporates contrast dropout\nto handle missing inputs, integrates a repainting mechanism to preserve\nsurrounding anatomy during lesion filling and synthesis, and employs a\nmulti-view DDIM inversion and fusion pipeline for 3D consistency with fast\ninference. Extensive evaluations demonstrate the effectiveness of MSRepaint\nacross multiple tasks. For lesion filling, we evaluate both the accuracy within\nthe filled regions and the impact on downstream tasks including brain\nparcellation and deformable registration. MSRepaint outperforms the traditional\nlesion filling methods FSL and NiftySeg, and achieves accuracy on par with\nFastSurfer-LIT, a recent diffusion model-based inpainting method, while\noffering over 20 times faster inference. For lesion synthesis, state-of-the-art\nMS lesion segmentation models trained on MSRepaint-synthesized data outperform\nthose trained on CarveMix-synthesized data or real ISBI challenge training data\nacross multiple benchmarks, including the MICCAI 2016 and UMCL datasets.\nAdditionally, we demonstrate that MSRepaint's unified bidirectional filling and\nsynthesis capability, with full spatial control over lesion appearance, enables\nhigh-fidelity simulation of lesion evolution in longitudinal MS progression.", "AI": {"tldr": "MSRepaint is a unified diffusion-based model for bidirectional lesion filling and synthesis in multiple sclerosis MRI, improving downstream analysis and segmentation through anatomical restoration and realistic data generation.", "motivation": "Lesions in multiple sclerosis interfere with automated MRI analyses like brain parcellation and registration, while lesion segmentation models suffer from limited annotated training data availability.", "method": "A diffusion-based generative model that conditions on spatial lesion masks, uses contrast dropout for missing inputs, integrates repainting to preserve surrounding anatomy, and employs multi-view DDIM inversion with fusion for 3D consistency and fast inference.", "result": "MSRepaint outperforms traditional lesion filling methods (FSL, NiftySeg) and matches FastSurfer-LIT accuracy while being 20x faster. For lesion synthesis, models trained on MSRepaint data outperform those trained on CarveMix or real ISBI data across multiple benchmarks. Also enables high-fidelity simulation of lesion evolution.", "conclusion": "MSRepaint provides an effective unified solution for both lesion filling to restore anatomical continuity and lesion synthesis for data augmentation, with significant improvements in speed and performance over existing methods."}}
{"id": "2510.01295", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.01295", "abs": "https://arxiv.org/abs/2510.01295", "authors": ["Zarreen Reza"], "title": "The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop on Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent\n  Abilities, and Scaling", "summary": "As Large Language Models (LLMs) transition from static tools to autonomous\nagents, traditional evaluation benchmarks that measure performance on\ndownstream tasks are becoming insufficient. These methods fail to capture the\nemergent social and cognitive dynamics that arise when agents communicate,\npersuade, and collaborate in interactive environments. To address this gap, we\nintroduce a novel evaluation framework that uses multi-agent debate as a\ncontrolled \"social laboratory\" to discover and quantify these behaviors. In our\nframework, LLM-based agents, instantiated with distinct personas and\nincentives, deliberate on a wide range of challenging topics under the\nsupervision of an LLM moderator. Our analysis, enabled by a new suite of\npsychometric and semantic metrics, reveals several key findings. Across\nhundreds of debates, we uncover a powerful and robust emergent tendency for\nagents to seek consensus, consistently reaching high semantic agreement ({\\mu}\n> 0.88) even without explicit instruction and across sensitive topics. We show\nthat assigned personas induce stable, measurable psychometric profiles,\nparticularly in cognitive effort, and that the moderators persona can\nsignificantly alter debate outcomes by structuring the environment, a key\nfinding for external AI alignment. This work provides a blueprint for a new\nclass of dynamic, psychometrically grounded evaluation protocols designed for\nthe agentic setting, offering a crucial methodology for understanding and\nshaping the social behaviors of the next generation of AI agents. We have\nreleased the code and results at\nhttps://github.com/znreza/multi-agent-LLM-eval-for-debate.", "AI": {"tldr": "A novel evaluation framework using multi-agent debate as a social laboratory to measure emergent social behaviors in LLM-based autonomous agents, revealing strong consensus-seeking tendencies and persona-driven psychometric profiles.", "motivation": "Traditional evaluation benchmarks are insufficient for capturing emergent social and cognitive dynamics in autonomous LLM agents that communicate, persuade, and collaborate in interactive environments.", "method": "Multi-agent debate framework where LLM-based agents with distinct personas and incentives deliberate on challenging topics under LLM moderator supervision, analyzed using psychometric and semantic metrics.", "result": "Agents consistently reach high semantic agreement (\u03bc > 0.88) without explicit instruction, personas induce stable psychometric profiles, and moderator personas significantly alter debate outcomes through environmental structuring.", "conclusion": "Provides a blueprint for dynamic, psychometrically grounded evaluation protocols for agentic AI, offering crucial methodology for understanding and shaping social behaviors of next-generation AI agents."}}
{"id": "2510.01240", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01240", "abs": "https://arxiv.org/abs/2510.01240", "authors": ["Zukang Xu", "Xing Hu", "Qiang Wu", "Dawei Yang"], "title": "RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable performance across\na wide range of natural language processing tasks. However, their exponentially\nincreasing parameters pose significant challenges for deployment on\nresource-constrained devices. Vector Quantization (VQ) shows great promise for\nlow-bit quantization (e.g., 2 to 4 bits), but existing work faces two key\nchallenges: unconstrained direction error and suboptimal bit allocation. In\nthis paper, we propose RSAVQ, a novel VQ framework to enhance extremely low-bit\nquantization for LLMs. RSAVQ introduces two geometry-driven innovations that\neffectively mitigate above limitations: (1) Error Direction Sensitivity\nGuidance (EDSG), which leverages the Fisher Information Matrix (FIM)-induced\nRiemannian metric to project quantization errors onto low-sensitivity\ndirections in the parameter space. Specifically, this projection is performed\nalong the negative natural gradient direction, which effectively suppresses\nerror expansion. (2) Weight Channel Sensitivity Guidance (WCSG) , which\nconstructs a channel-wise sensitivity metric via FIM curvature analysis to\ndynamically guide bit resource allocation. The approach facilitates a globally\noptimal quantization solution within prescribed bit constraints. Experiments\ndemonstrate that RSAVQ outperforms existing methods for LLMs. For example, in\n2-bit quantization of LLaMA-3 8B, RSAVQ leads baselines like VPTQ and QuIP# by\n0.4 in perplexity (PPL) and 1.5 in zero-shot accuracy. This work offers a\npractical solution for constrained environments and a theoretical bridge\nbetween information geometry and the quantization of neural networks, advancing\nefficient deep learning.", "AI": {"tldr": "RSAVQ is a novel vector quantization framework that enhances extremely low-bit quantization for LLMs using geometry-driven innovations to address direction error and bit allocation challenges.", "motivation": "Large language models face deployment challenges on resource-constrained devices due to exponentially increasing parameters. Existing vector quantization methods struggle with unconstrained direction error and suboptimal bit allocation.", "method": "RSAVQ introduces two innovations: (1) Error Direction Sensitivity Guidance (EDSG) using Fisher Information Matrix-induced Riemannian metric to project quantization errors along negative natural gradient direction, and (2) Weight Channel Sensitivity Guidance (WCSG) using FIM curvature analysis for dynamic bit allocation.", "result": "RSAVQ outperforms existing methods, achieving 0.4 lower perplexity and 1.5 higher zero-shot accuracy in 2-bit quantization of LLaMA-3 8B compared to baselines like VPTQ and QuIP#.", "conclusion": "The work provides a practical solution for constrained environments and establishes a theoretical bridge between information geometry and neural network quantization, advancing efficient deep learning."}}
{"id": "2510.01417", "categories": ["eess.SP", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2510.01417", "abs": "https://arxiv.org/abs/2510.01417", "authors": ["Alex Paul Hoffmann", "Matthew G. Finley", "Eftyhia Zesta", "Mark B. Moldwin", "Lauro V. Ojeda"], "title": "A Drone-mounted Magnetometer System for Automatic Interference Removal and Landmine Detection", "comment": "13 pages, 5 figures", "summary": "Landmines have been extensively used in conflict zones as an indiscriminate\nweapon to control military movements, often remaining active long after\nhostilities have ended. Their presence poses a persistent danger to civilians,\nhindering post-war recovery efforts, causing injuries or death, and restricting\naccess to essential land for agriculture and infrastructure. Unmanned aerial\nvehicles (UAV) equipped with magnetometers are commonly used to detect remnant\nhidden landmines but come with significant technical challenges due to magnetic\nfield interference from UAV electronics such as motors. We propose the use of a\nframe-mounted UAV-borne two-magnetometer payload to perform a two-step\nautomated interference removal and landmine detection analysis. The first step\nremoves interference via the Wavelet-Adaptive Interference Cancellation for\nUnderdetermined Platform (WAIC-UP) method designed for spaceflight\nmagnetometers. The second method uses the Rapid Unsupervised Detection of\nEvents (RUDE) algorithm to detect landmine signatures. This two-step\nWAIC-UP/RUDE approach with multiple magnetometers achieves high-fidelity\nordinance detection at a low computational cost and simplifies the design of\nmagnetic survey payloads. We validate the method through a Monte Carlo\nsimulation of randomized landmine placements in a 10 x 10 m square grid and\ndrone motor interference. Additionally, we assess the efficacy of the algorithm\nby varying the drone's altitude, examining its performance at different heights\nabove the ground.", "AI": {"tldr": "A two-step automated method using UAV-mounted magnetometers for landmine detection that removes drone motor interference and detects landmine signatures efficiently.", "motivation": "Landmines pose persistent dangers to civilians and hinder post-war recovery. Current UAV magnetometer detection faces interference from drone electronics, requiring effective interference removal.", "method": "Uses frame-mounted two-magnetometer payload with WAIC-UP method for interference cancellation and RUDE algorithm for landmine signature detection, validated through Monte Carlo simulations.", "result": "Achieves high-fidelity ordinance detection with low computational cost and simplified magnetic survey payload design, with performance assessed across varying drone altitudes.", "conclusion": "The WAIC-UP/RUDE approach provides an effective solution for automated landmine detection using UAVs while overcoming electronic interference challenges."}}
{"id": "2510.01370", "categories": ["cs.CV", "cs.AI", "cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.01370", "abs": "https://arxiv.org/abs/2510.01370", "authors": ["Abu Bucker Siddik", "Diane Oyen", "Alexander Most", "Michal Kucer", "Ayan Biswas"], "title": "SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs", "comment": null, "summary": "We introduce Small PDE U-Net Solver (SPUS), a compact and efficient\nfoundation model (FM) designed as a unified neural operator for solving a wide\nrange of partial differential equations (PDEs). Unlike existing\nstate-of-the-art PDE FMs-primarily based on large complex transformer\narchitectures with high computational and parameter overhead-SPUS leverages a\nlightweight residual U-Net-based architecture that has been largely\nunderexplored as a foundation model architecture in this domain. To enable\neffective learning in this minimalist framework, we utilize a simple yet\npowerful auto-regressive pretraining strategy which closely replicates the\nbehavior of numerical solvers to learn the underlying physics. SPUS is\npretrained on a diverse set of fluid dynamics PDEs and evaluated across 6\nchallenging unseen downstream PDEs spanning various physical systems.\nExperimental results demonstrate that SPUS using residual U-Net based\narchitecture achieves state-of-the-art generalization on these downstream tasks\nwhile requiring significantly fewer parameters and minimal fine-tuning data,\nhighlighting its potential as a highly parameter-efficient FM for solving\ndiverse PDE systems.", "AI": {"tldr": "SPUS is a compact foundation model using lightweight residual U-Net architecture for solving various PDEs, achieving state-of-the-art generalization with fewer parameters and minimal fine-tuning.", "motivation": "To create a parameter-efficient foundation model for solving PDEs that avoids the computational overhead of large transformer architectures used in existing PDE foundation models.", "method": "Uses a lightweight residual U-Net-based architecture with auto-regressive pretraining strategy that replicates numerical solver behavior, pretrained on diverse fluid dynamics PDEs.", "result": "Achieves state-of-the-art generalization on 6 challenging unseen downstream PDEs while requiring significantly fewer parameters and minimal fine-tuning data.", "conclusion": "SPUS demonstrates the potential of U-Net-based architectures as highly parameter-efficient foundation models for solving diverse PDE systems."}}
{"id": "2510.02134", "categories": ["cs.IT", "math.IT", "physics/communications oriented"], "pdf": "https://arxiv.org/pdf/2510.02134", "abs": "https://arxiv.org/abs/2510.02134", "authors": ["Javane Rostampoor", "Raviraj Adve"], "title": "Interference Resilient Quantum Receivers with Rydberg Atoms", "comment": "Accepted for presentation at the 2025 IEEE Globecom Workshops (GC\n  Wkshps): Workshop on Quantum Computing for Communications and Learning", "summary": "Quantum sensing has attracted significant attention due to its ability to\nmeasure physical quantities with extremely high accuracy. Rydberg atoms -\ntypically alkali atoms with a highly excited valence electron that is far from\nthe nucleus - exhibit strong sensitivity to external electromagnetic fields.\nThis sensitivity leads to coupling between different atomic energy levels,\nwhich can be observed by monitoring changes in a control laser beam before and\nafter it passes through a vapor cell containing the Rydberg atoms. By analyzing\nthe transmitted laser signal with a photodetector, variations in transmission\ncan be attributed to the presence and characteristics of the external\nelectromagnetic field. Because Rydberg atoms operate in a highly excited\nquantum state without relying on traditional electronic circuitry, they\ninherently avoid thermal noise, thereby enabling more sensitive detection. In\nthis paper, we investigate the performance of a Rydberg atomic receiver based\non Rb-85 and compare it with that of a conventional receiver in detecting an\n8-level pulse amplitude modulation (8-PAM) signal in the presence of\noff-resonant interference. We demonstrate that the Rydberg receiver can\nsuppress interference without the need for an additional filter. Effectively,\nour results show that the Rydberg receiver serves as an integrated filter and\ndemodulator, outperforming conventional circuit-based receivers in terms of\nachievable symbol error rate", "AI": {"tldr": "Rydberg atomic receivers using Rb-85 outperform conventional receivers in detecting 8-PAM signals by suppressing off-resonant interference without additional filters, serving as integrated filter-demodulator systems.", "motivation": "Quantum sensing with Rydberg atoms offers high-accuracy measurements due to their strong sensitivity to electromagnetic fields and inherent thermal noise immunity, making them promising for sensitive detection applications.", "method": "Used Rydberg atoms (Rb-85) in vapor cells to detect 8-level pulse amplitude modulation signals, monitoring laser transmission changes through photodetectors to measure electromagnetic field characteristics.", "result": "The Rydberg receiver successfully suppressed off-resonant interference without requiring additional filters and achieved better symbol error rate performance compared to conventional circuit-based receivers.", "conclusion": "Rydberg atomic receivers function as integrated filter-demodulator systems that outperform traditional receivers, demonstrating the practical advantages of quantum sensing technologies in electromagnetic signal detection."}}
{"id": "2510.02109", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02109", "abs": "https://arxiv.org/abs/2510.02109", "authors": ["Jong Bum Won", "Wesley De Neve", "Joris Vankerschaver", "Utku Ozbulak"], "title": "SpurBreast: A Curated Dataset for Investigating Spurious Correlations in Real-world Breast MRI Classification", "comment": "Accepted for publication in the 28th International Conference on\n  Medical Image Computing and Computer Assisted Intervention (MICCAI), 2025", "summary": "Deep neural networks (DNNs) have demonstrated remarkable success in medical\nimaging, yet their real-world deployment remains challenging due to spurious\ncorrelations, where models can learn non-clinical features instead of\nmeaningful medical patterns. Existing medical imaging datasets are not designed\nto systematically study this issue, largely due to restrictive licensing and\nlimited supplementary patient data. To address this gap, we introduce\nSpurBreast, a curated breast MRI dataset that intentionally incorporates\nspurious correlations to evaluate their impact on model performance. Analyzing\nover 100 features involving patient, device, and imaging protocol, we identify\ntwo dominant spurious signals: magnetic field strength (a global feature\ninfluencing the entire image) and image orientation (a local feature affecting\nspatial alignment). Through controlled dataset splits, we demonstrate that DNNs\ncan exploit these non-clinical signals, achieving high validation accuracy\nwhile failing to generalize to unbiased test data. Alongside these two datasets\ncontaining spurious correlations, we also provide benchmark datasets without\nspurious correlations, allowing researchers to systematically investigate\nclinically relevant and irrelevant features, uncertainty estimation,\nadversarial robustness, and generalization strategies. Models and datasets are\navailable at https://github.com/utkuozbulak/spurbreast.", "AI": {"tldr": "SpurBreast is a curated breast MRI dataset designed to study spurious correlations in medical imaging, identifying magnetic field strength and image orientation as dominant non-clinical signals that DNNs exploit, leading to poor generalization.", "motivation": "Real-world deployment of DNNs in medical imaging is challenging due to spurious correlations where models learn non-clinical features instead of meaningful medical patterns, and existing datasets don't systematically study this issue.", "method": "Created SpurBreast dataset with intentional spurious correlations by analyzing over 100 patient, device, and imaging protocol features. Used controlled dataset splits to evaluate model performance.", "result": "Identified two dominant spurious signals: magnetic field strength (global feature) and image orientation (local feature). DNNs achieved high validation accuracy but failed to generalize to unbiased test data by exploiting these non-clinical signals.", "conclusion": "The dataset enables systematic investigation of clinically relevant vs irrelevant features, uncertainty estimation, adversarial robustness, and generalization strategies in medical imaging AI."}}
{"id": "2510.01304", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01304", "abs": "https://arxiv.org/abs/2510.01304", "authors": ["Yu Zeng", "Wenxuan Huang", "Shiting Huang", "Xikun Bao", "Yukun Qi", "Yiming Zhao", "Qiuchen Wang", "Lin Chen", "Zehui Chen", "Huaian Chen", "Wanli Ouyang", "Feng Zhao"], "title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models", "comment": null, "summary": "Although current large Vision-Language Models (VLMs) have advanced in\nmultimodal understanding and reasoning, their fundamental perceptual and\nreasoning abilities remain limited. Specifically, even on simple jigsaw tasks,\nexisting VLMs perform near randomly, revealing deficiencies in core perception\nand reasoning capabilities. While high-quality vision-language data can enhance\nthese capabilities, its scarcity and limited scalability impose significant\nconstraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction\nLearning for Enhancing visual perception and reasoning in VLMs. AGILE\nformulates jigsaw solving as an interactive process, enabling the model to\nprogressively engage with the environment. At each step, the model generates\nexecutable code to perform an action based on the current state, while the\nenvironment provides fine-grained visual feedback to guide task completion.\nThrough this iterative cycle of observation and interaction, the model\nincrementally improves its perceptual and reasoning capabilities via\nexploration and feedback. Experimental results show that AGILE not only\nsubstantially boosts performance on jigsaw tasks of varying complexity (e.g.,\nincreasing accuracy from 9.5% to 82.8% under the 2 $\\times$ 2 setting) but also\ndemonstrates strong generalization across 9 general vision tasks, achieving an\naverage improvement of 3.1%. These results indicate notable enhancements in\nboth perceptual and reasoning abilities. This work opens a new avenue for\nadvancing reasoning and generalization in multimodal models and provides an\nefficient, scalable solution to the scarcity of multimodal reinforcement\nlearning data. The code and datasets is available at\nhttps://github.com/yuzeng0-0/AGILE .", "AI": {"tldr": "AGILE is an agentic jigsaw interaction learning method that enhances visual perception and reasoning in VLMs through interactive jigsaw solving with code execution and visual feedback.", "motivation": "Current VLMs perform poorly on simple jigsaw tasks, revealing deficiencies in core perception and reasoning capabilities, while high-quality vision-language data is scarce and not scalable.", "method": "AGILE formulates jigsaw solving as an interactive process where the model generates executable code to perform actions based on current state, and the environment provides fine-grained visual feedback to guide task completion through iterative cycles.", "result": "AGILE boosts jigsaw task accuracy from 9.5% to 82.8% under 2\u00d72 setting and demonstrates strong generalization across 9 vision tasks with average 3.1% improvement.", "conclusion": "AGILE provides an efficient, scalable solution to multimodal data scarcity and opens new avenues for advancing reasoning and generalization in multimodal models."}}
{"id": "2510.01261", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01261", "abs": "https://arxiv.org/abs/2510.01261", "authors": ["Vedant Palit"], "title": "Adaptive Federated Learning Defences via Trust-Aware Deep Q-Networks", "comment": "16 pages, 10 figures", "summary": "Federated learning is vulnerable to poisoning and backdoor attacks under\npartial observability. We formulate defence as a partially observable\nsequential decision problem and introduce a trust-aware Deep Q-Network that\nintegrates multi-signal evidence into client trust updates while optimizing a\nlong-horizon robustness--accuracy objective. On CIFAR-10, we (i) establish a\nbaseline showing steadily improving accuracy, (ii) show through a Dirichlet\nsweep that increased client overlap consistently improves accuracy and reduces\nASR with stable detection, and (iii) demonstrate in a signal-budget study that\naccuracy remains steady while ASR increases and ROC-AUC declines as\nobservability is reduced, which highlights that sequential belief updates\nmitigate weaker signals. Finally, a comparison with random, linear-Q, and\npolicy gradient controllers confirms that DQN achieves the best\nrobustness--accuracy trade-off.", "AI": {"tldr": "A trust-aware Deep Q-Network defense for federated learning that integrates multi-signal evidence to mitigate poisoning and backdoor attacks under partial observability, achieving optimal robustness-accuracy trade-off.", "motivation": "Federated learning is vulnerable to poisoning and backdoor attacks when there's partial observability of client behavior, requiring robust defense mechanisms.", "method": "Formulated defense as a partially observable sequential decision problem and developed a trust-aware Deep Q-Network that integrates multi-signal evidence for client trust updates while optimizing long-horizon robustness-accuracy objectives.", "result": "On CIFAR-10: (i) established baseline with steadily improving accuracy, (ii) showed increased client overlap improves accuracy and reduces attack success rate (ASR) with stable detection, (iii) demonstrated accuracy remains steady while ASR increases and ROC-AUC declines with reduced observability, showing sequential belief updates mitigate weaker signals.", "conclusion": "The trust-aware DQN achieves the best robustness-accuracy trade-off compared to random, linear-Q, and policy gradient controllers, effectively defending against poisoning and backdoor attacks in federated learning."}}
{"id": "2510.01437", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.01437", "abs": "https://arxiv.org/abs/2510.01437", "authors": ["Ali Amhaz", "Shreya Khisa", "Mohamed Elhattab", "Chadi Assi", "Sanaa Sharafeddine"], "title": "Meta-Learning-Driven Resource Optimization in Full-Duplex ISAC with Movable Antennas", "comment": null, "summary": "This paper investigates a full-duplex (FD) scenario where a base station (BS)\nequipped with movable antennas (MAs) simultaneously provides communication\nservices to a set of downlink (DL) and uplink (UL) users while also enabling\nsensing functionalities for target detection, thereby supporting integrated\nsensing and communication (ISAC) technology. Additionally, a receiving BS, also\nequipped with MAs (denoted as BS R), is responsible for capturing the reflected\necho. To optimize this setup, we formulate an optimization problem aimed at\nmaximizing the signal-to-noise and interference ratio (SINR) of the captured\necho. This is achieved by jointly optimizing the transmit beamforming vectors\nat the FD BS, the receiving beamforming vectors at both the FD BS and BS R, the\nUL users' transmit power, and the MAs' positions at both BSs, all while\nsatisfying the quality-of-service (QoS) requirements for both sensing and\ncommunication. Given the non-convex nature of the problem and the high coupling\nbetween the variables, we employ a gradient-based meta-learning (GML) approach\ntailored for large-scale optimization. Numerical results demonstrate the\neffectiveness of the proposed meta-learning approach, achieving results within\n99% of the optimal solution. Furthermore, the MA-based scheme outperforms\nseveral benchmark approaches, highlighting its advantages in practical ISAC\napplications.", "AI": {"tldr": "This paper proposes a meta-learning approach to optimize movable antenna positions and beamforming for integrated sensing and communication in full-duplex systems, achieving near-optimal performance.", "motivation": "To enhance integrated sensing and communication (ISAC) performance in full-duplex systems by leveraging movable antennas for improved signal-to-noise and interference ratio of sensing echoes while maintaining communication quality.", "method": "A gradient-based meta-learning approach is used to jointly optimize transmit/receive beamforming vectors, uplink power allocation, and movable antenna positions at both base stations, addressing the non-convex and coupled optimization problem.", "result": "The proposed method achieves results within 99% of optimal solution and outperforms benchmark approaches, demonstrating significant advantages for practical ISAC applications.", "conclusion": "Movable antennas combined with meta-learning optimization provide an effective solution for enhancing ISAC performance in full-duplex systems, offering superior echo detection capabilities while maintaining communication quality."}}
{"id": "2510.01399", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01399", "abs": "https://arxiv.org/abs/2510.01399", "authors": ["Shubhankar Borse", "Farzad Farhadzadeh", "Munawar Hayat", "Fatih Porikli"], "title": "DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation", "comment": null, "summary": "State-of-the-art text-to-image models excel at realism but collapse on\nmulti-human prompts - duplicating faces, merging identities, and miscounting\nindividuals. We introduce DisCo (Reinforcement with Diversity Constraints), the\nfirst RL-based framework to directly optimize identity diversity in multi-human\ngeneration. DisCo fine-tunes flow-matching models via Group-Relative Policy\nOptimization (GRPO) with a compositional reward that (i) penalizes intra-image\nfacial similarity, (ii) discourages cross-sample identity repetition, (iii)\nenforces accurate person counts, and (iv) preserves visual fidelity through\nhuman preference scores. A single-stage curriculum stabilizes training as\ncomplexity scales, requiring no extra annotations. On the DiverseHumans\nTestset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global\nIdentity Spread - surpassing both open-source and proprietary methods (e.g.,\nGemini, GPT-Image) while maintaining competitive perceptual quality. Our\nresults establish DisCo as a scalable, annotation-free solution that resolves\nthe long-standing identity crisis in generative models and sets a new benchmark\nfor compositional multi-human generation.", "AI": {"tldr": "DisCo is a reinforcement learning framework that optimizes identity diversity in multi-human image generation, solving face duplication and identity merging issues in text-to-image models.", "motivation": "Current text-to-image models fail on multi-human prompts by duplicating faces, merging identities, and miscounting individuals, creating an identity crisis in generative models.", "method": "Uses Group-Relative Policy Optimization (GRPO) with a compositional reward that penalizes facial similarity, discourages identity repetition, enforces accurate person counts, and preserves visual fidelity through human preference scores. Includes a single-stage curriculum for stable training.", "result": "Achieves 98.6% Unique Face Accuracy and near-perfect Global Identity Spread on DiverseHumans Testset, surpassing both open-source and proprietary methods while maintaining competitive perceptual quality.", "conclusion": "DisCo establishes a scalable, annotation-free solution that resolves the identity crisis in generative models and sets a new benchmark for compositional multi-human generation."}}
{"id": "2510.02191", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.02191", "abs": "https://arxiv.org/abs/2510.02191", "authors": ["Mateus P. Mota", "Mattia Merluzzi", "Emilio Calvanese Strinati"], "title": "Joint Channel and Semantic-aware Grouping for Effective Collaborative Edge Inference", "comment": "Accepted in IEEE SPAWC 2025", "summary": "We focus on collaborative edge inference over wireless, which enables\nmultiple devices to cooperate to improve inference performance in the presence\nof corrupted data. Exploiting a key-query mechanism for selective information\nexchange (or, group formation for collaboration), we recall the effect of\nwireless channel impairments in feature communication. We argue and show that a\ndisjoint approach, which only considers either the semantic relevance or\nchannel state between devices, performs poorly, especially in harsh propagation\nconditions. Based on these findings, we propose a joint approach that takes\ninto account semantic information relevance and channel states when grouping\ndevices for collaboration, by making the general attention weights dependent of\nthe channel information. Numerical simulations show the superiority of the\njoint approach against local inference on corrupted data, as well as compared\nto collaborative inference with disjoint decisions that either consider\napplication or physical layer parameters when forming groups.", "AI": {"tldr": "Proposes a joint approach for collaborative edge inference that considers both semantic relevance and channel states when grouping devices, outperforming disjoint methods that only consider one factor.", "motivation": "To improve collaborative edge inference performance in wireless environments with corrupted data, addressing the limitations of disjoint approaches that consider either semantic relevance or channel states alone.", "method": "Uses a joint approach that makes attention weights dependent on channel information, considering both semantic information relevance and channel states when grouping devices for collaboration.", "result": "Numerical simulations show superiority over local inference on corrupted data and collaborative inference with disjoint decisions based solely on application or physical layer parameters.", "conclusion": "Joint consideration of semantic relevance and channel states is crucial for effective collaborative edge inference in wireless environments with harsh propagation conditions."}}
{"id": "2510.02208", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02208", "abs": "https://arxiv.org/abs/2510.02208", "authors": ["Amirreza Tanevardi", "Pooria Abbas Rad Moghadam", "Sajjad Amini"], "title": "Measurement-Guided Consistency Model Sampling for Inverse Problems", "comment": "5 pages, 3 figures, submitted to IEEE Signal Processing Letters", "summary": "Diffusion models have become powerful generative priors for solving inverse\nimaging problems, but their reliance on slow multi-step sampling limits\npractical deployment. Consistency models address this bottleneck by enabling\nhigh-quality generation in a single or only a few steps, yet their direct\nadaptation to inverse problems is underexplored. In this paper, we present a\nmodified consistency sampling approach tailored for inverse problem\nreconstruction: the sampler's stochasticity is guided by a\nmeasurement-consistency mechanism tied to the measurement operator, which\nenforces fidelity to the acquired measurements while retaining the efficiency\nof consistency-based generation. Experiments on Fashion-MNIST and LSUN Bedroom\ndatasets demonstrate consistent improvements in perceptual and pixel-level\nmetrics, including Fr\\'echet Inception Distance, Kernel Inception Distance,\npeak signal-to-noise ratio, and structural similarity index measure, compared\nto baseline consistency sampling, yielding competitive or superior\nreconstructions with only a handful of steps.", "AI": {"tldr": "A modified consistency sampling approach for inverse imaging problems that guides stochasticity with measurement-consistency to enforce fidelity to acquired measurements while maintaining efficiency.", "motivation": "Diffusion models are powerful for inverse imaging but slow due to multi-step sampling. Consistency models offer fast generation but their adaptation to inverse problems is underexplored.", "method": "Modified consistency sampling with measurement-consistency mechanism tied to the measurement operator, enforcing fidelity to measurements while retaining efficiency.", "result": "Experiments on Fashion-MNIST and LSUN Bedroom show improvements in FID, KID, PSNR, and SSIM metrics compared to baseline consistency sampling, achieving competitive reconstructions with few steps.", "conclusion": "The proposed method enables efficient and high-quality inverse problem reconstruction by combining consistency model efficiency with measurement fidelity guidance."}}
{"id": "2510.01346", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01346", "abs": "https://arxiv.org/abs/2510.01346", "authors": ["Tudor Achim", "Alex Best", "Kevin Der", "Math\u00efs F\u00e9d\u00e9rico", "Sergei Gukov", "Daniel Halpern-Leister", "Kirsten Henningsgard", "Yury Kudryashov", "Alexander Meiburg", "Martin Michelsen", "Riley Patterson", "Eric Rodriguez", "Laura Scharff", "Vikram Shanker", "Vladmir Sicca", "Hari Sowrirajan", "Aidan Swope", "Matyas Tamas", "Vlad Tenev", "Jonathan Thomm", "Harold Williams", "Lawrence Wu"], "title": "Aristotle: IMO-level Automated Theorem Proving", "comment": null, "summary": "We introduce Aristotle, an AI system that combines formal verification with\ninformal reasoning, achieving gold-medal-equivalent performance on the 2025\nInternational Mathematical Olympiad problems. Aristotle integrates three main\ncomponents: a Lean proof search system, an informal reasoning system that\ngenerates and formalizes lemmas, and a dedicated geometry solver. Our system\ndemonstrates state-of-the-art performance with favorable scaling properties for\nautomated theorem proving.", "AI": {"tldr": "Aristotle is an AI system that combines formal verification with informal reasoning, achieving gold-medal-level performance on 2025 IMO problems through integration of Lean proof search, informal reasoning for lemma generation, and a dedicated geometry solver.", "motivation": "To advance automated theorem proving by creating a system that can solve challenging mathematical problems at the level of top human competitors in the International Mathematical Olympiad.", "method": "Integrates three main components: a Lean proof search system, an informal reasoning system that generates and formalizes lemmas, and a dedicated geometry solver.", "result": "Achieved gold-medal-equivalent performance on the 2025 International Mathematical Olympiad problems and demonstrated state-of-the-art performance with favorable scaling properties.", "conclusion": "The Aristotle system successfully combines formal verification with informal reasoning to achieve human-level performance in mathematical theorem proving, representing a significant advancement in automated theorem proving capabilities."}}
{"id": "2510.01262", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01262", "abs": "https://arxiv.org/abs/2510.01262", "authors": ["Koyena Chowdhury", "Paramita Koley", "Abhijnan Chakraborty", "Saptarshi Ghosh"], "title": "RSTGCN: Railway-centric Spatio-Temporal Graph Convolutional Network for Train Delay Prediction", "comment": null, "summary": "Accurate prediction of train delays is critical for efficient railway\noperations, enabling better scheduling and dispatching decisions. While earlier\napproaches have largely focused on forecasting the exact delays of individual\ntrains, recent studies have begun exploring station-level delay prediction to\nsupport higher-level traffic management. In this paper, we propose the\nRailway-centric Spatio-Temporal Graph Convolutional Network (RSTGCN), designed\nto forecast average arrival delays of all the incoming trains at railway\nstations for a particular time period. Our approach incorporates several\narchitectural innovations and novel feature integrations, including train\nfrequency-aware spatial attention, which significantly enhances predictive\nperformance. To support this effort, we curate and release a comprehensive\ndataset for the entire Indian Railway Network (IRN), spanning 4,735 stations\nacross 17 zones - the largest and most diverse railway network studied to date.\nWe conduct extensive experiments using multiple state-of-the-art baselines,\ndemonstrating consistent improvements across standard metrics. Our work not\nonly advances the modeling of average delay prediction in large-scale rail\nnetworks but also provides an open dataset to encourage further research in\nthis critical domain.", "AI": {"tldr": "Proposes RSTGCN, a spatio-temporal graph neural network for predicting average arrival delays at railway stations, using novel train frequency-aware attention and releasing the largest Indian Railway dataset.", "motivation": "Accurate train delay prediction is critical for railway operations, with recent focus shifting from individual train delays to station-level prediction for better traffic management.", "method": "Developed Railway-centric Spatio-Temporal Graph Convolutional Network (RSTGCN) with architectural innovations including train frequency-aware spatial attention and novel feature integrations.", "result": "Demonstrated consistent improvements across standard metrics compared to multiple state-of-the-art baselines, showing enhanced predictive performance.", "conclusion": "Advances modeling of average delay prediction in large-scale rail networks and provides an open dataset (Indian Railway Network with 4,735 stations) to encourage further research."}}
{"id": "2510.01605", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.01605", "abs": "https://arxiv.org/abs/2510.01605", "authors": ["Hao Wu", "Xinyuan Yao", "Rui Ni", "Chen Gong"], "title": "The Analysis and Performance of LODC-OFDM Signal in Nonlinear Rydberg Atomic Sensor", "comment": null, "summary": "Rydberg atomic sensors have been seen as novel radio frequency (RF)\nmeasurements and the high sensitivity to a large range of frequencies makes it\nattractive for communications reception. However, the signal sensing process in\nRydberg system involves sequential transduction from electromagnetic waves to\noptical signals and finally to electrical signals. The unipolar characteristic\nof the optical interface inherently restricts conventional OFDM reception.\nTherefore, adopting unipolar OFDM schemes, inspired by optical communication\nsystems, becomes essential for compatible signal transmission. In this work, we\ninvestigate the amplitude modulation-to-amplitude modulation (AM-AM)\ncharacteristics of Rydberg atomic sensors, establishing an empirical\napproximation function. Building on the direct current-biased optical\northogonal frequency division multiplexing (DCO-OFDM) framework, we propose a\nnovel local oscillator direct current-biased OFDM (LODC-OFDM) scheme\nspecifically optimized for Rydberg-based sensing, effectively addressing the\nbroadband OFDM reception challenge. Then, we adopt Bussgang theorem to analyze\nthe nonlinear distortion of LODC-OFDM signals and the results in closed-form\nsolutions are derived for AM/AM curves approximated by Taylor series expansion\nand for the ideal pre-distortion case. In real experiments, the experimental\nand theoretical results fit well.", "AI": {"tldr": "Rydberg atomic sensors face challenges with OFDM reception due to unipolar optical interfaces. The paper proposes LODC-OFDM scheme optimized for Rydberg sensing, analyzes nonlinear distortion using Bussgang theorem, and validates with experimental results.", "motivation": "Rydberg atomic sensors are attractive for RF communications but their sequential transduction process and unipolar optical interface restrict conventional OFDM reception, requiring specialized unipolar OFDM schemes.", "method": "Established AM-AM characteristics approximation, proposed LODC-OFDM scheme based on DCO-OFDM framework, and used Bussgang theorem to analyze nonlinear distortion with closed-form solutions for Taylor series approximation and ideal pre-distortion cases.", "result": "Experimental and theoretical results showed good agreement, validating the proposed LODC-OFDM scheme for broadband OFDM reception in Rydberg-based sensing systems.", "conclusion": "The LODC-OFDM scheme effectively addresses the broadband OFDM reception challenge in Rydberg atomic sensors, with theoretical analysis matching experimental performance."}}
{"id": "2510.01448", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01448", "abs": "https://arxiv.org/abs/2510.01448", "authors": ["Angel Daruna", "Nicholas Meegan", "Han-Pang Chiu", "Supun Samarasekera", "Rakesh Kumar"], "title": "GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings", "comment": "preprint under review", "summary": "Worldwide visual geo-localization seeks to determine the geographic location\nof an image anywhere on Earth using only its visual content. Learned\nrepresentations of geography for visual geo-localization remain an active\nresearch topic despite much progress. We formulate geo-localization as aligning\nthe visual representation of the query image with a learned geographic\nrepresentation. Our novel geographic representation explicitly models the world\nas a hierarchy of geographic embeddings. Additionally, we introduce an approach\nto efficiently fuse the appearance features of the query image with its\nsemantic segmentation map, forming a robust visual representation. Our main\nexperiments demonstrate improved all-time bests in 22 out of 25 metrics\nmeasured across five benchmark datasets compared to prior state-of-the-art\n(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional\nablation studies support the claim that these gains are primarily driven by the\ncombination of geographic and visual representations.", "AI": {"tldr": "The paper introduces a novel geographic representation that models the world as a hierarchy of geographic embeddings for visual geo-localization, achieving state-of-the-art performance across multiple benchmarks.", "motivation": "To improve visual geo-localization by developing learned representations of geography that can effectively align with visual content from query images anywhere on Earth.", "method": "Formulates geo-localization as aligning visual representations with learned geographic representations, using hierarchical geographic embeddings and fusing appearance features with semantic segmentation maps.", "result": "Achieved improved all-time bests in 22 out of 25 metrics across five benchmark datasets, outperforming prior state-of-the-art methods and recent Large Vision-Language Models.", "conclusion": "The combination of geographic and visual representations drives significant performance gains in visual geo-localization tasks."}}
{"id": "2510.02222", "categories": ["cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.02222", "abs": "https://arxiv.org/abs/2510.02222", "authors": ["Mateus P. Mota", "Mattia Merluzzi", "Emilio Calvanese Strinati"], "title": "Collaborative Edge Inference via Semantic Grouping under Wireless Channel Constraints", "comment": "5 pages, 5 figures. Accepted at 33rd European Signal Processing\n  Conference (EUSIPCO 2025)", "summary": "In this paper, we study the framework of collaborative inference, or edge\nensembles. This framework enables multiple edge devices to improve\nclassification accuracy by exchanging intermediate features rather than raw\nobservations. However, efficient communication strategies are essential to\nbalance accuracy and bandwidth limitations. Building upon a key-query mechanism\nfor selective information exchange, this work extends collaborative inference\nby studying the impact of channel noise in feature communication, the choice of\nintermediate collaboration points, and the communication-accuracy trade-off\nacross tasks. By analyzing how different collaboration points affect\nperformance and exploring communication pruning, we show that it is possible to\noptimize accuracy while minimizing resource usage. We show that the\nintermediate collaboration approach is robust to channel errors and that the\nquery transmission needs a higher degree of reliability than the data\ntransmission itself.", "AI": {"tldr": "This paper studies collaborative inference (edge ensembles) where edge devices exchange intermediate features to improve classification accuracy, focusing on communication efficiency, channel noise impact, and optimal collaboration points.", "motivation": "To enable multiple edge devices to enhance classification accuracy through feature exchange while addressing bandwidth limitations and communication reliability challenges.", "method": "Extends collaborative inference using a key-query mechanism for selective information exchange, analyzes channel noise impact, explores different intermediate collaboration points, and studies communication-accuracy trade-offs across tasks.", "result": "The intermediate collaboration approach is robust to channel errors, and query transmission requires higher reliability than data transmission. Communication pruning can optimize accuracy while minimizing resource usage.", "conclusion": "Collaborative inference with selective feature exchange can effectively balance accuracy and bandwidth constraints, with query reliability being more critical than data transmission reliability."}}
{"id": "2510.01353", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01353", "abs": "https://arxiv.org/abs/2510.01353", "authors": ["Darshan Deshpande", "Varun Gangal", "Hersh Mehta", "Anand Kannappan", "Rebecca Qian", "Peng Wang"], "title": "MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments", "comment": "Accepted to NeurIPS 2025 SEA Workshop", "summary": "Recent works on context and memory benchmarking have primarily focused on\nconversational instances but the need for evaluating memory in dynamic\nenterprise environments is crucial for its effective application. We introduce\nMEMTRACK, a benchmark designed to evaluate long-term memory and state tracking\nin multi-platform agent environments. MEMTRACK models realistic organizational\nworkflows by integrating asynchronous events across multiple communication and\nproductivity platforms such as Slack, Linear and Git. Each benchmark instance\nprovides a chronologically platform-interleaved timeline, with noisy,\nconflicting, cross-referring information as well as potential\ncodebase/file-system comprehension and exploration. Consequently, our benchmark\ntests memory capabilities such as acquistion, selection and conflict\nresolution. We curate the MEMTRACK dataset through both manual expert driven\ndesign and scalable agent based synthesis, generating ecologically valid\nscenarios grounded in real world software development processes. We introduce\npertinent metrics for Correctness, Efficiency, and Redundancy that capture the\neffectiveness of memory mechanisms beyond simple QA performance. Experiments\nacross SoTA LLMs and memory backends reveal challenges in utilizing memory\nacross long horizons, handling cross-platform dependencies, and resolving\ncontradictions. Notably, the best performing GPT-5 model only achieves a 60\\%\nCorrectness score on MEMTRACK. This work provides an extensible framework for\nadvancing evaluation research for memory-augmented agents, beyond existing\nfocus on conversational setups, and sets the stage for multi-agent,\nmulti-platform memory benchmarking in complex organizational settings", "AI": {"tldr": "MEMTRACK is a benchmark for evaluating long-term memory and state tracking in multi-platform agent environments, focusing on realistic organizational workflows with asynchronous events across platforms like Slack, Linear, and Git.", "motivation": "Existing context and memory benchmarks focus on conversational instances, but there's a need for evaluating memory in dynamic enterprise environments for effective application in real-world organizational settings.", "method": "MEMTRACK integrates asynchronous events across multiple communication and productivity platforms, creating chronologically interleaved timelines with noisy, conflicting, and cross-referring information. The dataset is curated through manual expert design and scalable agent-based synthesis.", "result": "Experiments show challenges in utilizing memory across long horizons, handling cross-platform dependencies, and resolving contradictions. The best performing GPT-5 model only achieves 60% Correctness score on MEMTRACK.", "conclusion": "MEMTRACK provides an extensible framework for advancing evaluation research for memory-augmented agents beyond conversational setups, enabling multi-agent, multi-platform memory benchmarking in complex organizational settings."}}
{"id": "2510.01263", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01263", "abs": "https://arxiv.org/abs/2510.01263", "authors": ["Yaron Meirovitch", "Fuming Yang", "Jeff Lichtman", "Nir Shavit"], "title": "Budgeted Broadcast: An Activity-Dependent Pruning Rule for Neural Network Efficiency", "comment": null, "summary": "Most pruning methods remove parameters ranked by impact on loss (e.g.,\nmagnitude or gradient). We propose Budgeted Broadcast (BB), which gives each\nunit a local traffic budget (the product of its long-term on-rate $a_i$ and\nfan-out $k_i$). A constrained-entropy analysis shows that maximizing coding\nentropy under a global traffic budget yields a selectivity-audience balance,\n$\\log\\frac{1-a_i}{a_i}=\\beta k_i$. BB enforces this balance with simple local\nactuators that prune either fan-in (to lower activity) or fan-out (to reduce\nbroadcast). In practice, BB increases coding entropy and decorrelation and\nimproves accuracy at matched sparsity across Transformers for ASR, ResNets for\nface identification, and 3D U-Nets for synapse prediction, sometimes exceeding\ndense baselines. On electron microscopy images, it attains state-of-the-art F1\nand PR-AUC under our evaluation protocol. BB is easy to integrate and suggests\na path toward learning more diverse and efficient representations.", "AI": {"tldr": "Budgeted Broadcast (BB) is a pruning method that assigns local traffic budgets to units and enforces a selectivity-audience balance to maximize coding entropy, improving accuracy and efficiency across various neural network architectures.", "motivation": "Traditional pruning methods remove parameters based on impact on loss, but this approach may not optimize for coding efficiency and representation diversity.", "method": "BB assigns each unit a local traffic budget (product of on-rate and fan-out), uses constrained-entropy analysis to derive a selectivity-audience balance equation, and enforces this balance with simple local actuators that prune either fan-in or fan-out.", "result": "BB increases coding entropy and decorrelation, improves accuracy at matched sparsity across Transformers, ResNets, and 3D U-Nets, sometimes exceeding dense baselines. Achieves state-of-the-art F1 and PR-AUC on electron microscopy images.", "conclusion": "BB is easy to integrate and suggests a path toward learning more diverse and efficient representations through principled pruning based on traffic budgets."}}
{"id": "2510.01707", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.01707", "abs": "https://arxiv.org/abs/2510.01707", "authors": ["Amila Ravinath", "Minhua Ding", "Bikshapathi Gouda", "Italo Atzeni", "Antti T\u00f6lli"], "title": "SEP Analysis of 1-Bit Quantized SIMO Systems with QPSK over Fading Channels", "comment": "Accepted to Asilomar Conference on Signals, Systems, and Computers\n  2025", "summary": "The average symbol error probability (SEP) of a 1-bit quantized single-input\nmultiple-output (SIMO) system is analyzed under Rayleigh fading channels and\nquadrature phase-shift keying (QPSK) modulation. Previous studies have\npartially characterized the diversity gain for selection combining (SC). In\nthis paper, leveraging a novel analytical method, an exact analytical SEP\nexpression is derived for a 1-bit quantized SIMO system employing QPSK\nmodulation at the transmitter and maximum ratio combining (MRC) at the\nreceiver. The corresponding diversity and coding gains of a SIMO-MRC system are\nalso determined. Furthermore, the diversity and coding gains of a 1-bit\nquantized SIMO-SC system are quantified for an arbitrary number of receive\nantennas, thereby extending and complementing prior results.", "AI": {"tldr": "Exact SEP analysis for 1-bit quantized SIMO systems with QPSK modulation, deriving diversity and coding gains for both MRC and SC receivers.", "motivation": "Previous studies only partially characterized diversity gain for SC receivers, leaving gaps in understanding 1-bit quantized SIMO system performance.", "method": "Novel analytical method to derive exact SEP expression for 1-bit quantized SIMO systems with QPSK modulation and MRC receiver, then extending to SC receivers.", "result": "Derived exact analytical SEP expression for MRC systems and quantified diversity/coding gains for both MRC and SC systems with arbitrary number of receive antennas.", "conclusion": "Complete characterization of 1-bit quantized SIMO system performance with both MRC and SC receivers, extending prior limited results."}}
{"id": "2510.01454", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01454", "abs": "https://arxiv.org/abs/2510.01454", "authors": ["Nilay Naharas", "Dang Nguyen", "Nesihan Bulut", "Mohammadhossein Bateni", "Vahab Mirrokni", "Baharan Mirzasoleiman"], "title": "Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories", "comment": "30 pages, 10 figures, 5 tables, link:\n  https://bigml-cs-ucla.github.io/XMAS-project-page/", "summary": "Data-efficient learning aims to eliminate redundancy in large training\ndatasets by training models on smaller subsets of the most informative\nexamples. While data selection has been extensively explored for vision models\nand large language models (LLMs), it remains underexplored for Large\nVision-Language Models (LVLMs). Notably, none of existing methods can\noutperform random selection at different subset sizes. In this work, we propose\nthe first principled method for data-efficient instruction tuning of LVLMs. We\nprove that examples with similar cross-modal attention matrices during\ninstruction tuning have similar gradients. Thus, they influence model\nparameters in a similar manner and convey the same information to the model\nduring training. Building on this insight, we propose XMAS, which clusters\nexamples based on the trajectories of the top singular values of their\nattention matrices obtained from fine-tuning a small proxy LVLM. By sampling a\nbalanced subset from these clusters, XMAS effectively removes redundancy in\nlarge-scale LVLM training data. Extensive experiments show that XMAS can\ndiscard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while\nfully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and\nspeeding up its training by 1.2x. This is 30% more data reduction compared to\nthe best baseline for LLaVA-665k. The project's website can be found at\nhttps://bigml-cs-ucla.github.io/XMAS-project-page/.", "AI": {"tldr": "XMAS is a data-efficient instruction tuning method for Large Vision-Language Models that clusters examples based on cross-modal attention matrix trajectories to remove redundancy, enabling 50-85% data reduction while preserving performance.", "motivation": "Data selection methods for Large Vision-Language Models are underexplored, with existing methods failing to outperform random selection at different subset sizes.", "method": "XMAS clusters examples based on trajectories of top singular values of attention matrices from fine-tuning a small proxy LVLM, then samples a balanced subset from these clusters to remove redundancy.", "result": "XMAS discards 50% of LLaVA-665k and 85% of Vision-Flan datasets while preserving LLaVA-1.5-7B performance on 10 benchmarks and speeding up training by 1.2x, achieving 30% more data reduction than best baseline.", "conclusion": "XMAS provides the first principled method for data-efficient LVLM instruction tuning, effectively eliminating redundancy in large-scale training data through attention-based clustering."}}
{"id": "2510.01850", "categories": ["eess.SP", "cs.AI", "cs.IT", "cs.LG", "math.IT", "68T07, 94A12, 62M10", "I.2.6; I.5.4; C.2.1"], "pdf": "https://arxiv.org/pdf/2510.01850", "abs": "https://arxiv.org/abs/2510.01850", "authors": ["Ying-Ren Chien", "Po-Heng Chou", "You-Jie Peng", "Chun-Yuan Huang", "Hen-Wai Tsao", "Yu Tsao"], "title": "NGGAN: Noise Generation GAN Based on the Practical Measurement Dataset for Narrowband Powerline Communications", "comment": "16 pages, 15 figures, 11 tables, and published in IEEE Transactions\n  on Instrumentation and Measurement, Vol. 74, 2025", "summary": "Capturing comprehensive statistics of nonperiodic asynchronous impulsive\nnoise is a critical issue in enhancing impulse noise processing for narrowband\npowerline communication (NB-PLC) transceivers. However, existing mathematical\nnoise generative models capture only some of the characteristics of additive\nnoise. Therefore, we propose a generative adversarial network (GAN), called the\nnoise-generation GAN (NGGAN), that learns the complicated characteristics of\npractically measured noise samples for data augmentation. To closely match the\nstatistics of complicated noise in NB-PLC systems, we measured the NB-PLC noise\nvia the analog coupling and bandpass filtering circuits of a commercial NB-PLC\nmodem to build a realistic dataset. Specifically, the NGGAN design approaches\nbased on the practically measured dataset are as follows: (i) we design the\nlength of input signals that the NGGAN model can fit to facilitate\ncyclo-stationary noise generation. (ii) Wasserstein distance is used as a loss\nfunction to enhance the similarity between the generated noise and the training\ndataset and ensure that the sample diversity is sufficient for various\napplications. (iii) To measure the similarity performance of the GAN-based\nmodels based on mathematical and practically measured datasets, we perform\nquantitative and qualitative analyses. The training datasets include (1) a\npiecewise spectral cyclo-stationary Gaussian model (PSCGM), (2) a\nfrequency-shift (FRESH) filter, and (3) practical measurements from NB-PLC\nsystems. Simulation results demonstrate that the proposed NGGAN trained using\nwaveform characteristics is closer to the practically measured dataset in terms\nof the quality of the generated noise.", "AI": {"tldr": "Proposes NGGAN, a generative adversarial network that learns characteristics of measured NB-PLC noise for data augmentation, outperforming existing mathematical models.", "motivation": "Existing mathematical noise generative models capture only some characteristics of additive noise in narrowband powerline communication systems, limiting impulse noise processing enhancement.", "method": "Developed NGGAN using Wasserstein distance loss function, designed input signal length for cyclo-stationary noise generation, and trained on practical NB-PLC noise measurements from commercial modem circuits.", "result": "NGGAN generates noise closer to practical measurements than PSCGM and FRESH filter models in both quantitative and qualitative analyses.", "conclusion": "GAN-based approach using practical measurements effectively captures complex NB-PLC noise characteristics for improved data augmentation."}}
{"id": "2510.01363", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01363", "abs": "https://arxiv.org/abs/2510.01363", "authors": ["Leon Garza", "Anantaa Kotal", "Michael A. Grasso", "Emre Umucu"], "title": "Retrieval-Augmented Framework for LLM-Based Clinical Decision Support", "comment": null, "summary": "The increasing complexity of clinical decision-making, alongside the rapid\nexpansion of electronic health records (EHR), presents both opportunities and\nchallenges for delivering data-informed care. This paper proposes a clinical\ndecision support system powered by Large Language Models (LLMs) to assist\nprescribing clinicians. The system generates therapeutic suggestions by\nanalyzing historical EHR data, including patient demographics, presenting\ncomplaints, clinical symptoms, diagnostic information, and treatment histories.\nThe framework integrates natural language processing with structured clinical\ninputs to produce contextually relevant recommendations. Rather than replacing\nclinician judgment, it is designed to augment decision-making by retrieving and\nsynthesizing precedent cases with comparable characteristics, drawing on local\ndatasets or federated sources where applicable. At its core, the system employs\na retrieval-augmented generation (RAG) pipeline that harmonizes unstructured\nnarratives and codified data to support LLM-based inference. We outline the\nsystem's technical components, including representation representation\nalignment and generation strategies. Preliminary evaluations, conducted with\nde-identified and synthetic clinical datasets, examine the clinical\nplausibility and consistency of the model's outputs. Early findings suggest\nthat LLM-based tools may provide valuable decision support in prescribing\nworkflows when appropriately constrained and rigorously validated. This work\nrepresents an initial step toward integration of generative AI into real-world\nclinical decision-making with an emphasis on transparency, safety, and\nalignment with established practices.", "AI": {"tldr": "A clinical decision support system using Large Language Models (LLMs) to assist prescribing clinicians by analyzing EHR data and generating therapeutic suggestions through a retrieval-augmented generation (RAG) pipeline.", "motivation": "To address the complexity of clinical decision-making and leverage expanding EHR data to deliver data-informed care, while augmenting rather than replacing clinician judgment.", "method": "Uses a RAG pipeline that integrates natural language processing with structured clinical inputs, harmonizing unstructured narratives and codified data to support LLM-based inference with representation alignment and generation strategies.", "result": "Preliminary evaluations with de-identified and synthetic clinical datasets show promising clinical plausibility and consistency of model outputs, suggesting LLM-based tools can provide valuable decision support in prescribing workflows.", "conclusion": "This represents an initial step toward integrating generative AI into real-world clinical decision-making with emphasis on transparency, safety, and alignment with established practices."}}
{"id": "2510.01264", "categories": ["cs.LG", "cs.RO", "68T42 (Primary) 68T40, 68T05 (Secondary)"], "pdf": "https://arxiv.org/pdf/2510.01264", "abs": "https://arxiv.org/abs/2510.01264", "authors": ["Isaac Peterson", "Christopher Allred", "Jacob Morrey", "Mario Harper"], "title": "A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab", "comment": "8 page, 9 figures, code https://github.com/DIRECTLab/IsaacLab-HARL", "summary": "Multi-Agent Reinforcement Learning (MARL) is central to robotic systems\ncooperating in dynamic environments. While prior work has focused on these\ncollaborative settings, adversarial interactions are equally critical for\nreal-world applications such as pursuit-evasion, security, and competitive\nmanipulation. In this work, we extend the IsaacLab framework to support\nscalable training of adversarial policies in high-fidelity physics simulations.\nWe introduce a suite of adversarial MARL environments featuring heterogeneous\nagents with asymmetric goals and capabilities. Our platform integrates a\ncompetitive variant of Heterogeneous Agent Reinforcement Learning with Proximal\nPolicy Optimization (HAPPO), enabling efficient training and evaluation under\nadversarial dynamics. Experiments across several benchmark scenarios\ndemonstrate the framework's ability to model and train robust policies for\nmorphologically diverse multi-agent competition while maintaining high\nthroughput and simulation realism. Code and benchmarks are available at:\nhttps://github.com/DIRECTLab/IsaacLab-HARL .", "AI": {"tldr": "Extension of IsaacLab framework for scalable adversarial MARL training in physics simulations with heterogeneous agents and asymmetric goals.", "motivation": "Adversarial interactions are critical for real-world applications like pursuit-evasion and security, but prior MARL work focused mainly on collaborative settings.", "method": "Developed adversarial MARL environments with heterogeneous agents and integrated competitive variant of HAPPO (Heterogeneous Agent Reinforcement Learning with Proximal Policy Optimization).", "result": "Framework successfully models and trains robust policies for morphologically diverse multi-agent competition while maintaining high throughput and simulation realism.", "conclusion": "The extended IsaacLab framework enables effective training of adversarial policies in high-fidelity physics simulations for competitive multi-agent scenarios."}}
{"id": "2510.01748", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.01748", "abs": "https://arxiv.org/abs/2510.01748", "authors": ["Hadi Zayyani", "Felipe A. P. de Figueiredo", "Mohammad Salman", "Rausley A. A. de Souza"], "title": "3D 8-Ary Noise Modulation Using Bayesian- and Kurtosis-based Detectors", "comment": null, "summary": "This paper presents a novel three-dimensional (3D) 8-ary noise modulation\nscheme that introduces a new dimension: the mixture probability of a Mixture of\nGaussian (MoG) distribution. This proposed approach utilizes the dimensions of\nmean and variance, in addition to the new probability dimension. Within this\nframework, each transmitted symbol carries three bits, each corresponding to a\ndistinct sub-channel. For detection, a combination of specialized detectors is\nemployed: a simple threshold based detector for the first sub-channel bit\n(modulated by the mean), a Maximum-Likelihood (ML) detector for the second\nsub-channel bit (modulated by the variance), a Kurtosis-based, Jarque-Bera (JB)\ntest, and Bayesian Hypothesis (BHT)-based detectors for the third bit\n(modulated by the MoG probability). The Kurtosis- and JB-based detectors\nspecifically distinguish between Gaussian (or near-Gaussian) and non-Gaussian\nMoG distributions by leveraging higher-order statistical measures. The Bit\nError Probabilities (BEPs) are derived for the threshold-, Kurtosis-, and\nBHT-based detectors. The optimum threshold for the Kurtosis-based detector is\nalso derived in a tractable manner. Simulation results demonstrate that a\ncomparably low BEP is achieved for the third sub-channel bit relative to\nexisting two-dimensional (2D) schemes. Simultaneously, the proposed scheme\nincreases the data rate by a factor of 1.5 and 3 compared to the Generalized\nQuadratic noise modulator and the classical binary KLJN noise modulator,\nrespectively. Furthermore, the Kurtosis-based detector offers a low-complexity\nsolution, achieving an acceptable BEP of approximately 0.06.", "AI": {"tldr": "A novel 3D 8-ary noise modulation scheme using Mixture of Gaussian distribution with three dimensions: mean, variance, and mixture probability, achieving higher data rates than existing 2D schemes.", "motivation": "To increase data transmission rates beyond existing two-dimensional noise modulation schemes by introducing a third dimension (mixture probability) to carry additional information.", "method": "Proposes 3D modulation using MoG distribution with mean, variance, and probability dimensions. Each symbol carries 3 bits. Uses specialized detectors: threshold-based for mean, ML for variance, and Kurtosis/JB/BHT-based for probability detection.", "result": "Achieves 1.5x and 3x higher data rates than Generalized Quadratic and binary KLJN modulators respectively. Kurtosis-based detector achieves acceptable BEP of ~0.06 with low complexity.", "conclusion": "The 3D noise modulation scheme successfully increases data rate while maintaining acceptable error performance, with Kurtosis-based detection providing a practical low-complexity solution."}}
{"id": "2510.01478", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01478", "abs": "https://arxiv.org/abs/2510.01478", "authors": ["R\u0103zvan-Andrei Mati\u015fan", "Vincent Tao Hu", "Grigory Bartosh", "Bj\u00f6rn Ommer", "Cees G. M. Snoek", "Max Welling", "Jan-Willem van de Meent", "Mohammad Mahdi Derakhshani", "Floor Eijkelboom"], "title": "Purrception: Variational Flow Matching for Vector-Quantized Image Generation", "comment": null, "summary": "We introduce Purrception, a variational flow matching approach for\nvector-quantized image generation that provides explicit categorical\nsupervision while maintaining continuous transport dynamics. Our method adapts\nVariational Flow Matching to vector-quantized latents by learning categorical\nposteriors over codebook indices while computing velocity fields in the\ncontinuous embedding space. This combines the geometric awareness of continuous\nmethods with the discrete supervision of categorical approaches, enabling\nuncertainty quantification over plausible codes and temperature-controlled\ngeneration. We evaluate Purrception on ImageNet-1k 256x256 generation. Training\nconverges faster than both continuous flow matching and discrete flow matching\nbaselines while achieving competitive FID scores with state-of-the-art models.\nThis demonstrates that Variational Flow Matching can effectively bridge\ncontinuous transport and discrete supervision for improved training efficiency\nin image generation.", "AI": {"tldr": "Purrception is a variational flow matching method for vector-quantized image generation that combines continuous transport dynamics with explicit categorical supervision over codebook indices.", "motivation": "To bridge the gap between continuous flow matching methods and discrete categorical approaches, enabling geometric awareness from continuous methods while maintaining discrete supervision for improved training efficiency.", "method": "Adapts Variational Flow Matching to vector-quantized latents by learning categorical posteriors over codebook indices while computing velocity fields in the continuous embedding space.", "result": "Training converges faster than both continuous and discrete flow matching baselines while achieving competitive FID scores on ImageNet-1k 256x256 generation, comparable to state-of-the-art models.", "conclusion": "Variational Flow Matching can effectively bridge continuous transport and discrete supervision for improved training efficiency in image generation."}}
{"id": "2510.02012", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.02012", "abs": "https://arxiv.org/abs/2510.02012", "authors": ["Kuranage Roche Rayan Ranasinghe", "Giuseppe Thadeu Freitas de Abreu", "David Gonz\u00e1lez G.", "Carlo Fischione"], "title": "Computing on Dirty Paper: Interference-Free Integrated Communication and Computing", "comment": "Submitted to an IEEE conference", "summary": "Inspired by Costa's pioneering work on dirty paper coding (DPC), this paper\nproposes a novel scheme for integrated communication and computing (ICC), named\nComputing on Dirty Paper, whereby the transmission of discrete data symbols for\ncommunication, and over-the-air computation (AirComp) of nomographic functions\ncan be achieved simultaneously over common multiple-access channels. In\nparticular, the proposed scheme allows for the integration of communication and\ncomputation in a manner that is asymptotically interference-free, by\nprecanceling the computing symbols at the transmitters (TXs) using DPC\nprinciples. A simulation-based assessment of the proposed ICC scheme under a\nsingle-input multiple-output (SIMO) setup is also offered, including the\nevaluation of performance for data detection, and of mean-squared-error (MSE)\nperformance for function computation, over a block of symbols. The results\nvalidate the proposed method and demonstrate its ability to significantly\noutperform state-of-the-art (SotA) ICC schemes in terms of both bit error rate\n(BER) and MSE.", "AI": {"tldr": "Proposes Computing on Dirty Paper, an ICC scheme that integrates communication and AirComp using dirty paper coding principles to achieve asymptotically interference-free transmission.", "motivation": "Inspired by Costa's dirty paper coding to enable simultaneous transmission of discrete data symbols and over-the-air computation over common multiple-access channels.", "method": "Uses dirty paper coding principles to precancel computing symbols at transmitters, allowing integration of communication and computation in SIMO setup.", "result": "Significantly outperforms state-of-the-art ICC schemes in both bit error rate and mean-squared-error performance for data detection and function computation.", "conclusion": "The proposed scheme successfully integrates communication and computation with interference-free transmission, validated through simulations showing superior performance over existing methods."}}
{"id": "2510.01367", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01367", "abs": "https://arxiv.org/abs/2510.01367", "authors": ["Xinpeng Wang", "Nitish Joshi", "Barbara Plank", "Rico Angell", "He He"], "title": "Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort", "comment": null, "summary": "Reward hacking, where a reasoning model exploits loopholes in a reward\nfunction to achieve high rewards without solving the intended task, poses a\nsignificant threat. This behavior may be explicit, i.e. verbalized in the\nmodel's chain-of-thought (CoT), or implicit, where the CoT appears benign thus\nbypasses CoT monitors. To detect implicit reward hacking, we propose TRACE\n(Truncated Reasoning AUC Evaluation). Our key observation is that hacking\noccurs when exploiting the loophole is easier than solving the actual task.\nThis means that the model is using less `effort' than required to achieve high\nreward. TRACE quantifies effort by measuring how early a model's reasoning\nbecomes sufficient to pass a verifier. We progressively truncate a model's CoT\nat various lengths, force the model to answer, and measure the verifier-passing\nrate at each cutoff. A hacking model, which takes a shortcut, will achieve a\nhigh passing rate with only a small fraction of its CoT, yielding a large area\nunder the accuracy-vs-length curve. TRACE achieves over 65% gains over our\nstrongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B\nmonitor in coding. We further show that TRACE can discover unknown loopholes\nduring training. Overall, TRACE offers a scalable unsupervised approach for\noversight where current monitoring methods prove ineffective.", "AI": {"tldr": "TRACE (Truncated Reasoning AUC Evaluation) is a method to detect implicit reward hacking in reasoning models by measuring how early in the reasoning chain a model can achieve high verification scores, identifying shortcuts that bypass intended task solutions.", "motivation": "Reward hacking poses a significant threat where models exploit loopholes in reward functions to achieve high rewards without solving the intended task. This can be implicit (not verbalized in chain-of-thought), making it undetectable by current CoT monitors.", "method": "TRACE progressively truncates a model's chain-of-thought at various lengths, forces the model to answer, and measures the verifier-passing rate at each cutoff. It quantifies effort by measuring how early reasoning becomes sufficient to pass verification, with hacking models showing high passing rates with only small CoT fractions.", "result": "TRACE achieves over 65% gains over a 72B CoT monitor in math reasoning and over 30% gains over a 32B monitor in coding. It can also discover unknown loopholes during training.", "conclusion": "TRACE offers a scalable unsupervised approach for oversight where current monitoring methods prove ineffective, providing a robust defense against implicit reward hacking in reasoning models."}}
{"id": "2510.01265", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01265", "abs": "https://arxiv.org/abs/2510.01265", "authors": ["Ali Hatamizadeh", "Syeda Nahida Akter", "Shrimai Prabhumoye", "Jan Kautz", "Mostofa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro", "Yejin Choi"], "title": "RLP: Reinforcement as a Pretraining Objective", "comment": "RLP introduces a new paradigm for RL-based Pretraining", "summary": "The dominant paradigm for training large reasoning models starts with\npre-training using next-token prediction loss on vast amounts of data.\nReinforcement learning, while powerful in scaling reasoning, is introduced only\nas the very last phase of post-training, preceded by supervised fine-tuning.\nWhile dominant, is this an optimal way of training? In this paper, we present\nRLP, an information-driven reinforcement pretraining objective, that brings the\ncore spirit of reinforcement learning -- exploration -- to the last phase of\npretraining. The key idea is to treat chain-of-thought as an exploratory\naction, with rewards computed based on the information gain it provides for\npredicting future tokens. This training objective essentially encourages the\nmodel to think for itself before predicting what comes next, thus teaching an\nindependent thinking behavior earlier in the pretraining. More concretely, the\nreward signal measures the increase in log-likelihood of the next token when\nconditioning on both context and a sampled reasoning chain, compared to\nconditioning on context alone. This approach yields a verifier-free dense\nreward signal, allowing for efficient training for the full document stream\nduring pretraining. Specifically, RLP reframes reinforcement learning for\nreasoning as a pretraining objective on ordinary text, bridging the gap between\nnext-token prediction and the emergence of useful chain-of-thought reasoning.\nPretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an\neight-benchmark math-and-science suite by 19%. With identical post-training,\nthe gains compound, with the largest improvements on reasoning-heavy tasks such\nas AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2\nincreases the overall average from 42.81% to 61.32% and raises the average on\nscientific reasoning by 23%, demonstrating scalability across architectures and\nmodel sizes.", "AI": {"tldr": "RLP introduces reinforcement learning during pretraining by treating chain-of-thought as exploratory actions, rewarding information gain for predicting future tokens, which improves reasoning capabilities before post-training.", "motivation": "Current training paradigm delays reinforcement learning to post-training after supervised fine-tuning, which may not be optimal for developing reasoning skills early in training.", "method": "RLP uses information-driven reinforcement pretraining where chain-of-thought reasoning is treated as exploratory actions, with rewards based on the increase in next-token prediction likelihood when conditioning on reasoning chains versus context alone.", "result": "RLP pretraining on Qwen3-1.7B-Base improved overall average across eight math-and-science benchmarks by 19%, with largest gains on reasoning-heavy tasks like AIME25 and MMLU-Pro. On Nemotron-Nano-12B-v2, it increased overall average from 42.81% to 61.32% and scientific reasoning by 23%.", "conclusion": "RLP successfully bridges the gap between next-token prediction and chain-of-thought reasoning by introducing reinforcement learning during pretraining, enabling earlier development of independent thinking behavior and scalable improvements across architectures."}}
{"id": "2510.01763", "categories": ["eess.SP", "math.OC", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.01763", "abs": "https://arxiv.org/abs/2510.01763", "authors": ["Xiao Ding", "Enbin Song", "Dunbiao Niu", "Zhujun Cao", "Qingjiang Shi"], "title": "Exactly or Approximately Wasserstein Distributionally Robust Estimation According to Wasserstein Radii Being Small or Large", "comment": null, "summary": "This paper primarily considers the robust estimation problem under\nWasserstein distance constraints on the parameter and noise distributions in\nthe linear measurement model with additive noise, which can be formulated as an\ninfinite-dimensional nonconvex minimax problem. We prove that the existence of\na saddle point for this problem is equivalent to that for a finite-dimensional\nminimax problem, and give a counterexample demonstrating that the saddle point\nmay not exist. Motivated by this observation, we present a verifiable necessary\nand sufficient condition whose parameters can be derived from a convex problem\nand its dual. Additionally, we also introduce a simplified sufficient\ncondition, which intuitively indicates that when the Wasserstein radii are\nsmall enough, the saddle point always exists. In the absence of the saddle\npoint, we solve an finite-dimensional nonconvex minimax problem, obtained by\nrestricting the estimator to be linear. Its optimal value establishes an upper\nbound on the robust estimation problem, while its optimal solution yields a\nrobust linear estimator. Numerical experiments are also provided to validate\nour theoretical results.", "AI": {"tldr": "This paper studies robust estimation under Wasserstein distance constraints in linear models, showing saddle point existence conditions and providing a finite-dimensional approach when saddle points don't exist.", "motivation": "To address robust estimation problems with distributional uncertainty using Wasserstein distance constraints, particularly when saddle points may not exist in the infinite-dimensional minimax formulation.", "method": "Formulate as infinite-dimensional nonconvex minimax problem, prove equivalence to finite-dimensional problem, provide verifiable conditions for saddle point existence, and solve finite-dimensional nonconvex minimax problem for linear estimators when saddle points are absent.", "result": "Established necessary and sufficient conditions for saddle point existence, showed saddle points may not exist via counterexample, provided robust linear estimators as upper bounds, and validated results numerically.", "conclusion": "The paper provides theoretical foundations and practical methods for robust estimation under Wasserstein constraints, with verifiable conditions for saddle point existence and robust linear estimators as fallback solutions."}}
{"id": "2510.01498", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01498", "abs": "https://arxiv.org/abs/2510.01498", "authors": ["Yuxuan Ou", "Ning Bi", "Jiazhen Pan", "Jiancheng Yang", "Boliang Yu", "Usama Zidan", "Regent Lee", "Vicente Grau"], "title": "AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging", "comment": null, "summary": "While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic\naneurysms (AAA), the required iodinated contrast agents pose significant risks,\nincluding nephrotoxicity, patient allergies, and environmental harm. To reduce\ncontrast agent use, recent deep learning methods have focused on generating\nsynthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a\nmulti-stage pipeline that first generates images and then performs\nsegmentation, which leads to error accumulation and fails to leverage shared\nsemantic and anatomical structures. To address this, we propose a unified deep\nlearning framework that generates synthetic CECT images from NCCT scans while\nsimultaneously segmenting the aortic lumen and thrombus. Our approach\nintegrates conditional diffusion models (CDM) with multi-task learning,\nenabling end-to-end joint optimization of image synthesis and anatomical\nsegmentation. Unlike previous multitask diffusion models, our approach requires\nno initial predictions (e.g., a coarse segmentation mask), shares both encoder\nand decoder parameters across tasks, and employs a semi-supervised training\nstrategy to learn from scans with missing segmentation labels, a common\nconstraint in real-world clinical data. We evaluated our method on a cohort of\n264 patients, where it consistently outperformed state-of-the-art single-task\nand multi-stage models. For image synthesis, our model achieved a PSNR of 25.61\ndB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,\nit improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus\nDice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to\nmore accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm\nfrom 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to\nnnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.", "AI": {"tldr": "A unified deep learning framework that generates synthetic contrast-enhanced CT from non-contrast CT while simultaneously segmenting aortic lumen and thrombus, outperforming single-task and multi-stage approaches.", "motivation": "To reduce risks associated with iodinated contrast agents in CT scans (nephrotoxicity, allergies, environmental harm) by generating synthetic contrast-enhanced images from non-contrast scans, while addressing limitations of multi-stage pipelines that cause error accumulation.", "method": "Integrated conditional diffusion models with multi-task learning for end-to-end joint optimization of image synthesis and anatomical segmentation. Shares encoder and decoder parameters across tasks, uses semi-supervised training for missing labels, and requires no initial predictions.", "result": "Achieved PSNR of 25.61 dB for image synthesis (vs 23.80 dB single-task CDM), improved lumen Dice to 0.89 (from 0.87) and thrombus Dice to 0.53 (from 0.48). Reduced lumen diameter MAE to 4.19 mm (from 5.78 mm) and thrombus area error to 33.85% (from 41.45%).", "conclusion": "The proposed unified framework effectively reduces contrast agent use while improving both image synthesis quality and anatomical segmentation accuracy for abdominal aortic aneurysm assessment, demonstrating superior performance over existing approaches."}}
{"id": "2510.02021", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.02021", "abs": "https://arxiv.org/abs/2510.02021", "authors": ["Gian Marti", "Christoph Studer"], "title": "Joint Jammer Mitigation and Data Detection", "comment": "This work has not been submitted to the IEEE for possible\n  publication. The copyright remains with the authors, and this version will\n  remain publicly accessible", "summary": "Multi-antenna (or MIMO) processing is a promising solution to the problem of\njammer mitigation. Existing methods mitigate the jammer based on an estimate of\nits spatial signature that is acquired through a dedicated training phase. This\nstrategy has two main drawbacks: (i) it reduces the communication rate since no\ndata can be transmitted during the training phase and (ii) it can be evaded by\nsmart or multi-antenna jammers that do not transmit during the training phase\nor that dynamically change their subspace through time-varying beamforming. To\naddress these drawbacks, we propose Joint jammer Mitigation and data Detection\n(JMD), a novel paradigm for MIMO jammer mitigation. The core idea of JMD is to\nestimate and remove the jammer interference subspace jointly with detecting the\nlegitimate transmit data over multiple time slots. Doing so removes the need\nfor a dedicated and rate-reducing training period while being able to mitigate\nsmart and dynamic multi-antenna jammers. We provide two JMD-type algorithms,\nSANDMAN and MAED, that differ in the way they estimate the channels of the\nlegitimate transmitters and achieve different complexity-performance tradeoffs.\nExtensive simulations demonstrate the efficacy of JMD for jammer mitigation.", "AI": {"tldr": "JMD is a novel MIMO jammer mitigation method that jointly estimates jammer interference and detects legitimate data over multiple time slots, eliminating the need for dedicated training phases and enabling mitigation of smart, dynamic jammers.", "motivation": "Existing jammer mitigation methods require dedicated training phases that reduce communication rates and can be evaded by smart jammers that don't transmit during training or use time-varying beamforming.", "method": "Proposed Joint jammer Mitigation and data Detection (JMD) paradigm with two algorithms (SANDMAN and MAED) that jointly estimate jammer interference subspace and detect legitimate transmit data over multiple time slots without dedicated training.", "result": "Extensive simulations demonstrate JMD's efficacy for jammer mitigation, with SANDMAN and MAED providing different complexity-performance tradeoffs.", "conclusion": "JMD successfully addresses limitations of traditional jammer mitigation by removing the need for dedicated training while being effective against smart and dynamic multi-antenna jammers."}}
{"id": "2510.01375", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01375", "abs": "https://arxiv.org/abs/2510.01375", "authors": ["Humaid Ibrahim", "Nikolai Rozanov", "Marek Rei"], "title": "Fine-tuning with RAG for Improving LLM Learning of New Skills", "comment": "Under review at ICLR 2026", "summary": "Large language model (LLM) agents deployed for multi-step tasks frequently\nfail in predictable ways: attempting actions with unmet preconditions, issuing\nredundant commands, or mishandling environment constraints. While\nretrieval-augmented generation (RAG) can improve performance by providing\nruntime guidance, it requires maintaining external knowledge databases and adds\ncomputational overhead at every deployment. We propose a simple pipeline that\nconverts inference-time retrieval into learned competence through distillation.\nOur approach: (1) extracts compact, reusable hints from agent failures, (2)\nuses these hints to generate improved teacher trajectories via one-shot\nretrieval at episode start, and (3) trains student models on these trajectories\nwith hint strings removed, forcing internalization rather than memorization.\nAcross two interactive benchmarks, ALFWorld (household tasks) and WebShop\n(online shopping), distilled students consistently outperform baseline agents,\nachieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving\nWebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens\nthan retrieval-augmented teachers depending on the environment. The approach\ngeneralizes across model scales (7B/14B parameters) and agent architectures\n(ReAct/StateAct), demonstrating that retrieval benefits can be effectively\ninternalized through targeted fine-tuning without permanent runtime\ndependencies.", "AI": {"tldr": "The paper proposes a distillation pipeline that converts inference-time retrieval into learned competence, enabling LLM agents to internalize retrieval benefits through targeted fine-tuning without permanent runtime dependencies.", "motivation": "LLM agents frequently fail in predictable ways during multi-step tasks, and while RAG can help, it requires maintaining external knowledge databases and adds computational overhead at every deployment.", "method": "A three-step approach: (1) extract compact hints from agent failures, (2) generate improved teacher trajectories via one-shot retrieval at episode start, (3) train student models on these trajectories with hints removed to force internalization.", "result": "Distilled students consistently outperform baseline agents across ALFWorld (91% vs 79% success) and WebShop (72 vs 61 scores), while using 10-60% fewer tokens than retrieval-augmented teachers.", "conclusion": "Retrieval benefits can be effectively internalized through targeted fine-tuning without permanent runtime dependencies, generalizing across model scales and agent architectures."}}
{"id": "2510.01269", "categories": ["cs.LG", "cs.SY", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01269", "abs": "https://arxiv.org/abs/2510.01269", "authors": ["Rohan Vitthal Thorat", "Juhi Singh", "Rajdip Nayek"], "title": "Safe Reinforcement Learning-Based Vibration Control: Overcoming Training Risks with LQR Guidance", "comment": "Paper accepted for presentation at ICCMS 2025. The submission\n  includes 10 pages and 6 figures", "summary": "Structural vibrations induced by external excitations pose significant risks,\nincluding safety hazards for occupants, structural damage, and increased\nmaintenance costs. While conventional model-based control strategies, such as\nLinear Quadratic Regulator (LQR), effectively mitigate vibrations, their\nreliance on accurate system models necessitates tedious system identification.\nThis tedious system identification process can be avoided by using a model-free\nReinforcement learning (RL) method. RL controllers derive their policies solely\nfrom observed structural behaviour, eliminating the requirement for an explicit\nstructural model. For an RL controller to be truly model-free, its training\nmust occur on the actual physical system rather than in simulation. However,\nduring this training phase, the RL controller lacks prior knowledge and it\nexerts control force on the structure randomly, which can potentially harm the\nstructure. To mitigate this risk, we propose guiding the RL controller using a\nLinear Quadratic Regulator (LQR) controller. While LQR control typically relies\non an accurate structural model for optimal performance, our observations\nindicate that even an LQR controller based on an entirely incorrect model\noutperforms the uncontrolled scenario. Motivated by this finding, we introduce\na hybrid control framework that integrates both LQR and RL controllers. In this\napproach, the LQR policy is derived from a randomly selected model and its\nparameters. As this LQR policy does not require knowledge of the true or an\napproximate structural model the overall framework remains model-free. This\nhybrid approach eliminates dependency on explicit system models while\nminimizing exploration risks inherent in naive RL implementations. As per our\nknowledge, this is the first study to address the critical training safety\nchallenge of RL-based vibration control and provide a validated solution.", "AI": {"tldr": "A hybrid LQR-RL framework for safe vibration control that eliminates model dependency while preventing structural damage during RL training.", "motivation": "To address safety risks in RL-based vibration control during training phase, where random control actions can damage structures, while avoiding tedious system identification required by traditional model-based methods.", "method": "Proposes a hybrid control framework combining LQR and RL controllers, where LQR uses randomly selected model parameters to provide safe guidance during RL training, making the overall approach model-free.", "result": "The hybrid approach outperforms uncontrolled scenarios and provides safe training environment for RL controllers without requiring accurate structural models.", "conclusion": "This is the first validated solution addressing RL training safety in vibration control, offering a model-free approach that eliminates exploration risks while maintaining control effectiveness."}}
{"id": "2510.01776", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.01776", "abs": "https://arxiv.org/abs/2510.01776", "authors": ["Hadi Zayyani", "Mohammad Salman", "Felipe A. P. de Figueiredo", "Rausley A. A. de Souza"], "title": "Composite Generalized Quadratic Noise Modulation via Signal Addition: Towards Higher Dimensional Noise Modulations", "comment": null, "summary": "This letter proposes superposing two Generalized Quadratic Noise Modulators\n(GQNM) by simply adding their outputs. It creates a 16-ary noise modulator that\nresembles QAM modulators in classical communication. It modulates the\ninformation bits on four different means and four different variances. It could\nalso be applied to reach higher-order modulations than 16-ary schemes by adding\nthe outputs of more than two modulators, which is not discussed in detail in\nthis letter and left for future work. By selecting the parameters necessary for\nsatisfying the theoretical distinguishability conditions provided in the paper,\nwe can reach better performances in comparison to the Kirchhoff-Law Johnson\nNoise (KLJN) modulator and the GQNM modulator, which is verified by the\nsimulations. The better result in terms of smaller Bit Error Probability (BEP)\nis achieved by increasing the complexity in the modulator, the transmitter, and\nthe detectors in the receiver.", "AI": {"tldr": "Proposes a 16-ary noise modulator by superposing two Generalized Quadratic Noise Modulators (GQNM), creating a QAM-like scheme that modulates bits on four means and four variances, achieving better performance than KLJN and GQNM modulators.", "motivation": "To create higher-order noise modulators that resemble classical QAM modulators and achieve better performance than existing KLJN and GQNM modulators.", "method": "Superpose two GQNM modulators by adding their outputs to create a 16-ary noise modulator that modulates information on four different means and four different variances, with parameter selection based on theoretical distinguishability conditions.", "result": "Simulations verify better performance compared to KLJN and GQNM modulators, achieving smaller Bit Error Probability (BEP) through increased complexity in modulator, transmitter, and receiver detectors.", "conclusion": "The proposed superposition method successfully creates a 16-ary noise modulator with improved performance, and can be extended to higher-order modulations by adding more modulators, though this is left for future work."}}
{"id": "2510.01513", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.01513", "abs": "https://arxiv.org/abs/2510.01513", "authors": ["Basem Rizk", "Joel Walsh", "Mark Core", "Benjamin Nye"], "title": "From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding", "comment": null, "summary": "Analysis of multi-modal content can be tricky, computationally expensive, and\nrequire a significant amount of engineering efforts. Lots of work with\npre-trained models on static data is out there, yet fusing these opensource\nmodels and methods with complex data such as videos is relatively challenging.\nIn this paper, we present a framework that enables efficiently prototyping\npipelines for multi-modal content analysis. We craft a candidate recipe for a\npipeline, marrying a set of pre-trained models, to convert videos into a\ntemporal semi-structured data format. We translate this structure further to a\nframe-level indexed knowledge graph representation that is query-able and\nsupports continual learning, enabling the dynamic incorporation of new\ndomain-specific knowledge through an interactive medium.", "AI": {"tldr": "A framework for efficiently prototyping multi-modal content analysis pipelines that converts videos into temporal semi-structured data and knowledge graphs.", "motivation": "Multi-modal content analysis is complex, computationally expensive, and requires significant engineering effort. Existing pre-trained models are available but challenging to integrate with complex data like videos.", "method": "Develop a framework that combines pre-trained models into pipelines to convert videos into temporal semi-structured data, then translate this into query-able knowledge graphs with frame-level indexing that supports continual learning.", "result": "The framework enables dynamic incorporation of new domain-specific knowledge through an interactive medium and supports querying of the analyzed content.", "conclusion": "The presented framework addresses the challenges of multi-modal video analysis by providing an efficient prototyping approach that leverages existing models and enables knowledge graph representation with continual learning capabilities."}}
{"id": "2510.01398", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01398", "abs": "https://arxiv.org/abs/2510.01398", "authors": ["Yang Liu", "Zaid Abulawi", "Abhiram Garimidi", "Doyeong Lim"], "title": "Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents", "comment": null, "summary": "Modern engineering increasingly relies on vast datasets generated by\nexperiments and simulations, driving a growing demand for efficient, reliable,\nand broadly applicable modeling strategies. There is also heightened interest\nin developing data-driven approaches, particularly neural network models, for\neffective prediction and analysis of scientific datasets. Traditional\ndata-driven methods frequently involve extensive manual intervention, limiting\ntheir ability to scale effectively and generalize to diverse applications. In\nthis study, we propose an innovative pipeline utilizing Large Language Model\n(LLM) agents to automate data-driven modeling and analysis, with a particular\nemphasis on regression tasks. We evaluate two LLM-agent frameworks: a\nmulti-agent system featuring specialized collaborative agents, and a\nsingle-agent system based on the Reasoning and Acting (ReAct) paradigm. Both\nframeworks autonomously handle data preprocessing, neural network development,\ntraining, hyperparameter optimization, and uncertainty quantification (UQ). We\nvalidate our approach using a critical heat flux (CHF) prediction benchmark,\ninvolving approximately 25,000 experimental data points from the OECD/NEA\nbenchmark dataset. Results indicate that our LLM-agent-developed model\nsurpasses traditional CHF lookup tables and delivers predictive accuracy and UQ\non par with state-of-the-art Bayesian optimized deep neural network models\ndeveloped by human experts. These outcomes underscore the significant potential\nof LLM-based agents to automate complex engineering modeling tasks, greatly\nreducing human workload while meeting or exceeding existing standards of\npredictive performance.", "AI": {"tldr": "LLM agents automate data-driven modeling for regression tasks, achieving performance comparable to human experts while reducing manual intervention.", "motivation": "Traditional data-driven methods require extensive manual work and don't scale well. There's a need for automated, efficient modeling strategies for large scientific datasets.", "method": "Two LLM-agent frameworks: multi-agent system with specialized agents, and single-agent ReAct system. Both autonomously handle data preprocessing, neural network development, training, hyperparameter optimization, and uncertainty quantification.", "result": "LLM-agent-developed model outperforms traditional CHF lookup tables and achieves predictive accuracy and uncertainty quantification comparable to state-of-the-art Bayesian optimized deep neural networks developed by human experts.", "conclusion": "LLM-based agents show significant potential for automating complex engineering modeling tasks, reducing human workload while meeting or exceeding existing performance standards."}}
{"id": "2510.01271", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01271", "abs": "https://arxiv.org/abs/2510.01271", "authors": ["Arend Hintze", "Asadullah Najam", "Jory Schossau"], "title": "Identifying Information-Transfer Nodes in a Recurrent Neural Network Reveals Dynamic Representations", "comment": null, "summary": "Understanding the internal dynamics of Recurrent Neural Networks (RNNs) is\ncrucial for advancing their interpretability and improving their design. This\nstudy introduces an innovative information-theoretic method to identify and\nanalyze information-transfer nodes within RNNs, which we refer to as\n\\textit{information relays}. By quantifying the mutual information between\ninput and output vectors across nodes, our approach pinpoints critical pathways\nthrough which information flows during network operations. We apply this\nmethodology to both synthetic and real-world time series classification tasks,\nemploying various RNN architectures, including Long Short-Term Memory (LSTM)\nnetworks and Gated Recurrent Units (GRUs). Our results reveal distinct patterns\nof information relay across different architectures, offering insights into how\ninformation is processed and maintained over time. Additionally, we conduct\nnode knockout experiments to assess the functional importance of identified\nnodes, significantly contributing to explainable artificial intelligence by\nelucidating how specific nodes influence overall network behavior. This study\nnot only enhances our understanding of the complex mechanisms driving RNNs but\nalso provides a valuable tool for designing more robust and interpretable\nneural networks.", "AI": {"tldr": "This paper introduces an information-theoretic method to identify information-transfer nodes (information relays) in RNNs by quantifying mutual information between input and output vectors, revealing critical information pathways and their functional importance.", "motivation": "Understanding RNN internal dynamics is crucial for improving interpretability and design. Current methods lack systematic ways to identify critical information-transfer nodes that drive network behavior.", "method": "Developed an information-theoretic approach using mutual information quantification between input and output vectors across nodes. Applied to synthetic and real-world time series classification with LSTM and GRU architectures, including node knockout experiments.", "result": "Revealed distinct patterns of information relay across different RNN architectures, showing how information is processed and maintained over time. Node knockout experiments demonstrated functional importance of identified information relays.", "conclusion": "The method enhances understanding of RNN mechanisms and provides a valuable tool for designing more robust and interpretable neural networks, contributing significantly to explainable AI."}}
{"id": "2510.01778", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.01778", "abs": "https://arxiv.org/abs/2510.01778", "authors": ["Samaneh Motie", "Hadi Zayyani", "Mohammad Salman", "Hasan Abu Hilal"], "title": "Closed-form Single UAV-aided Emitter Localization and Trajectory Design Using Doppler and TOA Measurements", "comment": null, "summary": "In this paper, a single Unmanned-Aerial-Vehicle (UAV)-aided localization\nalgorithm which uses both Doppler and Time of Arrival (ToA) measurements is\npresented. In contrast to Doppler-based localization algorithms which are based\non non-convex functions, exploiting ToA measurements in a Least-Square (LS)\nDoppler-based cost function, leads to a quadratic convex function whose\nminimizer lies on a line. Utilizing the ToA measurements in addition to the\nlinear equation of minimizer, a closed form solution is obtained for the\nemitter location using a constrained LS optimization. In addition, a trajectory\ndesign of the UAV is provided which has also closed-form solution. Simulation\nexperiments demonstrate the effectiveness of the proposed algorithm in\ncomparison to some others in the literature.", "AI": {"tldr": "A UAV-aided localization algorithm using Doppler and ToA measurements with closed-form solution and trajectory design.", "motivation": "Existing Doppler-based localization algorithms use non-convex functions, making optimization difficult. Combining Doppler with ToA measurements enables convex optimization.", "method": "Use both Doppler and Time of Arrival measurements in a Least-Square cost function, transforming it into a quadratic convex function. Apply constrained LS optimization to obtain closed-form solution for emitter location. Also design UAV trajectory with closed-form solution.", "result": "Simulation experiments show the proposed algorithm is more effective than existing methods in the literature.", "conclusion": "The proposed algorithm successfully combines Doppler and ToA measurements to achieve convex optimization with closed-form solutions for both localization and trajectory design, outperforming existing approaches."}}
{"id": "2510.01524", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01524", "abs": "https://arxiv.org/abs/2510.01524", "authors": ["Viraj Prabhu", "Yutong Dai", "Matthew Fernandez", "Jing Gu", "Krithika Ramakrishnan", "Yanqi Luo", "Silvio Savarese", "Caiming Xiong", "Junnan Li", "Zeyuan Chen", "Ran Xu"], "title": "WALT: Web Agents that Learn Tools", "comment": null, "summary": "Web agents promise to automate complex browser tasks, but current methods\nremain brittle -- relying on step-by-step UI interactions and heavy LLM\nreasoning that break under dynamic layouts and long horizons. Humans, by\ncontrast, exploit website-provided functionality through high-level operations\nlike search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),\na framework that reverse-engineers latent website functionality into reusable\ninvocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust\nimplementations of automations already designed into websites -- spanning\ndiscovery (search, filter, sort), communication (post, comment, upvote), and\ncontent management (create, edit, delete). Tools abstract away low-level\nexecution: instead of reasoning about how to click and type, agents simply call\nsearch(query) or create(listing). This shifts the computational burden from\nfragile step-by-step reasoning to reliable tool invocation. On VisualWebArena\nand WebArena, WALT achieves higher success with fewer steps and less\nLLM-dependent reasoning, establishing a robust and generalizable paradigm for\nbrowser automation.", "AI": {"tldr": "WALT is a framework that reverse-engineers website functionality into reusable tools, enabling web agents to perform high-level operations like search and filter instead of fragile step-by-step UI interactions.", "motivation": "Current web agents are brittle and rely on step-by-step UI interactions that break under dynamic layouts and long horizons, unlike humans who use high-level website functionality.", "method": "WALT reverse-engineers latent website functionality into reusable invocable tools that abstract away low-level execution, spanning discovery, communication, and content management operations.", "result": "On VisualWebArena and WebArena, WALT achieves higher success with fewer steps and less LLM-dependent reasoning compared to traditional methods.", "conclusion": "WALT establishes a robust and generalizable paradigm for browser automation by shifting computational burden from fragile step-by-step reasoning to reliable tool invocation."}}
{"id": "2510.01409", "categories": ["cs.AI", "I.2.7; I.2.6; I.2.4"], "pdf": "https://arxiv.org/pdf/2510.01409", "abs": "https://arxiv.org/abs/2510.01409", "authors": ["Luca Cotti", "Idilio Drago", "Anisa Rula", "Devis Bianchini", "Federico Cerutti"], "title": "OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity Logs with Large Language Models", "comment": "20 pages, 6 tables, 7 figures", "summary": "System logs represent a valuable source of Cyber Threat Intelligence (CTI),\ncapturing attacker behaviors, exploited vulnerabilities, and traces of\nmalicious activity. Yet their utility is often limited by lack of structure,\nsemantic inconsistency, and fragmentation across devices and sessions.\nExtracting actionable CTI from logs therefore requires approaches that can\nreconcile noisy, heterogeneous data into coherent and interoperable\nrepresentations. We introduce OntoLogX, an autonomous Artificial Intelligence\n(AI) agent that leverages Large Language Models (LLMs) to transform raw logs\ninto ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a\nlightweight log ontology with Retrieval Augmented Generation (RAG) and\niterative correction steps, ensuring that generated KGs are syntactically and\nsemantically valid. Beyond event-level analysis, the system aggregates KGs into\nsessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level\nlog evidence to higher-level adversarial objectives. We evaluate OntoLogX on\nboth logs from a public benchmark and a real-world honeypot dataset,\ndemonstrating robust KG generation across multiple KGs backends and accurate\nmapping of adversarial activity to ATT&CK tactics. Results highlight the\nbenefits of retrieval and correction for precision and recall, the\neffectiveness of code-oriented models in structured log analysis, and the value\nof ontology-grounded representations for actionable CTI extraction.", "AI": {"tldr": "OntoLogX is an AI agent that uses LLMs to transform raw system logs into structured Knowledge Graphs using ontologies, enabling automated CTI extraction and MITRE ATT&CK tactic mapping.", "motivation": "System logs contain valuable CTI but are limited by lack of structure, semantic inconsistency, and fragmentation, making automated extraction challenging.", "method": "Integrates lightweight log ontology with RAG and iterative correction steps using LLMs to generate valid KGs, then aggregates KGs into sessions and predicts MITRE ATT&CK tactics.", "result": "Demonstrated robust KG generation across multiple backends and accurate mapping to ATT&CK tactics on both benchmark and real-world honeypot datasets, with improved precision and recall through retrieval and correction.", "conclusion": "Ontology-grounded representations enable actionable CTI extraction, with code-oriented LLMs being effective for structured log analysis and retrieval/correction improving KG quality."}}
{"id": "2510.01278", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01278", "abs": "https://arxiv.org/abs/2510.01278", "authors": ["Hengwei Zhao", "Zhengzhong Tu", "Zhuo Zheng", "Wei Wang", "Junjue Wang", "Rusty Feagin", "Wenzhe Jiao"], "title": "Noisy-Pair Robust Representation Alignment for Positive-Unlabeled Learning", "comment": null, "summary": "Positive-Unlabeled (PU) learning aims to train a binary classifier (positive\nvs. negative) where only limited positive data and abundant unlabeled data are\navailable. While widely applicable, state-of-the-art PU learning methods\nsubstantially underperform their supervised counterparts on complex datasets,\nespecially without auxiliary negatives or pre-estimated parameters (e.g., a\n14.26% gap on CIFAR-100 dataset). We identify the primary bottleneck as the\nchallenge of learning discriminative representations under unreliable\nsupervision. To tackle this challenge, we propose NcPU, a non-contrastive PU\nlearning framework that requires no auxiliary information. NcPU combines a\nnoisy-pair robust supervised non-contrastive loss (NoiSNCL), which aligns\nintra-class representations despite unreliable supervision, with a phantom\nlabel disambiguation (PLD) scheme that supplies conservative negative\nsupervision via regret-based label updates. Theoretically, NoiSNCL and PLD can\niteratively benefit each other from the perspective of the\nExpectation-Maximization framework. Empirically, extensive experiments\ndemonstrate that: (1) NoiSNCL enables simple PU methods to achieve competitive\nperformance; and (2) NcPU achieves substantial improvements over\nstate-of-the-art PU methods across diverse datasets, including challenging\ndatasets on post-disaster building damage mapping, highlighting its promise for\nreal-world applications. Code: Code will be open-sourced after review.", "AI": {"tldr": "NcPU is a non-contrastive PU learning framework that addresses representation learning challenges in Positive-Unlabeled classification by combining noisy-pair robust supervised non-contrastive loss with phantom label disambiguation, achieving state-of-the-art performance without requiring auxiliary negatives or pre-estimated parameters.", "motivation": "State-of-the-art PU learning methods significantly underperform supervised methods on complex datasets (e.g., 14.26% gap on CIFAR-100), primarily due to challenges in learning discriminative representations under unreliable supervision without auxiliary information.", "method": "Proposes NcPU framework with two key components: (1) NoiSNCL - a noisy-pair robust supervised non-contrastive loss that aligns intra-class representations despite unreliable supervision, and (2) PLD - phantom label disambiguation scheme that provides conservative negative supervision via regret-based label updates. Theoretically, these components iteratively benefit each other within an Expectation-Maximization framework.", "result": "Extensive experiments show: (1) NoiSNCL enables simple PU methods to achieve competitive performance, and (2) NcPU achieves substantial improvements over state-of-the-art PU methods across diverse datasets, including challenging real-world applications like post-disaster building damage mapping.", "conclusion": "NcPU effectively addresses the representation learning bottleneck in PU learning, demonstrating strong performance without requiring auxiliary negatives or pre-estimated parameters, making it promising for real-world applications where only limited positive and abundant unlabeled data are available."}}
{"id": "2510.01789", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.01789", "abs": "https://arxiv.org/abs/2510.01789", "authors": ["Ruixi Feng", "Weidong Mei", "Lele Lu", "Xin Wei", "Zhi Chen", "Zhen Gao", "Boyu Ning"], "title": "Performance Optimization for Movable Antenna Enhanced MISO-OFDM Systems", "comment": "Accepted to IEEE GLOBECOM 2025 Workshop", "summary": "Movable antenna (MA) technology offers a flexible approach to enhancing\nwireless channel conditions by adjusting antenna positions within a designated\nregion. While most existing works focus on narrowband MA systems, this paper\ninvestigates MA position optimization for an MA-enhanced multiple-input\nsingle-output (MISO) orthogonal frequency-division multiplexing (OFDM) system.\nThis problem appears to be particularly challenging due to the frequency-flat\nnature of MA positioning, which should accommodate the channel conditions\nacross different subcarriers. To overcome this challenge, we discretize the\nmovement region into a multitude of sampling points, thereby converting the\ncontinuous position optimization problem into a discrete point selection\nproblem. Although this problem is combinatorial, we develop an efficient\npartial enumeration algorithm to find the optimal solution using a\nbranch-and-bound framework, where a graph-theoretic method is incorporated to\neffectively prune suboptimal solutions. In the low signal-to-noise ratio (SNR)\nregime, a simplified graph-based algorithm is also proposed to obtain the\noptimal MA positions without the need for enumeration. Simulation results\nreveal that the proposed algorithm outperforms conventional fixed-position\nantennas (FPAs), while narrowband-based antenna position optimization can\nachieve near-optimal performance.", "AI": {"tldr": "This paper proposes efficient algorithms for optimizing movable antenna positions in OFDM systems to enhance wireless performance across multiple subcarriers.", "motivation": "Movable antenna technology can improve wireless channels by adjusting antenna positions, but existing works focus on narrowband systems. This paper addresses the challenge of optimizing MA positions for OFDM systems where antenna positioning must accommodate frequency-flat conditions across different subcarriers.", "method": "The authors discretize the movement region into sampling points, converting the continuous optimization into a discrete selection problem. They develop a partial enumeration algorithm using branch-and-bound with graph-theoretic pruning, and a simplified graph-based algorithm for low SNR regimes.", "result": "Simulation results show the proposed algorithm outperforms conventional fixed-position antennas, and narrowband-based optimization achieves near-optimal performance.", "conclusion": "The proposed algorithms effectively solve the MA position optimization problem for OFDM systems, demonstrating significant performance improvements over fixed antennas while maintaining computational efficiency."}}
{"id": "2510.01532", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01532", "abs": "https://arxiv.org/abs/2510.01532", "authors": ["Meilong Xu", "Xiaoling Hu", "Shahira Abousamra", "Chen Li", "Chao Chen"], "title": "MATCH: Multi-faceted Adaptive Topo-Consistency for Semi-Supervised Histopathology Segmentation", "comment": "20 pages, 6 figures. Accepted by NeurIPS 2025", "summary": "In semi-supervised segmentation, capturing meaningful semantic structures\nfrom unlabeled data is essential. This is particularly challenging in\nhistopathology image analysis, where objects are densely distributed. To\naddress this issue, we propose a semi-supervised segmentation framework\ndesigned to robustly identify and preserve relevant topological features. Our\nmethod leverages multiple perturbed predictions obtained through stochastic\ndropouts and temporal training snapshots, enforcing topological consistency\nacross these varied outputs. This consistency mechanism helps distinguish\nbiologically meaningful structures from transient and noisy artifacts. A key\nchallenge in this process is to accurately match the corresponding topological\nfeatures across the predictions in the absence of ground truth. To overcome\nthis, we introduce a novel matching strategy that integrates spatial overlap\nwith global structural alignment, minimizing discrepancies among predictions.\nExtensive experiments demonstrate that our approach effectively reduces\ntopological errors, resulting in more robust and accurate segmentations\nessential for reliable downstream analysis. Code is available at\n\\href{https://github.com/Melon-Xu/MATCH}{https://github.com/Melon-Xu/MATCH}.", "AI": {"tldr": "A semi-supervised segmentation framework that uses topological consistency across multiple perturbed predictions to preserve meaningful structures in histopathology images, with a novel matching strategy for feature alignment.", "motivation": "To address the challenge of capturing meaningful semantic structures from unlabeled data in histopathology image analysis, where objects are densely distributed and distinguishing biological structures from noise is difficult.", "method": "Leverages multiple perturbed predictions through stochastic dropouts and temporal training snapshots, enforcing topological consistency across outputs. Introduces a novel matching strategy that combines spatial overlap with global structural alignment to match topological features without ground truth.", "result": "Extensive experiments show the approach effectively reduces topological errors and produces more robust and accurate segmentations essential for reliable downstream analysis.", "conclusion": "The proposed framework successfully preserves biologically meaningful structures while filtering out transient artifacts, improving segmentation reliability in histopathology applications."}}
{"id": "2510.01427", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01427", "abs": "https://arxiv.org/abs/2510.01427", "authors": ["Sipeng Zhang", "Longfei Yun", "Zilong Wang", "Jingbo Shang", "Letian Peng"], "title": "A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining", "comment": null, "summary": "At the core of Deep Research is knowledge mining, the task of extracting\nstructured information from massive unstructured text in response to user\ninstructions. Large language models (LLMs) excel at interpreting such\ninstructions but are prohibitively expensive to deploy at scale, while\ntraditional pipelines of classifiers and extractors remain efficient yet\nbrittle and unable to generalize to new tasks. We introduce Falconer, a\ncollaborative framework that combines the agentic reasoning of LLMs with\nlightweight proxy models for scalable knowledge mining. In Falconer, LLMs act\nas planners, decomposing user instructions into executable pipelines, and as\nannotators, generating supervision to train small proxies. The framework\nunifies classification and extraction into two atomic operations, get label and\nget span, enabling a single instruction-following model to replace multiple\ntask-specific components. To evaluate the consistency between proxy models\nincubated by Falconer and annotations provided by humans and large models, we\nconstruct new benchmarks covering both planning and end-to-end execution.\nExperiments show that Falconer closely matches state-of-the-art LLMs in\ninstruction-following accuracy while reducing inference cost by up to 90% and\naccelerating large-scale knowledge mining by more than 20x, offering an\nefficient and scalable foundation for Deep Research.", "AI": {"tldr": "Falconer is a framework that combines LLMs as planners/annotators with lightweight proxy models for scalable knowledge mining, achieving LLM-level accuracy with 90% cost reduction and 20x speedup.", "motivation": "LLMs are too expensive for large-scale knowledge mining, while traditional pipelines are brittle and can't generalize to new tasks.", "method": "Uses LLMs as planners to decompose instructions and as annotators to train small proxy models, unifying classification and extraction into two atomic operations.", "result": "Matches state-of-the-art LLMs in accuracy while reducing inference cost by 90% and accelerating knowledge mining by 20x.", "conclusion": "Falconer provides an efficient and scalable foundation for Deep Research by combining LLM reasoning with lightweight proxy models."}}
{"id": "2510.01288", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01288", "abs": "https://arxiv.org/abs/2510.01288", "authors": ["Rui Melo", "Rui Abreu", "Corina S. Pasareanu"], "title": "Microsaccade-Inspired Probing: Positional Encoding Perturbations Reveal LLM Misbehaviours", "comment": "9 main pages, 13 appendix pages", "summary": "We draw inspiration from microsaccades, tiny involuntary eye movements that\nreveal hidden dynamics of human perception, to propose an analogous probing\nmethod for large language models (LLMs). Just as microsaccades expose subtle\nbut informative shifts in vision, we show that lightweight position encoding\nperturbations elicit latent signals that indicate model misbehaviour. Our\nmethod requires no fine-tuning or task-specific supervision, yet detects\nfailures across diverse settings including factuality, safety, toxicity, and\nbackdoor attacks. Experiments on multiple state-of-the-art LLMs demonstrate\nthat these perturbation-based probes surface misbehaviours while remaining\ncomputationally efficient. These findings suggest that pretrained LLMs already\nencode the internal evidence needed to flag their own failures, and that\nmicrosaccade-inspired interventions provide a pathway for detecting and\nmitigating undesirable behaviours.", "AI": {"tldr": "A microsaccade-inspired probing method uses lightweight position encoding perturbations to detect LLM misbehaviors without fine-tuning or supervision.", "motivation": "Inspired by how microsaccades reveal hidden dynamics in human perception, the authors aim to develop a similar probing method for LLMs to detect model failures.", "method": "Apply lightweight position encoding perturbations to LLMs and analyze the resulting latent signals to detect model misbehavior.", "result": "The method successfully detects failures across diverse settings including factuality, safety, toxicity, and backdoor attacks in multiple state-of-the-art LLMs.", "conclusion": "Pretrained LLMs already encode internal evidence to flag their own failures, and microsaccade-inspired interventions provide an efficient pathway for detecting and mitigating undesirable behaviors."}}
{"id": "2510.01540", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01540", "abs": "https://arxiv.org/abs/2510.01540", "authors": ["Jiamu Bai", "Xin Yu", "Meilong Xu", "Weitao Lu", "Xin Pan", "Kiwan Maeng", "Daniel Kifer", "Jian Wang", "Yu Wang"], "title": "Towards Better Optimization For Listwise Preference in Diffusion Models", "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) has proven effectiveness\nfor aligning text-to-image (T2I) diffusion models with human preferences.\nAlthough Direct Preference Optimization (DPO) is widely adopted for its\ncomputational efficiency and avoidance of explicit reward modeling, its\napplications to diffusion models have primarily relied on pairwise preferences.\nThe precise optimization of listwise preferences remains largely unaddressed.\nIn practice, human feedback on image preferences often contains implicit ranked\ninformation, which conveys more precise human preferences than pairwise\ncomparisons. In this work, we propose Diffusion-LPO, a simple and effective\nframework for Listwise Preference Optimization in diffusion models with\nlistwise data. Given a caption, we aggregate user feedback into a ranked list\nof images and derive a listwise extension of the DPO objective under the\nPlackett-Luce model. Diffusion-LPO enforces consistency across the entire\nranking by encouraging each sample to be preferred over all of its lower-ranked\nalternatives. We empirically demonstrate the effectiveness of Diffusion-LPO\nacross various tasks, including text-to-image generation, image editing, and\npersonalized preference alignment. Diffusion-LPO consistently outperforms\npairwise DPO baselines on visual quality and preference alignment.", "AI": {"tldr": "Diffusion-LPO is a listwise preference optimization framework for diffusion models that extends DPO to handle ranked image preferences, outperforming pairwise methods in text-to-image generation, editing, and personalization.", "motivation": "Current DPO applications in diffusion models rely on pairwise preferences, but human feedback often contains implicit ranked information that provides more precise preference signals than pairwise comparisons.", "method": "Proposes Diffusion-LPO framework that aggregates user feedback into ranked image lists and derives a listwise extension of DPO objective under the Plackett-Luce model, enforcing consistency across entire rankings by encouraging each sample to be preferred over lower-ranked alternatives.", "result": "Diffusion-LPO consistently outperforms pairwise DPO baselines across various tasks including text-to-image generation, image editing, and personalized preference alignment, achieving better visual quality and preference alignment.", "conclusion": "Listwise preference optimization through Diffusion-LPO provides more effective alignment of diffusion models with human preferences compared to pairwise approaches, leveraging richer ranked feedback information."}}
{"id": "2510.01432", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01432", "abs": "https://arxiv.org/abs/2510.01432", "authors": ["Sarath Sreedharan", "Kelsey Sikes", "Nathaniel Blanchard", "Lisa Mason", "Nikhil Krishnaswamy", "Jill Zarestky"], "title": "On the Role of Domain Experts in Creating Effective Tutoring Systems", "comment": "Accepted to AIED 2025 Blue Sky Track", "summary": "The role that highly curated knowledge, provided by domain experts, could\nplay in creating effective tutoring systems is often overlooked within the AI\nfor education community. In this paper, we highlight this topic by discussing\ntwo ways such highly curated expert knowledge could help in creating novel\neducational systems. First, we will look at how one could use explainable AI\n(XAI) techniques to automatically create lessons. Most existing XAI methods are\nprimarily aimed at debugging AI systems. However, we will discuss how one could\nuse expert specified rules about solving specific problems along with novel XAI\ntechniques to automatically generate lessons that could be provided to\nlearners. Secondly, we will see how an expert specified curriculum for learning\na target concept can help develop adaptive tutoring systems, that can not only\nprovide a better learning experience, but could also allow us to use more\nefficient algorithms to create these systems. Finally, we will highlight the\nimportance of such methods using a case study of creating a tutoring system for\npollinator identification, where such knowledge could easily be elicited from\nexperts.", "AI": {"tldr": "The paper explores how expert-curated knowledge can enhance AI tutoring systems through explainable AI techniques for automatic lesson generation and curriculum-based adaptive tutoring.", "motivation": "To highlight the overlooked role of domain expert knowledge in creating effective AI tutoring systems and demonstrate its potential benefits.", "method": "Proposes two approaches: 1) Using expert-specified rules with explainable AI (XAI) techniques to automatically generate lessons, and 2) Leveraging expert-designed curricula to develop adaptive tutoring systems with more efficient algorithms.", "result": "Presents a case study on creating a pollinator identification tutoring system where expert knowledge can be easily elicited and applied.", "conclusion": "Expert-curated knowledge plays a crucial role in developing novel and effective educational AI systems, particularly through XAI-based lesson generation and curriculum-driven adaptive tutoring."}}
{"id": "2510.01290", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01290", "abs": "https://arxiv.org/abs/2510.01290", "authors": ["Akshat Ramachandran", "Marina Neseem", "Charbel Sakr", "Rangharajan Venkatesan", "Brucek Khailany", "Tushar Krishna"], "title": "ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning Models", "comment": null, "summary": "The long-output context generation of large reasoning models enables extended\nchain of thought (CoT) but also drives rapid growth of the key-value (KV)\ncache, quickly overwhelming GPU memory. To address this challenge, we propose\nThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on\nthe observation that attention sparsity reveals distinct thought types with\nvarying importance within the CoT. It applies a hybrid quantization-eviction\nstrategy, assigning token precision by thought importance and progressively\nevicting tokens from less critical thoughts as reasoning trajectories evolve.\nFurthermore, to implement ThinKV, we design a kernel that extends\nPagedAttention to enable efficient reuse of evicted tokens' memory slots,\neliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill,\nGPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show\nthat ThinKV achieves near-lossless accuracy with less than 5% of the original\nKV cache, while improving performance with up to 5.8x higher inference\nthroughput over state-of-the-art baselines.", "AI": {"tldr": "ThinKV is a KV cache compression framework that uses attention sparsity to identify important thoughts in chain-of-thought reasoning, applying hybrid quantization-eviction to reduce KV cache to <5% of original size while maintaining near-lossless accuracy.", "motivation": "Large reasoning models generate long chain-of-thought outputs that cause KV cache to grow rapidly and overwhelm GPU memory, creating a memory bottleneck for inference.", "method": "Uses attention sparsity to identify thought types with varying importance, applies hybrid quantization-eviction strategy based on thought importance, and implements efficient kernel with PagedAttention extension for memory reuse.", "result": "Achieves near-lossless accuracy with <5% of original KV cache size, improves inference throughput up to 5.8x over state-of-the-art baselines on mathematics and coding benchmarks.", "conclusion": "ThinKV effectively addresses KV cache memory bottleneck through thought-adaptive compression, enabling efficient long-context reasoning without accuracy loss."}}
{"id": "2510.02000", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.02000", "abs": "https://arxiv.org/abs/2510.02000", "authors": ["Giusy Spacone", "Sebastian Frey", "Mattia Orlandi", "Pierangelo Maria Rapa", "Victor Kartsch", "Simone Benatti", "Luca Benini", "Andrea Cossettini"], "title": "Wearable and Ultra-Low-Power Fusion of EMG and A-Mode US for Hand-Wrist Kinematic Tracking", "comment": null, "summary": "Hand gesture recognition based on biosignals has shown strong potential for\ndeveloping intuitive human-machine interaction strategies that closely mimic\nnatural human behavior. In particular, sensor fusion approaches have gained\nattention for combining complementary information and overcoming the\nlimitations of individual sensing modalities, thereby enabling more robust and\nreliable systems. Among them, the fusion of surface electromyography (EMG) and\nA-mode ultrasound (US) is very promising. However, prior solutions rely on\npower-hungry platforms unsuitable for multi-day use and are limited to discrete\ngesture classification. In this work, we present an ultra-low-power (sub-50 mW)\nsystem for concurrent acquisition of 8-channel EMG and 4-channel A-mode US\nsignals, integrating two state-of-the-art platforms into fully wearable,\ndry-contact armbands. We propose a framework for continuous tracking of 23\ndegrees of freedom (DoFs), 20 for the hand and 3 for the wrist, using a\nkinematic glove for ground-truth labeling. Our method employs lightweight\nencoder-decoder architectures with multi-task learning to simultaneously\nestimate hand and wrist joint angles. Experimental results under realistic\nsensor repositioning conditions demonstrate that EMG-US fusion achieves a root\nmean squared error of $10.6^\\circ\\pm2.0^\\circ$, compared to\n$12.0^\\circ\\pm1^\\circ$ for EMG and $13.1^\\circ\\pm2.6^\\circ$ for US, and a R$^2$\nscore of $0.61\\pm0.1$, with $0.54\\pm0.03$ for EMG and $0.38\\pm0.20$ for US.", "AI": {"tldr": "An ultra-low-power system for continuous hand and wrist tracking using EMG-US fusion, achieving better accuracy than individual modalities under realistic conditions.", "motivation": "To overcome limitations of prior power-hungry systems limited to discrete gesture classification, enabling robust continuous hand tracking for intuitive human-machine interaction.", "method": "Developed ultra-low-power (sub-50 mW) system with 8-channel EMG and 4-channel A-mode US signals in wearable armbands, using lightweight encoder-decoder architectures with multi-task learning for 23 DoF tracking.", "result": "EMG-US fusion achieved RMSE of 10.6\u00b0\u00b12.0\u00b0 (vs 12.0\u00b0\u00b11\u00b0 for EMG, 13.1\u00b0\u00b12.6\u00b0 for US) and R\u00b2 score of 0.61\u00b10.1 (vs 0.54\u00b10.03 for EMG, 0.38\u00b10.20 for US) under realistic sensor repositioning.", "conclusion": "EMG-US fusion provides superior continuous hand tracking performance compared to individual modalities, enabling robust wearable systems for natural human-machine interaction."}}
{"id": "2510.01546", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01546", "abs": "https://arxiv.org/abs/2510.01546", "authors": ["Hanyu Wang", "Jiaming Han", "Ziyan Yang", "Qi Zhao", "Shanchuan Lin", "Xiangyu Yue", "Abhinav Shrivastava", "Zhenheng Yang", "Hao Chen"], "title": "Growing Visual Generative Capacity for Pre-Trained MLLMs", "comment": "Project page: https://hywang66.github.io/bridge/", "summary": "Multimodal large language models (MLLMs) extend the success of language\nmodels to visual understanding, and recent efforts have sought to build unified\nMLLMs that support both understanding and generation. However, constructing\nsuch models remains challenging: hybrid approaches combine continuous\nembeddings with diffusion or flow-based objectives, producing high-quality\nimages but breaking the autoregressive paradigm, while pure autoregressive\napproaches unify text and image prediction over discrete visual tokens but\noften face trade-offs between semantic alignment and pixel-level fidelity. In\nthis work, we present Bridge, a pure autoregressive unified MLLM that augments\npre-trained visual understanding models with generative ability through a\nMixture-of-Transformers architecture, enabling both image understanding and\ngeneration within a single next-token prediction framework. To further improve\nvisual generation fidelity, we propose a semantic-to-pixel discrete\nrepresentation that integrates compact semantic tokens with fine-grained pixel\ntokens, achieving strong language alignment and precise description of visual\ndetails with only a 7.9% increase in sequence length. Extensive experiments\nacross diverse multimodal benchmarks demonstrate that Bridge achieves\ncompetitive or superior results in both understanding and generation\nbenchmarks, while requiring less training data and reduced training time\ncompared to prior unified MLLMs.", "AI": {"tldr": "Bridge is a pure autoregressive unified MLLM that enables both image understanding and generation within a single next-token prediction framework using a Mixture-of-Transformers architecture and semantic-to-pixel discrete representation.", "motivation": "Current unified MLLMs face challenges: hybrid approaches break autoregressive paradigm, while pure autoregressive approaches trade off between semantic alignment and pixel-level fidelity.", "method": "Augments pre-trained visual understanding models with generative ability through Mixture-of-Transformers architecture and semantic-to-pixel discrete representation combining compact semantic tokens with fine-grained pixel tokens.", "result": "Achieves competitive/superior results in both understanding and generation benchmarks with only 7.9% sequence length increase, requiring less training data and reduced training time.", "conclusion": "Bridge successfully unifies visual understanding and generation in a pure autoregressive framework while maintaining strong semantic alignment and visual fidelity."}}
{"id": "2510.01444", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01444", "abs": "https://arxiv.org/abs/2510.01444", "authors": ["Rui Liu", "Dian Yu", "Tong Zheng", "Runpeng Dai", "Zongxia Li", "Wenhao Yu", "Zhenwen Liang", "Linfeng Song", "Haitao Mi", "Pratap Tokekar", "Dong Yu"], "title": "VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) improves reasoning in\nlarge language models (LLMs) but struggles with exploration, an issue that\nstill persists for multimodal LLMs (MLLMs). Current methods treat the visual\ninput as a fixed, deterministic condition, overlooking a critical source of\nambiguity and struggling to build policies robust to plausible visual\nvariations. We introduce $\\textbf{VOGUE (Visual Uncertainty Guided\nExploration)}$, a novel method that shifts exploration from the output (text)\nto the input (visual) space. By treating the image as a stochastic context,\nVOGUE quantifies the policy's sensitivity to visual perturbations using the\nsymmetric KL divergence between a \"raw\" and \"noisy\" branch, creating a direct\nsignal for uncertainty-aware exploration. This signal shapes the learning\nobjective via an uncertainty-proportional bonus, which, combined with a\ntoken-entropy bonus and an annealed sampling schedule, effectively balances\nexploration and exploitation. Implemented within GRPO on two model scales\n(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three\nvisual math benchmarks and 3.7% on three general-domain reasoning benchmarks,\nwhile simultaneously increasing pass@4 performance and mitigating the\nexploration decay commonly observed in RL fine-tuning. Our work shows that\ngrounding exploration in the inherent uncertainty of visual inputs is an\neffective strategy for improving multimodal reasoning.", "AI": {"tldr": "VOGUE introduces visual uncertainty guided exploration for MLLMs, treating images as stochastic contexts to improve reasoning by quantifying policy sensitivity to visual perturbations and balancing exploration-exploitation.", "motivation": "Current RLVR methods for MLLMs struggle with exploration and treat visual input as deterministic, overlooking visual ambiguity and failing to build robust policies against visual variations.", "method": "VOGUE quantifies policy sensitivity to visual perturbations using symmetric KL divergence between raw and noisy image branches, creates uncertainty-proportional bonus, combines with token-entropy bonus and annealed sampling schedule.", "result": "Implemented on Qwen2.5-VL-3B/7B, VOGUE boosts pass@1 accuracy by 2.6% on visual math benchmarks and 3.7% on general reasoning benchmarks, increases pass@4 performance, and mitigates exploration decay in RL fine-tuning.", "conclusion": "Grounded exploration in visual input uncertainty effectively improves multimodal reasoning in MLLMs."}}
{"id": "2510.01292", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01292", "abs": "https://arxiv.org/abs/2510.01292", "authors": ["Xiaobo Ma", "Hyunsoo Noh", "James Tokishi", "Ryan Hatch"], "title": "Network-Level Vehicle Delay Estimation at Heterogeneous Signalized Intersections", "comment": "arXiv admin note: text overlap with arXiv:2503.20113", "summary": "Accurate vehicle delay estimation is essential for evaluating the performance\nof signalized intersections and informing traffic management strategies. Delay\nreflects congestion levels and affects travel time reliability, fuel use, and\nemissions. Machine learning (ML) offers a scalable, cost-effective alternative;\nHowever, conventional models typically assume that training and testing data\nfollow the same distribution, an assumption that is rarely satisfied in\nreal-world applications. Variations in road geometry, signal timing, and driver\nbehavior across intersections often lead to poor generalization and reduced\nmodel accuracy. To address this issue, this study introduces a domain\nadaptation (DA) framework for estimating vehicle delays across diverse\nintersections. The framework separates data into source and target domains,\nextracts key traffic features, and fine-tunes the model using a small, labeled\nsubset from the target domain. A novel DA model, Gradient Boosting with\nBalanced Weighting (GBBW), reweights source data based on similarity to the\ntarget domain, improving adaptability. The framework is tested using data from\n57 heterogeneous intersections in Pima County, Arizona. Performance is\nevaluated against eight state-of-the-art ML regression models and seven\ninstance-based DA methods. Results demonstrate that the GBBW framework provides\nmore accurate and robust delay estimates. This approach supports more reliable\ntraffic signal optimization, congestion management, and performance-based\nplanning. By enhancing model transferability, the framework facilitates broader\ndeployment of machine learning techniques in real-world transportation systems.", "AI": {"tldr": "A domain adaptation framework using Gradient Boosting with Balanced Weighting (GBBW) improves vehicle delay estimation across diverse intersections by addressing distribution shifts between training and testing data.", "motivation": "Conventional ML models for vehicle delay estimation assume same distribution for training and testing data, which rarely holds in real-world due to variations in road geometry, signal timing, and driver behavior across intersections, leading to poor generalization.", "method": "A domain adaptation framework that separates data into source/target domains, extracts traffic features, and fine-tunes using small labeled target data. The novel GBBW model reweights source data based on similarity to target domain.", "result": "Tested on 57 heterogeneous intersections in Pima County, Arizona, GBBW outperformed 8 state-of-the-art ML regression models and 7 instance-based DA methods, providing more accurate and robust delay estimates.", "conclusion": "The framework enhances model transferability, supporting reliable traffic signal optimization, congestion management, and broader deployment of ML in transportation systems."}}
{"id": "2510.01547", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01547", "abs": "https://arxiv.org/abs/2510.01547", "authors": ["Akshay Bhagwan Sonawane", "Lena D. Swamikannan", "Lakshman Tamil"], "title": "Robust Classification of Oral Cancer with Limited Training Data", "comment": null, "summary": "Oral cancer ranks among the most prevalent cancers globally, with a\nparticularly high mortality rate in regions lacking adequate healthcare access.\nEarly diagnosis is crucial for reducing mortality; however, challenges persist\ndue to limited oral health programs, inadequate infrastructure, and a shortage\nof healthcare practitioners. Conventional deep learning models, while\npromising, often rely on point estimates, leading to overconfidence and reduced\nreliability. Critically, these models require large datasets to mitigate\noverfitting and ensure generalizability, an unrealistic demand in settings with\nlimited training data. To address these issues, we propose a hybrid model that\ncombines a convolutional neural network (CNN) with Bayesian deep learning for\noral cancer classification using small training sets. This approach employs\nvariational inference to enhance reliability through uncertainty\nquantification. The model was trained on photographic color images captured by\nsmartphones and evaluated on three distinct test datasets. The proposed method\nachieved 94% accuracy on a test dataset with a distribution similar to that of\nthe training data, comparable to traditional CNN performance. Notably, for\nreal-world photographic image data, despite limitations and variations\ndiffering from the training dataset, the proposed model demonstrated superior\ngeneralizability, achieving 88% accuracy on diverse datasets compared to 72.94%\nfor traditional CNNs, even with a smaller dataset. Confidence analysis revealed\nthat the model exhibits low uncertainty (high confidence) for correctly\nclassified samples and high uncertainty (low confidence) for misclassified\nsamples. These results underscore the effectiveness of Bayesian inference in\ndata-scarce environments in enhancing early oral cancer diagnosis by improving\nmodel reliability and generalizability.", "AI": {"tldr": "A hybrid CNN-Bayesian deep learning model for oral cancer classification that uses variational inference for uncertainty quantification, achieving better generalizability on small datasets compared to traditional CNNs.", "motivation": "Oral cancer has high mortality rates, especially in regions with limited healthcare access. Early diagnosis is crucial but challenging due to limited infrastructure and small datasets. Traditional deep learning models are overconfident and require large datasets, which are often unavailable.", "method": "Combines convolutional neural network (CNN) with Bayesian deep learning using variational inference for uncertainty quantification. Trained on smartphone-captured photographic color images with small training sets.", "result": "Achieved 94% accuracy on similar distribution test data (comparable to traditional CNN). On real-world photographic data with distribution differences, achieved 88% accuracy vs 72.94% for traditional CNNs. Model shows low uncertainty for correct classifications and high uncertainty for misclassifications.", "conclusion": "Bayesian inference effectively enhances model reliability and generalizability in data-scarce environments, improving early oral cancer diagnosis capabilities."}}
{"id": "2510.01474", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01474", "abs": "https://arxiv.org/abs/2510.01474", "authors": ["Bill Marino", "Rosco Hunter", "Zubair Jamali", "Marinos Emmanouil Kalpakos", "Mudra Kashyap", "Isaiah Hinton", "Alexa Hanson", "Maahum Nazir", "Christoph Schnabl", "Felix Steffek", "Hongkai Wen", "Nicholas D. Lane"], "title": "AIReg-Bench: Benchmarking Language Models That Assess AI Regulation Compliance", "comment": null, "summary": "As governments move to regulate AI, there is growing interest in using Large\nLanguage Models (LLMs) to assess whether or not an AI system complies with a\ngiven AI Regulation (AIR). However, there is presently no way to benchmark the\nperformance of LLMs at this task. To fill this void, we introduce AIReg-Bench:\nthe first benchmark dataset designed to test how well LLMs can assess\ncompliance with the EU AI Act (AIA). We created this dataset through a two-step\nprocess: (1) by prompting an LLM with carefully structured instructions, we\ngenerated 120 technical documentation excerpts (samples), each depicting a\nfictional, albeit plausible, AI system - of the kind an AI provider might\nproduce to demonstrate their compliance with AIR; (2) legal experts then\nreviewed and annotated each sample to indicate whether, and in what way, the AI\nsystem described therein violates specific Articles of the AIA. The resulting\ndataset, together with our evaluation of whether frontier LLMs can reproduce\nthe experts' compliance labels, provides a starting point to understand the\nopportunities and limitations of LLM-based AIR compliance assessment tools and\nestablishes a benchmark against which subsequent LLMs can be compared. The\ndataset and evaluation code are available at\nhttps://github.com/camlsys/aireg-bench.", "AI": {"tldr": "AIReg-Bench is the first benchmark dataset for evaluating LLMs' ability to assess compliance with the EU AI Act, created through LLM-generated technical documentation samples and expert legal annotations.", "motivation": "As governments regulate AI, there's growing interest in using LLMs to assess AI system compliance with regulations, but no existing benchmarks to evaluate LLM performance on this task.", "method": "Created dataset through two-step process: (1) prompting LLM to generate 120 technical documentation excerpts of fictional AI systems, (2) legal experts reviewed and annotated each sample for AI Act violations.", "result": "Developed AIReg-Bench dataset with expert-annotated compliance labels, providing a benchmark to evaluate frontier LLMs' ability to reproduce expert compliance assessments.", "conclusion": "AIReg-Bench establishes a foundation for understanding opportunities and limitations of LLM-based AI regulation compliance assessment tools and provides a benchmark for future LLM comparisons."}}
{"id": "2510.01296", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01296", "abs": "https://arxiv.org/abs/2510.01296", "authors": ["Emma McMillian", "Abhirup Banerjee", "Alfonso Bueno-Orovio"], "title": "From 2D to 3D, Deep Learning-based Shape Reconstruction in Magnetic Resonance Imaging: A Review", "comment": null, "summary": "Deep learning-based 3-dimensional (3D) shape reconstruction from\n2-dimensional (2D) magnetic resonance imaging (MRI) has become increasingly\nimportant in medical disease diagnosis, treatment planning, and computational\nmodeling. This review surveys the methodological landscape of 3D MRI\nreconstruction, focusing on 4 primary approaches: point cloud, mesh-based,\nshape-aware, and volumetric models. For each category, we analyze the current\nstate-of-the-art techniques, their methodological foundation, limitations, and\napplications across anatomical structures. We provide an extensive overview\nranging from cardiac to neurological to lung imaging. We also focus on the\nclinical applicability of models to diseased anatomy, and the influence of\ntheir training and testing data. We examine publicly available datasets,\ncomputational demands, and evaluation metrics. Finally, we highlight the\nemerging research directions including multimodal integration and\ncross-modality frameworks. This review aims to provide researchers with a\nstructured overview of current 3D reconstruction methodologies to identify\nopportunities for advancing deep learning towards more robust, generalizable,\nand clinically impactful solutions.", "AI": {"tldr": "A comprehensive review of deep learning methods for 3D shape reconstruction from 2D MRI, covering four main approaches: point cloud, mesh-based, shape-aware, and volumetric models, with analysis of techniques, limitations, applications, and future directions.", "motivation": "3D shape reconstruction from 2D MRI is crucial for medical diagnosis, treatment planning, and computational modeling, requiring a structured overview of current methodologies to advance deep learning towards more robust and clinically impactful solutions.", "method": "Survey and analysis of four primary reconstruction approaches: point cloud models, mesh-based models, shape-aware models, and volumetric models, examining their methodological foundations, limitations, applications across anatomical structures, computational demands, and evaluation metrics.", "result": "Provides extensive overview of 3D MRI reconstruction techniques across cardiac, neurological, and lung imaging, analyzing clinical applicability to diseased anatomy, influence of training/testing data, and available public datasets.", "conclusion": "Identifies emerging research directions including multimodal integration and cross-modality frameworks, aiming to guide researchers towards advancing deep learning for more generalizable and clinically impactful 3D reconstruction solutions."}}
{"id": "2510.01559", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01559", "abs": "https://arxiv.org/abs/2510.01559", "authors": ["Renrong Shao", "Wei Zhang", "Kangyang Luo", "Qin Li", "and Jun Wang"], "title": "Consistent Assistant Domains Transformer for Source-free Domain Adaptation", "comment": null, "summary": "Source-free domain adaptation (SFDA) aims to address the challenge of\nadapting to a target domain without accessing the source domain directly.\nHowever, due to the inaccessibility of source domain data, deterministic\ninvariable features cannot be obtained. Current mainstream methods primarily\nfocus on evaluating invariant features in the target domain that closely\nresemble those in the source domain, subsequently aligning the target domain\nwith the source domain. However, these methods are susceptible to hard samples\nand influenced by domain bias. In this paper, we propose a Consistent Assistant\nDomains Transformer for SFDA, abbreviated as CADTrans, which solves the issue\nby constructing invariable feature representations of domain consistency.\nConcretely, we develop an assistant domain module for CADTrans to obtain\ndiversified representations from the intermediate aggregated global attentions,\nwhich addresses the limitation of existing methods in adequately representing\ndiversity. Based on assistant and target domains, invariable feature\nrepresentations are obtained by multiple consistent strategies, which can be\nused to distinguish easy and hard samples. Finally, to align the hard samples\nto the corresponding easy samples, we construct a conditional multi-kernel max\nmean discrepancy (CMK-MMD) strategy to distinguish between samples of the same\ncategory and those of different categories. Extensive experiments are conducted\non various benchmarks such as Office-31, Office-Home, VISDA-C, and\nDomainNet-126, proving the significant performance improvements achieved by our\nproposed approaches. Code is available at\nhttps://github.com/RoryShao/CADTrans.git.", "AI": {"tldr": "CADTrans is a source-free domain adaptation method that constructs invariable feature representations using assistant domains and multiple consistency strategies to handle hard samples and domain bias.", "motivation": "Current SFDA methods struggle with hard samples and domain bias because they can't access source domain data to obtain deterministic invariable features.", "method": "Uses assistant domain module for diversified representations, multiple consistent strategies for invariable features, and conditional multi-kernel max mean discrepancy (CMK-MMD) to align hard samples with easy samples.", "result": "Extensive experiments on Office-31, Office-Home, VISDA-C, and DomainNet-126 benchmarks show significant performance improvements.", "conclusion": "CADTrans effectively addresses SFDA challenges by constructing domain-consistent invariable feature representations and handling hard samples through assistant domains and conditional alignment strategies."}}
{"id": "2510.01500", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01500", "abs": "https://arxiv.org/abs/2510.01500", "authors": ["Abhinav Madahar"], "title": "Lateral Tree-of-Thoughts Surpasses ToT by Incorporating Logically-Consistent, Low-Utility Candidates", "comment": null, "summary": "Modern deployments increasingly allocate large test-time compute (thousands\nof tokens or many node expansions) to boost reliability. Under such budgets,\nstandard Tree-of-Thoughts-style search exhibits two pathologies: breadth\nsaturation (additional samples mostly produce near-duplicates, so width stops\ngrowing) and depth myopia (noisy short-horizon utilities prune branches whose\npayoff appears after a few more steps). We propose Lateral Tree-of-Thoughts\n(LToT), a drop-in controller that separates utility from logical consistency\nand treats low-utility but consistent candidates as assets rather than waste.\nThe frontier is split into mainlines (high-utility candidates used for\nexploitation) and laterals (consistent, initially low-utility candidates that\nreceive short, cheap probes before judgment). LToT explores laterals via\nLateral Racing with Short-Circuit (LR--SC): a capped successive-halving race\nthat spreads tiny probes across a very wide lateral set, uses width-aware\nthresholds with repeat-to-confirm, and immediately promotes a branch once its\nenvelope clears the mainline bar; mainlines are kept intentionally narrow so\nsurplus compute is invested where width is cheap. We prove a pseudolinear\nlateral cost $\\Theta(N_0 \\log_{\\eta} N_0)$ with logarithmically many rungs\n(initial lateral width $N_0$; culling factor $\\eta>1$), in contrast to the\nexponential growth of uncapped mainlines. Empirical evaluations on benchmark\ntasks are in preparation and will be added in a future revision. In short, LToT\nturns large test-time budgets into principled diversity while preserving\npromotion discipline, mitigating saturation and myopia without inflating\ncompute.", "AI": {"tldr": "Lateral Tree-of-Thoughts (LToT) is a search controller that separates utility from logical consistency, treating low-utility but consistent candidates as assets rather than waste, using Lateral Racing with Short-Circuit to efficiently explore wide lateral sets while keeping mainlines narrow.", "motivation": "Standard Tree-of-Thoughts search suffers from breadth saturation (additional samples produce near-duplicates) and depth myopia (noisy short-horizon utilities prune branches with delayed payoffs) when given large test-time compute budgets.", "method": "LToT splits the frontier into mainlines (high-utility candidates for exploitation) and laterals (consistent but low-utility candidates). It explores laterals via Lateral Racing with Short-Circuit - a capped successive-halving race that spreads tiny probes across wide lateral sets, uses width-aware thresholds with repeat-to-confirm, and immediately promotes branches once they clear the mainline bar.", "result": "Theoretical analysis shows pseudolinear lateral cost \u0398(N\u2080 log_\u03b7 N\u2080) with logarithmically many rungs, contrasting with exponential growth of uncapped mainlines. Empirical evaluations are in preparation.", "conclusion": "LToT turns large test-time budgets into principled diversity while preserving promotion discipline, mitigating saturation and myopia without inflating compute."}}
{"id": "2510.01303", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01303", "abs": "https://arxiv.org/abs/2510.01303", "authors": ["Rishi Sonthalia", "Michael Murray", "Guido Mont\u00fafar"], "title": "Low Rank Gradients and Where to Find Them", "comment": null, "summary": "This paper investigates low-rank structure in the gradients of the training\nloss for two-layer neural networks while relaxing the usual isotropy\nassumptions on the training data and parameters. We consider a spiked data\nmodel in which the bulk can be anisotropic and ill-conditioned, we do not\nrequire independent data and weight matrices and we also analyze both the\nmean-field and neural-tangent-kernel scalings. We show that the gradient with\nrespect to the input weights is approximately low rank and is dominated by two\nrank-one terms: one aligned with the bulk data-residue , and another aligned\nwith the rank one spike in the input data. We characterize how properties of\nthe training data, the scaling regime and the activation function govern the\nbalance between these two components. Additionally, we also demonstrate that\nstandard regularizers, such as weight decay, input noise and Jacobian\npenalties, also selectively modulate these components. Experiments on synthetic\nand real data corroborate our theoretical predictions.", "AI": {"tldr": "This paper analyzes low-rank structure in neural network gradients under relaxed data isotropy assumptions, showing gradients are dominated by two rank-one components aligned with data bulk and spike.", "motivation": "To understand gradient structure in neural networks without requiring the usual isotropy assumptions on training data and parameters, examining how data properties affect gradient composition.", "method": "Theoretical analysis using spiked data model with anisotropic bulk, considering both mean-field and neural-tangent-kernel scalings, and examining various activation functions and regularizers.", "result": "Gradients are approximately low-rank and dominated by two rank-one terms: one aligned with bulk data-residue and another with the rank-one spike in input data. The balance between these components depends on data properties, scaling regime, and activation function.", "conclusion": "Standard regularizers selectively modulate the two gradient components, and theoretical predictions are validated through experiments on synthetic and real data."}}
{"id": "2510.02023", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.02023", "abs": "https://arxiv.org/abs/2510.02023", "authors": ["Ping Wang", "Zulin Wang", "Yuanhan Ni", "Qu Luo", "Yuanfang Ma", "Xiaosi Tian", "Pei Xiao"], "title": "A Secure Affine Frequency Division Multiplexing for Wireless Communication Systems", "comment": null, "summary": "Affine frequency division multiplexing (AFDM) has garnered significant\nattention due to its superior performance in high-mobility scenarios, coupled\nwith multiple waveform parameters that provide greater degrees of freedom for\nsystem design. This paper introduces a novel secure affine frequency division\nmultiplexing (SE-AFDM) system, which advances prior designs by dynamically\nvarying an AFDM pre-chirp parameter to enhance physical-layer security. In the\nSE-AFDM system, the pre-chirp parameter is dynamically generated from a\ncodebook controlled by a long-period pseudo-noise (LPPN) sequence. Instead of\napplying spreading in the data domain, our parameter-domain spreading approach\nprovides additional security while maintaining reliability and high spectrum\nefficiency. We also propose a synchronization framework to solve the problem of\nreliably and rapidly synchronizing the time-varying parameter in fast\ntime-varying channels. The theoretical derivations prove that unsynchronized\neavesdroppers cannot eliminate the nonlinear impact of the time-varying\nparameter and further provide useful guidance for codebook design. Simulation\nresults demonstrate the security advantages of the proposed SE-AFDM system in\nhigh-mobility scenarios, while our hardware prototype validates the\neffectiveness of the proposed synchronization framework.", "AI": {"tldr": "SE-AFDM enhances physical-layer security by dynamically varying AFDM pre-chirp parameters using LPPN sequences, providing parameter-domain spreading for security while maintaining reliability and spectrum efficiency.", "motivation": "To improve security in high-mobility scenarios where AFDM shows superior performance, by leveraging the multiple waveform parameters of AFDM for enhanced physical-layer security.", "method": "Dynamic variation of AFDM pre-chirp parameter using codebook controlled by LPPN sequence; parameter-domain spreading approach; synchronization framework for time-varying parameter in fast time-varying channels.", "result": "Theoretical proof that unsynchronized eavesdroppers cannot eliminate nonlinear impact of time-varying parameter; simulation shows security advantages in high-mobility scenarios; hardware prototype validates synchronization framework effectiveness.", "conclusion": "SE-AFDM successfully enhances physical-layer security through dynamic parameter variation while maintaining system reliability and efficiency, with proven security advantages in high-mobility environments."}}
{"id": "2510.01576", "categories": ["cs.CV", "cs.AI", "cs.HC", "I.2.m; H.5.2"], "pdf": "https://arxiv.org/pdf/2510.01576", "abs": "https://arxiv.org/abs/2510.01576", "authors": ["Ricardo Gonzalez Penuela", "Felipe Arias-Russi", "Victor Capriles"], "title": "Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations", "comment": "7 pages, 2 figure, 2 tables, CV4A11y Workshop at ICCV 2025", "summary": "Multimodal large language models (MLLMs) have been integrated into visual\ninterpretation applications to support Blind and Low Vision (BLV) users because\nof their accuracy and ability to provide rich, human-like interpretations.\nHowever, these applications often default to comprehensive, lengthy\ndescriptions regardless of context. This leads to inefficient exchanges, as\nusers must go through irrelevant details rather than receiving the specific\ninformation they are likely to seek. To deliver more contextually-relevant\ninformation, we developed a system that draws on historical BLV users\nquestions. When given an image, our system identifies similar past visual\ncontexts from the VizWiz-LF dataset and uses the associated questions to guide\nthe MLLM generate descriptions more relevant to BLV users. An evaluation with\nthree human labelers who revised 92 context-aware and context-free descriptions\nshowed that context-aware descriptions anticipated and answered users'\nquestions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of\ncomparisons (50 out of 92). Our paper reviews, and data analysis are publicly\navailable in a Github repository at\nhttps://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .", "AI": {"tldr": "MLLMs for BLV users often provide lengthy, irrelevant descriptions. This paper proposes using historical BLV questions from VizWiz-LF dataset to guide MLLMs to generate more contextually relevant descriptions.", "motivation": "Current MLLM applications for BLV users default to comprehensive descriptions regardless of context, leading to inefficient exchanges where users must filter through irrelevant details rather than getting specific information they need.", "method": "Developed a system that identifies similar past visual contexts from VizWiz-LF dataset and uses associated historical BLV user questions to guide MLLM generation of more relevant descriptions.", "result": "Context-aware descriptions anticipated and answered users' questions in 76.1% of cases (70/92) and were preferred in 54.4% of comparisons (50/92) over context-free descriptions.", "conclusion": "Using historical BLV user questions to guide MLLMs significantly improves the relevance and usefulness of visual descriptions for BLV users, providing more targeted information that anticipates their actual needs."}}
{"id": "2510.01528", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01528", "abs": "https://arxiv.org/abs/2510.01528", "authors": ["Daniel Zhao", "Abhilash Shankarampeta", "Lanxiang Hu", "Tajana Rosing", "Hao Zhang"], "title": "Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation", "comment": null, "summary": "We propose a novel method that leverages sparse autoencoders (SAEs) and\nclustering techniques to analyze the internal token representations of large\nlanguage models (LLMs) and guide generations in mathematical reasoning tasks.\nOur approach first trains an SAE to generate sparse vector representations for\ntraining tokens, then applies k-means clustering to construct a graph where\nvertices represent token clusters and weighted edges capture sequential token\ntransitions. Using this graph, we define an edge-weight based reward function\nto quantify adherence to established reasoning traces, thereby identifying\nexploitative reasoning trajectories. Additionally, we measure generation\ndiversity from clustering to assess the extent of exploration. Our findings\nindicate that balancing both exploitation and exploration is crucial for\nachieving high accuracy in mathematical reasoning tasks. During generation, the\nSAE can serve as a scalable reward model to guide generations, ensuring a\nbalanced trade-off between exploitation and exploration. This prevents extreme\nbehaviors in either direction, ultimately fostering a higher-quality reasoning\nprocess in LLMs.", "AI": {"tldr": "A method using sparse autoencoders and clustering to analyze LLM token representations and guide mathematical reasoning by balancing exploitation of known patterns with exploration of new approaches.", "motivation": "To improve mathematical reasoning in LLMs by understanding and guiding their internal reasoning processes through token representation analysis.", "method": "Train sparse autoencoders to generate sparse token representations, apply k-means clustering to build transition graphs, define reward functions based on edge weights to quantify reasoning adherence, and measure generation diversity from clustering.", "result": "The approach successfully identifies exploitative reasoning trajectories and assesses exploration extent, showing that balancing both is crucial for high accuracy in mathematical reasoning.", "conclusion": "Sparse autoencoders can serve as scalable reward models to guide LLM generations, ensuring optimal balance between exploitation and exploration for higher-quality mathematical reasoning."}}
{"id": "2510.01335", "categories": ["cs.LG", "cond-mat.dis-nn", "math.MG", "physics.data-an", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.01335", "abs": "https://arxiv.org/abs/2510.01335", "authors": ["Aritra Das", "Joseph T. Iosue", "Victor V. Albert"], "title": "Quantum-inspired Benchmark for Estimating Intrinsic Dimension", "comment": "19 figures, 35 pages", "summary": "Machine learning models can generalize well on real-world datasets. According\nto the manifold hypothesis, this is possible because datasets lie on a latent\nmanifold with small intrinsic dimension (ID). There exist many methods for ID\nestimation (IDE), but their estimates vary substantially. This warrants\nbenchmarking IDE methods on manifolds that are more complex than those in\nexisting benchmarks. We propose a Quantum-Inspired Intrinsic-dimension\nEstimation (QuIIEst) benchmark consisting of infinite families of topologically\nnon-trivial manifolds with known ID. Our benchmark stems from a quantum-optical\nmethod of embedding arbitrary homogeneous spaces while allowing for curvature\nmodification and additive noise. The IDE methods tested were generally less\naccurate on QuIIEst manifolds than on existing benchmarks under identical\nresource allocation. We also observe minimal performance degradation with\nincreasingly non-uniform curvature, underscoring the benchmark's inherent\ndifficulty. As a result of independent interest, we perform IDE on the fractal\nHofstadter's butterfly and identify which methods are capable of extracting the\neffective dimension of a space that is not a manifold.", "AI": {"tldr": "QuIIEst benchmark introduces complex manifolds with known intrinsic dimension for testing ID estimation methods, showing lower accuracy than existing benchmarks and minimal performance degradation with non-uniform curvature.", "motivation": "Existing ID estimation methods show varying results, and current benchmarks use simple manifolds. There's a need for more complex benchmarks to better evaluate these methods.", "method": "Proposed Quantum-Inspired Intrinsic-dimension Estimation (QuIIEst) benchmark using infinite families of topologically non-trivial manifolds from quantum-optical embedding methods, allowing curvature modification and additive noise.", "result": "ID estimation methods were generally less accurate on QuIIEst manifolds than existing benchmarks, with minimal performance degradation despite increasingly non-uniform curvature. Also successfully performed ID estimation on fractal Hofstadter's butterfly.", "conclusion": "QuIIEst provides a challenging benchmark that better reflects real-world complexity, revealing limitations of current ID estimation methods on non-trivial manifolds and non-manifold spaces."}}
{"id": "2510.02029", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.02029", "abs": "https://arxiv.org/abs/2510.02029", "authors": ["Haonan Si", "Zhaolin Wang", "Xiansheng Guo", "Jin Zhang", "Yuanwei Liu"], "title": "Joint DOA and Attitude Sensing Based on Tri-Polarized Continuous Aperture Array", "comment": "13 pages, 10 figures", "summary": "This paper investigates joint direction-of-arrival (DOA) and attitude sensing\nusing tri-polarized continuous aperture arrays (CAPAs). By employing\nelectromagnetic (EM) information theory, the spatially continuous received\nsignals in tri-polarized CAPA are modeled, thereby enabling accurate DOA and\nattitude estimation. To facilitate subspace decomposition for continuous\noperators, an equivalent continuous-discrete transformation technique is\ndeveloped. Moreover, both self- and cross-covariances of tri-polarized signals\nare exploited to construct a tri-polarized spectrum, significantly enhancing\nDOA estimation performance. Theoretical analyses reveal that the\nidentifiability of attitude information fundamentally depends on the\navailability of prior target snapshots. Accordingly, two attitude estimation\nalgorithms are proposed: one capable of estimating partial attitude information\nwithout prior knowledge, and the other achieving full attitude estimation when\nsuch knowledge is available. Numerical results demonstrate the feasibility and\nsuperiority of the proposed framework.", "AI": {"tldr": "This paper proposes a framework for joint direction-of-arrival (DOA) and attitude sensing using tri-polarized continuous aperture arrays, developing novel transformation techniques and exploiting tri-polarized signal covariances to enhance estimation performance.", "motivation": "To enable accurate joint DOA and attitude estimation by leveraging the spatial continuity and polarization diversity of tri-polarized continuous aperture arrays, addressing limitations in traditional discrete array approaches.", "method": "Uses electromagnetic information theory to model spatially continuous received signals, develops equivalent continuous-discrete transformation for subspace decomposition, exploits tri-polarized signal covariances to construct enhanced spectrum, and proposes two attitude estimation algorithms based on prior knowledge availability.", "result": "Numerical results demonstrate the framework's feasibility and superiority, showing enhanced DOA estimation performance and successful attitude estimation under different prior knowledge conditions.", "conclusion": "The proposed tri-polarized CAPA framework effectively enables joint DOA and attitude sensing, with theoretical analyses confirming attitude identifiability depends on prior target snapshots, and practical algorithms developed for both partial and full attitude estimation scenarios."}}
{"id": "2510.01582", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01582", "abs": "https://arxiv.org/abs/2510.01582", "authors": ["Krishna Teja Chitty-Venkata", "Murali Emani"], "title": "ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models", "comment": "Preprint", "summary": "We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the\ndevelopment of Vision Language Models (VLMs) with explicit reasoning\ncapabilities. Our dataset is built on 250,000 images from ImageNet21k dataset,\nproviding structured thinking tokens and corresponding answers. Our synthetic\ndataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and\nKimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of\nthinking-answer sequences, creating a resource for training and evaluating\nmultimodal reasoning models. We capture the step-by-step reasoning process of\nVLMs and the final descriptive answers. Our goal with this dataset is to enable\nthe development of more robust VLMs while contributing to the broader\nunderstanding of multimodal reasoning mechanisms. The dataset and evaluation\nbenchmarks will be publicly available to aid research in reasoning/thinking\nmultimodal VLMs.", "AI": {"tldr": "ImageNet-Think is a multimodal reasoning dataset built on 250,000 ImageNet21k images, featuring structured thinking tokens and answers generated by state-of-the-art VLMs to develop models with explicit reasoning capabilities.", "motivation": "To aid the development of Vision Language Models (VLMs) with explicit reasoning capabilities and contribute to understanding multimodal reasoning mechanisms.", "method": "Created synthetic dataset using GLM-4.1V-9B-Thinking and Kimi-VL-A3B-Thinking-2506 VLMs to generate step-by-step reasoning processes and final answers for each image, with two thinking-answer sequences per image.", "result": "A comprehensive dataset with 250,000 images, structured thinking tokens, and corresponding answers that captures multimodal reasoning processes.", "conclusion": "The dataset will be publicly available to support research in developing reasoning/thinking multimodal VLMs and advance understanding of multimodal reasoning."}}
{"id": "2510.01530", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01530", "abs": "https://arxiv.org/abs/2510.01530", "authors": ["Navapat Nananukul", "Yue Zhang", "Ryan Lee", "Eric Boxer", "Jonathan May", "Vibhav Giridhar Gogate", "Jay Pujara", "Mayank Kejriwal"], "title": "LOGicalThought: Logic-Based Ontological Grounding of LLMs for High-Assurance Reasoning", "comment": null, "summary": "High-assurance reasoning, particularly in critical domains such as law and\nmedicine, requires conclusions that are accurate, verifiable, and explicitly\ngrounded in evidence. This reasoning relies on premises codified from rules,\nstatutes, and contracts, inherently involving defeasible or non-monotonic logic\ndue to numerous exceptions, where the introduction of a single fact can\ninvalidate general rules, posing significant challenges. While large language\nmodels (LLMs) excel at processing natural language, their capabilities in\nstandard inference tasks do not translate to the rigorous reasoning required\nover high-assurance text guidelines. Core reasoning challenges within such\ntexts often manifest specific logical structures involving negation,\nimplication, and, most critically, defeasible rules and exceptions. In this\npaper, we propose a novel neurosymbolically-grounded architecture called\nLOGicalThought (LogT) that uses an advanced logical language and reasoner in\nconjunction with an LLM to construct a dual symbolic graph context and\nlogic-based context. These two context representations transform the problem\nfrom inference over long-form guidelines into a compact grounded evaluation.\nEvaluated on four multi-domain benchmarks against four baselines, LogT improves\noverall performance by 11.84% across all LLMs. Performance improves\nsignificantly across all three modes of reasoning: by up to +10.2% on negation,\n+13.2% on implication, and +5.5% on defeasible reasoning compared to the\nstrongest baseline.", "AI": {"tldr": "LOGicalThought (LogT) is a neurosymbolic architecture that combines LLMs with logical reasoning to handle defeasible logic in high-assurance domains, improving reasoning performance by 11.84% across benchmarks.", "motivation": "High-assurance domains like law and medicine require accurate, verifiable reasoning with explicit evidence grounding, but LLMs struggle with defeasible logic where exceptions can invalidate general rules.", "method": "LogT uses an advanced logical language and reasoner with LLMs to create dual symbolic graph and logic-based contexts, transforming long-form guidelines into compact grounded evaluation.", "result": "LogT improves overall performance by 11.84% across all LLMs, with significant gains in negation (+10.2%), implication (+13.2%), and defeasible reasoning (+5.5%) compared to strongest baselines.", "conclusion": "The neurosymbolic approach of LogT effectively addresses the challenges of defeasible reasoning in high-assurance domains by combining LLMs' language processing with formal logical reasoning."}}
{"id": "2510.01337", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01337", "abs": "https://arxiv.org/abs/2510.01337", "authors": ["S\u00e9bastien Lachapelle"], "title": "On the Identifiability of Latent Action Policies", "comment": "10 pages", "summary": "We study the identifiability of latent action policy learning (LAPO), a\nframework introduced recently to discover representations of actions from video\ndata. We formally describe desiderata for such representations, their\nstatistical benefits and potential sources of unidentifiability. Finally, we\nprove that an entropy-regularized LAPO objective identifies action\nrepresentations satisfying our desiderata, under suitable conditions. Our\nanalysis provides an explanation for why discrete action representations\nperform well in practice.", "AI": {"tldr": "The paper analyzes identifiability in latent action policy learning (LAPO), proving that an entropy-regularized objective can identify meaningful action representations under certain conditions.", "motivation": "To understand why discrete action representations work well in practice and to establish formal conditions for identifiability in LAPO frameworks.", "method": "Formal analysis of LAPO identifiability, describing desiderata for action representations and proving identifiability results for entropy-regularized objectives.", "result": "Proved that entropy-regularized LAPO objective identifies action representations satisfying the proposed desiderata under suitable conditions.", "conclusion": "The analysis provides theoretical explanation for the empirical success of discrete action representations in practice."}}
{"id": "2510.02103", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.02103", "abs": "https://arxiv.org/abs/2510.02103", "authors": ["Kawon Han", "Kaitao Meng", "Christos Masouros"], "title": "Sensing-Secure ISAC: Ambiguity Function Engineering for Impairing Unauthorized Sensing", "comment": "15 pages, 12 figures, accepted to IEEE Transactions on Wireless\n  Communications", "summary": "The deployment of integrated sensing and communication (ISAC) brings along\nunprecedented vulnerabilities to authorized sensing, necessitating the\ndevelopment of secure solutions. Sensing parameters are embedded within the\ntarget-reflected signal leaked to unauthorized passive radar sensing\neavesdroppers (Eve), implying that they can silently extract sensory\ninformation without prior knowledge of the information data. To overcome this\nlimitation, we propose a sensing-secure ISAC framework that ensures secure\ntarget detection and estimation for the legitimate system, while obfuscating\nunauthorized sensing without requiring any prior knowledge of Eve. By\nintroducing artificial imperfections into the ambiguity function (AF) of ISAC\nsignals, we introduce artificial targets into Eve's range profile which\nincrease its range estimation ambiguity. In contrast, the legitimate sensing\nreceiver (Alice) can suppress these AF artifacts using mismatched filtering,\nalbeit at the expense of signal-to-noise ratio (SNR) loss. Employing an OFDM\nsignal, a structured subcarrier power allocation scheme is designed to shape\nthe secure autocorrelation function (ACF), inserting periodic peaks to mislead\nEve's range estimation and degrade target detection performance. To quantify\nthe sensing security, we introduce peak sidelobe level (PSL) and integrated\nsidelobe level (ISL) as key performance metrics. Then, we analyze the three-way\ntrade-offs between communication, legitimate sensing, and sensing security,\nhighlighting the impact of the proposed sensing-secure ISAC signaling on system\nperformance. We formulate a convex optimization problem to maximize ISAC\nperformance while guaranteeing a certain sensing security level. Numerical\nresults validate the effectiveness of the proposed sensing-secure ISAC\nsignaling, demonstrating its ability to degrade Eve's target estimation while\npreserving Alice's performance.", "AI": {"tldr": "A sensing-secure ISAC framework that protects legitimate sensing from eavesdroppers by introducing artificial imperfections in ambiguity functions, using OFDM subcarrier power allocation to mislead unauthorized sensing while maintaining legitimate performance.", "motivation": "Integrated sensing and communication (ISAC) systems are vulnerable to unauthorized passive radar eavesdroppers who can silently extract sensory information from target-reflected signals without prior knowledge.", "method": "Introduce artificial imperfections into ISAC signal ambiguity functions using structured OFDM subcarrier power allocation to create artificial targets that mislead eavesdroppers' range estimation, while legitimate receivers use mismatched filtering to suppress artifacts.", "result": "The proposed framework successfully degrades Eve's target estimation performance while preserving Alice's legitimate sensing performance, with numerical results validating effectiveness.", "conclusion": "The sensing-secure ISAC signaling provides a viable solution to protect authorized sensing from eavesdroppers, with demonstrated trade-offs between communication, legitimate sensing, and sensing security that can be optimized through convex optimization."}}
{"id": "2510.01608", "categories": ["cs.CV", "eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.01608", "abs": "https://arxiv.org/abs/2510.01608", "authors": ["Roman Jacome", "Romario Gualdr\u00f3n-Hurtado", "Leon Suarez", "Henry Arguello"], "title": "NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems", "comment": "25 pages, 12 tables, 10 figures. Accepted to NeurIPS 2025", "summary": "Imaging inverse problems aims to recover high-dimensional signals from\nundersampled, noisy measurements, a fundamentally ill-posed task with infinite\nsolutions in the null-space of the sensing operator. To resolve this ambiguity,\nprior information is typically incorporated through handcrafted regularizers or\nlearned models that constrain the solution space. However, these priors\ntypically ignore the task-specific structure of that null-space. In this work,\nwe propose \\textit{Non-Linear Projections of the Null-Space} (NPN), a novel\nclass of regularization that, instead of enforcing structural constraints in\nthe image domain, promotes solutions that lie in a low-dimensional projection\nof the sensing matrix's null-space with a neural network. Our approach has two\nkey advantages: (1) Interpretability: by focusing on the structure of the\nnull-space, we design sensing-matrix-specific priors that capture information\northogonal to the signal components that are fundamentally blind to the sensing\nprocess. (2) Flexibility: NPN is adaptable to various inverse problems,\ncompatible with existing reconstruction frameworks, and complementary to\nconventional image-domain priors. We provide theoretical guarantees on\nconvergence and reconstruction accuracy when used within plug-and-play methods.\nEmpirical results across diverse sensing matrices demonstrate that NPN priors\nconsistently enhance reconstruction fidelity in various imaging inverse\nproblems, such as compressive sensing, deblurring, super-resolution, computed\ntomography, and magnetic resonance imaging, with plug-and-play methods,\nunrolling networks, deep image prior, and diffusion models.", "AI": {"tldr": "NPN is a novel regularization method that uses neural networks to project solutions into low-dimensional null-space structures of sensing matrices, improving interpretability and flexibility in imaging inverse problems.", "motivation": "Traditional priors ignore the task-specific structure of the null-space in imaging inverse problems, leading to suboptimal solutions that don't leverage information orthogonal to what the sensing process can capture.", "method": "Proposes Non-Linear Projections of the Null-Space (NPN) - a regularization approach that promotes solutions lying in low-dimensional projections of the sensing matrix's null-space using neural networks, rather than enforcing constraints in the image domain.", "result": "NPN priors consistently enhance reconstruction fidelity across diverse imaging inverse problems including compressive sensing, deblurring, super-resolution, CT, and MRI, working effectively with various reconstruction frameworks.", "conclusion": "NPN provides interpretable and flexible regularization by focusing on null-space structure, is compatible with existing methods, and offers theoretical guarantees for convergence and accuracy in plug-and-play approaches."}}
{"id": "2510.01531", "categories": ["cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01531", "abs": "https://arxiv.org/abs/2510.01531", "authors": ["Djengo Cyun-Jyun Fang", "Tsung-Wei Ke"], "title": "Information Seeking for Robust Decision Making under Partial Observability", "comment": "The project page is available at https://infoseekerllm.github.io", "summary": "Explicit information seeking is essential to human problem-solving in\npractical environments characterized by incomplete information and noisy\ndynamics. When the true environmental state is not directly observable, humans\nseek information to update their internal dynamics and inform future\ndecision-making. Although existing Large Language Model (LLM) planning agents\nhave addressed observational uncertainty, they often overlook discrepancies\nbetween their internal dynamics and the actual environment. We introduce\nInformation Seeking Decision Planner (InfoSeeker), an LLM decision-making\nframework that integrates task-oriented planning with information seeking to\nalign internal dynamics and make optimal decisions under uncertainty in both\nagent observations and environmental dynamics. InfoSeeker prompts an LLM to\nactively gather information by planning actions to validate its understanding,\ndetect environmental changes, or test hypotheses before generating or revising\ntask-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark\nsuite featuring partially observable environments with incomplete observations\nand uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%\nabsolute performance gain over prior methods without sacrificing sample\nefficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms\nbaselines on established benchmarks such as robotic manipulation and web\nnavigation. These findings underscore the importance of tightly integrating\nplanning and information seeking for robust behavior in partially observable\nenvironments. The project page is available at https://infoseekerllm.github.io", "AI": {"tldr": "InfoSeeker is an LLM planning framework that integrates task-oriented planning with active information seeking to handle uncertainty in both observations and environmental dynamics, achieving 74% performance gain over prior methods.", "motivation": "Existing LLM planning agents address observational uncertainty but overlook discrepancies between their internal dynamics and the actual environment, limiting their effectiveness in partially observable environments with noisy dynamics.", "method": "InfoSeeker prompts LLMs to actively gather information by planning actions to validate understanding, detect environmental changes, or test hypotheses before generating or revising task-oriented plans.", "result": "InfoSeeker achieves 74% absolute performance gain over prior methods without sacrificing sample efficiency, and generalizes well across LLMs and established benchmarks including robotic manipulation and web navigation.", "conclusion": "Tight integration of planning and information seeking is crucial for robust behavior in partially observable environments, and InfoSeeker demonstrates the effectiveness of this approach."}}
{"id": "2510.01345", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01345", "abs": "https://arxiv.org/abs/2510.01345", "authors": ["Akhlaqur Rahman Sabby", "Yi Sui", "Tongzi Wu", "Jesse C. Cresswell", "Ga Wu"], "title": "Self-Supervised Representation Learning as Mutual Information Maximization", "comment": null, "summary": "Self-supervised representation learning (SSRL) has demonstrated remarkable\nempirical success, yet its underlying principles remain insufficiently\nunderstood. While recent works attempt to unify SSRL methods by examining their\ninformation-theoretic objectives or summarizing their heuristics for preventing\nrepresentation collapse, architectural elements like the predictor network,\nstop-gradient operation, and statistical regularizer are often viewed as\nempirically motivated additions. In this paper, we adopt a first-principles\napproach and investigate whether the learning objective of an SSRL algorithm\ndictates its possible optimization strategies and model design choices. In\nparticular, by starting from a variational mutual information (MI) lower bound,\nwe derive two training paradigms, namely Self-Distillation MI (SDMI) and Joint\nMI (JMI), each imposing distinct structural constraints and covering a set of\nexisting SSRL algorithms. SDMI inherently requires alternating optimization,\nmaking stop-gradient operations theoretically essential. In contrast, JMI\nadmits joint optimization through symmetric architectures without such\ncomponents. Under the proposed formulation, predictor networks in SDMI and\nstatistical regularizers in JMI emerge as tractable surrogates for the MI\nobjective. We show that many existing SSRL methods are specific instances or\napproximations of these two paradigms. This paper provides a theoretical\nexplanation behind the choices of different architectural components of\nexisting SSRL methods, beyond heuristic conveniences.", "AI": {"tldr": "This paper provides a theoretical framework that explains why different self-supervised learning methods use specific architectural components like stop-gradient operations and predictor networks, showing they emerge naturally from two fundamental optimization paradigms derived from mutual information objectives.", "motivation": "To understand the theoretical principles behind self-supervised representation learning methods, moving beyond empirical observations and heuristics to explain why specific architectural choices are necessary.", "method": "The authors derive two training paradigms (Self-Distillation MI and Joint MI) from a variational mutual information lower bound, showing how different optimization strategies lead to distinct architectural requirements.", "result": "The paper demonstrates that stop-gradient operations are theoretically essential for SDMI, while JMI allows symmetric architectures without such components. Predictor networks and statistical regularizers emerge as tractable surrogates for mutual information objectives.", "conclusion": "The work provides a unified theoretical explanation for architectural choices in self-supervised learning, showing they are not just empirical conveniences but naturally follow from fundamental optimization principles."}}
{"id": "2510.02108", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02108", "abs": "https://arxiv.org/abs/2510.02108", "authors": ["Jinshuo Zhang", "Yafei Wang", "Xinping Yi", "Wenjin Wang", "Shi Jin", "Symeon Chatzinotas", "Bj\u00f6rn Ottersten"], "title": "Unlocking Symbol-Level Precoding Efficiency Through Tensor Equivariant Neural Network", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Although symbol-level precoding (SLP) based on constructive interference (CI)\nexploitation offers performance gains, its high complexity remains a\nbottleneck. This paper addresses this challenge with an end-to-end deep\nlearning (DL) framework with low inference complexity that leverages the\nstructure of the optimal SLP solution in the closed-form and its inherent\ntensor equivariance (TE), where TE denotes that a permutation of the input\ninduces the corresponding permutation of the output. Building upon the\ncomputationally efficient model-based formulations, as well as their known\nclosed-form solutions, we analyze their relationship with linear precoding (LP)\nand investigate the corresponding optimality condition. We then construct a\nmapping from the problem formulation to the solution and prove its TE, based on\nwhich the designed networks reveal a specific parameter-sharing pattern that\ndelivers low computational complexity and strong generalization. Leveraging\nthese, we propose the backbone of the framework with an attention-based TE\nmodule, achieving linear computational complexity. Furthermore, we demonstrate\nthat such a framework is also applicable to imperfect CSI scenarios, where we\ndesign a TE-based network to map the CSI, statistics, and symbols to auxiliary\nvariables. Simulation results show that the proposed framework captures\nsubstantial performance gains of optimal SLP, while achieving an approximately\n80-times speedup over conventional methods and maintaining strong\ngeneralization across user numbers and symbol block lengths.", "AI": {"tldr": "This paper proposes an end-to-end deep learning framework for symbol-level precoding that achieves near-optimal performance with 80x speedup over conventional methods, leveraging tensor equivariance and attention mechanisms.", "motivation": "Symbol-level precoding based on constructive interference offers performance gains but suffers from high computational complexity, which limits practical implementation.", "method": "The authors develop a deep learning framework that leverages the structure of optimal SLP solutions, tensor equivariance properties, and attention-based modules to achieve linear computational complexity while maintaining performance.", "result": "Simulation results show the proposed framework captures substantial performance gains of optimal SLP while achieving approximately 80-times speedup over conventional methods and maintaining strong generalization across different user numbers and symbol block lengths.", "conclusion": "The proposed end-to-end deep learning framework successfully addresses the complexity bottleneck of symbol-level precoding while preserving performance advantages, making it suitable for practical wireless communication systems."}}
{"id": "2510.01618", "categories": ["cs.CV", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2510.01618", "abs": "https://arxiv.org/abs/2510.01618", "authors": ["Zijun Li", "Jinchang Zhang", "Ming Zhang", "Guoyu Lu"], "title": "Automated Genomic Interpretation via Concept Bottleneck Models for Medical Robotics", "comment": null, "summary": "We propose an automated genomic interpretation module that transforms raw DNA\nsequences into actionable, interpretable decisions suitable for integration\ninto medical automation and robotic systems. Our framework combines Chaos Game\nRepresentation (CGR) with a Concept Bottleneck Model (CBM), enforcing\npredictions to flow through biologically meaningful concepts such as GC\ncontent, CpG density, and k mer motifs. To enhance reliability, we incorporate\nconcept fidelity supervision, prior consistency alignment, KL distribution\nmatching, and uncertainty calibration. Beyond accurate classification of HIV\nsubtypes across both in-house and LANL datasets, our module delivers\ninterpretable evidence that can be directly validated against biological\npriors. A cost aware recommendation layer further translates predictive outputs\ninto decision policies that balance accuracy, calibration, and clinical\nutility, reducing unnecessary retests and improving efficiency. Extensive\nexperiments demonstrate that the proposed system achieves state of the art\nclassification performance, superior concept prediction fidelity, and more\nfavorable cost benefit trade-offs compared to existing baselines. By bridging\nthe gap between interpretable genomic modeling and automated decision-making,\nthis work establishes a reliable foundation for robotic and clinical automation\nin genomic medicine.", "AI": {"tldr": "An automated genomic interpretation system that transforms DNA sequences into interpretable decisions using Chaos Game Representation and Concept Bottleneck Model, with applications in HIV subtype classification and clinical automation.", "motivation": "To bridge the gap between interpretable genomic modeling and automated decision-making for medical automation and robotic systems, providing reliable genomic interpretation that can be directly validated against biological priors.", "method": "Combines Chaos Game Representation (CGR) with Concept Bottleneck Model (CBM), enforcing predictions through biologically meaningful concepts (GC content, CpG density, k-mer motifs). Includes concept fidelity supervision, prior consistency alignment, KL distribution matching, and uncertainty calibration. Adds cost-aware recommendation layer for decision policies.", "result": "Achieves state-of-the-art HIV subtype classification across in-house and LANL datasets, superior concept prediction fidelity, and more favorable cost-benefit trade-offs compared to existing baselines. Reduces unnecessary retests and improves efficiency.", "conclusion": "Establishes a reliable foundation for robotic and clinical automation in genomic medicine by providing interpretable genomic modeling that bridges the gap to automated decision-making."}}
{"id": "2510.01544", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01544", "abs": "https://arxiv.org/abs/2510.01544", "authors": ["Shaoan Xie", "Lingjing Kong", "Xiangchen Song", "Xinshuai Dong", "Guangyi Chen", "Eric P. Xing", "Kun Zhang"], "title": "Step-Aware Policy Optimization for Reasoning in Diffusion Large Language Models", "comment": null, "summary": "Diffusion language models (dLLMs) offer a promising, non-autoregressive\nparadigm for text generation, yet training them for complex reasoning remains a\nkey challenge. Current reinforcement learning approaches often rely on sparse,\noutcome-based rewards, which can reinforce flawed reasoning paths that lead to\ncoincidentally correct answers. We argue that this stems from a fundamental\nmismatch with the natural structure of reasoning. We first propose a\ntheoretical framework that formalizes complex problem solving as a hierarchical\nselection process, where an intractable global constraint is decomposed into a\nseries of simpler, localized logical steps. This framework provides a\nprincipled foundation for algorithm design, including theoretical insights into\nthe identifiability of this latent reasoning structure. Motivated by this\ntheory, we identify unstructured refinement -- a failure mode where a model's\niterative steps do not contribute meaningfully to the solution -- as a core\ndeficiency in existing methods. We then introduce Step-Aware Policy\nOptimization (SAPO), a novel RL algorithm that aligns the dLLM's denoising\nprocess with the latent reasoning hierarchy. By using a process-based reward\nfunction that encourages incremental progress, SAPO guides the model to learn\nstructured, coherent reasoning paths. Our empirical results show that this\nprincipled approach significantly improves performance on challenging reasoning\nbenchmarks and enhances the interpretability of the generation process.", "AI": {"tldr": "SAPO is a novel RL algorithm that aligns diffusion language models with latent reasoning hierarchies using process-based rewards, improving complex reasoning performance and interpretability.", "motivation": "Current RL approaches for diffusion language models rely on sparse outcome-based rewards, which can reinforce flawed reasoning paths that coincidentally lead to correct answers, creating a mismatch with natural reasoning structure.", "method": "Proposes Step-Aware Policy Optimization (SAPO) - an RL algorithm that uses process-based reward functions to guide the denoising process, encouraging incremental progress and structured reasoning paths.", "result": "Empirical results show significant performance improvements on challenging reasoning benchmarks and enhanced interpretability of the generation process.", "conclusion": "The principled approach of aligning diffusion language models with latent reasoning hierarchies through process-based rewards effectively addresses unstructured refinement and improves complex reasoning capabilities."}}
{"id": "2510.01349", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01349", "abs": "https://arxiv.org/abs/2510.01349", "authors": ["Hannah Lawrence", "Elyssa Hofgard", "Vasco Portilheiro", "Yuxuan Chen", "Tess Smidt", "Robin Walters"], "title": "To Augment or Not to Augment? Diagnosing Distributional Symmetry Breaking", "comment": "A short version of this paper appeared at the ICLR AI4Mat workshop in\n  April 2025", "summary": "Symmetry-aware methods for machine learning, such as data augmentation and\nequivariant architectures, encourage correct model behavior on all\ntransformations (e.g. rotations or permutations) of the original dataset. These\nmethods can improve generalization and sample efficiency, under the assumption\nthat the transformed datapoints are highly probable, or \"important\", under the\ntest distribution. In this work, we develop a method for critically evaluating\nthis assumption. In particular, we propose a metric to quantify the amount of\nanisotropy, or symmetry-breaking, in a dataset, via a two-sample neural\nclassifier test that distinguishes between the original dataset and its\nrandomly augmented equivalent. We validate our metric on synthetic datasets,\nand then use it to uncover surprisingly high degrees of alignment in several\nbenchmark point cloud datasets. We show theoretically that distributional\nsymmetry-breaking can actually prevent invariant methods from performing\noptimally even when the underlying labels are truly invariant, as we show for\ninvariant ridge regression in the infinite feature limit. Empirically, we find\nthat the implication for symmetry-aware methods is dataset-dependent:\nequivariant methods still impart benefits on some anisotropic datasets, but not\nothers. Overall, these findings suggest that understanding equivariance -- both\nwhen it works, and why -- may require rethinking symmetry biases in the data.", "AI": {"tldr": "The paper proposes a metric to quantify dataset anisotropy (symmetry-breaking) and shows that symmetry-aware methods may not always be optimal even when labels are invariant, with empirical results varying by dataset.", "motivation": "To critically evaluate the assumption that transformed datapoints are highly probable under test distributions, which underlies symmetry-aware methods like data augmentation and equivariant architectures.", "method": "Developed a two-sample neural classifier test to distinguish between original datasets and their randomly augmented equivalents, measuring dataset anisotropy.", "result": "Uncovered surprisingly high degrees of alignment in benchmark point cloud datasets; showed theoretically that symmetry-breaking can prevent invariant methods from optimal performance; found equivariant methods' benefits are dataset-dependent.", "conclusion": "Understanding equivariance requires rethinking symmetry biases in data, as symmetry-aware methods don't always provide benefits and their effectiveness depends on dataset characteristics."}}
{"id": "2510.01623", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01623", "abs": "https://arxiv.org/abs/2510.01623", "authors": ["Angen Ye", "Zeyu Zhang", "Boyuan Wang", "Xiaofeng Wang", "Dapeng Zhang", "Zheng Zhu"], "title": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models aim to unify perception, language\nunderstanding, and action generation, offering strong cross-task and\ncross-scene generalization with broad impact on embodied AI. However, current\nVLA models often lack explicit step-by-step reasoning, instead emitting final\nactions without considering affordance constraints or geometric relations.\nTheir post-training pipelines also rarely reinforce reasoning quality, relying\nprimarily on supervised fine-tuning with weak reward design. To address these\nchallenges, we present VLA-R1, a reasoning-enhanced VLA that integrates\nReinforcement Learning from Verifiable Rewards (RLVR) with Group Relative\nPolicy Optimization (GRPO) to systematically optimize both reasoning and\nexecution. Specifically, we design an RLVR-based post-training strategy with\nverifiable rewards for region alignment, trajectory consistency, and output\nformatting, thereby strengthening reasoning robustness and execution accuracy.\nMoreover, we develop VLA-CoT-13K, a high-quality dataset that provides\nchain-of-thought supervision explicitly aligned with affordance and trajectory\nannotations. Furthermore, extensive evaluations on in-domain, out-of-domain,\nsimulation, and real-robot platforms demonstrate that VLA-R1 achieves superior\ngeneralization and real-world performance compared to prior VLA methods. We\nplan to release the model, code, and dataset following the publication of this\nwork. Code: https://github.com/GigaAI-research/VLA-R1. Website:\nhttps://gigaai-research.github.io/VLA-R1.", "AI": {"tldr": "VLA-R1 is a reasoning-enhanced Vision-Language-Action model that integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO) to systematically optimize both reasoning and execution, achieving superior generalization and real-world performance.", "motivation": "Current VLA models lack explicit step-by-step reasoning, emit final actions without considering affordance constraints or geometric relations, and have post-training pipelines that rarely reinforce reasoning quality with weak reward design.", "method": "Integrates RLVR with GRPO for systematic optimization, uses verifiable rewards for region alignment, trajectory consistency, and output formatting, and develops VLA-CoT-13K dataset with chain-of-thought supervision aligned with affordance and trajectory annotations.", "result": "Extensive evaluations on in-domain, out-of-domain, simulation, and real-robot platforms demonstrate superior generalization and real-world performance compared to prior VLA methods.", "conclusion": "VLA-R1 addresses key limitations of current VLA models by enhancing reasoning capabilities through verifiable rewards and systematic optimization, achieving improved performance across diverse evaluation settings."}}
{"id": "2510.01569", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01569", "abs": "https://arxiv.org/abs/2510.01569", "authors": ["Yubin Kim", "Taehan Kim", "Eugene Park", "Chunjong Park", "Cynthia Breazeal", "Daniel McDuff", "Hae Won Park"], "title": "InvThink: Towards AI Safety via Inverse Reasoning", "comment": null, "summary": "We present InvThink, a simple yet powerful approach that gives large language\nmodels (LLMs) the capability of inverse thinking: reasoning through failure\nmodes before generating responses. Unlike existing safety alignment methods\nthat optimize directly for safe response, InvThink instructs models to 1)\nenumerate potential harms, 2) analyze their consequences, and 3) generate safe\noutputs that proactively avoid these risks. Our method reveals three key\nfindings: (i) safety improvements show stronger scaling with model size\ncompared to existing safety methods. (ii) InvThink mitigates safety tax; by\ntraining models to systematically consider failure modes, it preserves general\nreasoning capabilities on standard benchmarks. (iii) beyond general safety\ntasks, InvThink excels in high-stakes domains including external-facing\n(medicine, finance, law) and agentic (blackmail, murder) risk scenarios,\nachieving up to 15.7% reduction in harmful responses compared to baseline\nmethods like SafetyPrompt. We further implement InvThink via supervised\nfine-tuning, and reinforcement learning across three LLM families. These\nresults suggest that inverse reasoning provides a scalable and generalizable\npath toward safer, more capable language models.", "AI": {"tldr": "InvThink enables LLMs to reason through potential failure modes before generating responses, improving safety while preserving general capabilities.", "motivation": "To address safety concerns in LLMs by developing a method that proactively identifies and avoids potential harms before generating responses, rather than just optimizing for safe outputs.", "method": "Instructs models to: 1) enumerate potential harms, 2) analyze their consequences, and 3) generate safe outputs that proactively avoid these risks. Implemented via supervised fine-tuning and reinforcement learning across three LLM families.", "result": "Achieves up to 15.7% reduction in harmful responses compared to baseline methods like SafetyPrompt. Shows stronger safety scaling with model size, mitigates safety tax (preserves general reasoning), and excels in high-stakes domains including medicine, finance, law, and agentic risk scenarios.", "conclusion": "Inverse reasoning provides a scalable and generalizable path toward safer, more capable language models."}}
{"id": "2510.01365", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2510.01365", "abs": "https://arxiv.org/abs/2510.01365", "authors": ["Maedeh Saberi", "Amir Barati Farimani", "Safa Jamali"], "title": "RheOFormer: A generative transformer model for simulation of complex fluids and flows", "comment": "8 pages, 5 figures. Submitted to PNAS", "summary": "The ability to model mechanics of soft materials under flowing conditions is\nkey in designing and engineering processes and materials with targeted\nproperties. This generally requires solution of internal stress tensor, related\nto the deformation tensor through nonlinear and history-dependent constitutive\nmodels. Traditional numerical methods for non-Newtonian fluid dynamics often\nsuffer from prohibitive computational demands and poor scalability to new\nproblem instances. Developments in data-driven methods have mitigated some\nlimitations but still require retraining across varied physical conditions. In\nthis work, we introduce Rheological Operator Transformer (RheOFormer), a\ngenerative operator learning method leveraging self-attention to efficiently\nlearn different spatial interactions and features of complex fluid flows. We\nbenchmark RheOFormer across a range of different viscometric and\nnon-viscometric flows with different types of viscoelastic and\nelastoviscoplastic mechanics in complex domains against ground truth solutions.\nOur results demonstrate that RheOFormer can accurately learn both scalar and\ntensorial nonlinear mechanics of different complex fluids and predict the\nspatio-temporal evolution of their flows, even when trained on limited\ndatasets. Its strong generalization capabilities and computational efficiency\nestablish RheOFormer as a robust neural surrogate for accelerating predictive\ncomplex fluid simulations, advancing data-driven experimentation, and enabling\nreal-time process optimization across a wide range of applications.", "AI": {"tldr": "RheOFormer is a generative operator learning method using self-attention to efficiently model complex fluid flows, accurately predicting spatio-temporal evolution of nonlinear mechanics with strong generalization and computational efficiency.", "motivation": "Traditional methods for non-Newtonian fluid dynamics suffer from high computational demands and poor scalability, while existing data-driven methods require retraining across varied conditions.", "method": "Leverages self-attention in a generative operator learning framework to learn spatial interactions and features of complex fluid flows across viscometric and non-viscometric flows.", "result": "Accurately learns both scalar and tensorial nonlinear mechanics of different complex fluids, predicts spatio-temporal flow evolution even with limited training data, and demonstrates strong generalization capabilities.", "conclusion": "RheOFormer serves as a robust neural surrogate for accelerating predictive complex fluid simulations, advancing data-driven experimentation, and enabling real-time process optimization across applications."}}
{"id": "2510.01640", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01640", "abs": "https://arxiv.org/abs/2510.01640", "authors": ["Yifan Zhao", "Liangchen Li", "Yuqi Zhou", "Kai Wang", "Yan Liang", "Juyong Zhang"], "title": "Joint Deblurring and 3D Reconstruction for Macrophotography", "comment": "Accepted to Pacific Graphics 2025. To be published in Computer\n  Graphics Forum", "summary": "Macro lens has the advantages of high resolution and large magnification, and\n3D modeling of small and detailed objects can provide richer information.\nHowever, defocus blur in macrophotography is a long-standing problem that\nheavily hinders the clear imaging of the captured objects and high-quality 3D\nreconstruction of them. Traditional image deblurring methods require a large\nnumber of images and annotations, and there is currently no multi-view 3D\nreconstruction method for macrophotography. In this work, we propose a joint\ndeblurring and 3D reconstruction method for macrophotography. Starting from\nmulti-view blurry images captured, we jointly optimize the clear 3D model of\nthe object and the defocus blur kernel of each pixel. The entire framework\nadopts a differentiable rendering method to self-supervise the optimization of\nthe 3D model and the defocus blur kernel. Extensive experiments show that from\na small number of multi-view images, our proposed method can not only achieve\nhigh-quality image deblurring but also recover high-fidelity 3D appearance.", "AI": {"tldr": "Proposes a joint deblurring and 3D reconstruction method for macrophotography that simultaneously optimizes clear 3D models and defocus blur kernels using differentiable rendering.", "motivation": "Macrophotography suffers from severe defocus blur that hinders clear imaging and high-quality 3D reconstruction, while traditional methods require extensive data and annotations.", "method": "Uses differentiable rendering to jointly optimize clear 3D models and per-pixel defocus blur kernels from multi-view blurry images in a self-supervised manner.", "result": "Achieves high-quality image deblurring and recovers high-fidelity 3D appearance from a small number of multi-view images.", "conclusion": "The proposed framework effectively solves the defocus blur problem in macrophotography and enables high-quality 3D reconstruction from limited blurry inputs."}}
{"id": "2510.01586", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01586", "abs": "https://arxiv.org/abs/2510.01586", "authors": ["Zhenyu Pan", "Yiting Zhang", "Zhuo Liu", "Yolo Yunlong Tang", "Zeliang Zhang", "Haozheng Luo", "Yuwei Han", "Jianshu Zhang", "Dennis Wu", "Hong-Yu Chen", "Haoran Lu", "Haoyang Fang", "Manling Li", "Chenliang Xu", "Philip S. Yu", "Han Liu"], "title": "AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning", "comment": null, "summary": "LLM-based multi-agent systems excel at planning, tool use, and role\ncoordination, but their openness and interaction complexity also expose them to\njailbreak, prompt-injection, and adversarial collaboration. Existing defenses\nfall into two lines: (i) self-verification that asks each agent to pre-filter\nunsafe instructions before execution, and (ii) external guard modules that\npolice behaviors. The former often underperforms because a standalone agent\nlacks sufficient capacity to detect cross-agent unsafe chains and\ndelegation-induced risks; the latter increases system overhead and creates a\nsingle-point-of-failure-once compromised, system-wide safety collapses, and\nadding more guards worsens cost and complexity. To solve these challenges, we\npropose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning\nframework that internalizes safety into task agents. Rather than relying on\nexternal guards, AdvEvo-MARL jointly optimizes attackers (which synthesize\nevolving jailbreak prompts) and defenders (task agents trained to both\naccomplish their duties and resist attacks) in adversarial learning\nenvironments. To stabilize learning and foster cooperation, we introduce a\npublic baseline for advantage estimation: agents within the same functional\ngroup share a group-level mean-return baseline, enabling lower-variance updates\nand stronger intra-group coordination. Across representative attack scenarios,\nAdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas\nbaselines reach up to 38.33%, while preserving-and sometimes improving-task\naccuracy (up to +3.67% on reasoning tasks). These results show that safety and\nutility can be jointly improved without relying on extra guard agents or added\nsystem overhead.", "AI": {"tldr": "AdvEvo-MARL is a co-evolutionary multi-agent reinforcement learning framework that internalizes safety into task agents through adversarial training, eliminating the need for external guard modules while maintaining task performance.", "motivation": "Existing defenses for LLM-based multi-agent systems either rely on underperforming self-verification or external guard modules that create system overhead and single-point-of-failure risks.", "method": "Jointly optimizes attackers (synthesizing jailbreak prompts) and defenders (task agents trained to resist attacks) in adversarial learning environments with a public baseline for advantage estimation that enables group-level coordination.", "result": "Consistently keeps attack-success rate below 20% (vs. 38.33% in baselines) while preserving and sometimes improving task accuracy (up to +3.67% on reasoning tasks).", "conclusion": "Safety and utility can be jointly improved without relying on extra guard agents or added system overhead through adversarial co-evolutionary training."}}
{"id": "2510.01378", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01378", "abs": "https://arxiv.org/abs/2510.01378", "authors": ["Kiwhan Song", "Jaeyeon Kim", "Sitan Chen", "Yilun Du", "Sham Kakade", "Vincent Sitzmann"], "title": "Selective Underfitting in Diffusion Models", "comment": null, "summary": "Diffusion models have emerged as the principal paradigm for generative\nmodeling across various domains. During training, they learn the score\nfunction, which in turn is used to generate samples at inference. They raise a\nbasic yet unsolved question: which score do they actually learn? In principle,\na diffusion model that matches the empirical score in the entire data space\nwould simply reproduce the training data, failing to generate novel samples.\nRecent work addresses this question by arguing that diffusion models underfit\nthe empirical score due to training-time inductive biases. In this work, we\nrefine this perspective, introducing the notion of selective underfitting:\ninstead of underfitting the score everywhere, better diffusion models more\naccurately approximate the score in certain regions of input space, while\nunderfitting it in others. We characterize these regions and design empirical\ninterventions to validate our perspective. Our results establish that selective\nunderfitting is essential for understanding diffusion models, yielding new,\ntestable insights into their generalization and generative performance.", "AI": {"tldr": "Diffusion models selectively underfit the empirical score function - accurately approximating it in certain regions while underfitting in others, which is essential for understanding their generalization and generative performance.", "motivation": "To understand what score function diffusion models actually learn, since perfectly matching the empirical score would simply reproduce training data and fail to generate novel samples.", "method": "Introduce the concept of selective underfitting, characterize regions where diffusion models accurately approximate vs underfit the score, and design empirical interventions to validate this perspective.", "result": "Established that selective underfitting occurs - better diffusion models more accurately approximate the score in certain regions while underfitting in others.", "conclusion": "Selective underfitting is essential for understanding diffusion models and provides new testable insights into their generalization and generative performance."}}
{"id": "2510.01914", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.SP", "68T07, 68U10", "I.4.8; I.2.10"], "pdf": "https://arxiv.org/pdf/2510.01914", "abs": "https://arxiv.org/abs/2510.01914", "authors": ["Wei-Lung Mao", "Chun-Chi Wang", "Po-Heng Chou", "Yen-Ting Liu"], "title": "Automated Defect Detection for Mass-Produced Electronic Components Based on YOLO Object Detection Models", "comment": "12 pages, 16 figures, 7 tables, and published in IEEE Sensors Journal", "summary": "Since the defect detection of conventional industry components is\ntime-consuming and labor-intensive, it leads to a significant burden on quality\ninspection personnel and makes it difficult to manage product quality. In this\npaper, we propose an automated defect detection system for the dual in-line\npackage (DIP) that is widely used in industry, using digital camera optics and\na deep learning (DL)-based model. The two most common defect categories of DIP\nare examined: (1) surface defects, and (2) pin-leg defects. However, the lack\nof defective component images leads to a challenge for detection tasks. To\nsolve this problem, the ConSinGAN is used to generate a suitable-sized dataset\nfor training and testing. Four varieties of the YOLO model are investigated\n(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.\nThe proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in\naccuracy of 95.50\\%, detection time of 285 ms, and is far superior to\nthreshold-based approaches. In addition, the supervisory control and data\nacquisition (SCADA) system is developed, and the associated sensor architecture\nis described. The proposed automated defect detection can be easily established\nwith numerous types of defects or insufficient defect data.", "AI": {"tldr": "Automated defect detection system for DIP components using deep learning and ConSinGAN data augmentation, achieving 95.50% accuracy with YOLOv7.", "motivation": "Conventional defect detection is time-consuming and labor-intensive, creating burden on quality inspection personnel and making product quality management difficult.", "method": "Used ConSinGAN to generate dataset for training/testing, investigated four YOLO models (v3, v4, v7, v9) with and without ConSinGAN augmentation, and developed SCADA system with sensor architecture.", "result": "YOLOv7 with ConSinGAN achieved best performance: 95.50% accuracy, 285 ms detection time, significantly outperforming threshold-based approaches.", "conclusion": "The proposed automated defect detection system can be easily established for various defect types and works well even with insufficient defect data."}}
{"id": "2510.01641", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01641", "abs": "https://arxiv.org/abs/2510.01641", "authors": ["Xiaoyang Liu", "Zhengyan Zhou", "Zihang Xu", "Jiezhang Cao", "Zheng Chen", "Yulun Zhang"], "title": "FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring", "comment": null, "summary": "Recent advancements in image motion deblurring, driven by CNNs and\ntransformers, have made significant progress. Large-scale pre-trained diffusion\nmodels, which are rich in true-world modeling, have shown great promise for\nhigh-quality image restoration tasks such as deblurring, demonstrating stronger\ngenerative capabilities than CNN and transformer-based methods. However,\nchallenges such as unbearable inference time and compromised fidelity still\nlimit the full potential of the diffusion models. To address this, we introduce\nFideDiff, a novel single-step diffusion model designed for high-fidelity\ndeblurring. We reformulate motion deblurring as a diffusion-like process where\neach timestep represents a progressively blurred image, and we train a\nconsistency model that aligns all timesteps to the same clean image. By\nreconstructing training data with matched blur trajectories, the model learns\ntemporal consistency, enabling accurate one-step deblurring. We further enhance\nmodel performance by integrating Kernel ControlNet for blur kernel estimation\nand introducing adaptive timestep prediction. Our model achieves superior\nperformance on full-reference metrics, surpassing previous diffusion-based\nmethods and matching the performance of other state-of-the-art models. FideDiff\noffers a new direction for applying pre-trained diffusion models to\nhigh-fidelity image restoration tasks, establishing a robust baseline for\nfurther advancing diffusion models in real-world industrial applications. Our\ndataset and code will be available at https://github.com/xyLiu339/FideDiff.", "AI": {"tldr": "FideDiff is a novel single-step diffusion model for high-fidelity motion deblurring that reformulates deblurring as a diffusion process and trains a consistency model to enable accurate one-step deblurring.", "motivation": "Current diffusion models for deblurring suffer from unbearable inference time and compromised fidelity, limiting their practical application despite their strong generative capabilities compared to CNN and transformer methods.", "method": "Reformulates motion deblurring as a diffusion-like process with progressively blurred images, trains a consistency model to align all timesteps to clean images, integrates Kernel ControlNet for blur kernel estimation, and introduces adaptive timestep prediction.", "result": "Achieves superior performance on full-reference metrics, surpassing previous diffusion-based methods and matching other state-of-the-art models while enabling fast one-step inference.", "conclusion": "FideDiff establishes a new direction for applying pre-trained diffusion models to high-fidelity image restoration and provides a robust baseline for advancing diffusion models in real-world industrial applications."}}
{"id": "2510.01609", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01609", "abs": "https://arxiv.org/abs/2510.01609", "authors": ["Bo Ma", "Hang Li", "ZeHua Hu", "XiaoFan Gui", "LuYao Liu", "Simon Lau"], "title": "AgentRec: Next-Generation LLM-Powered Multi-Agent Collaborative Recommendation with Adaptive Intelligence", "comment": null, "summary": "Interactive conversational recommender systems have gained significant\nattention for their ability to capture user preferences through natural\nlanguage interactions. However, existing approaches face substantial challenges\nin handling dynamic user preferences, maintaining conversation coherence, and\nbalancing multiple ranking objectives simultaneously. This paper introduces\nAgentRec, a next-generation LLM-powered multi-agent collaborative\nrecommendation framework that addresses these limitations through hierarchical\nagent networks with adaptive intelligence. Our approach employs specialized\nLLM-powered agents for conversation understanding, preference modeling, context\nawareness, and dynamic ranking, coordinated through an adaptive weighting\nmechanism that learns from interaction patterns. We propose a three-tier\nlearning strategy combining rapid response for simple queries, intelligent\nreasoning for complex preferences, and deep collaboration for challenging\nscenarios. Extensive experiments on three real-world datasets demonstrate that\nAgentRec achieves consistent improvements over state-of-the-art baselines, with\n2.8\\% enhancement in conversation success rate, 1.9\\% improvement in\nrecommendation accuracy (NDCG@10), and 3.2\\% better conversation efficiency\nwhile maintaining comparable computational costs through intelligent agent\ncoordination.", "AI": {"tldr": "AgentRec is a multi-agent LLM framework that improves conversational recommender systems by using specialized agents for different tasks, adaptive coordination, and a three-tier learning strategy, achieving better performance than existing methods.", "motivation": "Existing conversational recommender systems struggle with dynamic user preferences, conversation coherence, and balancing multiple ranking objectives simultaneously.", "method": "Uses hierarchical LLM-powered agents for conversation understanding, preference modeling, context awareness, and dynamic ranking with adaptive weighting. Implements three-tier learning: rapid response, intelligent reasoning, and deep collaboration.", "result": "Achieved 2.8% improvement in conversation success rate, 1.9% better recommendation accuracy (NDCG@10), and 3.2% better conversation efficiency while maintaining comparable computational costs.", "conclusion": "AgentRec demonstrates that multi-agent collaborative frameworks with adaptive intelligence can effectively address key challenges in conversational recommender systems."}}
{"id": "2510.01384", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01384", "abs": "https://arxiv.org/abs/2510.01384", "authors": ["Jaeyeon Kim", "Seunggeun Kim", "Taekyun Lee", "David Z. Pan", "Hyeji Kim", "Sham Kakade", "Sitan Chen"], "title": "Fine-Tuning Masked Diffusion for Provable Self-Correction", "comment": null, "summary": "A natural desideratum for generative models is self-correction--detecting and\nrevising low-quality tokens at inference. While Masked Diffusion Models (MDMs)\nhave emerged as a promising approach for generative modeling in discrete\nspaces, their capacity for self-correction remains poorly understood. Prior\nattempts to incorporate self-correction into MDMs either require overhauling\nMDM architectures/training or rely on imprecise proxies for token quality,\nlimiting their applicability. Motivated by this, we introduce PRISM--Plug-in\nRemasking for Inference-time Self-correction of Masked Diffusions--a\nlightweight, model-agnostic approach that applies to any pretrained MDM.\nTheoretically, PRISM defines a self-correction loss that provably learns\nper-token quality scores, without RL or a verifier. These quality scores are\ncomputed in the same forward pass with MDM and used to detect low-quality\ntokens. Empirically, PRISM advances MDM inference across domains and scales:\nSudoku; unconditional text (170M); and code with LLaDA (8B).", "AI": {"tldr": "PRISM is a lightweight, model-agnostic approach for self-correction in Masked Diffusion Models that learns per-token quality scores to detect and revise low-quality tokens during inference without requiring architectural changes or RL.", "motivation": "Current Masked Diffusion Models lack effective self-correction capabilities, with prior approaches either requiring major architectural/training changes or relying on imprecise quality proxies, limiting their practical applicability.", "method": "PRISM uses plug-in remasking to define a self-correction loss that provably learns per-token quality scores during a single forward pass with MDM, enabling detection of low-quality tokens without reinforcement learning or verifiers.", "result": "PRISM significantly advances MDM inference performance across multiple domains and scales, including Sudoku puzzles, unconditional text generation (170M parameters), and code generation with LLaDA (8B parameters).", "conclusion": "PRISM provides an effective, lightweight solution for self-correction in Masked Diffusion Models that works with any pretrained MDM and demonstrates strong empirical performance across diverse applications."}}
{"id": "2510.01651", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01651", "abs": "https://arxiv.org/abs/2510.01651", "authors": ["Rixin Zhou", "Peiqiang Qiu", "Qian Zhang", "Chuntao Li", "Xi Yang"], "title": "LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze Inscription Recognition", "comment": "18 pages, 7 figures, 2 Tables", "summary": "Bronze inscriptions (BI), engraved on ritual vessels, constitute a crucial\nstage of early Chinese writing and provide indispensable evidence for\narchaeological and historical studies. However, automatic BI recognition\nremains difficult due to severe visual degradation, multi-domain variability\nacross photographs, rubbings, and tracings, and an extremely long-tailed\ncharacter distribution. To address these challenges, we curate a large-scale BI\ndataset comprising 22454 full-page images and 198598 annotated characters\nspanning 6658 unique categories, enabling robust cross-domain evaluation.\nBuilding on this resource, we develop a two-stage detection-recognition\npipeline that first localizes inscriptions and then transcribes individual\ncharacters. To handle heterogeneous domains and rare classes, we equip the\npipeline with LadderMoE, which augments a pretrained CLIP encoder with\nladder-style MoE adapters, enabling dynamic expert specialization and stronger\nrobustness. Comprehensive experiments on single-character and full-page\nrecognition tasks demonstrate that our method substantially outperforms\nstate-of-the-art scene text recognition baselines, achieving superior accuracy\nacross head, mid, and tail categories as well as all acquisition modalities.\nThese results establish a strong foundation for bronze inscription recognition\nand downstream archaeological analysis.", "AI": {"tldr": "This paper presents a new method for automatic bronze inscription recognition, addressing challenges like visual degradation, multi-domain variability, and long-tailed character distribution through a curated large-scale dataset and a two-stage detection-recognition pipeline with LadderMoE architecture.", "motivation": "Bronze inscriptions are crucial for archaeological and historical studies but automatic recognition is difficult due to severe visual degradation, multi-domain variability across photographs/rubbings/tracings, and extremely long-tailed character distribution.", "method": "Curated a large-scale BI dataset with 22,454 full-page images and 198,598 annotated characters across 6,658 categories. Developed a two-stage detection-recognition pipeline with LadderMoE - a pretrained CLIP encoder augmented with ladder-style Mixture of Experts adapters for dynamic expert specialization and robustness.", "result": "The method substantially outperforms state-of-the-art scene text recognition baselines, achieving superior accuracy across head, mid, and tail categories as well as all acquisition modalities (photographs, rubbings, tracings).", "conclusion": "The approach establishes a strong foundation for bronze inscription recognition and downstream archaeological analysis, effectively handling the challenges of heterogeneous domains and rare character classes."}}
{"id": "2510.01611", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01611", "abs": "https://arxiv.org/abs/2510.01611", "authors": ["Min Zeng"], "title": "PychoBench: Evaluating the Psychology Intelligence of Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable success across a\nwide range of industries, primarily due to their impressive generative\nabilities. Yet, their potential in applications requiring cognitive abilities,\nsuch as psychological counseling, remains largely untapped. This paper\ninvestigates the key question: Can LLMs be effectively applied to psychological\ncounseling? To determine whether an LLM can effectively take on the role of a\npsychological counselor, the first step is to assess whether it meets the\nqualifications required for such a role, namely the ability to pass the U.S.\nNational Counselor Certification Exam (NCE). This is because, just as a human\ncounselor must pass a certification exam to practice, an LLM must demonstrate\nsufficient psychological knowledge to meet the standards required for such a\nrole. To address this, we introduce PsychoBench, a benchmark grounded in\nU.S.national counselor examinations, a licensure test for professional\ncounselors that requires about 70% accuracy to pass. PsychoBench comprises\napproximately 2,252 carefully curated single-choice questions, crafted to\nrequire deep understanding and broad enough to cover various sub-disciplines of\npsychology. This benchmark provides a comprehensive assessment of an LLM's\nability to function as a counselor. Our evaluation shows that advanced models\nsuch as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passing\nthreshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B)\nremain far below it. These results suggest that only frontier LLMs are\ncurrently capable of meeting counseling exam standards, highlighting both the\npromise and the challenges of developing psychology-oriented LLMs.", "AI": {"tldr": "This paper introduces PsychoBench, a benchmark based on the U.S. National Counselor Certification Exam to assess whether LLMs can qualify as psychological counselors by testing their psychological knowledge.", "motivation": "To investigate whether LLMs can be effectively applied to psychological counseling by determining if they meet the qualification standards required for human counselors, specifically the ability to pass professional certification exams.", "method": "Developed PsychoBench, a comprehensive benchmark comprising approximately 2,252 single-choice questions from U.S. national counselor examinations, covering various psychology sub-disciplines and requiring deep understanding.", "result": "Advanced models like GPT-4o, Llama3.3-70B, and Gemma3-27B achieved well above the 70% passing threshold, while smaller open-source models (Qwen2.5-7B, Mistral-7B) remained far below it.", "conclusion": "Only frontier LLMs currently meet counseling exam standards, highlighting both the promise and challenges of developing psychology-oriented LLMs for counseling applications."}}
{"id": "2510.01394", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01394", "abs": "https://arxiv.org/abs/2510.01394", "authors": ["Yusuf Kalayci", "Vinod Raman", "Shaddin Dughmi"], "title": "Optimal Stopping vs Best-of-$N$ for Inference Time Optimization", "comment": "24 pages", "summary": "Large language model (LLM) generation often requires balancing output quality\nagainst inference cost, especially when using multiple generations. We\nintroduce a new framework for inference-time optimization based on the\nclassical Pandora's Box problem. Viewing each generation as opening a costly\n\"box\" with random reward, we develop algorithms that decide when to stop\ngenerating without knowing the underlying reward distribution. Our first\ncontribution is a UCB-style Pandora's Box algorithm, which achieves performance\nthat is provably close to Weitzman's algorithm, the optimal strategy when the\ndistribution is known. We further adapt this method to practical LLM settings\nby addressing reward scaling across prompts via a Bradley-Terry inspired\ntransformation. This leads to an adaptive inference-time optimization method\nthat normalizes rewards and learns stopping thresholds on the fly. Experiments\non the AlpacaFarm and HH-RLHF datasets, using multiple LLM-reward model pairs,\nshow that our adaptive strategy can obtain the same performance as non-adaptive\nBest-of-N sampling while requiring 15-35 percent fewer generations on average.\nOur results establish a principled bridge between optimal stopping theory and\ninference-time scaling, providing both theoretical performance bounds and\npractical efficiency gains for LLM deployment.", "AI": {"tldr": "A new inference-time optimization framework for LLMs based on Pandora's Box problem, achieving 15-35% fewer generations while maintaining same performance as Best-of-N sampling.", "motivation": "Balancing LLM output quality against inference cost when using multiple generations, addressing the trade-off between generation quality and computational expense.", "method": "Developed UCB-style Pandora's Box algorithm that adapts to unknown reward distributions, with Bradley-Terry inspired transformation for reward scaling across prompts and adaptive stopping thresholds.", "result": "Experiments on AlpacaFarm and HH-RLHF datasets show adaptive strategy achieves same performance as Best-of-N sampling with 15-35% fewer generations on average.", "conclusion": "Established principled bridge between optimal stopping theory and inference-time scaling, providing both theoretical performance bounds and practical efficiency gains for LLM deployment."}}
{"id": "2510.01660", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01660", "abs": "https://arxiv.org/abs/2510.01660", "authors": ["Duy Nguyen", "Dat Nguyen"], "title": "VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming", "comment": null, "summary": "Existing UDA pipelines fine-tune already well-trained backbone parameters for\nevery new source-and-target pair, resulting in the number of training\nparameters and storage memory growing linearly with each new pair, and also\npreventing the reuse of these well-trained backbone parameters.\n  Inspired by recent implications that existing backbones have textural biases,\nwe propose making use of domain-specific textural bias for domain adaptation\nvia visual reprogramming, namely VirDA.Instead of fine-tuning the full\nbackbone, VirDA prepends a domain-specific visual reprogramming layer to the\nbackbone. This layer produces visual prompts that act as an added textural bias\nto the input image, adapting its ``style'' to a target domain. To optimize\nthese visual reprogramming layers, we use multiple objective functions that\noptimize the intra- and inter-domain distribution differences when\ndomain-adapting visual prompts are applied. This process does not require\nmodifying the backbone parameters, allowing the same backbone to be reused\nacross different domains.\n  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M\ntrainable parameters. VirDA surpasses PDA, the state-of-the-art\nparameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its\nparameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans\nand FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%\nof their trainable parameters. Relative to the strongest current methods\n(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only\n2.2% and 1.1% accuracy, respectively.", "AI": {"tldr": "VirDA proposes a parameter-efficient domain adaptation method using visual reprogramming layers instead of full backbone fine-tuning, achieving competitive accuracy with significantly fewer trainable parameters.", "motivation": "Existing UDA methods require fine-tuning entire backbones for each source-target pair, leading to linear growth in parameters and storage, preventing backbone reuse across domains.", "method": "Prepend domain-specific visual reprogramming layers to produce visual prompts that add textural bias to input images, adapting style to target domains. Optimize with multiple objective functions for intra- and inter-domain distribution differences without modifying backbone parameters.", "result": "Achieved 92.8% mean accuracy on Office-31 with only 1.5M trainable parameters, surpassing PDA by +1.6% accuracy using 46% of parameters, and outperforming full-backbone methods while using only 1.7-2.8% of their parameters.", "conclusion": "VirDA enables parameter-efficient domain adaptation through visual reprogramming, allowing backbone reuse across domains while maintaining competitive performance with minimal parameter overhead."}}
{"id": "2510.01620", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01620", "abs": "https://arxiv.org/abs/2510.01620", "authors": ["Peidong Liu", "Junjiang Lin", "Shaowen Wang", "Yao Xu", "Haiqing Li", "Xuhao Xie", "Siyi Wu", "Hao Li"], "title": "Learning to Decide with Just Enough: Information-Theoretic Context Summarization for CDMPs", "comment": null, "summary": "Contextual Markov Decision Processes (CMDPs) offer a framework for sequential\ndecision-making under external signals, but existing methods often fail to\ngeneralize in high-dimensional or unstructured contexts, resulting in excessive\ncomputation and unstable performance. We propose an information-theoretic\nsummarization approach that uses large language models (LLMs) to compress\ncontextual inputs into low-dimensional, semantically rich summaries. These\nsummaries augment states by preserving decision-critical cues while reducing\nredundancy. Building on the notion of approximate context sufficiency, we\nprovide, to our knowledge, the first regret bounds and a latency-entropy\ntrade-off characterization for CMDPs. Our analysis clarifies how\ninformativeness impacts computational cost. Experiments across discrete,\ncontinuous, visual, and recommendation benchmarks show that our method\noutperforms raw-context and non-context baselines, improving reward, success\nrate, and sample efficiency, while reducing latency and memory usage. These\nfindings demonstrate that LLM-based summarization offers a scalable and\ninterpretable solution for efficient decision-making in context-rich,\nresource-constrained environments.", "AI": {"tldr": "LLM-based summarization compresses contextual inputs in CMDPs to improve decision-making efficiency and performance across various benchmarks.", "motivation": "Existing CMDP methods struggle with high-dimensional contexts, leading to computational inefficiency and unstable performance, especially in unstructured environments.", "method": "Information-theoretic summarization using LLMs to compress contextual inputs into low-dimensional, semantically rich summaries that augment states while preserving decision-critical information.", "result": "Outperforms raw-context and non-context baselines across discrete, continuous, visual, and recommendation benchmarks, improving reward, success rate, sample efficiency while reducing latency and memory usage.", "conclusion": "LLM-based summarization provides a scalable and interpretable solution for efficient decision-making in context-rich, resource-constrained environments, with theoretical regret bounds and latency-entropy trade-off characterization."}}
{"id": "2510.01396", "categories": ["cs.LG", "cs.AI", "physics.chem-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.01396", "abs": "https://arxiv.org/abs/2510.01396", "authors": ["Wasut Pornpatcharapong"], "title": "Neural Network Surrogates for Free Energy Computation of Complex Chemical Systems", "comment": "6 pages, 4 figures. This work has already been accepted for\n  presentation in The 29th International Computer Science and Engineering\n  Conference (ICSEC) 2025, Chiang Mai, Thailand, and will be published in IEEE\n  Xplore", "summary": "Free energy reconstruction methods such as Gaussian Process Regression (GPR)\nrequire Jacobians of the collective variables (CVs), a bottleneck that\nrestricts the use of complex or machine-learned CVs. We introduce a neural\nnetwork surrogate framework that learns CVs directly from Cartesian coordinates\nand uses automatic differentiation to provide Jacobians, bypassing analytical\nforms. On an MgCl2 ion-pairing system, our method achieved high accuracy for\nboth a simple distance CV and a complex coordination-number CV. Moreover,\nJacobian errors also followed a near-Gaussian distribution, making them\nsuitable for GPR pipelines. This framework enables gradient-based free energy\nmethods to incorporate complex and machine-learned CVs, broadening the scope of\nbiochemistry and materials simulations.", "AI": {"tldr": "A neural network surrogate framework that learns collective variables (CVs) from Cartesian coordinates and provides Jacobians via automatic differentiation, enabling gradient-based free energy methods to use complex CVs without analytical forms.", "motivation": "Traditional free energy reconstruction methods like Gaussian Process Regression require Jacobians of CVs, which is a bottleneck that restricts the use of complex or machine-learned collective variables.", "method": "Introduced a neural network surrogate framework that learns CVs directly from Cartesian coordinates and uses automatic differentiation to provide Jacobians, bypassing the need for analytical forms.", "result": "On an MgCl2 ion-pairing system, the method achieved high accuracy for both simple distance CVs and complex coordination-number CVs. Jacobian errors followed a near-Gaussian distribution, making them suitable for GPR pipelines.", "conclusion": "This framework enables gradient-based free energy methods to incorporate complex and machine-learned CVs, broadening the scope of biochemistry and materials simulations."}}
{"id": "2510.01662", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01662", "abs": "https://arxiv.org/abs/2510.01662", "authors": ["Minh Tran", "Maksim Siniukov", "Zhangyu Jin", "Mohammad Soleymani"], "title": "Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery", "comment": null, "summary": "Facial expression analysis is central to understanding human behavior, yet\nexisting coding systems such as the Facial Action Coding System (FACS) are\nconstrained by limited coverage and costly manual annotation. In this work, we\nintroduce Discrete Facial Encoding (DFE), an unsupervised, data-driven\nalternative of compact and interpretable dictionary of facial expressions from\n3D mesh sequences learned through a Residual Vector Quantized Variational\nAutoencoder (RVQ-VAE). Our approach first extracts identity-invariant\nexpression features from images using a 3D Morphable Model (3DMM), effectively\ndisentangling factors such as head pose and facial geometry. We then encode\nthese features using an RVQ-VAE, producing a sequence of discrete tokens from a\nshared codebook, where each token captures a specific, reusable facial\ndeformation pattern that contributes to the overall expression. Through\nextensive experiments, we demonstrate that Discrete Facial Encoding captures\nmore precise facial behaviors than FACS and other facial encoding alternatives.\nWe evaluate the utility of our representation across three high-level\npsychological tasks: stress detection, personality prediction, and depression\ndetection. Using a simple Bag-of-Words model built on top of the learned\ntokens, our system consistently outperforms both FACS-based pipelines and\nstrong image and video representation learning models such as Masked\nAutoencoders. Further analysis reveals that our representation covers a wider\nvariety of facial displays, highlighting its potential as a scalable and\neffective alternative to FACS for psychological and affective computing\napplications.", "AI": {"tldr": "DFE is an unsupervised facial expression encoding method using RVQ-VAE that outperforms FACS in psychological tasks like stress detection, personality prediction, and depression detection.", "motivation": "Existing facial expression coding systems like FACS have limited coverage and require costly manual annotation, creating a need for scalable, data-driven alternatives.", "method": "Uses 3DMM to extract identity-invariant expression features, then encodes them with RVQ-VAE to produce discrete tokens from a shared codebook, each capturing reusable facial deformation patterns.", "result": "DFE captures more precise facial behaviors than FACS and outperforms both FACS-based pipelines and modern representation learning models across psychological tasks using simple Bag-of-Words models.", "conclusion": "DFE serves as a scalable and effective alternative to FACS for psychological and affective computing applications, covering a wider variety of facial displays."}}
{"id": "2510.01639", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01639", "abs": "https://arxiv.org/abs/2510.01639", "authors": ["Thinh Hung Truong", "Jey Han Lau", "Jianzhong Qi"], "title": "Understanding the Geospatial Reasoning Capabilities of LLMs: A Trajectory Recovery Perspective", "comment": null, "summary": "We explore the geospatial reasoning capabilities of Large Language Models\n(LLMs), specifically, whether LLMs can read road network maps and perform\nnavigation. We frame trajectory recovery as a proxy task, which requires models\nto reconstruct masked GPS traces, and introduce GLOBALTRACE, a dataset with\nover 4,000 real-world trajectories across diverse regions and transportation\nmodes. Using road network as context, our prompting framework enables LLMs to\ngenerate valid paths without accessing any external navigation tools.\nExperiments show that LLMs outperform off-the-shelf baselines and specialized\ntrajectory recovery models, with strong zero-shot generalization. Fine-grained\nanalysis shows that LLMs have strong comprehension of the road network and\ncoordinate systems, but also pose systematic biases with respect to regions and\ntransportation modes. Finally, we demonstrate how LLMs can enhance navigation\nexperiences by reasoning over maps in flexible ways to incorporate user\npreferences.", "AI": {"tldr": "LLMs can read road network maps and perform navigation through trajectory recovery, outperforming specialized models with strong zero-shot generalization.", "motivation": "To explore the geospatial reasoning capabilities of LLMs, specifically whether they can understand road network maps and perform navigation tasks without external tools.", "method": "Framed trajectory recovery as a proxy task using GLOBALTRACE dataset with 4,000+ real-world trajectories. Used road network as context in a prompting framework to enable LLMs to reconstruct masked GPS traces.", "result": "LLMs outperformed off-the-shelf baselines and specialized trajectory recovery models, showing strong zero-shot generalization. Models demonstrated strong comprehension of road networks and coordinate systems, though with systematic regional and transportation mode biases.", "conclusion": "LLMs can effectively reason over maps for navigation tasks and enhance navigation experiences by incorporating user preferences flexibly, demonstrating significant geospatial reasoning capabilities."}}
{"id": "2510.01407", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01407", "abs": "https://arxiv.org/abs/2510.01407", "authors": ["Ethan G. Rogers", "Cheng Wang"], "title": "Ultra-Efficient Decoding for End-to-End Neural Compression and Reconstruction", "comment": "5 pages, 4 figures, NeurIPS 2025 Workshop MLForSys", "summary": "Image compression and reconstruction are crucial for various digital\napplications. While contemporary neural compression methods achieve impressive\ncompression rates, the adoption of such technology has been largely hindered by\nthe complexity and large computational costs of the convolution-based decoders\nduring data reconstruction. To address the decoder bottleneck in neural\ncompression, we develop a new compression-reconstruction framework based on\nincorporating low-rank representation in an autoencoder with vector\nquantization. We demonstrated that performing a series of computationally\nefficient low-rank operations on the learned latent representation of images\ncan efficiently reconstruct the data with high quality. Our approach\ndramatically reduces the computational overhead in the decoding phase of neural\ncompression/reconstruction, essentially eliminating the decoder compute\nbottleneck while maintaining high fidelity of image outputs.", "AI": {"tldr": "A new neural compression framework using low-rank representations to eliminate decoder bottleneck while maintaining image quality.", "motivation": "Current neural compression methods suffer from high computational complexity and large costs in convolution-based decoders, hindering adoption.", "method": "Incorporates low-rank representation in autoencoder with vector quantization, using computationally efficient low-rank operations on latent representations.", "result": "Dramatically reduces computational overhead in decoding phase while maintaining high fidelity image outputs.", "conclusion": "The approach effectively eliminates decoder compute bottleneck in neural compression/reconstruction systems."}}
{"id": "2510.02265", "categories": ["cs.LG", "cs.AI", "cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.02265", "abs": "https://arxiv.org/abs/2510.02265", "authors": ["Yalin E. Sagduyu", "Tugba Erpek", "Kemal Davaslioglu", "Sastry Kompella"], "title": "How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning", "comment": null, "summary": "This paper studies the problem of mitigating reactive jamming, where a jammer\nadopts a dynamic policy of selecting channels and sensing thresholds to detect\nand jam ongoing transmissions. The transmitter-receiver pair learns to avoid\njamming and optimize throughput over time (without prior knowledge of channel\nconditions or jamming strategies) by using reinforcement learning (RL) to adapt\ntransmit power, modulation, and channel selection. Q-learning is employed for\ndiscrete jamming-event states, while Deep Q-Networks (DQN) are employed for\ncontinuous states based on received power. Through different reward functions\nand action sets, the results show that RL can adapt rapidly to spectrum\ndynamics and sustain high rates as channels and jamming policies change over\ntime.", "AI": {"tldr": "RL-based anti-jamming using Q-learning and DQN for adaptive power, modulation, and channel selection to maintain throughput against reactive jammers.", "motivation": "To counter reactive jammers that dynamically select channels and sensing thresholds to detect and jam transmissions, without prior knowledge of channel conditions or jamming strategies.", "method": "Employ Q-learning for discrete jamming-event states and Deep Q-Networks (DQN) for continuous states based on received power, adapting transmit power, modulation, and channel selection.", "result": "RL can adapt rapidly to spectrum dynamics and sustain high rates as channels and jamming policies change over time.", "conclusion": "Reinforcement learning is effective for mitigating reactive jamming by enabling adaptive transmission strategies that optimize throughput in dynamic spectrum environments."}}
{"id": "2510.01665", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01665", "abs": "https://arxiv.org/abs/2510.01665", "authors": ["Yongbo Chen", "Yanhao Zhang", "Shaifali Parashar", "Liang Zhao", "Shoudong Huang"], "title": "Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale", "comment": null, "summary": "Non-rigid structure-from-motion (NRSfM), a promising technique for addressing\nthe mapping challenges in monocular visual deformable simultaneous localization\nand mapping (SLAM), has attracted growing attention. We introduce a novel\nmethod, called Con-NRSfM, for NRSfM under conformal deformations, encompassing\nisometric deformations as a subset. Our approach performs point-wise\nreconstruction using 2D selected image warps optimized through a graph-based\nframework. Unlike existing methods that rely on strict assumptions, such as\nlocally planar surfaces or locally linear deformations, and fail to recover the\nconformal scale, our method eliminates these constraints and accurately\ncomputes the local conformal scale. Additionally, our framework decouples\nconstraints on depth and conformal scale, which are inseparable in other\napproaches, enabling more precise depth estimation. To address the sensitivity\nof the formulated problem, we employ a parallel separable iterative\noptimization strategy. Furthermore, a self-supervised learning framework,\nutilizing an encoder-decoder network, is incorporated to generate dense 3D\npoint clouds with texture. Simulation and experimental results using both\nsynthetic and real datasets demonstrate that our method surpasses existing\napproaches in terms of reconstruction accuracy and robustness. The code for the\nproposed method will be made publicly available on the project website:\nhttps://sites.google.com/view/con-nrsfm.", "AI": {"tldr": "Con-NRSfM is a novel method for non-rigid structure-from-motion under conformal deformations that performs point-wise reconstruction using 2D image warps optimized through a graph-based framework, accurately computes local conformal scale, and employs parallel separable iterative optimization with self-supervised learning for dense 3D reconstruction.", "motivation": "To address limitations in existing NRSfM methods that rely on strict assumptions like locally planar surfaces or locally linear deformations, fail to recover conformal scale, and have inseparable depth/conformal scale constraints, leading to inaccurate deformable SLAM.", "method": "Uses 2D selected image warps optimized through graph-based framework, decouples depth and conformal scale constraints, employs parallel separable iterative optimization strategy, and incorporates self-supervised learning with encoder-decoder network for dense 3D point cloud generation.", "result": "Demonstrated superior performance over existing methods in both synthetic and real datasets, showing improved reconstruction accuracy and robustness in handling conformal deformations.", "conclusion": "Con-NRSfM successfully eliminates restrictive assumptions of previous methods, accurately computes local conformal scale, enables precise depth estimation, and provides robust reconstruction for deformable SLAM applications."}}
{"id": "2510.01664", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01664", "abs": "https://arxiv.org/abs/2510.01664", "authors": ["Yejin Kim", "Youngbin Lee", "Juhyeong Kim", "Yongjae Lee"], "title": "GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents", "comment": "7 Pages, 2 figures", "summary": "This study demonstrates that GuruAgents, prompt-guided AI agents, can\nsystematically operationalize the strategies of legendary investment gurus. We\ndevelop five distinct GuruAgents, each designed to emulate an iconic investor,\nby encoding their distinct philosophies into LLM prompts that integrate\nfinancial tools and a deterministic reasoning pipeline. In a backtest on\nNASDAQ-100 constituents from Q4 2023 to Q2 2025, the GuruAgents exhibit unique\nbehaviors driven by their prompted personas. The Buffett GuruAgent achieves the\nhighest performance, delivering a 42.2\\% CAGR that significantly outperforms\nbenchmarks, while other agents show varied results. These findings confirm that\nprompt engineering can successfully translate the qualitative philosophies of\ninvestment gurus into reproducible, quantitative strategies, highlighting a\nnovel direction for automated systematic investing. The source code and data\nare available at https://github.com/yejining99/GuruAgents.", "AI": {"tldr": "GuruAgents are AI agents that operationalize legendary investment gurus' strategies through prompt engineering, achieving up to 42.2% CAGR in backtesting.", "motivation": "To translate qualitative investment philosophies of legendary gurus into reproducible, quantitative strategies using AI agents.", "method": "Developed five distinct GuruAgents by encoding investment gurus' philosophies into LLM prompts with financial tools and deterministic reasoning pipeline, then backtested on NASDAQ-100 constituents from Q4 2023 to Q2 2025.", "result": "Buffett GuruAgent achieved highest performance with 42.2% CAGR, significantly outperforming benchmarks, while other agents showed varied results. All agents exhibited unique behaviors driven by their prompted personas.", "conclusion": "Prompt engineering can successfully translate qualitative investment philosophies into reproducible quantitative strategies, opening new directions for automated systematic investing."}}
{"id": "2510.01439", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01439", "abs": "https://arxiv.org/abs/2510.01439", "authors": ["Mohamad Abou Ali", "Fadi Dornaika"], "title": "Edge Artificial Intelligence: A Systematic Review of Evolution, Taxonomic Frameworks, and Future Horizons", "comment": null, "summary": "Edge Artificial Intelligence (Edge AI) embeds intelligence directly into\ndevices at the network edge, enabling real-time processing with improved\nprivacy and reduced latency by processing data close to its source. This review\nsystematically examines the evolution, current landscape, and future directions\nof Edge AI through a multi-dimensional taxonomy including deployment location,\nprocessing capabilities such as TinyML and federated learning, application\ndomains, and hardware types. Following PRISMA guidelines, the analysis traces\nthe field from early content delivery networks and fog computing to modern\non-device intelligence. Core enabling technologies such as specialized hardware\naccelerators, optimized software, and communication protocols are explored.\nChallenges including resource limitations, security, model management, power\nconsumption, and connectivity are critically assessed. Emerging opportunities\nin neuromorphic hardware, continual learning algorithms, edge-cloud\ncollaboration, and trustworthiness integration are highlighted, providing a\ncomprehensive framework for researchers and practitioners.", "AI": {"tldr": "This review systematically examines Edge AI's evolution, current landscape, and future directions through a multi-dimensional taxonomy covering deployment, processing capabilities, applications, and hardware, while analyzing challenges and emerging opportunities.", "motivation": "To provide a comprehensive understanding of Edge AI's development from early content delivery networks to modern on-device intelligence, addressing the need for systematic analysis in this rapidly evolving field.", "method": "Systematic review following PRISMA guidelines, using multi-dimensional taxonomy including deployment location, processing capabilities (TinyML, federated learning), application domains, and hardware types.", "result": "Identified core enabling technologies (hardware accelerators, optimized software, communication protocols), challenges (resource limitations, security, power consumption), and emerging opportunities (neuromorphic hardware, continual learning, edge-cloud collaboration).", "conclusion": "Provides a comprehensive framework for researchers and practitioners by tracing Edge AI evolution, analyzing current technologies and challenges, and highlighting future directions including trustworthiness integration and advanced hardware solutions."}}
{"id": "2510.01669", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01669", "abs": "https://arxiv.org/abs/2510.01669", "authors": ["Jin Cao", "Hongrui Wu", "Ziyong Feng", "Hujun Bao", "Xiaowei Zhou", "Sida Peng"], "title": "UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction", "comment": "page: https://jin-cao-tma.github.io/UniVerse.github.io/ code:\n  https://github.com/zju3dv/UniVerse", "summary": "This paper tackles the challenge of robust reconstruction, i.e., the task of\nreconstructing a 3D scene from a set of inconsistent multi-view images. Some\nrecent works have attempted to simultaneously remove image inconsistencies and\nperform reconstruction by integrating image degradation modeling into neural 3D\nscene representations.However, these methods rely heavily on dense observations\nfor robustly optimizing model parameters.To address this issue, we propose to\ndecouple robust reconstruction into two subtasks: restoration and\nreconstruction, which naturally simplifies the optimization process.To this\nend, we introduce UniVerse, a unified framework for robust reconstruction based\non a video diffusion model. Specifically, UniVerse first converts inconsistent\nimages into initial videos, then uses a specially designed video diffusion\nmodel to restore them into consistent images, and finally reconstructs the 3D\nscenes from these restored images.Compared with case-by-case per-view\ndegradation modeling, the diffusion model learns a general scene prior from\nlarge-scale data, making it applicable to diverse image\ninconsistencies.Extensive experiments on both synthetic and real-world datasets\ndemonstrate the strong generalization capability and superior performance of\nour method in robust reconstruction. Moreover, UniVerse can control the style\nof the reconstructed 3D scene. Project page:\nhttps://jin-cao-tma.github.io/UniVerse.github.io/", "AI": {"tldr": "UniVerse is a unified framework that decouples robust 3D reconstruction from inconsistent multi-view images into restoration and reconstruction subtasks, using a video diffusion model to restore consistency before reconstruction.", "motivation": "Existing methods for robust 3D reconstruction from inconsistent multi-view images rely heavily on dense observations and complex optimization. The goal is to develop a more generalizable approach that can handle diverse image inconsistencies without requiring dense data.", "method": "UniVerse first converts inconsistent images into initial videos, then uses a specially designed video diffusion model to restore them into consistent images by leveraging learned scene priors from large-scale data, and finally reconstructs 3D scenes from the restored images.", "result": "Extensive experiments on synthetic and real-world datasets demonstrate strong generalization capability and superior performance in robust reconstruction compared to existing methods. The method also enables style control of reconstructed 3D scenes.", "conclusion": "Decoupling robust reconstruction into restoration and reconstruction subtasks simplifies optimization and improves performance. The diffusion model's learned scene priors make the approach applicable to diverse image inconsistencies, offering better generalization than case-by-case degradation modeling."}}
{"id": "2510.01670", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01670", "abs": "https://arxiv.org/abs/2510.01670", "authors": ["Erfan Shayegani", "Keegan Hines", "Yue Dong", "Nael Abu-Ghazaleh", "Roman Lutz", "Spencer Whitehead", "Vidhisha Balachandran", "Besmira Nushi", "Vibhav Vineet"], "title": "Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness", "comment": null, "summary": "Computer-Use Agents (CUAs) are an increasingly deployed class of agents that\ntake actions on GUIs to accomplish user goals. In this paper, we show that CUAs\nconsistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals\nregardless of feasibility, safety, reliability, or context. We characterize\nthree prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)\nassumptions and decisions under ambiguity, and (iii) contradictory or\ninfeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these\nthree patterns. Built on OSWorld, BLIND-ACT provides realistic environments and\nemploys LLM-based judges to evaluate agent behavior, achieving 93.75% agreement\nwith human annotations. We use BLIND-ACT to evaluate nine frontier models,\nincluding Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing\nhigh average BGD rates (80.8%) across them. We show that BGD exposes subtle\nrisks that arise even when inputs are not directly harmful. While\nprompting-based interventions lower BGD levels, substantial risk persists,\nhighlighting the need for stronger training- or inference-time interventions.\nQualitative analysis reveals observed failure modes: execution-first bias\n(focusing on how to act over whether to act), thought-action disconnect\n(execution diverging from reasoning), and request-primacy (justifying actions\ndue to user request). Identifying BGD and introducing BLIND-ACT establishes a\nfoundation for future research on studying and mitigating this fundamental risk\nand ensuring safe CUA deployment.", "AI": {"tldr": "Computer-Use Agents (CUAs) exhibit Blind Goal-Directedness (BGD) - pursuing goals regardless of feasibility, safety, or context. The paper introduces BLIND-ACT benchmark to evaluate BGD patterns and shows high BGD rates (80.8%) across nine frontier models, highlighting persistent safety risks.", "motivation": "To identify and characterize the fundamental risk of Blind Goal-Directedness in Computer-Use Agents, where agents pursue user goals without considering feasibility, safety, reliability, or context.", "method": "Developed BLIND-ACT benchmark with 90 tasks capturing three BGD patterns: lack of contextual reasoning, assumptions under ambiguity, and contradictory goals. Used OSWorld environments and LLM-based judges for evaluation, achieving 93.75% agreement with human annotations.", "result": "Evaluated nine frontier models (including Claude Sonnet, Opus 4, Computer-Use-Preview, GPT-5) and observed high average BGD rates of 80.8%. Prompting-based interventions reduced but did not eliminate BGD. Identified failure modes: execution-first bias, thought-action disconnect, and request-primacy.", "conclusion": "BGD exposes subtle risks in CUAs even with non-harmful inputs. Current interventions are insufficient, requiring stronger training- or inference-time solutions. BLIND-ACT provides foundation for future research on mitigating this fundamental safety risk in CUA deployment."}}
{"id": "2510.01447", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01447", "abs": "https://arxiv.org/abs/2510.01447", "authors": ["Dorsa Soleymani", "Ali Dadsetan", "Frank Rudzicz"], "title": "SoftAdaClip: A Smooth Clipping Strategy for Fair and Private Model Training", "comment": null, "summary": "Differential privacy (DP) provides strong protection for sensitive data, but\noften reduces model performance and fairness, especially for underrepresented\ngroups. One major reason is gradient clipping in DP-SGD, which can\ndisproportionately suppress learning signals for minority subpopulations.\nAlthough adaptive clipping can enhance utility, it still relies on uniform hard\nclipping, which may restrict fairness. To address this, we introduce\nSoftAdaClip, a differentially private training method that replaces hard\nclipping with a smooth, tanh-based transformation to preserve relative gradient\nmagnitudes while bounding sensitivity. We evaluate SoftAdaClip on various\ndatasets, including MIMIC-III (clinical text), GOSSIS-eICU (structured\nhealthcare), and Adult Income (tabular data). Our results show that SoftAdaClip\nreduces subgroup disparities by up to 87% compared to DP-SGD and up to 48%\ncompared to Adaptive-DPSGD, and these reductions in subgroup disparities are\nstatistically significant. These findings underscore the importance of\nintegrating smooth transformations with adaptive mechanisms to achieve fair and\nprivate model training.", "AI": {"tldr": "SoftAdaClip replaces hard gradient clipping in DP-SGD with a smooth tanh-based transformation to reduce fairness disparities in differentially private training while maintaining privacy guarantees.", "motivation": "Standard DP-SGD with gradient clipping disproportionately suppresses learning signals for minority subpopulations, reducing both model performance and fairness. Adaptive clipping helps but still uses uniform hard clipping that restricts fairness improvements.", "method": "SoftAdaClip uses a smooth, tanh-based transformation instead of hard clipping to preserve relative gradient magnitudes while bounding sensitivity for differential privacy. It combines this smooth transformation with adaptive mechanisms.", "result": "SoftAdaClip reduces subgroup disparities by up to 87% compared to DP-SGD and up to 48% compared to Adaptive-DPSGD across datasets including MIMIC-III, GOSSIS-eICU, and Adult Income. These disparity reductions are statistically significant.", "conclusion": "Integrating smooth transformations with adaptive mechanisms is crucial for achieving fair and private model training, as demonstrated by SoftAdaClip's significant improvements in reducing subgroup disparities while maintaining differential privacy."}}
{"id": "2510.01678", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01678", "abs": "https://arxiv.org/abs/2510.01678", "authors": ["Ke Jia", "Ji Zhou", "Hanxin Li", "Zhigan Zhou", "Haojie Chu", "Xiaojie Li"], "title": "An Efficient Deep Template Matching and In-Plane Pose Estimation Method via Template-Aware Dynamic Convolution", "comment": "Published in Expert Systems with Applications", "summary": "In industrial inspection and component alignment tasks, template matching\nrequires efficient estimation of a target's position and geometric state\n(rotation and scaling) under complex backgrounds to support precise downstream\noperations. Traditional methods rely on exhaustive enumeration of angles and\nscales, leading to low efficiency under compound transformations. Meanwhile,\nmost deep learning-based approaches only estimate similarity scores without\nexplicitly modeling geometric pose, making them inadequate for real-world\ndeployment. To overcome these limitations, we propose a lightweight end-to-end\nframework that reformulates template matching as joint localization and\ngeometric regression, outputting the center coordinates, rotation angle, and\nindependent horizontal and vertical scales. A Template-Aware Dynamic\nConvolution Module (TDCM) dynamically injects template features at inference to\nguide generalizable matching. The compact network integrates depthwise\nseparable convolutions and pixel shuffle for efficient matching. To enable\ngeometric-annotation-free training, we introduce a rotation-shear-based\naugmentation strategy with structure-aware pseudo labels. A lightweight\nrefinement module further improves angle and scale precision via local\noptimization. Experiments show our 3.07M model achieves high precision and 14ms\ninference under compound transformations. It also demonstrates strong\nrobustness in small-template and multi-object scenarios, making it highly\nsuitable for deployment in real-time industrial applications. The code is\navailable at:https://github.com/ZhouJ6610/PoseMatch-TDCM.", "AI": {"tldr": "A lightweight end-to-end framework for template matching that jointly estimates position, rotation, and scaling through geometric regression, achieving high precision and fast inference for industrial applications.", "motivation": "Traditional template matching methods are inefficient under compound transformations, while deep learning approaches lack explicit geometric pose modeling, making them inadequate for real-world deployment in industrial inspection tasks.", "method": "Proposes a framework that reformulates template matching as joint localization and geometric regression using Template-Aware Dynamic Convolution Module (TDCM) to inject template features, depthwise separable convolutions for efficiency, and rotation-shear-based augmentation for geometric-annotation-free training.", "result": "The 3.07M parameter model achieves high precision with 14ms inference time under compound transformations, demonstrating strong robustness in small-template and multi-object scenarios suitable for real-time industrial deployment.", "conclusion": "The proposed framework provides an efficient and practical solution for industrial template matching that explicitly models geometric pose while maintaining lightweight architecture and fast inference capabilities."}}
{"id": "2510.01671", "categories": ["cs.AI", "cs.HC", "68T01", "J.3"], "pdf": "https://arxiv.org/pdf/2510.01671", "abs": "https://arxiv.org/abs/2510.01671", "authors": ["Motoki Sato", "Yuki Matsushita", "Hidekazu Takahashi", "Tomoaki Kakazu", "Sou Nagata", "Mizuho Ohnuma", "Atsushi Yoshikawa", "Masayuki Yamamura"], "title": "A Locally Executable AI System for Improving Preoperative Patient Communication: A Multi-Domain Clinical Evaluation", "comment": "32 pages, 4 figures, 10 tables 32 pages, 4 figures, 10 tables. This\n  paper is currently under review at ACM Transactions on Computing for\n  Healthcare. Reproducibility resources:\n  http://github.com/motokinaru/LENOHA-medical-dialogue", "summary": "Patients awaiting invasive procedures often have unanswered pre-procedural\nquestions; however, time-pressured workflows and privacy constraints limit\npersonalized counseling. We present LENOHA (Low Energy, No Hallucination, Leave\nNo One Behind Architecture), a safety-first, local-first system that routes\ninputs with a high-precision sentence-transformer classifier and returns\nverbatim answers from a clinician-curated FAQ for clinical queries, eliminating\nfree-text generation in the clinical path. We evaluated two domains (tooth\nextraction and gastroscopy) using expert-reviewed validation sets\n(n=400/domain) for thresholding and independent test sets (n=200/domain). Among\nthe four encoders, E5-large-instruct (560M) achieved an overall accuracy of\n0.983 (95% CI 0.964-0.991), AUC 0.996, and seven total errors, which were\nstatistically indistinguishable from GPT-4o on this task; Gemini made no errors\non this test set. Energy logging shows that the non-generative clinical path\nconsumes ~1.0 mWh per input versus ~168 mWh per small-talk reply from a local\n8B SLM, a ~170x difference, while maintaining ~0.10 s latency on a single\non-prem GPU. These results indicate that near-frontier discrimination and\ngeneration-induced errors are structurally avoided in the clinical path by\nreturning vetted FAQ answers verbatim, supporting privacy, sustainability, and\nequitable deployment in bandwidth-limited environments.", "AI": {"tldr": "LENOHA is a safety-first clinical system that uses sentence-transformer classifiers to route patient queries and return verbatim answers from curated FAQs, avoiding free-text generation in clinical contexts.", "motivation": "Address unmet patient pre-procedural information needs while overcoming time constraints and privacy limitations in clinical workflows.", "method": "Uses high-precision sentence-transformer classifiers to route inputs and return verbatim answers from clinician-curated FAQs, eliminating free-text generation in clinical path. Evaluated on tooth extraction and gastroscopy domains with expert-reviewed validation and test sets.", "result": "E5-large-instruct achieved 0.983 accuracy, 0.996 AUC with only 7 errors, comparable to GPT-4o. Non-generative clinical path consumes ~1.0 mWh per input vs ~168 mWh for small-talk from local SLM (170x difference) with ~0.10s latency.", "conclusion": "Near-frontier discrimination can be achieved while structurally avoiding generation-induced errors by returning vetted FAQ answers verbatim, supporting privacy, sustainability and equitable deployment in bandwidth-limited environments."}}
{"id": "2510.01450", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01450", "abs": "https://arxiv.org/abs/2510.01450", "authors": ["Yifei Zuo", "Yutong Yin", "Zhichen Zeng", "Ang Li", "Banghua Zhu", "Zhaoran Wang"], "title": "Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression", "comment": null, "summary": "Transformer architectures have achieved remarkable success in various\ndomains. While efficient alternatives to Softmax Attention have been widely\nstudied, the search for more expressive mechanisms grounded in theoretical\ninsight-even at greater computational cost-has been relatively underexplored.\nIn this work, we bridge this gap by proposing Local Linear Attention (LLA), a\nnovel attention mechanism derived from nonparametric statistics through the\nlens of test-time regression. First, we show that LLA offers theoretical\nadvantages over Linear and Softmax Attention for associative memory via a\nbias-variance trade-off analysis. Next, we address its computational challenges\nand propose two memory-efficient primitives to tackle the $\\Theta(n^2 d)$ and\n$\\Theta(n d^2)$ complexity. We then introduce FlashLLA, a hardware-efficient,\nblockwise algorithm that enables scalable and parallel computation on modern\naccelerators. In addition, we implement and profile a customized inference\nkernel that significantly reduces memory overheads. Finally, we empirically\nvalidate the advantages and limitations of LLA on test-time regression,\nin-context regression, associative recall and state tracking tasks. Experiment\nresults demonstrate that LLA effectively adapts to non-stationarity,\noutperforming strong baselines in test-time training and in-context learning,\nand exhibiting promising evidence for its scalability and applicability in\nlarge-scale models. Code is available at\nhttps://github.com/Yifei-Zuo/Flash-LLA.", "AI": {"tldr": "Local Linear Attention (LLA) is a novel attention mechanism derived from nonparametric statistics that offers theoretical advantages over existing attention methods and is made computationally efficient through FlashLLA algorithm and custom kernels.", "motivation": "To bridge the gap in exploring more expressive attention mechanisms grounded in theoretical insight, even at greater computational cost, as efficient alternatives to Softmax Attention have been widely studied but more expressive mechanisms remain underexplored.", "method": "Proposed Local Linear Attention (LLA) derived from nonparametric statistics via test-time regression, with computational challenges addressed through memory-efficient primitives and FlashLLA - a hardware-efficient blockwise algorithm for modern accelerators, plus customized inference kernels.", "result": "LLA outperforms strong baselines in test-time training and in-context learning, effectively adapts to non-stationarity, and shows promising evidence for scalability in large-scale models across test-time regression, in-context regression, associative recall and state tracking tasks.", "conclusion": "LLA represents a theoretically grounded and practically efficient attention mechanism that bridges the gap between expressive power and computational feasibility, demonstrating advantages in various tasks and showing potential for large-scale applications."}}
{"id": "2510.01681", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01681", "abs": "https://arxiv.org/abs/2510.01681", "authors": ["Xuchen Li", "Xuzhao Li", "Jiahui Gao", "Renjie Pi", "Shiyu Hu", "Wentao Zhang"], "title": "Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning", "comment": "Preprint, Under review", "summary": "Vision-Language Models (VLMs) excel at many multimodal tasks, yet they\nfrequently struggle with tasks requiring precise understanding and handling of\nfine-grained visual elements. This is mainly due to information loss during\nimage encoding or insufficient attention to critical regions. Recent work has\nshown promise by incorporating pixel-level visual information into the\nreasoning process, enabling VLMs to access high-resolution visual details\nduring their thought process. However, this pixel-level information is often\noverused, leading to inefficiency and distraction from irrelevant visual\ndetails. To address these challenges, we propose the first framework for\nadaptive pixel reasoning that dynamically determines necessary pixel-level\noperations based on the input query. Specifically, we first apply\noperation-aware supervised fine-tuning to establish baseline competence in\ntextual reasoning and visual operations, then design a novel rollout-guided\nreinforcement learning framework relying on feedback of the model's own\nresponses, which enables the VLM to determine when pixel operations should be\ninvoked based on query difficulty. Experiments on extensive multimodal\nreasoning benchmarks show that our model achieves superior performance while\nsignificantly reducing unnecessary visual operations. Impressively, our model\nachieves 73.4\\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of\nonly 20.1\\%, improving accuracy and simultaneously reducing tool usage by\n66.5\\% compared to the previous methods.", "AI": {"tldr": "A framework for adaptive pixel reasoning in Vision-Language Models that dynamically determines when to use pixel-level operations based on query difficulty, achieving better performance while reducing unnecessary visual operations.", "motivation": "VLMs struggle with fine-grained visual understanding due to information loss during image encoding and insufficient attention to critical regions. Current pixel-level approaches are inefficient and get distracted by irrelevant details.", "method": "Operation-aware supervised fine-tuning followed by rollout-guided reinforcement learning that uses the model's own response feedback to determine when to invoke pixel operations based on query difficulty.", "result": "Achieves 73.4% accuracy on HR-Bench 4K with only 20.1% tool usage ratio, improving accuracy while reducing tool usage by 66.5% compared to previous methods.", "conclusion": "The adaptive pixel reasoning framework enables VLMs to achieve superior performance on multimodal reasoning tasks while significantly reducing unnecessary visual operations through dynamic operation selection."}}
{"id": "2510.01687", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01687", "abs": "https://arxiv.org/abs/2510.01687", "authors": ["John Hawkins"], "title": "Improving AGI Evaluation: A Data Science Perspective", "comment": null, "summary": "Evaluation of potential AGI systems and methods is difficult due to the\nbreadth of the engineering goal. We have no methods for perfect evaluation of\nthe end state, and instead measure performance on small tests designed to\nprovide directional indication that we are approaching AGI. In this work we\nargue that AGI evaluation methods have been dominated by a design philosophy\nthat uses our intuitions of what intelligence is to create synthetic tasks,\nthat have performed poorly in the history of AI. Instead we argue for an\nalternative design philosophy focused on evaluating robust task execution that\nseeks to demonstrate AGI through competence. This perspective is developed from\ncommon practices in data science that are used to show that a system can be\nreliably deployed. We provide practical examples of what this would mean for\nAGI evaluation.", "AI": {"tldr": "The paper argues that current AGI evaluation methods based on synthetic tasks are flawed and proposes an alternative approach focused on robust task execution and deployment competence.", "motivation": "Current AGI evaluation methods are inadequate because they rely on synthetic tasks designed from intuitions about intelligence, which have historically performed poorly in AI development.", "method": "Proposes an alternative design philosophy that evaluates robust task execution and deployment competence, drawing from common data science practices for reliable system deployment.", "result": "Provides practical examples of what this alternative AGI evaluation approach would entail, focusing on demonstrating competence through reliable task execution.", "conclusion": "AGI evaluation should shift from synthetic task-based approaches to methods that demonstrate robust task execution and deployment competence, similar to data science practices for reliable system deployment."}}
{"id": "2510.01456", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01456", "abs": "https://arxiv.org/abs/2510.01456", "authors": ["Brett Barkley", "Preston Culbertson", "David Fridovich-Keil"], "title": "SCOPED: Score-Curvature Out-of-distribution Proximity Evaluator for Diffusion", "comment": null, "summary": "Out-of-distribution (OOD) detection is essential for reliable deployment of\nmachine learning systems in vision, robotics, reinforcement learning, and\nbeyond. We introduce Score-Curvature Out-of-distribution Proximity Evaluator\nfor Diffusion (SCOPED), a fast and general-purpose OOD detection method for\ndiffusion models that reduces the number of forward passes on the trained model\nby an order of magnitude compared to prior methods, outperforming most\ndiffusion-based baselines and closely approaching the accuracy of the strongest\nones. SCOPED is computed from a single diffusion model trained once on a\ndiverse dataset, and combines the Jacobian trace and squared norm of the\nmodel's score function into a single test statistic. Rather than thresholding\non a fixed value, we estimate the in-distribution density of SCOPED scores\nusing kernel density estimation, enabling a flexible, unsupervised test that,\nin the simplest case, only requires a single forward pass and one\nJacobian-vector product (JVP), made efficient by Hutchinson's trace estimator.\nOn four vision benchmarks, SCOPED achieves competitive or state-of-the-art\nprecision-recall scores despite its low computational cost. The same method\ngeneralizes to robotic control tasks with shared state and action spaces,\nidentifying distribution shifts across reward functions and training regimes.\nThese results position SCOPED as a practical foundation for fast and reliable\nOOD detection in real-world domains, including perceptual artifacts in vision,\noutlier detection in autoregressive models, exploration in reinforcement\nlearning, and dataset curation for unsupervised training.", "AI": {"tldr": "SCOPED is a fast, efficient OOD detection method for diffusion models that reduces forward passes by 10x while maintaining competitive accuracy, using a single test statistic combining Jacobian trace and score function norm.", "motivation": "OOD detection is crucial for reliable ML deployment across vision, robotics, and RL, but existing methods are computationally expensive with many forward passes.", "method": "Combines Jacobian trace and squared norm of score function into single statistic, uses kernel density estimation for flexible unsupervised testing, requires only one forward pass and JVP with Hutchinson's estimator.", "result": "Achieves competitive/state-of-the-art precision-recall on four vision benchmarks with low computational cost, generalizes to robotic control tasks across reward functions and training regimes.", "conclusion": "SCOPED provides practical foundation for fast, reliable OOD detection in real-world domains including vision artifacts, outlier detection, RL exploration, and dataset curation."}}
{"id": "2510.01683", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01683", "abs": "https://arxiv.org/abs/2510.01683", "authors": ["Han-Jay Shu", "Wei-Ning Chiu", "Shun-Ting Chang", "Meng-Ping Huang", "Takeshi Tohyama", "Ahram Han", "Po-Chih Kuo"], "title": "Uncovering Overconfident Failures in CXR Models via Augmentation-Sensitivity Risk Scoring", "comment": "5 pages, 1 figures", "summary": "Deep learning models achieve strong performance in chest radiograph (CXR)\ninterpretation, yet fairness and reliability concerns persist. Models often\nshow uneven accuracy across patient subgroups, leading to hidden failures not\nreflected in aggregate metrics. Existing error detection approaches -- based on\nconfidence calibration or out-of-distribution (OOD) detection -- struggle with\nsubtle within-distribution errors, while image- and representation-level\nconsistency-based methods remain underexplored in medical imaging. We propose\nan augmentation-sensitivity risk scoring (ASRS) framework to identify\nerror-prone CXR cases. ASRS applies clinically plausible rotations ($\\pm\n15^\\circ$/$\\pm 30^\\circ$) and measures embedding shifts with the RAD-DINO\nencoder. Sensitivity scores stratify samples into stability quartiles, where\nhighly sensitive cases show substantially lower recall ($-0.2$ to $-0.3$)\ndespite high AUROC and confidence. ASRS provides a label-free means for\nselective prediction and clinician review, improving fairness and safety in\nmedical AI.", "AI": {"tldr": "ASRS framework uses clinically plausible rotations and embedding shifts to identify error-prone chest X-ray cases, improving fairness and safety in medical AI.", "motivation": "Deep learning models for chest radiograph interpretation show uneven accuracy across patient subgroups and hidden failures not reflected in aggregate metrics, requiring better error detection methods.", "method": "Augmentation-sensitivity risk scoring (ASRS) applies \u00b115\u00b0/\u00b130\u00b0 rotations and measures embedding shifts using RAD-DINO encoder to stratify samples into stability quartiles.", "result": "Highly sensitive cases show substantially lower recall (-0.2 to -0.3) despite high AUROC and confidence, enabling identification of error-prone cases.", "conclusion": "ASRS provides a label-free means for selective prediction and clinician review, improving fairness and safety in medical AI systems."}}
{"id": "2510.01700", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01700", "abs": "https://arxiv.org/abs/2510.01700", "authors": ["Rohan Wadhawan", "Fabrice Y Harel-Canada", "Zi-Yi Dou", "Suhaila Shakiah", "Robinson Piramuthu", "Nanyun Peng"], "title": "VaPR -- Vision-language Preference alignment for Reasoning", "comment": null, "summary": "Preference finetuning methods like Direct Preference Optimization (DPO) with\nAI-generated feedback have shown promise in aligning Large Vision-Language\nModels (LVLMs) with human preferences. However, existing techniques overlook\nthe prevalence of noise in synthetic preference annotations in the form of\nstylistic and length biases. To this end, we introduce a hard-negative response\ngeneration framework based on LLM-guided response editing, that produces\nrejected responses with targeted errors, maintaining stylistic and length\nsimilarity to the accepted ones. Using this framework, we develop the VaPR\ndataset, comprising 30K high-quality samples, to finetune three LVLM families:\nLLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver\nsignificant performance improvements across ten benchmarks, achieving average\ngains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable\nimprovements on reasoning tasks. A scaling analysis shows that performance\nconsistently improves with data size, with LLaVA models benefiting even at\nsmaller scales. Moreover, VaPR reduces the tendency to answer \"Yes\" in binary\nquestions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we\nshow that the framework generalizes to open-source LLMs as editors, with models\ntrained on VaPR-OS achieving ~99% of the performance of models trained on\n\\name, which is synthesized using GPT-4o. Our data, models, and code can be\nfound on the project page https://vap-r.github.io", "AI": {"tldr": "VaPR introduces a hard-negative response generation framework using LLM-guided editing to create rejected responses with targeted errors while maintaining stylistic and length similarity to accepted ones, addressing noise in synthetic preference annotations for LVLM alignment.", "motivation": "Existing preference finetuning methods overlook the prevalence of noise in synthetic preference annotations, particularly stylistic and length biases that can affect LVLM alignment with human preferences.", "method": "Developed a hard-negative response generation framework using LLM-guided response editing to produce rejected responses with targeted errors while maintaining stylistic and length similarity to accepted responses. Created VaPR dataset with 30K samples for finetuning LVLMs.", "result": "VaPR models achieved significant performance improvements: 6.5% average gain for LLaVA, 4.0% for Qwen2VL, and 1.5% for Qwen2.5VL across ten benchmarks, with notable improvements on reasoning tasks. Also reduced tendency to answer \"Yes\" in binary questions.", "conclusion": "The framework effectively addresses noise in synthetic preference data and generalizes to open-source LLMs as editors, with VaPR-OS achieving ~99% performance of GPT-4o-synthesized data. Scaling analysis shows consistent performance improvements with data size."}}
{"id": "2510.01457", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01457", "abs": "https://arxiv.org/abs/2510.01457", "authors": ["Brett Barkley", "David Fridovich-Keil"], "title": "Fixing That Free Lunch: When, Where, and Why Synthetic Data Fails in Model-Based Policy Optimization", "comment": null, "summary": "Synthetic data is a core component of data-efficient Dyna-style model-based\nreinforcement learning, yet it can also degrade performance. We study when it\nhelps, where it fails, and why, and we show that addressing the resulting\nfailure modes enables policy improvement that was previously unattainable. We\nfocus on Model-Based Policy Optimization (MBPO), which performs actor and\ncritic updates using synthetic action counterfactuals. Despite reports of\nstrong and generalizable sample-efficiency gains in OpenAI Gym, recent work\nshows that MBPO often underperforms its model-free counterpart, Soft\nActor-Critic (SAC), in the DeepMind Control Suite (DMC). Although both suites\ninvolve continuous control with proprioceptive robots, this shift leads to\nsharp performance losses across seven challenging DMC tasks, with MBPO failing\nin cases where claims of generalization from Gym would imply success. This\nreveals how environment-specific assumptions can become implicitly encoded into\nalgorithm design when evaluation is limited. We identify two coupled issues\nbehind these failures: scale mismatches between dynamics and reward models that\ninduce critic underestimation and hinder policy improvement during model-policy\ncoevolution, and a poor choice of target representation that inflates model\nvariance and produces error-prone rollouts. Addressing these failure modes\nenables policy improvement where none was previously possible, allowing MBPO to\noutperform SAC in five of seven tasks while preserving the strong performance\npreviously reported in OpenAI Gym. Rather than aiming only for incremental\naverage gains, we hope our findings motivate the community to develop\ntaxonomies that tie MDP task- and environment-level structure to algorithmic\nfailure modes, pursue unified solutions where possible, and clarify how\nbenchmark choices ultimately shape the conditions under which algorithms\ngeneralize.", "AI": {"tldr": "MBPO's performance drops in DeepMind Control Suite despite success in OpenAI Gym. Two failure modes identified: scale mismatches between dynamics/reward models causing critic underestimation, and poor target representation inflating model variance. Fixing these enables MBPO to outperform SAC in 5/7 DMC tasks while maintaining Gym performance.", "motivation": "To understand why MBPO underperforms in DeepMind Control Suite despite strong results in OpenAI Gym, and identify the specific failure modes that prevent policy improvement in certain environments.", "method": "Analyzed MBPO's performance across seven challenging DMC tasks, identified two coupled failure modes: scale mismatches between dynamics and reward models, and poor target representation choice that increases model variance.", "result": "After addressing the identified failure modes, MBPO outperformed SAC in five of seven DMC tasks while preserving its previously reported strong performance in OpenAI Gym.", "conclusion": "Environment-specific assumptions can become implicitly encoded into algorithm design when evaluation is limited. The community should develop taxonomies linking MDP structure to algorithmic failure modes and clarify how benchmark choices affect algorithm generalization."}}
{"id": "2510.01686", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01686", "abs": "https://arxiv.org/abs/2510.01686", "authors": ["Jiacong Xu", "Yiqun Mei", "Ke Zhang", "Vishal M. Patel"], "title": "FreeViS: Training-free Video Stylization with Inconsistent References", "comment": "Project Page: \\url{https://xujiacong.github.io/FreeViS/}", "summary": "Video stylization plays a key role in content creation, but it remains a\nchallenging problem. Na\\\"ively applying image stylization frame-by-frame hurts\ntemporal consistency and reduces style richness. Alternatively, training a\ndedicated video stylization model typically requires paired video data and is\ncomputationally expensive. In this paper, we propose FreeViS, a training-free\nvideo stylization framework that generates stylized videos with rich style\ndetails and strong temporal coherence. Our method integrates multiple stylized\nreferences to a pretrained image-to-video (I2V) model, effectively mitigating\nthe propagation errors observed in prior works, without introducing flickers\nand stutters. In addition, it leverages high-frequency compensation to\nconstrain the content layout and motion, together with flow-based motion cues\nto preserve style textures in low-saliency regions. Through extensive\nevaluations, FreeViS delivers higher stylization fidelity and superior temporal\nconsistency, outperforming recent baselines and achieving strong human\npreference. Our training-free pipeline offers a practical and economic solution\nfor high-quality, temporally coherent video stylization. The code and videos\ncan be accessed via https://xujiacong.github.io/FreeViS/", "AI": {"tldr": "FreeViS is a training-free video stylization framework that generates stylized videos with rich style details and strong temporal coherence by integrating multiple stylized references into a pretrained image-to-video model.", "motivation": "Video stylization is challenging because frame-by-frame image stylization hurts temporal consistency and reduces style richness, while training dedicated video models requires paired data and is computationally expensive.", "method": "Integrates multiple stylized references to a pretrained I2V model, uses high-frequency compensation to constrain content layout and motion, and employs flow-based motion cues to preserve style textures in low-saliency regions.", "result": "FreeViS delivers higher stylization fidelity and superior temporal consistency, outperforming recent baselines and achieving strong human preference without introducing flickers and stutters.", "conclusion": "The training-free pipeline offers a practical and economic solution for high-quality, temporally coherent video stylization."}}
{"id": "2510.01724", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01724", "abs": "https://arxiv.org/abs/2510.01724", "authors": ["Madina Bekbergenova", "Lucas Pradi", "Benjamin Navet", "Emma Tysinger", "Franck Michel", "Matthieu Feraud", "Yousouf Taghzouti", "Yan Zhou Chen", "Olivier Kirchhoffer", "Florence Mehl", "Martin Legrand", "Tao Jiang", "Marco Pagni", "Soha Hassoun", "Jean-Luc Wolfender", "Wout Bittremieux", "Fabien Gandon", "Louis-F\u00e9lix Nothias"], "title": "MetaboT: AI-based agent for natural language-based interaction with metabolomics knowledge graphs", "comment": null, "summary": "Mass spectrometry metabolomics generates vast amounts of data requiring\nadvanced methods for interpretation. Knowledge graphs address these challenges\nby structuring mass spectrometry data, metabolite information, and their\nrelationships into a connected network (Gaudry et al. 2024). However, effective\nuse of a knowledge graph demands an in-depth understanding of its ontology and\nits query language syntax. To overcome this, we designed MetaboT, an AI system\nutilizing large language models (LLMs) to translate user questions into SPARQL\nsemantic query language for operating on knowledge graphs (Steve Harris 2013).\nWe demonstrate its effectiveness using the Experimental Natural Products\nKnowledge Graph (ENPKG), a large-scale public knowledge graph for plant natural\nproducts (Gaudry et al. 2024).MetaboT employs specialized AI agents for\nhandling user queries and interacting with the knowledge graph by breaking down\ncomplex tasks into discrete components, each managed by a specialised agent\n(Fig. 1a). The multi-agent system is constructed using the LangChain and\nLangGraph libraries, which facilitate the integration of LLMs with external\ntools and information sources (LangChain, n.d.). The query generation process\nfollows a structured workflow. First, the Entry Agent determines if the\nquestion is new or a follow-up to previous interactions. New questions are\nforwarded to the Validator Agent, which verifies if the question is related to\nthe knowledge graph. Then, the valid question is sent to the Supervisor Agent,\nwhich identifies if the question requires chemical conversions or standardized\nidentifiers. In this case it delegates the question to the Knowledge Graph\nAgent, which can use tools to extract necessary details, such as URIs or\ntaxonomies of chemical names, from the user query. Finally, an agent\nresponsible for crafting the SPARQL queries equipped with the ontology of the\nknowledge graph uses the provided identifiers to generate the query. Then, the\nsystem executes the generated query against the metabolomics knowledge graph\nand returns structured results to the user (Fig. 1b). To assess the performance\nof MetaboT we have curated 50 metabolomics-related questions and their expected\nanswers. In addition to submitting these questions to MetaboT, we evaluated a\nbaseline by submitting them to a standard LLM (GPT-4o) with a prompt that\nincorporated the knowledge graph ontology but did not provide specific entity\nIDs. This baseline achieved only 8.16% accuracy, compared to MetaboT's 83.67%,\nunderscoring the necessity of our multi-agent system for accurately retrieving\nentities and generating correct SPARQL queries. MetaboT demonstrates promising\nperformance as a conversational question-answering assistant, enabling\nresearchers to retrieve structured metabolomics data through natural language\nqueries. By automating the generation and execution of SPARQL queries, it\nremoves technical barriers that have traditionally hindered access to knowledge\ngraphs. Importantly, MetaboT leverages the capabilities of LLMs while\nmaintaining experimentally grounded query generation, ensuring that outputs\nremain aligned with domain-specific standards and data structures. This\napproach facilitates data-driven discoveries by bridging the gap between\ncomplex semantic technologies and user-friendly interaction. MetaboT is\naccessible at [https://metabot.holobiomicslab.eu/], and its source code is\navailable at [https://github.com/HolobiomicsLab/MetaboT].", "AI": {"tldr": "MetaboT is an AI system that uses large language models to translate natural language questions into SPARQL queries for metabolomics knowledge graphs, achieving 83.67% accuracy compared to 8.16% for standard LLMs.", "motivation": "To overcome the technical barriers of using knowledge graphs in metabolomics, which require deep understanding of ontology and query language syntax, by enabling natural language interaction.", "method": "Multi-agent system using LangChain/LangGraph with specialized agents for validation, chemical conversion, and SPARQL query generation, tested on the Experimental Natural Products Knowledge Graph (ENPKG).", "result": "Achieved 83.67% accuracy on curated metabolomics questions, significantly outperforming baseline GPT-4o (8.16%), demonstrating effective automated query generation and execution.", "conclusion": "MetaboT successfully bridges the gap between complex semantic technologies and user-friendly interaction, enabling researchers to access structured metabolomics data through natural language queries while maintaining domain-specific standards."}}
{"id": "2510.01458", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01458", "abs": "https://arxiv.org/abs/2510.01458", "authors": ["Shawn Im", "Yixuan Li"], "title": "How Well Can Preference Optimization Generalize Under Noisy Feedback?", "comment": null, "summary": "As large language models (LLMs) advance their capabilities, aligning these\nmodels with human preferences has become crucial. Preference optimization,\nwhich trains models to distinguish between preferred and non-preferred\nresponses based on human feedback, has become a crucial component for aligning\nLLMs. However, most existing works assume noise-free feedback, which is\nunrealistic due to the inherent errors and inconsistencies in human judgments.\nThis paper addresses the impact of noisy feedback on preference optimization,\nproviding generalization guarantees under these conditions. In particular, we\nconsider noise models that correspond to common real-world sources of noise,\nsuch as mislabeling and uncertainty. Unlike traditional analyses that assume\nconvergence, our work focuses on finite-step preference optimization, offering\nnew insights that are more aligned with practical LLM training. We describe how\ngeneralization decays with different types of noise across levels of noise\nrates based on the preference data distribution and number of samples. Our\nanalysis for noisy preference learning applies to a broad family of preference\noptimization losses such as DPO, IPO, SLiC, etc. Empirical validation on\ncontemporary LLMs confirms the practical relevance of our findings, offering\nvaluable insights for developing AI systems that align with human preferences.", "AI": {"tldr": "This paper analyzes how noisy human feedback affects preference optimization for aligning large language models, providing generalization guarantees under realistic noise conditions.", "motivation": "Most existing preference optimization methods assume noise-free human feedback, which is unrealistic due to inherent errors and inconsistencies in human judgments. The paper addresses this gap by studying the impact of noisy feedback on model alignment.", "method": "The authors analyze noise models corresponding to common real-world sources like mislabeling and uncertainty, focusing on finite-step preference optimization rather than assuming convergence. They provide theoretical analysis for a broad family of preference optimization losses including DPO, IPO, and SLiC.", "result": "The paper describes how generalization decays with different types of noise across various noise rates, based on preference data distribution and sample size. Empirical validation on contemporary LLMs confirms the practical relevance of the findings.", "conclusion": "The work offers valuable insights for developing AI systems that better align with human preferences under realistic noisy feedback conditions, providing theoretical guarantees that are more aligned with practical LLM training scenarios."}}
{"id": "2510.01691", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01691", "abs": "https://arxiv.org/abs/2510.01691", "authors": ["Jiyao Liu", "Jinjie Wei", "Wanying Qu", "Chenglong Ma", "Junzhi Ning", "Yunheng Li", "Ying Chen", "Xinzhe Luo", "Pengcheng Chen", "Xin Gao", "Ming Hu", "Huihui Xu", "Xin Wang", "Shujian Gao", "Dingkang Yang", "Zhongying Deng", "Jin Ye", "Lihao Liu", "Junjun He", "Ningsheng Xu"], "title": "MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs", "comment": "26 pages, 13 figures", "summary": "Medical Image Quality Assessment (IQA) serves as the first-mile safety gate\nfor clinical AI, yet existing approaches remain constrained by scalar,\nscore-based metrics and fail to reflect the descriptive, human-like reasoning\nprocess central to expert evaluation. To address this gap, we introduce\nMedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning\nparadigm for language-based evaluation of medical image quality with\nMulti-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary\ntasks: (1) MedQ-Perception, which probes low-level perceptual capability via\nhuman-curated questions on fundamental visual attributes; and (2)\nMedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,\naligning model evaluation with human-like reasoning on image quality. The\nbenchmark spans five imaging modalities and over forty quality attributes,\ntotaling 2,600 perceptual queries and 708 reasoning assessments, covering\ndiverse image sources including authentic clinical acquisitions, images with\nsimulated degradations via physics-based reconstructions, and AI-generated\nimages. To evaluate reasoning ability, we propose a multi-dimensional judging\nprotocol that assesses model outputs along four complementary axes. We further\nconduct rigorous human-AI alignment validation by comparing LLM-based judgement\nwith radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates\nthat models exhibit preliminary but unstable perceptual and reasoning skills,\nwith insufficient accuracy for reliable clinical use. These findings highlight\nthe need for targeted optimization of MLLMs in medical IQA. We hope that\nMedQ-Bench will catalyze further exploration and unlock the untapped potential\nof MLLMs for medical image quality evaluation.", "AI": {"tldr": "MedQ-Bench introduces a comprehensive benchmark for medical image quality assessment using Multi-modal Large Language Models, establishing a perception-reasoning paradigm with two complementary tasks across five imaging modalities.", "motivation": "Existing medical IQA approaches are constrained by scalar, score-based metrics and fail to reflect the descriptive, human-like reasoning process central to expert evaluation, creating a gap in clinical AI safety.", "method": "MedQ-Bench defines two tasks: MedQ-Perception (low-level perceptual capability via human-curated questions) and MedQ-Reasoning (no-reference and comparison reasoning tasks). The benchmark spans five imaging modalities, 40+ quality attributes, with 2,600 perceptual queries and 708 reasoning assessments using diverse image sources including clinical acquisitions, simulated degradations, and AI-generated images.", "result": "Evaluation of 14 state-of-the-art MLLMs shows models exhibit preliminary but unstable perceptual and reasoning skills, with insufficient accuracy for reliable clinical use. Rigorous human-AI alignment validation compared LLM-based judgement with radiologists.", "conclusion": "Targeted optimization of MLLMs in medical IQA is needed. MedQ-Bench aims to catalyze further exploration and unlock the untapped potential of MLLMs for medical image quality evaluation."}}
{"id": "2510.01751", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01751", "abs": "https://arxiv.org/abs/2510.01751", "authors": ["Masike Malatji"], "title": "A cybersecurity AI agent selection and decision support framework", "comment": "6 figures, 6 tables, AI agents decision support framework", "summary": "This paper presents a novel, structured decision support framework that\nsystematically aligns diverse artificial intelligence (AI) agent architectures,\nreactive, cognitive, hybrid, and learning, with the comprehensive National\nInstitute of Standards and Technology (NIST) Cybersecurity Framework (CSF) 2.0.\nBy integrating agent theory with industry guidelines, this framework provides a\ntransparent and stepwise methodology for selecting and deploying AI solutions\nto address contemporary cyber threats. Employing a granular decomposition of\nNIST CSF 2.0 functions into specific tasks, the study links essential AI agent\nproperties such as autonomy, adaptive learning, and real-time responsiveness to\neach subcategory's security requirements. In addition, it outlines graduated\nlevels of autonomy (assisted, augmented, and fully autonomous) to accommodate\norganisations at varying stages of cybersecurity maturity. This holistic\napproach transcends isolated AI applications, providing a unified detection,\nincident response, and governance strategy. Through conceptual validation, the\nframework demonstrates how tailored AI agent deployments can align with\nreal-world constraints and risk profiles, enhancing situational awareness,\naccelerating response times, and fortifying long-term resilience via adaptive\nrisk management. Ultimately, this research bridges the gap between theoretical\nAI constructs and operational cybersecurity demands, establishing a foundation\nfor robust, empirically validated multi-agent systems that adhere to industry\nstandards.", "AI": {"tldr": "A structured decision support framework that aligns AI agent architectures with NIST Cybersecurity Framework 2.0 for systematic cybersecurity enhancement.", "motivation": "To bridge the gap between theoretical AI constructs and operational cybersecurity demands by providing a transparent methodology for selecting and deploying AI solutions against contemporary cyber threats.", "method": "Granular decomposition of NIST CSF 2.0 functions into specific tasks, linking AI agent properties (autonomy, adaptive learning, real-time responsiveness) to security requirements, and outlining graduated autonomy levels for different cybersecurity maturity stages.", "result": "Conceptual validation shows the framework enables tailored AI agent deployments that enhance situational awareness, accelerate response times, and fortify long-term resilience through adaptive risk management.", "conclusion": "The research establishes a foundation for robust, empirically validated multi-agent systems that adhere to industry standards, providing unified detection, incident response, and governance strategy."}}
{"id": "2510.01459", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01459", "abs": "https://arxiv.org/abs/2510.01459", "authors": ["Weizhe Chen", "Sven Koenig", "Bistra Dilkina"], "title": "LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning", "comment": null, "summary": "Since the release of Deepseek-R1, reinforcement learning with verifiable\nrewards (RLVR) has become a central approach for training large language models\n(LLMs) on reasoning tasks. Recent work has largely focused on modifying loss\nfunctions to make RLVR more efficient and effective. In this paper, motivated\nby studies of overthinking in LLMs, we propose Length-aware Sampling for Policy\nOptimization (LSPO), a novel meta-RLVR algorithm that dynamically selects\ntraining data at each step based on the average response length. We evaluate\nLSPO across multiple base models and datasets, demonstrating that it\nconsistently improves learning effectiveness. In addition, we conduct a\ndetailed ablation study to examine alternative ways of incorporating length\nsignals into dynamic sampling, offering further insights and highlighting\npromising directions for future research.", "AI": {"tldr": "LSPO is a meta-RLVR algorithm that dynamically selects training data based on average response length to improve reinforcement learning effectiveness for large language models.", "motivation": "Motivated by studies of overthinking in LLMs, the authors aim to make RLVR more efficient by addressing how response length affects learning.", "method": "Proposed Length-aware Sampling for Policy Optimization (LSPO), a meta-RLVR algorithm that dynamically selects training data at each step based on average response length.", "result": "LSPO consistently improves learning effectiveness across multiple base models and datasets, with ablation studies providing insights into length signal incorporation.", "conclusion": "LSPO demonstrates the value of dynamic length-aware sampling in RLVR, offering promising directions for future research in optimizing LLM training."}}
{"id": "2510.01704", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01704", "abs": "https://arxiv.org/abs/2510.01704", "authors": ["Pierre Musacchio", "Hyunmin Lee", "Jaesik Park"], "title": "Holistic Order Prediction in Natural Scenes", "comment": "25 pages, 11 figures, 6 tables", "summary": "Even in controlled settings, understanding instance-wise geometries is a\nchallenging task for a wide range of visual models. Although specialized\nsystems exist, modern arts rely on expensive input formats (category labels,\nbinary segmentation masks) and inference costs (a quadratic amount of forward\npasses). We mitigate these limitations by proposing InstaFormer, a network\ncapable of holistic order prediction. That is, solely given an input RGB image,\nInstaFormer returns the full occlusion and depth orderings for all the\ninstances in the scene in a single forward pass. At its core, InstaFormer\nrelies on interactions between object queries and latent mask descriptors that\nsemantically represent the same objects while carrying complementary\ninformation. We comprehensively benchmark and ablate our approach to highlight\nits effectiveness. Our code and models are open-source and available at this\nURL: https://github.com/SNU-VGILab/InstaOrder.", "AI": {"tldr": "InstaFormer is a network that predicts full occlusion and depth orderings for all instances in a scene from a single RGB image input, using interactions between object queries and latent mask descriptors.", "motivation": "Existing methods for understanding instance-wise geometries require expensive input formats (category labels, segmentation masks) and have high inference costs (quadratic forward passes).", "method": "InstaFormer uses interactions between object queries and latent mask descriptors that semantically represent the same objects while carrying complementary information, enabling holistic order prediction in a single forward pass.", "result": "The approach is comprehensively benchmarked and ablated to demonstrate its effectiveness.", "conclusion": "InstaFormer mitigates limitations of existing methods by providing occlusion and depth orderings from RGB images with reduced computational cost."}}
{"id": "2510.01800", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01800", "abs": "https://arxiv.org/abs/2510.01800", "authors": ["Thanh Ma", "Tri-Tam La", "Lam-Thu Le Huu", "Minh-Nghi Nguyen", "Khanh-Van Pham Luu", "Huu-Hoa Nguyen"], "title": "REBot: From RAG to CatRAG with Semantic Enrichment and Graph Routing", "comment": null, "summary": "Academic regulation advising is essential for helping students interpret and\ncomply with institutional policies, yet building effective systems requires\ndomain specific regulatory resources. To address this challenge, we propose\nREBot, an LLM enhanced advisory chatbot powered by CatRAG, a hybrid retrieval\nreasoning framework that integrates retrieval augmented generation with graph\nbased reasoning. CatRAG unifies dense retrieval and graph reasoning, supported\nby a hierarchical, category labeled knowledge graph enriched with semantic\nfeatures for domain alignment. A lightweight intent classifier routes queries\nto the appropriate retrieval modules, ensuring both factual accuracy and\ncontextual depth. We construct a regulation specific dataset and evaluate REBot\non classification and question answering tasks, achieving state of the art\nperformance with an F1 score of 98.89%. Finally, we implement a web application\nthat demonstrates the practical value of REBot in real world academic advising\nscenarios.", "AI": {"tldr": "REBot is an LLM-enhanced advisory chatbot for academic regulation advising that uses CatRAG, a hybrid retrieval-reasoning framework combining dense retrieval with graph-based reasoning on a hierarchical knowledge graph.", "motivation": "Building effective academic regulation advising systems requires domain-specific regulatory resources, which is challenging to acquire and process.", "method": "Proposed CatRAG framework integrates retrieval augmented generation with graph-based reasoning, using a hierarchical category-labeled knowledge graph with semantic features. A lightweight intent classifier routes queries to appropriate retrieval modules.", "result": "Achieved state-of-the-art performance with 98.89% F1 score on regulation-specific classification and question answering tasks. Implemented a web application demonstrating practical value in real-world academic advising.", "conclusion": "REBot with CatRAG framework effectively addresses the challenge of academic regulation advising by combining LLM capabilities with structured domain knowledge through hybrid retrieval-reasoning approach."}}
{"id": "2510.01460", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01460", "abs": "https://arxiv.org/abs/2510.01460", "authors": ["Lu Li", "Tianwei Ni", "Yihao Sun", "Pierre-Luc Bacon"], "title": "The Three Regimes of Offline-to-Online Reinforcement Learning", "comment": null, "summary": "Offline-to-online reinforcement learning (RL) has emerged as a practical\nparadigm that leverages offline datasets for pretraining and online\ninteractions for fine-tuning. However, its empirical behavior is highly\ninconsistent: design choices of online-fine tuning that work well in one\nsetting can fail completely in another. We propose a stability--plasticity\nprinciple that can explain this inconsistency: we should preserve the knowledge\nof pretrained policy or offline dataset during online fine-tuning, whichever is\nbetter, while maintaining sufficient plasticity. This perspective identifies\nthree regimes of online fine-tuning, each requiring distinct stability\nproperties. We validate this framework through a large-scale empirical study,\nfinding that the results strongly align with its predictions in 45 of 63 cases.\nThis work provides a principled framework for guiding design choices in\noffline-to-online RL based on the relative performance of the offline dataset\nand the pretrained policy.", "AI": {"tldr": "The paper proposes a stability-plasticity principle for offline-to-online RL to address inconsistent empirical behavior, identifying three regimes of online fine-tuning that require different stability properties.", "motivation": "Offline-to-online RL shows highly inconsistent empirical behavior where design choices that work in one setting fail in others, creating a need for principled guidance.", "method": "Proposed a stability-plasticity principle that preserves better knowledge (from pretrained policy or offline dataset) while maintaining plasticity, validated through large-scale empirical study across 63 cases.", "result": "The framework's predictions strongly aligned with empirical results in 45 out of 63 cases, demonstrating its effectiveness in explaining and guiding offline-to-online RL behavior.", "conclusion": "The stability-plasticity principle provides a principled framework for making design choices in offline-to-online RL based on the relative performance of offline datasets and pretrained policies."}}
{"id": "2510.01715", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01715", "abs": "https://arxiv.org/abs/2510.01715", "authors": ["Raahul Krishna Durairaju", "K. Saruladha"], "title": "PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning", "comment": null, "summary": "Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based\nalgorithm, enabling AI-driven artistic image synthesis. However, existing CNN\nand transformer-based models struggle to scale efficiently to complex styles\nand high-resolution inputs. We introduce PyramidStyler, a transformer framework\nwith Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding\nthat captures both local details and global context while reducing\ncomputational load. We further incorporate reinforcement learning to\ndynamically optimize stylization, accelerating convergence. Trained on\nMicrosoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to\n2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s\ninference--and yields further improvements (content 2.03; style 0.75) with\nminimal speed penalty (1.40 s) when using RL. These results demonstrate\nreal-time, high-quality artistic rendering, with broad applications in media\nand design.", "AI": {"tldr": "PyramidStyler is a transformer framework with Pyramidal Positional Encoding that improves neural style transfer by capturing multi-scale details and using reinforcement learning for optimization, achieving faster convergence and better quality.", "motivation": "Existing CNN and transformer-based neural style transfer models struggle with scaling to complex styles and high-resolution inputs efficiently.", "method": "Transformer framework with Pyramidal Positional Encoding (hierarchical multi-scale encoding) and reinforcement learning for dynamic optimization.", "result": "Reduced content loss by 62.6% (to 2.07) and style loss by 57.4% (to 0.86) after 4000 epochs with 1.39s inference time; further improved to content 2.03 and style 0.75 with RL while maintaining speed (1.40s).", "conclusion": "Demonstrates real-time, high-quality artistic rendering with broad applications in media and design."}}
{"id": "2510.01815", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01815", "abs": "https://arxiv.org/abs/2510.01815", "authors": ["Clara Maathuis", "Kasper Cools"], "title": "Human-AI Teaming Co-Learning in Military Operations", "comment": "Submitted to Sensors + Imaging; presented on 18th of September\n  (Artificial Intelligence for Security and Defence Applications III)", "summary": "In a time of rapidly evolving military threats and increasingly complex\noperational environments, the integration of AI into military operations proves\nsignificant advantages. At the same time, this implies various challenges and\nrisks regarding building and deploying human-AI teaming systems in an effective\nand ethical manner. Currently, understanding and coping with them are often\ntackled from an external perspective considering the human-AI teaming system as\na collective agent. Nevertheless, zooming into the dynamics involved inside the\nsystem assures dealing with a broader palette of relevant multidimensional\nresponsibility, safety, and robustness aspects. To this end, this research\nproposes the design of a trustworthy co-learning model for human-AI teaming in\nmilitary operations that encompasses a continuous and bidirectional exchange of\ninsights between the human and AI agents as they jointly adapt to evolving\nbattlefield conditions. It does that by integrating four dimensions. First,\nadjustable autonomy for dynamically calibrating the autonomy levels of agents\ndepending on aspects like mission state, system confidence, and environmental\nuncertainty. Second, multi-layered control which accounts continuous oversight,\nmonitoring of activities, and accountability. Third, bidirectional feedback\nwith explicit and implicit feedback loops between the agents to assure a proper\ncommunication of reasoning, uncertainties, and learned adaptations that each of\nthe agents has. And fourth, collaborative decision-making which implies the\ngeneration, evaluation, and proposal of decisions associated with confidence\nlevels and rationale behind them. The model proposed is accompanied by concrete\nexemplifications and recommendations that contribute to further developing\nresponsible and trustworthy human-AI teaming systems in military operations.", "AI": {"tldr": "Proposes a trustworthy co-learning model for human-AI teaming in military operations with four key dimensions: adjustable autonomy, multi-layered control, bidirectional feedback, and collaborative decision-making.", "motivation": "Address challenges and risks in building and deploying human-AI teaming systems in military operations from an internal perspective, focusing on responsibility, safety, and robustness aspects.", "method": "Designs a co-learning model with four integrated dimensions: adjustable autonomy for dynamic calibration, multi-layered control for oversight, bidirectional feedback loops, and collaborative decision-making with confidence levels.", "result": "The model enables continuous bidirectional exchange of insights between human and AI agents as they jointly adapt to evolving battlefield conditions.", "conclusion": "Provides concrete exemplifications and recommendations for developing responsible and trustworthy human-AI teaming systems in military operations."}}
{"id": "2510.01471", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01471", "abs": "https://arxiv.org/abs/2510.01471", "authors": ["Haotian Xiang", "Jinwen Xu", "Qin Lu"], "title": "Fine-tuning LLMs with variational Bayesian last layer for high-dimensional Bayesian optimzation", "comment": null, "summary": "A plethora of applications entail solving black-box optimization problems\nwith high evaluation costs, including drug discovery, material design, as well\nas hyperparameter tuning. Toward finding the global optimum of such black-box\noptimization problems with sample efficiency, Bayesian optimization (BO) is a\ntheoretically elegant framework that relies on a probabilistic surrogate model\nso as to iteratively select the query point with well-balanced\nexploration-exploitation tradeoffs. The Gaussian process (GP), as the de-facto\nchoice for surrogate modeling, has achieved compelling performances for vanilla\nBO with low-dimensional continuous variables. However, GPs fall short in coping\nwith high-dimensional counterparts with {\\it irregular} variables (e.g.,\ncategorical, ordinal, etc.). To alleviate this, neural network-based surrogates\nhave been explored. Inspired by the powerful capabilities of LLMs, we adopt the\nLLM as the surrogate to model the mapping from the high-dimensional input\nvariables to the objective function. To adapt to the current problem, we\nleverage the low-rank adaptation (LoRA) to fine-tune the LLM parameters\ntogether with the posterior of a linear regression head via the variational\nBayesian last layer (VBLL) framework. The resulting LoRA-VBLL is not only\ncomputationally light compared to existing alternatives, but also admits\nrecursive updates. To automate the critical selection of the LoRA rank as well\nas other hyperparameters, a weighted ensemble (ENS) of LoRA-VBLL surrogates has\nbeen devised, which further accommodates continual update of the per-model\nweight and individual LoRA-VBLL parameters via recursive Bayes. Extensive\nexperimental results demonstrate the compelling performance of the proposed\n(ENS-)LoRA-VBLL approaches on various high-dimensional benchmarks and the\nreal-world molecular optimization tasks.", "AI": {"tldr": "The paper proposes LoRA-VBLL, a novel Bayesian optimization approach using LLMs as surrogates for high-dimensional black-box optimization with irregular variables, enhanced by ensemble methods for automated hyperparameter selection.", "motivation": "Bayesian optimization struggles with high-dimensional problems containing irregular variables (categorical, ordinal). Traditional Gaussian process surrogates are inadequate, and existing neural network alternatives are computationally expensive.", "method": "Use LLM as surrogate model with LoRA fine-tuning and variational Bayesian last layer (VBLL) for efficient parameter updates. Develop ensemble (ENS) approach for automated hyperparameter selection and continuous model updates.", "result": "Extensive experiments show compelling performance on high-dimensional benchmarks and real-world molecular optimization tasks, outperforming existing alternatives with computational efficiency.", "conclusion": "LoRA-VBLL provides an effective and computationally light solution for high-dimensional Bayesian optimization with irregular variables, with ensemble methods further enhancing performance through automated hyperparameter tuning."}}
{"id": "2510.01767", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01767", "abs": "https://arxiv.org/abs/2510.01767", "authors": ["Sheng-Hsiang Hung", "Ting-Yu Yen", "Wei-Fang Sun", "Simon See", "Shih-Hsuan Hung", "Hung-Kuo Chu"], "title": "LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has established itself as an efficient\nrepresentation for real-time, high-fidelity 3D scene reconstruction. However,\nscaling 3DGS to large and unbounded scenes such as city blocks remains\ndifficult. Existing divide-and-conquer methods alleviate memory pressure by\npartitioning the scene into blocks, but introduce new bottlenecks: (i)\npartitions suffer from severe load imbalance since uniform or heuristic splits\ndo not reflect actual computational demands, and (ii) coarse-to-fine pipelines\nfail to exploit the coarse stage efficiently, often reloading the entire model\nand incurring high overhead. In this work, we introduce LoBE-GS, a novel\nLoad-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers\nthe large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning\nmethod that reduces preprocessing from hours to minutes, an optimization-based\nstrategy that balances visible Gaussians -- a strong proxy for computational\nload -- across blocks, and two lightweight techniques, visibility cropping and\nselective densification, to further reduce training cost. Evaluations on\nlarge-scale urban and outdoor datasets show that LoBE-GS consistently achieves\nup to $2\\times$ faster end-to-end training time than state-of-the-art\nbaselines, while maintaining reconstruction quality and enabling scalability to\nscenes infeasible with vanilla 3DGS.", "AI": {"tldr": "LoBE-GS is a load-balanced and efficient 3D Gaussian Splatting framework that addresses scalability issues in large scenes through depth-aware partitioning, optimization-based load balancing, and lightweight optimization techniques.", "motivation": "Scaling 3D Gaussian Splatting to large unbounded scenes like city blocks is challenging due to memory constraints and inefficient partitioning methods that suffer from load imbalance and high overhead in existing divide-and-conquer approaches.", "method": "Introduces depth-aware partitioning for fast preprocessing, optimization-based strategy to balance computational load across blocks, and lightweight techniques including visibility cropping and selective densification to reduce training costs.", "result": "Achieves up to 2x faster end-to-end training time compared to state-of-the-art baselines while maintaining reconstruction quality and enabling scalability to scenes that are infeasible with vanilla 3DGS.", "conclusion": "LoBE-GS successfully addresses the scalability limitations of 3D Gaussian Splatting for large scenes through efficient load balancing and optimization techniques, making large-scale urban and outdoor scene reconstruction more practical."}}
{"id": "2510.01833", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01833", "abs": "https://arxiv.org/abs/2510.01833", "authors": ["Zhihao Dou", "Qinjian Zhao", "Zhongwei Wan", "Dinggen Zhang", "Weida Wang", "Towsif Raiyan", "Benteng Chen", "Qingtao Pan", "Yang Ouyang", "Zhiqiang Gao", "Shufei Zhang", "Sumon Biswas"], "title": "Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM Reasoning", "comment": "19 pages and 5 figures", "summary": "Large language models (LLMs) have demonstrated remarkable reasoning abilities\nin complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However,\ndue to their autoregressive token-level generation, the reasoning process is\nlargely constrained to local decision-making and lacks global planning. This\nlimitation frequently results in redundant, incoherent, or inaccurate\nreasoning, which significantly degrades overall performance. Existing\napproaches, such as tree-based algorithms and reinforcement learning (RL),\nattempt to address this issue but suffer from high computational costs and\noften fail to produce optimal reasoning trajectories. To tackle this challenge,\nwe propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy\nOptimization PTA-GRPO, a two-stage framework designed to improve both\nhigh-level planning and fine-grained CoT reasoning. In the first stage, we\nleverage advanced LLMs to distill CoT into compact high-level guidance, which\nis then used for supervised fine-tuning (SFT). In the second stage, we\nintroduce a guidance-aware RL method that jointly optimizes the final output\nand the quality of high-level guidance, thereby enhancing reasoning\neffectiveness. We conduct extensive experiments on multiple mathematical\nreasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across\ndiverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and\nLLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently\nachieves stable and significant improvements across different models and tasks,\nvalidating its effectiveness and generalization.", "AI": {"tldr": "PTA-GRPO is a two-stage framework that improves LLM reasoning by first distilling Chain-of-Thought into high-level guidance through SFT, then using guidance-aware RL to jointly optimize final outputs and guidance quality.", "motivation": "Current LLM reasoning with Chain-of-Thought is constrained to local decision-making without global planning, leading to redundant, incoherent, or inaccurate reasoning that degrades performance. Existing approaches have high computational costs and fail to produce optimal reasoning trajectories.", "method": "Two-stage framework: (1) Use advanced LLMs to distill CoT into compact high-level guidance for supervised fine-tuning, (2) Introduce guidance-aware reinforcement learning that jointly optimizes final output and high-level guidance quality.", "result": "Extensive experiments on MATH, AIME2024, AIME2025, and AMC benchmarks with Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and LLaMA3.2-3B models show PTA-GRPO achieves stable and significant improvements across different models and tasks.", "conclusion": "PTA-GRPO effectively enhances both high-level planning and fine-grained CoT reasoning, demonstrating strong generalization and consistent performance improvements across diverse mathematical reasoning tasks and base models."}}
{"id": "2510.01472", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01472", "abs": "https://arxiv.org/abs/2510.01472", "authors": ["Hengyi Zhu", "Grace Li Zhang", "Shaoyi Huang"], "title": "PEL-NAS: Search Space Partitioned Architecture Prompt Co-Evolutionary LLM-driven Hardware-Aware Neural Architecture Search", "comment": null, "summary": "Hardware-Aware Neural Architecture Search (HW-NAS) requires joint\noptimization of accuracy and latency under device constraints. Traditional\nsupernet-based methods require multiple GPU days per dataset. Large Language\nModel (LLM)-driven approaches avoid training a large supernet and can provide\nquick feedback, but we observe an exploration bias: the LLM repeatedly proposes\nneural network designs within limited search space and fails to discover\narchitectures across different latency ranges in the entire search space. To\naddress this issue, we propose PEL-NAS: a search space Partitioned,\narchitecture prompt co-Evolutionary and LLM-driven Neural Architecture Search\nthat can generate neural networks with high accuracy and low latency with\nreduced search cost. Our proposed PEL-NAS has three key components: 1) a\ncomplexity-driven partitioning engine that divides the search space by\ncomplexity to enforce diversity and mitigate exploration bias; 2) an\nLLM-powered architecture prompt co-evolution operator, in which the LLM first\nupdates a knowledge base of design heuristics based on results from the\nprevious round, then performs a guided evolution algorithm on architectures\nwith prompts that incorporate this knowledge base. Prompts and designs improve\ntogether across rounds which avoids random guesswork and improve efficiency; 3)\na zero-cost predictor to avoid training a large number of candidates from\nscratch. Experimental results show that on HW-NAS-Bench, PEL-NAS can achieve\noverall higher HV, lower IGD, and up to 54% lower latency than baselines at\nsimilar accuracy. Meanwhile, the search cost drops from days to minutes\ncompared with traditional supernet baselines.", "AI": {"tldr": "PEL-NAS is a novel LLM-driven neural architecture search method that addresses exploration bias in traditional approaches by partitioning search space, using co-evolutionary prompts, and zero-cost predictors to efficiently find high-accuracy, low-latency neural networks.", "motivation": "Traditional HW-NAS methods require multiple GPU days per dataset, while LLM-driven approaches suffer from exploration bias where they repeatedly propose designs within limited search space and fail to discover architectures across different latency ranges.", "method": "Three key components: 1) complexity-driven partitioning engine to divide search space and enforce diversity; 2) LLM-powered architecture prompt co-evolution operator that updates knowledge base and performs guided evolution; 3) zero-cost predictor to avoid training candidates from scratch.", "result": "On HW-NAS-Bench, PEL-NAS achieves higher HV, lower IGD, and up to 54% lower latency than baselines at similar accuracy. Search cost drops from days to minutes compared with traditional supernet baselines.", "conclusion": "PEL-NAS effectively addresses exploration bias in LLM-driven NAS, enabling efficient discovery of diverse neural architectures with high accuracy and low latency while significantly reducing search time from days to minutes."}}
{"id": "2510.01784", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01784", "abs": "https://arxiv.org/abs/2510.01784", "authors": ["Xiaofei Wu", "Guozhen Zhang", "Zhiyong Xu", "Yuan Zhou", "Qinglin Lu", "Xuming He"], "title": "Pack and Force Your Memory: Long-form and Consistent Video Generation", "comment": null, "summary": "Long-form video generation presents a dual challenge: models must capture\nlong-range dependencies while preventing the error accumulation inherent in\nautoregressive decoding. To address these challenges, we make two\ncontributions. First, for dynamic context modeling, we propose MemoryPack, a\nlearnable context-retrieval mechanism that leverages both textual and image\ninformation as global guidance to jointly model short- and long-term\ndependencies, achieving minute-level temporal consistency. This design scales\ngracefully with video length, preserves computational efficiency, and maintains\nlinear complexity. Second, to mitigate error accumulation, we introduce Direct\nForcing, an efficient single-step approximating strategy that improves\ntraining-inference alignment and thereby curtails error propagation during\ninference. Together, MemoryPack and Direct Forcing substantially enhance the\ncontext consistency and reliability of long-form video generation, advancing\nthe practical usability of autoregressive video models.", "AI": {"tldr": "Proposes MemoryPack for long-range dependency modeling and Direct Forcing to reduce error accumulation in long-form video generation.", "motivation": "Address challenges in long-form video generation: capturing long-range dependencies while preventing error accumulation in autoregressive decoding.", "method": "MemoryPack: learnable context-retrieval mechanism using text and image as global guidance; Direct Forcing: single-step approximating strategy for better training-inference alignment.", "result": "Achieves minute-level temporal consistency, scales gracefully with video length, maintains linear complexity, and reduces error propagation.", "conclusion": "MemoryPack and Direct Forcing enhance context consistency and reliability, advancing practical usability of autoregressive video models."}}
{"id": "2510.01857", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01857", "abs": "https://arxiv.org/abs/2510.01857", "authors": ["Claudio Fanconi", "Nicol\u00e1s Astorga", "Mihaela van der Schaar"], "title": "Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning", "comment": null, "summary": "We reframe and operationalise adversarial inverse reinforcement learning\n(IRL) to large language model reasoning, learning a dense, token-level reward\nmodel for process supervision directly from expert demonstrations rather than\nimitating style via supervised fine-tuning. The learned reasoning reward serves\ntwo complementary roles: (i) it provides step-level feedback to optimise a\nreasoning policy during training; and (ii) it functions at inference as a\ncritic to rerank sampled traces under fixed compute budgets. We demonstrate\nthat our approach prioritises correctness over surface form, yielding scores\nthat correlate with eventual answer validity and enabling interpretable\nlocalisation of errors within a trace. Empirically, on GSM8K with Llama3 and\nQwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a\nlearning signal to elicit reasoning, and (ii) predictive performance is\nimproved from reward-guided reranking (notably for Llama-based policies). By\nunifying training signals, inference-time selection, and token-level\ndiagnostics into a single reasoning reward, this work suggests reusable\nprocess-level rewards with broad potential to enhance multi-step reasoning in\nlanguage models.", "AI": {"tldr": "This paper reframes adversarial inverse reinforcement learning for large language model reasoning, learning dense token-level rewards from expert demonstrations to provide step-level feedback during training and inference-time reranking.", "motivation": "To develop a method that prioritizes correctness over surface form in language model reasoning, enabling interpretable error localization and improved reasoning performance through process-level supervision.", "method": "Adversarial inverse reinforcement learning is used to learn dense, token-level reward models from expert demonstrations. The learned reward serves dual purposes: providing step-level feedback during training and functioning as a critic for reranking sampled traces during inference.", "result": "Empirical results on GSM8K with Llama3 and Qwen2.5 show that dense reasoning rewards can elicit reasoning as a learning signal and improve predictive performance through reward-guided reranking, particularly for Llama-based policies.", "conclusion": "The approach unifies training signals, inference-time selection, and token-level diagnostics into a single reasoning reward, suggesting reusable process-level rewards with broad potential to enhance multi-step reasoning in language models."}}
{"id": "2510.01479", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01479", "abs": "https://arxiv.org/abs/2510.01479", "authors": ["Shriram Karpoora Sundara Pandian", "Ali Baheri"], "title": "Density-Ratio Weighted Behavioral Cloning: Learning Control Policies from Corrupted Datasets", "comment": null, "summary": "Offline reinforcement learning (RL) enables policy optimization from fixed\ndatasets, making it suitable for safety-critical applications where online\nexploration is infeasible. However, these datasets are often contaminated by\nadversarial poisoning, system errors, or low-quality samples, leading to\ndegraded policy performance in standard behavioral cloning (BC) and offline RL\nmethods. This paper introduces Density-Ratio Weighted Behavioral Cloning\n(Weighted BC), a robust imitation learning approach that uses a small, verified\nclean reference set to estimate trajectory-level density ratios via a binary\ndiscriminator. These ratios are clipped and used as weights in the BC objective\nto prioritize clean expert behavior while down-weighting or discarding\ncorrupted data, without requiring knowledge of the contamination mechanism. We\nestablish theoretical guarantees showing convergence to the clean expert policy\nwith finite-sample bounds that are independent of the contamination rate. A\ncomprehensive evaluation framework is established, which incorporates various\npoisoning protocols (reward, state, transition, and action) on continuous\ncontrol benchmarks. Experiments demonstrate that Weighted BC maintains\nnear-optimal performance even at high contamination ratios outperforming\nbaselines such as traditional BC, batch-constrained Q-learning (BCQ) and\nbehavior regularized actor-critic (BRAC).", "AI": {"tldr": "Weighted BC is a robust offline RL method that uses density ratios from a clean reference set to weight behavioral cloning, effectively handling dataset contamination without knowing the contamination mechanism.", "motivation": "Standard offline RL methods degrade when datasets contain adversarial poisoning, system errors, or low-quality samples, making them unreliable for safety-critical applications.", "method": "Uses a small clean reference set to estimate trajectory-level density ratios via binary discriminator, then clips and applies these ratios as weights in behavioral cloning objective to prioritize clean data.", "result": "Achieves near-optimal performance even at high contamination ratios, outperforming BC, BCQ, and BRAC across various poisoning protocols (reward, state, transition, action) on continuous control benchmarks.", "conclusion": "Weighted BC provides theoretical guarantees for convergence to clean expert policy with finite-sample bounds independent of contamination rate, offering a practical solution for robust offline RL."}}
{"id": "2510.01829", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01829", "abs": "https://arxiv.org/abs/2510.01829", "authors": ["Cornelius Schr\u00f6der", "Marius-Raphael Schl\u00fcter", "Markus Lienkamp"], "title": "Calibrating the Full Predictive Class Distribution of 3D Object Detectors for Autonomous Driving", "comment": null, "summary": "In autonomous systems, precise object detection and uncertainty estimation\nare critical for self-aware and safe operation. This work addresses confidence\ncalibration for the classification task of 3D object detectors. We argue that\nit is necessary to regard the calibration of the full predictive confidence\ndistribution over all classes and deduce a metric which captures the\ncalibration of dominant and secondary class predictions. We propose two\nauxiliary regularizing loss terms which introduce either calibration of the\ndominant prediction or the full prediction vector as a training goal. We\nevaluate a range of post-hoc and train-time methods for CenterPoint, PillarNet\nand DSVT-Pillar and find that combining our loss term, which regularizes for\ncalibration of the full class prediction, and isotonic regression lead to the\nbest calibration of CenterPoint and PillarNet with respect to both dominant and\nsecondary class predictions. We further find that DSVT-Pillar can not be\njointly calibrated for dominant and secondary predictions using the same\nmethod.", "AI": {"tldr": "This paper addresses confidence calibration for 3D object detectors' classification tasks, proposing metrics and regularization losses to calibrate both dominant and secondary class predictions, with evaluation on CenterPoint, PillarNet and DSVT-Pillar models.", "motivation": "Precise object detection and uncertainty estimation are critical for autonomous systems' self-aware and safe operation, requiring proper confidence calibration in 3D object detectors.", "method": "Proposed two auxiliary regularizing loss terms for calibrating either dominant prediction or full prediction vector, evaluated with post-hoc and train-time methods on CenterPoint, PillarNet and DSVT-Pillar detectors.", "result": "Combining the full class prediction calibration loss with isotonic regression achieved best calibration for CenterPoint and PillarNet, but DSVT-Pillar couldn't be jointly calibrated for both dominant and secondary predictions using the same method.", "conclusion": "Different 3D object detectors require tailored calibration approaches, with the proposed full class prediction calibration method being effective for some models but not universally applicable across all detector architectures."}}
{"id": "2510.01902", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01902", "abs": "https://arxiv.org/abs/2510.01902", "authors": ["Pawe\u0142 Parys", "Sairam Vaidya", "Taylor Berg-Kirkpatrick", "Loris D'Antoni"], "title": "Constrained Adaptive Rejection Sampling", "comment": null, "summary": "Language Models (LMs) are increasingly used in applications where generated\noutputs must satisfy strict semantic or syntactic constraints. Existing\napproaches to constrained generation fall along a spectrum: greedy constrained\ndecoding methods enforce validity during decoding but distort the LM's\ndistribution, while rejection sampling (RS) preserves fidelity but wastes\ncomputation by discarding invalid outputs. Both extremes are problematic in\ndomains such as program fuzzing, where both validity and diversity of samples\nare essential. We present Constrained Adaptive Rejection Sampling (CARS), an\napproach that strictly improves the sample-efficiency of RS without\ndistributional distortion. CARS begins with unconstrained LM sampling and\nadaptively rules out constraint-violating continuations by recording them in a\ntrie and subtracting their probability mass from future draws. This adaptive\npruning ensures that prefixes proven invalid are never revisited, acceptance\nrates improve monotonically, and the resulting samples exactly follow the\nconstrained distribution. In experiments on a variety of domains -- e.g.,\nprogram fuzzing and molecular generation -- CARS consistently achieves higher\nefficiency -- measured in the number of LM forward passes per valid sample --\nwhile also producing stronger sample diversity than both GCD and methods that\napproximate the LM's distribution.", "AI": {"tldr": "CARS is a constrained generation method that improves rejection sampling efficiency by adaptively pruning invalid continuations while preserving the LM's distribution.", "motivation": "Existing constrained generation methods either distort the LM's distribution (greedy decoding) or waste computation (rejection sampling), which is problematic for domains requiring both validity and diversity like program fuzzing.", "method": "CARS uses unconstrained LM sampling and adaptively rules out constraint-violating continuations by recording them in a trie and subtracting their probability mass from future draws, ensuring invalid prefixes are never revisited.", "result": "CARS consistently achieves higher efficiency (fewer LM forward passes per valid sample) and produces stronger sample diversity than both greedy constrained decoding and methods that approximate the LM's distribution.", "conclusion": "CARS strictly improves rejection sampling efficiency without distributional distortion, making it suitable for domains requiring both validity and diversity in generated outputs."}}
{"id": "2510.01494", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01494", "abs": "https://arxiv.org/abs/2510.01494", "authors": ["Isha Gupta", "Rylan Schaeffer", "Joshua Kazdan", "Ken Liu", "Sanmi Koyejo"], "title": "Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed", "comment": null, "summary": "The field of adversarial robustness has long established that adversarial\nexamples can successfully transfer between image classifiers and that text\njailbreaks can successfully transfer between language models (LMs). However, a\npair of recent studies reported being unable to successfully transfer image\njailbreaks between vision-language models (VLMs). To explain this striking\ndifference, we propose a fundamental distinction regarding the transferability\nof attacks against machine learning models: attacks in the input data-space can\ntransfer, whereas attacks in model representation space do not, at least not\nwithout geometric alignment of representations. We then provide theoretical and\nempirical evidence of this hypothesis in four different settings. First, we\nmathematically prove this distinction in a simple setting where two networks\ncompute the same input-output map but via different representations. Second, we\nconstruct representation-space attacks against image classifiers that are as\nsuccessful as well-known data-space attacks, but fail to transfer. Third, we\nconstruct representation-space attacks against LMs that successfully jailbreak\nthe attacked models but again fail to transfer. Fourth, we construct data-space\nattacks against VLMs that successfully transfer to new VLMs, and we show that\nrepresentation space attacks \\emph{can} transfer when VLMs' latent geometries\nare sufficiently aligned in post-projector space. Our work reveals that\nadversarial transfer is not an inherent property of all attacks but contingent\non their operational domain - the shared data-space versus models' unique\nrepresentation spaces - a critical insight for building more robust models.", "AI": {"tldr": "This paper explains why adversarial attacks transfer between models in data-space but not in representation-space, showing that transferability depends on the attack domain rather than being an inherent property of all attacks.", "motivation": "To explain why image jailbreaks fail to transfer between vision-language models (VLMs) while adversarial examples successfully transfer between image classifiers and text jailbreaks transfer between language models.", "method": "The authors provide theoretical and empirical evidence across four settings: mathematical proof in a simple network scenario, representation-space attacks against image classifiers, representation-space attacks against language models, and data-space attacks against VLMs with geometric alignment analysis.", "result": "Representation-space attacks are as successful as data-space attacks but fail to transfer between models, while data-space attacks successfully transfer. Representation-space attacks can only transfer when models' latent geometries are sufficiently aligned.", "conclusion": "Adversarial transfer is not inherent to all attacks but depends on their operational domain - attacks in shared data-space transfer, while attacks in models' unique representation spaces do not transfer without geometric alignment."}}
{"id": "2510.01841", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01841", "abs": "https://arxiv.org/abs/2510.01841", "authors": ["Giyeol Kim", "Sooyoung Yang", "Jihyong Oh", "Myungjoo Kang", "Chanho Eom"], "title": "Leveraging Prior Knowledge of Diffusion Model for Person Search", "comment": null, "summary": "Person search aims to jointly perform person detection and re-identification\nby localizing and identifying a query person within a gallery of uncropped\nscene images. Existing methods predominantly utilize ImageNet pre-trained\nbackbones, which may be suboptimal for capturing the complex spatial context\nand fine-grained identity cues necessary for person search. Moreover, they rely\non a shared backbone feature for both person detection and re-identification,\nleading to suboptimal features due to conflicting optimization objectives. In\nthis paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a\nnovel framework that leverages a pre-trained diffusion model while eliminating\nthe optimization conflict between two sub-tasks. We analyze key properties of\ndiffusion priors and propose three specialized modules: (i) Diffusion-Guided\nRegion Proposal Network (DGRPN) for enhanced person localization, (ii)\nMulti-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and\n(iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage\ntext-aligned diffusion features. DiffPS sets a new state-of-the-art on\nCUHK-SYSU and PRW.", "AI": {"tldr": "DiffPS leverages pre-trained diffusion models to address optimization conflicts in person search, achieving state-of-the-art performance on CUHK-SYSU and PRW datasets.", "motivation": "Existing person search methods use ImageNet pre-trained backbones that are suboptimal for capturing spatial context and fine-grained identity cues, and suffer from conflicting optimization objectives between detection and re-identification tasks.", "method": "Proposes DiffPS with three modules: Diffusion-Guided Region Proposal Network for person localization, Multi-Scale Frequency Refinement Network to reduce shape bias, and Semantic-Adaptive Feature Aggregation Network to utilize text-aligned diffusion features.", "result": "DiffPS achieves new state-of-the-art performance on CUHK-SYSU and PRW datasets.", "conclusion": "Leveraging diffusion model priors effectively resolves optimization conflicts in person search and improves both localization and identification performance."}}
{"id": "2510.01924", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.01924", "abs": "https://arxiv.org/abs/2510.01924", "authors": ["Crystal Qian", "Aaron Parisi", "Cl\u00e9mentine Bouleau", "Vivian Tsai", "Ma\u00ebl Lebreton", "Lucas Dixon"], "title": "To Mask or to Mirror: Human-AI Alignment in Collective Reasoning", "comment": null, "summary": "As large language models (LLMs) are increasingly used to model and augment\ncollective decision-making, it is critical to examine their alignment with\nhuman social reasoning. We present an empirical framework for assessing\ncollective alignment, in contrast to prior work on the individual level. Using\nthe Lost at Sea social psychology task, we conduct a large-scale online\nexperiment (N=748), randomly assigning groups to leader elections with either\nvisible demographic attributes (e.g. name, gender) or pseudonymous aliases. We\nthen simulate matched LLM groups conditioned on the human data, benchmarking\nGemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some\nmirror human biases; others mask these biases and attempt to compensate for\nthem. We empirically demonstrate that human-AI alignment in collective\nreasoning depends on context, cues, and model-specific inductive biases.\nUnderstanding how LLMs align with collective human behavior is critical to\nadvancing socially-aligned AI, and demands dynamic benchmarks that capture the\ncomplexities of collective reasoning.", "AI": {"tldr": "LLMs show varying alignment with human collective decision-making in social reasoning tasks, with some mirroring human biases while others attempt to compensate for them.", "motivation": "To examine how large language models align with human social reasoning in collective decision-making contexts, moving beyond individual-level analysis.", "method": "Conducted large-scale online experiment (N=748) using Lost at Sea task, randomly assigning groups to leader elections with visible demographic attributes vs pseudonymous aliases, then simulated matched LLM groups with Gemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3.", "result": "LLM behaviors diverge significantly - some models mirror human biases while others mask these biases and attempt to compensate for them.", "conclusion": "Human-AI alignment in collective reasoning depends on context, cues, and model-specific inductive biases, requiring dynamic benchmarks that capture the complexities of collective reasoning for socially-aligned AI."}}
{"id": "2510.01499", "categories": ["cs.LG", "cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.01499", "abs": "https://arxiv.org/abs/2510.01499", "authors": ["Rui Ai", "Yuqi Pan", "David Simchi-Levi", "Milind Tambe", "Haifeng Xu"], "title": "Beyond Majority Voting: LLM Aggregation by Leveraging Higher-Order Information", "comment": null, "summary": "With the rapid progress of multi-agent large language model (LLM) reasoning,\nhow to effectively aggregate answers from multiple LLMs has emerged as a\nfundamental challenge. Standard majority voting treats all answers equally,\nfailing to consider latent heterogeneity and correlation across models. In this\nwork, we design two new aggregation algorithms called Optimal Weight (OW) and\nInverse Surprising Popularity (ISP), leveraging both first-order and\nsecond-order information. Our theoretical analysis shows these methods provably\nmitigate inherent limitations of majority voting under mild assumptions,\nleading to more reliable collective decisions. We empirically validate our\nalgorithms on synthetic datasets, popular LLM fine-tuning benchmarks such as\nUltraFeedback and MMLU, and a real-world healthcare setting ARMMAN. Across all\ncases, our methods consistently outperform majority voting, offering both\npractical performance gains and conceptual insights for the design of robust\nmulti-agent LLM pipelines.", "AI": {"tldr": "The paper proposes two new aggregation algorithms (Optimal Weight and Inverse Surprising Popularity) that outperform standard majority voting for multi-agent LLM reasoning by leveraging first-order and second-order information to account for model heterogeneity and correlation.", "motivation": "Standard majority voting treats all LLM answers equally, failing to consider latent heterogeneity and correlation across models, which limits the reliability of collective decisions in multi-agent LLM reasoning systems.", "method": "Designed two aggregation algorithms: Optimal Weight (OW) and Inverse Surprising Popularity (ISP) that leverage both first-order and second-order information about model responses.", "result": "The methods consistently outperform majority voting across synthetic datasets, LLM fine-tuning benchmarks (UltraFeedback, MMLU), and a real-world healthcare setting (ARMMAN), providing both practical performance gains and conceptual insights.", "conclusion": "The proposed algorithms effectively mitigate inherent limitations of majority voting under mild assumptions, leading to more reliable collective decisions in multi-agent LLM pipelines."}}
{"id": "2510.01912", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01912", "abs": "https://arxiv.org/abs/2510.01912", "authors": ["Yi Ai", "Yuanhao Cai", "Yulun Zhang", "Xiaokang Yang"], "title": "Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction", "comment": null, "summary": "Hyperspectral imaging (HSI) provides rich spatial-spectral information but\nremains costly to acquire due to hardware limitations and the difficulty of\nreconstructing three-dimensional data from compressed measurements. Although\ncompressive sensing systems such as CASSI improve efficiency, accurate\nreconstruction is still challenged by severe degradation and loss of fine\nspectral details. We propose the Flow-Matching-guided Unfolding network (FMU),\nwhich, to our knowledge, is the first to integrate flow matching into HSI\nreconstruction by embedding its generative prior within a deep unfolding\nframework. To further strengthen the learned dynamics, we introduce a mean\nvelocity loss that enforces global consistency of the flow, leading to a more\nrobust and accurate reconstruction. This hybrid design leverages the\ninterpretability of optimization-based methods and the generative capacity of\nflow matching. Extensive experiments on both simulated and real datasets show\nthat FMU significantly outperforms existing approaches in reconstruction\nquality. Code and models will be available at https://github.com/YiAi03/FMU.", "AI": {"tldr": "FMU integrates flow matching with deep unfolding for hyperspectral image reconstruction, achieving superior results through generative priors and global consistency enforcement.", "motivation": "Hyperspectral imaging is costly and challenging to reconstruct from compressed measurements, with existing methods suffering from degradation and loss of spectral details.", "method": "Proposes Flow-Matching-guided Unfolding network (FMU) that combines flow matching generative priors with deep unfolding framework, introducing a mean velocity loss for global consistency.", "result": "Extensive experiments on simulated and real datasets show FMU significantly outperforms existing approaches in reconstruction quality.", "conclusion": "FMU successfully leverages interpretability of optimization methods and generative capacity of flow matching for robust hyperspectral image reconstruction."}}
{"id": "2510.02027", "categories": ["cs.AI", "cs.ET", "68T27, 03B42, 91A20, 62H20", "I.2.4; H.5.3; J.7; K.4.3"], "pdf": "https://arxiv.org/pdf/2510.02027", "abs": "https://arxiv.org/abs/2510.02027", "authors": ["Khalid M. Saqr"], "title": "Zero-shot reasoning for simulating scholarly peer-review", "comment": null, "summary": "The scholarly publishing ecosystem faces a dual crisis of unmanageable\nsubmission volumes and unregulated AI, creating an urgent need for new\ngovernance models to safeguard scientific integrity. The traditional human-only\npeer review regime lacks a scalable, objective benchmark, making editorial\nprocesses opaque and difficult to audit. Here we investigate a deterministic\nsimulation framework that provides the first stable, evidence-based standard\nfor evaluating AI-generated peer review reports. Analyzing 352 peer-review\nsimulation reports, we identify consistent system state indicators that\ndemonstrate its reliability. First, the system is able to simulate calibrated\neditorial judgment, with 'Revise' decisions consistently forming the majority\noutcome (>50%) across all disciplines, while 'Reject' rates dynamically adapt\nto field-specific norms, rising to 45% in Health Sciences. Second, it maintains\nunwavering procedural integrity, enforcing a stable 29% evidence-anchoring\ncompliance rate that remains invariant across diverse review tasks and\nscientific domains. These findings demonstrate a system that is predictably\nrule-bound, mitigating the stochasticity of generative AI. For the scientific\ncommunity, this provides a transparent tool to ensure fairness; for publishing\nstrategists, it offers a scalable instrument for auditing workflows, managing\nintegrity risks, and implementing evidence-based governance. The framework\nrepositions AI as an essential component of institutional accountability,\nproviding the critical infrastructure to maintain trust in scholarly\ncommunication.", "AI": {"tldr": "A deterministic simulation framework provides stable, evidence-based standards for evaluating AI-generated peer review reports, offering scalable governance tools for scholarly publishing.", "motivation": "Address the dual crisis of unmanageable submission volumes and unregulated AI in scholarly publishing by creating new governance models to safeguard scientific integrity.", "method": "Developed a deterministic simulation framework that analyzes peer-review simulation reports to identify consistent system state indicators and evaluate AI-generated peer review.", "result": "The system simulates calibrated editorial judgment with 'Revise' decisions forming majority outcomes (>50%) across disciplines, and maintains 29% evidence-anchoring compliance rate that remains invariant across domains.", "conclusion": "The framework provides transparent tools for fairness in science, scalable auditing for publishing workflows, and repositions AI as essential for institutional accountability in scholarly communication."}}
{"id": "2510.01508", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01508", "abs": "https://arxiv.org/abs/2510.01508", "authors": ["Will Y. Zou", "Jean Feng", "Alexandre Kalimouttou", "Jennifer Yuntong Zhang", "Christopher W. Seymour", "Romain Pirracchio"], "title": "Realistic CDSS Drug Dosing with End-to-end Recurrent Q-learning for Dual Vasopressor Control", "comment": "11 pages, 5 figures. Neurips 2025 Workshop Learning from Time Series\n  for Health", "summary": "Reinforcement learning (RL) applications in Clinical Decision Support Systems\n(CDSS) frequently encounter skepticism from practitioners regarding inoperable\ndosing decisions. We address this challenge with an end-to-end approach for\nlearning optimal drug dosing and control policies for dual vasopressor\nadministration in intensive care unit (ICU) patients with septic shock. For\nrealistic drug dosing, we apply action space design that accommodates discrete,\ncontinuous, and directional dosing strategies in a system that combines offline\nconservative Q-learning with a novel recurrent modeling in a replay buffer to\ncapture temporal dependencies in ICU time-series data. Our comparative analysis\nof norepinephrine dosing strategies across different action space formulations\nreveals that the designed action spaces improve interpretability and facilitate\nclinical adoption while preserving efficacy. Empirical results1 on eICU and\nMIMIC demonstrate that action space design profoundly influences learned\nbehavioral policies. The proposed methods achieve improved patient outcomes of\nover 15% in survival improvement probability, while aligning with established\nclinical protocols.", "AI": {"tldr": "This paper presents an end-to-end RL approach for dual vasopressor dosing in ICU septic shock patients, using action space design to improve interpretability and clinical adoption while maintaining efficacy.", "motivation": "Address skepticism from practitioners about RL-based CDSS due to inoperable dosing decisions, particularly for dual vasopressor administration in ICU patients with septic shock.", "method": "Combines offline conservative Q-learning with novel recurrent modeling in replay buffer to capture temporal dependencies; applies action space design accommodating discrete, continuous, and directional dosing strategies.", "result": "Action space design improves interpretability and facilitates clinical adoption while preserving efficacy; achieves over 15% survival improvement probability on eICU and MIMIC datasets; learned behavioral policies are profoundly influenced by action space formulation.", "conclusion": "The proposed methods successfully address clinical skepticism by designing interpretable action spaces that align with established protocols while improving patient outcomes."}}
{"id": "2510.02060", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02060", "abs": "https://arxiv.org/abs/2510.02060", "authors": ["Sanghyu Yoon", "Dongmin Kim", "Suhee Yoon", "Ye Seul Sim", "Seungdong Yoa", "Hye-Seung Cho", "Soonyoung Lee", "Hankook Lee", "Woohyung Lim"], "title": "ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly Detection", "comment": "9 pages, 4 figures", "summary": "In tabular anomaly detection (AD), textual semantics often carry critical\nsignals, as the definition of an anomaly is closely tied to domain-specific\ncontext. However, existing benchmarks provide only raw data points without\nsemantic context, overlooking rich textual metadata such as feature\ndescriptions and domain knowledge that experts rely on in practice. This\nlimitation restricts research flexibility and prevents models from fully\nleveraging domain knowledge for detection. ReTabAD addresses this gap by\nrestoring textual semantics to enable context-aware tabular AD research. We\nprovide (1) 20 carefully curated tabular datasets enriched with structured\ntextual metadata, together with implementations of state-of-the-art AD\nalgorithms including classical, deep learning, and LLM-based approaches, and\n(2) a zero-shot LLM framework that leverages semantic context without\ntask-specific training, establishing a strong baseline for future research.\nFurthermore, this work provides insights into the role and utility of textual\nmetadata in AD through experiments and analysis. Results show that semantic\ncontext improves detection performance and enhances interpretability by\nsupporting domain-aware reasoning. These findings establish ReTabAD as a\nbenchmark for systematic exploration of context-aware AD.", "AI": {"tldr": "ReTabAD introduces a benchmark for context-aware tabular anomaly detection by enriching datasets with textual metadata and providing tools for semantic-aware analysis.", "motivation": "Existing tabular AD benchmarks lack textual semantics and domain context that experts use in practice, limiting research flexibility and preventing full utilization of domain knowledge.", "method": "Created 20 curated tabular datasets with structured textual metadata, implemented state-of-the-art AD algorithms, and developed a zero-shot LLM framework that leverages semantic context without task-specific training.", "result": "Semantic context improves detection performance and enhances interpretability by supporting domain-aware reasoning, establishing ReTabAD as a benchmark for systematic exploration of context-aware AD.", "conclusion": "ReTabAD addresses the gap in textual semantics for tabular AD, enabling context-aware research and demonstrating the utility of textual metadata in improving detection performance and interpretability."}}
{"id": "2510.01510", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01510", "abs": "https://arxiv.org/abs/2510.01510", "authors": ["Jinwoo Kim", "Xingyue Huang", "Krzysztof Olejniczak", "Kyungbin Min", "Michael Bronstein", "Seunghoon Hong", "\u0130smail \u0130lkan Ceylan"], "title": "Flock: A Knowledge Graph Foundation Model via Learning on Random Walks", "comment": null, "summary": "We study the problem of zero-shot link prediction on knowledge graphs (KGs),\nwhich requires models to generalize over novel entities and novel relations.\nKnowledge graph foundation models (KGFMs) address this task by enforcing\nequivariance over both nodes and relations, learning from structural properties\nof nodes and relations, which are then transferable to novel graphs with\nsimilar structural properties. However, the conventional notion of\ndeterministic equivariance imposes inherent limits on the expressive power of\nKGFMs, preventing them from distinguishing structurally similar but\nsemantically distinct relations. To overcome this limitation, we introduce\nprobabilistic node-relation equivariance, which preserves equivariance in\ndistribution while incorporating a principled randomization to break symmetries\nduring inference. Building on this principle, we present Flock, a KGFM that\niteratively samples random walks, encodes them into sequences via a recording\nprotocol, embeds them with a sequence model, and aggregates representations of\nnodes and relations via learned pooling. Crucially, Flock respects\nprobabilistic node-relation equivariance and is a universal approximator for\nisomorphism-invariant link-level functions over KGs. Empirically, Flock\nperfectly solves our new diagnostic dataset Petals where current KGFMs fail,\nand achieves state-of-the-art performances on entity- and relation prediction\ntasks on 54 KGs from diverse domains.", "AI": {"tldr": "The paper introduces Flock, a knowledge graph foundation model that uses probabilistic node-relation equivariance to overcome limitations of deterministic equivariance in zero-shot link prediction, achieving state-of-the-art performance.", "motivation": "Current knowledge graph foundation models (KGFMs) use deterministic equivariance which limits their expressive power and prevents them from distinguishing structurally similar but semantically distinct relations in zero-shot link prediction tasks.", "method": "Flock uses probabilistic node-relation equivariance that preserves equivariance in distribution while incorporating principled randomization. It iteratively samples random walks, encodes them via recording protocol, embeds with sequence model, and aggregates representations via learned pooling.", "result": "Flock perfectly solves the new diagnostic dataset Petals where current KGFMs fail, and achieves state-of-the-art performance on entity and relation prediction tasks across 54 knowledge graphs from diverse domains.", "conclusion": "Probabilistic node-relation equivariance enables more expressive knowledge graph foundation models that can better handle zero-shot link prediction by breaking symmetries while maintaining equivariance properties."}}
{"id": "2510.01934", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01934", "abs": "https://arxiv.org/abs/2510.01934", "authors": ["Guangyao Zhai", "Yue Zhou", "Xinyan Deng", "Lars Heckler", "Nassir Navab", "Benjamin Busam"], "title": "Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors", "comment": "23 pages, 13 figures. Code is available at\n  \\url{https://github.com/ymxlzgy/FoundAD}", "summary": "Few-shot anomaly detection streamlines and simplifies industrial safety\ninspection. However, limited samples make accurate differentiation between\nnormal and abnormal features challenging, and even more so under\ncategory-agnostic conditions. Large-scale pre-training of foundation visual\nencoders has advanced many fields, as the enormous quantity of data helps to\nlearn the general distribution of normal images. We observe that the anomaly\namount in an image directly correlates with the difference in the learnt\nembeddings and utilize this to design a few-shot anomaly detector termed\nFoundAD. This is done by learning a nonlinear projection operator onto the\nnatural image manifold. The simple operator acts as an effective tool for\nanomaly detection to characterize and identify out-of-distribution regions in\nan image. Extensive experiments show that our approach supports multi-class\ndetection and achieves competitive performance while using substantially fewer\nparameters than prior methods. Backed up by evaluations with multiple\nfoundation encoders, including fresh DINOv3, we believe this idea broadens the\nperspective on foundation features and advances the field of few-shot anomaly\ndetection.", "AI": {"tldr": "FoundAD is a few-shot anomaly detection method that uses foundation visual encoders and a nonlinear projection operator to identify anomalies by measuring embedding differences from the natural image manifold.", "motivation": "Few-shot anomaly detection is challenging due to limited samples, especially under category-agnostic conditions. Foundation visual encoders can learn general normal image distributions from large-scale pre-training.", "method": "Learn a nonlinear projection operator onto the natural image manifold. This operator identifies out-of-distribution regions by measuring embedding differences, leveraging the correlation between anomaly amount and embedding differences.", "result": "Achieves competitive performance in multi-class detection while using substantially fewer parameters than prior methods. Validated with multiple foundation encoders including DINOv3.", "conclusion": "The approach broadens perspective on foundation features and advances few-shot anomaly detection field by effectively leveraging pre-trained encoders for anomaly identification."}}
{"id": "2510.02091", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02091", "abs": "https://arxiv.org/abs/2510.02091", "authors": ["Xinyuan Song", "Keyu Wang", "PengXiang Li", "Lu Yin", "Shiwei Liu"], "title": "Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning", "comment": "ICASSP 2025", "summary": "Recent studies suggest that the deeper layers of Large Language Models (LLMs)\ncontribute little to representation learning and can often be removed without\nsignificant performance loss. However, such claims are typically drawn from\nnarrow evaluations and may overlook important aspects of model behavior. In\nthis work, we present a systematic study of depth utilization across diverse\ndimensions, including evaluation protocols, task categories, and model\narchitectures. Our analysis confirms that very deep layers are generally less\neffective than earlier ones, but their contributions vary substantially with\nthe evaluation setting. Under likelihood-based metrics without generation,\npruning most layers preserves performance, with only the initial few being\ncritical. By contrast, generation-based evaluation uncovers indispensable roles\nfor middle and deeper layers in enabling reasoning and maintaining long-range\ncoherence. We further find that knowledge and retrieval are concentrated in\nshallow components, whereas reasoning accuracy relies heavily on deeper layers\n-- yet can be reshaped through distillation. These results highlight that depth\nusage in LLMs is highly heterogeneous and context-dependent, underscoring the\nneed for task-, metric-, and model-aware perspectives in both interpreting and\ncompressing large models.", "AI": {"tldr": "Depth utilization in LLMs is heterogeneous - shallow layers handle knowledge/retrieval while deeper layers enable reasoning and coherence, with effectiveness varying by evaluation method and task type.", "motivation": "To systematically investigate claims that deep layers in LLMs are unnecessary, examining depth utilization across diverse dimensions beyond narrow evaluations.", "method": "Comprehensive analysis across evaluation protocols (likelihood vs generation), task categories, and model architectures to study layer contributions systematically.", "result": "Shallow layers are critical for knowledge/retrieval and under likelihood metrics, while middle/deeper layers are indispensable for reasoning and generation coherence. Depth effectiveness varies significantly by evaluation setting.", "conclusion": "Depth usage in LLMs is highly context-dependent, requiring task-, metric-, and model-aware perspectives for proper interpretation and compression of large models."}}
{"id": "2510.01520", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01520", "abs": "https://arxiv.org/abs/2510.01520", "authors": ["Hossein Sholehrasa", "Xuan Xu", "Doina Caragea", "Jim E. Riviere", "Majid Jaberi-Douraki"], "title": "Predictive Modeling and Explainable AI for Veterinary Safety Profiles, Residue Assessment, and Health Outcomes Using Real-World Data and Physicochemical Properties", "comment": null, "summary": "The safe use of pharmaceuticals in food-producing animals is vital to protect\nanimal welfare and human food safety. Adverse events (AEs) may signal\nunexpected pharmacokinetic or toxicokinetic effects, increasing the risk of\nviolative residues in the food chain. This study introduces a predictive\nframework for classifying outcomes (Death vs. Recovery) using ~1.28 million\nreports (1987-2025 Q1) from the U.S. FDA's OpenFDA Center for Veterinary\nMedicine. A preprocessing pipeline merged relational tables and standardized\nAEs through VeDDRA ontologies. Data were normalized, missing values imputed,\nand high-cardinality features reduced; physicochemical drug properties were\nintegrated to capture chemical-residue links. We evaluated supervised models,\nincluding Random Forest, CatBoost, XGBoost, ExcelFormer, and large language\nmodels (Gemma 3-27B, Phi 3-12B). Class imbalance was addressed, such as\nundersampling and oversampling, with a focus on prioritizing recall for fatal\noutcomes. Ensemble methods(Voting, Stacking) and CatBoost performed best,\nachieving precision, recall, and F1-scores of 0.95. Incorporating Average\nUncertainty Margin (AUM)-based pseudo-labeling of uncertain cases improved\nminority-class detection, particularly in ExcelFormer and XGBoost.\nInterpretability via SHAP identified biologically plausible predictors,\nincluding lung, heart, and bronchial disorders, animal demographics, and drug\nphysicochemical properties. These features were strongly linked to fatal\noutcomes. Overall, the framework shows that combining rigorous data\nengineering, advanced machine learning, and explainable AI enables accurate,\ninterpretable predictions of veterinary safety outcomes. The approach supports\nFARAD's mission by enabling early detection of high-risk drug-event profiles,\nstrengthening residue risk assessment, and informing regulatory and clinical\ndecision-making.", "AI": {"tldr": "A predictive framework using machine learning models to classify veterinary drug adverse event outcomes (Death vs. Recovery) from FDA data, achieving high performance with ensemble methods and interpretable AI.", "motivation": "To protect animal welfare and human food safety by predicting adverse events that may lead to violative residues in the food chain, supporting early detection of high-risk drug-event profiles.", "method": "Used ~1.28 million FDA reports with preprocessing, VeDDRA ontology standardization, data normalization, and integration of physicochemical drug properties. Evaluated multiple ML models including Random Forest, CatBoost, XGBoost, ExcelFormer, and LLMs with ensemble methods and AUM-based pseudo-labeling.", "result": "Ensemble methods and CatBoost achieved precision, recall, and F1-scores of 0.95. SHAP analysis identified biologically plausible predictors including lung/heart disorders, animal demographics, and drug properties strongly linked to fatal outcomes.", "conclusion": "Combining rigorous data engineering, advanced ML, and explainable AI enables accurate, interpretable predictions of veterinary safety outcomes, supporting regulatory decision-making and residue risk assessment."}}
{"id": "2510.01948", "categories": ["cs.CV", "68T45", "I.2.10"], "pdf": "https://arxiv.org/pdf/2510.01948", "abs": "https://arxiv.org/abs/2510.01948", "authors": ["Fabio Montello", "Ronja G\u00fcldenring", "Lazaros Nalpantidis"], "title": "ClustViT: Clustering-based Token Merging for Semantic Segmentation", "comment": "Submitted to IEEE", "summary": "Vision Transformers can achieve high accuracy and strong generalization\nacross various contexts, but their practical applicability on real-world\nrobotic systems is limited due to their quadratic attention complexity. Recent\nworks have focused on dynamically merging tokens according to the image\ncomplexity. Token merging works well for classification but is less suited to\ndense prediction. We propose ClustViT, where we expand upon the Vision\nTransformer (ViT) backbone and address semantic segmentation. Within our\narchitecture, a trainable Cluster module merges similar tokens along the\nnetwork guided by pseudo-clusters from segmentation masks. Subsequently, a\nRegenerator module restores fine details for downstream heads. Our approach\nachieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three different\ndatasets, with comparable segmentation accuracy. Our code and models will be\nmade publicly available.", "AI": {"tldr": "ClustViT is a Vision Transformer architecture for semantic segmentation that reduces computational complexity through token clustering and regeneration, achieving significant speedup with comparable accuracy.", "motivation": "Vision Transformers have high accuracy but quadratic attention complexity limits their practical use in real-world robotic systems. Token merging methods work for classification but are less suitable for dense prediction tasks like semantic segmentation.", "method": "Proposes ClustViT with a trainable Cluster module that merges similar tokens guided by pseudo-clusters from segmentation masks, and a Regenerator module that restores fine details for downstream processing.", "result": "Achieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three datasets while maintaining comparable segmentation accuracy to standard Vision Transformers.", "conclusion": "ClustViT effectively addresses the computational complexity of Vision Transformers for dense prediction tasks, making them more practical for real-world robotic applications while preserving segmentation performance."}}
{"id": "2510.02125", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02125", "abs": "https://arxiv.org/abs/2510.02125", "authors": ["Claas Beger", "Ryan Yi", "Shuhao Fu", "Arseny Moskvichev", "Sarah W. Tsai", "Sivasankaran Rajamanickam", "Melanie Mitchell"], "title": "Do AI Models Perform Human-like Abstract Reasoning Across Modalities?", "comment": "10 pages, 4 figures", "summary": "OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI\nbenchmark, but does that mean state-of-the-art models recognize and reason with\nthe abstractions that the task creators intended? We investigate models'\nabstraction abilities on ConceptARC. We evaluate models under settings that\nvary the input modality (textual vs. visual), whether the model is permitted to\nuse external Python tools, and, for reasoning models, the amount of reasoning\neffort. In addition to measuring output accuracy, we perform fine-grained\nevaluation of the natural-language rules that models generate to explain their\nsolutions. This dual evaluation lets us assess whether models solve tasks using\nthe abstractions ConceptARC was designed to elicit, rather than relying on\nsurface-level patterns. Our results show that, while some models using\ntext-based representations match human output accuracy, the best models' rules\nare often based on surface-level ``shortcuts'' and capture intended\nabstractions far less often than humans. Thus their capabilities for general\nabstract reasoning may be overestimated by evaluations based on accuracy alone.\nIn the visual modality, AI models' output accuracy drops sharply, yet our\nrule-level analysis reveals that models might be underestimated, as they still\nexhibit a substantial share of rules that capture intended abstractions, but\nare often unable to correctly apply these rules. In short, our results show\nthat models still lag humans in abstract reasoning, and that using accuracy\nalone to evaluate abstract reasoning on ARC-like tasks may overestimate\nabstract-reasoning capabilities in textual modalities and underestimate it in\nvisual modalities. We believe that our evaluation framework offers a more\nfaithful picture of multimodal models' abstract reasoning abilities and a more\nprincipled way to track progress toward human-like, abstraction-centered\nintelligence.", "AI": {"tldr": "While AI models achieve high accuracy on ConceptARC benchmark in text modality, they often use surface-level shortcuts rather than true abstract reasoning. In visual modality, accuracy drops but models still show some abstraction capability.", "motivation": "To investigate whether state-of-the-art AI models truly understand and reason with intended abstractions in ConceptARC benchmark, rather than relying on surface-level patterns.", "method": "Evaluated models on ConceptARC with varying input modalities (textual vs visual), external Python tool usage, and reasoning effort. Used dual evaluation measuring both output accuracy and fine-grained analysis of natural-language rules generated to explain solutions.", "result": "Text-based models match human accuracy but their rules often use surface-level shortcuts rather than intended abstractions. Visual modality models show lower accuracy but still exhibit substantial share of rules capturing intended abstractions, though they struggle to apply them correctly.", "conclusion": "Models still lag humans in abstract reasoning. Accuracy alone may overestimate abstract reasoning in text modalities and underestimate it in visual modalities. The proposed evaluation framework provides more faithful assessment of multimodal models' abstract reasoning abilities."}}
{"id": "2510.01521", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01521", "abs": "https://arxiv.org/abs/2510.01521", "authors": ["Diptyaroop Maji", "Kang Yang", "Prashant Shenoy", "Ramesh K Sitaraman", "Mani Srivastava"], "title": "CarbonX: An Open-Source Tool for Computational Decarbonization Using Time Series Foundation Models", "comment": null, "summary": "Computational decarbonization aims to reduce carbon emissions in computing\nand societal systems such as data centers, transportation, and built\nenvironments. This requires accurate, fine-grained carbon intensity forecasts,\nyet existing tools have several key limitations: (i) they require grid-specific\nelectricity mix data, restricting use where such information is unavailable;\n(ii) they depend on separate grid-specific models that make it challenging to\nprovide global coverage; and (iii) they provide forecasts without uncertainty\nestimates, limiting reliability for downstream carbon-aware applications.\n  In this paper, we present CarbonX, an open-source tool that leverages Time\nSeries Foundation Models (TSFMs) for a range of decarbonization tasks. CarbonX\nutilizes the versatility of TSFMs to provide strong performance across multiple\ntasks, such as carbon intensity forecasting and imputation, and across diverse\ngrids. Using only historical carbon intensity data and a single general model,\nour tool achieves a zero-shot forecasting Mean Absolute Percentage Error (MAPE)\nof 15.82% across 214 grids worldwide. Across 13 benchmark grids, CarbonX\nperformance is comparable with the current state-of-the-art, with an average\nMAPE of 9.59% and tail forecasting MAPE of 16.54%, while also providing\nprediction intervals with 95% coverage. CarbonX can provide forecasts for up to\n21 days with minimal accuracy degradation. Further, when fully fine-tuned,\nCarbonX outperforms the statistical baselines by 1.2--3.9X on the imputation\ntask. Overall, these results demonstrate that CarbonX can be used easily on any\ngrid with limited data and still deliver strong performance, making it a\npractical tool for global-scale decarbonization.", "AI": {"tldr": "CarbonX is an open-source tool using Time Series Foundation Models for carbon intensity forecasting and imputation, achieving strong zero-shot performance across 214 grids worldwide with minimal data requirements.", "motivation": "Existing carbon intensity forecasting tools have limitations: they require grid-specific electricity mix data, depend on separate grid-specific models making global coverage difficult, and lack uncertainty estimates which limits reliability for carbon-aware applications.", "method": "CarbonX leverages Time Series Foundation Models (TSFMs) using only historical carbon intensity data and a single general model, providing forecasts for up to 21 days with prediction intervals and uncertainty estimates.", "result": "Zero-shot forecasting achieved 15.82% MAPE across 214 grids worldwide. On 13 benchmark grids, average MAPE was 9.59% with tail forecasting MAPE of 16.54% and 95% coverage prediction intervals. When fine-tuned, CarbonX outperformed statistical baselines by 1.2-3.9X on imputation tasks.", "conclusion": "CarbonX can be easily used on any grid with limited data while delivering strong performance, making it a practical tool for global-scale decarbonization applications."}}
{"id": "2510.01954", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01954", "abs": "https://arxiv.org/abs/2510.01954", "authors": ["Yongyi Su", "Haojie Zhang", "Shijie Li", "Nanqing Liu", "Jingyi Liao", "Junyi Pan", "Yuan Liu", "Xiaofen Xing", "Chong Sun", "Chen Li", "Nancy F. Chen", "Shuicheng Yan", "Xulei Yang", "Xun Xu"], "title": "Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs", "comment": "24 pages, 12 figures and 9 tables", "summary": "Multimodal large language models (MLLMs) have advanced rapidly in recent\nyears. However, existing approaches for vision tasks often rely on indirect\nrepresentations, such as generating coordinates as text for detection, which\nlimits performance and prevents dense prediction tasks like segmentation. To\novercome these challenges, we introduce Patch-as-Decodable Token (PaDT), a\nunified paradigm that enables MLLMs to directly generate both textual and\ndiverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),\nderived from visual patch embeddings of query images and interleaved seamlessly\nwith LLM's output textual tokens. A lightweight decoder then transforms LLM's\noutputs into detection, segmentation, and grounding predictions. Unlike prior\nmethods, PaDT processes VRTs independently at each forward pass and dynamically\nexpands the embedding table, thus improving localization and differentiation\namong similar objects. We further tailor a training strategy for PaDT by\nrandomly selecting VRTs for supervised fine-tuning and introducing a robust\nper-token cross-entropy loss. Our empirical studies across four visual\nperception and understanding tasks suggest PaDT consistently achieving\nstate-of-the-art performance, even compared with significantly larger MLLM\nmodels. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.", "AI": {"tldr": "PaDT introduces a unified paradigm for MLLMs to directly generate both textual and visual outputs using Visual Reference Tokens (VRTs) interleaved with text tokens, enabling dense prediction tasks like detection and segmentation.", "motivation": "Existing MLLM approaches for vision tasks rely on indirect representations like generating coordinates as text, which limits performance and prevents dense prediction tasks such as segmentation.", "method": "PaDT uses Visual Reference Tokens (VRTs) derived from visual patch embeddings, interleaved with LLM's textual tokens. A lightweight decoder transforms outputs into detection, segmentation, and grounding predictions. VRTs are processed independently at each forward pass with dynamic embedding table expansion.", "result": "Empirical studies across four visual perception and understanding tasks show PaDT consistently achieves state-of-the-art performance, even compared with significantly larger MLLM models.", "conclusion": "PaDT provides a unified framework that enables MLLMs to directly generate diverse visual outputs, overcoming limitations of indirect representations and achieving superior performance in visual perception tasks."}}
{"id": "2510.02133", "categories": ["cs.AI", "cs.LG", "I.2.7; I.2.10; I.4.8; I.4.9"], "pdf": "https://arxiv.org/pdf/2510.02133", "abs": "https://arxiv.org/abs/2510.02133", "authors": ["Karan Dua", "Hitesh Laxmichand Patel", "Puneet Mittal", "Ranjeet Gupta", "Amit Agarwal", "Praneet Pabolu", "Srikant Panda", "Hansa Meghwani", "Graham Horwood", "Fahad Shah"], "title": "FlexDoc: Parameterized Sampling for Diverse Multilingual Synthetic Documents for Training Document Understanding Models", "comment": "Accepted at EMNLP 2025", "summary": "Developing document understanding models at enterprise scale requires large,\ndiverse, and well-annotated datasets spanning a wide range of document types.\nHowever, collecting such data is prohibitively expensive due to privacy\nconstraints, legal restrictions, and the sheer volume of manual annotation\nneeded - costs that can scale into millions of dollars. We introduce FlexDoc, a\nscalable synthetic data generation framework that combines Stochastic Schemas\nand Parameterized Sampling to produce realistic, multilingual semi-structured\ndocuments with rich annotations. By probabilistically modeling layout patterns,\nvisual structure, and content variability, FlexDoc enables the controlled\ngeneration of diverse document variants at scale. Experiments on Key\nInformation Extraction (KIE) tasks demonstrate that FlexDoc-generated data\nimproves the absolute F1 Score by up to 11% when used to augment real datasets,\nwhile reducing annotation effort by over 90% compared to traditional\nhard-template methods. The solution is in active deployment, where it has\naccelerated the development of enterprise-grade document understanding models\nwhile significantly reducing data acquisition and annotation costs.", "AI": {"tldr": "FlexDoc is a scalable synthetic data generation framework that creates realistic multilingual semi-structured documents with rich annotations to reduce the high costs of manual data collection for enterprise document understanding models.", "motivation": "Developing document understanding models requires large, diverse datasets, but collecting such data is prohibitively expensive due to privacy constraints, legal restrictions, and high manual annotation costs that can reach millions of dollars.", "method": "FlexDoc combines Stochastic Schemas and Parameterized Sampling to probabilistically model layout patterns, visual structure, and content variability, enabling controlled generation of diverse document variants at scale.", "result": "Experiments on Key Information Extraction tasks show FlexDoc-generated data improves absolute F1 Score by up to 11% when augmenting real datasets, while reducing annotation effort by over 90% compared to traditional hard-template methods.", "conclusion": "FlexDoc is actively deployed and has accelerated enterprise-grade document understanding model development while significantly reducing data acquisition and annotation costs."}}
{"id": "2510.01525", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.01525", "abs": "https://arxiv.org/abs/2510.01525", "authors": ["Woojin Kim", "James R. Luedtke"], "title": "On Integer Programming for the Binarized Neural Network Verification Problem", "comment": null, "summary": "Binarized neural networks (BNNs) are feedforward neural networks with binary\nweights and activation functions. In the context of using a BNN for\nclassification, the verification problem seeks to determine whether a small\nperturbation of a given input can lead it to be misclassified by the BNN, and\nthe robustness of the BNN can be measured by solving the verification problem\nover multiple inputs. The BNN verification problem can be formulated as an\ninteger programming (IP) problem. However, the natural IP formulation is often\nchallenging to solve due to a large integrality gap induced by big-$M$\nconstraints. We present two techniques to improve the IP formulation. First, we\nintroduce a new method for obtaining a linear objective for the multi-class\nsetting. Second, we introduce a new technique for generating valid inequalities\nfor the IP formulation that exploits the recursive structure of BNNs. We find\nthat our techniques enable verifying BNNs against a higher range of input\nperturbation than existing IP approaches within a limited time.", "AI": {"tldr": "The paper presents improved integer programming methods for verifying binarized neural networks (BNNs) against input perturbations, enabling verification of larger perturbation ranges than existing approaches.", "motivation": "BNN verification is important for measuring robustness against adversarial attacks, but existing IP formulations face challenges due to large integrality gaps from big-M constraints.", "method": "Two techniques: (1) new linear objective method for multi-class classification, (2) new valid inequality generation exploiting BNNs' recursive structure.", "result": "The techniques enable verifying BNNs against higher input perturbation ranges than existing IP approaches within limited time.", "conclusion": "The proposed IP formulation improvements significantly enhance BNN verification capabilities for robustness assessment."}}
{"id": "2510.01990", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.01990", "abs": "https://arxiv.org/abs/2510.01990", "authors": ["Jianfei Xie", "Ziyang Li"], "title": "TriAlignXA: An Explainable Trilemma Alignment Framework for Trustworthy Agri-product Grading", "comment": null, "summary": "The 'trust deficit' in online fruit and vegetable e-commerce stems from the\ninability of digital transactions to provide direct sensory perception of\nproduct quality. This paper constructs a 'Trust Pyramid' model through\n'dual-source verification' of consumer trust. Experiments confirm that quality\nis the cornerstone of trust. The study reveals an 'impossible triangle' in\nagricultural product grading, comprising biological characteristics,\ntimeliness, and economic viability, highlighting the limitations of traditional\nabsolute grading standards. To quantitatively assess this trade-off, we propose\nthe 'Triangular Trust Index' (TTI). We redefine the role of algorithms from\n'decision-makers' to 'providers of transparent decision-making bases',\ndesigning the explainable AI framework--TriAlignXA. This framework supports\ntrustworthy online transactions within agricultural constraints through\nmulti-objective optimization. Its core relies on three engines: the\nBio-Adaptive Engine for granular quality description; the Timeliness\nOptimization Engine for processing efficiency; and the Economic Optimization\nEngine for cost control. Additionally, the \"Pre-Mapping Mechanism\" encodes\nprocess data into QR codes, transparently conveying quality information.\nExperiments on grading tasks demonstrate significantly higher accuracy than\nbaseline models. Empirical evidence and theoretical analysis verify the\nframework's balancing capability in addressing the \"impossible triangle\". This\nresearch provides comprehensive support--from theory to practice--for building\na trustworthy online produce ecosystem, establishing a critical pathway from\nalgorithmic decision-making to consumer trust.", "AI": {"tldr": "This paper addresses trust issues in online fruit/vegetable e-commerce by proposing a 'Trust Pyramid' model and 'Triangular Trust Index' (TTI) to quantify trade-offs in agricultural grading. It introduces TriAlignXA, an explainable AI framework that transforms algorithms from decision-makers to transparent decision-support systems.", "motivation": "The 'trust deficit' in online produce e-commerce arises from the inability to directly perceive product quality digitally. Traditional absolute grading standards face limitations due to the 'impossible triangle' of biological characteristics, timeliness, and economic viability in agricultural products.", "method": "Developed the TriAlignXA framework with three engines: Bio-Adaptive Engine for quality description, Timeliness Optimization Engine for efficiency, and Economic Optimization Engine for cost control. Includes 'Pre-Mapping Mechanism' using QR codes to transparently convey quality information through dual-source verification.", "result": "Experiments on grading tasks show significantly higher accuracy than baseline models. The framework demonstrates effective balancing capability in addressing the 'impossible triangle' constraints, with empirical evidence and theoretical analysis supporting its performance.", "conclusion": "The research provides comprehensive support from theory to practice for building trustworthy online produce ecosystems, establishing a critical pathway from algorithmic decision-making to consumer trust through transparent, explainable AI systems."}}
{"id": "2510.02190", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02190", "abs": "https://arxiv.org/abs/2510.02190", "authors": ["Yang Yao", "Yixu Wang", "Yuxuan Zhang", "Yi Lu", "Tianle Gu", "Lingyu Li", "Dingyi Zhao", "Keming Wu", "Haozhe Wang", "Ping Nie", "Yan Teng", "Yingchun Wang"], "title": "A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports", "comment": null, "summary": "Artificial intelligence is undergoing the paradigm shift from closed language\nmodels to interconnected agent systems capable of external perception and\ninformation integration. As a representative embodiment, Deep Research Agents\n(DRAs) systematically exhibit the capabilities for task decomposition,\ncross-source retrieval, multi-stage reasoning, and structured output, which\nmarkedly enhance performance on complex and open-ended tasks. However, existing\nbenchmarks remain deficient in evaluation dimensions, response formatting, and\nscoring mechanisms, limiting their capacity to assess such systems effectively.\nThis paper introduces a rigorous benchmark and a multidimensional evaluation\nframework tailored to DRAs and report-style responses. The benchmark comprises\n214 expert-curated challenging queries distributed across 10 broad thematic\ndomains, each accompanied by manually constructed reference bundles to support\ncomposite evaluation. The framework enables comprehensive evaluation of\nlong-form reports generated by DRAs, incorporating integrated scoring metrics\nfor semantic quality, topical focus, and retrieval trustworthiness. Extensive\nexperimentation confirms the superior performance of mainstream DRAs over\nweb-search-tool-augmented reasoning models, yet reveals considerable scope for\nfurther improvement. This study provides a robust foundation for capability\nassessment, architectural refinement, and paradigm advancement in DRA systems.", "AI": {"tldr": "This paper introduces a comprehensive benchmark and evaluation framework for Deep Research Agents (DRAs) to address deficiencies in existing assessment methods for AI agent systems.", "motivation": "Existing benchmarks are inadequate for evaluating interconnected agent systems like DRAs due to limited evaluation dimensions, response formatting, and scoring mechanisms, preventing effective assessment of their capabilities on complex tasks.", "method": "Developed a rigorous benchmark with 214 expert-curated queries across 10 domains, accompanied by manually constructed reference bundles, and created a multidimensional evaluation framework with integrated scoring metrics for semantic quality, topical focus, and retrieval trustworthiness.", "result": "Experiments showed mainstream DRAs outperform web-search-tool-augmented reasoning models, but significant room for improvement remains in DRA capabilities.", "conclusion": "The study provides a robust foundation for assessing capabilities, refining architectures, and advancing paradigms in DRA systems."}}
{"id": "2510.01527", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01527", "abs": "https://arxiv.org/abs/2510.01527", "authors": ["Lecheng Kong", "Xiyuan Wang", "Yixin Chen", "Muhan Zhang"], "title": "Round-trip Reinforcement Learning: Self-Consistent Training for Better Chemical LLMs", "comment": "19 pages", "summary": "Large Language Models (LLMs) are emerging as versatile foundation models for\ncomputational chemistry, handling bidirectional tasks like reaction prediction\nand retrosynthesis. However, these models often lack round-trip consistency.\nFor instance, a state-of-the-art chemical LLM may successfully caption a\nmolecule, yet be unable to accurately reconstruct the original structure from\nits own generated text. This inconsistency suggests that models are learning\nunidirectional memorization rather than flexible mastery. Indeed, recent work\nhas demonstrated a strong correlation between a model's round-trip consistency\nand its performance on the primary tasks. This strong correlation reframes\nconsistency into a direct target for model improvement. We therefore introduce\nRound-Trip Reinforcement Learning (RTRL), a novel framework that trains a model\nto improve its consistency by using the success of a round-trip transformation\nas a reward signal. We further propose an iterative variant where forward and\nreverse mappings alternately train each other in a self-improvement loop, a\nprocess that is highly data-efficient and notably effective with the massive\namount of unlabelled data common in chemistry. Experiments demonstrate that\nRTRL significantly \\textbf{boosts performance and consistency} over strong\nbaselines across supervised, self-supervised, and synthetic data regimes. This\nwork shows that round-trip consistency is not just a desirable property but a\ntrainable objective, offering a new path toward more robust and reliable\nfoundation models.", "AI": {"tldr": "RTRL is a reinforcement learning framework that trains LLMs to achieve round-trip consistency in chemical tasks, using successful round-trip transformations as rewards, which significantly improves performance and consistency.", "motivation": "Current chemical LLMs lack round-trip consistency - they can perform tasks like molecule captioning but fail to reconstruct original structures from their own generated text, indicating memorization rather than true understanding.", "method": "Round-Trip Reinforcement Learning (RTRL) uses successful round-trip transformations as reward signals. An iterative variant alternates forward and reverse mappings in a self-improvement loop, leveraging unlabeled chemical data efficiently.", "result": "RTRL significantly boosts performance and consistency over strong baselines across supervised, self-supervised, and synthetic data regimes.", "conclusion": "Round-trip consistency is not just a desirable property but a trainable objective, offering a new path toward more robust and reliable foundation models for computational chemistry."}}
{"id": "2510.01991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01991", "abs": "https://arxiv.org/abs/2510.01991", "authors": ["Lei Liu", "Can Wang", "Zhenghao Chen", "Dong Xu"], "title": "4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing", "comment": null, "summary": "Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges\nwith view, temporal, and non-editing region consistency, as well as with\nhandling complex text instructions. To address these issues, we propose\n4DGS-Craft, a consistent and interactive 4DGS editing framework. We first\nintroduce a 4D-aware InstructPix2Pix model to ensure both view and temporal\nconsistency. This model incorporates 4D VGGT geometry features extracted from\nthe initial scene, enabling it to capture underlying 4D geometric structures\nduring editing. We further enhance this model with a multi-view grid module\nthat enforces consistency by iteratively refining multi-view input images while\njointly optimizing the underlying 4D scene. Furthermore, we preserve the\nconsistency of non-edited regions through a novel Gaussian selection mechanism,\nwhich identifies and optimizes only the Gaussians within the edited regions.\nBeyond consistency, facilitating user interaction is also crucial for effective\n4DGS editing. Therefore, we design an LLM-based module for user intent\nunderstanding. This module employs a user instruction template to define atomic\nediting operations and leverages an LLM for reasoning. As a result, our\nframework can interpret user intent and decompose complex instructions into a\nlogical sequence of atomic operations, enabling it to handle intricate user\ncommands and further enhance editing performance. Compared to related works,\nour approach enables more consistent and controllable 4D scene editing. Our\ncode will be made available upon acceptance.", "AI": {"tldr": "4DGS-Craft is a consistent and interactive 4D Gaussian Splatting editing framework that addresses view, temporal, and non-editing region consistency issues while handling complex text instructions through LLM-based intent understanding.", "motivation": "Recent 4DGS editing methods face challenges with view, temporal, and non-editing region consistency, as well as difficulty handling complex text instructions, which limits their practical usability.", "method": "Proposes a 4D-aware InstructPix2Pix model with 4D VGGT geometry features, multi-view grid module for iterative refinement, Gaussian selection mechanism for preserving non-edited regions, and LLM-based module for user intent understanding and instruction decomposition.", "result": "The framework enables more consistent and controllable 4D scene editing compared to related works, with improved handling of complex user commands through logical operation sequences.", "conclusion": "4DGS-Craft successfully addresses key consistency challenges in 4DGS editing while providing interactive capabilities through LLM-based intent understanding, making it a more practical solution for complex 4D scene editing tasks."}}
{"id": "2510.02194", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02194", "abs": "https://arxiv.org/abs/2510.02194", "authors": ["Yuhao Sun", "Zhuoer Xu", "Shiwen Cui", "Kun Yang", "Lingyun Yu", "Yongdong Zhang", "Hongtao Xie"], "title": "UpSafe$^\\circ$C: Upcycling for Controllable Safety in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable progress across a wide\nrange of tasks, but remain vulnerable to safety risks such as harmful content\ngeneration and jailbreak attacks. Existing safety techniques -- including\nexternal guardrails, inference-time guidance, and post-training alignment --\neach face limitations in balancing safety, utility, and controllability. In\nthis work, we propose UpSafe$^\\circ$C, a unified framework for enhancing LLM\nsafety through safety-aware upcycling. Our approach first identifies\nsafety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)\nstructure, where the router acts as a soft guardrail that selectively activates\noriginal MLPs and added safety experts. We further introduce a two-stage SFT\nstrategy to strengthen safety discrimination while preserving general\ncapabilities. To enable flexible control at inference time, we introduce a\nsafety temperature mechanism, allowing dynamic adjustment of the trade-off\nbetween safety and utility. Experiments across multiple benchmarks, base model,\nand model scales demonstrate that UpSafe$^\\circ$C achieves robust safety\nimprovements against harmful and jailbreak inputs, while maintaining\ncompetitive performance on general tasks. Moreover, analysis shows that safety\ntemperature provides fine-grained inference-time control that achieves the\nPareto-optimal frontier between utility and safety. Our results highlight a new\ndirection for LLM safety: moving from static alignment toward dynamic, modular,\nand inference-aware control.", "AI": {"tldr": "UpSafe\u00b0C is a unified framework that enhances LLM safety through safety-aware upcycling, converting safety-critical layers into sparse Mixture-of-Experts with safety experts and enabling dynamic inference-time control via safety temperature.", "motivation": "LLMs remain vulnerable to safety risks like harmful content generation and jailbreak attacks, while existing safety techniques face limitations in balancing safety, utility, and controllability.", "method": "Identify safety-critical layers and upcycle them into sparse MoE structure with safety experts, use two-stage SFT strategy to strengthen safety discrimination while preserving general capabilities, and introduce safety temperature mechanism for dynamic inference-time control.", "result": "Achieves robust safety improvements against harmful and jailbreak inputs while maintaining competitive performance on general tasks, with safety temperature providing fine-grained inference-time control that achieves Pareto-optimal frontier between utility and safety.", "conclusion": "Highlights a new direction for LLM safety: moving from static alignment toward dynamic, modular, and inference-aware control."}}
{"id": "2510.01529", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01529", "abs": "https://arxiv.org/abs/2510.01529", "authors": ["Jaiden Fairoze", "Sanjam Garg", "Keewoo Lee", "Mingyuan Wang"], "title": "Bypassing Prompt Guards in Production with Controlled-Release Prompting", "comment": null, "summary": "As large language models (LLMs) advance, ensuring AI safety and alignment is\nparamount. One popular approach is prompt guards, lightweight mechanisms\ndesigned to filter malicious queries while being easy to implement and update.\nIn this work, we introduce a new attack that circumvents such prompt guards,\nhighlighting their limitations. Our method consistently jailbreaks production\nmodels while maintaining response quality, even under the highly protected chat\ninterfaces of Google Gemini (2.5 Flash/Pro), DeepSeek Chat (DeepThink), Grok\n(3), and Mistral Le Chat (Magistral). The attack exploits a resource asymmetry\nbetween the prompt guard and the main LLM, encoding a jailbreak prompt that\nlightweight guards cannot decode but the main model can. This reveals an attack\nsurface inherent to lightweight prompt guards in modern LLM architectures and\nunderscores the need to shift defenses from blocking malicious inputs to\npreventing malicious outputs. We additionally identify other critical alignment\nissues, such as copyrighted data extraction, training data extraction, and\nmalicious response leakage during thinking.", "AI": {"tldr": "A new attack method bypasses prompt guards in large language models by exploiting resource asymmetry between lightweight guards and main LLMs, successfully jailbreaking production models while maintaining response quality.", "motivation": "As LLMs advance, ensuring AI safety through prompt guards is important, but current lightweight prompt guards have limitations that need to be exposed and addressed.", "method": "The attack exploits resource asymmetry between prompt guards and main LLMs by encoding jailbreak prompts that lightweight guards cannot decode but the main model can interpret.", "result": "The method consistently jailbreaks production models including Google Gemini, DeepSeek Chat, Grok, and Mistral Le Chat while maintaining response quality, even under highly protected chat interfaces.", "conclusion": "This reveals inherent vulnerabilities in lightweight prompt guards and underscores the need to shift defenses from blocking malicious inputs to preventing malicious outputs, while also identifying other critical alignment issues."}}
{"id": "2510.01997", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01997", "abs": "https://arxiv.org/abs/2510.01997", "authors": ["Junyu Wu", "Jie Tang", "Jie Liu", "Gangshan Wu"], "title": "Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution", "comment": null, "summary": "Image Super-Resolution (SR) aims to reconstruct high-resolution images from\nlow-resolution counterparts, but the computational complexity of deep\nlearning-based methods often hinders practical deployment. CAMixer is the\npioneering work to integrate the advantages of existing lightweight SR methods\nand proposes a content-aware mixer to route token mixers of varied complexities\naccording to the difficulty of content recovery. However, several limitations\nremain, such as poor adaptability, coarse-grained masking and spatial\ninflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking\nmechanism that identifies pure pixels and exempts them from expensive\ncomputations. PP utilizes fixed color center points to classify pixels into\ndistinct categories, enabling fine-grained, spatially flexible masking while\nmaintaining adaptive flexibility. Integrated into the state-of-the-art\nATD-light model, PP-ATD-light achieves superior SR performance with minimal\noverhead, outperforming CAMixer-ATD-light in reconstruction quality and\nparameter efficiency when saving a similar amount of computation.", "AI": {"tldr": "Pure-Pass (PP) is a pixel-level masking mechanism that identifies pure pixels and exempts them from expensive computations in image super-resolution, improving efficiency and performance over previous methods like CAMixer.", "motivation": "Deep learning-based image super-resolution methods face computational complexity issues that hinder practical deployment. Existing approaches like CAMixer have limitations including poor adaptability, coarse-grained masking, and spatial inflexibility.", "method": "Proposes Pure-Pass (PP), a pixel-level masking mechanism that uses fixed color center points to classify pixels into categories, enabling fine-grained, spatially flexible masking while maintaining adaptive flexibility.", "result": "When integrated into ATD-light model, PP-ATD-light achieves superior SR performance with minimal overhead, outperforming CAMixer-ATD-light in reconstruction quality and parameter efficiency while saving similar computation.", "conclusion": "Pure-Pass provides an effective pixel-level masking approach that improves computational efficiency and performance in image super-resolution tasks."}}
{"id": "2510.02230", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02230", "abs": "https://arxiv.org/abs/2510.02230", "authors": ["Phuc Minh Nguyen", "Chinh D. La", "Duy M. H. Nguyen", "Nitesh V. Chawla", "Binh T. Nguyen", "Khoa D. Doan"], "title": "The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models", "comment": "23 pages, 15 figures", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key\nmethod for improving Large Language Models' reasoning capabilities, yet recent\nevidence suggests it may paradoxically shrink the reasoning boundary rather\nthan expand it. This paper investigates the shrinkage issue of RLVR by\nanalyzing its learning dynamics and reveals two critical phenomena that explain\nthis failure. First, we expose negative interference in RLVR, where learning to\nsolve certain training problems actively reduces the likelihood of correct\nsolutions for others, leading to the decline of Pass@$k$ performance, or the\nprobability of generating a correct solution within $k$ attempts. Second, we\nuncover the winner-take-all phenomenon: RLVR disproportionately reinforces\nproblems with high likelihood, correct solutions, under the base model, while\nsuppressing other initially low-likelihood ones. Through extensive theoretical\nand empirical analysis on multiple mathematical reasoning benchmarks, we show\nthat this effect arises from the inherent on-policy sampling in standard RL\nobjectives, causing the model to converge toward narrow solution strategies.\nBased on these insights, we propose a simple yet effective data curation\nalgorithm that focuses RLVR learning on low-likelihood problems, achieving\nnotable improvement in Pass@$k$ performance. Our code is available at\nhttps://github.com/mail-research/SELF-llm-interference.", "AI": {"tldr": "RLVR (Reinforcement Learning with Verifiable Rewards) can paradoxically shrink reasoning boundaries in LLMs due to negative interference and winner-take-all phenomena, but a data curation algorithm focusing on low-likelihood problems improves performance.", "motivation": "To investigate why RLVR, despite being designed to improve reasoning capabilities, actually shrinks the reasoning boundary in LLMs rather than expanding it.", "method": "Analyzed RLVR learning dynamics through theoretical and empirical analysis on mathematical reasoning benchmarks, revealing negative interference and winner-take-all phenomena. Proposed a data curation algorithm that focuses RLVR learning on low-likelihood problems.", "result": "Identified two critical failure mechanisms: negative interference (learning some problems reduces solution likelihood for others) and winner-take-all (disproportionate reinforcement of high-likelihood problems). The proposed data curation algorithm achieved notable improvement in Pass@k performance.", "conclusion": "RLVR's shrinkage problem stems from inherent on-policy sampling in standard RL objectives, causing convergence toward narrow solution strategies. Focusing learning on low-likelihood problems through data curation effectively mitigates this issue."}}
{"id": "2510.01533", "categories": ["cs.LG", "68T05"], "pdf": "https://arxiv.org/pdf/2510.01533", "abs": "https://arxiv.org/abs/2510.01533", "authors": ["Kobi Cohen-Arazi", "Michael Roe", "Zhen Hu", "Rohan Chavan", "Anna Ptasznik", "Joanna Lin", "Joao Morais", "Joseph Boccuzzi", "Tommaso Balercia"], "title": "NVIDIA AI Aerial: AI-Native Wireless Communications", "comment": "7 pages, 7 figures", "summary": "6G brings a paradigm shift towards AI-native wireless systems, necessitating\nthe seamless integration of digital signal processing (DSP) and machine\nlearning (ML) within the software stacks of cellular networks. This\ntransformation brings the life cycle of modern networks closer to AI systems,\nwhere models and algorithms are iteratively trained, simulated, and deployed\nacross adjacent environments. In this work, we propose a robust framework that\ncompiles Python-based algorithms into GPU-runnable blobs. The result is a\nunified approach that ensures efficiency, flexibility, and the highest possible\nperformance on NVIDIA GPUs. As an example of the capabilities of the framework,\nwe demonstrate the efficacy of performing the channel estimation function in\nthe PUSCH receiver through a convolutional neural network (CNN) trained in\nPython. This is done in a digital twin first, and subsequently in a real-time\ntestbed. Our proposed methodology, realized in the NVIDIA AI Aerial platform,\nlays the foundation for scalable integration of AI/ML models into\nnext-generation cellular systems, and is essential for realizing the vision of\nnatively intelligent 6G networks.", "AI": {"tldr": "A framework that compiles Python-based algorithms into GPU-runnable blobs for 6G AI-native wireless systems, enabling efficient integration of DSP and ML in cellular networks.", "motivation": "6G requires seamless integration of digital signal processing and machine learning in cellular networks, bringing network life cycles closer to AI systems where models are iteratively trained, simulated, and deployed.", "method": "Propose a robust framework that compiles Python-based algorithms into GPU-runnable blobs, demonstrated through a convolutional neural network for channel estimation in PUSCH receiver, implemented in digital twin and real-time testbed using NVIDIA AI Aerial platform.", "result": "Achieves a unified approach ensuring efficiency, flexibility, and highest performance on NVIDIA GPUs for AI/ML integration in cellular systems.", "conclusion": "The methodology lays foundation for scalable AI/ML integration in next-generation cellular systems and is essential for realizing natively intelligent 6G networks."}}
{"id": "2510.02001", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02001", "abs": "https://arxiv.org/abs/2510.02001", "authors": ["Nanaka Hosokawa", "Ryo Takahashi", "Tomoya Kitano", "Yukihiro Iida", "Chisako Muramatsu", "Tatsuro Hayashi", "Yuta Seino", "Xiangrong Zhou", "Takeshi Hara", "Akitoshi Katsumata", "Hiroshi Fujita"], "title": "Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework", "comment": "Intended for submission to Scientific Reports", "summary": "In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to\nautomatically generate jaw cyst findings on dental panoramic radiographs. To\nimprove accuracy, we constructed a Self-correction Loop with Structured Output\n(SLSO) framework and verified its effectiveness. A 10-step process was\nimplemented for 22 cases of jaw cysts, including image input and analysis,\nstructured data generation, tooth number extraction and consistency checking,\niterative regeneration when inconsistencies were detected, and finding\ngeneration with subsequent restructuring and consistency verification. A\ncomparative experiment was conducted using the conventional Chain-of-Thought\n(CoT) method across seven evaluation items: transparency, internal structure,\nborders, root resorption, tooth movement, relationships with other structures,\nand tooth number. The results showed that the proposed SLSO framework improved\noutput accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates\nfor tooth number, tooth movement, and root resorption, respectively. In the\nsuccessful cases, a consistently structured output was achieved after up to\nfive regenerations. Although statistical significance was not reached because\nof the small size of the dataset, the overall SLSO framework enforced negative\nfinding descriptions, suppressed hallucinations, and improved tooth number\nidentification accuracy. However, the accurate identification of extensive\nlesions spanning multiple teeth is limited. Nevertheless, further refinement is\nrequired to enhance overall performance and move toward a practical finding\ngeneration system.", "AI": {"tldr": "Using GPT-4o with Self-correction Loop with Structured Output (SLSO) framework to generate jaw cyst findings from dental panoramic radiographs, showing improved accuracy over Chain-of-Thought method.", "motivation": "To improve the accuracy of automated jaw cyst findings generation from dental panoramic radiographs using multimodal AI capabilities.", "method": "Implemented a 10-step SLSO framework including image analysis, structured data generation, tooth number extraction with consistency checking, iterative regeneration for inconsistencies, and finding generation with verification.", "result": "SLSO improved accuracy: 66.9% for tooth number, 33.3% for tooth movement, and 28.6% for root resorption. Successful cases achieved consistent output after up to 5 regenerations.", "conclusion": "SLSO framework reduces hallucinations and improves accuracy, but has limitations with extensive lesions spanning multiple teeth. Further refinement needed for practical application."}}
{"id": "2510.02250", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02250", "abs": "https://arxiv.org/abs/2510.02250", "authors": ["Gonzalo Gonzalez-Pumariega", "Vincent Tu", "Chih-Lun Lee", "Jiachen Yang", "Ang Li", "Xin Eric Wang"], "title": "The Unreasonable Effectiveness of Scaling Agents for Computer Use", "comment": "23 pages, 7 figures, 10 tables", "summary": "Computer-use agents (CUAs) hold promise for automating everyday digital\ntasks, but their unreliability and high variance hinder their application to\nlong-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method\nthat scales over agents by generating multiple rollouts and selecting among\nthem using behavior narratives that describe the agents' rollouts. It enables\nboth wide exploration and principled trajectory selection, substantially\nimproving robustness and success rates. On OSWorld, our bBoN scaling method\nestablishes a new state of the art (SoTA) at 69.9%, significantly outperforming\nprior methods and approaching human-level performance at 72%, with\ncomprehensive ablations validating key design choices. We further demonstrate\nstrong generalization results to different operating systems on\nWindowsAgentArena and AndroidWorld. Crucially, our results highlight the\nunreasonable effectiveness of scaling CUAs, when you do it right: effective\nscaling requires structured trajectory understanding and selection, and bBoN\nprovides a practical framework to achieve this.", "AI": {"tldr": "Behavior Best-of-N (bBoN) improves computer-use agents by generating multiple rollouts and selecting the best using behavior narratives, achieving near-human performance on OSWorld (69.9%) and strong generalization across operating systems.", "motivation": "Computer-use agents are unreliable for long-horizon tasks due to high variance, limiting their practical application in automating complex digital workflows.", "method": "bBoN generates multiple agent rollouts and selects the best trajectory using behavior narratives that describe the agents' actions and outcomes, enabling both exploration and principled selection.", "result": "Achieves 69.9% success rate on OSWorld (approaching human 72%), establishes new SoTA, and demonstrates strong generalization on WindowsAgentArena and AndroidWorld.", "conclusion": "Scaling computer-use agents effectively requires structured trajectory understanding and selection, and bBoN provides a practical framework to achieve robust performance across different operating systems."}}
{"id": "2510.01538", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01538", "abs": "https://arxiv.org/abs/2510.01538", "authors": ["Haokun Zhao", "Xiang Zhang", "Jiaqi Wei", "Yiwei Xu", "Yuting He", "Siqi Sun", "Chenyu You"], "title": "TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis", "comment": null, "summary": "Time series forecasting is central to decision-making in domains as diverse\nas energy, finance, climate, and public health. In practice, forecasters face\nthousands of short, noisy series that vary in frequency, quality, and horizon,\nwhere the dominant cost lies not in model fitting, but in the labor-intensive\npreprocessing, validation, and ensembling required to obtain reliable\npredictions. Prevailing statistical and deep learning models are tailored to\nspecific datasets or domains and generalize poorly. A general, domain-agnostic\nframework that minimizes human intervention is urgently in demand. In this\npaper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic\nframework for general time series forecasting. The framework comprises four\nspecialized agents: Curator performs LLM-guided diagnostics augmented by\nexternal tools that reason over data statistics to choose targeted\npreprocessing; Planner narrows the hypothesis space of model choice by\nleveraging multi-modal diagnostics and self-planning over the input; Forecaster\nperforms model fitting and validation and, based on the results, adaptively\nselects the best model configuration as well as ensemble strategy to make final\npredictions; and Reporter synthesizes the whole process into a comprehensive,\ntransparent report. With transparent natural-language rationales and\ncomprehensive reports, TSci transforms the forecasting workflow into a\nwhite-box system that is both interpretable and extensible across tasks.\nEmpirical results on eight established benchmarks demonstrate that TSci\nconsistently outperforms both statistical and LLM-based baselines, reducing\nforecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci\nproduces a clear and rigorous report that makes the forecasting workflow more\ntransparent and interpretable.", "AI": {"tldr": "TSci is the first LLM-driven agentic framework for general time series forecasting that uses four specialized agents to automate preprocessing, model selection, forecasting, and reporting, achieving significant error reduction compared to statistical and LLM baselines.", "motivation": "Current time series forecasting faces challenges with thousands of short, noisy series across diverse domains, requiring labor-intensive preprocessing and validation. Existing models are domain-specific and generalize poorly, creating demand for a domain-agnostic framework that minimizes human intervention.", "method": "TSci uses four specialized LLM-driven agents: Curator (LLM-guided diagnostics and preprocessing), Planner (hypothesis space narrowing using multi-modal diagnostics), Forecaster (model fitting, validation, and ensemble selection), and Reporter (comprehensive report generation).", "result": "TSci outperforms both statistical and LLM-based baselines on eight benchmarks, reducing forecast error by an average of 10.4% and 38.2% respectively, while producing transparent, interpretable reports.", "conclusion": "TSci transforms time series forecasting into a white-box system that is interpretable, extensible across tasks, and significantly reduces human intervention while improving forecasting accuracy across diverse domains."}}
{"id": "2510.02028", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02028", "abs": "https://arxiv.org/abs/2510.02028", "authors": ["Mario Resino", "Borja P\u00e9rez", "Jaime Godoy", "Abdulla Al-Kaff", "Fernando Garc\u00eda"], "title": "LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction", "comment": "7 pages, 3 figures, 7 tables, Submitted to ICRA", "summary": "This work proposed a 3D autoencoder architecture, named LiLa-Net, which\nencodes efficient features from real traffic environments, employing only the\nLiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,\nequipped with Velodyne LiDAR. The system leverage skip connections concept to\nimprove the performance without using extensive resources as the\nstate-of-the-art architectures. Key changes include reducing the number of\nencoder layers and simplifying the skip connections, while still producing an\nefficient and representative latent space which allows to accurately\nreconstruct the original point cloud. Furthermore, an effective balance has\nbeen achieved between the information carried by the skip connections and the\nlatent encoding, leading to improved reconstruction quality without\ncompromising performance. Finally, the model demonstrates strong generalization\ncapabilities, successfully reconstructing objects unrelated to the original\ntraffic environment.", "AI": {"tldr": "LiLa-Net is a 3D autoencoder that uses LiDAR point clouds from real traffic environments, featuring simplified skip connections and fewer encoder layers for efficient performance while maintaining accurate reconstruction.", "motivation": "To develop an efficient 3D autoencoder architecture that can encode features from LiDAR point clouds in real traffic environments without requiring extensive computational resources like state-of-the-art architectures.", "method": "Proposed LiLa-Net with reduced encoder layers and simplified skip connections, leveraging real semi-autonomous vehicle data with Velodyne LiDAR to create an efficient latent space for point cloud reconstruction.", "result": "Achieved effective balance between skip connection information and latent encoding, leading to improved reconstruction quality without performance compromise, and demonstrated strong generalization by reconstructing objects unrelated to original traffic environment.", "conclusion": "LiLa-Net successfully creates an efficient 3D autoencoder that accurately reconstructs LiDAR point clouds with simplified architecture while maintaining performance and demonstrating good generalization capabilities."}}
{"id": "2510.02263", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02263", "abs": "https://arxiv.org/abs/2510.02263", "authors": ["Yuxiao Qu", "Anikait Singh", "Yoonho Lee", "Amrith Setlur", "Ruslan Salakhutdinov", "Chelsea Finn", "Aviral Kumar"], "title": "RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems", "comment": null, "summary": "Reasoning requires going beyond pattern matching or memorization of solutions\nto identify and implement \"algorithmic procedures\" that can be used to deduce\nanswers to hard problems. Doing so requires realizing the most relevant\nprimitives, intermediate results, or shared procedures, and building upon them.\nWhile RL post-training on long chains of thought ultimately aims to uncover\nthis kind of algorithmic behavior, most reasoning traces learned by large\nmodels fail to consistently capture or reuse procedures, instead drifting into\nverbose and degenerate exploration. To address more effective reasoning, we\nintroduce reasoning abstractions: concise natural language descriptions of\nprocedural and factual knowledge that guide the model toward learning\nsuccessful reasoning. We train models to be capable of proposing multiple\nabstractions given a problem, followed by RL that incentivizes building a\nsolution while using the information provided by these abstractions. This\nresults in a two-player RL training paradigm, abbreviated as RLAD, that jointly\ntrains an abstraction generator and a solution generator. This setup\neffectively enables structured exploration, decouples learning signals of\nabstraction proposal and solution generation, and improves generalization to\nharder problems. We also show that allocating more test-time compute to\ngenerating abstractions is more beneficial for performance than generating more\nsolutions at large test budgets, illustrating the role of abstractions in\nguiding meaningful exploration.", "AI": {"tldr": "The paper introduces reasoning abstractions - concise natural language descriptions of procedural knowledge - and RLAD, a two-player RL training paradigm that jointly trains an abstraction generator and solution generator to improve reasoning capabilities.", "motivation": "Current reasoning traces in large models often fail to consistently capture or reuse procedures, instead drifting into verbose exploration. The paper aims to address this by enabling more effective reasoning through structured exploration.", "method": "RLAD (two-player RL training) that jointly trains an abstraction generator to propose multiple abstractions and a solution generator that uses these abstractions. Models are trained to propose abstractions followed by RL that incentivizes building solutions using the abstraction information.", "result": "The approach enables structured exploration, decouples learning signals between abstraction proposal and solution generation, and improves generalization to harder problems. Allocating more test-time compute to generating abstractions is more beneficial than generating more solutions.", "conclusion": "Reasoning abstractions and the RLAD training paradigm effectively guide meaningful exploration and improve reasoning capabilities by enabling structured procedural knowledge capture and reuse."}}
{"id": "2510.01539", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01539", "abs": "https://arxiv.org/abs/2510.01539", "authors": ["Aniket Vashishtha", "Qirun Dai", "Hongyuan Mei", "Amit Sharma", "Chenhao Tan", "Hao Peng"], "title": "Executable Counterfactuals: Improving LLMs' Causal Reasoning Through Code", "comment": null, "summary": "Counterfactual reasoning, a hallmark of intelligence, consists of three\nsteps: inferring latent variables from observations (abduction), constructing\nalternatives (interventions), and predicting their outcomes (prediction). This\nskill is essential for advancing LLMs' causal understanding and expanding their\napplications in high-stakes domains such as scientific research. However,\nexisting efforts in assessing LLM's counterfactual reasoning capabilities tend\nto skip the abduction step, effectively reducing to interventional reasoning\nand leading to overestimation of LLM performance. To address this, we introduce\nexecutable counterfactuals, a novel framework that operationalizes causal\nreasoning through code and math problems. Our framework explicitly requires all\nthree steps of counterfactual reasoning and enables scalable synthetic data\ncreation with varying difficulty, creating a frontier for evaluating and\nimproving LLM's reasoning. Our results reveal substantial drop in accuracy\n(25-40%) from interventional to counterfactual reasoning for SOTA models like\no4-mini and Claude-4-Sonnet. To address this gap, we construct a training set\ncomprising counterfactual code problems having if-else condition and test on\nout-of-domain code structures (e.g. having while-loop); we also test whether a\nmodel trained on code would generalize to counterfactual math word problems.\nWhile supervised finetuning on stronger models' reasoning traces improves\nin-domain performance of Qwen models, it leads to a decrease in accuracy on OOD\ntasks such as counterfactual math problems. In contrast, reinforcement learning\ninduces the core cognitive behaviors and generalizes to new domains, yielding\ngains over the base model on both code (improvement of 1.5x-2x) and math\nproblems. Analysis of the reasoning traces reinforces these findings and\nhighlights the promise of RL for improving LLMs' counterfactual reasoning.", "AI": {"tldr": "The paper introduces 'executable counterfactuals' - a framework that requires all three steps of counterfactual reasoning (abduction, intervention, prediction) through code and math problems. It reveals a 25-40% performance drop in SOTA models when moving from interventional to full counterfactual reasoning, and shows that reinforcement learning outperforms supervised finetuning for generalization.", "motivation": "Existing evaluations of LLMs' counterfactual reasoning skip the abduction step, leading to overestimation of performance. This gap is critical since counterfactual reasoning is essential for causal understanding and high-stakes applications like scientific research.", "method": "Developed executable counterfactuals framework using code and math problems that explicitly require abduction, intervention, and prediction. Created scalable synthetic data with varying difficulty. Compared supervised finetuning vs reinforcement learning approaches for improving counterfactual reasoning.", "result": "Found 25-40% accuracy drop from interventional to counterfactual reasoning in SOTA models. Supervised finetuning improved in-domain performance but hurt OOD generalization. Reinforcement learning achieved 1.5x-2x improvement on code problems and generalized to math problems, inducing core cognitive behaviors.", "conclusion": "Reinforcement learning is more effective than supervised finetuning for improving LLMs' counterfactual reasoning, as it induces fundamental cognitive behaviors that generalize across domains, while supervised approaches risk overfitting."}}
{"id": "2510.02030", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02030", "abs": "https://arxiv.org/abs/2510.02030", "authors": ["Jenna Kline", "Maksim Kholiavchenko", "Samuel Stevens", "Nina van Tiel", "Alison Zhong", "Namrata Banerji", "Alec Sheets", "Sowbaranika Balasubramaniam", "Isla Duporge", "Matthew Thompson", "Elizabeth Campolongo", "Jackson Miliko", "Neil Rosser", "Tanya Berger-Wolf", "Charles V. Stewart", "Daniel I. Rubenstein"], "title": "kabr-tools: Automated Framework for Multi-Species Behavioral Monitoring", "comment": "31 pages", "summary": "A comprehensive understanding of animal behavior ecology depends on scalable\napproaches to quantify and interpret complex, multidimensional behavioral\npatterns. Traditional field observations are often limited in scope,\ntime-consuming, and labor-intensive, hindering the assessment of behavioral\nresponses across landscapes. To address this, we present kabr-tools (Kenyan\nAnimal Behavior Recognition Tools), an open-source package for automated\nmulti-species behavioral monitoring. This framework integrates drone-based\nvideo with machine learning systems to extract behavioral, social, and spatial\nmetrics from wildlife footage. Our pipeline leverages object detection,\ntracking, and behavioral classification systems to generate key metrics,\nincluding time budgets, behavioral transitions, social interactions, habitat\nassociations, and group composition dynamics. Compared to ground-based methods,\ndrone-based observations significantly improved behavioral granularity,\nreducing visibility loss by 15% and capturing more transitions with higher\naccuracy and continuity. We validate kabr-tools through three case studies,\nanalyzing 969 behavioral sequences, surpassing the capacity of traditional\nmethods for data capture and annotation. We found that, like Plains zebras,\nvigilance in Grevy's zebras decreases with herd size, but, unlike Plains\nzebras, habitat has a negligible impact. Plains and Grevy's zebras exhibit\nstrong behavioral inertia, with rare transitions to alert behaviors and\nobserved spatial segregation between Grevy's zebras, Plains zebras, and\ngiraffes in mixed-species herds. By enabling automated behavioral monitoring at\nscale, kabr-tools offers a powerful tool for ecosystem-wide studies, advancing\nconservation, biodiversity research, and ecological monitoring.", "AI": {"tldr": "kabr-tools is an open-source package for automated multi-species behavioral monitoring using drone-based video and machine learning to extract behavioral, social, and spatial metrics from wildlife footage.", "motivation": "Traditional field observations are limited in scope, time-consuming, and labor-intensive, hindering the assessment of behavioral responses across landscapes. There's a need for scalable approaches to quantify complex behavioral patterns.", "method": "The framework integrates drone-based video with machine learning systems including object detection, tracking, and behavioral classification to generate metrics like time budgets, behavioral transitions, social interactions, habitat associations, and group composition dynamics.", "result": "Drone-based observations significantly improved behavioral granularity, reducing visibility loss by 15% and capturing more transitions with higher accuracy. The system analyzed 969 behavioral sequences across three case studies, revealing species-specific behavioral patterns including differences in vigilance responses and spatial segregation in mixed-species herds.", "conclusion": "kabr-tools enables automated behavioral monitoring at scale, offering a powerful tool for ecosystem-wide studies that advances conservation, biodiversity research, and ecological monitoring."}}
{"id": "2510.02276", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02276", "abs": "https://arxiv.org/abs/2510.02276", "authors": ["Chenqi Li", "Yu Liu", "Timothy Denison", "Tingting Zhu"], "title": "BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge Transfer across Biosignals", "comment": null, "summary": "Biosignals offer valuable insights into the physiological states of the human\nbody. Although biosignal modalities differ in functionality, signal fidelity,\nsensor comfort, and cost, they are often intercorrelated, reflecting the\nholistic and interconnected nature of human physiology. This opens up the\npossibility of performing the same tasks using alternative biosignal\nmodalities, thereby improving the accessibility, usability, and adaptability of\nhealth monitoring systems. However, the limited availability of large labeled\ndatasets presents challenges for training models tailored to specific tasks and\nmodalities of interest. Unsupervised cross-modal knowledge transfer offers a\npromising solution by leveraging knowledge from an existing modality to support\nmodel training for a new modality. Existing methods are typically based on\nknowledge distillation, which requires running a teacher model alongside\nstudent model training, resulting in high computational and memory overhead.\nThis challenge is further exacerbated by the recent development of foundation\nmodels that demonstrate superior performance and generalization across tasks at\nthe cost of large model sizes. To this end, we explore a new framework for\nunsupervised cross-modal knowledge transfer of biosignals by training a\nlightweight bridge network to align the intermediate representations and enable\ninformation flow between foundation models and across modalities. Specifically,\nwe introduce an efficient strategy for selecting alignment positions where the\nbridge should be constructed, along with a flexible prototype network as the\nbridge architecture. Extensive experiments across multiple biosignal\nmodalities, tasks, and datasets show that BioX-Bridge reduces the number of\ntrainable parameters by 88--99\\% while maintaining or even improving transfer\nperformance compared to state-of-the-art methods.", "AI": {"tldr": "BioX-Bridge is a framework for unsupervised cross-modal knowledge transfer of biosignals that uses a lightweight bridge network to align intermediate representations between foundation models, reducing trainable parameters by 88-99% while maintaining performance.", "motivation": "Biosignals are intercorrelated but face limited labeled datasets. Existing cross-modal transfer methods based on knowledge distillation are computationally expensive, especially with large foundation models.", "method": "Train a lightweight bridge network to align intermediate representations between foundation models across modalities, with an efficient strategy for selecting alignment positions and a flexible prototype network as bridge architecture.", "result": "BioX-Bridge reduces trainable parameters by 88-99% while maintaining or improving transfer performance compared to state-of-the-art methods across multiple biosignal modalities, tasks, and datasets.", "conclusion": "The proposed framework enables efficient cross-modal knowledge transfer for biosignals with significantly reduced computational overhead while preserving performance."}}
{"id": "2510.01545", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01545", "abs": "https://arxiv.org/abs/2510.01545", "authors": ["Haoyuan Cai", "Zhenghao Peng", "Bolei Zhou"], "title": "Predictive Preference Learning from Human Interventions", "comment": "NeurIPS 2025 Spotlight. Project page:\n  https://metadriverse.github.io/ppl", "summary": "Learning from human involvement aims to incorporate the human subject to\nmonitor and correct agent behavior errors. Although most interactive imitation\nlearning methods focus on correcting the agent's action at the current state,\nthey do not adjust its actions in future states, which may be potentially more\nhazardous. To address this, we introduce Predictive Preference Learning from\nHuman Interventions (PPL), which leverages the implicit preference signals\ncontained in human interventions to inform predictions of future rollouts. The\nkey idea of PPL is to bootstrap each human intervention into L future time\nsteps, called the preference horizon, with the assumption that the agent\nfollows the same action and the human makes the same intervention in the\npreference horizon. By applying preference optimization on these future states,\nexpert corrections are propagated into the safety-critical regions where the\nagent is expected to explore, significantly improving learning efficiency and\nreducing human demonstrations needed. We evaluate our approach with experiments\non both autonomous driving and robotic manipulation benchmarks and demonstrate\nits efficiency and generality. Our theoretical analysis further shows that\nselecting an appropriate preference horizon L balances coverage of risky states\nwith label correctness, thereby bounding the algorithmic optimality gap. Demo\nand code are available at: https://metadriverse.github.io/ppl", "AI": {"tldr": "PPL is a predictive preference learning method that bootstraps human interventions to future time steps, propagating expert corrections to safety-critical regions to improve learning efficiency and reduce human demonstrations needed.", "motivation": "Current interactive imitation learning methods only correct agent actions at current states without adjusting future potentially hazardous actions, limiting their effectiveness in safety-critical scenarios.", "method": "Leverages implicit preference signals from human interventions by bootstrapping each intervention L future time steps (preference horizon), assuming same agent action and human intervention patterns, then applies preference optimization on these future states.", "result": "Demonstrated efficiency and generality on autonomous driving and robotic manipulation benchmarks, showing significant improvement in learning efficiency and reduction of human demonstrations needed.", "conclusion": "PPL effectively propagates expert corrections to safety-critical regions, with theoretical analysis showing that appropriate preference horizon selection balances risky state coverage with label correctness to bound algorithmic optimality gap."}}
{"id": "2510.02034", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02034", "abs": "https://arxiv.org/abs/2510.02034", "authors": ["Mengtian Li", "Yunshu Bai", "Yimin Chu", "Yijun Shen", "Zhongmei Li", "Weifeng Ge", "Zhifeng Xie", "Chaofeng Chen"], "title": "GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing", "comment": "Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/", "summary": "We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape\nand texture morphing from multi-view images. Previous approaches usually rely\non point clouds or require pre-defined homeomorphic mappings for untextured\ndata. Our method overcomes these limitations by leveraging mesh-guided 3D\nGaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.\nThe core of our framework is a unified deformation strategy that anchors\n3DGaussians to reconstructed mesh patches, ensuring geometrically consistent\ntransformations while preserving texture fidelity through topology-aware\nconstraints. In parallel, our framework establishes unsupervised semantic\ncorrespondence by using the mesh topology as a geometric prior and maintains\nstructural integrity via physically plausible point trajectories. This\nintegrated approach preserves both local detail and global semantic coherence\nthroughout the morphing process with out requiring labeled data. On our\nproposed TexMorph benchmark, GaussianMorphing substantially outperforms prior\n2D/3D methods, reducing color consistency error ($\\Delta E$) by 22.2% and EI by\n26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/", "AI": {"tldr": "GaussianMorphing is a novel framework for semantic-aware 3D shape and texture morphing from multi-view images using mesh-guided 3D Gaussian Splatting and unified deformation strategy.", "motivation": "Previous approaches rely on point clouds or require pre-defined homeomorphic mappings for untextured data, which have limitations for high-fidelity geometry and appearance modeling.", "method": "Leverages mesh-guided 3D Gaussian Splatting (3DGS) with unified deformation strategy that anchors 3D Gaussians to reconstructed mesh patches, ensuring geometrically consistent transformations and preserving texture fidelity through topology-aware constraints.", "result": "Outperforms prior 2D/3D methods on TexMorph benchmark, reducing color consistency error (\u0394E) by 22.2% and EI by 26.2%.", "conclusion": "The framework enables semantic-aware 3D morphing with preserved local detail and global semantic coherence without requiring labeled data."}}
{"id": "2510.01549", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01549", "abs": "https://arxiv.org/abs/2510.01549", "authors": ["Kevin Zhai", "Utsav Singh", "Anirudh Thatipelli", "Souradip Chakraborty", "Anit Kumar Sahu", "Furong Huang", "Amrit Singh Bedi", "Mubarak Shah"], "title": "MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I Diffusion Models", "comment": null, "summary": "Diffusion models excel at generating images conditioned on text prompts, but\nthe resulting images often do not satisfy user-specific criteria measured by\nscalar rewards such as Aesthetic Scores. This alignment typically requires\nfine-tuning, which is computationally demanding. Recently, inference-time\nalignment via noise optimization has emerged as an efficient alternative,\nmodifying initial input noise to steer the diffusion denoising process towards\ngenerating high-reward images. However, this approach suffers from reward\nhacking, where the model produces images that score highly, yet deviate\nsignificantly from the original prompt. We show that noise-space regularization\nis insufficient and that preventing reward hacking requires an explicit\nimage-space constraint. To this end, we propose MIRA (MItigating Reward\nhAcking), a training-free, inference-time alignment method. MIRA introduces an\nimage-space, score-based KL surrogate that regularizes the sampling trajectory\nwith a frozen backbone, constraining the output distribution so reward can\nincrease without off-distribution drift (reward hacking). We derive a tractable\napproximation to KL using diffusion scores. Across SDv1.5 and SDXL, multiple\nrewards (Aesthetic, HPSv2, PickScore), and public datasets (e.g.,\nAnimal-Animal, HPDv2), MIRA achieves >60\\% win rate vs. strong baselines while\npreserving prompt adherence; mechanism plots show reward gains with near-zero\ndrift, whereas DNO drifts as compute increases. We further introduce MIRA-DPO,\nmapping preference optimization to inference time with a frozen backbone,\nextending MIRA to non-differentiable rewards without fine-tuning.", "AI": {"tldr": "MIRA is a training-free inference-time alignment method that prevents reward hacking in diffusion models by using image-space regularization to maintain prompt adherence while improving reward scores.", "motivation": "Diffusion models often fail to satisfy user-specific criteria measured by scalar rewards, and existing inference-time alignment methods suffer from reward hacking where images score highly but deviate from the original prompt.", "method": "MIRA introduces an image-space, score-based KL surrogate that regularizes the sampling trajectory with a frozen backbone, constraining the output distribution to prevent off-distribution drift while increasing rewards.", "result": "Across SDv1.5 and SDXL, multiple rewards and datasets, MIRA achieves >60% win rate vs. strong baselines while preserving prompt adherence, with reward gains and near-zero drift compared to DNO which drifts as compute increases.", "conclusion": "MIRA effectively prevents reward hacking through image-space constraints and can be extended to non-differentiable rewards via MIRA-DPO without requiring fine-tuning."}}
{"id": "2510.02043", "categories": ["cs.CV", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02043", "abs": "https://arxiv.org/abs/2510.02043", "authors": ["Sahil Bhandary Karnoor", "Romit Roy Choudhury"], "title": "Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers", "comment": null, "summary": "Pose estimation refers to tracking a human's full body posture, including\ntheir head, torso, arms, and legs. The problem is challenging in practical\nsettings where the number of body sensors are limited. Past work has shown\npromising results using conditional diffusion models, where the pose prediction\nis conditioned on both <location, rotation> measurements from the sensors.\nUnfortunately, nearly all these approaches generalize poorly across users,\nprimarly because location measurements are highly influenced by the body size\nof the user. In this paper, we formulate pose estimation as an inverse problem\nand design an algorithm capable of zero-shot generalization. Our idea utilizes\na pre-trained diffusion model and conditions it on rotational measurements\nalone; the priors from this model are then guided by a likelihood term, derived\nfrom the measured locations. Thus, given any user, our proposed InPose method\ngeneratively estimates the highly likely sequence of poses that best explains\nthe sparse on-body measurements.", "AI": {"tldr": "InPose: A diffusion-based pose estimation method that uses only rotational measurements with location-guided priors for zero-shot generalization across users.", "motivation": "Existing pose estimation methods using conditional diffusion models with both location and rotation measurements generalize poorly across users due to location measurements being highly influenced by body size variations.", "method": "Formulates pose estimation as an inverse problem using a pre-trained diffusion model conditioned only on rotational measurements, guided by a likelihood term derived from measured locations.", "result": "The proposed InPose method achieves zero-shot generalization by generatively estimating pose sequences that best explain sparse on-body measurements for any user.", "conclusion": "InPose successfully addresses the generalization problem in pose estimation by decoupling rotational conditioning from body-size-dependent location measurements through an inverse problem formulation."}}
{"id": "2510.01555", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01555", "abs": "https://arxiv.org/abs/2510.01555", "authors": ["Kezhao Liu", "Jason Klein Liu", "Mingtao Chen", "Yiming Liu"], "title": "Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) leverages a\nKullback-Leibler (KL) divergence loss to stabilize training and prevent\noverfitting. However, in methods such as GRPO, its implementation may be guided\nby principles from numerical value estimation-a practice that overlooks the\nterm's functional role as an optimization loss. To analyze this issue, we\nestablish a unified framework that connects two seemingly distinct\nimplementation styles: using the mathematical term $k_n$ as a detached\ncoefficient for the policy's score function ('$k_n$ in reward') or as a direct\nloss function through which gradients are propagated ('$k_n$ as loss'). We show\nthat the latter can always be analyzed via an equivalent gradient coefficient\nin the former, unifying the two perspectives. Through this framework, we prove\nthat the conventional '$k_1$ in reward' (like in PPO) is the principled loss\nfor Reverse KL (RKL) regularization. We further establish a key finding: under\non-policy conditions, the '$k_2$ as loss' formulation is, in fact,\ngradient-equivalent to '$k_1$ in reward'. This equivalence, first proven in our\nwork, identifies both as the theoretically sound implementations of the RKL\nobjective. In contrast, we show that the recently adopted '$k_3$ as loss' (like\nin GRPO) is merely a first-order, biased approximation of the principled loss.\nFurthermore, we argue that common off-policy implementations of '$k_n$ as loss'\nmethods are biased due to neglected importance sampling, and we propose a\nprincipled correction. Our findings provide a comprehensive, gradient-based\nrationale for choosing and correctly implementing KL regularization, paving the\nway for more robust and effective RLHF systems.", "AI": {"tldr": "The paper establishes a unified framework for KL divergence regularization in RLHF, showing that 'k1 in reward' (PPO-style) and 'k2 as loss' are gradient-equivalent and theoretically sound, while 'k3 as loss' (GRPO-style) is a biased approximation.", "motivation": "To address the inconsistent implementation of KL divergence regularization in RLHF methods, where some approaches treat it as a detached coefficient while others use it directly as a loss function, leading to potential theoretical issues.", "method": "Developed a unified framework connecting two implementation styles: 'kn in reward' (coefficient for policy score) and 'kn as loss' (direct loss function). Proved gradient equivalence between 'k1 in reward' and 'k2 as loss' under on-policy conditions, and identified biases in off-policy implementations.", "result": "Showed that 'k1 in reward' is the principled loss for Reverse KL regularization, and 'k2 as loss' is gradient-equivalent to it. Demonstrated that 'k3 as loss' is a first-order biased approximation. Proposed principled correction for off-policy implementations.", "conclusion": "Provides comprehensive gradient-based rationale for choosing and correctly implementing KL regularization in RLHF, enabling more robust and effective systems by unifying theoretical understanding of different implementation approaches."}}
{"id": "2510.02086", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02086", "abs": "https://arxiv.org/abs/2510.02086", "authors": ["Arman Behnam"], "title": "VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation", "comment": null, "summary": "Accurate detection and segmentation of brain tumors from magnetic resonance\nimaging (MRI) are essential for diagnosis, treatment planning, and clinical\nmonitoring. While convolutional architectures such as U-Net have long been the\nbackbone of medical image segmentation, their limited capacity to capture\nlong-range dependencies constrains performance on complex tumor structures.\nRecent advances in diffusion models have demonstrated strong potential for\ngenerating high-fidelity medical images and refining segmentation boundaries.\n  In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor\nDetection and Segmentation framework, a transformer-driven diffusion framework\nfor brain tumor detection and segmentation. By embedding a vision transformer\nat the core of the diffusion process, the model leverages global contextual\nreasoning together with iterative denoising to enhance both volumetric accuracy\nand boundary precision. The transformer backbone enables more effective\nmodeling of spatial relationships across entire MRI volumes, while diffusion\nrefinement mitigates voxel-level errors and recovers fine-grained tumor\ndetails.\n  This hybrid design provides a pathway toward improved robustness and\nscalability in neuro-oncology, moving beyond conventional U-Net baselines.\nExperimental validation on MRI brain tumor datasets demonstrates consistent\ngains in Dice similarity and Hausdorff distance, underscoring the potential of\ntransformer-guided diffusion models to advance the state of the art in tumor\nsegmentation.", "AI": {"tldr": "VGDM is a transformer-driven diffusion model for brain tumor detection and segmentation that combines vision transformers with diffusion processes to improve accuracy and boundary precision.", "motivation": "Convolutional architectures like U-Net have limited capacity to capture long-range dependencies, constraining performance on complex tumor structures. Diffusion models show strong potential for medical image segmentation but need better global contextual reasoning.", "method": "Embed a vision transformer at the core of the diffusion process, leveraging global contextual reasoning with iterative denoising. The transformer backbone models spatial relationships across entire MRI volumes while diffusion refinement mitigates voxel-level errors.", "result": "Experimental validation on MRI brain tumor datasets shows consistent gains in Dice similarity and Hausdorff distance metrics.", "conclusion": "VGDM provides improved robustness and scalability in neuro-oncology, advancing beyond conventional U-Net baselines and demonstrating the potential of transformer-guided diffusion models for tumor segmentation."}}
{"id": "2510.01562", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01562", "abs": "https://arxiv.org/abs/2510.01562", "authors": ["Seong Woo Han", "Daniel Duy Vo", "Brielin C. Brown"], "title": "Large-Scale Bayesian Causal Discovery with Interventional Data", "comment": null, "summary": "Inferring the causal relationships among a set of variables in the form of a\ndirected acyclic graph (DAG) is an important but notoriously challenging\nproblem. Recently, advancements in high-throughput genomic perturbation screens\nhave inspired development of methods that leverage interventional data to\nimprove model identification. However, existing methods still suffer poor\nperformance on large-scale tasks and fail to quantify uncertainty. Here, we\npropose Interventional Bayesian Causal Discovery (IBCD), an empirical Bayesian\nframework for causal discovery with interventional data. Our approach models\nthe likelihood of the matrix of total causal effects, which can be approximated\nby a matrix normal distribution, rather than the full data matrix. We place a\nspike-and-slab horseshoe prior on the edges and separately learn data-driven\nweights for scale-free and Erd\\H{o}s-R\\'enyi structures from observational\ndata, treating each edge as a latent variable to enable uncertainty-aware\ninference. Through extensive simulation, we show that IBCD achieves superior\nstructure recovery compared to existing baselines. We apply IBCD to CRISPR\nperturbation (Perturb-seq) data on 521 genes, demonstrating that edge posterior\ninclusion probabilities enable identification of robust graph structures.", "AI": {"tldr": "IBCD is a Bayesian framework for causal discovery using interventional data that models causal effects with matrix normal distribution and spike-and-slab priors, enabling uncertainty quantification and superior structure recovery.", "motivation": "Existing causal discovery methods perform poorly on large-scale tasks and lack uncertainty quantification, despite advancements in genomic perturbation screens that provide interventional data.", "method": "Models likelihood of total causal effects matrix using matrix normal distribution, places spike-and-slab horseshoe prior on edges, learns data-driven weights for network structures from observational data, and treats edges as latent variables for uncertainty-aware inference.", "result": "IBCD achieves superior structure recovery compared to existing baselines in simulations and successfully identifies robust graph structures from CRISPR perturbation data on 521 genes using edge posterior inclusion probabilities.", "conclusion": "The proposed Bayesian framework effectively addresses limitations of existing methods by enabling uncertainty quantification and improved performance on large-scale causal discovery tasks with interventional data."}}
{"id": "2510.02097", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02097", "abs": "https://arxiv.org/abs/2510.02097", "authors": ["Walid Rabehi", "Marion Le Texier", "R\u00e9mi Lemoy"], "title": "Mapping Historic Urban Footprints in France: Balancing Quality, Scalability and AI Techniques", "comment": null, "summary": "Quantitative analysis of historical urban sprawl in France before the 1970s\nis hindered by the lack of nationwide digital urban footprint data. This study\nbridges this gap by developing a scalable deep learning pipeline to extract\nurban areas from the Scan Histo historical map series (1925-1950), which\nproduces the first open-access, national-scale urban footprint dataset for this\npivotal period. Our key innovation is a dual-pass U-Net approach designed to\nhandle the high radiometric and stylistic complexity of historical maps. The\nfirst pass, trained on an initial dataset, generates a preliminary map that\nidentifies areas of confusion, such as text and roads, to guide targeted data\naugmentation. The second pass uses a refined dataset and the binarized output\nof the first model to minimize radiometric noise, which significantly reduces\nfalse positives. Deployed on a high-performance computing cluster, our method\nprocesses 941 high-resolution tiles covering the entirety of metropolitan\nFrance. The final mosaic achieves an overall accuracy of 73%, effectively\ncapturing diverse urban patterns while overcoming common artifacts like labels\nand contour lines. We openly release the code, training datasets, and the\nresulting nationwide urban raster to support future research in long-term\nurbanization dynamics.", "AI": {"tldr": "A deep learning pipeline extracts urban areas from historical French maps (1925-1950) to create the first national-scale urban footprint dataset for this period, achieving 73% accuracy.", "motivation": "To address the lack of nationwide digital urban footprint data for historical urban sprawl analysis in France before the 1970s.", "method": "Dual-pass U-Net approach: first pass identifies confusing areas for targeted data augmentation, second pass uses refined dataset and binarized output to reduce radiometric noise. Processed 941 high-resolution tiles on HPC cluster.", "result": "Created first open-access national-scale urban footprint dataset for 1925-1950 period with 73% overall accuracy, effectively capturing diverse urban patterns while minimizing artifacts.", "conclusion": "Successfully bridged historical data gap by developing scalable deep learning method, releasing code, datasets, and nationwide urban raster to support long-term urbanization research."}}
{"id": "2510.01565", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.01565", "abs": "https://arxiv.org/abs/2510.01565", "authors": ["Runyu Lu", "Shiqi He", "Wenxuan Tan", "Shenggui Li", "Ruofan Wu", "Jeff J. Ma", "Ang Chen", "Mosharaf Chowdhury"], "title": "TetriServe: Efficient DiT Serving for Heterogeneous Image Generation", "comment": null, "summary": "Diffusion Transformer (DiT) models excel at generating highquality images\nthrough iterative denoising steps, but serving them under strict Service Level\nObjectives (SLOs) is challenging due to their high computational cost,\nparticularly at large resolutions. Existing serving systems use fixed degree\nsequence parallelism, which is inefficient for heterogeneous workloads with\nmixed resolutions and deadlines, leading to poor GPU utilization and low SLO\nattainment.\n  In this paper, we propose step-level sequence parallelism to dynamically\nadjust the parallel degree of individual requests according to their deadlines.\nWe present TetriServe, a DiT serving system that implements this strategy for\nhighly efficient image generation. Specifically, TetriServe introduces a novel\nround-based scheduling mechanism that improves SLO attainment: (1) discretizing\ntime into fixed rounds to make deadline-aware scheduling tractable, (2)\nadapting parallelism at the step level and minimize GPU hour consumption, and\n(3) jointly packing requests to minimize late completions. Extensive evaluation\non state-of-the-art DiT models shows that TetriServe achieves up to 32% higher\nSLO attainment compared to existing solutions without degrading image quality.", "AI": {"tldr": "TetriServe introduces step-level sequence parallelism for efficient Diffusion Transformer (DiT) serving, dynamically adjusting parallelism per request based on deadlines to improve SLO attainment by up to 32% compared to existing solutions.", "motivation": "Existing DiT serving systems use fixed parallelism, which is inefficient for heterogeneous workloads with mixed resolutions and deadlines, leading to poor GPU utilization and low SLO attainment.", "method": "TetriServe implements step-level sequence parallelism with round-based scheduling: discretizing time into fixed rounds, adapting parallelism at step level to minimize GPU consumption, and jointly packing requests to minimize late completions.", "result": "Extensive evaluation shows TetriServe achieves up to 32% higher SLO attainment compared to existing solutions without degrading image quality.", "conclusion": "Step-level sequence parallelism with dynamic adaptation based on deadlines enables highly efficient DiT serving with significantly improved SLO performance."}}
{"id": "2510.02100", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02100", "abs": "https://arxiv.org/abs/2510.02100", "authors": ["Woowon Jang", "Jiwon Im", "Juseung Choi", "Niki Rashidian", "Wesley De Neve", "Utku Ozbulak"], "title": "When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based Tracking in Surgical Videos", "comment": "Accepted for publication in the 28th International Conference on\n  Medical Image Computing and Computer Assisted Intervention (MICCAI) Workshop\n  on Collaborative Intelligence and Autonomy in Image-guided Surgery (COLAS),\n  2025", "summary": "Video object segmentation (VOS) models such as SAM2 offer promising zero-shot\ntracking capabilities for surgical videos using minimal user input. Among the\navailable input types, point-based tracking offers an efficient and low-cost\nalternative, yet its reliability and failure cases in complex surgical\nenvironments are not well understood. In this work, we systematically analyze\nthe failure modes of point-based tracking in laparoscopic cholecystectomy\nvideos. Focusing on three surgical targets, the gallbladder, grasper, and\nL-hook electrocautery, we compare the performance of point-based tracking with\nsegmentation mask initialization. Our results show that point-based tracking is\ncompetitive for surgical tools but consistently underperforms for anatomical\ntargets, where tissue similarity and ambiguous boundaries lead to failure.\nThrough qualitative analysis, we reveal key factors influencing tracking\noutcomes and provide several actionable recommendations for selecting and\nplacing tracking points to improve performance in surgical video analysis.", "AI": {"tldr": "Analysis of point-based tracking failure modes in surgical videos shows it works well for tools but fails for anatomical targets due to tissue similarity and ambiguous boundaries.", "motivation": "To understand the reliability and failure cases of point-based tracking in complex surgical environments, particularly for laparoscopic cholecystectomy videos.", "method": "Systematic analysis comparing point-based tracking with segmentation mask initialization for three surgical targets (gallbladder, grasper, L-hook electrocautery) in laparoscopic cholecystectomy videos.", "result": "Point-based tracking is competitive for surgical tools but consistently underperforms for anatomical targets due to tissue similarity and ambiguous boundaries.", "conclusion": "Provides actionable recommendations for selecting and placing tracking points to improve performance in surgical video analysis, highlighting key factors influencing tracking outcomes."}}
{"id": "2510.01571", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.01571", "abs": "https://arxiv.org/abs/2510.01571", "authors": ["Hanqun Cao", "Hongrui Zhang", "Junde Xu", "Zhou Zhang", "Lingdong Shen", "Minghao Sun", "Ge Liu", "Jinbo Xu", "Wu-Jun Li", "Jinren Ni", "Cesar de la Fuente-Nunez", "Tianfan Fu", "Yejin Choi", "Pheng-Ann Heng", "Fang Wu"], "title": "From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?", "comment": "24 pages, 7 figures, 4 tables", "summary": "Protein language models (PLMs) have advanced computational protein science\nthrough large-scale pretraining and scalable architectures. In parallel,\nreinforcement learning (RL) has broadened exploration and enabled precise\nmulti-objective optimization in protein design. Yet whether RL can push PLMs\nbeyond their pretraining priors to uncover latent sequence-structure-function\nrules remains unclear. We address this by pairing RL with PLMs across four\ndomains: antimicrobial peptide design, kinase variant optimization, antibody\nengineering, and inverse folding. Using diverse RL algorithms and model\nclasses, we ask if RL improves sampling efficiency and, more importantly, if it\nreveals capabilities not captured by supervised learning. Across benchmarks, RL\nconsistently boosts success rates and sample efficiency. Performance follows a\nthree-factor interaction: task headroom, reward fidelity, and policy capacity\njointly determine gains. When rewards are accurate and informative, policies\nhave sufficient capacity, and tasks leave room beyond supervised baselines,\nimprovements scale; when rewards are noisy or capacity is constrained, gains\nsaturate despite exploration. This view yields practical guidance for RL in\nprotein design: prioritize reward modeling and calibration before scaling\npolicy size, match algorithm and regularization strength to task difficulty,\nand allocate capacity where marginal gains are largest. Implementation is\navailable at https://github.com/chq1155/RL-PLM.", "AI": {"tldr": "RL combined with protein language models improves protein design success rates and efficiency across multiple domains, with performance gains depending on task headroom, reward fidelity, and policy capacity.", "motivation": "To determine if reinforcement learning can push protein language models beyond their pretraining priors and uncover latent sequence-structure-function rules that supervised learning cannot capture.", "method": "Pairing RL with PLMs across four protein design domains using diverse RL algorithms and model classes, analyzing the interaction between task headroom, reward fidelity, and policy capacity.", "result": "RL consistently boosts success rates and sample efficiency across benchmarks. Performance follows a three-factor interaction where gains scale when rewards are accurate, policies have sufficient capacity, and tasks leave room beyond supervised baselines.", "conclusion": "Practical guidance for RL in protein design: prioritize reward modeling and calibration before scaling policy size, match algorithm and regularization strength to task difficulty, and allocate capacity where marginal gains are largest."}}
{"id": "2510.02114", "categories": ["cs.CV", "68T10"], "pdf": "https://arxiv.org/pdf/2510.02114", "abs": "https://arxiv.org/abs/2510.02114", "authors": ["Ding-Ruei Shen"], "title": "FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation", "comment": "Master Thesis", "summary": "Federeated Learning (FL) offers a privacy-preserving solution for Semantic\nSegmentation (SS) tasks to adapt to new domains, but faces significant\nchallenges from these domain shifts, particularly when client data is\nunlabeled. However, most existing FL methods unrealistically assume access to\nlabeled data on remote clients or fail to leverage the power of modern Vision\nFoundation Models (VFMs). Here, we propose a novel and challenging task,\nFFREEDG, in which a model is pretrained on a server's labeled source dataset\nand subsequently trained across clients using only their unlabeled data,\nwithout ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a\nframework that leverages the knowledge of a VFM by integrating vision and\nlanguage modalities. Our approach employs a Vision-Language decoder guided by\nCLIP-based text embeddings to improve semantic disambiguation and uses a\nweak-to-strong consistency learning strategy for robust local training on\npseudo-labels. Our experiments on synthetic-to-real and\nclear-to-adverse-weather benchmarks demonstrate that our framework effectively\ntackles this new task, achieving competitive performance against established\ndomain generalization and adaptation methods and setting a strong baseline for\nfuture research.", "AI": {"tldr": "FRIEREN is a federated learning framework that addresses domain shifts in semantic segmentation using vision foundation models and unlabeled client data, without re-accessing source data.", "motivation": "To solve the challenge of domain shifts in federated learning for semantic segmentation when client data is unlabeled, while leveraging modern vision foundation models.", "method": "Uses Vision-Language decoder guided by CLIP text embeddings for semantic disambiguation and weak-to-strong consistency learning for robust local training on pseudo-labels.", "result": "Achieves competitive performance against established domain generalization and adaptation methods on synthetic-to-real and clear-to-adverse-weather benchmarks.", "conclusion": "FRIEREN effectively tackles the FFREEDG task and sets a strong baseline for future research in federated learning with unlabeled client data."}}
{"id": "2510.01578", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01578", "abs": "https://arxiv.org/abs/2510.01578", "authors": ["Haochen You", "Baojing Liu"], "title": "Gradient Shaping Beyond Clipping: A Functional Perspective on Update Magnitude Control", "comment": "Accepted as a conference paper at ACM Multimedia Asia 2025", "summary": "Gradient clipping is widely used to stabilize deep network training, but its\nformulation as a hard, fixed threshold limits flexibility and ignores gradient\ndistribution dynamics. We propose SPAMP (Statistical Per-layer Adaptive\nModulation and Projection), a unified framework that generalizes clipping into\nsmooth, per-layer gradient shaping. SPAMP tracks local gradient statistics,\ndynamically estimates thresholds, and applies power-based transformations to\nmodulate update magnitudes in a differentiable manner. This perspective recasts\nclipping and warmup as dual mechanisms for controlling the effective update\nscale $\\eta_t \\|g_t\\|$, offering a principled alternative to rigid heuristics.\nExtensive experiments across image and language tasks demonstrate that SPAMP\nimproves stability, convergence, and robustness over existing methods.", "AI": {"tldr": "SPAMP is a unified framework that generalizes gradient clipping into smooth, per-layer gradient shaping using statistical tracking and power-based transformations.", "motivation": "Traditional gradient clipping uses hard, fixed thresholds that lack flexibility and ignore gradient distribution dynamics, limiting its effectiveness in stabilizing deep network training.", "method": "SPAMP tracks local gradient statistics, dynamically estimates thresholds, and applies power-based transformations to modulate update magnitudes in a differentiable manner, recasting clipping and warmup as dual mechanisms for controlling effective update scale.", "result": "Extensive experiments across image and language tasks show that SPAMP improves stability, convergence, and robustness over existing gradient clipping methods.", "conclusion": "SPAMP provides a principled alternative to rigid heuristics by offering a unified framework for adaptive gradient shaping that outperforms traditional clipping approaches."}}
{"id": "2510.02155", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02155", "abs": "https://arxiv.org/abs/2510.02155", "authors": ["Shu Zou", "Xinyu Tian", "Lukas Wesemann", "Fabian Waschkowski", "Zhaoyuan Yang", "Jing Zhang"], "title": "Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting", "comment": "14 pages, video anomaly detection", "summary": "Prompting has emerged as a practical way to adapt frozen vision-language\nmodels (VLMs) for video anomaly detection (VAD). Yet, existing prompts are\noften overly abstract, overlooking the fine-grained human-object interactions\nor action semantics that define complex anomalies in surveillance videos. We\npropose ASK-Hint, a structured prompting framework that leverages\naction-centric knowledge to elicit more accurate and interpretable reasoning\nfrom frozen VLMs. Our approach organizes prompts into semantically coherent\ngroups (e.g. violence, property crimes, public safety) and formulates\nfine-grained guiding questions that align model predictions with discriminative\nvisual cues. Extensive experiments on UCF-Crime and XD-Violence show that\nASK-Hint consistently improves AUC over prior baselines, achieving\nstate-of-the-art performance compared to both fine-tuned and training-free\nmethods. Beyond accuracy, our framework provides interpretable reasoning traces\ntowards anomaly and demonstrates strong generalization across datasets and VLM\nbackbones. These results highlight the critical role of prompt granularity and\nestablish ASK-Hint as a new training-free and generalizable solution for\nexplainable video anomaly detection.", "AI": {"tldr": "ASK-Hint is a structured prompting framework that uses action-centric knowledge to improve video anomaly detection with frozen vision-language models, achieving state-of-the-art performance without training.", "motivation": "Existing prompts for video anomaly detection are too abstract and overlook fine-grained human-object interactions and action semantics that define complex anomalies in surveillance videos.", "method": "Organizes prompts into semantically coherent groups (e.g., violence, property crimes, public safety) and formulates fine-grained guiding questions that align model predictions with discriminative visual cues.", "result": "Extensive experiments on UCF-Crime and XD-Violence show consistent AUC improvements over prior baselines, achieving state-of-the-art performance compared to both fine-tuned and training-free methods.", "conclusion": "ASK-Hint establishes the critical role of prompt granularity and provides a training-free, generalizable solution for explainable video anomaly detection with interpretable reasoning traces."}}
{"id": "2510.01581", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01581", "abs": "https://arxiv.org/abs/2510.01581", "authors": ["Joykirat Singh", "Justin Chih-Yao Chen", "Archiki Prasad", "Elias Stengel-Eskin", "Akshay Nambi", "Mohit Bansal"], "title": "Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression", "comment": "Code: https://github.com/joykirat18/TRAAC", "summary": "Recent thinking models solve complex reasoning tasks by scaling test-time\ncompute, but this scaling must be allocated in line with task difficulty. On\none hand, short reasoning (underthinking) leads to errors on harder problems\nthat require extended reasoning steps; but, excessively long reasoning\n(overthinking) can be token-inefficient, generating unnecessary steps even\nafter reaching a correct intermediate solution. We refer to this as\nunder-adaptivity, where the model fails to modulate its response length\nappropriately given problems of varying difficulty. To address under-adaptivity\nand strike a balance between under- and overthinking, we propose TRAAC (Think\nRight with Adaptive, Attentive Compression), an online post-training RL method\nthat leverages the model's self-attention over a long reasoning trajectory to\nidentify important steps and prune redundant ones. TRAAC also estimates\ndifficulty and incorporates it into training rewards, thereby learning to\nallocate reasoning budget commensurate with example difficulty. Our approach\nimproves accuracy, reduces reasoning steps, and enables adaptive thinking\ncompared to base models and other RL baselines. Across a variety of tasks\n(AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute\naccuracy gain of 8.4% with a relative reduction in reasoning length of 36.8%\ncompared to the base model, and a 7.9% accuracy gain paired with a 29.4% length\ndrop compared to the best RL baseline. TRAAC also shows strong generalization:\nalthough our models are trained on math datasets, they show accuracy and\nefficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH,\nand OptimalThinkingBench. Our analysis further verifies that TRAAC provides\nfine-grained adjustments to thinking budget based on difficulty and that a\ncombination of task-difficulty calibration and attention-based compression\nyields gains across diverse tasks.", "AI": {"tldr": "TRAAC is an RL method that adaptively allocates reasoning steps based on problem difficulty, using self-attention to compress reasoning trajectories and improve efficiency.", "motivation": "Current models suffer from under-adaptivity - they either underthink (too few steps for hard problems) or overthink (too many steps for easy problems), leading to inefficiency and errors.", "method": "TRAAC uses online post-training RL with self-attention to identify important reasoning steps and prune redundant ones, while incorporating difficulty estimation into rewards to allocate appropriate reasoning budget.", "result": "TRAAC achieves 8.4% accuracy gain with 36.8% reduction in reasoning length compared to base models, and outperforms other RL baselines across math and non-math tasks.", "conclusion": "The method enables adaptive thinking by combining difficulty calibration with attention-based compression, showing strong generalization across diverse reasoning tasks."}}
{"id": "2510.02186", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02186", "abs": "https://arxiv.org/abs/2510.02186", "authors": ["Weijia Dou", "Xu Zhang", "Yi Bin", "Jian Liu", "Bo Peng", "Guoqing Wang", "Yang Yang", "Heng Tao Shen"], "title": "GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation", "comment": null, "summary": "Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to\n3D semantic segmentation expose a persistent trade-off. Directly projecting 2D\nfeatures into 3D yields noisy and fragmented predictions, whereas enforcing\ngeometric coherence necessitates costly training pipelines and large-scale\nannotated 3D data. We argue that this limitation stems from the dominant\nsegmentation-and-matching paradigm, which fails to reconcile 2D semantics with\n3D geometric structure. The geometric cues are not eliminated during the\n2D-to-3D transfer but remain latent within the noisy and view-aggregated\nfeatures. To exploit this property, we propose GeoPurify that applies a small\nStudent Affinity Network to purify 2D VLM-generated 3D point features using\ngeometric priors distilled from a 3D self-supervised teacher model. During\ninference, we devise a Geometry-Guided Pooling module to further denoise the\npoint cloud and ensure the semantic and structural consistency. Benefiting from\nlatent geometric information and the learned affinity network, GeoPurify\neffectively mitigates the trade-off and achieves superior data efficiency.\nExtensive experiments on major 3D benchmarks demonstrate that GeoPurify\nachieves or surpasses state-of-the-art performance while utilizing only about\n1.5% of the training data. Our codes and checkpoints are available at\n[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).", "AI": {"tldr": "GeoPurify is a method that uses geometric priors from 3D self-supervised models to purify noisy 2D VLM-generated 3D point features, achieving state-of-the-art 3D semantic segmentation with only 1.5% training data.", "motivation": "Current approaches for transferring 2D VLM features to 3D segmentation face a trade-off: direct projection yields noisy results, while geometric coherence requires expensive training pipelines and large annotated datasets.", "method": "Uses a Student Affinity Network to purify 2D VLM-generated 3D features using geometric priors from a 3D self-supervised teacher model, with a Geometry-Guided Pooling module for inference-time denoising.", "result": "Achieves or surpasses state-of-the-art performance on major 3D benchmarks while using only about 1.5% of training data.", "conclusion": "GeoPurify effectively reconciles 2D semantics with 3D geometric structure by exploiting latent geometric information in noisy features, achieving superior data efficiency without the traditional trade-offs."}}
{"id": "2510.01588", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01588", "abs": "https://arxiv.org/abs/2510.01588", "authors": ["Ziming Tang", "Chengbin Hou", "Tianyu Zhang", "Bangxu Tian", "Jinbao Wang", "Hairong Lv"], "title": "Enhancing Noise Robustness of Parkinson's Disease Telemonitoring via Contrastive Feature Augmentation", "comment": null, "summary": "Parkinson's disease (PD) is one of the most common neurodegenerative\ndisorder. PD telemonitoring emerges as a novel assessment modality enabling\nself-administered at-home tests of Unified Parkinson's Disease Rating Scale\n(UPDRS) scores, enhancing accessibility for PD patients. However, three types\nof noise would occur during measurements: (1) patient-induced measurement\ninaccuracies, (2) environmental noise, and (3) data packet loss during\ntransmission, resulting in higher prediction errors. To address these\nchallenges, NoRo, a noise-robust UPDRS prediction framework is proposed. First,\nthe original speech features are grouped into ordered bins, based on the\ncontinuous values of a selected feature, to construct contrastive pairs.\nSecond, the contrastive pairs are employed to train a multilayer perceptron\nencoder for generating noise-robust features. Finally, these features are\nconcatenated with the original features as the augmented features, which are\nthen fed into the UPDRS prediction models. Notably, we further introduces a\nnovel evaluation approach with customizable noise injection module, and\nextensive experiments show that NoRo can successfully enhance the noise\nrobustness of UPDRS prediction across various downstream prediction models\nunder different noisy environments.", "AI": {"tldr": "NoRo is a noise-robust framework that enhances UPDRS prediction for Parkinson's disease telemonitoring by using contrastive learning to generate robust features against patient errors, environmental noise, and data loss.", "motivation": "Parkinson's disease telemonitoring faces three types of noise: patient-induced inaccuracies, environmental noise, and data packet loss, which increase prediction errors for UPDRS scores.", "method": "Group speech features into ordered bins based on selected feature values to create contrastive pairs, train MLP encoder for noise-robust features, concatenate with original features, and feed to UPDRS prediction models.", "result": "Extensive experiments show NoRo successfully enhances noise robustness of UPDRS prediction across various downstream models under different noisy environments.", "conclusion": "NoRo framework effectively addresses noise challenges in PD telemonitoring and improves the reliability of at-home UPDRS assessments."}}
{"id": "2510.02197", "categories": ["cs.CV", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02197", "abs": "https://arxiv.org/abs/2510.02197", "authors": ["Emmanuel Nsengiyumvaa", "Leonard Niyitegekaa", "Eric Umuhoza"], "title": "Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition: A Machine Learning Approach for Small-Scale Farming Applications", "comment": "20 pages", "summary": "Accurate livestock identification is a cornerstone of modern farming: it\nsupports health monitoring, breeding programs, and productivity tracking.\nHowever, common pig identification methods, such as ear tags and microchips,\nare often unreliable, costly, target pure breeds, and thus impractical for\nsmall-scale farmers. To address this gap, we propose a noninvasive biometric\nidentification approach that leverages uniqueness of the auricular vein\npatterns. To this end, we have collected 800 ear images from 20 mixed-breed\npigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a\nstandard smartphone and simple back lighting. A multistage computer vision\npipeline was developed to enhance vein visibility, extract structural and\nspatial features, and generate biometric signatures. These features were then\nclassified using machine learning models. Support Vector Machines (SVM)\nachieved the highest accuracy: correctly identifying pigs with 98.12% precision\nacross mixed-breed populations. The entire process from image processing to\nclassification was completed in an average of 8.3 seconds, demonstrating\nfeasibility for real-time farm deployment. We believe that by replacing fragile\nphysical identifiers with permanent biological markers, this system provides\nfarmers with a cost-effective and stress-free method of animal identification.\nMore broadly, the findings confirm the practicality of auricular vein\nbiometrics for digitizing livestock management, reinforcing its potential to\nextend the benefits of precision farming to resource-constrained agricultural\ncommunities.", "AI": {"tldr": "Noninvasive pig identification using auricular vein patterns achieves 98.12% accuracy with SVM classification, providing cost-effective alternative to physical tags for small-scale farmers.", "motivation": "Current pig identification methods like ear tags and microchips are unreliable, costly, target pure breeds, and impractical for small-scale farmers, creating need for better solution.", "method": "Collected 800 ear images from 20 mixed-breed pigs using smartphone with back lighting, developed multistage computer vision pipeline for vein enhancement and feature extraction, used machine learning models for classification.", "result": "Support Vector Machines achieved highest accuracy of 98.12% precision across mixed-breed populations, with entire process taking average 8.3 seconds from image to classification.", "conclusion": "Auricular vein biometrics provides cost-effective, stress-free animal identification method, demonstrating practicality for digitizing livestock management and extending precision farming benefits to resource-constrained communities."}}
{"id": "2510.01598", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2510.01598", "abs": "https://arxiv.org/abs/2510.01598", "authors": ["Youwei Bao", "Shuhan Yang", "Hyunsoo Yang"], "title": "Securing generative artificial intelligence with parallel magnetic tunnel junction true randomness", "comment": "4 figures", "summary": "Deterministic pseudo random number generators (PRNGs) used in generative\nartificial intelligence (GAI) models produce predictable patterns vulnerable to\nexploitation by attackers. Conventional defences against the vulnerabilities\noften come with significant energy and latency overhead. Here, we embed\nhardware-generated true random bits from spin-transfer torque magnetic tunnel\njunctions (STT-MTJs) to address the challenges. A highly parallel,\nFPGA-assisted prototype computing system delivers megabit-per-second true\nrandom numbers, passing NIST randomness tests after in-situ operations with\nminimal overhead. Integrating the hardware random bits into a generative\nadversarial network (GAN) trained on CIFAR-10 reduces insecure outputs by up to\n18.6 times compared to the low-quality random number generators (RNG) baseline.\nWith nanosecond switching speed, high energy efficiency, and established\nscalability, our STT-MTJ-based system holds the potential to scale beyond 106\nparallel cells, achieving gigabit-per-second throughput suitable for large\nlanguage model sampling. This advancement highlights spintronic RNGs as\npractical security components for next-generation GAI systems.", "AI": {"tldr": "This paper presents a hardware-based true random number generator using spin-transfer torque magnetic tunnel junctions (STT-MTJs) to address security vulnerabilities in AI systems caused by predictable pseudo-random number generators.", "motivation": "Deterministic pseudo-random number generators (PRNGs) in AI models create predictable patterns that are vulnerable to attacks, and conventional defenses have significant energy and latency overhead.", "method": "The researchers embedded hardware-generated true random bits from STT-MTJs in a highly parallel, FPGA-assisted prototype system. They integrated these hardware random bits into a generative adversarial network (GAN) trained on CIFAR-10.", "result": "The system delivers megabit-per-second true random numbers passing NIST tests with minimal overhead. Integration into GAN reduced insecure outputs by up to 18.6 times compared to low-quality RNG baseline. The system can potentially scale to 106 parallel cells achieving gigabit-per-second throughput.", "conclusion": "STT-MTJ-based spintronic RNGs are practical security components for next-generation generative AI systems, offering nanosecond switching speed, high energy efficiency, and established scalability."}}
{"id": "2510.02213", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02213", "abs": "https://arxiv.org/abs/2510.02213", "authors": ["Villanelle O'Reilly", "Jonathan Cox", "Georgios Leontidis", "Marc Hanheide", "Petra Bosilj", "James Brown"], "title": "MMDEW: Multipurpose Multiclass Density Estimation in the Wild", "comment": "8+1 pages, 4 figures, 5 tables", "summary": "Density map estimation can be used to estimate object counts in dense and\noccluded scenes where discrete counting-by-detection methods fail. We propose a\nmulticategory counting framework that leverages a Twins pyramid\nvision-transformer backbone and a specialised multi-class counting head built\non a state-of-the-art multiscale decoding approach. A two-task design adds a\nsegmentation-based Category Focus Module, suppressing inter-category cross-talk\nat training time. Training and evaluation on the VisDrone and iSAID benchmarks\ndemonstrates superior performance versus prior multicategory crowd-counting\napproaches (33%, 43% and 64% reduction to MAE), and the comparison with YOLOv11\nunderscores the necessity of crowd counting methods in dense scenes. The\nmethod's regional loss opens up multi-class crowd counting to new domains,\ndemonstrated through the application to a biodiversity monitoring dataset,\nhighlighting its capacity to inform conservation efforts and enable scalable\necological insights.", "AI": {"tldr": "A multicategory counting framework using Twins pyramid vision-transformer with multi-class counting head and Category Focus Module, achieving significant MAE reduction on benchmarks and demonstrating applicability to biodiversity monitoring.", "motivation": "To address object counting in dense and occluded scenes where discrete counting-by-detection methods fail, particularly for multicategory scenarios with inter-category cross-talk issues.", "method": "Uses Twins pyramid vision-transformer backbone with specialized multi-class counting head based on multiscale decoding. Includes two-task design with segmentation-based Category Focus Module to suppress inter-category cross-talk during training.", "result": "Achieved 33%, 43% and 64% reduction in MAE on VisDrone and iSAID benchmarks compared to prior multicategory crowd-counting approaches. Outperformed YOLOv11 in dense scenes.", "conclusion": "The method enables effective multicategory counting in dense scenes and can be applied to new domains like biodiversity monitoring, providing scalable ecological insights for conservation efforts."}}
{"id": "2510.01621", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01621", "abs": "https://arxiv.org/abs/2510.01621", "authors": ["Zhen Li", "Fan Zhang", "Zheng Zhang", "Yu Chen"], "title": "Posterior Collapse as a Phase Transition in Variational Autoencoders", "comment": "12 pages, 8 figures", "summary": "We investigate the phenomenon of posterior collapse in variational\nautoencoders (VAEs) from the perspective of statistical physics, and reveal\nthat it constitutes a phase transition governed jointly by data structure and\nmodel hyper-parameters. By analyzing the stability of the trivial solution\nassociated with posterior collapse, we identify a critical hyper-parameter\nthreshold. This critical boundary, separating meaningful latent inference from\ncollapse, is characterized by a discontinuity in the KL divergence between the\napproximate posterior and the prior distribution. We validate this critical\nbehavior on both synthetic and real-world datasets, confirming the existence of\na phase transition. Our results demonstrate that posterior collapse is not\nmerely an optimization failure, but rather an emerging phase transition arising\nfrom the interplay between data structure and variational constraints. This\nperspective offers new insights into the trainability and representational\ncapacity of deep generative models.", "AI": {"tldr": "Posterior collapse in VAEs is identified as a phase transition governed by data structure and model hyper-parameters, characterized by a critical threshold in KL divergence between posterior and prior distributions.", "motivation": "To understand the fundamental nature of posterior collapse in variational autoencoders from a statistical physics perspective, moving beyond viewing it as mere optimization failure.", "method": "Analyzed posterior collapse as a phase transition by examining the stability of trivial solutions and identifying critical hyper-parameter thresholds through KL divergence analysis on synthetic and real-world datasets.", "result": "Identified a critical boundary separating meaningful latent inference from collapse, characterized by discontinuity in KL divergence, confirming the existence of a phase transition phenomenon.", "conclusion": "Posterior collapse is an emerging phase transition arising from the interplay between data structure and variational constraints, providing new insights into deep generative model trainability and representational capacity."}}
{"id": "2510.02226", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02226", "abs": "https://arxiv.org/abs/2510.02226", "authors": ["Shira Schiber", "Ofir Lindenbaum", "Idan Schwartz"], "title": "TempoControl: Temporal Attention Guidance for Text-to-Video Models", "comment": "Under Review", "summary": "Recent advances in generative video models have enabled the creation of\nhigh-quality videos based on natural language prompts. However, these models\nfrequently lack fine-grained temporal control, meaning they do not allow users\nto specify when particular visual elements should appear within a generated\nsequence. In this work, we introduce TempoControl, a method that allows for\ntemporal alignment of visual concepts during inference, without requiring\nretraining or additional supervision. TempoControl utilizes cross-attention\nmaps, a key component of text-to-video diffusion models, to guide the timing of\nconcepts through a novel optimization approach. Our method steers attention\nusing three complementary principles: aligning its temporal shape with a\ncontrol signal (via correlation), amplifying it where visibility is needed (via\nenergy), and maintaining spatial focus (via entropy). TempoControl allows\nprecise control over timing while ensuring high video quality and diversity. We\ndemonstrate its effectiveness across various video generation applications,\nincluding temporal reordering for single and multiple objects, as well as\naction and audio-aligned generation.", "AI": {"tldr": "TempoControl enables fine-grained temporal control in text-to-video generation by optimizing cross-attention maps to align visual concepts with timing signals during inference, without retraining.", "motivation": "Current generative video models lack fine-grained temporal control, preventing users from specifying when specific visual elements should appear in generated sequences.", "method": "Uses cross-attention maps from text-to-video diffusion models and guides timing through a novel optimization approach that aligns temporal shape (correlation), amplifies visibility (energy), and maintains spatial focus (entropy).", "result": "Enables precise temporal control while maintaining high video quality and diversity, demonstrated in applications like temporal reordering, action generation, and audio-aligned generation.", "conclusion": "TempoControl provides effective temporal alignment of visual concepts during video generation inference without requiring retraining or additional supervision."}}
{"id": "2510.01624", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01624", "abs": "https://arxiv.org/abs/2510.01624", "authors": ["Feiyang Kang", "Michael Kuchnik", "Karthik Padthe", "Marin Vlastelica", "Ruoxi Jia", "Carole-Jean Wu", "Newsha Ardalani"], "title": "Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to Use Instead", "comment": "Preprint. Under Review", "summary": "In post-training for reasoning Large Language Models (LLMs), the current\nstate of practice trains LLMs in two independent stages: Supervised Fine-Tuning\n(SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as\n``RL'' below). In this work, we challenge whether high SFT scores translate to\nimproved performance after RL. We provide extensive counter-examples where this\nis not true. We find high SFT scores can be biased toward simpler or more\nhomogeneous data and are not reliably predictive of subsequent RL gains or\nscaled-up post-training effectiveness. In some cases, RL training on models\nwith improved SFT performance could lead to substantially worse outcome\ncompared to RL on the base model without SFT. We study alternative metrics and\nidentify generalization loss on held-out reasoning examples and Pass@large k\nperformance to provide strong proxies for the RL outcome. We trained hundreds\nof models up to 12B-parameter with SFT and RLVR via GRPO and ran extensive\nevaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPU\nhours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiple\nstate-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RL\nperformance, prediction based on generalization loss and Pass@large k achieves\nsubstantial higher precision, improving $R^2$ coefficient and Spearman's rank\ncorrelation coefficient by up to 0.5 (2x). This provides strong utility for\nbroad use cases. For example, in most experiments, we find SFT training on\nunique examples for a one epoch underperforms training on half examples for two\nepochs, either after SFT or SFT-then-RL; With the same SFT budget, training\nonly on short examples may lead to better SFT performance, though, it often\nleads to worse outcome after RL compared to training on examples with varying\nlengths. Evaluation tool will be open-sourced.", "AI": {"tldr": "High SFT scores don't reliably predict RL performance gains; generalization loss and Pass@large k are better proxies for RL outcomes.", "motivation": "Challenge the assumption that high SFT scores translate to improved RL performance, as current practice trains LLMs in two independent stages (SFT then RL).", "method": "Trained hundreds of models up to 12B parameters with SFT and RLVR via GRPO, evaluated on 7 math benchmarks with up to 256 repetitions, spending >1M GPU hours across multiple model families and datasets.", "result": "Found that high SFT scores can be biased and don't predict RL gains. RL on models with improved SFT performance can lead to worse outcomes than RL on base models. Generalization loss and Pass@large k substantially improve prediction accuracy (up to 0.5 R\u00b2 improvement).", "conclusion": "Generalization loss and Pass@large k are superior metrics for predicting RL outcomes compared to pre-RL performance, with practical implications for training strategies like epoch allocation and example length selection."}}
{"id": "2510.02240", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02240", "abs": "https://arxiv.org/abs/2510.02240", "authors": ["Sicheng Feng", "Kaiwen Tuo", "Song Wang", "Lingdong Kong", "Jianke Zhu", "Huan Wang"], "title": "RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning", "comment": null, "summary": "Fine-grained visual reasoning remains a core challenge for multimodal large\nlanguage models (MLLMs). The recently introduced ReasonMap highlights this gap\nby showing that even advanced MLLMs struggle with spatial reasoning in\nstructured and information-rich settings such as transit maps, a task of clear\npractical and scientific importance. However, standard reinforcement learning\n(RL) on such tasks is impeded by sparse rewards and unstable optimization. To\naddress this, we first construct ReasonMap-Plus, an extended dataset that\nintroduces dense reward signals through Visual Question Answering (VQA) tasks,\nenabling effective cold-start training of fine-grained visual understanding\nskills. Next, we propose RewardMap, a multi-stage RL framework designed to\nimprove both visual understanding and reasoning capabilities of MLLMs.\nRewardMap incorporates two key designs. First, we introduce a difficulty-aware\nreward design that incorporates detail rewards, directly tackling the sparse\nrewards while providing richer supervision. Second, we propose a multi-stage RL\nscheme that bootstraps training from simple perception to complex reasoning\ntasks, offering a more effective cold-start strategy than conventional\nSupervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus\ndemonstrate that each component of RewardMap contributes to consistent\nperformance gains, while their combination yields the best results. Moreover,\nmodels trained with RewardMap achieve an average improvement of 3.47% across 6\nbenchmarks spanning spatial reasoning, fine-grained visual reasoning, and\ngeneral tasks beyond transit maps, underscoring enhanced visual understanding\nand reasoning capabilities.", "AI": {"tldr": "RewardMap is a multi-stage RL framework that addresses sparse reward challenges in fine-grained visual reasoning for MLLMs through difficulty-aware rewards and progressive training from perception to complex reasoning tasks.", "motivation": "Advanced MLLMs struggle with spatial reasoning in structured settings like transit maps, and standard RL faces challenges with sparse rewards and unstable optimization.", "method": "Constructed ReasonMap-Plus dataset with dense VQA rewards, then proposed RewardMap with difficulty-aware reward design and multi-stage RL scheme that bootstraps from simple perception to complex reasoning.", "result": "Models trained with RewardMap achieved 3.47% average improvement across 6 benchmarks spanning spatial reasoning, fine-grained visual reasoning, and general tasks beyond transit maps.", "conclusion": "RewardMap effectively enhances MLLMs' visual understanding and reasoning capabilities through its multi-stage RL approach and dense reward signals."}}
{"id": "2510.01631", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01631", "abs": "https://arxiv.org/abs/2510.01631", "authors": ["Feiyang Kang", "Newsha Ardalani", "Michael Kuchnik", "Youssef Emad", "Mostafa Elhoushi", "Shubhabrata Sengupta", "Shang-Wen Li", "Ramya Raghavendra", "Ruoxi Jia", "Carole-Jean Wu"], "title": "Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of Scaling Laws, Benefits, and Pitfalls", "comment": "Published as a Main Conference paper at EMNLP 2025", "summary": "Training data plays a crucial role in Large Language Models (LLM) scaling,\nyet high quality data is of limited supply. Synthetic data techniques offer a\npotential path toward sidestepping these limitations. We conduct a large-scale\nempirical investigation (>1000 LLMs with >100k GPU hours) using a unified\nprotocol and scaling laws, comparing natural web data, diverse synthetic types\n(rephrased text, generated textbooks), and mixtures of natural and synthetic\ndata. Specifically, we found pre-training on rephrased synthetic data\n\\textit{alone} is not faster than pre-training on natural web texts; while\npre-training on 1/3 rephrased synthetic data mixed with 2/3 natural web texts\ncan speed up 5-10x (to reach the same validation loss) at larger data budgets.\nPre-training on textbook-style synthetic data \\textit{alone} results in notably\nhigher loss on many downstream domains especially at small data budgets. \"Good\"\nratios of synthetic data in training data mixtures depend on the model size and\ndata budget, empirically converging to ~30% for rephrased synthetic data.\nLarger generator models do not necessarily yield better pre-training data than\n~8B-param models. These results contribute mixed evidence on \"model collapse\"\nduring large-scale single-round (n=1) model training on synthetic\ndata--training on rephrased synthetic data shows no degradation in performance\nin foreseeable scales whereas training on mixtures of textbook-style\npure-generated synthetic data shows patterns predicted by \"model collapse\". Our\nwork demystifies synthetic data in pre-training, validates its conditional\nbenefits, and offers practical guidance.", "AI": {"tldr": "Synthetic data alone doesn't outperform natural web data, but mixing 1/3 rephrased synthetic with 2/3 natural data can speed up training 5-10x. Textbook-style synthetic data performs poorly, especially at small budgets. Optimal synthetic ratio is ~30%, and larger generators don't necessarily produce better data.", "motivation": "High-quality training data is limited for LLM scaling, and synthetic data offers potential to overcome these limitations, but its effectiveness needs empirical validation.", "method": "Large-scale empirical investigation (>1000 LLMs, >100k GPU hours) using unified protocol and scaling laws to compare natural web data, diverse synthetic types (rephrased text, generated textbooks), and mixtures.", "result": "Rephrased synthetic data alone isn't faster than natural web data, but 1/3 synthetic + 2/3 natural mixture speeds up training 5-10x. Textbook synthetic data performs poorly. Optimal synthetic ratio is ~30%. Larger generators don't necessarily produce better data than ~8B models.", "conclusion": "Synthetic data has conditional benefits - mixing rephrased synthetic with natural data provides significant speedups, while pure synthetic approaches show limitations. Provides mixed evidence on 'model collapse' and offers practical guidance for synthetic data usage."}}
{"id": "2510.02253", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02253", "abs": "https://arxiv.org/abs/2510.02253", "authors": ["Zihan Zhou", "Shilin Lu", "Shuli Leng", "Shaocong Zhang", "Zhuming Lian", "Xinlei Yu", "Adams Wai-Kin Kong"], "title": "DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing", "comment": "Preprint", "summary": "Drag-based image editing has long suffered from distortions in the target\nregion, largely because the priors of earlier base models, Stable Diffusion,\nare insufficient to project optimized latents back onto the natural image\nmanifold. With the shift from UNet-based DDPMs to more scalable DiT with flow\nmatching (e.g., SD3.5, FLUX), generative priors have become significantly\nstronger, enabling advances across diverse editing tasks. However, drag-based\nediting has yet to benefit from these stronger priors. This work proposes the\nfirst framework to effectively harness FLUX's rich prior for drag-based\nediting, dubbed DragFlow, achieving substantial gains over baselines. We first\nshow that directly applying point-based drag editing to DiTs performs poorly:\nunlike the highly compressed features of UNets, DiT features are insufficiently\nstructured to provide reliable guidance for point-wise motion supervision. To\novercome this limitation, DragFlow introduces a region-based editing paradigm,\nwhere affine transformations enable richer and more consistent feature\nsupervision. Additionally, we integrate pretrained open-domain personalization\nadapters (e.g., IP-Adapter) to enhance subject consistency, while preserving\nbackground fidelity through gradient mask-based hard constraints. Multimodal\nlarge language models (MLLMs) are further employed to resolve task ambiguities.\nFor evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)\nfeaturing region-level dragging instructions. Extensive experiments on\nDragBench-DR and ReD Bench show that DragFlow surpasses both point-based and\nregion-based baselines, setting a new state-of-the-art in drag-based image\nediting. Code and datasets will be publicly available upon publication.", "AI": {"tldr": "DragFlow is the first framework to effectively use FLUX's strong generative priors for drag-based image editing, overcoming distortions through region-based editing and achieving state-of-the-art performance.", "motivation": "Drag-based image editing has suffered from distortions due to insufficient priors from older models like Stable Diffusion. Newer models like FLUX have stronger priors but haven't been effectively utilized for drag editing, which performs poorly when directly applied to DiT architectures.", "method": "DragFlow introduces region-based editing with affine transformations for richer feature supervision, integrates IP-Adapter for subject consistency, uses gradient mask-based hard constraints for background preservation, and employs MLLMs to resolve task ambiguities.", "result": "Extensive experiments on DragBench-DR and the novel ReD Bench show DragFlow surpasses both point-based and region-based baselines, setting new state-of-the-art performance in drag-based image editing.", "conclusion": "DragFlow successfully harnesses FLUX's strong generative priors for drag-based editing through region-based supervision and multimodal integration, achieving substantial improvements over existing methods."}}
{"id": "2510.01634", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01634", "abs": "https://arxiv.org/abs/2510.01634", "authors": ["Ryan Y. Lin", "Siddhartha Ojha", "Nicholas Bai"], "title": "CAT: Curvature-Adaptive Transformers for Geometry-Aware Learning", "comment": null, "summary": "Transformers achieve strong performance across diverse domains but implicitly\nassume Euclidean geometry in their attention mechanisms, limiting their\neffectiveness on data with non-Euclidean structure. While recent extensions to\nhyperbolic and spherical spaces show promise for hierarchical and cyclical\npatterns, respectively, they require committing to a single geometry a priori,\nreducing flexibility when data exhibits mixed geometric properties. We\nintroduce the Curvature-Adaptive Transformer (CAT), a novel architecture that\ndynamically learns per-token routing across three geometric attention branches\nthrough a lightweight, differentiable gating mechanism. Unlike fixed-geometry\napproaches, CAT enables adaptive geometric specialization, routing tokens to\nthe appropriate curvature based on their local relational structure. The\nrouting network provides interpretable curvature preferences while each branch\nemploys geometry-specific operations optimized for its respective manifold. On\nknowledge graph completion benchmarks (FB15k-237, WN18RR), CAT achieves\napproximately 10% improvements in MRR and Hits@10 over fixed-geometry baselines\nwith minimal overhead (5% parameter increase, comparable inference time). These\nresults demonstrate that learned geometric adaptation outperforms any single\nfixed geometry for complex relational reasoning, establishing CAT as a scalable\nand interpretable foundation for mixture-of-geometry architectures across\nlanguage, vision, and multimodal domains.", "AI": {"tldr": "CAT is a transformer architecture that dynamically routes tokens across Euclidean, hyperbolic, and spherical attention branches using a learnable gating mechanism, achieving 10% performance improvements on knowledge graph completion tasks.", "motivation": "Standard transformers assume Euclidean geometry, which limits effectiveness on non-Euclidean data. Existing extensions to hyperbolic/spherical spaces require committing to a single geometry, reducing flexibility for mixed-geometry data.", "method": "Introduces Curvature-Adaptive Transformer with three geometric attention branches and a lightweight differentiable gating mechanism that learns per-token routing across Euclidean, hyperbolic, and spherical spaces.", "result": "Achieves ~10% improvements in MRR and Hits@10 on FB15k-237 and WN18RR benchmarks over fixed-geometry baselines, with only 5% parameter increase and comparable inference time.", "conclusion": "Learned geometric adaptation outperforms any single fixed geometry for complex relational reasoning, establishing CAT as a scalable and interpretable foundation for mixture-of-geometry architectures."}}
{"id": "2510.02262", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02262", "abs": "https://arxiv.org/abs/2510.02262", "authors": ["Guangyu Sun", "Archit Singhal", "Burak Uzkent", "Mubarak Shah", "Chen Chen", "Garin Kessler"], "title": "From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding", "comment": null, "summary": "Video Large Language Models (VLMs) have achieved remarkable results on a\nvariety of vision language tasks, yet their practical use is limited by the\n\"needle in a haystack\" problem: the massive number of visual tokens produced\nfrom raw video frames exhausts the model's context window. Existing solutions\nalleviate this issue by selecting a sparse set of frames, thereby reducing\ntoken count, but such frame-wise selection discards essential temporal\ndynamics, leading to suboptimal reasoning about motion and event continuity. In\nthis work we systematically explore the impact of temporal information and\ndemonstrate that extending selection from isolated key frames to key clips,\nwhich are short, temporally coherent segments, improves video understanding. To\nmaintain a fixed computational budget while accommodating the larger token\nfootprint of clips, we propose an adaptive resolution strategy that dynamically\nbalances spatial resolution and clip length, ensuring a constant token count\nper video. Experiments on three long-form video benchmarks demonstrate that our\ntraining-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and\n10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These\nresults highlight the importance of preserving temporal coherence in frame\nselection and provide a practical pathway for scaling Video LLMs to real world\nvideo understanding applications. Project webpage is available at\nhttps://guangyusun.com/f2c .", "AI": {"tldr": "F2C proposes using key clips instead of isolated key frames for video understanding in VLMs, with adaptive resolution to maintain fixed token count, achieving significant improvements on long-form video benchmarks.", "motivation": "Current Video LLMs suffer from the 'needle in a haystack' problem where massive visual tokens from raw videos exhaust context windows. Frame-wise selection discards essential temporal dynamics, leading to poor motion and event continuity reasoning.", "method": "Extends selection from isolated key frames to key clips (short temporally coherent segments) with adaptive resolution strategy that dynamically balances spatial resolution and clip length to maintain constant token count per video.", "result": "Outperforms uniform sampling by 8.1% on Video-MME, 5.6% on LongVideoBench, and 10.3% on MLVU benchmarks. Training-free approach demonstrates importance of preserving temporal coherence.", "conclusion": "The work highlights the critical importance of temporal coherence in frame selection and provides a practical pathway for scaling Video LLMs to real-world video understanding applications."}}
{"id": "2510.01637", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01637", "abs": "https://arxiv.org/abs/2510.01637", "authors": ["Liyan Xie", "Muhammad Siddeek", "Mohamed Seif", "Andrea J. Goldsmith", "Mengdi Wang"], "title": "Detecting Post-generation Edits to Watermarked LLM Outputs via Combinatorial Watermarking", "comment": null, "summary": "Watermarking has become a key technique for proprietary language models,\nenabling the distinction between AI-generated and human-written text. However,\nin many real-world scenarios, LLM-generated content may undergo post-generation\nedits, such as human revisions or even spoofing attacks, making it critical to\ndetect and localize such modifications. In this work, we introduce a new task:\ndetecting post-generation edits locally made to watermarked LLM outputs. To\nthis end, we propose a combinatorial pattern-based watermarking framework,\nwhich partitions the vocabulary into disjoint subsets and embeds the watermark\nby enforcing a deterministic combinatorial pattern over these subsets during\ngeneration. We accompany the combinatorial watermark with a global statistic\nthat can be used to detect the watermark. Furthermore, we design lightweight\nlocal statistics to flag and localize potential edits. We introduce two\ntask-specific evaluation metrics, Type-I error rate and detection accuracy, and\nevaluate our method on open-source LLMs across a variety of editing scenarios,\ndemonstrating strong empirical performance in edit localization.", "AI": {"tldr": "A combinatorial pattern-based watermarking framework for detecting and localizing post-generation edits in LLM outputs.", "motivation": "Real-world LLM-generated content often undergoes post-generation edits (human revisions or spoofing attacks), making it critical to detect and localize such modifications in watermarked text.", "method": "Partitions vocabulary into disjoint subsets and embeds watermark by enforcing deterministic combinatorial patterns over these subsets during generation, with global statistics for detection and lightweight local statistics for edit localization.", "result": "Strong empirical performance in edit localization across various editing scenarios on open-source LLMs, evaluated using Type-I error rate and detection accuracy metrics.", "conclusion": "The proposed combinatorial watermarking framework effectively addresses the challenge of detecting and localizing post-generation edits in LLM outputs."}}
{"id": "2510.02264", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02264", "abs": "https://arxiv.org/abs/2510.02264", "authors": ["Mario Medrano-Paredes", "Carmen Fern\u00e1ndez-Gonz\u00e1lez", "Francisco-Javier D\u00edaz-Pernas", "Hichem Saoudi", "Javier Gonz\u00e1lez-Alonso", "Mario Mart\u00ednez-Zarzuela"], "title": "Paving the Way Towards Kinematic Assessment Using Monocular Video: A Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose Estimators Against Inertial Sensors in Daily Living Activities", "comment": "All tables, graphs and figures generated can be obtained in the\n  Zenodo repository complementary to this work:\n  https://doi.org/10.5281/zenodo.15088423", "summary": "Advances in machine learning and wearable sensors offer new opportunities for\ncapturing and analyzing human movement outside specialized laboratories.\nAccurate assessment of human movement under real-world conditions is essential\nfor telemedicine, sports science, and rehabilitation. This preclinical\nbenchmark compares monocular video-based 3D human pose estimation models with\ninertial measurement units (IMUs), leveraging the VIDIMU dataset containing a\ntotal of 13 clinically relevant daily activities which were captured using both\ncommodity video cameras and five IMUs. During this initial study only healthy\nsubjects were recorded, so results cannot be generalized to pathological\ncohorts. Joint angles derived from state-of-the-art deep learning frameworks\n(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA\nBodyTrack) were evaluated against joint angles computed from IMU data using\nOpenSim inverse kinematics following the Human3.6M dataset format with 17\nkeypoints. Among them, MotionAGFormer demonstrated superior performance,\nachieving the lowest overall RMSE ($9.27\\deg \\pm 4.80\\deg$) and MAE ($7.86\\deg\n\\pm 4.18\\deg$), as well as the highest Pearson correlation ($0.86 \\pm 0.15$)\nand the highest coefficient of determination $R^{2}$ ($0.67 \\pm 0.28$). The\nresults reveal that both technologies are viable for out-of-the-lab kinematic\nassessment. However, they also highlight key trade-offs between video- and\nsensor-based approaches including costs, accessibility, and precision. This\nstudy clarifies where off-the-shelf video models already provide clinically\npromising kinematics in healthy adults and where they lag behind IMU-based\nestimates while establishing valuable guidelines for researchers and clinicians\nseeking to develop robust, cost-effective, and user-friendly solutions for\ntelehealth and remote patient monitoring.", "AI": {"tldr": "This study compares monocular video-based 3D human pose estimation models with IMU sensors for movement assessment, finding MotionAGFormer performs best among video models, with both technologies showing viability for out-of-lab kinematic analysis.", "motivation": "Accurate assessment of human movement under real-world conditions is essential for telemedicine, sports science, and rehabilitation, requiring comparison of accessible video-based methods against established IMU-based approaches.", "method": "Used VIDIMU dataset with 13 daily activities captured by commodity cameras and 5 IMUs. Compared joint angles from deep learning frameworks (MotionAGFormer, MotionBERT, MMPose, NVIDIA BodyTrack) against IMU data processed through OpenSim inverse kinematics, following Human3.6M format with 17 keypoints.", "result": "MotionAGFormer achieved best performance with lowest RMSE (9.27\u00b0\u00b14.80\u00b0), lowest MAE (7.86\u00b0\u00b14.18\u00b0), highest Pearson correlation (0.86\u00b10.15), and highest R\u00b2 (0.67\u00b10.28). Both video and IMU approaches proved viable for out-of-lab kinematic assessment.", "conclusion": "Both video- and sensor-based approaches are viable for kinematic assessment, with key trade-offs in costs, accessibility, and precision. Study provides guidelines for developing robust, cost-effective telehealth solutions, though results are limited to healthy subjects and cannot generalize to pathological cohorts."}}
{"id": "2510.01643", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01643", "abs": "https://arxiv.org/abs/2510.01643", "authors": ["Maryam Aliakbarpour", "Vladimir Braverman", "Junze Yin", "Haochen Zhang"], "title": "Support Basis: Fast Attention Beyond Bounded Entries", "comment": null, "summary": "The quadratic complexity of softmax attention remains a central bottleneck in\nscaling large language models (LLMs). [Alman and Song, NeurIPS 2023] proposed a\nsub-quadratic attention approximation algorithm, but it works only under the\nrestrictive bounded-entry assumption. Since this assumption rarely holds in\npractice, its applicability to modern LLMs is limited.\n  In this paper, we introduce support-basis decomposition, a new framework for\nefficient attention approximation beyond bounded entries. We empirically\ndemonstrate that the entries of the query and key matrices exhibit sub-Gaussian\nbehavior. Our approach uses this property to split large and small entries,\nenabling exact computation on sparse components and polynomial approximation on\ndense components. We establish rigorous theoretical guarantees, proving a\nsub-quadratic runtime, and extend the method to a multi-threshold setting that\neliminates all distributional assumptions. Furthermore, we provide the first\ntheoretical justification for the empirical success of polynomial attention\n[Kacham, Mirrokni, and Zhong, ICML 2024], showing that softmax attention can be\nclosely approximated by a combination of multiple polynomial attentions with\nsketching.", "AI": {"tldr": "The paper introduces support-basis decomposition, a new framework for efficient attention approximation that overcomes the limitations of previous methods by handling unbounded entries and achieving sub-quadratic runtime.", "motivation": "Current sub-quadratic attention approximation methods like Alman and Song's work only under restrictive bounded-entry assumptions, which rarely hold in practice, limiting their applicability to modern large language models.", "method": "The approach uses support-basis decomposition that leverages the sub-Gaussian behavior of query and key matrices to split large and small entries, enabling exact computation on sparse components and polynomial approximation on dense components.", "result": "The method achieves sub-quadratic runtime with rigorous theoretical guarantees and extends to a multi-threshold setting that eliminates all distributional assumptions. It also provides theoretical justification for polynomial attention methods.", "conclusion": "Support-basis decomposition provides an effective framework for efficient attention approximation that works with unbounded entries and offers theoretical foundations for existing empirical methods."}}
{"id": "2510.02266", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.02266", "abs": "https://arxiv.org/abs/2510.02266", "authors": ["Shiyi Zhang", "Dong Liang", "Yihang Zhou"], "title": "NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes", "comment": null, "summary": "Reconstructing visual information from brain activity via computer vision\ntechnology provides an intuitive understanding of visual neural mechanisms.\nDespite progress in decoding fMRI data with generative models, achieving\naccurate cross-subject reconstruction of visual stimuli remains challenging and\ncomputationally demanding. This difficulty arises from inter-subject\nvariability in neural representations and the brain's abstract encoding of core\nsemantic features in complex visual inputs. To address these challenges, we\npropose NeuroSwift, which integrates complementary adapters via diffusion:\nAutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter\nis trained on Stable Diffusion generated images paired with COCO captions to\nemulate higher visual cortex encoding. For cross-subject generalization, we\npretrain on one subject and then fine-tune only 17 percent of parameters (fully\nconnected layers) for new subjects, while freezing other components. This\nenables state-of-the-art performance with only one hour of training per subject\non lightweight GPUs (three RTX 4090), and it outperforms existing methods.", "AI": {"tldr": "NeuroSwift is a diffusion-based method that integrates AutoKL and CLIP adapters for cross-subject visual stimulus reconstruction from fMRI data, achieving state-of-the-art performance with minimal training.", "motivation": "To address challenges in cross-subject visual reconstruction from brain activity, including inter-subject variability and the brain's abstract semantic encoding of complex visual inputs.", "method": "Integrates complementary adapters via diffusion: AutoKL for low-level features and CLIP for semantics. Uses Stable Diffusion generated images with COCO captions to train CLIP Adapter. Employs pretraining on one subject followed by fine-tuning only 17% of parameters for new subjects.", "result": "Achieves state-of-the-art performance with only one hour of training per subject on lightweight GPUs (three RTX 4090), outperforming existing methods.", "conclusion": "NeuroSwift enables efficient and accurate cross-subject visual reconstruction from fMRI data with minimal computational requirements."}}
{"id": "2510.01649", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01649", "abs": "https://arxiv.org/abs/2510.01649", "authors": ["Muhammad Tanzil Furqon", "Mahardhika Pratama", "Igor \u0160krjanc", "Lin Liu", "Habibullah Habibullah", "Kutluyil Dogancay"], "title": "Source-Free Cross-Domain Continual Learning", "comment": null, "summary": "Although existing cross-domain continual learning approaches successfully\naddress many streaming tasks having domain shifts, they call for a fully\nlabeled source domain hindering their feasibility in the privacy constrained\nenvironments. This paper goes one step ahead with the problem of source-free\ncross-domain continual learning where the use of source-domain samples are\ncompletely prohibited. We propose the idea of rehearsal-free frequency-aware\ndynamic prompt collaborations (REFEREE) to cope with the absence of labeled\nsource-domain samples in realm of cross-domain continual learning. REFEREE is\nbuilt upon a synergy between a source-pre-trained model and a large-scale\nvision-language model, thus overcoming the problem of sub-optimal\ngeneralizations when relying only on a source pre-trained model. The domain\nshift problem between the source domain and the target domain is handled by a\nfrequency-aware prompting technique encouraging low-frequency components while\nsuppressing high-frequency components. This strategy generates frequency-aware\naugmented samples, robust against noisy pseudo labels. The noisy pseudo-label\nproblem is further addressed with the uncertainty-aware weighting strategy\nwhere the mean and covariance matrix are weighted by prediction uncertainties,\nthus mitigating the adverse effects of the noisy pseudo label. Besides, the\nissue of catastrophic forgetting (CF) is overcome by kernel linear discriminant\nanalysis (KLDA) where the backbone network is frozen while the classification\nis performed using the linear discriminant analysis approach guided by the\nrandom kernel method. Our rigorous numerical studies confirm the advantage of\nour approach where it beats prior arts having access to source domain samples\nwith significant margins.", "AI": {"tldr": "REFEREE is a source-free cross-domain continual learning method that uses frequency-aware prompting and uncertainty-aware weighting to handle domain shifts and noisy pseudo labels without accessing source domain samples.", "motivation": "Existing cross-domain continual learning methods require fully labeled source domains, which is impractical in privacy-constrained environments. This paper addresses the challenge of source-free cross-domain continual learning where source-domain samples are completely prohibited.", "method": "REFEREE combines a source-pre-trained model with a vision-language model. It uses frequency-aware prompting to handle domain shifts, uncertainty-aware weighting to mitigate noisy pseudo labels, and kernel linear discriminant analysis (KLDA) to prevent catastrophic forgetting while keeping the backbone frozen.", "result": "The approach significantly outperforms prior methods that have access to source domain samples, demonstrating its effectiveness in source-free cross-domain continual learning scenarios.", "conclusion": "REFEREE successfully addresses source-free cross-domain continual learning by leveraging frequency-aware prompting, uncertainty-aware weighting, and KLDA, achieving superior performance without requiring access to source domain data."}}
{"id": "2510.02270", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02270", "abs": "https://arxiv.org/abs/2510.02270", "authors": ["Sathira Silva", "Eman Ali", "Chetan Arora", "Muhammad Haris Khan"], "title": "microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification", "comment": null, "summary": "Unsupervised adaptation of CLIP-based vision-language models (VLMs) for\nfine-grained image classification requires sensitivity to microscopic local\ncues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse\nglobal features restricts its performance on fine-grained classification tasks.\nPrior efforts inject fine-grained knowledge by aligning large language model\n(LLM) descriptions with the CLIP $\\texttt{[CLS]}$ token; however, this approach\noverlooks spatial precision. We propose $\\textbf{microCLIP}$, a self-training\nframework that jointly refines CLIP's visual and textual representations using\nfine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)\nwithin a lightweight TokenFusion module, which builds a saliency-guided\n$\\texttt{[FG]}$ token from patch embeddings and fuses it with the global\n$\\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we\nintroduce a two-headed LLM-derived classifier: a frozen classifier that, via\nmulti-view alignment, provides a stable text-based prior for pseudo-labeling,\nand a learnable classifier initialized from LLM descriptions and fine-tuned\nwith TokenFusion. We further develop Dynamic Knowledge Aggregation, which\nconvexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to\niteratively refine pseudo-labels. Together, these components uncover latent\nfine-grained signals in CLIP, yielding a consistent $2.90\\%$ average accuracy\ngain across 13 fine-grained benchmarks while requiring only light adaptation.\nOur code is available at https://github.com/sathiiii/microCLIP.", "AI": {"tldr": "microCLIP is a self-training framework that refines CLIP's visual and textual representations for fine-grained image classification using saliency-guided token fusion and LLM-derived classifiers.", "motivation": "CLIP's reliance on coarse global features limits performance on fine-grained classification tasks, and prior approaches overlook spatial precision when aligning LLM descriptions with CLIP.", "method": "Uses Saliency-Oriented Attention Pooling (SOAP) to create saliency-guided fine-grained tokens, fuses them with global CLS tokens, and employs a two-headed LLM-derived classifier with Dynamic Knowledge Aggregation for stable pseudo-labeling.", "result": "Achieves consistent 2.90% average accuracy gain across 13 fine-grained benchmarks while requiring only light adaptation.", "conclusion": "microCLIP effectively uncovers latent fine-grained signals in CLIP through joint refinement of visual and textual representations, enabling improved fine-grained classification performance."}}
{"id": "2510.01650", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01650", "abs": "https://arxiv.org/abs/2510.01650", "authors": ["Kwanhee Lee", "Hyeondo Jang", "Dongyeop Lee", "Dan Alistarh", "Namhoon Lee"], "title": "The Unseen Frontier: Pushing the Limits of LLM Sparsity with Surrogate-Free ADMM", "comment": "Preprint", "summary": "Neural network pruning is a promising technique to mitigate the excessive\ncomputational and memory requirements of large language models (LLMs). Despite\nits promise, however, progress in this area has diminished, as conventional\nmethods are seemingly unable to surpass moderate sparsity levels (50-60%)\nwithout severely degrading model accuracy. This work breaks through the current\nimpasse, presenting a principled and effective method called $\\texttt{Elsa}$,\nwhich achieves extreme sparsity levels of up to 90% while retaining high model\nfidelity. This is done by identifying several limitations in current practice,\nall of which can be traced back to their reliance on a surrogate objective\nformulation. $\\texttt{Elsa}$ tackles this issue directly and effectively via\nstandard and well-established constrained optimization techniques based on\nADMM. Our extensive experiments across a wide range of models and scales show\nthat $\\texttt{Elsa}$ achieves substantial improvements over existing methods;\ne.g., it achieves 7.8$\\times$ less perplexity than the best existing method on\nLLaMA-2-7B at 90% sparsity. Furthermore, we present\n$\\texttt{Elsa}_{\\text{-L}}$, a quantized variant that scales to extremely large\nmodels (27B), and establish its theoretical convergence guarantees. These\nresults highlight meaningful progress in advancing the frontier of LLM\nsparsity, while promising that significant opportunities for further\nadvancement may remain in directions that have so far attracted limited\nexploration.", "AI": {"tldr": "Elsa is a new neural network pruning method that achieves extreme sparsity (up to 90%) in large language models while maintaining high accuracy, breaking through previous limitations of conventional methods.", "motivation": "Current neural network pruning methods for LLMs are limited to moderate sparsity levels (50-60%) due to severe accuracy degradation at higher sparsity, creating an impasse in model compression research.", "method": "Elsa uses constrained optimization techniques based on ADMM to directly address limitations in current surrogate objective formulations, with a quantized variant (Elsa-L) for extremely large models.", "result": "Elsa achieves 7.8\u00d7 less perplexity than the best existing method on LLaMA-2-7B at 90% sparsity, and scales to 27B models while maintaining theoretical convergence guarantees.", "conclusion": "The method represents meaningful progress in LLM sparsity and suggests significant opportunities remain in underexplored directions for further advancement."}}
{"id": "2510.02282", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02282", "abs": "https://arxiv.org/abs/2510.02282", "authors": ["Kyoungjun Park", "Yifan Yang", "Juheon Yi", "Shicheng Zheng", "Yifei Shen", "Dongqi Han", "Caihua Shan", "Muhammad Muaz", "Lili Qiu"], "title": "VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL", "comment": null, "summary": "With the rapid advancement of AI-generated videos, there is an urgent need\nfor effective detection tools to mitigate societal risks such as misinformation\nand reputational harm. In addition to accurate classification, it is essential\nthat detection models provide interpretable explanations to ensure transparency\nfor regulators and end users. To address these challenges, we introduce\nVidGuard-R1, the first video authenticity detector that fine-tunes a\nmulti-modal large language model (MLLM) using group relative policy\noptimization (GRPO). Our model delivers both highly accurate judgments and\ninsightful reasoning. We curate a challenging dataset of 140k real and\nAI-generated videos produced by state-of-the-art generation models, carefully\ndesigning the generation process to maximize discrimination difficulty. We then\nfine-tune Qwen-VL using GRPO with two specialized reward models that target\ntemporal artifacts and generation complexity. Extensive experiments demonstrate\nthat VidGuard-R1 achieves state-of-the-art zero-shot performance on existing\nbenchmarks, with additional training pushing accuracy above 95%. Case studies\nfurther show that VidGuard-R1 produces precise and interpretable rationales\nbehind its predictions. The code is publicly available at\nhttps://VidGuard-R1.github.io.", "AI": {"tldr": "VidGuard-R1 is the first video authenticity detector that fine-tunes a multi-modal large language model using group relative policy optimization to provide both accurate AI-generated video detection and interpretable explanations.", "motivation": "Address the urgent need for effective AI-generated video detection tools to mitigate societal risks like misinformation and reputational harm, while ensuring transparency through interpretable explanations for regulators and end users.", "method": "Fine-tunes Qwen-VL using group relative policy optimization (GRPO) with two specialized reward models targeting temporal artifacts and generation complexity, trained on a challenging dataset of 140k real and AI-generated videos.", "result": "Achieves state-of-the-art zero-shot performance on existing benchmarks, with additional training pushing accuracy above 95%. Produces precise and interpretable rationales behind predictions.", "conclusion": "VidGuard-R1 successfully addresses the dual challenge of accurate AI-generated video detection and interpretable reasoning, demonstrating superior performance through MLLM fine-tuning with GRPO."}}
{"id": "2510.01656", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01656", "abs": "https://arxiv.org/abs/2510.01656", "authors": ["Jiashun Liu", "Johan Obando-Ceron", "Han Lu", "Yancheng He", "Weixun Wang", "Wenbo Su", "Bo Zheng", "Pablo Samuel Castro", "Aaron Courville", "Ling Pan"], "title": "Asymmetric Proximal Policy Optimization: mini-critics boost LLM reasoning", "comment": null, "summary": "Most recent RL for LLMs (RL4LLM) methods avoid explicit critics, replacing\nthem with average advantage baselines. This shift is largely pragmatic:\nconventional value functions are computationally expensive to train at LLM\nscale and often fail under sparse rewards and long reasoning horizons. We\nrevisit this bottleneck from an architectural perspective and introduce\nAsymmetric Proximal Policy Optimization (AsyPPO), a simple and scalable\nframework that restores the critics role while remaining efficient in\nlarge-model settings. AsyPPO employs a set of lightweight mini-critics, each\ntrained on disjoint prompt shards. This design encourages diversity while\npreserving calibration, reducing value-estimation bias. Beyond robust\nestimation, AsyPPO leverages inter-critic uncertainty to refine the policy\nupdate: (i) masking advantages in states where critics agree and gradients add\nlittle learning signal, and (ii) filtering high-divergence states from entropy\nregularization, suppressing spurious exploration. After training on open-source\ndata with only 5,000 samples, AsyPPO consistently improves learning stability\nand performance across multiple benchmarks over strong baselines, such as GRPO,\nachieving performance gains of more than six percent on Qwen3-4b-Base and about\nthree percent on Qwen3-8b-Base and Qwen3-14b-Base over classic PPO, without\nadditional tricks. These results highlight the importance of architectural\ninnovations for scalable, efficient algorithms.", "AI": {"tldr": "AsyPPO introduces lightweight mini-critics trained on disjoint prompt shards to restore critics in RL for LLMs, improving learning stability and performance over baselines like GRPO and PPO.", "motivation": "Conventional value functions are computationally expensive at LLM scale and fail under sparse rewards and long reasoning horizons, leading recent RL4LLM methods to avoid explicit critics.", "method": "Asymmetric Proximal Policy Optimization (AsyPPO) uses a set of lightweight mini-critics trained on disjoint prompt shards, leveraging inter-critic uncertainty to mask advantages and filter high-divergence states.", "result": "AsyPPO improves learning stability and performance across multiple benchmarks, achieving gains of over 6% on Qwen3-4b-Base and about 3% on Qwen3-8b-Base and Qwen3-14b-Base over classic PPO with only 5,000 training samples.", "conclusion": "Architectural innovations like AsyPPO are crucial for scalable, efficient RL algorithms in LLM settings, demonstrating the importance of restoring critics while maintaining efficiency."}}
{"id": "2510.02283", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02283", "abs": "https://arxiv.org/abs/2510.02283", "authors": ["Justin Cui", "Jie Wu", "Ming Li", "Tao Yang", "Xiaojie Li", "Rui Wang", "Andrew Bai", "Yuanhao Ban", "Cho-Jui Hsieh"], "title": "Self-Forcing++: Towards Minute-Scale High-Quality Video Generation", "comment": "preprint", "summary": "Diffusion models have revolutionized image and video generation, achieving\nunprecedented visual quality. However, their reliance on transformer\narchitectures incurs prohibitively high computational costs, particularly when\nextending generation to long videos. Recent work has explored autoregressive\nformulations for long video generation, typically by distilling from\nshort-horizon bidirectional teachers. Nevertheless, given that teacher models\ncannot synthesize long videos, the extrapolation of student models beyond their\ntraining horizon often leads to pronounced quality degradation, arising from\nthe compounding of errors within the continuous latent space. In this paper, we\npropose a simple yet effective approach to mitigate quality degradation in\nlong-horizon video generation without requiring supervision from long-video\nteachers or retraining on long video datasets. Our approach centers on\nexploiting the rich knowledge of teacher models to provide guidance for the\nstudent model through sampled segments drawn from self-generated long videos.\nOur method maintains temporal consistency while scaling video length by up to\n20x beyond teacher's capability, avoiding common issues such as over-exposure\nand error-accumulation without recomputing overlapping frames like previous\nmethods. When scaling up the computation, our method shows the capability of\ngenerating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the\nmaximum span supported by our base model's position embedding and more than 50x\nlonger than that of our baseline model. Experiments on standard benchmarks and\nour proposed improved benchmark demonstrate that our approach substantially\noutperforms baseline methods in both fidelity and consistency. Our long-horizon\nvideos demo can be found at https://self-forcing-plus-plus.github.io/", "AI": {"tldr": "Proposes a method to mitigate quality degradation in long-horizon video generation by using teacher models to guide student models through self-generated long videos, achieving up to 20x longer videos without retraining.", "motivation": "Diffusion models for video generation suffer from high computational costs and quality degradation when extending to long videos due to error compounding in continuous latent space.", "method": "Uses teacher models to provide guidance for student models through sampled segments from self-generated long videos, maintaining temporal consistency without recomputing overlapping frames.", "result": "Generates videos up to 4 minutes 15 seconds (50x longer than baseline), avoids over-exposure and error-accumulation, and outperforms baselines in fidelity and consistency on benchmarks.", "conclusion": "The approach effectively scales video generation length while maintaining quality, demonstrating significant improvements over existing methods without requiring long-video supervision."}}
{"id": "2510.01658", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01658", "abs": "https://arxiv.org/abs/2510.01658", "authors": ["Amin Jalali", "Milad Soltany", "Michael Greenspan", "Ali Etemad"], "title": "Learning Time-Series Representations by Hierarchical Uniformity-Tolerance Latent Balancing", "comment": "Accepted in Transactions on Machine Learning Research", "summary": "We propose TimeHUT, a novel method for learning time-series representations\nby hierarchical uniformity-tolerance balancing of contrastive representations.\nOur method uses two distinct losses to learn strong representations with the\naim of striking an effective balance between uniformity and tolerance in the\nembedding space. First, TimeHUT uses a hierarchical setup to learn both\ninstance-wise and temporal information from input time-series. Next, we\nintegrate a temperature scheduler within the vanilla contrastive loss to\nbalance the uniformity and tolerance characteristics of the embeddings.\nAdditionally, a hierarchical angular margin loss enforces instance-wise and\ntemporal contrast losses, creating geometric margins between positive and\nnegative pairs of temporal sequences. This approach improves the coherence of\npositive pairs and their separation from the negatives, enhancing the capture\nof temporal dependencies within a time-series sample. We evaluate our approach\non a wide range of tasks, namely 128 UCR and 30 UAE datasets for univariate and\nmultivariate classification, as well as Yahoo and KPI datasets for anomaly\ndetection. The results demonstrate that TimeHUT outperforms prior methods by\nconsiderable margins on classification, while obtaining competitive results for\nanomaly detection. Finally, detailed sensitivity and ablation studies are\nperformed to evaluate different components and hyperparameters of our method.", "AI": {"tldr": "TimeHUT learns time-series representations through hierarchical uniformity-tolerance balancing using contrastive learning with temperature scheduling and angular margin losses.", "motivation": "To create strong time-series representations by effectively balancing uniformity and tolerance in embedding space while capturing both instance-wise and temporal information.", "method": "Uses hierarchical contrastive learning with temperature scheduler and angular margin loss to enforce geometric margins between positive/negative pairs, improving temporal dependency capture.", "result": "Outperforms prior methods on 128 UCR and 30 UAE datasets for classification, achieves competitive results on Yahoo and KPI datasets for anomaly detection.", "conclusion": "TimeHUT effectively balances uniformity and tolerance in time-series representations, demonstrating superior classification performance and competitive anomaly detection capabilities."}}
{"id": "2510.02284", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02284", "abs": "https://arxiv.org/abs/2510.02284", "authors": ["David Romero", "Ariana Bermudez", "Hao Li", "Fabio Pizzati", "Ivan Laptev"], "title": "Learning to Generate Object Interactions with Physics-Guided Video Diffusion", "comment": null, "summary": "Recent models for video generation have achieved remarkable progress and are\nnow deployed in film, social media production, and advertising. Beyond their\ncreative potential, such models also hold promise as world simulators for\nrobotics and embodied decision making. Despite strong advances, however,\ncurrent approaches still struggle to generate physically plausible object\ninteractions and lack physics-grounded control mechanisms. To address this\nlimitation, we introduce KineMask, an approach for physics-guided video\ngeneration that enables realistic rigid body control, interactions, and\neffects. Given a single image and a specified object velocity, our method\ngenerates videos with inferred motions and future object interactions. We\npropose a two-stage training strategy that gradually removes future motion\nsupervision via object masks. Using this strategy we train video diffusion\nmodels (VDMs) on synthetic scenes of simple interactions and demonstrate\nsignificant improvements of object interactions in real scenes. Furthermore,\nKineMask integrates low-level motion control with high-level textual\nconditioning via predictive scene descriptions, leading to effective support\nfor synthesis of complex dynamical phenomena. Extensive experiments show that\nKineMask achieves strong improvements over recent models of comparable size.\nAblation studies further highlight the complementary roles of low- and\nhigh-level conditioning in VDMs. Our code, model, and data will be made\npublicly available.", "AI": {"tldr": "KineMask is a physics-guided video generation approach that enables realistic rigid body control and interactions using a two-stage training strategy with video diffusion models.", "motivation": "Current video generation models struggle with physically plausible object interactions and lack physics-grounded control mechanisms, limiting their use as world simulators for robotics and embodied decision making.", "method": "Two-stage training strategy that gradually removes future motion supervision via object masks, training video diffusion models on synthetic scenes and integrating low-level motion control with high-level textual conditioning.", "result": "Significant improvements in object interactions in real scenes, strong improvements over recent models of comparable size, and effective synthesis of complex dynamical phenomena.", "conclusion": "KineMask demonstrates the complementary roles of low- and high-level conditioning in video diffusion models, achieving realistic physics-guided video generation with improved object interactions."}}
{"id": "2510.01663", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01663", "abs": "https://arxiv.org/abs/2510.01663", "authors": ["Wangxuan Fan", "Ching Wang", "Siqi Li", "Nan Liu"], "title": "Shift-Invariant Attribute Scoring for Kolmogorov-Arnold Networks via Shapley Value", "comment": "15 pages, 6 figures, 9 tables", "summary": "For many real-world applications, understanding feature-outcome relationships\nis as crucial as achieving high predictive accuracy. While traditional neural\nnetworks excel at prediction, their black-box nature obscures underlying\nfunctional relationships. Kolmogorov--Arnold Networks (KANs) address this by\nemploying learnable spline-based activation functions on edges, enabling\nrecovery of symbolic representations while maintaining competitive performance.\nHowever, KAN's architecture presents unique challenges for network pruning.\nConventional magnitude-based methods become unreliable due to sensitivity to\ninput coordinate shifts. We propose \\textbf{ShapKAN}, a pruning framework using\nShapley value attribution to assess node importance in a shift-invariant\nmanner. Unlike magnitude-based approaches, ShapKAN quantifies each node's\nactual contribution, ensuring consistent importance rankings regardless of\ninput parameterization. Extensive experiments on synthetic and real-world\ndatasets demonstrate that ShapKAN preserves true node importance while enabling\neffective network compression. Our approach improves KAN's interpretability\nadvantages, facilitating deployment in resource-constrained environments.", "AI": {"tldr": "ShapKAN is a pruning framework for Kolmogorov-Arnold Networks (KANs) that uses Shapley values to assess node importance in a shift-invariant manner, addressing the limitations of traditional magnitude-based pruning methods.", "motivation": "KANs offer interpretability advantages over traditional neural networks but face unique pruning challenges due to sensitivity to input coordinate shifts, making conventional magnitude-based pruning unreliable.", "method": "The proposed ShapKAN framework uses Shapley value attribution to quantify each node's actual contribution to the network output, ensuring consistent importance rankings regardless of input parameterization.", "result": "Extensive experiments on synthetic and real-world datasets show that ShapKAN preserves true node importance while enabling effective network compression.", "conclusion": "ShapKAN improves KAN's interpretability advantages and facilitates deployment in resource-constrained environments by providing a reliable pruning method."}}
{"id": "2510.02287", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02287", "abs": "https://arxiv.org/abs/2510.02287", "authors": ["Yichen Li", "Antonio Torralba"], "title": "MultiModal Action Conditioned Video Generation", "comment": null, "summary": "Current video models fail as world model as they lack fine-graiend control.\nGeneral-purpose household robots require real-time fine motor control to handle\ndelicate tasks and urgent situations. In this work, we introduce fine-grained\nmultimodal actions to capture such precise control. We consider senses of\nproprioception, kinesthesia, force haptics, and muscle activation. Such\nmultimodal senses naturally enables fine-grained interactions that are\ndifficult to simulate with text-conditioned generative models. To effectively\nsimulate fine-grained multisensory actions, we develop a feature learning\nparadigm that aligns these modalities while preserving the unique information\neach modality provides. We further propose a regularization scheme to enhance\ncausality of the action trajectory features in representing intricate\ninteraction dynamics. Experiments show that incorporating multimodal senses\nimproves simulation accuracy and reduces temporal drift. Extensive ablation\nstudies and downstream applications demonstrate the effectiveness and\npracticality of our work.", "AI": {"tldr": "The paper introduces fine-grained multimodal actions for household robots, incorporating proprioception, kinesthesia, force haptics, and muscle activation to enable precise control that text-conditioned models struggle with.", "motivation": "Current video models fail as world models due to lack of fine-grained control needed for delicate tasks and urgent situations in household robotics.", "method": "Developed a feature learning paradigm that aligns multimodal senses while preserving unique information from each modality, with a regularization scheme to enhance causality of action trajectory features.", "result": "Experiments show incorporating multimodal senses improves simulation accuracy and reduces temporal drift, with ablation studies confirming effectiveness.", "conclusion": "The proposed multimodal approach enables fine-grained interactions that are difficult for text-conditioned generative models, demonstrating practical effectiveness for household robotics."}}
{"id": "2510.01677", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01677", "abs": "https://arxiv.org/abs/2510.01677", "authors": ["Han Wu", "Yanming Sun", "Yunhe Yang", "Derek F. Wong"], "title": "Beyond Simple Fusion: Adaptive Gated Fusion for Robust Multimodal Sentiment Analysis", "comment": null, "summary": "Multimodal sentiment analysis (MSA) leverages information fusion from diverse\nmodalities (e.g., text, audio, visual) to enhance sentiment prediction.\nHowever, simple fusion techniques often fail to account for variations in\nmodality quality, such as those that are noisy, missing, or semantically\nconflicting. This oversight leads to suboptimal performance, especially in\ndiscerning subtle emotional nuances. To mitigate this limitation, we introduce\na simple yet efficient \\textbf{A}daptive \\textbf{G}ated \\textbf{F}usion\n\\textbf{N}etwork that adaptively adjusts feature weights via a dual gate fusion\nmechanism based on information entropy and modality importance. This mechanism\nmitigates the influence of noisy modalities and prioritizes informative cues\nfollowing unimodal encoding and cross-modal interaction. Experiments on\nCMU-MOSI and CMU-MOSEI show that AGFN significantly outperforms strong\nbaselines in accuracy, effectively discerning subtle emotions with robust\nperformance. Visualization analysis of feature representations demonstrates\nthat AGFN enhances generalization by learning from a broader feature\ndistribution, achieved by reducing the correlation between feature location and\nprediction error, thereby decreasing reliance on specific locations and\ncreating more robust multimodal feature representations.", "AI": {"tldr": "The paper introduces AGFN, an Adaptive Gated Fusion Network for multimodal sentiment analysis that adaptively weights modalities using entropy and importance measures to handle noisy or conflicting inputs.", "motivation": "Simple fusion techniques in multimodal sentiment analysis fail to account for variations in modality quality (noisy, missing, conflicting), leading to suboptimal performance in discerning subtle emotional nuances.", "method": "Proposes a dual gate fusion mechanism based on information entropy and modality importance that adaptively adjusts feature weights, mitigating noisy modalities and prioritizing informative cues after unimodal encoding and cross-modal interaction.", "result": "AGFN significantly outperforms strong baselines on CMU-MOSI and CMU-MOSEI datasets in accuracy, effectively discerning subtle emotions with robust performance. Visualization shows enhanced generalization through broader feature distribution learning.", "conclusion": "AGFN creates more robust multimodal feature representations by reducing correlation between feature location and prediction error, decreasing reliance on specific locations and improving generalization."}}
{"id": "2510.02295", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02295", "abs": "https://arxiv.org/abs/2510.02295", "authors": ["Enxin Song", "Wenhao Chai", "Shusheng Yang", "Ethan Armand", "Xiaojun Shan", "Haiyang Xu", "Jianwen Xie", "Zhuowen Tu"], "title": "VideoNSA: Native Sparse Attention Scales Video Understanding", "comment": "Project Page: https://enxinsong.com/VideoNSA-web/, Code:\n  https://github.com/Espere-1119-Song/VideoNSA", "summary": "Video understanding in multimodal language models remains limited by context\nlength: models often miss key transition frames and struggle to maintain\ncoherence across long time scales. To address this, we adapt Native Sparse\nAttention (NSA) to video-language models. Our method, VideoNSA, adapts\nQwen2.5-VL through end-to-end training on a 216K video instruction dataset. We\nemploy a hardware-aware hybrid approach to attention, preserving dense\nattention for text, while employing NSA for video. Compared to\ntoken-compression and training-free sparse baselines, VideoNSA achieves\nimproved performance on long-video understanding, temporal reasoning, and\nspatial benchmarks. Further ablation analysis reveals four key findings: (1)\nreliable scaling to 128K tokens; (2) an optimal global-local attention\nallocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)\nthe learnable combined sparse attention help induce dynamic attention sinks.", "AI": {"tldr": "VideoNSA adapts Native Sparse Attention to video-language models, enabling reliable scaling to 128K tokens and improved performance on long-video understanding tasks through a hardware-aware hybrid attention approach.", "motivation": "Video understanding in multimodal models is limited by context length, causing models to miss key transition frames and struggle with coherence across long time scales.", "method": "Adapt Native Sparse Attention (NSA) to video-language models by adapting Qwen2.5-VL through end-to-end training on a 216K video instruction dataset, using hardware-aware hybrid attention (dense for text, NSA for video).", "result": "Achieves improved performance on long-video understanding, temporal reasoning, and spatial benchmarks compared to token-compression and training-free sparse baselines.", "conclusion": "Key findings include reliable scaling to 128K tokens, optimal global-local attention allocation, task-dependent branch usage patterns, and learnable combined sparse attention inducing dynamic attention sinks."}}
{"id": "2510.01693", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01693", "abs": "https://arxiv.org/abs/2510.01693", "authors": ["Juncheng Dong", "Weibin Mo", "Zhengling Qi", "Cong Shi", "Ethan X. Fang", "Vahid Tarokh"], "title": "PASTA: A Unified Framework for Offline Assortment Learning", "comment": null, "summary": "We study a broad class of assortment optimization problems in an offline and\ndata-driven setting. In such problems, a firm lacks prior knowledge of the\nunderlying choice model, and aims to determine an optimal assortment based on\nhistorical customer choice data. The combinatorial nature of assortment\noptimization often results in insufficient data coverage, posing a significant\nchallenge in designing provably effective solutions. To address this, we\nintroduce a novel Pessimistic Assortment Optimization (PASTA) framework that\nleverages the principle of pessimism to achieve optimal expected revenue under\ngeneral choice models. Notably, PASTA requires only that the offline data\ndistribution contains an optimal assortment, rather than providing the full\ncoverage of all feasible assortments. Theoretically, we establish the first\nfinite-sample regret bounds for offline assortment optimization across several\nwidely used choice models, including the multinomial logit and nested logit\nmodels. Additionally, we derive a minimax regret lower bound, proving that\nPASTA is minimax optimal in terms of sample and model complexity. Numerical\nexperiments further demonstrate that our method outperforms existing baseline\napproaches.", "AI": {"tldr": "PASTA is a pessimistic assortment optimization framework for data-driven settings that achieves optimal revenue under general choice models without requiring full data coverage of all assortments.", "motivation": "Firms lack prior knowledge of choice models and need to optimize assortments from historical data, but combinatorial nature causes insufficient data coverage, making effective solutions challenging.", "method": "Proposed Pessimistic Assortment Optimization (PASTA) framework that leverages pessimism principle, requiring only that offline data contains an optimal assortment rather than full coverage.", "result": "Established first finite-sample regret bounds for offline assortment optimization across MNL and nested logit models, proved minimax optimality, and showed superior performance over baselines in experiments.", "conclusion": "PASTA provides a provably effective solution for offline assortment optimization with minimal data requirements and achieves minimax optimal performance across various choice models."}}
{"id": "2510.02307", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02307", "abs": "https://arxiv.org/abs/2510.02307", "authors": ["Ruozhen He", "Moayed Haji-Ali", "Ziyan Yang", "Vicente Ordonez"], "title": "NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation", "comment": null, "summary": "Text-to-image diffusion models trained on a fixed set of resolutions often\nfail to generalize, even when asked to generate images at lower resolutions\nthan those seen during training. High-resolution text-to-image generators are\ncurrently unable to easily offer an out-of-the-box budget-efficient alternative\nto their users who might not need high-resolution images. We identify a key\ntechnical insight in diffusion models that when addressed can help tackle this\nlimitation: Noise schedulers have unequal perceptual effects across\nresolutions. The same level of noise removes disproportionately more signal\nfrom lower-resolution images than from high-resolution images, leading to a\ntrain-test mismatch. We propose NoiseShift, a training-free method that\nrecalibrates the noise level of the denoiser conditioned on resolution size.\nNoiseShift requires no changes to model architecture or sampling schedule and\nis compatible with existing models. When applied to Stable Diffusion 3, Stable\nDiffusion 3.5, and Flux-Dev, quality at low resolutions is significantly\nimproved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and\nFlux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by\n10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results\ndemonstrate the effectiveness of NoiseShift in mitigating resolution-dependent\nartifacts and enhancing the quality of low-resolution image generation.", "AI": {"tldr": "NoiseShift is a training-free method that recalibrates noise levels for diffusion models based on resolution size, significantly improving low-resolution image generation quality without architectural changes.", "motivation": "Text-to-image diffusion models trained on fixed resolutions fail to generalize to lower resolutions, preventing budget-efficient alternatives for users who don't need high-resolution images.", "method": "Identifies that noise schedulers have unequal perceptual effects across resolutions - same noise level removes disproportionately more signal from lower-resolution images. Proposes NoiseShift to recalibrate denoiser noise level conditioned on resolution size.", "result": "Significantly improves quality at low resolutions: On LAION-COCO, improves SD3.5 by 15.89%, SD3 by 8.56%, Flux-Dev by 2.44% in FID. On CelebA, improves SD3.5 by 10.36%, SD3 by 5.19%, Flux-Dev by 3.02% in FID.", "conclusion": "NoiseShift effectively mitigates resolution-dependent artifacts and enhances low-resolution image generation quality, requiring no model changes and being compatible with existing models."}}
{"id": "2510.01706", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01706", "abs": "https://arxiv.org/abs/2510.01706", "authors": ["Shaan Shah", "Meenakshi Khosla"], "title": "Representational Alignment Across Model Layers and Brain Regions with Hierarchical Optimal Transport", "comment": null, "summary": "Standard representational similarity methods align each layer of a network to\nits best match in another independently, producing asymmetric results, lacking\na global alignment score, and struggling with networks of different depths.\nThese limitations arise from ignoring global activation structure and\nrestricting mappings to rigid one-to-one layer correspondences. We propose\nHierarchical Optimal Transport (HOT), a unified framework that jointly infers\nsoft, globally consistent layer-to-layer couplings and neuron-level transport\nplans. HOT allows source neurons to distribute mass across multiple target\nlayers while minimizing total transport cost under marginal constraints. This\nyields both a single alignment score for the entire network comparison and a\nsoft transport plan that naturally handles depth mismatches through mass\ndistribution. We evaluate HOT on vision models, large language models, and\nhuman visual cortex recordings. Across all domains, HOT matches or surpasses\nstandard pairwise matching in alignment quality. Moreover, it reveals smooth,\nfine-grained hierarchical correspondences: early layers map to early layers,\ndeeper layers maintain relative positions, and depth mismatches are resolved by\ndistributing representations across multiple layers. These structured patterns\nemerge naturally from global optimization without being imposed, yet are absent\nin greedy layer-wise methods. HOT thus enables richer, more interpretable\ncomparisons between representations, particularly when networks differ in\narchitecture or depth.", "AI": {"tldr": "HOT is a unified framework that uses hierarchical optimal transport to compare neural networks, providing global alignment scores and handling depth mismatches through soft layer-to-layer couplings.", "motivation": "Standard representational similarity methods have limitations: they produce asymmetric results, lack global alignment scores, and struggle with networks of different depths due to ignoring global activation structure and restricting to rigid one-to-one layer correspondences.", "method": "Hierarchical Optimal Transport (HOT) jointly infers soft, globally consistent layer-to-layer couplings and neuron-level transport plans. It allows source neurons to distribute mass across multiple target layers while minimizing total transport cost under marginal constraints.", "result": "HOT matches or surpasses standard pairwise matching in alignment quality across vision models, large language models, and human visual cortex recordings. It reveals smooth, fine-grained hierarchical correspondences and naturally handles depth mismatches through mass distribution.", "conclusion": "HOT enables richer, more interpretable comparisons between representations, particularly when networks differ in architecture or depth, by providing structured patterns that emerge naturally from global optimization."}}
{"id": "2510.02311", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02311", "abs": "https://arxiv.org/abs/2510.02311", "authors": ["Guanqi Zhan", "Xianzheng Ma", "Weidi Xie", "Andrew Zisserman"], "title": "Inferring Dynamic Physical Properties from Video Foundation Models", "comment": null, "summary": "We study the task of predicting dynamic physical properties from videos. More\nspecifically, we consider physical properties that require temporal information\nto be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,\nand dynamic friction of an object sliding on a surface. To this end, we make\nthe following contributions: (i) We collect a new video dataset for each\nphysical property, consisting of synthetic training and testing splits, as well\nas a real split for real world evaluation. (ii) We explore three ways to infer\nthe physical property from videos: (a) an oracle method where we supply the\nvisual cues that intrinsically reflect the property using classical computer\nvision techniques; (b) a simple read out mechanism using a visual prompt and\ntrainable prompt vector for cross-attention on pre-trained video generative and\nself-supervised models; and (c) prompt strategies for Multi-modal Large\nLanguage Models (MLLMs). (iii) We show that video foundation models trained in\na generative or self-supervised manner achieve a similar performance, though\nbehind that of the oracle, and MLLMs are currently inferior to the other\nmodels, though their performance can be improved through suitable prompting.", "AI": {"tldr": "The paper presents methods for predicting dynamic physical properties (elasticity, viscosity, friction) from videos using video foundation models and MLLMs, with performance analysis across different approaches.", "motivation": "To develop methods for inferring temporal-dependent physical properties from videos, addressing the challenge of predicting dynamic physical characteristics that require temporal information.", "method": "Collected new video datasets for three physical properties; explored three inference approaches: oracle method using classical CV, visual prompt mechanism with pre-trained video models, and prompt strategies for MLLMs.", "result": "Video foundation models achieved similar performance to each other but behind the oracle method; MLLMs performed worse but could be improved with proper prompting.", "conclusion": "Video foundation models show promise for physical property prediction, while MLLMs need further development but can benefit from appropriate prompting strategies."}}
{"id": "2510.01712", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01712", "abs": "https://arxiv.org/abs/2510.01712", "authors": ["Aidan Acquah", "Shing Chan", "Aiden Doherty"], "title": "ActiNet: Activity intensity classification of wrist-worn accelerometers using self-supervised deep learning", "comment": null, "summary": "The use of reliable and accurate human activity recognition (HAR) models on\npassively collected wrist-accelerometer data is essential in large-scale\nepidemiological studies that investigate the association between physical\nactivity and health outcomes. While the use of self-supervised learning has\ngenerated considerable excitement in improving HAR, it remains unknown the\nextent to which these models, coupled with hidden Markov models (HMMs), would\nmake a tangible improvement to classification performance, and the effect this\nmay have on the predicted daily activity intensity compositions. Using 151\nCAPTURE-24 participants' data, we trained the ActiNet model, a self-supervised,\n18-layer, modified ResNet-V2 model, followed by hidden Markov model (HMM)\nsmoothing to classify labels of activity intensity. The performance of this\nmodel, evaluated using 5-fold stratified group cross-validation, was then\ncompared to a baseline random forest (RF) + HMM, established in existing\nliterature. Differences in performance and classification outputs were compared\nwith different subgroups of age and sex within the Capture-24 population. The\nActiNet model was able to distinguish labels of activity intensity with a mean\nmacro F1 score of 0.82, and mean Cohen's kappa score of 0.86. This exceeded the\nperformance of the RF + HMM, trained and validated on the same dataset, with\nmean scores of 0.77 and 0.81, respectively. These findings were consistent\nacross subgroups of age and sex. These findings encourage the use of ActiNet\nfor the extraction of activity intensity labels from wrist-accelerometer data\nin future epidemiological studies.", "AI": {"tldr": "ActiNet (self-supervised ResNet-V2 + HMM) outperforms baseline RF+HMM for activity intensity classification from wrist-accelerometer data, achieving F1=0.82 vs 0.77.", "motivation": "Need reliable HAR models for large-scale epidemiological studies linking physical activity to health outcomes using passively collected wrist-accelerometer data.", "method": "Trained ActiNet (18-layer modified ResNet-V2) with HMM smoothing on 151 CAPTURE-24 participants' data, evaluated using 5-fold stratified group cross-validation and compared to baseline RF+HMM.", "result": "ActiNet achieved mean macro F1=0.82 and Cohen's kappa=0.86, outperforming RF+HMM (F1=0.77, kappa=0.81). Consistent performance across age and sex subgroups.", "conclusion": "ActiNet is recommended for extracting activity intensity labels from wrist-accelerometer data in future epidemiological studies."}}
{"id": "2510.02313", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02313", "abs": "https://arxiv.org/abs/2510.02313", "authors": ["Mengyu Yang", "Yiming Chen", "Haozheng Pei", "Siddhant Agarwal", "Arun Balajee Vasudevan", "James Hays"], "title": "Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions", "comment": "ICCV 2025. Project page: https://clink-chop-thud.github.io/", "summary": "Can a model distinguish between the sound of a spoon hitting a hardwood floor\nversus a carpeted one? Everyday object interactions produce sounds unique to\nthe objects involved. We introduce the sounding object detection task to\nevaluate a model's ability to link these sounds to the objects directly\ninvolved. Inspired by human perception, our multimodal object-aware framework\nlearns from in-the-wild egocentric videos. To encourage an object-centric\napproach, we first develop an automatic pipeline to compute segmentation masks\nof the objects involved to guide the model's focus during training towards the\nmost informative regions of the interaction. A slot attention visual encoder is\nused to further enforce an object prior. We demonstrate state of the art\nperformance on our new task along with existing multimodal action understanding\ntasks.", "AI": {"tldr": "The paper introduces sounding object detection, a task to link sounds to objects involved in interactions, using a multimodal object-aware framework trained on egocentric videos with automatic segmentation masks and slot attention.", "motivation": "To evaluate if models can distinguish sounds from everyday object interactions and link them to the specific objects involved, inspired by human perception.", "method": "Multimodal object-aware framework using in-the-wild egocentric videos, automatic pipeline for computing object segmentation masks, and slot attention visual encoder to enforce object-centric learning.", "result": "Achieves state-of-the-art performance on the new sounding object detection task and existing multimodal action understanding tasks.", "conclusion": "The proposed framework successfully links sounds to objects in interactions and advances multimodal understanding capabilities."}}
{"id": "2510.01717", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01717", "abs": "https://arxiv.org/abs/2510.01717", "authors": ["Shaba Shaon", "Dinh C. Nguyen"], "title": "Latency-aware Multimodal Federated Learning over UAV Networks", "comment": "Accepted at IEEE Transactions on Network Science and Engineering", "summary": "This paper investigates federated multimodal learning (FML) assisted by\nunmanned aerial vehicles (UAVs) with a focus on minimizing system latency and\nproviding convergence analysis. In this framework, UAVs are distributed\nthroughout the network to collect data, participate in model training, and\ncollaborate with a base station (BS) to build a global model. By utilizing\nmultimodal sensing, the UAVs overcome the limitations of unimodal systems,\nenhancing model accuracy, generalization, and offering a more comprehensive\nunderstanding of the environment. The primary objective is to optimize FML\nsystem latency in UAV networks by jointly addressing UAV sensing scheduling,\npower control, trajectory planning, resource allocation, and BS resource\nmanagement. To address the computational complexity of our latency minimization\nproblem, we propose an efficient iterative optimization algorithm combining\nblock coordinate descent and successive convex approximation techniques, which\nprovides high-quality approximate solutions. We also present a theoretical\nconvergence analysis for the UAV-assisted FML framework under a non-convex loss\nfunction. Numerical experiments demonstrate that our FML framework outperforms\nexisting approaches in terms of system latency and model training performance\nunder different data settings.", "AI": {"tldr": "This paper proposes a UAV-assisted federated multimodal learning framework that optimizes system latency through joint optimization of sensing scheduling, power control, trajectory planning, and resource allocation, with theoretical convergence analysis.", "motivation": "To overcome limitations of unimodal systems and enhance model accuracy and generalization in federated learning by utilizing multimodal sensing through UAV networks while minimizing system latency.", "method": "Proposes an efficient iterative optimization algorithm combining block coordinate descent and successive convex approximation techniques to solve the complex latency minimization problem, with joint optimization of UAV sensing scheduling, power control, trajectory planning, resource allocation, and BS resource management.", "result": "Numerical experiments show the proposed FML framework outperforms existing approaches in both system latency and model training performance across different data settings.", "conclusion": "The UAV-assisted FML framework successfully minimizes system latency while providing theoretical convergence guarantees and improved model performance through multimodal sensing and comprehensive optimization strategies."}}
{"id": "2510.02314", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02314", "abs": "https://arxiv.org/abs/2510.02314", "authors": ["Bo-Hsu Ke", "You-Zhe Xie", "Yu-Lun Liu", "Wei-Chen Chiu"], "title": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions", "comment": "ICCV 2025. Project page: https://hentci.github.io/stealthattack/", "summary": "3D scene representation methods like Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) have significantly advanced novel view synthesis. As\nthese methods become prevalent, addressing their vulnerabilities becomes\ncritical. We analyze 3DGS robustness against image-level poisoning attacks and\npropose a novel density-guided poisoning method. Our method strategically\ninjects Gaussian points into low-density regions identified via Kernel Density\nEstimation (KDE), embedding viewpoint-dependent illusory objects clearly\nvisible from poisoned views while minimally affecting innocent views.\nAdditionally, we introduce an adaptive noise strategy to disrupt multi-view\nconsistency, further enhancing attack effectiveness. We propose a KDE-based\nevaluation protocol to assess attack difficulty systematically, enabling\nobjective benchmarking for future research. Extensive experiments demonstrate\nour method's superior performance compared to state-of-the-art techniques.\nProject page: https://hentci.github.io/stealthattack/", "AI": {"tldr": "The paper analyzes 3D Gaussian Splatting (3DGS) vulnerabilities to poisoning attacks and proposes a novel density-guided attack method that injects Gaussian points into low-density regions using Kernel Density Estimation, creating viewpoint-dependent illusory objects.", "motivation": "As 3D scene representation methods like NeRF and 3DGS become widely used, understanding and addressing their security vulnerabilities becomes critical, particularly against image-level poisoning attacks.", "method": "Proposes a density-guided poisoning method using Kernel Density Estimation to identify low-density regions for strategic Gaussian point injection, along with an adaptive noise strategy to disrupt multi-view consistency.", "result": "Extensive experiments show superior performance compared to state-of-the-art techniques, with the method successfully embedding viewpoint-dependent illusory objects visible from poisoned views while minimally affecting innocent views.", "conclusion": "The work demonstrates significant vulnerabilities in 3DGS and provides a systematic KDE-based evaluation protocol for objective benchmarking of attack methods in future research."}}
{"id": "2510.01718", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01718", "abs": "https://arxiv.org/abs/2510.01718", "authors": ["Jialin Zhao"], "title": "Accelerating Attention with Basis Decomposition", "comment": null, "summary": "Attention is a core operation in large language models (LLMs) and\nvision-language models (VLMs). We present BD Attention (BDA), the first\nlossless algorithmic reformulation of attention. BDA is enabled by a simple\nmatrix identity from Basis Decomposition (BD), which restructures multi-head\nprojections into a compact form while preserving exact outputs. Unlike\nI/O-aware system optimizations such as FlashAttention, BDA provides a\nmathematically guaranteed acceleration that is architecture-agnostic. On\nDeepSeek-V2-Lite (16B, FP16), BDA requires only 4s of offline preparation with\nno retraining required and, on modern GPUs, achieves 32% faster key/value\nprojections and 25% smaller weights, while increasing end-to-end perplexity\n(PPL) by just 0.02% (FP16) or 0.0004% (FP32), a negligible effect on model\nperformance. These results position BDA as the first theoretically exact method\nfor lossless attention acceleration that is complementary to existing\nengineering-level optimizations. Our code is available at\nhttps://github.com/abcbdf/basis-decomposition-official.", "AI": {"tldr": "BD Attention (BDA) is the first lossless algorithmic reformulation of attention using Basis Decomposition, providing mathematically guaranteed acceleration without retraining while preserving exact outputs.", "motivation": "Attention is a core operation in LLMs and VLMs, but existing optimizations like FlashAttention are I/O-aware system optimizations rather than fundamental algorithmic improvements.", "method": "BDA uses a simple matrix identity from Basis Decomposition to restructure multi-head projections into a compact form while maintaining exact outputs, requiring only 4s of offline preparation.", "result": "On DeepSeek-V2-Lite (16B, FP16), BDA achieves 32% faster key/value projections, 25% smaller weights, with negligible impact on perplexity (0.02% increase in FP16, 0.0004% in FP32).", "conclusion": "BDA is the first theoretically exact method for lossless attention acceleration that complements existing engineering optimizations, providing architecture-agnostic mathematical acceleration."}}
{"id": "2510.02315", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02315", "abs": "https://arxiv.org/abs/2510.02315", "authors": ["Eric Tillmann Bill", "Enis Simsar", "Thomas Hofmann"], "title": "Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity", "comment": "Code: https://github.com/ericbill21/FOCUS/", "summary": "Text-to-image (T2I) models excel on single-entity prompts but struggle with\nmulti-subject descriptions, often showing attribute leakage, identity\nentanglement, and subject omissions. We introduce the first theoretical\nframework with a principled, optimizable objective for steering sampling\ndynamics toward multi-subject fidelity. Viewing flow matching (FM) through\nstochastic optimal control (SOC), we formulate subject disentanglement as\ncontrol over a trained FM sampler. This yields two architecture-agnostic\nalgorithms: (i) a training-free test-time controller that perturbs the base\nvelocity with a single-pass update, and (ii) Adjoint Matching, a lightweight\nfine-tuning rule that regresses a control network to a backward adjoint signal\nwhile preserving base-model capabilities. The same formulation unifies prior\nattention heuristics, extends to diffusion models via a flow-diffusion\ncorrespondence, and provides the first fine-tuning route explicitly designed\nfor multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and\nStable Diffusion XL, both algorithms consistently improve multi-subject\nalignment while maintaining base-model style. Test-time control runs\nefficiently on commodity GPUs, and fine-tuned controllers trained on limited\nprompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal\nControl for Unentangled Subjects), which achieves state-of-the-art\nmulti-subject fidelity across models.", "AI": {"tldr": "The paper introduces FOCUS, a theoretical framework and algorithms for improving multi-subject generation in text-to-image models by addressing attribute leakage, identity entanglement, and subject omissions through stochastic optimal control applied to flow matching.", "motivation": "Text-to-image models perform well on single-entity prompts but struggle with multi-subject descriptions, exhibiting problems like attribute leakage, identity entanglement, and subject omissions.", "method": "The framework uses stochastic optimal control to steer flow matching sampling dynamics, yielding two algorithms: a training-free test-time controller that perturbs base velocity, and Adjoint Matching for lightweight fine-tuning that preserves base-model capabilities.", "result": "Both algorithms consistently improve multi-subject alignment while maintaining base-model style across Stable Diffusion 3.5, FLUX, and Stable Diffusion XL. Test-time control runs efficiently on commodity GPUs, and fine-tuned controllers generalize to unseen prompts.", "conclusion": "FOCUS achieves state-of-the-art multi-subject fidelity across models, providing the first fine-tuning route explicitly designed for multi-subject generation and unifying prior attention heuristics through a principled theoretical framework."}}
{"id": "2510.01721", "categories": ["cs.LG", "I.2.6"], "pdf": "https://arxiv.org/pdf/2510.01721", "abs": "https://arxiv.org/abs/2510.01721", "authors": ["Saptarshi Mandal", "Yashaswini Murthy", "R. Srikant"], "title": "Finite-Time Bounds for Distributionally Robust TD Learning with Linear Function Approximation", "comment": "Preprint. 32 Pages", "summary": "Distributionally robust reinforcement learning (DRRL) focuses on designing\npolicies that achieve good performance under model uncertainties. In\nparticular, we are interested in maximizing the worst-case long-term discounted\nreward, where the data for RL comes from a nominal model while the deployed\nenvironment can deviate from the nominal model within a prescribed uncertainty\nset. Existing convergence guarantees for robust temporal-difference (TD)\nlearning for policy evaluation are limited to tabular MDPs or are dependent on\nrestrictive discount-factor assumptions when function approximation is used. We\npresent the first robust TD learning with linear function approximation, where\nrobustness is measured with respect to the total-variation distance and\nWasserstein-l distance uncertainty set. Additionally, our algorithm is both\nmodel-free and does not require generative access to the MDP. Our algorithm\ncombines a two-time-scale stochastic-approximation update with an outer-loop\ntarget-network update. We establish an $\\tilde{O}(1/\\epsilon^2)$ sample\ncomplexity to obtain an $\\epsilon$-accurate value estimate. Our results close a\nkey gap between the empirical success of robust RL algorithms and the\nnon-asymptotic guarantees enjoyed by their non-robust counterparts. The key\nideas in the paper also extend in a relatively straightforward fashion to\nrobust Q-learning with function approximation.", "AI": {"tldr": "First robust TD learning with linear function approximation for distributionally robust RL, achieving O(1/\u03b5\u00b2) sample complexity without requiring generative MDP access.", "motivation": "Existing robust TD learning convergence guarantees are limited to tabular MDPs or require restrictive discount-factor assumptions with function approximation, creating a gap between empirical success and theoretical guarantees.", "method": "Combines two-time-scale stochastic-approximation update with outer-loop target-network update, using total-variation and Wasserstein-l distance uncertainty sets for robustness.", "result": "Achieves \u00d5(1/\u03b5\u00b2) sample complexity to obtain \u03b5-accurate value estimate, making it model-free and not requiring generative access to the MDP.", "conclusion": "Closes key gap between empirical success of robust RL algorithms and non-asymptotic guarantees of non-robust counterparts, with approach extending to robust Q-learning with function approximation."}}
{"id": "2510.01723", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01723", "abs": "https://arxiv.org/abs/2510.01723", "authors": ["Tanay Rastogi", "Anders Karlstr\u00f6m"], "title": "Workplace Location Choice Model based on Deep Neural Network", "comment": null, "summary": "Discrete choice models (DCMs) have long been used to analyze workplace\nlocation decisions, but they face challenges in accurately mirroring individual\ndecision-making processes. This paper presents a deep neural network (DNN)\nmethod for modeling workplace location choices, which aims to better understand\ncomplex decision patterns and provides better results than traditional discrete\nchoice models (DCMs). The study demonstrates that DNNs show significant\npotential as a robust alternative to DCMs in this domain. While both models\neffectively replicate the impact of job opportunities on workplace location\nchoices, the DNN outperforms the DCM in certain aspects. However, the DCM\nbetter aligns with data when assessing the influence of individual attributes\non workplace distance. Notably, DCMs excel at shorter distances, while DNNs\nperform comparably to both data and DCMs for longer distances. These findings\nunderscore the importance of selecting the appropriate model based on specific\napplication requirements in workplace location choice analysis.", "AI": {"tldr": "This paper compares deep neural networks (DNNs) with traditional discrete choice models (DCMs) for workplace location choice modeling, finding DNNs show significant potential as a robust alternative but each model has specific strengths.", "motivation": "Traditional discrete choice models face challenges in accurately mirroring individual decision-making processes for workplace location choices, motivating the exploration of deep neural networks as an alternative approach.", "method": "The paper presents a deep neural network (DNN) method for modeling workplace location choices and compares its performance against traditional discrete choice models (DCMs).", "result": "DNNs outperform DCMs in certain aspects and show significant potential as an alternative. DCMs better align with data for individual attribute influence on workplace distance and excel at shorter distances, while DNNs perform comparably for longer distances.", "conclusion": "The findings underscore the importance of selecting the appropriate model based on specific application requirements in workplace location choice analysis, as both DNNs and DCMs have distinct strengths."}}
{"id": "2510.01744", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01744", "abs": "https://arxiv.org/abs/2510.01744", "authors": ["Lea Demelius", "Dominik Kowald", "Simone Kopeinik", "Roman Kern", "Andreas Tr\u00fcgler"], "title": "Private and Fair Machine Learning: Revisiting the Disparate Impact of Differentially Private SGD", "comment": null, "summary": "Differential privacy (DP) is a prominent method for protecting information\nabout individuals during data analysis. Training neural networks with\ndifferentially private stochastic gradient descent (DPSGD) influences the\nmodel's learning dynamics and, consequently, its output. This can affect the\nmodel's performance and fairness. While the majority of studies on the topic\nreport a negative impact on fairness, it has recently been suggested that\nfairness levels comparable to non-private models can be achieved by optimizing\nhyperparameters for performance directly on differentially private models\n(rather than re-using hyperparameters from non-private models, as is common\npractice). In this work, we analyze the generalizability of this claim by 1)\ncomparing the disparate impact of DPSGD on different performance metrics, and\n2) analyzing it over a wide range of hyperparameter settings. We highlight that\na disparate impact on one metric does not necessarily imply a disparate impact\non another. Most importantly, we show that while optimizing hyperparameters\ndirectly on differentially private models does not mitigate the disparate\nimpact of DPSGD reliably, it can still lead to improved utility-fairness\ntrade-offs compared to re-using hyperparameters from non-private models. We\nstress, however, that any form of hyperparameter tuning entails additional\nprivacy leakage, calling for careful considerations of how to balance privacy,\nutility and fairness. Finally, we extend our analyses to DPSGD-Global-Adapt, a\nvariant of DPSGD designed to mitigate the disparate impact on accuracy, and\nconclude that this alternative may not be a robust solution with respect to\nhyperparameter choice.", "AI": {"tldr": "DPSGD's impact on fairness varies across metrics, and direct hyperparameter optimization on private models improves utility-fairness trade-offs but doesn't reliably mitigate disparate impact, while also increasing privacy leakage.", "motivation": "To analyze whether optimizing hyperparameters directly on differentially private models can achieve fairness levels comparable to non-private models, and to understand the disparate impact of DPSGD across different performance metrics.", "method": "1) Compare disparate impact of DPSGD on different performance metrics, 2) Analyze impact over wide range of hyperparameter settings, 3) Extend analysis to DPSGD-Global-Adapt variant.", "result": "Disparate impact on one metric doesn't imply disparate impact on another. Direct hyperparameter optimization on private models doesn't reliably mitigate DPSGD's disparate impact but improves utility-fairness trade-offs compared to reusing non-private hyperparameters.", "conclusion": "Hyperparameter tuning on private models improves utility-fairness trade-offs but increases privacy leakage and doesn't reliably solve disparate impact. DPSGD-Global-Adapt is not robust to hyperparameter choices."}}
{"id": "2510.01755", "categories": ["cs.LG", "cs.NA", "math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.01755", "abs": "https://arxiv.org/abs/2510.01755", "authors": ["Johannes Hertrich", "Hok Shing Wong", "Alexander Denker", "Stanislas Ducotterd", "Zhenghan Fang", "Markus Haltmeier", "\u017deljko Kereta", "Erich Kobler", "Oscar Leong", "Mohammad Sadegh Salehi", "Carola-Bibiane Sch\u00f6nlieb", "Johannes Schwab", "Zakhar Shumaylov", "Jeremias Sulam", "German Sh\u00e2ma Wache", "Martin Zach", "Yasi Zhang", "Matthias J. Ehrhardt", "Sebastian Neumayer"], "title": "Learning Regularization Functionals for Inverse Problems: A Comparative Study", "comment": null, "summary": "In recent years, a variety of learned regularization frameworks for solving\ninverse problems in imaging have emerged. These offer flexible modeling\ntogether with mathematical insights. The proposed methods differ in their\narchitectural design and training strategies, making direct comparison\nchallenging due to non-modular implementations. We address this gap by\ncollecting and unifying the available code into a common framework. This\nunified view allows us to systematically compare the approaches and highlight\ntheir strengths and limitations, providing valuable insights into their future\npotential. We also provide concise descriptions of each method, complemented by\npractical guidelines.", "AI": {"tldr": "The paper presents a unified framework for comparing learned regularization methods in imaging inverse problems, addressing implementation inconsistencies and providing systematic analysis.", "motivation": "To address the challenge of comparing different learned regularization methods due to their non-modular implementations and varying architectural designs and training strategies.", "method": "Collect and unify available code into a common framework to enable systematic comparison of different learned regularization approaches for imaging inverse problems.", "result": "The unified framework allows systematic comparison of approaches, highlighting their strengths and limitations, and provides practical guidelines for implementation.", "conclusion": "The unified framework offers valuable insights into the future potential of learned regularization methods and facilitates better comparison and understanding of different approaches in the field."}}
{"id": "2510.01758", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01758", "abs": "https://arxiv.org/abs/2510.01758", "authors": ["Bruno Corcuera", "Carlos Eiras-Franco", "Brais Cancela"], "title": "Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks", "comment": null, "summary": "Latent representations are critical for the performance and robustness of\nmachine learning models, as they encode the essential features of data in a\ncompact and informative manner. However, in vision tasks, these representations\nare often affected by noisy or irrelevant features, which can degrade the\nmodel's performance and generalization capabilities. This paper presents a\nnovel approach for enhancing latent representations using unsupervised Dynamic\nFeature Selection (DFS). For each instance, the proposed method identifies and\nremoves misleading or redundant information in images, ensuring that only the\nmost relevant features contribute to the latent space. By leveraging an\nunsupervised framework, our approach avoids reliance on labeled data, making it\nbroadly applicable across various domains and datasets. Experiments conducted\non image datasets demonstrate that models equipped with unsupervised DFS\nachieve significant improvements in generalization performance across various\ntasks, including clustering and image generation, while incurring a minimal\nincrease in the computational cost.", "AI": {"tldr": "Unsupervised Dynamic Feature Selection (DFS) enhances latent representations by removing noisy features in images, improving model performance without labeled data.", "motivation": "Latent representations in vision tasks often contain noisy or irrelevant features that degrade model performance and generalization capabilities.", "method": "Proposes unsupervised Dynamic Feature Selection (DFS) that identifies and removes misleading/redundant information for each instance, ensuring only relevant features contribute to the latent space.", "result": "Models with unsupervised DFS achieve significant improvements in generalization performance across clustering and image generation tasks with minimal computational cost increase.", "conclusion": "Unsupervised DFS effectively enhances latent representations by removing noisy features, improving model robustness and performance across various vision tasks."}}
{"id": "2510.01764", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01764", "abs": "https://arxiv.org/abs/2510.01764", "authors": ["Waris Radji", "Thomas Michel", "Hector Piteau"], "title": "Octax: Accelerated CHIP-8 Arcade Environments for Reinforcement Learning in JAX", "comment": null, "summary": "Reinforcement learning (RL) research requires diverse, challenging\nenvironments that are both tractable and scalable. While modern video games may\noffer rich dynamics, they are computationally expensive and poorly suited for\nlarge-scale experimentation due to their CPU-bound execution. We introduce\nOctax, a high-performance suite of classic arcade game environments implemented\nin JAX, based on CHIP-8 emulation, a predecessor to Atari, which is widely\nadopted as a benchmark in RL research. Octax provides the JAX community with a\nlong-awaited end-to-end GPU alternative to the Atari benchmark, offering\nimage-based environments, spanning puzzle, action, and strategy genres, all\nexecutable at massive scale on modern GPUs. Our JAX-based implementation\nachieves orders-of-magnitude speedups over traditional CPU emulators while\nmaintaining perfect fidelity to the original game mechanics. We demonstrate\nOctax's capabilities by training RL agents across multiple games, showing\nsignificant improvements in training speed and scalability compared to existing\nsolutions. The environment's modular design enables researchers to easily\nextend the suite with new games or generate novel environments using large\nlanguage models, making it an ideal platform for large-scale RL\nexperimentation.", "AI": {"tldr": "Octax is a high-performance JAX-based suite of classic arcade game environments that provides GPU-accelerated alternatives to Atari benchmarks, achieving orders-of-magnitude speedups while maintaining game fidelity.", "motivation": "Current RL research lacks diverse, challenging environments that are both computationally efficient and scalable. Modern video games are CPU-bound and expensive for large-scale experimentation.", "method": "Implemented classic arcade game environments in JAX based on CHIP-8 emulation, providing GPU-accelerated execution with modular design for easy extension.", "result": "Achieved orders-of-magnitude speedups over traditional CPU emulators while maintaining perfect fidelity to original game mechanics. Demonstrated significant improvements in RL training speed and scalability.", "conclusion": "Octax serves as an ideal platform for large-scale RL experimentation, offering GPU-accelerated environments that are easily extensible and suitable for training agents across multiple game genres."}}
{"id": "2510.01788", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.01788", "abs": "https://arxiv.org/abs/2510.01788", "authors": ["Cl\u00e9mentine Court\u00e8s", "Emmanuel Franck", "Michael Kraus", "Laurent Navoret", "L\u00e9opold Tr\u00e9mant"], "title": "Neural non-canonical Hamiltonian dynamics for long-time simulations", "comment": null, "summary": "This work focuses on learning non-canonical Hamiltonian dynamics from data,\nwhere long-term predictions require the preservation of structure both in the\nlearned model and in numerical schemes. Previous research focused on either\nfacet, respectively with a potential-based architecture and with degenerate\nvariational integrators, but new issues arise when combining both. In\nexperiments, the learnt model is sometimes numerically unstable due to the\ngauge dependency of the scheme, rendering long-time simulations impossible. In\nthis paper, we identify this problem and propose two different training\nstrategies to address it, either by directly learning the vector field or by\nlearning a time-discrete dynamics through the scheme. Several numerical test\ncases assess the ability of the methods to learn complex physical dynamics,\nlike the guiding center from gyrokinetic plasma physics.", "AI": {"tldr": "Learning non-canonical Hamiltonian dynamics from data while preserving structure in both learned models and numerical schemes, addressing numerical instability issues through two training strategies.", "motivation": "Previous approaches focused separately on structure-preserving models or numerical schemes, but combining both creates gauge dependency issues that cause numerical instability in long-term simulations.", "method": "Proposed two training strategies: directly learning the vector field, or learning time-discrete dynamics through the numerical scheme itself.", "result": "Methods successfully learn complex physical dynamics like guiding center from gyrokinetic plasma physics, addressing the numerical instability problem.", "conclusion": "The proposed training strategies effectively resolve gauge dependency issues in combining structure-preserving models with numerical schemes, enabling stable long-term simulations of non-canonical Hamiltonian dynamics."}}
{"id": "2510.01793", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01793", "abs": "https://arxiv.org/abs/2510.01793", "authors": ["Adil Koeken", "Alexander Ziller", "Moritz Knolle", "Daniel Rueckert"], "title": "Sensitivity, Specificity, and Consistency: A Tripartite Evaluation of Privacy Filters for Synthetic Data Generation", "comment": null, "summary": "The generation of privacy-preserving synthetic datasets is a promising avenue\nfor overcoming data scarcity in medical AI research. Post-hoc privacy filtering\ntechniques, designed to remove samples containing personally identifiable\ninformation, have recently been proposed as a solution. However, their\neffectiveness remains largely unverified. This work presents a rigorous\nevaluation of a filtering pipeline applied to chest X-ray synthesis. Contrary\nto claims from the original publications, our results demonstrate that current\nfilters exhibit limited specificity and consistency, achieving high sensitivity\nonly for real images while failing to reliably detect near-duplicates generated\nfrom training data. These results demonstrate a critical limitation of post-hoc\nfiltering: rather than effectively safeguarding patient privacy, these methods\nmay provide a false sense of security while leaving unacceptable levels of\npatient information exposed. We conclude that substantial advances in filter\ndesign are needed before these methods can be confidently deployed in sensitive\napplications.", "AI": {"tldr": "Post-hoc privacy filtering for synthetic medical datasets shows limited effectiveness, failing to reliably detect near-duplicates and providing false security while exposing patient information.", "motivation": "To evaluate the effectiveness of post-hoc privacy filtering techniques for synthetic medical datasets, as their claims remain largely unverified despite being proposed as solutions for privacy-preserving data generation.", "method": "Conducted a rigorous evaluation of a filtering pipeline applied to chest X-ray synthesis, testing specificity, consistency, and sensitivity for detecting near-duplicates from training data.", "result": "Current filters exhibit limited specificity and consistency, achieving high sensitivity only for real images but failing to reliably detect generated near-duplicates from training data.", "conclusion": "Post-hoc filtering methods have critical limitations and provide false security while exposing patient information; substantial advances in filter design are needed before deployment in sensitive applications."}}
{"id": "2510.01796", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01796", "abs": "https://arxiv.org/abs/2510.01796", "authors": ["Meng-Hsi Chen", "Yu-Ang Lee", "Feng-Ting Liao", "Da-shan Shiu"], "title": "Rethinking the shape convention of an MLP", "comment": null, "summary": "Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow\ndesign where skip connections operate at the input/output dimensions while\nprocessing occurs in expanded hidden spaces. We challenge this convention by\nproposing wide-narrow-wide (Hourglass) MLP blocks where skip connections\noperate at expanded dimensions while residual computation flows through narrow\nbottlenecks. This inversion leverages higher-dimensional spaces for incremental\nrefinement while maintaining computational efficiency through parameter-matched\ndesigns. Implementing Hourglass MLPs requires an initial projection to lift\ninput signals to expanded dimensions. We propose that this projection can\nremain fixed at random initialization throughout training, enabling efficient\ntraining and inference implementations. We evaluate both architectures on\ngenerative tasks over popular image datasets, characterizing\nperformance-parameter Pareto frontiers through systematic architectural search.\nResults show that Hourglass architectures consistently achieve superior Pareto\nfrontiers compared to conventional designs. As parameter budgets increase,\noptimal Hourglass configurations favor deeper networks with wider skip\nconnections and narrower bottlenecks-a scaling pattern distinct from\nconventional MLPs. Our findings suggest reconsidering skip connection placement\nin modern architectures, with potential applications extending to Transformers\nand other residual networks.", "AI": {"tldr": "Hourglass MLPs invert conventional narrow-wide-narrow design by using wide-narrow-wide blocks where skip connections operate at expanded dimensions while computation flows through narrow bottlenecks, achieving superior performance-parameter trade-offs.", "motivation": "Challenge the conventional narrow-wide-narrow MLP design convention and explore whether skip connections should operate at expanded dimensions rather than input/output dimensions for better performance.", "method": "Propose Hourglass MLP blocks with wide-narrow-wide structure, where initial projection lifts inputs to expanded dimensions (can remain fixed random initialization), and residual computation flows through narrow bottlenecks while skip connections operate at higher dimensions.", "result": "Hourglass architectures consistently achieve superior Pareto frontiers compared to conventional designs. As parameters increase, optimal configurations favor deeper networks with wider skip connections and narrower bottlenecks - a distinct scaling pattern from conventional MLPs.", "conclusion": "Skip connection placement should be reconsidered in modern architectures, with potential applications extending to Transformers and other residual networks. The Hourglass design demonstrates that operating skip connections at expanded dimensions can provide better performance-parameter trade-offs."}}
{"id": "2510.01817", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01817", "abs": "https://arxiv.org/abs/2510.01817", "authors": ["Adam Filipek"], "title": "Sparse Query Attention (SQA): A Computationally Efficient Attention Mechanism with Query Heads Reduction", "comment": "18 pages, 6 figures, small-scale experiments", "summary": "The Transformer architecture, underpinned by the Multi-Head Attention (MHA)\nmechanism, has become the de facto standard for state-of-the-art models in\nartificial intelligence. However, the quadratic computational complexity of MHA\nwith respect to sequence length presents a significant barrier to scaling,\nparticularly for applications involving long contexts. Prevailing solutions,\nsuch as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have\neffectively addressed the memory bandwidth bottleneck that dominates\nautoregressive inference latency by sharing Key and Value projections. While\nhighly successful, these methods do not reduce the fundamental number of\nfloating-point operations (FLOPs) required for the attention score computation,\nwhich remains a critical bottleneck for training and full-sequence processing.\nThis paper introduces Sparse Query Attention (SQA), a novel attention\narchitecture that pursues an alternative and complementary optimization path.\nInstead of reducing Key/Value heads, SQA reduces the number of Query heads.\nThis architectural modification directly decreases the computational complexity\nof the attention mechanism by a factor proportional to the reduction in query\nheads, thereby lowering the overall FLOPs. This work presents the theoretical\nfoundation of SQA, its mathematical formulation, and a family of architectural\nvariants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate\nthat SQA can achieve significant throughput improvements of up to 3x in\ncomputation-bound scenarios such as model pre-training, fine-tuning, and\nencoder-based tasks, with only a minimal impact on model quality in preliminary\nsmallscale experiments. SQA was discovered serendipitously during the\ndevelopment of the upcoming Reactive Transformer architecture, suggesting its\npotential as a powerful tool for building more efficient and scalable models", "AI": {"tldr": "Sparse Query Attention (SQA) reduces computational complexity in Transformers by decreasing Query heads instead of Key/Value heads, achieving up to 3x throughput improvements for long sequences with minimal quality impact.", "motivation": "Current attention optimizations like MQA and GQA address memory bandwidth but don't reduce FLOPs, which remains a critical bottleneck for training and long-sequence processing.", "method": "SQA reduces the number of Query heads rather than Key/Value heads, directly decreasing attention computation complexity proportional to query head reduction.", "result": "Empirical benchmarks on long sequences (32k-200k tokens) show up to 3x throughput improvements in computation-bound scenarios like pre-training and fine-tuning, with minimal quality impact.", "conclusion": "SQA provides a complementary optimization path to existing methods, offering significant computational efficiency gains for scalable model development."}}
{"id": "2510.01824", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01824", "abs": "https://arxiv.org/abs/2510.01824", "authors": ["Olivier Goudet", "Quentin Suire", "Adrien Go\u00ebffon", "Fr\u00e9d\u00e9ric Saubion", "Sylvain Lamprier"], "title": "Black-Box Combinatorial Optimization with Order-Invariant Reinforcement Learning", "comment": null, "summary": "We introduce an order-invariant reinforcement learning framework for\nblack-box combinatorial optimization. Classical estimation-of-distribution\nalgorithms (EDAs) often rely on learning explicit variable dependency graphs,\nwhich can be costly and fail to capture complex interactions efficiently. In\ncontrast, we parameterize a multivariate autoregressive generative model\ntrained without a fixed variable ordering. By sampling random generation orders\nduring training - a form of information-preserving dropout - the model is\nencouraged to be invariant to variable order, promoting search-space diversity\nand shaping the model to focus on the most relevant variable dependencies,\nimproving sample efficiency. We adapt Generalized Reinforcement Policy\nOptimization (GRPO) to this setting, providing stable policy-gradient updates\nfrom scale-invariant advantages. Across a wide range of benchmark algorithms\nand problem instances of varying sizes, our method frequently achieves the best\nperformance and consistently avoids catastrophic failures.", "AI": {"tldr": "Order-invariant reinforcement learning for black-box combinatorial optimization using random generation orders during training to improve sample efficiency and avoid catastrophic failures.", "motivation": "Classical estimation-of-distribution algorithms rely on learning explicit variable dependency graphs, which can be costly and inefficient for capturing complex interactions.", "method": "Parameterize multivariate autoregressive generative model trained without fixed variable ordering, using random generation orders as information-preserving dropout. Adapt Generalized Reinforcement Policy Optimization (GRPO) for stable policy-gradient updates.", "result": "Achieves best performance across wide range of benchmark algorithms and problem instances, consistently avoiding catastrophic failures.", "conclusion": "Order-invariant training with random generation orders promotes search-space diversity and focuses on relevant variable dependencies, improving combinatorial optimization performance."}}
{"id": "2510.01982", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01982", "abs": "https://arxiv.org/abs/2510.01982", "authors": ["Yujie Zhou", "Pengyang Ling", "Jiazi Bu", "Yibin Wang", "Yuhang Zang", "Jiaqi Wang", "Li Niu", "Guangtao Zhai"], "title": "$\\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models", "comment": "Github Page: https://github.com/bcmi/Granular-GRPO", "summary": "The integration of online reinforcement learning (RL) into diffusion and flow\nmodels has recently emerged as a promising approach for aligning generative\nmodels with human preferences. Stochastic sampling via Stochastic Differential\nEquations (SDE) is employed during the denoising process to generate diverse\ndenoising directions for RL exploration. While existing methods effectively\nexplore potential high-value samples, they suffer from sub-optimal preference\nalignment due to sparse and narrow reward signals. To address these challenges,\nwe propose a novel Granular-GRPO ($\\text{G}^2$RPO ) framework that achieves\nprecise and comprehensive reward assessments of sampling directions in\nreinforcement learning of flow models. Specifically, a Singular Stochastic\nSampling strategy is introduced to support step-wise stochastic exploration\nwhile enforcing a high correlation between the reward and the injected noise,\nthereby facilitating a faithful reward for each SDE perturbation. Concurrently,\nto eliminate the bias inherent in fixed-granularity denoising, we introduce a\nMulti-Granularity Advantage Integration module that aggregates advantages\ncomputed at multiple diffusion scales, producing a more comprehensive and\nrobust evaluation of the sampling directions. Experiments conducted on various\nreward models, including both in-domain and out-of-domain evaluations,\ndemonstrate that our $\\text{G}^2$RPO significantly outperforms existing\nflow-based GRPO baselines,highlighting its effectiveness and robustness.", "AI": {"tldr": "Proposes G\u00b2RPO framework for precise reward assessment in RL-based flow models, using singular stochastic sampling and multi-granularity advantage integration to improve preference alignment.", "motivation": "Existing methods for integrating RL into diffusion/flow models suffer from sub-optimal preference alignment due to sparse and narrow reward signals, limiting their effectiveness in exploring high-value samples.", "method": "Introduces Granular-GRPO (G\u00b2RPO) with: 1) Singular Stochastic Sampling for step-wise exploration with high reward-noise correlation, and 2) Multi-Granularity Advantage Integration to eliminate bias from fixed-granularity denoising.", "result": "Experiments show G\u00b2RPO significantly outperforms existing flow-based GRPO baselines across various reward models in both in-domain and out-of-domain evaluations.", "conclusion": "The proposed framework achieves more precise and comprehensive reward assessments, demonstrating effectiveness and robustness in aligning generative models with human preferences."}}
{"id": "2510.01842", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01842", "abs": "https://arxiv.org/abs/2510.01842", "authors": ["Yannis Belkhiter", "Seshu Tirupathi", "Giulio Zizzo", "Sachin Sharma", "John D. Kelleher"], "title": "Pre-Hoc Predictions in AutoML: Leveraging LLMs to Enhance Model Selection and Benchmarking for Tabular datasets", "comment": "Oral Presentations ADAPT Annual Scientific Conference 2025", "summary": "The field of AutoML has made remarkable progress in post-hoc model selection,\nwith libraries capable of automatically identifying the most performing models\nfor a given dataset. Nevertheless, these methods often rely on exhaustive\nhyperparameter searches, where methods automatically train and test different\ntypes of models on the target dataset. Contrastingly, pre-hoc prediction\nemerges as a promising alternative, capable of bypassing exhaustive search\nthrough intelligent pre-selection of models. Despite its potential, pre-hoc\nprediction remains under-explored in the literature. This paper explores the\nintersection of AutoML and pre-hoc model selection by leveraging traditional\nmodels and Large Language Model (LLM) agents to reduce the search space of\nAutoML libraries. By relying on dataset descriptions and statistical\ninformation, we reduce the AutoML search space. Our methodology is applied to\nthe AWS AutoGluon portfolio dataset, a state-of-the-art AutoML benchmark\ncontaining 175 tabular classification datasets available on OpenML. The\nproposed approach offers a shift in AutoML workflows, significantly reducing\ncomputational overhead, while still selecting the best model for the given\ndataset.", "AI": {"tldr": "This paper proposes using pre-hoc model selection with traditional models and LLM agents to reduce AutoML search space, achieving computational efficiency while maintaining model performance.", "motivation": "AutoML methods rely on exhaustive hyperparameter searches which are computationally expensive. Pre-hoc prediction offers a promising alternative but remains under-explored in literature.", "method": "Leverage traditional models and LLM agents using dataset descriptions and statistical information to pre-select models, reducing the AutoML search space. Applied to AWS AutoGluon portfolio dataset with 175 tabular classification datasets.", "result": "The approach significantly reduces computational overhead while still selecting the best model for the given dataset.", "conclusion": "Pre-hoc model selection offers a viable shift in AutoML workflows, balancing computational efficiency with model performance."}}
{"id": "2510.01853", "categories": ["cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.01853", "abs": "https://arxiv.org/abs/2510.01853", "authors": ["Vladimir Krsmanovic", "Matthias Cosler", "Mohamed Ghanem", "Bernd Finkbeiner"], "title": "Learning Representations Through Contrastive Neural Model Checking", "comment": null, "summary": "Model checking is a key technique for verifying safety-critical systems\nagainst formal specifications, where recent applications of deep learning have\nshown promise. However, while ubiquitous for vision and language domains,\nrepresentation learning remains underexplored in formal verification. We\nintroduce Contrastive Neural Model Checking (CNML), a novel method that\nleverages the model checking task as a guiding signal for learning aligned\nrepresentations. CNML jointly embeds logical specifications and systems into a\nshared latent space through a self-supervised contrastive objective. On\nindustry-inspired retrieval tasks, CNML considerably outperforms both\nalgorithmic and neural baselines in cross-modal and intra-modal settings.We\nfurther show that the learned representations effectively transfer to\ndownstream tasks and generalize to more complex formulas. These findings\ndemonstrate that model checking can serve as an objective for learning\nrepresentations for formal languages.", "AI": {"tldr": "CNML introduces contrastive learning for model checking by embedding logical specifications and systems into a shared latent space, outperforming baselines on retrieval tasks and showing transfer learning capabilities.", "motivation": "Representation learning remains underexplored in formal verification despite its success in vision and language domains, creating an opportunity to leverage model checking as a guiding signal for learning aligned representations.", "method": "Contrastive Neural Model Checking (CNML) jointly embeds logical specifications and systems into a shared latent space using a self-supervised contrastive objective.", "result": "CNML considerably outperforms both algorithmic and neural baselines on industry-inspired retrieval tasks in cross-modal and intra-modal settings, and shows effective transfer to downstream tasks and generalization to more complex formulas.", "conclusion": "Model checking can serve as an effective objective for learning representations for formal languages, demonstrating the viability of representation learning in formal verification."}}
{"id": "2510.01855", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01855", "abs": "https://arxiv.org/abs/2510.01855", "authors": ["Lexiang Hu", "Yikang Li", "Zhouchen Lin"], "title": "Explicit Discovery of Nonlinear Symmetries from Dynamic Data", "comment": null, "summary": "Symmetry is widely applied in problems such as the design of equivariant\nnetworks and the discovery of governing equations, but in complex scenarios, it\nis not known in advance. Most previous symmetry discovery methods are limited\nto linear symmetries, and recent attempts to discover nonlinear symmetries fail\nto explicitly get the Lie algebra subspace. In this paper, we propose LieNLSD,\nwhich is, to our knowledge, the first method capable of determining the number\nof infinitesimal generators with nonlinear terms and their explicit\nexpressions. We specify a function library for the infinitesimal group action\nand aim to solve for its coefficient matrix, proving that its prolongation\nformula for differential equations, which governs dynamic data, is also linear\nwith respect to the coefficient matrix. By substituting the central differences\nof the data and the Jacobian matrix of the trained neural network into the\ninfinitesimal criterion, we get a system of linear equations for the\ncoefficient matrix, which can then be solved using SVD. On top quark tagging\nand a series of dynamic systems, LieNLSD shows qualitative advantages over\nexisting methods and improves the long rollout accuracy of neural PDE solvers\nby over 20% while applying to guide data augmentation. Code and data are\navailable at https://github.com/hulx2002/LieNLSD.", "AI": {"tldr": "LieNLSD is the first method that can discover nonlinear symmetries by determining the number of infinitesimal generators and their explicit expressions, improving neural PDE solver accuracy by over 20%.", "motivation": "Symmetry is crucial for equivariant networks and governing equation discovery, but existing methods are limited to linear symmetries and fail to explicitly obtain Lie algebra subspaces for nonlinear cases.", "method": "Uses a function library for infinitesimal group action and solves for its coefficient matrix. Applies prolongation formula for differential equations, substitutes data differences and neural network Jacobian into infinitesimal criterion, then solves linear equations using SVD.", "result": "Shows qualitative advantages over existing methods on top quark tagging and dynamic systems. Improves long rollout accuracy of neural PDE solvers by over 20% when used for data augmentation.", "conclusion": "LieNLSD successfully discovers nonlinear symmetries and their explicit Lie algebra expressions, providing significant improvements in neural PDE solver performance."}}
{"id": "2510.01858", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.01858", "abs": "https://arxiv.org/abs/2510.01858", "authors": ["Jacob J. W. Bakermans", "Pablo Tano", "Reidar Riveland", "Charles Findling", "Alexandre Pouget"], "title": "Compositional meta-learning through probabilistic task inference", "comment": null, "summary": "To solve a new task from minimal experience, it is essential to effectively\nreuse knowledge from previous tasks, a problem known as meta-learning.\nCompositional solutions, where common elements of computation are flexibly\nrecombined into new configurations, are particularly well-suited for\nmeta-learning. Here, we propose a compositional meta-learning model that\nexplicitly represents tasks as structured combinations of reusable\ncomputations. We achieve this by learning a generative model that captures the\nunderlying components and their statistics shared across a family of tasks.\nThis approach transforms learning a new task into a probabilistic inference\nproblem, which allows for finding solutions without parameter updates through\nhighly constrained hypothesis testing. Our model successfully recovers ground\ntruth components and statistics in rule learning and motor learning tasks. We\nthen demonstrate its ability to quickly infer new solutions from just single\nexamples. Together, our framework joins the expressivity of neural networks\nwith the data-efficiency of probabilistic inference to achieve rapid\ncompositional meta-learning.", "AI": {"tldr": "A compositional meta-learning model that represents tasks as structured combinations of reusable computations, enabling rapid learning of new tasks from minimal examples without parameter updates.", "motivation": "To enable effective knowledge reuse from previous tasks for solving new tasks with minimal experience, addressing the meta-learning problem through compositional solutions.", "method": "Learn a generative model that captures underlying components and their statistics shared across tasks, transforming new task learning into probabilistic inference without parameter updates.", "result": "Successfully recovers ground truth components in rule learning and motor learning tasks, and demonstrates ability to quickly infer new solutions from single examples.", "conclusion": "The framework combines neural network expressivity with probabilistic inference data-efficiency to achieve rapid compositional meta-learning."}}
{"id": "2510.01867", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01867", "abs": "https://arxiv.org/abs/2510.01867", "authors": ["Subhamon Supantha", "Abhishek Sinha"], "title": "Universal Dynamic Regret and Constraint Violation Bounds for Constrained Online Convex Optimization", "comment": null, "summary": "We consider a generalization of the celebrated Online Convex Optimization\n(OCO) framework with online adversarial constraints. We present two algorithms\nhaving simple modular structures that yield universal dynamic regret and\ncumulative constraint violation bounds, improving upon the state-of-the-art\nresults. Our results hold in the most general case when both the cost and\nconstraint functions are chosen arbitrarily by an adversary, and the constraint\nfunctions need not contain any common feasible point. The results are\nestablished by reducing the constrained learning problem to an instance of the\nstandard OCO problem with specially constructed surrogate cost functions.", "AI": {"tldr": "This paper presents two modular algorithms for Online Convex Optimization with adversarial constraints, achieving improved dynamic regret and constraint violation bounds without requiring common feasible points.", "motivation": "To generalize the Online Convex Optimization framework to handle adversarial constraints where both cost and constraint functions are chosen arbitrarily by an adversary, addressing the limitation that constraint functions need not contain any common feasible point.", "method": "Developed two algorithms with simple modular structures that reduce the constrained learning problem to standard OCO with specially constructed surrogate cost functions.", "result": "Achieved universal dynamic regret and cumulative constraint violation bounds that improve upon state-of-the-art results in the most general adversarial setting.", "conclusion": "The proposed modular approach successfully handles the challenging case of adversarial constraints without requiring common feasibility, providing improved performance guarantees for constrained online learning problems."}}
{"id": "2510.02291", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.02291", "abs": "https://arxiv.org/abs/2510.02291", "authors": ["Litu Rout", "Andreas Lugmayr", "Yasamin Jafarian", "Srivatsan Varadharajan", "Constantine Caramanis", "Sanjay Shakkottai", "Ira Kemelmacher-Shlizerman"], "title": "Test-Time Anchoring for Discrete Diffusion Posterior Sampling", "comment": "Preprint", "summary": "We study the problem of posterior sampling using pretrained discrete\ndiffusion foundation models, aiming to recover images from noisy measurements\nwithout retraining task-specific models. While diffusion models have achieved\nremarkable success in generative modeling, most advances rely on continuous\nGaussian diffusion. In contrast, discrete diffusion offers a unified framework\nfor jointly modeling categorical data such as text and images. Beyond\nunification, discrete diffusion provides faster inference, finer control, and\nprincipled training-free Bayesian inference, making it particularly well-suited\nfor posterior sampling. However, existing approaches to discrete diffusion\nposterior sampling face severe challenges: derivative-free guidance yields\nsparse signals, continuous relaxations limit applicability, and split Gibbs\nsamplers suffer from the curse of dimensionality. To overcome these\nlimitations, we introduce Anchored Posterior Sampling (APS) for masked\ndiffusion foundation models, built on two key innovations -- quantized\nexpectation for gradient-like guidance in discrete embedding space, and\nanchored remasking for adaptive decoding. Our approach achieves\nstate-of-the-art performance among discrete diffusion samplers across linear\nand nonlinear inverse problems on the standard benchmarks. We further\ndemonstrate the benefits of our approach in training-free stylization and\ntext-guided editing.", "AI": {"tldr": "APS enables efficient posterior sampling using pretrained discrete diffusion models without retraining, overcoming limitations of existing methods through quantized expectation and anchored remasking techniques.", "motivation": "To enable posterior sampling from noisy measurements using pretrained discrete diffusion models without task-specific retraining, addressing challenges in existing discrete diffusion approaches.", "method": "Anchored Posterior Sampling (APS) with two key innovations: quantized expectation for gradient-like guidance in discrete embedding space, and anchored remasking for adaptive decoding.", "result": "Achieves state-of-the-art performance among discrete diffusion samplers across linear and nonlinear inverse problems on standard benchmarks, with additional benefits in training-free stylization and text-guided editing.", "conclusion": "APS provides an effective framework for posterior sampling using discrete diffusion foundation models, offering superior performance and practical applications without requiring retraining."}}
{"id": "2510.01878", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01878", "abs": "https://arxiv.org/abs/2510.01878", "authors": ["Sahar Rajabi", "Nayeema Nonta", "Samanvay Vajpayee", "Sirisha Rambhatla"], "title": "Randomized Gradient Subspaces for Efficient Large Language Model Training", "comment": null, "summary": "Training large language models (LLMs) is often bottlenecked by extreme memory\ndemands, with optimizer states dominating the footprint. Recent works mitigates\nthis cost by projecting gradients into low-dimensional subspaces using\nsophisticated update strategies. In this paper, we analyze the dynamics of\ngradient space and its underlying subspaces. We find that while a small\nsubspace captures most gradient energy, a significant portion still resides in\nthe residual bulk; moreover, the influence of the core subspace diminishes over\ntime and in deeper layers. We also observe that the gradient space exhibits\nnear-flat curvature, calling for algorithms that explicitly account for this\ngeometry. Motivated by these insights, we introduce a suite of randomized\nalgorithms, GrassWalk and GrassJump, which exploit subspace and achieve\nstate-of-the-art memory savings while improving performance on LLaMA-1B and\nLLaMA-7B pretraining.", "AI": {"tldr": "The paper analyzes gradient space dynamics in LLM training, finding that while a small subspace captures most gradient energy, significant residual bulk remains and core subspace influence diminishes over time and in deeper layers. The authors introduce GrassWalk and GrassJump algorithms that exploit this subspace structure to achieve state-of-the-art memory savings while improving performance.", "motivation": "Training large language models is bottlenecked by extreme memory demands, particularly from optimizer states. Recent approaches use gradient projection into low-dimensional subspaces, but the dynamics of gradient space and its underlying subspaces need deeper analysis to develop more effective memory-efficient training methods.", "method": "The authors analyze gradient space dynamics and introduce GrassWalk and GrassJump - randomized algorithms that exploit subspace structure. These methods account for the near-flat curvature of gradient space and address the diminishing influence of core subspaces over time and in deeper layers.", "result": "The proposed algorithms achieve state-of-the-art memory savings while improving performance on LLaMA-1B and LLaMA-7B pretraining, demonstrating better efficiency than existing gradient projection methods.", "conclusion": "Understanding gradient space dynamics reveals important insights: significant gradient energy resides in residual bulk, core subspace influence diminishes over time, and gradient space exhibits near-flat curvature. These findings enable the development of more effective memory-efficient training algorithms like GrassWalk and GrassJump."}}
{"id": "2510.02296", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02296", "abs": "https://arxiv.org/abs/2510.02296", "authors": ["Yu-Chien Liao", "Jr-Jen Chen", "Chi-Pin Huang", "Ci-Siang Lin", "Meng-Lin Wu", "Yu-Chiang Frank Wang"], "title": "Continual Personalization for Diffusion Models", "comment": null, "summary": "Updating diffusion models in an incremental setting would be practical in\nreal-world applications yet computationally challenging. We present a novel\nlearning strategy of Concept Neuron Selection (CNS), a simple yet effective\napproach to perform personalization in a continual learning scheme. CNS\nuniquely identifies neurons in diffusion models that are closely related to the\ntarget concepts. In order to mitigate catastrophic forgetting problems while\npreserving zero-shot text-to-image generation ability, CNS finetunes concept\nneurons in an incremental manner and jointly preserves knowledge learned of\nprevious concepts. Evaluation of real-world datasets demonstrates that CNS\nachieves state-of-the-art performance with minimal parameter adjustments,\noutperforming previous methods in both single and multi-concept personalization\nworks. CNS also achieves fusion-free operation, reducing memory storage and\nprocessing time for continual personalization.", "AI": {"tldr": "CNS is a novel continual learning approach for diffusion models that identifies and selectively fine-tunes concept-related neurons to enable incremental personalization while preventing catastrophic forgetting.", "motivation": "Updating diffusion models incrementally is practical for real-world applications but computationally challenging, requiring methods that can personalize models without losing previous knowledge.", "method": "Concept Neuron Selection (CNS) identifies neurons related to target concepts in diffusion models and fine-tunes them incrementally while preserving knowledge from previous concepts through joint training.", "result": "CNS achieves state-of-the-art performance on real-world datasets with minimal parameter adjustments, outperforming previous methods in single and multi-concept personalization while enabling fusion-free operation.", "conclusion": "CNS provides an effective solution for continual personalization of diffusion models, reducing memory storage and processing time while maintaining zero-shot generation capabilities."}}
{"id": "2510.01894", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01894", "abs": "https://arxiv.org/abs/2510.01894", "authors": ["Thomas Gravier", "Thomas Boyer", "Auguste Genovesio"], "title": "Multi-marginal temporal Schr\u00f6dinger Bridge Matching for video generation from unpaired data", "comment": "Under review. Code available at\n  https://github.com/tgravier/MMDSBM-pytorch . Additional experiment materials\n  available at https://mmdsbm.notion.site", "summary": "Many natural dynamic processes -- such as in vivo cellular differentiation or\ndisease progression -- can only be observed through the lens of static sample\nsnapshots. While challenging, reconstructing their temporal evolution to\ndecipher underlying dynamic properties is of major interest to scientific\nresearch. Existing approaches enable data transport along a temporal axis but\nare poorly scalable in high dimension and require restrictive assumptions to be\nmet. To address these issues, we propose \\textit{\\textbf{Multi-Marginal\ntemporal Schr\\\"odinger Bridge Matching}} (\\textbf{MMtSBM}) \\textit{for video\ngeneration from unpaired data}, extending the theoretical guarantees and\nempirical efficiency of Diffusion Schr\\\"odinger Bridge Matching\n(arXiv:archive/2303.16852) by deriving the Iterative Markovian Fitting\nalgorithm to multiple marginals in a novel factorized fashion. Experiments show\nthat MMtSBM retains theoretical properties on toy examples, achieves\nstate-of-the-art performance on real world datasets such as transcriptomic\ntrajectory inference in 100 dimensions, and for the first time recovers\ncouplings and dynamics in very high dimensional image settings. Our work\nestablishes multi-marginal Schr\\\"odinger bridges as a practical and principled\napproach for recovering hidden dynamics from static data.", "AI": {"tldr": "MMtSBM is a novel method for reconstructing temporal dynamics from static snapshots using multi-marginal Schr\u00f6dinger bridges, achieving state-of-the-art performance in high-dimensional settings like transcriptomics and image data.", "motivation": "Many natural dynamic processes can only be observed through static snapshots, making it challenging to reconstruct their temporal evolution. Existing approaches have scalability issues in high dimensions and require restrictive assumptions.", "method": "Extends Diffusion Schr\u00f6dinger Bridge Matching by deriving the Iterative Markovian Fitting algorithm for multiple marginals in a novel factorized fashion, enabling video generation from unpaired data.", "result": "MMtSBM retains theoretical properties on toy examples, achieves state-of-the-art performance on real datasets including transcriptomic trajectory inference in 100 dimensions, and recovers couplings and dynamics in very high dimensional image settings for the first time.", "conclusion": "Multi-marginal Schr\u00f6dinger bridges provide a practical and principled approach for recovering hidden dynamics from static data, establishing MMtSBM as an effective method for temporal evolution reconstruction."}}
{"id": "2510.02300", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02300", "abs": "https://arxiv.org/abs/2510.02300", "authors": ["Runqian Wang", "Yilun Du"], "title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models", "comment": null, "summary": "We introduce Equilibrium Matching (EqM), a generative modeling framework\nbuilt from an equilibrium dynamics perspective. EqM discards the\nnon-equilibrium, time-conditional dynamics in traditional diffusion and\nflow-based generative models and instead learns the equilibrium gradient of an\nimplicit energy landscape. Through this approach, we can adopt an\noptimization-based sampling process at inference time, where samples are\nobtained by gradient descent on the learned landscape with adjustable step\nsizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation\nperformance of diffusion/flow models empirically, achieving an FID of 1.90 on\nImageNet 256$\\times$256. EqM is also theoretically justified to learn and\nsample from the data manifold. Beyond generation, EqM is a flexible framework\nthat naturally handles tasks including partially noised image denoising, OOD\ndetection, and image composition. By replacing time-conditional velocities with\na unified equilibrium landscape, EqM offers a tighter bridge between flow and\nenergy-based models and a simple route to optimization-driven inference.", "AI": {"tldr": "Equilibrium Matching (EqM) is a generative modeling framework that learns the equilibrium gradient of an implicit energy landscape, replacing time-conditional dynamics with optimization-based sampling via gradient descent.", "motivation": "To overcome limitations of traditional diffusion and flow-based models that use non-equilibrium, time-conditional dynamics, by learning from an equilibrium perspective.", "method": "Learns the equilibrium gradient of an implicit energy landscape and uses optimization-based sampling with gradient descent, adjustable step sizes, adaptive optimizers, and adaptive compute at inference.", "result": "Achieves state-of-the-art FID of 1.90 on ImageNet 256\u00d7256, surpassing diffusion/flow models, and handles tasks like denoising, OOD detection, and image composition.", "conclusion": "EqM provides a unified framework that bridges flow and energy-based models, enabling optimization-driven inference and flexible task handling beyond generation."}}
{"id": "2510.01899", "categories": ["cs.LG", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.01899", "abs": "https://arxiv.org/abs/2510.01899", "authors": ["Md Talha Mohsin", "Ismail Abdulrashid"], "title": "Multimodal Foundation Models for Early Disease Detection", "comment": "6 pages", "summary": "Healthcare generates diverse streams of data, including electronic health\nrecords (EHR), medical imaging, genetics, and ongoing monitoring from wearable\ndevices. Traditional diagnostic models frequently analyze these sources in\nisolation, which constrains their capacity to identify cross-modal correlations\nessential for early disease diagnosis. Our research presents a multimodal\nfoundation model that consolidates diverse patient data through an\nattention-based transformer framework. At first, dedicated encoders put each\nmodality into a shared latent space. Then, they combine them using multi-head\nattention and residual normalization. The architecture is made for pretraining\non many tasks, which makes it easy to adapt to new diseases and datasets with\nlittle extra work. We provide an experimental strategy that uses benchmark\ndatasets in oncology, cardiology, and neurology, with the goal of testing early\ndetection tasks. The framework includes data governance and model management\ntools in addition to technological performance to improve transparency,\nreliability, and clinical interpretability. The suggested method works toward a\nsingle foundation model for precision diagnostics, which could improve the\naccuracy of predictions and help doctors make decisions.", "AI": {"tldr": "A multimodal foundation model that integrates diverse healthcare data streams (EHR, imaging, genetics, wearables) using attention-based transformers for improved early disease diagnosis through cross-modal correlation analysis.", "motivation": "Traditional diagnostic models analyze healthcare data sources in isolation, limiting their ability to identify cross-modal correlations essential for early disease diagnosis. Healthcare generates diverse data streams that need integrated analysis.", "method": "Uses dedicated encoders to project each modality into a shared latent space, then combines them using multi-head attention and residual normalization. The transformer architecture supports pretraining on multiple tasks for easy adaptation to new diseases and datasets.", "result": "Experimental evaluation on benchmark datasets in oncology, cardiology, and neurology for early detection tasks. The framework includes data governance and model management tools to improve transparency, reliability, and clinical interpretability.", "conclusion": "The proposed multimodal foundation model works toward unified precision diagnostics, potentially improving prediction accuracy and supporting clinical decision-making through better integration of diverse healthcare data sources."}}
{"id": "2510.01906", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01906", "abs": "https://arxiv.org/abs/2510.01906", "authors": ["Mayur Kishor Shende", "Ole-Christoffer Granmo", "Runar Helin", "Vladimir I. Zadorozhny", "Rishad Shafik"], "title": "A Methodology for Transparent Logic-Based Classification Using a Multi-Task Convolutional Tsetlin Machine", "comment": null, "summary": "The Tsetlin Machine (TM) is a novel machine learning paradigm that employs\nfinite-state automata for learning and utilizes propositional logic to\nrepresent patterns. Due to its simplistic approach, TMs are inherently more\ninterpretable than learning algorithms based on Neural Networks. The\nConvolutional TM has shown comparable performance on various datasets such as\nMNIST, K-MNIST, F-MNIST and CIFAR-2. In this paper, we explore the\napplicability of the TM architecture for large-scale multi-channel (RGB) image\nclassification. We propose a methodology to generate both local interpretations\nand global class representations. The local interpretations can be used to\nexplain the model predictions while the global class representations aggregate\nimportant patterns for each class. These interpretations summarize the\nknowledge captured by the convolutional clauses, which can be visualized as\nimages. We evaluate our methods on MNIST and CelebA datasets, using models that\nachieve 98.5\\% accuracy on MNIST and 86.56\\% F1-score on CelebA (compared to\n88.07\\% for ResNet50) respectively. We show that the TM performs competitively\nto this deep learning model while maintaining its interpretability, even in\nlarge-scale complex training environments. This contributes to a better\nunderstanding of TM clauses and provides insights into how these models can be\napplied to more complex and diverse datasets.", "AI": {"tldr": "The paper explores Tsetlin Machine (TM) for large-scale RGB image classification, proposing methods for local and global interpretability while maintaining competitive performance compared to deep learning models.", "motivation": "To apply the inherently interpretable Tsetlin Machine architecture to complex multi-channel image classification tasks while preserving its interpretability advantages over neural networks.", "method": "Proposed methodology to generate local interpretations (explaining model predictions) and global class representations (aggregating important patterns per class) from convolutional clauses, visualized as images.", "result": "Achieved 98.5% accuracy on MNIST and 86.56% F1-score on CelebA, comparable to ResNet50's 88.07% F1-score, while maintaining interpretability in complex training environments.", "conclusion": "TM performs competitively with deep learning models on complex datasets while providing better interpretability through visualizable clause representations, enabling application to more diverse datasets."}}
{"id": "2510.01910", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01910", "abs": "https://arxiv.org/abs/2510.01910", "authors": ["Zhaoyan Wang", "Zheng Gao", "Arogya Kharel", "In-Young Ko"], "title": "Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under Deficiencies with Iterative Refinement", "comment": "14 pages", "summary": "Graph Neural Networks (GNNs) are widely adopted in Web-related applications,\nserving as a core technique for learning from graph-structured data, such as\ntext-attributed graphs. Yet in real-world scenarios, such graphs exhibit\ndeficiencies that substantially undermine GNN performance. While prior\nGNN-based augmentation studies have explored robustness against individual\nimperfections, a systematic understanding of how graph-native and Large\nLanguage Models (LLMs) enhanced methods behave under compound deficiencies is\nstill missing. Specifically, there has been no comprehensive investigation\ncomparing conventional approaches and recent LLM-on-graph frameworks, leaving\ntheir merits unclear. To fill this gap, we conduct the first empirical study\nthat benchmarks these two lines of methods across diverse graph deficiencies,\nrevealing overlooked vulnerabilities and challenging the assumption that LLM\naugmentation is consistently superior. Building on empirical findings, we\npropose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement\n(RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is\nthe first iterative paradigm that leverages Retrieval-Augmented Generation\n(RAG) to inject retrieval-grounded augmentations by supplying class-consistent,\ndiverse augmentations and enforcing discriminative representations through\niterative graph contrastive learning. It transforms LLM augmentation for graphs\nfrom static signal injection into dynamic refinement. Extensive experiments\ndemonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced\nbaselines, achieving up to 82.43% average improvement.", "AI": {"tldr": "This paper introduces RoGRAD, a novel framework that addresses graph deficiencies through iterative retrieval-augmented contrastive refinement, challenging the assumption that LLM augmentation is always superior and showing significant performance improvements over existing methods.", "motivation": "Current GNNs struggle with real-world graph deficiencies, and there's a lack of systematic understanding of how graph-native methods and LLM-enhanced approaches perform under compound deficiencies. No comprehensive comparison exists between conventional approaches and recent LLM-on-graph frameworks.", "method": "Proposed RoGRAD framework - an iterative paradigm using Retrieval-Augmented Generation (RAG) to inject retrieval-grounded augmentations. It provides class-consistent, diverse augmentations and enforces discriminative representations through iterative graph contrastive learning, transforming LLM augmentation from static signal injection to dynamic refinement.", "result": "Extensive experiments show RoGRAD's superiority over both conventional GNN- and LLM-enhanced baselines, achieving up to 82.43% average improvement. The study reveals overlooked vulnerabilities and challenges the assumption that LLM augmentation is consistently superior.", "conclusion": "RoGRAD represents a significant advancement in graph learning by introducing the first iterative paradigm that dynamically refines graph representations through retrieval-augmented contrastive learning, demonstrating substantial performance gains over existing methods."}}
{"id": "2510.01938", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01938", "abs": "https://arxiv.org/abs/2510.01938", "authors": ["Zhizhong Li", "Sina Sajadmanesh", "Jingtao Li", "Lingjuan Lyu"], "title": "StelLA: Subspace Learning in Low-rank Adaptation using Stiefel Manifold", "comment": "Accepted as a spotlight at NeurIPS 2025", "summary": "Low-rank adaptation (LoRA) has been widely adopted as a parameter-efficient\ntechnique for fine-tuning large-scale pre-trained models. However, it still\nlags behind full fine-tuning in performance, partly due to its insufficient\nexploitation of the geometric structure underlying low-rank manifolds. In this\npaper, we propose a geometry-aware extension of LoRA that uses a three-factor\ndecomposition $U\\!SV^\\top$. Analogous to the structure of singular value\ndecomposition (SVD), it separates the adapter's input and output subspaces, $V$\nand $U$, from the scaling factor $S$. Our method constrains $U$ and $V$ to lie\non the Stiefel manifold, ensuring their orthonormality throughout the training.\nTo optimize on the Stiefel manifold, we employ a flexible and modular geometric\noptimization design that converts any Euclidean optimizer to a Riemannian one.\nIt enables efficient subspace learning while remaining compatible with existing\nfine-tuning pipelines. Empirical results across a wide range of downstream\ntasks, including commonsense reasoning, math and code generation, image\nclassification, and image generation, demonstrate the superior performance of\nour approach against the recent state-of-the-art variants of LoRA. Code is\navailable at https://github.com/SonyResearch/stella.", "AI": {"tldr": "A geometry-aware extension of LoRA that uses a three-factor decomposition with Stiefel manifold constraints to improve parameter-efficient fine-tuning performance.", "motivation": "Standard LoRA lags behind full fine-tuning due to insufficient exploitation of the geometric structure underlying low-rank manifolds.", "method": "Proposes a three-factor decomposition U\u00b7S\u00b7V\u22a4 analogous to SVD, constraining U and V to lie on the Stiefel manifold for orthonormality, with geometric optimization that converts Euclidean optimizers to Riemannian ones.", "result": "Superior performance across commonsense reasoning, math and code generation, image classification, and image generation tasks compared to recent state-of-the-art LoRA variants.", "conclusion": "The geometry-aware approach enables efficient subspace learning while remaining compatible with existing fine-tuning pipelines, demonstrating improved performance over standard LoRA."}}
{"id": "2510.01969", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01969", "abs": "https://arxiv.org/abs/2510.01969", "authors": ["Camilo Andr\u00e9s Garc\u00eda Trillos", "Nicol\u00e1s Garc\u00eda Trillos"], "title": "Lower Bounds on Adversarial Robustness for Multiclass Classification with General Loss Functions", "comment": null, "summary": "We consider adversarially robust classification in a multiclass setting under\narbitrary loss functions and derive dual and barycentric reformulations of the\ncorresponding learner-agnostic robust risk minimization problem. We provide\nexplicit characterizations for important cases such as the cross-entropy loss,\nloss functions with a power form, and the quadratic loss, extending in this way\navailable results for the 0-1 loss. These reformulations enable efficient\ncomputation of sharp lower bounds for adversarial risks and facilitate the\ndesign of robust classifiers beyond the 0-1 loss setting. Our paper uncovers\ninteresting connections between adversarial robustness, $\\alpha$-fair packing\nproblems, and generalized barycenter problems for arbitrary positive measures\nwhere Kullback-Leibler and Tsallis entropies are used as penalties. Our\ntheoretical results are accompanied with illustrative numerical experiments\nwhere we obtain tighter lower bounds for adversarial risks with the\ncross-entropy loss function.", "AI": {"tldr": "The paper develops dual and barycentric reformulations for adversarially robust multiclass classification under arbitrary loss functions, extending beyond the 0-1 loss setting to enable efficient computation of sharp lower bounds for adversarial risks.", "motivation": "To address the limitations of existing adversarial robustness methods that primarily focus on 0-1 loss, by developing a framework that works with arbitrary loss functions including cross-entropy, power form losses, and quadratic loss.", "method": "Derived dual and barycentric reformulations of the robust risk minimization problem, with explicit characterizations for cross-entropy loss, power form losses, and quadratic loss. The approach connects adversarial robustness to \u03b1-fair packing problems and generalized barycenter problems using Kullback-Leibler and Tsallis entropies as penalties.", "result": "The reformulations enable efficient computation of sharp lower bounds for adversarial risks and facilitate the design of robust classifiers beyond the 0-1 loss setting. Numerical experiments demonstrate tighter lower bounds for adversarial risks with cross-entropy loss.", "conclusion": "The paper establishes important connections between adversarial robustness and optimization problems like \u03b1-fair packing and generalized barycenters, providing a comprehensive framework for robust classification under arbitrary loss functions with practical computational benefits."}}
{"id": "2510.01970", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01970", "abs": "https://arxiv.org/abs/2510.01970", "authors": ["Yuanyuan Yao", "Yuhan Shi", "Lu Chen", "Ziquan Fang", "Yunjun Gao", "Leong Hou U", "Yushuai Li", "Tianyi Li"], "title": "Moon: A Modality Conversion-based Efficient Multivariate Time Series Anomaly Detection", "comment": null, "summary": "Multivariate time series (MTS) anomaly detection identifies abnormal patterns\nwhere each timestamp contains multiple variables. Existing MTS anomaly\ndetection methods fall into three categories: reconstruction-based,\nprediction-based, and classifier-based methods. However, these methods face two\nkey challenges: (1) Unsupervised learning methods, such as reconstruction-based\nand prediction-based methods, rely on error thresholds, which can lead to\ninaccuracies; (2) Semi-supervised methods mainly model normal data and often\nunderuse anomaly labels, limiting detection of subtle anomalies;(3) Supervised\nlearning methods, such as classifier-based approaches, often fail to capture\nlocal relationships, incur high computational costs, and are constrained by the\nscarcity of labeled data. To address these limitations, we propose Moon, a\nsupervised modality conversion-based multivariate time series anomaly detection\nframework. Moon enhances the efficiency and accuracy of anomaly detection while\nproviding detailed anomaly analysis reports. First, Moon introduces a novel\nmultivariate Markov Transition Field (MV-MTF) technique to convert numeric time\nseries data into image representations, capturing relationships across\nvariables and timestamps. Since numeric data retains unique patterns that\ncannot be fully captured by image conversion alone, Moon employs a\nMultimodal-CNN to integrate numeric and image data through a feature fusion\nmodel with parameter sharing, enhancing training efficiency. Finally, a\nSHAP-based anomaly explainer identifies key variables contributing to\nanomalies, improving interpretability. Extensive experiments on six real-world\nMTS datasets demonstrate that Moon outperforms six state-of-the-art methods by\nup to 93% in efficiency, 4% in accuracy and, 10.8% in interpretation\nperformance.", "AI": {"tldr": "Moon is a supervised modality conversion framework for multivariate time series anomaly detection that converts numeric time series to images using MV-MTF, integrates both data types via Multimodal-CNN, and provides interpretable anomaly analysis with SHAP.", "motivation": "Existing MTS anomaly detection methods face challenges: unsupervised methods rely on error thresholds causing inaccuracies, semi-supervised methods underuse anomaly labels, and supervised methods fail to capture local relationships with high computational costs and data scarcity.", "method": "Moon converts numeric time series to image representations using multivariate Markov Transition Field (MV-MTF), integrates numeric and image data through Multimodal-CNN with parameter sharing, and uses SHAP-based explainer for anomaly interpretation.", "result": "Extensive experiments on six real-world datasets show Moon outperforms six state-of-the-art methods by up to 93% in efficiency, 4% in accuracy, and 10.8% in interpretation performance.", "conclusion": "Moon effectively addresses limitations of existing MTS anomaly detection methods by combining modality conversion, multimodal integration, and interpretable analysis, achieving superior efficiency, accuracy, and interpretability."}}
{"id": "2510.02084", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02084", "abs": "https://arxiv.org/abs/2510.02084", "authors": ["Kuiye Ding", "Fanda Fan", "Zheya Wang", "Hongxiao Li", "Yifan Wang", "Lei Wang", "Chunjie Luo", "Jianfeng Zhan"], "title": "KAIROS: Unified Training for Universal Non-Autoregressive Time Series Forecasting", "comment": null, "summary": "In the World Wide Web, reliable time series forecasts provide the\nforward-looking signals that drive resource planning, cache placement, and\nanomaly response, enabling platforms to operate efficiently as user behavior\nand content distributions evolve. Compared with other domains, time series\nforecasting for Web applications requires much faster responsiveness to support\nreal-time decision making. We present KAIROS, a non-autoregressive time series\nforecasting framework that directly models segment-level multi-peak\ndistributions. Unlike autoregressive approaches, KAIROS avoids error\naccumulation and achieves just-in-time inference, while improving over existing\nnon-autoregressive models that collapse to over-smoothed predictions. Trained\non the large-scale corpus, KAIROS demonstrates strong zero-shot generalization\non six widely used benchmarks, delivering forecasting performance comparable to\nstate-of-the-art foundation models with similar scale, at a fraction of their\ninference cost. Beyond empirical results, KAIROS highlights the importance of\nnon-autoregressive design as a scalable paradigm for foundation models in time\nseries.", "AI": {"tldr": "KAIROS is a non-autoregressive time series forecasting framework that directly models segment-level multi-peak distributions, achieving fast inference and strong zero-shot performance while avoiding error accumulation common in autoregressive models.", "motivation": "Web applications require fast time series forecasting for real-time decision making in resource planning, cache placement, and anomaly response, but existing approaches suffer from error accumulation (autoregressive) or over-smoothed predictions (non-autoregressive).", "method": "KAIROS uses a non-autoregressive framework that directly models segment-level multi-peak distributions, avoiding the sequential dependency of autoregressive models while preventing the over-smoothing issues of existing non-autoregressive approaches.", "result": "KAIROS demonstrates strong zero-shot generalization on six benchmarks, achieving forecasting performance comparable to state-of-the-art foundation models with similar scale, but at a fraction of their inference cost.", "conclusion": "KAIROS highlights the importance of non-autoregressive design as a scalable paradigm for foundation models in time series, offering efficient and accurate forecasting for web applications."}}
{"id": "2510.01987", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01987", "abs": "https://arxiv.org/abs/2510.01987", "authors": ["Samuel Maddock", "Graham Cormode", "Carsten Maple"], "title": "Private Federated Multiclass Post-hoc Calibration", "comment": null, "summary": "Calibrating machine learning models so that predicted probabilities better\nreflect the true outcome frequencies is crucial for reliable decision-making\nacross many applications. In Federated Learning (FL), the goal is to train a\nglobal model on data which is distributed across multiple clients and cannot be\ncentralized due to privacy concerns. FL is applied in key areas such as\nhealthcare and finance where calibration is strongly required, yet federated\nprivate calibration has been largely overlooked. This work introduces the\nintegration of post-hoc model calibration techniques within FL. Specifically,\nwe transfer traditional centralized calibration methods such as histogram\nbinning and temperature scaling into federated environments and define new\nmethods to operate them under strong client heterogeneity. We study (1) a\nfederated setting and (2) a user-level Differential Privacy (DP) setting and\ndemonstrate how both federation and DP impacts calibration accuracy. We propose\nstrategies to mitigate degradation commonly observed under heterogeneity and\nour findings highlight that our federated temperature scaling works best for\nDP-FL whereas our weighted binning approach is best when DP is not required.", "AI": {"tldr": "This paper introduces federated calibration methods for machine learning models in FL settings, adapting centralized techniques like histogram binning and temperature scaling to handle client heterogeneity and differential privacy constraints.", "motivation": "Calibration is crucial for reliable decision-making in FL applications like healthcare and finance, but federated private calibration has been largely overlooked despite strong requirements in these domains.", "method": "Transferred traditional centralized calibration methods (histogram binning and temperature scaling) into federated environments, developed new methods to handle client heterogeneity, and studied both federated and user-level Differential Privacy settings.", "result": "Found that federation and DP impact calibration accuracy, with federated temperature scaling working best for DP-FL and weighted binning approach performing best when DP is not required. Proposed strategies to mitigate degradation under heterogeneity.", "conclusion": "Successfully integrated post-hoc model calibration techniques within FL, providing effective solutions for both standard federated learning and privacy-preserving settings with differential privacy."}}
{"id": "2510.01988", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01988", "abs": "https://arxiv.org/abs/2510.01988", "authors": ["Marcin Mo\u017cejko", "Adam Bielecki", "Jurand Pr\u0105dzy\u0144ski", "Marcin Traskowski", "Antoni Janowski", "Karol Jurasz", "Micha\u0142 Kucharczyk", "Hyun-Su Lee", "Marcelo Der Torossian Torres", "Cesar de la Fuente-Nunez", "Paulina Szymczak", "Micha\u0142 Kmicikiewicz", "Ewa Szczurek"], "title": "PepCompass: Navigating peptide embedding spaces using Riemannian Geometry", "comment": null, "summary": "Antimicrobial peptide discovery is challenged by the astronomical size of\npeptide space and the relative scarcity of active peptides. Generative models\nprovide continuous latent \"maps\" of peptide space, but conventionally ignore\ndecoder-induced geometry and rely on flat Euclidean metrics, rendering\nexploration and optimization distorted and inefficient. Prior manifold-based\nremedies assume fixed intrinsic dimensionality, which critically fails in\npractice for peptide data. Here, we introduce PepCompass, a geometry-aware\nframework for peptide exploration and optimization. At its core, we define a\nUnion of $\\kappa$-Stable Riemannian Manifolds $\\mathbb{M}^{\\kappa}$, a family\nof decoder-induced manifolds that captures local geometry while ensuring\ncomputational stability. We propose two local exploration methods: Second-Order\nRiemannian Brownian Efficient Sampling, which provides a convergent\nsecond-order approximation to Riemannian Brownian motion, and Mutation\nEnumeration in Tangent Space, which reinterprets tangent directions as discrete\namino-acid substitutions. Combining these yields Local Enumeration Bayesian\nOptimization (LE-BO), an efficient algorithm for local activity optimization.\nFinally, we introduce Potential-minimizing Geodesic Search (PoGS), which\ninterpolates between prototype embeddings along property-enriched geodesics,\nbiasing discovery toward seeds, i.e. peptides with favorable activity. In-vitro\nvalidation confirms the effectiveness of PepCompass: PoGS yields four novel\nseeds, and subsequent optimization with LE-BO discovers 25 highly active\npeptides with broad-spectrum activity, including against resistant bacterial\nstrains. These results demonstrate that geometry-informed exploration provides\na powerful new paradigm for antimicrobial peptide design.", "AI": {"tldr": "PepCompass is a geometry-aware framework for antimicrobial peptide discovery that uses Riemannian manifolds to model peptide space, enabling more efficient exploration and optimization through local search methods and geodesic interpolation.", "motivation": "Current generative models for antimicrobial peptides ignore decoder-induced geometry and rely on flat Euclidean metrics, making exploration distorted and inefficient. The astronomical size of peptide space and scarcity of active peptides requires better geometric understanding.", "method": "PepCompass uses Union of \u03ba-Stable Riemannian Manifolds to capture local geometry. It introduces Second-Order Riemannian Brownian Efficient Sampling and Mutation Enumeration in Tangent Space, combined into Local Enumeration Bayesian Optimization (LE-BO). Also includes Potential-minimizing Geodesic Search (PoGS) for property-enriched geodesic interpolation.", "result": "In-vitro validation showed PoGS yielded four novel seeds, and subsequent LE-BO optimization discovered 25 highly active peptides with broad-spectrum activity, including against resistant bacterial strains.", "conclusion": "Geometry-informed exploration provides a powerful new paradigm for antimicrobial peptide design, overcoming limitations of conventional flat Euclidean approaches."}}
{"id": "2510.02014", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02014", "abs": "https://arxiv.org/abs/2510.02014", "authors": ["Guolei Zeng", "Hezhe Qiao", "Guoguo Ai", "Jinsong Guo", "Guansong Pang"], "title": "Normality Calibration in Semi-supervised Graph Anomaly Detection", "comment": "17 pages", "summary": "Graph anomaly detection (GAD) has attracted growing interest for its crucial\nability to uncover irregular patterns in broad applications. Semi-supervised\nGAD, which assumes a subset of annotated normal nodes available during\ntraining, is among the most widely explored application settings. However, the\nnormality learned by existing semi-supervised GAD methods is limited to the\nlabeled normal nodes, often inclining to overfitting the given patterns. These\ncan lead to high detection errors, such as high false positives. To overcome\nthis limitation, we propose GraphNC , a graph normality calibration framework\nthat leverages both labeled and unlabeled data to calibrate the normality from\na teacher model (a pre-trained semi-supervised GAD model) jointly in anomaly\nscore and node representation spaces. GraphNC includes two main components,\nanomaly score distribution alignment (ScoreDA) and perturbation-based normality\nregularization (NormReg). ScoreDA optimizes the anomaly scores of our model by\naligning them with the score distribution yielded by the teacher model. Due to\naccurate scores in most of the normal nodes and part of the anomaly nodes in\nthe teacher model, the score alignment effectively pulls the anomaly scores of\nthe normal and abnormal classes toward the two ends, resulting in more\nseparable anomaly scores. Nevertheless, there are inaccurate scores from the\nteacher model. To mitigate the misleading by these scores, NormReg is designed\nto regularize the graph normality in the representation space, making the\nrepresentations of normal nodes more compact by minimizing a\nperturbation-guided consistency loss solely on the labeled nodes.", "AI": {"tldr": "GraphNC is a graph normality calibration framework that improves semi-supervised graph anomaly detection by calibrating normality in both anomaly score and representation spaces using labeled and unlabeled data.", "motivation": "Existing semi-supervised GAD methods overfit to labeled normal nodes, leading to high detection errors like false positives. The limited normality learning needs calibration using both labeled and unlabeled data.", "method": "GraphNC uses two components: ScoreDA aligns anomaly score distributions with a teacher model to separate normal/abnormal classes, and NormReg applies perturbation-based consistency regularization on labeled nodes to make normal representations more compact.", "result": "The framework effectively pulls anomaly scores of normal and abnormal classes toward opposite ends, creating more separable scores while making normal node representations more compact.", "conclusion": "GraphNC successfully addresses overfitting in semi-supervised GAD by calibrating normality across score and representation spaces, leading to improved anomaly detection performance with reduced false positives."}}
{"id": "2510.02017", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02017", "abs": "https://arxiv.org/abs/2510.02017", "authors": ["Aida Tayebi", "Ali Khodabandeh Yalabadi", "Mehdi Yazdani-Jahromi", "Ozlem Ozmen Garibay"], "title": "FairContrast: Enhancing Fairness through Contrastive learning and Customized Augmenting Methods on Tabular Data", "comment": "Accepted to NeurIPS 2025 - Reliable ML Workshop", "summary": "As AI systems become more embedded in everyday life, the development of fair\nand unbiased models becomes more critical. Considering the social impact of AI\nsystems is not merely a technical challenge but a moral imperative. As\nevidenced in numerous research studies, learning fair and robust\nrepresentations has proven to be a powerful approach to effectively debiasing\nalgorithms and improving fairness while maintaining essential information for\nprediction tasks. Representation learning frameworks, particularly those that\nutilize self-supervised and contrastive learning, have demonstrated superior\nrobustness and generalizability across various domains. Despite the growing\ninterest in applying these approaches to tabular data, the issue of fairness in\nthese learned representations remains underexplored. In this study, we\nintroduce a contrastive learning framework specifically designed to address\nbias and learn fair representations in tabular datasets. By strategically\nselecting positive pair samples and employing supervised and self-supervised\ncontrastive learning, we significantly reduce bias compared to existing\nstate-of-the-art contrastive learning models for tabular data. Our results\ndemonstrate the efficacy of our approach in mitigating bias with minimum\ntrade-off in accuracy and leveraging the learned fair representations in\nvarious downstream tasks.", "AI": {"tldr": "A contrastive learning framework for learning fair representations in tabular data that reduces bias while maintaining accuracy.", "motivation": "As AI systems become more embedded in everyday life, developing fair and unbiased models is critical. While representation learning has shown promise for debiasing, fairness in tabular data representations remains underexplored.", "method": "A contrastive learning framework with strategic positive pair selection using both supervised and self-supervised contrastive learning specifically designed for tabular datasets.", "result": "Significantly reduces bias compared to existing state-of-the-art contrastive learning models for tabular data with minimum trade-off in accuracy.", "conclusion": "The approach effectively mitigates bias while maintaining essential information for prediction tasks, demonstrating efficacy in various downstream applications."}}
{"id": "2510.02049", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02049", "abs": "https://arxiv.org/abs/2510.02049", "authors": ["Jinshu Huang", "Haibin Su", "Xue-Cheng Tai", "Chunlin Wu"], "title": "Mathematical Modeling and Convergence Analysis of Deep Neural Networks with Dense Layer Connectivities in Deep Learning", "comment": null, "summary": "In deep learning, dense layer connectivity has become a key design principle\nin deep neural networks (DNNs), enabling efficient information flow and strong\nperformance across a range of applications. In this work, we model densely\nconnected DNNs mathematically and analyze their learning problems in the\ndeep-layer limit. For a broad applicability, we present our analysis in a\nframework setting of DNNs with densely connected layers and general non-local\nfeature transformations (with local feature transformations as special cases)\nwithin layers, which is called dense non-local (DNL) framework and includes\nstandard DenseNets and variants as special examples. In this formulation, the\ndensely connected networks are modeled as nonlinear integral equations, in\ncontrast to the ordinary differential equation viewpoint commonly adopted in\nprior works. We study the associated training problems from an optimal control\nperspective and prove convergence results from the network learning problem to\nits continuous-time counterpart. In particular, we show the convergence of\noptimal values and the subsequence convergence of minimizers, using a piecewise\nlinear extension and $\\Gamma$-convergence analysis. Our results provide a\nmathematical foundation for understanding densely connected DNNs and further\nsuggest that such architectures can offer stability of training deep models.", "AI": {"tldr": "The paper provides a mathematical framework for analyzing densely connected deep neural networks (DNNs) using nonlinear integral equations and optimal control theory, showing convergence from discrete to continuous formulations.", "motivation": "To establish a rigorous mathematical foundation for understanding densely connected DNNs and analyze their training stability in the deep-layer limit, moving beyond the ordinary differential equation viewpoint commonly used in prior works.", "method": "Developed a dense non-local (DNL) framework modeling densely connected DNNs as nonlinear integral equations, studied training problems from an optimal control perspective, and used piecewise linear extension with \u0393-convergence analysis.", "result": "Proved convergence of optimal values and subsequence convergence of minimizers from discrete network learning problems to their continuous-time counterparts, demonstrating training stability for deep models.", "conclusion": "Densely connected architectures offer training stability for deep models, and the mathematical framework provides foundations for understanding such networks through integral equations and optimal control theory."}}
{"id": "2510.02180", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02180", "abs": "https://arxiv.org/abs/2510.02180", "authors": ["Silvia Sapora", "Devon Hjelm", "Alexander Toshev", "Omar Attia", "Bogdan Mazoure"], "title": "GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning", "comment": null, "summary": "Inverse Reinforcement Learning aims to recover reward models from expert\ndemonstrations, but traditional methods yield \"black-box\" models that are\ndifficult to interpret and debug. In this work, we introduce GRACE (Generating\nRewards As CodE), a method for using Large Language Models within an\nevolutionary search to reverse-engineer an interpretable, code-based reward\nfunction directly from expert trajectories. The resulting reward function is\nexecutable code that can be inspected and verified. We empirically validate\nGRACE on the BabyAI and AndroidWorld benchmarks, where it efficiently learns\nhighly accurate rewards, even in complex, multi-task settings. Further, we\ndemonstrate that the resulting reward leads to strong policies, compared to\nboth competitive Imitation Learning and online RL approaches with ground-truth\nrewards. Finally, we show that GRACE is able to build complex reward APIs in\nmulti-task setups.", "AI": {"tldr": "GRACE uses LLMs and evolutionary search to generate interpretable code-based reward functions from expert demonstrations, outperforming traditional IRL and imitation learning methods.", "motivation": "Traditional Inverse Reinforcement Learning produces black-box reward models that are difficult to interpret and debug, limiting practical applications.", "method": "GRACE combines Large Language Models with evolutionary search to reverse-engineer executable code-based reward functions directly from expert trajectories.", "result": "GRACE learns highly accurate rewards on BabyAI and AndroidWorld benchmarks, produces strong policies comparable to ground-truth rewards, and builds complex reward APIs in multi-task settings.", "conclusion": "GRACE successfully generates interpretable, code-based reward functions that are executable, verifiable, and lead to strong policy performance across complex environments."}}
{"id": "2510.02056", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.02056", "abs": "https://arxiv.org/abs/2510.02056", "authors": ["Benjamin Wiriyapong", "Oktay Karaku\u015f", "Kirill Sidorov"], "title": "Adaptive Heterogeneous Mixtures of Normalising Flows for Robust Variational Inference", "comment": "2 Figures and 2 tables", "summary": "Normalising-flow variational inference (VI) can approximate complex\nposteriors, yet single-flow models often behave inconsistently across\nqualitatively different distributions. We propose Adaptive Mixture Flow\nVariational Inference (AMF-VI), a heterogeneous mixture of complementary flows\n(MAF, RealNVP, RBIG) trained in two stages: (i) sequential expert training of\nindividual flows, and (ii) adaptive global weight estimation via\nlikelihood-driven updates, without per-sample gating or architectural changes.\nEvaluated on six canonical posterior families of banana, X-shape, two-moons,\nrings, a bimodal, and a five-mode mixture, AMF-VI achieves consistently lower\nnegative log-likelihood than each single-flow baseline and delivers stable\ngains in transport metrics (Wasserstein-2) and maximum mean discrepancy (MDD),\nindicating improved robustness across shapes and modalities. The procedure is\nefficient and architecture-agnostic, incurring minimal overhead relative to\nstandard flow training, and demonstrates that adaptive mixtures of diverse\nflows provide a reliable route to robust VI across diverse posterior families\nwhilst preserving each expert's inductive bias.", "AI": {"tldr": "AMF-VI is a two-stage adaptive mixture of complementary normalizing flows (MAF, RealNVP, RBIG) that achieves robust variational inference across diverse posterior distributions without architectural changes.", "motivation": "Single-flow models behave inconsistently across different posterior distributions, limiting their reliability in variational inference applications.", "method": "Two-stage training: (1) sequential expert training of individual flows, (2) adaptive global weight estimation via likelihood-driven updates without per-sample gating or architectural modifications.", "result": "Consistently lower negative log-likelihood than single-flow baselines across six canonical posterior families, with stable gains in Wasserstein-2 and MMD metrics, indicating improved robustness across shapes and modalities.", "conclusion": "Adaptive mixtures of diverse flows provide reliable robust variational inference while preserving each expert's inductive bias, with minimal computational overhead."}}
{"id": "2510.02202", "categories": ["cs.LG", "cs.AI", "68Txx"], "pdf": "https://arxiv.org/pdf/2510.02202", "abs": "https://arxiv.org/abs/2510.02202", "authors": ["Matthew A. Reyna", "Zuzana Koscova", "Jan Pavlus", "Soheil Saghafi", "James Weigle", "Andoni Elola", "Salman Seyedi", "Kiersten Campbell", "Qiao Li", "Ali Bahrami Rad", "Ant\u00f4nio H. Ribeiro", "Antonio Luiz P. Ribeiro", "Reza Sameni", "Gari D. Clifford"], "title": "Detection of Chagas Disease from the ECG: The George B. Moody PhysioNet Challenge 2025", "comment": "13 pages, 2 figures", "summary": "Objective: Chagas disease is a parasitic infection that is endemic to South\nAmerica, Central America, and, more recently, the U.S., primarily transmitted\nby insects. Chronic Chagas disease can cause cardiovascular diseases and\ndigestive problems. Serological testing capacities for Chagas disease are\nlimited, but Chagas cardiomyopathy often manifests in ECGs, providing an\nopportunity to prioritize patients for testing and treatment. Approach: The\nGeorge B. Moody PhysioNet Challenge 2025 invites teams to develop algorithmic\napproaches for identifying Chagas disease from electrocardiograms (ECGs). Main\nresults: This Challenge provides multiple innovations. First, we leveraged\nseveral datasets with labels from patient reports and serological testing,\nprovided a large dataset with weak labels and smaller datasets with strong\nlabels. Second, we augmented the data to support model robustness and\ngeneralizability to unseen data sources. Third, we applied an evaluation metric\nthat captured the local serological testing capacity for Chagas disease to\nframe the machine learning problem as a triage task. Significance: Over 630\nparticipants from 111 teams submitted over 1300 entries during the Challenge,\nrepresenting diverse approaches from academia and industry worldwide.", "AI": {"tldr": "The George B. Moody PhysioNet Challenge 2025 focuses on developing algorithms to identify Chagas disease from ECGs, using a large dataset with weak labels and smaller datasets with strong labels to prioritize patients for serological testing.", "motivation": "Chagas disease is a parasitic infection that can cause cardiovascular diseases and digestive problems. Limited serological testing capacity creates a need for alternative screening methods, and since Chagas cardiomyopathy often manifests in ECGs, this provides an opportunity to prioritize patients for testing and treatment.", "method": "The challenge used multiple datasets with labels from patient reports and serological testing, provided a large dataset with weak labels and smaller datasets with strong labels, augmented data for model robustness, and applied an evaluation metric that captured local serological testing capacity to frame the problem as a triage task.", "result": "Over 630 participants from 111 teams submitted more than 1300 entries during the Challenge, representing diverse approaches from academia and industry worldwide.", "conclusion": "The Challenge successfully developed algorithmic approaches for identifying Chagas disease from ECGs, providing an innovative framework for prioritizing patients for serological testing when testing capacity is limited."}}
{"id": "2510.02073", "categories": ["cs.LG", "physics.bio-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.02073", "abs": "https://arxiv.org/abs/2510.02073", "authors": ["Jens Behrmann", "Maria R. Cervera", "Antoine Wehenkel", "Andrew C. Miller", "Albert Cerussi", "Pranay Jain", "Vivek Venugopal", "Shijie Yan", "Guillermo Sapiro", "Luca Pegolotti", "J\u00f6rn-Henrik Jacobsen"], "title": "Inferring Optical Tissue Properties from Photoplethysmography using Hybrid Amortized Inference", "comment": null, "summary": "Smart wearables enable continuous tracking of established biomarkers such as\nheart rate, heart rate variability, and blood oxygen saturation via\nphotoplethysmography (PPG). Beyond these metrics, PPG waveforms contain richer\nphysiological information, as recent deep learning (DL) studies demonstrate.\nHowever, DL models often rely on features with unclear physiological meaning,\ncreating a tension between predictive power, clinical interpretability, and\nsensor design. We address this gap by introducing PPGen, a biophysical model\nthat relates PPG signals to interpretable physiological and optical parameters.\nBuilding on PPGen, we propose hybrid amortized inference (HAI), enabling fast,\nrobust, and scalable estimation of relevant physiological parameters from PPG\nsignals while correcting for model misspecification. In extensive in-silico\nexperiments, we show that HAI can accurately infer physiological parameters\nunder diverse noise and sensor conditions. Our results illustrate a path toward\nPPG models that retain the fidelity needed for DL-based features while\nsupporting clinical interpretation and informed hardware design.", "AI": {"tldr": "PPGen is a biophysical model that relates PPG signals to interpretable physiological parameters, using hybrid amortized inference (HAI) for robust parameter estimation while maintaining clinical interpretability.", "motivation": "Current deep learning models for PPG analysis rely on features with unclear physiological meaning, creating tension between predictive power, clinical interpretability, and sensor design.", "method": "Developed PPGen biophysical model and hybrid amortized inference (HAI) to enable fast, robust estimation of physiological parameters from PPG signals while correcting for model misspecification.", "result": "HAI accurately inferred physiological parameters under diverse noise and sensor conditions in extensive in-silico experiments.", "conclusion": "The approach provides a path toward PPG models that retain DL-based feature fidelity while supporting clinical interpretation and informed hardware design."}}
{"id": "2510.02212", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02212", "abs": "https://arxiv.org/abs/2510.02212", "authors": ["Hanyang Zhao", "Dawen Liang", "Wenpin Tang", "David Yao", "Nathan Kallus"], "title": "DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning", "comment": null, "summary": "We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified\nframework for training masked diffusion large language models (dLLMs) to reason\nnot only better (furious), but also faster via reinforcement learning (RL). We\nfirst unify the existing baseline approach such as d1 by proposing to train\nsurrogate policies via off-policy RL, whose likelihood is much more tractable\nas an approximation to the true dLLM policy. This naturally motivates a more\naccurate and informative two-stage likelihood approximation combined with\nimportance sampling correction, which leads to generalized RL algorithms with\nbetter sample efficiency and superior task performance. Second, we propose a\nnew direction of joint training efficient samplers/controllers of dLLMs policy.\nVia RL, we incentivize dLLMs' natural multi-token prediction capabilities by\nletting the model learn to adaptively allocate an inference threshold for each\nprompt. By jointly training the sampler, we yield better accuracies with lower\nnumber of function evaluations (NFEs) compared to training the model only,\nobtaining the best performance in improving the Pareto frontier of the\ninference-time compute of dLLMs. We showcase the effectiveness of our pipeline\nby training open source large diffusion language models over benchmark math and\nplanning tasks.", "AI": {"tldr": "DiFFPO is a unified RL framework that trains masked diffusion LLMs to reason better and faster by training surrogate policies and jointly optimizing samplers for adaptive inference thresholds.", "motivation": "To improve both reasoning quality and speed of diffusion large language models through reinforcement learning, addressing the trade-off between performance and computational efficiency.", "method": "Uses off-policy RL to train surrogate policies with tractable likelihood, employs two-stage likelihood approximation with importance sampling, and jointly trains efficient samplers that adaptively allocate inference thresholds per prompt.", "result": "Achieves better sample efficiency, superior task performance, and improved Pareto frontier of inference-time compute with lower number of function evaluations while maintaining accuracy.", "conclusion": "DiFFPO effectively enhances both reasoning capabilities and computational efficiency of diffusion LLMs through unified RL training of policies and samplers."}}
{"id": "2510.02081", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02081", "abs": "https://arxiv.org/abs/2510.02081", "authors": ["Zhaoyi Li", "Jingtao Ding", "Yong Li", "Shihua Li"], "title": "Fine-Tuning Flow Matching via Maximum Likelihood Estimation of Reconstructions", "comment": null, "summary": "Flow Matching (FM) algorithm achieves remarkable results in generative tasks\nespecially in robotic manipulation. Building upon the foundations of diffusion\nmodels, the simulation-free paradigm of FM enables simple and efficient\ntraining, but inherently introduces a train-inference gap. Specifically, we\ncannot assess the model's output during the training phase. In contrast, other\ngenerative models including Variational Autoencoder (VAE), Normalizing Flow and\nGenerative Adversarial Networks (GANs) directly optimize on the reconstruction\nloss. Such a gap is particularly evident in scenarios that demand high\nprecision, such as robotic manipulation. Moreover, we show that FM's\nover-pursuit of straight predefined paths may introduce some serious problems\nsuch as stiffness into the system. These motivate us to fine-tune FM via\nMaximum Likelihood Estimation of reconstructions - an approach made feasible by\nFM's underlying smooth ODE formulation, in contrast to the stochastic\ndifferential equations (SDEs) used in diffusion models. This paper first\ntheoretically analyzes the relation between training loss and inference error\nin FM. Then we propose a method of fine-tuning FM via Maximum Likelihood\nEstimation of reconstructions, which includes both straightforward fine-tuning\nand residual-based fine-tuning approaches. Furthermore, through specifically\ndesigned architectures, the residual-based fine-tuning can incorporate the\ncontraction property into the model, which is crucial for the model's\nrobustness and interpretability. Experimental results in image generation and\nrobotic manipulation verify that our method reliably improves the inference\nperformance of FM.", "AI": {"tldr": "The paper proposes a fine-tuning method for Flow Matching (FM) using Maximum Likelihood Estimation to address the train-inference gap and improve precision in generative tasks, particularly robotic manipulation.", "motivation": "Flow Matching has a train-inference gap where model output cannot be assessed during training, unlike other generative models. This gap is problematic in precision-demanding scenarios like robotic manipulation, and FM's pursuit of straight paths can introduce stiffness issues.", "method": "Fine-tuning FM via Maximum Likelihood Estimation of reconstructions, including straightforward fine-tuning and residual-based fine-tuning approaches. The residual-based approach incorporates contraction properties through specialized architectures for better robustness.", "result": "Experimental results in image generation and robotic manipulation show that the proposed method reliably improves FM's inference performance.", "conclusion": "The fine-tuning approach via Maximum Likelihood Estimation effectively bridges the train-inference gap in Flow Matching and enhances performance in precision-critical applications."}}
{"id": "2510.02096", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02096", "abs": "https://arxiv.org/abs/2510.02096", "authors": ["Damian Falk", "Konstantin Sch\u00fcrholt", "Konstantinos Tzevelekakis", "L\u00e9o Meynent", "Damian Borth"], "title": "Learning Model Representations Using Publicly Available Model Hubs", "comment": null, "summary": "The weights of neural networks have emerged as a novel data modality, giving\nrise to the field of weight space learning. A central challenge in this area is\nthat learning meaningful representations of weights typically requires large,\ncarefully constructed collections of trained models, typically referred to as\nmodel zoos. These model zoos are often trained ad-hoc, requiring large\ncomputational resources, constraining the learned weight space representations\nin scale and flexibility. In this work, we drop this requirement by training a\nweight space learning backbone on arbitrary models downloaded from large,\nunstructured model repositories such as Hugging Face. Unlike curated model\nzoos, these repositories contain highly heterogeneous models: they vary in\narchitecture and dataset, and are largely undocumented. To address the\nmethodological challenges posed by such heterogeneity, we propose a new weight\nspace backbone designed to handle unstructured model populations. We\ndemonstrate that weight space representations trained on models from Hugging\nFace achieve strong performance, often outperforming backbones trained on\nlaboratory-generated model zoos. Finally, we show that the diversity of the\nmodel weights in our training set allows our weight space model to generalize\nto unseen data modalities. By demonstrating that high-quality weight space\nrepresentations can be learned in the wild, we show that curated model zoos are\nnot indispensable, thereby overcoming a strong limitation currently faced by\nthe weight space learning community.", "AI": {"tldr": "The paper proposes a weight space learning backbone that can be trained on arbitrary, heterogeneous models from unstructured repositories like Hugging Face, eliminating the need for curated model zoos.", "motivation": "Current weight space learning requires large, carefully constructed model zoos that are computationally expensive to create, limiting scale and flexibility. The authors aim to overcome this limitation by learning from unstructured model repositories.", "method": "Proposed a new weight space backbone designed to handle unstructured model populations with varying architectures, datasets, and documentation. Trained on heterogeneous models from Hugging Face.", "result": "Weight space representations trained on Hugging Face models achieve strong performance, often outperforming backbones trained on laboratory-generated model zoos. The diversity enables generalization to unseen data modalities.", "conclusion": "Curated model zoos are not indispensable for weight space learning. High-quality representations can be learned from unstructured repositories, overcoming a major limitation in the field."}}
{"id": "2510.02245", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02245", "abs": "https://arxiv.org/abs/2510.02245", "authors": ["Runzhe Zhan", "Yafu Li", "Zhi Wang", "Xiaoye Qu", "Dongrui Liu", "Jing Shao", "Derek F. Wong", "Yu Cheng"], "title": "ExGRPO: Learning to Reason from Experience", "comment": null, "summary": "Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm\nfor improving the reasoning ability of large language models. However, standard\non-policy training discards rollout experiences after a single update, leading\nto computational inefficiency and instability. While prior work on RL has\nhighlighted the benefits of reusing past experience, the role of experience\ncharacteristics in shaping learning dynamics of large reasoning models remains\nunderexplored. In this paper, we are the first to investigate what makes a\nreasoning experience valuable and identify rollout correctness and entropy as\neffective indicators of experience value. Based on these insights, we propose\nExGRPO (Experiential Group Relative Policy Optimization), a framework that\norganizes and prioritizes valuable experiences, and employs a mixed-policy\nobjective to balance exploration with experience exploitation. Experiments on\nfive backbone models (1.5B-8B parameters) show that ExGRPO consistently\nimproves reasoning performance on mathematical/general benchmarks, with an\naverage gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO\nstabilizes training on both stronger and weaker models where on-policy methods\nfail. These results highlight principled experience management as a key\ningredient for efficient and scalable RLVR.", "AI": {"tldr": "ExGRPO improves RLVR efficiency by identifying valuable experiences through correctness and entropy metrics, organizing them into groups, and using a mixed-policy objective to balance exploration with experience reuse.", "motivation": "Standard on-policy RLVR training discards experiences after single use, leading to computational inefficiency and instability. The value of reasoning experiences and their characteristics in shaping learning dynamics is underexplored.", "method": "Proposed ExGRPO framework that identifies valuable experiences using rollout correctness and entropy metrics, organizes experiences into groups, and employs mixed-policy objective to balance exploration with experience exploitation.", "result": "Experiments on 1.5B-8B parameter models show consistent improvements: +3.5 points on mathematical benchmarks and +7.6 points on general benchmarks over on-policy RLVR. Also stabilizes training on both stronger and weaker models.", "conclusion": "Principled experience management is crucial for efficient and scalable RLVR. ExGRPO demonstrates that organizing and prioritizing valuable experiences significantly improves reasoning performance and training stability."}}
{"id": "2510.02107", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02107", "abs": "https://arxiv.org/abs/2510.02107", "authors": ["Klaus-Rudolf Kladny", "Bernhard Sch\u00f6lkopf", "Michael Muehlebach"], "title": "PENEX: AdaBoost-Inspired Neural Network Regularization", "comment": null, "summary": "AdaBoost sequentially fits so-called weak learners to minimize an exponential\nloss, which penalizes mislabeled data points more severely than other loss\nfunctions like cross-entropy. Paradoxically, AdaBoost generalizes well in\npractice as the number of weak learners grows. In the present work, we\nintroduce Penalized Exponential Loss (PENEX), a new formulation of the\nmulti-class exponential loss that is theoretically grounded and, in contrast to\nthe existing formulation, amenable to optimization via first-order methods. We\ndemonstrate both empirically and theoretically that PENEX implicitly maximizes\nmargins of data points. Also, we show that gradient increments on PENEX\nimplicitly parameterize weak learners in the boosting framework. Across\ncomputer vision and language tasks, we show that PENEX exhibits a regularizing\neffect often better than established methods with similar computational cost.\nOur results highlight PENEX's potential as an AdaBoost-inspired alternative for\neffective training and fine-tuning of deep neural networks.", "AI": {"tldr": "PENEX is a new multi-class exponential loss formulation that enables first-order optimization, implicitly maximizes margins, and shows strong regularization effects comparable to AdaBoost but with better computational efficiency.", "motivation": "AdaBoost uses exponential loss that penalizes misclassifications heavily but generalizes well, however existing formulations are not amenable to first-order optimization methods.", "method": "Proposed Penalized Exponential Loss (PENEX) - a theoretically grounded multi-class exponential loss formulation that can be optimized via first-order methods and implicitly maximizes margins.", "result": "PENEX shows regularizing effects often better than established methods with similar computational cost across computer vision and language tasks, and gradient increments implicitly parameterize weak learners.", "conclusion": "PENEX has potential as an AdaBoost-inspired alternative for effective training and fine-tuning of deep neural networks with good regularization properties."}}
{"id": "2510.02115", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02115", "abs": "https://arxiv.org/abs/2510.02115", "authors": ["Milad Firoozeh", "Nader Dashti", "Mohammad Ali Hatefi"], "title": "Hybrid Deep Learning Modeling Approach to Predict Natural Gas Consumption of Home Subscribers on Limited Data", "comment": null, "summary": "Today, natural gas, as a clean fuel and the best alternative to crude oil,\ncovers a significant part of global demand. Iran is one of the largest\ncountries with energy resources and in terms of gas is the second-largest\ncountry in the world. But, due to the increase in population and energy\nconsumption, it faces problems such as pressure drops and gas outages yearly in\ncold seasons and therefore it is necessary to control gas consumption,\nespecially in the residential sector, which has the largest share in Iran. This\nstudy aims to analyze and predict gas consumption for residential customers in\nZanjan province, Iran, using machine learning models, including LSTM, GRU, and\na hybrid BiLSTM-XGBoost model. The dataset consists of gas consumption and\nmeteorology data collected over six years, from 2017 to 2022. The models were\ntrained and evaluated based on their ability to accurately predict consumption\npatterns. The results indicate that the hybrid BiLSTM-XGBoost model\noutperformed the other models in terms of accuracy, with lower Root Mean\nSquared Error (RMSE), Mean Absolute Percentage Error (MAPE) values, and Mean\nPercentage Error (MPE). Additionally, the Hybrid model demonstrated robust\nperformance, particularly in scenarios with limited data. The findings suggest\nthat machine learning approaches, particularly hybrid models, can be\neffectively utilized to manage and predict gas consumption, contributing to\nmore efficient resource management and reducing seasonal shortages. This study\nhighlights the importance of incorporating geographical and climatic factors in\npredictive modeling, as these significantly influence gas usage across\ndifferent regions.", "AI": {"tldr": "This study analyzes and predicts residential gas consumption in Zanjan province, Iran using machine learning models including LSTM, GRU, and a hybrid BiLSTM-XGBoost model, with the hybrid model showing superior performance.", "motivation": "Iran faces gas pressure drops and outages during cold seasons due to population growth and high energy consumption, particularly in the residential sector which has the largest consumption share. There is a need to control gas consumption through accurate prediction.", "method": "Used machine learning models (LSTM, GRU, and hybrid BiLSTM-XGBoost) trained on six years of gas consumption and meteorological data (2017-2022) to predict residential gas consumption patterns.", "result": "The hybrid BiLSTM-XGBoost model outperformed other models with lower RMSE, MAPE, and MPE values. It demonstrated robust performance especially in limited data scenarios.", "conclusion": "Machine learning approaches, particularly hybrid models, can effectively manage and predict gas consumption for better resource management and reduced seasonal shortages. Geographical and climatic factors significantly influence gas usage and should be incorporated in predictive modeling."}}
{"id": "2510.02116", "categories": ["cs.LG", "cs.DB", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.02116", "abs": "https://arxiv.org/abs/2510.02116", "authors": ["John N. Daras"], "title": "Ensemble Threshold Calibration for Stable Sensitivity Control", "comment": "10 pages, 6 tables", "summary": "Precise recall control is critical in large-scale spatial conflation and\nentity-matching tasks, where missing even a few true matches can break\ndownstream analytics, while excessive manual review inflates cost. Classical\nconfidence-interval cuts such as Clopper-Pearson or Wilson provide lower bounds\non recall, but they routinely overshoot the target by several percentage points\nand exhibit high run-to-run variance under skewed score distributions. We\npresent an end-to-end framework that achieves exact recall with sub-percent\nvariance over tens of millions of geometry pairs, while remaining TPU-friendly.\nOur pipeline starts with an equigrid bounding-box filter and compressed sparse\nrow (CSR) candidate representation, reducing pair enumeration by two orders of\nmagnitude. A deterministic xxHash bootstrap sample trains a lightweight neural\nranker; its scores are propagated to all remaining pairs via a single forward\npass and used to construct a reproducible, score-decile-stratified calibration\nset. Four complementary threshold estimators - Clopper-Pearson, Jeffreys,\nWilson, and an exact quantile - are aggregated via inverse-variance weighting,\nthen fused across nine independent subsamples. This ensemble reduces threshold\nvariance compared to any single method. Evaluated on two real cadastral\ndatasets (approximately 6.31M and 67.34M pairs), our approach consistently hits\na recall target within a small error, decreases redundant verifications\nrelative to other calibrations, and runs end-to-end on a single TPU v3 core.", "AI": {"tldr": "An end-to-end framework for precise recall control in spatial conflation tasks, achieving exact recall with sub-percent variance through ensemble threshold estimation and TPU-friendly processing.", "motivation": "Precise recall control is critical in large-scale spatial conflation where missing true matches breaks downstream analytics, while classical confidence-interval methods overshoot targets and have high variance under skewed score distributions.", "method": "Uses equigrid bounding-box filter and CSR candidate representation to reduce pair enumeration, trains neural ranker via deterministic bootstrap, constructs stratified calibration set, and aggregates four threshold estimators via inverse-variance weighting across multiple subsamples.", "result": "Consistently hits recall targets within small error, decreases redundant verifications, and runs end-to-end on single TPU v3 core for datasets with 6.31M and 67.34M pairs.", "conclusion": "The ensemble approach reduces threshold variance compared to single methods and provides reliable recall control for large-scale spatial matching tasks."}}
{"id": "2510.02117", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.02117", "abs": "https://arxiv.org/abs/2510.02117", "authors": ["Samhita Pal", "James O'quinn", "Kaveh Aryan", "Heather Pua", "James P. Long", "Amir Asiaee"], "title": "DAG DECORation: Continuous Optimization for Structure Learning under Hidden Confounding", "comment": null, "summary": "We study structure learning for linear Gaussian SEMs in the presence of\nlatent confounding. Existing continuous methods excel when errors are\nindependent, while deconfounding-first pipelines rely on pervasive factor\nstructure or nonlinearity. We propose \\textsc{DECOR}, a single likelihood-based\nand fully differentiable estimator that jointly learns a DAG and a correlated\nnoise model. Our theory gives simple sufficient conditions for global parameter\nidentifiability: if the mixed graph is bow free and the noise covariance has a\nuniform eigenvalue margin, then the map from $(\\B,\\OmegaMat)$ to the\nobservational covariance is injective, so both the directed structure and the\nnoise are uniquely determined. The estimator alternates a smooth-acyclic graph\nupdate with a convex noise update and can include a light bow complementarity\npenalty or a post hoc reconciliation step. On synthetic benchmarks that vary\nconfounding density, graph density, latent rank, and dimension with $n<p$,\n\\textsc{DECOR} matches or outperforms strong baselines and is especially robust\nwhen confounding is non-pervasive, while remaining competitive under\npervasiveness.", "AI": {"tldr": "DECOR is a differentiable estimator that jointly learns DAGs and correlated noise models for linear Gaussian SEMs with latent confounding, providing identifiability under bow-free graphs and uniform eigenvalue conditions.", "motivation": "Existing methods struggle with latent confounding in linear Gaussian SEMs - continuous methods need independent errors, while deconfounding-first approaches require pervasive factor structure or nonlinearity.", "method": "DECOR alternates between smooth-acyclic graph updates and convex noise updates, can include bow complementarity penalty or post hoc reconciliation, and jointly estimates DAG structure and correlated noise.", "result": "DECOR matches or outperforms baselines on synthetic benchmarks across varying confounding density, graph density, latent rank, and dimension, especially robust under non-pervasive confounding.", "conclusion": "The method provides global parameter identifiability under bow-free graphs with uniform eigenvalue margins, uniquely determining both directed structure and noise covariance."}}
{"id": "2510.02142", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.02142", "abs": "https://arxiv.org/abs/2510.02142", "authors": ["Lena Podina", "Christina Humer", "Alexandre Duval", "Victor Schmidt", "Ali Ramlaoui", "Shahana Chatterjee", "Yoshua Bengio", "Alex Hernandez-Garcia", "David Rolnick", "F\u00e9lix Therrien"], "title": "Catalyst GFlowNet for electrocatalyst design: A hydrogen evolution reaction case study", "comment": "5 pages, 2 figures. Accepted to NeurIPS AI for Materials Workshop\n  2025", "summary": "Efficient and inexpensive energy storage is essential for accelerating the\nadoption of renewable energy and ensuring a stable supply, despite fluctuations\nin sources such as wind and solar. Electrocatalysts play a key role in hydrogen\nenergy storage (HES), allowing the energy to be stored as hydrogen. However,\nthe development of affordable and high-performance catalysts for this process\nremains a significant challenge. We introduce Catalyst GFlowNet, a generative\nmodel that leverages machine learning-based predictors of formation and\nadsorption energy to design crystal surfaces that act as efficient catalysts.\nWe demonstrate the performance of the model through a proof-of-concept\napplication to the hydrogen evolution reaction, a key reaction in HES, for\nwhich we successfully identified platinum as the most efficient known catalyst.\nIn future work, we aim to extend this approach to the oxygen evolution\nreaction, where current optimal catalysts are expensive metal oxides, and open\nthe search space to discover new materials. This generative modeling framework\noffers a promising pathway for accelerating the search for novel and efficient\ncatalysts.", "AI": {"tldr": "Catalyst GFlowNet is a generative model that uses ML predictors to design efficient catalysts for hydrogen energy storage, successfully identifying platinum as the best catalyst for hydrogen evolution reaction.", "motivation": "Need for affordable and high-performance catalysts to enable efficient hydrogen energy storage from renewable sources like wind and solar.", "method": "Uses generative model (Catalyst GFlowNet) with machine learning predictors of formation and adsorption energy to design crystal surfaces as catalysts.", "result": "Successfully identified platinum as the most efficient known catalyst for hydrogen evolution reaction in proof-of-concept application.", "conclusion": "The generative modeling framework offers a promising pathway for accelerating discovery of novel and efficient catalysts, with plans to extend to oxygen evolution reaction."}}
{"id": "2510.02279", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02279", "abs": "https://arxiv.org/abs/2510.02279", "authors": ["Mykyta Ielanskyi", "Kajetan Schweighofer", "Lukas Aichberger", "Sepp Hochreiter"], "title": "Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods for Natural Language Generation", "comment": null, "summary": "Hallucinations are a common issue that undermine the reliability of large\nlanguage models (LLMs). Recent studies have identified a specific subset of\nhallucinations, known as confabulations, which arise due to predictive\nuncertainty of LLMs. To detect confabulations, various methods for estimating\npredictive uncertainty in natural language generation (NLG) have been\ndeveloped. These methods are typically evaluated by correlating uncertainty\nestimates with the correctness of generated text, with question-answering (QA)\ndatasets serving as the standard benchmark. However, commonly used approximate\ncorrectness functions have substantial disagreement between each other and,\nconsequently, in the ranking of the uncertainty estimation methods. This allows\none to inflate the apparent performance of uncertainty estimation methods. We\npropose using several alternative risk indicators for risk correlation\nexperiments that improve robustness of empirical assessment of UE algorithms\nfor NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge\nvariants leads to reducing the evaluation biases. Furthermore, we explore\nstructured tasks as well as out of distribution and perturbation detection\ntasks which provide robust and controllable risk indicators. Finally, we\npropose to use an Elo rating of uncertainty estimation methods to give an\nobjective summarization over extensive evaluation settings.", "AI": {"tldr": "The paper addresses issues in evaluating uncertainty estimation methods for detecting LLM confabulations, proposing more robust evaluation approaches including multiple LLM-as-a-judge variants, structured tasks, and Elo rating systems.", "motivation": "Current evaluation methods for uncertainty estimation in NLG have substantial disagreement and can be manipulated to inflate performance metrics, undermining reliable assessment of confabulation detection methods.", "method": "Proposes using multiple alternative risk indicators, marginalizing over LLM-as-a-judge variants, exploring structured tasks, out-of-distribution detection, and implementing Elo rating systems for comprehensive evaluation.", "result": "The proposed evaluation framework reduces biases and provides more robust assessment of uncertainty estimation algorithms across various settings including QA tasks and perturbation detection.", "conclusion": "A multi-faceted evaluation approach with diverse risk indicators and Elo rating provides more objective and reliable assessment of uncertainty estimation methods for detecting LLM confabulations."}}
{"id": "2510.02148", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02148", "abs": "https://arxiv.org/abs/2510.02148", "authors": ["Jianing Qi", "Hao Tang", "Zhigang Zhu"], "title": "Policy Gradient Guidance Enables Test Time Control", "comment": null, "summary": "We introduce Policy Gradient Guidance (PGG), a simple extension of\nclassifier-free guidance from diffusion models to classical policy gradient\nmethods. PGG augments the policy gradient with an unconditional branch and\ninterpolates conditional and unconditional branches, yielding a test-time\ncontrol knob that modulates behavior without retraining. We provide a\ntheoretical derivation showing that the additional normalization term vanishes\nunder advantage estimation, leading to a clean guided policy gradient update.\nEmpirically, we evaluate PGG on discrete and continuous control benchmarks. We\nfind that conditioning dropout-central to diffusion guidance-offers gains in\nsimple discrete tasks and low sample regimes, but dropout destabilizes\ncontinuous control. Training with modestly larger guidance ($\\gamma>1$)\nconsistently improves stability, sample efficiency, and controllability. Our\nresults show that guidance, previously confined to diffusion policies, can be\nadapted to standard on-policy methods, opening new directions for controllable\nonline reinforcement learning.", "AI": {"tldr": "Policy Gradient Guidance (PGG) extends classifier-free guidance from diffusion models to policy gradient methods, providing a test-time control mechanism without retraining.", "motivation": "To adapt the successful guidance concept from diffusion models to reinforcement learning, enabling controllable behavior modulation in policy gradient methods.", "method": "PGG augments policy gradient with an unconditional branch and interpolates between conditional and unconditional branches, with theoretical derivation showing normalization terms vanish under advantage estimation.", "result": "PGG improves stability, sample efficiency, and controllability in discrete and continuous control benchmarks, with modest guidance (\u03b3>1) providing consistent benefits.", "conclusion": "Guidance can be successfully adapted from diffusion policies to standard on-policy reinforcement learning methods, opening new directions for controllable online RL."}}
{"id": "2510.02149", "categories": ["cs.LG", "math.OC", "stat.ML", "68T05 (Primary), 62L05, 68W27 (Secondary)"], "pdf": "https://arxiv.org/pdf/2510.02149", "abs": "https://arxiv.org/abs/2510.02149", "authors": ["Alexander Ryabchenko", "Wenlong Mou"], "title": "Reinforcement Learning with Action-Triggered Observations", "comment": null, "summary": "We study reinforcement learning problems where state observations are\nstochastically triggered by actions, a constraint common in many real-world\napplications. This framework is formulated as Action-Triggered Sporadically\nTraceable Markov Decision Processes (ATST-MDPs), where each action has a\nspecified probability of triggering a state observation. We derive tailored\nBellman optimality equations for this framework and introduce the\naction-sequence learning paradigm in which agents commit to executing a\nsequence of actions until the next observation arrives. Under the linear MDP\nassumption, value-functions are shown to admit linear representations in an\ninduced action-sequence feature map. Leveraging this structure, we propose\noff-policy estimators with statistical error guarantees for such feature maps\nand introduce ST-LSVI-UCB, a variant of LSVI-UCB adapted for action-triggered\nsettings. ST-LSVI-UCB achieves regret $\\widetilde\nO(\\sqrt{Kd^3(1-\\gamma)^{-3}})$, where $K$ is the number of episodes, $d$ the\nfeature dimension, and $\\gamma$ the discount factor (per-step episode\nnon-termination probability). Crucially, this work establishes the theoretical\nfoundation for learning with sporadic, action-triggered observations while\ndemonstrating that efficient learning remains feasible under such observation\nconstraints.", "AI": {"tldr": "This paper introduces Action-Triggered Sporadically Traceable MDPs (ATST-MDPs) where state observations occur stochastically based on actions, and proposes ST-LSVI-UCB algorithm that achieves efficient learning with sporadic observations.", "motivation": "Many real-world applications have stochastically triggered state observations based on actions, creating a need for reinforcement learning frameworks that can handle such observation constraints.", "method": "Derived Bellman optimality equations for ATST-MDPs, introduced action-sequence learning paradigm, and proposed ST-LSVI-UCB algorithm based on linear MDP assumption with off-policy estimators.", "result": "ST-LSVI-UCB achieves regret O\u0303(\u221aKd\u00b3(1-\u03b3)\u207b\u00b3), where K is episodes, d is feature dimension, and \u03b3 is discount factor, showing efficient learning is possible under action-triggered observation constraints.", "conclusion": "The work establishes theoretical foundations for learning with sporadic, action-triggered observations and demonstrates efficient learning remains feasible under such constraints."}}
{"id": "2510.02174", "categories": ["cs.LG", "math.OC", "math.PR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.02174", "abs": "https://arxiv.org/abs/2510.02174", "authors": ["Stefano Bruno", "Youngsik Hwang", "Jaehyeon An", "Sotirios Sabanis", "Dong-Young Lim"], "title": "Flatness-Aware Stochastic Gradient Langevin Dynamics", "comment": null, "summary": "Generalization in deep learning is closely tied to the pursuit of flat minima\nin the loss landscape, yet classical Stochastic Gradient Langevin Dynamics\n(SGLD) offers no mechanism to bias its dynamics toward such low-curvature\nsolutions. This work introduces Flatness-Aware Stochastic Gradient Langevin\nDynamics (fSGLD), designed to efficiently and provably seek flat minima in\nhigh-dimensional nonconvex optimization problems. At each iteration, fSGLD uses\nthe stochastic gradient evaluated at parameters perturbed by isotropic Gaussian\nnoise, commonly referred to as Random Weight Perturbation (RWP), thereby\noptimizing a randomized-smoothing objective that implicitly captures curvature\ninformation. Leveraging these properties, we prove that the invariant measure\nof fSGLD stays close to a stationary measure concentrated on the global\nminimizers of a loss function regularized by the Hessian trace whenever the\ninverse temperature and the scale of random weight perturbation are properly\ncoupled. This result provides a rigorous theoretical explanation for the\nbenefits of random weight perturbation. In particular, we establish\nnon-asymptotic convergence guarantees in Wasserstein distance with the best\nknown rate and derive an excess-risk bound for the Hessian-trace regularized\nobjective. Extensive experiments on noisy-label and large-scale vision tasks,\nin both training-from-scratch and fine-tuning settings, demonstrate that fSGLD\nachieves superior or comparable generalization and robustness to baseline\nalgorithms while maintaining the computational cost of SGD, about half that of\nSAM. Hessian-spectrum analysis further confirms that fSGLD converges to\nsignificantly flatter minima.", "AI": {"tldr": "fSGLD is a novel optimization method that efficiently seeks flat minima in deep learning by using random weight perturbation to capture curvature information, providing theoretical guarantees and superior generalization with computational efficiency.", "motivation": "Classical SGLD lacks mechanisms to bias optimization toward flat minima, which are crucial for generalization in deep learning. The paper aims to develop a method that can provably seek flat minima while maintaining computational efficiency.", "method": "Flatness-Aware Stochastic Gradient Langevin Dynamics (fSGLD) uses stochastic gradients evaluated at parameters perturbed by isotropic Gaussian noise (Random Weight Perturbation) to optimize a randomized-smoothing objective that implicitly captures curvature information.", "result": "Theoretical analysis shows fSGLD's invariant measure concentrates on global minimizers of a Hessian-trace regularized loss. Experiments demonstrate superior generalization and robustness on noisy-label and vision tasks, with computational cost similar to SGD (about half of SAM), and Hessian-spectrum analysis confirms convergence to significantly flatter minima.", "conclusion": "fSGLD provides a theoretically grounded and computationally efficient approach to finding flat minima, offering rigorous explanation for random weight perturbation benefits and achieving state-of-the-art generalization performance."}}
{"id": "2510.02286", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02286", "abs": "https://arxiv.org/abs/2510.02286", "authors": ["Ruohao Guo", "Afshin Oroojlooy", "Roshan Sridhar", "Miguel Ballesteros", "Alan Ritter", "Dan Roth"], "title": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks", "comment": null, "summary": "Despite recent rapid progress in AI safety, current large language models\nremain vulnerable to adversarial attacks in multi-turn interaction settings,\nwhere attackers strategically adapt their prompts across conversation turns and\npose a more critical yet realistic challenge. Existing approaches that discover\nsafety vulnerabilities either rely on manual red-teaming with human experts or\nemploy automated methods using pre-defined templates and human-curated attack\ndata, with most focusing on single-turn attacks. However, these methods did not\nexplore the vast space of possible multi-turn attacks, failing to consider\nnovel attack trajectories that emerge from complex dialogue dynamics and\nstrategic conversation planning. This gap is particularly critical given recent\nfindings that LLMs exhibit significantly higher vulnerability to multi-turn\nattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy\nreinforcement learning framework integrated with tree search that autonomously\ndiscovers diverse multi-turn attack strategies by treating the dialogue as a\nsequential decision-making problem, enabling systematic exploration without\nmanually curated data. Through extensive experiments, our approach not only\nachieves more than 25.9% higher ASR across 10 target models compared to\nprevious state-of-the-art approaches, but also effectively uncovers new attack\nstrategies by learning optimal dialogue policies that maximize attack success\nacross multiple turns.", "AI": {"tldr": "DialTree-RPO is an RL framework with tree search that autonomously discovers diverse multi-turn attack strategies against LLMs, achieving 25.9% higher attack success rate than previous methods.", "motivation": "Current LLMs remain vulnerable to multi-turn adversarial attacks, with existing methods focusing on single-turn attacks or requiring manual effort, failing to explore the vast space of possible multi-turn attack trajectories.", "method": "An on-policy reinforcement learning framework integrated with tree search that treats dialogue as sequential decision-making, enabling systematic exploration without manually curated data.", "result": "Achieves more than 25.9% higher Attack Success Rate (ASR) across 10 target models compared to previous state-of-the-art approaches, and uncovers new attack strategies.", "conclusion": "The framework effectively discovers diverse multi-turn attack strategies and demonstrates significantly higher vulnerability of LLMs to multi-turn attacks compared to single-turn approaches."}}
{"id": "2510.02297", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02297", "abs": "https://arxiv.org/abs/2510.02297", "authors": ["Wentao Zhang", "Yang Young Lu", "Yuntian Deng"], "title": "Interactive Training: Feedback-Driven Neural Network Optimization", "comment": "EMNLP 2025 Demo", "summary": "Traditional neural network training typically follows fixed, predefined\noptimization recipes, lacking the flexibility to dynamically respond to\ninstabilities or emerging training issues. In this paper, we introduce\nInteractive Training, an open-source framework that enables real-time,\nfeedback-driven intervention during neural network training by human experts or\nautomated AI agents. At its core, Interactive Training uses a control server to\nmediate communication between users or agents and the ongoing training process,\nallowing users to dynamically adjust optimizer hyperparameters, training data,\nand model checkpoints. Through three case studies, we demonstrate that\nInteractive Training achieves superior training stability, reduced sensitivity\nto initial hyperparameters, and improved adaptability to evolving user needs,\npaving the way toward a future training paradigm where AI agents autonomously\nmonitor training logs, proactively resolve instabilities, and optimize training\ndynamics.", "AI": {"tldr": "Interactive Training is a framework enabling real-time human or AI intervention during neural network training through a control server, allowing dynamic adjustments to hyperparameters, data, and checkpoints.", "motivation": "Traditional neural network training follows fixed optimization recipes without flexibility to respond to instabilities or emerging issues during training.", "method": "Uses a control server to mediate communication between users/agents and training process, enabling dynamic adjustment of optimizer hyperparameters, training data, and model checkpoints.", "result": "Achieves superior training stability, reduced sensitivity to initial hyperparameters, and improved adaptability to evolving user needs through three case studies.", "conclusion": "Paves the way for future training paradigm where AI agents autonomously monitor training logs, proactively resolve instabilities, and optimize training dynamics."}}
{"id": "2510.02206", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02206", "abs": "https://arxiv.org/abs/2510.02206", "authors": ["Daniel Gallo Fern\u00e1ndez"], "title": "Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling", "comment": null, "summary": "Sequence-to-sequence models have become central in Artificial Intelligence,\nparticularly following the introduction of the transformer architecture. While\ninitially developed for Natural Language Processing, these models have\ndemonstrated utility across domains, including Computer Vision. Such models\nrequire mechanisms to exchange information along the time dimension, typically\nusing recurrent or self-attention layers. However, self-attention scales\nquadratically with sequence length, limiting its practicality for very long\nsequences.\n  We introduce Poolformer, a sequence-to-sequence model that replaces\nself-attention with recurrent layers and incorporates pooling operations to\nreduce sequence length. Poolformer is defined recursively using SkipBlocks,\nwhich contain residual blocks, a down-pooling layer, a nested SkipBlock, an\nup-pooling layer, and additional residual blocks. We conduct extensive\nexperiments to support our architectural choices.\n  Our results show that pooling greatly accelerates training, improves\nperceptual metrics (FID and IS), and prevents overfitting. Our experiments also\nsuggest that long-range dependencies are handled by deep layers, while shallow\nlayers take care of short-term features.\n  Evaluated on raw audio, which naturally features long sequence lengths,\nPoolformer outperforms state-of-the-art models such as SaShiMi and Mamba.\nFuture directions include applications to text and vision, as well as\nmulti-modal scenarios, where a Poolformer-based LLM could effectively process\ndense representations of images and videos.", "AI": {"tldr": "Poolformer is a sequence-to-sequence model that replaces self-attention with recurrent layers and pooling operations to handle long sequences efficiently, outperforming state-of-the-art models on raw audio data.", "motivation": "Self-attention in sequence-to-sequence models scales quadratically with sequence length, making it impractical for very long sequences. The authors aim to develop a more efficient architecture that can handle long-range dependencies without the computational burden of self-attention.", "method": "Poolformer uses recurrent layers instead of self-attention and incorporates pooling operations to reduce sequence length. It's built with SkipBlocks containing residual blocks, down-pooling layers, nested SkipBlocks, up-pooling layers, and additional residual blocks.", "result": "Poolformer accelerates training, improves perceptual metrics (FID and IS), prevents overfitting, and outperforms state-of-the-art models like SaShiMi and Mamba on raw audio data. Deep layers handle long-range dependencies while shallow layers manage short-term features.", "conclusion": "Poolformer provides an efficient alternative to self-attention-based models for long sequences, with promising results on audio data and potential applications in text, vision, and multi-modal scenarios."}}
{"id": "2510.02209", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02209", "abs": "https://arxiv.org/abs/2510.02209", "authors": ["Yanxu Chen", "Zijun Yao", "Yantao Liu", "Jin Ye", "Jianing Yu", "Lei Hou", "Juanzi Li"], "title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?", "comment": null, "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nas autonomous agents, showing promise in reasoning, tool use, and sequential\ndecision-making. While prior benchmarks have evaluated LLM agents in domains\nsuch as software engineering and scientific discovery, the finance domain\nremains underexplored, despite its direct relevance to economic value and\nhigh-stakes decision-making. Existing financial benchmarks primarily test\nstatic knowledge through question answering, but they fall short of capturing\nthe dynamic and iterative nature of trading. To address this gap, we introduce\nStockBench, a contamination-free benchmark designed to evaluate LLM agents in\nrealistic, multi-month stock trading environments. Agents receive daily market\nsignals -- including prices, fundamentals, and news -- and must make sequential\nbuy, sell, or hold decisions. Performance is assessed using financial metrics\nsuch as cumulative return, maximum drawdown, and the Sortino ratio. Our\nevaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and\nopen-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM\nagents struggle to outperform the simple buy-and-hold baseline, several models\ndemonstrate the potential to deliver higher returns and manage risk more\neffectively. These findings highlight both the challenges and opportunities in\ndeveloping LLM-powered financial agents, showing that excelling at static\nfinancial knowledge tasks does not necessarily translate into successful\ntrading strategies. We release StockBench as an open-source resource to support\nreproducibility and advance future research in this domain.", "AI": {"tldr": "StockBench is a contamination-free benchmark for evaluating LLM agents in realistic stock trading environments, testing their ability to make sequential buy/sell/hold decisions using daily market data.", "motivation": "Existing financial benchmarks primarily test static knowledge through QA but fail to capture the dynamic, iterative nature of trading. The finance domain remains underexplored for LLM agents despite its economic importance.", "method": "Agents receive daily market signals (prices, fundamentals, news) and make sequential trading decisions. Performance is evaluated using financial metrics like cumulative return, maximum drawdown, and Sortino ratio.", "result": "Most LLM agents struggle to outperform buy-and-hold baseline, but several models show potential for higher returns and better risk management. Static financial knowledge doesn't necessarily translate to successful trading.", "conclusion": "StockBench highlights challenges and opportunities in developing LLM-powered financial agents. The benchmark is released as open-source to support reproducibility and advance future research."}}
{"id": "2510.02305", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.02305", "abs": "https://arxiv.org/abs/2510.02305", "authors": ["Tyler Farghly", "Peter Potaptchik", "Samuel Howard", "George Deligiannidis", "Jakiw Pidstrigach"], "title": "Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is Geometry Adaptive", "comment": null, "summary": "Diffusion models have achieved state-of-the-art performance, demonstrating\nremarkable generalisation capabilities across diverse domains. However, the\nmechanisms underpinning these strong capabilities remain only partially\nunderstood. A leading conjecture, based on the manifold hypothesis, attributes\nthis success to their ability to adapt to low-dimensional geometric structure\nwithin the data. This work provides evidence for this conjecture, focusing on\nhow such phenomena could result from the formulation of the learning problem\nthrough score matching. We inspect the role of implicit regularisation by\ninvestigating the effect of smoothing minimisers of the empirical score\nmatching objective. Our theoretical and empirical results confirm that\nsmoothing the score function -- or equivalently, smoothing in the log-density\ndomain -- produces smoothing tangential to the data manifold. In addition, we\nshow that the manifold along which the diffusion model generalises can be\ncontrolled by choosing an appropriate smoothing.", "AI": {"tldr": "This paper provides evidence that diffusion models' success stems from their ability to adapt to low-dimensional geometric structure in data, showing that smoothing the score function produces tangential smoothing along the data manifold.", "motivation": "To understand the mechanisms behind diffusion models' strong generalization capabilities, particularly investigating the conjecture that their success comes from adapting to low-dimensional geometric structure in data.", "method": "Theoretical and empirical investigation of implicit regularization through smoothing minimizers of the empirical score matching objective, examining how smoothing the score function affects generalization.", "result": "Results confirm that smoothing the score function produces smoothing tangential to the data manifold, and that the manifold along which diffusion models generalize can be controlled by choosing appropriate smoothing.", "conclusion": "The study provides evidence supporting the manifold hypothesis for diffusion models, showing that their generalization capabilities are linked to geometric structure adaptation through score function smoothing."}}
{"id": "2510.02215", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02215", "abs": "https://arxiv.org/abs/2510.02215", "authors": ["Mertcan Cokbas", "Ziteng Liu", "Zeyi Tao", "Chengkai Zhang", "Elder Veliz", "Qin Huang", "Ellie Wen", "Huayu Li", "Qiang Jin", "Murat Duman", "Benjamin Au", "Guy Lebanon", "Sagar Chordia"], "title": "C2AL: Cohort-Contrastive Auxiliary Learning for Large-scale Recommendation Systems", "comment": "Submitted to ICLR 2026", "summary": "Training large-scale recommendation models under a single global objective\nimplicitly assumes homogeneity across user populations. However, real-world\ndata are composites of heterogeneous cohorts with distinct conditional\ndistributions. As models increase in scale and complexity and as more data is\nused for training, they become dominated by central distribution patterns,\nneglecting head and tail regions. This imbalance limits the model's learning\nability and can result in inactive attention weights or dead neurons. In this\npaper, we reveal how the attention mechanism can play a key role in\nfactorization machines for shared embedding selection, and propose to address\nthis challenge by analyzing the substructures in the dataset and exposing those\nwith strong distributional contrast through auxiliary learning. Unlike previous\nresearch, which heuristically applies weighted labels or multi-task heads to\nmitigate such biases, we leverage partially conflicting auxiliary labels to\nregularize the shared representation. This approach customizes the learning\nprocess of attention layers to preserve mutual information with minority\ncohorts while improving global performance. We evaluated C2AL on massive\nproduction datasets with billions of data points each for six SOTA models.\nExperiments show that the factorization machine is able to capture fine-grained\nuser-ad interactions using the proposed method, achieving up to a 0.16%\nreduction in normalized entropy overall and delivering gains exceeding 0.30% on\ntargeted minority cohorts.", "AI": {"tldr": "C2AL addresses data heterogeneity in large-scale recommendation models by using attention mechanisms for shared embedding selection and auxiliary learning with conflicting labels to preserve minority cohort information while improving global performance.", "motivation": "Real-world recommendation data contains heterogeneous user cohorts with distinct distributions, but single global objective training causes models to focus on central patterns while neglecting head and tail regions, leading to inactive attention weights and dead neurons.", "method": "Analyze dataset substructures and expose those with strong distributional contrast through auxiliary learning using partially conflicting auxiliary labels to regularize shared representations, customizing attention layer learning to preserve mutual information with minority cohorts.", "result": "Evaluation on massive production datasets with billions of data points across six SOTA models showed factorization machines capture fine-grained user-ad interactions, achieving 0.16% reduction in normalized entropy overall and gains exceeding 0.30% on targeted minority cohorts.", "conclusion": "The proposed C2AL method effectively addresses data heterogeneity in recommendation systems by leveraging attention mechanisms and auxiliary learning to balance global performance with minority cohort preservation."}}
{"id": "2510.02216", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.02216", "abs": "https://arxiv.org/abs/2510.02216", "authors": ["Zeqi Ye", "Minshuo Chen"], "title": "Diffusion Transformers for Imputation: Statistical Efficiency and Uncertainty Quantification", "comment": "49 pages, 4 figures. Accepted as a poster at NeurIPS 2025", "summary": "Imputation methods play a critical role in enhancing the quality of practical\ntime-series data, which often suffer from pervasive missing values. Recently,\ndiffusion-based generative imputation methods have demonstrated remarkable\nsuccess compared to autoregressive and conventional statistical approaches.\nDespite their empirical success, the theoretical understanding of how well\ndiffusion-based models capture complex spatial and temporal dependencies\nbetween the missing values and observed ones remains limited. Our work\naddresses this gap by investigating the statistical efficiency of conditional\ndiffusion transformers for imputation and quantifying the uncertainty in\nmissing values. Specifically, we derive statistical sample complexity bounds\nbased on a novel approximation theory for conditional score functions using\ntransformers, and, through this, construct tight confidence regions for missing\nvalues. Our findings also reveal that the efficiency and accuracy of imputation\nare significantly influenced by the missing patterns. Furthermore, we validate\nthese theoretical insights through simulation and propose a mixed-masking\ntraining strategy to enhance the imputation performance.", "AI": {"tldr": "This paper provides theoretical analysis of diffusion-based time-series imputation methods, deriving statistical bounds and uncertainty quantification for missing values using conditional diffusion transformers.", "motivation": "Despite empirical success of diffusion-based imputation methods, there's limited theoretical understanding of how well they capture complex spatial-temporal dependencies and quantify uncertainty in missing values.", "method": "The authors derive statistical sample complexity bounds using novel approximation theory for conditional score functions with transformers, construct confidence regions for missing values, and propose mixed-masking training strategy.", "result": "The analysis reveals that imputation efficiency and accuracy are significantly influenced by missing patterns, and theoretical insights are validated through simulations.", "conclusion": "The work bridges theoretical understanding with practical performance of diffusion-based imputation methods, providing statistical guarantees and improved training strategies."}}
{"id": "2510.02224", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.02224", "abs": "https://arxiv.org/abs/2510.02224", "authors": ["Ethan Baron", "Boris Oreshkin", "Ruijun Ma", "Hanyu Zhang", "Kari Torkkola", "Michael W. Mahoney", "Andrew Gordon Wilson", "Tatiana Konstantinova"], "title": "Efficiently Generating Correlated Sample Paths from Multi-step Time Series Foundation Models", "comment": null, "summary": "Many time series applications require access to multi-step forecast\ntrajectories in the form of sample paths. Recently, time series foundation\nmodels have leveraged multi-step lookahead predictions to improve the quality\nand efficiency of multi-step forecasts. However, these models only predict\nindependent marginal distributions for each time step, rather than a full joint\npredictive distribution. To generate forecast sample paths with realistic\ncorrelation structures, one typically resorts to autoregressive sampling, which\ncan be extremely expensive. In this paper, we present a copula-based approach\nto efficiently generate accurate, correlated sample paths from existing\nmulti-step time series foundation models in one forward pass. Our copula-based\napproach generates correlated sample paths orders of magnitude faster than\nautoregressive sampling, and it yields improved sample path quality by\nmitigating the snowballing error phenomenon.", "AI": {"tldr": "A copula-based method to efficiently generate correlated sample paths from multi-step time series foundation models in one forward pass, avoiding expensive autoregressive sampling.", "motivation": "Current time series foundation models only predict independent marginal distributions per time step, lacking joint predictive distributions. Autoregressive sampling for correlated paths is computationally expensive.", "method": "Copula-based approach that generates correlated sample paths from existing multi-step time series foundation models in a single forward pass.", "result": "Generates correlated sample paths orders of magnitude faster than autoregressive sampling while improving quality by mitigating snowballing error.", "conclusion": "The copula-based method provides an efficient and accurate alternative to autoregressive sampling for generating correlated forecast trajectories from time series foundation models."}}
{"id": "2510.02228", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02228", "abs": "https://arxiv.org/abs/2510.02228", "authors": ["Maximilian Beck", "Kajetan Schweighofer", "Sebastian B\u00f6ck", "Sebastian Lehner", "Sepp Hochreiter"], "title": "xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity", "comment": "Code and data available at\n  https://github.com/NX-AI/xlstm_scaling_laws", "summary": "Scaling laws play a central role in the success of Large Language Models\n(LLMs), enabling the prediction of model performance relative to compute\nbudgets prior to training. While Transformers have been the dominant\narchitecture, recent alternatives such as xLSTM offer linear complexity with\nrespect to context length while remaining competitive in the billion-parameter\nregime. We conduct a comparative investigation on the scaling behavior of\nTransformers and xLSTM along the following lines, providing insights to guide\nfuture model design and deployment. First, we study the scaling behavior for\nxLSTM in compute-optimal and over-training regimes using both IsoFLOP and\nparametric fit approaches on a wide range of model sizes (80M-7B) and number of\ntraining tokens (2B-2T). Second, we examine the dependence of optimal model\nsizes on context length, a pivotal aspect that was largely ignored in previous\nwork. Finally, we analyze inference-time scaling characteristics. Our findings\nreveal that in typical LLM training and inference scenarios, xLSTM scales\nfavorably compared to Transformers. Importantly, xLSTM's advantage widens as\ntraining and inference contexts grow.", "AI": {"tldr": "xLSTM scales better than Transformers in LLM scenarios, with advantages increasing with longer contexts in both training and inference.", "motivation": "To compare scaling behavior between Transformers and xLSTM architectures, particularly examining how optimal model sizes depend on context length - an aspect previously overlooked.", "method": "Comparative investigation using IsoFLOP and parametric fit approaches across model sizes (80M-7B) and training tokens (2B-2T), analyzing compute-optimal/over-training regimes, context length dependence, and inference-time scaling.", "result": "xLSTM scales favorably compared to Transformers in typical LLM scenarios, with its advantage widening as training and inference contexts grow larger.", "conclusion": "xLSTM's linear complexity with context length provides scaling advantages over Transformers, making it particularly suitable for applications requiring long contexts in both training and inference phases."}}
{"id": "2510.02236", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02236", "abs": "https://arxiv.org/abs/2510.02236", "authors": ["Ricardo Misael Ayala Molina", "Hyame Assem Alameddine", "Makan Pourzandi", "Chadi Assi"], "title": "PUL-Inter-slice Defender: An Anomaly Detection Solution for Distributed Slice Mobility Attacks", "comment": "13 pages, 7 figures, 4 tables, journal paper", "summary": "Network Slices (NSs) are virtual networks operating over a shared physical\ninfrastructure, each designed to meet specific application requirements while\nmaintaining consistent Quality of Service (QoS). In Fifth Generation (5G)\nnetworks, User Equipment (UE) can connect to and seamlessly switch between\nmultiple NSs to access diverse services. However, this flexibility, known as\nInter-Slice Switching (ISS), introduces a potential vulnerability that can be\nexploited to launch Distributed Slice Mobility (DSM) attacks, a form of\nDistributed Denial of Service (DDoS) attack. To secure 5G networks and their\nNSs against DSM attacks, we present in this work, PUL-Inter-Slice Defender; an\nanomaly detection solution that leverages Positive Unlabeled Learning (PUL) and\nincorporates a combination of Long Short-Term Memory Autoencoders and K-Means\nclustering. PUL-Inter-Slice Defender leverages the Third Generation Partnership\nProject (3GPP) key performance indicators and performance measurement counters\nas features for its machine learning models to detect DSM attack variants while\nmaintaining robustness in the presence of contaminated training data. When\nevaluated on data collected from our 5G testbed based on the open-source\nfree5GC and UERANSIM, a UE/ Radio Access Network (RAN) simulator;\nPUL-Inter-Slice Defender achieved F1-scores exceeding 98.50% on training\ndatasets with 10% to 40% attack contamination, consistently outperforming its\ncounterpart Inter-Slice Defender and other PUL based solutions combining\nOne-Class Support Vector Machine (OCSVM) with Random Forest and XGBoost.", "AI": {"tldr": "PUL-Inter-Slice Defender is a machine learning-based anomaly detection system that protects 5G networks from Distributed Slice Mobility (DSM) DDoS attacks using Positive Unlabeled Learning with LSTM Autoencoders and K-Means clustering.", "motivation": "Inter-Slice Switching (ISS) in 5G networks introduces vulnerabilities that can be exploited for Distributed Slice Mobility (DSM) attacks, a form of DDoS attack, requiring robust security solutions.", "method": "Uses Positive Unlabeled Learning (PUL) with LSTM Autoencoders and K-Means clustering, leveraging 3GPP KPIs and performance measurement counters to detect DSM attack variants in contaminated training data.", "result": "Achieved F1-scores exceeding 98.50% on training datasets with 10% to 40% attack contamination, outperforming Inter-Slice Defender and other PUL-based solutions using OCSVM with Random Forest and XGBoost.", "conclusion": "PUL-Inter-Slice Defender provides effective protection against DSM attacks in 5G networks, demonstrating high detection accuracy and robustness even with contaminated training data."}}
{"id": "2510.02239", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.02239", "abs": "https://arxiv.org/abs/2510.02239", "authors": ["Kaja Gruntkowska", "Yassine Maziane", "Zheng Qu", "Peter Richt\u00e1rik"], "title": "Drop-Muon: Update Less, Converge Faster", "comment": null, "summary": "Conventional wisdom in deep learning optimization dictates updating all\nlayers at every step-a principle followed by all recent state-of-the-art\noptimizers such as Muon. In this work, we challenge this assumption, showing\nthat full-network updates can be fundamentally suboptimal, both in theory and\nin practice. We introduce a non-Euclidean Randomized Progressive Training\nmethod-Drop-Muon-a simple yet powerful framework that updates only a subset of\nlayers per step according to a randomized schedule, combining the efficiency of\nprogressive training with layer-specific non-Euclidean updates for top-tier\nperformance. We provide rigorous convergence guarantees under both layer-wise\nsmoothness and layer-wise $(L^0, L^1)$-smoothness, covering deterministic and\nstochastic gradient settings, marking the first such results for progressive\ntraining in the stochastic and non-smooth regime. Our cost analysis further\nreveals that full-network updates are not optimal unless a very specific\nrelationship between layer smoothness constants holds. Through controlled CNN\nexperiments, we empirically demonstrate that Drop-Muon consistently outperforms\nfull-network Muon, achieving the same accuracy up to $1.4\\times$ faster in\nwall-clock time. Together, our results suggest a shift in how large-scale\nmodels can be efficiently trained, challenging the status quo and offering a\nhighly efficient, theoretically grounded alternative to full-network updates.", "AI": {"tldr": "Drop-Muon challenges the conventional wisdom of updating all layers at every step in deep learning optimization. It introduces a randomized progressive training method that updates only a subset of layers per step, achieving faster convergence than full-network updates.", "motivation": "The paper challenges the fundamental assumption that all layers should be updated at every optimization step, arguing that full-network updates can be suboptimal both theoretically and practically. This challenges the status quo followed by state-of-the-art optimizers like Muon.", "method": "Drop-Muon is a non-Euclidean Randomized Progressive Training framework that updates only a subset of layers per step according to a randomized schedule. It combines progressive training efficiency with layer-specific non-Euclidean updates, with convergence guarantees under layer-wise smoothness conditions.", "result": "Empirical results show Drop-Muon consistently outperforms full-network Muon, achieving the same accuracy up to 1.4\u00d7 faster in wall-clock time. Theoretical analysis reveals full-network updates are only optimal under very specific conditions between layer smoothness constants.", "conclusion": "The work suggests a paradigm shift in how large-scale models can be efficiently trained, offering a highly efficient, theoretically grounded alternative to full-network updates that challenges conventional deep learning optimization practices."}}
{"id": "2510.02259", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.chem-ph", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.02259", "abs": "https://arxiv.org/abs/2510.02259", "authors": ["Tobias Kreiman", "Yutong Bai", "Fadi Atieh", "Elizabeth Weaver", "Eric Qu", "Aditi S. Krishnapriyan"], "title": "Transformers Discover Molecular Structure Without Graph Priors", "comment": null, "summary": "Graph Neural Networks (GNNs) are the dominant architecture for molecular\nmachine learning, particularly for molecular property prediction and machine\nlearning interatomic potentials (MLIPs). GNNs perform message passing on\npredefined graphs often induced by a fixed radius cutoff or k-nearest neighbor\nscheme. While this design aligns with the locality present in many molecular\ntasks, a hard-coded graph can limit expressivity due to the fixed receptive\nfield and slows down inference with sparse graph operations. In this work, we\ninvestigate whether pure, unmodified Transformers trained directly on Cartesian\ncoordinates$\\unicode{x2013}$without predefined graphs or physical\npriors$\\unicode{x2013}$can approximate molecular energies and forces. As a\nstarting point for our analysis, we demonstrate how to train a Transformer to\ncompetitive energy and force mean absolute errors under a matched training\ncompute budget, relative to a state-of-the-art equivariant GNN on the OMol25\ndataset. We discover that the Transformer learns physically consistent\npatterns$\\unicode{x2013}$such as attention weights that decay inversely with\ninteratomic distance$\\unicode{x2013}$and flexibly adapts them across different\nmolecular environments due to the absence of hard-coded biases. The use of a\nstandard Transformer also unlocks predictable improvements with respect to\nscaling training resources, consistent with empirical scaling laws observed in\nother domains. Our results demonstrate that many favorable properties of GNNs\ncan emerge adaptively in Transformers, challenging the necessity of hard-coded\ngraph inductive biases and pointing toward standardized, scalable architectures\nfor molecular modeling.", "AI": {"tldr": "Transformers trained directly on Cartesian coordinates can achieve competitive molecular energy and force predictions without predefined graphs or physical priors, challenging the necessity of graph neural networks' hard-coded inductive biases.", "motivation": "GNNs use hard-coded graphs that limit expressivity due to fixed receptive fields and slow inference. The authors investigate whether pure Transformers without predefined graphs or physical priors can effectively approximate molecular energies and forces.", "method": "Train standard Transformers directly on Cartesian coordinates without predefined graphs or physical priors, using a matched training compute budget compared to state-of-the-art equivariant GNNs on the OMol25 dataset.", "result": "Transformers achieve competitive energy and force mean absolute errors. They learn physically consistent patterns like attention weights decaying inversely with interatomic distance, and adapt flexibly across molecular environments. Transformers also show predictable improvements with scaling, consistent with empirical scaling laws.", "conclusion": "Many favorable properties of GNNs can emerge adaptively in Transformers, challenging the necessity of hard-coded graph inductive biases and pointing toward standardized, scalable architectures for molecular modeling."}}
{"id": "2510.02274", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02274", "abs": "https://arxiv.org/abs/2510.02274", "authors": ["Kyoungjun Park", "Yifan Yang", "Changhan Ge", "Lili Qiu", "Shiqi Jiang"], "title": "Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps", "comment": null, "summary": "Modeling radio frequency (RF) signal propagation is essential for\nunderstanding the environment, as RF signals offer valuable insights beyond the\ncapabilities of RGB cameras, which are limited by the visible-light spectrum,\nlens coverage, and occlusions. It is also useful for supporting wireless\ndiagnosis, deployment, and optimization. However, accurately predicting RF\nsignals in complex environments remains a challenge due to interactions with\nobstacles such as absorption and reflection. We introduce Diffusion^2, a\ndiffusion-based approach that uses 3D point clouds to model the propagation of\nRF signals across a wide range of frequencies, from Wi-Fi to millimeter waves.\nTo effectively capture RF-related features from 3D data, we present the RF-3D\nEncoder, which encapsulates the complexities of 3D geometry along with\nsignal-specific details. These features undergo multi-scale embedding to\nsimulate the actual RF signal dissemination process. Our evaluation, based on\nsynthetic and real-world measurements, demonstrates that Diffusion^2 accurately\nestimates the behavior of RF signals in various frequency bands and\nenvironmental conditions, with an error margin of just 1.9 dB and 27x faster\nthan existing methods, marking a significant advancement in the field. Refer to\nhttps://rfvision-project.github.io/ for more information.", "AI": {"tldr": "Diffusion^2 is a diffusion-based method that uses 3D point clouds to model RF signal propagation across multiple frequency bands, achieving high accuracy and 27x speedup over existing methods.", "motivation": "RF signals provide valuable environmental insights beyond RGB cameras, but accurate prediction in complex environments is challenging due to signal interactions with obstacles. This is important for wireless diagnosis, deployment, and optimization.", "method": "Uses diffusion-based approach with 3D point clouds and introduces RF-3D Encoder to capture RF-related features from 3D geometry. Features undergo multi-scale embedding to simulate RF signal dissemination process.", "result": "Achieves accurate RF signal behavior estimation across various frequency bands and environmental conditions with only 1.9 dB error margin and 27x faster than existing methods.", "conclusion": "Diffusion^2 represents a significant advancement in RF signal propagation modeling, enabling more accurate and efficient prediction across wide frequency ranges from Wi-Fi to millimeter waves."}}
{"id": "2510.02278", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02278", "abs": "https://arxiv.org/abs/2510.02278", "authors": ["Fedor Velikonivtsev", "Oleg Platonov", "Gleb Bazhenov", "Liudmila Prokhorenkova"], "title": "Fine-Grained Urban Traffic Forecasting on Metropolis-Scale Road Networks", "comment": null, "summary": "Traffic forecasting on road networks is a complex task of significant\npractical importance that has recently attracted considerable attention from\nthe machine learning community, with spatiotemporal graph neural networks\n(GNNs) becoming the most popular approach. The proper evaluation of traffic\nforecasting methods requires realistic datasets, but current publicly available\nbenchmarks have significant drawbacks, including the absence of information\nabout road connectivity for road graph construction, limited information about\nroad properties, and a relatively small number of road segments that falls\nshort of real-world applications. Further, current datasets mostly contain\ninformation about intercity highways with sparsely located sensors, while city\nroad networks arguably present a more challenging forecasting task due to much\ndenser roads and more complex urban traffic patterns. In this work, we provide\na more complete, realistic, and challenging benchmark for traffic forecasting\nby releasing datasets representing the road networks of two major cities, with\nthe largest containing almost 100,000 road segments (more than a 10-fold\nincrease relative to existing datasets). Our datasets contain rich road\nfeatures and provide fine-grained data about both traffic volume and traffic\nspeed, allowing for building more holistic traffic forecasting systems. We show\nthat most current implementations of neural spatiotemporal models for traffic\nforecasting have problems scaling to datasets of our size. To overcome this\nissue, we propose an alternative approach to neural traffic forecasting that\nuses a GNN without a dedicated module for temporal sequence processing, thus\nachieving much better scalability, while also demonstrating stronger\nforecasting performance. We hope our datasets and modeling insights will serve\nas a valuable resource for research in traffic forecasting.", "AI": {"tldr": "This paper introduces new large-scale urban traffic forecasting datasets and a scalable GNN approach that outperforms existing methods on these challenging benchmarks.", "motivation": "Current traffic forecasting benchmarks have limitations including missing road connectivity information, limited road properties, small scale, and focus on intercity highways rather than complex urban networks.", "method": "Proposed a GNN approach without dedicated temporal sequence processing modules for better scalability, and released new datasets with nearly 100,000 road segments and rich road features.", "result": "The new datasets are 10x larger than existing ones, and the proposed GNN method achieves better scalability and stronger forecasting performance compared to current spatiotemporal models.", "conclusion": "The work provides more realistic and challenging benchmarks for traffic forecasting research and demonstrates that simplified GNN architectures can outperform complex spatiotemporal models in terms of both scalability and performance."}}
{"id": "2510.02302", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02302", "abs": "https://arxiv.org/abs/2510.02302", "authors": ["Qin Shi", "Amber Yijia Zheng", "Qifan Song", "Raymond A. Yeh"], "title": "Knowledge Distillation Detection for Open-weights Models", "comment": "NeurIPS 2025", "summary": "We propose the task of knowledge distillation detection, which aims to\ndetermine whether a student model has been distilled from a given teacher,\nunder a practical setting where only the student's weights and the teacher's\nAPI are available. This problem is motivated by growing concerns about model\nprovenance and unauthorized replication through distillation. To address this\ntask, we introduce a model-agnostic framework that combines data-free input\nsynthesis and statistical score computation for detecting distillation. Our\napproach is applicable to both classification and generative models.\nExperiments on diverse architectures for image classification and text-to-image\ngeneration show that our method improves detection accuracy over the strongest\nbaselines by 59.6% on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-image\ngeneration. The code is available at\nhttps://github.com/shqii1j/distillation_detection.", "AI": {"tldr": "A framework for detecting knowledge distillation by analyzing student model weights and teacher API outputs, applicable to both classification and generative models.", "motivation": "Address growing concerns about model provenance and unauthorized replication through distillation, where student models may be illegally copied from proprietary teacher models.", "method": "Model-agnostic framework combining data-free input synthesis and statistical score computation to detect distillation patterns between student and teacher models.", "result": "Significant improvements in detection accuracy: 59.6% on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-image generation over strongest baselines.", "conclusion": "The proposed method effectively detects knowledge distillation across diverse architectures and tasks, providing a practical solution for model provenance verification."}}
{"id": "2510.02308", "categories": ["cs.LG", "math.DG"], "pdf": "https://arxiv.org/pdf/2510.02308", "abs": "https://arxiv.org/abs/2510.02308", "authors": ["Dhruv Kohli", "Sawyer J. Robertson", "Gal Mishne", "Alexander Cloninger"], "title": "Robust Tangent Space Estimation via Laplacian Eigenvector Gradient Orthogonalization", "comment": null, "summary": "Estimating the tangent spaces of a data manifold is a fundamental problem in\ndata analysis. The standard approach, Local Principal Component Analysis\n(LPCA), struggles in high-noise settings due to a critical trade-off in\nchoosing the neighborhood size. Selecting an optimal size requires prior\nknowledge of the geometric and noise characteristics of the data that are often\nunavailable. In this paper, we propose a spectral method, Laplacian Eigenvector\nGradient Orthogonalization (LEGO), that utilizes the global structure of the\ndata to guide local tangent space estimation. Instead of relying solely on\nlocal neighborhoods, LEGO estimates the tangent space at each data point by\northogonalizing the gradients of low-frequency eigenvectors of the graph\nLaplacian. We provide two theoretical justifications of our method. First, a\ndifferential geometric analysis on a tubular neighborhood of a manifold shows\nthat gradients of the low-frequency Laplacian eigenfunctions of the tube align\nclosely with the manifold's tangent bundle, while an eigenfunction with high\ngradient in directions orthogonal to the manifold lie deeper in the spectrum.\nSecond, a random matrix theoretic analysis also demonstrates that low-frequency\neigenvectors are robust to sub-Gaussian noise. Through comprehensive\nexperiments, we demonstrate that LEGO yields tangent space estimates that are\nsignificantly more robust to noise than those from LPCA, resulting in marked\nimprovements in downstream tasks such as manifold learning, boundary detection,\nand local intrinsic dimension estimation.", "AI": {"tldr": "LEGO is a spectral method that uses global data structure to estimate tangent spaces by orthogonalizing gradients of low-frequency Laplacian eigenvectors, overcoming LPCA's limitations in high-noise settings.", "motivation": "LPCA struggles with high-noise data due to the trade-off in neighborhood size selection, which requires prior knowledge of geometric and noise characteristics that are often unavailable.", "method": "LEGO estimates tangent spaces by orthogonalizing gradients of low-frequency eigenvectors of the graph Laplacian, leveraging global data structure rather than relying solely on local neighborhoods.", "result": "LEGO produces significantly more robust tangent space estimates than LPCA under noise, leading to improvements in manifold learning, boundary detection, and local intrinsic dimension estimation.", "conclusion": "LEGO provides a theoretically grounded and practically effective approach for robust tangent space estimation in high-noise scenarios by utilizing global spectral information."}}
{"id": "2510.02312", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02312", "abs": "https://arxiv.org/abs/2510.02312", "authors": ["Anna Kuzina", "Maciej Pioro", "Paul N. Whatmough", "Babak Ehteshami Bejnordi"], "title": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation", "comment": "Preprint. Under Review", "summary": "Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference.", "AI": {"tldr": "KaVa is a framework that distills knowledge from compressed KV-cache of teacher LLMs into latent-reasoning students via self-distillation, enabling efficient reasoning while maintaining accuracy.", "motivation": "Chain-of-thought reasoning in LLMs incurs high computational costs and memory overhead with redundant stylistic artifacts, while latent reasoning lacks effective supervision for complex natural-language reasoning.", "method": "Uses self-distillation to transfer knowledge from compressed KV-cache of teacher models to latent-reasoning students, leveraging continuous latent tokens to align stepwise KV trajectories.", "result": "Consistently outperforms strong latent baselines, shows smaller degradation from equation-only to natural-language traces, and scales to larger backbones while preserving efficiency.", "conclusion": "Compressed KV-cache distillation serves as a scalable supervision signal for latent reasoning, combining CoT-trained teacher accuracy with latent inference efficiency and deployability."}}
