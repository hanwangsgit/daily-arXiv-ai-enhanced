<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 99]
- [cs.LG](#cs.LG) [Total: 92]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.IT](#cs.IT) [Total: 8]
- [cs.AI](#cs.AI) [Total: 22]
- [eess.SP](#eess.SP) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SimULi: Real-Time LiDAR and Camera Simulation with Unscented Transforms](https://arxiv.org/abs/2510.12901)
*Haithem Turki,Qi Wu,Xin Kang,Janick Martinez Esturo,Shengyu Huang,Ruilong Li,Zan Gojcic,Riccardo de Lutio*

Main category: cs.CV

TL;DR: SimULi is a real-time neural rendering method that supports arbitrary camera models and LiDAR data for autonomous robot testing, addressing cross-sensor inconsistencies and outperforming existing methods in speed and fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing neural rendering methods have limitations in rendering speed, support for complex camera models, and handling multi-sensor data consistently, which hinders their application in rigorous testing of autonomous robots like self-driving vehicles.

Method: Extends 3DGUT with LiDAR support using automated tiling for arbitrary spinning LiDAR models and ray-based culling. Uses factorized 3D Gaussian representation and anchoring strategy to address cross-sensor inconsistencies.

Result: Renders 10-20x faster than ray tracing and 1.5-10x faster than prior rasterization-based methods. Reduces mean camera and depth error by up to 40% compared to existing methods. Matches or exceeds state-of-the-art fidelity on autonomous driving datasets.

Conclusion: SimULi enables real-time, high-fidelity simulation of arbitrary camera models and LiDAR data, making it suitable for rigorous testing of autonomous robots in scenarios beyond real-world data collection.

Abstract: Rigorous testing of autonomous robots, such as self-driving vehicles, is
essential to ensure their safety in real-world deployments. This requires
building high-fidelity simulators to test scenarios beyond those that can be
safely or exhaustively collected in the real-world. Existing neural rendering
methods based on NeRF and 3DGS hold promise but suffer from low rendering
speeds or can only render pinhole camera models, hindering their suitability to
applications that commonly require high-distortion lenses and LiDAR data.
Multi-sensor simulation poses additional challenges as existing methods handle
cross-sensor inconsistencies by favoring the quality of one modality at the
expense of others. To overcome these limitations, we propose SimULi, the first
method capable of rendering arbitrary camera models and LiDAR data in
real-time. Our method extends 3DGUT, which natively supports complex camera
models, with LiDAR support, via an automated tiling strategy for arbitrary
spinning LiDAR models and ray-based culling. To address cross-sensor
inconsistencies, we design a factorized 3D Gaussian representation and
anchoring strategy that reduces mean camera and depth error by up to 40%
compared to existing methods. SimULi renders 10-20x faster than ray tracing
approaches and 1.5-10x faster than prior rasterization-based work (and handles
a wider range of camera models). When evaluated on two widely benchmarked
autonomous driving datasets, SimULi matches or exceeds the fidelity of existing
state-of-the-art methods across numerous camera and LiDAR metrics.

</details>


### [2] [State-Change Learning for Prediction of Future Events in Endoscopic Videos](https://arxiv.org/abs/2510.12904)
*Saurav Sharma,Chinedu Innocent Nwoye,Didier Mutter,Nicolas Padoy*

Main category: cs.CV

TL;DR: SurgFUTR reframes surgical future prediction as state-change learning using a teacher-student architecture with Sinkhorn-Knopp clustering and Action Dynamics module, achieving improved performance across multiple surgical prediction tasks.


<details>
  <summary>Details</summary>
Motivation: Current surgical AI focuses on understanding present events rather than predicting future ones, lacks unified approaches for both short-term and long-term horizons, and struggles with generalization across different surgical contexts.

Method: Proposes SurgFUTR with teacher-student architecture that compresses video clips into state representations using Sinkhorn-Knopp clustering. Teacher learns from current and future clips, while student predicts future states from current videos alone using Action Dynamics module.

Result: Experiments across four datasets and three procedures show consistent improvements. Cross-procedure transfer validates generalizability of the approach.

Conclusion: Reframing surgical future prediction as state-change learning rather than feature forecasting enables better generalization and performance across diverse surgical contexts and prediction horizons.

Abstract: Surgical future prediction, driven by real-time AI analysis of surgical
video, is critical for operating room safety and efficiency. It provides
actionable insights into upcoming events, their timing, and risks-enabling
better resource allocation, timely instrument readiness, and early warnings for
complications (e.g., bleeding, bile duct injury). Despite this need, current
surgical AI research focuses on understanding what is happening rather than
predicting future events. Existing methods target specific tasks in isolation,
lacking unified approaches that span both short-term (action triplets, events)
and long-term horizons (remaining surgery duration, phase transitions). These
methods rely on coarse-grained supervision while fine-grained surgical action
triplets and steps remain underexplored. Furthermore, methods based only on
future feature prediction struggle to generalize across different surgical
contexts and procedures. We address these limits by reframing surgical future
prediction as state-change learning. Rather than forecasting raw observations,
our approach classifies state transitions between current and future timesteps.
We introduce SurgFUTR, implementing this through a teacher-student
architecture. Video clips are compressed into state representations via
Sinkhorn-Knopp clustering; the teacher network learns from both current and
future clips, while the student network predicts future states from current
videos alone, guided by our Action Dynamics (ActDyn) module. We establish
SFPBench with five prediction tasks spanning short-term (triplets, events) and
long-term (remaining surgery duration, phase and step transitions) horizons.
Experiments across four datasets and three procedures show consistent
improvements. Cross-procedure transfer validates generalizability.

</details>


### [3] [Robust Plant Disease Diagnosis with Few Target-Domain Samples](https://arxiv.org/abs/2510.12909)
*Takafumi Nogami,Satoshi Kagiwada,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: TMPS is a metric learning framework that improves plant disease diagnosis robustness by using limited target domain samples to address domain gaps and symptom variability.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning systems for plant disease diagnosis fail to maintain accuracy when deployed in different conditions due to domain gaps and limited training data diversity.

Method: Target-Aware Metric Learning with Prioritized Sampling (TMPS) - a metric learning approach that leverages limited labeled samples from target domains to improve generalization.

Result: TMPS achieves 7.3 and 3.6 point F1 score improvements over combined training and fine-tuning approaches, and 18.7/17.1 point improvements over baseline and conventional metric learning when using just 10 target samples per disease.

Conclusion: TMPS effectively addresses domain adaptation challenges in plant disease diagnosis by strategically using limited target domain data through metric learning.

Abstract: Various deep learning-based systems have been proposed for accurate and
convenient plant disease diagnosis, achieving impressive performance. However,
recent studies show that these systems often fail to maintain diagnostic
accuracy on images captured under different conditions from the training
environment -- an essential criterion for model robustness. Many deep learning
methods have shown high accuracy in plant disease diagnosis. However, they
often struggle to generalize to images taken in conditions that differ from the
training setting. This drop in performance stems from the subtle variability of
disease symptoms and domain gaps -- differences in image context and
environment. The root cause is the limited diversity of training data relative
to task complexity, making even advanced models vulnerable in unseen domains.
To tackle this challenge, we propose a simple yet highly adaptable learning
framework called Target-Aware Metric Learning with Prioritized Sampling (TMPS),
grounded in metric learning. TMPS operates under the assumption of access to a
limited number of labeled samples from the target (deployment) domain and
leverages these samples effectively to improve diagnostic robustness. We assess
TMPS on a large-scale automated plant disease diagnostic task using a dataset
comprising 223,073 leaf images sourced from 23 agricultural fields, spanning 21
diseases and healthy instances across three crop species. By incorporating just
10 target domain samples per disease into training, TMPS surpasses models
trained using the same combined source and target samples, and those fine-tuned
with these target samples after pre-training on source data. It achieves
average macro F1 score improvements of 7.3 and 3.6 points, respectively, and a
remarkable 18.7 and 17.1 point improvement over the baseline and conventional
metric learning.

</details>


### [4] [Unifying Vision-Language Latents for Zero-label Image Caption Enhancement](https://arxiv.org/abs/2510.12931)
*Sanghyun Byun,Jung Ick Guack,Mohanad Odema,Baisub Lee,Jacob Song,Woo Seong Chung*

Main category: cs.CV

TL;DR: ViZer is a zero-label enhancement framework that improves vision-language models' captioning capabilities without requiring labeled data by aligning vision and language representations during training.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models rely heavily on labeled datasets, limiting scalability and leaving vast amounts of unlabeled image data underutilized.

Method: ViZer actively aligns vision and language representation features during training, enabling existing VLMs to generate improved captions without requiring text labels or full retraining.

Result: ViZer shows consistent qualitative improvements on SmolVLM-Base and Qwen2-VL, producing more grounded and descriptive captions than baseline models.

Conclusion: ViZer provides a practical starting point for broader zero-label adaptation in vision-language tasks, demonstrating that automated caption metrics may penalize valuable details absent in reference captions.

Abstract: Vision-language models (VLMs) achieve remarkable performance through
large-scale image-text pretraining. However, their reliance on labeled image
datasets limits scalability and leaves vast amounts of unlabeled image data
underutilized. To address this, we propose Unified Vision-Language Alignment
for Zero-Label Enhancement (ViZer), an enhancement training framework that
enables zero-label learning in image captioning, providing a practical starting
point for broader zero-label adaptation in vision-language tasks. Unlike prior
approaches that rely on human or synthetically annotated datasets, ViZer
actively aligns vision and language representation features during training,
enabling existing VLMs to generate improved captions without requiring text
labels or full retraining. We demonstrate ViZer's advantage in qualitative
evaluation, as automated caption metrics such as CIDEr and BERTScore often
penalize details that are absent in reference captions. Applying ViZer on
SmolVLM-Base and Qwen2-VL, we observe consistent qualitative improvements,
producing captions that are more grounded and descriptive than their baseline.

</details>


### [5] [Epistemic-aware Vision-Language Foundation Model for Fetal Ultrasound Interpretation](https://arxiv.org/abs/2510.12953)
*Xiao He,Huangxuan Zhao,Guojia Wan,Wei Zhou,Yanxing Liu,Juhua Liu,Yongchao Xu,Yong Luo,Dacheng Tao,Bo Du*

Main category: cs.CV

TL;DR: FetalMind is a medical AI system for fetal ultrasound that addresses challenges like multi-view reasoning and disease variability through Salient Epistemic Disentanglement and a large-scale dataset FetalSigma-1M, achieving superior performance in report generation and diagnosis.


<details>
  <summary>Details</summary>
Motivation: Existing medical vision-language models underperform in fetal ultrasound due to challenges like multi-view image reasoning, numerous diseases, and image diversity, creating a gap in specialized fetal ultrasound AI systems.

Method: Proposed Salient Epistemic Disentanglement (SED) that injects expert-curated bipartite graph to decouple view-disease associations and uses reinforcement learning for clinically faithful preference selection. Created FetalSigma-1M dataset with 20K reports from 12 medical centers.

Result: FetalMind outperforms open- and closed-source baselines across all gestational stages with +14% average gains and +61.2% higher accuracy on critical conditions, while being efficient, stable, and scalable.

Conclusion: FetalMind successfully bridges the gap in fetal ultrasound AI by addressing domain-specific challenges through clinical workflow-guided design and large-scale data curation, demonstrating superior performance and clinical alignment.

Abstract: Recent medical vision-language models have shown promise on tasks such as
VQA, report generation, and anomaly detection. However, most are adapted to
structured adult imaging and underperform in fetal ultrasound, which poses
challenges of multi-view image reasoning, numerous diseases, and image
diversity. To bridge this gap, we introduce FetalMind, a medical AI system
tailored to fetal ultrasound for both report generation and diagnosis. Guided
by clinical workflow, we propose Salient Epistemic Disentanglement (SED), which
injects an expert-curated bipartite graph into the model to decouple
view-disease associations and to steer preference selection along clinically
faithful steps via reinforcement learning. This design mitigates variability
across diseases and heterogeneity across views, reducing learning bottlenecks
while aligning the model's inference with obstetric practice. To train
FetalMind at scale, we curate FetalSigma-1M dataset, the first large-scale
fetal ultrasound report corpus, comprising 20K reports from twelve medical
centers, addressing the scarcity of domain data. Extensive experiments show
that FetalMind outperforms open- and closed-source baselines across all
gestational stages, achieving +14% average gains and +61.2% higher accuracy on
critical conditions while remaining efficient, stable, and scalable. Project
Page: https://hexiao0275.github.io/FetalMind.

</details>


### [6] [CADE 2.5 - ZeResFDG: Frequency-Decoupled, Rescaled and Zero-Projected Guidance for SD/SDXL Latent Diffusion Models](https://arxiv.org/abs/2510.12954)
*Denis Rychkovskiy,GPT-5*

Main category: cs.CV

TL;DR: CADE 2.5 introduces ZeResFDG, a sampler-level guidance stack for SD/SDXL latent diffusion models that improves sharpness, prompt adherence, and artifact control through frequency-decoupled guidance, energy rescaling, and zero-projection.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance of SD/SDXL latent diffusion models by improving image quality, sharpness, and prompt adherence without requiring retraining, while maintaining stability and reducing artifacts.

Method: Uses ZeResFDG module with three components: frequency-decoupled guidance, energy rescaling, and zero-projection. Includes spectral EMA with hysteresis switching and QSilk Micrograin Stabilizer for inference-time stabilization.

Result: Improves sharpness, prompt adherence, and artifact control across SD/SDXL samplers at moderate guidance scales. Enhances robustness and produces natural high-frequency micro-texture at high resolutions with minimal overhead.

Conclusion: CADE 2.5 provides an effective training-free solution for enhancing SD/SDXL model performance through improved guidance mechanisms and stabilization techniques, compatible with existing samplers without retraining requirements.

Abstract: We introduce CADE 2.5 (Comfy Adaptive Detail Enhancer), a sampler-level
guidance stack for SD/SDXL latent diffusion models. The central module,
ZeResFDG, unifies (i) frequency-decoupled guidance that reweights low- and
high-frequency components of the guidance signal, (ii) energy rescaling that
matches the per-sample magnitude of the guided prediction to the positive
branch, and (iii) zero-projection that removes the component parallel to the
unconditional direction. A lightweight spectral EMA with hysteresis switches
between a conservative and a detail-seeking mode as structure crystallizes
during sampling. Across SD/SDXL samplers, ZeResFDG improves sharpness, prompt
adherence, and artifact control at moderate guidance scales without any
retraining. In addition, we employ a training-free inference-time stabilizer,
QSilk Micrograin Stabilizer (quantile clamp + depth/edge-gated micro-detail
injection), which improves robustness and yields natural high-frequency
micro-texture at high resolutions with negligible overhead. For completeness we
note that the same rule is compatible with alternative parameterizations (e.g.,
velocity), which we briefly discuss in the Appendix; however, this paper
focuses on SD/SDXL latent diffusion models.

</details>


### [7] [Scope: Selective Cross-modal Orchestration of Visual Perception Experts](https://arxiv.org/abs/2510.12974)
*Tianyu Zhang,Suyuchen Wang,Chao Wang,Juan Rodriguez,Ahmed Masry,Xiangru Jian,Yoshua Bengio,Perouz Taslakian*

Main category: cs.CV

TL;DR: SCOPE is a Mixture-of-Encoders framework that dynamically selects one specialized vision encoder per image-text pair using instance-level routing, outperforming models that use all encoders simultaneously while reducing computation by 24-49%.


<details>
  <summary>Details</summary>
Motivation: Vision-language models benefit from multiple vision encoders, but naively stacking them yields diminishing returns while multiplying inference costs. The goal is to achieve better performance with less computation through intelligent encoder selection.

Method: SCOPE maintains a shared encoder and a pool of routed encoders. A lightweight router uses cross-attention between text prompts and shared visual features to select the optimal encoder from the routed encoders. Training uses dual entropy regularization with auxiliary losses to balance dataset-level load distribution with instance-level routing confidence.

Result: SCOPE with one shared plus one routed encoder outperforms models using all four extra encoders simultaneously, while reducing compute by 24-49%.

Conclusion: Intelligent encoder selection beats brute-force aggregation, challenging the prevailing paradigm in multi-encoder VLMs.

Abstract: Vision-language models (VLMs) benefit from multiple vision encoders, but
naively stacking them yields diminishing returns while multiplying inference
costs. We propose SCOPE, a Mixture-of-Encoders (MoEnc) framework that
dynamically selects one specialized encoder per image-text pair via
instance-level routing, unlike token-level routing in traditional MoE. SCOPE
maintains a shared encoder and a pool of routed encoders. A lightweight router
uses cross-attention between text prompts and shared visual features to select
the optimal encoder from the routed encoders. To train this router, we
introduce dual entropy regularization with auxiliary losses to balance
dataset-level load distribution with instance-level routing confidence.
Remarkably, SCOPE with one shared plus one routed encoder outperforms models
using all four extra encoders simultaneously, while reducing compute by
24-49\%. This demonstrates that intelligent encoder selection beats brute-force
aggregation, challenging the prevailing paradigm in multi-encoder VLMs.

</details>


### [8] [SVAG-Bench: A Large-Scale Benchmark for Multi-Instance Spatio-temporal Video Action Grounding](https://arxiv.org/abs/2510.13016)
*Tanveer Hannan,Shuaicong Wu,Mark Weber,Suprosanna Shit,Jindong Gu,Rajat Koner,Aljoša Ošep,Laura Leal-Taixé,Thomas Seidl*

Main category: cs.CV

TL;DR: The paper introduces SVAG, a new task for joint spatio-temporal video action grounding that requires detecting, tracking, and temporally localizing objects based on natural language action descriptions.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on either coarse-grained action recognition or generic object tracking, but overlook the challenge of jointly detecting and tracking multiple objects according to their actions with temporal grounding.

Method: Proposed SVAGFormer, a baseline framework that adapts state-of-the-art vision language models for joint spatial and temporal grounding, and created SVAG-Bench benchmark with 688 videos and 19,590 annotated records.

Result: Empirical results show existing models perform poorly on SVAG, especially in dense or complex scenes, highlighting limitations in fine-grained object-action reasoning.

Conclusion: The SVAG task reveals significant gaps in current video understanding capabilities and underscores the need for more advanced reasoning over fine-grained object-action interactions in long videos.

Abstract: Understanding fine-grained actions and accurately localizing their
corresponding actors in space and time are fundamental capabilities for
advancing next-generation AI systems, including embodied agents, autonomous
platforms, and human-AI interaction frameworks. Despite recent progress in
video understanding, existing methods predominantly address either
coarse-grained action recognition or generic object tracking, thereby
overlooking the challenge of jointly detecting and tracking multiple objects
according to their actions while grounding them temporally. To address this
gap, we introduce Spatio-temporal Video Action Grounding (SVAG), a novel task
that requires models to simultaneously detect, track, and temporally localize
all referent objects in videos based on natural language descriptions of their
actions. To support this task, we construct SVAG-Bench, a large-scale benchmark
comprising 688 videos, 19,590 annotated records, and 903 unique verbs, covering
a diverse range of objects, actions, and real-world scenes. We further propose
SVAGFormer, a baseline framework that adapts state of the art vision language
models for joint spatial and temporal grounding, and introduce SVAGEval, a
standardized evaluation toolkit for fair and reproducible benchmarking.
Empirical results show that existing models perform poorly on SVAG,
particularly in dense or complex scenes, underscoring the need for more
advanced reasoning over fine-grained object-action interactions in long videos.

</details>


### [9] [SeqBench: Benchmarking Sequential Narrative Generation in Text-to-Video Models](https://arxiv.org/abs/2510.13042)
*Zhengxu Tang,Zizheng Wang,Luning Wang,Zitao Shuai,Chenhao Zhang,Siyu Qian,Yirui Wu,Bohao Wang,Haosong Rao,Zhenyu Yang,Chenwei Wu*

Main category: cs.CV

TL;DR: SeqBench is a new benchmark for evaluating sequential narrative coherence in text-to-video generation, addressing limitations in current models' ability to create logically progressing multi-event narratives.


<details>
  <summary>Details</summary>
Motivation: Current T2V models struggle with generating coherent sequential narratives that require logical progression through multiple events, and existing benchmarks focus mainly on visual quality rather than narrative coherence.

Method: Created SeqBench with 320 prompts across various narrative complexities and 2,560 human-annotated videos from 8 state-of-the-art T2V models. Designed a Dynamic Temporal Graphs (DTG)-based automatic evaluation metric to capture long-range dependencies and temporal ordering efficiently.

Result: The DTG-based metric shows strong correlation with human annotations. Evaluation reveals critical limitations: failure to maintain consistent object states, physically implausible results in multi-object scenarios, and difficulties preserving realistic timing and ordering relationships.

Conclusion: SeqBench provides the first systematic framework for evaluating narrative coherence in T2V generation and offers concrete insights for improving sequential reasoning capabilities in future models.

Abstract: Text-to-video (T2V) generation models have made significant progress in
creating visually appealing videos. However, they struggle with generating
coherent sequential narratives that require logical progression through
multiple events. Existing T2V benchmarks primarily focus on visual quality
metrics but fail to evaluate narrative coherence over extended sequences. To
bridge this gap, we present SeqBench, a comprehensive benchmark for evaluating
sequential narrative coherence in T2V generation. SeqBench includes a carefully
designed dataset of 320 prompts spanning various narrative complexities, with
2,560 human-annotated videos generated from 8 state-of-the-art T2V models.
Additionally, we design a Dynamic Temporal Graphs (DTG)-based automatic
evaluation metric, which can efficiently capture long-range dependencies and
temporal ordering while maintaining computational efficiency. Our DTG-based
metric demonstrates a strong correlation with human annotations. Through
systematic evaluation using SeqBench, we reveal critical limitations in current
T2V models: failure to maintain consistent object states across multi-action
sequences, physically implausible results in multi-object scenarios, and
difficulties in preserving realistic timing and ordering relationships between
sequential actions. SeqBench provides the first systematic framework for
evaluating narrative coherence in T2V generation and offers concrete insights
for improving sequential reasoning capabilities in future models. Please refer
to https://videobench.github.io/SeqBench.github.io/ for more details.

</details>


### [10] [SceneAdapt: Scene-aware Adaptation of Human Motion Diffusion](https://arxiv.org/abs/2510.13044)
*Jungbin Cho,Minsu Kim,Jisoo Kim,Ce Zheng,Laszlo A. Jeni,Ming-Hsuan Yang,Youngjae Yu,Seonjoo Kim*

Main category: cs.CV

TL;DR: SceneAdapt is a framework that injects scene awareness into text-conditioned motion models by leveraging disjoint scene-motion and text-motion datasets through two adaptation stages: inbetweening and scene-aware inbetweening.


<details>
  <summary>Details</summary>
Motivation: Existing motion generation approaches address either motion semantics or scene-awareness in isolation, but constructing large-scale datasets with both rich text-motion coverage and precise scene interactions is extremely challenging.

Method: Uses motion inbetweening as a proxy task to bridge two distinct datasets. First stage introduces keyframing layers that modulate motion latents for inbetweening while preserving the latent manifold. Second stage adds a scene-conditioning layer that injects scene geometry by adaptively querying local context through cross-attention.

Result: Experimental results show that SceneAdapt effectively injects scene awareness into text-to-motion models.

Conclusion: The framework successfully bridges the gap between text-conditioned motion generation and scene awareness by leveraging disjoint datasets through a two-stage adaptation approach.

Abstract: Human motion is inherently diverse and semantically rich, while also shaped
by the surrounding scene. However, existing motion generation approaches
address either motion semantics or scene-awareness in isolation, since
constructing large-scale datasets with both rich text--motion coverage and
precise scene interactions is extremely challenging. In this work, we introduce
SceneAdapt, a framework that injects scene awareness into text-conditioned
motion models by leveraging disjoint scene--motion and text--motion datasets
through two adaptation stages: inbetweening and scene-aware inbetweening. The
key idea is to use motion inbetweening, learnable without text, as a proxy task
to bridge two distinct datasets and thereby inject scene-awareness to
text-to-motion models. In the first stage, we introduce keyframing layers that
modulate motion latents for inbetweening while preserving the latent manifold.
In the second stage, we add a scene-conditioning layer that injects scene
geometry by adaptively querying local context through cross-attention.
Experimental results show that SceneAdapt effectively injects scene awareness
into text-to-motion models, and we further analyze the mechanisms through which
this awareness emerges. Code and models will be released.

</details>


### [11] [One Dimensional CNN ECG Mamba for Multilabel Abnormality Classification in 12 Lead ECG](https://arxiv.org/abs/2510.13046)
*Huawei Jiang,Husna Mutahira,Gan Huang,Mannan Saeed Muhammad*

Main category: cs.CV

TL;DR: A hybrid model combining 1D CNN with Mamba state space model achieves superior ECG classification performance on PhysioNet datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning models have limited performance with long ECG sequences, while state space models offer efficient sequence modeling alternatives.

Method: Proposed 1D-CNN-ECG-Mamba framework combining convolutional feature extraction with bidirectional Mamba (Vision Mamba) for enhanced temporal dependency representation.

Result: Achieved substantially higher AUPRC and AUROC scores than previous best methods on PhysioNet 2020/2021 datasets for 12-lead ECG classification.

Conclusion: Mamba-based architectures show strong potential for advancing reliable ECG classification, supporting early diagnosis and telemedicine applications.

Abstract: Accurate detection of cardiac abnormalities from electrocardiogram recordings
is regarded as essential for clinical diagnostics and decision support.
Traditional deep learning models such as residual networks and transformer
architectures have been applied successfully to this task, but their
performance has been limited when long sequential signals are processed.
Recently, state space models have been introduced as an efficient alternative.
In this study, a hybrid framework named One Dimensional Convolutional Neural
Network Electrocardiogram Mamba is introduced, in which convolutional feature
extraction is combined with Mamba, a selective state space model designed for
effective sequence modeling. The model is built upon Vision Mamba, a
bidirectional variant through which the representation of temporal dependencies
in electrocardiogram data is enhanced. Comprehensive experiments on the
PhysioNet Computing in Cardiology Challenges of 2020 and 2021 were conducted,
and superior performance compared with existing methods was achieved.
Specifically, the proposed model achieved substantially higher AUPRC and AUROC
scores than those reported by the best previously published algorithms on
twelve lead electrocardiograms. These results demonstrate the potential of
Mamba-based architectures to advance reliable ECG classification. This
capability supports early diagnosis and personalized treatment, while enhancing
accessibility in telemedicine and resource-constrained healthcare systems.

</details>


### [12] [True Self-Supervised Novel View Synthesis is Transferable](https://arxiv.org/abs/2510.13063)
*Thomas W. Mitchel,Hyunwoo Ryu,Vincent Sitzmann*

Main category: cs.CV

TL;DR: XFactor is the first geometry-free self-supervised model for novel view synthesis that achieves transferable pose representations without 3D inductive biases.


<details>
  <summary>Details</summary>
Motivation: Prior self-supervised NVS models lack transferability - their predicted poses don't work across different scenes, indicating they're not truly learning camera geometry.

Method: Combines pair-wise pose estimation with input/output augmentation to disentangle camera pose from scene content, using unconstrained latent pose variables without SE(3) parameterization.

Result: XFactor achieves true transferability, significantly outperforms prior pose-free NVS transformers, and shows high correlation between latent poses and real-world poses.

Conclusion: Geometry-free self-supervised NVS with transferable pose representations is possible without explicit 3D geometric priors.

Abstract: In this paper, we identify that the key criterion for determining whether a
model is truly capable of novel view synthesis (NVS) is transferability:
Whether any pose representation extracted from one video sequence can be used
to re-render the same camera trajectory in another. We analyze prior work on
self-supervised NVS and find that their predicted poses do not transfer: The
same set of poses lead to different camera trajectories in different 3D scenes.
Here, we present XFactor, the first geometry-free self-supervised model capable
of true NVS. XFactor combines pair-wise pose estimation with a simple
augmentation scheme of the inputs and outputs that jointly enables
disentangling camera pose from scene content and facilitates geometric
reasoning. Remarkably, we show that XFactor achieves transferability with
unconstrained latent pose variables, without any 3D inductive biases or
concepts from multi-view geometry -- such as an explicit parameterization of
poses as elements of SE(3). We introduce a new metric to quantify
transferability, and through large-scale experiments, we demonstrate that
XFactor significantly outperforms prior pose-free NVS transformers, and show
that latent poses are highly correlated with real-world poses through probing
experiments.

</details>


### [13] [Direction-aware multi-scale gradient loss for infrared and visible image fusion](https://arxiv.org/abs/2510.13067)
*Kaixuan Yang,Wei Xiang,Zhenshuai Chen,Tong Jin,Yunpeng Liu*

Main category: cs.CV

TL;DR: The paper proposes a direction-aware, multi-scale gradient loss for infrared and visible image fusion that preserves gradient direction information across scales, improving edge fidelity and texture preservation without changing model architectures.


<details>
  <summary>Details</summary>
Motivation: Existing learning-based approaches use gradient magnitude loss which removes directional information, leading to ambiguous supervision and suboptimal edge fidelity in fused images.

Method: Introduces a direction-aware, multi-scale gradient loss that supervises horizontal and vertical gradient components separately while preserving their sign across different scales.

Result: Experiments show the approach promotes sharper, better-aligned edges and richer texture preservation in fused images compared to traditional gradient magnitude methods.

Conclusion: The proposed direction-aware gradient loss provides clear directional guidance at multiple resolutions and effectively improves infrared and visible image fusion quality without requiring architectural changes.

Abstract: Infrared and visible image fusion aims to integrate complementary information
from co-registered source images to produce a single, informative result. Most
learning-based approaches train with a combination of structural similarity
loss, intensity reconstruction loss, and a gradient-magnitude term. However,
collapsing gradients to their magnitude removes directional information,
yielding ambiguous supervision and suboptimal edge fidelity. We introduce a
direction-aware, multi-scale gradient loss that supervises horizontal and
vertical components separately and preserves their sign across scales. This
axis-wise, sign-preserving objective provides clear directional guidance at
both fine and coarse resolutions, promoting sharper, better-aligned edges and
richer texture preservation without changing model architectures or training
protocols. Experiments on open-source model and multiple public benchmarks
demonstrate effectiveness of our approach.

</details>


### [14] [Unsupervised Domain Adaptation via Content Alignment for Hippocampus Segmentation](https://arxiv.org/abs/2510.13075)
*Hoda Kalabizadeh,Ludovica Griffanti,Pak-Hei Yeung,Ana I. L. Namburete,Nicola K. Dinsdale,Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: Novel unsupervised domain adaptation framework for cross-domain hippocampus segmentation that addresses both style and content domain shifts through z-normalization and bidirectional deformable image registration.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for medical image segmentation struggle with domain shifts across datasets, particularly variations in image appearance (style) and population-dependent anatomical characteristics (content) in cross-domain hippocampus segmentation.

Method: Combines efficient style harmonization through z-normalization with bidirectional deformable image registration (DIR) strategy. DIR network is jointly trained with segmentation and discriminator networks to guide registration with respect to ROI and generate anatomically plausible transformations aligning source to target domain.

Result: Outperforms existing baselines across all experiments. Achieves up to 15% relative improvement in Dice score compared to standard augmentation methods when transferring from young healthy populations to clinical dementia patients, with largest gains in scenarios with substantial content shift.

Conclusion: The framework demonstrates efficacy for accurate hippocampus segmentation across diverse populations, particularly effective in handling substantial content domain shifts.

Abstract: Deep learning models for medical image segmentation often struggle when
deployed across different datasets due to domain shifts - variations in both
image appearance, known as style, and population-dependent anatomical
characteristics, referred to as content. This paper presents a novel
unsupervised domain adaptation framework that directly addresses domain shifts
encountered in cross-domain hippocampus segmentation from MRI, with specific
emphasis on content variations. Our approach combines efficient style
harmonisation through z-normalisation with a bidirectional deformable image
registration (DIR) strategy. The DIR network is jointly trained with
segmentation and discriminator networks to guide the registration with respect
to a region of interest and generate anatomically plausible transformations
that align source images to the target domain. We validate our approach through
comprehensive evaluations on both a synthetic dataset using Morpho-MNIST (for
controlled validation of core principles) and three MRI hippocampus datasets
representing populations with varying degrees of atrophy. Across all
experiments, our method outperforms existing baselines. For hippocampus
segmentation, when transferring from young, healthy populations to clinical
dementia patients, our framework achieves up to 15% relative improvement in
Dice score compared to standard augmentation methods, with the largest gains
observed in scenarios with substantial content shift. These results highlight
the efficacy of our approach for accurate hippocampus segmentation across
diverse populations.

</details>


### [15] [Counting Hallucinations in Diffusion Models](https://arxiv.org/abs/2510.13080)
*Shuai Fu,Jian Zhou,Qi Chen,Huang Jing,Huy Anh Nguyen,Xiaohan Liu,Zhixiong Zeng,Lin Ma,Quanshi Zhang,Qi Wu*

Main category: cs.CV

TL;DR: This paper introduces a systematic framework for quantifying counting hallucinations in diffusion probabilistic models (DPMs), where models generate incorrect numbers of objects despite such patterns being absent from training data.


<details>
  <summary>Details</summary>
Motivation: DPMs often produce hallucinated samples that conflict with real-world knowledge, but there's a lack of feasible methodologies for systematically quantifying such hallucinations, hindering progress in addressing this challenge.

Method: Constructed CountHalluSet dataset suite with well-defined counting criteria (ToyShape, SimObject, RealHand), developed standardized evaluation protocol, and systematically examined how different sampling conditions affect counting hallucination levels.

Result: Revealed that common evaluation metrics like FID fail to capture counting hallucinations consistently, and systematically quantified how different sampling conditions (solver type, ODE solver order, sampling steps, initial noise) affect hallucination levels.

Conclusion: This work takes the first step toward systematically quantifying hallucinations in diffusion models and offers new insights into investigating hallucination phenomena in image generation.

Abstract: Diffusion probabilistic models (DPMs) have demonstrated remarkable progress
in generative tasks, such as image and video synthesis. However, they still
often produce hallucinated samples (hallucinations) that conflict with
real-world knowledge, such as generating an implausible duplicate cup floating
beside another cup. Despite their prevalence, the lack of feasible
methodologies for systematically quantifying such hallucinations hinders
progress in addressing this challenge and obscures potential pathways for
designing next-generation generative models under factual constraints. In this
work, we bridge this gap by focusing on a specific form of hallucination, which
we term counting hallucination, referring to the generation of an incorrect
number of instances or structured objects, such as a hand image with six
fingers, despite such patterns being absent from the training data. To this
end, we construct a dataset suite CountHalluSet, with well-defined counting
criteria, comprising ToyShape, SimObject, and RealHand. Using these datasets,
we develop a standardized evaluation protocol for quantifying counting
hallucinations, and systematically examine how different sampling conditions in
DPMs, including solver type, ODE solver order, sampling steps, and initial
noise, affect counting hallucination levels. Furthermore, we analyze their
correlation with common evaluation metrics such as FID, revealing that this
widely used image quality metric fails to capture counting hallucinations
consistently. This work aims to take the first step toward systematically
quantifying hallucinations in diffusion models and offer new insights into the
investigation of hallucination phenomena in image generation.

</details>


### [16] [Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar Propagation](https://arxiv.org/abs/2510.13084)
*Yi Zuo,Zitao Wang,Lingling Li,Xu Liu,Fang Liu,Licheng Jiao*

Main category: cs.CV

TL;DR: Edit-Your-Interest is a lightweight, text-driven, zero-shot video editing method that uses spatio-temporal feature memory and propagation to reduce computational overhead while maintaining temporal consistency and visual fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing video editing methods suffer from high computational overhead, memory consumption, and produce undesirable temporal inconsistencies and artifacts like blurring and mosaic patterns.

Method: Introduces Spatio-Temporal Feature Memory (SFM) to cache features from previous frames, Feature Most-Similar Propagation (FMP) to propagate relevant tokens across frames, and uses cross-attention maps to automatically extract masks for targeted editing while preserving background integrity.

Result: Extensive experiments show that Edit-Your-Interest outperforms state-of-the-art methods in both efficiency and visual fidelity, with significantly reduced computational overhead compared to full-sequence spatio-temporal modeling approaches.

Conclusion: The proposed method provides superior effectiveness and practicality for text-driven video editing, achieving high accuracy while robustly preserving temporal consistency and background integrity.

Abstract: Text-to-image (T2I) diffusion models have recently demonstrated significant
progress in video editing.
  However, existing video editing methods are severely limited by their high
computational overhead and memory consumption.
  Furthermore, these approaches often sacrifice visual fidelity, leading to
undesirable temporal inconsistencies and artifacts such as blurring and
pronounced mosaic-like patterns.
  We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot video
editing method.
  Edit-Your-Interest introduces a spatio-temporal feature memory to cache
features from previous frames, significantly reducing computational overhead
compared to full-sequence spatio-temporal modeling approaches.
  Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM),
which is designed to efficiently cache and retain the crucial image tokens
processed by spatial attention.
  Second, we propose the Feature Most-Similar Propagation (FMP) method. FMP
propagates the most relevant tokens from previous frames to subsequent ones,
preserving temporal consistency.
  Finally, we introduce an SFM update algorithm that continuously refreshes the
cached features, ensuring their long-term relevance and effectiveness
throughout the video sequence.
  Furthermore, we leverage cross-attention maps to automatically extract masks
for the instances of interest.
  These masks are seamlessly integrated into the diffusion denoising process,
enabling fine-grained control over target objects and allowing
Edit-Your-Interest to perform highly accurate edits while robustly preserving
the background integrity.
  Extensive experiments decisively demonstrate that the proposed
Edit-Your-Interest outperforms state-of-the-art methods in both efficiency and
visual fidelity, validating its superior effectiveness and practicality.

</details>


### [17] [EgoSocial: Benchmarking Proactive Intervention Ability of Omnimodal LLMs via Egocentric Social Interaction Perception](https://arxiv.org/abs/2510.13105)
*Xijun Wang,Tanay Sharma,Achin Kulshrestha,Abhimitra Meka,Aveek Purohit,Dinesh Manocha*

Main category: cs.CV

TL;DR: EgoSocial dataset and EgoSoD method address AI's lack of social awareness in AR/VR by detecting optimal intervention timing in social interactions using multimodal contextual cues.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack social awareness for determining appropriate intervention timing in AR/VR contexts, leading to disruptive responses that negatively impact user focus and natural conversation flow.

Method: Created EgoSocial dataset with 13,500 social video-question pairs, analyzed OLLMs' limitations, and proposed EgoSoD - an end-to-end method integrating multimodal cues into a social thinking graph to dynamically model participants and interactions.

Result: OLLMs struggle with intervention timing detection (14.4% for Gemini 2.5 Pro). EgoSoD improves Phi-4 by 45.6% and Gemini 2.5 Pro by 9.9% on Intervention Timing, and improves Phi-4 by 20.4% and Gemini 2.5 Pro by 6.9% on overall Social Interaction performance.

Conclusion: EgoSoD effectively addresses the social awareness gap in AI assistants by robustly detecting intervention timing through multimodal social dynamics modeling, significantly outperforming current OLLMs.

Abstract: As AR/VR technologies become integral to daily life, there's a growing need
for AI that understands human social dynamics from an egocentric perspective.
However, current LLMs often lack the social awareness to discern when to
intervene as AI assistant. This leads to constant, socially unaware responses
that may disrupt natural conversation and negatively impact user focus. To
address these limitations, we introduce EgoSocial, a large-scale egocentric
dataset with 13,500 social video-question pairs, specifically designed to
benchmark intervention in social interaction perception. We also present an
in-depth analysis of current omnimodal LLMs (OLLMs) to assess their
effectiveness in detecting diverse social contextual cues. Experiments show
that OLLMs still struggle to detect the intervention timing (14.4% for Gemini
2.5 Pro). We also propose EgoSoD (EgoSocial Detection), an end-to-end method
for robustly discerning social dynamics. Informed by our OLLM analysis, EgoSoD
integrates multimodal contextual cues (e.g., audio and visual cues) into a
social thinking graph, dynamically modeling participants and interactions. Our
method proactively detects intervention timing and social interactions,
precisely determining when to intervene. Our EgoSoD improves Phi-4 by 45.6% and
Gemini 2.5 Pro by 9.9% on Intervention Timing performance, and improves Phi-4
by 20.4% and Gemini 2.5 Pro by 6.9% on overall Social Interaction performance.
We will release the dataset and code soon.

</details>


### [18] [DriveCritic: Towards Context-Aware, Human-Aligned Evaluation for Autonomous Driving with Vision-Language Models](https://arxiv.org/abs/2510.13108)
*Jingyu Song,Zhenxin Li,Shiyi Lan,Xinglong Sun,Nadine Chang,Maying Shen,Joshua Chen,Katherine A. Skinner,Jose M. Alvarez*

Main category: cs.CV

TL;DR: DriveCritic is a novel framework for evaluating autonomous driving planners that addresses the lack of context awareness in existing metrics like EPDMS through a curated dataset and VLM-based evaluator trained with human preferences.


<details>
  <summary>Details</summary>
Motivation: Existing autonomous driving evaluation metrics like EPDMS lack context awareness in nuanced scenarios, making it challenging to align planner assessments with human judgment.

Method: Two key contributions: (1) DriveCritic dataset - curated challenging scenarios with pairwise human preference annotations, (2) DriveCritic model - Vision-Language Model evaluator fine-tuned using two-stage supervised and reinforcement learning pipeline to integrate visual and symbolic context.

Result: DriveCritic significantly outperforms existing metrics and baselines in matching human preferences and demonstrates strong context awareness.

Conclusion: DriveCritic provides a more reliable, human-aligned foundation for evaluating autonomous driving systems.

Abstract: Benchmarking autonomous driving planners to align with human judgment remains
a critical challenge, as state-of-the-art metrics like the Extended Predictive
Driver Model Score (EPDMS) lack context awareness in nuanced scenarios. To
address this, we introduce DriveCritic, a novel framework featuring two key
contributions: the DriveCritic dataset, a curated collection of challenging
scenarios where context is critical for correct judgment and annotated with
pairwise human preferences, and the DriveCritic model, a Vision-Language Model
(VLM) based evaluator. Fine-tuned using a two-stage supervised and
reinforcement learning pipeline, the DriveCritic model learns to adjudicate
between trajectory pairs by integrating visual and symbolic context.
Experiments show DriveCritic significantly outperforms existing metrics and
baselines in matching human preferences and demonstrates strong context
awareness. Overall, our work provides a more reliable, human-aligned foundation
to evaluating autonomous driving systems.

</details>


### [19] [VPREG: An Optimal Control Formulation for Diffeomorphic Image Registration Based on the Variational Principle Grid Generation Method](https://arxiv.org/abs/2510.13109)
*Zicong Zhou,Baihan Zhao,Andreas Mang,Guojun Liao*

Main category: cs.CV

TL;DR: VPreg is a novel diffeomorphic image registration method that ensures positive Jacobian determinants and provides accurate inverse transformations within the diffeomorphism group, outperforming state-of-the-art methods in brain scan registration.


<details>
  <summary>Details</summary>
Motivation: To improve upon existing mesh generation and diffeomorphic image registration methods by achieving better registration accuracy while controlling transformation quality and providing accurate inverse transformations essential for neuroimaging workflows.

Method: Uses a Variational Principle (VP) grid generation approach that constructs non-folding grids with prescribed Jacobian determinant and curl, generating diffeomorphic spatial transformations within the group of diffeomorphisms rather than operating on image space.

Result: Evaluation on 150 brain scans from OASIS-1 dataset shows VPreg outperforms ANTs-SyN, Freesurfer-Easyreg, and FSL-Fnirt in Dice scores for 35 regions of interest, transformation regularity, and accuracy/consistency of inverse maps.

Conclusion: VPreg provides superior registration performance with guaranteed diffeomorphic properties and more accurate inverse transformations than existing methods, making it valuable for computational anatomy and morphometry applications.

Abstract: This paper introduces VPreg, a novel diffeomorphic image registration method.
This work provides several improvements to our past work on mesh generation and
diffeomorphic image registration. VPreg aims to achieve excellent registration
accuracy while controlling the quality of the registration transformations. It
ensures a positive Jacobian determinant of the spatial transformation and
provides an accurate approximation of the inverse of the registration, a
crucial property for many neuroimaging workflows. Unlike conventional methods,
VPreg generates this inverse transformation within the group of diffeomorphisms
rather than operating on the image space. The core of VPreg is a grid
generation approach, referred to as \emph{Variational Principle} (VP), which
constructs non-folding grids with prescribed Jacobian determinant and curl.
These VP-generated grids guarantee diffeomorphic spatial transformations
essential for computational anatomy and morphometry, and provide a more
accurate inverse than existing methods. To assess the potential of the proposed
approach, we conduct a performance analysis for 150 registrations of brain
scans from the OASIS-1 dataset. Performance evaluation based on Dice scores for
35 regions of interest, along with an empirical analysis of the properties of
the computed spatial transformations, demonstrates that VPreg outperforms
state-of-the-art methods in terms of Dice scores, regularity properties of the
computed transformation, and accuracy and consistency of the provided inverse
map. We compare our results to ANTs-SyN, Freesurfer-Easyreg, and FSL-Fnirt.

</details>


### [20] [OS-HGAdapter: Open Semantic Hypergraph Adapter for Large Language Models Assisted Entropy-Enhanced Image-Text Alignment](https://arxiv.org/abs/2510.13131)
*Rongjun Chen,Chengsi Yao,Jinchang Ren,Xianxian Zeng,Peixian Wang,Jun Yuan,Jiawen Li,Huimin Zhao,Xu Lu*

Main category: cs.CV

TL;DR: The paper proposes OS-HGAdapter, a method that uses LLM semantic knowledge and hypergraph adapters to address text-image alignment imbalance by enhancing text modality entropy and correcting semantic matching errors.


<details>
  <summary>Details</summary>
Motivation: To solve the imbalance in cross-modal retrieval caused by different information entropy between texts and images, and to reproduce human-like alignment capabilities.

Method: Two-step approach: 1) LLM-based prompt template to enhance text polysemy descriptions and increase text entropy; 2) Hypergraph adapter to construct multilateral text-image connections and correct semantic matching errors while reducing noise.

Result: Achieved 16.8% text-to-image and 40.1% image-to-text retrieval gains on Flickr30K and MS-COCO benchmarks, establishing new state-of-the-art performance.

Conclusion: The proposed OS-HGAdapter effectively addresses text-image alignment imbalance through entropy enhancement and hypergraph-based semantic correction, significantly improving cross-modal retrieval performance.

Abstract: Text-image alignment constitutes a foundational challenge in multimedia
content understanding, where effective modeling of cross-modal semantic
correspondences critically enhances retrieval system performance through joint
embedding space optimization. Given the inherent difference in information
entropy between texts and images, conventional approaches often show an
imbalance in the mutual retrieval of these two modalities. To address this
particular challenge, we propose to use the open semantic knowledge of Large
Language Model (LLM) to fill for the entropy gap and reproduce the alignment
ability of humans in these tasks. Our entropy-enhancing alignment is achieved
through a two-step process: 1) a new prompt template that does not rely on
explicit knowledge in the task domain is designed to use LLM to enhance the
polysemy description of the text modality. By analogy, the information entropy
of the text modality relative to the visual modality is increased; 2) A
hypergraph adapter is used to construct multilateral connections between the
text and image modalities, which can correct the positive and negative matching
errors for synonymous semantics in the same fixed embedding space, whilst
reducing the noise caused by open semantic entropy by mapping the reduced
dimensions back to the original dimensions. Comprehensive evaluations on the
Flickr30K and MS-COCO benchmarks validate the superiority of our Open Semantic
Hypergraph Adapter (OS-HGAdapter), showcasing 16.8\% (text-to-image) and 40.1\%
(image-to-text) cross-modal retrieval gains over existing methods while
establishing new state-of-the-art performance in semantic alignment tasks.

</details>


### [21] [Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN](https://arxiv.org/abs/2510.13137)
*Madhumati Pol,Anvay Anturkar,Anushka Khot,Ayush Andure,Aniruddha Ghosh,Anvit Magadum,Anvay Bahadur*

Main category: cs.CV

TL;DR: This paper compares 3D CNNs and LSTMs for real-time ASL recognition, finding 3D CNNs achieve higher accuracy (92.4%) but with higher computational cost, while LSTMs offer better efficiency with 86.7% accuracy.


<details>
  <summary>Details</summary>
Motivation: To evaluate different neural network architectures for real-time American Sign Language recognition, particularly focusing on the trade-offs between accuracy and computational efficiency for practical assistive technology applications.

Method: Used 3D CNNs and LSTM networks on a dataset of 1,200 ASL signs across 50 classes, comparing accuracy, computational efficiency, and latency under similar training conditions. Also tested a hybrid 3D CNN-LSTM model.

Result: 3D CNNs achieved 92.4% recognition accuracy but required 3.2% more processing time per frame. LSTMs maintained 86.7% accuracy with significantly lower resource consumption. The hybrid model showed decent performance.

Conclusion: Context-dependent architecture selection is crucial for practical implementation, with trade-offs between recognition precision and real-time operational requirements in edge computing environments.

Abstract: This study investigates the performance of 3D Convolutional Neural Networks
(3D CNNs) and Long Short-Term Memory (LSTM) networks for real-time American
Sign Language (ASL) recognition. Though 3D CNNs are good at spatiotemporal
feature extraction from video sequences, LSTMs are optimized for modeling
temporal dependencies in sequential data. We evaluate both architectures on a
dataset containing 1,200 ASL signs across 50 classes, comparing their accuracy,
computational efficiency, and latency under similar training conditions.
Experimental results demonstrate that 3D CNNs achieve 92.4% recognition
accuracy but require 3.2% more processing time per frame compared to LSTMs,
which maintain 86.7% accuracy with significantly lower resource consumption.
The hybrid 3D CNNLSTM model shows decent performance, which suggests that
context-dependent architecture selection is crucial for practical
implementation.This project provides professional benchmarks for developing
assistive technologies, highlighting trade-offs between recognition precision
and real-time operational requirements in edge computing environments.

</details>


### [22] [Foveation Improves Payload Capacity in Steganography](https://arxiv.org/abs/2510.13151)
*Lifeng Qiu Lin,Henry Kam,Qi Sun,Kaan Akşit*

Main category: cs.CV

TL;DR: Improved steganography models achieve 5x higher capacity (500 bits vs 100 bits) with better accuracy (1 failure bit per 2000 bits) while maintaining good visual quality.


<details>
  <summary>Details</summary>
Motivation: To enhance steganography capabilities in visual media for applications like metadata embedding and watermarking by overcoming existing capacity limitations.

Method: Utilized efficient latent representations and foveated rendering to train models with novel perceptual design for creating multi-modal latent representations.

Result: Achieved 500-bit capacity (vs previous 100-bit limit), 99.95% accuracy (1 failure bit per 2000), 31.47 dB PSNR, and 0.13 LPIPS visual quality metrics.

Conclusion: The novel perceptual design effectively creates multi-modal latent representations that significantly improve steganography capacity and accuracy while maintaining visual quality.

Abstract: Steganography finds its use in visual medium such as providing metadata and
watermarking. With support of efficient latent representations and foveated
rendering, we trained models that improve existing capacity limits from 100 to
500 bits, while achieving better accuracy of up to 1 failure bit out of 2000,
at 200K test bits. Finally, we achieve a comparable visual quality of 31.47 dB
PSNR and 0.13 LPIPS, showing the effectiveness of novel perceptual design in
creating multi-modal latent representations in steganography.

</details>


### [23] [DP-TTA: Test-time Adaptation for Transient Electromagnetic Signal Denoising via Dictionary-driven Prior Regularization](https://arxiv.org/abs/2510.13160)
*Meng Yang,Kecheng Chen,Wei Luo,Xianjie Chen,Yong Jia,Mingyue Wang,Fanqiang Lin*

Main category: cs.CV

TL;DR: Proposes DP-TTA, a test-time adaptation method using dictionary-driven physical priors to improve TEM signal denoising across different geographical regions.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning TEM denoising models trained on simulated or single real-world data fail in new environments due to varying noise characteristics from different geological conditions, equipment, and interference.

Method: Dictionary-driven Prior Regularization Test-time Adaptation (DP-TTA) encodes intrinsic TEM signal characteristics (exponential decay, smoothness) as dictionary priors, integrated into DTEMDNet network, and guides adaptation via self-supervised losses during testing.

Result: Extensive experiments show the method achieves significantly better performance than existing TEM denoising methods and TTA approaches.

Conclusion: Using intrinsic physical characteristics as prior knowledge enables effective test-time adaptation for TEM denoising across diverse geographical environments.

Abstract: Transient Electromagnetic (TEM) method is widely used in various geophysical
applications, providing valuable insights into subsurface properties. However,
time-domain TEM signals are often submerged in various types of noise. While
recent deep learning-based denoising models have shown strong performance,
these models are mostly trained on simulated or single real-world scenario
data, overlooking the significant differences in noise characteristics from
different geographical regions. Intuitively, models trained in one environment
often struggle to perform well in new settings due to differences in geological
conditions, equipment, and external interference, leading to reduced denoising
performance. To this end, we propose the Dictionary-driven Prior Regularization
Test-time Adaptation (DP-TTA). Our key insight is that TEM signals possess
intrinsic physical characteristics, such as exponential decay and smoothness,
which remain consistent across different regions regardless of external
conditions. These intrinsic characteristics serve as ideal prior knowledge for
guiding the TTA strategy, which helps the pre-trained model dynamically adjust
parameters by utilizing self-supervised losses, improving denoising performance
in new scenarios. To implement this, we customized a network, named DTEMDNet.
Specifically, we first use dictionary learning to encode these intrinsic
characteristics as a dictionary-driven prior, which is integrated into the
model during training. At the testing stage, this prior guides the model to
adapt dynamically to new environments by minimizing self-supervised losses
derived from the dictionary-driven consistency and the signal one-order
variation. Extensive experimental results demonstrate that the proposed method
achieves much better performance than existing TEM denoising methods and TTA
methods.

</details>


### [24] [STT-GS: Sample-Then-Transmit Edge Gaussian Splatting with Joint Client Selection and Power Control](https://arxiv.org/abs/2510.13186)
*Zhen Li,Xibin Jin,Guoliang Li,Shuai Wang,Miaowen Wen,Huseyin Arslan,Derrick Wing Kwan Ng,Chengzhong Xu*

Main category: cs.CV

TL;DR: Edge Gaussian splatting (EGS) for scene reconstruction requires maximizing GS quality rather than traditional metrics. Proposed STT-GS strategy samples pilot images first, then prioritizes communication to valuable clients using feature-domain clustering and transmission time minimization.


<details>
  <summary>Details</summary>
Motivation: Traditional edge resource management methods focus on communication throughput or general learning performance, but EGS specifically aims to maximize Gaussian splatting quality, making existing approaches inapplicable.

Method: Proposed sample-then-transmit EGS (STT-GS) strategy: first samples subset of images as pilot data from each client for loss prediction, then prioritizes communication resources based on value. Uses feature-domain clustering for representative data selection and pilot transmission time minimization to reduce overhead. Joint client selection and power control framework with penalty alternating majorization minimization algorithm.

Result: Significantly outperforms existing benchmarks on real-world datasets. GS-oriented objective can be accurately predicted with low sampling ratios (e.g., 10%). Achieves excellent tradeoff between view contributions and communication costs.

Conclusion: The proposed STT-GS strategy effectively addresses the causality dilemma in EGS by first sampling pilot data, then optimizing resource allocation, achieving superior performance in scene reconstruction quality while managing communication constraints.

Abstract: Edge Gaussian splatting (EGS), which aggregates data from distributed clients
and trains a global GS model at the edge server, is an emerging paradigm for
scene reconstruction. Unlike traditional edge resource management methods that
emphasize communication throughput or general-purpose learning performance, EGS
explicitly aims to maximize the GS qualities, rendering existing approaches
inapplicable. To address this problem, this paper formulates a novel
GS-oriented objective function that distinguishes the heterogeneous view
contributions of different clients. However, evaluating this function in turn
requires clients' images, leading to a causality dilemma. To this end, this
paper further proposes a sample-then-transmit EGS (or STT-GS for short)
strategy, which first samples a subset of images as pilot data from each client
for loss prediction. Based on the first-stage evaluation, communication
resources are then prioritized towards more valuable clients. To achieve
efficient sampling, a feature-domain clustering (FDC) scheme is proposed to
select the most representative data and pilot transmission time minimization
(PTTM) is adopted to reduce the pilot overhead.Subsequently, we develop a joint
client selection and power control (JCSPC) framework to maximize the
GS-oriented function under communication resource constraints. Despite the
nonconvexity of the problem, we propose a low-complexity efficient solution
based on the penalty alternating majorization minimization (PAMM) algorithm.
Experiments unveil that the proposed scheme significantly outperforms existing
benchmarks on real-world datasets. It is found that the GS-oriented objective
can be accurately predicted with low sampling ratios (e.g.,10%), and our method
achieves an excellent tradeoff between view contributions and communication
costs.

</details>


### [25] [Complementary Information Guided Occupancy Prediction via Multi-Level Representation Fusion](https://arxiv.org/abs/2510.13198)
*Rongtao Xu,Jinzhou Lin,Jialei Zhou,Jiahua Dong,Changwei Wang,Ruisheng Wang,Li Guo,Shibiao Xu,Xiaodan Liang*

Main category: cs.CV

TL;DR: CIGOcc is a two-stage occupancy prediction framework that fuses multi-level features (segmentation, graphics, depth) using deformable fusion and SAM knowledge distillation, achieving SOTA performance on SemanticKITTI without extra training costs.


<details>
  <summary>Details</summary>
Motivation: Existing camera-based occupancy prediction methods focus on structural improvements but underutilize the rich diversity of features in 2D images, particularly from the perspective of representation fusion.

Method: Two-stage framework with deformable multi-level fusion of segmentation, graphics, and depth features, plus knowledge distillation from SAM to enhance prediction accuracy.

Result: Achieves state-of-the-art performance on the SemanticKITTI benchmark without increasing training costs.

Conclusion: CIGOcc demonstrates that effective multi-level representation fusion can significantly improve occupancy prediction performance in autonomous driving perception.

Abstract: Camera-based occupancy prediction is a mainstream approach for 3D perception
in autonomous driving, aiming to infer complete 3D scene geometry and semantics
from 2D images. Almost existing methods focus on improving performance through
structural modifications, such as lightweight backbones and complex cascaded
frameworks, with good yet limited performance. Few studies explore from the
perspective of representation fusion, leaving the rich diversity of features in
2D images underutilized. Motivated by this, we propose \textbf{CIGOcc, a
two-stage occupancy prediction framework based on multi-level representation
fusion. \textbf{CIGOcc extracts segmentation, graphics, and depth features from
an input image and introduces a deformable multi-level fusion mechanism to fuse
these three multi-level features. Additionally, CIGOcc incorporates knowledge
distilled from SAM to further enhance prediction accuracy. Without increasing
training costs, CIGOcc achieves state-of-the-art performance on the
SemanticKITTI benchmark. The code is provided in the supplementary material and
will be released https://github.com/VitaLemonTea1/CIGOcc

</details>


### [26] [Paper Copilot: Tracking the Evolution of Peer Review in AI Conferences](https://arxiv.org/abs/2510.13201)
*Jing Yang,Qiyao Wei,Jiaxin Pei*

Main category: cs.CV

TL;DR: Paper Copilot creates digital archives of peer reviews across CS venues, providing an open dataset and infrastructure for large-scale analysis of peer review evolution, particularly in ICLR.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of AI conferences strains peer-review systems with heavy workloads, expertise mismatches, inconsistent standards, superficial reviews, and limited accountability under compressed timelines.

Method: Developed Paper Copilot system that creates durable digital archives of peer reviews across computer-science venues and conducts large-scale empirical analysis of ICLR reviews spanning multiple years.

Result: Created an open dataset and infrastructure that enables researchers to study peer review at scale and supports reproducible research on the evolution of peer review.

Conclusion: The resources help the community track changes, diagnose failure modes, and inform evidence-based improvements toward a more robust, transparent, and reliable peer-review system.

Abstract: The rapid growth of AI conferences is straining an already fragile
peer-review system, leading to heavy reviewer workloads, expertise mismatches,
inconsistent evaluation standards, superficial or templated reviews, and
limited accountability under compressed timelines. In response, conference
organizers have introduced new policies and interventions to preserve review
standards. Yet these ad-hoc changes often create further concerns and confusion
about the review process, leaving how papers are ultimately accepted - and how
practices evolve across years - largely opaque. We present Paper Copilot, a
system that creates durable digital archives of peer reviews across a wide
range of computer-science venues, an open dataset that enables researchers to
study peer review at scale, and a large-scale empirical analysis of ICLR
reviews spanning multiple years. By releasing both the infrastructure and the
dataset, Paper Copilot supports reproducible research on the evolution of peer
review. We hope these resources help the community track changes, diagnose
failure modes, and inform evidence-based improvements toward a more robust,
transparent, and reliable peer-review system.

</details>


### [27] [MimicParts: Part-aware Style Injection for Speech-Driven 3D Motion Generation](https://arxiv.org/abs/2510.13208)
*Lianlian Liu,YongKang He,Zhaojie Chu,Xiaofen Xing,Xiangmin Xu*

Main category: cs.CV

TL;DR: MimicParts is a novel framework that generates stylized 3D human motion from speech by using part-aware style injection and denoising to capture regional motion differences and dynamically adapt to speech rhythm and emotion cues.


<details>
  <summary>Details</summary>
Motivation: Current methods oversimplify stylistic diversity and ignore regional motion style differences (upper vs. lower body), limiting motion realism. They also fail to dynamically adapt motion style to changes in speech rhythm and emotion.

Method: Proposes MimicParts framework with part-aware style injection and part-aware denoising network that divides body into regions to encode localized motion styles. Uses part-aware attention block to allow rhythm and emotion cues to guide each body region precisely.

Result: Experimental results show the method outperforms existing methods, producing more natural and expressive 3D human motion sequences.

Conclusion: The proposed MimicParts framework successfully addresses limitations of current approaches by capturing fine-grained regional motion differences and dynamically adapting motion style to speech rhythm and emotion variations.

Abstract: Generating stylized 3D human motion from speech signals presents substantial
challenges, primarily due to the intricate and fine-grained relationships among
speech signals, individual styles, and the corresponding body movements.
Current style encoding approaches either oversimplify stylistic diversity or
ignore regional motion style differences (e.g., upper vs. lower body), limiting
motion realism. Additionally, motion style should dynamically adapt to changes
in speech rhythm and emotion, but existing methods often overlook this. To
address these issues, we propose MimicParts, a novel framework designed to
enhance stylized motion generation based on part-aware style injection and
part-aware denoising network. It divides the body into different regions to
encode localized motion styles, enabling the model to capture fine-grained
regional differences. Furthermore, our part-aware attention block allows rhythm
and emotion cues to guide each body region precisely, ensuring that the
generated motion aligns with variations in speech rhythm and emotional state.
Experimental results show that our method outperforming existing methods
showcasing naturalness and expressive 3D human motion sequences.

</details>


### [28] [Prompt-based Adaptation in Large-scale Vision Models: A Survey](https://arxiv.org/abs/2510.13219)
*Xi Xiao,Yunbei Zhang,Lin Zhao,Yiyang Liu,Xiaoying Liao,Zheda Mai,Xingjian Li,Xiao Wang,Hao Xu,Jihun Hamm,Xue Lin,Min Xu,Qifan Wang,Tianyang Wang,Cheng Han*

Main category: cs.CV

TL;DR: This survey provides a comprehensive review of Visual Prompting (VP) and Visual Prompt Tuning (VPT), conceptualizing them under a unified Prompt-based Adaptation (PA) framework with taxonomy and applications across various domains.


<details>
  <summary>Details</summary>
Motivation: To address the blurred conceptual boundaries between VP and VPT in current research and provide systematic distinction between these techniques and their applications.

Method: Revisits VP and VPT designs from first principles, conceptualizes them under unified Prompt-based Adaptation framework, provides taxonomy categorizing methods into learnable, generative, and non-learnable prompts organized by injection granularity.

Result: Establishes comprehensive taxonomy and framework for PA, examines integrations across medical imaging, 3D point clouds, vision-language tasks, test-time adaptation, and trustworthy AI, and summarizes current benchmarks.

Conclusion: Provides first comprehensive survey dedicated to PA's methodologies and applications, offering clear roadmap for researchers and practitioners to understand and explore the evolving landscape of PA-related research.

Abstract: In computer vision, Visual Prompting (VP) and Visual Prompt Tuning (VPT) have
recently emerged as lightweight and effective alternatives to full fine-tuning
for adapting large-scale vision models within the ``pretrain-then-finetune''
paradigm. However, despite rapid progress, their conceptual boundaries remain
blurred, as VP and VPT are frequently used interchangeably in current research,
reflecting a lack of systematic distinction between these techniques and their
respective applications. In this survey, we revisit the designs of VP and VPT
from first principles, and conceptualize them within a unified framework termed
Prompt-based Adaptation (PA). We provide a taxonomy that categorizes existing
methods into learnable, generative, and non-learnable prompts, and further
organizes them by injection granularity -- pixel-level and token-level. Beyond
the core methodologies, we examine PA's integrations across diverse domains,
including medical imaging, 3D point clouds, and vision-language tasks, as well
as its role in test-time adaptation and trustworthy AI. We also summarize
current benchmarks and identify key challenges and future directions. To the
best of our knowledge, we are the first comprehensive survey dedicated to PA's
methodologies and applications in light of their distinct characteristics. Our
survey aims to provide a clear roadmap for researchers and practitioners in all
area to understand and explore the evolving landscape of PA-related research.

</details>


### [29] [Sample-Centric Multi-Task Learning for Detection and Segmentation of Industrial Surface Defects](https://arxiv.org/abs/2510.13226)
*Hang-Cheng Dong,Yibo Jiao,Fupeng Wei,Guodong Liu,Dong Ye,Bingguo Liu*

Main category: cs.CV

TL;DR: Proposes sample-centric multi-task learning for industrial defect inspection, addressing pixel-centric limitations by jointly learning sample-level classification and pixel-level localization with decision-linked metrics.


<details>
  <summary>Details</summary>
Motivation: Industrial defect inspection faces challenges from extreme foreground-background imbalance, defect sparsity with long-tailed scale distribution, and low contrast. Existing models achieve strong pixel-overlap metrics but lack sample-level stability, especially for sparse/slender defects due to mismatch between optimization objective and QC decision granularity.

Method: Sample-centric multi-task learning framework with shared-encoder architecture that jointly learns sample-level defect classification and pixel-level mask localization. Sample-level supervision modulates feature distribution and boosts recall for small/low-contrast defects, while segmentation preserves boundary details.

Result: Experiments on two benchmark datasets show substantial improvement in reliability of sample-level decisions and completeness of defect localization compared to existing approaches.

Conclusion: The proposed framework addresses the fundamental mismatch between pixel-centric training and sample-level QC decisions, providing more stable and reliable industrial defect inspection through joint learning and decision-linked evaluation metrics.

Abstract: Industrial surface defect inspection for sample-wise quality control (QC)
must simultaneously decide whether a given sample contains defects and localize
those defects spatially. In real production lines, extreme
foreground-background imbalance, defect sparsity with a long-tailed scale
distribution, and low contrast are common. As a result, pixel-centric training
and evaluation are easily dominated by large homogeneous regions, making it
difficult to drive models to attend to small or low-contrast defects-one of the
main bottlenecks for deployment. Empirically, existing models achieve strong
pixel-overlap metrics (e.g., mIoU) but exhibit insufficient stability at the
sample level, especially for sparse or slender defects. The root cause is a
mismatch between the optimization objective and the granularity of QC
decisions. To address this, we propose a sample-centric multi-task learning
framework and evaluation suite. Built on a shared-encoder architecture, the
method jointly learns sample-level defect classification and pixel-level mask
localization. Sample-level supervision modulates the feature distribution and,
at the gradient level, continually boosts recall for small and low-contrast
defects, while the segmentation branch preserves boundary and shape details to
enhance per-sample decision stability and reduce misses. For evaluation, we
propose decision-linked metrics, Seg_mIoU and Seg_Recall, which remove the bias
of classical mIoU caused by empty or true-negative samples and tightly couple
localization quality with sample-level decisions. Experiments on two benchmark
datasets demonstrate that our approach substantially improves the reliability
of sample-level decisions and the completeness of defect localization.

</details>


### [30] [What "Not" to Detect: Negation-Aware VLMs via Structured Reasoning and Token Merging](https://arxiv.org/abs/2510.13232)
*Inha Kang,Youngsun Lim,Seonho Lee,Jiho Choi,Junsuk Choe,Hyunjung Shim*

Main category: cs.CV

TL;DR: The paper addresses the affirmative bias in vision-language models by proposing CoVAND dataset and NegToMe module to improve negation understanding in described object detection tasks.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art vision-language models suffer from critical failure in understanding negation (affirmative bias), particularly severe in described object detection tasks.

Method: Two main contributions: (1) CoVAND dataset pipeline using chain-of-thought and VQA-based approach for high-quality negation data, (2) NegToMe text token merging module that groups negation cues with attributes into semantic phrases to prevent structural loss during tokenization.

Result: Significantly improves performance on negation benchmarks with lowered false positive rate, boosting NMS-AP by up to +10.8 points on OVDEval and demonstrating generalization to state-of-the-art VLMs.

Conclusion: This work marks a crucial step forward in addressing negation understanding for real-world detection applications through architectural improvements and high-quality data generation.

Abstract: State-of-the-art vision-language models (VLMs) suffer from a critical failure
in understanding negation, often referred to as affirmative bias. This
limitation is particularly severe in described object detection (DOD) tasks. To
address this, we propose two primary contributions: (1) a new dataset pipeline
and (2) a novel, lightweight adaptation recipe. First, we introduce CoVAND, a
dataset constructed with a systematic chain-of-thought (CoT) and VQA-based
pipeline to generate high-quality, instance-grounded negation data. Second, we
propose NegToMe, a novel text token merging module that directly tackles the
architectural cause of affirmative bias. NegToMe fundamentally addresses the
structural loss of negation cues in tokenization, grouping them with attributes
into coherent semantic phrases. It maintains correct polarity at the input
level, enabling robust negation understanding even with limited data. For
instance, to prevent a model from treating the fragmented tokens "not" and
"girl" as simply "girl", NegToMe binds them into a single token whose meaning
is correctly distinguished from that of "girl" alone. This module is integrated
with a parameter-efficient and strategic LoRA fine-tuning approach. Our method
significantly improves performance on challenging negation benchmarks with a
lowered false positive rate, boosting NMS-AP by up to +10.8 points on OVDEval
and demonstrating generalization to SoTA VLMs. This work marks a crucial step
forward in addressing negation understanding for real-world detection
applications.

</details>


### [31] [UniVector: Unified Vector Extraction via Instance-Geometry Interaction](https://arxiv.org/abs/2510.13234)
*Yinglong Yan,Jun Yue,Shaobo Xia,Hanmeng Sun,Tianxu Ying,Chengcheng Wu,Sifan Lan,Min He,Pedram Ghamisi,Leyuan Fang*

Main category: cs.CV

TL;DR: UniVector is a unified framework for extracting multiple vector types (polygons, polylines, line segments) from raster images using a single model, overcoming limitations of existing methods that require separate models for each vector type.


<details>
  <summary>Details</summary>
Motivation: Existing vector extraction methods are tailored to single vector types and treat instance attributes and geometric attributes independently, limiting their ability to capture complex structures. The authors were inspired by how the human brain simultaneously uses semantic and spatial interactions in visual perception.

Method: UniVector encodes vectors as structured queries containing both instance- and geometry-level information, iteratively updates them through an interaction module for cross-level context exchange, and applies a dynamic shape constraint to refine global structures and key points.

Result: UniVector sets a new state of the art on both single- and multi-structure vector extraction tasks. The authors also introduced the Multi-Vector dataset to benchmark multi-structure scenarios.

Conclusion: The proposed UniVector framework successfully unifies multiple vector type extraction within a single model through instance-geometry interaction, demonstrating superior performance compared to specialized single-type methods.

Abstract: Vector extraction retrieves structured vector geometry from raster images,
offering high-fidelity representation and broad applicability. Existing
methods, however, are usually tailored to a single vector type (e.g., polygons,
polylines, line segments), requiring separate models for different structures.
This stems from treating instance attributes (category, structure) and
geometric attributes (point coordinates, connections) independently, limiting
the ability to capture complex structures. Inspired by the human brain's
simultaneous use of semantic and spatial interactions in visual perception, we
propose UniVector, a unified VE framework that leverages instance-geometry
interaction to extract multiple vector types within a single model. UniVector
encodes vectors as structured queries containing both instance- and
geometry-level information, and iteratively updates them through an interaction
module for cross-level context exchange. A dynamic shape constraint further
refines global structures and key points. To benchmark multi-structure
scenarios, we introduce the Multi-Vector dataset with diverse polygons,
polylines, and line segments. Experiments show UniVector sets a new state of
the art on both single- and multi-structure VE tasks. Code and dataset will be
released at https://github.com/yyyyll0ss/UniVector.

</details>


### [32] [EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking](https://arxiv.org/abs/2510.13235)
*Yukuan Zhang,Jiarui Zhao,Shangqing Nie,Jin Kuang,Shengsheng Wang*

Main category: cs.CV

TL;DR: EPIPTrack is a unified multimodal vision-language tracking framework that uses explicit and implicit prompts for dynamic target modeling, outperforming existing trackers on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing methods use static textual descriptions from LLMs which lack adaptability to real-time target state changes and are prone to hallucinations, limiting their effectiveness in dynamic tracking scenarios.

Method: Uses explicit prompts (spatial motion info as natural language) and implicit prompts (pseudo-words with learnable descriptors) that dynamically adjust via CLIP text encoder, plus a Discriminative Feature Augmentor for enhanced representations.

Result: Extensive experiments on MOT17, MOT20, and DanceTrack show EPIPTrack outperforms existing trackers in diverse scenarios with robust adaptability and superior performance.

Conclusion: The proposed EPIPTrack framework effectively addresses limitations of static textual descriptions by leveraging dynamic multimodal prompts, demonstrating strong performance across various tracking benchmarks.

Abstract: Multimodal semantic cues, such as textual descriptions, have shown strong
potential in enhancing target perception for tracking. However, existing
methods rely on static textual descriptions from large language models, which
lack adaptability to real-time target state changes and prone to
hallucinations. To address these challenges, we propose a unified multimodal
vision-language tracking framework, named EPIPTrack, which leverages explicit
and implicit prompts for dynamic target modeling and semantic alignment.
Specifically, explicit prompts transform spatial motion information into
natural language descriptions to provide spatiotemporal guidance. Implicit
prompts combine pseudo-words with learnable descriptors to construct
individualized knowledge representations capturing appearance attributes. Both
prompts undergo dynamic adjustment via the CLIP text encoder to respond to
changes in target state. Furthermore, we design a Discriminative Feature
Augmentor to enhance visual and cross-modal representations. Extensive
experiments on MOT17, MOT20, and DanceTrack demonstrate that EPIPTrack
outperforms existing trackers in diverse scenarios, exhibiting robust
adaptability and superior performance.

</details>


### [33] [Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models](https://arxiv.org/abs/2510.13237)
*Haochuan Xu,Yun Sing Koh,Shuhuai Huang,Zirun Zhou,Di Wang,Jun Sakuma,Jingfeng Zhang*

Main category: cs.CV

TL;DR: This paper proposes EDPA, a model-agnostic adversarial patch attack for Vision-Language-Action models that disrupts semantic alignment and maximizes representation discrepancy, along with a defense strategy using adversarial fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Vision-Language-Action models have achieved significant progress in robot learning, but their adversarial robustness remains underexplored, creating security vulnerabilities in robotic systems.

Method: The authors introduce Embedding Disruption Patch Attack (EDPA) that generates adversarial patches to disrupt semantic alignment between visual and textual representations and maximize latent representation discrepancies. They also propose an adversarial fine-tuning defense that optimizes the visual encoder to produce similar representations for clean and adversarial inputs.

Result: Extensive evaluations on the LIBERO benchmark show EDPA substantially increases task failure rates of state-of-the-art VLA models, while the proposed defense effectively mitigates this performance degradation.

Conclusion: The work demonstrates significant adversarial vulnerabilities in VLA models and provides both effective attack and defense strategies, highlighting the importance of robustness in vision-language-action systems for robotics.

Abstract: Vision-Language-Action (VLA) models have achieved revolutionary progress in
robot learning, enabling robots to execute complex physical robot tasks from
natural language instructions. Despite this progress, their adversarial
robustness remains underexplored. In this work, we propose both adversarial
patch attack and corresponding defense strategies for VLA models. We first
introduce the Embedding Disruption Patch Attack (EDPA), a model-agnostic
adversarial attack that generates patches directly placeable within the
camera's view. In comparison to prior methods, EDPA can be readily applied to
different VLA models without requiring prior knowledge of the model
architecture, or the controlled robotic manipulator. EDPA constructs these
patches by (i) disrupting the semantic alignment between visual and textual
latent representations, and (ii) maximizing the discrepancy of latent
representations between adversarial and corresponding clean visual inputs.
Through the optimization of these objectives, EDPA distorts the VLA's
interpretation of visual information, causing the model to repeatedly generate
incorrect actions and ultimately result in failure to complete the given
robotic task. To counter this, we propose an adversarial fine-tuning scheme for
the visual encoder, in which the encoder is optimized to produce similar latent
representations for both clean and adversarially perturbed visual inputs.
Extensive evaluations on the widely recognized LIBERO robotic simulation
benchmark demonstrate that EDPA substantially increases the task failure rate
of cutting-edge VLA models, while our proposed defense effectively mitigates
this degradation. The codebase is accessible via the homepage at
https://edpa-attack.github.io/.

</details>


### [34] [FlyAwareV2: A Multimodal Cross-Domain UAV Dataset for Urban Scene Understanding](https://arxiv.org/abs/2510.13243)
*Francesco Barbato,Matteo Caligiuri,Pietro Zanuttigh*

Main category: cs.CV

TL;DR: FlyAwareV2 is a multimodal dataset combining real and synthetic UAV imagery for urban scene understanding, featuring RGB, depth, and semantic labels across diverse environmental conditions with benchmarks for semantic segmentation and domain adaptation studies.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of collecting and annotating real-world UAV data, which is extremely costly and difficult, by providing a comprehensive dataset that enables development of computer vision algorithms for UAV applications in urban environments.

Method: Built upon existing SynDrone and FlyAware datasets, FlyAwareV2 introduces multimodal data (RGB, depth, semantic labels) across varying weather and daytime conditions, computes depth maps for real samples using monocular depth estimation, and provides benchmarks for semantic segmentation and domain adaptation studies.

Result: The dataset provides rich annotations and environmental diversity, enabling research on UAV-based 3D urban scene understanding through comprehensive multimodal data and established benchmarks.

Conclusion: FlyAwareV2 serves as a valuable resource for advancing research in UAV-based computer vision applications by providing a comprehensive multimodal dataset that addresses the limitations of real-world data collection and annotation.

Abstract: The development of computer vision algorithms for Unmanned Aerial Vehicle
(UAV) applications in urban environments heavily relies on the availability of
large-scale datasets with accurate annotations. However, collecting and
annotating real-world UAV data is extremely challenging and costly. To address
this limitation, we present FlyAwareV2, a novel multimodal dataset encompassing
both real and synthetic UAV imagery tailored for urban scene understanding
tasks. Building upon the recently introduced SynDrone and FlyAware datasets,
FlyAwareV2 introduces several new key contributions: 1) Multimodal data (RGB,
depth, semantic labels) across diverse environmental conditions including
varying weather and daytime; 2) Depth maps for real samples computed via
state-of-the-art monocular depth estimation; 3) Benchmarks for RGB and
multimodal semantic segmentation on standard architectures; 4) Studies on
synthetic-to-real domain adaptation to assess the generalization capabilities
of models trained on the synthetic data. With its rich set of annotations and
environmental diversity, FlyAwareV2 provides a valuable resource for research
on UAV-based 3D urban scene understanding.

</details>


### [35] [CymbaDiff: Structured Spatial Diffusion for Sketch-based 3D Semantic Urban Scene Generation](https://arxiv.org/abs/2510.13245)
*Li Liang,Bo Miao,Xinyu Wang,Naveed Akhtar,Jordan Vice,Ajmal Mian*

Main category: cs.CV

TL;DR: SketchSem3D is the first large-scale benchmark for 3D outdoor semantic scene generation from sketches and satellite images, with proposed CymbaDiff method achieving superior spatial coherence and semantic consistency.


<details>
  <summary>Details</summary>
Motivation: Advances in outdoor 3D semantic scene generation are constrained by the absence of publicly available, well-annotated datasets for applications like urban simulation and autonomous driving.

Method: Proposed Cylinder Mamba Diffusion (CymbaDiff) that imposes structured spatial ordering, captures cylindrical continuity and vertical hierarchy, and preserves physical neighborhood relationships and global context.

Result: Extensive experiments on SketchSem3D demonstrate that CymbaDiff achieves superior semantic consistency, spatial realism, and cross-dataset generalization.

Conclusion: The introduced SketchSem3D benchmark and CymbaDiff method significantly enhance outdoor 3D scene generation capabilities, with code and dataset to be made publicly available.

Abstract: Outdoor 3D semantic scene generation produces realistic and semantically rich
environments for applications such as urban simulation and autonomous driving.
However, advances in this direction are constrained by the absence of publicly
available, well-annotated datasets. We introduce SketchSem3D, the first
large-scale benchmark for generating 3D outdoor semantic scenes from abstract
freehand sketches and pseudo-labeled annotations of satellite images.
SketchSem3D includes two subsets, Sketch-based SemanticKITTI and Sketch-based
KITTI-360 (containing LiDAR voxels along with their corresponding sketches and
annotated satellite images), to enable standardized, rigorous, and diverse
evaluations. We also propose Cylinder Mamba Diffusion (CymbaDiff) that
significantly enhances spatial coherence in outdoor 3D scene generation.
CymbaDiff imposes structured spatial ordering, explicitly captures cylindrical
continuity and vertical hierarchy, and preserves both physical neighborhood
relationships and global context within the generated scenes. Extensive
experiments on SketchSem3D demonstrate that CymbaDiff achieves superior
semantic consistency, spatial realism, and cross-dataset generalization. The
code and dataset will be available at
https://github.com/Lillian-research-hub/CymbaDiff

</details>


### [36] [Real-Time Crowd Counting for Embedded Systems with Lightweight Architecture](https://arxiv.org/abs/2510.13250)
*Zhiyuan Zhao,Yubin Wen,Siyu Yang,Lichen Ning,Yuandong Liu,Junyu Gao*

Main category: cs.CV

TL;DR: A super real-time crowd counting model with stem-encoder-decoder structure that achieves the fastest inference speed while maintaining competitive accuracy, designed specifically for embedded systems.


<details>
  <summary>Details</summary>
Motivation: Existing crowd counting methods have excessive model parameters and complex calculations, making them unsuitable for practical embedded system applications that require real-time performance.

Method: Uses stem-encoder-decoder structure with large convolution kernels in stem network for receptive field, conditional channel weighting and multi-branch local fusion block in encoder for multi-scale feature merging, and feature pyramid networks to address incomplete fusion problems.

Result: Achieves 381.7 FPS on NVIDIA GTX 1080Ti and 71.9 FPS on NVIDIA Jetson TX1, making it the fastest model while maintaining competitive accuracy on three benchmarks.

Conclusion: The proposed network is suitable for super real-time crowd counting on embedded systems, offering the fastest inference speed while ensuring competitive counting accuracy.

Abstract: Crowd counting is a task of estimating the number of the crowd through
images, which is extremely valuable in the fields of intelligent security,
urban planning, public safety management, and so on. However, the existing
counting methods have some problems in practical application on embedded
systems for these fields, such as excessive model parameters, abundant complex
calculations, etc. The practical application of embedded systems requires the
model to be real-time, which means that the model is fast enough. Considering
the aforementioned problems, we design a super real-time model with a
stem-encoder-decoder structure for crowd counting tasks, which achieves the
fastest inference compared with state-of-the-arts. Firstly, large convolution
kernels in the stem network are used to enlarge the receptive field, which
effectively extracts detailed head information. Then, in the encoder part, we
use conditional channel weighting and multi-branch local fusion block to merge
multi-scale features with low computational consumption. This part is crucial
to the super real-time performance of the model. Finally, the feature pyramid
networks are added to the top of the encoder to alleviate its incomplete fusion
problems. Experiments on three benchmarks show that our network is suitable for
super real-time crowd counting on embedded systems, ensuring competitive
accuracy. At the same time, the proposed network reasoning speed is the
fastest. Specifically, the proposed network achieves 381.7 FPS on NVIDIA GTX
1080Ti and 71.9 FPS on NVIDIA Jetson TX1.

</details>


### [37] [Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs](https://arxiv.org/abs/2510.13251)
*Minji Kim,Taekyung Kim,Bohyung Han*

Main category: cs.CV

TL;DR: This paper investigates the internal information flow of Video Large Language Models (VideoLLMs) using mechanistic interpretability, revealing consistent patterns in temporal reasoning and showing that models can maintain performance while pruning substantial attention connections.


<details>
  <summary>Details</summary>
Motivation: Despite advances in VideoLLMs, their internal mechanisms for extracting and propagating video and textual information remain poorly understood. The study aims to uncover how these models perform temporal reasoning through internal information flow analysis.

Method: The researchers used mechanistic interpretability techniques to analyze the internal information flow of VideoLLMs across diverse video question answering tasks, examining cross-frame interactions and video-language integration patterns.

Result: Analysis revealed four key patterns: (1) early-to-middle layers show active cross-frame interactions for temporal reasoning, (2) middle layers progressively integrate video and language through alignment of video representations with temporal linguistic embeddings, (3) middle-to-late layers generate correct answers after integration, and (4) models can retain VideoQA performance while pruning up to 58% of attention edges in LLaVA-NeXT-7B-Video-FT.

Conclusion: The findings provide a blueprint for understanding how VideoLLMs perform temporal reasoning and offer practical insights for improving model interpretability and downstream generalization. The identified effective information pathways can guide model optimization and pruning strategies.

Abstract: Video Large Language Models (VideoLLMs) extend the capabilities of
vision-language models to spatiotemporal inputs, enabling tasks such as video
question answering (VideoQA). Despite recent advances in VideoLLMs, their
internal mechanisms on where and how they extract and propagate video and
textual information remain less explored. In this study, we investigate the
internal information flow of VideoLLMs using mechanistic interpretability
techniques. Our analysis reveals consistent patterns across diverse VideoQA
tasks: (1) temporal reasoning in VideoLLMs initiates with active cross-frame
interactions in early-to-middle layers, (2) followed by progressive
video-language integration in middle layers. This is facilitated by alignment
between video representations and linguistic embeddings containing temporal
concepts. (3) Upon completion of this integration, the model is ready to
generate correct answers in middle-to-late layers. (4) Based on our analysis,
we show that VideoLLMs can retain their VideoQA performance by selecting these
effective information pathways while suppressing a substantial amount of
attention edges, e.g., 58% in LLaVA-NeXT-7B-Video-FT. These findings provide a
blueprint on how VideoLLMs perform temporal reasoning and offer practical
insights for improving model interpretability and downstream generalization.
Our project page with the source code is available at
https://map-the-flow.github.io

</details>


### [38] [End-to-End Multi-Modal Diffusion Mamba](https://arxiv.org/abs/2510.13253)
*Chunhao Lu,Qiang Lu,Meichen Dong,Jake Luo*

Main category: cs.CV

TL;DR: MDM (Multi-modal Diffusion Mamba) is a unified architecture that uses a Mamba-based diffusion model with variational autoencoder for joint multi-modal processing, outperforming existing end-to-end models and competing with SOTA models.


<details>
  <summary>Details</summary>
Motivation: Current end-to-end multi-modal models use separate encoders and decoders for different modalities, which hinders joint representation learning across modalities.

Method: Proposes MDM using Mamba-based multi-step selection diffusion model with unified variational autoencoder for both encoding and decoding, enabling progressive generation and refinement of modality-specific information.

Result: MDM significantly outperforms existing end-to-end models (MonoFormer, LlamaGen, Chameleon) and competes effectively with SOTA models like GPT-4V, Gemini Pro, and Mistral in image generation, captioning, VQA, text comprehension, and reasoning tasks.

Conclusion: MDM effectively unifies multi-modal processes while maintaining computational efficiency, establishing a new direction for end-to-end multi-modal architectures.

Abstract: Current end-to-end multi-modal models utilize different encoders and decoders
to process input and output information. This separation hinders the joint
representation learning of various modalities. To unify multi-modal processing,
we propose a novel architecture called MDM (Multi-modal Diffusion Mamba). MDM
utilizes a Mamba-based multi-step selection diffusion model to progressively
generate and refine modality-specific information through a unified variational
autoencoder for both encoding and decoding. This innovative approach allows MDM
to achieve superior performance when processing high-dimensional data,
particularly in generating high-resolution images and extended text sequences
simultaneously. Our evaluations in areas such as image generation, image
captioning, visual question answering, text comprehension, and reasoning tasks
demonstrate that MDM significantly outperforms existing end-to-end models
(MonoFormer, LlamaGen, and Chameleon etc.) and competes effectively with SOTA
models like GPT-4V, Gemini Pro, and Mistral. Our results validate MDM's
effectiveness in unifying multi-modal processes while maintaining computational
efficiency, establishing a new direction for end-to-end multi-modal
architectures.

</details>


### [39] [MMLongCite: A Benchmark for Evaluating Fidelity of Long-Context Vision-Language Models](https://arxiv.org/abs/2510.13276)
*Keyan Zhou,Zecheng Tang,Lingfeng Ming,Guanghao Zhou,Qiguang Chen,Dan Qiao,Zheming Yang,Libo Qin,Minghui Qiu,Juntao Li,Min Zhang*

Main category: cs.CV

TL;DR: MMLongCite is a new benchmark that evaluates large vision language models' faithfulness in long-context scenarios across text, images, and videos, revealing current models' limitations in effectively utilizing extended multimodal contexts.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of long-context faithfulness focus mainly on text-only domains, while multimodal assessments are limited to short contexts, creating a gap in understanding how LVLMs handle extended multimodal contexts.

Method: Developed MMLongCite benchmark with 8 distinct tasks spanning 6 context length intervals, incorporating diverse modalities including text, images, and videos to systematically evaluate LVLMs' long-context faithfulness.

Result: Evaluation of state-of-the-art LVLMs revealed limited faithfulness in handling long multimodal contexts, with performance affected by both context length and the position of crucial content within the context.

Conclusion: Extended context windows in LVLMs don't guarantee effective context utilization, highlighting the need for improved multimodal long-context handling capabilities and better evaluation frameworks like MMLongCite.

Abstract: The rapid advancement of large vision language models (LVLMs) has led to a
significant expansion of their context windows. However, an extended context
window does not guarantee the effective utilization of the context, posing a
critical challenge for real-world applications. Current evaluations of such
long-context faithfulness are predominantly focused on the text-only domain,
while multimodal assessments remain limited to short contexts. To bridge this
gap, we introduce MMLongCite, a comprehensive benchmark designed to evaluate
the fidelity of LVLMs in long-context scenarios. MMLongCite comprises 8
distinct tasks spanning 6 context length intervals and incorporates diverse
modalities, including text, images, and videos. Our evaluation of
state-of-the-art LVLMs reveals their limited faithfulness in handling long
multimodal contexts. Furthermore, we provide an in-depth analysis of how
context length and the position of crucial content affect the faithfulness of
these models.

</details>


### [40] [Universal Image Restoration Pre-training via Masked Degradation Classification](https://arxiv.org/abs/2510.13282)
*JiaKui Hu,Zhengjian Yao,Lujia Jin,Yinghao Chen,Yanye Lu*

Main category: cs.CV

TL;DR: MaskDCPT is a pre-training method that uses masked degradation classification to improve image restoration by identifying degradation types and reconstructing high-quality images, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive image restoration pre-training method that leverages degradation type classification as weak supervision while enhancing performance through image reconstruction, addressing limitations of conventional pre-training approaches.

Method: Uses an encoder with two decoders: encoder extracts features from masked low-quality images, classification decoder identifies degradation types, and reconstruction decoder reconstructs high-quality images. Combines masked image modeling and contrastive learning.

Result: Achieves minimum 3.77 dB PSNR increase in 5D all-in-one restoration and 34.8% PIQE reduction in real-world scenarios. Shows strong generalization to unseen degradation types and levels. Works well for both CNNs and Transformers.

Conclusion: MaskDCPT provides an effective pre-training approach for universal image restoration, demonstrating substantial performance improvements and strong generalization capabilities across various degradation scenarios.

Abstract: This study introduces a Masked Degradation Classification Pre-Training method
(MaskDCPT), designed to facilitate the classification of degradation types in
input images, leading to comprehensive image restoration pre-training. Unlike
conventional pre-training methods, MaskDCPT uses the degradation type of the
image as an extremely weak supervision, while simultaneously leveraging the
image reconstruction to enhance performance and robustness. MaskDCPT includes
an encoder and two decoders: the encoder extracts features from the masked
low-quality input image. The classification decoder uses these features to
identify the degradation type, whereas the reconstruction decoder aims to
reconstruct a corresponding high-quality image. This design allows the
pre-training to benefit from both masked image modeling and contrastive
learning, resulting in a generalized representation suited for restoration
tasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained
encoder can be used to address universal image restoration and achieve
outstanding performance. Implementing MaskDCPT significantly improves
performance for both convolution neural networks (CNNs) and Transformers, with
a minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and
a 34.8% reduction in PIQE compared to baseline in real-world degradation
scenarios. It also emergences strong generalization to previously unseen
degradation types and levels. In addition, we curate and release the UIR-2.5M
dataset, which includes 2.5 million paired restoration samples across 19
degradation types and over 200 degradation levels, incorporating both synthetic
and real-world data. The dataset, source code, and models are available at
https://github.com/MILab-PKU/MaskDCPT.

</details>


### [41] [Automated document processing system for government agencies using DBNET++ and BART models](https://arxiv.org/abs/2510.13303)
*Aya Kaysan Bahjat*

Main category: cs.CV

TL;DR: An automatic document classification system that detects text in images and categorizes documents into Invoice, Report, Letter, and Form using DBNet++ for text detection and BART for classification.


<details>
  <summary>Details</summary>
Motivation: To address practical challenges in document classification including variable illumination, arbitrary orientation, curved/occluded text, low resolution, and distant text in both offline and real-time image sources.

Method: Four-stage pipeline: image capture/preprocessing, text detection using DBNet++, text classification using BART, all integrated in a Python PyQt5 user interface.

Result: Achieved 92.88% text detection accuracy on Total-Text dataset with challenging high-resolution images after 10 hours of testing.

Conclusion: The proposed approach is effective for practical, mixed-source document categorization in unconstrained imaging scenarios.

Abstract: An automatic document classification system is presented that detects textual
content in images and classifies documents into four predefined categories
(Invoice, Report, Letter, and Form). The system supports both offline images
(e.g., files on flash drives, HDDs, microSD) and real-time capture via
connected cameras, and is designed to mitigate practical challenges such as
variable illumination, arbitrary orientation, curved or partially occluded
text, low resolution, and distant text. The pipeline comprises four stages:
image capture and preprocessing, text detection [1] using a DBNet++
(Differentiable Binarization Network Plus) detector, and text classification
[2] using a BART (Bidirectional and Auto-Regressive Transformers) classifier,
all integrated within a user interface implemented in Python with PyQt5. The
achieved results by the system for text detection in images were good at about
92.88% through 10 hours on Total-Text dataset that involve high resolution
images simulate a various and very difficult challenges. The results indicate
the proposed approach is effective for practical, mixed-source document
categorization in unconstrained imaging scenarios.

</details>


### [42] [Novel Class Discovery for Point Cloud Segmentation via Joint Learning of Causal Representation and Reasoning](https://arxiv.org/abs/2510.13307)
*Yang Li,Aming Wu,Zihao Zhang,Yahong Han*

Main category: cs.CV

TL;DR: This paper proposes a novel method for 3D Novel Class Discovery (3D-NCD) using structural causal modeling to learn causal representations and enable reasoning from base to novel classes for point cloud segmentation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve the challenge of segmenting unlabeled (novel) 3D classes using only supervision from labeled (base) 3D classes, where traditional correlation learning approaches may lead to confusion in novel class inference.

Method: The authors introduce a structural causal model (SCM) to reformulate 3D-NCD and propose Joint Learning of Causal Representation and Reasoning. They analyze hidden confounders in base class representations, devise causal representation prototypes to eliminate confounders, and use graph structures to model causal relationships between base and novel class prototypes.

Result: Extensive experiments and visualization results on 3D and 2D NCD semantic segmentation demonstrate the superior performance of the proposed method.

Conclusion: By imposing causal relationships as strong correlated constraints and learning causal representations, the method successfully uncovers essential point cloud representations that accurately correspond to classes, enabling effective novel class discovery in 3D segmentation tasks.

Abstract: In this paper, we focus on Novel Class Discovery for Point Cloud Segmentation
(3D-NCD), aiming to learn a model that can segment unlabeled (novel) 3D classes
using only the supervision from labeled (base) 3D classes. The key to this task
is to setup the exact correlations between the point representations and their
base class labels, as well as the representation correlations between the
points from base and novel classes. A coarse or statistical correlation
learning may lead to the confusion in novel class inference. lf we impose a
causal relationship as a strong correlated constraint upon the learning
process, the essential point cloud representations that accurately correspond
to the classes should be uncovered. To this end, we introduce a structural
causal model (SCM) to re-formalize the 3D-NCD problem and propose a new method,
i.e., Joint Learning of Causal Representation and Reasoning. Specifically, we
first analyze hidden confounders in the base class representations and the
causal relationships between the base and novel classes through SCM. We devise
a causal representation prototype that eliminates confounders to capture the
causal representations of base classes. A graph structure is then used to model
the causal relationships between the base classes' causal representation
prototypes and the novel class prototypes, enabling causal reasoning from base
to novel classes. Extensive experiments and visualization results on 3D and 2D
NCD semantic segmentation demonstrate the superiorities of our method.

</details>


### [43] [Self-Augmented Visual Contrastive Decoding](https://arxiv.org/abs/2510.13315)
*Eun Woo Im,Muhammad Kashif Ali,Vivek Gupta*

Main category: cs.CV

TL;DR: A training-free decoding strategy for Large Vision-Language Models that reduces hallucinations through query-dependent visual augmentation and adaptive thresholding.


<details>
  <summary>Details</summary>
Motivation: LVLMs inherit hallucination tendencies from language models, and existing visual contrastive methods use generic augmentations that ignore text query context, limiting effectiveness.

Method: Two key components: 1) Self-augmentation prompting that uses model's intrinsic knowledge to align semantics between query and visual augmentation, 2) Adaptive thresholding algorithm that adjusts next token candidate size based on output sparsity using full logit distribution.

Result: Extensive experiments across four LVLMs and seven benchmarks show significant improvement in factual consistency compared to state-of-the-art decoding methods.

Conclusion: Integrating query-dependent augmentation and entropy-aware decoding is crucial for improving effective generation of LVLMs and reducing hallucinations.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable multimodal
capabilities, but they inherit the tendency to hallucinate from their
underlying language models. While visual contrastive decoding has been proposed
to mitigate this issue, existing methods often apply generic visual
augmentations that disregard the specific context provided by the text query,
limiting their effectiveness. This study introduces a novel training-free
decoding strategy that addresses these limitations, featuring two key
contributions. First, a self-augmentation prompting strategy that leverages the
intrinsic knowledge of the model to dynamically align semantics between the
query and the visual augmentation. Second, an adaptive thresholding algorithm
that adaptively adjusts next token candidate size based on the output sparsity,
utilizing full information from the logit distribution. Extensive experiments
across four LVLMs and seven benchmarks demonstrate that the proposed decoding
significantly enhances factual consistency compared to state-of-the-art
decoding methods. This work highlights the importance of integrating
query-dependent augmentation and entropy-aware decoding for improving effective
generation of LVLMs.

</details>


### [44] [InstantSfM: Fully Sparse and Parallel Structure-from-Motion](https://arxiv.org/abs/2510.13310)
*Jiankun Zhong,Zitong Zhan,Quankai Gao,Ziyu Chen,Haozhe Lou,Jiageng Mao,Ulrich Neumann,Yue Wang*

Main category: cs.CV

TL;DR: A GPU-accelerated Structure-from-Motion method that achieves up to 40x speedup over COLMAP while maintaining comparable accuracy, especially effective for large-scale scenarios with thousands of images.


<details>
  <summary>Details</summary>
Motivation: Traditional SfM methods like COLMAP and GLOMAP suffer from computational overhead in large-scale scenarios and lack flexibility, while deep learning approaches like VGGSfM and VGGT cannot scale to thousands of views due to GPU memory limitations.

Method: Leverages GPU parallel computation to accelerate all critical stages of SfM pipeline, extending sparse-aware bundle adjustment optimization techniques to both bundle adjustment and global positioning within a unified global SfM framework.

Result: Achieves up to 40x speedup over COLMAP while maintaining comparable or improved reconstruction accuracy, successfully handling datasets with 5000+ images where other methods fail due to memory constraints.

Conclusion: The proposed GPU-accelerated SfM method effectively addresses the speed-accuracy trade-off in traditional SfM and scalability limitations in learning-based approaches, demonstrating significant performance improvements for large-scale 3D reconstruction.

Abstract: Structure-from-Motion (SfM), a method that recovers camera poses and scene
geometry from uncalibrated images, is a central component in robotic
reconstruction and simulation. Despite the state-of-the-art performance of
traditional SfM methods such as COLMAP and its follow-up work, GLOMAP, naive
CPU-specialized implementations of bundle adjustment (BA) or global positioning
(GP) introduce significant computational overhead when handling large-scale
scenarios, leading to a trade-off between accuracy and speed in SfM. Moreover,
the blessing of efficient C++-based implementations in COLMAP and GLOMAP comes
with the curse of limited flexibility, as they lack support for various
external optimization options. On the other hand, while deep learning based SfM
pipelines like VGGSfM and VGGT enable feed-forward 3D reconstruction, they are
unable to scale to thousands of input views at once as GPU memory consumption
increases sharply as the number of input views grows. In this paper, we unleash
the full potential of GPU parallel computation to accelerate each critical
stage of the standard SfM pipeline. Building upon recent advances in
sparse-aware bundle adjustment optimization, our design extends these
techniques to accelerate both BA and GP within a unified global SfM framework.
Through extensive experiments on datasets of varying scales (e.g. 5000 images
where VGGSfM and VGGT run out of memory), our method demonstrates up to about
40 times speedup over COLMAP while achieving consistently comparable or even
improved reconstruction accuracy. Our project page can be found at
https://cre185.github.io/InstantSfM/.

</details>


### [45] [Visual Interestingness Decoded: How GPT-4o Mirrors Human Interests](https://arxiv.org/abs/2510.13316)
*Fitim Abdullahu,Helmut Grabner*

Main category: cs.CV

TL;DR: This paper explores how well Large Multimodal Models (LMMs) like GPT-4o capture visual interestingness concepts and align with human assessments, using this alignment to create training data for learning-to-rank models.


<details>
  <summary>Details</summary>
Motivation: To understand how well modern LMMs capture the concept of visual interestingness and examine the alignment between human assessments and AI model predictions, given the importance of attention and interestingness in daily life.

Method: Comparative analysis between human assessments and GPT-4o's predictions of visual interestingness, using the aligned results to create labeled image pairs for training a learning-to-rank model through knowledge distillation.

Result: Partial alignment between humans and GPT-4o was found, with GPT-4o capturing the concept of visual interestingness better than state-of-the-art methods, enabling effective labeling of image pairs for training.

Conclusion: The findings provide insights for deeper understanding of human interest and demonstrate the potential of using LMMs to capture and model visual interestingness concepts.

Abstract: Our daily life is highly influenced by what we consume and see. Attracting
and holding one's attention -- the definition of (visual) interestingness -- is
essential. The rise of Large Multimodal Models (LMMs) trained on large-scale
visual and textual data has demonstrated impressive capabilities. We explore
these models' potential to understand to what extent the concepts of visual
interestingness are captured and examine the alignment between human
assessments and GPT-4o's, a leading LMM, predictions through comparative
analysis. Our studies reveal partial alignment between humans and GPT-4o. It
already captures the concept as best compared to state-of-the-art methods.
Hence, this allows for the effective labeling of image pairs according to their
(commonly) interestingness, which are used as training data to distill the
knowledge into a learning-to-rank model. The insights pave the way for a deeper
understanding of human interest.

</details>


### [46] [Language as a Label: Zero-Shot Multimodal Classification of Everyday Postures under Data Scarcity](https://arxiv.org/abs/2510.13364)
*MingZe Tang,Jubal Chandy Jacob*

Main category: cs.CV

TL;DR: Simple prompts outperform detailed ones in zero-shot classification of human postures using VLMs, with MetaCLIP 2 and OpenCLIP showing best results with basic prompts while SigLip benefits from more descriptive prompts.


<details>
  <summary>Details</summary>
Motivation: To understand how prompt design affects zero-shot classification of visually similar categories like human postures in Vision-Language Models, especially under data-scarce conditions.

Method: Evaluated modern VLMs (OpenCLIP, MetaCLIP 2, SigLip) on a 285-image COCO-derived dataset using three-tiered prompt design that systematically increases linguistic detail for classifying sitting, standing, and walking/running.

Result: Counter-intuitive trend: highest-performing models (MetaCLIP 2 and OpenCLIP) achieved best results with simplest prompts (68.8% accuracy), while detailed prompts degraded performance (55.1% accuracy). Lower-performing SigLip improved with descriptive, body-cue-based prompts.

Conclusion: Prompt overfitting occurs when adding descriptive detail harms performance in high-performing VLMs, while lower-performing models may benefit from more specific prompts, suggesting optimal prompt design depends on model capabilities.

Abstract: Recent Vision-Language Models (VLMs) enable zero-shot classification by
aligning images and text in a shared space, a promising approach for
data-scarce conditions. However, the influence of prompt design on recognizing
visually similar categories, such as human postures, is not well understood.
This study investigates how prompt specificity affects the zero-shot
classification of sitting, standing, and walking/running on a small, 285-image
COCO-derived dataset. A suite of modern VLMs, including OpenCLIP, MetaCLIP 2,
and SigLip, were evaluated using a three-tiered prompt design that
systematically increases linguistic detail. Our findings reveal a compelling,
counter-intuitive trend: for the highest-performing models (MetaCLIP 2 and
OpenCLIP), the simplest, most basic prompts consistently achieve the best
results. Adding descriptive detail significantly degrades performance for
instance, MetaCLIP 2's multi-class accuracy drops from 68.8\% to 55.1\% a
phenomenon we term "prompt overfitting". Conversely, the lower-performing
SigLip model shows improved classification on ambiguous classes when given more
descriptive, body-cue-based prompts.

</details>


### [47] [Removing Cost Volumes from Optical Flow Estimators](https://arxiv.org/abs/2510.13317)
*Simon Kiefhaber,Stefan Roth,Simone Schaub-Meyer*

Main category: cs.CV

TL;DR: Training strategy that removes cost volumes from optical flow estimators after sufficient training, leading to faster inference and lower memory usage while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Cost volumes are computationally expensive and memory-intensive, limiting processing speed and input resolution in optical flow estimation.

Method: Introduce a training strategy that removes cost volumes once other network components are sufficiently trained, creating three models for different compute budgets.

Result: Most accurate model achieves state-of-the-art accuracy with 1.2x faster inference and 6x lower memory footprint; fastest model processes Full HD at 20 FPS using only 500MB GPU memory.

Conclusion: Cost volumes can be successfully removed from optical flow estimators through proper training strategy, enabling significant improvements in speed and memory efficiency without sacrificing accuracy.

Abstract: Cost volumes are used in every modern optical flow estimator, but due to
their computational and space complexity, they are often a limiting factor
regarding both processing speed and the resolution of input frames. Motivated
by our empirical observation that cost volumes lose their importance once all
other network parts of, e.g., a RAFT-based pipeline have been sufficiently
trained, we introduce a training strategy that allows removing the cost volume
from optical flow estimators throughout training. This leads to significantly
improved inference speed and reduced memory requirements. Using our training
strategy, we create three different models covering different compute budgets.
Our most accurate model reaches state-of-the-art accuracy while being
$1.2\times$ faster and having a $6\times$ lower memory footprint than
comparable models; our fastest model is capable of processing Full HD frames at
$20\,\mathrm{FPS}$ using only $500\,\mathrm{MB}$ of GPU memory.

</details>


### [48] [DEF-YOLO: Leveraging YOLO for Concealed Weapon Detection in Thermal Imagin](https://arxiv.org/abs/2510.13326)
*Divya Bhardwaj,Arnav Ramamoorthy,Poonam Goyal*

Main category: cs.CV

TL;DR: Proposes DEF-YOLO, an enhanced YOLOv8 architecture for concealed weapon detection in thermal imagery, along with a new large-scale Thermal Imaging Concealed Weapon (TICW) dataset.


<details>
  <summary>Details</summary>
Motivation: To provide a real-time, 24x7 surveillance solution that is low-cost and privacy-preserved for concealed weapon detection, addressing limitations of other imaging modalities like poor resolution in microwave and privacy concerns in millimeter wave imaging.

Method: Enhanced YOLOv8 with deformable convolutions at SPPF layer to exploit multi-scale features, backbone and neck layers to extract features at different levels, and incorporated focal loss to handle class imbalance. Introduced a new large-scale TICW dataset.

Result: The proposed DEF-YOLO architecture achieves adaptive focus on localization around objects in thermal homogeneous regions without sacrificing speed and throughput, establishing a new benchmark for concealed weapon detection in thermal imagery.

Conclusion: The work provides an effective solution for concealed weapon detection using thermal imaging, overcoming limitations of other modalities while maintaining real-time performance and privacy preservation.

Abstract: Concealed weapon detection aims at detecting weapons hidden beneath a
person's clothing or luggage. Various imaging modalities like Millimeter Wave,
Microwave, Terahertz, Infrared, etc., are exploited for the concealed weapon
detection task. These imaging modalities have their own limitations, such as
poor resolution in microwave imaging, privacy concerns in millimeter wave
imaging, etc. To provide a real-time, 24 x 7 surveillance, low-cost, and
privacy-preserved solution, we opted for thermal imaging in spite of the lack
of availability of a benchmark dataset. We propose a novel approach and a
dataset for concealed weapon detection in thermal imagery. Our YOLO-based
architecture, DEF-YOLO, is built with key enhancements in YOLOv8 tailored to
the unique challenges of concealed weapon detection in thermal vision. We adopt
deformable convolutions at the SPPF layer to exploit multi-scale features;
backbone and neck layers to extract low, mid, and high-level features, enabling
DEF-YOLO to adaptively focus on localization around the objects in thermal
homogeneous regions, without sacrificing much of the speed and throughput. In
addition to these simple yet effective key architectural changes, we introduce
a new, large-scale Thermal Imaging Concealed Weapon dataset, TICW, featuring a
diverse set of concealed weapons and capturing a wide range of scenarios. To
the best of our knowledge, this is the first large-scale contributed dataset
for this task. We also incorporate focal loss to address the significant class
imbalance inherent in the concealed weapon detection task. The efficacy of the
proposed work establishes a new benchmark through extensive experimentation for
concealed weapon detection in thermal imagery.

</details>


### [49] [Group-Wise Optimization for Self-Extensible Codebooks in Vector Quantized Models](https://arxiv.org/abs/2510.13331)
*Hong-Kai Zheng,Piji Li*

Main category: cs.CV

TL;DR: Group-VQ improves VQ-VAE by using group-wise codebook optimization to address codebook collapse issues, achieving better reconstruction quality while enabling flexible post-training codebook size adjustment.


<details>
  <summary>Details</summary>
Motivation: VQ-VAEs suffer from codebook collapse problems, and existing solutions using static codebooks or joint optimization limit learning capability and reduce reconstruction quality.

Method: Proposes Group-VQ with group-wise codebook optimization where each group is optimized independently with joint optimization within groups, plus a training-free codebook resampling method for post-training size adjustment.

Result: Group-VQ shows improved performance on reconstruction metrics in various image reconstruction experiments, and the codebook sampling method achieves desired flexibility in adjusting codebook size.

Conclusion: Group-VQ provides a better trade-off between codebook utilization and reconstruction performance while enabling flexible post-training codebook size adjustment.

Abstract: Vector Quantized Variational Autoencoders (VQ-VAEs) leverage self-supervised
learning through reconstruction tasks to represent continuous vectors using the
closest vectors in a codebook. However, issues such as codebook collapse
persist in the VQ model. To address these issues, existing approaches employ
implicit static codebooks or jointly optimize the entire codebook, but these
methods constrain the codebook's learning capability, leading to reduced
reconstruction quality. In this paper, we propose Group-VQ, which performs
group-wise optimization on the codebook. Each group is optimized independently,
with joint optimization performed within groups. This approach improves the
trade-off between codebook utilization and reconstruction performance.
Additionally, we introduce a training-free codebook resampling method, allowing
post-training adjustment of the codebook size. In image reconstruction
experiments under various settings, Group-VQ demonstrates improved performance
on reconstruction metrics. And the post-training codebook sampling method
achieves the desired flexibility in adjusting the codebook size.

</details>


### [50] [No-Reference Rendered Video Quality Assessment: Dataset and Metrics](https://arxiv.org/abs/2510.13349)
*Sipeng Yang,Jiayu Ji,Qingchuan Zhu,Zhiyao Yang,Xiaogang Jin*

Main category: cs.CV

TL;DR: A new no-reference video quality assessment (NR-VQA) metric and dataset specifically designed for rendered videos, addressing the limitations of existing camera-focused metrics.


<details>
  <summary>Details</summary>
Motivation: Existing NR-VQA methods are biased for rendered videos because they focus on camera-captured content and don't properly handle temporal artifacts common in rendered videos. There's a need for specialized assessment tools for computer graphics applications like games, VR, and AR.

Method: Created a large rendering-oriented video dataset with subjective quality annotations across various 3D scenes and rendering settings. Designed a NR-VQA metric that evaluates both image quality and temporal stability specifically for rendered videos.

Result: The proposed metric demonstrates superior performance compared to existing NR-VQA metrics when applied to rendered videos. It effectively benchmarks supersampling methods and assesses frame generation strategies in real-time rendering.

Conclusion: The specialized NR-VQA metric and dataset successfully address the unique quality assessment needs of rendered videos, providing better evaluation tools for computer graphics applications where temporal artifacts are common.

Abstract: Quality assessment of videos is crucial for many computer graphics
applications, including video games, virtual reality, and augmented reality,
where visual performance has a significant impact on user experience. When test
videos cannot be perfectly aligned with references or when references are
unavailable, the significance of no-reference video quality assessment (NR-VQA)
methods is undeniable. However, existing NR-VQA datasets and metrics are
primarily focused on camera-captured videos; applying them directly to rendered
videos would result in biased predictions, as rendered videos are more prone to
temporal artifacts. To address this, we present a large rendering-oriented
video dataset with subjective quality annotations, as well as a designed NR-VQA
metric specific to rendered videos. The proposed dataset includes a wide range
of 3D scenes and rendering settings, with quality scores annotated for various
display types to better reflect real-world application scenarios. Building on
this dataset, we calibrate our NR-VQA metric to assess rendered video quality
by looking at both image quality and temporal stability. We compare our metric
to existing NR-VQA metrics, demonstrating its superior performance on rendered
videos. Finally, we demonstrate that our metric can be used to benchmark
supersampling methods and assess frame generation strategies in real-time
rendering.

</details>


### [51] [DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning](https://arxiv.org/abs/2510.13375)
*Tianyuan Yuan,Yicheng Liu,Chenhao Lu,Zhuoguang Chen,Tao Jiang,Hang Zhao*

Main category: cs.CV

TL;DR: DepthVLA is a Vision-Language-Action model that enhances spatial reasoning by incorporating a pretrained depth prediction module, outperforming state-of-the-art approaches in both real-world and simulated environments.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models suffer from limited spatial reasoning capabilities inherited from VLMs, requiring extensive action-data pretraining that reduces efficiency and still provides insufficient spatial understanding.

Method: DepthVLA uses a mixture-of-transformers design that unifies a VLM, a depth transformer, and an action expert with fully shared attentions, forming an end-to-end model with explicit spatial awareness through pretrained depth prediction.

Result: DepthVLA achieves 78.5% vs. 65.0% progress in real-world tasks, 94.9% vs. 93.6% in LIBERO simulator, and 74.8% vs. 58.8% in Simpler simulator, outperforming state-of-the-art approaches.

Conclusion: Explicitly incorporating spatial awareness through depth prediction significantly enhances VLA performance on tasks requiring precise spatial reasoning, demonstrating the effectiveness of the proposed DepthVLA architecture.

Abstract: Vision-Language-Action (VLA) models have recently shown impressive
generalization and language-guided manipulation capabilities. However, their
performance degrades on tasks requiring precise spatial reasoning due to
limited spatial reasoning inherited from Vision-Language Models (VLMs).
Existing VLAs rely on extensive action-data pretraining to ground VLMs in 3D
space, which reduces training efficiency and is still insufficient for accurate
spatial understanding. In this work, we present DepthVLA, a simple yet
effective VLA architecture that explicitly incorporates spatial awareness
through a pretrained depth prediction module. DepthVLA adopts a
mixture-of-transformers design that unifies a VLM, a depth transformer, and an
action expert with fully shared attentions, forming an end-to-end model with
enhanced spatial reasoning. Extensive evaluations in both real-world and
simulated environments show that DepthVLA outperforms state-of-the-art
approaches, achieving 78.5% vs. 65.0% progress in real-world tasks, 94.9% vs.
93.6% in the LIBERO simulator, and 74.8% vs. 58.8% in the Simpler simulator.
Our code will be made publicly available.

</details>


### [52] [Leveraging 2D Priors and SDF Guidance for Dynamic Urban Scene Rendering](https://arxiv.org/abs/2510.13381)
*Siddharth Tourani,Jayaram Reddy,Akash Kumbar,Satyajit Tourani,Nishant Goyal,Madhava Krishna,N. Dinesh Reddy,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: A novel method combining Signed Distance Functions (SDFs) with 3D Gaussian Splatting (3DGS) for dynamic scene rendering and reconstruction, reducing dependency on LiDAR data and ground-truth annotations while achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of existing 3DGS methods that require camera and LiDAR data, ground-truth 3D segmentations, and motion data for dynamic urban scene modeling.

Method: Integrates SDFs with 3DGS using 2D object-agnostic priors (depth and point tracking) in a unified optimization framework to enhance geometric accuracy and deformation modeling.

Result: Achieves state-of-the-art rendering metrics without LiDAR data, and further improves with LiDAR for reconstruction and novel view generation across diverse object categories without ground-truth 3D motion annotation.

Conclusion: The method enables robust dynamic scene representation and supports various scene editing tasks like decomposition and composition, demonstrating effectiveness in reducing data requirements while maintaining high performance.

Abstract: Dynamic scene rendering and reconstruction play a crucial role in computer
vision and augmented reality. Recent methods based on 3D Gaussian Splatting
(3DGS), have enabled accurate modeling of dynamic urban scenes, but for urban
scenes they require both camera and LiDAR data, ground-truth 3D segmentations
and motion data in the form of tracklets or pre-defined object templates such
as SMPL. In this work, we explore whether a combination of 2D object agnostic
priors in the form of depth and point tracking coupled with a signed distance
function (SDF) representation for dynamic objects can be used to relax some of
these requirements. We present a novel approach that integrates Signed Distance
Functions (SDFs) with 3D Gaussian Splatting (3DGS) to create a more robust
object representation by harnessing the strengths of both methods. Our unified
optimization framework enhances the geometric accuracy of 3D Gaussian splatting
and improves deformation modeling within the SDF, resulting in a more adaptable
and precise representation. We demonstrate that our method achieves
state-of-the-art performance in rendering metrics even without LiDAR data on
urban scenes. When incorporating LiDAR, our approach improved further in
reconstructing and generating novel views across diverse object categories,
without ground-truth 3D motion annotation. Additionally, our method enables
various scene editing tasks, including scene decomposition, and scene
composition.

</details>


### [53] [UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning](https://arxiv.org/abs/2510.13515)
*Tiancheng Gu,Kaicheng Yang,Kaichen Zhang,Xiang An,Ziyong Feng,Yueyi Zhang,Weidong Cai,Jiankang Deng,Lidong Bing*

Main category: cs.CV

TL;DR: UniME-V2 introduces MLLM-as-a-Judge mechanism to improve multimodal embedding by generating soft semantic matching scores for better hard negative mining and enhanced discriminative capacity, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal embedding models struggle with capturing subtle semantic differences, lack diversity in negative samples, and have limited ability to distinguish false/hard negatives.

Method: Constructs hard negative set via global retrieval, uses MLLMs to generate soft semantic matching scores for hard negative mining, aligns similarity matrix with soft scores, and introduces UniME-V2-Reranker with joint pairwise and listwise optimization.

Result: Achieves state-of-the-art performance on MMEB benchmark and multiple retrieval tasks across all tasks on average.

Conclusion: The MLLM-as-a-Judge mechanism effectively enhances representation learning by providing better hard negative mining and semantic distinction, leading to superior multimodal embedding performance.

Abstract: Universal multimodal embedding models are foundational to various tasks.
Existing approaches typically employ in-batch negative mining by measuring the
similarity of query-candidate pairs. However, these methods often struggle to
capture subtle semantic differences among candidates and lack diversity in
negative samples. Moreover, the embeddings exhibit limited discriminative
ability in distinguishing false and hard negatives. In this paper, we leverage
the advanced understanding capabilities of MLLMs to enhance representation
learning and present a novel Universal Multimodal Embedding (UniME-V2) model.
Our approach first constructs a potential hard negative set through global
retrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes
MLLMs to assess the semantic alignment of query-candidate pairs and generate
soft semantic matching scores. These scores serve as a foundation for hard
negative mining, mitigating the impact of false negatives and enabling the
identification of diverse, high-quality hard negatives. Furthermore, the
semantic matching scores are used as soft labels to mitigate the rigid
one-to-one mapping constraint. By aligning the similarity matrix with the soft
semantic matching score matrix, the model learns semantic distinctions among
candidates, significantly enhancing its discriminative capacity. To further
improve performance, we propose UniME-V2-Reranker, a reranking model trained on
our mined hard negatives through a joint pairwise and listwise optimization
approach. We conduct comprehensive experiments on the MMEB benchmark and
multiple retrieval tasks, demonstrating that our method achieves
state-of-the-art performance on average across all tasks.

</details>


### [54] [Generalizing WiFi Gesture Recognition via Large-Model-Aware Semantic Distillation and Alignment](https://arxiv.org/abs/2510.13390)
*Feng-Qi Cui,Yu-Tong Guo,Tianyue Zheng,Jinyang Huang*

Main category: cs.CV

TL;DR: GLSDA is a WiFi-based gesture recognition framework that uses large foundation models to improve generalization and semantic understanding, achieving state-of-the-art performance while reducing model size and latency.


<details>
  <summary>Details</summary>
Motivation: Existing WiFi gesture recognition methods suffer from limited generalization and semantic expressiveness due to domain-sensitive Channel State Information and lack of high-level gesture abstraction.

Method: Uses dual-path CSI encoding (CSI-Ratio phase sequences and Doppler spectrograms), Multiscale Semantic Encoder with cross-modal attention, Semantic-Aware Soft Supervision for inter-class correlations, and Robust Dual-Distillation to compress the model.

Result: Outperforms state-of-the-art methods in both in-domain and cross-domain gesture recognition on Widar3.0 benchmark, while significantly reducing model size and inference latency.

Conclusion: Provides a scalable and deployable solution for generalized RF-based gesture interfaces in real-world AIoT applications.

Abstract: WiFi-based gesture recognition has emerged as a promising RF sensing paradigm
for enabling non-contact and privacy-preserving human-computer interaction in
AIoT environments. However, existing methods often suffer from limited
generalization and semantic expressiveness due to the domain-sensitive nature
of Channel State Information and the lack of high-level gesture abstraction. To
address these challenges, we propose a novel generalization framework, termed
Large-Model-Aware Semantic Distillation and Alignment (GLSDA), which leverages
the semantic prior of pre-trained large foundation models to enhance gesture
representation learning in both in-domain and cross-domain scenarios.
Specifically, we first design a dual-path CSI encoding pipeline that captures
geometric and dynamic gesture patterns via CSI-Ratio phase sequences and
Doppler spectrograms. These representations are then fed into a Multiscale
Semantic Encoder, which learns robust temporal embeddings and aligns them with
gesture semantics through cross-modal attention mechanisms. To further enhance
category discrimination, we introduce a Semantic-Aware Soft Supervision scheme
that encodes inter-class correlations and reduces label ambiguity, especially
for semantically similar gestures. Finally, we develop a Robust
Dual-Distillation strategy to compress the aligned model into a lightweight
student network, jointly distilling intermediate features and semantic-informed
soft labels from the teacher model. Extensive experiments on the Widar3.0
benchmark show that GLSDA consistently outperforms state-of-the-art methods in
both in-domain and cross-domain gesture recognition tasks, while significantly
reducing model size and inference latency. Our method offers a scalable and
deployable solution for generalized RF-based gesture interfaces in real-world
AIoT applications.

</details>


### [55] [Spatial-DISE: A Unified Benchmark for Evaluating Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.13394)
*Xinmiao Huang,Qisong He,Zhenglin Huang,Boxuan Wang,Zhuoyun Li,Guangliang Cheng,Yi Dong,Xiaowei Huang*

Main category: cs.CV

TL;DR: Spatial-DISE is a unified benchmark for evaluating spatial reasoning in Vision Language Models (VLMs) based on a cognitively grounded taxonomy, addressing limitations in existing benchmarks and revealing significant gaps between current VLMs and human performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are inadequate for assessing spatial reasoning ability, particularly intrinsic-dynamic spatial reasoning which is fundamental to human spatial cognition. There's a need for better evaluation tools to support real-world applications in robotics, AR, and autonomous navigation.

Method: Developed a scalable automated pipeline to generate diverse and verifiable spatial reasoning questions, creating the Spatial-DISE dataset with 559 evaluation VQA pairs and 12K+ training pairs. Based on a taxonomy categorizing tasks into four quadrants: Intrinsic-Static, Intrinsic-Dynamic, Extrinsic-Static, and Extrinsic-Dynamic spatial reasoning.

Result: Evaluation of 28 state-of-the-art VLMs shows large and consistent gaps to human competence, especially on multi-step multi-view spatial reasoning tasks.

Conclusion: Spatial-DISE provides a robust framework, valuable dataset, and clear direction for future research toward achieving human-like spatial intelligence in VLMs.

Abstract: Spatial reasoning ability is crucial for Vision Language Models (VLMs) to
support real-world applications in diverse domains including robotics,
augmented reality, and autonomous navigation. Unfortunately, existing
benchmarks are inadequate in assessing spatial reasoning ability, especially
the \emph{intrinsic-dynamic} spatial reasoning which is a fundamental aspect of
human spatial cognition. In this paper, we propose a unified benchmark,
\textbf{Spatial-DISE}, based on a cognitively grounded taxonomy that
categorizes tasks into four fundamental quadrants:
\textbf{I}ntrinsic-\textbf{S}tatic, Intrinsic-\textbf{D}ynamic,
\textbf{E}xtrinsic-Static, and Extrinsic-Dynamic spatial reasoning. Moreover,
to address the issue of data scarcity, we develop a scalable and automated
pipeline to generate diverse and verifiable spatial reasoning questions,
resulting in a new \textbf{Spatial-DISE} dataset that includes Spatial-DISE
Bench (559 evaluation VQA pairs) and Spatial-DISE-12K (12K+ training VQA
pairs). Our comprehensive evaluation across 28 state-of-the-art VLMs reveals
that, current VLMs have a large and consistent gap to human competence,
especially on multi-step multi-view spatial reasoning. Spatial-DISE offers a
robust framework, valuable dataset, and clear direction for future research
toward human-like spatial intelligence. Benchmark, dataset, and code will be
publicly released.

</details>


### [56] [Modeling Cultural Bias in Facial Expression Recognition with Adaptive Agents](https://arxiv.org/abs/2510.13557)
*David Freire-Obregón,José Salas-Cáceres,Javier Lorenzo-Navarro,Oliverio J. Santana,Daniel Hernández-Sosa,Modesto Castrillón-Santana*

Main category: cs.CV

TL;DR: The paper introduces an agent-based benchmark to study how cultural composition and progressive blurring affect facial expression recognition robustness, revealing asymmetric degradation patterns between cultural groups.


<details>
  <summary>Details</summary>
Motivation: FER needs robustness under cultural variation and degraded visual conditions, but most evaluations assume homogeneous data and high-quality imagery.

Method: Agent-based streaming benchmark with frozen CLIP features and lightweight residual adapters, testing monocultural/mixed populations on a 5x5 lattice with sigma-scheduled Gaussian blur.

Result: JAFFE (Asian) populations maintain higher performance at low blur but drop sharply at intermediate stages, while KDEF (Western) populations degrade more uniformly. Mixed populations show intermediate patterns with balanced mixtures mitigating early degradation.

Conclusion: Cultural composition and interaction structure significantly influence FER robustness as perceptual conditions deteriorate, with clear asymmetric degradation curves between cultural groups.

Abstract: Facial expression recognition (FER) must remain robust under both cultural
variation and perceptually degraded visual conditions, yet most existing
evaluations assume homogeneous data and high-quality imagery. We introduce an
agent-based, streaming benchmark that reveals how cross-cultural composition
and progressive blurring interact to shape face recognition robustness. Each
agent operates in a frozen CLIP feature space with a lightweight residual
adapter trained online at sigma=0 and fixed during testing. Agents move and
interact on a 5x5 lattice, while the environment provides inputs with
sigma-scheduled Gaussian blur. We examine monocultural populations
(Western-only, Asian-only) and mixed environments with balanced (5/5) and
imbalanced (8/2, 2/8) compositions, as well as different spatial contact
structures. Results show clear asymmetric degradation curves between cultural
groups: JAFFE (Asian) populations maintain higher performance at low blur but
exhibit sharper drops at intermediate stages, whereas KDEF (Western)
populations degrade more uniformly. Mixed populations exhibit intermediate
patterns, with balanced mixtures mitigating early degradation, but imbalanced
settings amplify majority-group weaknesses under high blur. These findings
quantify how cultural composition and interaction structure influence the
robustness of FER as perceptual conditions deteriorate.

</details>


### [57] [Reinforcement Learning Meets Masked Generative Models: Mask-GRPO for Text-to-Image Generation](https://arxiv.org/abs/2510.13418)
*Yifu Luo,Xinhao Hu,Keyu Fan,Haoyuan Sun,Zeyu Chen,Bo Xia,Tiantian Zhang,Yongzhe Chang,Xueqian Wang*

Main category: cs.CV

TL;DR: Mask-GRPO is the first RL method for masked generative models in text-to-image generation, reformulating unmasking as multi-step decision-making and achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Most RL approaches for text-to-image generation focus on diffusion or autoregressive models, overlooking masked generative models which are an important alternative paradigm.

Method: Proposes Mask-GRPO incorporating Group Relative Policy Optimization into masked generative models by redefining transition probability and formulating unmasking as multi-step decision-making, with strategies like removing KL constraint and filtering low-quality samples.

Result: Substantially improves base model Show-o on standard T2I benchmarks and preference alignment, outperforming existing state-of-the-art approaches.

Conclusion: Mask-GRPO successfully bridges the gap in RL approaches for masked generative models and demonstrates superior performance in text-to-image generation.

Abstract: Reinforcement learning (RL) has garnered increasing attention in
text-to-image (T2I) generation. However, most existing RL approaches are
tailored to either diffusion models or autoregressive models, overlooking an
important alternative: masked generative models. In this work, we propose
Mask-GRPO, the first method to incorporate Group Relative Policy Optimization
(GRPO)-based RL into this overlooked paradigm. Our core insight is to redefine
the transition probability, which is different from current approaches, and
formulate the unmasking process as a multi-step decision-making problem. To
further enhance our method, we explore several useful strategies, including
removing the KL constraint, applying the reduction strategy, and filtering out
low-quality samples. Using Mask-GRPO, we improve a base model, Show-o, with
substantial improvements on standard T2I benchmarks and preference alignment,
outperforming existing state-of-the-art approaches. The code is available on
https://github.com/xingzhejun/Mask-GRPO

</details>


### [58] [Ultra High-Resolution Image Inpainting with Patch-Based Content Consistency Adapter](https://arxiv.org/abs/2510.13419)
*Jianhui Zhang,Sheng Cheng,Qirui Sun,Jia Liu,Wang Luyang,Chaoyu Feng,Chen Fang,Lei Lei,Jue Wang,Shuaicheng Liu*

Main category: cs.CV

TL;DR: Patch-Adapter is a framework for high-resolution text-guided image inpainting that achieves 4K+ resolution through a two-stage adapter architecture, addressing content consistency and prompt alignment challenges.


<details>
  <summary>Details</summary>
Motivation: Existing methods are limited to lower resolutions and struggle with maintaining content consistency and prompt alignment as resolution increases, especially with complex textures.

Method: Uses a two-stage adapter architecture: (1) Dual Context Adapter for global structural consistency at reduced resolutions, and (2) Reference Patch Adapter with patch-level attention for full-resolution inpainting and local detail preservation.

Result: Achieves state-of-the-art performance on OpenImages and Photo-Concept-Bucket datasets, resolving artifacts in large-scale inpainting and outperforming existing methods in perceptual quality and text-prompt adherence.

Conclusion: Patch-Adapter effectively bridges the scalability gap in high-resolution inpainting by decoupling global semantics from localized refinement, enabling 4K+ resolution inpainting without structural overhauls.

Abstract: In this work, we present Patch-Adapter, an effective framework for
high-resolution text-guided image inpainting. Unlike existing methods limited
to lower resolutions, our approach achieves 4K+ resolution while maintaining
precise content consistency and prompt alignment, two critical challenges in
image inpainting that intensify with increasing resolution and texture
complexity. Patch-Adapter leverages a two-stage adapter architecture to scale
the diffusion model's resolution from 1K to 4K+ without requiring structural
overhauls: (1) Dual Context Adapter learns coherence between masked and
unmasked regions at reduced resolutions to establish global structural
consistency; and (2) Reference Patch Adapter implements a patch-level attention
mechanism for full-resolution inpainting, preserving local detail fidelity
through adaptive feature fusion. This dual-stage architecture uniquely
addresses the scalability gap in high-resolution inpainting by decoupling
global semantics from localized refinement. Experiments demonstrate that
Patch-Adapter not only resolves artifacts common in large-scale inpainting but
also achieves state-of-the-art performance on the OpenImages and
Photo-Concept-Bucket datasets, outperforming existing methods in both
perceptual quality and text-prompt adherence.

</details>


### [59] [CoDS: Enhancing Collaborative Perception in Heterogeneous Scenarios via Domain Separation](https://arxiv.org/abs/2510.13432)
*Yushan Han,Hui Zhang,Honglei Zhang,Chuntao Ding,Yuanzhouhan Cao,Yidong Li*

Main category: cs.CV

TL;DR: CoDS is a collaborative perception method for autonomous driving that addresses feature discrepancies in heterogeneous scenarios using domain separation, achieving better accuracy-efficiency trade-off.


<details>
  <summary>Details</summary>
Motivation: Existing collaborative perception methods assume identical encoders for all agents, which doesn't hold in real-world heterogeneous scenarios. Current methods are vulnerable to noise from domain gaps and inefficient on mobile devices.

Method: Proposes CoDS with two alignment modules: Lightweight Spatial-Channel Resizer (LSCR) for spatial-channel alignment using lightweight conv layers, and Distribution Alignment via Domain Separation (DADS) with encoder-specific and encoder-agnostic modules. Uses Domain Alignment Mutual Information (DAMI) loss for training.

Result: CoDS effectively mitigates feature discrepancies in heterogeneous scenarios and achieves a good trade-off between detection accuracy and inference efficiency through its fully convolutional architecture.

Conclusion: CoDS successfully addresses feature discrepancies in heterogeneous collaborative perception scenarios while maintaining high inference efficiency, making it suitable for real-world autonomous driving applications.

Abstract: Collaborative perception has been proven to improve individual perception in
autonomous driving through multi-agent interaction. Nevertheless, most methods
often assume identical encoders for all agents, which does not hold true when
these models are deployed in real-world applications. To realize collaborative
perception in actual heterogeneous scenarios, existing methods usually align
neighbor features to those of the ego vehicle, which is vulnerable to noise
from domain gaps and thus fails to address feature discrepancies effectively.
Moreover, they adopt transformer-based modules for domain adaptation, which
causes the model inference inefficiency on mobile devices. To tackle these
issues, we propose CoDS, a Collaborative perception method that leverages
Domain Separation to address feature discrepancies in heterogeneous scenarios.
The CoDS employs two feature alignment modules, i.e., Lightweight
Spatial-Channel Resizer (LSCR) and Distribution Alignment via Domain Separation
(DADS). Besides, it utilizes the Domain Alignment Mutual Information (DAMI)
loss to ensure effective feature alignment. Specifically, the LSCR aligns the
neighbor feature across spatial and channel dimensions using a lightweight
convolutional layer. Subsequently, the DADS mitigates feature distribution
discrepancy with encoder-specific and encoder-agnostic domain separation
modules. The former removes domain-dependent information and the latter
captures task-related information. During training, the DAMI loss maximizes the
mutual information between aligned heterogeneous features to enhance the domain
separation process. The CoDS employs a fully convolutional architecture, which
ensures high inference efficiency. Extensive experiments demonstrate that the
CoDS effectively mitigates feature discrepancies in heterogeneous scenarios and
achieves a trade-off between detection accuracy and inference efficiency.

</details>


### [60] [Beyond Pixels: A Differentiable Pipeline for Probing Neuronal Selectivity in 3D](https://arxiv.org/abs/2510.13433)
*Pavithra Elumalai,Mohammad Bashiri,Goirik Chakrabarty,Suhas Shrinivasan,Fabian H. Sinz*

Main category: cs.CV

TL;DR: A differentiable rendering pipeline that optimizes deformable meshes in 3D to probe neuronal selectivity to interpretable 3D scene properties like shape, pose, and lighting, bridging inverse graphics with systems neuroscience.


<details>
  <summary>Details</summary>
Motivation: Current approaches mainly operate on 2D pixels, making it difficult to isolate neuronal selectivity for physical scene properties like shape, pose, and lighting in visual perception.

Method: Introduced a differentiable rendering pipeline that optimizes deformable meshes parameterized with radial basis functions, learning offsets and scales that maximize neuronal responses while enforcing geometric regularity.

Result: Applied to models of monkey area V4, the approach enables probing neuronal selectivity to interpretable 3D factors such as pose and lighting.

Conclusion: This method bridges inverse graphics with systems neuroscience, offering a way to probe neural selectivity with physically grounded, 3D stimuli beyond conventional pixel-based methods.

Abstract: Visual perception relies on inference of 3D scene properties such as shape,
pose, and lighting. To understand how visual sensory neurons enable robust
perception, it is crucial to characterize their selectivity to such physically
interpretable factors. However, current approaches mainly operate on 2D pixels,
making it difficult to isolate selectivity for physical scene properties. To
address this limitation, we introduce a differentiable rendering pipeline that
optimizes deformable meshes to obtain MEIs directly in 3D. The method
parameterizes mesh deformations with radial basis functions and learns offsets
and scales that maximize neuronal responses while enforcing geometric
regularity. Applied to models of monkey area V4, our approach enables probing
neuronal selectivity to interpretable 3D factors such as pose and lighting.
This approach bridges inverse graphics with systems neuroscience, offering a
way to probe neural selectivity with physically grounded, 3D stimuli beyond
conventional pixel-based methods.

</details>


### [61] [CanvasMAR: Improving Masked Autoregressive Video Generation With Canvas](https://arxiv.org/abs/2510.13669)
*Zian Li,Muhan Zhang*

Main category: cs.CV

TL;DR: CanvasMAR is a video masked autoregressive model that introduces a canvas mechanism to address slow-start problems and error accumulation in video generation, achieving high-quality results with fewer steps.


<details>
  <summary>Details</summary>
Motivation: Video masked autoregressive models suffer from slow-start problems due to lack of global structure early in sampling and error accumulation across spatial and temporal dimensions during autoregression.

Method: Proposes CanvasMAR with a canvas mechanism - a blurred global prediction of the next frame used as starting point for masked generation. Also introduces compositional classifier-free guidance and noise-based canvas augmentation.

Result: Experiments on BAIR and Kinetics-600 benchmarks show CanvasMAR produces high-quality videos with fewer autoregressive steps, achieving remarkable performance among autoregressive models and rivaling diffusion-based methods.

Conclusion: CanvasMAR effectively mitigates slow-start and error accumulation issues in video MAR models through the canvas mechanism, enabling faster and more coherent video generation.

Abstract: Masked autoregressive models (MAR) have recently emerged as a powerful
paradigm for image and video generation, combining the flexibility of masked
modeling with the potential of continuous tokenizer. However, video MAR models
suffer from two major limitations: the slow-start problem, caused by the lack
of a structured global prior at early sampling stages, and error accumulation
across the autoregression in both spatial and temporal dimensions. In this
work, we propose CanvasMAR, a novel video MAR model that mitigates these issues
by introducing a canvas mechanism--a blurred, global prediction of the next
frame, used as the starting point for masked generation. The canvas provides
global structure early in sampling, enabling faster and more coherent frame
synthesis. Furthermore, we introduce compositional classifier-free guidance
that jointly enlarges spatial (canvas) and temporal conditioning, and employ
noise-based canvas augmentation to enhance robustness. Experiments on the BAIR
and Kinetics-600 benchmarks demonstrate that CanvasMAR produces high-quality
videos with fewer autoregressive steps. Our approach achieves remarkable
performance among autoregressive models on Kinetics-600 dataset and rivals
diffusion-based methods.

</details>


### [62] [Near-Infrared Hyperspectral Imaging Applications in Food Analysis -- Improving Algorithms and Methodologies](https://arxiv.org/abs/2510.13452)
*Ole-Christian Galbo Engstrøm*

Main category: cs.CV

TL;DR: This thesis explores using near-infrared hyperspectral imaging (NIR-HSI) with CNN and PLS models for food quality analysis, finding that CNN-based approaches outperform PLS for spatial-chemical analysis, while PLS works well for mean chemical content analysis.


<details>
  <summary>Details</summary>
Motivation: To investigate the effectiveness of NIR-HSI combined with machine learning models for food quality assessment, comparing CNN and PLS approaches for different types of analysis.

Method: Used four studies with five research hypotheses, comparing convolutional neural networks (CNNs) and partial least squares (PLS) models. Applied joint spatio-spectral analysis with CNNs, including 2D CNNs augmented with spectral convolution layers.

Result: CNN-based approaches outperformed PLS for modeling parameters where chemical and physical visual information are relevant. PLS performed equally well for mean chemical content analysis. CNN with spectral convolution layer successfully generated smooth chemical maps for fat content in pork bellies, overcoming PLS limitations.

Conclusion: CNN-based spatio-spectral analysis is superior for complex food quality parameters, while PLS remains effective for mean chemical content analysis. The thesis also produced two open-source Python packages for fast PLS modeling and cross-validation.

Abstract: This thesis investigates the application of near-infrared hyperspectral
imaging (NIR-HSI) for food quality analysis. The investigation is conducted
through four studies operating with five research hypotheses. For several
analyses, the studies compare models based on convolutional neural networks
(CNNs) and partial least squares (PLS). Generally, joint spatio-spectral
analysis with CNNs outperforms spatial analysis with CNNs and spectral analysis
with PLS when modeling parameters where chemical and physical visual
information are relevant. When modeling chemical parameters with a
2-dimensional (2D) CNN, augmenting the CNN with an initial layer dedicated to
performing spectral convolution enhances its predictive performance by learning
a spectral preprocessing similar to that applied by domain experts. Still,
PLS-based spectral modeling performs equally well for analysis of the mean
content of chemical parameters in samples and is the recommended approach.
Modeling the spatial distribution of chemical parameters with NIR-HSI is
limited by the ability to obtain spatially resolved reference values.
Therefore, a study used bulk mean references for chemical map generation of fat
content in pork bellies. A PLS-based approach gave non-smooth chemical maps and
pixel-wise predictions outside the range of 0-100\%. Conversely, a 2D CNN
augmented with a spectral convolution layer mitigated all issues arising with
PLS. The final study attempted to model barley's germinative capacity by
analyzing NIR spectra, RGB images, and NIR-HSI images. However, the results
were inconclusive due to the dataset's low degree of germination. Additionally,
this thesis has led to the development of two open-sourced Python packages. The
first facilitates fast PLS-based modeling, while the second facilitates very
fast cross-validation of PLS and other classical machine learning models with a
new algorithm.

</details>


### [63] [MVCustom: Multi-View Customized Diffusion via Geometric Latent Rendering and Completion](https://arxiv.org/abs/2510.13702)
*Minjung Shin,Hyunin Cho,Sooyeon Go,Jin-Hwa Kim,Youngjung Uh*

Main category: cs.CV

TL;DR: MVCustom is a diffusion-based framework that achieves both multi-view camera pose control and prompt-based customization with geometric consistency, addressing limitations in existing models.


<details>
  <summary>Details</summary>
Motivation: Existing multi-view generation models lack customization with geometric consistency, while customization models lack explicit viewpoint control. The paper introduces multi-view customization to unify both capabilities.

Method: MVCustom uses feature-field representation to learn subject identity and geometry, enhanced with dense spatio-temporal attention for temporal coherence. It employs depth-aware feature rendering and consistent-aware latent completion for geometric consistency.

Result: Extensive experiments show MVCustom is the only framework that simultaneously achieves faithful multi-view generation and customization, outperforming existing approaches.

Conclusion: MVCustom successfully bridges the gap between multi-view generation and customization, providing a unified solution for controllable generative models with both camera pose control and prompt-based personalization.

Abstract: Multi-view generation with camera pose control and prompt-based customization
are both essential elements for achieving controllable generative models.
However, existing multi-view generation models do not support customization
with geometric consistency, whereas customization models lack explicit
viewpoint control, making them challenging to unify. Motivated by these gaps,
we introduce a novel task, multi-view customization, which aims to jointly
achieve multi-view camera pose control and customization. Due to the scarcity
of training data in customization, existing multi-view generation models, which
inherently rely on large-scale datasets, struggle to generalize to diverse
prompts. To address this, we propose MVCustom, a novel diffusion-based
framework explicitly designed to achieve both multi-view consistency and
customization fidelity. In the training stage, MVCustom learns the subject's
identity and geometry using a feature-field representation, incorporating the
text-to-video diffusion backbone enhanced with dense spatio-temporal attention,
which leverages temporal coherence for multi-view consistency. In the inference
stage, we introduce two novel techniques: depth-aware feature rendering
explicitly enforces geometric consistency, and consistent-aware latent
completion ensures accurate perspective alignment of the customized subject and
surrounding backgrounds. Extensive experiments demonstrate that MVCustom is the
only framework that simultaneously achieves faithful multi-view generation and
customization.

</details>


### [64] [VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator](https://arxiv.org/abs/2510.13454)
*Hyojun Go,Dominik Narnhofer,Goutam Bhat,Prune Truong,Federico Tombari,Konrad Schindler*

Main category: cs.CV

TL;DR: VIST3A combines text-to-video generators with 3D reconstruction models through model stitching and alignment to create high-quality text-to-3D scene generation.


<details>
  <summary>Details</summary>
Motivation: To leverage the strengths of modern text-to-video models and 3D reconstruction systems by combining them into a unified text-to-3D generation framework.

Method: Uses model stitching to connect text-to-video generator latents with 3D decoder layers, followed by direct reward finetuning for alignment to ensure consistent 3D geometry.

Result: Marked improvement over prior text-to-3D models, enabling high-quality text-to-Gaussian splats and text-to-pointmap generation.

Conclusion: VIST3A successfully bridges text-to-video and 3D reconstruction capabilities to create effective text-to-3D scene generation systems.

Abstract: The rapid progress of large, pretrained models for both visual content
generation and 3D reconstruction opens up new possibilities for text-to-3D
generation. Intuitively, one could obtain a formidable 3D scene generator if
one were able to combine the power of a modern latent text-to-video model as
"generator" with the geometric abilities of a recent (feedforward) 3D
reconstruction system as "decoder". We introduce VIST3A, a general framework
that does just that, addressing two main challenges. First, the two components
must be joined in a way that preserves the rich knowledge encoded in their
weights. We revisit model stitching, i.e., we identify the layer in the 3D
decoder that best matches the latent representation produced by the
text-to-video generator and stitch the two parts together. That operation
requires only a small dataset and no labels. Second, the text-to-video
generator must be aligned with the stitched 3D decoder, to ensure that the
generated latents are decodable into consistent, perceptually convincing 3D
scene geometry. To that end, we adapt direct reward finetuning, a popular
technique for human preference alignment. We evaluate the proposed VIST3A
approach with different video generators and 3D reconstruction models. All
tested pairings markedly improve over prior text-to-3D models that output
Gaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also
enables high-quality text-to-pointmap generation.

</details>


### [65] [Through the Lens of Doubt: Robust and Efficient Uncertainty Estimation for Visual Place Recognition](https://arxiv.org/abs/2510.13464)
*Emily Miller,Michael Milford,Muhammad Burhan Hafez,SD Ramchurn,Shoaib Ehsan*

Main category: cs.CV

TL;DR: Proposes three training-free uncertainty metrics (Similarity Distribution, Ratio Spread, Statistical Uncertainty) for Visual Place Recognition that estimate prediction confidence by analyzing statistical patterns in similarity scores, enabling robust uncertainty estimation across varying visual conditions without additional training.


<details>
  <summary>Details</summary>
Motivation: VPR systems face challenges with varying visual environments, lighting, seasons, and viewpoints. Failure-critical applications like SLAM loop closure require robust uncertainty estimation for place matching, but existing methods lack effective confidence estimation.

Method: Three training-free metrics: Similarity Distribution (measures score separation between candidates), Ratio Spread (evaluates competitive ambiguity among top matches), and Statistical Uncertainty (combines SD and RS). All operate without additional training, architectural changes, or geometric verification.

Result: Comprehensive evaluation across 9 VPR methods and 6 datasets shows the metrics excel at discriminating correct/incorrect matches, outperform existing approaches, maintain negligible computational overhead, and improve precision-recall performance across varied environmental conditions.

Conclusion: The proposed uncertainty metrics provide effective confidence estimation for VPR systems, generalize across datasets and methods without validation data, and are deployable for real-time robotic applications with improved reliability.

Abstract: Visual Place Recognition (VPR) enables robots and autonomous vehicles to
identify previously visited locations by matching current observations against
a database of known places. However, VPR systems face significant challenges
when deployed across varying visual environments, lighting conditions, seasonal
changes, and viewpoints changes. Failure-critical VPR applications, such as
loop closure detection in simultaneous localization and mapping (SLAM)
pipelines, require robust estimation of place matching uncertainty. We propose
three training-free uncertainty metrics that estimate prediction confidence by
analyzing inherent statistical patterns in similarity scores from any existing
VPR method. Similarity Distribution (SD) quantifies match distinctiveness by
measuring score separation between candidates; Ratio Spread (RS) evaluates
competitive ambiguity among top-scoring locations; and Statistical Uncertainty
(SU) is a combination of SD and RS that provides a unified metric that
generalizes across datasets and VPR methods without requiring validation data
to select the optimal metric. All three metrics operate without additional
model training, architectural modifications, or computationally expensive
geometric verification. Comprehensive evaluation across nine state-of-the-art
VPR methods and six benchmark datasets confirms that our metrics excel at
discriminating between correct and incorrect VPR matches, and consistently
outperform existing approaches while maintaining negligible computational
overhead, making it deployable for real-time robotic applications across varied
environmental conditions with improved precision-recall performance.

</details>


### [66] [ExpressNet-MoE: A Hybrid Deep Neural Network for Emotion Recognition](https://arxiv.org/abs/2510.13493)
*Deeptimaan Banerjee,Prateek Gothwal,Ashis Kumer Biswas*

Main category: cs.CV

TL;DR: ExpressNet-MoE is a hybrid deep learning model combining CNNs and Mixture of Experts framework for facial emotion recognition, achieving state-of-the-art results across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Real-world facial emotion recognition faces challenges like variable head positions, occlusions, illumination changes, and demographic diversity. Current models struggle with engagement detection in applications like virtual learning and customer services.

Method: Proposes ExpressNet-MoE with CNN-based feature extractors, MoE module for adaptive feature selection, and residual network backbone. Uses multi-scale feature extraction to capture both global and local facial features, dynamically selecting relevant expert networks.

Result: Achieved accuracies of 74.77% on AffectNet (v7), 72.55% on AffectNet (v8), 84.29% on RAF-DB, and 64.66% on FER-2013, outperforming current state-of-the-art methods.

Conclusion: The model demonstrates strong adaptability and practical applicability for end-to-end emotion recognition systems in real-world settings, with publicly available reproducible code.

Abstract: In many domains, including online education, healthcare, security, and
human-computer interaction, facial emotion recognition (FER) is essential.
Real-world FER is still difficult despite its significance because of some
factors such as variable head positions, occlusions, illumination shifts, and
demographic diversity. Engagement detection, which is essential for
applications like virtual learning and customer services, is frequently
challenging due to FER limitations by many current models. In this article, we
propose ExpressNet-MoE, a novel hybrid deep learning model that blends both
Convolution Neural Networks (CNNs) and Mixture of Experts (MoE) framework, to
overcome the difficulties. Our model dynamically chooses the most pertinent
expert networks, thus it aids in the generalization and providing flexibility
to model across a wide variety of datasets. Our model improves on the accuracy
of emotion recognition by utilizing multi-scale feature extraction to collect
both global and local facial features. ExpressNet-MoE includes numerous
CNN-based feature extractors, a MoE module for adaptive feature selection, and
finally a residual network backbone for deep feature learning. To demonstrate
efficacy of our proposed model we evaluated on several datasets, and compared
with current state-of-the-art methods. Our model achieves accuracies of 74.77%
on AffectNet (v7), 72.55% on AffectNet (v8), 84.29% on RAF-DB, and 64.66% on
FER-2013. The results show how adaptive our model is and how it may be used to
develop end-to-end emotion recognition systems in practical settings.
Reproducible codes and results are made publicly accessible at
https://github.com/DeeptimaanB/ExpressNet-MoE.

</details>


### [67] [Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient Vision GNNs](https://arxiv.org/abs/2510.13740)
*Mustafa Munir,Alex Zhang,Radu Marculescu*

Main category: cs.CV

TL;DR: LogViG is a novel hybrid CNN-GNN model that uses Logarithmic Scalable Graph Construction (LSGC) to enhance Vision Graph Neural Networks by limiting long-range links and incorporating multi-scale high-resolution features, outperforming existing ViG, CNN, and ViT architectures in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing graph construction methods like KNN are expensive for larger images, and methods like SVGA with fixed step scales can cause over-squashing and miss important long-range connections that could provide valuable information.

Method: Proposed Logarithmic Scalable Graph Construction (LSGC) to limit long-range links, and developed LogViG - a hybrid CNN-GNN model with high-resolution branch and multi-scale feature fusion between high and low-resolution branches.

Result: LogViG outperforms existing ViG, CNN, and ViT architectures in accuracy, GMACs, and parameters. Ti-LogViG achieves 79.9% top-1 accuracy on ImageNet-1K (1.7% higher than Vision GNN) with 24.3% parameter reduction and 35.3% GMACs reduction.

Conclusion: Leveraging long-range links in graph construction through LSGC can exceed the performance of current state-of-the-art Vision GNNs, demonstrating the effectiveness of the proposed LogViG architecture.

Abstract: Vision graph neural networks (ViG) have demonstrated promise in vision tasks
as a competitive alternative to conventional convolutional neural nets (CNN)
and transformers (ViTs); however, common graph construction methods, such as
k-nearest neighbor (KNN), can be expensive on larger images. While methods such
as Sparse Vision Graph Attention (SVGA) have shown promise, SVGA's fixed step
scale can lead to over-squashing and missing multiple connections to gain the
same information that could be gained from a long-range link. Through this
observation, we propose a new graph construction method, Logarithmic Scalable
Graph Construction (LSGC) to enhance performance by limiting the number of
long-range links. To this end, we propose LogViG, a novel hybrid CNN-GNN model
that utilizes LSGC. Furthermore, inspired by the successes of multi-scale and
high-resolution architectures, we introduce and apply a high-resolution branch
and fuse features between our high-resolution and low-resolution branches for a
multi-scale high-resolution Vision GNN network. Extensive experiments show that
LogViG beats existing ViG, CNN, and ViT architectures in terms of accuracy,
GMACs, and parameters on image classification and semantic segmentation tasks.
Our smallest model, Ti-LogViG, achieves an average top-1 accuracy on
ImageNet-1K of 79.9% with a standard deviation of 0.2%, 1.7% higher average
accuracy than Vision GNN with a 24.3% reduction in parameters and 35.3%
reduction in GMACs. Our work shows that leveraging long-range links in graph
construction for ViGs through our proposed LSGC can exceed the performance of
current state-of-the-art ViGs. Code is available at
https://github.com/mmunir127/LogViG-Official.

</details>


### [68] [RECODE: Reasoning Through Code Generation for Visual Question Answering](https://arxiv.org/abs/2510.13756)
*Junhong Shen,Mu Cai,Bo Hu,Ameet Talwalkar,David A Ross,Cordelia Schmid,Alireza Fathi*

Main category: cs.CV

TL;DR: RECODE is an agentic framework that uses derendering (reverse-engineering visuals into executable code) to enable verifiable visual reasoning, significantly outperforming existing methods on visual reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Multimodal Large Language Models struggle with precise reasoning for structured visuals like charts and diagrams because pixel-based perception lacks verification mechanisms.

Method: RECODE generates multiple candidate programs to reproduce input images, uses a critic to select the most faithful reconstruction, and iteratively refines the code through an agentic framework.

Result: RECODE significantly outperforms methods that don't leverage code or only use code for drawing auxiliary lines or cropping on benchmarks like CharXiv, ChartQA, and Geometry3K.

Conclusion: Grounding visual perception in executable code provides a new path toward more accurate and verifiable multimodal reasoning.

Abstract: Multimodal Large Language Models (MLLMs) struggle with precise reasoning for
structured visuals like charts and diagrams, as pixel-based perception lacks a
mechanism for verification. To address this, we propose to leverage derendering
-- the process of reverse-engineering visuals into executable code -- as a new
modality for verifiable visual reasoning. Specifically, we propose RECODE, an
agentic framework that first generates multiple candidate programs to reproduce
the input image. It then uses a critic to select the most faithful
reconstruction and iteratively refines the code. This process not only
transforms an ambiguous perceptual task into a verifiable, symbolic problem,
but also enables precise calculations and logical inferences later on. On
various visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K,
RECODE significantly outperforms methods that do not leverage code or only use
code for drawing auxiliary lines or cropping. Our work demonstrates that
grounding visual perception in executable code provides a new path toward more
accurate and verifiable multimodal reasoning.

</details>


### [69] [High Semantic Features for the Continual Learning of Complex Emotions: a Lightweight Solution](https://arxiv.org/abs/2510.13534)
*Thibault Geoffroy,gauthier Gerspacher,Lionel Prevost*

Main category: cs.CV

TL;DR: The paper proposes using Action Units (facial muscle movements) as non-transient features for incremental learning of complex emotion recognition, achieving 0.75 accuracy on CFEE dataset with lightweight model.


<details>
  <summary>Details</summary>
Motivation: To address catastrophic forgetting in incremental learning for emotion recognition by using stable features that transfer well between tasks, similar to how humans learn emotions incrementally.

Method: Using Action Units describing facial muscle movements as non-transient, semantic features instead of features from CNN networks, enabling incremental learning from basic to complex emotions.

Result: Achieved 0.75 accuracy on CFEE dataset for complex emotion recognition, outperforming both shallow and deep CNN features, with lightweight model and small memory footprint.

Conclusion: Action Units are effective non-transient features for incremental emotion learning, preventing catastrophic forgetting and enabling competitive performance with state-of-the-art methods while maintaining model efficiency.

Abstract: Incremental learning is a complex process due to potential catastrophic
forgetting of old tasks when learning new ones. This is mainly due to transient
features that do not fit from task to task. In this paper, we focus on complex
emotion recognition. First, we learn basic emotions and then, incrementally,
like humans, complex emotions. We show that Action Units, describing facial
muscle movements, are non-transient, highly semantical features that outperform
those extracted by both shallow and deep convolutional neural networks. Thanks
to this ability, our approach achieves interesting results when learning
incrementally complex, compound emotions with an accuracy of 0.75 on the CFEE
dataset and can be favorably compared to state-of-the-art results. Moreover, it
results in a lightweight model with a small memory footprint.

</details>


### [70] [Scaling Vision Transformers for Functional MRI with Flat Maps](https://arxiv.org/abs/2510.13768)
*Connor Lane,Daniel Z. Kaplan,Tanishq Mathew Abraham,Paul S. Scotti*

Main category: cs.CV

TL;DR: Transform 4D fMRI data into 2D activity flat map videos and train Vision Transformers using spatiotemporal masked autoencoder framework on large-scale fMRI data from Human Connectome Project.


<details>
  <summary>Details</summary>
Motivation: To bridge the modality gap between fMRI and natural images for adapting modern deep learning architectures to fMRI data.

Method: Transform 4D volumetric fMRI data into videos of 2D fMRI activity flat maps, then train Vision Transformers on 2.3K hours of fMRI data using spatiotemporal masked autoencoder (MAE) framework.

Result: Masked fMRI modeling performance improves with dataset size according to a strict power scaling law. Model learns rich representations supporting both fine-grained state decoding across subjects and subject-specific trait decoding across brain state changes.

Conclusion: This work establishes foundation models for fMRI data through open science approach, with code and datasets publicly available.

Abstract: A key question for adapting modern deep learning architectures to functional
MRI (fMRI) is how to represent the data for model input. To bridge the modality
gap between fMRI and natural images, we transform the 4D volumetric fMRI data
into videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3K
hours of fMRI flat map videos from the Human Connectome Project using the
spatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRI
modeling performance improves with dataset size according to a strict power
scaling law. Downstream classification benchmarks show that our model learns
rich representations supporting both fine-grained state decoding across
subjects, as well as subject-specific trait decoding across changes in brain
state. This work is part of an ongoing open science project to build foundation
models for fMRI data. Our code and datasets are available at
https://github.com/MedARC-AI/fmri-fm.

</details>


### [71] [Learning Neural Parametric 3D Breast Shape Models for Metrical Surface Reconstruction From Monocular RGB Videos](https://arxiv.org/abs/2510.13540)
*Maximilian Weiherer,Antonia von Riedheim,Vanessa Brébant,Bernhard Egger,Christoph Palm*

Main category: cs.CV

TL;DR: A neural parametric 3D breast shape model (liRBSM) and reconstruction pipeline that recovers accurate breast geometry from monocular RGB videos without specialized hardware.


<details>
  <summary>Details</summary>
Motivation: To provide a low-cost, accessible alternative to expensive commercial 3D breast scanning solutions that doesn't require specialized hardware or proprietary software.

Method: Uses Structure-from-motion pipeline with a parametric breast model that decomposes the implicit breast domain into multiple local neural SDFs anchored at anatomical landmarks, unlike the global SDF approach of iRBSM.

Result: Significantly outperforms iRBSM in reconstruction quality, recovers high-quality 3D breast geometry within 2mm error margin, and takes less than six minutes to process.

Conclusion: The proposed liRBSM model and pipeline provide fast, accurate, open-source 3D breast reconstruction accessible with any RGB video recording device.

Abstract: We present a neural parametric 3D breast shape model and, based on this
model, introduce a low-cost and accessible 3D surface reconstruction pipeline
capable of recovering accurate breast geometry from a monocular RGB video. In
contrast to widely used, commercially available yet prohibitively expensive 3D
breast scanning solutions and existing low-cost alternatives, our method
requires neither specialized hardware nor proprietary software and can be used
with any device that is able to record RGB videos. The key building blocks of
our pipeline are a state-of-the-art, off-the-shelf Structure-from-motion
pipeline, paired with a parametric breast model for robust and metrically
correct surface reconstruction. Our model, similarly to the recently proposed
implicit Regensburg Breast Shape Model (iRBSM), leverages implicit neural
representations to model breast shapes. However, unlike the iRBSM, which
employs a single global neural signed distance function (SDF), our approach --
inspired by recent state-of-the-art face models -- decomposes the implicit
breast domain into multiple smaller regions, each represented by a local neural
SDF anchored at anatomical landmark positions. When incorporated into our
surface reconstruction pipeline, the proposed model, dubbed liRBSM (short for
localized iRBSM), significantly outperforms the iRBSM in terms of
reconstruction quality, yielding more detailed surface reconstruction than its
global counterpart. Overall, we find that the introduced pipeline is able to
recover high-quality 3D breast geometry within an error margin of less than 2
mm. Our method is fast (requires less than six minutes), fully transparent and
open-source, and -- together with the model -- publicly available at
https://rbsm.re-mic.de/local-implicit.

</details>


### [72] [Accelerated Feature Detectors for Visual SLAM: A Comparative Study of FPGA vs GPU](https://arxiv.org/abs/2510.13546)
*Ruiqi Ye,Mikel Luján*

Main category: cs.CV

TL;DR: This paper compares GPU vs FPGA acceleration for feature detectors in Visual SLAM, finding that GPUs perform better for traditional detectors (FAST, Harris) while FPGAs excel for learning-based detectors (SuperPoint), with FPGA achieving up to 3.1× speedup and 1.4× energy efficiency for SuperPoint.


<details>
  <summary>Details</summary>
Motivation: Feature detection is time-critical in SLAM systems deployed on power-constrained platforms like drones. With both GPUs and FPGAs available as accelerators, there's a need to understand which hardware platform performs better for different types of feature detectors in Visual SLAM pipelines.

Method: Comparative study of hardware-accelerated feature detectors (FAST, Harris, SuperPoint) implemented on both GPU and FPGA platforms using modern SoCs (Nvidia Jetson Orin and AMD Versal), evaluated within a complete Visual SLAM pipeline.

Result: For traditional detectors (FAST, Harris), GPU implementations outperform FPGA in runtime and energy efficiency. For learning-based SuperPoint, FPGA achieves 3.1× speedup and 1.4× energy efficiency over GPU. FPGA-accelerated V-SLAM achieves comparable performance to GPU in 2 out of 5 sequences, though GPU generally provides better accuracy. Hardware acceleration enables less frequent bundle adjustment without accuracy loss.

Conclusion: The choice between GPU and FPGA acceleration depends on the feature detector type: GPUs are better for traditional algorithms while FPGAs excel for learning-based detectors. Both platforms can improve V-SLAM performance by reducing bundle adjustment frequency while maintaining accuracy.

Abstract: Feature detection is a common yet time-consuming module in Simultaneous
Localization and Mapping (SLAM) implementations, which are increasingly
deployed on power-constrained platforms, such as drones. Graphics Processing
Units (GPUs) have been a popular accelerator for computer vision in general,
and feature detection and SLAM in particular.
  On the other hand, System-on-Chips (SoCs) with integrated Field Programmable
Gate Array (FPGA) are also widely available. This paper presents the first
study of hardware-accelerated feature detectors considering a Visual SLAM
(V-SLAM) pipeline. We offer new insights by comparing the best GPU-accelerated
FAST, Harris, and SuperPoint implementations against the FPGA-accelerated
counterparts on modern SoCs (Nvidia Jetson Orin and AMD Versal).
  The evaluation shows that when using a non-learning-based feature detector
such as FAST and Harris, their GPU implementations, and the GPU-accelerated
V-SLAM can achieve better run-time performance and energy efficiency than the
FAST and Harris FPGA implementations as well as the FPGA-accelerated V-SLAM.
However, when considering a learning-based detector such as SuperPoint, its
FPGA implementation can achieve better run-time performance and energy
efficiency (up to 3.1$\times$ and 1.4$\times$ improvements, respectively) than
the GPU implementation. The FPGA-accelerated V-SLAM can also achieve comparable
run-time performance compared to the GPU-accelerated V-SLAM, with better FPS in
2 out of 5 dataset sequences. When considering the accuracy, the results show
that the GPU-accelerated V-SLAM is more accurate than the FPGA-accelerated
V-SLAM in general. Last but not least, the use of hardware acceleration for
feature detection could further improve the performance of the V-SLAM pipeline
by having the global bundle adjustment module invoked less frequently without
sacrificing accuracy.

</details>


### [73] [Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs](https://arxiv.org/abs/2510.13795)
*Yi Zhang,Bolin Ni,Xin-Sheng Chen,Heng-Rui Zhang,Yongming Rao,Houwen Peng,Qinglin Lu,Han Hu,Meng-Hao Guo,Shi-Min Hu*

Main category: cs.CV

TL;DR: The paper introduces Honey-Data-15M, a high-quality SFT dataset with 15M QA pairs, and HoneyPipe data curation pipeline to address data quality gaps in open MLLMs, achieving SOTA performance with Bee-8B model.


<details>
  <summary>Details</summary>
Motivation: Fully open MLLMs lag behind proprietary models due to poor data quality in existing open-source datasets, which suffer from noise and lack complex reasoning data like Chain-of-Thought.

Method: Created Honey-Data-15M dataset with 15M QA pairs using multiple cleaning techniques and dual-level CoT enrichment strategy, plus HoneyPipe data curation pipeline and DataStudio framework.

Result: Bee-8B model trained on Honey-Data-15M establishes new SOTA for fully open MLLMs, achieving competitive performance with and sometimes surpassing semi-open models like InternVL3.5-8B.

Conclusion: Principled focus on data quality is key to developing competitive fully open MLLMs, demonstrated through dataset, pipeline, and model release to community.

Abstract: Fully open multimodal large language models (MLLMs) currently lag behind
proprietary counterparts, primarily due to a significant gap in data quality
for supervised fine-tuning (SFT). Existing open-source datasets are often
plagued by widespread noise and a critical deficit in complex reasoning data,
such as Chain-of-Thought (CoT), which hinders the development of advanced model
capabilities. Addressing these challenges, our work makes three primary
contributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising
approximately 15 million QA pairs, processed through multiple cleaning
techniques and enhanced with a novel dual-level (short and long) CoT enrichment
strategy. Second, we introduce HoneyPipe, the data curation pipeline, and its
underlying framework DataStudio, providing the community with a transparent and
adaptable methodology for data curation that moves beyond static dataset
releases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B
model on Honey-Data-15M. Experiments show that Bee-8B establishes a new
state-of-the-art (SOTA) for fully open MLLMs, achieving performance that is
competitive with, and in some cases surpasses, recent semi-open models such as
InternVL3.5-8B. Our work delivers to the community a suite of foundational
resources, including: the Honey-Data-15M corpus; the full-stack suite
comprising HoneyPipe and DataStudio; training recipes; an evaluation harness;
and the model weights. This effort demonstrates that a principled focus on data
quality is a key pathway to developing fully open MLLMs that are highly
competitive with their semi-open counterparts.

</details>


### [74] [XD-RCDepth: Lightweight Radar-Camera Depth Estimation with Explainability-Aligned and Distribution-Aware Distillation](https://arxiv.org/abs/2510.13565)
*Huawei Sun,Zixu Wang,Xiangyuan Peng,Julius Ott,Georg Stettinger,Lorenzo Servadei,Robert Wille*

Main category: cs.CV

TL;DR: XD-RCDepth is a lightweight radar-camera fusion depth estimation model that reduces parameters by 29.7% while maintaining accuracy through explainability-aligned and depth-distribution knowledge distillation.


<details>
  <summary>Details</summary>
Motivation: Depth estimation is crucial for autonomous driving, and radar-camera fusion provides robustness in adverse conditions through complementary geometric cues.

Method: Proposes XD-RCDepth with two knowledge-distillation strategies: explainability-aligned distillation that transfers teacher's saliency structure to student, and depth-distribution distillation that recasts depth regression as soft classification over discretized bins.

Result: Reduces parameters by 29.7% relative to state-of-the-art lightweight baseline while maintaining comparable accuracy, reduces MAE by 7.97% compared to direct training, and achieves competitive accuracy with real-time efficiency on nuScenes and ZJU-4DRadarCam datasets.

Conclusion: The proposed lightweight architecture with knowledge distillation strategies effectively reduces model size while preserving performance, making it suitable for real-time autonomous driving applications.

Abstract: Depth estimation remains central to autonomous driving, and radar-camera
fusion offers robustness in adverse conditions by providing complementary
geometric cues. In this paper, we present XD-RCDepth, a lightweight
architecture that reduces the parameters by 29.7% relative to the
state-of-the-art lightweight baseline while maintaining comparable accuracy. To
preserve performance under compression and enhance interpretability, we
introduce two knowledge-distillation strategies: an explainability-aligned
distillation that transfers the teacher's saliency structure to the student,
and a depth-distribution distillation that recasts depth regression as soft
classification over discretized bins. Together, these components reduce the MAE
compared with direct training with 7.97% and deliver competitive accuracy with
real-time efficiency on nuScenes and ZJU-4DRadarCam datasets.

</details>


### [75] [Generative Universal Verifier as Multimodal Meta-Reasoner](https://arxiv.org/abs/2510.13804)
*Xinchen Zhang,Xiaoying Zhang,Youbin Wu,Yanbin Cao,Renrui Zhang,Ruihang Chu,Ling Yang,Yujiu Yang*

Main category: cs.CV

TL;DR: Introduces Generative Universal Verifier for multimodal reasoning, featuring ViVerBench benchmark, OmniVerifier-7B trained on visual verification data, and OmniVerifier-TTS for test-time scaling, achieving significant improvements across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models consistently underperform in reliable visual verification tasks, showing substantial gap from human-level capability, highlighting the need for better reflection and refinement capabilities during multimodal reasoning.

Method: Built ViVerBench benchmark with 16 task categories, designed automated pipelines for large-scale visual verification data construction, trained OmniVerifier-7B as universal visual verifier, and proposed OmniVerifier-TTS for sequential test-time scaling with iterative optimization.

Result: OmniVerifier-7B achieves +8.3 improvement on ViVerBench, OmniVerifier-TTS achieves +3.7 on T2I-ReasonBench and +4.3 on GenEval++, outperforming existing methods like Best-of-N. Identified three atomic capabilities in visual verification that generalize synergistically.

Conclusion: Generative Universal Verifier advances reliable reflection during generation and scalable test-time refinement, marking progress toward more trustworthy and controllable next-generation multimodal reasoning systems.

Abstract: We introduce Generative Universal Verifier, a novel concept and plugin
designed for next-generation multimodal reasoning in vision-language models and
unified multimodal models, providing the fundamental capability of reflection
and refinement on visual outcomes during the reasoning and generation process.
This work makes three main contributions: (1) We build ViVerBench, a
comprehensive benchmark spanning 16 categories of critical tasks for evaluating
visual outcomes in multimodal reasoning. Results show that existing VLMs
consistently underperform across these tasks, underscoring a substantial gap
from human-level capability in reliable visual verification. (2) We design two
automated pipelines to construct large-scale visual verification data and train
OmniVerifier-7B, the first omni-capable generative verifier trained for
universal visual verification and achieves notable gains on ViVerBench(+8.3).
Through training, we identify three atomic capabilities in visual verification
and demonstrate how they generalize and interact synergistically. (3) We
propose OmniVerifier-TTS, a sequential test-time scaling paradigm that
leverages the universal verifier to bridge image generation and editing within
unified models, enhancing the upper bound of generative ability through
iterative fine-grained optimization. Beyond generation, we extend universal
verifier to broader world-modeling interleaved reasoning scenarios.
Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7),
and GenEval++(+4.3), outperforming existing parallel test-time scaling methods,
such as Best-of-N. By endowing multimodal reasoning with reliable visual
verification, OmniVerifier advances both reliable reflection during generation
and scalable test-time refinement, marking a step toward more trustworthy and
controllable next-generation reasoning systems.

</details>


### [76] [Fusion Meets Diverse Conditions: A High-diversity Benchmark and Baseline for UAV-based Multimodal Object Detection with Condition Cues](https://arxiv.org/abs/2510.13620)
*Chen Chen,Kangcheng Bin,Ting Hu,Jiahao Qi,Xingyue Liu,Tianpeng Liu,Zhen Liu,Yongxiang Liu,Ping Zhong*

Main category: cs.CV

TL;DR: The paper introduces ATR-UMOD, a high-diversity UAV dataset with RGB-IR image pairs covering varied altitudes, angles, and all-day/year conditions, and proposes PCDF, a prompt-guided dynamic fusion method for adaptive multimodal object detection.


<details>
  <summary>Details</summary>
Motivation: Existing UAV datasets lack real-world complexity for limited imaging conditions, necessitating a more comprehensive dataset and adaptive fusion methods for robust around-the-clock object detection using RGB and infrared images.

Method: Proposed PCDF (prompt-guided condition-aware dynamic fusion) that encodes imaging conditions as text prompts and uses task-specific soft-gating to adaptively reassign multimodal contributions, with a condition-decoupling module for practical use without annotations.

Result: Experiments on the ATR-UMOD dataset demonstrate the effectiveness of the proposed PCDF method for adaptive multimodal fusion in diverse imaging conditions.

Conclusion: The ATR-UMOD dataset addresses limitations of existing datasets by capturing real-world complexity, and PCDF provides an effective solution for adaptive multimodal fusion in varying UAV imaging conditions.

Abstract: Unmanned aerial vehicles (UAV)-based object detection with visible (RGB) and
infrared (IR) images facilitates robust around-the-clock detection, driven by
advancements in deep learning techniques and the availability of high-quality
dataset. However, the existing dataset struggles to fully capture real-world
complexity for limited imaging conditions. To this end, we introduce a
high-diversity dataset ATR-UMOD covering varying scenarios, spanning altitudes
from 80m to 300m, angles from 0{\deg} to 75{\deg}, and all-day, all-year time
variations in rich weather and illumination conditions. Moreover, each RGB-IR
image pair is annotated with 6 condition attributes, offering valuable
high-level contextual information. To meet the challenge raised by such diverse
conditions, we propose a novel prompt-guided condition-aware dynamic fusion
(PCDF) to adaptively reassign multimodal contributions by leveraging annotated
condition cues. By encoding imaging conditions as text prompts, PCDF
effectively models the relationship between conditions and multimodal
contributions through a task-specific soft-gating transformation. A
prompt-guided condition-decoupling module further ensures the availability in
practice without condition annotations. Experiments on ATR-UMOD dataset reveal
the effectiveness of PCDF.

</details>


### [77] [AVAR-Net: A Lightweight Audio-Visual Anomaly Recognition Framework with a Benchmark Dataset](https://arxiv.org/abs/2510.13630)
*Amjid Ali,Zulfiqar Ahmad Khan,Altaf Hussain,Muhammad Munsif,Adnan Hussain,Sung Wook Baik*

Main category: cs.CV

TL;DR: AVAR-Net is a lightweight audio-visual anomaly recognition framework that combines audio and visual features using Wav2Vec2 and MobileViT, with early fusion and MTCN for temporal modeling, achieving state-of-the-art performance on new VAAR dataset and XD-Violence benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly recognition methods rely only on visual data, making them unreliable under challenging conditions like occlusion, low illumination, and adverse weather. The lack of large-scale synchronized audio-visual datasets has hindered multimodal anomaly recognition progress.

Method: AVAR-Net uses Wav2Vec2 for audio feature extraction, MobileViT for visual feature extraction, early fusion mechanism to combine modalities, and Multi-Stage Temporal Convolutional Network (MTCN) to learn long-range temporal dependencies for spatiotemporal reasoning.

Result: AVAR-Net achieves 89.29% accuracy on the new VAAR dataset and 88.56% Average Precision on XD-Violence dataset, improving Average Precision by 2.8% over existing state-of-the-art methods.

Conclusion: The framework demonstrates effectiveness, efficiency, and generalization capability. The introduced VAAR dataset serves as a valuable benchmark for advancing multimodal anomaly recognition research.

Abstract: Anomaly recognition plays a vital role in surveillance, transportation,
healthcare, and public safety. However, most existing approaches rely solely on
visual data, making them unreliable under challenging conditions such as
occlusion, low illumination, and adverse weather. Moreover, the absence of
large-scale synchronized audio-visual datasets has hindered progress in
multimodal anomaly recognition. To address these limitations, this study
presents AVAR-Net, a lightweight and efficient audio-visual anomaly recognition
framework designed for real-world environments. AVAR-Net consists of four main
modules: an audio feature extractor, a video feature extractor, fusion
strategy, and a sequential pattern learning network that models cross-modal
relationships for anomaly recognition. Specifically, the Wav2Vec2 model
extracts robust temporal features from raw audio, while MobileViT captures both
local and global visual representations from video frames. An early fusion
mechanism combines these modalities, and a Multi-Stage Temporal Convolutional
Network (MTCN) model that learns long-range temporal dependencies within the
fused representation, enabling robust spatiotemporal reasoning. A novel
Visual-Audio Anomaly Recognition (VAAR) dataset, is also introduced, serving as
a medium-scale benchmark containing 3,000 real-world videos with synchronized
audio across ten diverse anomaly classes. Experimental evaluations demonstrate
that AVAR-Net achieves 89.29% accuracy on VAAR and 88.56% Average Precision on
the XD-Violence dataset, improving Average Precision by 2.8% over existing
state-of-the-art methods. These results highlight the effectiveness,
efficiency, and generalization capability of the proposed framework, as well as
the utility of VAAR as a benchmark for advancing multimodal anomaly recognition
research.

</details>


### [78] [Challenges, Advances, and Evaluation Metrics in Medical Image Enhancement: A Systematic Literature Review](https://arxiv.org/abs/2510.13638)
*Chun Wai Chin,Haniza Yazid,Hoi Leong Lee*

Main category: cs.CV

TL;DR: This systematic review analyzes 39 studies on medical image enhancement, finding that conventional methods dominate (29 studies) over deep learning (9 studies) and hybrid approaches (1 study). MRI and multi-modal imaging are most studied, while specialized modalities remain underexplored.


<details>
  <summary>Details</summary>
Motivation: Medical images often suffer from noise, artifacts, and low contrast that limit diagnostic potential, requiring robust enhancement methods to improve quality and interpretability for better clinical outcomes.

Method: Systematic literature review following PRISMA approach, analyzing 39 peer-reviewed studies on medical image enhancement across various imaging modalities.

Result: Low contrast and noise are the most frequent challenges. 65 image quality assessment metrics were identified, predominantly non-reference-based. Conventional mathematical methods are most common (29 studies), while deep learning (9 studies) and hybrid approaches (1 study) are less explored.

Conclusion: The review identifies current limitations and research gaps, particularly in specialized imaging modalities, and suggests future directions for advancing medical image enhancement technologies.

Abstract: Medical image enhancement is crucial for improving the quality and
interpretability of diagnostic images, ultimately supporting early detection,
accurate diagnosis, and effective treatment planning. Despite advancements in
imaging technologies such as X-ray, CT, MRI, and ultrasound, medical images
often suffer from challenges like noise, artifacts, and low contrast, which
limit their diagnostic potential. Addressing these challenges requires robust
preprocessing, denoising algorithms, and advanced enhancement methods, with
deep learning techniques playing an increasingly significant role. This
systematic literature review, following the PRISMA approach, investigates the
key challenges, recent advancements, and evaluation metrics in medical image
enhancement. By analyzing findings from 39 peer-reviewed studies, this review
provides insights into the effectiveness of various enhancement methods across
different imaging modalities and the importance of evaluation metrics in
assessing their impact. Key issues like low contrast and noise are identified
as the most frequent, with MRI and multi-modal imaging receiving the most
attention, while specialized modalities such as histopathology, endoscopy, and
bone scintigraphy remain underexplored. Out of the 39 studies, 29 utilize
conventional mathematical methods, 9 focus on deep learning techniques, and 1
explores a hybrid approach. In terms of image quality assessment, 18 studies
employ both reference-based and non-reference-based metrics, 9 rely solely on
reference-based metrics, and 12 use only non-reference-based metrics, with a
total of 65 IQA metrics introduced, predominantly non-reference-based. This
review highlights current limitations, research gaps, and potential future
directions for advancing medical image enhancement.

</details>


### [79] [Towards Adversarial Robustness and Uncertainty Quantification in DINOv2-based Few-Shot Anomaly Detection](https://arxiv.org/abs/2510.13643)
*Akib Mohammed Khan,Bartosz Krawczyk*

Main category: cs.CV

TL;DR: This paper examines the adversarial vulnerability and uncertainty calibration of DINOv2-based few-shot anomaly detectors, proposing a lightweight attack method and showing that post-hoc Platt scaling improves uncertainty estimation and enables attack detection.


<details>
  <summary>Details</summary>
Motivation: To investigate two unexamined questions about DINOv2-based anomaly detectors: their susceptibility to adversarial perturbations and how well their anomaly scores reflect calibrated uncertainty, which are essential for trustworthy real-world deployment.

Method: Built on AnomalyDINO, attached a lightweight linear head to frozen DINOv2 features for white-box gradient attacks, evaluated FGSM attacks on MVTec-AD and VisA datasets, and applied post-hoc Platt scaling for uncertainty calibration.

Result: Adversarial attacks consistently degraded performance metrics (F1, AUROC, AP, G-mean), showing imperceptible perturbations can flip nearest-neighbor relations. Platt scaling significantly improved uncertainty calibration, enabling attack detection through higher predictive entropy on perturbed inputs and reducing calibration error.

Conclusion: DINOv2-based few-shot anomaly detectors have concrete vulnerabilities to adversarial attacks and poor uncertainty calibration. Adversarial robustness and principled uncertainty quantification are essential capabilities for trustworthy anomaly detection systems in real-world deployment.

Abstract: Foundation models such as DINOv2 have shown strong performance in few-shot
anomaly detection, yet two key questions remain unexamined: (i) how susceptible
are these detectors to adversarial perturbations; and (ii) how well do their
anomaly scores reflect calibrated uncertainty? Building on AnomalyDINO, a
training-free deep nearest-neighbor detector over DINOv2 features, we present
one of the first systematic studies of adversarial attacks and uncertainty
estimation in this setting. To enable white-box gradient attacks while
preserving test-time behavior, we attach a lightweight linear head to frozen
DINOv2 features only for crafting perturbations. Using this heuristic, we
evaluate the impact of FGSM across the MVTec-AD and VisA datasets and observe
consistent drops in F1, AUROC, AP, and G-mean, indicating that imperceptible
perturbations can flip nearest-neighbor relations in feature space to induce
confident misclassification. Complementing robustness, we probe reliability and
find that raw anomaly scores are poorly calibrated, revealing a gap between
confidence and correctness that limits safety-critical use. As a simple, strong
baseline toward trustworthiness, we apply post-hoc Platt scaling to the anomaly
scores for uncertainty estimation. The resulting calibrated posteriors yield
significantly higher predictive entropy on adversarially perturbed inputs than
on clean ones, enabling a practical flagging mechanism for attack detection
while reducing calibration error (ECE). Our findings surface concrete
vulnerabilities in DINOv2-based few-shot anomaly detectors and establish an
evaluation protocol and baseline for robust, uncertainty-aware anomaly
detection. We argue that adversarial robustness and principled uncertainty
quantification are not optional add-ons but essential capabilities if anomaly
detection systems are to be trustworthy and ready for real-world deployment.

</details>


### [80] [Local-Global Context-Aware and Structure-Preserving Image Super-Resolution](https://arxiv.org/abs/2510.13649)
*Sanchar Palit,Subhasis Chaudhuri,Biplab Banerjee*

Main category: cs.CV

TL;DR: A contextually precise image super-resolution framework using Local-Global Context-Aware Attention and distribution-perceptual-aligned conditioning to generate high-quality images from degraded inputs.


<details>
  <summary>Details</summary>
Motivation: Existing approaches using pretrained diffusion models for super-resolution struggle with diverse and highly degraded images, leading to noise amplification and incorrect content generation.

Method: Proposes Local-Global Context-Aware Attention to maintain pixel relationships, and a distribution- and perceptual-aligned conditioning mechanism in pixel space to enhance perceptual fidelity while preserving structural information.

Result: Generates structurally consistent high-quality images that mitigate artifacts and ensure realistic detail restoration, as demonstrated through extensive experiments on multiple super-resolution benchmarks.

Conclusion: The proposed framework effectively addresses limitations of existing methods and produces high-fidelity, perceptually accurate reconstructions for image super-resolution tasks.

Abstract: Diffusion models have recently achieved significant success in various image
manipulation tasks, including image super-resolution and perceptual quality
enhancement. Pretrained text-to-image models, such as Stable Diffusion, have
exhibited strong capabilities in synthesizing realistic image content, which
makes them particularly attractive for addressing super-resolution tasks. While
some existing approaches leverage these models to achieve state-of-the-art
results, they often struggle when applied to diverse and highly degraded
images, leading to noise amplification or incorrect content generation. To
address these limitations, we propose a contextually precise image
super-resolution framework that effectively maintains both local and global
pixel relationships through Local-Global Context-Aware Attention, enabling the
generation of high-quality images. Furthermore, we propose a distribution- and
perceptual-aligned conditioning mechanism in the pixel space to enhance
perceptual fidelity. This mechanism captures fine-grained pixel-level
representations while progressively preserving and refining structural
information, transitioning from local content details to the global structural
composition. During inference, our method generates high-quality images that
are structurally consistent with the original content, mitigating artifacts and
ensuring realistic detail restoration. Extensive experiments on multiple
super-resolution benchmarks demonstrate the effectiveness of our approach in
producing high-fidelity, perceptually accurate reconstructions.

</details>


### [81] [EditCast3D: Single-Frame-Guided 3D Editing with Video Propagation and View Selection](https://arxiv.org/abs/2510.13652)
*Huaizhi Qu,Ruichen Zhang,Shuqing Luo,Luchao Qi,Zhihao Zhang,Xiaoming Liu,Roni Sengupta,Tianlong Chen*

Main category: cs.CV

TL;DR: EditCast3D is a pipeline that uses video generation foundation models to propagate edits from a single frame across entire datasets for 3D editing, addressing computational costs and consistency issues in existing methods.


<details>
  <summary>Details</summary>
Motivation: Foundation models show great progress in image editing but extending them to 3D editing faces challenges due to heavy computational demands, closed-source API restrictions, and poor multi-view consistency when applied independently across images.

Method: The pipeline employs video generation foundation models to propagate edits from a single first frame across the entire dataset before reconstruction. It introduces a view selection strategy to identify consistent and reconstruction-friendly views and uses feedforward reconstruction without costly refinement.

Result: EditCast3D demonstrates superior editing quality and high efficiency compared to state-of-the-art 3D editing baselines on commonly used datasets.

Conclusion: EditCast3D establishes a scalable and general paradigm for integrating foundation models into 3D editing pipelines, minimizing reliance on expensive image editing and mitigating prompt ambiguities.

Abstract: Recent advances in foundation models have driven remarkable progress in image
editing, yet their extension to 3D editing remains underexplored. A natural
approach is to replace the image editing modules in existing workflows with
foundation models. However, their heavy computational demands and the
restrictions and costs of closed-source APIs make plugging these models into
existing iterative editing strategies impractical. To address this limitation,
we propose EditCast3D, a pipeline that employs video generation foundation
models to propagate edits from a single first frame across the entire dataset
prior to reconstruction. While editing propagation enables dataset-level
editing via video models, its consistency remains suboptimal for 3D
reconstruction, where multi-view alignment is essential. To overcome this,
EditCast3D introduces a view selection strategy that explicitly identifies
consistent and reconstruction-friendly views and adopts feedforward
reconstruction without requiring costly refinement. In combination, the
pipeline both minimizes reliance on expensive image editing and mitigates
prompt ambiguities that arise when applying foundation models independently
across images. We evaluate EditCast3D on commonly used 3D editing datasets and
compare it against state-of-the-art 3D editing baselines, demonstrating
superior editing quality and high efficiency. These results establish
EditCast3D as a scalable and general paradigm for integrating foundation models
into 3D editing pipelines. The code is available at
https://github.com/UNITES-Lab/EditCast3D

</details>


### [82] [OmniGaze: Reward-inspired Generalizable Gaze Estimation In The Wild](https://arxiv.org/abs/2510.13660)
*Hongyu Qu,Jianan Wei,Xiangbo Shu,Yazhou Yao,Wenguan Wang,Jinhui Tang*

Main category: cs.CV

TL;DR: OmniGaze is a semi-supervised framework for 3D gaze estimation that uses large-scale unlabeled data from diverse real-world environments to overcome domain bias and improve generalization.


<details>
  <summary>Details</summary>
Motivation: Current 3D gaze estimation methods struggle with generalization across diverse domains due to limited annotated datasets and insufficient diversity in labeled data.

Method: OmniGaze collects diverse unlabeled facial images and uses pseudo-labeling with a reward model that assesses reliability using 3D direction vectors, visual embeddings, and semantic cues from a Multimodal Large Language Model.

Result: Achieves state-of-the-art performance on five datasets in both in-domain and cross-domain settings, and shows robust zero-shot generalization on four unseen datasets.

Conclusion: OmniGaze effectively mitigates domain bias and generalizes gaze estimation in the wild, serving as a scalable data engine for gaze estimation.

Abstract: Current 3D gaze estimation methods struggle to generalize across diverse data
domains, primarily due to i) the scarcity of annotated datasets, and ii) the
insufficient diversity of labeled data. In this work, we present OmniGaze, a
semi-supervised framework for 3D gaze estimation, which utilizes large-scale
unlabeled data collected from diverse and unconstrained real-world environments
to mitigate domain bias and generalize gaze estimation in the wild. First, we
build a diverse collection of unlabeled facial images, varying in facial
appearances, background environments, illumination conditions, head poses, and
eye occlusions. In order to leverage unlabeled data spanning a broader
distribution, OmniGaze adopts a standard pseudo-labeling strategy and devises a
reward model to assess the reliability of pseudo labels. Beyond pseudo labels
as 3D direction vectors, the reward model also incorporates visual embeddings
extracted by an off-the-shelf visual encoder and semantic cues from gaze
perspective generated by prompting a Multimodal Large Language Model to compute
confidence scores. Then, these scores are utilized to select high-quality
pseudo labels and weight them for loss computation. Extensive experiments
demonstrate that OmniGaze achieves state-of-the-art performance on five
datasets under both in-domain and cross-domain settings. Furthermore, we also
evaluate the efficacy of OmniGaze as a scalable data engine for gaze
estimation, which exhibits robust zero-shot generalization on four unseen
datasets.

</details>


### [83] [NTIRE 2025 Challenge on Low Light Image Enhancement: Methods and Results](https://arxiv.org/abs/2510.13670)
*Xiaoning Liu,Zongwei Wu,Florin-Alexandru Vasluianu,Hailong Yan,Bin Ren,Yulun Zhang,Shuhang Gu,Le Zhang,Ce Zhu,Radu Timofte,Kangbiao Shi,Yixu Feng,Tao Hu,Yu Cao,Peng Wu,Yijin Liang,Yanning Zhang,Qingsen Yan,Han Zhou,Wei Dong,Yan Min,Mohab Kishawy,Jun Chen,Pengpeng Yu,Anjin Park,Seung-Soo Lee,Young-Joon Park,Zixiao Hu,Junyv Liu,Huilin Zhang,Jun Zhang,Fei Wan,Bingxin Xu,Hongzhe Liu,Cheng Xu,Weiguo Pan,Songyin Dai,Xunpeng Yi,Qinglong Yan,Yibing Zhang,Jiayi Ma,Changhui Hu,Kerui Hu,Donghang Jing,Tiesheng Chen,Zhi Jin,Hongjun Wu,Biao Huang,Haitao Ling,Jiahao Wu,Dandan Zhan,G Gyaneshwar Rao,Vijayalaxmi Ashok Aralikatti,Nikhil Akalwadi,Ramesh Ashok Tabib,Uma Mudenagudi,Ruirui Lin,Guoxi Huang,Nantheera Anantrasirichai,Qirui Yang,Alexandru Brateanu,Ciprian Orhei,Cosmin Ancuti,Daniel Feijoo,Juan C. Benito,Álvaro García,Marcos V. Conde,Yang Qin,Raul Balmez,Anas M. Ali,Bilel Benjdira,Wadii Boulila,Tianyi Mao,Huan Zheng,Yanyan Wei,Shengeng Tang,Dan Guo,Zhao Zhang,Sabari Nathan,K Uma,A Sasithradevi,B Sathya Bama,S. Mohamed Mansoor Roomi,Ao Li,Xiangtao Zhang,Zhe Liu,Yijie Tang,Jialong Tang,Zhicheng Fu,Gong Chen,Joe Nasti,John Nicholson,Zeyu Xiao,Zhuoyuan Li,Ashutosh Kulkarni,Prashant W. Patil,Santosh Kumar Vipparthi,Subrahmanyam Murala,Duan Liu,Weile Li,Hangyuan Lu,Rixian Liu,Tengfeng Wang,Jinxing Liang,Chenxin Yu*

Main category: cs.CV

TL;DR: Review of NTIRE 2025 Low-Light Image Enhancement Challenge with 762 participants and 28 valid submissions, evaluating state-of-the-art methods for brighter, clearer images.


<details>
  <summary>Details</summary>
Motivation: To identify effective networks for producing brighter, clearer, and visually compelling images under diverse challenging low-light conditions.

Method: Comprehensive review and evaluation of proposed solutions from 28 teams that participated in the NTIRE 2025 LLIE Challenge.

Result: Significant progress in low-light image enhancement demonstrated through the challenge submissions, with 762 participants registering and 28 teams submitting valid entries.

Conclusion: The challenge successfully showcased state-of-the-art advancements in low-light image enhancement, highlighting the field's significant progress and effectiveness of current methods.

Abstract: This paper presents a comprehensive review of the NTIRE 2025 Low-Light Image
Enhancement (LLIE) Challenge, highlighting the proposed solutions and final
outcomes. The objective of the challenge is to identify effective networks
capable of producing brighter, clearer, and visually compelling images under
diverse and challenging conditions. A remarkable total of 762 participants
registered for the competition, with 28 teams ultimately submitting valid
entries. This paper thoroughly evaluates the state-of-the-art advancements in
LLIE, showcasing the significant progress.

</details>


### [84] [Seeing and Knowing in the Wild: Open-domain Visual Entity Recognition with Large-scale Knowledge Graphs via Contrastive Learning](https://arxiv.org/abs/2510.13675)
*Hongkuan Zhou,Lavdim Halilaj,Sebastian Monka,Stefan Schmid,Yuqicheng Zhu,Jingcheng Wu,Nadeem Nazer,Steffen Staab*

Main category: cs.CV

TL;DR: KnowCoL framework combines images, text descriptions, and Wikidata knowledge for open-domain visual entity recognition, achieving 10.5% accuracy improvement on unseen entities with a model 35x smaller than SOTA.


<details>
  <summary>Details</summary>
Motivation: Open-domain visual entity recognition faces challenges due to open-set conditions, limited supervision, visual ambiguity, and semantic disambiguation needs, requiring methods that can handle unseen entities and long-tail distributions.

Method: Knowledge-guided Contrastive Learning (KnowCoL) framework that integrates images and text descriptions into a shared semantic space using Wikidata's structured information (entity descriptions, type hierarchies, relational context) for zero-shot entity recognition.

Result: Significant accuracy improvements on OVEN benchmark, especially for rare and unseen entities. Smallest model achieves 10.5% accuracy improvement on unseen entities while being 35 times smaller than state-of-the-art methods.

Conclusion: Combining visual, textual, and structured knowledge greatly enhances open-domain visual entity recognition performance, demonstrating the effectiveness of knowledge-guided contrastive learning for handling unseen and rare entities.

Abstract: Open-domain visual entity recognition aims to identify and link entities
depicted in images to a vast and evolving set of real-world concepts, such as
those found in Wikidata. Unlike conventional classification tasks with fixed
label sets, it operates under open-set conditions, where most target entities
are unseen during training and exhibit long-tail distributions. This makes the
task inherently challenging due to limited supervision, high visual ambiguity,
and the need for semantic disambiguation. In this work, we propose a
Knowledge-guided Contrastive Learning (KnowCoL) framework that combines both
images and text descriptions into a shared semantic space grounded by
structured information from Wikidata. By abstracting visual and textual inputs
to a conceptual level, the model leverages entity descriptions, type
hierarchies, and relational context to support zero-shot entity recognition. We
evaluate our approach on the OVEN benchmark, a large-scale open-domain visual
recognition dataset with Wikidata IDs as the label space. Our experiments show
that using visual, textual, and structured knowledge greatly improves accuracy,
especially for rare and unseen entities. Our smallest model improves the
accuracy on unseen entities by 10.5% compared to the state-of-the-art, despite
being 35 times smaller.

</details>


### [85] [FlashWorld: High-quality 3D Scene Generation within Seconds](https://arxiv.org/abs/2510.13678)
*Xinyang Li,Tengfei Wang,Zixiao Gu,Shengchuan Zhang,Chunchao Guo,Liujuan Cao*

Main category: cs.CV

TL;DR: FlashWorld is a fast 3D scene generation model that produces 3D Gaussian representations directly from single images or text prompts, achieving 10-100x speedup over previous methods while maintaining superior rendering quality.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of conventional multi-view-oriented approaches that require time-consuming multi-view image generation followed by 3D reconstruction, and to address the poor visual quality typically associated with direct 3D-oriented methods.

Method: Uses a dual-mode pre-training phase followed by cross-mode post-training distillation, integrating both MV-oriented and 3D-oriented paradigms. Leverages video diffusion model priors and employs distribution matching from 3D-oriented to MV-oriented modes to enhance quality while maintaining 3D consistency.

Result: Achieves 10-100x faster generation than previous works while possessing superior rendering quality. Reduces required denoising steps for inference and enhances generalization to out-of-distribution inputs using massive single-view images and text prompts.

Conclusion: FlashWorld demonstrates superior efficiency and quality in 3D scene generation through its novel dual-mode training approach that effectively bridges the gap between speed and visual quality in 3D content creation.

Abstract: We propose FlashWorld, a generative model that produces 3D scenes from a
single image or text prompt in seconds, 10~100$\times$ faster than previous
works while possessing superior rendering quality. Our approach shifts from the
conventional multi-view-oriented (MV-oriented) paradigm, which generates
multi-view images for subsequent 3D reconstruction, to a 3D-oriented approach
where the model directly produces 3D Gaussian representations during multi-view
generation. While ensuring 3D consistency, 3D-oriented method typically suffers
poor visual quality. FlashWorld includes a dual-mode pre-training phase
followed by a cross-mode post-training phase, effectively integrating the
strengths of both paradigms. Specifically, leveraging the prior from a video
diffusion model, we first pre-train a dual-mode multi-view diffusion model,
which jointly supports MV-oriented and 3D-oriented generation modes. To bridge
the quality gap in 3D-oriented generation, we further propose a cross-mode
post-training distillation by matching distribution from consistent 3D-oriented
mode to high-quality MV-oriented mode. This not only enhances visual quality
while maintaining 3D consistency, but also reduces the required denoising steps
for inference. Also, we propose a strategy to leverage massive single-view
images and text prompts during this process to enhance the model's
generalization to out-of-distribution inputs. Extensive experiments demonstrate
the superiority and efficiency of our method.

</details>


### [86] [Generating healthy counterfactuals with denoising diffusion bridge models](https://arxiv.org/abs/2510.13684)
*Ana Lawry Aguila,Peirong Liu,Marina Crespo Aguirre,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: A novel method using denoising diffusion bridge models (DDBMs) to generate healthy counterfactuals from pathological medical images, outperforming previous approaches in segmentation and anomaly detection tasks.


<details>
  <summary>Details</summary>
Motivation: Generating healthy counterfactuals from pathological images is important for medical applications like anomaly detection and analysis tools designed for healthy scans. Current methods struggle to balance pathology removal with preservation of individual anatomical characteristics.

Method: Proposed denoising diffusion bridge models (DDBMs) that condition the diffusion process on both the initial healthy image and a corresponding synthetically generated pathological image, using the pathological image as a structurally informative prior.

Result: The DDBM approach outperforms previously proposed diffusion models and fully supervised approaches at segmentation and anomaly detection tasks.

Conclusion: DDBMs effectively generate healthy counterfactuals that closely match patient anatomy while selectively removing pathology, demonstrating superior performance over existing methods.

Abstract: Generating healthy counterfactuals from pathological images holds significant
promise in medical imaging, e.g., in anomaly detection or for application of
analysis tools that are designed for healthy scans. These counterfactuals
should represent what a patient's scan would plausibly look like in the absence
of pathology, preserving individual anatomical characteristics while modifying
only the pathological regions. Denoising diffusion probabilistic models (DDPMs)
have become popular methods for generating healthy counterfactuals of pathology
data. Typically, this involves training on solely healthy data with the
assumption that a partial denoising process will be unable to model disease
regions and will instead reconstruct a closely matched healthy counterpart.
More recent methods have incorporated synthetic pathological images to better
guide the diffusion process. However, it remains challenging to guide the
generative process in a way that effectively balances the removal of anomalies
with the retention of subject-specific features. To solve this problem, we
propose a novel application of denoising diffusion bridge models (DDBMs) -
which, unlike DDPMs, condition the diffusion process not only on the initial
point (i.e., the healthy image), but also on the final point (i.e., a
corresponding synthetically generated pathological image). Treating the
pathological image as a structurally informative prior enables us to generate
counterfactuals that closely match the patient's anatomy while selectively
removing pathology. The results show that our DDBM outperforms previously
proposed diffusion models and fully supervised approaches at segmentation and
anomaly detection tasks.

</details>


### [87] [Risk-adaptive Activation Steering for Safe Multimodal Large Language Models](https://arxiv.org/abs/2510.13698)
*Jonghyun Park,Minhyuk Seo,Jonghyun Choi*

Main category: cs.CV

TL;DR: RAS (Risk-adaptive Activation Steering) is a method that reformulates queries to enhance cross-modal attention to safety-critical image regions, enabling accurate risk assessment and adaptive activation steering for safe responses without iterative adjustments.


<details>
  <summary>Details</summary>
Motivation: Current AI models struggle with multimodal queries containing harmful intent in images. Training-based safety alignment is costly, while inference-time methods cause excessive refusals and slower inference due to iterative adjustments.

Method: Proposes query reformulation to strengthen cross-modal attention to safety-critical image regions, enabling risk assessment at query level. Uses assessed risk to adaptively steer activations for safe responses without iterative output adjustments.

Result: Extensive experiments show RAS significantly reduces attack success rates, preserves general task performance, and improves inference speed compared to prior inference-time defenses across multiple benchmarks.

Conclusion: RAS effectively addresses multimodal safety challenges by enabling accurate risk assessment and adaptive activation steering, achieving better safety, utility, and efficiency than existing methods.

Abstract: One of the key challenges of modern AI models is ensuring that they provide
helpful responses to benign queries while refusing malicious ones. But often,
the models are vulnerable to multimodal queries with harmful intent embedded in
images. One approach for safety alignment is training with extensive safety
datasets at the significant costs in both dataset curation and training.
Inference-time alignment mitigates these costs, but introduces two drawbacks:
excessive refusals from misclassified benign queries and slower inference speed
due to iterative output adjustments. To overcome these limitations, we propose
to reformulate queries to strengthen cross-modal attention to safety-critical
image regions, enabling accurate risk assessment at the query level. Using the
assessed risk, it adaptively steers activations to generate responses that are
safe and helpful without overhead from iterative output adjustments. We call
this Risk-adaptive Activation Steering (RAS). Extensive experiments across
multiple benchmarks on multimodal safety and utility demonstrate that the RAS
significantly reduces attack success rates, preserves general task performance,
and improves inference speed over prior inference-time defenses.

</details>


### [88] [Circle of Willis Centerline Graphs: A Dataset and Baseline Algorithm](https://arxiv.org/abs/2510.13720)
*Fabio Musio,Norman Juchler,Kaiyuan Yang,Suprosanna Shit,Chinmay Prabhakar,Bjoern Menze,Sven Hirsch*

Main category: cs.CV

TL;DR: Developed a baseline algorithm combining U-Net-based skeletonization with A* graph connection for extracting centerline graphs and morphometric features from Circle of Willis imaging data, achieving high anatomical accuracy and feature robustness.


<details>
  <summary>Details</summary>
Motivation: Conventional skeletonization techniques struggle with the complex geometry of the Circle of Willis, and publicly available centerline datasets are scarce, hindering automated quantitative analysis of this critical cerebrovascular network.

Method: Used thinning-based skeletonization to extract centerline graphs from TopCoW dataset (200 stroke patients with MRA/CTA), then developed baseline algorithm combining U-Net-based skeletonization with A* graph connection for anatomical centerline extraction.

Result: Algorithm achieved perfect graph topology reconstruction (F1 = 1), sub-voxel node distance accuracy, and strong feature robustness (median relative errors <5%, Pearson correlations >0.95). Features enabled prediction of fetal PCA variants, confirmation of bifurcation optimality, and detection of modality differences.

Conclusion: Learning-based skeletonization with graph connection enables anatomically plausible centerline extraction. Emphasizes importance of evaluating anatomical accuracy beyond voxel-based measures. Released dataset and algorithm to support further research.

Abstract: The Circle of Willis (CoW) is a critical network of arteries in the brain,
often implicated in cerebrovascular pathologies. Voxel-level segmentation is an
important first step toward an automated CoW assessment, but a full
quantitative analysis requires centerline representations. However,
conventional skeletonization techniques often struggle to extract reliable
centerlines due to the CoW's complex geometry, and publicly available
centerline datasets remain scarce. To address these challenges, we used a
thinning-based skeletonization algorithm to extract and curate centerline
graphs and morphometric features from the TopCoW dataset, which includes 200
stroke patients, each imaged with MRA and CTA. The curated graphs were used to
develop a baseline algorithm for centerline and feature extraction, combining
U-Net-based skeletonization with A* graph connection. Performance was evaluated
on a held-out test set, focusing on anatomical accuracy and feature robustness.
Further, we used the extracted features to predict the frequency of fetal PCA
variants, confirm theoretical bifurcation optimality relations, and detect
subtle modality differences. The baseline algorithm consistently reconstructed
graph topology with high accuracy (F1 = 1), and the average Euclidean node
distance between reference and predicted graphs was below one voxel. Features
such as segment radius, length, and bifurcation ratios showed strong
robustness, with median relative errors below 5% and Pearson correlations above
0.95. Our results demonstrate the utility of learning-based skeletonization
combined with graph connection for anatomically plausible centerline
extraction. We emphasize the importance of going beyond simple voxel-based
measures by evaluating anatomical accuracy and feature robustness. The dataset
and baseline algorithm have been released to support further method development
and clinical research.

</details>


### [89] [LiFMCR: Dataset and Benchmark for Light Field Multi-Camera Registration](https://arxiv.org/abs/2510.13729)
*Aymeric Fleith,Julian Zirbel,Daniel Cremers,Niclas Zeller*

Main category: cs.CV

TL;DR: LiFMCR is a novel dataset for multi-camera light field registration using micro lens array cameras, providing synchronized images from two high-resolution plenoptic cameras with precise 6-DoF ground truth poses from a Vicon system.


<details>
  <summary>Details</summary>
Motivation: Existing light field datasets are limited to single-camera setups and lack external ground truth, making it difficult to evaluate multi-camera registration methods rigorously.

Method: Two complementary registration approaches: 1) RANSAC-based 3D transformation estimation using cross-view point clouds, and 2) plenoptic PnP algorithm estimating extrinsic 6-DoF poses from single light field images. Both explicitly integrate the plenoptic camera model.

Result: Experiments show strong alignment with the ground truth, demonstrating accurate and scalable multi-camera registration.

Conclusion: LiFMCR enables reliable multi-view light field processing and provides a benchmark for evaluating multi-camera light field registration methods.

Abstract: We present LiFMCR, a novel dataset for the registration of multiple micro
lens array (MLA)-based light field cameras. While existing light field datasets
are limited to single-camera setups and typically lack external ground truth,
LiFMCR provides synchronized image sequences from two high-resolution Raytrix
R32 plenoptic cameras, together with high-precision 6-degrees of freedom (DoF)
poses recorded by a Vicon motion capture system. This unique combination
enables rigorous evaluation of multi-camera light field registration methods.
  As a baseline, we provide two complementary registration approaches: a robust
3D transformation estimation via a RANSAC-based method using cross-view point
clouds, and a plenoptic PnP algorithm estimating extrinsic 6-DoF poses from
single light field images. Both explicitly integrate the plenoptic camera
model, enabling accurate and scalable multi-camera registration. Experiments
show strong alignment with the ground truth, supporting reliable multi-view
light field processing.
  Project page: https://lifmcr.github.io/

</details>


### [90] [Cyclic Self-Supervised Diffusion for Ultra Low-field to High-field MRI Synthesis](https://arxiv.org/abs/2510.13735)
*Zhenxuan Zhang,Peiyuan Jing,Zi Wang,Ula Briski,Coraline Beitone,Yue Yang,Yinzhe Wu,Fanwen Wang,Liutao Yang,Jiahao Huang,Zhifan Gao,Zhaolin Chen,Kh Tohidul Islam,Guang Yang,Peter J. Lally*

Main category: cs.CV

TL;DR: CSS-Diff framework synthesizes high-field MRI from low-field MRI using cyclic self-supervised diffusion with anatomical preservation, achieving state-of-the-art performance in cross-field synthesis tasks.


<details>
  <summary>Details</summary>
Motivation: Low-field MRI is cheaper and more accessible but suffers from low resolution and poor signal-to-noise ratio. There's a need to preserve anatomical fidelity and bridge domain gaps in image contrast for clinical applications.

Method: Cyclic self-supervised diffusion framework with cycle-consistent constraint, slice-wise gap perception network for inter-slice alignment via contrastive learning, and local structure correction network for feature restoration through self-reconstruction of masked patches.

Result: Achieved 31.80 ± 2.70 dB PSNR, 0.943 ± 0.102 SSIM, and 0.0864 ± 0.0689 LPIPS. Preserved fine-grained anatomical structures with left cerebral white matter error dropping from 12.1% to 2.1% and cortex error from 4.2% to 3.7%.

Conclusion: CSS-Diff can synthesize images that are both quantitatively reliable and anatomically consistent, bridging the clinical fidelity gap in high-field MRI synthesis.

Abstract: Synthesizing high-quality images from low-field MRI holds significant
potential. Low-field MRI is cheaper, more accessible, and safer, but suffers
from low resolution and poor signal-to-noise ratio. This synthesis process can
reduce reliance on costly acquisitions and expand data availability. However,
synthesizing high-field MRI still suffers from a clinical fidelity gap. There
is a need to preserve anatomical fidelity, enhance fine-grained structural
details, and bridge domain gaps in image contrast. To address these issues, we
propose a \emph{cyclic self-supervised diffusion (CSS-Diff)} framework for
high-field MRI synthesis from real low-field MRI data. Our core idea is to
reformulate diffusion-based synthesis under a cycle-consistent constraint. It
enforces anatomical preservation throughout the generative process rather than
just relying on paired pixel-level supervision. The CSS-Diff framework further
incorporates two novel processes. The slice-wise gap perception network aligns
inter-slice inconsistencies via contrastive learning. The local structure
correction network enhances local feature restoration through
self-reconstruction of masked and perturbed patches. Extensive experiments on
cross-field synthesis tasks demonstrate the effectiveness of our method,
achieving state-of-the-art performance (e.g., 31.80 $\pm$ 2.70 dB in PSNR,
0.943 $\pm$ 0.102 in SSIM, and 0.0864 $\pm$ 0.0689 in LPIPS). Beyond pixel-wise
fidelity, our method also preserves fine-grained anatomical structures compared
with the original low-field MRI (e.g., left cerebral white matter error drops
from 12.1$\%$ to 2.1$\%$, cortex from 4.2$\%$ to 3.7$\%$). To conclude, our
CSS-Diff can synthesize images that are both quantitatively reliable and
anatomically consistent.

</details>


### [91] [UniCalli: A Unified Diffusion Framework for Column-Level Generation and Recognition of Chinese Calligraphy](https://arxiv.org/abs/2510.13745)
*Tianshuo Xu,Kai Wang,Zhifei Chen,Leyi Wu,Tianshui Wen,Fei Chao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: UniCalli is a unified diffusion framework that jointly trains Chinese calligraphy recognition and generation, achieving state-of-the-art quality with improved ligature continuity and layout fidelity while extending to other ancient scripts.


<details>
  <summary>Details</summary>
Motivation: Existing methods either create high-quality isolated characters while ignoring page-level aesthetics like ligatures and spacing, or attempt page synthesis at the expense of calligraphic correctness.

Method: Unified diffusion framework using asymmetric noising and rasterized box map for spatial priors, trained jointly on recognition and generation tasks with a curated dataset of 8,000+ digitized pieces.

Result: Achieves state-of-the-art generative quality with superior ligature continuity and layout fidelity, alongside stronger recognition performance. Successfully extends to Oracle bone inscriptions and Egyptian hieroglyphs.

Conclusion: Joint training of recognition and generation tasks creates synergy that improves both tasks, especially in limited-data regimes, and enables concept-level abstractions for better calligraphy replication.

Abstract: Computational replication of Chinese calligraphy remains challenging.
Existing methods falter, either creating high-quality isolated characters while
ignoring page-level aesthetics like ligatures and spacing, or attempting page
synthesis at the expense of calligraphic correctness. We introduce
\textbf{UniCalli}, a unified diffusion framework for column-level recognition
and generation. Training both tasks jointly is deliberate: recognition
constrains the generator to preserve character structure, while generation
provides style and layout priors. This synergy fosters concept-level
abstractions that improve both tasks, especially in limited-data regimes. We
curated a dataset of over 8,000 digitized pieces, with ~4,000 densely
annotated. UniCalli employs asymmetric noising and a rasterized box map for
spatial priors, trained on a mix of synthetic, labeled, and unlabeled data. The
model achieves state-of-the-art generative quality with superior ligature
continuity and layout fidelity, alongside stronger recognition. The framework
successfully extends to other ancient scripts, including Oracle bone
inscriptions and Egyptian hieroglyphs. Code and data can be viewed in
\href{https://github.com/EnVision-Research/UniCalli}{this URL}.

</details>


### [92] [InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue](https://arxiv.org/abs/2510.13747)
*Wenwen Tong,Hewei Guo,Dongchuan Ran,Jiangnan Chen,Jiefan Lu,Kaibin Wang,Keqiang Li,Xiaoxu Zhu,Jiakui Li,Kehan Li,Xueheng Li,Lumin Li,Chenxu Guo,Jiasheng Zhou,Jiandong Chen,Xianye Wu,Jiahao Wang,Silei Wu,Lei Chen,Hanming Deng,Yuxuan Song,Dinghao Zhou,Guiping Zhong,Ken Zheng,Shiyin Kang,Lewei Lu*

Main category: cs.CV

TL;DR: InteractiveOmni is a unified open-source omni-modal LLM (4B-8B parameters) that integrates vision, audio, language, and speech capabilities for multi-turn audio-visual interactions, achieving SOTA performance while being lightweight.


<details>
  <summary>Details</summary>
Motivation: To create a lightweight yet powerful omni-modal model that can handle complex multi-turn audio-visual interactions with human-like conversational abilities, addressing the need for accessible foundation models for next-generation interactive systems.

Method: Multi-stage training strategy: pre-training for omni-modal understanding, followed by post-training with speech conversation and audio-visual interaction. Uses a unified architecture integrating vision encoder, audio encoder, LLM, and speech decoder. Curates multi-turn training datasets for long-term conversational ability.

Result: Significantly outperforms leading open-source models, especially in long-term memory capabilities. InteractiveOmni-4B is comparable to Qwen2.5-Omni-7B on general benchmarks and retains 97% of 8B performance with 50% model size. Achieves SOTA results across image, audio, video understanding, and speech generation tasks.

Conclusion: InteractiveOmni provides an accessible, open-source foundation for next-generation intelligent interactive systems with comprehensive omni-modal understanding and speech generation capabilities in a lightweight package.

Abstract: We introduce InteractiveOmni, a unified and open-source omni-modal large
language model for audio-visual multi-turn interaction, ranging from 4B to 8B
parameters, designed to lead the field of lightweight models by offering
comprehensive omni-modal understanding and speech generation capabilities. To
achieve this, we integrate the vision encoder, audio encoder, large language
model, and speech decoder into a unified model for understanding and generation
tasks. We design a multi-stage training strategy to ensure robust cross-modal
capabilities, including pre-training for omni-modal understanding, followed by
post-training with speech conversation and audio-visual interaction. To enable
human-like long-term conversational ability, we meticulously curate a
multi-turn training dataset that enhances the model's ability to handle complex
and multi-turn interactions. To effectively evaluate the multi-turn memory and
speech interaction capabilities, we construct the multi-modal multi-turn memory
benchmark and the multi-turn speech interaction benchmark. Experiments
demonstrate that InteractiveOmni significantly outperforms leading open-source
models and provides a more intelligent multi-turn audio-visual experience,
particularly in its long-term memory capabilities. Notably, InteractiveOmni-4B
is comparable to the much larger model like Qwen2.5-Omni-7B on general
benchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B
while utilizing only 50% of the model size. Achieving state-of-the-art results
against similarly sized models across image, audio, video understanding, and
speech generation tasks, InteractiveOmni is an accessible, open-source
foundation for next-generation intelligent interactive systems.

</details>


### [93] [Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark](https://arxiv.org/abs/2510.13759)
*Kai Zou,Ziqi Huang,Yuhao Dong,Shulin Tian,Dian Zheng,Hongbo Liu,Jingwen He,Bin Liu,Yu Qiao,Ziwei Liu*

Main category: cs.CV

TL;DR: Uni-MMMU is a comprehensive benchmark that systematically evaluates the bidirectional synergy between visual understanding and generation across eight reasoning domains, revealing performance gaps and cross-modal dependencies in unified multimodal models.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks rarely examine the true integration of visual understanding and generation in unified multimodal models, treating these abilities in isolation or overlooking tasks that inherently couple them.

Method: Developed Uni-MMMU benchmark with bidirectionally coupled tasks across eight reasoning domains (science, coding, mathematics, puzzles, etc.), incorporating verifiable intermediate reasoning steps, unique ground truths, and reproducible scoring for both textual and visual outputs.

Result: Extensive evaluation revealed substantial performance disparities and cross-modal dependencies among state-of-the-art unified, generation-only, and understanding-only models, showing when and how these abilities reinforce each other.

Conclusion: Uni-MMMU establishes a reliable foundation for advancing unified models by providing systematic insights into the bidirectional synergy between visual understanding and generation capabilities.

Abstract: Unified multimodal models aim to jointly enable visual understanding and
generation, yet current benchmarks rarely examine their true integration.
Existing evaluations either treat the two abilities in isolation or overlook
tasks that inherently couple them. To address this gap, we present Uni-MMMU, a
comprehensive and discipline-aware benchmark that systematically unfolds the
bidirectional synergy between generation and understanding across eight
reasoning-centric domains, including science, coding, mathematics, and puzzles.
Each task is bidirectionally coupled, demanding models to (i) leverage
conceptual understanding to guide precise visual synthesis, or (ii) utilize
generation as a cognitive scaffold for analytical reasoning. Uni-MMMU
incorporates verifiable intermediate reasoning steps, unique ground truths, and
a reproducible scoring protocol for both textual and visual outputs. Through
extensive evaluation of state-of-the-art unified, generation-only, and
understanding-only models, we reveal substantial performance disparities and
cross-modal dependencies, offering new insights into when and how these
abilities reinforce one another, and establishing a reliable foundation for
advancing unified models.

</details>


### [94] [Adaptive Visual Conditioning for Semantic Consistency in Diffusion-Based Story Continuation](https://arxiv.org/abs/2510.13787)
*Seyed Mohammad Mousavi,Morteza Analoui*

Main category: cs.CV

TL;DR: AVC is a diffusion-based framework for story continuation that adaptively uses prior visual context through CLIP-based retrieval and controlled conditioning to maintain coherence while avoiding misleading information.


<details>
  <summary>Details</summary>
Motivation: The challenge in story continuation is effectively using prior visual context while ensuring semantic alignment with current text, avoiding injection of irrelevant or conflicting visual information.

Method: Uses CLIP to retrieve semantically aligned previous images, adaptively restricts visual conditioning to early diffusion stages when no relevant image is found, and improves data quality through LLM-based re-captioning.

Result: Quantitative results and human evaluations show AVC achieves superior coherence, semantic consistency, and visual fidelity compared to baselines, especially when prior visuals conflict with current input.

Conclusion: AVC successfully balances visual context utilization with semantic alignment through adaptive conditioning, demonstrating improved performance in challenging story continuation scenarios.

Abstract: Story continuation focuses on generating the next image in a narrative
sequence so that it remains coherent with both the ongoing text description and
the previously observed images. A central challenge in this setting lies in
utilizing prior visual context effectively, while ensuring semantic alignment
with the current textual input. In this work, we introduce AVC (Adaptive Visual
Conditioning), a framework for diffusion-based story continuation. AVC employs
the CLIP model to retrieve the most semantically aligned image from previous
frames. Crucially, when no sufficiently relevant image is found, AVC adaptively
restricts the influence of prior visuals to only the early stages of the
diffusion process. This enables the model to exploit visual context when
beneficial, while avoiding the injection of misleading or irrelevant
information. Furthermore, we improve data quality by re-captioning a noisy
dataset using large language models, thereby strengthening textual supervision
and semantic alignment. Quantitative results and human evaluations demonstrate
that AVC achieves superior coherence, semantic consistency, and visual fidelity
compared to strong baselines, particularly in challenging cases where prior
visuals conflict with the current input.

</details>


### [95] [NoisePrints: Distortion-Free Watermarks for Authorship in Private Diffusion Models](https://arxiv.org/abs/2510.13793)
*Nir Goren,Oren Katzir,Abhinav Nakarmi,Eyal Ronen,Mahmood Sharif,Or Patashnik*

Main category: cs.CV

TL;DR: NoisePrints is a lightweight watermarking scheme for diffusion models that uses the random seed as proof of authorship without modifying the generation process, enabling efficient verification without model weights.


<details>
  <summary>Details</summary>
Motivation: With the rapid adoption of diffusion models for visual content generation, proving authorship and protecting copyright have become critical, especially when model owners keep models private and third-party verification is essential.

Method: Utilizes the random seed used to initialize the diffusion process as a watermark by incorporating a hash function into noise sampling, ensuring seed recovery from content is infeasible while maintaining correlation between initial noise and generated content.

Result: Validated on multiple state-of-the-art diffusion models for images and videos, demonstrating efficient verification using only the seed and output without requiring model weights, and showing robustness under various manipulations.

Conclusion: NoisePrints provides a practical and scalable solution for authorship verification in diffusion models through cryptographic zero-knowledge proofs that prove ownership without revealing the seed, increasing watermark removal difficulty.

Abstract: With the rapid adoption of diffusion models for visual content generation,
proving authorship and protecting copyright have become critical. This
challenge is particularly important when model owners keep their models private
and may be unwilling or unable to handle authorship issues, making third-party
verification essential. A natural solution is to embed watermarks for later
verification. However, existing methods require access to model weights and
rely on computationally heavy procedures, rendering them impractical and
non-scalable. To address these challenges, we propose , a lightweight
watermarking scheme that utilizes the random seed used to initialize the
diffusion process as a proof of authorship without modifying the generation
process. Our key observation is that the initial noise derived from a seed is
highly correlated with the generated visual content. By incorporating a hash
function into the noise sampling process, we further ensure that recovering a
valid seed from the content is infeasible. We also show that sampling an
alternative seed that passes verification is infeasible, and demonstrate the
robustness of our method under various manipulations. Finally, we show how to
use cryptographic zero-knowledge proofs to prove ownership without revealing
the seed. By keeping the seed secret, we increase the difficulty of watermark
removal. In our experiments, we validate NoisePrints on multiple
state-of-the-art diffusion models for images and videos, demonstrating
efficient verification using only the seed and output, without requiring access
to model weights.

</details>


### [96] [Reasoning in Space via Grounding in the World](https://arxiv.org/abs/2510.13800)
*Yiming Chen,Zekun Qi,Wenyao Zhang,Xin Jin,Li Zhang,Peidong Liu*

Main category: cs.CV

TL;DR: GS-Reasoner is a 3D LLM that introduces a dual-path pooling mechanism to create unified 3D representations, enabling autoregressive grounding without external modules and achieving state-of-the-art performance in both 3D visual grounding and spatial reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing 3D LLMs lack unified representations that capture both semantic and geometric information, leading to poor grounding performance or excessive reliance on external modules, which hinders the integration of grounding and spatial reasoning.

Method: Proposes a dual-path pooling mechanism that aligns geometric features with semantic and positional cues to create unified image patch-based 3D representations, and introduces the Grounded Chain-of-Thought (GCoT) dataset with bounding box annotations and reasoning paths.

Result: GS-Reasoner achieves comparable performance to state-of-the-art models on 3D visual grounding without external modules, and significantly enhances spatial reasoning capabilities, leading to state-of-the-art performance.

Conclusion: The proposed unified 3D representation and GCoT dataset successfully bridge 3D visual grounding and spatial reasoning, establishing a self-contained framework that demonstrates the importance of grounding as a foundation for spatial reasoning.

Abstract: In this paper, we claim that 3D visual grounding is the cornerstone of
spatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to
explore the effective spatial representations that bridge the gap between them.
Existing 3D LLMs suffer from the absence of a unified 3D representation capable
of jointly capturing semantic and geometric information. This deficiency is
manifested either in poor performance on grounding or in an excessive reliance
on external modules, ultimately hindering the seamless integration of grounding
and spatial reasoning. To address this, we propose a simple yet effective
dual-path pooling mechanism that tightly aligns geometric features with both
semantic and positional cues, constructing a unified image patch-based 3D
representation that encapsulates all essential information without increasing
the number of input tokens. Leveraging this holistic representation,
GS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely
without external modules while delivering performance comparable to
state-of-the-art models, establishing a unified and self-contained framework
for 3D spatial reasoning. To further bridge grounding and spatial reasoning, we
introduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is
meticulously curated to include both 3D bounding box annotations for objects
referenced in reasoning questions and step-by-step reasoning paths that
integrate grounding as a core component of the problem-solving process.
Extensive experiments demonstrate that GS-Reasoner achieves impressive results
on 3D visual grounding, which in turn significantly enhances its spatial
reasoning capabilities, leading to state-of-the-art performance.

</details>


### [97] [Trace Anything: Representing Any Video in 4D via Trajectory Fields](https://arxiv.org/abs/2510.13802)
*Xinhang Liu,Yuxi Xiao,Donny Y. Chen,Jiashi Feng,Yu-Wing Tai,Chi-Keung Tang,Bingyi Kang*

Main category: cs.CV

TL;DR: Trace Anything is a neural network that represents videos as trajectory fields, predicting continuous 3D pixel trajectories in a single feed-forward pass using B-spline parameterization.


<details>
  <summary>Details</summary>
Motivation: To create effective spatio-temporal representations for video dynamics by modeling pixels as continuous 3D trajectories over time, which serve as primitive elements of motion.

Method: Proposes representing videos as Trajectory Fields - dense mappings assigning continuous 3D trajectory functions to each pixel. Uses a neural network that predicts control points for B-spline parameterization of trajectories in a single feed-forward pass.

Result: Achieves state-of-the-art performance on new trajectory field estimation benchmark, competitive results on established point-tracking benchmarks, significant efficiency gains through one-pass inference, and exhibits emergent abilities like goal-conditioned manipulation and motion forecasting.

Conclusion: The Trajectory Field representation and Trace Anything model provide an efficient and effective framework for video dynamics modeling, enabling single-pass trajectory prediction with emergent capabilities beyond traditional tracking.

Abstract: Effective spatio-temporal representation is fundamental to modeling,
understanding, and predicting dynamics in videos. The atomic unit of a video,
the pixel, traces a continuous 3D trajectory over time, serving as the
primitive element of dynamics. Based on this principle, we propose representing
any video as a Trajectory Field: a dense mapping that assigns a continuous 3D
trajectory function of time to each pixel in every frame. With this
representation, we introduce Trace Anything, a neural network that predicts the
entire trajectory field in a single feed-forward pass. Specifically, for each
pixel in each frame, our model predicts a set of control points that
parameterizes a trajectory (i.e., a B-spline), yielding its 3D position at
arbitrary query time instants. We trained the Trace Anything model on
large-scale 4D data, including data from our new platform, and our experiments
demonstrate that: (i) Trace Anything achieves state-of-the-art performance on
our new benchmark for trajectory field estimation and performs competitively on
established point-tracking benchmarks; (ii) it offers significant efficiency
gains thanks to its one-pass paradigm, without requiring iterative optimization
or auxiliary estimators; and (iii) it exhibits emergent abilities, including
goal-conditioned manipulation, motion forecasting, and spatio-temporal fusion.
Project page: https://trace-anything.github.io/.

</details>


### [98] [VisCoP: Visual Probing for Video Domain Adaptation of Vision Language Models](https://arxiv.org/abs/2510.13808)
*Dominick Reilly,Manish Kumar Govind,Le Xue,Srijan Das*

Main category: cs.CV

TL;DR: VisCoP introduces learnable visual probes to augment VLMs' vision encoders, enabling efficient domain adaptation with minimal parameter changes while preventing catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: VLMs perform poorly on novel domains with distribution shifts, and existing domain adaptation methods cause limited feature learning or catastrophic forgetting of prior capabilities.

Method: Augment VLM vision encoder with compact set of learnable visual probes for domain-specific adaptation, with minimal modification to pretrained parameters.

Result: VisCoP consistently outperforms existing adaptation strategies across cross-view, cross-modal, and cross-task settings, achieving superior target domain performance while retaining source-domain knowledge.

Conclusion: VisCoP provides an effective solution for domain adaptation in VLMs that balances adaptation performance with knowledge retention through lightweight probe-based augmentation.

Abstract: Large Vision-Language Models (VLMs) excel at general visual reasoning tasks
but exhibit sharp performance degradation when applied to novel domains with
substantial distribution shifts from pretraining data. Existing domain
adaptation approaches finetune different VLM components, but this often results
in limited domain-specific feature learning or catastrophic forgetting of prior
capabilities. To address these issues, we introduce Vision Contextualized
Probing (VisCoP), which augments the VLM's vision encoder with a compact set of
learnable visual probes. These probes enable efficient domain-specific
adaptation with minimal modification to pretrained parameters. We evaluate
VisCoP across three challenging domain adaptation settings-cross-view
(exocentric to egocentric), cross-modal (RGB to depth), and cross-task (human
understanding to robot control). Experiments show that VisCoP consistently
outperforms existing adaptation strategies, achieving superior performance on
target domains while effectively retaining source-domain knowledge.

</details>


### [99] [PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning](https://arxiv.org/abs/2510.13809)
*Sihui Ji,Xi Chen,Xin Tao,Pengfei Wan,Hengshuang Zhao*

Main category: cs.CV

TL;DR: PhysMaster enhances physics-awareness in video generation by using physical representations from input images and reinforcement learning with human feedback to guide models toward physically plausible dynamics.


<details>
  <summary>Details</summary>
Motivation: Current video generation models produce visually realistic videos but often violate physical laws, limiting their use as world models. The goal is to improve physics-awareness in video generation.

Method: Uses PhysEncoder to extract physical information from input images as extra conditioning. Applies reinforcement learning with human feedback and Direct Preference Optimization (DPO) to optimize physical representations end-to-end.

Result: PhysMaster improves physics-awareness in video generation, works on simple proxy tasks, and generalizes to various physical scenarios.

Conclusion: PhysMaster provides a generic, plug-in solution for physics-aware video generation through unified representation learning in reinforcement learning paradigm.

Abstract: Video generation models nowadays are capable of generating visually realistic
videos, but often fail to adhere to physical laws, limiting their ability to
generate physically plausible videos and serve as ''world models''. To address
this issue, we propose PhysMaster, which captures physical knowledge as a
representation for guiding video generation models to enhance their
physics-awareness. Specifically, PhysMaster is based on the image-to-video task
where the model is expected to predict physically plausible dynamics from the
input image. Since the input image provides physical priors like relative
positions and potential interactions of objects in the scenario, we devise
PhysEncoder to encode physical information from it as an extra condition to
inject physical knowledge into the video generation process. The lack of proper
supervision on the model's physical performance beyond mere appearance
motivates PhysEncoder to apply reinforcement learning with human feedback to
physical representation learning, which leverages feedback from generation
models to optimize physical representations with Direct Preference Optimization
(DPO) in an end-to-end manner. PhysMaster provides a feasible solution for
improving physics-awareness of PhysEncoder and thus of video generation,
proving its ability on a simple proxy task and generalizability to wide-ranging
physical scenarios. This implies that our PhysMaster, which unifies solutions
for various physical processes via representation learning in the reinforcement
learning paradigm, can act as a generic and plug-in solution for physics-aware
video generation and broader applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [100] [Local Timescale Gates for Timescale-Robust Continual Spiking Neural Networks](https://arxiv.org/abs/2510.12843)
*Ansh Tiwari,Ayush Chauhan*

Main category: cs.LG

TL;DR: LT-Gate is a spiking neuron model that uses dual time-constant dynamics with adaptive gating to enable both fast adaptation and long-term memory in continual learning, achieving 51% accuracy on temporal classification tasks.


<details>
  <summary>Details</summary>
Motivation: Spiking neural networks (SNNs) struggle with tasks requiring both fast adaptation and long-term memory in continual learning scenarios, creating a stability-plasticity dilemma that limits their performance on neuromorphic hardware.

Method: Proposes Local Timescale Gating (LT-Gate) where each spiking neuron tracks information on fast and slow timescales in parallel, with a learned gate locally adjusting their influence. Also introduces variance-tracking regularization for firing stability.

Result: Achieves about 51% final accuracy on challenging temporal classification benchmark, outperforming recent Hebbian continual-learning baseline (46%) and prior SNN methods. Successfully implemented on Intel's Loihi chip.

Conclusion: Multi-timescale gating substantially enhances continual learning in SNNs, narrowing the gap between spiking and conventional deep networks on lifelong-learning tasks while maintaining hardware compatibility.

Abstract: Spiking neural networks (SNNs) promise energy-efficient artificial
intelligence on neuromorphic hardware but struggle with tasks requiring both
fast adaptation and long-term memory, especially in continual learning. We
propose Local Timescale Gating (LT-Gate), a neuron model that combines dual
time-constant dynamics with an adaptive gating mechanism. Each spiking neuron
tracks information on a fast and a slow timescale in parallel, and a learned
gate locally adjusts their influence. This design enables individual neurons to
preserve slow contextual information while responding to fast signals,
addressing the stability-plasticity dilemma. We further introduce a
variance-tracking regularization that stabilizes firing activity, inspired by
biological homeostasis. Empirically, LT-Gate yields significantly improved
accuracy and retention in sequential learning tasks: on a challenging temporal
classification benchmark it achieves about 51 percent final accuracy, compared
to about 46 percent for a recent Hebbian continual-learning baseline and lower
for prior SNN methods. Unlike approaches that require external replay or
expensive orthogonalizations, LT-Gate operates with local updates and is fully
compatible with neuromorphic hardware. In particular, it leverages features of
Intel's Loihi chip (multiple synaptic traces with different decay rates) for
on-chip learning. Our results demonstrate that multi-timescale gating can
substantially enhance continual learning in SNNs, narrowing the gap between
spiking and conventional deep networks on lifelong-learning tasks.

</details>


### [101] [Lifting Manifolds to Mitigate Pseudo-Alignment in LLM4TS](https://arxiv.org/abs/2510.12847)
*Liangwei Nathan Zheng,Wenhao Liang,Wei Emma Zhang,Miao Xu,Olaf Maennel,Weitong Chen*

Main category: cs.LG

TL;DR: Pseudo-alignment in LLM4TS models causes underperformance. TimeSUP mitigates this by increasing time series manifold dimension to match language embeddings, improving forecasting performance.


<details>
  <summary>Details</summary>
Motivation: Pseudo-alignment is a pervasive challenge in LLM4TS models causing underperformance compared to linear models, but limited discussion exists on its root causes.

Method: TimeSUP increases the time series manifold dimension to match language embeddings' intrinsic dimension, allowing clear distinction of temporal signals while capturing shared structures across modalities.

Result: TimeSUP consistently outperforms state-of-the-art LLM4TS methods and lightweight baselines on long-term forecasting, and integrates seamlessly into existing pipelines with significant improvements.

Conclusion: Pseudo-alignment arises from cone effect in LLM components and low-dimensional time-series manifold. TimeSUP effectively addresses this by dimension matching, preserving modality uniqueness while learning commonalities.

Abstract: Pseudo-Alignment is a pervasive challenge in many large language models for
time series (LLM4TS) models, often causing them to underperform compared to
linear models or randomly initialised backbones. However, there is limited
discussion in the community for the reasons that pseudo-alignment occurs. In
this work, we conduct a thorough investigation into the root causes of
pseudo-alignment in LLM4TS and build a connection of pseudo-alignment to the
cone effect in LLM. We demonstrate that pseudo-alignment arises from the
interplay of cone effect within pretrained LLM components and the intrinsically
low-dimensional manifold of time-series data. In addition, we also introduce
\textit{\textbf{TimeSUP}}, a novel technique designed to mitigate this issue
and improve forecast performance in existing LLM4TS approaches. TimeSUP
addresses this by increasing the time series manifold to more closely match the
intrinsic dimension of language embeddings, allowing the model to distinguish
temporal signals clearly while still capturing shared structures across
modalities. As a result, representations for time and language tokens remain
distinct yet exhibit high cosine similarity, signifying that the model
preserves each modality unique features while learning their commonalities in a
unified embedding space. Empirically, TimeSUP consistently outperforms
state-of-the-art LLM4TS methods and other lightweight baselines on long-term
forecasting performance. Furthermore, it can be seamlessly integrated into four
existing LLM4TS pipelines and delivers significant improvements in forecasting
performance.

</details>


### [102] [FedGTEA: Federated Class-Incremental Learning with Gaussian Task Embedding and Alignment](https://arxiv.org/abs/2510.12927)
*Haolin Li,Hoda Bidkhori*

Main category: cs.LG

TL;DR: FedGTEA is a federated class incremental learning framework that uses Gaussian task embeddings and Wasserstein distance alignment to handle task heterogeneity while maintaining privacy and scalability.


<details>
  <summary>Details</summary>
Motivation: To address challenges in federated class incremental learning including statistical heterogeneity, model uncertainty, privacy constraints, and scalability across long task sequences.

Method: Uses Cardinality-Agnostic Task Encoder (CATE) for Gaussian task embeddings at clients, and 2-Wasserstein distance for inter-task alignment at server, avoiding direct transmission of embeddings for privacy.

Result: Achieves superior classification performance and significantly mitigates forgetting, consistently outperforming existing baselines on popular datasets.

Conclusion: FedGTEA provides an effective and privacy-preserving solution for federated class incremental learning with good scalability and performance.

Abstract: We introduce a novel framework for Federated Class Incremental Learning,
called Federated Gaussian Task Embedding and Alignment (FedGTEA). FedGTEA is
designed to capture task-specific knowledge and model uncertainty in a scalable
and communication-efficient manner. At the client side, the
Cardinality-Agnostic Task Encoder (CATE) produces Gaussian-distributed task
embeddings that encode task knowledge, address statistical heterogeneity, and
quantify data uncertainty. Importantly, CATE maintains a fixed parameter size
regardless of the number of tasks, which ensures scalability across long task
sequences. On the server side, FedGTEA utilizes the 2-Wasserstein distance to
measure inter-task gaps between Gaussian embeddings. We formulate the
Wasserstein loss to enforce inter-task separation. This probabilistic
formulation not only enhances representation learning but also preserves
task-level privacy by avoiding the direct transmission of latent embeddings,
aligning with the privacy constraints in federated learning. Extensive
empirical evaluations on popular datasets demonstrate that FedGTEA achieves
superior classification performance and significantly mitigates forgetting,
consistently outperforming strong existing baselines.

</details>


### [103] [Learning at the Speed of Physics: Equilibrium Propagation on Oscillator Ising Machines](https://arxiv.org/abs/2510.12934)
*Alex Gower*

Main category: cs.LG

TL;DR: Oscillator Ising Machines (OIMs) enable fast, energy-efficient machine learning by leveraging physical system dynamics that naturally perform energy descent, achieving competitive accuracy on MNIST and Fashion-MNIST while maintaining robustness under hardware constraints.


<details>
  <summary>Details</summary>
Motivation: Physical systems that naturally perform energy descent can accelerate machine learning by directly implementing optimization processes that conventional processors struggle with, particularly for energy-based models (EBMs).

Method: Combines Equilibrium Propagation (EP) with Oscillator Ising Machines (OIMs), using GHz-frequency dynamics that mirror gradient descent and Langevin dynamics, with local learning rules instead of global backpropagation.

Result: Achieves competitive accuracy: ~97.2 ± 0.1% on MNIST and ~88.0 ± 0.1% on Fashion-MNIST, while maintaining robustness under realistic hardware constraints like parameter quantization and phase noise.

Conclusion: OIMs provide a fast, energy-efficient substrate for neuromorphic learning, enabling practical realization of energy-based models on physical hardware whose dynamics directly perform optimization.

Abstract: Physical systems that naturally perform energy descent offer a direct route
to accelerating machine learning. Oscillator Ising Machines (OIMs) exemplify
this idea: their GHz-frequency dynamics mirror both the optimization of
energy-based models (EBMs) and gradient descent on loss landscapes, while
intrinsic noise corresponds to Langevin dynamics - supporting sampling as well
as optimization. Equilibrium Propagation (EP) unifies these processes into
descent on a single total energy landscape, enabling local learning rules
without global backpropagation. We show that EP on OIMs achieves competitive
accuracy ($\sim 97.2 \pm 0.1 \%$ on MNIST, $\sim 88.0 \pm 0.1 \%$ on
Fashion-MNIST), while maintaining robustness under realistic hardware
constraints such as parameter quantization and phase noise. These results
establish OIMs as a fast, energy-efficient substrate for neuromorphic learning,
and suggest that EBMs - often bottlenecked by conventional processors - may
find practical realization on physical hardware whose dynamics directly perform
their optimization.

</details>


### [104] [Pruning Cannot Hurt Robustness: Certified Trade-offs in Reinforcement Learning](https://arxiv.org/abs/2510.12939)
*James Pedley,Benjamin Etheridge,Stephen J. Roberts,Francesco Quinzan*

Main category: cs.LG

TL;DR: Pruning improves adversarial robustness in reinforcement learning without harming clean performance, revealing a performance-robustness frontier.


<details>
  <summary>Details</summary>
Motivation: RL policies need to remain reliable under adversarial attacks while modern deep RL agents are over-parameterized, raising costs and fragility concerns.

Method: Developed theoretical framework for certified robustness under pruning in SA-MDPs, proved element-wise pruning tightens robustness bounds, derived three-term regret decomposition, and empirically evaluated pruning schedules on continuous-control benchmarks.

Result: Pruning consistently finds 'sweet spots' at moderate sparsity where robustness improves substantially without harming clean performance, sometimes even enhancing it.

Conclusion: Pruning serves as a structural intervention for robust RL, not just compression, exposing fundamental performance-robustness trade-offs.

Abstract: Reinforcement learning (RL) policies deployed in real-world environments must
remain reliable under adversarial perturbations. At the same time, modern deep
RL agents are heavily over-parameterized, raising costs and fragility concerns.
While pruning has been shown to improve robustness in supervised learning, its
role in adversarial RL remains poorly understood. We develop the first
theoretical framework for certified robustness under pruning in
state-adversarial Markov decision processes (SA-MDPs). For Gaussian and
categorical policies with Lipschitz networks, we prove that element-wise
pruning can only tighten certified robustness bounds; pruning never makes the
policy less robust. Building on this, we derive a novel three-term regret
decomposition that disentangles clean-task performance, pruning-induced
performance loss, and robustness gains, exposing a fundamental
performance--robustness frontier. Empirically, we evaluate magnitude and
micro-pruning schedules on continuous-control benchmarks with strong
policy-aware adversaries. Across tasks, pruning consistently uncovers
reproducible ``sweet spots'' at moderate sparsity levels, where robustness
improves substantially without harming - and sometimes even enhancing - clean
performance. These results position pruning not merely as a compression tool
but as a structural intervention for robust RL.

</details>


### [105] [An Investigation of Memorization Risk in Healthcare Foundation Models](https://arxiv.org/abs/2510.12950)
*Sana Tonekaboni,Lena Stempfle,Adibvafa Fallahpour,Walter Gerych,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: The paper introduces a black-box evaluation framework to assess privacy risks from memorization in EHR foundation models, distinguishing between generalization and harmful memorization.


<details>
  <summary>Details</summary>
Motivation: Foundation models trained on EHR data can memorize patient information, raising privacy concerns, especially for vulnerable subgroups.

Method: A suite of black-box tests probing memorization at embedding and generative levels, with methods to distinguish harmful memorization from generalization.

Result: The approach was validated on a publicly available EHR foundation model, and an open-source toolkit was released for privacy assessments.

Conclusion: The framework enables reproducible privacy risk evaluation for healthcare AI models, addressing critical privacy concerns in clinical applications.

Abstract: Foundation models trained on large-scale de-identified electronic health
records (EHRs) hold promise for clinical applications. However, their capacity
to memorize patient information raises important privacy concerns. In this
work, we introduce a suite of black-box evaluation tests to assess
privacy-related memorization risks in foundation models trained on structured
EHR data. Our framework includes methods for probing memorization at both the
embedding and generative levels, and aims to distinguish between model
generalization and harmful memorization in clinically relevant settings. We
contextualize memorization in terms of its potential to compromise patient
privacy, particularly for vulnerable subgroups. We validate our approach on a
publicly available EHR foundation model and release an open-source toolkit to
facilitate reproducible and collaborative privacy assessments in healthcare AI.

</details>


### [106] [A Multimodal XAI Framework for Trustworthy CNNs and Bias Detection in Deep Representation Learning](https://arxiv.org/abs/2510.12957)
*Noor Islam S. Mohammad*

Main category: cs.LG

TL;DR: A multimodal Explainable AI framework combining attention-augmented feature fusion, Grad-CAM++ explanations, and Reveal-to-Revise feedback loop achieves high accuracy and explanation fidelity while detecting and mitigating biases in multimodal datasets.


<details>
  <summary>Details</summary>
Motivation: Standard benchmarks like MNIST fail to expose latent biases and multimodal complexities, limiting trustworthiness of deep neural networks in high-stakes applications.

Method: Unified multimodal XAI framework with attention-augmented feature fusion, Grad-CAM++-based local explanations, and Reveal-to-Revise feedback loop for bias detection and mitigation.

Result: Achieves 93.2% classification accuracy, 91.6% F1-score, and 78.1% explanation fidelity (IoU-XAI) on multimodal MNIST extensions, outperforming unimodal and non-explainable baselines.

Conclusion: The work bridges performance, transparency, and fairness gaps, providing a practical pathway for trustworthy AI in sensitive domains through interpretability-integrated bias-aware learning.

Abstract: Standard benchmark datasets, such as MNIST, often fail to expose latent
biases and multimodal feature complexities, limiting the trustworthiness of
deep neural networks in high-stakes applications. We propose a novel multimodal
Explainable AI (XAI) framework that unifies attention-augmented feature fusion,
Grad-CAM++-based local explanations, and a Reveal-to-Revise feedback loop for
bias detection and mitigation. Evaluated on multimodal extensions of MNIST, our
approach achieves 93.2% classification accuracy, 91.6% F1-score, and 78.1%
explanation fidelity (IoU-XAI), outperforming unimodal and non-explainable
baselines. Ablation studies demonstrate that integrating interpretability with
bias-aware learning enhances robustness and human alignment. Our work bridges
the gap between performance, transparency, and fairness, highlighting a
practical pathway for trustworthy AI in sensitive domains.

</details>


### [107] [Balancing Performance and Reject Inclusion: A Novel Confident Inlier Extrapolation Framework for Credit Scoring](https://arxiv.org/abs/2510.12967)
*Athyrson Machado Ribeiro,Marcos Medeiros Raimundo*

Main category: cs.LG

TL;DR: Proposes CI-EX framework for reject inference in credit scoring, using outlier detection to identify distribution of rejected clients and assign labels based on supervised classification probabilities, outperforming existing methods on RI-specific metrics.


<details>
  <summary>Details</summary>
Motivation: Traditional reject inference methods assume rejected clients' behavior can be extrapolated from accepted clients, ignoring potential distributional differences between populations.

Method: Confident Inlier Extrapolation (CI-EX) framework that iteratively identifies rejected client distribution using outlier detection and assigns labels based on supervised classification probabilities.

Result: CI-EX consistently outperforms existing RI models on RI-specific metrics (Kickout, Area under Kickout) while maintaining competitive AUC performance across most experiments on real-world credit datasets.

Conclusion: RI methods involve trade-off between AUC and RI-specific metrics, but CI-EX framework achieves better performance on RI-specific metrics while maintaining competitive AUC.

Abstract: Reject Inference (RI) methods aim to address sample bias by inferring missing
repayment data for rejected credit applicants. Traditional approaches often
assume that the behavior of rejected clients can be extrapolated from accepted
clients, despite potential distributional differences between the two
populations. To mitigate this blind extrapolation, we propose a novel Confident
Inlier Extrapolation framework (CI-EX). CI-EX iteratively identifies the
distribution of rejected client samples using an outlier detection model and
assigns labels to rejected individuals closest to the distribution of the
accepted population based on probabilities derived from a supervised
classification model. The effectiveness of our proposed framework is validated
through experiments on two large real-world credit datasets. Performance is
evaluated using the Area Under the Curve (AUC) as well as RI-specific metrics
such as Kickout and a novel metric introduced in this work, denoted as Area
under the Kickout. Our findings reveal that RI methods, including the proposed
framework, generally involve a trade-off between AUC and RI-specific metrics.
However, the proposed CI-EX framework consistently outperforms existing RI
models from the credit literature in terms of RI-specific metrics while
maintaining competitive performance in AUC across most experiments.

</details>


### [108] [A Connection Between Score Matching and Local Intrinsic Dimension](https://arxiv.org/abs/2510.12975)
*Eric Yeats,Aaron Jacobson,Darryl Hannan,Yiran Jia,Timothy Doster,Henry Kvinge,Scott Mahan*

Main category: cs.LG

TL;DR: The paper proposes using denoising score matching loss as a scalable local intrinsic dimension (LID) estimator that outperforms existing methods in accuracy and memory efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing LID estimation methods using diffusion models require many forward passes or gradient computation, making them impractical for compute- and memory-constrained scenarios.

Method: Show that LID is a lower bound on denoising score matching loss, and demonstrate that implicit score matching loss also approximates LID via normal dimension, relating to FLIPD estimator.

Result: Experiments on manifold benchmark and Stable Diffusion 3.5 show denoising score matching loss achieves superior accuracy and memory footprint under increasing problem size and quantization.

Conclusion: Denoising score matching loss provides a highly competitive and scalable LID estimator that addresses computational limitations of previous approaches.

Abstract: The local intrinsic dimension (LID) of data is a fundamental quantity in
signal processing and learning theory, but quantifying the LID of
high-dimensional, complex data has been a historically challenging task. Recent
works have discovered that diffusion models capture the LID of data through the
spectra of their score estimates and through the rate of change of their
density estimates under various noise perturbations. While these methods can
accurately quantify LID, they require either many forward passes of the
diffusion model or use of gradient computation, limiting their applicability in
compute- and memory-constrained scenarios.
  We show that the LID is a lower bound on the denoising score matching loss,
motivating use of the denoising score matching loss as a LID estimator.
Moreover, we show that the equivalent implicit score matching loss also
approximates LID via the normal dimension and is closely related to a recent
LID estimator, FLIPD. Our experiments on a manifold benchmark and with Stable
Diffusion 3.5 indicate that the denoising score matching loss is a highly
competitive and scalable LID estimator, achieving superior accuracy and memory
footprint under increasing problem size and quantization level.

</details>


### [109] [Reference-Specific Unlearning Metrics Can Hide the Truth: A Reality Check](https://arxiv.org/abs/2510.12981)
*Sungjun Cho,Dasol Hwang,Frederic Sala,Sangheum Hwang,Kyunghyun Cho,Sungmin Cha*

Main category: cs.LG

TL;DR: FADE is a new metric that measures distributional similarity between unlearned and reference models using bidirectional likelihood assignments, addressing limitations of current unlearning metrics that rely on predetermined references.


<details>
  <summary>Details</summary>
Motivation: Current unlearning metrics evaluate success based on reference responses or classifier outputs rather than assessing whether unlearned models behave indistinguishably from models that never saw unwanted data, creating systematic blind spots.

Method: Proposes Functional Alignment for Distributional Equivalence (FADE) that measures distributional similarity by comparing bidirectional likelihood assignments over generated samples between unlearned and reference models.

Result: Experiments on TOFU and UnlearnCanvas benchmarks show methods achieving near-optimal scores on traditional metrics fail to achieve distributional equivalence, with many becoming more distant from the gold standard than before unlearning.

Conclusion: FADE exposes fundamental gaps in current evaluation practices and provides a more robust foundation for developing and assessing truly effective unlearning methods.

Abstract: Current unlearning metrics for generative models evaluate success based on
reference responses or classifier outputs rather than assessing the core
objective: whether the unlearned model behaves indistinguishably from a model
that never saw the unwanted data. This reference-specific approach creates
systematic blind spots, allowing models to appear successful while retaining
unwanted knowledge accessible through alternative prompts or attacks. We
address these limitations by proposing Functional Alignment for Distributional
Equivalence (FADE), a novel metric that measures distributional similarity
between unlearned and reference models by comparing bidirectional likelihood
assignments over generated samples. Unlike existing approaches that rely on
predetermined references, FADE captures functional alignment across the entire
output distribution, providing a principled assessment of genuine unlearning.
Our experiments on the TOFU benchmark for LLM unlearning and the UnlearnCanvas
benchmark for text-to-image diffusion model unlearning reveal that methods
achieving near-optimal scores on traditional metrics fail to achieve
distributional equivalence, with many becoming more distant from the gold
standard than before unlearning. These findings expose fundamental gaps in
current evaluation practices and demonstrate that FADE provides a more robust
foundation for developing and assessing truly effective unlearning methods.

</details>


### [110] [CSI-4CAST: A Hybrid Deep Learning Model for CSI Prediction with Comprehensive Robustness and Generalization Testing](https://arxiv.org/abs/2510.12996)
*Sikai Cheng,Reza Zandehshahvar,Haoruo Zhao,Daniel A. Garcia-Ulloa,Alejandro Villena-Rodriguez,Carles Navarro Manchón,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: CSI-4CAST is a hybrid deep learning architecture for CSI prediction that combines CNN residuals, adaptive correction layers, ShuffleNet blocks, and Transformers to efficiently capture local and long-range dependencies, achieving superior accuracy with lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods for CSI prediction lack robustness to non-Gaussian noise, generalization across diverse channel conditions, and computational efficiency, limiting their practical deployment in mMIMO systems.

Method: Hybrid architecture integrating 4 components: Convolutional neural network residuals, Adaptive correction layers, ShuffleNet blocks, and Transformers. Also introduces CSI-RRG benchmark with 300,000+ samples across 3,060 scenarios for rigorous evaluation.

Result: Outperforms baselines in 88.9% of TDD and 43.8% of FDD scenarios, achieves best performance among all models while reducing FLOPs by 5x and 3x compared to strongest baseline LLM4CP.

Conclusion: CSI-4CAST provides superior prediction accuracy with substantially lower computational cost, and the publicly released dataset and evaluation protocols establish a standardized benchmark for future research on robust CSI prediction.

Abstract: Channel state information (CSI) prediction is a promising strategy for
ensuring reliable and efficient operation of massive multiple-input
multiple-output (mMIMO) systems by providing timely downlink (DL) CSI. While
deep learning-based methods have advanced beyond conventional model-driven and
statistical approaches, they remain limited in robustness to practical
non-Gaussian noise, generalization across diverse channel conditions, and
computational efficiency. This paper introduces CSI-4CAST, a hybrid deep
learning architecture that integrates 4 key components, i.e., Convolutional
neural network residuals, Adaptive correction layers, ShuffleNet blocks, and
Transformers, to efficiently capture both local and long-range dependencies in
CSI prediction. To enable rigorous evaluation, this work further presents a
comprehensive benchmark, CSI-RRG for Regular, Robustness and Generalization
testing, which includes more than 300,000 samples across 3,060 realistic
scenarios for both TDD and FDD systems. The dataset spans multiple channel
models, a wide range of delay spreads and user velocities, and diverse noise
types and intensity degrees. Experimental results show that CSI-4CAST achieves
superior prediction accuracy with substantially lower computational cost,
outperforming baselines in 88.9% of TDD scenarios and 43.8% of FDD scenario,
the best performance among all evaluated models, while reducing FLOPs by 5x and
3x compared to LLM4CP, the strongest baseline. In addition, evaluation over
CSI-RRG provides valuable insights into how different channel factors affect
the performance and generalization capability of deep learning models. Both the
dataset (https://huggingface.co/CSI-4CAST) and evaluation protocols
(https://github.com/AI4OPT/CSI-4CAST) are publicly released to establish a
standardized benchmark and to encourage further research on robust and
efficient CSI prediction.

</details>


### [111] [Max It or Miss It: Benchmarking LLM On Solving Extremal Problems](https://arxiv.org/abs/2510.12997)
*Binxin Gao,Jingjun Han*

Main category: cs.LG

TL;DR: LLMs' reasoning capabilities for mathematical extremal problems don't align with general mathematical benchmarks, revealing gaps in current evaluation methods.


<details>
  <summary>Details</summary>
Motivation: To understand the specific sources and mechanisms behind LLMs' reasoning capabilities, particularly for optimization reasoning which underpins critical applications like planning, control, and resource allocation.

Method: Introduced ExtremBench - a benchmark dataset of 93 standardized extrema-finding problems curated from Chinese Mathematical Olympiad inequality exercises, and evaluated various state-of-the-art open-source LLMs including Qwen3, GPT-OSS, and DeepSeek.

Result: LLMs' extremal-solving reasoning capabilities don't align with performance on current mathematical benchmarks like AIME25 and MATH-500, with some models showing strong general mathematical reasoning but poor extremal-solving skills, and vice versa.

Conclusion: Existing benchmarks may not comprehensively capture the full spectrum of mathematical reasoning abilities, highlighting a critical gap in current evaluation practices.

Abstract: Test-time scaling has enabled Large Language Models (LLMs) with remarkable
reasoning capabilities, particularly in mathematical domains, through
intermediate chain-of-thought (CoT) reasoning before generating final answers.
However, the specific sources and mechanisms underlying these reasoning
capabilities remain insufficiently understood. Optimization reasoning, i.e.
finding extrema under constraints, represents a fundamental abstraction that
underpins critical applications in planning, control, resource allocation, and
prompt search. To systematically evaluate this capability, we introduce
ExtremBench, a benchmark dataset for solving mathematical extremal problems,
curated from inequality exercises used for Chinese Mathematical Olympiad and
transformed into $93$ standardized extrema-finding problems. We conduct
extensive evaluations across various state-of-the-art open-source model
families, including the Qwen3, GPT-OSS, and DeepSeek. Our results reveal that
LLMs' extremal-solving reasoning capabilities do not always align with those of
current mathematical benchmarks such as AIME25 and MATH-500, with some models
showing strong general mathematical reasoning but poor extremal-solving skills,
and vice versa. This discrepancy highlights a critical gap in current
evaluation practices and suggests that existing benchmarks may not
comprehensively capture the full spectrum of mathematical reasoning abilities.

</details>


### [112] [AMORE: Adaptive Multi-Output Operator Network for Stiff Chemical Kinetics](https://arxiv.org/abs/2510.12999)
*Kamaljyoti Nath,Additi Pandey,Bryan T. Susi,Hessam Babaee,George Em Karniadakis*

Main category: cs.LG

TL;DR: AMORE is an adaptive multi-output operator network framework that uses neural operators (DeepONets) as surrogates for stiff chemical kinetics in combustion systems, with adaptive loss functions and constraints to handle multiple thermochemical states efficiently.


<details>
  <summary>Details</summary>
Motivation: Stiff systems in combustion and reactive transport require extremely small time steps or computationally intensive implicit methods, creating significant computational costs. Neural operators can serve as surrogates but need reliable learning strategies to handle multiple output variables with different error characteristics.

Method: Developed AMORE framework with adaptive loss functions that consider each state variable's and sample's error. Used invertible analytical map to transform n-dimensional species mass-fraction to (n-1)-dimensional space to enforce unity constraint. Implemented two-step training for DeepONet with extended adaptive loss functions for trunk and branch training.

Result: Demonstrated efficacy on syngas (12 states) and GRI-Mech 3.0 (24 active states out of 54) systems. The framework successfully handles multiple thermochemical states and enforces physical constraints like unity mass-fraction.

Conclusion: AMORE provides a general framework for operator learning in stiff systems, enabling efficient surrogate modeling for combustion applications. The proposed DeepONet serves as a backbone for accelerating turbulent combustion simulations in future CFD studies.

Abstract: Time integration of stiff systems is a primary source of computational cost
in combustion, hypersonics, and other reactive transport systems. This
stiffness can introduce time scales significantly smaller than those associated
with other physical processes, requiring extremely small time steps in explicit
schemes or computationally intensive implicit methods. Consequently, strategies
to alleviate challenges posed by stiffness are important. While neural
operators (DeepONets) can act as surrogates for stiff kinetics, a reliable
operator learning strategy is required to appropriately account for differences
in the error between output variables and samples. Here, we develop AMORE,
Adaptive Multi-Output Operator Network, a framework comprising an operator
capable of predicting multiple outputs and adaptive loss functions ensuring
reliable operator learning. The operator predicts all thermochemical states
from given initial conditions. We propose two adaptive loss functions within
the framework, considering each state variable's and sample's error to penalize
the loss function. We designed the trunk to automatically satisfy Partition of
Unity. To enforce unity mass-fraction constraint exactly, we propose an
invertible analytical map that transforms the $n$-dimensional species
mass-fraction vector into an ($n-1$)-dimensional space, where DeepONet training
is performed. We consider two-step training for DeepONet for multiple outputs
and extend adaptive loss functions for trunk and branch training. We
demonstrate the efficacy and applicability of our models through two examples:
the syngas (12 states) and GRI-Mech 3.0 (24 active states out of 54). The
proposed DeepONet will be a backbone for future CFD studies to accelerate
turbulent combustion simulations. AMORE is a general framework, and here, in
addition to DeepONet, we also demonstrate it for FNO.

</details>


### [113] [Escaping Local Optima in the Waddington Landscape: A Multi-Stage TRPO-PPO Approach for Single-Cell Perturbation Analysis](https://arxiv.org/abs/2510.13018)
*Francis Boabang,Samuel Asante Gyamerah*

Main category: cs.LG

TL;DR: A multistage reinforcement learning algorithm for single-cell perturbation modeling that combines natural gradient updates with proximal policy optimization to escape local optima and improve generalization.


<details>
  <summary>Details</summary>
Motivation: Existing data-driven models for cellular perturbation prediction get trapped in local optima of the nonconvex Waddington landscape, leading to spurious lineages and implausible differentiation outcomes.

Method: Two-stage approach: 1) Natural gradient update using Fisher-vector products with KL trust-region constraint for safe initialization, 2) Proximal policy optimization (PPO) with clipped surrogates for policy refinement.

Result: The proposed initialization substantially improves generalization on both scRNA-seq and scATAC-seq perturbation analysis.

Conclusion: The multistage reinforcement learning approach provides better convergence to proper lineages in single-cell perturbation modeling by escaping local optima through well-designed initialization.

Abstract: Modeling cellular responses to genetic and chemical perturbations remains a
central challenge in single-cell biology. Existing data-driven framework have
advanced perturbation prediction through variational autoencoders, chemically
conditioned autoencoders, and large-scale transformer pretraining. However,
these models are prone to local optima in the nonconvex Waddington landscape of
cell fate decisions, where poor initialization can trap trajectories in
spurious lineages or implausible differentiation outcomes. While executable
gene regulatory networks complement these approaches, automated design
frameworks incorporate biological priors through multi-agent optimization. Yet,
an approach that is completely data-driven with well-designed initialization to
escape local optima and converge to a proper lineage remains elusive. In this
work, we introduce a multistage reinforcement learning algorithm tailored for
single-cell perturbation modeling. We first compute an explicit natural
gradient update using Fisher-vector products and a conjugate gradient solver,
scaled by a KL trust-region constraint to provide a safe, curvature-aware the
first step for the policy. Starting with these preconditioned parameters, we
then apply a second phase of proximal policy optimization (PPO) with clipped
surrogates, exploiting minibatch efficiency to refine the policy. We
demonstrate that this initialization substantially improves generalization on
Single-cell RNA sequencing (scRNA-seq) and Single-cell ATAC sequencing
(scATAC-seq) pertubation analysis.

</details>


### [114] [Time-Varying Optimization for Streaming Data Via Temporal Weighting](https://arxiv.org/abs/2510.13052)
*Muhammad Faraz Ul Abrar,Nicolò Michelusi,Erik G. Larsson*

Main category: cs.LG

TL;DR: This paper analyzes time-varying optimization for streaming data using weighted formulations, deriving tight bounds on tracking error for uniform and discounted weighting schemes under gradient descent.


<details>
  <summary>Details</summary>
Motivation: To address decision-making in dynamic environments where classical optimization with fixed objectives is insufficient, particularly for streaming data scenarios.

Method: Introduces a structured weight-based formulation for streaming data optimization, analyzes two weighting strategies (uniform and discounted weights), and derives tracking error bounds under gradient descent updates.

Result: Under uniform weighting, tracking error vanishes asymptotically with O(1/t) decay rate. Under discounted weighting, there's a nonzero error floor controlled by discount factor and number of gradient updates.

Conclusion: The choice of weighting scheme significantly impacts tracking performance in time-varying optimization for streaming data, with uniform weights providing asymptotic convergence while discounted weights maintain a controllable error floor.

Abstract: Classical optimization theory deals with fixed, time-invariant objective
functions. However, time-varying optimization has emerged as an important
subject for decision-making in dynamic environments. In this work, we study the
problem of learning from streaming data through a time-varying optimization
lens. Unlike prior works that focus on generic formulations, we introduce a
structured, \emph{weight-based} formulation that explicitly captures the
streaming-data origin of the time-varying objective, where at each time step,
an agent aims to minimize a weighted average loss over all the past data
samples. We focus on two specific weighting strategies: (1) uniform weights,
which treat all samples equally, and (2) discounted weights, which
geometrically decay the influence of older data. For both schemes, we derive
tight bounds on the ``tracking error'' (TE), defined as the deviation between
the model parameter and the time-varying optimum at a given time step, under
gradient descent (GD) updates. We show that under uniform weighting, the TE
vanishes asymptotically with a $\mathcal{O}(1/t)$ decay rate, whereas
discounted weighting incurs a nonzero error floor controlled by the discount
factor and the number of gradient updates performed at each time step. Our
theoretical findings are validated through numerical simulations.

</details>


### [115] [Machine Learning-Based Ultrasonic Weld Characterization Using Hierarchical Wave Modeling and Diffusion-Driven Distribution Alignment](https://arxiv.org/abs/2510.13023)
*Joshua R. Tempelman,Adam J. Wachtor,Eric B. Flynn*

Main category: cs.LG

TL;DR: This paper presents an end-to-end ML workflow for automated ultrasonic weld inspection, addressing data scarcity and signal corruption challenges through reduced-order modeling, diffusion-based distribution alignment, and U-Net segmentation.


<details>
  <summary>Details</summary>
Motivation: Automated ultrasonic weld inspection faces challenges due to limited training data (from complex experimental curation or simulations) and environmental volatility causing measurement corruption in industrial settings, preventing effective end-to-end ML solutions.

Method: Proposed workflow includes: 1) Reduced-order Helmholtz model based on Lamb wave theory to generate comprehensive dataset; 2) Transfer learning using limited 3D elastodynamic simulations; 3) Guided diffusion to handle out-of-distribution experimental data; 4) U-Net-based segmentation and inversion models.

Result: The integrated framework successfully provides an end-to-end solution for automated weld inspection on real data by handling both data scarcity and signal corruption issues.

Conclusion: The proposed workflow effectively addresses key challenges in automated ultrasonic weld inspection, enabling practical deployment in industrial settings through combined reduced-order modeling, diffusion-based distribution alignment, and deep learning segmentation.

Abstract: Automated ultrasonic weld inspection remains a significant challenge in the
nondestructive evaluation (NDE) community to factors such as limited training
data (due to the complexity of curating experimental specimens or high-fidelity
simulations) and environmental volatility of many industrial settings
(resulting in the corruption of on-the-fly measurements). Thus, an end-to-end
machine learning (ML) workflow for acoustic weld inspection in realistic (i.e.,
industrial) settings has remained an elusive goal. This work addresses the
challenges of data curation and signal corruption by proposing workflow
consisting of a reduced-order modeling scheme, diffusion based distribution
alignment, and U-Net-based segmentation and inversion. A reduced-order
Helmholtz model based on Lamb wave theory is used to generate a comprehensive
dataset over varying weld heterogeneity and crack defects. The relatively
inexpensive low-order solutions provide a robust training dateset for inversion
models which are refined through a transfer learning stage using a limited set
of full 3D elastodynamic simulations. To handle out-of-distribution (OOD)
real-world measurements with varying and unpredictable noise distributions,
i.e., Laser Doppler Vibrometry scans, guided diffusion produces in-distribution
representations of OOD experimental LDV scans which are subsequently processed
by the inversion models. This integrated framework provides an end-to-end
solution for automated weld inspection on real data.

</details>


### [116] [Transformer-based Scalable Beamforming Optimization via Deep Residual Learning](https://arxiv.org/abs/2510.13077)
*Yubo Zhang,Xiao-Yang Liu,Xiaodong Wang*

Main category: cs.LG

TL;DR: Unsupervised deep learning framework for downlink beamforming using Transformer architecture with curriculum learning, semi-amortized learning, and sliding-window training to achieve fast inference and competitive performance.


<details>
  <summary>Details</summary>
Motivation: Need for real-time beamforming in dynamic MU-MISO channels with faster inference than iterative optimization methods like WMMSE.

Method: Multi-layer Transformer with residual connections, trained offline using curriculum learning, semi-amortized learning (gradient ascent refinement), and sliding-window training of subsets of blocks.

Result: Outperforms baselines at low-to-medium SNRs, approaches WMMSE performance at high SNRs, with substantially faster inference than iterative and online learning methods.

Conclusion: The proposed unsupervised deep learning framework enables efficient real-time beamforming with competitive performance through advanced training strategies.

Abstract: We develop an unsupervised deep learning framework for downlink beamforming
in large-scale MU-MISO channels. The model is trained offline, allowing
real-time inference through lightweight feedforward computations in dynamic
communication environments. Following the learning-to-optimize (L2O) paradigm,
a multi-layer Transformer iteratively refines both channel and beamformer
features via residual connections. To enhance training, three strategies are
introduced: (i) curriculum learning (CL) to improve early-stage convergence and
avoid local optima, (ii) semi-amortized learning to refine each Transformer
block with a few gradient ascent steps, and (iii) sliding-window training to
stabilize optimization by training only a subset of Transformer blocks at a
time. Extensive simulations show that the proposed scheme outperforms existing
baselines at low-to-medium SNRs and closely approaches WMMSE performance at
high SNRs, while achieving substantially faster inference than iterative and
online learning approaches.

</details>


### [117] [Information Shapes Koopman Representation](https://arxiv.org/abs/2510.13025)
*Xiaoyuan Cheng,Wenxuan Yuan,Yiming Yang,Yuanzhao Zhang,Sibo Cheng,Yi He,Zhuo Sun*

Main category: cs.LG

TL;DR: The paper proposes an information-theoretic approach to Koopman operator learning that balances simplicity and expressiveness using mutual information and von Neumann entropy, preventing latent space collapse and improving performance.


<details>
  <summary>Details</summary>
Motivation: Existing Koopman learning methods struggle with finding suitable finite-dimensional subspaces due to suboptimal representation learning, where latent variables fail to balance expressivity and simplicity - a tension related to the information bottleneck dilemma.

Method: Proposes an information-theoretic Lagrangian formulation that explicitly balances simplicity (promoted by latent mutual information) and expressiveness (sustained by von Neumann entropy). Develops a new algorithm based on this formulation.

Result: The approach leads to stable and interpretable Koopman representations, prevents latent space collapse, and demonstrates improved performance across diverse dynamical systems compared to existing methods.

Conclusion: The information-theoretic framework successfully addresses the expressivity-simplicity tradeoff in Koopman learning, producing more effective and interpretable representations for dynamical systems modeling.

Abstract: The Koopman operator provides a powerful framework for modeling dynamical
systems and has attracted growing interest from the machine learning community.
However, its infinite-dimensional nature makes identifying suitable
finite-dimensional subspaces challenging, especially for deep architectures. We
argue that these difficulties come from suboptimal representation learning,
where latent variables fail to balance expressivity and simplicity. This
tension is closely related to the information bottleneck (IB) dilemma:
constructing compressed representations that are both compact and predictive.
Rethinking Koopman learning through this lens, we demonstrate that latent
mutual information promotes simplicity, yet an overemphasis on simplicity may
cause latent space to collapse onto a few dominant modes. In contrast,
expressiveness is sustained by the von Neumann entropy, which prevents such
collapse and encourages mode diversity. This insight leads us to propose an
information-theoretic Lagrangian formulation that explicitly balances this
tradeoff. Furthermore, we propose a new algorithm based on the Lagrangian
formulation that encourages both simplicity and expressiveness, leading to a
stable and interpretable Koopman representation. Beyond quantitative
evaluations, we further visualize the learned manifolds under our
representations, observing empirical results consistent with our theoretical
predictions. Finally, we validate our approach across a diverse range of
dynamical systems, demonstrating improved performance over existing Koopman
learning methods. The implementation is publicly available at
https://github.com/Wenxuan52/InformationKoopman.

</details>


### [118] [Bridging Idealized and Operational Models: An Explainable AI Framework for Earth System Emulators](https://arxiv.org/abs/2510.13030)
*Pouria Behnoudfar,Charlotte Moser,Marc Bocquet,Sibo Cheng,Nan Chen*

Main category: cs.LG

TL;DR: An explainable AI framework bridges high-resolution operational models with coarse-grained idealized models to improve Earth system simulations, particularly for extreme events and statistical distributions.


<details>
  <summary>Details</summary>
Motivation: High-resolution operational models have persistent biases in simulating extreme events, while idealized models excel at specific statistical features but remain siloed by disciplinary boundaries.

Method: A reconfigured latent data assimilation technique that leverages sparse output from idealized models to bridge the model hierarchy, creating a hybrid emulator that combines high resolution with statistical accuracy.

Result: The bridging model significantly corrects biases in CMIP6 simulations of El Niño spatiotemporal patterns while maintaining computational efficiency and physical interpretability.

Conclusion: The framework enables effective physics-assisted digital twins and uncertainty quantification, highlighting the importance of advancing idealized model development and interdisciplinary communication.

Abstract: Computer models are indispensable tools for understanding the Earth system.
While high-resolution operational models have achieved many successes, they
exhibit persistent biases, particularly in simulating extreme events and
statistical distributions. In contrast, coarse-grained idealized models isolate
fundamental processes and can be precisely calibrated to excel in
characterizing specific dynamical and statistical features. However, different
models remain siloed by disciplinary boundaries. By leveraging the
complementary strengths of models of varying complexity, we develop an
explainable AI framework for Earth system emulators. It bridges the model
hierarchy through a reconfigured latent data assimilation technique, uniquely
suited to exploit the sparse output from the idealized models. The resulting
bridging model inherits the high resolution and comprehensive variables of
operational models while achieving global accuracy enhancements through
targeted improvements from idealized models. Crucially, the mechanism of AI
provides a clear rationale for these advancements, moving beyond black-box
correction to physically insightful understanding in a computationally
efficient framework that enables effective physics-assisted digital twins and
uncertainty quantification. We demonstrate its power by significantly
correcting biases in CMIP6 simulations of El Ni\~no spatiotemporal patterns,
leveraging statistically accurate idealized models. This work also highlights
the importance of pushing idealized model development and advancing
communication between modeling communities.

</details>


### [119] [Randomness and Interpolation Improve Gradient Descent](https://arxiv.org/abs/2510.13040)
*Jiawen Li,Pascal Lefevre,Anwar Pp Abdul Majeed*

Main category: cs.LG

TL;DR: The paper introduces two new SGD optimizers: IAGD (using Newton interpolation for faster convergence) and NRSGD (using noise regularization to prevent overfitting), showing improved performance on CIFAR datasets compared to classical optimizers.


<details>
  <summary>Details</summary>
Motivation: To improve Stochastic Gradient Descent by addressing convergence speed and overfitting issues through interpolation-based acceleration and noise regularization techniques.

Method: Developed two optimizers: IAGD uses second-order Newton interpolation to accelerate convergence by leveraging gradient relevance between iterations, while NRSGD incorporates controlled noise into gradients as a regularization technique to prevent overfitting.

Result: Experiments on CIFAR-10 and CIFAR-100 datasets with various CNNs showed that both IAGD and NRSGD outperformed classical optimizers from the Keras package, demonstrating their effectiveness.

Conclusion: The proposed IAGD and NRSGD optimizers represent viable improvements to SGD, with IAGD providing faster convergence and NRSGD offering better regularization against overfitting.

Abstract: Based on Stochastic Gradient Descent (SGD), the paper introduces two
optimizers, named Interpolational Accelerating Gradient Descent (IAGD) as well
as Noise-Regularized Stochastic Gradient Descent (NRSGD). IAGD leverages
second-order Newton Interpolation to expedite the convergence process during
training, assuming relevancy in gradients between iterations. To avoid
over-fitting, NRSGD incorporates a noise regularization technique that
introduces controlled noise to the gradients during the optimization process.
Comparative experiments of this research are conducted on the CIFAR-10, and
CIFAR-100 datasets, benchmarking different CNNs(Convolutional Neural Networks)
with IAGD and NRSGD against classical optimizers in Keras Package. Results
demonstrate the potential of those two viable improvement methods in SGD,
implicating the effectiveness of the advancements.

</details>


### [120] [An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting](https://arxiv.org/abs/2510.13050)
*Shreya Agrawal,Mohammed Alewi Hassen,Emmanuel Asiedu Brempong,Boris Babenko,Fred Zyda,Olivia Graham,Di Li,Samier Merchant,Santiago Hincapie Potes,Tyler Russell,Danny Cheresnick,Aditya Prakash Kakkirala,Stephan Rasp,Avinatan Hassidim,Yossi Matias,Nal Kalchbrenner,Pramod Gupta,Jason Hickey,Aaron Bell*

Main category: cs.LG

TL;DR: Global MetNet is a machine learning model for global precipitation nowcasting that uses satellite and NWP data to provide 12-hour forecasts at 5km/15min resolution, outperforming traditional methods especially in data-sparse regions like the Global South.


<details>
  <summary>Details</summary>
Motivation: Traditional NWP methods have high latency and low resolution, while existing ML methods can't work in the Global South due to sparse radar coverage, creating forecast disparities that endanger vulnerable communities.

Method: Leverages Global Precipitation Mission's CORRA dataset, geostationary satellite data, and global NWP data to train a machine learning model that predicts precipitation for the next 12 hours at 0.05° (~5km) spatial and 15-minute temporal resolution.

Result: Significantly outperforms industry-standard hourly forecasts, achieves higher skill than best NWP models in data-sparse regions, shows improvements across key metrics (critical success index, fractions skill score), and generates forecasts in under a minute.

Conclusion: The model reduces global forecast disparities, integrates sparse satellite observations into forecasting, and is already deployed for millions of users, representing a key advancement in operational global nowcasting.

Abstract: Precipitation nowcasting, which predicts rainfall up to a few hours ahead, is
a critical tool for vulnerable communities in the Global South frequently
exposed to intense, rapidly developing storms. Timely forecasts provide a
crucial window to protect lives and livelihoods. Traditional numerical weather
prediction (NWP) methods suffer from high latency, low spatial and temporal
resolution, and significant gaps in accuracy across the world. Recent machine
learning-based nowcasting methods, common in the Global North, cannot be
extended to the Global South due to extremely sparse radar coverage. We present
Global MetNet, an operational global machine learning nowcasting model. It
leverages the Global Precipitation Mission's CORRA dataset, geostationary
satellite data, and global NWP data to predict precipitation for the next 12
hours. The model operates at a high resolution of approximately 0.05{\deg}
(~5km) spatially and 15 minutes temporally. Global MetNet significantly
outperforms industry-standard hourly forecasts and achieves significantly
higher skill, making forecasts useful over a much larger area of the world than
previously available. Our model demonstrates better skill in data-sparse
regions than even the best high-resolution NWP models achieve in the US.
Validated using ground radar and satellite data, it shows significant
improvements across key metrics like the critical success index and fractions
skill score for all precipitation rates and lead times. Crucially, our model
generates forecasts in under a minute, making it readily deployable for
real-time applications. It is already deployed for millions of users on Google
Search. This work represents a key step in reducing global disparities in
forecast quality and integrating sparse, high-resolution satellite observations
into weather forecasting.

</details>


### [121] [Achieving Logarithmic Regret in KL-Regularized Zero-Sum Markov Games](https://arxiv.org/abs/2510.13060)
*Anupam Nayak,Tong Yang,Osman Yagan,Gauri Joshi,Yuejie Chi*

Main category: cs.LG

TL;DR: The paper analyzes the theoretical benefits of KL regularization in game-theoretic settings, developing algorithms (OMG for Matrix games and SOMG for Markov games) that achieve improved sample efficiency with logarithmic regret scaling inversely with KL regularization strength.


<details>
  <summary>Details</summary>
Motivation: While KL regularization is widely used in reinforcement learning and alignment methods, its theoretical benefits in game-theoretic settings remain poorly understood despite empirical success in self-play methods with pretrained language models.

Method: Developed OMG algorithm for two-player zero-sum Matrix games using best response sampling with optimistic bonuses, and extended to Markov games with SOMG algorithm using best response sampling and novel superoptimistic bonuses.

Result: Both algorithms achieve logarithmic regret in T that scales inversely with KL regularization strength β, in addition to standard Õ(√T) regret independent of β, demonstrating improved sample efficiency under KL regularization.

Conclusion: The work provides theoretical justification for KL regularization in game-theoretic settings, showing it enables provably better sample efficiency through algorithms that leverage the regularization strength to achieve improved regret bounds.

Abstract: Reverse Kullback-Leibler (KL) divergence-based regularization with respect to
a fixed reference policy is widely used in modern reinforcement learning to
preserve the desired traits of the reference policy and sometimes to promote
exploration (using uniform reference policy, known as entropy regularization).
Beyond serving as a mere anchor, the reference policy can also be interpreted
as encoding prior knowledge about good actions in the environment. In the
context of alignment, recent game-theoretic approaches have leveraged KL
regularization with pretrained language models as reference policies, achieving
notable empirical success in self-play methods. Despite these advances, the
theoretical benefits of KL regularization in game-theoretic settings remain
poorly understood. In this work, we develop and analyze algorithms that
provably achieve improved sample efficiency under KL regularization. We study
both two-player zero-sum Matrix games and Markov games: for Matrix games, we
propose OMG, an algorithm based on best response sampling with optimistic
bonuses, and extend this idea to Markov games through the algorithm SOMG, which
also uses best response sampling and a novel concept of superoptimistic
bonuses. Both algorithms achieve a logarithmic regret in $T$ that scales
inversely with the KL regularization strength $\beta$ in addition to the
standard $\widetilde{\mathcal{O}}(\sqrt{T})$ regret independent of $\beta$
which is attained in both regularized and unregularized settings

</details>


### [122] [Absolute indices for determining compactness, separability and number of clusters](https://arxiv.org/abs/2510.13065)
*Adil M. Bagirov,Ramiz M. Aliguliyev,Nargiz Sultanova,Sona Taheri*

Main category: cs.LG

TL;DR: The paper introduces novel absolute cluster indices for evaluating cluster compactness and separability, aiming to identify the true number of clusters in datasets.


<details>
  <summary>Details</summary>
Motivation: Existing cluster validity indices are relative and depend on data structure, making it challenging to find true clusters across different models and algorithms.

Method: Defines compactness functions for individual clusters and neighboring points for cluster pairs to determine cluster compactness and separability margins.

Result: The proposed indices successfully identify the true number of clusters in both synthetic and real-world datasets, outperforming other widely-used cluster validity indices.

Conclusion: The new absolute cluster indices provide effective tools for determining optimal cluster numbers by evaluating both compactness and separability of clusters.

Abstract: Finding "true" clusters in a data set is a challenging problem. Clustering
solutions obtained using different models and algorithms do not necessarily
provide compact and well-separated clusters or the optimal number of clusters.
Cluster validity indices are commonly applied to identify such clusters.
Nevertheless, these indices are typically relative, and they are used to
compare clustering algorithms or choose the parameters of a clustering
algorithm. Moreover, the success of these indices depends on the underlying
data structure. This paper introduces novel absolute cluster indices to
determine both the compactness and separability of clusters. We define a
compactness function for each cluster and a set of neighboring points for
cluster pairs. This function is utilized to determine the compactness of each
cluster and the whole cluster distribution. The set of neighboring points is
used to define the margin between clusters and the overall distribution margin.
The proposed compactness and separability indices are applied to identify the
true number of clusters. Using a number of synthetic and real-world data sets,
we demonstrate the performance of these new indices and compare them with other
widely-used cluster validity indices.

</details>


### [123] [NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models](https://arxiv.org/abs/2510.13068)
*Konstantinos Barmpas,Na Lee,Alexandros Koliousis,Yannis Panagakis,Dimitrios A. Adamos,Nikolaos Laskaris,Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: NeuroRVQ is a scalable Large Brainwave Model with a codebook-based tokenizer that improves EEG signal representation learning by preserving high-frequency dynamics through multi-scale feature extraction, hierarchical residual vector quantization, and phase-amplitude aware loss.


<details>
  <summary>Details</summary>
Motivation: Existing EEG foundation models have limited performance due to poor signal tokenization that fails to preserve high-frequency dynamics, hindering accurate EEG signal reconstruction and representation learning.

Method: Developed NeuroRVQ with a codebook-based tokenizer featuring: multi-scale feature extraction for full frequency spectrum capture, hierarchical residual vector quantization for high-resolution encoding, and phase-amplitude aware loss function for efficient training.

Result: NeuroRVQ achieves lower reconstruction error and outperforms existing LBMs on various downstream tasks, enabling robust generative masked modeling and accurate reconstruction across all frequency bands.

Conclusion: NeuroRVQ establishes a strong prior for codebook-based general-purpose brainwave models, advancing neural decoding, generative modeling, and multimodal biosignal integration.

Abstract: Electroencephalography (EEG) captures neural activity across multiple
temporal and spectral scales, yielding signals that are rich but complex for
representation learning. Recently, EEG foundation models trained to predict
masked signal-tokens have shown promise for learning generalizable
representations. However, their performance is hindered by their signal
tokenization modules. Existing neural tokenizers fail to preserve
high-frequency dynamics, limiting their ability to reconstruct EEG signals with
high fidelity. We introduce NeuroRVQ, a scalable Large Brainwave Model (LBM)
centered on a codebook-based tokenizer. Our tokenizer integrates: (i)
multi-scale feature extraction modules that capture the full frequency neural
spectrum; (ii) hierarchical residual vector quantization (RVQ) codebooks for
high-resolution encoding; and, (iii) an EEG signal phase- and amplitude-aware
loss function for efficient training. This design enables efficient EEG
compression while supporting accurate reconstruction across all frequency
bands, leading to robust generative masked modeling. Our empirical results
demonstrate that NeuroRVQ achieves lower reconstruction error and outperforms
existing LBMs on a variety of downstream tasks. More broadly, NeuroRVQ
tokenizer establishes a strong prior for codebook-based general-purpose
brainwave models, enabling advances in neural decoding, generative modeling and
multimodal biosignal integration.

</details>


### [124] [DeepCausalMMM: A Deep Learning Framework for Marketing Mix Modeling with Causal Inference](https://arxiv.org/abs/2510.13087)
*Aditya Puttaparthi Tirumala*

Main category: cs.LG

TL;DR: DeepCausalMMM is a Python package that combines deep learning, causal inference, and marketing science to improve Marketing Mix Modeling by automatically learning temporal patterns, channel dependencies, and saturation effects.


<details>
  <summary>Details</summary>
Motivation: Traditional MMM approaches struggle with capturing complex temporal dynamics, non-linear saturation effects, and channel interdependencies, relying on linear regression and independence assumptions.

Method: Uses GRUs for temporal pattern learning, DAG learning for causal structures between channels, Hill equation-based saturation curves, multi-region modeling with shared/specific parameters, Huber loss, and advanced regularization.

Result: The package provides data-driven hyperparameter learning, comprehensive response curve analysis, and extensive visualization with 14+ interactive dashboards for business insights.

Conclusion: DeepCausalMMM addresses limitations of traditional MMM by combining deep learning and causal inference to better model marketing impacts and optimize budget allocation.

Abstract: Marketing Mix Modeling (MMM) is a statistical technique used to estimate the
impact of marketing activities on business outcomes such as sales, revenue, or
customer visits. Traditional MMM approaches often rely on linear regression or
Bayesian hierarchical models that assume independence between marketing
channels and struggle to capture complex temporal dynamics and non-linear
saturation effects [@Hanssens2005; @Ng2021Bayesian].
  DeepCausalMMM is a Python package that addresses these limitations by
combining deep learning, causal inference, and advanced marketing science. The
package uses Gated Recurrent Units (GRUs) to automatically learn temporal
patterns such as adstock (carryover effects) and lag, while simultaneously
learning statistical dependencies and potential causal structures between
marketing channels through Directed Acyclic Graph (DAG) learning
[@Zheng2018NOTEARS; @Gong2024CausalMMM]. Additionally, it implements Hill
equation-based saturation curves to model diminishing returns and optimize
budget allocation.
  Key innovations include: (1) a data-driven design where hyperparameters and
transformations (e.g., adstock decay, saturation curves) are learned or
estimated from data with sensible defaults, rather than requiring fixed
heuristics or manual specification, (2) multi-region modeling with both shared
and region-specific parameters, (3) robust statistical methods including Huber
loss and advanced regularization, (4) comprehensive response curve analysis for
understanding channel saturation, and (5) an extensive visualization suite with
14+ interactive dashboards for business insights.

</details>


### [125] [Neural Triangular Transport Maps: A New Approach Towards Sampling in Lattice QCD](https://arxiv.org/abs/2510.13112)
*Andrey Bryutkin,Youssef Marzouk*

Main category: cs.LG

TL;DR: Proposes sparse triangular transport maps using monotone rectified neural networks to efficiently sample lattice field theories by exploiting conditional independence structure, achieving linear time complexity while maintaining expressivity.


<details>
  <summary>Details</summary>
Motivation: Sampling Boltzmann distributions in lattice field theories is challenging due to multimodality and long-range correlations. Normalizing flows offer promise but face prohibitive memory requirements and expressivity challenges for large lattices.

Method: Sparse triangular transport maps that exploit conditional independence structure using monotone rectified neural networks (MRNN). Framework balances exact sparsity (respecting conditional independence) and approximate sparsity (computational tractability). Each map component restricted to local past enables site-wise parallel evaluation.

Result: Achieves linear time complexity in lattice size N while preserving expressive, invertible structure. Analyzed how node labelings affect sparsity and performance using φ⁴ in 2D as test case.

Conclusion: Proposed method provides efficient alternative to Hybrid Monte Carlo and established flow approaches (RealNVP) for sampling lattice field theories, addressing memory and expressivity limitations of previous flow-based methods.

Abstract: Lattice field theories are fundamental testbeds for computational physics;
yet, sampling their Boltzmann distributions remains challenging due to
multimodality and long-range correlations. While normalizing flows offer a
promising alternative, their application to large lattices is often constrained
by prohibitive memory requirements and the challenge of maintaining sufficient
model expressivity. We propose sparse triangular transport maps that explicitly
exploit the conditional independence structure of the lattice graph under
periodic boundary conditions using monotone rectified neural networks (MRNN).
We introduce a comprehensive framework for triangular transport maps that
navigates the fundamental trade-off between \emph{exact sparsity} (respecting
marginal conditional independence in the target distribution) and
\emph{approximate sparsity} (computational tractability without fill-ins).
Restricting each triangular map component to a local past enables site-wise
parallel evaluation and linear time complexity in lattice size $N$, while
preserving the expressive, invertible structure. Using $\phi^4$ in two
dimensions as a controlled setting, we analyze how node labelings (orderings)
affect the sparsity and performance of triangular maps. We compare against
Hybrid Monte Carlo (HMC) and established flow approaches (RealNVP).

</details>


### [126] [On the Reasoning Abilities of Masked Diffusion Language Models](https://arxiv.org/abs/2510.13117)
*Anej Svete,Ashish Sabharwal*

Main category: cs.LG

TL;DR: Masked diffusion models (MDMs) for text are equivalent to polynomially-padded looped transformers and can solve all problems that chain-of-thought transformers can, while being more efficient for certain tasks like regular languages due to parallel generation.


<details>
  <summary>Details</summary>
Motivation: To characterize the computational capabilities and efficiency of masked diffusion models for text, which offer parallel generation as an alternative to autoregressive models but have unexplored limitations.

Method: Connecting MDMs to established reasoning frameworks (chain of thought and padded looped transformers) in the finite-precision log-width setting, proving equivalence relationships and efficiency comparisons.

Result: MDMs are equivalent to polynomially-padded looped transformers and can solve all problems that CoT-augmented transformers can. MDMs are inherently more efficient than CoT transformers for certain problem classes including regular languages.

Conclusion: Masked diffusion models provide a powerful parallel alternative to autoregressive models with proven computational capabilities and efficiency advantages for specific problem types.

Abstract: Masked diffusion models (MDMs) for text offer a compelling alternative to
traditional autoregressive language models. Parallel generation makes them
efficient, but their computational capabilities and the limitations inherent to
their parallelism remain largely unexplored. To this end, we characterize what
types of reasoning problems MDMs can provably solve and how efficiently. We do
this by connecting MDMs to the well-understood reasoning frameworks of chain of
thought (CoT) and padded looped transformers (PLTs) in the finite-precision
log-width setting: We show that MDMs and polynomially-padded PLTs are, in fact,
equivalent in this setting, and that MDMs can solve all problems that
CoT-augmented transformers can. Moreover, we showcase classes of problems
(including regular languages) for which MDMs are inherently more efficient than
CoT transformers, where parallel generation allows for substantially faster
reasoning.

</details>


### [127] [Cluster-Based Client Selection for Dependent Multi-Task Federated Learning in Edge Computing](https://arxiv.org/abs/2510.13132)
*Jieping Luo,Qiyue Li,Zhizhang Liu,Hang Qi,Jiaying Yin,Jingjin Wu*

Main category: cs.LG

TL;DR: CoDa-FL is a cluster-oriented and dependency-aware framework for federated learning in mobile edge computing that reduces total completion time through client clustering and dependent task assignment.


<details>
  <summary>Details</summary>
Motivation: To reduce the total time required to complete various learning tasks in federated learning within mobile edge computing environments, particularly under dependent multi-task settings.

Method: Uses Earth Mover's Distance for client clustering based on local data distributions, derives relationship between intra-cluster EMD and training rounds, and incorporates directed acyclic graph-based task scheduling for dependency management.

Result: Outperforms existing benchmarks with faster convergence, lower communication and computational costs, and higher learning accuracy under heterogeneous MEC settings.

Conclusion: CoDa-FL effectively reduces total completion time in federated learning through cluster-based client selection and dependency-aware task assignment in mobile edge computing environments.

Abstract: We study the client selection problem in Federated Learning (FL) within
mobile edge computing (MEC) environments, particularly under the dependent
multi-task settings, to reduce the total time required to complete various
learning tasks. We propose CoDa-FL, a Cluster-oriented and Dependency-aware
framework designed to reduce the total required time via cluster-based client
selection and dependent task assignment. Our approach considers Earth Mover's
Distance (EMD) for client clustering based on their local data distributions to
lower computational cost and improve communication efficiency. We derive a
direct and explicit relationship between intra-cluster EMD and the number of
training rounds required for convergence, thereby simplifying the otherwise
complex process of obtaining the optimal solution. Additionally, we incorporate
a directed acyclic graph-based task scheduling mechanism to effectively manage
task dependencies. Through numerical experiments, we validate that our proposed
CoDa-FL outperforms existing benchmarks by achieving faster convergence, lower
communication and computational costs, and higher learning accuracy under
heterogeneous MEC settings.

</details>


### [128] [Convergence, design and training of continuous-time dropout as a random batch method](https://arxiv.org/abs/2510.13134)
*Antonio Álvarez-López,Martín Hernández*

Main category: cs.LG

TL;DR: This paper analyzes dropout regularization in continuous-time models using random-batch methods, establishing convergence rates and stability properties while deriving optimal sampling parameters.


<details>
  <summary>Details</summary>
Motivation: To study dropout regularization in continuous-time models through stochastic sampling schemes that reduce computational cost, while maintaining theoretical guarantees.

Method: Constructs an unbiased estimator that mimics dropout by sampling neuron batches over time intervals, analyzes convergence rates, stability of continuity equations, and Pontryagin-based adjoint analysis for training deviations.

Result: Establishes linear convergence rate for expected uniform error, total-variation error of order h^{1/2} for distribution stability, and derives optimal sampling parameter h through cost-accuracy trade-off analysis.

Conclusion: The random-batch dropout approach provides theoretical guarantees, regularization effects, and favorable computational profiles, validated on neural ODE applications for classification and flow matching.

Abstract: We study dropout regularization in continuous-time models through the lens of
random-batch methods -- a family of stochastic sampling schemes originally
devised to reduce the computational cost of interacting particle systems. We
construct an unbiased, well-posed estimator that mimics dropout by sampling
neuron batches over time intervals of length $h$. Trajectory-wise convergence
is established with linear rate in $h$ for the expected uniform error. At the
distribution level, we establish stability for the associated continuity
equation, with total-variation error of order $h^{1/2}$ under mild moment
assumptions. During training with fixed batch sampling across epochs, a
Pontryagin-based adjoint analysis bounds deviations in the optimal cost and
control, as well as in gradient-descent iterates. On the design side, we
compare convergence rates for canonical batch sampling schemes, recover
standard Bernoulli dropout as a special case, and derive a cost--accuracy
trade-off yielding a closed-form optimal $h$. We then specialize to a
single-layer neural ODE and validate the theory on classification and flow
matching, observing the predicted rates, regularization effects, and favorable
runtime and memory profiles.

</details>


### [129] [Behavioral Embeddings of Programs: A Quasi-Dynamic Approach for Optimization Prediction](https://arxiv.org/abs/2510.13158)
*Haolin Pan,Jinyuan Dong,Hongbin Zhang,Hongyu Lin,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: A quasi-dynamic framework for program representation that models optimization sensitivity using Program Behavior Spectrum, encoded via compositional learning with Product Quantization and PQ-BERT Transformer.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of static representations (limited insight into program behavior) and dynamic representations (high overhead and non-determinism) for compiler optimization tasks.

Method: Probe program's IR with diverse optimization sequences, quantify changes in static features to create Program Behavior Spectrum, use Product Quantization to discretize reaction vectors, and train PQ-BERT Transformer model.

Result: Outperforms state-of-the-art static baselines on Best Pass Prediction and -Oz Benefit Prediction tasks.

Conclusion: The quasi-dynamic framework effectively bridges the gap between static and dynamic program representations for compiler optimization.

Abstract: Learning effective numerical representations, or embeddings, of programs is a
fundamental prerequisite for applying machine learning to automate and enhance
compiler optimization. Prevailing paradigms, however, present a dilemma. Static
representations, derived from source code or intermediate representation (IR),
are efficient and deterministic but offer limited insight into how a program
will behave or evolve under complex code transformations. Conversely, dynamic
representations, which rely on runtime profiling, provide profound insights
into performance bottlenecks but are often impractical for large-scale tasks
due to prohibitive overhead and inherent non-determinism. This paper transcends
this trade-off by proposing a novel quasi-dynamic framework for program
representation. The core insight is to model a program's optimization
sensitivity. We introduce the Program Behavior Spectrum, a new representation
generated by probing a program's IR with a diverse set of optimization
sequences and quantifying the resulting changes in its static features. To
effectively encode this high-dimensional, continuous spectrum, we pioneer a
compositional learning approach. Product Quantization is employed to discretize
the continuous reaction vectors into structured, compositional sub-words.
Subsequently, a multi-task Transformer model, termed PQ-BERT, is pre-trained to
learn the deep contextual grammar of these behavioral codes. Comprehensive
experiments on two representative compiler optimization tasks -- Best Pass
Prediction and -Oz Benefit Prediction -- demonstrate that our method
outperforms state-of-the-art static baselines. Our code is publicly available
at https://github.com/Panhaolin2001/PREP/.

</details>


### [130] [Universally Invariant Learning in Equivariant GNNs](https://arxiv.org/abs/2510.13169)
*Jiacheng Cen,Anyi Li,Ning Lin,Tingyang Xu,Yu Rong,Deli Zhao,Zihe Wang,Wenbing Huang*

Main category: cs.LG

TL;DR: Proposes an efficient framework for constructing complete equivariant GNNs using canonical forms and steerable basis sets, achieving completeness with fewer layers and lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing equivariant GNNs require deep architectures, high body orders, or complex steerable features for completeness, leading to high computational costs without polynomial-time solutions.

Method: Uses two key components: 1) a complete scalar function (canonical form of geometric graph), and 2) a full-rank steerable basis set. Applies this to EGNN and TFN models.

Result: Empirical results show superior completeness and excellent performance with only a few layers, significantly reducing computational overhead while maintaining strong practical efficacy.

Conclusion: The proposed framework provides a theoretically grounded, efficient, and practical approach for constructing complete equivariant GNNs.

Abstract: Equivariant Graph Neural Networks (GNNs) have demonstrated significant
success across various applications. To achieve completeness -- that is, the
universal approximation property over the space of equivariant functions -- the
network must effectively capture the intricate multi-body interactions among
different nodes. Prior methods attain this via deeper architectures, augmented
body orders, or increased degrees of steerable features, often at high
computational cost and without polynomial-time solutions. In this work, we
present a theoretically grounded framework for constructing complete
equivariant GNNs that is both efficient and practical. We prove that a complete
equivariant GNN can be achieved through two key components: 1) a complete
scalar function, referred to as the canonical form of the geometric graph; and
2) a full-rank steerable basis set. Leveraging this finding, we propose an
efficient algorithm for constructing complete equivariant GNNs based on two
common models: EGNN and TFN. Empirical results demonstrate that our model
demonstrates superior completeness and excellent performance with only a few
layers, thereby significantly reducing computational overhead while maintaining
strong practical efficacy.

</details>


### [131] [Information-Theoretic Criteria for Knowledge Distillation in Multimodal Learning](https://arxiv.org/abs/2510.13182)
*Rongrong Xie,Yizhou Xu,Guido Sanguinetti*

Main category: cs.LG

TL;DR: The paper introduces the Cross-modal Complementarity Hypothesis (CCH) to explain when cross-modal knowledge distillation works, showing it's effective when teacher-student mutual information exceeds student-label mutual information.


<details>
  <summary>Details</summary>
Motivation: Cross-modal knowledge distillation doesn't always improve performance due to limited theoretical understanding, creating a need for principled guidelines.

Method: Proposed Cross-modal Complementarity Hypothesis (CCH), theoretically validated in joint Gaussian model and empirically tested across multimodal datasets (image, text, video, audio, omics).

Result: The CCH criterion accurately predicts when cross-modal KD is effective, providing a theoretical framework and practical guidelines for selecting optimal teacher modalities.

Conclusion: The study establishes a novel theoretical foundation for cross-modal KD and offers practical criteria for modality selection to improve weaker modalities' performance.

Abstract: The rapid increase in multimodal data availability has sparked significant
interest in cross-modal knowledge distillation (KD) techniques, where richer
"teacher" modalities transfer information to weaker "student" modalities during
model training to improve performance. However, despite successes across
various applications, cross-modal KD does not always result in improved
outcomes, primarily due to a limited theoretical understanding that could
inform practice. To address this gap, we introduce the Cross-modal
Complementarity Hypothesis (CCH): we propose that cross-modal KD is effective
when the mutual information between teacher and student representations exceeds
the mutual information between the student representation and the labels. We
theoretically validate the CCH in a joint Gaussian model and further confirm it
empirically across diverse multimodal datasets, including image, text, video,
audio, and cancer-related omics data. Our study establishes a novel theoretical
framework for understanding cross-modal KD and offers practical guidelines
based on the CCH criterion to select optimal teacher modalities for improving
the performance of weaker modalities.

</details>


### [132] [CleverCatch: A Knowledge-Guided Weak Supervision Model for Fraud Detection](https://arxiv.org/abs/2510.13205)
*Amirhossein Mozafari,Kourosh Hashemi,Erfan Shafagh,Soroush Motamedi,Azar Taheri Tayebi,Mohammad A. Tayebi*

Main category: cs.LG

TL;DR: CleverCatch is a knowledge-guided weak supervision model that integrates domain expertise with neural networks to detect healthcare fraud, outperforming state-of-the-art methods while providing interpretability.


<details>
  <summary>Details</summary>
Motivation: Healthcare fraud detection faces challenges from limited labeled data, evolving fraud tactics, and high-dimensional medical records. Traditional supervised methods struggle with label scarcity, while unsupervised approaches often miss clinically meaningful anomalies.

Method: Integrates structured domain expertise into a neural architecture that aligns rules and data samples in a shared embedding space. Trains encoders jointly on synthetic compliance and violation data to learn soft rule embeddings that generalize to real-world datasets.

Result: Outperforms four state-of-the-art anomaly detection baselines with average improvements of 1.3% in AUC and 3.4% in recall on large-scale real-world datasets. Ablation study confirms the complementary role of expert rules.

Conclusion: Embedding expert rules into the learning process improves both detection accuracy and transparency, offering an interpretable approach suitable for high-stakes domains like healthcare fraud detection.

Abstract: Healthcare fraud detection remains a critical challenge due to limited
availability of labeled data, constantly evolving fraud tactics, and the high
dimensionality of medical records. Traditional supervised methods are
challenged by extreme label scarcity, while purely unsupervised approaches
often fail to capture clinically meaningful anomalies. In this work, we
introduce CleverCatch, a knowledge-guided weak supervision model designed to
detect fraudulent prescription behaviors with improved accuracy and
interpretability. Our approach integrates structured domain expertise into a
neural architecture that aligns rules and data samples within a shared
embedding space. By training encoders jointly on synthetic data representing
both compliance and violation, CleverCatch learns soft rule embeddings that
generalize to complex, real-world datasets. This hybrid design enables
data-driven learning to be enhanced by domain-informed constraints, bridging
the gap between expert heuristics and machine learning. Experiments on the
large-scale real-world dataset demonstrate that CleverCatch outperforms four
state-of-the-art anomaly detection baselines, yielding average improvements of
1.3\% in AUC and 3.4\% in recall. Our ablation study further highlights the
complementary role of expert rules, confirming the adaptability of the
framework. The results suggest that embedding expert rules into the learning
process not only improves detection accuracy but also increases transparency,
offering an interpretable approach for high-stakes domains such as healthcare
fraud detection.

</details>


### [133] [Performance Evaluation of Ising and QUBO Variable Encodings in Boltzmann Machine Learning](https://arxiv.org/abs/2510.13210)
*Yasushi Hasegawa,Masayuki Ohzeki*

Main category: cs.LG

TL;DR: Ising encoding provides better conditioning and faster convergence than QUBO for Boltzmann machine learning under SGD, but NGD achieves similar performance across encodings due to reparameterization invariance.


<details>
  <summary>Details</summary>
Motivation: To understand how different variable encodings (Ising vs QUBO) affect the information geometry and learning dynamics in Boltzmann machines, particularly focusing on convergence behavior under different optimization methods.

Method: Controlled comparison protocol fixing model, sampler, and step size; analysis using Fisher information matrix (FIM) and empirical moments; evaluation of SGD and natural gradient descent (NGD) performance across encodings.

Result: QUBO encoding creates more ill-conditioned FIM with smaller eigenvalues and lower spectral entropy, leading to slower SGD convergence. Ising encoding provides more isotropic curvature and faster convergence. NGD achieves similar convergence across encodings.

Conclusion: Ising encoding is preferable for SGD-based training due to better conditioning, while for QUBO, centering/scaling or NGD-style preconditioning can mitigate curvature issues. The study provides practical guidelines for variable encoding in Boltzmann machine learning.

Abstract: We compare Ising ({-1,+1}) and QUBO ({0,1}) encodings for Boltzmann machine
learning under a controlled protocol that fixes the model, sampler, and step
size. Exploiting the identity that the Fisher information matrix (FIM) equals
the covariance of sufficient statistics, we visualize empirical moments from
model samples and reveal systematic, representation-dependent differences. QUBO
induces larger cross terms between first- and second-order statistics, creating
more small-eigenvalue directions in the FIM and lowering spectral entropy. This
ill-conditioning explains slower convergence under stochastic gradient descent
(SGD). In contrast, natural gradient descent (NGD)-which rescales updates by
the FIM metric-achieves similar convergence across encodings due to
reparameterization invariance. Practically, for SGD-based training, the Ising
encoding provides more isotropic curvature and faster convergence; for QUBO,
centering/scaling or NGD-style preconditioning mitigates curvature pathologies.
These results clarify how representation shapes information geometry and
finite-time learning dynamics in Boltzmann machines and yield actionable
guidelines for variable encoding and preprocessing.

</details>


### [134] [Towards Understanding Valuable Preference Data for Large Language Model Alignment](https://arxiv.org/abs/2510.13212)
*Zizhuo Zhang,Qizhou Wang,Shanshan Ye,Jianing Zhu,Jiangchao Yao,Bo Han,Masashi Sugiyama*

Main category: cs.LG

TL;DR: The paper proposes a new approach for selecting high-quality preference data for LLM alignment using truncated influence functions and model-dependent scoring functions to identify genuinely beneficial data points.


<details>
  <summary>Details</summary>
Motivation: Existing LLM alignment methods use pre-processed preference data but don't assess whether individual data points are genuinely beneficial to specific models, as data quality is model-dependent.

Method: Proposed truncated influence function (TIF) to measure individual data influence, developed simpler scoring functions correlated with TIF, and combined them to offset errors for effective data selection.

Result: Experiments across diverse alignment benchmarks and LLM families show better alignment performance using less data, demonstrating the effectiveness of the proposed methods.

Conclusion: Preference data quality is model-dependent, and the proposed model-adaptive data selection approach enables more precise selection of valuable data for improved LLM alignment with less data.

Abstract: Large language model (LLM) alignment is typically achieved through learning
from human preference comparisons, making the quality of preference data
critical to its success. Existing studies often pre-process raw training
datasets to identify valuable preference pairs using external reward models or
off-the-shelf LLMs, achieving improved overall performance but rarely examining
whether individual, selected data point is genuinely beneficial. We assess data
quality through individual influence on validation data using our newly
proposed truncated influence function (TIF), which mitigates the over-scoring
present in traditional measures and reveals that preference data quality is
inherently a property of the model. In other words, a data pair that benefits
one model may harm another. This leaves the need to improve the preference data
selection approaches to be adapting to specific models. To this end, we
introduce two candidate scoring functions (SFs) that are computationally
simpler than TIF and positively correlated with it. They are also model
dependent and can serve as potential indicators of individual data quality for
preference data selection. Furthermore, we observe that these SFs inherently
exhibit errors when compared to TIF. To this end, we combine them to offset
their diverse error sources, resulting in a simple yet effective data selection
rule that enables the models to achieve a more precise selection of valuable
preference data. We conduct experiments across diverse alignment benchmarks and
various LLM families, with results demonstrating that better alignment
performance can be achieved using less data, showing the generality of our
findings and new methods.

</details>


### [135] [Rethinking Graph Domain Adaptation: A Spectral Contrastive Perspective](https://arxiv.org/abs/2510.13254)
*Haoyu Zhang,Yuxuan Cheng,Wenqi Fan,Yulong Chen,Yifan Zhang*

Main category: cs.LG

TL;DR: FracNet is a frequency-aware contrastive graph network that addresses domain adaptation in GNNs by decomposing graphs into low-frequency (global) and high-frequency (local) components, using spectral analysis and contrastive learning to handle structural distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Traditional GNNs struggle with domain adaptation due to structural distribution shifts and insufficient exploration of transferable patterns, as they don't discriminate between global and local patterns, leading to loss of local details after multi-layer processing.

Method: Proposes FracNet with two synergic modules: decomposes graphs into high-frequency (domain-specific local details) and low-frequency (domain-invariant global patterns) components using spectral analysis, and performs frequency-aware domain adaptation integrated with contrastive learning to address blurring boundary problems.

Result: Extensive experiments demonstrate significant improvements over state-of-the-art approaches in domain adaptation tasks.

Conclusion: FracNet effectively handles domain shifts in graph neural networks through spectral decomposition and contrastive learning, with both practical success and theoretical validation of its superiority.

Abstract: Graph neural networks (GNNs) have achieved remarkable success in various
domains, yet they often struggle with domain adaptation due to significant
structural distribution shifts and insufficient exploration of transferable
patterns. One of the main reasons behind this is that traditional approaches do
not treat global and local patterns discriminatingly so that some local details
in the graph may be violated after multi-layer GNN. Our key insight is that
domain shifts can be better understood through spectral analysis, where
low-frequency components often encode domain-invariant global patterns, and
high-frequency components capture domain-specific local details. As such, we
propose FracNet (\underline{\textbf{Fr}}equency \underline{\textbf{A}}ware
\underline{\textbf{C}}ontrastive Graph \underline{\textbf{Net}}work) with two
synergic modules to decompose the original graph into high-frequency and
low-frequency components and perform frequency-aware domain adaption. Moreover,
the blurring boundary problem of domain adaptation is improved by integrating
with a contrastive learning framework. Besides the practical implication, we
also provide rigorous theoretical proof to demonstrate the superiority of
FracNet. Extensive experiments further demonstrate significant improvements
over state-of-the-art approaches.

</details>


### [136] [Hypernetworks for Perspectivist Adaptation](https://arxiv.org/abs/2510.13259)
*Daniil Ignatev,Denis Paperno,Massimo Poesio*

Main category: cs.LG

TL;DR: The paper applies hypernetwork+adapters architecture to perspective-aware classification, achieving competitive performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Address the parametric efficiency bottleneck in perspective-aware classification that hasn't been sufficiently recognized in existing studies.

Method: Apply hypernetwork+adapters combination to perspectivist classification, creating an architecture-agnostic solution that works with various base models.

Result: The solution competes with specialized models in adopting user perspectives on hate speech and toxicity detection while using considerably fewer parameters.

Conclusion: The proposed approach provides an efficient, architecture-agnostic solution for perspective-aware classification that reduces parameter usage while maintaining competitive performance.

Abstract: The task of perspective-aware classification introduces a bottleneck in terms
of parametric efficiency that did not get enough recognition in existing
studies. In this article, we aim to address this issue by applying an existing
architecture, the hypernetwork+adapters combination, to perspectivist
classification. Ultimately, we arrive at a solution that can compete with
specialized models in adopting user perspectives on hate speech and toxicity
detection, while also making use of considerably fewer parameters. Our solution
is architecture-agnostic and can be applied to a wide range of base models out
of the box.

</details>


### [137] [BlendFL: Blended Federated Learning for Handling Multimodal Data Heterogeneity](https://arxiv.org/abs/2510.13266)
*Alejandro Guerra-Manzanares,Omar El-Herraoui,Michail Maniatakos,Farah E. Shamout*

Main category: cs.LG

TL;DR: BlendFL is a novel federated learning framework that combines horizontal and vertical FL principles to handle multimodal data heterogeneity across clients, enabling flexible participation and decentralized inference.


<details>
  <summary>Details</summary>
Motivation: Existing FL frameworks (horizontal/vertical) struggle with real-world multimodal data heterogeneity where neither all modalities nor all samples are represented across clients, creating gaps in collaborative learning.

Method: BlendFL seamlessly blends horizontal and vertical FL principles, allows clients to benefit from either or both approaches based on their available data, features decentralized inference, and uses BlendAvg adaptive aggregation strategy.

Result: Superior performance for multimodal and unimodal classification on large-scale medical dataset and benchmark, with faster convergence and accelerated collaborative learning compared to state-of-the-art baselines.

Conclusion: BlendFL shows strong potential for handling multimodal data heterogeneity in privacy-sensitive real-world applications like healthcare and finance.

Abstract: One of the key challenges of collaborative machine learning, without data
sharing, is multimodal data heterogeneity in real-world settings. While
Federated Learning (FL) enables model training across multiple clients,
existing frameworks, such as horizontal and vertical FL, are only effective in
`ideal' settings that meet specific assumptions. Hence, they struggle to
address scenarios where neither all modalities nor all samples are represented
across the participating clients. To address this gap, we propose BlendFL, a
novel FL framework that seamlessly blends the principles of horizontal and
vertical FL in a synchronized and non-restrictive fashion despite the asymmetry
across clients. Specifically, any client within BlendFL can benefit from either
of the approaches, or both simultaneously, according to its available dataset.
In addition, BlendFL features a decentralized inference mechanism, empowering
clients to run collaboratively trained local models using available local data,
thereby reducing latency and reliance on central servers for inference. We also
introduce BlendAvg, an adaptive global model aggregation strategy that
prioritizes collaborative model updates based on each client's performance. We
trained and evaluated BlendFL and other state-of-the-art baselines on three
classification tasks using a large-scale real-world multimodal medical dataset
and a popular multimodal benchmark. Our results highlight BlendFL's superior
performance for both multimodal and unimodal classification. Ablation studies
demonstrate BlendFL's faster convergence compared to traditional approaches,
accelerating collaborative learning. Overall, in our study we highlight the
potential of BlendFL for handling multimodal data heterogeneity for
collaborative learning in real-world settings where data privacy is crucial,
such as in healthcare and finance.

</details>


### [138] [To Steer or Not to Steer? Mechanistic Error Reduction with Abstention for Language Models](https://arxiv.org/abs/2510.13290)
*Anna Hedström,Salim I. Amoukou,Tom Bewley,Saumitra Mishra,Manuela Veloso*

Main category: cs.LG

TL;DR: MERA is a framework that improves language model error correction by optimizing intervention direction and calibrating steering strength, allowing for safe and effective error mitigation or abstention when confident correction isn't possible.


<details>
  <summary>Details</summary>
Motivation: Existing steering methods use fixed, manually tuned strengths that often lead to under or oversteering, resulting in suboptimal error correction performance.

Method: MERA optimizes intervention direction and calibrates when and how much to steer, enabling provable performance improvement or abstention when confident correction isn't possible.

Result: Experiments across diverse datasets and LM families show MERA provides safe, effective, non-degrading error correction and outperforms existing baselines. It can also enhance existing steering techniques.

Conclusion: MERA establishes itself as a general-purpose, efficient approach to mechanistic activation steering that can be applied on top of existing techniques for improved performance.

Abstract: We introduce Mechanistic Error Reduction with Abstention (MERA), a principled
framework for steering language models (LMs) to mitigate errors through
selective, adaptive interventions. Unlike existing methods that rely on fixed,
manually tuned steering strengths, often resulting in under or oversteering,
MERA addresses these limitations by (i) optimising the intervention direction,
and (ii) calibrating when, and how much to steer, thereby provably improving
performance or abstaining when no confident correction is possible. Experiments
across diverse datasets, and LM families demonstrate safe, effective,
non-degrading error correction, and that MERA outperforms existing baselines.
Moreover, MERA can be applied on top of existing steering techniques to further
enhance their performance, establishing it as a general-purpose, and efficient
approach to mechanistic activation steering.

</details>


### [139] [Federated Conditional Conformal Prediction via Generative Models](https://arxiv.org/abs/2510.13297)
*Rui Xu,Sihong Xie*

Main category: cs.LG

TL;DR: Fed-CCP is a federated conformal prediction method that uses generative models to achieve conditional coverage across heterogeneous clients without sharing raw data.


<details>
  <summary>Details</summary>
Motivation: Standard conformal prediction assumes i.i.d. data, which fails in federated learning where client distributions differ. Existing methods only guarantee marginal coverage per client, lacking input-conditional uncertainty adaptation.

Method: Uses generative models (normalizing flows or diffusion models) to approximate conditional data distributions locally. Clients calibrate conformal scores reflecting their unique uncertainty, with federated aggregation maintaining global consistency.

Result: Experiments on real datasets show Fed-CCP achieves more adaptive prediction sets compared to existing methods.

Conclusion: Fed-CCP successfully addresses federated data heterogeneity by providing conditional coverage through generative modeling while preserving data privacy.

Abstract: Conformal Prediction (CP) provides distribution-free uncertainty
quantification by constructing prediction sets that guarantee coverage of the
true labels. This reliability makes CP valuable for high-stakes federated
learning scenarios such as multi-center healthcare. However, standard CP
assumes i.i.d. data, which is violated in federated settings where client
distributions differ substantially. Existing federated CP methods address this
by maintaining marginal coverage on each client, but such guarantees often fail
to reflect input-conditional uncertainty. In this work, we propose Federated
Conditional Conformal Prediction (Fed-CCP) via generative models, which aims
for conditional coverage that adapts to local data heterogeneity. Fed-CCP
leverages generative models, such as normalizing flows or diffusion models, to
approximate conditional data distributions without requiring the sharing of raw
data. This enables each client to locally calibrate conformal scores that
reflect its unique uncertainty, while preserving global consistency through
federated aggregation. Experiments on real datasets demonstrate that Fed-CCP
achieves more adaptive prediction sets.

</details>


### [140] [Km-scale dynamical downscaling through conformalized latent diffusion models](https://arxiv.org/abs/2510.13301)
*Alessandro Brusaferri,Andrea Ballarino*

Main category: cs.LG

TL;DR: This paper proposes using conformal prediction to improve uncertainty quantification in generative diffusion models for dynamical downscaling of meteorological data, addressing overconfident predictions and miscalibrated uncertainty estimates.


<details>
  <summary>Details</summary>
Motivation: Generative diffusion models lack finite-sample guarantees against overconfident predictions, resulting in miscalibrated grid-point-level uncertainty estimates that hinder reliability in operational weather forecasting and renewable energy applications.

Method: Augmenting the downscaling pipeline with conformal prediction framework - post-processing diffusion model samples to derive conditional quantile estimates, then using conformalized quantile regression to create locally adaptive prediction intervals with finite-sample marginal validity.

Result: Evaluation on ERA5 reanalysis data over Italy downscaled to 2-km grid shows markedly improved coverage and stable probabilistic scores compared to diffusion model baseline, with grid-point-level uncertainty estimates demonstrating better calibration.

Conclusion: Conformalized generative models show potential for more trustworthy probabilistic downscaling to high-resolution meteorological fields by providing reliable uncertainty quantification.

Abstract: Dynamical downscaling is crucial for deriving high-resolution meteorological
fields from coarse-scale simulations, enabling detailed analysis for critical
applications such as weather forecasting and renewable energy modeling.
Generative Diffusion models (DMs) have recently emerged as powerful data-driven
tools for this task, offering reconstruction fidelity and more scalable
sampling supporting uncertainty quantification. However, DMs lack finite-sample
guarantees against overconfident predictions, resulting in miscalibrated
grid-point-level uncertainty estimates hindering their reliability in
operational contexts. In this work, we tackle this issue by augmenting the
downscaling pipeline with a conformal prediction framework. Specifically, the
DM's samples are post-processed to derive conditional quantile estimates,
incorporated into a conformalized quantile regression procedure targeting
locally adaptive prediction intervals with finite-sample marginal validity. The
proposed approach is evaluated on ERA5 reanalysis data over Italy, downscaled
to a 2-km grid. Results demonstrate grid-point-level uncertainty estimates with
markedly improved coverage and stable probabilistic scores relative to the DM
baseline, highlighting the potential of conformalized generative models for
more trustworthy probabilistic downscaling to high-resolution meteorological
fields.

</details>


### [141] [Isolation-based Spherical Ensemble Representations for Anomaly Detection](https://arxiv.org/abs/2510.13311)
*Yang Cao,Sikun Yang,Hao Tian,Kai He,Lianyong Qi,Ming Liu,Yujiu Yang*

Main category: cs.LG

TL;DR: ISER (Isolation-based Spherical Ensemble Representations) is a novel unsupervised anomaly detection method that uses hypersphere radii to encode local density information and maintains linear time/constant space complexity, outperforming 11 baseline methods on 22 datasets.


<details>
  <summary>Details</summary>
Motivation: Address fundamental challenges in unsupervised anomaly detection including conflicting distributional assumptions, computational inefficiency, and difficulty handling different anomaly types.

Method: Extends isolation-based methods using hypersphere radii as proxies for local density, constructs ensemble representations, and introduces similarity-based scoring that compares against theoretical anomaly reference patterns.

Result: Comprehensive experiments on 22 real-world datasets demonstrate ISER's superior performance over 11 baseline methods.

Conclusion: ISER effectively addresses key limitations in anomaly detection while maintaining computational efficiency and improving detection performance across diverse anomaly types.

Abstract: Anomaly detection is a critical task in data mining and management with
applications spanning fraud detection, network security, and log monitoring.
Despite extensive research, existing unsupervised anomaly detection methods
still face fundamental challenges including conflicting distributional
assumptions, computational inefficiency, and difficulty handling different
anomaly types. To address these problems, we propose ISER (Isolation-based
Spherical Ensemble Representations) that extends existing isolation-based
methods by using hypersphere radii as proxies for local density characteristics
while maintaining linear time and constant space complexity. ISER constructs
ensemble representations where hypersphere radii encode density information:
smaller radii indicate dense regions while larger radii correspond to sparse
areas. We introduce a novel similarity-based scoring method that measures
pattern consistency by comparing ensemble representations against a theoretical
anomaly reference pattern. Additionally, we enhance the performance of
Isolation Forest by using ISER and adapting the scoring function to address
axis-parallel bias and local anomaly detection limitations. Comprehensive
experiments on 22 real-world datasets demonstrate ISER's superior performance
over 11 baseline methods.

</details>


### [142] [RockNet: Distributed Learning on Ultra-Low-Power Devices](https://arxiv.org/abs/2510.13320)
*Alexander Gräfe,Fabian Mager,Marco Zimmerling,Sebastian Trimpe*

Main category: cs.LG

TL;DR: RockNet is a distributed TinyML method for ultra-low-power microcontrollers that achieves state-of-the-art accuracy in timeseries classification without offline pretraining, reducing memory, latency and energy consumption by up to 90% when scaling to 20 devices.


<details>
  <summary>Details</summary>
Motivation: As ML becomes integral to Cyber-Physical Systems, there is growing need to shift training from cloud to on-device processing due to privacy and latency concerns, but ultra-low-power microcontrollers have limited compute resources that make training challenging.

Method: Leverages that CPS consist of multiple devices to design distributed learning method integrating ML and wireless communication. Uses distributed training of specialized compute efficient classifiers with minimal communication overhead, combined with tailored wireless multi-hop communication protocols.

Result: Hardware experiments on 20 ultra-low-power devices show RockNet learns timeseries classification from scratch, surpassing latest neural network microcontroller training accuracy by up to 2x. Reduces memory, latency and energy consumption per device by up to 90% when scaling from 1 to 20 devices.

Conclusion: Tight integration of distributed ML, distributed computing, and communication enables training on ultra-low-power hardware with state-of-the-art accuracy for the first time.

Abstract: As Machine Learning (ML) becomes integral to Cyber-Physical Systems (CPS),
there is growing interest in shifting training from traditional cloud-based to
on-device processing (TinyML), for example, due to privacy and latency
concerns. However, CPS often comprise ultra-low-power microcontrollers, whose
limited compute resources make training challenging. This paper presents
RockNet, a new TinyML method tailored for ultra-low-power hardware that
achieves state-of-the-art accuracy in timeseries classification, such as fault
or malware detection, without requiring offline pretraining. By leveraging that
CPS consist of multiple devices, we design a distributed learning method that
integrates ML and wireless communication. RockNet leverages all devices for
distributed training of specialized compute efficient classifiers that need
minimal communication overhead for parallelization. Combined with tailored and
efficient wireless multi-hop communication protocols, our approach overcomes
the communication bottleneck that often occurs in distributed learning.
Hardware experiments on a testbed with 20 ultra-low-power devices demonstrate
RockNet's effectiveness. It successfully learns timeseries classification tasks
from scratch, surpassing the accuracy of the latest approach for neural network
microcontroller training by up to 2x. RockNet's distributed ML architecture
reduces memory, latency and energy consumption per device by up to 90 % when
scaling from one central device to 20 devices. Our results show that a tight
integration of distributed ML, distributed computing, and communication
enables, for the first time, training on ultra-low-power hardware with
state-of-the-art accuracy.

</details>


### [143] [When In Doubt, Abstain: The Impact of Abstention on Strategic Classification](https://arxiv.org/abs/2510.13327)
*Lina Alkarmi,Ziyuan Huang,Mingyan Liu*

Main category: cs.LG

TL;DR: This paper shows that classifier abstention (declining decisions when confidence is low) improves accuracy and deters strategic manipulation in algorithmic decision making, with optimal abstention ensuring the principal's utility is no worse than in non-abstention settings.


<details>
  <summary>Details</summary>
Motivation: Algorithmic decision making is vulnerable to strategic manipulation by agents seeking favorable outcomes. Prior research showed abstention increases classifier accuracy, but its impact in strategic classification contexts needs exploration.

Method: Model the interaction as a Stackelberg game where a principal (classifier) announces its decision policy first, then strategic agents manipulate their observable features to receive desired outcomes. Focus on binary classifiers with feature manipulation.

Result: Optimal abstention ensures the principal's utility is no worse than in non-abstention settings, even with strategic agents. Abstention also deters manipulation by making it costlier for less qualified agents to achieve positive outcomes when manipulation costs affect behavior.

Conclusion: Abstention is a valuable tool for reducing negative effects of strategic behavior in algorithmic decision making systems, improving accuracy and serving as a manipulation deterrent.

Abstract: Algorithmic decision making is increasingly prevalent, but often vulnerable
to strategic manipulation by agents seeking a favorable outcome. Prior research
has shown that classifier abstention (allowing a classifier to decline making a
decision due to insufficient confidence) can significantly increase classifier
accuracy. This paper studies abstention within a strategic classification
context, exploring how its introduction impacts strategic agents' responses and
how principals should optimally leverage it. We model this interaction as a
Stackelberg game where a principal, acting as the classifier, first announces
its decision policy, and then strategic agents, acting as followers, manipulate
their features to receive a desired outcome. Here, we focus on binary
classifiers where agents manipulate observable features rather than their true
features, and show that optimal abstention ensures that the principal's utility
(or loss) is no worse than in a non-abstention setting, even in the presence of
strategic agents. We also show that beyond improving accuracy, abstention can
also serve as a deterrent to manipulation, making it costlier for agents,
especially those less qualified, to manipulate to achieve a positive outcome
when manipulation costs are significant enough to affect agent behavior. These
results highlight abstention as a valuable tool for reducing the negative
effects of strategic behavior in algorithmic decision making systems.

</details>


### [144] [Thompson Sampling via Fine-Tuning of LLMs](https://arxiv.org/abs/2510.13328)
*Nicolas Menet,Aleksandar Terzić,Andreas Krause,Abbas Rahimi*

Main category: cs.LG

TL;DR: ToSFiT is a scalable Bayesian optimization method that uses Thompson sampling via fine-tuning of large language models to avoid expensive acquisition function maximization in large discrete spaces.


<details>
  <summary>Details</summary>
Motivation: Bayesian optimization in large unstructured discrete spaces is computationally expensive due to the need for acquisition function maximization without gradients.

Method: Thompson Sampling via Fine-Tuning (ToSFiT) parameterizes the probability that a candidate yields maximum reward, leveraging prompt-conditioned LLMs and incrementally adapting them toward the posterior.

Result: The method achieves strong theoretical regret bounds matching standard Thompson sampling and shows significant sample efficiency improvements in FAQ response refinement, protein search, and quantum circuit design tasks.

Conclusion: Online fine-tuning with ToSFiT improves sample efficiency with negligible computational cost impact, providing a scalable alternative for Bayesian optimization in discrete spaces.

Abstract: Bayesian optimization in large unstructured discrete spaces is often hindered
by the computational cost of maximizing acquisition functions due to the
absence of gradients. We propose a scalable alternative based on Thompson
sampling that eliminates the need for acquisition function maximization by
directly parameterizing the probability that a candidate yields the maximum
reward. Our approach, Thompson Sampling via Fine-Tuning (ToSFiT) leverages the
prior knowledge embedded in prompt-conditioned large language models, and
incrementally adapts them toward the posterior. Theoretically, we derive a
novel regret bound for a variational formulation of Thompson Sampling that
matches the strong guarantees of its standard counterpart. Our analysis reveals
the critical role of careful adaptation to the posterior probability of
maximality--a principle that underpins our ToSFiT algorithm. Empirically, we
validate our method on three diverse tasks: FAQ response refinement, thermally
stable protein search, and quantum circuit design. We demonstrate that online
fine-tuning significantly improves sample efficiency, with negligible impact on
computational efficiency.

</details>


### [145] [Kernel Representation and Similarity Measure for Incomplete Data](https://arxiv.org/abs/2510.13352)
*Yang Cao,Sikun Yang,Kai He,Wenjun Ma,Ming Liu,Yujiu Yang,Jian Weng*

Main category: cs.LG

TL;DR: The paper proposes a proximity kernel method for measuring similarity between incomplete data without explicit imputation, using data-dependent binning and proximity assignment in kernel space.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches discard incomplete data or use imputation, leading to information loss and biased similarity estimates in web mining, recommendation systems, and user behavior analysis.

Method: Introduces proximity kernel with data-dependent binning and proximity assignment to project data into high-dimensional sparse representation, plus cascading fallback strategy for missing value handling.

Result: Superior performance in clustering tasks across 12 real-world incomplete datasets compared to existing methods, while maintaining linear time complexity.

Conclusion: The proximity kernel provides an effective similarity measure for incomplete data without explicit imputation, outperforming traditional approaches.

Abstract: Measuring similarity between incomplete data is a fundamental challenge in
web mining, recommendation systems, and user behavior analysis. Traditional
approaches either discard incomplete data or perform imputation as a
preprocessing step, leading to information loss and biased similarity
estimates. This paper presents the proximity kernel, a new similarity measure
that directly computes similarity between incomplete data in kernel feature
space without explicit imputation in the original space. The proposed method
introduces data-dependent binning combined with proximity assignment to project
data into a high-dimensional sparse representation that adapts to local density
variations. For missing value handling, we propose a cascading fallback
strategy to estimate missing feature distributions. We conduct clustering tasks
on the proposed kernel representation across 12 real world incomplete datasets,
demonstrating superior performance compared to existing methods while
maintaining linear time complexity. All the code are available at
https://anonymous.4open.science/r/proximity-kernel-2289.

</details>


### [146] [Generalist++: A Meta-learning Framework for Mitigating Trade-off in Adversarial Training](https://arxiv.org/abs/2510.13361)
*Yisen Wang,Yichuan Mo,Hongjun Wang,Junyi Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: Generalist framework partitions generalization goals into sub-tasks assigned to specialized base learners, then interpolates their parameters to form a global learner, addressing adversarial training limitations of natural accuracy degradation and poor cross-attack robustness transfer.


<details>
  <summary>Details</summary>
Motivation: Address two major limitations of adversarial training: significant degradation of natural accuracy compared to standard training, and poor robustness transfer across attacks with different norm constraints.

Method: Partition generalization goals into sub-tasks assigned to dedicated base learners, then interpolate their parameters to form a global learner while periodically redistributing global parameters back to base learners to prevent optimization drift.

Result: Achieves lower generalization error and significantly alleviates trade-off problems compared to baseline methods, demonstrating improved performance in adversarial robustness.

Conclusion: Generalist provides a promising step toward developing fully robust classifiers by effectively addressing the limitations of current adversarial training approaches.

Abstract: Despite the rapid progress of neural networks, they remain highly vulnerable
to adversarial examples, for which adversarial training (AT) is currently the
most effective defense. While AT has been extensively studied, its practical
applications expose two major limitations: natural accuracy tends to degrade
significantly compared with standard training, and robustness does not transfer
well across attacks crafted under different norm constraints. Unlike prior
works that attempt to address only one issue within a single network, we
propose to partition the overall generalization goal into multiple sub-tasks,
each assigned to a dedicated base learner. By specializing in its designated
objective, each base learner quickly becomes an expert in its field. In the
later stages of training, we interpolate their parameters to form a
knowledgeable global learner, while periodically redistributing the global
parameters back to the base learners to prevent their optimization trajectories
from drifting too far from the shared target. We term this framework Generalist
and introduce three variants tailored to different application scenarios. Both
theoretical analysis and extensive experiments demonstrate that Generalist
achieves lower generalization error and significantly alleviates the trade-off
problems compared with baseline methods. Our results suggest that Generalist
provides a promising step toward developing fully robust classifiers in the
future.

</details>


### [147] [A New Perspective on Transformers in Online Reinforcement Learning for Continuous Control](https://arxiv.org/abs/2510.13367)
*Nikita Kachaev,Daniil Zelezetsky,Egor Cherepanov,Alexey K. Kovelev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: Transformers can be effective baselines for continuous control in online model-free RL when proper architectural and training strategies are implemented.


<details>
  <summary>Details</summary>
Motivation: Transformers are popular in offline/model-based RL but underexplored in online model-free RL due to sensitivity to training setups and design decisions.

Method: Investigated key design questions: input conditioning, component sharing between actor and critic, and sequential data slicing for training.

Result: Found stable architectural and training strategies that enable competitive performance across fully/partially observable tasks in vector- and image-based settings.

Conclusion: Provides practical guidance for applying transformers in online RL by identifying stable design and training approaches.

Abstract: Despite their effectiveness and popularity in offline or model-based
reinforcement learning (RL), transformers remain underexplored in online
model-free RL due to their sensitivity to training setups and model design
decisions such as how to structure the policy and value networks, share
components, or handle temporal information. In this paper, we show that
transformers can be strong baselines for continuous control in online
model-free RL. We investigate key design questions: how to condition inputs,
share components between actor and critic, and slice sequential data for
training. Our experiments reveal stable architectural and training strategies
enabling competitive performance across fully and partially observable tasks,
and in both vector- and image-based settings. These findings offer practical
guidance for applying transformers in online RL.

</details>


### [148] [Contrastive Learning-Based Dependency Modeling for Anomaly Detection in Cloud Services](https://arxiv.org/abs/2510.13368)
*Yue Xing,Yingnan Deng,Heyao Liu,Ming Wang,Yun Zi,Xiaoxuan Sun*

Main category: cs.LG

TL;DR: Proposes a dependency modeling and anomaly detection method using contrastive learning for cloud services, achieving superior performance on key metrics while maintaining robustness.


<details>
  <summary>Details</summary>
Motivation: Address challenges of complex dependencies and diverse anomaly patterns in cloud service environments.

Method: Abstracts service interactions into dependency graph, extracts temporal/structural features via embedding functions, uses graph convolution for context-aware representations, and employs contrastive learning with temporal consistency constraints.

Result: Significantly outperforms existing methods on Precision, Recall, F1-Score, and AUC metrics, with robustness under sparse labeling, monitoring noise, and traffic fluctuations.

Conclusion: Verifies effectiveness of integrating dependency modeling with contrastive learning, provides complete technical solution for cloud service anomaly detection with strong adaptability and stability.

Abstract: This paper addresses the challenges of complex dependencies and diverse
anomaly patterns in cloud service environments by proposing a dependency
modeling and anomaly detection method that integrates contrastive learning. The
method abstracts service interactions into a dependency graph, extracts
temporal and structural features through embedding functions, and employs a
graph convolution mechanism to aggregate neighborhood information for
context-aware service representations. A contrastive learning framework is then
introduced, constructing positive and negative sample pairs to enhance the
separability of normal and abnormal patterns in the representation space.
Furthermore, a temporal consistency constraint is designed to maintain
representation stability across time steps and reduce the impact of short-term
fluctuations and noise. The overall optimization combines contrastive loss and
temporal consistency loss to ensure stable and reliable detection across
multi-dimensional features. Experiments on public datasets systematically
evaluate the method from hyperparameter, environmental, and data sensitivity
perspectives. Results show that the proposed approach significantly outperforms
existing methods on key metrics such as Precision, Recall, F1-Score, and AUC,
while maintaining robustness under conditions of sparse labeling, monitoring
noise, and traffic fluctuations. This study verifies the effectiveness of
integrating dependency modeling with contrastive learning, provides a complete
technical solution for cloud service anomaly detection, and demonstrates strong
adaptability and stability in complex environments.

</details>


### [149] [Prediction Markets with Intermittent Contributions](https://arxiv.org/abs/2510.13385)
*Michael Vitali,Pierre Pinson*

Main category: cs.LG

TL;DR: A prediction market framework that combines forecasts from multiple agents while handling data ownership constraints, adapting to time-varying conditions, and allowing flexible agent participation.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of collaborative forecasting when stakeholders have data ownership and competitive interests that limit cooperation, going beyond traditional cooperative game-theoretic approaches.

Method: Uses robust regression models to learn optimal forecast combinations while handling missing submissions, with a payoff allocation mechanism considering both in-sample and out-of-sample performance.

Result: The proposed market design demonstrates effectiveness and adaptability in case studies using both simulated and real-world data.

Conclusion: The prediction market framework successfully enables collaborative forecasting while respecting data ownership constraints and maintaining desirable economic properties.

Abstract: Although both data availability and the demand for accurate forecasts are
increasing, collaboration between stakeholders is often constrained by data
ownership and competitive interests. In contrast to recent proposals within
cooperative game-theoretical frameworks, we place ourselves in a more general
framework, based on prediction markets. There, independent agents trade
forecasts of uncertain future events in exchange for rewards. We introduce and
analyse a prediction market that (i) accounts for the historical performance of
the agents, (ii) adapts to time-varying conditions, while (iii) permitting
agents to enter and exit the market at will. The proposed design employs robust
regression models to learn the optimal forecasts' combination whilst handling
missing submissions. Moreover, we introduce a pay-off allocation mechanism that
considers both in-sample and out-of-sample performance while satisfying several
desirable economic properties. Case-studies using simulated and real-world data
allow demonstrating the effectiveness and adaptability of the proposed market
design.

</details>


### [150] [Going with the Flow: Approximating Banzhaf Values via Graph Neural Networks](https://arxiv.org/abs/2510.13391)
*Benjamin Kempinski,Tal Kachman*

Main category: cs.LG

TL;DR: GNN-based approach for approximating Banzhaf values in network flow games, achieving high-fidelity results with order-of-magnitude speedups and strong zero-shot generalization across different network configurations.


<details>
  <summary>Details</summary>
Motivation: Exact computation of Banzhaf values is intractable for large systems (>20 agents) due to exponential complexity, and Monte Carlo methods suffer from high sample complexity and lack transferability across network configurations.

Method: Use Graph Neural Networks (GNNs) - specifically GAT, GINE, and EdgeConv architectures - to learn generalizable patterns of agent influence from network topology and control structure, treating it as a graph-level prediction task.

Result: Trained GNN models achieve high-fidelity Banzhaf value approximation with significant speedups compared to exact and sampling methods, and demonstrate strong zero-shot generalization to new networks with different structural properties without retraining.

Conclusion: GNNs establish as a practical tool for scalable cooperative game-theoretic analysis of complex networked systems, enabling efficient Banzhaf value computation for large-scale and dynamic systems.

Abstract: Computing the Banzhaf value in network flow games is fundamental for
quantifying agent influence in multi-agent systems, with applications ranging
from cybersecurity to infrastructure planning. However, exact computation is
intractable for systems with more than $\sim20$ agents due to exponential
complexity $\mathcal{O}(2^m)$. While Monte Carlo sampling methods provide
statistical estimates, they suffer from high sample complexity and cannot
transfer knowledge across different network configurations, making them
impractical for large-scale or dynamic systems. We present a novel
learning-based approach using Graph Neural Networks (GNNs) to approximate
Banzhaf values in cardinal network flow games. By framing the problem as a
graph-level prediction task, our method learns generalisable patterns of agent
influence directly from network topology and control structure. We conduct a
comprehensive empirical study comparing three state-of-the-art GNN
architectures-Graph Attention Networks (GAT), Graph Isomorphism Networks with
Edge features (GINE), and EdgeConv-on a large-scale synthetic dataset of
200,000 graphs per configuration, varying in size (20-100 nodes), agent count
(5-20), and edge probability (0.5-1.0). Our results demonstrate that trained
GNN models achieve high-fidelity Banzhaf value approximation with
order-of-magnitude speedups compared to exact and sampling-based methods. Most
significantly, we show strong zero-shot generalisation: models trained on
graphs of a specific size and topology accurately predict Banzhaf values for
entirely new networks with different structural properties, without requiring
retraining. This work establishes GNNs as a practical tool for scalable
cooperative game-theoretic analysis of complex networked systems.

</details>


### [151] [Rectify and Align GPS Points to Parking Spots via Rank-1 Constraint](https://arxiv.org/abs/2510.13439)
*Jiaxing Deng,Junbiao Pang,Zhicheng Wang,Haitao Yu*

Main category: cs.LG

TL;DR: An unsupervised low-rank method for correcting GPS point errors of parking spots by leveraging physical constraints that parking spots are parallel to road sides.


<details>
  <summary>Details</summary>
Motivation: GPS points of parking spots often drift due to high-rise buildings and equipment errors, making accurate location data essential for parking management and urban development applications.

Method: Proposes an unsupervised low-rank method that uses physical constraints (parking spots parallel to road sides) to rectify GPS errors and align points to actual parking spots in a unified framework.

Result: The method effectively corrects GPS point errors and demonstrates superiority in solving this practical problem through extensive experiments.

Conclusion: The proposed unconventional rectification and alignment method is simple yet effective for handling various types of GPS point errors in parking spot localization.

Abstract: Parking spots are essential components, providing vital mobile resources for
residents in a city. Accurate Global Positioning System (GPS) points of parking
spots are the core data for subsequent applications,e.g., parking management,
parking policy, and urban development. However, high-rise buildings tend to
cause GPS points to drift from the actual locations of parking spots; besides,
the standard lower-cost GPS equipment itself has a certain location error.
Therefore, it is a non-trivial task to correct a few wrong GPS points from a
large number of parking spots in an unsupervised approach. In this paper,
motivated by the physical constraints of parking spots (i.e., parking spots are
parallel to the sides of roads), we propose an unsupervised low-rank method to
effectively rectify errors in GPS points and further align them to the parking
spots in a unified framework. The proposed unconventional rectification and
alignment method is simple and yet effective for any type of GPS point errors.
Extensive experiments demonstrate the superiority of the proposed method to
solve a practical problem. The data set and the code are publicly accessible
at:https://github.com/pangjunbiao/ITS-Parking-spots-Dataset.

</details>


### [152] [Assessing the robustness of heterogeneous treatment effects in survival analysis under informative censoring](https://arxiv.org/abs/2510.13397)
*Yuxin Wang,Dennis Frauen,Jonas Schweisthal,Maresa Schröder,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: Proposes an assumption-lean framework using partial identification to derive bounds on conditional average treatment effects (CATE) in survival analysis with informative censoring, developing a novel meta-learner with double robustness properties.


<details>
  <summary>Details</summary>
Motivation: Addresses bias in treatment effect estimates caused by informative dropout/censoring in clinical studies, where up to half of patients may leave early due to side effects or other reasons.

Method: Uses partial identification to derive informative bounds on CATE instead of point estimates, and develops a novel meta-learner that can use arbitrary machine learning models with double robustness and quasi-oracle efficiency properties.

Result: The framework helps identify patient subgroups where treatment remains effective despite informative censoring, demonstrated through numerical experiments and application to a cancer drug trial.

Conclusion: Provides a practical tool for assessing robustness of treatment effects in survival data with censoring, promoting reliable use of survival data for evidence generation in medicine and epidemiology.

Abstract: Dropout is common in clinical studies, with up to half of patients leaving
early due to side effects or other reasons. When dropout is informative (i.e.,
dependent on survival time), it introduces censoring bias, because of which
treatment effect estimates are also biased. In this paper, we propose an
assumption-lean framework to assess the robustness of conditional average
treatment effect (CATE) estimates in survival analysis when facing censoring
bias. Unlike existing works that rely on strong assumptions, such as
non-informative censoring, to obtain point estimation, we use partial
identification to derive informative bounds on the CATE. Thereby, our framework
helps to identify patient subgroups where treatment is effective despite
informative censoring. We further develop a novel meta-learner that estimates
the bounds using arbitrary machine learning models and with favorable
theoretical properties, including double robustness and quasi-oracle
efficiency. We demonstrate the practical value of our meta-learner through
numerical experiments and in an application to a cancer drug trial. Together,
our framework offers a practical tool for assessing the robustness of estimated
treatment effects in the presence of censoring and thus promotes the reliable
use of survival data for evidence generation in medicine and epidemiology.

</details>


### [153] [Neural Sum-of-Squares: Certifying the Nonnegativity of Polynomials with Transformers](https://arxiv.org/abs/2510.13444)
*Nico Pelleriti,Christoph Spiegel,Shiwei Liu,David Martínez-Rubio,Max Zimmer,Sebastian Pokutta*

Main category: cs.LG

TL;DR: A learning-augmented algorithm using Transformers to predict minimal monomial bases for Sum of Squares (SOS) certification, achieving 100x speedups over state-of-the-art solvers.


<details>
  <summary>Details</summary>
Motivation: Certifying polynomial nonnegativity via SOS is NP-hard and computationally expensive due to large SDPs with quadratic growth in monomial basis size. Existing methods struggle with scalability.

Method: Train a Transformer model on 100+ million SOS polynomials to predict nearly-minimal monomial bases, reducing SDP size. Includes fallback mechanism for correctness.

Result: Achieved over 100x speedups on 200+ benchmark datasets, solving instances where competing approaches fail.

Conclusion: Learning-augmented SOS certification transforms practical scalability, enabling efficient solution of previously intractable problems.

Abstract: Certifying nonnegativity of polynomials is a well-known NP-hard problem with
direct applications spanning non-convex optimization, control, robotics, and
beyond. A sufficient condition for nonnegativity is the Sum of Squares (SOS)
property, i.e., it can be written as a sum of squares of other polynomials. In
practice, however, certifying the SOS criterion remains computationally
expensive and often involves solving a Semidefinite Program (SDP), whose
dimensionality grows quadratically in the size of the monomial basis of the SOS
expression; hence, various methods to reduce the size of the monomial basis
have been proposed. In this work, we introduce the first learning-augmented
algorithm to certify the SOS criterion. To this end, we train a Transformer
model that predicts an almost-minimal monomial basis for a given polynomial,
thereby drastically reducing the size of the corresponding SDP. Our overall
methodology comprises three key components: efficient training dataset
generation of over 100 million SOS polynomials, design and training of the
corresponding Transformer architecture, and a systematic fallback mechanism to
ensure correct termination, which we analyze theoretically. We validate our
approach on over 200 benchmark datasets, achieving speedups of over $100\times$
compared to state-of-the-art solvers and enabling the solution of instances
where competing approaches fail. Our findings provide novel insights towards
transforming the practical scalability of SOS programming.

</details>


### [154] [SWIR-LightFusion: Multi-spectral Semantic Fusion of Synthetic SWIR with {Thermal} IR {(LWIR/MWIR)} and RGB](https://arxiv.org/abs/2510.13404)
*Muhammad Ishfaq Hussain,Ma Van Linh,Zubia Naz,Unse Fatima,Yeongmin Ko,Moongu Jeon*

Main category: cs.LG

TL;DR: The paper introduces a method to synthetically generate SWIR-like images from LWIR data and proposes a multimodal fusion framework combining synthetic SWIR, LWIR, and RGB modalities using an encoder-decoder network with modality-specific encoders and softmax-gated fusion.


<details>
  <summary>Details</summary>
Motivation: Address limitations of conventional RGB and thermal infrared fusion in adverse visibility conditions, overcome scarcity of SWIR datasets, and leverage SWIR's advantages in atmospheric penetration and material differentiation.

Method: Synthetic SWIR generation from LWIR data using contrast enhancement techniques, followed by multimodal fusion framework with encoder-decoder architecture, modality-specific encoders, and softmax-gated fusion head.

Result: Improved fused-image quality (contrast, edge definition, structural fidelity) while maintaining real-time performance across multiple benchmarks, outperforming existing trimodal baselines.

Conclusion: The synthetic-SWIR-enhanced fusion framework shows substantial potential for real-world surveillance and autonomous systems applications, effectively addressing adverse visibility challenges.

Abstract: Enhancing scene understanding in adverse visibility conditions remains a
critical challenge for surveillance and autonomous navigation systems.
Conventional imaging modalities, such as RGB and thermal infrared (MWIR /
LWIR), when fused, often struggle to deliver comprehensive scene information,
particularly under conditions of atmospheric interference or inadequate
illumination. To address these limitations, Short-Wave Infrared (SWIR) imaging
has emerged as a promising modality due to its ability to penetrate atmospheric
disturbances and differentiate materials with improved clarity. However, the
advancement and widespread implementation of SWIR-based systems face
significant hurdles, primarily due to the scarcity of publicly accessible SWIR
datasets. In response to this challenge, our research introduces an approach to
synthetically generate SWIR-like structural/contrast cues (without claiming
spectral reproduction) images from existing LWIR data using advanced contrast
enhancement techniques. We then propose a multimodal fusion framework
integrating synthetic SWIR, LWIR, and RGB modalities, employing an optimized
encoder-decoder neural network architecture with modality-specific encoders and
a softmax-gated fusion head. Comprehensive experiments on public {RGB-LWIR
benchmarks (M3FD, TNO, CAMEL, MSRS, RoadScene) and an additional private real
RGB-MWIR-SWIR dataset} demonstrate that our synthetic-SWIR-enhanced fusion
framework improves fused-image quality (contrast, edge definition, structural
fidelity) while maintaining real-time performance. We also add fair trimodal
baselines (LP, LatLRR, GFF) and cascaded trimodal variants of
U2Fusion/SwinFusion under a unified protocol. The outcomes highlight
substantial potential for real-world applications in surveillance and
autonomous systems.

</details>


### [155] [DistilCLIP-EEG: Enhancing Epileptic Seizure Detection Through Multi-modal Learning and Knowledge Distillation](https://arxiv.org/abs/2510.13497)
*Zexin Wang,Lin Shi,Haoyu Wu,Junru Luo,Xiangzeng Kong,Jun Qi*

Main category: cs.LG

TL;DR: Proposes DistilCLIP-EEG, a multimodal model integrating EEG signals and text descriptions for epilepsy detection using CLIP framework and knowledge distillation to create efficient student models.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for epilepsy detection rely only on unimodal EEG signals, missing the benefits of multimodal information integration.

Method: Uses CLIP framework with EEG encoder based on Conformer architecture and text encoder with Learnable BERT (BERT-LP) for prompt learning, operating in shared latent space. Employs knowledge distillation where teacher model guides compact student model.

Result: Achieved accuracy >97% and F1-scores >0.94 across TUSZ, AUBMC, and CHB-MIT datasets. Student model has 58.1% parameters of teacher model while maintaining high performance.

Conclusion: The model shows strong potential for EEG-based epilepsy detection and provides foundation for deploying lightweight models in resource-constrained environments.

Abstract: Epilepsy is a prevalent neurological disorder marked by sudden, brief
episodes of excessive neuronal activity caused by abnormal electrical
discharges, which may lead to some mental disorders. Most existing deep
learning methods for epilepsy detection rely solely on unimodal EEG signals,
neglecting the potential benefits of multimodal information. To address this,
we propose a novel multimodal model, DistilCLIP-EEG, based on the CLIP
framework, which integrates both EEG signals and text descriptions to capture
comprehensive features of epileptic seizures. The model involves an EEG encoder
based on the Conformer architecture as a text encoder, the proposed Learnable
BERT (BERT-LP) as prompt learning within the encoders. Both operate in a shared
latent space for effective cross-modal representation learning. To enhance
efficiency and adaptability, we introduce a knowledge distillation method where
the trained DistilCLIP-EEG serves as a teacher to guide a more compact student
model to reduce training complexity and time. On the TUSZ, AUBMC, and CHB-MIT
datasets, both the teacher and student models achieved accuracy rates exceeding
97%. Across all datasets, the F1-scores were consistently above 0.94,
demonstrating the robustness and reliability of the proposed framework.
Moreover, the student model's parameter count and model size are approximately
58.1% of those of the teacher model, significantly reducing model complexity
and storage requirements while maintaining high performance. These results
highlight the potential of our proposed model for EEG-based epilepsy detection
and establish a solid foundation for deploying lightweight models in
resource-constrained settings.

</details>


### [156] [Optimizing Storage Overhead of User Behavior Log for ML-embedded Mobile Apps](https://arxiv.org/abs/2510.13405)
*Chen Gong,Yan Zhuang,Zhenzhe Zheng,Yiliu Chen,Sheng Wang,Fan Wu,Guihai Chen*

Main category: cs.LG

TL;DR: AdaLog is a lightweight system that reduces storage costs for user behavior logs in ML-embedded mobile apps by eliminating redundant data and optimizing storage layout, achieving 19-44% size reduction with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: ML models in mobile apps require extensive user behavior data, but current logging practices impose substantial storage costs that degrade system performance and lead to app uninstalls.

Method: AdaLog addresses two key inefficiencies: (1) eliminates feature-level redundant data via maximum weighted matching in hypergraphs with hierarchical algorithm, (2) uses virtually hashed attributes to distribute heterogeneous behaviors into dense log files, plus incremental updates for dynamic patterns.

Result: Evaluation on real-world data shows 19-44% reduction in behavior log size with only 2 seconds latency and 15 MB memory usage overhead.

Conclusion: AdaLog provides an efficient data foundation for broader adoption of on-device ML by significantly improving storage efficiency without compromising model accuracy or latency.

Abstract: Machine learning (ML) models are increasingly integrated into modern mobile
apps to enable personalized and intelligent services. These models typically
rely on rich input features derived from historical user behaviors to capture
user intents. However, as ML-driven services become more prevalent, recording
necessary user behavior data imposes substantial storage cost on mobile apps,
leading to lower system responsiveness and more app uninstalls. To address this
storage bottleneck, we present AdaLog, a lightweight and adaptive system
designed to improve the storage efficiency of user behavior log in ML-embedded
mobile apps, without compromising model inference accuracy or latency. We
identify two key inefficiencies in current industrial practices of user
behavior log: (i) redundant logging of overlapping behavior data across
different features and models, and (ii) sparse storage caused by storing
behaviors with heterogeneous attribute descriptions in a single log file. To
solve these issues, AdaLog first formulates the elimination of feature-level
redundant data as a maximum weighted matching problem in hypergraphs, and
proposes a hierarchical algorithm for efficient on-device deployment. Then,
AdaLog employs a virtually hashed attribute design to distribute heterogeneous
behaviors into a few log files with physically dense storage. Finally, to
ensure scalability to dynamic user behavior patterns, AdaLog designs an
incremental update mechanism to minimize the I/O operations needed for adapting
outdated behavior log. We implement a prototype of AdaLog and deploy it into
popular mobile apps in collaboration with our industry partner. Evaluations on
real-world user data show that AdaLog reduces behavior log size by 19% to 44%
with minimal system overhead (only 2 seconds latency and 15 MB memory usage),
providing a more efficient data foundation for broader adoption of on-device
ML.

</details>


### [157] [Offline and Online KL-Regularized RLHF under Differential Privacy](https://arxiv.org/abs/2510.13512)
*Yulian Wu,Rushil Thareja,Praneeth Vepakomma,Francesco Orabona*

Main category: cs.LG

TL;DR: This paper studies offline and online reinforcement learning from human feedback (RLHF) with KL-regularization under local differential privacy constraints on human preference labels, providing theoretical guarantees and algorithms for both settings.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns in RLHF for large language model alignment by studying the problem under local differential privacy constraints on human preference data, which is crucial for protecting sensitive user feedback.

Method: For offline setting: designed a pessimism-based algorithm with KL-regularization. For online setting: designed an optimism-based algorithm. Both approaches operate under ε-local differential privacy on human preference labels.

Result: Offline: achieved suboptimality gap of Õ(1/[(e^ε-1)^2 n]) under single-policy concentrability with matching lower bound. Online: achieved logarithmic regret bound of O(d_ℱlog(N_ℱ·T)/(e^ε-1)^2) where T is time steps, N_ℱ is reward function space cardinality, d_ℱ is eluder dimension variant.

Conclusion: The paper provides the first theoretical analysis of KL-regularized RLHF under local differential privacy, establishing optimal bounds for offline setting and introducing the first analysis for online setting, with implications for both private and non-private RLHF scenarios.

Abstract: In this paper, we study the offline and online settings of reinforcement
learning from human feedback (RLHF) with KL-regularization -- a widely used
objective function in large language model alignment -- under the $\epsilon$
local differential privacy ($\epsilon$-LDP) model on the label of the human
preference. In the offline setting, we design an algorithm based on the
principle of pessimism and derive a new suboptimality gap of
$\tilde{O}(1/[(e^\epsilon-1)^2 n])$ on the KL-regularized objective under
single-policy concentrability. We also prove its optimality by providing a
matching lower bound where $n$ is the sample size.
  In the online setting, we are the first one to theoretically investigate the
problem of KL-regularized RLHF with LDP. We design an optimism-based algorithm
and derive a logarithmic regret bound of $O(d_{\mathcal{F}}\log
(N_{\mathcal{F}}\cdot T) /(e^\epsilon-1)^2 )$, where $T$ is the total time
step, $N_{\mathcal{F}}$ is cardinality of the reward function space
$\mathcal{F}$ and $d_{\mathcal{F}}$ is a variant of eluder dimension for RLHF.
As a by-product of our analysis, our results also imply the first analysis for
online KL-regularized RLHF without privacy. We implement our algorithm in the
offline setting to verify our theoretical results and release our open source
code at: https://github.com/rushil-thareja/PPKL-RLHF-Official.

</details>


### [158] [When Embedding Models Meet: Procrustes Bounds and Applications](https://arxiv.org/abs/2510.13406)
*Lucas Maystre,Alvaro Ortega Gonzalez,Charles Park,Rares Dolga,Tudor Berariu,Yu Zhao,Kamil Ciosek*

Main category: cs.LG

TL;DR: The paper shows that if pairwise dot products are preserved between two embedding sets, an orthogonal transformation can align them, and provides a simple Procrustes post-processing method for making embedding models interoperable.


<details>
  <summary>Details</summary>
Motivation: Embedding models trained separately on similar data produce representations that aren't directly interchangeable, creating challenges in model retraining, partial upgrades, and multimodal search.

Method: Procrustes post-processing - an orthogonal transformation that aligns two embedding sets while preserving their geometry, based on the insight that preserved pairwise dot products enable isometric alignment.

Result: The method achieves state-of-the-art performance in mixed-modality search and effectively maintains compatibility across retrainings and combines different models for text retrieval.

Conclusion: A simple orthogonal transformation can make embedding models interoperable while preserving their geometric structure, solving practical challenges in model deployment and multimodal applications.

Abstract: Embedding models trained separately on similar data often produce
representations that encode stable information but are not directly
interchangeable. This lack of interoperability raises challenges in several
practical applications, such as model retraining, partial model upgrades, and
multimodal search. Driven by these challenges, we study when two sets of
embeddings can be aligned by an orthogonal transformation. We show that if
pairwise dot products are approximately preserved, then there exists an
isometry that closely aligns the two sets, and we provide a tight bound on the
alignment error. This insight yields a simple alignment recipe, Procrustes
post-processing, that makes two embedding models interoperable while preserving
the geometry of each embedding space. Empirically, we demonstrate its
effectiveness in three applications: maintaining compatibility across
retrainings, combining different models for text retrieval, and improving
mixed-modality search, where it achieves state-of-the-art performance.

</details>


### [159] [Modeling Adoptive Cell Therapy in Bladder Cancer from Sparse Biological Data using PINNs](https://arxiv.org/abs/2510.13431)
*Kayode Olumoyin,Katarzyna Rejniak*

Main category: cs.LG

TL;DR: A modified Physics-Informed Neural Network (PINN) framework is applied to oncology to learn time-varying interactions in combination therapy using sparse tumor volume data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of sparse experimental data in oncology by embedding biological constraints into neural networks to learn tumor dynamics under combination therapy.

Method: Extended PINN framework that incorporates observed biological constraints as regularization agents to learn dynamics of intermittent combination therapy in an ODE model.

Result: The algorithm successfully learns the ODE solution and time-varying model parameters, demonstrating strong convergence with low MSE, MAE, and MAPE metrics.

Conclusion: The modified PINN approach can effectively generalize with few training examples and provides a viable method for modeling complex biological systems with sparse data.

Abstract: Physics-informed neural networks (PINNs) are neural networks that embed the
laws of dynamical systems modeled by differential equations into their loss
function as constraints. In this work, we present a PINN framework applied to
oncology. Here, we seek to learn time-varying interactions due to a combination
therapy in a tumor microenvironment. In oncology, experimental data are often
sparse and composed of a few time points of tumor volume. By embedding
inductive biases derived from prior information about a dynamical system, we
extend the physics-informed neural networks (PINN) and incorporate observed
biological constraints as regularization agents. The modified PINN algorithm is
able to steer itself to a reasonable solution and can generalize well with only
a few training examples. We demonstrate the merit of our approach by learning
the dynamics of treatment applied intermittently in an ordinary differential
equation (ODE) model of a combination therapy. The algorithm yields a solution
to the ODE and time-varying forms of some of the ODE model parameters. We
demonstrate a strong convergence using metrics such as the mean squared error
(MSE), mean absolute error (MAE), and mean absolute percentage error (MAPE).

</details>


### [160] [K-Merge: Online Continual Merging of Adapters for On-device Large Language Models](https://arxiv.org/abs/2510.13537)
*Donald Shenaj,Ondrej Bohdal,Taha Ceritli,Mete Ozay,Pietro Zanuttigh,Umberto Michieli*

Main category: cs.LG

TL;DR: A data-free and efficient method for online continual merging of LoRA adapters on mobile devices to handle incremental task additions while maintaining performance on previous tasks under storage constraints.


<details>
  <summary>Details</summary>
Motivation: On-device LLM deployment uses LoRA adapters for diverse tasks, but mobile storage limitations require merging multiple adapters. However, LoRAs arrive incrementally as users request new tasks, creating a challenge for online continual merging while preserving previous task performance.

Method: Proposes a data-free and computationally efficient strategy for selecting and merging LoRAs when new ones become available, assuming limited adapter storage capacity on devices.

Result: Extensive experiments across real-world tasks show the approach outperforms alternative strategies while respecting on-device storage budgets and computational limitations.

Conclusion: The proposed method effectively addresses the challenge of online continual LoRA merging for on-device LLM deployment, enabling incremental task support while maintaining performance under resource constraints.

Abstract: On-device deployment of Large Language Models (LLMs) frequently leverages
Low-Rank Adapters (LoRAs) to support diverse downstream tasks under tight
resource constraints. To address the limited storage capacity of mobile
devices, recent works have explored model merging techniques to fuse multiple
LoRAs into a single one. In practice, however, LoRAs are often delivered
incrementally, as users request support for new tasks (e.g., novel problem
types or languages). This scenario introduces a new challenge: on-device online
continual merging, where the objective is to incorporate new LoRAs while
preserving the performance on previously supported tasks. In this paper, we
propose a data-free and computationally efficient strategy for selecting and
merging LoRAs when a new one becomes available, assuming the device can store
only a limited number of adapters. Extensive experiments across real-world
tasks demonstrate the superiority of our approach compared to alternative
strategies while adhering to the storage budget and compute limitations of
on-device settings.

</details>


### [161] [Hybrid Interval Type-2 Mamdani-TSK Fuzzy System for Regression Analysis](https://arxiv.org/abs/2510.13437)
*Ashish Bhatia,Renato Cordeiro de Amorim,Vito De Feo*

Main category: cs.LG

TL;DR: A novel fuzzy regression method combining Mamdani's interpretability with TSK's precision using hybrid rule structure and dual dominance types, achieving state-of-the-art performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional regression methods struggle with real-world data complexities like uncertainty and ambiguity, while deep learning lacks interpretability and risks over-fitting. Fuzzy systems offer uncertainty handling but face trade-offs between interpretability (Mamdani) and accuracy (TSK).

Method: Hybrid fuzzy regression approach with combined rule structure featuring fuzzy and crisp components, dual dominance types to enhance both accuracy and explainability.

Result: Achieved state-of-the-art performance in 4 out of 6 benchmark datasets, outperformed opaque models in 2 datasets, and produced best overall score in 1 dataset with RMSE improvements ranging from 0.4% to 19%.

Conclusion: The hybrid methodology provides a balanced tool for predictive modeling that addresses the interpretability-accuracy trade-off in fuzzy systems, offering both improved precision and maintained interpretability.

Abstract: Regression analysis is employed to examine and quantify the relationships
between input variables and a dependent and continuous output variable. It is
widely used for predictive modelling in fields such as finance, healthcare, and
engineering. However, traditional methods often struggle with real-world data
complexities, including uncertainty and ambiguity. While deep learning
approaches excel at capturing complex non-linear relationships, they lack
interpretability and risk over-fitting on small datasets. Fuzzy systems provide
an alternative framework for handling uncertainty and imprecision, with Mamdani
and Takagi-Sugeno-Kang (TSK) systems offering complementary strengths:
interpretability versus accuracy. This paper presents a novel fuzzy regression
method that combines the interpretability of Mamdani systems with the precision
of TSK models. The proposed approach introduces a hybrid rule structure with
fuzzy and crisp components and dual dominance types, enhancing both accuracy
and explainability. Evaluations on benchmark datasets demonstrate
state-of-the-art performance in several cases, with rules maintaining a
component similar to traditional Mamdani systems while improving precision
through improved rule outputs. This hybrid methodology offers a balanced and
versatile tool for predictive modelling, addressing the trade-off between
interpretability and accuracy inherent in fuzzy systems. In the 6 datasets
tested, the proposed approach gave the best fuzzy methodology score in 4
datasets, out-performed the opaque models in 2 datasets and produced the best
overall score in 1 dataset with the improvements in RMSE ranging from 0.4% to
19%.

</details>


### [162] [Message Passing on the Edge: Towards Scalable and Expressive GNNs](https://arxiv.org/abs/2510.13615)
*Pablo Barceló,Fabian Jogl,Alexander Kozachinskiy,Matthias Lanzinger,Stefan Neumann,Cristóbal Rojas*

Main category: cs.LG

TL;DR: EB-1WL is an edge-based color-refinement test and EB-GNN is a corresponding GNN architecture that explicitly uses triangles during message passing, achieving higher expressiveness than 1-WL with near-linear time/memory complexity.


<details>
  <summary>Details</summary>
Motivation: To develop a more expressive graph neural network architecture that can explicitly leverage triangle structures while maintaining computational efficiency, addressing limitations of standard MPNNs.

Method: Proposes EB-1WL (edge-based color-refinement test) and EB-GNN architecture inspired by Chiba and Nishizeki's triangle counting algorithm, explicitly incorporating triangles during message passing.

Result: EB-1WL is significantly more expressive than 1-WL, provides complete logical characterization via first-order logic, achieves near-linear time/memory complexity, and EB-GNN outperforms simple MPNNs while remaining competitive with specialized GNNs.

Conclusion: EB-GNN represents an efficient general-purpose GNN architecture that balances expressiveness and computational efficiency, making it suitable for practical graph learning tasks.

Abstract: We propose EB-1WL, an edge-based color-refinement test, and a corresponding
GNN architecture, EB-GNN. Our architecture is inspired by a classic triangle
counting algorithm by Chiba and Nishizeki, and explicitly uses triangles during
message passing. We achieve the following results: (1)~EB-1WL is significantly
more expressive than 1-WL. Further, we provide a complete logical
characterization of EB-1WL based on first-order logic, and matching
distinguishability results based on homomorphism counting. (2)~In an important
distinction from previous proposals for more expressive GNN architectures,
EB-1WL and EB-GNN require near-linear time and memory on practical graph
learning tasks. (3)~Empirically, we show that EB-GNN is a highly-efficient
general-purpose architecture: It substantially outperforms simple MPNNs, and
remains competitive with task-specialized GNNs while being significantly more
computationally efficient.

</details>


### [163] [Time Series Foundation Models: Benchmarking Challenges and Requirements](https://arxiv.org/abs/2510.13654)
*Marcel Meyer,Sascha Kaltenpoth,Kevin Zalipski,Oliver Müller*

Main category: cs.LG

TL;DR: The paper identifies critical challenges in evaluating Time Series Foundation Models (TSFMs), including data integrity issues, benchmarking flaws, and risks of information leakage, calling for robust evaluation methodologies.


<details>
  <summary>Details</summary>
Motivation: To address the growing challenges in TSFM evaluation similar to those faced by LLMs, particularly concerning data integrity, benchmarking representativeness, and preventing inflated performance estimates due to data contamination.

Method: The authors investigate existing TSFM evaluation practices, analyzing issues with benchmark datasets, data partitioning, spatiotemporal evaluation gaps, and risks of information leakage from overlapping datasets and memorization of global patterns.

Result: Findings reveal widespread confusion in data partitions, risks of inflated performance estimates, incorrect knowledge transfer from global to local time series, and multiple evaluation challenges including lack of spatiotemporal assessment and information leakage risks.

Conclusion: The paper calls for developing robust evaluation methodologies for TSFMs, advocating for principled approaches like evaluations on truly out-of-sample future data to safeguard assessment integrity and prevent pitfalls observed in LLM and classical time series benchmarking.

Abstract: Time Series Foundation Models (TSFMs) represent a new paradigm for time
series forecasting, offering zero-shot forecasting capabilities without the
need for domain-specific pre-training or fine-tuning. However, as with Large
Language Models (LLMs), evaluating TSFMs is tricky, as with ever more extensive
training sets, it becomes more and more challenging to ensure the integrity of
benchmarking data. Our investigation of existing TSFM evaluation highlights
multiple challenges, ranging from the representativeness of the benchmark
datasets, over the lack of spatiotemporal evaluation, to risks of information
leakage due to overlapping and obscure datasets, and the memorization of global
patterns caused by external shocks like economic crises or pandemics. Our
findings reveal widespread confusion regarding data partitions, risking
inflated performance estimates and incorrect transfer of global knowledge to
local time series. We argue for the development of robust evaluation
methodologies to prevent pitfalls already observed in LLM and classical time
series benchmarking, and call upon the research community to design new,
principled approaches, such as evaluations on truly out-of-sample future data,
to safeguard the integrity of TSFM assessment.

</details>


### [164] [$L_2$-Regularized Empirical Risk Minimization Guarantees Small Smooth Calibration Error](https://arxiv.org/abs/2510.13450)
*Masahiro Fujisawa,Futoshi Futami*

Main category: cs.LG

TL;DR: L2-regularized empirical risk minimization directly controls smooth calibration error without post-hoc correction or specialized regularizers, with theoretical guarantees and experimental validation.


<details>
  <summary>Details</summary>
Motivation: Understanding how standard training procedures yield well-calibrated models, as calibration is critical for reliable machine learning but poorly understood in standard ERM.

Method: Theoretical analysis of L2-regularized ERM with finite-sample generalization bounds for smooth calibration error based on optimization error, regularization strength, and Rademacher complexity, instantiated for kernel ridge and logistic regression.

Result: First theoretical proof that canonical L2-regularized ERM directly controls smooth calibration error without post-hoc correction, with experiments confirming these guarantees.

Conclusion: L2-regularized ERM can provide well-calibrated models without boosting or post-hoc recalibration, offering a principled approach to calibration through standard training procedures.

Abstract: Calibration of predicted probabilities is critical for reliable machine
learning, yet it is poorly understood how standard training procedures yield
well-calibrated models. This work provides the first theoretical proof that
canonical $L_{2}$-regularized empirical risk minimization directly controls the
smooth calibration error (smCE) without post-hoc correction or specialized
calibration-promoting regularizer. We establish finite-sample generalization
bounds for smCE based on optimization error, regularization strength, and the
Rademacher complexity. We then instantiate this theory for models in
reproducing kernel Hilbert spaces, deriving concrete guarantees for kernel
ridge and logistic regression. Our experiments confirm these specific
guarantees, demonstrating that $L_{2}$-regularized ERM can provide a
well-calibrated model without boosting or post-hoc recalibration. The source
code to reproduce all experiments is available at
https://github.com/msfuji0211/erm_calibration.

</details>


### [165] [Axial Neural Networks for Dimension-Free Foundation Models](https://arxiv.org/abs/2510.13665)
*Hyunsu Kim,Jonggeon Park,Joan Bruna,Hongseok Yang,Juho Lee*

Main category: cs.LG

TL;DR: Proposes XNN, a dimension-agnostic neural network architecture for physics foundation models that handles varying tensor dimensions across PDE systems while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Foundation models in AI struggle with physics data due to varying dimensionalities across different PDE systems, leading to inefficient training approaches that either fix maximum dimensions or use separate encoders.

Method: Developed Axial Neural Network (XNN) inspired by parameter-sharing structures like Deep Sets and Graph Neural Networks, which generalizes across varying tensor dimensions. Converted existing PDE foundation models into XNNs and evaluated across three training scenarios: from scratch, pretraining on multiple PDEs, and fine-tuning on single PDE.

Result: XNNs perform competitively with original models and show superior generalization to unseen dimensions, demonstrating the importance of multidimensional pretraining for foundation models.

Conclusion: The XNN architecture successfully addresses the dimensionality challenge in physics foundation models, enabling efficient training and better generalization across varying dimensional PDE systems.

Abstract: The advent of foundation models in AI has significantly advanced
general-purpose learning, enabling remarkable capabilities in zero-shot
inference and in-context learning. However, training such models on physics
data, including solutions to partial differential equations (PDEs), poses a
unique challenge due to varying dimensionalities across different systems.
Traditional approaches either fix a maximum dimension or employ separate
encoders for different dimensionalities, resulting in inefficiencies. To
address this, we propose a dimension-agnostic neural network architecture, the
Axial Neural Network (XNN), inspired by parameter-sharing structures such as
Deep Sets and Graph Neural Networks. XNN generalizes across varying tensor
dimensions while maintaining computational efficiency. We convert existing PDE
foundation models into axial neural networks and evaluate their performance
across three training scenarios: training from scratch, pretraining on multiple
PDEs, and fine-tuning on a single PDE. Our experiments show that XNNs perform
competitively with original models and exhibit superior generalization to
unseen dimensions, highlighting the importance of multidimensional pretraining
for foundation models.

</details>


### [166] [Towards Blackwell Optimality: Bellman Optimality Is All You Can Get](https://arxiv.org/abs/2510.13476)
*Victor Boone,Adrienne Tuynman*

Main category: cs.LG

TL;DR: This paper develops learning algorithms to identify bias-optimal policies in Markov Decision Processes, characterizes when finite-time identification is possible, and provides a tractable stopping rule.


<details>
  <summary>Details</summary>
Motivation: Average gain optimality in MDPs is too asymptotic, so incorporating immediate loss measures through bias optimality hierarchy (up to Blackwell optimality) provides more comprehensive performance evaluation.

Method: Construct learning algorithms for each optimality order with vanishing error probability, characterize MDPs where finite-time identification is possible, and develop a tractable stopping rule.

Result: The class of MDPs allowing finite-time identification corresponds to those with a unique Bellman optimal policy, regardless of the optimality order considered.

Conclusion: The proposed learning algorithms can identify bias-optimal policies with vanishing error, and the stopping rule triggers in finite time when possible, providing a complete solution for bias-optimal policy identification.

Abstract: Although average gain optimality is a commonly adopted performance measure in
Markov Decision Processes (MDPs), it is often too asymptotic. Further
incorporating measures of immediate losses leads to the hierarchy of bias
optimalities, all the way up to Blackwell optimality. In this paper, we
investigate the problem of identifying policies of such optimality orders. To
that end, for each order, we construct a learning algorithm with vanishing
probability of error. Furthermore, we characterize the class of MDPs for which
identification algorithms can stop in finite time. That class corresponds to
the MDPs with a unique Bellman optimal policy, and does not depend on the
optimality order considered. Lastly, we provide a tractable stopping rule that
when coupled to our learning algorithm triggers in finite time whenever it is
possible to do so.

</details>


### [167] [Tahakom LLM guidelines and receipts: from pre-training data to an Arabic LLM](https://arxiv.org/abs/2510.13481)
*Areej AlOtaibi,Lina Alyahya,Raghad Alshabanah,Shahad Alfawzan,Shuruq Alarefei,Reem Alsabti,Nouf Alsubaie,Abdulaziz Alhuzaymi,Lujain Alkhelb,Majd Alsayari,Waad Alahmed,Omar Talabay,Jalal Alowibdi,Salem Alelyani,Adel Bibi*

Main category: cs.LG

TL;DR: This paper addresses the challenges in developing Large Language Models for Arabic, focusing on data curation, tokenizer design, and evaluation frameworks.


<details>
  <summary>Details</summary>
Motivation: Developing LLMs for Arabic presents unique challenges that need to be systematically addressed to advance natural language processing capabilities for this language.

Method: The authors detail their approach to Arabic data collection and filtration, assess various tokenizer designs' impact on performance, and propose a corrective methodology for existing Arabic evaluation frameworks.

Result: The research provides insights into effective data curation practices, optimal tokenizer designs for Arabic, and improved evaluation methodologies for Arabic language models.

Conclusion: The paper contributes to advancing Arabic language modeling by sharing data and methodologies, promoting transparency and collaborative development in this specialized field.

Abstract: Large Language Models (LLMs) have significantly advanced the field of natural
language processing, enhancing capabilities in both language understanding and
generation across diverse domains. However, developing LLMs for Arabic presents
unique challenges. This paper explores these challenges by focusing on critical
aspects such as data curation, tokenizer design, and evaluation. We detail our
approach to the collection and filtration of Arabic pre-training datasets,
assess the impact of various tokenizer designs on model performance, and
examine the limitations of existing Arabic evaluation frameworks, for which we
propose a systematic corrective methodology. To promote transparency and
facilitate collaborative development, we share our data and methodologies,
contributing to the advancement of language modeling, particularly for the
Arabic language.

</details>


### [168] [Simplicial Embeddings Improve Sample Efficiency in Actor-Critic Agents](https://arxiv.org/abs/2510.13704)
*Johan Obando-Ceron,Walter Mayor,Samuel Lavoie,Scott Fujimoto,Aaron Courville,Pablo Samuel Castro*

Main category: cs.LG

TL;DR: Simplicial embeddings improve RL sample efficiency by constraining representations to geometric structures, enhancing critic bootstrapping and policy gradients without runtime cost.


<details>
  <summary>Details</summary>
Motivation: Large-scale parallelization in actor-critic methods still requires many environment interactions; well-structured representations can improve generalization and sample efficiency.

Method: Use simplicial embeddings - lightweight representation layers that constrain embeddings to simplicial structures, creating sparse and discrete features.

Result: Consistent improvements in sample efficiency and final performance across FastTD3, FastSAC, and PPO on various continuous- and discrete-control environments.

Conclusion: Simplicial embeddings provide an effective geometric inductive bias that stabilizes learning and improves RL performance without sacrificing runtime speed.

Abstract: Recent works have proposed accelerating the wall-clock training time of
actor-critic methods via the use of large-scale environment parallelization;
unfortunately, these can sometimes still require large number of environment
interactions to achieve a desired level of performance. Noting that
well-structured representations can improve the generalization and sample
efficiency of deep reinforcement learning (RL) agents, we propose the use of
simplicial embeddings: lightweight representation layers that constrain
embeddings to simplicial structures. This geometric inductive bias results in
sparse and discrete features that stabilize critic bootstrapping and strengthen
policy gradients. When applied to FastTD3, FastSAC, and PPO, simplicial
embeddings consistently improve sample efficiency and final performance across
a variety of continuous- and discrete-control environments, without any loss in
runtime speed.

</details>


### [169] [ProtoTopic: Prototypical Network for Few-Shot Medical Topic Modeling](https://arxiv.org/abs/2510.13542)
*Martin Licht,Sara Ketabi,Farzad Khalvati*

Main category: cs.LG

TL;DR: ProtoTopic is a prototypical network-based topic model designed for medical paper abstracts that improves topic coherence and diversity in low-data scenarios.


<details>
  <summary>Details</summary>
Motivation: Standard topic modeling techniques perform poorly on medical texts due to limited documents available for some healthcare topics, requiring specialized approaches for few-shot learning scenarios.

Method: Uses prototypical networks that compute distances between input datapoints and prototype representations, making them effective for low-data scenarios in medical text analysis.

Result: ProtoTopic demonstrates improved topic coherence and diversity compared to existing topic modeling baselines, generating medically relevant topics even with limited data.

Conclusion: Prototypical networks are effective for topic modeling in medical domains with limited data, providing better performance than traditional approaches while maintaining explainability.

Abstract: Topic modeling is a useful tool for analyzing large corpora of written
documents, particularly academic papers. Despite a wide variety of proposed
topic modeling techniques, these techniques do not perform well when applied to
medical texts. This can be due to the low number of documents available for
some topics in the healthcare domain. In this paper, we propose ProtoTopic, a
prototypical network-based topic model used for topic generation for a set of
medical paper abstracts. Prototypical networks are efficient, explainable
models that make predictions by computing distances between input datapoints
and a set of prototype representations, making them particularly effective in
low-data or few-shot learning scenarios. With ProtoTopic, we demonstrate
improved topic coherence and diversity compared to two topic modeling baselines
used in the literature, demonstrating the ability of our model to generate
medically relevant topics even with limited data.

</details>


### [170] [Multi-Objective $\textit{min-max}$ Online Convex Optimization](https://arxiv.org/abs/2510.13560)
*Rahul Vaze,Sumiran Mishra*

Main category: cs.LG

TL;DR: The paper extends online convex optimization to multi-objective settings with K loss sequences, introducing min-max regret as performance measure and proposing a simple Hedge+OGD algorithm achieving O(√(T log K)) regret.


<details>
  <summary>Details</summary>
Motivation: To broaden online convex optimization beyond single loss sequences by considering multiple objectives simultaneously, capturing tradeoffs between tracking different loss sequences.

Method: Proposes a simple algorithm combining Hedge and online gradient descent (OGD) for the i.i.d. input setting where loss functions are generated from an unknown distribution.

Result: The algorithm achieves expected min-max regret of O(√(T log K)) with a remarkably simple proof.

Conclusion: Multi-objective OCO with min-max regret is feasible with simple algorithms achieving sublinear regret bounds in the i.i.d. setting.

Abstract: In online convex optimization (OCO), a single loss function sequence is
revealed over a time horizon of $T$, and an online algorithm has to choose its
action at time $t$, before the loss function at time $t$ is revealed. The goal
of the online algorithm is to incur minimal penalty (called $\textit{regret}$
compared to a static optimal action made by an optimal offline algorithm
knowing all functions of the sequence in advance.
  In this paper, we broaden the horizon of OCO, and consider multi-objective
OCO, where there are $K$ distinct loss function sequences, and an algorithm has
to choose its action at time $t$, before the $K$ loss functions at time $t$ are
revealed. To capture the tradeoff between tracking the $K$ different sequences,
we consider the $\textit{min-max}$ regret, where the benchmark (optimal offline
algorithm) takes a static action across all time slots that minimizes the
maximum of the total loss (summed across time slots) incurred by each of the
$K$ sequences. An online algorithm is allowed to change its action across time
slots, and its {\it min-max} regret is defined as the difference between its
$\textit{min-max}$ cost and that of the benchmark. The $\textit{min-max}$
regret is a stringent performance measure and an algorithm with small regret
needs to `track' all loss function sequences closely at all times.
  We consider this $\textit{min-max}$ regret in the i.i.d. input setting where
all loss functions are i.i.d. generated from an unknown distribution. For the
i.i.d. model we propose a simple algorithm that combines the well-known
$\textit{Hedge}$ and online gradient descent (OGD) and show via a remarkably
simple proof that its expected $\textit{min-max}$ regret is $O(\sqrt{T \log
K})$.

</details>


### [171] [DOLFIN: Balancing Stability and Plasticity in Federated Continual Learning](https://arxiv.org/abs/2510.13567)
*Omayma Moussadek,Riccardo Salami,Simone Calderara*

Main category: cs.LG

TL;DR: DOLFIN is a federated continual learning method that combines Vision Transformers with low-rank adapters (LoRA) to efficiently learn new tasks while preventing forgetting through DualGradient Projection Memory, achieving superior performance with minimal communication overhead.


<details>
  <summary>Details</summary>
Motivation: Current federated continual learning methods struggle to balance performance, privacy preservation, and communication efficiency when learning new tasks across distributed clients without forgetting previous knowledge.

Method: Proposes DOLFIN method using Vision Transformers with low-rank adapters (LoRA) for minimal communication overhead, and incorporates DualGradient Projection Memory (DualGPM) to prevent catastrophic forgetting in federated environments.

Result: DOLFIN consistently outperforms six strong baselines in final average accuracy on CIFAR-100, ImageNet-R, ImageNet-A, and CUB-200 datasets under two Dirichlet heterogeneity settings, while maintaining the same memory footprint.

Conclusion: Orthogonal low-rank adapters provide an effective and scalable solution for privacy-preserving continual learning in federated settings, offering superior performance with efficient communication.

Abstract: Federated continual learning (FCL) enables models to learn new tasks across
multiple distributed clients, protecting privacy and without forgetting
previously acquired knowledge. However, current methods face challenges
balancing performance, privacy preservation, and communication efficiency. We
introduce a Distributed Online LoRA for Federated INcremental learning method
DOLFIN, a novel approach combining Vision Transformers with low-rank adapters
designed to efficiently and stably learn new tasks in federated environments.
Our method leverages LoRA for minimal communication overhead and incorporates
DualGradient Projection Memory (DualGPM) to prevent forgetting. Evaluated on
CIFAR-100, ImageNet-R, ImageNet-A, and CUB-200 under two Dirichlet
heterogeneity settings, DOLFIN consistently surpasses six strong baselines in
final average accuracy while matching their memory footprint. Orthogonal
low-rank adapters offer an effective and scalable solution for
privacy-preserving continual learning in federated settings.

</details>


### [172] [The Art of Scaling Reinforcement Learning Compute for LLMs](https://arxiv.org/abs/2510.13786)
*Devvrit Khatri,Lovish Madaan,Rishabh Tiwari,Rachit Bansal,Sai Surya Duvvuri,Manzil Zaheer,Inderjit S. Dhillon,David Brandfonbrener,Rishabh Agarwal*

Main category: cs.LG

TL;DR: First systematic study of RL scaling in LLMs, providing a framework to predict performance and identify best practices through 400K+ GPU-hour experiments.


<details>
  <summary>Details</summary>
Motivation: RL is crucial for training LLMs but lacks predictive scaling methodologies like those in pre-training, making it hard to evaluate algorithmic improvements despite rising compute budgets.

Method: Conducted large-scale systematic study with 400K+ GPU-hours, fitting sigmoidal compute-performance curves, ablating design choices, and analyzing effects on performance and efficiency.

Result: Found that recipes differ in asymptotic performance; details like loss aggregation affect efficiency but not asymptote; stable recipes enable predictable scaling; proposed ScaleRL recipe successfully predicted performance at 100K GPU-hours.

Conclusion: Provides scientific framework for RL scaling analysis and practical recipe that makes RL training more predictable, similar to pre-training methodologies.

Abstract: Reinforcement learning (RL) has become central to training large language
models (LLMs), yet the field lacks predictive scaling methodologies comparable
to those established for pre-training. Despite rapidly rising compute budgets,
there is no principled understanding of how to evaluate algorithmic
improvements for scaling RL compute. We present the first large-scale
systematic study, amounting to more than 400,000 GPU-hours, that defines a
principled framework for analyzing and predicting RL scaling in LLMs. We fit
sigmoidal compute-performance curves for RL training and ablate a wide range of
common design choices to analyze their effects on asymptotic performance and
compute efficiency. We observe: (1) Not all recipes yield similar asymptotic
performance, (2) Details such as loss aggregation, normalization, curriculum,
and off-policy algorithm primarily modulate compute efficiency without
materially shifting the asymptote, and (3) Stable, scalable recipes follow
predictable scaling trajectories, enabling extrapolation from smaller-scale
runs. Combining these insights, we propose a best-practice recipe, ScaleRL, and
demonstrate its effectiveness by successfully scaling and predicting validation
performance on a single RL run scaled up to 100,000 GPU-hours. Our work
provides both a scientific framework for analyzing scaling in RL and a
practical recipe that brings RL training closer to the predictability long
achieved in pre-training.

</details>


### [173] [Selective Adversarial Attacks on LLM Benchmarks](https://arxiv.org/abs/2510.13570)
*Ivan Dubrovsky,Anastasia Orlova,Illarion Iov,Nina Gubina,Irena Gureeva,Alexey Zaytsev*

Main category: cs.LG

TL;DR: Selective adversarial attacks can manipulate LLM benchmark rankings by degrading specific models' performance while minimally affecting others, challenging the fairness of leaderboard-based evaluations.


<details>
  <summary>Details</summary>
Motivation: To investigate whether adversarial perturbations can be selectively targeted to degrade or enhance specific LLM performance on benchmarks like MMLU, potentially manipulating relative rankings and challenging evaluation fairness.

Method: Used canonical attacks from TextAttack framework, developed a custom constraint for selectivity, and created a surrogate-LLM pipeline to generate selective perturbations on the MMLU benchmark.

Result: Found that selective adversarial attacks exist and can materially alter relative model rankings, demonstrating that even subtle edits can shift comparative judgments between LLMs.

Conclusion: Benchmark evaluations are vulnerable to selective adversarial attacks, motivating the need for perturbation-aware reporting and robustness diagnostics to ensure fair and transparent LLM evaluation.

Abstract: Benchmarking outcomes increasingly govern trust, selection, and deployment of
LLMs, yet these evaluations remain vulnerable to semantically equivalent
adversarial perturbations. Prior work on adversarial robustness in NLP has
emphasized text attacks that affect many models equally, leaving open the
question of whether it is possible to selectively degrade or enhance
performance while minimally affecting other models. We formalize this problem
and study selective adversarial attacks on MMLU - a widely used benchmark
designed to measure a language model's broad general knowledge and reasoning
ability across different subjects. Using canonical attacks integrated into
TextAttack framework, we introduce a protocol for selectivity assessment,
develop a custom constraint to increase selectivity of attacks and propose a
surrogate-LLM pipeline that generates selective perturbations. Empirically, we
find that selective adversarial attacks exist and can materially alter relative
rankings, challenging the fairness, reproducibility, and transparency of
leaderboard-driven evaluation. Our results motivate perturbation-aware
reporting and robustness diagnostics for LLM evaluation and demonstrate that
even subtle edits can shift comparative judgments.

</details>


### [174] [Provably Invincible Adversarial Attacks on Reinforcement Learning Systems: A Rate-Distortion Information-Theoretic Approach](https://arxiv.org/abs/2510.13792)
*Ziqing Lu,Lifeng Lai,Weiyu Xu*

Main category: cs.LG

TL;DR: Proposes an 'invincible' adversarial attack on RL systems using rate-distortion information theory to randomly manipulate observations, preventing agents from learning the true environment dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing deterministic adversarial attacks can be countered by RL agents. Need for more robust attacks that cannot be easily defeated to improve RL system security.

Method: Uses rate-distortion information-theoretic approach to randomly alter agents' observations of transition kernel and other properties, limiting information gain about ground-truth environment.

Result: Derived information-theoretic lower bound on agent's reward regret. Showed impact on both model-based and model-free RL algorithms. Extended approach to other attack types like state observation attacks.

Conclusion: Information-theoretic adversarial attacks provide provably uncounterable threats to RL systems, highlighting fundamental security vulnerabilities in current RL approaches.

Abstract: Reinforcement learning (RL) for the Markov Decision Process (MDP) has emerged
in many security-related applications, such as autonomous driving, financial
decisions, and drone/robot algorithms. In order to improve the
robustness/defense of RL systems against adversaries, studying various
adversarial attacks on RL systems is very important. Most previous work
considered deterministic adversarial attack strategies in MDP, which the
recipient (victim) agent can defeat by reversing the deterministic attacks. In
this paper, we propose a provably ``invincible'' or ``uncounterable'' type of
adversarial attack on RL. The attackers apply a rate-distortion
information-theoretic approach to randomly change agents' observations of the
transition kernel (or other properties) so that the agent gains zero or very
limited information about the ground-truth kernel (or other properties) during
the training. We derive an information-theoretic lower bound on the recipient
agent's reward regret and show the impact of rate-distortion attacks on
state-of-the-art model-based and model-free algorithms. We also extend this
notion of an information-theoretic approach to other types of adversarial
attack, such as state observation attacks.

</details>


### [175] [ArtNet: Hierarchical Clustering-Based Artificial Netlist Generator for ML and DTCO Application](https://arxiv.org/abs/2510.13582)
*Andrew B. Kahng. Seokhyeong Kang,Seonghyeon Park,Dooseok Yoon*

Main category: cs.LG

TL;DR: ArtNet is an artificial netlist generator that creates realistic training data for ML models and enables efficient design space exploration for DTCO, improving PPA optimization in advanced semiconductor nodes.


<details>
  <summary>Details</summary>
Motivation: Address limitations in ML and DTCO approaches caused by lack of diverse training data and long design flow turnaround times in advanced semiconductor nodes.

Method: Propose ArtNet, a novel artificial netlist generator that replicates key topological characteristics to produce realistic artificial datasets matching target parameters.

Result: In CNN-based DRV prediction, ArtNet's data augmentation improves F1 score by 0.16. In DTCO context, ArtNet-generated mini-brains achieve PPA match up to 97.94% with target full-scale block designs.

Conclusion: ArtNet effectively enhances ML model generalization and supports broader design space exploration, enabling more efficient PPA optimization in semiconductor design.

Abstract: In advanced nodes, optimization of power, performance and area (PPA) has
become highly complex and challenging. Machine learning (ML) and
design-technology co-optimization (DTCO) provide promising mitigations, but
face limitations due to a lack of diverse training data as well as long design
flow turnaround times (TAT). We propose ArtNet, a novel artificial netlist
generator designed to tackle these issues. Unlike previous methods, ArtNet
replicates key topological characteristics, enhancing ML model generalization
and supporting broader design space exploration for DTCO. By producing
realistic artificial datasets that moreclosely match given target parameters,
ArtNet enables more efficient PPAoptimization and exploration of flows and
design enablements. In the context of CNN-based DRV prediction, ArtNet's data
augmentationimproves F1 score by 0.16 compared to using only the original
(real) dataset. In the DTCO context, ArtNet-generated mini-brains achieve a PPA
match up to 97.94%, demonstrating close alignment with design metrics of
targeted full-scale block designs.

</details>


### [176] [EEGChaT: A Transformer-Based Modular Channel Selector for SEEG Analysis](https://arxiv.org/abs/2510.13592)
*Chen Wang,Yansen Wang,Dongqi Han,Zilong Wang,Dongsheng Li*

Main category: cs.LG

TL;DR: EEGChaT is a Transformer-based channel selection module that automatically identifies task-relevant channels in SEEG data using Channel Aggregation Tokens and improved Attention Rollout for interpretable channel importance scores.


<details>
  <summary>Details</summary>
Motivation: SEEG signal analysis faces challenges due to large numbers of input channels and heterogeneous relevance, with traditional channel selection methods struggling to scale or provide meaningful interpretability.

Method: Proposed EEGChaT uses Channel Aggregation Tokens (CATs) to aggregate information across channels and leverages improved Attention Rollout technique to compute interpretable channel importance scores.

Result: On DuIN dataset, EEGChaT integration with existing classification models improved decoding accuracy by up to 17% absolute gains, with channel weights showing substantial overlap with manually selected channels.

Conclusion: EEGChaT is an effective and generalizable solution for channel selection in high-dimensional SEEG analysis, offering both enhanced performance and insights into neural signal relevance.

Abstract: Analyzing stereoelectroencephalography (SEEG) signals is critical for
brain-computer interface (BCI) applications and neuroscience research, yet
poses significant challenges due to the large number of input channels and
their heterogeneous relevance. Traditional channel selection methods struggle
to scale or provide meaningful interpretability for SEEG data. In this work, we
propose EEGChaT, a novel Transformer-based channel selection module designed to
automatically identify the most task-relevant channels in SEEG recordings.
EEGChaT introduces Channel Aggregation Tokens (CATs) to aggregate information
across channels, and leverages an improved Attention Rollout technique to
compute interpretable, quantitative channel importance scores. We evaluate
EEGChaT on the DuIN dataset, demonstrating that integrating EEGChaT with
existing classification models consistently improves decoding accuracy,
achieving up to 17\% absolute gains. Furthermore, the channel weights produced
by EEGChaT show substantial overlap with manually selected channels, supporting
the interpretability of the approach. Our results suggest that EEGChaT is an
effective and generalizable solution for channel selection in high-dimensional
SEEG analysis, offering both enhanced performance and insights into neural
signal relevance.

</details>


### [177] [Physics-augmented Multi-task Gaussian Process for Modeling Spatiotemporal Dynamics](https://arxiv.org/abs/2510.13601)
*Xizhuo Zhang,Bing Yao*

Main category: cs.LG

TL;DR: A physics-augmented multi-task Gaussian Process framework for spatiotemporal dynamic systems that combines geometry-aware modeling with physics-based regularization to improve prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: To address challenges in modeling high-dimensional spatiotemporal data with irregular spatial structures, rapid temporal dynamics, and the need for joint prediction of multiple interrelated physical variables.

Method: Developed a geometry-aware multi-task Gaussian Process model to capture spatiotemporal structure and inter-task dependencies, enhanced with physics-based regularization that constrains predictions to be consistent with governing physical laws.

Result: Validated on 3D cardiac electrodynamics modeling, demonstrating significant improvement in prediction accuracy over existing methods by incorporating domain-specific physical constraints and geometric priors.

Conclusion: The proposed P-M-GP framework effectively enhances model fidelity and robustness for spatiotemporal dynamic systems by integrating physical laws with geometric-aware probabilistic modeling.

Abstract: Recent advances in sensing and imaging technologies have enabled the
collection of high-dimensional spatiotemporal data across complex geometric
domains. However, effective modeling of such data remains challenging due to
irregular spatial structures, rapid temporal dynamics, and the need to jointly
predict multiple interrelated physical variables. This paper presents a
physics-augmented multi-task Gaussian Process (P-M-GP) framework tailored for
spatiotemporal dynamic systems. Specifically, we develop a geometry-aware,
multi-task Gaussian Process (M-GP) model to effectively capture intrinsic
spatiotemporal structure and inter-task dependencies. To further enhance the
model fidelity and robustness, we incorporate governing physical laws through a
physics-based regularization scheme, thereby constraining predictions to be
consistent with governing dynamical principles. We validate the proposed P-M-GP
framework on a 3D cardiac electrodynamics modeling task. Numerical experiments
demonstrate that our method significantly improves prediction accuracy over
existing methods by effectively incorporating domain-specific physical
constraints and geometric prior.

</details>


### [178] [Towards Robust Knowledge Removal in Federated Learning with High Data Heterogeneity](https://arxiv.org/abs/2510.13606)
*Riccardo Santi,Riccardo Salami,Simone Calderara*

Main category: cs.LG

TL;DR: Proposes a fast client influence removal method for federated learning using Task Arithmetic and Neural Tangent Kernel to avoid model unavailability during removal.


<details>
  <summary>Details</summary>
Motivation: Current knowledge removal methods in federated learning require multiple communication rounds, making models unavailable during removal and causing disservice to users.

Method: Uses Task Arithmetic and Neural Tangent Kernel to rapidly remove a client's influence from the model without multiple communication rounds.

Result: Enables fast removal of client contributions while maintaining model availability throughout the process.

Conclusion: The proposed solution addresses the need for efficient and timely client influence removal in privacy-preserving distributed AI training.

Abstract: Nowdays, there are an abundance of portable devices capable of collecting
large amounts of data and with decent computational power. This opened the
possibility to train AI models in a distributed manner, preserving the
participating clients' privacy. However, because of privacy regulations and
safety requirements, elimination upon necessity of a client contribution to the
model has become mandatory. The cleansing process must satisfy specific
efficacy and time requirements. In recent years, research efforts have produced
several knowledge removal methods, but these require multiple communication
rounds between the data holders and the process coordinator. This can cause the
unavailability of an effective model up to the end of the removal process,
which can result in a disservice to the system users. In this paper, we
introduce an innovative solution based on Task Arithmetic and the Neural
Tangent Kernel, to rapidly remove a client's influence from a model.

</details>


### [179] [Manifold Decoders: A Framework for Generative Modeling from Nonlinear Embeddings](https://arxiv.org/abs/2510.13622)
*Riddhish Thakare,Kingdom Mutala Akugri*

Main category: cs.LG

TL;DR: This paper introduces a framework for adding bidirectional mapping capabilities to classical NLDR methods like t-SNE, Isomap, and LLE, and explores diffusion-based generation on learned manifolds, but finds fundamental limitations in generative quality.


<details>
  <summary>Details</summary>
Motivation: Classical NLDR methods lack the ability to map embeddings back to original space, limiting their use in generative applications. The paper aims to address this critical gap by enabling bidirectional mapping.

Method: Systematic framework for constructing neural decoder architectures for NLDR methods, extended with diffusion-based generative process operating directly in learned manifold spaces.

Result: Decoders successfully reconstruct data but are surpassed by autoencoders. Manifold-constrained diffusion yields poor-quality samples due to discrete/sparse nature of NLDR embeddings being ill-suited for continuous interpolation required by generative models.

Conclusion: There are inherent challenges in retrofitting generative capabilities onto NLDR methods designed primarily for visualization and analysis, highlighting fundamental trade-offs between reconstruction quality and generative performance.

Abstract: Classical nonlinear dimensionality reduction (NLDR) techniques like t-SNE,
Isomap, and LLE excel at creating low-dimensional embeddings for data
visualization but fundamentally lack the ability to map these embeddings back
to the original high-dimensional space. This one-way transformation limits
their use in generative applications. This paper addresses this critical gap by
introducing a system- atic framework for constructing neural decoder
architectures for prominent NLDR methods, enabling bidirectional mapping for
the first time. We extend this framework by implementing a diffusion-based
generative process that operates directly within these learned manifold spaces.
Through experiments on the CelebA dataset, we evaluate the reconstruction and
generative performance of our approach against autoencoder and standard
diffusion model baselines. Our findings reveal a fundamental trade- off: while
the decoders successfully reconstruct data, their quality is surpassed by
end-to-end optimized autoencoders. Moreover, manifold-constrained diffusion
yields poor-quality samples, suggesting that the discrete and sparse nature of
classical NLDR embeddings is ill-suited for the continuous inter- polation
required by generative models. This work highlights the inherent challenges in
retrofitting generative capabilities onto NLDR methods designed primarily for
visualization and analysis.

</details>


### [180] [Multivariate Time Series Forecasting with Gate-Based Quantum Reservoir Computing on NISQ Hardware](https://arxiv.org/abs/2510.13634)
*Wissal Hamhoum,Soumaya Cherkaoui,Jean-Frederic Laprade,Ola Ahmed,Shengrui Wang*

Main category: cs.LG

TL;DR: Gate-based quantum reservoir computing for multivariate time series forecasting that works well on current quantum hardware, with surprising performance benefits from device noise.


<details>
  <summary>Details</summary>
Motivation: Most quantum reservoir computing studies focus on univariate signals and don't consider near-term hardware constraints, limiting practical deployment on NISQ devices.

Method: Multivariate time series QRC (MTS-QRC) using paired injection and memory qubits with Trotterized nearest-neighbor transverse-field Ising evolution optimized for current device connectivity and depth constraints.

Result: Achieved MSE of 0.0087 on Lorenz-63 and 0.0036 on ENSO, performing competitively with classical reservoir computing and learned RNNs. On IBM Heron R2 hardware, maintained accuracy with realistic depths and even outperformed noiseless simulator on ENSO due to noise-induced regularization.

Conclusion: Gate-based QRC is practical for multivariate time series forecasting on NISQ hardware, and device noise can sometimes benefit performance by acting as an implicit regularizer for linear readout.

Abstract: Quantum reservoir computing (QRC) offers a hardware-friendly approach to
temporal learning, yet most studies target univariate signals and overlook
near-term hardware constraints. This work introduces a gate-based QRC for
multivariate time series (MTS-QRC) that pairs injection and memory qubits and
uses a Trotterized nearest-neighbor transverse-field Ising evolution optimized
for current device connectivity and depth. On Lorenz-63 and ENSO, the method
achieves a mean square error (MSE) of 0.0087 and 0.0036, respectively,
performing on par with classical reservoir computing on Lorenz and above
learned RNNs on both, while NVAR and clustered ESN remain stronger on some
settings. On IBM Heron R2, MTS-QRC sustains accuracy with realistic depths and,
interestingly, outperforms a noiseless simulator on ENSO; singular value
analysis indicates that device noise can concentrate variance in feature
directions, acting as an implicit regularizer for linear readout in this
regime. These findings support the practicality of gate-based QRC for MTS
forecasting on NISQ hardware and motivate systematic studies on when and how
hardware noise benefits QRC readouts.

</details>


### [181] [What is the objective of reasoning with reinforcement learning?](https://arxiv.org/abs/2510.13651)
*Damek Davis,Benjamin Recht*

Main category: cs.LG

TL;DR: Several RL algorithms for LLMs with binary rewards are shown to be equivalent to stochastic gradient ascent on monotone transforms of correct answer probabilities.


<details>
  <summary>Details</summary>
Motivation: To establish a unified mathematical framework connecting different reinforcement learning algorithms used in large language models with binary reward signals.

Method: Analyze popular RL algorithms (rejection sampling and GRPO) by showing they correspond to stochastic gradient ascent on monotone transformations of the probability of correct answers given prompts.

Result: Identified specific transformations: rejection sampling algorithms use logarithmic transform, while GRPO algorithm uses arcsine of square root transform.

Conclusion: Different RL algorithms for LLMs with binary rewards can be understood as gradient ascent methods on monotone transformations of the underlying probability distribution, providing a unified theoretical perspective.

Abstract: We show that several popular algorithms for reinforcement learning in large
language models with binary rewards can be viewed as stochastic gradient ascent
on a monotone transform of the probability of a correct answer given a prompt.
In particular, the transformation associated with rejection sampling algorithms
is the logarithm and that associated with the GRPO algorithm is the arcsine of
the square root.

</details>


### [182] [Rebalancing with Calibrated Sub-classes (RCS): An Enhanced Approach for Robust Imbalanced Classification](https://arxiv.org/abs/2510.13656)
*Priyobrata Mondal,Faizanuddin Ansari,Swagatam Das*

Main category: cs.LG

TL;DR: RCS is a distribution calibration method for imbalanced classification that uses weighted Gaussian mixture components from majority and intermediate classes to estimate minority class distributions, preventing overgeneralization and improving classification performance.


<details>
  <summary>Details</summary>
Motivation: To address the class imbalance problem where classifiers become biased toward majority classes due to insufficient minority class data, and to mitigate overgeneralization issues in existing distribution calibration methods.

Method: Uses an encoder-decoder network to preserve imbalanced data structure, then applies distribution calibration with weighted Gaussian mixture components from majority and intermediate classes to generate synthetic minority samples.

Result: Achieves superior classification performance compared to baseline and state-of-the-art methods across diverse image, text, and tabular datasets.

Conclusion: The proposed RCS method effectively handles class imbalance by leveraging neighboring class distributions for calibration, preventing overgeneralization and improving classifier robustness.

Abstract: The class imbalance problem refers to the insufficiency of data in certain
classes, which causes a classifier to be biased toward the majority class.
Distribution calibration is a technique that seeks to estimate a more accurate
class distribution based on an observed or estimated one. To address this
issue, we propose a distribution calibration-based method-Rebalancing with
Calibrated Sub-classes (RCS): An Enhanced Approach for Robust Imbalanced
Classification, which estimates the distribution parameters of the minority
classes using weighted parameters derived from a mixture of Gaussian components
from both the majority and intermediate classes. An encoder-decoder network is
trained to preserve the structure of the imbalanced data and prevent
disentanglement. After training, feature vectors extracted from the encoder are
used to generate synthetic samples through our distribution calibration
strategy. This approach effectively mitigates the overgeneralization problem
that arises when only the distribution of the majority class is used to
approximate the minority class statistics. Instead, our method calibrates the
parameters by leveraging the distribution of data points in neighboring
regions. Experimental results demonstrate that the proposed method achieves
superior classification performance compared to several baseline and
state-of-the-art techniques across a diverse range of image, text, and tabular
datasets.

</details>


### [183] [Adam or Gauss-Newton? A Comparative Study In Terms of Basis Alignment and SGD Noise](https://arxiv.org/abs/2510.13680)
*Bingbin Liu,Rachit Bansal,Depen Morwani,Nikhil Vyas,David Alvarez-Melis,Sham M. Kakade*

Main category: cs.LG

TL;DR: This paper compares Adam and Gauss-Newton diagonal preconditioners, analyzing their performance based on basis choice and gradient noise, showing Adam can outperform GN methods in some full-batch settings but behaves similarly to GN^{-1/2} in stochastic regimes.


<details>
  <summary>Details</summary>
Motivation: To understand the comparative performance of Adam and Gauss-Newton diagonal preconditioners, examining how basis choice and gradient noise affect optimization performance in deep learning.

Method: Theoretical analysis on quadratic objectives and logistic regression across all four quadrants, comparing Adam with GN^{-1} and GN^{-1/2} methods in both full-batch and stochastic settings.

Result: In full-batch settings, Adam can outperform both GN^{-1} and GN^{-1/2} regardless of basis choice. In stochastic regimes, Adam behaves similarly to GN^{-1/2} for linear regression under Gaussian data assumptions.

Conclusion: The choice between Adam and Gauss-Newton diagonal preconditioners depends on the optimization setting, with Adam showing advantages in some full-batch scenarios and behaving similarly to GN^{-1/2} in stochastic settings.

Abstract: Diagonal preconditioners are computationally feasible approximate to
second-order optimizers, which have shown significant promise in accelerating
training of deep learning models. Two predominant approaches are based on Adam
and Gauss-Newton (GN) methods: the former leverages statistics of current
gradients and is the de-factor optimizers for neural networks, and the latter
uses the diagonal elements of the Gauss-Newton matrix and underpins some of the
recent diagonal optimizers such as Sophia.
  In this work, we compare these two diagonal preconditioning methods through
the lens of two key factors: the choice of basis in the preconditioner, and the
impact of gradient noise from mini-batching. To gain insights, we analyze these
optimizers on quadratic objectives and logistic regression under all four
quadrants. We show that regardless of the basis, there exist instances where
Adam outperforms both GN$^{-1}$ and GN$^{-1/2}$ in full-batch settings.
Conversely, in the stochastic regime, Adam behaves similarly to GN$^{-1/2}$ for
linear regression under a Gaussian data assumption. These theoretical results
are supported by empirical studies on both convex and non-convex objectives.

</details>


### [184] [Information-Theoretic Reward Modeling for Stable RLHF: Detecting and Mitigating Reward Hacking](https://arxiv.org/abs/2510.13694)
*Yuchun Miao,Liang Ding,Sen Zhang,Rong Bao,Lefei Zhang,Dacheng Tao*

Main category: cs.LG

TL;DR: InfoRM addresses reward hacking in RLHF through information-theoretic reward modeling and distribution-level regularization, with a new metric (MOP) to quantify reward hacking severity.


<details>
  <summary>Details</summary>
Motivation: Reward hacking remains a major challenge in RLHF due to reward misgeneralization and lack of suitable regularization during RL optimization.

Method: Propose InfoRM framework based on Information Bottleneck principle to filter preference-irrelevant information, and IBL regularization that penalizes deviations in IB latent space using Mahalanobis distance.

Result: Extensive experiments show effectiveness of InfoRM and IBL across diverse LLMs and datasets, with MOP providing reliable quantification of reward hacking severity.

Conclusion: The proposed methods collectively advance RLHF by mitigating reward hacking through principled information filtering and regularization.

Abstract: Despite the success of Reinforcement Learning from Human Feedback (RLHF) in
aligning language models with human values, reward hacking-or reward
over-optimization-remains a major challenge. We identify two key obstacles to
its mitigation: (1) reward misgeneralization in reward modeling, where reward
models overfit to spurious, preference-irrelevant features; and (2) the lack of
suitable regularization during RL optimization, as existing token-level
constraints often over-restrict the policy space. To address these issues, we
propose InfoRM, an information-theoretic reward modeling framework based on the
Information Bottleneck (IB) principle, which filters out preference-irrelevant
information to alleviate reward misgeneralization. We further observe that
reward-hacked responses manifest as pronounced outliers in InfoRM's IB latent
space, measured by Mahalanobis distance from the SFT-induced distribution.
Motivated by this, we introduce IBL, a distribution-level regularization that
penalizes such deviations, effectively expanding the optimization landscape
while maintaining alignment. We prove that IBL is theoretically equivalent to
the pessimistic RL objective within the IB latent space. Finally, we present
Mahalanobis Outlier Probability (MOP), a statistical metric for quantifying
reward hacking severity, enabling principled hyperparameter tuning and online
mitigation such as early stopping. Extensive experiments across diverse LLMs
and datasets confirm the generality of our findings, the effectiveness of
InfoRM and IBL, and the reliability of MOP as a diagnostic tool-collectively
advancing the state of RLHF.

</details>


### [185] [Don't Be Greedy, Just Relax! Pruning LLMs via Frank-Wolfe](https://arxiv.org/abs/2510.13713)
*Christophe Roux,Max Zimmer,Alexandre d'Aspremont,Sebastian Pokutta*

Main category: cs.LG

TL;DR: The paper proposes a novel pruning method for Large Language Models using Frank-Wolfe algorithm to solve the convex relaxation of the combinatorial pruning problem, achieving better performance than greedy heuristics while remaining memory-efficient.


<details>
  <summary>Details</summary>
Motivation: Existing LLM pruning methods use greedy heuristics that ignore weight interactions and suffer from suboptimal solutions. The hard combinatorial nature of finding optimal pruning masks makes exact solutions intractable, necessitating better approximation methods.

Method: The authors use convex relaxation of combinatorial pruning constraints and solve it using the Frank-Wolfe algorithm. This approach considers weight interactions in the pruning objective and provides theoretical convergence guarantees.

Result: The method drastically reduces per-layer pruning error, outperforms strong baselines on state-of-the-art GPT architectures, and remains memory-efficient while providing approximate solutions to the original combinatorial problem.

Conclusion: The Frank-Wolfe based approach provides a theoretically justified and practically effective alternative to greedy pruning methods, offering better performance while maintaining computational efficiency for LLM pruning.

Abstract: Pruning is a common technique to reduce the compute and storage requirements
of Neural Networks. While conventional approaches typically retrain the model
to recover pruning-induced performance degradation, state-of-the-art Large
Language Model (LLM) pruning methods operate layer-wise, minimizing the
per-layer pruning error on a small calibration dataset to avoid full
retraining, which is considered computationally prohibitive for LLMs. However,
finding the optimal pruning mask is a hard combinatorial problem and solving it
to optimality is intractable. Existing methods hence rely on greedy heuristics
that ignore the weight interactions in the pruning objective. In this work, we
instead consider the convex relaxation of these combinatorial constraints and
solve the resulting problem using the Frank-Wolfe (FW) algorithm. Our method
drastically reduces the per-layer pruning error, outperforms strong baselines
on state-of-the-art GPT architectures, and remains memory-efficient. We provide
theoretical justification by showing that, combined with the convergence
guarantees of the FW algorithm, we obtain an approximate solution to the
original combinatorial problem upon rounding the relaxed solution to
integrality.

</details>


### [186] [Assessing the Geographic Generalization and Physical Consistency of Generative Models for Climate Downscaling](https://arxiv.org/abs/2510.13722)
*Carlo Saccardi,Maximilian Pierzyna,Haitz Sáez de Ocáriz Borde,Simone Monaco,Cristian Meo,Pietro Liò,Rudolf Saathof,Geethu Joseph,Justin Dauwels*

Main category: cs.LG

TL;DR: Deep learning models for climate downscaling show poor geographic generalization and physical consistency despite strong performance metrics. A power spectral density loss function is proposed to improve small-scale structure reconstruction.


<details>
  <summary>Details</summary>
Motivation: Kilometer-scale weather data is computationally intensive to produce with traditional simulations. Deep learning offers faster alternatives but reliability is questionable due to evaluation with standard ML metrics rather than atmospheric physics insights.

Method: Benchmark recent state-of-the-art deep learning models (including CorrDiff) with physics-inspired diagnostics. Introduce power spectral density loss function to improve geographic generalization and physical consistency.

Result: Models trained on limited European geographies struggle to generalize to other regions (Iberia, Morocco, Scandinavia). They fail to accurately capture second-order variables like divergence and vorticity, even in in-distribution geographies. Power spectral density loss empirically improves geographic generalization.

Conclusion: Current deep learning models for climate downscaling have significant limitations in geographic generalization and physical consistency. The proposed power spectral density loss offers a simple initial solution to improve reconstruction of small-scale physical structures.

Abstract: Kilometer-scale weather data is crucial for real-world applications but
remains computationally intensive to produce using traditional weather
simulations. An emerging solution is to use deep learning models, which offer a
faster alternative for climate downscaling. However, their reliability is still
in question, as they are often evaluated using standard machine learning
metrics rather than insights from atmospheric and weather physics. This paper
benchmarks recent state-of-the-art deep learning models and introduces
physics-inspired diagnostics to evaluate their performance and reliability,
with a particular focus on geographic generalization and physical consistency.
Our experiments show that, despite the seemingly strong performance of models
such as CorrDiff, when trained on a limited set of European geographies (e.g.,
central Europe), they struggle to generalize to other regions such as Iberia,
Morocco in the south, or Scandinavia in the north. They also fail to accurately
capture second-order variables such as divergence and vorticity derived from
predicted velocity fields. These deficiencies appear even in in-distribution
geographies, indicating challenges in producing physically consistent
predictions. We propose a simple initial solution: introducing a power spectral
density loss function that empirically improves geographic generalization by
encouraging the reconstruction of small-scale physical structures. The code for
reproducing the experimental results can be found at
https://github.com/CarloSaccardi/PSD-Downscaling

</details>


### [187] [Asymptotically optimal reinforcement learning in Block Markov Decision Processes](https://arxiv.org/abs/2510.13748)
*Thomas van Vuren,Fiona Sloothaak,Maarten G. Wolf,Jaron Sanders*

Main category: cs.LG

TL;DR: This paper presents a reinforcement learning algorithm for Block Markov Decision Processes that uses clustering to learn latent structure, achieving optimal regret bounds that improve upon prior work.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning struggles with large state/action spaces in real-world settings, but many environments have exploitable latent structure. The paper aims to formalize how clustering methods can accelerate learning by recovering this latent structure.

Method: A two-phase RL algorithm: first learns latent structure through random exploration and clustering, then switches to an optimism-guided strategy adapted to the uncovered structure.

Result: Achieves regret O(√T + n) on BMDPs susceptible to clustering, improving the prior best bound of O(√T + n²). Proves this is asymptotically optimal - no algorithm can achieve lower regret uniformly on this class.

Conclusion: Accurate latent state estimation through clustering can effectively speed up learning in structured environments, and the proposed algorithm achieves optimal regret bounds for this class of BMDPs.

Abstract: The curse of dimensionality renders Reinforcement Learning (RL) impractical
in many real-world settings with exponentially large state and action spaces.
Yet, many environments exhibit exploitable structure that can accelerate
learning. To formalize this idea, we study RL in Block Markov Decision
Processes (BMDPs). BMDPs model problems with large observation spaces, but
where transition dynamics are fully determined by latent states. Recent
advances in clustering methods have enabled the efficient recovery of this
latent structure. However, a regret analysis that exploits these techniques to
determine their impact on learning performance remained open. We are now
addressing this gap by providing a regret analysis that explicitly leverages
clustering, demonstrating that accurate latent state estimation can indeed
effectively speed up learning.
  Concretely, this paper analyzes a two-phase RL algorithm for BMDPs that first
learns the latent structure through random exploration and then switches to an
optimism-guided strategy adapted to the uncovered structure. This algorithm
achieves a regret that is $O(\sqrt{T}+n)$ on a large class of BMDPs susceptible
to clustering. Here, $T$ denotes the number of time steps, $n$ is the
cardinality of the observation space, and the Landau notation $O(\cdot)$ holds
up to constants and polylogarithmic factors. This improves the best prior
bound, $O(\sqrt{T}+n^2)$, especially when $n$ is large. Moreover, we prove that
no algorithm can achieve lower regret uniformly on this same class of BMDPs.
This establishes that, on this class, the algorithm achieves asymptotic
optimality.

</details>


### [188] [Progressive multi-fidelity learning for physical system predictions](https://arxiv.org/abs/2510.13762)
*Paolo Conti,Mengwu Guo,Attilio Frangi,Andrea Manzoni*

Main category: cs.LG

TL;DR: A progressive multi-fidelity surrogate model that sequentially incorporates diverse data types using tailored encoders and neural networks, with dual connections enabling additive corrections and preventing performance degradation when integrating new data.


<details>
  <summary>Details</summary>
Motivation: High-fidelity data is expensive and time-consuming to acquire, while low-fidelity data is more accessible but less accurate. Practical challenges include different data types, modalities, and non-concurrent availability, making surrogate modeling difficult.

Method: Progressive model with tailored encoders for different data types, neural networks for multi-fidelity regression, and dual connections: concatenations among encoded inputs and additive connections among final outputs for additive corrections.

Result: The model reliably integrates multi-modal data, provides accurate predictions, and maintains performance when generalizing across time and parameter variations, as demonstrated on numerical benchmarks and a real-world case study.

Conclusion: The proposed progressive multi-fidelity surrogate model effectively addresses challenges of diverse data types and modalities, enabling accurate predictions while preventing performance degradation during data integration.

Abstract: Highly accurate datasets from numerical or physical experiments are often
expensive and time-consuming to acquire, posing a significant challenge for
applications that require precise evaluations, potentially across multiple
scenarios and in real-time. Even building sufficiently accurate surrogate
models can be extremely challenging with limited high-fidelity data.
Conversely, less expensive, low-fidelity data can be computed more easily and
encompass a broader range of scenarios. By leveraging multi-fidelity
information, prediction capabilities of surrogates can be improved. However, in
practical situations, data may be different in types, come from sources of
different modalities, and not be concurrently available, further complicating
the modeling process. To address these challenges, we introduce a progressive
multi-fidelity surrogate model. This model can sequentially incorporate diverse
data types using tailored encoders. Multi-fidelity regression from the encoded
inputs to the target quantities of interest is then performed using neural
networks. Input information progressively flows from lower to higher fidelity
levels through two sets of connections: concatenations among all the encoded
inputs, and additive connections among the final outputs. This dual connection
system enables the model to exploit correlations among different datasets while
ensuring that each level makes an additive correction to the previous level
without altering it. This approach prevents performance degradation as new
input data are integrated into the model and automatically adapts predictions
based on the available inputs. We demonstrate the effectiveness of the approach
on numerical benchmarks and a real-world case study, showing that it reliably
integrates multi-modal data and provides accurate predictions, maintaining
performance when generalizing across time and parameter variations.

</details>


### [189] [Tensor Gaussian Processes: Efficient Solvers for Nonlinear PDEs](https://arxiv.org/abs/2510.13772)
*Qiwei Yuan,Zhitong Xu,Yinghao Chen,Yiming Xu,Houman Owhadi,Shandian Zhe*

Main category: cs.LG

TL;DR: TGPS is a tensor-GP-based solver that uses one-dimensional GPs along each input dimension combined via tensor decomposition to efficiently solve PDEs, overcoming scalability issues of traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning PDE solvers face limitations: neural network solvers rely on inefficient stochastic training, while Gaussian process/kernel-based solvers suffer from scalability issues with large collocation sets needed for challenging PDEs.

Method: TGPS models factor functions along each input dimension using one-dimensional GPs and combines them via tensor decomposition. For nonlinear PDEs, it uses partial freezing strategy and Newton's method for linearization, with an alternating least squares approach for efficient training.

Result: The method achieves superior accuracy and efficiency compared to existing approaches on several benchmark PDEs, with theoretical guarantees on expressivity, convergence, and error analysis.

Conclusion: TGPS provides an efficient and scalable alternative to traditional PDE solvers by leveraging tensor decomposition and one-dimensional GPs, substantially reducing computational complexity while maintaining theoretical guarantees.

Abstract: Machine learning solvers for partial differential equations (PDEs) have
attracted growing interest. However, most existing approaches, such as neural
network solvers, rely on stochastic training, which is inefficient and
typically requires a great many training epochs. Gaussian process
(GP)/kernel-based solvers, while mathematical principled, suffer from
scalability issues when handling large numbers of collocation points often
needed for challenging or higher-dimensional PDEs.
  To overcome these limitations, we propose TGPS, a tensor-GP-based solver that
models factor functions along each input dimension using one-dimensional GPs
and combines them via tensor decomposition to approximate the full solution.
This design reduces the task to learning a collection of one-dimensional GPs,
substantially lowering computational complexity, and enabling scalability to
massive collocation sets.
  For efficient nonlinear PDE solving, we use a partial freezing strategy and
Newton's method to linerize the nonlinear terms. We then develop an alternating
least squares (ALS) approach that admits closed-form updates, thereby
substantially enhancing the training efficiency. We establish theoretical
guarantees on the expressivity of our model, together with convergence proof
and error analysis under standard regularity assumptions. Experiments on
several benchmark PDEs demonstrate that our method achieves superior accuracy
and efficiency compared to existing approaches.

</details>


### [190] [UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of Robust Spatial Representations](https://arxiv.org/abs/2510.13774)
*Dominik J. Mühlematter,Lin Che,Ye Hong,Martin Raubal,Nina Wiedemann*

Main category: cs.LG

TL;DR: UrbanFusion is a Geo-Foundation Model that uses Stochastic Multimodal Fusion to integrate diverse geospatial data (street view, remote sensing, maps, POIs) for urban forecasting tasks, achieving superior performance across 41 tasks in 56 cities worldwide.


<details>
  <summary>Details</summary>
Motivation: Current methods use task-specific models and lack multimodal fusion capabilities, while existing foundation models support limited modalities. There's a need for a unified approach that can effectively integrate various geospatial data types for urban forecasting.

Method: Uses modality-specific encoders for different inputs (street view, remote sensing, cartographic maps, POIs) and integrates them via a Transformer-based fusion module that learns unified representations. Features Stochastic Multimodal Fusion (SMF) to flexibly use any subset of available modalities.

Result: Outperforms prior foundation models on location-encoding, allows multimodal input during inference, and generalizes well to regions unseen during training. Demonstrates strong generalization and predictive performance across 41 tasks in 56 cities worldwide.

Conclusion: UrbanFusion provides a flexible and effective Geo-Foundation Model that can handle diverse data availability scenarios, enabling broad applicability in urban forecasting tasks while maintaining strong performance and generalization capabilities.

Abstract: Forecasting urban phenomena such as housing prices and public health
indicators requires the effective integration of various geospatial data.
Current methods primarily utilize task-specific models, while recent foundation
models for spatial representations often support only limited modalities and
lack multimodal fusion capabilities. To overcome these challenges, we present
UrbanFusion, a Geo-Foundation Model (GeoFM) that features Stochastic Multimodal
Fusion (SMF). The framework employs modality-specific encoders to process
different types of inputs, including street view imagery, remote sensing data,
cartographic maps, and points of interest (POIs) data. These multimodal inputs
are integrated via a Transformer-based fusion module that learns unified
representations. An extensive evaluation across 41 tasks in 56 cities worldwide
demonstrates UrbanFusion's strong generalization and predictive performance
compared to state-of-the-art GeoAI models. Specifically, it 1) outperforms
prior foundation models on location-encoding, 2) allows multimodal input during
inference, and 3) generalizes well to regions unseen during training.
UrbanFusion can flexibly utilize any subset of available modalities for a given
location during both pretraining and inference, enabling broad applicability
across diverse data availability scenarios. All source code is available at
https://github.com/DominikM198/UrbanFusion.

</details>


### [191] [T3former: Temporal Graph Classification with Topological Machine Learning](https://arxiv.org/abs/2510.13789)
*Md. Joshem Uddin,Soham Changani,Baris Coskunuzer*

Main category: cs.LG

TL;DR: T3former is a Topological Temporal Transformer that uses sliding-window topological and spectral descriptors as tokens with Descriptor-Attention, achieving state-of-the-art performance in temporal graph classification across various domains.


<details>
  <summary>Details</summary>
Motivation: Temporal graph classification is underexplored compared to other temporal graph tasks. Existing methods lose fine-grained temporal information, struggle with long-range dependencies, and suffer from oversmoothing/oversquashing issues.

Method: Uses sliding-window topological and spectral descriptors as first-class tokens, integrated via specialized Descriptor-Attention mechanism. This preserves temporal fidelity and enables principled cross-modal fusion without rigid discretization.

Result: Achieves state-of-the-art performance across multiple benchmarks including dynamic social networks, brain functional connectivity datasets, and traffic networks. Provides theoretical guarantees of stability under temporal and structural perturbations.

Conclusion: Combining topological and spectral insights advances temporal graph learning frontier, demonstrating the power of this approach for temporal graph classification tasks.

Abstract: Temporal graph classification plays a critical role in applications such as
cybersecurity, brain connectivity analysis, social dynamics, and traffic
monitoring. Despite its significance, this problem remains underexplored
compared to temporal link prediction or node forecasting. Existing methods
often rely on snapshot-based or recurrent architectures that either lose
fine-grained temporal information or struggle with long-range dependencies.
Moreover, local message-passing approaches suffer from oversmoothing and
oversquashing, limiting their ability to capture complex temporal structures.
  We introduce T3former, a novel Topological Temporal Transformer that
leverages sliding-window topological and spectral descriptors as first-class
tokens, integrated via a specialized Descriptor-Attention mechanism. This
design preserves temporal fidelity, enhances robustness, and enables principled
cross-modal fusion without rigid discretization. T3former achieves
state-of-the-art performance across multiple benchmarks, including dynamic
social networks, brain functional connectivity datasets, and traffic networks.
It also offers theoretical guarantees of stability under temporal and
structural perturbations. Our results highlight the power of combining
topological and spectral insights for advancing the frontier of temporal graph
learning.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [192] [Approximate Bilevel Graph Structure Learning for Histopathology Image Classification](https://arxiv.org/abs/2510.13188)
*Sudipta Paul,Amanda W. Lund,George Jour,Iman Osman,Bülent Yener*

Main category: eess.IV

TL;DR: ABiG-Net is a graph-based learning framework for histopathology image analysis that learns optimal patch interactions and node embeddings through hierarchical modeling and bilevel optimization, achieving state-of-the-art performance on colorectal cancer and melanoma classification tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods use fixed graphs with predefined edges, which limits their ability to capture the true biological complexity of tissue interactions in histopathology images.

Method: Hierarchical modeling at local and global scales: local patch-level graphs from cellular orientation, and global image-level graph learning via first-order approximate bilevel optimization that captures sparse, biologically meaningful connections between patches.

Result: On Extended CRC dataset: 97.33±1.15% accuracy for three-class colorectal cancer grading and 98.33±0.58% for binary classification; on melanoma dataset: 96.27±0.74% for tumor-lymphocyte ROI classification.

Conclusion: ABiG-Net effectively unifies local structural information with global contextual relationships, enhancing both interpretability and downstream performance in histopathology image analysis.

Abstract: The structural and spatial arrangements of cells within tissues represent
their functional states, making graph-based learning highly suitable for
histopathology image analysis. Existing methods often rely on fixed graphs with
predefined edges, limiting their ability to capture the true biological
complexity of tissue interactions. In this work, we propose ABiG-Net
(Approximate Bilevel Optimization for Graph Structure Learning via Neural
Networks), a novel framework designed to learn optimal interactions between
patches within whole slide images (WSI) or large regions of interest (ROI)
while simultaneously learning discriminative node embeddings for the downstream
image classification task. Our approach hierarchically models the tissue
architecture at local and global scales. At the local scale, we construct
patch-level graphs from cellular orientation within each patch and extract
features to quantify local structures. At the global scale, we learn an
image-level graph that captures sparse, biologically meaningful connections
between patches through a first-order approximate bilevel optimization
strategy. The learned global graph is optimized in response to classification
performance, capturing the long-range contextual dependencies across the image.
By unifying local structural information with global contextual relationships,
ABiG-Net enhances interpretability and downstream performance. Experiments on
two histopathology datasets demonstrate its effectiveness: on the Extended CRC
dataset, ABiG-Net achieves 97.33 $\pm$ 1.15 % accuracy for three-class
colorectal cancer grading and 98.33 $\pm$ 0.58 % for binary classification; on
the melanoma dataset, it attains 96.27 $\pm$ 0.74 % for tumor-lymphocyte ROI
classification.

</details>


### [193] [DIGITWISE: Digital Twin-based Modeling of Adaptive Video Streaming Engagement](https://arxiv.org/abs/2510.13267)
*Emanuele Artioli,Farzad Tashtarian,Christian Timmerer*

Main category: eess.IV

TL;DR: DIGITWISE is a digital twin-based approach that models individual user engagement in adaptive video streaming, reducing prediction errors by up to 5.8% and increasing engagement by up to 8.6% compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional ABR algorithms assume uniform user reactions to streaming issues, but individual user sensitivities vary significantly. Understanding these differences is crucial for customer loyalty, content personalization, ad relevance, and A/B testing.

Method: Uses digital twins (digital replicas of users) that receive streaming event data and employ supervised machine learning (XGBoost models) to predict engagement. System includes data processing pipeline, individual digital twin models, and unified engagement prediction model.

Result: Reduces user engagement prediction error by up to 5.8% compared to non-user-aware models. Identifies features that maximize engagement, providing average engagement increase of up to 8.6%.

Conclusion: Digital twin-based modeling effectively captures individual user sensitivities in video streaming, enabling more accurate engagement prediction and optimization of content delivery.

Abstract: As the popularity of video streaming entertainment continues to grow,
understanding how users engage with the content and react to its changes
becomes a critical success factor for every stakeholder. User engagement, i.e.,
the percentage of video the user watches before quitting, is central to
customer loyalty, content personalization, ad relevance, and A/B testing. This
paper presents DIGITWISE, a digital twin-based approach for modeling adaptive
video streaming engagement. Traditional adaptive bitrate (ABR) algorithms
assume that all users react similarly to video streaming artifacts and network
issues, neglecting individual user sensitivities. DIGITWISE leverages the
concept of a digital twin, a digital replica of a physical entity, to model
user engagement based on past viewing sessions. The digital twin receives input
about streaming events and utilizes supervised machine learning to predict user
engagement for a given session. The system model consists of a data processing
pipeline, machine learning models acting as digital twins, and a unified model
to predict engagement. DIGITWISE employs the XGBoost model in both digital
twins and unified models. The proposed architecture demonstrates the importance
of personal user sensitivities, reducing user engagement prediction error by up
to 5.8% compared to non-user-aware models. Furthermore, DIGITWISE can optimize
content provisioning and delivery by identifying the features that maximize
engagement, providing an average engagement increase of up to 8.6%.

</details>


### [194] [Semantic Communication Enabled Holographic Video Processing and Transmission](https://arxiv.org/abs/2510.13408)
*Jingkai Ying,Zhiyuan Qi,Yulong Feng,Zhijin Qin,Zhu Han,Rahim Tafazolli,Yonina C. Eldar*

Main category: eess.IV

TL;DR: This paper proposes a semantic-enabled architecture for holographic video communication systems, including key technologies like semantic sampling, joint semantic-channel coding, and semantic-aware transmission, with demonstrated performance gains.


<details>
  <summary>Details</summary>
Motivation: Holographic video communication is emerging as a paradigm shift for immersive visual experiences, requiring new system architectures to handle its unique demands.

Method: The authors present a semantic-enabled architecture with three key technologies: semantic sampling, joint semantic-channel coding, and semantic-aware transmission, validated through two use cases.

Result: The proposed methods demonstrate performance gains in holographic video communication systems, showing the effectiveness of the semantic-enabled approach.

Conclusion: Semantic-enabled holographic video communication shows promise, with identified potential research topics needed to realize its full implementation.

Abstract: Holographic video communication is considered a paradigm shift in visual
communications, becoming increasingly popular for its ability to offer
immersive experiences. This article provides an overview of holographic video
communication and outlines the requirements of a holographic video
communication system. Particularly, following a brief review of semantic com-
munication, an architecture for a semantic-enabled holographic video
communication system is presented. Key technologies, including semantic
sampling, joint semantic-channel coding, and semantic-aware transmission, are
designed based on the proposed architecture. Two related use cases are
presented to demonstrate the performance gain of the proposed methods. Finally,
potential research topics are discussed to pave the way for the realization of
semantic-enabled holographic video communications.

</details>


### [195] [How to Adapt Wireless DJSCC Symbols to Rate Constrained Wired Networks?](https://arxiv.org/abs/2510.13422)
*Jiangyuan Guo,Wei Chen,Yuxuan Sun,Bo Ai*

Main category: eess.IV

TL;DR: Proposes RCWA framework for efficient wired transmission of DJSCC symbols in hybrid wireless-wired networks, achieving redundancy-aware coding and controllable variable-rate transmission to minimize end-to-end distortion.


<details>
  <summary>Details</summary>
Motivation: Existing DJSCC approaches focus on point-to-point wireless scenarios but neglect efficiency in hybrid wireless-wired networks like 5G/6G systems, where redundancy for wireless channels becomes inefficient for wired transmission and symbols must adapt to varying wired network rates.

Method: RCWA framework with redundancy-aware coding that removes wireless channel redundancy and encodes only source-relevant information, plus Lagrangian multiplier method for controllable continuous variable-rate coding that encodes features into expected rates.

Result: Superior rate-distortion performance and robustness compared to baselines, achieving up to 0.7dB PSNR gain over neural network methods and 4dB gain over digital baselines on CIFAR-10 dataset.

Conclusion: RCWA effectively addresses wired transmission efficiency in hybrid networks, demonstrating significant performance improvements and validating its potential for wired resource utilization in hybrid transmission scenarios.

Abstract: Deep joint source-channel coding (DJSCC) has emerged as a robust alternative
to traditional separate coding for communications through wireless channels.
Existing DJSCC approaches focus primarily on point-to-point wireless
communication scenarios, while neglecting end-to-end communication efficiency
in hybrid wireless-wired networks such as 5G and 6G communication systems.
Considerable redundancy in DJSCC symbols against wireless channels becomes
inefficient for long-distance wired transmission. Furthermore, DJSCC symbols
must adapt to the varying transmission rate of the wired network to avoid
congestion. In this paper, we propose a novel framework designed for efficient
wired transmission of DJSCC symbols within hybrid wireless-wired networks,
namely Rate-Controllable Wired Adaptor (RCWA). RCWA achieves redundancy-aware
coding for DJSCC symbols to improve transmission efficiency, which removes
considerable redundancy present in DJSCC symbols for wireless channels and
encodes only source-relevant information into bits. Moreover, we leverage the
Lagrangian multiplier method to achieve controllable and continuous
variable-rate coding, which can encode given features into expected rates,
thereby minimizing end-to-end distortion while satisfying given constraints.
Extensive experiments on diverse datasets demonstrate the superior RD
performance and robustness of RCWA compared to existing baselines, validating
its potential for wired resource utilization in hybrid transmission scenarios.
Specifically, our method can obtain peak signal-to-noise ratio gain of up to
0.7dB and 4dB compared to neural network-based methods and digital baselines on
CIFAR-10 dataset, respectively.

</details>


### [196] [Dedelayed: Deleting remote inference delay via on-device correction](https://arxiv.org/abs/2510.13714)
*Dan Jacobellis,Mateen Ulhaq,Fabien Racapé,Hyomin Choi,Neeraja J. Yadwadkar*

Main category: eess.IV

TL;DR: Dedelayed is a delay-corrective method that mitigates arbitrary remote inference delays by combining lightweight local processing with heavyweight remote model features from past frames, enabling real-time low-latency outputs.


<details>
  <summary>Details</summary>
Motivation: Remote inference causes stale predictions due to communication network latency, making it unsuitable for real-time tasks that require alignment with current world state.

Method: Uses a lightweight local model to process current frame and fuses features from a heavyweight remote model computed from past frames, creating delay-corrective split inference.

Result: Improves semantic segmentation accuracy by 6.4 mIoU over local-only and 9.8 mIoU over remote-only baselines at 100ms delay, with advantages growing under longer delays and higher-motion scenes.

Conclusion: Dedelayed provides clear advantages for real-time tasks by sustaining accuracy more effectively than baseline methods while maintaining alignment with current world state.

Abstract: Remote inference allows lightweight devices to leverage powerful cloud
models. However, communication network latency makes predictions stale and
unsuitable for real-time tasks. To address this, we introduce Dedelayed, a
delay-corrective method that mitigates arbitrary remote inference delays,
allowing the local device to produce low-latency outputs in real time. Our
method employs a lightweight local model that processes the current frame and
fuses in features that a heavyweight remote model computes from past frames. On
video from the BDD100K driving dataset, Dedelayed improves semantic
segmentation accuracy over the stronger of the local-only and remote-only
baselines across all realistic communication network delays beyond 33 ms.
Without incurring additional delay, it improves accuracy by 6.4 mIoU compared
to fully local inference and 9.8 mIoU compared to remote inference, for a
round-trip delay of 100 ms. The advantage grows under longer delays and
higher-motion scenes, as delay-mitigated split inference sustains accuracy more
effectively, providing clear advantages for real-time tasks that must remain
aligned with the current world state.

</details>


### [197] [Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for Medical AI Assistants on the Edge](https://arxiv.org/abs/2510.13760)
*Mikolaj Walczak,Uttej Kallakuri,Edward Humes,Xiaomin Lin,Tinoosh Mohsenin*

Main category: eess.IV

TL;DR: BiTMedViT is an edge-deployable Vision Transformer for medical imaging that uses ternary quantization to achieve 43x model size reduction and 41x energy efficiency while maintaining 86% diagnostic accuracy on MedMNIST datasets.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers show strong medical imaging capabilities but have high computational demands that prevent deployment on resource-constrained mobile and wearable clinical devices.

Method: Uses ternary-quantized linear layers with multi-query attention, task-aware distillation from high-capacity teacher models, and custom CUDA kernel mapping for efficient deployment on Jetson Orin Nano.

Result: Achieves 86% diagnostic accuracy (vs 89% SOTA) on MedMNIST across 12 datasets, with 43x model size reduction, 39x memory traffic reduction, 16.8 ms inference time, and 41x energy efficiency at 183.62 GOPs/J.

Conclusion: Provides a practical route for extreme-precision medical imaging ViTs deployable on edge devices, bridging the gap between algorithmic advances and clinical deployment.

Abstract: Vision Transformers (ViTs) have demonstrated strong capabilities in
interpreting complex medical imaging data. However, their significant
computational and memory demands pose challenges for deployment in real-time,
resource-constrained mobile and wearable devices used in clinical environments.
We introduce, BiTMedViT, a new class of Edge ViTs serving as medical AI
assistants that perform structured analysis of medical images directly on the
edge. BiTMedViT utilizes ternary- quantized linear layers tailored for medical
imaging and com- bines a training procedure with multi-query attention,
preserving stability under ternary weights with low-precision activations.
Furthermore, BiTMedViT employs task-aware distillation from a high-capacity
teacher to recover accuracy lost due to extreme quantization. Lastly, we also
present a pipeline that maps the ternarized ViTs to a custom CUDA kernel for
efficient memory bandwidth utilization and latency reduction on the Jetson Orin
Nano. Finally, BiTMedViT achieves 86% diagnostic accuracy (89% SOTA) on
MedMNIST across 12 datasets, while reducing model size by 43x, memory traffic
by 39x, and enabling 16.8 ms inference at an energy efficiency up to 41x that
of SOTA models at 183.62 GOPs/J on the Orin Nano. Our results demonstrate a
practical and scientifically grounded route for extreme-precision medical
imaging ViTs deployable on the edge, narrowing the gap between algorithmic
advances and deployable clinical tools.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [198] [On the performance of Active STAR-RIS-Assisted Cell-Free Massive MIMO Systems with Phase Errors and Channel Aging](https://arxiv.org/abs/2510.13171)
*Jun Qian,Ross Murch,Khaled B. Letaief*

Main category: cs.IT

TL;DR: Analysis of phase errors and channel aging in active STAR-RIS-assisted cell-free massive MIMO systems, showing that active STAR-RISs can compensate for these effects and providing design guidelines.


<details>
  <summary>Details</summary>
Motivation: To overcome attenuation in RIS cascaded links and analyze the combined effects of phase errors and channel aging in active STAR-RIS systems.

Method: Leveraged spatially correlated Rayleigh fading model, derived MMSE-based channel estimates, and formulated closed-form expressions for downlink spectral efficiency.

Result: Active STAR-RISs effectively compensate for phase errors and channel aging; increasing APs, STAR-RIS elements, and amplification factor can alleviate performance degradation.

Conclusion: Active STAR-RISs are effective against phase errors and channel aging, with practical resource-block-length design guidelines proposed to counter channel aging effects.

Abstract: Active reconfigurable intelligent surfaces (RISs) employ amplification to
overcome attenuation caused by the RIS cascaded link. In this paper, we analyze
the effects of phase errors and channel aging in active simultaneously
transmitting and reflecting (STAR) RIS-assisted cell-free massive
multiple-input multiple-output (MIMO) systems. By leveraging a spatially
correlated Rayleigh fading model, this paper derives minimum mean square error
estimate-based channel estimates and formulates closed-form expressions for
downlink spectral efficiency. This analytical framework enables a comprehensive
evaluation of the effects of channel aging and uniformly distributed phase
errors on system performance. The results demonstrate that active STAR-RISs can
effectively compensate for the adverse effects of phase errors and channel
aging. To counteract the impact of channel aging, we propose practical
guidelines for resource-block-length design. Also, an increase in APs and
STAR-RIS elements, along with a larger amplification factor, can alleviate
performance degradation.

</details>


### [199] [A Dimension-Keeping Semi-Tensor Product Framework for Compressed Sensing](https://arxiv.org/abs/2510.13180)
*Qi Qi,Abdelhamid Tayebi,Daizhan Cheng,Jun-e Feng*

Main category: cs.IT

TL;DR: Proposes DK-STP-CS method that enhances compressed sensing by leveraging intra-group correlations while maintaining inter-group incoherence in measurement matrix design, achieving better image reconstruction than traditional CS approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional compressed sensing frameworks rely on measurement matrix incoherence, but don't effectively utilize signal correlations. The paper aims to improve CS performance by designing better measurement matrices that exploit signal structure.

Method: Integrates Dimension-Keeping Semi-Tensor Product (DK-STP) algorithm into sensing matrix design, enabling dimensionality reduction while preserving recovery capability. Leverages intra-group correlations while maintaining inter-group incoherence.

Result: Significantly outperforms traditional CS and STP-CS approaches with higher PSNR values. Achieves notable noise suppression and improved visual fidelity in image reconstruction. Shows robustness under noisy conditions and varying sampling rates.

Conclusion: DK-STP-CS is a promising approach for practical compressed sensing applications in resource-constrained environments, offering enhanced reconstruction quality and noise resilience compared to existing methods.

Abstract: In compressed sensing (CS), sparse signals can be reconstructed from
significantly fewer samples than required by the Nyquist-Shannon sampling
theorem. While non-sparse signals can be sparsely represented in appropriate
transformation domains, conventional CS frameworks rely on the incoherence of
the measurement matrix columns to guarantee reconstruction performance. This
paper proposes a novel method termed Dimension-Keeping Semi-Tensor Product
Compressed Sensing (DK-STP-CS), which leverages intra-group correlations while
maintaining inter-group incoherence to enhance the measurement matrix design.
Specifically, the DK-STP algorithm is integrated into the design of the sensing
matrix, enabling dimensionality reduction while preserving signal recovery
capability. For image compression and reconstruction tasks, the proposed method
achieves notable noise suppression and improves visual fidelity. Experimental
results demonstrate that DK-STP-CS significantly outperforms traditional CS and
STP-CS approaches, as evidenced by higher Peak Signal-to-Noise Ratio (PSNR)
values between the reconstructed and original images. The robustness of
DK-STP-CS is further validated under noisy conditions and varying sampling
rates, highlighting its potential for practical applications in
resource-constrained environments.

</details>


### [200] [Movable and Reconfigurable Antennas for 6G: Unlocking Electromagnetic-Domain Design and Optimization](https://arxiv.org/abs/2510.13209)
*Lipeng Zhu,Haobin Mao,Ge Yan,Wenyan Ma,Zhenyu Xiao,Rui Zhang*

Main category: cs.IT

TL;DR: Overview of movable and reconfigurable antennas for 6G networks, highlighting their benefits over traditional fixed antennas through dynamic control of electromagnetic properties.


<details>
  <summary>Details</summary>
Motivation: The growing demands of 6G mobile communication networks require advanced antenna technologies to enhance wireless system performance.

Method: The article examines application scenarios, hardware architectures, and design methods for movable and reconfigurable antennas, using field tests and simulations for evaluation.

Result: Field test and simulation results demonstrate performance benefits of movable and reconfigurable antennas over conventional fixed/non-reconfigurable antennas.

Conclusion: Movable and reconfigurable antennas provide rich electromagnetic-domain degrees of freedom that significantly enhance wireless system design and performance for 6G networks.

Abstract: The growing demands of 6G mobile communication networks necessitate advanced
antenna technologies. Movable antennas (MAs) and reconfigurable antennas (RAs)
enable dynamic control over antenna's position, orientation, radiation,
polarization, and frequency response, introducing rich electromagnetic-domain
degrees of freedom for the design and performance enhancement of wireless
systems. This article overviews their application scenarios, hardware
architectures, and design methods. Field test and simulation results highlight
their performance benefits over conventional fixed/non-reconfigurable antennas.

</details>


### [201] [Non-Linear Precoding via Dirty Paper Coding for Near-Field Downlink MISO Communications](https://arxiv.org/abs/2510.13485)
*Akash Kulkarni,Rajshekhar V Bhat*

Main category: cs.IT

TL;DR: Proposes nonlinear DPC precoding for 6G near-field communication to overcome ZF limitations, achieving significant sum-rate gains especially for closely spaced users.


<details>
  <summary>Details</summary>
Motivation: Existing linear precoding techniques like ZF in near-field communication suffer from performance degradation due to high transmit power requirements for interference suppression.

Method: Develops a nonlinear precoding framework based on Dirty Paper Coding (DPC) that pre-cancels known interference, with optimal power allocation strategies for both DPC and ZF schemes.

Result: DPC achieves substantial sum-rate gains over ZF across various near-field configurations, with most pronounced improvements for closely spaced users.

Conclusion: DPC-based nonlinear precoding is superior to linear ZF for 6G near-field communication systems, offering significant performance improvements particularly in dense user scenarios.

Abstract: In 6G systems, extremely large-scale antenna arrays operating at terahertz
frequencies extend the near-field region to typical user distances from the
base station, enabling near-field communication (NFC) with fine spatial
resolution through beamfocusing. Existing multiuser NFC systems predominantly
employ linear precoding techniques such as zero-forcing (ZF), which suffer from
performance degradation due to the high transmit power required to suppress
interference. This paper proposes a nonlinear precoding framework based on
Dirty Paper Coding (DPC), which pre-cancels known interference to maximize the
sum-rate performance. We formulate and solve the corresponding sum-rate
maximization problems, deriving optimal power allocation strategies for both
DPC and ZF schemes. Extensive simulations demonstrate that DPC achieves
substantial sum-rate gains over ZF across various near-field configurations,
with the most pronounced improvements observed for closely spaced users.

</details>


### [202] [Simulating Mediumband Wireless Communication Systems: A Concise Description](https://arxiv.org/abs/2510.13532)
*Dushyantha A Basnayaka*

Main category: cs.IT

TL;DR: This paper provides MATLAB simulation procedures for mediumband digital wireless communication systems, focusing on detailed PHY layer operations that are often simplified in conventional simulations.


<details>
  <summary>Details</summary>
Motivation: Conventional wireless communication simulations often ignore key PHY operations like pulse shaping, upconversion, and synchronization, which are insufficient for accurately capturing mediumband communication dynamics including deep fading effects.

Method: Describes detailed MATLAB simulation procedures for a single TX to single RX mediumband wireless communication scenario, elaborating key PHY subsystems operations that are typically simplified.

Result: The proposed simulation approach successfully captures the delicate dynamics of mediumband wireless communication, including deep fading avoidance effects that are missed in conventional simulations.

Conclusion: Accurate simulation of mediumband wireless communication requires detailed modeling of PHY layer operations beyond the typical discrete-time complex baseband approach, and the described MATLAB procedures provide this capability for both beginners and experts.

Abstract: In this paper, we describe the necessary procedures for accurately simulating
digital wireless communication systems operating in the mediumband, aimed at
both beginners and experts. In the research literature, digital wireless
communication systems are typically simulated in the discrete-time complex
baseband domain, where pulse shaping, upconversion, mixing, carrier
synchronization, and symbol timing synchronization are often ignored. These
assumptions are indeed sufficient in most cases, but to capture the essence of
communication in the mediumband, certain physical layer (PHY) operations should
be simulated in detail. In this paper, we concisely describe how to simulate a
mediumband wireless communication scenario from a single transmitter (TX) to a
single receiver (RX) in MATLAB, elaborating the operation of key PHY
subsystems. The approach described here ensures that the simulated system
captures the delicate dynamics of mediumband wireless communication, including
the effect of deep fading avoidance.

</details>


### [203] [Local Information-Theoretic Security via Euclidean Geometry](https://arxiv.org/abs/2510.13661)
*Emmanouil M. Athanasakos,Nicholas Kalouptsidis,Hariprasad Manjunath*

Main category: cs.IT

TL;DR: A Euclidean information theory framework for analyzing local properties of secure communication over wiretap channels, transforming non-convex optimization into tractable quadratic programming to derive approximate local secrecy capacity and new local contraction coefficients.


<details>
  <summary>Details</summary>
Motivation: To investigate local properties of secure communication over discrete memoryless wiretap channels by developing a methodology that can handle the inherent non-convexity of information-theoretic security problems.

Method: Formulated a constrained optimization problem maximizing legitimate user's information rate while bounding information leakage and encoding cost. Used local geometric approximations to transform the non-convex problem into quadratic programming, with Lagrange multipliers found via linear programming derived from KKT conditions and generalized eigenvalues.

Result: Derived analytical formula for approximate local secrecy capacity and defined new class of secret local contraction coefficients that quantify maximum achievable ratio of utility to leakage. Established bounds connecting local coefficients to global counterparts.

Conclusion: The proposed Euclidean information theory framework effectively analyzes local security properties of wiretap channels, providing tractable solutions and new theoretical insights through local contraction coefficients, with demonstrated efficacy across various channel types.

Abstract: This paper introduces a methodology based on Euclidean information theory to
investigate local properties of secure communication over discrete memoryless
wiretap channels. We formulate a constrained optimization problem that
maximizes a legitimate user's information rate while imposing explicit upper
bounds on both the information leakage to an eavesdropper and the informational
cost of encoding the secret message. By leveraging local geometric
approximations, this inherently non-convex problem is transformed into a
tractable quadratic programming structure. It is demonstrated that the optimal
Lagrange multipliers governing this approximated problem can be found by
solving a linear program. The constraints of this linear program are derived
from Karush-Kuhn-Tucker conditions and are expressed in terms of the
generalized eigenvalues of channel-derived matrices. This framework facilitates
the derivation of an analytical formula for an approximate local secrecy
capacity. Furthermore, we define and analyze a new class of secret local
contraction coefficients. These coefficients, characterized as the largest
generalized eigenvalues of a matrix pencil, quantify the maximum achievable
ratio of approximate utility to approximate leakage, thus measuring the
intrinsic local leakage efficiency of the channel. We establish bounds
connecting these local coefficients to their global counterparts defined over
true mutual information measures. The efficacy of the proposed framework is
demonstrated through detailed analysis and numerical illustrations for both
general multi-mode channels and the canonical binary symmetric wiretap channel.

</details>


### [204] [Combinatorial Bounds for List Recovery via Discrete Brascamp--Lieb Inequalities](https://arxiv.org/abs/2510.13775)
*Joshua Brakensiek,Yeyuan Chen,Manik Dhar,Zihan Zhang*

Main category: cs.IT

TL;DR: Novel combinatorial bounds on list recovery for linear codes show that when approaching capacity (ρ = 1 - R - ε), list size L is at most (ℓ/(R+ε))^{O(R/ε)}, resolving whether L can be polynomially bounded in ℓ.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental limits of list recovery in coding theory, particularly bounding the maximum list size L for various code families when approaching capacity.

Method: Application of a discrete entropic Brascamp-Lieb inequality to relate local coordinate structure with global recovered list structure, generalizing recent results on folded Reed-Solomon codes.

Result: For codes of rate R with ρ = 1 - R - ε, list size L ≤ (ℓ/(R+ε))^{O(R/ε)} for random linear codes, Reed-Solomon codes, folded Reed-Solomon codes, and univariate multiplicity codes.

Conclusion: The paper resolves the long-standing question about polynomial bounding of list size in ℓ and provides optimal bounds in the zero-error regime that match known lower bounds.

Abstract: In coding theory, the problem of list recovery asks one to find all codewords
$c$ of a given code $C$ which such that at least $1-\rho$ fraction of the
symbols of $c$ lie in some predetermined set of $\ell$ symbols for each
coordinate of the code. A key question is bounding the maximum possible list
size $L$ of such codewords for the given code $C$.
  In this paper, we give novel combinatorial bounds on the list recoverability
of various families of linear and folded linear codes, including random linear
codes, random Reed--Solomon codes, explicit folded Reed--Solomon codes, and
explicit univariate multiplicity codes. Our main result is that in all of these
settings, we show that for code of rate $R$, when $\rho = 1 - R - \epsilon$
approaches capacity, the list size $L$ is at most
$(\ell/(R+\epsilon))^{O(R/\epsilon)}$. These results also apply in the
average-radius regime. Our result resolves a long-standing open question on
whether $L$ can be bounded by a polynomial in $\ell$. In the zero-error regime,
our bound on $L$ perfectly matches known lower bounds.
  The primary technique is a novel application of a discrete entropic
Brascamp--Lieb inequality to the problem of list recovery, allowing us to
relate the local structure of each coordinate with the global structure of the
recovered list. As a result of independent interest, we show that a recent
result by Chen and Zhang (STOC 2025) on the list decodability of folded
Reed--Solomon codes can be generalized into a novel Brascamp--Lieb type
inequality.

</details>


### [205] [From Random to Explicit via Subspace Designs With Applications to Local Properties and Matroids](https://arxiv.org/abs/2510.13777)
*Joshua Brakensiek,Yeyuan Chen,Manik Dhar,Zihan Zhang*

Main category: cs.IT

TL;DR: This paper extends the local property analysis framework to subspace designable codes, establishes equivalence between random linear codes and subspace design codes, and applies results to matroid theory with improved computational complexity.


<details>
  <summary>Details</summary>
Motivation: To understand threshold rates of local properties for subspace designable codes and bridge the gap between random codes and explicit constructions, while also advancing applications in matroid theory.

Method: Extends the unified framework from previous work to study local properties of subspace designable codes, establishes local equivalence between random linear codes and subspace design codes, and applies results to matroid theory.

Result: Shows local equivalence between random linear codes and optimal subspace design codes, provides first explicit folded linear codes with all local properties of random codes, and improves computational complexity for matroid problems.

Conclusion: The framework successfully extends to subspace design codes, enabling explicit constructions with optimal local properties while advancing applications in coding theory and matroid theory.

Abstract: In coding theory, a common question is to understand the threshold rates of
various local properties of codes, such as their list decodability and list
recoverability. A recent work Levi, Mosheiff, and Shagrithaya (FOCS 2025) gave
a novel unified framework for calculating the threshold rates of local
properties for random linear and random Reed--Solomon codes.
  In this paper, we extend their framework to studying the local properties of
subspace designable codes, including explicit folded Reed-Solomon and
univariate multiplicity codes. Our first main result is a local equivalence
between random linear codes and (nearly) optimal subspace design codes up to an
arbitrarily small rate decrease. We show any local property of random linear
codes applies to all subspace design codes. As such, we give the first explicit
construction of folded linear codes that simultaneously attain all local
properties of random linear codes. Conversely, we show that any local property
which applies to all subspace design codes also applies to random linear codes.
  Our second main result is an application to matroid theory. We show that the
correctable erasure patterns in a maximally recoverable tensor code can be
identified in deterministic polynomial time, assuming a positive answer to a
matroid-theoretic question due to Mason (1981). This improves on a result of
Jackson and Tanigawa (JCTB 2024) who gave a complexity characterization of
$\mathsf{RP} \cap \mathsf{coNP}$ assuming a stronger conjecture. Our result
also applies to the generic bipartite rigidity and matrix completion matroids.
  As a result of additional interest, we study the existence and limitations of
subspace designs. In particular, we tighten the analysis of family of subspace
designs constructioned by Guruswami and Kopparty (Combinatorica 2016) and show
that better subspace designs do not exist over algebraically closed fields.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [206] [From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models](https://arxiv.org/abs/2510.12864)
*Imran Khan*

Main category: cs.AI

TL;DR: The RID Framework is a zero-shot meta-prompting technique that helps LLMs overcome rule-rigidity by enabling human-aligned exception handling without expensive fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LLMs exhibit rigid adherence to explicit rules that leads to decisions misaligned with human common sense and intent, creating a barrier to trustworthy autonomous agents. Existing SFT solutions are computationally expensive and inaccessible.

Method: The Rule-Intent Distinction (RID) Framework - a low-compute meta-prompting technique that provides structured cognitive schema for deconstructing tasks, classifying rules, weighing conflicting outcomes, and justifying decisions.

Result: Achieved 95% Human Alignment Score compared to 80% for baseline and 75% for Chain-of-Thought prompting, with consistently higher-quality, intent-driven reasoning across 20 diverse scenarios.

Conclusion: The RID framework presents a practical, accessible method for steering LLMs from literal instruction-following to goal-oriented reasoning, enabling more reliable AI agents.

Abstract: Large Language Models (LLMs) are increasingly being deployed as the reasoning
engines for agentic AI systems, yet they exhibit a critical flaw: a rigid
adherence to explicit rules that leads to decisions misaligned with human
common sense and intent. This "rule-rigidity" is a significant barrier to
building trustworthy autonomous agents. While prior work has shown that
supervised fine-tuning (SFT) with human explanations can mitigate this issue,
SFT is computationally expensive and inaccessible to many practitioners. To
address this gap, we introduce the Rule-Intent Distinction (RID) Framework, a
novel, low-compute meta-prompting technique designed to elicit human-aligned
exception handling in LLMs in a zero-shot manner. The RID framework provides
the model with a structured cognitive schema for deconstructing tasks,
classifying rules, weighing conflicting outcomes, and justifying its final
decision. We evaluated the RID framework against baseline and Chain-of-Thought
(CoT) prompting on a custom benchmark of 20 scenarios requiring nuanced
judgment across diverse domains. Our human-verified results demonstrate that
the RID framework significantly improves performance, achieving a 95% Human
Alignment Score (HAS), compared to 80% for the baseline and 75% for CoT.
Furthermore, it consistently produces higher-quality, intent-driven reasoning.
This work presents a practical, accessible, and effective method for steering
LLMs from literal instruction-following to liberal, goal-oriented reasoning,
paving the way for more reliable and pragmatic AI agents.

</details>


### [207] [DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage Shaping](https://arxiv.org/abs/2510.12979)
*Wei Fan,Wenlin Yao,Zheng Li,Feng Yao,Xin Liu,Liang Qiu,Qingyu Yin,Yangqiu Song,Bing Yin*

Main category: cs.AI

TL;DR: DeepPlanner is an RL framework that improves planning in language models by addressing high entropy in planning tokens through entropy-based advantage shaping and selective sample weighting.


<details>
  <summary>Details</summary>
Motivation: Existing approaches fail to systematically optimize the planning stage in language models, leading to uncertain decision points with high entropy in planning tokens under vanilla RL.

Method: Proposes DeepPlanner with token-level advantage shaping using entropy-based terms to prioritize high-entropy tokens, and selective sample-level advantage weighting for planning-intensive rollouts.

Result: Extensive experiments across seven deep research benchmarks show improved planning quality and state-of-the-art results with substantially lower training budget.

Conclusion: DeepPlanner effectively enhances planning capabilities in language models by systematically addressing planning optimization through entropy-aware RL techniques.

Abstract: Large language models (LLMs) augmented with multi-step reasoning and action
generation abilities have shown promise in leveraging external tools to tackle
complex tasks that require long-horizon planning. However, existing approaches
either rely on implicit planning in the reasoning stage or introduce explicit
planners without systematically addressing how to optimize the planning stage.
As evidence, we observe that under vanilla reinforcement learning (RL),
planning tokens exhibit significantly higher entropy than other action tokens,
revealing uncertain decision points that remain under-optimized. To address
this, we propose DeepPlanner, an end-to-end RL framework that effectively
enhances the planning capabilities of deep research agents. Our approach shapes
token-level advantage with an entropy-based term to allocate larger updates to
high entropy tokens, and selectively upweights sample-level advantages for
planning-intensive rollouts. Extensive experiments across seven deep research
benchmarks demonstrate that DeepPlanner improves planning quality and achieves
state-of-the-art results under a substantially lower training budget.

</details>


### [208] [SENTINEL: A Multi-Level Formal Framework for Safety Evaluation of LLM-based Embodied Agents](https://arxiv.org/abs/2510.12985)
*Simon Sinong Zhan,Yao Liu,Philip Wang,Zinan Wang,Qineng Wang,Zhian Ruan,Xiangyu Shi,Xinyu Cao,Frank Yang,Kangrui Wang,Huajie Shao,Manling Li,Qi Zhu*

Main category: cs.AI

TL;DR: Sentinel is a framework for formally evaluating physical safety of LLM-based embodied agents using temporal logic verification across semantic, plan, and trajectory levels.


<details>
  <summary>Details</summary>
Motivation: Prior methods rely on heuristic rules or subjective LLM judgments, lacking formal grounding for precise safety evaluation in physical environments.

Method: Multi-level verification pipeline: semantic level (formalizing requirements into temporal logic), plan level (verifying action plans), and trajectory level (verifying execution trajectories).

Result: Applied in VirtualHome and ALFRED, Sentinel exposed safety violations overlooked by previous methods and provided insights into failure modes of LLM agents.

Conclusion: Sentinel provides a rigorous foundation for systematically evaluating LLM-based embodied agents' physical safety through formal temporal logic verification.

Abstract: We present Sentinel, the first framework for formally evaluating the physical
safety of Large Language Model(LLM-based) embodied agents across the semantic,
plan, and trajectory levels. Unlike prior methods that rely on heuristic rules
or subjective LLM judgments, Sentinel grounds practical safety requirements in
formal temporal logic (TL) semantics that can precisely specify state
invariants, temporal dependencies, and timing constraints. It then employs a
multi-level verification pipeline where (i) at the semantic level, intuitive
natural language safety requirements are formalized into TL formulas and the
LLM agent's understanding of these requirements is probed for alignment with
the TL formulas; (ii) at the plan level, high-level action plans and subgoals
generated by the LLM agent are verified against the TL formulas to detect
unsafe plans before execution; and (iii) at the trajectory level, multiple
execution trajectories are merged into a computation tree and efficiently
verified against physically-detailed TL specifications for a final safety
check. We apply Sentinel in VirtualHome and ALFRED, and formally evaluate
multiple LLM-based embodied agents against diverse safety requirements. Our
experiments show that by grounding physical safety in temporal logic and
applying verification methods across multiple levels, Sentinel provides a
rigorous foundation for systematically evaluating LLM-based embodied agents in
physical environments, exposing safety violations overlooked by previous
methods and offering insights into their failure modes.

</details>


### [209] [From Narratives to Probabilistic Reasoning: Predicting and Interpreting Drivers' Hazardous Actions in Crashes Using Large Language Model](https://arxiv.org/abs/2510.13002)
*Boyou Chen,Gerui Xu,Zifei Wang,Huizhong Guo,Ananna Ahmed,Zhaonan Sun,Zhen Hu,Kaihan Zhang,Shan Bao*

Main category: cs.AI

TL;DR: A framework using fine-tuned Llama 3.2 1B model to automatically identify Driver Hazardous Actions (DHAs) from crash narratives, achieving 80% accuracy and outperforming traditional ML models, with probabilistic reasoning for interpretability.


<details>
  <summary>Details</summary>
Motivation: Manual coding of Driver Hazardous Actions in crash databases is inconsistent and labor-intensive, limiting reliability of DHA data for understanding crash causation in the prevalent two-vehicle crashes (70% of roadway crashes).

Method: Fine-tuned Llama 3.2 1B model on five years of two-vehicle crash narratives from MTCF, benchmarked against Random Forest, XGBoost, CatBoost, and neural network classifiers, with probabilistic reasoning for interpretability across counterfactual scenarios.

Result: Fine-tuned LLM achieved 80% overall accuracy, surpassing all baseline models, with pronounced improvements in imbalanced data scenarios. Probabilistic analysis showed distraction increases 'General Unsafe Driving' probability, mutual distraction maximizes 'Both Drivers Took Hazardous Actions', and teen drivers elevate 'Speed and Stopping Violations' probability.

Conclusion: The framework provides robust and interpretable automated DHA detection for large-scale traffic safety analysis, offering new opportunities for intervention and improved crash causation understanding.

Abstract: Vehicle crashes involve complex interactions between road users, split-second
decisions, and challenging environmental conditions. Among these, two-vehicle
crashes are the most prevalent, accounting for approximately 70% of roadway
crashes and posing a significant challenge to traffic safety. Identifying
Driver Hazardous Action (DHA) is essential for understanding crash causation,
yet the reliability of DHA data in large-scale databases is limited by
inconsistent and labor-intensive manual coding practices. Here, we present an
innovative framework that leverages a fine-tuned large language model to
automatically infer DHAs from textual crash narratives, thereby improving the
validity and interpretability of DHA classifications. Using five years of
two-vehicle crash data from MTCF, we fine-tuned the Llama 3.2 1B model on
detailed crash narratives and benchmarked its performance against conventional
machine learning classifiers, including Random Forest, XGBoost, CatBoost, and a
neural network. The fine-tuned LLM achieved an overall accuracy of 80%,
surpassing all baseline models and demonstrating pronounced improvements in
scenarios with imbalanced data. To increase interpretability, we developed a
probabilistic reasoning approach, analyzing model output shifts across original
test sets and three targeted counterfactual scenarios: variations in driver
distraction and age. Our analysis revealed that introducing distraction for one
driver substantially increased the likelihood of "General Unsafe Driving";
distraction for both drivers maximized the probability of "Both Drivers Took
Hazardous Actions"; and assigning a teen driver markedly elevated the
probability of "Speed and Stopping Violations." Our framework and analytical
methods provide a robust and interpretable solution for large-scale automated
DHA detection, offering new opportunities for traffic safety analysis and
intervention.

</details>


### [210] [Toward Reasoning-Centric Time-Series Analysis](https://arxiv.org/abs/2510.13029)
*Xinlei Wang,Mingtian Tan,Jing Qiu,Junhua Zhao,Jinjin Gu*

Main category: cs.AI

TL;DR: The paper advocates for using LLMs in time series analysis as reasoning tools for causal structure and explainability, rather than just for numerical regression.


<details>
  <summary>Details</summary>
Motivation: Traditional time series analysis fails in real-world dynamic settings where policies shift and unexpected events occur. LLMs offer new opportunities but current methods misuse them for numerical regression instead of leveraging their reasoning potential.

Method: Proposes rethinking time series analysis with LLMs as a reasoning task that prioritizes causal structure and explainability, integrating multimodal inputs.

Result: Shifts time series analysis toward human-aligned understanding, enabling transparent and context-aware insights in complex real-world environments.

Conclusion: LLMs should be used for their reasoning capabilities in time series analysis to uncover actual driving forces behind trends, moving beyond surface-level pattern recognition.

Abstract: Traditional time series analysis has long relied on pattern recognition,
trained on static and well-established benchmarks. However, in real-world
settings -- where policies shift, human behavior adapts, and unexpected events
unfold -- effective analysis must go beyond surface-level trends to uncover the
actual forces driving them. The recent rise of Large Language Models (LLMs)
presents new opportunities for rethinking time series analysis by integrating
multimodal inputs. However, as the use of LLMs becomes popular, we must remain
cautious, asking why we use LLMs and how to exploit them effectively. Most
existing LLM-based methods still employ their numerical regression ability and
ignore their deeper reasoning potential. This paper argues for rethinking time
series with LLMs as a reasoning task that prioritizes causal structure and
explainability. This shift brings time series analysis closer to human-aligned
understanding, enabling transparent and context-aware insights in complex
real-world environments.

</details>


### [211] [Repairing Reward Functions with Human Feedback to Mitigate Reward Hacking](https://arxiv.org/abs/2510.13036)
*Stephane Hatgis-Kessell,Logan Mondal Bhamidipaty,Emma Brunskill*

Main category: cs.AI

TL;DR: PBRR is an automated framework that repairs human-designed proxy reward functions by learning additive corrections from human preferences, addressing reward misalignment with fewer preferences than learning from scratch.


<details>
  <summary>Details</summary>
Motivation: Human-designed reward functions are often misaligned with true objectives, causing reward hacking, while learning rewards from preferences is costly. PBRR aims to combine the benefits of both approaches.

Method: PBRR learns an additive, transition-dependent correction term to repair proxy reward functions using targeted exploration and a new preference-learning objective from human preferences over trajectory pairs.

Result: PBRR matches prior preference-based RL methods' regret bounds in tabular domains and outperforms baselines on reward-hacking benchmarks, requiring substantially fewer preferences to learn high-performing policies.

Conclusion: PBRR effectively repairs misaligned reward functions using fewer human preferences than learning from scratch, providing a practical solution to reward misalignment in RL.

Abstract: Human-designed reward functions for reinforcement learning (RL) agents are
frequently misaligned with the humans' true, unobservable objectives, and thus
act only as proxies. Optimizing for a misspecified proxy reward function often
induces reward hacking, resulting in a policy misaligned with the human's true
objectives. An alternative is to perform RL from human feedback, which involves
learning a reward function from scratch by collecting human preferences over
pairs of trajectories. However, building such datasets is costly. To address
the limitations of both approaches, we propose Preference-Based Reward Repair
(PBRR): an automated iterative framework that repairs a human-specified proxy
reward function by learning an additive, transition-dependent correction term
from preferences. A manually specified reward function can yield policies that
are highly suboptimal under the ground-truth objective, yet corrections on only
a few transitions may suffice to recover optimal performance. To identify and
correct for those transitions, PBRR uses a targeted exploration strategy and a
new preference-learning objective. We prove in tabular domains PBRR has a
cumulative regret that matches, up to constants, that of prior preference-based
RL methods. In addition, on a suite of reward-hacking benchmarks, PBRR
consistently outperforms baselines that learn a reward function from scratch
from preferences or modify the proxy reward function using other approaches,
requiring substantially fewer preferences to learn high performing policies.

</details>


### [212] [Emotional Cognitive Modeling Framework with Desire-Driven Objective Optimization for LLM-empowered Agent in Social Simulation](https://arxiv.org/abs/2510.13195)
*Qun Ma,Xiao Xue,Xuwen Zhang,Zihan Zhao,Yuwei Guo,Ming Zhang*

Main category: cs.AI

TL;DR: This paper proposes an emotional cognition framework for LLM-based agents that incorporates desire generation and objective management to achieve emotion alignment with humans, enabling more realistic social simulations.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based agents have severe limitations in affective cognition - they fail to simulate bounded rationality essential for bridging virtual and real-world services, and lack empirically validated integration mechanisms for embedding emotions within decision architectures.

Method: Constructed an emotional cognition framework incorporating desire generation and objective management, modeling the complete decision-making process including state evolution, desire generation, objective optimization, decision generation, and action execution. Implemented within a proprietary multi-agent interaction environment.

Result: Agents governed by the framework exhibit behaviors congruent with their emotional states, demonstrate superior ecological validity, and generate decision outcomes that significantly more closely approximate human behavioral patterns compared to other agent types.

Conclusion: The proposed emotional cognition framework successfully enables LLM-based agents to achieve emotion alignment with humans, addressing key limitations in affective cognition and producing more realistic social simulations.

Abstract: The advent of large language models (LLMs) has enabled agents to represent
virtual humans in societal simulations, facilitating diverse interactions
within complex social systems. However, existing LLM-based agents exhibit
severe limitations in affective cognition: They fail to simulate the bounded
rationality essential for bridging virtual and real-world services; They lack
empirically validated integration mechanisms embedding emotions within agent
decision architectures. This paper constructs an emotional cognition framework
incorporating desire generation and objective management, designed to achieve
emotion alignment between LLM-based agents and humans, modeling the complete
decision-making process of LLM-based agents, encompassing state evolution,
desire generation, objective optimization, decision generation, and action
execution. This study implements the proposed framework within our proprietary
multi-agent interaction environment. Experimental results demonstrate that
agents governed by our framework not only exhibit behaviors congruent with
their emotional states but also, in comparative assessments against other agent
types, demonstrate superior ecological validity and generate decision outcomes
that significantly more closely approximate human behavioral patterns.

</details>


### [213] [Adaptive Reasoning Executor: A Collaborative Agent System for Efficient Reasoning](https://arxiv.org/abs/2510.13214)
*Zehui Ling,Deshu Chen,Yichi Zhang,Yuchen Liu,Xigui Li,Xin Guo,Yuan Cheng*

Main category: cs.AI

TL;DR: A complementary agent system that uses small LLMs for initial answers and large LLMs for verification and deep reasoning only when needed, reducing computational costs by over 50% for simple problems with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To address the computational expense of applying deep reasoning and chain-of-thought prompting to all problems in multi-agent systems, while maintaining performance on complex tasks.

Method: Proposes a complementary agent system where a small LLM generates initial answers, which are verified by a large LLM. If correct, the answer is adopted; otherwise, the large LLM performs in-depth reasoning.

Result: Experimental results show over 50% reduction in computational cost for large LLMs on simple problems with negligible accuracy loss, while maintaining robust performance on complex tasks.

Conclusion: The proposed complementary agent system effectively balances computational efficiency and performance by strategically deploying large LLMs only when necessary.

Abstract: Recent advances in Large Language Models (LLMs) demonstrate that
chain-of-thought prompting and deep reasoning substantially enhance performance
on complex tasks, and multi-agent systems can further improve accuracy by
enabling model debates. However, applying deep reasoning to all problems is
computationally expensive. To mitigate these costs, we propose a complementary
agent system integrating small and large LLMs. The small LLM first generates an
initial answer, which is then verified by the large LLM. If correct, the answer
is adopted directly; otherwise, the large LLM performs in-depth reasoning.
Experimental results show that, for simple problems, our approach reduces the
computational cost of the large LLM by more than 50% with negligible accuracy
loss, while consistently maintaining robust performance on complex tasks.

</details>


### [214] [Personalized Learning Path Planning with Goal-Driven Learner State Modeling](https://arxiv.org/abs/2510.13215)
*Joy Jia Yin Lim,Ye He,Jifan Yu,Xin Cong,Daniel Zhang-Li,Zhiyuan Liu,Huiqin Liu,Lei Hou,Juanzi Li,Bin Xu*

Main category: cs.AI

TL;DR: Pxplore is a novel framework for Personalized Learning Path Planning that uses reinforcement learning and LLMs to create goal-aligned learning paths, validated through real-world deployment.


<details>
  <summary>Details</summary>
Motivation: Existing LLM approaches for personalized learning lack effective goal-aligned planning mechanisms, creating a need for structured frameworks that can translate abstract learning objectives into actionable paths.

Method: Integrates reinforcement-based training with LLM-driven architecture, using structured learner state modeling, automated reward functions, and policy training combining supervised fine-tuning with Group Relative Policy Optimization.

Result: Extensive experiments validate Pxplore's effectiveness in producing coherent, personalized, and goal-driven learning paths, with successful deployment in a real-world learning platform.

Conclusion: Pxplore demonstrates a successful integration of reinforcement learning and LLMs for personalized education, providing a framework that transforms abstract learning objectives into computable, goal-aligned learning paths.

Abstract: Personalized Learning Path Planning (PLPP) aims to design adaptive learning
paths that align with individual goals. While large language models (LLMs) show
potential in personalizing learning experiences, existing approaches often lack
mechanisms for goal-aligned planning. We introduce Pxplore, a novel framework
for PLPP that integrates a reinforcement-based training paradigm and an
LLM-driven educational architecture. We design a structured learner state model
and an automated reward function that transforms abstract objectives into
computable signals. We train the policy combining supervised fine-tuning (SFT)
and Group Relative Policy Optimization (GRPO), and deploy it within a
real-world learning platform. Extensive experiments validate Pxplore's
effectiveness in producing coherent, personalized, and goal-driven learning
paths. We release our code and dataset to facilitate future research.

</details>


### [215] [EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems](https://arxiv.org/abs/2510.13220)
*Yufei He,Juncheng Liu,Yue Liu,Yibo Li,Tri Cao,Zhiyuan Hu,Xinxing Xu,Bryan Hooi*

Main category: cs.AI

TL;DR: EvoTest is an evolutionary test-time learning framework that improves AI agents without fine-tuning by evolving the agent system after each episode, outperforming existing methods on the J-TTL benchmark.


<details>
  <summary>Details</summary>
Motivation: Current AI agents cannot learn complex skills at test time, behaving like 'clever but clueless interns' in novel environments, which limits their practical utility.

Method: EvoTest uses an evolutionary framework with two roles: Actor Agent plays the game, and Evolver Agent analyzes episode transcripts to propose revised configurations including prompt rewriting, memory updates, hyperparameter tuning, and tool-use learning.

Result: EvoTest consistently increases performance on the J-TTL benchmark, outperforming reflection, memory-only baselines, and online fine-tuning methods. It is the only method capable of winning two games (Detective and Library) while all baselines fail.

Conclusion: The evolutionary test-time learning approach enables AI agents to improve their performance across consecutive episodes without gradient-based fine-tuning, addressing a fundamental limitation in current agent capabilities.

Abstract: A fundamental limitation of current AI agents is their inability to learn
complex skills on the fly at test time, often behaving like "clever but
clueless interns" in novel environments. This severely limits their practical
utility. To systematically measure and drive progress on this challenge, we
first introduce the Jericho Test-Time Learning (J-TTL) benchmark. J-TTL is a
new evaluation setup where an agent must play the same game for several
consecutive episodes, attempting to improve its performance from one episode to
the next. On J-TTL, we find that existing adaptation methods like reflection,
memory, or reinforcement learning struggle. To address the challenges posed by
our benchmark, we present EvoTest, an evolutionary test-time learning framework
that improves an agent without any fine-tuning or gradients-by evolving the
entire agentic system after every episode. EvoTest has two roles: the Actor
Agent, which plays the game, and the Evolver Agent, which analyzes the episode
transcript to propose a revised configuration for the next run. This
configuration rewrites the prompt, updates memory by logging effective
state-action choices, tunes hyperparameters, and learns the tool-use routines.
On our J-TTL benchmark, EvoTest consistently increases performance,
outperforming not only reflection and memory-only baselines but also more
complex online fine-tuning methods. Notably, our method is the only one capable
of winning two games (Detective and Library), while all baselines fail to win
any.

</details>


### [216] [An Analytical Framework to Enhance Autonomous Vehicle Perception for Smart Cities](https://arxiv.org/abs/2510.13230)
*Jalal Khan,Manzoor Khan,Sherzod Turaev,Sumbal Malik,Hesham El-Sayed,Farman Ullah*

Main category: cs.AI

TL;DR: A utility-based analytical model for autonomous vehicle perception systems that evaluates deep learning models' performance to determine the most suitable perception for AVs, using YOLOv8s for object detection on custom datasets.


<details>
  <summary>Details</summary>
Motivation: There is a need to develop accurate perception models that can detect multiple objects on the road and predict driver perception to control autonomous vehicles, requiring evaluation of deep learning models' utility for smart mobility.

Method: Proposed a utility-based analytical model with three modules: custom dataset acquisition with distinctive objects, YOLOv8s deep learning model for object detection, and a utility measurement module based on trained model performance. Validated against nuScense dataset benchmarks.

Result: Three best-performing YOLOv8s instances achieved mAP@0.5: SGD-based (0.832), Adam-based (0.810), and AdamW-based (0.822). AdamW-based model showed superior class-level performance (car: 0.921, motorcyclist: 0.899, truck: 0.793) compared to SGD-based model.

Conclusion: The proposed perception model successfully evaluates learning models' utility and determines appropriate perception for autonomous vehicles, with AdamW-based YOLOv8s showing the best overall performance despite slightly lower mAP than SGD-based model.

Abstract: The driving environment perception has a vital role for autonomous driving
and nowadays has been actively explored for its realization. The research
community and relevant stakeholders necessitate the development of Deep
Learning (DL) models and AI-enabled solutions to enhance autonomous vehicles
(AVs) for smart mobility. There is a need to develop a model that accurately
perceives multiple objects on the road and predicts the driver's perception to
control the car's movements. This article proposes a novel utility-based
analytical model that enables perception systems of AVs to understand the
driving environment. The article consists of modules: acquiring a custom
dataset having distinctive objects, i.e., motorcyclists, rickshaws, etc; a
DL-based model (YOLOv8s) for object detection; and a module to measure the
utility of perception service from the performance values of trained model
instances. The perception model is validated based on the object detection
task, and its process is benchmarked by state-of-the-art deep learning models'
performance metrics from the nuScense dataset. The experimental results show
three best-performing YOLOv8s instances based on mAP@0.5 values, i.e.,
SGD-based (0.832), Adam-based (0.810), and AdamW-based (0.822). However, the
AdamW-based model (i.e., car: 0.921, motorcyclist: 0.899, truck: 0.793, etc.)
still outperforms the SGD-based model (i.e., car: 0.915, motorcyclist: 0.892,
truck: 0.781, etc.) because it has better class-level performance values,
confirmed by the proposed perception model. We validate that the proposed
function is capable of finding the right perception for AVs. The results above
encourage using the proposed perception model to evaluate the utility of
learning models and determine the appropriate perception for AVs.

</details>


### [217] [SAJA: A State-Action Joint Attack Framework on Multi-Agent Deep Reinforcement Learning](https://arxiv.org/abs/2510.13262)
*Weiqi Guo,Guanjun Liu,Ziyuan Zhou*

Main category: cs.AI

TL;DR: SAJA is a joint state-action attack framework for MADRL that combines state and action perturbations synergistically to enhance attack effectiveness and stealthiness.


<details>
  <summary>Details</summary>
Motivation: Existing MADRL models are vulnerable to attacks, but current studies only focus on state-only or action-only attacks without exploiting their synergistic effects.

Method: Two-phase attack: (1) multi-step gradient ascent using actor and critic networks for state perturbation, (2) gradient ascent using critic network for action perturbation with a heuristic regularizer.

Result: SAJA outperforms state-only and action-only attacks in effectiveness and stealthiness, and existing defense methods cannot defend against it.

Conclusion: The proposed joint state-action attack framework demonstrates superior attack capabilities and highlights the need for more robust MADRL defenses.

Abstract: Multi-Agent Deep Reinforcement Learning (MADRL) has shown potential for
cooperative and competitive tasks such as autonomous driving and strategic
gaming. However, models trained by MADRL are vulnerable to adversarial
perturbations on states and actions. Therefore, it is essential to investigate
the robustness of MADRL models from an attack perspective. Existing studies
focus on either state-only attacks or action-only attacks, but do not consider
how to effectively joint them. Simply combining state and action perturbations
such as randomly perturbing states and actions does not exploit their potential
synergistic effects. In this paper, we propose the State-Action Joint Attack
(SAJA) framework that has a good synergistic effects. SAJA consists of two
important phases: (1) In the state attack phase, a multi-step gradient ascent
method utilizes both the actor network and the critic network to compute an
adversarial state, and (2) in the action attack phase, based on the perturbed
state, a second gradient ascent uses the critic network to craft the final
adversarial action. Additionally, a heuristic regularizer measuring the
distance between the perturbed actions and the original clean ones is added
into the loss function to enhance the effectiveness of the critic's guidance.
We evaluate SAJA in the Multi-Agent Particle Environment (MPE), demonstrating
that (1) it outperforms and is more stealthy than state-only or action-only
attacks, and (2) existing state or action defense methods cannot defend its
attacks.

</details>


### [218] [Learnable Game-theoretic Policy Optimization for Data-centric Self-explanation Rationalization](https://arxiv.org/abs/2510.13393)
*Yunxiao Zhao,Zhiqiang Wang,Xingtong Yu,Xiaoli Li,Jiye Liang,Ru Li*

Main category: cs.AI

TL;DR: PORAT addresses mode collapse in rationalization by using game-theoretic policy optimization to guide models toward optimal equilibria, achieving up to 8.1% performance improvements.


<details>
  <summary>Details</summary>
Motivation: Conventional rationalization methods suffer from mode collapse where generators output collapsed patterns, lacking unified solutions for this fundamental problem.

Method: Proposes PORAT with game-theoretic policy optimization that introduces progressive policy interventions to address suboptimal equilibria in cooperative rationalization games.

Result: Validated on 9 real-world datasets and 2 synthetic settings, achieving up to 8.1% performance improvements over state-of-the-art methods.

Conclusion: PORAT successfully addresses mode collapse through game-theoretic interventions, providing a unified solution for rationalization equilibrium problems.

Abstract: Rationalization, a data-centric framework, aims to build self-explanatory
models to explain the prediction outcome by generating a subset of
human-intelligible pieces of the input data. It involves a cooperative game
model where a generator generates the most human-intelligible parts of the
input (i.e., rationales), followed by a predictor that makes predictions based
on these generated rationales. Conventional rationalization methods typically
impose constraints via regularization terms to calibrate or penalize undesired
generation. However, these methods are suffering from a problem called mode
collapse, in which the predictor produces correct predictions yet the generator
consistently outputs rationales with collapsed patterns. Moreover, existing
studies are typically designed separately for specific collapsed patterns,
lacking a unified consideration. In this paper, we systematically revisit
cooperative rationalization from a novel game-theoretic perspective and
identify the fundamental cause of this problem: the generator no longer tends
to explore new strategies to uncover informative rationales, ultimately leading
the system to converge to a suboptimal game equilibrium (correct predictions
v.s collapsed rationales). To solve this problem, we then propose a novel
approach, Game-theoretic Policy Optimization oriented RATionalization (PORAT),
which progressively introduces policy interventions to address the game
equilibrium in the cooperative game process, thereby guiding the model toward a
more optimal solution state. We theoretically analyse the cause of such a
suboptimal equilibrium and prove the feasibility of the proposed method.
Furthermore, we validate our method on nine widely used real-world datasets and
two synthetic settings, where PORAT achieves up to 8.1% performance
improvements over existing state-of-the-art methods.

</details>


### [219] [Assessing LLM Reasoning Through Implicit Causal Chain Discovery in Climate Discourse](https://arxiv.org/abs/2510.13417)
*Liesbeth Allein,Nataly Pineda-Castañeda,Andrea Rocci,Marie-Francine Moens*

Main category: cs.AI

TL;DR: LLMs can generate coherent causal chains for cause-effect pairs but rely on pattern matching rather than genuine causal reasoning, with variations in granularity and number of steps produced.


<details>
  <summary>Details</summary>
Motivation: To evaluate how well LLMs can perform mechanistic causal reasoning by discovering implicit intermediate causal steps between cause-effect pairs, particularly in argumentation contexts like climate change discussions.

Method: Diagnostic evaluation framework where nine LLMs generate all possible intermediate causal steps for cause-effect pairs from climate change argumentation resources, analyzing chain structures, consistency, and reasoning patterns.

Result: LLMs vary in granularity and number of causal steps, show self-consistency and confidence in generated chains, but rely on associative pattern matching rather than true causal reasoning. Human evaluation confirms logical coherence.

Conclusion: The baseline approach, diagnostic insights, and benchmark dataset provide foundation for advancing implicit mechanistic causal reasoning in argumentation, despite current limitations in genuine causal understanding.

Abstract: How does a cause lead to an effect, and which intermediate causal steps
explain their connection? This work scrutinizes the mechanistic causal
reasoning capabilities of large language models (LLMs) to answer these
questions through the task of implicit causal chain discovery. In a diagnostic
evaluation framework, we instruct nine LLMs to generate all possible
intermediate causal steps linking given cause-effect pairs in causal chain
structures. These pairs are drawn from recent resources in argumentation
studies featuring polarized discussion on climate change. Our analysis reveals
that LLMs vary in the number and granularity of causal steps they produce.
Although they are generally self-consistent and confident about the
intermediate causal connections in the generated chains, their judgments are
mainly driven by associative pattern matching rather than genuine causal
reasoning. Nonetheless, human evaluations confirmed the logical coherence and
integrity of the generated chains. Our baseline causal chain discovery
approach, insights from our diagnostic evaluation, and benchmark dataset with
causal chains lay a solid foundation for advancing future work in implicit,
mechanistic causal reasoning in argumentation settings.

</details>


### [220] [Mobile Coverage Analysis using Crowdsourced Data](https://arxiv.org/abs/2510.13459)
*Timothy Wong,Tom Freeman,Joseph Feehily*

Main category: cs.AI

TL;DR: A novel framework using crowdsourced QoE data and One-Class SVM to analyze mobile network coverage and identify service weak spots at cell and site levels.


<details>
  <summary>Details</summary>
Motivation: To enhance user Quality of Experience (QoE) by accurately assessing mobile network coverage and precisely identifying service weak spots for network operators.

Method: Uses crowdsourced QoE data with One-Class Support Vector Machine (OC-SVM) algorithm to model coverage contours at individual cell level, then aggregates to site level using empirical geolocation data. Extends same methodology to analyze service loss reports.

Result: The framework effectively maps mobile coverage and identifies granular areas of signal deficiency, particularly in complex urban environments.

Conclusion: The novel framework demonstrates efficacy in accurately mapping mobile coverage and highlighting localized weak spots using crowdsourced data and OC-SVM approach.

Abstract: Effective assessment of mobile network coverage and the precise
identification of service weak spots are paramount for network operators
striving to enhance user Quality of Experience (QoE). This paper presents a
novel framework for mobile coverage and weak spot analysis utilising
crowdsourced QoE data. The core of our methodology involves coverage analysis
at the individual cell (antenna) level, subsequently aggregated to the site
level, using empirical geolocation data. A key contribution of this research is
the application of One-Class Support Vector Machine (OC-SVM) algorithm for
calculating mobile network coverage. This approach models the decision
hyperplane as the effective coverage contour, facilitating robust calculation
of coverage areas for individual cells and entire sites. The same methodology
is extended to analyse crowdsourced service loss reports, thereby identifying
and quantifying geographically localised weak spots. Our findings demonstrate
the efficacy of this novel framework in accurately mapping mobile coverage and,
crucially, in highlighting granular areas of signal deficiency, particularly
within complex urban environments.

</details>


### [221] [Confidence as a Reward: Transforming LLMs into Reward Models](https://arxiv.org/abs/2510.13501)
*He Du,Bowen Li,Chengxing Xie,Chang Gao,Kai Chen,Dacheng Tao*

Main category: cs.AI

TL;DR: CRew uses token-level confidence as a training-free reward method for LLMs, outperforming existing approaches on math reasoning tasks and enabling effective data filtering and improved training via CRew-DPO.


<details>
  <summary>Details</summary>
Motivation: To address the high costs of training reward models and leverage LLMs' intrinsic reasoning abilities, exploring confidence as a simple yet effective reward metric for close-ended tasks.

Method: Systematically investigates Confidence-as-a-Reward (CRew) using token-level confidence in final answers, and proposes CRew-DPO training strategy that constructs preference data from confidence scores with correctness signals.

Result: CRew outperforms existing training-free reward approaches on MATH500 and RewardMATH benchmarks, surpasses most trained reward models, shows strong correlation with reasoning performance, and effectively filters high-quality training data.

Conclusion: CRew is a powerful training-free reward method, and CRew-DPO further enhances model judging capabilities, consistently outperforming existing self-training methods.

Abstract: Reward models can significantly enhance the reasoning capabilities of large
language models (LLMs), but they typically require extensive curated data and
costly training. To mitigate these challenges, training-free approaches such as
LLM-as-a-Judge leverage the intrinsic reasoning abilities of LLMs to evaluate
responses, achieving promising results. Recent works have also indicated that
model confidence can serve effectively as a reward metric, distinguishing
between chain-of-thought (CoT) and non-CoT paths. However, the concept of using
confidence as a reward has not been comprehensively studied. In this work, we
systematically investigate Confidence-as-a-Reward (CRew), a simple yet powerful
training-free method that utilizes token-level confidence in the model's final
answers as a proxy for reward, especially suitable for close-ended tasks.
Through extensive experiments on mathematical reasoning tasks, we demonstrate
that CRew outperforms existing training-free reward approaches on the MATH500
and RewardMATH benchmarks, and even surpasses most trained reward models. We
further identify a strong correlation between CRew scores and the actual
reasoning performance of the model. Additionally, we find that CRew can
effectively filter high-quality training data. Building upon these insights, we
propose CRew-DPO, a training strategy that constructs preference data from
confidence scores combined with correctness signals. Finetuning with CRew-DPO
further enhances the model's judging capabilities and consistently outperforms
existing self-training methods.

</details>


### [222] [A Methodology for Assessing the Risk of Metric Failure in LLMs Within the Financial Domain](https://arxiv.org/abs/2510.13524)
*William Flanagan,Mukunda Das,Rajitha Ramanyake,Swaunja Maslekar,Meghana Manipuri,Joong Ho Choi,Shruti Nair,Shambhavi Bhusan,Sanjana Dulam,Mouni Pendharkar,Nidhi Singh,Vashisth Doshi,Sachi Shah Paresh*

Main category: cs.AI

TL;DR: This paper addresses the challenge of measuring GenAI performance in financial services by proposing a Risk Assessment Framework that combines SME evaluation with machine learning metrics.


<details>
  <summary>Details</summary>
Motivation: Traditional ML metrics often fail to generalize to GenAI workloads in finance, and existing benchmarks from research institutions don't work well for industrial applications, creating adoption barriers.

Method: The paper develops a Risk Assessment Framework that systematically accounts for unique risks in metric selection and better integrates Subject Matter Expert evaluation with machine learning metrics.

Result: The framework enables more effective performance measurement of GenAI models in financial services by addressing metric generalization issues and risk considerations.

Conclusion: A structured risk assessment approach is needed to properly evaluate GenAI performance in financial services, combining SME insights with appropriate metrics to overcome adoption barriers.

Abstract: As Generative Artificial Intelligence is adopted across the financial
services industry, a significant barrier to adoption and usage is measuring
model performance. Historical machine learning metrics can oftentimes fail to
generalize to GenAI workloads and are often supplemented using Subject Matter
Expert (SME) Evaluation. Even in this combination, many projects fail to
account for various unique risks present in choosing specific metrics.
Additionally, many widespread benchmarks created by foundational research labs
and educational institutions fail to generalize to industrial use. This paper
explains these challenges and provides a Risk Assessment Framework to allow for
better application of SME and machine learning Metrics

</details>


### [223] [Tandem Training for Language Models](https://arxiv.org/abs/2510.13551)
*Robert West,Ashton Anderson,Ece Kamar,Eric Horvitz*

Main category: cs.AI

TL;DR: Tandem training is a reinforcement learning method that forces strong language models to produce solutions intelligible to weaker models by randomly handing off control during rollouts, ensuring both correctness and interpretability.


<details>
  <summary>Details</summary>
Motivation: As language models improve, their reasoning becomes too complex for humans and weaker agents to follow, undermining interpretability and oversight. The goal is to develop methods that maintain intelligibility while preserving performance.

Method: Introduces tandem training - an RL paradigm where rollout tokens are intermittently sampled from a frozen weak model instead of the strong model being trained. This forces solutions to be continuable by weaker models.

Result: In GSM8K math reasoning, tandem training reliably teaches models to abandon jargon and adapt language to weaker partners while maintaining high task accuracy.

Conclusion: Tandem training provides a promising approach for building AI systems that remain auditable by weaker agents, with important implications for human-AI collaboration and multi-agent communication.

Abstract: As language models continue to rapidly improve, we can expect their actions
and reasoning to become difficult or impossible for weaker agents and humans to
follow, undermining interpretability and oversight. With an eye on long-term
futures, we pursue methods that encourage models to produce solutions that
remain intelligible to weaker collaborators. We formalize intelligibility as
handoff robustness: a strong model's solution is intelligible to a weaker model
if randomly handing off control to the weaker model along the solution path
does not cause failure. Building on this criterion, we introduce tandem
training for language models, a reinforcement learning (RL) paradigm in which
rollout tokens are intermittently and randomly sampled from a frozen weak model
rather than the strong model being trained. Because rollouts succeed only when
the strong model's actions and reasoning process can be continued by the weak
model -- when the two can co-construct a successful solution -- optimizing
standard RL objectives with tandem training implicitly incentivizes both
correctness and intelligibility. In the GSM8K math reasoning task, tandem
training reliably teaches models to abandon jargon and adapt their language to
weaker partners while keeping task accuracy high. Our results demonstrate a
promising route to building AI systems that remain auditable by weaker agents,
with implications for human--AI collaboration and multi-agent communication.

</details>


### [224] [A Modal Logic for Temporal and Jurisdictional Classifier Models](https://arxiv.org/abs/2510.13691)
*Cecilia Di Florio,Huimin Dong,Antonino Rotolo*

Main category: cs.AI

TL;DR: A modal logic framework for machine learning classifiers in legal case-based reasoning, incorporating temporal dimensions and court hierarchy to resolve precedent conflicts.


<details>
  <summary>Details</summary>
Motivation: To formally capture legal case-based reasoning performed by ML classifiers and enable verification tools for legal ML systems.

Method: Developed a modal logic of classifiers that incorporates temporal dimensions of cases and court hierarchy principles for conflict resolution between precedents.

Result: Created a formal logic framework that can model legal reasoning with ML classifiers while addressing precedent conflicts through temporal and hierarchical considerations.

Conclusion: The proposed modal logic provides a foundation for building verification tools for ML classifiers in legal applications by formally capturing legal case-based reasoning principles.

Abstract: Logic-based models can be used to build verification tools for machine
learning classifiers employed in the legal field. ML classifiers predict the
outcomes of new cases based on previous ones, thereby performing a form of
case-based reasoning (CBR). In this paper, we introduce a modal logic of
classifiers designed to formally capture legal CBR. We incorporate principles
for resolving conflicts between precedents, by introducing into the logic the
temporal dimension of cases and the hierarchy of courts within the legal
system.

</details>


### [225] [Training LLM Agents to Empower Humans](https://arxiv.org/abs/2510.13709)
*Evan Ellis,Vivek Myers,Jens Tuyls,Sergey Levine,Anca Dragan,Benjamin Eysenbach*

Main category: cs.AI

TL;DR: Empower is a new method for tuning assistive language models that maximizes human empowerment rather than task completion, using only offline text data without requiring explicit human feedback.


<details>
  <summary>Details</summary>
Motivation: Current assistive agents often complete tasks independently rather than truly assisting humans, and require costly explicit human feedback for training.

Method: Proposes empowerment-maximizing method (Empower) that fine-tunes language models to increase human's ability to effect desired changes in the environment, using only offline text data.

Result: In user studies, participants preferred Empower assistant 78% of the time with 31% higher acceptance rate and 38% fewer suggestions. Simulated human programmers showed 192% higher success rate on coding tasks compared to baseline.

Conclusion: Empowerment objective provides a framework for creating useful aligned AI agents at scale using only offline data without additional human feedback or verifiable rewards.

Abstract: Assistive agents should not only take actions on behalf of a human, but also
step out of the way and cede control when there are important decisions to be
made. However, current methods for building assistive agents, whether via
mimicking expert humans or via RL finetuning on an inferred reward, often
encourage agents to complete tasks on their own rather than truly assisting the
human attain their objectives. Additionally, these methods often require costly
explicit human feedback to provide a training signal. We propose a new approach
to tuning assistive language models based on maximizing the human's
empowerment, their ability to effect desired changes in the environment. Our
empowerment-maximizing method, Empower, only requires offline text data,
providing a self-supervised method for fine-tuning language models to better
assist humans. To study the efficacy of our approach, we conducted an 18-person
user study comparing our empowerment assistant with a strong baseline.
Participants preferred our assistant 78% of the time (p=0.015), with a 31%
higher acceptance rate and 38% fewer suggestions. Additionally, we introduce a
new environment for evaluating multi-turn code assistance using simulated
humans. Using this environment, we show that agents trained with Empower
increase the success rate of a simulated human programmer on challenging coding
questions by an average of 192% over an SFT baseline. With this empowerment
objective, we provide a framework for useful aligned AI agents at scale using
only offline data without the need for any additional human feedback or
verifiable rewards.

</details>


### [226] [From Refusal to Recovery: A Control-Theoretic Approach to Generative AI Guardrails](https://arxiv.org/abs/2510.13727)
*Ravi Pandya,Madison Bland,Duy P. Nguyen,Changliu Liu,Jaime Fernández Fisac,Andrea Bajcsy*

Main category: cs.AI

TL;DR: This paper proposes control-theoretic guardrails for AI agents that proactively correct risky outputs to safe ones in real-time, treating AI safety as a sequential decision problem rather than relying on static classification.


<details>
  <summary>Details</summary>
Motivation: Current AI guardrails are brittle and reactive, relying on output classification and simply blocking unsafe actions, which doesn't address downstream hazards in dynamic environments where AI agents interact with the world.

Method: The approach formalizes AI safety through safety-critical control theory within the AI model's latent representation, enabling predictive guardrails that monitor and proactively correct risky outputs in real-time using safety-critical reinforcement learning.

Result: Experiments in simulated driving and e-commerce settings show the guardrails reliably prevent catastrophic outcomes (collisions, bankruptcy) while maintaining task performance, outperforming traditional flag-and-block approaches.

Conclusion: Control-theoretic guardrails provide a principled dynamic alternative to current static safety approaches, enabling proactive safety management for agentic AI systems through sequential decision-making and real-time correction.

Abstract: Generative AI systems are increasingly assisting and acting on behalf of end
users in practical settings, from digital shopping assistants to
next-generation autonomous cars. In this context, safety is no longer about
blocking harmful content, but about preempting downstream hazards like
financial or physical harm. Yet, most AI guardrails continue to rely on output
classification based on labeled datasets and human-specified criteria,making
them brittle to new hazardous situations. Even when unsafe conditions are
flagged, this detection offers no path to recovery: typically, the AI system
simply refuses to act--which is not always a safe choice. In this work, we
argue that agentic AI safety is fundamentally a sequential decision problem:
harmful outcomes arise from the AI system's continually evolving interactions
and their downstream consequences on the world. We formalize this through the
lens of safety-critical control theory, but within the AI model's latent
representation of the world. This enables us to build predictive guardrails
that (i) monitor an AI system's outputs (actions) in real time and (ii)
proactively correct risky outputs to safe ones, all in a model-agnostic manner
so the same guardrail can be wrapped around any AI model. We also offer a
practical training recipe for computing such guardrails at scale via
safety-critical reinforcement learning. Our experiments in simulated driving
and e-commerce settings demonstrate that control-theoretic guardrails can
reliably steer LLM agents clear of catastrophic outcomes (from collisions to
bankruptcy) while preserving task performance, offering a principled dynamic
alternative to today's flag-and-block guardrails.

</details>


### [227] [Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math](https://arxiv.org/abs/2510.13744)
*Shrey Pandit,Austin Xu,Xuan-Phi Nguyen,Yifei Ming,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: Hard2Verify is a human-annotated benchmark for evaluating step-level verification in mathematical reasoning, showing that most open-source verifiers lag behind closed-source models in catching errors in LLM-generated proofs.


<details>
  <summary>Details</summary>
Motivation: LLM-based reasoning systems need strong verifiers to catch step-level mistakes in challenging open-ended mathematical proofs, but existing verification capabilities are insufficient for frontier-level performance.

Method: Created Hard2Verify benchmark with 500+ hours of human annotation, evaluating 29 generative critics and process reward models on their ability to provide step-level annotations or identify first errors in LLM-generated responses to recent challenging math questions.

Result: Beyond a few standout performers, most open-source verifiers significantly lag behind closed-source models in step-level verification capabilities.

Conclusion: The study reveals performance gaps in step-level verification, analyzes factors driving poor performance, and explores fundamental questions about scaling, self-verification, and verification-generation dynamics in mathematical reasoning systems.

Abstract: Large language model (LLM)-based reasoning systems have recently achieved
gold medal-level performance in the IMO 2025 competition, writing mathematical
proofs where, to receive full credit, each step must be not only correct but
also sufficiently supported. To train LLM-based reasoners in such challenging,
open-ended settings, strong verifiers capable of catching step-level mistakes
are necessary prerequisites. We introduce Hard2Verify, a human-annotated,
step-level verification benchmark produced with over 500 hours of human labor.
Hard2Verify is designed to rigorously assess step-level verifiers at the
frontier: Verifiers must provide step-level annotations or identify the first
error in responses generated by frontier LLMs for very recent, challenging, and
open-ended math questions. We evaluate 29 generative critics and process reward
models, demonstrating that, beyond a few standouts, open-source verifiers lag
closed source models. We subsequently analyze what drives poor performance in
step-level verification, the impacts of scaling verifier compute, as well as
fundamental questions such as self-verification and verification-generation
dynamics.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [228] [Effective Connectivity-Based Unsupervised Channel Selection Method for EEG](https://arxiv.org/abs/2510.12910)
*Neda Abdollahpour,N. Sertac Artan,Ian Daly,Mohammadreza Yazdchi,Zahra Baharlouei*

Main category: eess.SP

TL;DR: The paper introduces ICEC, an unsupervised channel selection method for EEG data that uses effective connectivity metrics to identify the most relevant channels, reducing electrode count while improving classification accuracy.


<details>
  <summary>Details</summary>
Motivation: EEG data is high-dimensional with many channels, but not all provide equally meaningful information. Selecting relevant channels is crucial for computational efficiency and robust neural dynamics analysis.

Method: Proposed ICEC criterion quantifies effective connectivity in each channel using five metrics (PDC, GPDC, RPDC, DTF, dDTF). Used unsupervised channel selection based on interaction intensity, with CSP for feature extraction and SVM for classification.

Result: Consistent performance improvements and significant electrode reduction across all participants. Achieved highest accuracies: 82% (13/22 channels), 86.01% (29/59 channels), and 87.56% (48/118 channels) on three datasets, outperforming state-of-the-art methods.

Conclusion: The ICEC-based channel selection method effectively reduces electrode count while maintaining or improving classification accuracy, demonstrating its utility for efficient EEG data analysis.

Abstract: Analyzing neural data such as Electroencephalography (EEG) data often
involves dealing with high-dimensional datasets, where not all channels provide
equally meaningful informa- tion. Selecting the most relevant channels is
crucial for improving computational efficiency and ensuring robust insights
into neural dynamics. This study introduces the Importance of Channels based on
Effective Connectivity (ICEC) criterion for quantifying effective connectivity
(EC) in each channel. Effective connectivity refers to the causal influence one
neural region exerts over another, providing insights into the directional flow
of information. Using this criterion, we propose an unsupervised channel
selection method that accounts for the intensity of interactions among
channels. To evaluate the proposed channel selection method, we applied it to
three well-known EEG datasets across four categories. The assessment involved
calculating the ICEC criterion using five effective connectivity metrics:
partial directed coherence (PDC), generalized PDC (GPDC), renormalized PDC
(RPDC), directed transfer function (DTF), and direct DTF (dDTF). To focus on
the effect of channel selection, we employed the Common Spatial Pattern (CSP)
algorithm for feature extraction and a Support Vector Machine (SVM) for
classification across all participants. Results were compared with other
CSP-based methods. The evaluation included comparing participant- specific
accuracies with and without the proposed method across five effective
connectivity metrics. The results showed consistent performance improvements
and a significant reduction in the number of selected electrodes for all
participants. Compared to state-of-the-art methods, our approach achieved the
highest accuracies: 82% (13 out of 22 channels), 86.01% (29 out of 59
channels), and 87.56% (48 out of 118 channels) across three datasets.

</details>


### [229] [Enabling Full Duplex ISAC Leveraging Waveform Domain Separability](https://arxiv.org/abs/2510.12912)
*Abdelali Arous,Hamza Haif,Huseyin Arslan*

Main category: eess.SP

TL;DR: Proposes a novel waveform-domain self-interference cancellation technique for ISAC in monostatic IBFD systems using OFDM for communication and AFDM for radar sensing, leveraging affine domain properties to treat interference as AWGN.


<details>
  <summary>Details</summary>
Motivation: Address significant self-interference challenges in ISAC monostatic in-band full-duplex systems during concurrent communication and radar operations.

Method: Designs integrated dual-functionality frame with OFDM for communication and AFDM for radar sensing using same modulator. Projects received signal to affine domain where SI appears as AWGN, applies iterative low-complexity windowing, and time-domain spreading.

Result: Demonstrates superior performance in detection probability, target range and velocity RMSE while maintaining high spectral efficiency and minimal computational complexity.

Conclusion: The proposed waveform-domain SIC technique effectively mitigates self-interference in ISAC systems through affine domain processing and achieves excellent radar performance with minimal complexity.

Abstract: Integrated sensing and communication (ISAC) in monostatic in-band full-duplex
(IBFD) systems encounters significant challenges due to self-interference (SI)
at the radar receiver during concurrent communication and radar operations.
This paper proposes a novel waveform-domain self-interference cancellation
(SIC) technique that leverages the unique properties of orthogonal frequency
division multiplexing (OFDM) and affine frequency division multiplexing (AFDM)
signals. The proposed approach designs the integrated dual-functionality frame
to utilize OFDM for communication and AFDM for radar sensing, both generated
using the same modulator block. Then, we establish the conditions under which a
wide sense stationary (WSS) process in the time domain appears as WSS in the
affine domain and demonstrate that the interfering OFDM signal behaves as an
additive white Gaussian noise (AWGN) in this domain. Exploiting this property,
the received signal is projected into the affine domain, where the SI appears
as AWGN, enabling its subtraction with minimal residual interference. To
further mitigate the residual SI, an iterative low-complexity windowing scheme
is applied, selectively locking onto the radar signal to reduce the processed
signal space. A subsequent time-domain spreading step is applied after
converting the SIC-processed signal into the post-coded time domain, wherein
the SI diminishes separately across the delay and Doppler axes. The proposed
method demonstrates superior performance in terms of detection probability,
target range and velocity root mean square error (RMSE), while maintaining high
spectral efficiency and minimal computational complexity.

</details>


### [230] [Passive Microwave Tag Classification Using RF Fingerprinting and Machine Learning](https://arxiv.org/abs/2510.12930)
*Cory Hilton,Mohammad Rashid,Faiz Sherman,Steven Bush,Jeffrey A. Nanzer*

Main category: eess.SP

TL;DR: This paper presents a method for identifying wireless microwave tags using RF fingerprinting and machine learning, achieving 95% classification accuracy between two tags.


<details>
  <summary>Details</summary>
Motivation: To develop a low-cost, simple wireless identification system using the unique nonlinear characteristics of diodes in microwave tags for device identification.

Method: Used 2.0 GHz tags with patch antennas and a single diode, interrogated with multi-tone continuous wave signals. Applied machine learning to analyze spectral differences in the nonlinear diode responses.

Result: Achieved 95% real-time classification accuracy between two tags using the proposed RF fingerprinting approach.

Conclusion: RF fingerprinting combined with machine learning can effectively identify wireless devices based on unique nonlinear characteristics of simple diode-based tags.

Abstract: We present an approach to identifying wireless microwave tags using radio
frequency (RF) fingerprinting and machine learning. The tags are designed for
low cost and simplicity, consisting of only two antennas and a single nonlinear
element (a diode). An interrogating transceiver transmits a signal consisting
of a set of individual frequency tones that is captured by the tag. The signal
response of the diode is nonlinear, and can be represented by an infinite power
series, the coefficients of which are similar but not identical for different
physical diodes due to small manufacturing perturbations. The small differences
in the signal responses manifest in the spectral signal response of the tag,
which is retransmitted back to the interrogating transceiver. Input into
machine learning algorithms, the slight differences in the spectral responses
of the diodes can be used to uniquely identify devices. To demonstrate the
concept, we designed 2.0 GHz tags consisting of patch antennas and a single
diode, along with a bi-static radar system operating at the 2.0 GHz 802.11
Wi-Fi band transmitting multi-tone continuous wave signals representing common
802.11 training fields. The received signals were processed using a set of
algorithms for comparison purposes. A real-time classification accuracy of 95%
between two tags was achieved.

</details>


### [231] [Computationally Efficient Neural Receivers via Axial Self-Attention](https://arxiv.org/abs/2510.12941)
*SaiKrishna Saketh Yellapragada,Atchutaram K. Kocharlakota,Mário Costa,Esa Ollila,Sergiy A. Vorobyov*

Main category: eess.SP

TL;DR: An axial self-attention transformer neural receiver for 6G wireless systems that achieves state-of-the-art BLER performance with improved computational efficiency by factorizing attention along temporal and spectral axes.


<details>
  <summary>Details</summary>
Motivation: To develop efficient neural receivers for next-generation wireless systems that can handle the computational demands of 6G while maintaining high performance in dynamic environments.

Method: Proposes an axial self-attention transformer that factorizes attention operations along temporal and spectral axes, reducing quadratic complexity from O((TF)^2) to O(T^2F+TF^2) compared to global self-attention.

Result: Outperforms all evaluated receiver architectures including global self-attention, convolutional neural receivers, and traditional LS-LMMSE at 10% BLER with reduced computational complexity. Maintains robust performance at 1% BLER where traditional receivers fail.

Conclusion: The axial neural receiver provides a structured, scalable, and efficient framework for AI-Native 6G RAN systems, enabling deployment in resource-constrained edge environments for URLLC communication.

Abstract: Deep learning-based neural receivers are redefining physical-layer signal
processing for next-generation wireless systems. We propose an axial
self-attention transformer neural receiver designed for applicability to 6G and
beyond wireless systems, validated through 5G-compliant experimental
configurations, that achieves state-of-the-art block error rate (BLER)
performance with significantly improved computational efficiency. By
factorizing attention operations along temporal and spectral axes, the proposed
architecture reduces the quadratic complexity of conventional multi-head
self-attention from $O((TF)^2)$ to $O(T^2F+TF^2)$, yielding substantially fewer
total floating-point operations and attention matrix multiplications per
transformer block compared to global self-attention. Relative to convolutional
neural receiver baselines, the axial neural receiver achieves significantly
lower computational cost with a fraction of the parameters. Experimental
validation under 3GPP Clustered Delay Line (CDL) channels demonstrates
consistent performance gains across varying mobility scenarios. Under
non-line-of-sight CDL-C conditions, the axial neural receiver consistently
outperforms all evaluated receiver architectures, including global
self-attention, convolutional neural receivers, and traditional LS-LMMSE at
10\% BLER with reduced computational complexity per inference. At stringent
reliability targets of 1\% BLER, the axial receiver maintains robust symbol
detection at high user speeds, whereas the traditional LS-LMMSE receiver fails
to converge, underscoring its suitability for ultra-reliable low-latency
(URLLC) communication in dynamic 6G environments and beyond. These results
establish the axial neural receiver as a structured, scalable, and efficient
framework for AI-Native 6G RAN systems, enabling deployment in
resource-constrained edge environments.

</details>


### [232] [Towards Spectrally Efficient and Physically Reconfigurable Architectures for Multibeam-Waveform Co-Design in Joint Communication and Sensing](https://arxiv.org/abs/2510.12968)
*Najme Ebrahimi,Arun Paidmarri,Alexandra Gallyas-Sanhueza,Yuan Ma,Haoling Li,Basem Abdelaziz Abdelmagid,Tzu-Yuan Huang,Hua Wang*

Main category: eess.SP

TL;DR: This paper analyzes multibeam architectures for Joint Communication and Sensing (JCAS) systems, comparing various waveform and beamforming techniques across different domains to optimize performance tradeoffs.


<details>
  <summary>Details</summary>
Motivation: JCAS platforms are emerging as foundational for next-generation mmWave and sub-THz systems, enabling both high-throughput communication and angular localization within shared signal paths, requiring optimized multibeam architectures.

Method: The paper compares multiple JCAS architectures including OFDM, Frequency Modulated Arrays (FMA), Time-Modulated Arrays (TMA), direct RF/MMW modulation, and CDMA-based systems, analyzing them across time, frequency, code, and direct analog/RF domains.

Result: The analysis reveals architecture-specific tradeoffs among beam agility, efficiency, accuracy, resolution, and complexity, highlighting performance differences in spectral efficiency, beam orthogonality, latency, and AoA estimation accuracy.

Conclusion: The paper provides a framework for selecting JCAS front ends optimized for power, latency, inter-beam and multi-user interference, and rapid system reconfiguration based on specific application requirements.

Abstract: Joint Communication and Sensing (JCAS) platforms are emerging as a foundation
of next-generation mmWave (MMW) and sub-THz systems, enabling both
high-throughput data transfer and angular localization within a shared signal
path. This paper investigates multibeam architectures for JCAS that
simultaneously optimize waveform shaping and beamforming across the time,
frequency, code, and direct analog/ radio frequency (RF) domains. The paper
compares Orthogonal Frequency-Division Multiplexing (OFDM), Frequency Modulated
Arrays (FMA), Time-Modulated Arrays (TMA), direct RF/MMW modulation, and
Code-Division Multiple Access (CDMA)-based systems with respect to spectral
efficiency, beam orthogonality, latency, and Angle-of-Arrival (AoA) estimation
accuracy. The results highlight architecture-specific tradeoffs among beam
agility, efficiency, accuracy and resolution, and complexity. It also provides
a framework for selecting JCAS front ends optimized for power, latency,
inter-beam and multi-user interference, and rapid system reconfiguration

</details>


### [233] [Constellation Design in OFDM-ISAC over Data Payloads: From MSE Analysis to Experimentation](https://arxiv.org/abs/2510.13101)
*Kawon Han,Kaitao Meng,Alexandra Chatzicharistou,Christos Masouros*

Main category: eess.SP

TL;DR: This paper analyzes OFDM-based ISAC systems for multi-target delay estimation, develops an estimation-theoretic framework, derives MSE expressions for MF and RF receivers, and proposes a receiver-dependent constellation design for sensing-communication trade-off.


<details>
  <summary>Details</summary>
Motivation: OFDM is widely used in ISAC systems but its sensing performance for multi-target delay estimation under different receiver architectures needs better theoretical understanding and optimization.

Method: Developed an estimation-theoretic framework to characterize sensing performance with random communication payloads, derived closed-form MSE expressions for MF and RF receivers, and proposed a receiver-dependent constellation design.

Result: MF receiver performance depends on constellation's fourth-order moment in multi-target scenarios, while RF performance depends on inverse second-order moment regardless of target count. Proposed constellation design enables flexible sensing-communication trade-off.

Conclusion: The study provides fundamental limits for OFDM-ISAC delay estimation, reveals receiver-dependent constellation impacts, and demonstrates practical constellation design for optimizing sensing-communication trade-offs, validated through simulations and experiments.

Abstract: Orthogonal frequency division multiplexing (OFDM) is one of the most widely
adopted waveforms for integrated sensing and communication (ISAC) systems,
owing to its high spectral efficiency and compatibility with modern
communication standards. This paper investigates the sensing performance of
OFDM-based ISAC for multi-target delay (range) estimation under specific radar
receiver processing schemes. An estimation-theoretic framework is developed to
characterize sensing performance with random communication payloads. We
establish the fundamental limit of delay estimation accuracy by deriving the
closed-form expression of the mean-square error (MSE) achieved using matched
filtering (MF) and reciprocal filtering (RF) receivers. The results show that,
in multi-target scenarios, the impact of signal constellations on the delay
estimation MSE differs across receivers: MF performance depends on the
fourth-order moment of the zero-mean, unit-power constellation in the presence
of multiple targets, whereas RF performance depends on its inverse second-order
moment, irrespective of the number of targets. Building on this analysis, we
present a ISAC constellation design under specific receiver architecture that
brings a receiver-dependent flexible trade-off between sensing and
communication in OFDM-ISAC systems. The theoretical findings are validated
through simulations and proof-of-concept experiments, and also the sensing and
communication performance trade-off is experimentally shown with the proposed
constellation design.

</details>


### [234] [Working Memory Functional Connectivity Analysis for Dementia Classification using EEG](https://arxiv.org/abs/2510.13399)
*Shivani Ranjan,Anant Jain,Robin Badal,Amit Kumar,Harshal Shende,Deepak Joshi,Pramod Yadav,Lalan Kumar*

Main category: eess.SP

TL;DR: EEG-based working memory functional connectivity analysis using Cross-Plot Transition Entropy (CPTE) achieves 97.53% accuracy in classifying dementia stages, outperforming traditional Phase Lag Index methods.


<details>
  <summary>Details</summary>
Motivation: Early detection of dementia, particularly at the Mild Cognitive Impairment stage, is crucial for timely intervention. Working memory impairment serves as a key early indicator, and EEG offers a cost-effective method to assess brain network alterations.

Method: EEG signals from 24 participants (AD, MCI, healthy controls) during working memory tasks were analyzed. Functional connectivity was quantified using CPTE and PLI, with network metrics analyzed using SVM, Random Forest, and XGBoost classifiers.

Result: CPTE-based connectivity achieved 97.53% classification accuracy during retrieval phase with Random Forest. AD subjects showed higher synchronization patterns than healthy controls. SHD and HHD features demonstrated strong discriminative potential.

Conclusion: The integration of working memory tasks with EEG-based functional connectivity analysis provides a robust, scalable, non-invasive diagnostic tool for early detection and monitoring of neurodegenerative diseases.

Abstract: Background: Dementia, particularly Alzheimer's Disease (AD), is a progressive
neurodegenerative disorder marked by cognitive decline. Early detection,
especially at the Mild Cognitive Impairment (MCI) stage, is essential for
timely intervention. Working Memory (WM) impairment is a key early indicator of
neurodegeneration, affecting higher cognitive processes. Electroencephalography
(EEG), with its high temporal resolution, offers a cost-effective method to
assess brain dynamics. This study investigates WM-related EEG functional
connectivity (FC) to identify brain network alterations across dementia stages.
Methods: EEG signals were recorded from 24 participants (8 AD, 8 MCI, and 8
healthy controls) during WM tasks, including encoding, recall, and retrieval
stages. Data preprocessing involved noise reduction and feature extraction
using Spherical and Head Harmonic Decomposition (SHD, HHD). FC was quantified
using Cross-Plot Transition Entropy (CPTE) and Phase Lag Index (PLI). Network
metrics such as Degree and Eigenvector Centrality were analyzed using Support
Vector Machine, Random Forest, and XGBoost classifiers. Results: The CPTE-based
connectivity metrics outperformed the traditional PLI approach in
differentiating dementia stages, attaining a peak classification accuracy of
97.53% during the retrieval phase with the Random Forest model. A connectivity
threshold of 0.5 was optimal for network discrimination. SHD and HHD features
also demonstrated strong discriminative potential. AD subjects exhibited higher
synchronization patterns during WM tasks than healthy controls. Conclusions:
The integration of WM tasks with EEG-based FC analysis provides a robust
framework for dementia classification. The proposed CPTE-based approach offers
a robust, scalable, non-invasive, and effective diagnostic tool for early
detection and monitoring of neurodegenerative diseases.

</details>


### [235] [Oscillator Drift Compensation by Line-of-Sight Tracking for Distributed Multisensor ISAC](https://arxiv.org/abs/2510.13442)
*Lorenz Mohr,Marc Miranda,Sebastian Semper,Julia Beuster,Carsten Andrich,Sebastian Giehl,Christian Schneider,Reiner S. Thomä*

Main category: eess.SP

TL;DR: Proposed Kalman filtering for LoS tracking to correct synchronization mismatches in mobile multisensor ISAC systems, improving phase coherence and reducing estimation errors.


<details>
  <summary>Details</summary>
Motivation: Synchronization mismatches in mobile multisensor channel sounding measurements cause non-smooth phase progressions and drifts, which disrupt Doppler estimation requiring coherent nodes.

Method: Extended traditional geometry-based drift compensation with Kalman filtering for LoS tracking, and used relative residual power after HRPE as a metric for comparing synchronization methods.

Result: Reduced relative residual power by more than 5 dB and decreased delay-Doppler estimate RMSEs by approximately 60%, outperforming traditional LoS estimation heuristics.

Conclusion: The proposed approach effectively corrects time-varying drifts while preserving relative sensor motion, enabling reliable Doppler estimation in distributed ISAC systems.

Abstract: We observed synchronization mismatches in the form of non-smooth phase
progressions and drifts within mobile multisensor channel sounding
measurements. However, performing Doppler estimation in a distributed
multisensor integrated sensing and communications (ISAC) system requires
coherence among the nodes, which implies a continuously differentiable phase
progression of the received signals. To correct the sounding data in
post-processing, we extend traditional geometry-based drift compensation
algorithms by utilizing Kalman filtering for line-of-sight (LoS) tracking,
which improves the robustness of the LoS estimate in multipath scenarios. This
approach smooths the phase progression and enables the correction of
time-varying drifts while preserving relative sensor motion. Furthermore, we
propose using the relative residual power after high-resolution parameter
estimation (HRPE) as a metric for ground-truth-independent comparison of
post-processing synchronization methods for recorded channel sounding data.
Results show that the proposed approach outperforms traditional LoS estimation
heuristics, reducing the relative residual power by more than 5 dB and the
delay-Doppler estimate root mean square errors (RMSEs) by approximately 60 %.

</details>


### [236] [Radio over Fiber with Cascaded Structure: Algorithm for Uplink Positioning](https://arxiv.org/abs/2510.13495)
*Dexin Kong,Diana Pamela Moya Osorio,Erik G. Larsson*

Main category: eess.SP

TL;DR: Proposes novel cascaded radio-over-fiber structure for indoor positioning using maximum-likelihood and non-linear least-squares estimators, demonstrating satisfactory performance despite non-linear power amplifier effects.


<details>
  <summary>Details</summary>
Motivation: Leverage recent polymer microwave fiber technology advancements to create robust, low-cost, high-speed sub-THz radio-over-fiber communications for indoor scenarios, addressing cascaded distortion effects from non-linear power amplifiers.

Method: Propose maximum-likelihood and non-linear least-squares algorithms to estimate propagation distance and time-of-arrival, derive Cramér-Rao lower bound for linear PAs case, and investigate uplink positioning using measured PMF characteristics.

Result: Simulation results show proposed estimators perform satisfactorily even with cascaded non-linear PA effects, enabling new cost-effective opportunities for high-resolution indoor positioning.

Conclusion: The cascaded RoF structure deployment can enable new cost-effective opportunities for high-resolution positioning in indoor scenarios using polymer microwave fiber technology.

Abstract: Recent advancements in polymer microwave fiber (PMF) technology have created
significant opportunities for robust, low-cost, and high-speed sub-terahertz
(THz) radio-over- fiber communications. Recognizing these potential benefits,
this paper explores a novel radio-over-fiber (RoF) structure that interconnects
multiple radio units (RUs) in cascade via fiber, envi- sioning its application
in indoor scenarios. This structure creates a number of research opportunities
when considering cascaded distortion effects introduced by non-linear power
amplifiers (PAs) at the RUs and the propagation channel over the fiber. We
propose maximum-likelihood and non-linear least-squares algorithms to estimate
the propagation distance along the RoF and the time-of-arrival between the RoF
and the user equipment. For the case of linear PAs, we derive the Cram\'er-Rao
lower bound to benchmark the performance of the estimators. Finally, we
investigate the use of the system for uplink positioning. Our simulation
results demonstrate that the proposed estimators perform satisfactorily even
with the cascaded effects of non- linear PAs, and that the deployment of this
RoF structure can enable new cost-effective opportunities for high-resolution
positioning in indoor scenarios. In the numerical evaluation, we also use
measured PMF characteristics for high-density polyethylene fibers.

</details>


### [237] [A Robust EDM Optimization Approach for 3D Single-Source Localization with Angle and Range Measurements](https://arxiv.org/abs/2510.13498)
*Mingyu Zhao,Qingna Li,Hou-Duo Qi*

Main category: eess.SP

TL;DR: A robust EDM optimization model for 3D single-source localization that incorporates range measurements, angle measurements, and least absolute deviation criterion, with angle measurements represented as distance box constraints.


<details>
  <summary>Details</summary>
Motivation: To develop a computationally tractable model that simultaneously incorporates range measurements, angle measurements, and robust least absolute deviation criterion for accurate source localization.

Method: Represent angle measurements as box constraints on distances by reducing each 3D angle measurement to a 2D nonlinear optimization problem, then develop an efficient algorithm for the EDM optimization model.

Result: The model achieves high quality localization as demonstrated through extensive numerical experiments comparing with leading 3DSSL solvers.

Conclusion: The proposed EDM model successfully integrates three key elements for robust source localization and provides an efficient computational approach with superior performance.

Abstract: For the problem of source localization, three elements usually play a very
important role in accurate localization. They are the range measurements, the
angle measurements and the least absolute deviation criterion, which is
regarded as a robust metric for denoising the measurements. Building the three
elements into a computationally tractable model is challenging. In this paper,
we introduce a robust Euclidean Distance Matrix (EDM) optimization model that
simultaneously incorporates the three elements. For the first time, we show
that for the case of 3D single-source localization (3DSSL), the angle
measurements can be represented as a simple box constraint of distances. It is
achieved by reducing each of the 3D angle measurements to a two-dimensional
nonlinear optimization problem, whose global minimum and maximum solutions can
be characterized and utilized to get the lower and upper bounds of the
distances from the unknown source to the sensors. We further develop an
efficient algorithm. The high quality of the localization by the new EDM model
is assessed through extensive numerical experiments in comparison with leading
solvers for 3DSSL.

</details>
