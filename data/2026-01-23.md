<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 68]
- [cs.LG](#cs.LG) [Total: 56]
- [eess.IV](#eess.IV) [Total: 8]
- [cs.AI](#cs.AI) [Total: 52]
- [cs.IT](#cs.IT) [Total: 15]
- [cs.RO](#cs.RO) [Total: 23]
- [eess.SP](#eess.SP) [Total: 19]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [AI-Based Culvert-Sewer Inspection](https://arxiv.org/abs/2601.15366)
*Christina Thrainer*

Main category: cs.CV

TL;DR: This thesis proposes three methods to improve automated defect segmentation in culverts and sewer pipes under data scarcity: data preprocessing strategies, a novel FORTRESS architecture, and few-shot learning approaches.


<details>
  <summary>Details</summary>
Motivation: Culverts and sewer pipes are critical infrastructure components whose failure poses serious risks. However, collecting and annotating defect data is cumbersome and requires domain expertise, making large datasets infeasible for structural defect detection.

Method: Three approaches: 1) Preprocessing strategies including traditional data augmentation and dynamic label injection; 2) FORTRESS architecture combining depthwise separable convolutions, adaptive Kolmogorov-Arnold Networks (KAN), and multi-scale attention mechanisms; 3) Few-shot semantic segmentation using bidirectional prototypical networks with attention mechanisms.

Result: Preprocessing techniques significantly improved segmentation performance (IoU and F1 scores). FORTRESS achieved state-of-the-art performance on culvert sewer pipe defect dataset while reducing trainable parameters and computational cost. Few-shot learning approach achieved satisfactory results across evaluation metrics.

Conclusion: The thesis successfully addresses data scarcity in culvert/sewer pipe defect segmentation through three complementary methods that enhance training data, optimize model architecture, and enable learning from limited samples, demonstrating applicability to real-world scenarios.

Abstract: Culverts and sewer pipes are critical components of drainage systems, and their failure can lead to serious risks to public safety and the environment. In this thesis, we explore methods to improve automated defect segmentation in culverts and sewer pipes. Collecting and annotating data in this field is cumbersome and requires domain knowledge. Having a large dataset for structural defect detection is therefore not feasible. Our proposed methods are tested under conditions with limited annotated data to demonstrate applicability to real-world scenarios. Overall, this thesis proposes three methods to significantly enhance defect segmentation and handle data scarcity. This can be addressed either by enhancing the training data or by adjusting a models architecture.
  First, we evaluate preprocessing strategies, including traditional data augmentation and dynamic label injection. These techniques significantly improve segmentation performance, increasing both Intersection over Union (IoU) and F1 score. Second, we introduce FORTRESS, a novel architecture that combines depthwise separable convolutions, adaptive Kolmogorov-Arnold Networks (KAN), and multi-scale attention mechanisms. FORTRESS achieves state-of-the-art performance on the culvert sewer pipe defect dataset, while significantly reducing the number of trainable parameters, as well as its computational cost. Finally, we investigate few-shot semantic segmentation and its applicability to defect detection. Few-shot learning aims to train models with only limited data available. By employing a bidirectional prototypical network with attention mechanisms, the model achieves richer feature representations and achieves satisfactory results across evaluation metrics.

</details>


### [2] [Evaluating Multimodal Large Language Models for Heterogeneous Face Recognition](https://arxiv.org/abs/2601.15406)
*Hatef Otroshi Shahreza,Anjith George,Sébastien Marcel*

Main category: cs.CV

TL;DR: MLLMs perform poorly for heterogeneous face recognition compared to classical systems, especially in cross-spectral scenarios.


<details>
  <summary>Details</summary>
Motivation: To evaluate the potential of Multimodal Large Language Models (MLLMs) for biometric applications, specifically heterogeneous face recognition (HFR) where enrollment and probe images come from different sensing modalities.

Method: Systematic evaluation of state-of-the-art MLLMs across multiple cross-modality scenarios (VIS-NIR, VIS-SWIR, VIS-THERMAL) using biometric protocols and metrics including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR).

Result: Substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, despite recent advances in MLLMs.

Conclusion: Current MLLMs have limitations for HFR applications, highlighting the importance of rigorous biometric evaluation before deployment in face recognition systems.

Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated strong performance on a wide range of vision-language tasks, raising interest in their potential use for biometric applications. In this paper, we conduct a systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition (HFR), where enrollment and probe images are from different sensing modalities, including visual (VIS), near infrared (NIR), short-wave infrared (SWIR), and thermal camera. We benchmark multiple open-source MLLMs across several cross-modality scenarios, including VIS-NIR, VIS-SWIR, and VIS-THERMAL face recognition. The recognition performance of MLLMs is evaluated using biometric protocols and based on different metrics, including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR). Our results reveal substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, in spite of recent advances in MLLMs. Our findings highlight the limitations of current MLLMs for HFR and also the importance of rigorous biometric evaluation when considering their deployment in face recognition systems.

</details>


### [3] [CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation](https://arxiv.org/abs/2601.15408)
*Pablo Messina,Andrés Villa,Juan León Alcázar,Karen Sánchez,Carlos Hinojosa,Denis Parra,Álvaro Soto,Bernard Ghanem*

Main category: cs.CV

TL;DR: CURE is an error-aware curriculum learning framework that improves visual grounding and reduces hallucinations in medical vision-language models for radiology report generation without needing additional data.


<details>
  <summary>Details</summary>
Motivation: Existing medical vision-language models struggle with accurate visual grounding and factual consistency, often misaligning textual findings with visual evidence, leading to unreliable or weakly grounded predictions in radiology report generation.

Method: CURE fine-tunes a multimodal instructional model using curriculum learning on three tasks: phrase grounding, grounded report generation, and anatomy-grounded report generation. It dynamically adjusts sample sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment.

Result: CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6% compared to existing methods.

Conclusion: CURE is a data-efficient framework that enhances both grounding accuracy and report reliability in medical vision-language models for radiology report generation without requiring additional training data.

Abstract: Medical vision-language models can automate the generation of radiology reports but struggle with accurate visual grounding and factual consistency. Existing models often misalign textual findings with visual evidence, leading to unreliable or weakly grounded predictions. We present CURE, an error-aware curriculum learning framework that improves grounding and report quality without any additional data. CURE fine-tunes a multimodal instructional model on phrase grounding, grounded report generation, and anatomy-grounded report generation using public datasets. The method dynamically adjusts sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a data-efficient framework that enhances both grounding accuracy and report reliability. Code is available at https://github.com/PabloMessina/CURE and model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure

</details>


### [4] [DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction](https://arxiv.org/abs/2601.15416)
*Cuong Tran Van,Trong-Thang Pham,Ngoc-Son Nguyen,Duy Minh Ho Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: DuFal is a dual-frequency-aware learning framework for sparse-view CBCT reconstruction that uses a dual-path architecture with global and local Fourier neural operators to better preserve high-frequency anatomical details.


<details>
  <summary>Details</summary>
Motivation: Sparse-view CBCT reconstruction struggles with recovering fine-grained anatomical details (high-frequency components) because conventional CNN-based methods are biased toward learning low-frequency information, leading to loss of important structural details.

Method: DuFal integrates frequency-domain and spatial-domain processing via a dual-path architecture with: 1) High-Local Factorized Fourier Neural Operator with global and local branches, 2) Spectral-Channel Factorization for efficiency, 3) Cross-Attention Frequency Fusion module, and 4) Feature Decoder with Intensity Field Decoding pipeline.

Result: Experimental results on LUNA16 and ToothFairy datasets show DuFal significantly outperforms state-of-the-art methods in preserving high-frequency anatomical features, especially under extremely sparse-view settings.

Conclusion: The proposed DuFal framework effectively addresses the high-frequency recovery challenge in sparse-view CBCT reconstruction through dual-frequency-aware learning, demonstrating superior performance in preserving fine anatomical details compared to existing methods.

Abstract: Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray projections remains a challenging problem in medical imaging due to the inherent undersampling of fine-grained anatomical details, which correspond to high-frequency components. Conventional CNN-based methods often struggle to recover these fine structures, as they are typically biased toward learning low-frequency information. To address this challenge, this paper presents DuFal (Dual-Frequency-Aware Learning), a novel framework that integrates frequency-domain and spatial-domain processing via a dual-path architecture. The core innovation lies in our High-Local Factorized Fourier Neural Operator, which comprises two complementary branches: a Global High-Frequency Enhanced Fourier Neural Operator that captures global frequency patterns and a Local High-Frequency Enhanced Fourier Neural Operator that processes spatially partitioned patches to preserve spatial locality that might be lost in global frequency analysis. To improve efficiency, we design a Spectral-Channel Factorization scheme that reduces the Fourier Neural Operator parameter count. We also design a Cross-Attention Frequency Fusion module to integrate spatial and frequency features effectively. The fused features are then decoded through a Feature Decoder to produce projection representations, which are subsequently processed through an Intensity Field Decoding pipeline to reconstruct a final Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy datasets demonstrate that DuFal significantly outperforms existing state-of-the-art methods in preserving high-frequency anatomical features, particularly under extremely sparse-view settings.

</details>


### [5] [DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection](https://arxiv.org/abs/2601.15453)
*Morteza Poudineh,Marc Lalonde*

Main category: cs.CV

TL;DR: A deviation-guided prompt learning framework for few-normal shot anomaly detection that combines CLIP's semantic power with statistical deviation-based scoring for better patch-level anomaly localization.


<details>
  <summary>Details</summary>
Motivation: Existing few-normal shot anomaly detection methods using vision-language models have weak discriminability between normal/abnormal prompts and lack principled scoring mechanisms for patch-level anomalies.

Method: Deviation-guided prompt learning with learnable context vectors (shared across prompts) and anomaly-specific suffix tokens, plus deviation loss with Top-K Multiple Instance Learning to model patch features as Gaussian deviations from normal distribution.

Result: Superior pixel-level detection performance on MVTecAD and VISA benchmarks compared to PromptAD and other baselines, with ablation studies validating the effectiveness of key components.

Conclusion: The proposed framework effectively integrates vision-language model semantics with statistical reliability for improved anomaly detection and localization in few-normal shot scenarios.

Abstract: Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.

</details>


### [6] [Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events](https://arxiv.org/abs/2601.15475)
*Yunshan Qi,Lin Zhu,Nan Bao,Yifan Zhao,Jia Li*

Main category: cs.CV

TL;DR: A NeRF framework that uses sensor-physics modeling to achieve sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding event data.


<details>
  <summary>Details</summary>
Motivation: Existing methods for novel view synthesis from blurry LDR images with event data ignore sensor-physics mismatches between camera output and real-world radiance, leading to suboptimal HDR and deblurring results.

Method: Proposes a unified sensor-physics grounded NeRF framework with: 1) NeRF representing actual HDR scene radiance, 2) pixel-wise RGB mapping field aligning rendered values with sensor-recorded LDR values, 3) event mapping field bridging physical scene dynamics with actual event sensor output. Both mapping fields are jointly optimized with NeRF network.

Result: Achieves state-of-the-art deblurring HDR novel view synthesis results on collected and public datasets using single-exposure blurry LDR images and corresponding events.

Conclusion: The proposed sensor-physics grounded approach effectively addresses the limitations of existing methods by modeling the physical relationships between scene radiance, camera sensors, and event data, enabling superior HDR novel view synthesis from challenging blurry LDR inputs.

Abstract: Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.

</details>


### [7] [Hybrid Vision Transformer_GAN Attribute Neutralizer for Mitigating Bias in Chest X_Ray Diagnosis](https://arxiv.org/abs/2601.15490)
*Jobeal Solomon,Ali Mohammed Mansoor Alsahag,Seyed Sahand Mohammadi Ziabari*

Main category: cs.CV

TL;DR: Vision Transformer backbone in Attribute-Neutral Framework reduces demographic bias in chest X-ray classifiers better than convolutional U-Net, cutting sex-recognition AUC by ~10 percentage points while maintaining diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: Chest X-ray classifiers often have bias from sex- and age-related shortcuts, causing underdiagnosis in minority subgroups. Previous convolutional encoder methods reduce but don't fully eliminate attribute leakage at clinically usable edit strengths.

Method: Replaced U-Net convolutional encoder with Vision Transformer backbone (DeiT-S) in Attribute-Neutral Framework. Trained on ChestX-ray14 dataset, generated edited images across 11 edit-intensity levels, evaluated with independent AI judge for attribute leakage and CNN for disease prediction.

Result: At moderate edit level (alpha=0.5), ViT neutralizer reduces sex-recognition AUC to ~0.80 (10 percentage points below original convolutional U-Net), trained for half as many epochs. Diagnostic accuracy maintained: macro ROC AUC across 15 findings within 5 percentage points of baseline, worst-case subgroup AUC near 0.70.

Conclusion: Global self-attention vision models can further suppress attribute leakage without sacrificing clinical utility, offering practical route toward fairer chest X-ray AI by reducing demographic bias while preserving diagnostic performance.

Abstract: Bias in chest X-ray classifiers frequently stems from sex- and age-related shortcuts, leading to systematic underdiagnosis of minority subgroups. Previous pixel-space attribute neutralizers, which rely on convolutional encoders, lessen but do not fully remove this attribute leakage at clinically usable edit strengths. This study evaluates whether substituting the U-Net convolutional encoder with a Vision Transformer backbone in the Attribute-Neutral Framework can reduce demographic attribute leakage while preserving diagnostic accuracy. A data-efficient Image Transformer Small (DeiT-S) neutralizer was trained on the ChestX-ray14 dataset. Its edited images, generated across eleven edit-intensity levels, were evaluated with an independent AI judge for attribute leakage and with a convolutional neural network (ConvNet) for disease prediction. At a moderate edit level (alpha = 0.5), the Vision Transformer (ViT) neutralizer reduces patient sex-recognition area under the curve (AUC) to approximately 0.80, about 10 percentage points below the original framework's convolutional U-Net encoder, despite being trained for only half as many epochs. Meanwhile, macro receiver operating characteristic area under the curve (ROC AUC) across 15 findings stays within five percentage points of the unedited baseline, and the worst-case subgroup AUC remains near 0.70. These results indicate that global self-attention vision models can further suppress attribute leakage without sacrificing clinical utility, suggesting a practical route toward fairer chest X-ray AI.

</details>


### [8] [Controllable Layered Image Generation for Real-World Editing](https://arxiv.org/abs/2601.15507)
*Jinrui Yang,Qing Liu,Yijun Li,Mengwei Ren,Letian Zhang,Zhe Lin,Cihang Xie,Yuyin Zhou*

Main category: cs.CV

TL;DR: LASAGNA is a unified framework that generates images with layered representations (background + transparent foreground) for controllable editing, using a new dataset and benchmark for layer-based image generation.


<details>
  <summary>Details</summary>
Motivation: Current image generation models struggle with controllable editing of specific elements in existing images. Existing layered representation approaches fail to produce coherent compositing relationships and lack realistic visual effects like shadows and reflections.

Method: Proposes LASAGNA framework that jointly generates images with composing layers (photorealistic background + high-quality transparent foreground with visual effects). Learns correct image composition from multiple conditioning inputs: text prompts, foreground, background, and location masks. Introduces LASAGNA-48K dataset of clean backgrounds and RGBA foregrounds with physically grounded visual effects, and LASAGNABENCH benchmark for layer editing.

Result: LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects.

Conclusion: LASAGNA offers greater controllability for real-world applications through unified layer generation, with publicly released dataset and benchmark to foster open research in layer-based image editing.

Abstract: Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/.

</details>


### [9] [DeltaDorsal: Enhancing Hand Pose Estimation with Dorsal Features in Egocentric Views](https://arxiv.org/abs/2601.15516)
*William Huang,Siyou Pei,Leyi Zou,Eric J. Gonzalez,Ishan Chatterjee,Yang Zhang*

Main category: cs.CV

TL;DR: A novel hand pose estimation method using dorsal skin deformation features to overcome finger occlusion challenges in XR devices, achieving 18% error reduction in occluded scenarios.


<details>
  <summary>Details</summary>
Motivation: Egocentric hand pose estimation in XR devices faces significant challenges due to frequent finger occlusions, which degrade performance and reliability for interaction tasks.

Method: Proposes a dual-stream delta encoder that learns pose by contrasting features from dynamic hand movements against a baseline relaxed position, using only cropped dorsal images and leveraging recent dense visual featurizers.

Result: Reduces Mean Per Joint Angle Error (MPJAE) by 18% in self-occluded scenarios (fingers >=50% occluded) compared to state-of-the-art methods, while using smaller models and only dorsal images.

Conclusion: The approach enhances reliability for occluded interaction tasks (like pinch and tap estimation) and enables new interaction paradigms such as detecting isometric force for surface "clicks" without visible movement.

Abstract: The proliferation of XR devices has made egocentric hand pose estimation a vital task, yet this perspective is inherently challenged by frequent finger occlusions. To address this, we propose a novel approach that leverages the rich information in dorsal hand skin deformation, unlocked by recent advances in dense visual featurizers. We introduce a dual-stream delta encoder that learns pose by contrasting features from a dynamic hand with a baseline relaxed position. Our evaluation demonstrates that, using only cropped dorsal images, our method reduces the Mean Per Joint Angle Error (MPJAE) by 18% in self-occluded scenarios (fingers >=50% occluded) compared to state-of-the-art techniques that depend on the whole hand's geometry and large model backbones. Consequently, our method not only enhances the reliability of downstream tasks like index finger pinch and tap estimation in occluded scenarios but also unlocks new interaction paradigms, such as detecting isometric force for a surface "click" without visible movement while minimizing model size.

</details>


### [10] [VIOLA: Towards Video In-Context Learning with Minimal Annotations](https://arxiv.org/abs/2601.15549)
*Ryo Fujii,Hideo Saito,Ryo Hachiuma*

Main category: cs.CV

TL;DR: VIOLA is a label-efficient framework for adapting MLLMs to novel video domains using minimal expert annotations by combining density-uncertainty sampling and confidence-aware mechanisms.


<details>
  <summary>Details</summary>
Motivation: Adapting MLLMs to specialized video domains (industrial/surgical settings) is challenging due to scarce labeled data and impractical expert annotations, while standard ICL methods require large annotated pools.

Method: 1) Density-uncertainty-weighted sampling to maximize annotation efficiency by selecting diverse, representative, and informative samples. 2) Hybrid pool with confidence-aware retrieval (composite similarity-confidence scoring) and confidence-aware prompting (adaptive distinction between ground truths and noisy pseudo-labels).

Result: Extensive experiments across nine diverse benchmarks using four MLLMs show VIOLA significantly outperforms baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.

Conclusion: VIOLA provides an effective label-efficient framework for adapting MLLMs to novel video domains by synergizing minimal expert supervision with abundant unlabeled data through intelligent sampling and confidence-aware mechanisms.

Abstract: Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.

</details>


### [11] [Relative Classification Accuracy: A Calibrated Metric for Identity Consistency in Fine-Grained K-pop Face Generation](https://arxiv.org/abs/2601.15560)
*Sylvey Lin,Eranki Vasistha*

Main category: cs.CV

TL;DR: DDPMs struggle with semantic controllability in fine-grained single-domain tasks like K-pop idol face generation, showing high visual quality but severe identity misalignment due to semantic mode collapse.


<details>
  <summary>Details</summary>
Motivation: Standard metrics like FID and IS fail to detect identity misalignment in specialized domains with high inter-class similarity, creating a need for better evaluation of semantic controllability in conditional generative models.

Method: Proposed Relative Classification Accuracy (RCA) metric that normalizes generative performance against an oracle classifier's baseline, applied to Class-Conditional DDPMs for K-pop idol face generation at 32x32 resolution.

Result: Model achieves high visual quality (FID 8.93) but suffers from severe semantic mode collapse (RCA 0.27), especially for visually ambiguous identities, with failure modes attributed to resolution constraints and intra-gender ambiguity.

Conclusion: The RCA framework provides a rigorous standard for verifying identity consistency in conditional generative models, revealing critical trade-offs between visual quality and semantic controllability in fine-grained domains.

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable success in high-fidelity image generation. However, evaluating their semantic controllability-specifically for fine-grained, single-domain tasks-remains challenging. Standard metrics like FID and Inception Score (IS) often fail to detect identity misalignment in such specialized contexts. In this work, we investigate Class-Conditional DDPMs for K-pop idol face generation (32x32), a domain characterized by high inter-class similarity. We propose a calibrated metric, Relative Classification Accuracy (RCA), which normalizes generative performance against an oracle classifier's baseline. Our evaluation reveals a critical trade-off: while the model achieves high visual quality (FID 8.93), it suffers from severe semantic mode collapse (RCA 0.27), particularly for visually ambiguous identities. We analyze these failure modes through confusion matrices and attribute them to resolution constraints and intra-gender ambiguity. Our framework provides a rigorous standard for verifying identity consistency in conditional generative models.

</details>


### [12] [Region-aware Spatiotemporal Modeling with Collaborative Domain Generalization for Cross-Subject EEG Emotion Recognition](https://arxiv.org/abs/2601.15615)
*Weiwei Wu,Yueyang Li,Yuhu Shi,Weiming Zeng,Lang Qin,Yang Yang,Ke Zhou,Zhiguo Zhang,Wai Ting Siok,Nizhuan Wang*

Main category: cs.CV

TL;DR: RSM-CoDG is a novel framework for cross-subject EEG emotion recognition that integrates region-aware spatial modeling, multi-scale temporal modeling, and collaborative domain generalization to address inter-subject variability and improve generalization to unseen individuals.


<details>
  <summary>Details</summary>
Motivation: Cross-subject EEG emotion recognition faces challenges due to strong inter-subject variability causing distribution shifts in EEG signals, and the complexity of emotion-related neural representations in both spatial organization and temporal evolution. Existing approaches typically address spatial modeling, temporal modeling, or generalization strategies in isolation, limiting their ability to align representations across subjects while capturing multi-scale dynamics and suppressing subject-specific bias.

Method: The proposed RSM-CoDG framework incorporates: 1) Neuroscience priors from functional brain region partitioning to construct region-level spatial representations for better cross-subject comparability; 2) Multi-scale temporal modeling to characterize dynamic evolution of emotion-evoked neural activity; 3) Collaborative domain generalization strategy with multidimensional constraints to reduce subject-specific bias in fully unseen target subject settings.

Result: Extensive experimental results on SEED series datasets demonstrate that RSM-CoDG consistently outperforms existing competing methods, providing an effective approach for improving robustness in cross-subject EEG emotion recognition.

Conclusion: RSM-CoDG offers a unified framework that successfully addresses the challenges of cross-subject EEG emotion recognition by integrating region-aware spatial modeling, multi-scale temporal analysis, and collaborative domain generalization, leading to superior performance and better generalization to unknown individuals.

Abstract: Cross-subject EEG-based emotion recognition (EER) remains challenging due to strong inter-subject variability, which induces substantial distribution shifts in EEG signals, as well as the high complexity of emotion-related neural representations in both spatial organization and temporal evolution. Existing approaches typically improve spatial modeling, temporal modeling, or generalization strategies in isolation, which limits their ability to align representations across subjects while capturing multi-scale dynamics and suppressing subject-specific bias within a unified framework. To address these gaps, we propose a Region-aware Spatiotemporal Modeling framework with Collaborative Domain Generalization (RSM-CoDG) for cross-subject EEG emotion recognition. RSM-CoDG incorporates neuroscience priors derived from functional brain region partitioning to construct region-level spatial representations, thereby improving cross-subject comparability. It also employs multi-scale temporal modeling to characterize the dynamic evolution of emotion-evoked neural activity. In addition, the framework employs a collaborative domain generalization strategy, incorporating multidimensional constraints to reduce subject-specific bias in a fully unseen target subject setting, which enhances the generalization to unknown individuals. Extensive experimental results on SEED series datasets demonstrate that RSM-CoDG consistently outperforms existing competing methods, providing an effective approach for improving robustness. The source code is available at https://github.com/RyanLi-X/RSM-CoDG.

</details>


### [13] [Explainable Deepfake Detection with RL Enhanced Self-Blended Images](https://arxiv.org/abs/2601.15624)
*Ning Jiang,Dingheng Zeng,Yanhong Liu,Haiyang Yi,Shijie Yu,Minghe Weng,Haifeng Shen,Ying Li*

Main category: cs.CV

TL;DR: Proposes RL-enhanced MLLM framework for interpretable deepfake detection with automated CoT data generation to address annotation scarcity.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods lack explainability, and MLLMs face data scarcity due to costly detailed forgery attribution annotations. RL shows promise for improving cross-domain generalization in visual tasks.

Method: Automated Chain-of-Thought data generation framework based on Self-Blended Images, plus RL-enhanced deepfake detection framework with tailored reward mechanism and feedback-driven synthetic data generation.

Result: Achieves competitive performance with SOTA approaches across multiple cross-dataset benchmarks. Validates effectiveness of CoT data pipeline, reward mechanism, and synthetic data approach.

Conclusion: Proposed framework enables MLLM adoption in deepfake detection with reduced annotation costs and demonstrates RL's potential for improving interpretable detection with cross-domain generalization.

Abstract: Most prior deepfake detection methods lack explainable outputs. With the growing interest in multimodal large language models (MLLMs), researchers have started exploring their use in interpretable deepfake detection. However, a major obstacle in applying MLLMs to this task is the scarcity of high-quality datasets with detailed forgery attribution annotations, as textual annotation is both costly and challenging - particularly for high-fidelity forged images or videos. Moreover, multiple studies have shown that reinforcement learning (RL) can substantially enhance performance in visual tasks, especially in improving cross-domain generalization. To facilitate the adoption of mainstream MLLM frameworks in deepfake detection with reduced annotation cost, and to investigate the potential of RL in this context, we propose an automated Chain-of-Thought (CoT) data generation framework based on Self-Blended Images, along with an RL-enhanced deepfake detection framework. Extensive experiments validate the effectiveness of our CoT data construction pipeline, tailored reward mechanism, and feedback-driven synthetic data generation approach. Our method achieves performance competitive with state-of-the-art (SOTA) approaches across multiple cross-dataset benchmarks. Implementation details are available at https://github.com/deon1219/rlsbi.

</details>


### [14] [Evolving Without Ending: Unifying Multimodal Incremental Learning for Continual Panoptic Perception](https://arxiv.org/abs/2601.15643)
*Bo Yuan,Danpei Zhao,Wentao Li,Tian Li,Zhiguo Jiang*

Main category: cs.CV

TL;DR: Proposes Continual Panoptic Perception (CPP) for multimodal multi-task continual learning, addressing catastrophic forgetting and semantic obfuscation through collaborative cross-modal encoding, knowledge inheritance, and cross-modal consistency constraints.


<details>
  <summary>Details</summary>
Motivation: Current continual learning research focuses too much on single-task scenarios, limiting potential in multi-task and multimodal applications. Multi-task continual learning introduces semantic obfuscation across multimodal alignment, causing severe model degradation during incremental training.

Method: Proposes CPP model with collaborative cross-modal encoder for multimodal embedding, malleable knowledge inheritance via contrastive feature and instance distillation, cross-modal consistency constraints (CPP+), and asymmetric pseudo-labeling for exemplar-free learning.

Result: Extensive experiments on multimodal datasets and diverse CL tasks demonstrate superiority, particularly in fine-grained continual learning tasks, showing effectiveness in addressing catastrophic forgetting and semantic alignment issues.

Conclusion: CPP successfully extends continual learning to multimodal multi-task scenarios, integrating pixel-level, instance-level, and image-level joint interpretation for comprehensive image perception while maintaining semantic alignment across modalities.

Abstract: Continual learning (CL) is a great endeavour in developing intelligent perception AI systems. However, the pioneer research has predominantly focus on single-task CL, which restricts the potential in multi-task and multimodal scenarios. Beyond the well-known issue of catastrophic forgetting, the multi-task CL also brings semantic obfuscation across multimodal alignment, leading to severe model degradation during incremental training steps. In this paper, we extend CL to continual panoptic perception (CPP), integrating multimodal and multi-task CL to enhance comprehensive image perception through pixel-level, instance-level, and image-level joint interpretation. We formalize the CL task in multimodal scenarios and propose an end-to-end continual panoptic perception model. Concretely, CPP model features a collaborative cross-modal encoder (CCE) for multimodal embedding. We also propose a malleable knowledge inheritance module via contrastive feature distillation and instance distillation, addressing catastrophic forgetting from task-interactive boosting manner. Furthermore, we propose a cross-modal consistency constraint and develop CPP+, ensuring multimodal semantic alignment for model updating under multi-task incremental scenarios. Additionally, our proposed model incorporates an asymmetric pseudo-labeling manner, enabling model evolving without exemplar replay. Extensive experiments on multimodal datasets and diverse CL tasks demonstrate the superiority of the proposed model, particularly in fine-grained CL tasks.

</details>


### [15] [SuperOcc: Toward Cohesive Temporal Modeling for Superquadric-based Occupancy Prediction](https://arxiv.org/abs/2601.15644)
*Zichen Yu,Quanli Liu,Wei Wang,Liyong Zhang,Xiaoguang Zhao*

Main category: cs.CV

TL;DR: SuperOcc is a novel 3D occupancy prediction framework that uses superquadric representations instead of dense voxels, addressing sparsity-efficiency trade-offs with improved temporal modeling and efficient splatting.


<details>
  <summary>Details</summary>
Motivation: Existing 3D occupancy prediction methods use dense scene representations that overlook the inherent sparsity of real-world driving scenes. Superquadric representations offer a promising sparse alternative but suffer from insufficient temporal modeling, poor trade-off between query sparsity and geometric expressiveness, and inefficient superquadric-to-voxel splatting.

Method: SuperOcc incorporates three key designs: (1) cohesive temporal modeling mechanism exploiting both view-centric and object-centric temporal cues, (2) multi-superquadric decoding strategy to enhance geometric expressiveness without sacrificing query sparsity, and (3) efficient superquadric-to-voxel splatting scheme for computational efficiency.

Result: Extensive experiments on SurroundOcc and Occ3D benchmarks demonstrate that SuperOcc achieves state-of-the-art performance while maintaining superior efficiency compared to existing methods.

Conclusion: SuperOcc successfully addresses the limitations of existing superquadric frameworks for 3D occupancy prediction, providing an effective sparse representation solution that balances geometric expressiveness, computational efficiency, and temporal modeling for autonomous driving applications.

Abstract: 3D occupancy prediction plays a pivotal role in the realm of autonomous driving, as it provides a comprehensive understanding of the driving environment. Most existing methods construct dense scene representations for occupancy prediction, overlooking the inherent sparsity of real-world driving scenes. Recently, 3D superquadric representation has emerged as a promising sparse alternative to dense scene representations due to the strong geometric expressiveness of superquadrics. However, existing superquadric frameworks still suffer from insufficient temporal modeling, a challenging trade-off between query sparsity and geometric expressiveness, and inefficient superquadric-to-voxel splatting. To address these issues, we propose SuperOcc, a novel framework for superquadric-based 3D occupancy prediction. SuperOcc incorporates three key designs: (1) a cohesive temporal modeling mechanism to simultaneously exploit view-centric and object-centric temporal cues; (2) a multi-superquadric decoding strategy to enhance geometric expressiveness without sacrificing query sparsity; and (3) an efficient superquadric-to-voxel splatting scheme to improve computational efficiency. Extensive experiments on the SurroundOcc and Occ3D benchmarks demonstrate that SuperOcc achieves state-of-the-art performance while maintaining superior efficiency. The code is available at https://github.com/Yzichen/SuperOcc.

</details>


### [16] [Event-VStream: Event-Driven Real-Time Understanding for Long Video Streams](https://arxiv.org/abs/2601.15655)
*Zhenghui Guo,Yuanbin Man,Junyuan Sheng,Bowen Lin,Ahmed Ahmed,Bo Jiang,Boyuan Zhang,Miao Yin,Sian Jin,Omprakash Gnawal,Chengming Zhang*

Main category: cs.CV

TL;DR: Event-VStream: An event-aware framework for real-time long video understanding that processes video as discrete semantic events instead of fixed frames, improving efficiency and temporal reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs struggle with real-time long video streams due to redundant frame processing and forgetting past context. Current streaming systems use fixed-interval decoding or cache pruning, leading to repetitive outputs or loss of crucial temporal information.

Method: Represents continuous video as sequence of discrete, semantically coherent events. Detects meaningful state transitions using motion, semantic, and predictive cues, triggering language generation only at event boundaries. Each event embedding is consolidated into persistent memory bank for long-horizon reasoning.

Result: Achieves +10.4 points improvement over VideoLLM-Online-8B baseline on OVOBench-Realtime. Performs close to Flash-VStream-7B despite using only general-purpose LLaMA-3-8B text backbone. Maintains ~70% GPT-5 win rate on 2-hour Ego4D streams.

Conclusion: Event-VStream effectively addresses challenges in real-time long video understanding by event-based processing, enabling competitive performance with low latency and improved temporal reasoning capabilities.

Abstract: Real-time understanding of long video streams remains challenging for multimodal large language models (VLMs) due to redundant frame processing and rapid forgetting of past context. Existing streaming systems rely on fixed-interval decoding or cache pruning, which either produce repetitive outputs or discard crucial temporal information. We introduce Event-VStream, an event-aware framework that represents continuous video as a sequence of discrete, semantically coherent events. Our system detects meaningful state transitions by integrating motion, semantic, and predictive cues, and triggers language generation only at those boundaries. Each event embedding is consolidated into a persistent memory bank, enabling long-horizon reasoning while maintaining low latency. Across OVOBench-Realtime, and long-form Ego4D evaluations, Event-VStream achieves competitive performance. It improves over a VideoLLM-Online-8B baseline by +10.4 points on OVOBench-Realtime, achieves performance close to Flash-VStream-7B despite using only a general-purpose LLaMA-3-8B text backbone, and maintains around 70% GPT-5 win rate on 2-hour Ego4D streams.

</details>


### [17] [Skywork UniPic 3.0: Unified Multi-Image Composition via Sequence Modeling](https://arxiv.org/abs/2601.15664)
*Hongyang Wei,Hongbo Liu,Zidong Wang,Yi Peng,Baixin Xu,Size Wu,Xuying Zhang,Xianglong He,Zexiang Liu,Peiyu Wang,Xuchen Song,Yangguang Li,Yang Liu,Yahui Zhou*

Main category: cs.CV

TL;DR: Skywork UniPic 3.0 is a unified multimodal framework that excels at both single-image editing and multi-image composition, achieving state-of-the-art performance with an efficient 8-step inference and 12.5x speedup.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the community's strong interest in multi-image composition tasks, particularly Human-Object Interaction (HOI) scenarios, where existing models lack disclosed methods for achieving high-quality fusion and consistency compared to single-image editing.

Method: The authors present Skywork UniPic 3.0 with: 1) A comprehensive data collection, filtering, and synthesis pipeline using only 700K high-quality samples; 2) A novel training paradigm treating multi-image composition as sequence modeling; 3) Post-training integration of trajectory mapping and distribution matching for accelerated inference.

Result: The model achieves state-of-the-art performance on single-image editing benchmarks and surpasses both Nano-Banana and Seedream 4.0 on multi-image composition benchmarks, with 8-step inference providing 12.5x speedup over standard sampling.

Conclusion: Skywork UniPic 3.0 demonstrates the effectiveness of the proposed data pipeline and training paradigm for unified multimodal image composition, with publicly available code, models, and dataset.

Abstract: The recent surge in popularity of Nano-Banana and Seedream 4.0 underscores the community's strong interest in multi-image composition tasks. Compared to single-image editing, multi-image composition presents significantly greater challenges in terms of consistency and quality, yet existing models have not disclosed specific methodological details for achieving high-quality fusion. Through statistical analysis, we identify Human-Object Interaction (HOI) as the most sought-after category by the community. We therefore systematically analyze and implement a state-of-the-art solution for multi-image composition with a primary focus on HOI-centric tasks. We present Skywork UniPic 3.0, a unified multimodal framework that integrates single-image editing and multi-image composition. Our model supports an arbitrary (1~6) number and resolution of input images, as well as arbitrary output resolutions (within a total pixel budget of 1024x1024). To address the challenges of multi-image composition, we design a comprehensive data collection, filtering, and synthesis pipeline, achieving strong performance with only 700K high-quality training samples. Furthermore, we introduce a novel training paradigm that formulates multi-image composition as a sequence-modeling problem, transforming conditional generation into unified sequence synthesis. To accelerate inference, we integrate trajectory mapping and distribution matching into the post-training stage, enabling the model to produce high-fidelity samples in just 8 steps and achieve a 12.5x speedup over standard synthesis sampling. Skywork UniPic 3.0 achieves state-of-the-art performance on single-image editing benchmark and surpasses both Nano-Banana and Seedream 4.0 on multi-image composition benchmark, thereby validating the effectiveness of our data pipeline and training paradigm. Code, models and dataset are publicly available.

</details>


### [18] [Consistency-Regularized GAN for Few-Shot SAR Target Recognition](https://arxiv.org/abs/2601.15681)
*Yikui Zhai,Shikuang Liu,Wenlve Zhou,Hongsheng Zhang,Zhiheng Zhou,Xiaolin Tian,C. L. Philip Chen*

Main category: cs.CV

TL;DR: Cr-GAN is a consistency-regularized GAN framework that synthesizes diverse SAR images from limited data, enabling effective few-shot recognition through self-supervised pre-training and fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Few-shot SAR recognition faces data scarcity, and conventional GANs need abundant data, creating a paradox. There's a need for generative models that work with severe data limitations to enable effective few-shot learning.

Method: Proposes Cr-GAN with dual-branch discriminator that decouples adversarial training from representation learning. Uses channel-wise feature interpolation to create novel latent features and dual-domain cycle consistency for semantic integrity. Framework is adaptable to various GAN architectures.

Result: Achieves 71.21% accuracy on MSTAR and 51.64% on SRSDD datasets in 8-shot setting, significantly outperforming baselines while using only ~5% parameters of state-of-the-art diffusion models.

Conclusion: Cr-GAN successfully resolves the data scarcity paradox in few-shot SAR recognition by generating high-quality synthetic data from limited samples, enabling effective self-supervised pre-training and fine-tuning.

Abstract: Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5 of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.

</details>


### [19] [Performance-guided Reinforced Active Learning for Object Detection](https://arxiv.org/abs/2601.15688)
*Zhixuan Liang,Xingyu Zeng,Rui Zhao,Ping Luo*

Main category: cs.CV

TL;DR: MGRAL is a reinforcement learning-based active learning method for object detection that uses mAP improvement as reward to guide sample selection, outperforming existing approaches on VOC and COCO benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current active learning strategies for object detection focus on data distribution or intrinsic information content rather than directly optimizing for downstream task performance (mAP), leading to suboptimal sample selection.

Method: MGRAL uses reinforcement learning with a sampling agent that selects informative batches using policy gradient optimization, where the reward is mAP improvement. It employs expected model output changes as informativeness measure and uses fast look-up tables for efficient mAP estimation.

Result: MGRAL achieves the highest active learning curve on PASCAL VOC and COCO benchmarks, demonstrating superior performance in selecting informative samples for object detection with minimal labeling effort.

Conclusion: MGRAL establishes a new paradigm for reinforcement learning-driven active object detection by directly optimizing for task performance (mAP) rather than indirect informativeness measures, with practical deployment feasibility through efficient computation methods.

Abstract: Active learning (AL) strategies aim to train high-performance models with minimal labeling efforts, only selecting the most informative instances for annotation. Current approaches to evaluating data informativeness predominantly focus on the data's distribution or intrinsic information content and do not directly correlate with downstream task performance, such as mean average precision (mAP) in object detection. Thus, we propose Performance-guided (i.e. mAP-guided) Reinforced Active Learning for Object Detection (MGRAL), a novel approach that leverages the concept of expected model output changes as informativeness. To address the combinatorial explosion challenge of batch sample selection and the non-differentiable correlation between model performance and selected batches, MGRAL skillfully employs a reinforcement learning-based sampling agent that optimizes selection using policy gradient with mAP improvement as reward. Moreover, to reduce the computational overhead of mAP estimation with unlabeled samples, MGRAL utilizes an unsupervised way with fast look-up tables, ensuring feasible deployment. We evaluate MGRAL's active learning performance on detection tasks over PASCAL VOC and COCO benchmarks. Our approach demonstrates the highest AL curve with convincing visualizations, establishing a new paradigm in reinforcement learning-driven active object detection.

</details>


### [20] [Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs](https://arxiv.org/abs/2601.15698)
*Mingyu Yu,Lana Liu,Zhehao Zhao,Wei Wang,Sujuan Qin*

Main category: cs.CV

TL;DR: BVS is a novel jailbreaking framework that probes visual safety boundaries of MLLMs using reconstruction-then-generation strategy with visual splicing and inductive recomposition to generate harmful images.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges at the intersection of textual and visual safety. Existing schemes have explored MLLM vulnerabilities, but investigation into their visual safety boundaries remains insufficient.

Method: BVS employs a "reconstruction-then-generation" strategy using neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby inducing MLLMs to generate harmful images.

Result: BVS achieves a remarkable jailbreak success rate of 98.21% against GPT-5 (12 January 2026 release), exposing critical vulnerabilities in the visual safety alignment of current MLLMs.

Conclusion: The findings reveal significant security gaps in MLLM visual safety mechanisms, highlighting the need for more robust visual safety alignment in multimodal AI systems.

Abstract: The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges, particularly at the intersection of textual and visual safety. While existing schemes have explored the security vulnerabilities of MLLMs, the investigation into their visual safety boundaries remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a novel image-text pair jailbreaking framework specifically designed to probe the visual safety boundaries of MLLMs. BVS employs a "reconstruction-then-generation" strategy, leveraging neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby leading MLLMs to be induced into generating harmful images. Experimental results demonstrate that BVS achieves a remarkable jailbreak success rate of 98.21\% against GPT-5 (12 January 2026 release). Our findings expose critical vulnerabilities in the visual safety alignment of current MLLMs.

</details>


### [21] [Enhanced LULC Segmentation via Lightweight Model Refinements on ALOS-2 SAR Data](https://arxiv.org/abs/2601.15705)
*Ali Caglayan,Nevrez Imamoglu,Toru Kouyama*

Main category: cs.CV

TL;DR: This paper improves national-scale LULC semantic segmentation from ALOS-2 SAR data using three lightweight refinements to address common SAR failure modes without increasing pipeline complexity.


<details>
  <summary>Details</summary>
Motivation: The paper addresses common failure modes in SAR dense-prediction tasks: boundary over-smoothing, missed thin/slender structures, and rare-class degradation under long-tailed label distributions, particularly for national-scale land-use/land-cover segmentation using single-polarization SAR data.

Method: Three lightweight refinements are introduced: (1) injecting high-resolution features into multi-scale decoding, (2) a progressive refine-up head that alternates convolutional refinement and stepwise upsampling, and (3) an α-scale factor that tempers class reweighting within a focal+dice objective. These build upon SAR-W-MixMAE self-supervised pretraining.

Result: The resulting model yields consistent improvements on the Japan-wide ALOS-2 LULC benchmark, particularly for under-represented classes, and improves water detection across standard evaluation metrics.

Conclusion: The proposed lightweight refinements effectively address SAR-specific segmentation challenges without increasing pipeline complexity, demonstrating improved performance for national-scale LULC mapping and water detection tasks using single-polarization SAR data.

Abstract: This work focuses on national-scale land-use/land-cover (LULC) semantic segmentation using ALOS-2 single-polarization (HH) SAR data over Japan, together with a companion binary water detection task. Building on SAR-W-MixMAE self-supervised pretraining [1], we address common SAR dense-prediction failure modes, boundary over-smoothing, missed thin/slender structures, and rare-class degradation under long-tailed labels, without increasing pipeline complexity. We introduce three lightweight refinements: (i) injecting high-resolution features into multi-scale decoding, (ii) a progressive refine-up head that alternates convolutional refinement and stepwise upsampling, and (iii) an $α$-scale factor that tempers class reweighting within a focal+dice objective. The resulting model yields consistent improvements on the Japan-wide ALOS-2 LULC benchmark, particularly for under-represented classes, and improves water detection across standard evaluation metrics.

</details>


### [22] [Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework](https://arxiv.org/abs/2601.15711)
*Shubham Shukla,Kunal Sonalkar*

Main category: cs.CV

TL;DR: VLMs achieve 64% macro-F1 for fashion attribute prediction but struggle with detecting when attributes are applicable vs. not applicable (34% NA-F1), while excelling at fine-grained classification (71% F1). Efficient models offer 90% of flagship performance at lower cost.


<details>
  <summary>Details</summary>
Motivation: Fine-grained attribute prediction is crucial for fashion retail applications like catalog enrichment, visual search, and recommendations. While VLMs offer zero-shot capabilities, their systematic evaluation on multi-attribute fashion tasks with conditional attributes (where some attributes may not be applicable) remains underexplored.

Method: Introduced a three-tier evaluation framework: (1) overall task performance across all classes including NA (not applicable), (2) attribute applicability detection, and (3) fine-grained classification when attributes are determinable. Benchmarked nine VLMs across flagship, efficient, and ultra-efficient tiers against classifiers trained on Fashion-CLIP embeddings using DeepFashion-MultiModal dataset with 5,000 images across 18 attributes.

Result: Zero-shot VLMs achieve 64.0% macro-F1 (3x improvement over logistic regression on Fashion-CLIP). VLMs excel at fine-grained classification (Tier 3: 70.8% F1) but struggle with applicability detection (Tier 2: 34.1% NA-F1). Efficient models achieve over 90% of flagship performance at lower cost.

Conclusion: The diagnostic framework helps practitioners pinpoint whether errors stem from visibility detection or classification, guiding targeted improvements. VLMs show strong zero-shot performance but applicability detection remains a key bottleneck. Efficient models offer practical deployment paths with minimal performance loss.

Abstract: Fine-grained attribute prediction is essential for fashion retail applications including catalog enrichment, visual search, and recommendation systems. Vision-Language Models (VLMs) offer zero-shot prediction without task-specific training, yet their systematic evaluation on multi-attribute fashion tasks remains underexplored. A key challenge is that fashion attributes are often conditional. For example, "outer fabric" is undefined when no outer garment is visible. This requires models to detect attribute applicability before attempting classification. We introduce a three-tier evaluation framework that decomposes this challenge: (1) overall task performance across all classes (including NA class: suggesting attribute is not applicable) for all attributes, (2) attribute applicability detection, and (3) fine-grained classification when attributes are determinable. Using DeepFashion-MultiModal, which explicitly defines NA (meaning attribute doesn't exist or is not visible) within attribute label spaces, we benchmark nine VLMs spanning flagship (GPT-5, Gemini 2.5 Pro), efficient (GPT-5 Mini, Gemini 2.5 Flash), and ultra-efficient tiers (GPT-5 Nano, Gemini 2.5 Flash-Lite) against classifiers trained on pretrained Fashion-CLIP embeddings on 5,000 images across 18 attributes. Our findings reveal that: (1) zero-shot VLMs achieve 64.0% macro-F1, a threefold improvement over logistic regression on pretrained Fashion-CLIP embeddings; (2) VLMs excel at fine-grained classification (Tier 3: 70.8% F1) but struggle with applicability detection (Tier 2: 34.1% NA-F1), identifying a key bottleneck; (3) efficient models achieve over 90% of flagship performance at lower cost, offering practical deployment paths. This diagnostic framework enables practitioners to pinpoint whether errors stem from visibility detection or classification, guiding targeted improvements for production systems.

</details>


### [23] [VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning](https://arxiv.org/abs/2601.15724)
*Chenglin Li,Qianglong Chen,Feng Han,Yikun Wang,Xingxi Yin,Yan Gong,Ruilin Li,Yin Zhang,Jiaqi Wang*

Main category: cs.CV

TL;DR: VideoThinker is an agentic Video Large Language Model trained on synthetic tool interaction trajectories for long-form video understanding, outperforming caption-only agents and video baselines.


<details>
  <summary>Details</summary>
Motivation: Long-form video understanding is challenging for current Video LLMs due to static reasoning over uniformly sampled frames, which causes temporal localization issues and information loss. Agentic tools (temporal retrieval, spatial zoom, temporal zoom) could help but require training data that needs models with strong long-form comprehension, creating a circular dependency.

Method: Convert videos into rich captions, use a powerful agentic language model to generate multi-step tool use sequences in caption space, then ground these trajectories back to video by replacing captions with corresponding frames. This creates a large-scale synthetic dataset of interleaved video and tool reasoning without requiring long-form understanding from the underlying model.

Result: VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks.

Conclusion: The approach demonstrates effectiveness of tool-augmented synthetic data and adaptive retrieval/zoom reasoning for long-form video understanding, breaking the circular dependency in agentic video understanding data creation.

Abstract: Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding.

</details>


### [24] [FAIR-ESI: Feature Adaptive Importance Refinement for Electrophysiological Source Imaging](https://arxiv.org/abs/2601.15731)
*Linyong Zou,Liang Zhang,Xiongfei Wang,Jia-Hong Gao,Yi Sun,Shurong Sheng,Kuntao Xiao,Wanli Yang,Pengfei Teng,Guoming Luan,Zhao Lv,Zikang Xu*

Main category: cs.CV

TL;DR: FAIR-ESI is a novel electrophysiological source imaging framework that adaptively refines feature importance across multiple views (spectral, temporal, patch-wise) to improve brain disorder diagnosis.


<details>
  <summary>Details</summary>
Motivation: Accurate feature selection and refinement is a central challenge for precise electrophysiological source imaging (ESI) in brain disorder diagnosis, despite promising results from model-based optimization and deep learning methods.

Method: FAIR-ESI adaptively refines feature importance across three views: 1) FFT-based spectral feature refinement, 2) weighted temporal feature refinement, and 3) self-attention-based patch-wise feature refinement.

Result: Extensive experiments on two simulation datasets with diverse configurations and two real-world clinical datasets validate the framework's efficacy.

Conclusion: FAIR-ESI has potential to advance brain disorder diagnosis and offer new insights into brain function through adaptive multi-view feature refinement.

Abstract: An essential technique for diagnosing brain disorders is electrophysiological source imaging (ESI). While model-based optimization and deep learning methods have achieved promising results in this field, the accurate selection and refinement of features remains a central challenge for precise ESI. This paper proposes FAIR-ESI, a novel framework that adaptively refines feature importance across different views, including FFT-based spectral feature refinement, weighted temporal feature refinement, and self-attention-based patch-wise feature refinement. Extensive experiments on two simulation datasets with diverse configurations and two real-world clinical datasets validate our framework's efficacy, highlighting its potential to advance brain disorder diagnosis and offer new insights into brain function.

</details>


### [25] [Keyframe-Based Feed-Forward Visual Odometry](https://arxiv.org/abs/2601.16020)
*Weichen Dai,Wenhan Su,Da Kong,Yuhang Ming,Wanzeng Kong*

Main category: cs.CV

TL;DR: Proposes a keyframe-based feed-forward visual odometry method using reinforcement learning for adaptive keyframe selection, improving efficiency and accuracy over current foundation model approaches.


<details>
  <summary>Details</summary>
Motivation: Current foundation model based VO methods process all frames indiscriminately, causing computational redundancy and degraded performance from low inter-frame parallax. Traditional geometric heuristics don't work well with these models due to their reliance on high-dimensional latent representations rather than explicit geometry.

Method: Uses reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. Instead of hand-crafted rules, learns optimal keyframe selection strategy.

Result: Trained on TartanAir dataset and evaluated across several real-world datasets. Achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.

Conclusion: The proposed reinforcement learning-based keyframe selection method successfully bridges the gap between traditional geometric heuristics and modern foundation models, improving both efficiency and accuracy in visual odometry.

Abstract: The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.

</details>


### [26] [Sub-Region-Aware Modality Fusion and Adaptive Prompting for Multi-Modal Brain Tumor Segmentation](https://arxiv.org/abs/2601.15734)
*Shadi Alijani,Fereshteh Aghaee Meibodi,Homayoun Najjaran*

Main category: cs.CV

TL;DR: A novel framework for adapting foundation models to multi-modal medical imaging using sub-region-aware modality attention and adaptive prompt engineering, validated on brain tumor segmentation with superior performance.


<details>
  <summary>Details</summary>
Motivation: Existing foundation models struggle with effective multi-modal fusion and adaptation to heterogeneous pathological tissues in medical imaging, creating a critical unresolved challenge.

Method: Two key innovations: 1) Sub-region-aware modality attention that learns optimal modality combinations for each tumor sub-region, and 2) Adaptive prompt engineering that leverages foundation model capabilities to refine segmentation accuracy.

Result: Validated on BraTS 2020 brain tumor segmentation dataset, significantly outperforms baseline methods, particularly in the challenging necrotic core sub-region.

Conclusion: Provides a principled and effective approach to multi-modal fusion and prompting, paving the way for more accurate and robust foundation model-based solutions in medical imaging.

Abstract: The successful adaptation of foundation models to multi-modal medical imaging is a critical yet unresolved challenge. Existing models often struggle to effectively fuse information from multiple sources and adapt to the heterogeneous nature of pathological tissues. To address this, we introduce a novel framework for adapting foundation models to multi-modal medical imaging, featuring two key technical innovations: sub-region-aware modality attention and adaptive prompt engineering. The attention mechanism enables the model to learn the optimal combination of modalities for each tumor sub-region, while the adaptive prompting strategy leverages the inherent capabilities of foundation models to refine segmentation accuracy. We validate our framework on the BraTS 2020 brain tumor segmentation dataset, demonstrating that our approach significantly outperforms baseline methods, particularly in the challenging necrotic core sub-region. Our work provides a principled and effective approach to multi-modal fusion and prompting, paving the way for more accurate and robust foundation model-based solutions in medical imaging.

</details>


### [27] [DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models](https://arxiv.org/abs/2601.16065)
*Chenyang Li,Jieyuan Liu,Bin Li,Bo Gao,Yilin Yuan,Yangfan He,Yuchen Li,Jingqun Tang*

Main category: cs.CV

TL;DR: A plug-and-play Distracting Token Pruning (DTP) framework that dynamically detects and prunes distracting image tokens in Vision-Language Action models to improve task success rates without modifying model architecture.


<details>
  <summary>Details</summary>
Motivation: VLA models often overly attend to task-irrelevant image regions (distracting tokens), which disturbs action generation and reduces task success rates. The authors aim to correct visual attention patterns to improve performance.

Method: Proposes Distracting Token Pruning (DTP) framework that dynamically identifies and prunes distracting image tokens during inference. It's a plug-and-play approach that doesn't alter the original VLA architecture or require additional inputs.

Result: Experiments on SIMPLER Benchmark show consistent relative improvements in task success rates across different VLA models. Analysis reveals negative correlation between task success rate and attention to task-irrelevant regions across all tested models.

Conclusion: DTP effectively improves VLA model performance by pruning distracting tokens, demonstrating generalizability to transformer-based VLAs. The identified attention pattern issue is a common phenomenon that could guide future VLA research.

Abstract: Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as 'distracting tokens'. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model's visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.

</details>


### [28] [Breaking the Resolution Barrier: Arbitrary-resolution Deep Image Steganography Framework](https://arxiv.org/abs/2601.15739)
*Xinjue Hu,Chi Wang,Boyu Wang,Xiang Zhang,Zhenshan Tan,Zhangjie Fu*

Main category: cs.CV

TL;DR: ARDIS is the first arbitrary resolution deep image steganography framework that enables hiding and recovering secret images at any resolution without requiring pre-resampling or known resolution values.


<details>
  <summary>Details</summary>
Motivation: Current deep image steganography methods require secret and cover images to have the same resolution, forcing resampling that causes detail loss and preventing recovery to original resolution when resolution is unknown.

Method: 1) Frequency Decoupling Architecture: Disentangles secret into resolution-aligned global basis and resolution-agnostic high-frequency latent for hiding in fixed-resolution cover. 2) Latent-Guided Implicit Reconstructor: Uses recovered detail latent to modulate continuous implicit function for high-frequency residual rendering. 3) Implicit Resolution Coding: Transforms discrete resolution values into dense feature maps hidden in feature domain for blind recovery.

Result: ARDIS significantly outperforms state-of-the-art methods in both invisibility and cross-resolution recovery fidelity, enabling faithful restoration of original details at arbitrary resolutions.

Conclusion: ARDIS successfully shifts the paradigm from discrete mapping to reference-guided continuous signal reconstruction, solving resolution mismatch problems in deep image steganography and enabling arbitrary resolution hiding and recovery.

Abstract: Deep image steganography (DIS) has achieved significant results in capacity and invisibility. However, current paradigms enforce the secret image to maintain the same resolution as the cover image during hiding and revealing. This leads to two challenges: secret images with inconsistent resolutions must undergo resampling beforehand which results in detail loss during recovery, and the secret image cannot be recovered to its original resolution when the resolution value is unknown. To address these, we propose ARDIS, the first Arbitrary Resolution DIS framework, which shifts the paradigm from discrete mapping to reference-guided continuous signal reconstruction. Specifically, to minimize the detail loss caused by resolution mismatch, we first design a Frequency Decoupling Architecture in hiding stage. It disentangles the secret into a resolution-aligned global basis and a resolution-agnostic high-frequency latent to hide in a fixed-resolution cover. Second, for recovery, we propose a Latent-Guided Implicit Reconstructor to perform deterministic restoration. The recovered detail latent code modulates a continuous implicit function to accurately query and render high-frequency residuals onto the recovered global basis, ensuring faithful restoration of original details. Furthermore, to achieve blind recovery, we introduce an Implicit Resolution Coding strategy. By transforming discrete resolution values into dense feature maps and hiding them in the redundant space of the feature domain, the reconstructor can correctly decode the secret's resolution directly from the steganographic representation. Experimental results demonstrate that ARDIS significantly outperforms state-of-the-art methods in both invisibility and cross-resolution recovery fidelity.

</details>


### [29] [White-Box mHC: Electromagnetic Spectrum-Aware and Interpretable Stream Interactions for Hyperspectral Image Classification](https://arxiv.org/abs/2601.15757)
*Yimin Zhu,Lincoln Linlin Xu,Zhengsen Xu,Zack Dewis,Mabel Heffring,Saeid Taleghanidoozdoozan,Motasem Alkayid,Quinn Ledingham,Megan Greenwood*

Main category: cs.CV

TL;DR: ES-mHC is a white-box hyperspectral image classification framework that uses structured hyper-connection matrices to explicitly model electromagnetic spectrum interactions, improving interpretability by separating feature representation from interaction structure.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models for hyperspectral image classification rely on opaque spectral-spatial feature mixing, which limits interpretability and prevents understanding of internal decision mechanisms. There's a need for more transparent models that reveal how different electromagnetic spectrum groupings interact.

Method: ES-mHC uses a hyper-connection framework with structured, directional matrices to explicitly model interactions among different electromagnetic spectrum groupings (residual streams). It separates feature representation from interaction structure, allowing electromagnetic spectrum grouping specialization and reducing redundancy.

Result: The learned hyper-connection matrices exhibit coherent spatial patterns and asymmetric interaction behaviors, providing mechanistic insight into model internal dynamics. Increasing the expansion rate accelerates the emergence of structured interaction patterns.

Conclusion: ES-mHC transforms hyperspectral image classification from a black-box prediction task into a structurally transparent, partially white-box learning process that exposes internal information flow for direct visualization and spatial analysis.

Abstract: In hyperspectral image classification (HSIC), most deep learning models rely on opaque spectral-spatial feature mixing, limiting their interpretability and hindering understanding of internal decision mechanisms. We present physical spectrum-aware white-box mHC, named ES-mHC, a hyper-connection framework that explicitly models interactions among different electromagnetic spectrum groupings (residual stream in mHC) interactions using structured, directional matrices. By separating feature representation from interaction structure, ES-mHC promotes electromagnetic spectrum grouping specialization, reduces redundancy, and exposes internal information flow that can be directly visualized and spatially analyzed. Using hyperspectral image classification as a representative testbed, we demonstrate that the learned hyper-connection matrices exhibit coherent spatial patterns and asymmetric interaction behaviors, providing mechanistic insight into the model internal dynamics. Furthermore, we find that increasing the expansion rate accelerates the emergence of structured interaction patterns. These results suggest that ES-mHC transforms HSIC from a purely black-box prediction task into a structurally transparent, partially white-box learning process.

</details>


### [30] [Atlas-Assisted Segment Anything Model for Fetal Brain MRI (FeTal-SAM)](https://arxiv.org/abs/2601.15759)
*Qi Zeng,Weide Liu,Bo Li,Ryne Didier,P. Ellen Grant,Davood Karimi*

Main category: cs.CV

TL;DR: FeTal-SAM adapts Segment Anything Model for fetal brain MRI segmentation using atlas-based prompts, enabling flexible segmentation without retraining for different label definitions.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning methods require large annotated datasets for fixed label sets and lack flexibility when clinical/research needs change. There's also limited insight into whether segmentations are driven by genuine image contrast or learned spatial priors.

Method: Integrates atlas-based prompts with foundation-model principles. Uses multi-atlas registration to generate spatially aligned label templates as dense prompts, plus bounding-box prompts for SAM's segmentation decoder. Performs binary segmentation per structure and fuses results to reconstruct full 3D volumes.

Result: Achieves Dice scores comparable to state-of-the-art baselines for well-contrasted structures (cortical plate, cerebellum) across gestational ages on dHCP and in-house datasets. Shows slightly lower accuracy for subtle, low-contrast structures (hippocampus, amygdala). Maintains flexibility to segment any user-specified anatomy without exhaustive retraining.

Conclusion: FeTal-SAM represents a promising step toward clinically adaptable fetal brain MRI analysis tools, serving as a general-purpose segmentation model that addresses limitations of traditional methods while maintaining performance for well-contrasted structures.

Abstract: This paper presents FeTal-SAM, a novel adaptation of the Segment Anything Model (SAM) tailored for fetal brain MRI segmentation. Traditional deep learning methods often require large annotated datasets for a fixed set of labels, making them inflexible when clinical or research needs change. By integrating atlas-based prompts and foundation-model principles, FeTal-SAM addresses two key limitations in fetal brain MRI segmentation: (1) the need to retrain models for varying label definitions, and (2) the lack of insight into whether segmentations are driven by genuine image contrast or by learned spatial priors. We leverage multi-atlas registration to generate spatially aligned label templates that serve as dense prompts, alongside a bounding-box prompt, for SAM's segmentation decoder. This strategy enables binary segmentation on a per-structure basis, which is subsequently fused to reconstruct the full 3D segmentation volumes. Evaluations on two datasets, the dHCP dataset and an in-house dataset demonstrate FeTal-SAM's robust performance across gestational ages. Notably, it achieves Dice scores comparable to state-of-the-art baselines which were trained for each dataset and label definition for well-contrasted structures like cortical plate and cerebellum, while maintaining the flexibility to segment any user-specified anatomy. Although slightly lower accuracy is observed for subtle, low-contrast structures (e.g., hippocampus, amygdala), our results highlight FeTal-SAM's potential to serve as a general-purpose segmentation model without exhaustive retraining. This method thus constitutes a promising step toward clinically adaptable fetal brain MRI analysis tools.

</details>


### [31] [LL-GaussianMap: Zero-shot Low-Light Image Enhancement via 2D Gaussian Splatting Guided Gain Maps](https://arxiv.org/abs/2601.15766)
*Yuhan Chen,Ying Fang,Guofa Li,Wenxuan Yu,Yicui Shi,Jingrui Zhang,Kefei Qian,Wenbo Chu,Keqiang Li*

Main category: cs.CV

TL;DR: LL-GaussianMap is the first unsupervised framework using 2D Gaussian Splatting for low-light image enhancement, formulating enhancement as gain map generation guided by 2DGS primitives to preserve edges and suppress artifacts.


<details>
  <summary>Details</summary>
Motivation: Most existing low-light enhancement methods operate in pixel domain or use implicit features, neglecting intrinsic geometric structural priors. 2D Gaussian Splatting has superior structural fitting capabilities but hasn't been explored for low-level vision tasks.

Method: Two-stage approach: 1) High-fidelity structural reconstruction using 2DGS, 2) Data-driven enhancement dictionary coefficients rendered via Gaussian splatting rasterization through a unified enhancement module. Formulates enhancement as gain map generation guided by 2DGS primitives.

Result: Achieves superior enhancement performance with extremely low storage footprint, demonstrating effectiveness of explicit Gaussian representations for image enhancement while preserving edges and suppressing artifacts.

Conclusion: LL-GaussianMap successfully bridges the gap by incorporating 2DGS into low-light enhancement, showing that explicit Gaussian representations can effectively preserve structural priors and achieve high-quality enhancement with minimal storage requirements.

Abstract: Significant progress has been made in low-light image enhancement with respect to visual quality. However, most existing methods primarily operate in the pixel domain or rely on implicit feature representations. As a result, the intrinsic geometric structural priors of images are often neglected. 2D Gaussian Splatting (2DGS) has emerged as a prominent explicit scene representation technique characterized by superior structural fitting capabilities and high rendering efficiency. Despite these advantages, the utilization of 2DGS in low-level vision tasks remains unexplored. To bridge this gap, LL-GaussianMap is proposed as the first unsupervised framework incorporating 2DGS into low-light image enhancement. Distinct from conventional methodologies, the enhancement task is formulated as a gain map generation process guided by 2DGS primitives. The proposed method comprises two primary stages. First, high-fidelity structural reconstruction is executed utilizing 2DGS. Then, data-driven enhancement dictionary coefficients are rendered via the rasterization mechanism of Gaussian splatting through an innovative unified enhancement module. This design effectively incorporates the structural perception capabilities of 2DGS into gain map generation, thereby preserving edges and suppressing artifacts during enhancement. Additionally, the reliance on paired data is circumvented through unsupervised learning. Experimental results demonstrate that LL-GaussianMap achieves superior enhancement performance with an extremely low storage footprint, highlighting the effectiveness of explicit Gaussian representations for image enhancement.

</details>


### [32] [LL-GaussianImage: Efficient Image Representation for Zero-shot Low-Light Enhancement with 2D Gaussian Splatting](https://arxiv.org/abs/2601.15772)
*Yuhan Chen,Wenxuan Yu,Guofa Li,Yijun Xu,Ying Fang,Yicui Shi,Long Cao,Wenbo Chu,Keqiang Li*

Main category: cs.CV

TL;DR: LL-GaussianImage: First zero-shot unsupervised framework for low-light enhancement directly within 2D Gaussian Splatting compressed representation domain, enabling compression-as-enhancement without decompression.


<details>
  <summary>Details</summary>
Motivation: Existing low-light enhancement algorithms work in pixel domain, requiring inefficient decompression-enhancement-recompression pipeline for 2DGS-compressed images, causing efficiency loss and secondary degradation.

Method: 1) Semantic-guided Mixture-of-Experts framework applies dynamic adaptive transformations to sparse 2DGS attribute space using rendered images as guidance. 2) Multi-objective collaborative loss function constrains smoothness and fidelity. 3) Two-stage optimization with single-scale reconstruction ensures base representation accuracy.

Result: Achieves high-quality low-light image enhancement while maintaining high compression ratios, validating feasibility of direct processing within compressed representation domain.

Conclusion: LL-GaussianImage demonstrates superior paradigm for direct enhancement within 2DGS compressed domain, eliminating need for decompression-recompression pipeline while preserving compression efficiency.

Abstract: 2D Gaussian Splatting (2DGS) is an emerging explicit scene representation method with significant potential for image compression due to high fidelity and high compression ratios. However, existing low-light enhancement algorithms operate predominantly within the pixel domain. Processing 2DGS-compressed images necessitates a cumbersome decompression-enhancement-recompression pipeline, which compromises efficiency and introduces secondary degradation. To address these limitations, we propose LL-GaussianImage, the first zero-shot unsupervised framework designed for low-light enhancement directly within the 2DGS compressed representation domain. Three primary advantages are offered by this framework. First, a semantic-guided Mixture-of-Experts enhancement framework is designed. Dynamic adaptive transformations are applied to the sparse attribute space of 2DGS using rendered images as guidance to enable compression-as-enhancement without full decompression to a pixel grid. Second, a multi-objective collaborative loss function system is established to strictly constrain smoothness and fidelity during enhancement, suppressing artifacts while improving visual quality. Third, a two-stage optimization process is utilized to achieve reconstruction-as-enhancement. The accuracy of the base representation is ensured through single-scale reconstruction and network robustness is enhanced. High-quality enhancement of low-light images is achieved while high compression ratios are maintained. The feasibility and superiority of the paradigm for direct processing within the compressed representation domain are validated through experimental results.

</details>


### [33] [Diffusion Model-Based Data Augmentation for Enhanced Neuron Segmentation](https://arxiv.org/abs/2601.15779)
*Liuyun Jiang,Yanchao Zhang,Jinyue Guo,Yizhuo Lu,Ruining Zhou,Hua Han*

Main category: cs.CV

TL;DR: A diffusion-based data augmentation framework generates diverse, structurally plausible image-label pairs for neuron segmentation in EM, improving performance in low-annotation regimes.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods for neuron segmentation in electron microscopy require large annotated datasets, which are time-consuming to create. Traditional augmentation methods produce highly correlated samples lacking structural diversity.

Method: Proposes a diffusion-based framework with: 1) resolution-aware conditional diffusion model with multi-scale conditioning and EM resolution priors for voxel-level image synthesis from 3D masks, and 2) biology-guided mask remodeling module for enhanced structural realism in augmented masks.

Result: On AC3 and AC4 datasets under low-annotation regimes, improves ARAND metric by 32.1% and 30.7% respectively when combined with two different post-processing methods.

Conclusion: The diffusion-based augmentation framework effectively enriches training sets and improves neuron segmentation performance, especially valuable when manual annotations are limited.

Abstract: Neuron segmentation in electron microscopy (EM) aims to reconstruct the complete neuronal connectome; however, current deep learning-based methods are limited by their reliance on large-scale training data and extensive, time-consuming manual annotations. Traditional methods augment the training set through geometric and photometric transformations; however, the generated samples remain highly correlated with the original images and lack structural diversity. To address this limitation, we propose a diffusion-based data augmentation framework capable of generating diverse and structurally plausible image-label pairs for neuron segmentation. Specifically, the framework employs a resolution-aware conditional diffusion model with multi-scale conditioning and EM resolution priors to enable voxel-level image synthesis from 3D masks. It further incorporates a biology-guided mask remodeling module that produces augmented masks with enhanced structural realism. Together, these components effectively enrich the training set and improve segmentation performance. On the AC3 and AC4 datasets under low-annotation regimes, our method improves the ARAND metric by 32.1% and 30.7%, respectively, when combined with two different post-processing methods. Our code is available at https://github.com/HeadLiuYun/NeuroDiff.

</details>


### [34] [Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video](https://arxiv.org/abs/2601.15780)
*Pascal Benschop,Justin Dauwels,Jan van Gemert*

Main category: cs.CV

TL;DR: VLMs struggle with spatial reasoning tasks involving subtle temporal/geometric cues, performing near chance on a new synthetic benchmark testing situational and spatial awareness through minimal video pairs.


<details>
  <summary>Details</summary>
Motivation: Spatial reasoning in VLMs remains fragile when semantics depend on subtle temporal or geometric cues, highlighting the need for better evaluation of their situational and spatial awareness capabilities.

Method: Introduce a synthetic benchmark with minimal video pairs testing three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment.

Result: Recent VLMs perform only slightly above chance across all tasks. Stable color cues partly reduce assailant role confusions but don't resolve underlying weaknesses.

Conclusion: The benchmark provides reproducible diagnostics and seeds exploration of lightweight spatial priors to complement large-scale pretraining, as current VLMs show fundamental limitations in spatial reasoning.

Abstract: Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.

</details>


### [35] [A Mobile Application for Flower Recognition System Based on Convolutional Neural Networks](https://arxiv.org/abs/2601.15810)
*Mustafa Yurdakul,Enes Ayan,Fahrettin Horasan,Sakir Tasdemir*

Main category: cs.CV

TL;DR: Mobile app using CNN models for flower classification, with DenseNet-121 + SGD achieving best performance (95.84% accuracy).


<details>
  <summary>Details</summary>
Motivation: Flowers have many uses but identifying them requires expert knowledge, which isn't always accessible. Need for a mobile solution to provide quick and easy flower type information to non-specialists.

Method: Developed mobile application using three CNN models (MobileNet, DenseNet121, Xception) and evaluated them with seven different optimization algorithms to find the best combination for mobile deployment.

Result: DenseNet-121 with stochastic gradient descent (SGD) optimization achieved the best performance: 95.84% accuracy, 96.00% precision, recall, and F1-score.

Conclusion: CNNs can be effectively used for flower classification in mobile applications, with DenseNet-121 + SGD being the most suitable model for this purpose.

Abstract: A convolutional neural network (CNN) is a deep learning algorithm that has been specifically designed for computer vision applications. The CNNs proved successful in handling the increasing amount of data in many computer vision problems, where classical machine learning algorithms were insufficient. Flowers have many uses in our daily lives, from decorating to making medicines to detoxifying the environment. Identifying flower types requires expert knowledge. However, accessing experts at any time and in any location may not always be feasible. In this study a mobile application based on CNNs was developed to recognize different types of flowers to provide non-specialists with quick and easy access to information about flower types. The study employed three distinct CNN models, namely MobileNet, DenseNet121, and Xception, to determine the most suitable model for the mobile application. The classification performances of the models were evaluated by training them with seven different optimization algorithms. The DenseNet-121 architecture, which uses the stochastic gradient descent (SGD) optimization algorithm, was the most successful, achieving 95.84 % accuracy, 96.00% precision, recall, and F1-score. This result shows that CNNs can be used for flower classification in mobile applications.

</details>


### [36] [Beyond Off-the-Shelf Models: A Lightweight and Accessible Machine Learning Pipeline for Ecologists Working with Image Data](https://arxiv.org/abs/2601.15813)
*Clare Chemery,Hendrik Edelhoff,Ludwig Bothmann*

Main category: cs.CV

TL;DR: A lightweight ML pipeline for ecologists to build custom image classifiers for wildlife monitoring without advanced ML expertise, demonstrated with 90-96% accuracy on deer age/sex classification.


<details>
  <summary>Details</summary>
Motivation: Lower the barrier for ecologists to apply ML to image classification tasks in ecological research, enabling them to move beyond off-the-shelf models and develop tailored solutions for specific local datasets and research questions.

Method: Developed a lightweight experimentation pipeline combining a command-line interface for preprocessing, training, and evaluation with a graphical interface for annotation, error analysis, and model comparison. Applied to classify red deer by age and sex using 4352 cropped images from camera traps, experimenting with multiple backbone architectures, parameters, and data augmentation strategies.

Result: Best-performing models achieved 90.77% accuracy for age classification and 96.15% for sex classification on red deer images, demonstrating reliable demographic classification is feasible even with limited data for well-defined ecological problems.

Conclusion: The framework provides ecologists with an accessible tool for developing ML models tailored to specific research questions, enabling broader adoption of ML in wildlife monitoring and demographic analysis without requiring advanced ML expertise.

Abstract: We introduce a lightweight experimentation pipeline designed to lower the barrier for applying machine learning (ML) methods for classifying images in ecological research. We enable ecologists to experiment with ML models independently, thus they can move beyond off-the-shelf models and generate insights tailored to local datasets and specific classification tasks and target variables. Our tool combines a simple command-line interface for preprocessing, training, and evaluation with a graphical interface for annotation, error analysis, and model comparison. This design enables ecologists to build and iterate on compact, task-specific classifiers without requiring advanced ML expertise. As a proof of concept, we apply the pipeline to classify red deer (Cervus elaphus) by age and sex from 3392 camera trap images collected in the Veldenstein Forest, Germany. Using 4352 cropped images containing individual deer labeled by experts, we trained and evaluated multiple backbone architectures with a wide variety of parameters and data augmentation strategies. Our best-performing models achieved 90.77% accuracy for age classification and 96.15% for sex classification. These results demonstrate that reliable demographic classification is feasible even with limited data to answer narrow, well-defined ecological problems. More broadly, the framework provides ecologists with an accessible tool for developing ML models tailored to specific research questions, paving the way for broader adoption of ML in wildlife monitoring and demographic analysis.

</details>


### [37] [Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion](https://arxiv.org/abs/2601.15829)
*Yonghao Xu,Pedram Ghamisi,Qihao Weng*

Main category: cs.CV

TL;DR: First application of dataset distillation to remote sensing imagery using diffusion models with classifier guidance and latent clustering to create compact, representative training datasets.


<details>
  <summary>Details</summary>
Motivation: Address challenges of large-scale remote sensing datasets: high storage/computational costs and data leakage risks, especially for sensitive categories.

Method: Train text-to-image diffusion model to condense datasets; add classifier-driven guidance via classification consistency loss; use latent space clustering for representative prototypes and visual language model for aggregated text descriptions.

Result: Method successfully distills realistic and diverse samples for downstream training on three high-resolution remote sensing scene classification benchmarks.

Conclusion: Dataset distillation is viable for remote sensing imagery, enabling efficient model training while mitigating data privacy concerns, with code and models publicly available.

Abstract: Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD).

</details>


### [38] [An IoT-Based Smart Plant Monitoring and Irrigation System with Real-Time Environmental Sensing, Automated Alerts, and Cloud Analytics](https://arxiv.org/abs/2601.15830)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: IoT-based smart plant monitoring system using ESP32 with multiple sensors (temperature, humidity, water level, soil moisture) that automates irrigation, reduces water consumption by 40%, and provides cloud-based monitoring via ThingSpeak at $45.20 cost.


<details>
  <summary>Details</summary>
Motivation: Global demand for sustainable agriculture requires intelligent monitoring to optimize resources and plant health. Traditional farming methods lead to water wastage, inconsistent growth, and delayed response to environmental changes.

Method: ESP32 microcontroller collects real-time data from DHT22 (temperature/humidity), HC-SR04 (water level), and soil moisture sensors. System includes OLED display, buzzer alerts, and transmits data wirelessly to ThingSpeak cloud platform for remote monitoring and automated alerts.

Result: System maintains optimal soil moisture with 92% accuracy, reduces water consumption by approximately 40% compared to conventional methods, and provides comprehensive web dashboard visualization. Total implementation cost is $45.20.

Conclusion: The IoT-based smart plant monitoring system provides an affordable, scalable solution for precision agriculture, suitable for both small-scale gardening and commercial applications, addressing sustainable agriculture needs through automation and cloud analytics.

Abstract: The increasing global demand for sustainable agriculture necessitates intelligent monitoring systems that optimize resource utilization and plant health management. Traditional farming methods rely on manual observation and periodic watering, often leading to water wastage, inconsistent plant growth, and delayed response to environmental changes. This paper presents a comprehensive IoT-based smart plant monitoring system that integrates multiple environmental sensors with automated irrigation and cloud analytics. The proposed system utilizes an ESP32 microcontroller to collect real-time data from DHT22 (temperature/humidity), HC-SR04 (water level), and soil moisture sensors, with visual feedback through an OLED display and auditory alerts via a buzzer. All sensor data is wirelessly transmitted to the ThingSpeak cloud platform for remote monitoring, historical analysis, and automated alert generation. Experimental results demonstrate the system's effectiveness in maintaining optimal soil moisture levels (with 92\% accuracy), providing real-time environmental monitoring, and reducing water consumption by approximately 40\% compared to conventional irrigation methods. The integrated web dashboard offers comprehensive visualization of plant health parameters, making it suitable for both small-scale gardening and commercial agriculture applications. With a total implementation cost of \$45.20, this system provides an affordable, scalable solution for precision agriculture and smart farming.

</details>


### [39] [TinySense: Effective CSI Compression for Scalable and Accurate Wi-Fi Sensing](https://arxiv.org/abs/2601.15838)
*Toan Gian,Dung T. Tran,Viet Quoc Pham,Francesco Restuccia,Van-Dinh Nguyen*

Main category: cs.CV

TL;DR: TinySense: A VQGAN-based compression framework for efficient Wi-Fi sensing that reduces CSI data size while maintaining human pose estimation accuracy, with dynamic bitrate adjustment and transformer-based robustness enhancement.


<details>
  <summary>Details</summary>
Motivation: Existing Wi-Fi sensing methods for human pose estimation process large amounts of CSI data, straining networking resources. There's a need for device-free, privacy-preserving sensing solutions that are more efficient and scalable.

Method: Uses a vector quantization-based generative adversarial network (VQGAN) to compress CSI data. Employs K-means algorithm to dynamically adjust compression bitrates by clustering pre-trained codebook into smaller subsets. Incorporates a Transformer model to mitigate bitrate loss and enhance robustness in unreliable networking conditions.

Result: TinySense achieves up to 1.5x higher HPE accuracy score (PCK20) under same compression rate compared to state-of-the-art methods. Reduces latency by up to 5x and networking overhead by up to 2.5x. Prototyped on Jetson Nano and Raspberry Pi testbed.

Conclusion: TinySense provides an efficient compression framework that enhances scalability of Wi-Fi-based human sensing while maintaining accuracy, making it suitable for practical deployment with reduced resource requirements.

Abstract: With the growing demand for device-free and privacy-preserving sensing solutions, Wi-Fi sensing has emerged as a promising approach for human pose estimation (HPE). However, existing methods often process vast amounts of channel state information (CSI) data directly, ultimately straining networking resources. This paper introduces TinySense, an efficient compression framework that enhances the scalability of Wi-Fi-based human sensing. Our approach is based on a new vector quantization-based generative adversarial network (VQGAN). Specifically, by leveraging a VQGAN-learned codebook, TinySense significantly reduces CSI data while maintaining the accuracy required for reliable HPE. To optimize compression, we employ the K-means algorithm to dynamically adjust compression bitrates to cluster a large-scale pre-trained codebook into smaller subsets. Furthermore, a Transformer model is incorporated to mitigate bitrate loss, enhancing robustness in unreliable networking conditions. We prototype TinySense on an experimental testbed using Jetson Nano and Raspberry Pi to measure latency and network resource use. Extensive results demonstrate that TinySense significantly outperforms state-of-the-art compression schemes, achieving up to 1.5x higher HPE accuracy score (PCK20) under the same compression rate. It also reduces latency and networking overhead, respectively, by up to 5x and 2.5x. The code repository is available online at here.

</details>


### [40] [A Lightweight Brain-Inspired Machine Learning Framework for Coronary Angiography: Hybrid Neural Representation and Robust Learning Strategies](https://arxiv.org/abs/2601.15865)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: Brain-inspired lightweight CNN framework achieves robust coronary angiography classification despite complex lesions, class imbalance, and limited computational resources.


<details>
  <summary>Details</summary>
Motivation: Real-world coronary angiography images present challenges including complex lesion morphology, severe class imbalance, label uncertainty, and limited computational resources, which hinder conventional deep learning approaches' robustness and generalization.

Method: Uses pretrained CNN for lightweight hybrid neural representation with selective neural plasticity training, brain-inspired attention-modulated loss (Focal Loss + label smoothing), class-imbalance-aware sampling, and cosine annealing with warm restarts to mimic biological neural regulation.

Result: The brain-inspired lightweight model achieves strong, stable performance in binary coronary angiography classification with competitive accuracy, recall, F1-score, and AUC metrics while maintaining high computational efficiency.

Conclusion: Validates effectiveness of brain-inspired learning mechanisms for lightweight medical image analysis, providing a biologically plausible and deployable solution for intelligent clinical decision support under limited computational resources.

Abstract: Background: Coronary angiography (CAG) is a cornerstone imaging modality for assessing coronary artery disease and guiding interventional treatment decisions. However, in real-world clinical settings, angiographic images are often characterized by complex lesion morphology, severe class imbalance, label uncertainty, and limited computational resources, posing substantial challenges to conventional deep learning approaches in terms of robustness and generalization.Methods: The proposed framework is built upon a pretrained convolutional neural network to construct a lightweight hybrid neural representation. A selective neural plasticity training strategy is introduced to enable efficient parameter adaptation. Furthermore, a brain-inspired attention-modulated loss function, combining Focal Loss with label smoothing, is employed to enhance sensitivity to hard samples and uncertain annotations. Class-imbalance-aware sampling and cosine annealing with warm restarts are adopted to mimic rhythmic regulation and attention allocation mechanisms observed in biological neural systems.Results: Experimental results demonstrate that the proposed lightweight brain-inspired model achieves strong and stable performance in binary coronary angiography classification, yielding competitive accuracy, recall, F1-score, and AUC metrics while maintaining high computational efficiency.Conclusion: This study validates the effectiveness of brain-inspired learning mechanisms in lightweight medical image analysis and provides a biologically plausible and deployable solution for intelligent clinical decision support under limited computational resources.

</details>


### [41] [Out-of-Distribution Detection Based on Total Variation Estimation](https://arxiv.org/abs/2601.15867)
*Dabiao Ma,Zhiba Su,Jian Yang,Haojun Fei*

Main category: cs.CV

TL;DR: TV-OOD is a new OOD detection method that uses Total Variation Network Estimator to calculate each input's contribution to total variation, creating a score to distinguish in-distribution from out-of-distribution data.


<details>
  <summary>Details</summary>
Motivation: To improve security of ML model deployments against distribution shifts in practical applications, addressing limitations of existing OOD detection methods.

Method: Leverages Total Variation Network Estimator to calculate each input's contribution to overall total variation, defining this as the total variation score for OOD discrimination.

Result: Tested across various models and datasets, consistently yielding comparable or superior results to state-of-the-art OOD detection techniques across all evaluation metrics in image classification tasks.

Conclusion: TV-OOD provides an effective novel approach for OOD detection that outperforms or matches existing methods, enhancing ML model security against distribution shifts.

Abstract: This paper introduces a novel approach to securing machine learning model deployments against potential distribution shifts in practical applications, the Total Variation Out-of-Distribution (TV-OOD) detection method. Existing methods have produced satisfactory results, but TV-OOD improves upon these by leveraging the Total Variation Network Estimator to calculate each input's contribution to the overall total variation. By defining this as the total variation score, TV-OOD discriminates between in- and out-of-distribution data. The method's efficacy was tested across a range of models and datasets, consistently yielding results in image classification tasks that were either comparable or superior to those achieved by leading-edge out-of-distribution detection techniques across all evaluation metrics.

</details>


### [42] [PMPBench: A Paired Multi-Modal Pan-Cancer Benchmark for Medical Image Synthesis](https://arxiv.org/abs/2601.15884)
*Yifan Chen,Fei Yin,Hao Chen,Jia Wu,Chao Li*

Main category: cs.CV

TL;DR: Introduces PMPBench: first public, fully paired pan-cancer medical imaging dataset spanning 11 organs with complete DCE-MR sequences and paired CT/CTC, enabling rigorous evaluation of contrast synthesis methods.


<details>
  <summary>Details</summary>
Motivation: Contrast medium is crucial for radiological imaging but not always feasible due to patient health or resource constraints. Existing datasets are limited to brain MR, have partial pairing, missing modalities, poor alignment, and lack phase labeling. Most resources remain private.

Method: Created a comprehensive public dataset with complete dynamic contrast-enhanced (DCE) MR sequences (all three phases DCE1-DCE3) and paired non-contrast/contrast-enhanced CT (CTC) across 11 human organs. Curated for anatomical correspondence to enable evaluation of 1-to-1, N-to-1, and N-to-N translation settings.

Result: Established a comprehensive benchmark with results from representative contemporary image-to-image translation baselines. Released the dataset and benchmark publicly to catalyze research on safe, effective contrast synthesis.

Conclusion: PMPBench bridges the data gap in contrast synthesis research by providing the first public, fully paired pan-cancer imaging dataset, enabling rigorous evaluation and advancing multi-organ oncology imaging workflows.

Abstract: Contrast medium plays a pivotal role in radiological imaging, as it amplifies lesion conspicuity and improves detection for the diagnosis of tumor-related diseases. However, depending on the patient's health condition or the medical resources available, the use of contrast medium is not always feasible. Recent work has explored AI-based image translation to synthesize contrast-enhanced images directly from non-contrast scans, aims to reduce side effects and streamlines clinical workflows. Progress in this direction has been constrained by data limitations: (1) existing public datasets focus almost exclusively on brain-related paired MR modalities; (2) other collections include partially paired data but suffer from missing modalities/timestamps and imperfect spatial alignment; (3) explicit labeling of CT vs. CTC or DCE phases is often absent; (4) substantial resources remain private. To bridge this gap, we introduce the first public, fully paired, pan-cancer medical imaging dataset spanning 11 human organs. The MR data include complete dynamic contrast-enhanced (DCE) sequences covering all three phases (DCE1-DCE3), while the CT data provide paired non-contrast and contrast-enhanced acquisitions (CTC). The dataset is curated for anatomical correspondence, enabling rigorous evaluation of 1-to-1, N-to-1, and N-to-N translation settings (e.g., predicting DCE phases from non-contrast inputs). Built upon this resource, we establish a comprehensive benchmark. We report results from representative baselines of contemporary image-to-image translation. We release the dataset and benchmark to catalyze research on safe, effective contrast synthesis, with direct relevance to multi-organ oncology imaging workflows. Our code and dataset are publicly available at https://github.com/YifanChen02/PMPBench.

</details>


### [43] [Understanding the Transfer Limits of Vision Foundation Models](https://arxiv.org/abs/2601.15888)
*Shiqi Huang,Yipei Wang,Natasha Thorley,Alexander Ng,Shaheer Saeed,Mark Emberton,Shonit Punwani,Veeru Kasivisvanathan,Dean Barratt,Daniel Alexander,Yipeng Hu*

Main category: cs.CV

TL;DR: Vision foundation models show uneven performance across tasks due to pretraining-downstream objective mismatch; better alignment correlates with improved transfer learning in medical imaging applications.


<details>
  <summary>Details</summary>
Motivation: Vision foundation models (VFMs) require substantial computational investment but show inconsistent improvements across downstream tasks compared to language foundation models. The authors hypothesize this stems from a mismatch between pretraining objectives (like masked image reconstruction or contrastive learning) and the specific requirements of downstream vision tasks such as segmentation, classification, or image synthesis.

Method: The study evaluates two VFMs - a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet) - on five prostate multiparametric MR imaging tasks. They investigate how task alignment between pretraining and downstream applications influences transfer performance by measuring divergence metrics like maximum-mean-discrepancy (MMD) between features before and after fine-tuning.

Result: Better alignment between pretraining objectives and downstream tasks correlates with greater performance improvements and faster convergence. The MMD metric effectively measures this alignment and predicts transfer learning success in clinical imaging applications.

Conclusion: Pretraining objectives should be designed and analyzed with downstream applicability in mind, as task alignment significantly impacts the effectiveness of vision foundation models in transfer learning scenarios, particularly in specialized domains like medical imaging.

Abstract: Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind.

</details>


### [44] [RadJEPA: Radiology Encoder for Chest X-Rays via Joint Embedding Predictive Architecture](https://arxiv.org/abs/2601.15891)
*Anas Anwarul Haq Khan,Mariam Husain,Kshitij Jadhav*

Main category: cs.CV

TL;DR: RadJEPA is a self-supervised framework for radiology that learns visual representations without language supervision by predicting masked image regions in latent space, outperforming state-of-the-art methods on medical tasks.


<details>
  <summary>Details</summary>
Motivation: Medical vision language models are limited by the need for paired image-text data. The authors question whether robust radiology encoders can be learned without language supervision, aiming to overcome data constraints in medical imaging.

Method: RadJEPA uses a Joint Embedding Predictive Architecture trained solely on unlabeled chest X-ray images. It learns by predicting latent representations of masked image regions, focusing on latent-space prediction rather than aligning global representations across views or modalities.

Result: RadJEPA achieves performance exceeding state-of-the-art approaches including Rad-DINO across disease classification, semantic segmentation, and report generation benchmarks.

Conclusion: The framework demonstrates that robust radiology encoders can be effectively learned without language supervision through self-supervised predictive objectives, offering a promising alternative to language-supervised approaches in medical imaging.

Abstract: Recent advances in medical vision language models guide the learning of visual representations; however, this form of supervision is constrained by the availability of paired image text data, raising the question of whether robust radiology encoders can be learned without relying on language supervision. In this work, we introduce RadJEPA, a self-supervised framework built on a Joint Embedding Predictive Architecture that learns without language supervision. Pre-trained solely on unlabeled chest X-ray images, the model learns to predict latent representations of masked image regions. This predictive objective differs fundamentally from both image text pre-training and DINO-style self-distillation: rather than aligning global representations across views or modalities, RadJEPA explicitly models latent-space prediction. We evaluate the learned encoder on disease classification, semantic segmentation, and report generation tasks. Across benchmarks, RadJEPA achieves performance exceeding state-of-the-art approaches, including Rad-DINO.

</details>


### [45] [ThermoSplat: Cross-Modal 3D Gaussian Splatting with Feature Modulation and Geometry Decoupling](https://arxiv.org/abs/2601.15897)
*Zhaoqi Su,Shihai Chen,Xinyan Lin,Liqin Huang,Zhipeng Su,Xiaoqiang Lu*

Main category: cs.CV

TL;DR: ThermoSplat: A novel framework for multi-modal 3D scene reconstruction that integrates RGB and thermal infrared data using cross-modal feature modulation and adaptive geometry decoupling to achieve state-of-the-art rendering quality.


<details>
  <summary>Details</summary>
Motivation: Multi-modal scene reconstruction combining RGB and thermal data is crucial for robust environmental perception in varying lighting and weather conditions. Current 3D Gaussian Splatting approaches struggle to effectively leverage complementary multi-modal information, often neglecting cross-modal correlations or using shared representations that fail to handle complex structural correlations and physical discrepancies between spectrums.

Method: 1. Cross-Modal FiLM Modulation: Dynamically conditions shared latent features on thermal structural priors to guide visible texture synthesis with cross-modal geometric cues. 2. Modality-Adaptive Geometric Decoupling: Learns independent opacity offsets and executes separate rasterization for thermal branch to handle modality-specific geometric inconsistencies. 3. Hybrid rendering pipeline: Integrates explicit Spherical Harmonics with implicit neural decoding for semantic consistency and high-frequency detail preservation.

Result: Extensive experiments on the RGBT-Scenes dataset demonstrate that ThermoSplat achieves state-of-the-art rendering quality across both visible and thermal spectrums.

Conclusion: ThermoSplat effectively addresses the limitations of existing multi-modal 3DGS approaches by enabling deep spectral-aware reconstruction through active feature modulation and adaptive geometry decoupling, providing robust multi-spectral scene reconstruction capabilities.

Abstract: Multi-modal scene reconstruction integrating RGB and thermal infrared data is essential for robust environmental perception across diverse lighting and weather conditions. However, extending 3D Gaussian Splatting (3DGS) to multi-spectral scenarios remains challenging. Current approaches often struggle to fully leverage the complementary information of multi-modal data, typically relying on mechanisms that either tend to neglect cross-modal correlations or leverage shared representations that fail to adaptively handle the complex structural correlations and physical discrepancies between spectrums. To address these limitations, we propose ThermoSplat, a novel framework that enables deep spectral-aware reconstruction through active feature modulation and adaptive geometry decoupling. First, we introduce a Cross-Modal FiLM Modulation mechanism that dynamically conditions shared latent features on thermal structural priors, effectively guiding visible texture synthesis with reliable cross-modal geometric cues. Second, to accommodate modality-specific geometric inconsistencies, we propose a Modality-Adaptive Geometric Decoupling scheme that learns independent opacity offsets and executes an independent rasterization pass for the thermal branch. Additionally, a hybrid rendering pipeline is employed to integrate explicit Spherical Harmonics with implicit neural decoding, ensuring both semantic consistency and high-frequency detail preservation. Extensive experiments on the RGBT-Scenes dataset demonstrate that ThermoSplat achieves state-of-the-art rendering quality across both visible and thermal spectrums.

</details>


### [46] [Opening the Black Box: Preliminary Insights into Affective Modeling in Multimodal Foundation Models](https://arxiv.org/abs/2601.15906)
*Zhen Zhang,Runhao Zeng,Sicheng Zhao,Xiping Hu*

Main category: cs.CV

TL;DR: Affective capabilities in multimodal foundation models are primarily mediated by feed-forward gating projections (gate_proj), not attention modules, enabling efficient emotion understanding with minimal parameter tuning.


<details>
  <summary>Details</summary>
Motivation: Despite strong empirical performance of affective models, the internal architectural mechanisms supporting emotion understanding in multimodal foundation models remain poorly understood. The paper aims to systematically study how emotion-oriented supervision reshapes model parameters.

Method: Conducted systematic mechanistic study across multiple architectures, training strategies, and affective tasks. Used controlled module transfer, targeted single-module adaptation, and destructive ablation to analyze parameter localization. Specifically investigated gate_proj (feed-forward gating projection) versus attention modules.

Result: Found consistent pattern: affective adaptation localizes to gate_proj, not attention modules. Gate_proj is sufficient, efficient, and necessary for affective understanding/generation. Tuning only ~24.5% of parameters (vs AffectGPT) achieves 96.6% average performance across eight affective tasks.

Conclusion: Affective capabilities in foundation models are structurally mediated by feed-forward gating mechanisms, with gate_proj identified as the central architectural locus for affective modeling, enabling parameter-efficient emotion understanding.

Abstract: Understanding where and how emotions are represented in large-scale foundation models remains an open problem, particularly in multimodal affective settings. Despite the strong empirical performance of recent affective models, the internal architectural mechanisms that support affective understanding and generation are still poorly understood. In this work, we present a systematic mechanistic study of affective modeling in multimodal foundation models. Across multiple architectures, training strategies, and affective tasks, we analyze how emotion-oriented supervision reshapes internal model parameters. Our results consistently reveal a clear and robust pattern: affective adaptation does not primarily focus on the attention module, but instead localizes to the feed-forward gating projection (\texttt{gate\_proj}). Through controlled module transfer, targeted single-module adaptation, and destructive ablation, we further demonstrate that \texttt{gate\_proj} is sufficient, efficient, and necessary for affective understanding and generation. Notably, by tuning only approximately 24.5\% of the parameters tuned by AffectGPT, our approach achieves 96.6\% of its average performance across eight affective tasks, highlighting substantial parameter efficiency. Together, these findings provide empirical evidence that affective capabilities in foundation models are structurally mediated by feed-forward gating mechanisms and identify \texttt{gate\_proj} as a central architectural locus of affective modeling.

</details>


### [47] [The Latency Wall: Benchmarking Off-the-Shelf Emotion Recognition for Real-Time Virtual Avatars](https://arxiv.org/abs/2601.15914)
*Yarin Benyamin*

Main category: cs.CV

TL;DR: Benchmarking SOTA models for zero-shot facial expression recognition on virtual characters reveals a "latency wall" in classification, with general-purpose transformers failing to meet real-time VR requirements for ASD therapy.


<details>
  <summary>Details</summary>
Motivation: Real-time emotion recognition in VR has potential to help individuals with ASD improve social skills, but requires strict latency constraints (<140ms MTP latency) that current off-the-shelf DL models don't prioritize.

Method: Benchmarked SOTA models for zero-shot FER on virtual characters using UIBVFED dataset. Evaluated YOLO variants (v8, v11, v12 Medium/Nano) for face detection and general-purpose Vision Transformers (CLIP, SigLIP, ViT-FER) on CPU-only inference.

Result: Face detection on stylized avatars is robust (100% accuracy), but classification has a "latency wall." YOLOv11n offers optimal detection balance (~54ms). General-purpose transformers fail to achieve viable accuracy (<23%) or speed (>150ms) for real-time loops.

Conclusion: Lightweight, domain-specific architectures are necessary to enable accessible, real-time AI in therapeutic VR settings, as current general-purpose models cannot meet the strict latency-accuracy trade-off required.

Abstract: In the realm of Virtual Reality (VR) and Human-Computer Interaction (HCI), real-time emotion recognition shows promise for supporting individuals with Autism Spectrum Disorder (ASD) in improving social skills. This task requires a strict latency-accuracy trade-off, with motion-to-photon (MTP) latency kept below 140 ms to maintain contingency. However, most off-the-shelf Deep Learning models prioritize accuracy over the strict timing constraints of commodity hardware. As a first step toward accessible VR therapy, we benchmark State-of-the-Art (SOTA) models for Zero-Shot Facial Expression Recognition (FER) on virtual characters using the UIBVFED dataset. We evaluate Medium and Nano variants of YOLO (v8, v11, and v12) for face detection, alongside general-purpose Vision Transformers including CLIP, SigLIP, and ViT-FER.Our results on CPU-only inference demonstrate that while face detection on stylized avatars is robust (100% accuracy), a "Latency Wall" exists in the classification stage. The YOLOv11n architecture offers the optimal balance for detection (~54 ms). However, general-purpose Transformers like CLIP and SigLIP fail to achieve viable accuracy (<23%) or speed (>150 ms) for real-time loops. This study highlights the necessity for lightweight, domain-specific architectures to enable accessible, real-time AI in therapeutic settings.

</details>


### [48] [A Multi-View Pipeline and Benchmark Dataset for 3D Hand Pose Estimation in Surgery](https://arxiv.org/abs/2601.15918)
*Valery Fischer,Alan Magdaleno,Anna-Katharina Calek,Nicola Cavalcanti,Nathan Hoffman,Christoph Germann,Joschua Wüthrich,Max Krähenmann,Mazda Farshad,Philipp Fürnstahl,Lilian Calvet*

Main category: cs.CV

TL;DR: Proposed a robust multi-view 3D hand pose estimation pipeline for surgery using off-the-shelf models without domain fine-tuning, plus a new surgical benchmark dataset with 68k+ frames and 3k annotated hand poses.


<details>
  <summary>Details</summary>
Motivation: 3D hand pose estimation is valuable for surgical applications like skill assessment and robot-assisted interventions, but surgical environments present severe challenges including intense lighting, frequent occlusions, uniform glove appearance, and scarcity of annotated datasets.

Method: Multi-view pipeline using off-the-shelf pretrained models: integrates person detection, whole-body pose estimation, 2D hand keypoint prediction on tracked hand crops, followed by constrained 3D optimization. Also introduces new surgical benchmark dataset with 68,000+ frames and 3,000 manually annotated 2D hand poses with triangulated 3D ground truth.

Result: Method consistently outperforms baselines with 31% reduction in 2D mean joint error and 76% reduction in 3D mean per-joint position error.

Conclusion: Establishes strong baseline for 3D hand pose estimation in surgery, providing both training-free pipeline and comprehensive annotated dataset to facilitate future surgical computer vision research.

Abstract: Purpose: Accurate 3D hand pose estimation supports surgical applications such as skill assessment, robot-assisted interventions, and geometry-aware workflow analysis. However, surgical environments pose severe challenges, including intense and localized lighting, frequent occlusions by instruments or staff, and uniform hand appearance due to gloves, combined with a scarcity of annotated datasets for reliable model training.
  Method: We propose a robust multi-view pipeline for 3D hand pose estimation in surgical contexts that requires no domain-specific fine-tuning and relies solely on off-the-shelf pretrained models. The pipeline integrates reliable person detection, whole-body pose estimation, and state-of-the-art 2D hand keypoint prediction on tracked hand crops, followed by a constrained 3D optimization. In addition, we introduce a novel surgical benchmark dataset comprising over 68,000 frames and 3,000 manually annotated 2D hand poses with triangulated 3D ground truth, recorded in a replica operating room under varying levels of scene complexity.
  Results: Quantitative experiments demonstrate that our method consistently outperforms baselines, achieving a 31% reduction in 2D mean joint error and a 76% reduction in 3D mean per-joint position error.
  Conclusion: Our work establishes a strong baseline for 3D hand pose estimation in surgery, providing both a training-free pipeline and a comprehensive annotated dataset to facilitate future research in surgical computer vision.

</details>


### [49] [Class Confidence Aware Reweighting for Long Tailed Learning](https://arxiv.org/abs/2601.15924)
*Brainard Philemon Jagati,Jitendra Tembhurne,Harsh Goud,Rudra Pratap Singh,Chandrashekhar Meshram*

Main category: cs.CV

TL;DR: A class and confidence-aware re-weighting scheme for long-tailed learning that modulates training contributions based on prediction confidence and class frequency, complementing existing logit adjustment methods.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks degrade significantly in long-tailed data distributions where head classes dominate training data and tail classes have few examples. Existing methods focus mainly on logit-level adjustments for class-prior bias, with insufficient attention to optimization process adjustments based on sample confidence differences.

Method: Proposes a class and confidence-aware re-weighting scheme operating at the loss level. Uses an Ω(p_t, f_c) function to modulate training contributions based on both prediction confidence values and relative class frequencies. This approach complements existing logit adjustment methods.

Result: Significant experimental results on CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 datasets under various imbalance factors validate the theoretical discussions and demonstrate the effectiveness of the proposed approach.

Conclusion: The proposed confidence-aware re-weighting scheme effectively addresses long-tailed learning by considering both class frequency and prediction confidence, providing a complementary approach to existing logit adjustment methods for improved performance on imbalanced datasets.

Abstract: Deep neural network models degrade significantly in the long-tailed data distribution, with the overall training data dominated by a small set of classes in the head, and the tail classes obtaining less training examples. Addressing the imbalance in the classes, attention in the related literature was given mainly to the adjustments carried out in the decision space in terms of either corrections performed at the logit level in order to compensate class-prior bias, with the least attention to the optimization process resulting from the adjustments introduced through the differences in the confidences among the samples. In the current study, we present the design of a class and confidence-aware re-weighting scheme for long-tailed learning. This scheme is purely based upon the loss level and has a complementary nature to the existing methods performing the adjustment of the logits. In the practical implementation stage of the proposed scheme, we use an Ω(p_t, f_c) function. This function enables the modulation of the contribution towards the training task based upon the confidence value of the prediction, as well as the relative frequency of the corresponding class. Our observations in the experiments are corroborated by significant experimental results performed on the CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 datasets under various values of imbalance factors that clearly authenticate the theoretical discussions above.

</details>


### [50] [NeuroMamba: Multi-Perspective Feature Interaction with Visual Mamba for Neuron Segmentation](https://arxiv.org/abs/2601.15929)
*Liuyun Jiang,Yizhuo Lu,Yanchao Zhang,Jiazheng Liu,Hua Han*

Main category: cs.CV

TL;DR: NeuroMamba: A multi-perspective framework for neuron segmentation that combines Mamba's linear complexity for global modeling with local feature extraction to handle irregular neuron morphology and dense structures, achieving SOTA performance on EM datasets.


<details>
  <summary>Details</summary>
Motivation: Neuron segmentation is crucial for reconstructing neuronal connectomes to understand brain function, but current methods have limitations: CNN-based approaches lack long-range context for ambiguous boundaries, while Transformer-based methods lose voxel-level details during patch partitioning, leading to boundary imprecision.

Method: Proposes NeuroMamba with three key components: 1) Channel-gated Boundary Discriminative Feature Extractor (BDFE) for enhanced local morphological cues, 2) Spatial Continuous Feature Extractor (SCFE) with resolution-aware scanning mechanism integrated into Visual Mamba architecture for adaptive global dependency modeling, and 3) Cross-modulation mechanism to synergistically fuse multi-perspective features.

Result: Demonstrates state-of-the-art performance across four public EM datasets, showing exceptional adaptability to both anisotropic and isotropic resolutions.

Conclusion: NeuroMamba effectively addresses limitations of existing methods by combining patch-free global modeling via Mamba's linear complexity with complementary local feature extraction, enabling efficient long-range dependency capture while preserving fine-grained voxel details for superior neuron segmentation.

Abstract: Neuron segmentation is the cornerstone of reconstructing comprehensive neuronal connectomes, which is essential for deciphering the functional organization of the brain. The irregular morphology and densely intertwined structures of neurons make this task particularly challenging. Prevailing CNN-based methods often fail to resolve ambiguous boundaries due to the lack of long-range context, whereas Transformer-based methods suffer from boundary imprecision caused by the loss of voxel-level details during patch partitioning. To address these limitations, we propose NeuroMamba, a multi-perspective framework that exploits the linear complexity of Mamba to enable patch-free global modeling and synergizes this with complementary local feature modeling, thereby efficiently capturing long-range dependencies while meticulously preserving fine-grained voxel details. Specifically, we design a channel-gated Boundary Discriminative Feature Extractor (BDFE) to enhance local morphological cues. Complementing this, we introduce the Spatial Continuous Feature Extractor (SCFE), which integrates a resolution-aware scanning mechanism into the Visual Mamba architecture to adaptively model global dependencies across varying data resolutions. Finally, a cross-modulation mechanism synergistically fuses these multi-perspective features. Our method demonstrates state-of-the-art performance across four public EM datasets, validating its exceptional adaptability to both anisotropic and isotropic resolutions. The source code will be made publicly available.

</details>


### [51] [EVolSplat4D: Efficient Volume-based Gaussian Splatting for 4D Urban Scene Synthesis](https://arxiv.org/abs/2601.15951)
*Sheng Miao,Sijin Li,Pan Wang,Dongfeng Bai,Bingbing Liu,Yue Wang,Andreas Geiger,Yiyi Liao*

Main category: cs.CV

TL;DR: EvolSplat4D is a feed-forward framework for novel view synthesis of urban scenes that unifies volume-based and pixel-based Gaussian prediction across three specialized branches to handle static close-range, dynamic actors, and far-field regions, achieving superior reconstruction quality without per-scene optimization.


<details>
  <summary>Details</summary>
Motivation: Existing methods for novel view synthesis in urban scenes struggle to balance reconstruction time with quality. Neural radiance fields and 3D Gaussian Splatting achieve photorealism but require time-consuming per-scene optimization, while feed-forward methods using per-pixel Gaussian representations suffer from 3D inconsistencies in complex dynamic environments.

Method: EvolSplat4D uses a three-branch architecture: 1) For close-range static regions, predicts consistent 3D Gaussian geometry from a 3D feature volume with semantically-enhanced image-based rendering; 2) For dynamic actors, uses object-centric canonical spaces and motion-adjusted rendering to aggregate temporal features; 3) For far-field scenery, employs an efficient per-pixel Gaussian branch for full-scene coverage.

Result: Experimental results on KITTI-360, KITTI, Waymo, and PandaSet datasets show EvolSplat4D reconstructs both static and dynamic environments with superior accuracy and consistency, outperforming both per-scene optimization methods and state-of-the-art feed-forward baselines.

Conclusion: EvolSplat4D successfully addresses the trade-off between reconstruction time and quality in urban scene synthesis by moving beyond per-pixel paradigms through a unified multi-branch approach that handles different scene components appropriately, achieving high-quality 4D reconstruction without per-scene optimization.

Abstract: Novel view synthesis (NVS) of static and dynamic urban scenes is essential for autonomous driving simulation, yet existing methods often struggle to balance reconstruction time with quality. While state-of-the-art neural radiance fields and 3D Gaussian Splatting approaches achieve photorealism, they often rely on time-consuming per-scene optimization. Conversely, emerging feed-forward methods frequently adopt per-pixel Gaussian representations, which lead to 3D inconsistencies when aggregating multi-view predictions in complex, dynamic environments. We propose EvolSplat4D, a feed-forward framework that moves beyond existing per-pixel paradigms by unifying volume-based and pixel-based Gaussian prediction across three specialized branches. For close-range static regions, we predict consistent geometry of 3D Gaussians over multiple frames directly from a 3D feature volume, complemented by a semantically-enhanced image-based rendering module for predicting their appearance. For dynamic actors, we utilize object-centric canonical spaces and a motion-adjusted rendering module to aggregate temporal features, ensuring stable 4D reconstruction despite noisy motion priors. Far-Field scenery is handled by an efficient per-pixel Gaussian branch to ensure full-scene coverage. Experimental results on the KITTI-360, KITTI, Waymo, and PandaSet datasets show that EvolSplat4D reconstructs both static and dynamic environments with superior accuracy and consistency, outperforming both per-scene optimization and state-of-the-art feed-forward baselines.

</details>


### [52] [HyperAlign: Hypernetwork for Efficient Test-Time Alignment of Diffusion Models](https://arxiv.org/abs/2601.15968)
*Xin Xie,Jiaxian Guo,Dong Gong*

Main category: cs.CV

TL;DR: HyperAlign trains a hypernetwork to generate low-rank adaptation weights for test-time alignment of diffusion models, balancing performance and efficiency while reducing reward hacking.


<details>
  <summary>Details</summary>
Motivation: Diffusion models often fail to align with human preferences, producing images with poor aesthetics and semantic inconsistencies. Existing methods face trade-offs: fine-tuning loses diversity through reward over-optimization, while test-time scaling is computationally expensive and under-optimizes.

Method: HyperAlign trains a hypernetwork that dynamically generates low-rank adaptation weights to modulate diffusion model's generation operators based on input latents, timesteps, and prompts. Multiple variants balance application frequency for performance-efficiency trade-off. The hypernetwork is optimized with reward score objective regularized with preference data to reduce reward hacking.

Result: HyperAlign significantly outperforms existing fine-tuning and test-time scaling baselines in enhancing semantic consistency and visual appeal across multiple generative paradigms including Stable Diffusion and FLUX.

Conclusion: HyperAlign provides an efficient and effective test-time alignment framework that addresses limitations of existing methods, enabling better alignment with human preferences without sacrificing diversity or computational efficiency.

Abstract: Diffusion models achieve state-of-the-art performance but often fail to generate outputs that align with human preferences and intentions, resulting in images with poor aesthetic quality and semantic inconsistencies. Existing alignment methods present a difficult trade-off: fine-tuning approaches suffer from loss of diversity with reward over-optimization, while test-time scaling methods introduce significant computational overhead and tend to under-optimize. To address these limitations, we propose HyperAlign, a novel framework that trains a hypernetwork for efficient and effective test-time alignment. Instead of modifying latent states, HyperAlign dynamically generates low-rank adaptation weights to modulate the diffusion model's generation operators. This allows the denoising trajectory to be adaptively adjusted based on input latents, timesteps and prompts for reward-conditioned alignment. We introduce multiple variants of HyperAlign that differ in how frequently the hypernetwork is applied, balancing between performance and efficiency. Furthermore, we optimize the hypernetwork using a reward score objective regularized with preference data to reduce reward hacking. We evaluate HyperAlign on multiple extended generative paradigms, including Stable Diffusion and FLUX. It significantly outperforms existing fine-tuning and test-time scaling baselines in enhancing semantic consistency and visual appeal.

</details>


### [53] [PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models](https://arxiv.org/abs/2601.16007)
*Chak-Wing Mak,Guanyu Zhu,Boyi Zhang,Hongji Li,Xiaowei Chi,Kevin Zhang,Yichen Wu,Yangfan He,Chun-Kai Fan,Wentao Lu,Kuangzhi Ge,Xinyu Fang,Hongyang He,Kuan Lu,Tianxiang Xu,Li Zhang,Yongxin Ni,Youhua Li,Shanghang Zhang*

Main category: cs.CV

TL;DR: PhysicsMind is a new benchmark for evaluating physics understanding in multimodal models, testing both reasoning (VQA) and generation (video) capabilities across three physical principles: Center of Mass, Lever Equilibrium, and Newton's First Law.


<details>
  <summary>Details</summary>
Motivation: Current multimodal models excel at mathematical, common-sense, and visual reasoning but lack proper understanding of physical laws. Existing benchmarks are fragmented, using synthetic data or focusing on perceptual quality rather than physical consistency.

Method: Created a unified benchmark with real and simulation environments. Includes two main tasks: 1) VQA tasks to test reasoning about physical quantities from images/videos, and 2) Video Generation tasks to evaluate if predicted motion trajectories obey physical constraints (center-of-mass, torque, inertia).

Result: Evaluation of recent models shows they rely on appearance heuristics and often violate basic mechanics, indicating current scaling and training approaches are insufficient for robust physical understanding.

Conclusion: PhysicsMind serves as a focused testbed for physics-aware multimodal models, highlighting gaps in current approaches and providing a unified benchmark for evaluating physical understanding in AI systems.

Abstract: Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.

</details>


### [54] [PAINT: Pathology-Aware Integrated Next-Scale Transformation for Virtual Immunohistochemistry](https://arxiv.org/abs/2601.16024)
*Rongze Ma,Mengkang Lu,Zhenyu Xiang,Yongsheng Pan,Yicheng Wu,Qingjie Zeng,Yong Xia*

Main category: cs.CV

TL;DR: PAINT is a structure-first autoregressive framework for virtual immunohistochemistry that synthesizes molecular staining patterns from H&E images by conditioning on global structural layouts rather than direct appearance translation.


<details>
  <summary>Details</summary>
Motivation: Virtual IHC offers cost-effective alternatives to physical staining, but existing methods struggle with semantic inconsistencies due to ambiguous H&E morphology cues and insufficient structural priors when similar tissue structures can have distinct molecular states.

Method: PAINT reformulates synthesis as structure-first conditional generation using a Spatial Structural Start Map (3S-Map) to ground autoregressive initialization in observed morphology, enforcing causal order where molecular details are resolved conditioned on global structural layout.

Result: Experiments on IHC4BC and MIST datasets show PAINT outperforms state-of-the-art methods in structural fidelity and clinical downstream tasks.

Conclusion: The structure-guided autoregressive modeling approach validates the potential for more accurate virtual IHC synthesis by prioritizing structural consistency over direct appearance translation.

Abstract: Virtual immunohistochemistry (IHC) aims to computationally synthesize molecular staining patterns from routine Hematoxylin and Eosin (H\&E) images, offering a cost-effective and tissue-efficient alternative to traditional physical staining. However, this task is particularly challenging: H\&E morphology provides ambiguous cues about protein expression, and similar tissue structures may correspond to distinct molecular states. Most existing methods focus on direct appearance synthesis to implicitly achieve cross-modal generation, often resulting in semantic inconsistencies due to insufficient structural priors. In this paper, we propose Pathology-Aware Integrated Next-Scale Transformation (PAINT), a visual autoregressive framework that reformulates the synthesis process as a structure-first conditional generation task. Unlike direct image translation, PAINT enforces a causal order by resolving molecular details conditioned on a global structural layout. Central to this approach is the introduction of a Spatial Structural Start Map (3S-Map), which grounds the autoregressive initialization in observed morphology, ensuring deterministic, spatially aligned synthesis. Experiments on the IHC4BC and MIST datasets demonstrate that PAINT outperforms state-of-the-art methods in structural fidelity and clinical downstream tasks, validating the potential of structure-guided autoregressive modeling.

</details>


### [55] [ProGiDiff: Prompt-Guided Diffusion-Based Medical Image Segmentation](https://arxiv.org/abs/2601.16060)
*Yuan Lin,Murong Xu,Marc Hölle,Chinmay Prabhakar,Andreas Maier,Vasileios Belagiannis,Bjoern Menze,Suprosanna Shit*

Main category: cs.CV

TL;DR: ProGiDiff: A novel framework that adapts pre-trained diffusion models for medical image segmentation using ControlNet-style conditioning with custom encoders, enabling multi-class segmentation via natural language prompts and cross-modality adaptation.


<details>
  <summary>Details</summary>
Motivation: Existing medical image segmentation methods are deterministic, lack natural language prompt capability, cannot estimate multiple proposals, and have limited human interaction and cross-modality adaptation. Text-to-image diffusion models show promise but require large datasets for training and are often limited to binary segmentation without natural language conditioning.

Method: ProGiDiff leverages existing image generation models with a ControlNet-style conditioning mechanism featuring a custom encoder suitable for image conditioning. This steers a pre-trained diffusion model to output segmentation masks, naturally extending to multi-class segmentation by prompting the target organ.

Result: Experiments on organ segmentation from CT images show strong performance compared to previous methods. The framework benefits from expert-in-the-loop settings to leverage multiple proposals. The learned conditioning mechanism can be easily transferred through low-rank, few-shot adaptation to segment MR images.

Conclusion: ProGiDiff successfully bridges the gap between deterministic segmentation methods and flexible, prompt-based approaches by adapting pre-trained diffusion models for medical image segmentation, enabling natural language prompting, multi-class capability, and cross-modality adaptation with minimal data requirements.

Abstract: Widely adopted medical image segmentation methods, although efficient, are primarily deterministic and remain poorly amenable to natural language prompts. Thus, they lack the capability to estimate multiple proposals, human interaction, and cross-modality adaptation. Recently, text-to-image diffusion models have shown potential to bridge the gap. However, training them from scratch requires a large dataset-a limitation for medical image segmentation. Furthermore, they are often limited to binary segmentation and cannot be conditioned on a natural language prompt. To this end, we propose a novel framework called ProGiDiff that leverages existing image generation models for medical image segmentation purposes. Specifically, we propose a ControlNet-style conditioning mechanism with a custom encoder, suitable for image conditioning, to steer a pre-trained diffusion model to output segmentation masks. It naturally extends to a multi-class setting simply by prompting the target organ. Our experiment on organ segmentation from CT images demonstrates strong performance compared to previous methods and could greatly benefit from an expert-in-the-loop setting to leverage multiple proposals. Importantly, we demonstrate that the learned conditioning mechanism can be easily transferred through low-rank, few-shot adaptation to segment MR images.

</details>


### [56] [DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models](https://arxiv.org/abs/2601.16073)
*Hanwen Zhang,Qiaojin Shen,Yuxi Liu,Yuesheng Zhu,Guibo Luo*

Main category: cs.CV

TL;DR: DSFedMed is a dual-scale federated framework for medical image segmentation that enables mutual knowledge distillation between a centralized foundation model and lightweight client models, achieving better performance with 90% reduced communication/inference costs.


<details>
  <summary>Details</summary>
Motivation: Foundation models have strong generalization but face deployment challenges in federated settings due to high computational demands, communication overhead, and inference costs, especially for medical image segmentation tasks.

Method: Proposes DSFedMed with dual-scale federated framework using mutual knowledge distillation between centralized foundation model and lightweight client models. Uses generated high-quality medical images instead of real public datasets, and employs learnability-guided sample selection strategy to enhance efficiency in dual-scale distillation.

Result: Achieves average 2% improvement in Dice score while reducing communication costs and inference time by nearly 90% compared to existing federated foundation model baselines across five medical imaging segmentation datasets.

Conclusion: DSFedMed demonstrates significant efficiency gains and scalability for resource-limited federated deployments, enabling practical deployment of foundation models in medical imaging while maintaining performance.

Abstract: Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.

</details>


### [57] [Masked Modeling for Human Motion Recovery Under Occlusions](https://arxiv.org/abs/2601.16079)
*Zhiyin Qian,Siwei Zhang,Bharat Lal Bhatnagar,Federica Bogo,Siyu Tang*

Main category: cs.CV

TL;DR: MoRo: A real-time human motion reconstruction framework using masked modeling that handles occlusions by learning multi-modal priors from heterogeneous datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods for human motion reconstruction from monocular videos face trade-offs: regression-based methods are efficient but fragile to occlusions, while optimization/diffusion methods are robust but slow. There's a need for occlusion-robust, real-time motion reconstruction.

Method: MoRo uses masked modeling for occlusion-robust motion recovery. It employs a cross-modality learning scheme with three components: (1) trajectory-aware motion prior from MoCap datasets, (2) image-conditioned pose prior from image-pose datasets, and (3) video-conditioned masked transformer that fuses these priors, fine-tuned on video-motion datasets.

Result: MoRo substantially outperforms state-of-the-art methods in accuracy and motion realism under occlusions on EgoBody and RICH datasets, while performing on-par in non-occluded scenarios. It achieves real-time inference at 70 FPS on a single H200 GPU.

Conclusion: MoRo presents an effective solution for occlusion-robust human motion reconstruction that combines the robustness of generative methods with the efficiency of regression-based approaches, enabling real-time performance while handling challenging real-world occlusion scenarios.

Abstract: Human motion reconstruction from monocular videos is a fundamental challenge in computer vision, with broad applications in AR/VR, robotics, and digital content creation, but remains challenging under frequent occlusions in real-world settings.Existing regression-based methods are efficient but fragile to missing observations, while optimization- and diffusion-based approaches improve robustness at the cost of slow inference speed and heavy preprocessing steps. To address these limitations, we leverage recent advances in generative masked modeling and present MoRo: Masked Modeling for human motion Recovery under Occlusions. MoRo is an occlusion-robust, end-to-end generative framework that formulates motion reconstruction as a video-conditioned task, and efficiently recover human motion in a consistent global coordinate system from RGB videos. By masked modeling, MoRo naturally handles occlusions while enabling efficient, end-to-end inference. To overcome the scarcity of paired video-motion data, we design a cross-modality learning scheme that learns multi-modal priors from a set of heterogeneous datasets: (i) a trajectory-aware motion prior trained on MoCap datasets, (ii) an image-conditioned pose prior trained on image-pose datasets, capturing diverse per-frame poses, and (iii) a video-conditioned masked transformer that fuses motion and pose priors, finetuned on video-motion datasets to integrate visual cues with motion dynamics for robust inference. Extensive experiments on EgoBody and RICH demonstrate that MoRo substantially outperforms state-of-the-art methods in accuracy and motion realism under occlusions, while performing on-par in non-occluded scenarios. MoRo achieves real-time inference at 70 FPS on a single H200 GPU.

</details>


### [58] [SAMTok: Representing Any Mask with Two Words](https://arxiv.org/abs/2601.16093)
*Yikang Zhou,Tao Zhang,Dengxian Gong,Yuanzheng Wu,Ye Tian,Haochen Wang,Haobo Yuan,Jiacong Wang,Lu Qi,Hao Fei,Anran Wang,Zhuochen Wang,Yujing Wang,Cheng Chen,Shunping Ji,Xiangtai Li*

Main category: cs.CV

TL;DR: SAMTok introduces a discrete mask tokenizer that converts region masks into special tokens, enabling MLLMs to learn pixel-wise capabilities through standard language modeling without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Pixel-wise capabilities are essential for interactive intelligent systems, but current pixel-wise MLLMs are difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives.

Method: SAMTok converts any region mask into two special tokens using a mask encoder and residual vector quantizer trained on 209M diverse masks. This allows base MLLMs to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning with a textual answer-matching reward, without architectural modifications.

Result: QwenVL-SAMTok achieves state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. Reinforcement learning with textual answer-matching reward delivers substantial improvements on GRES and GCG benchmarks.

Conclusion: SAMTok demonstrates a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities by treating masks as language tokens, enabling integration without architectural changes or specialized loss designs.

Abstract: Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.

</details>


### [59] [Clustering-Guided Spatial-Spectral Mamba for Hyperspectral Image Classification](https://arxiv.org/abs/2601.16098)
*Zack Dewis,Yimin Zhu,Zhengsen Xu,Mabel Heffring,Saeid Taleghanidoozdoozan,Quinn Ledingham,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: CSSMamba is a clustering-guided spatial-spectral Mamba framework for hyperspectral image classification that integrates clustering mechanisms into Mamba architecture to create efficient token sequences and improve feature learning.


<details>
  <summary>Details</summary>
Motivation: Mamba models for HSI classification face challenges in defining efficient and adaptive token sequences for improved performance, requiring better spatial-spectral feature learning.

Method: Four key contributions: 1) Cluster-guided spatial Mamba module (CSpaMamba) integrating clustering into spatial Mamba, 2) Integration with spectral Mamba module (SpeMamba) for complete spatial-spectral framework, 3) Attention-Driven Token Selection mechanism for optimized token sequencing, 4) Learnable Clustering Module for adaptive cluster membership learning.

Result: Experiments on Pavia University, Indian Pines, and Liao-Ning 01 datasets show CSSMamba achieves higher accuracy and better boundary preservation compared to state-of-the-art CNN, Transformer, and Mamba-based methods.

Conclusion: CSSMamba effectively addresses token sequence challenges in Mamba models for HSI classification through clustering-guided spatial-spectral integration, achieving superior performance in both accuracy and boundary preservation.

Abstract: Although Mamba models greatly improve Hyperspectral Image (HSI) classification, they have critical challenges in terms defining efficient and adaptive token sequences for improve performance. This paper therefore presents CSSMamba (Clustering-guided Spatial-Spectral Mamba) framework to better address the challenges, with the following contributions. First, to achieve efficient and adaptive token sequences for improved Mamba performance, we integrate the clustering mechanism into a spatial Mamba architecture, leading to a cluster-guided spatial Mamba module (CSpaMamba) that reduces the Mamba sequence length and improves Mamba feature learning capability. Second, to improve the learning of both spatial and spectral information, we integrate the CSpaMamba module with a spectral mamba module (SpeMamba), leading to a complete clustering-guided spatial-spectral Mamba framework. Third, to further improve feature learning capability, we introduce an Attention-Driven Token Selection mechanism to optimize Mamba token sequencing. Last, to seamlessly integrate clustering into the Mamba model in a coherent manner, we design a Learnable Clustering Module that learns the cluster memberships in an adaptive manner. Experiments on the Pavia University, Indian Pines, and Liao-Ning 01 datasets demonstrate that CSSMamba achieves higher accuracy and better boundary preservation compared to state-of-the-art CNN, Transformer, and Mamba-based methods.

</details>


### [60] [Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing](https://arxiv.org/abs/2601.16125)
*Tingyu Song,Yanzhao Zhang,Mingxin Li,Zhuoning Guo,Dingkun Long,Pengjun Xie,Siyue Zhang,Yilun Zhao,Shu Wu*

Main category: cs.CV

TL;DR: EDIR is a new fine-grained Composed Image Retrieval benchmark created using image editing to generate diverse queries across 5 categories and 15 subcategories, revealing significant gaps in current multimodal models.


<details>
  <summary>Details</summary>
Motivation: Current CIR benchmarks have limited query categories and fail to represent real-world diversity, creating an evaluation gap that needs to be addressed.

Method: Used image editing for precise control over modification types and content to synthesize queries, constructing EDIR with 5,000 high-quality queries across 5 main categories and 15 subcategories.

Result: Evaluation of 13 multimodal embedding models showed significant capability gaps - even state-of-the-art models struggle across all subcategories. Also uncovered limitations in existing benchmarks like modality biases and insufficient categorical coverage.

Conclusion: EDIR provides a rigorous benchmark that reveals current model limitations and distinguishes between solvable categories (with targeted data) and those exposing intrinsic architectural limitations.

Abstract: Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.

</details>


### [61] [Learning to Watermark in the Latent Space of Generative Models](https://arxiv.org/abs/2601.16140)
*Sylvestre-Alvise Rebuffi,Tuan Tran,Valeriu Lacatusu,Pierre Fernandez,Tomáš Souček,Nikola Jovanović,Tom Sander,Hady Elsahar,Alexandre Mourachko*

Main category: cs.CV

TL;DR: DistSeal: Latent space watermarking for AI-generated images that's faster and more robust than pixel-space methods


<details>
  <summary>Details</summary>
Motivation: Existing pixel-space watermarking methods for AI-generated images cause computational overhead and visual artifacts. There's a need for more efficient watermarking that works across different generative models.

Method: Train post-hoc watermarking models in the latent space of generative models (diffusion and autoregressive). These latent watermarkers can be distilled into either the generative model itself or the latent decoder, enabling in-model watermarking.

Result: Latent watermarks achieve competitive robustness with similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Distilling latent watermarkers outperforms distilling pixel-space ones in both efficiency and robustness.

Conclusion: Latent space watermarking (DistSeal) provides a unified, efficient solution for watermarking AI-generated images across different model types, offering significant speed improvements while maintaining robustness and imperceptibility.

Abstract: Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc watermarking models in the latent space of generative models. We demonstrate that these latent watermarkers can be effectively distilled either into the generative model itself or into the latent decoder, enabling in-model watermarking. The resulting latent watermarks achieve competitive robustness while offering similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Our experiments further reveal that distilling latent watermarkers outperforms distilling pixel-space ones, providing a solution that is both more efficient and more robust.

</details>


### [62] [ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion](https://arxiv.org/abs/2601.16148)
*Remy Sabathier,David Novotny,Niloy J. Mitra,Tom Monnier*

Main category: cs.CV

TL;DR: ActionMesh is a generative model that produces production-ready animated 3D meshes from various inputs (video, text, or 3D mesh + text) using temporal 3D diffusion, achieving state-of-the-art performance with fast generation and topology consistency.


<details>
  <summary>Details</summary>
Motivation: Existing 3D animation generation methods have practical limitations: they're difficult to apply due to limited setups, long runtimes, or poor quality. There's a need for a fast, production-ready solution that can generate animated 3D meshes from diverse inputs while maintaining rig-free, topology-consistent outputs for practical applications.

Method: Two-stage approach: 1) Adapt 3D diffusion models to include temporal axis (temporal 3D diffusion) generating synchronized latents for time-varying shapes. 2) Temporal 3D autoencoder translates independent shape sequences into deformations of a reference shape to create animations. Inspired by early video models, this enables feed-forward generation from monocular video, text, or 3D mesh with animation text prompt.

Result: State-of-the-art performance on standard video-to-4D benchmarks (Consistent4D, Objaverse) for both geometric accuracy and temporal consistency. The model delivers animated 3D meshes with unprecedented speed and quality, producing rig-free, topology-consistent results suitable for texturing and retargeting applications.

Conclusion: ActionMesh successfully addresses practical limitations of existing 3D animation generation methods by providing a fast, feed-forward approach that generates production-ready animated 3D meshes from diverse inputs while maintaining high quality and practical applicability for real-world use cases.

Abstract: Generating animated 3D objects is at the heart of many applications, yet most advanced works are typically difficult to apply in practice because of their limited setup, their long runtime, or their limited quality. We introduce ActionMesh, a generative model that predicts production-ready 3D meshes "in action" in a feed-forward manner. Drawing inspiration from early video models, our key insight is to modify existing 3D diffusion models to include a temporal axis, resulting in a framework we dubbed "temporal 3D diffusion". Specifically, we first adapt the 3D diffusion stage to generate a sequence of synchronized latents representing time-varying and independent 3D shapes. Second, we design a temporal 3D autoencoder that translates a sequence of independent shapes into the corresponding deformations of a pre-defined reference shape, allowing us to build an animation. Combining these two components, ActionMesh generates animated 3D meshes from different inputs like a monocular video, a text description, or even a 3D mesh with a text prompt describing its animation. Besides, compared to previous approaches, our method is fast and produces results that are rig-free and topology consistent, hence enabling rapid iteration and seamless applications like texturing and retargeting. We evaluate our model on standard video-to-4D benchmarks (Consistent4D, Objaverse) and report state-of-the-art performances on both geometric accuracy and temporal consistency, demonstrating that our model can deliver animated 3D meshes with unprecedented speed and quality.

</details>


### [63] [HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval](https://arxiv.org/abs/2601.16155)
*Zequn Xie,Xin Liu,Boyun Zhang,Yuxiao Lin,Sihang Cai,Tao Jin*

Main category: cs.CV

TL;DR: HVD model improves text-video retrieval by mimicking human cognitive behavior with coarse-to-fine alignment, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current CLIP-based text-video retrieval methods suffer from "blind" feature interaction where models struggle to distinguish key visual information from background noise due to sparse textual queries, leading to inefficient feature matching.

Method: Proposes Human Vision-Driven (HVD) model with coarse-to-fine alignment: 1) Frame Features Selection Module (FFSM) mimics human macro-perception to select key frames and eliminate temporal redundancy; 2) Patch Features Compression Module (PFCM) simulates micro-perception by aggregating patch features into salient visual entities using advanced attention mechanism for precise entity-level matching.

Result: Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance in text-video retrieval.

Conclusion: The HVD framework successfully addresses the "blind" feature interaction problem by mimicking human cognitive behavior, establishing an effective coarse-to-fine alignment mechanism that improves text-video retrieval performance.

Abstract: The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from "blind" feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.

</details>


### [64] [360Anything: Geometry-Free Lifting of Images and Videos to 360°](https://arxiv.org/abs/2601.16192)
*Ziyi Wu,Daniel Watson,Andrea Tagliasacchi,David J. Fleet,Marcus A. Brubaker,Saurabh Saxena*

Main category: cs.CV

TL;DR: 360Anything is a geometry-free framework that uses pre-trained diffusion transformers to generate 360° panoramas from perspective images/videos without requiring camera metadata, achieving SOTA performance and addressing seam artifacts.


<details>
  <summary>Details</summary>
Motivation: Existing methods for lifting perspective images to 360° panoramas rely on explicit geometric alignment requiring known camera metadata, which limits application to in-the-wild data where such calibration is often absent or noisy.

Method: A geometry-free framework built on pre-trained diffusion transformers that treats perspective input and panorama targets as token sequences, learning the perspective-to-equirectangular mapping purely data-driven. Introduces Circular Latent Encoding to address seam artifacts from VAE encoder zero-padding.

Result: Achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. Shows competitive results in zero-shot camera FoV and orientation estimation benchmarks.

Conclusion: 360Anything demonstrates deep geometric understanding without explicit camera metadata, enabling broader utility in computer vision tasks and effective handling of in-the-wild data through purely data-driven learning.

Abstract: Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.

</details>


### [65] [Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders](https://arxiv.org/abs/2601.16208)
*Shengbang Tong,Boyang Zheng,Ziteng Wang,Bingda Tang,Nanye Ma,Ellis Brown,Jihan Yang,Rob Fergus,Yann LeCun,Saining Xie*

Main category: cs.CV

TL;DR: RAEs outperform VAEs for large-scale text-to-image generation, showing better convergence, quality, and stability during finetuning without catastrophic overfitting.


<details>
  <summary>Details</summary>
Motivation: To investigate whether Representation Autoencoders (RAEs) can scale effectively from ImageNet to large-scale, freeform text-to-image generation, and to compare them against state-of-the-art VAEs like FLUX VAE.

Method: Scaling RAE decoders on frozen SigLIP-2 encoder using web, synthetic, and text-rendering data; stress-testing RAE design choices; conducting controlled comparison of RAE vs FLUX VAE across diffusion transformer scales (0.5B to 9.8B parameters).

Result: RAEs consistently outperform VAEs across all model scales during pretraining. During finetuning on high-quality data, VAE models catastrophically overfit after 64 epochs while RAE models remain stable through 256 epochs with better performance. RAEs show faster convergence and better generation quality.

Conclusion: RAEs are a simpler and stronger foundation than VAEs for large-scale T2I generation. The shared representation space enables unified multimodal models that can directly reason over generated latents.

Abstract: Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.

</details>


### [66] [PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation](https://arxiv.org/abs/2601.16210)
*Onkar Susladkar,Tushar Prakash,Adheesh Juvekar,Kiet A. Nguyen,Dong-Hwan Jang,Inderjit S Dhillon,Ismini Lourentzou*

Main category: cs.CV

TL;DR: PyraTok is a language-aligned pyramidal video tokenizer that learns multi-scale discrete latents with shared binary codebooks, achieving SOTA performance across video generation and understanding tasks.


<details>
  <summary>Details</summary>
Motivation: Existing video VAEs use single-scale tokenizers with limited vocabularies and weak language supervision, resulting in poor cross-modal alignment and zero-shot transfer capabilities.

Method: PyraTok builds on pretrained video VAEs with Language-aligned Pyramidal Quantization (LaPQ) that discretizes encoder features at multiple depths using shared large binary codebooks, jointly optimizing multi-scale text-guided quantization and global autoregressive objectives.

Result: Achieves SOTA video reconstruction, improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling to 4K/8K resolutions.

Conclusion: PyraTok's pyramidal tokenization with language alignment enables superior cross-modal understanding and transfer capabilities for video generation and analysis tasks.

Abstract: Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.

</details>


### [67] [Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition](https://arxiv.org/abs/2601.16211)
*Geo Ahn,Inwoong Lee,Taeoh Kim,Minho Shim,Dongyoon Wee,Jinwoo Choi*

Main category: cs.CV

TL;DR: RCORE framework addresses object-driven verb shortcuts in Zero-Shot Compositional Action Recognition by enforcing temporally grounded verb learning through composition-aware augmentation and temporal order regularization.


<details>
  <summary>Details</summary>
Motivation: Existing ZS-CAR models fail due to object-driven verb shortcuts, where models ignore visual evidence and overfit to co-occurrence statistics, preventing true compositional understanding of unseen verb-object combinations.

Method: Proposes RCORE framework with: 1) composition-aware augmentation to diversify verb-object combinations without corrupting motion cues, and 2) temporal order regularization loss to penalize shortcut behaviors by explicitly modeling temporal structure.

Result: RCORE significantly improves unseen composition accuracy across Sth-com and newly constructed EK100-com benchmarks, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps.

Conclusion: Object-driven shortcuts are a critical limiting factor in ZS-CAR, and addressing them through temporally grounded verb learning is essential for robust compositional video understanding.

Abstract: We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.

</details>


### [68] [CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback](https://arxiv.org/abs/2601.16214)
*Wenhang Ge,Guibao Shen,Jiawei Feng,Luozhou Wang,Hao Lu,Xingye Tian,Xin Tao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: CamPilot improves camera controllability in video diffusion models by introducing a camera-aware 3D decoder that converts video latent into 3D Gaussians for efficient reward computation, addressing limitations of existing ReFL approaches.


<details>
  <summary>Details</summary>
Motivation: Current camera-controlled video diffusion models have limited camera controllability, and existing Reward Feedback Learning (ReFL) approaches face three key challenges: lack of reward models for video-camera alignment assessment, computational overhead from RGB decoding, and neglect of 3D geometric information during video decoding.

Method: Proposes an efficient camera-aware 3D decoder that decodes video latent along with camera pose into 3D Gaussians. The camera pose serves as both input and projection parameter, where misalignment causes geometric distortions. The method optimizes pixel-level consistency between rendered novel views and ground-truth as reward, with a visibility term that selectively supervises only deterministic regions derived via geometric warping.

Result: Extensive experiments on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of the proposed method in improving camera controllability.

Conclusion: CamPilot successfully addresses the limitations of existing ReFL approaches by leveraging 3D geometric information for efficient reward computation, significantly enhancing camera controllability in video diffusion models.

Abstract: Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [69] [Empowering LLMs for Structure-Based Drug Design via Exploration-Augmented Latent Inference](https://arxiv.org/abs/2601.15333)
*Xuanning Hu,Anchen Li,Qianli Xing,Jinglong Ji,Hao Tuo,Bo Yang*

Main category: cs.LG

TL;DR: ELILLM enhances LLMs for drug design by treating generation as encoding, latent exploration, and decoding, using Bayesian optimization for systematic exploration and knowledge-guided decoding for chemical validity.


<details>
  <summary>Details</summary>
Motivation: LLMs have strong capabilities but are limited in structure-based drug design due to insufficient protein structure understanding and unpredictable molecular generation.

Method: ELILLM reinterprets LLM generation as encoding, latent space exploration, and decoding workflow. Uses Bayesian optimization for systematic exploration of latent embeddings and position-aware surrogate model for binding affinity prediction. Knowledge-guided decoding imposes chemical validity constraints.

Result: Demonstrated on CrossDocked2020 benchmark, showing strong controlled exploration and high binding affinity scores compared with seven baseline methods.

Conclusion: ELILLM can effectively enhance LLMs capabilities for structure-based drug design by addressing their limitations in protein structure understanding and molecular generation.

Abstract: Large Language Models (LLMs) possess strong representation and reasoning capabilities, but their application to structure-based drug design (SBDD) is limited by insufficient understanding of protein structures and unpredictable molecular generation. To address these challenges, we propose Exploration-Augmented Latent Inference for LLMs (ELILLM), a framework that reinterprets the LLM generation process as an encoding, latent space exploration, and decoding workflow. ELILLM explicitly explores portions of the design problem beyond the model's current knowledge while using a decoding module to handle familiar regions, generating chemically valid and synthetically reasonable molecules. In our implementation, Bayesian optimization guides the systematic exploration of latent embeddings, and a position-aware surrogate model efficiently predicts binding affinity distributions to inform the search. Knowledge-guided decoding further reduces randomness and effectively imposes chemical validity constraints. We demonstrate ELILLM on the CrossDocked2020 benchmark, showing strong controlled exploration and high binding affinity scores compared with seven baseline methods. These results demonstrate that ELILLM can effectively enhance LLMs capabilities for SBDD.

</details>


### [70] [Language Models Entangle Language and Culture](https://arxiv.org/abs/2601.15337)
*Shourya Jain,Paras Chopra*

Main category: cs.LG

TL;DR: LLMs provide lower quality answers in low-resource languages and language choice significantly impacts cultural context in responses, affecting answer quality.


<details>
  <summary>Details</summary>
Motivation: To ensure fairness in LLM interactions across languages - users should not be systemically disadvantaged by their language choice, and to investigate how language and culture are entangled in LLM responses.

Method: Created real-world open-ended questions from WildChat dataset to evaluate answer quality across languages. Used LLM-as-a-Judge to identify cultural context in responses. Evaluated LLMs on translated subset of CulturalBench benchmark across multiple languages.

Result: LLMs consistently provide lower quality answers to open-ended questions in low-resource languages. Language significantly impacts the cultural context used by models, and this difference in context affects downstream answer quality.

Conclusion: There are systematic disparities in LLM response quality across languages, with language choice affecting both answer quality and cultural context, highlighting fairness issues in multilingual LLM deployment.

Abstract: Users should not be systemically disadvantaged by the language they use for interacting with LLMs; i.e. users across languages should get responses of similar quality irrespective of language used. In this work, we create a set of real-world open-ended questions based on our analysis of the WildChat dataset and use it to evaluate whether responses vary by language, specifically, whether answer quality depends on the language used to query the model. We also investigate how language and culture are entangled in LLMs such that choice of language changes the cultural information and context used in the response by using LLM-as-a-Judge to identify the cultural context present in responses. To further investigate this, we evaluate LLMs on a translated subset of the CulturalBench benchmark across multiple languages. Our evaluations reveal that LLMs consistently provide lower quality answers to open-ended questions in low resource languages. We find that language significantly impacts the cultural context used by the model. This difference in context impacts the quality of the downstream answer.

</details>


### [71] [Improving MoE Compute Efficiency by Composing Weight and Data Sparsity](https://arxiv.org/abs/2601.15370)
*Maciej Kilian,Oleg Mkrtchyan,Luke Zettlemoyer,Akshat Shrivastava,Armen Aghajanyan*

Main category: cs.LG

TL;DR: The paper proposes using null experts in MoE layers to achieve data sparsity without violating causality, improving compute efficiency for vision-language models by routing low-information vision tokens to zero-compute experts.


<details>
  <summary>Details</summary>
Motivation: Current MoE layers achieve compute efficiency through weight sparsity (each token activates only some experts), but data sparsity (each expert processes only some tokens) offers complementary benefits. However, existing data sparsity methods like expert-choice routing violate causality in autoregressive models, creating train-inference mismatch.

Method: Introduce zero-compute (null) experts within the routing pool of causal token-choice MoE. When tokens route to null experts, those slots consume no compute. The standard load balancing objective trains the model to uniformly use all experts (real and null), creating data sparsity in expectation without causality violations.

Result: At matched expected FLOPs, combining weight and data sparsity yields better compute efficiency than weight sparsity alone, with improvements in training loss and downstream performance. The model learns implicit modality-aware allocation, routing vision tokens to null experts more aggressively than text tokens without explicit modality routing.

Conclusion: Null experts enable data sparsity within causal MoE frameworks, addressing the causality-violation problem of expert-choice routing. This approach is particularly effective for vision-language models where data heterogeneity is pronounced, allowing models to naturally allocate compute based on token information density.

Abstract: Mixture-of-Experts layers achieve compute efficiency through weight sparsity: each token activates only a subset of experts. Data sparsity, where each expert processes only a subset of tokens, offers a complementary axis. Expert-choice routing implements data sparsity directly but violates causality in autoregressive models, creating train-inference mismatch. We recover data sparsity within causal token-choice MoE by leveraging zero-compute (null) experts within the routing pool. When a token routes to null experts, those slots consume no compute. The standard load balancing objective trains the model to uniformly use all experts (real and null) therefore creating data sparsity in expectation without the causality violations. We evaluate on vision-language model training, where data heterogeneity is pronounced: vision encoders produce many low-information tokens while text tokens are denser. At matched expected FLOPs, composing weight and data sparsity yields a more compute-efficient frontier than weight sparsity alone, with gains in training loss and downstream performance. The model learns implicit modality-aware allocation, routing vision tokens to null experts more aggressively than text, without explicit modality routing.

</details>


### [72] [You Need Better Attention Priors](https://arxiv.org/abs/2601.15380)
*Elon Litman,Gabe Guo*

Main category: cs.LG

TL;DR: GOAT introduces a generalized attention mechanism based on Entropic Optimal Transport with learnable priors, addressing attention sinks and improving length generalization while maintaining FlashAttention compatibility.


<details>
  <summary>Details</summary>
Motivation: Standard attention mechanisms have implicit limitations: they assume a uniform prior in the transport formulation, suffer from attention sinks (tokens that disproportionately attract attention), and have trade-offs between learned positional embeddings (flexible but poor generalization) and fixed encodings (good generalization but rigid).

Method: The paper frames attention through Entropic Optimal Transport and introduces GOAT (Generalized Optimal transport Attention with Trainable priors), which replaces the implicit uniform prior with a learnable continuous prior. This maintains compatibility with optimized kernels like FlashAttention, provides an EOT-based explanation for attention sinks, and absorbs spatial information into core attention computation.

Result: GOAT addresses attention sinks without representational trade-offs, learns extrapolatable priors that combine the flexibility of learned positional embeddings with the length generalization of fixed encodings, and maintains computational efficiency through FlashAttention compatibility.

Conclusion: By viewing attention through the lens of Entropic Optimal Transport and introducing trainable priors, GOAT provides a more principled and flexible attention mechanism that solves key limitations of standard attention while maintaining computational efficiency.

Abstract: We generalize the attention mechanism by viewing it through the lens of Entropic Optimal Transport, revealing that standard attention corresponds to a transport problem regularized by an implicit uniform prior. We introduce Generalized Optimal transport Attention with Trainable priors (GOAT), a new attention mechanism that replaces this naive assumption with a learnable, continuous prior. This prior maintains full compatibility with optimized kernels such as FlashAttention. GOAT also provides an EOT-based explanation of attention sinks and materializes a solution for them, avoiding the representational trade-offs of standard attention. Finally, by absorbing spatial information into the core attention computation, GOAT learns an extrapolatable prior that combines the flexibility of learned positional embeddings with the length generalization of fixed encodings.

</details>


### [73] [FedUMM: A General Framework for Federated Learning with Unified Multimodal Models](https://arxiv.org/abs/2601.15390)
*Zhaolong Su,Leheng Zhao,Xiaoying Wu,Ziyue Xu,Jindong Wang*

Main category: cs.LG

TL;DR: FedUMM is a federated learning framework for unified multimodal models that enables privacy-preserving training across distributed clients with low communication cost using LoRA adapters.


<details>
  <summary>Details</summary>
Motivation: Current unified multimodal models (UMMs) are trained in centralized settings, which limits deployment in privacy-sensitive and geographically distributed scenarios where data cannot be gathered centrally.

Method: Built on NVIDIA FLARE, FedUMM uses parameter-efficient fine-tuning with LoRA adapters: clients train lightweight adapters while freezing foundation models, and the server aggregates only adapter updates to reduce communication costs.

Result: Evaluation on VQA v2 and GenEval benchmarks shows slight performance degradation as client count and heterogeneity increase, but remains competitive with centralized training. Adapter-only federation reduces per-round communication by over an order of magnitude compared to full fine-tuning.

Conclusion: FedUMM enables practical federated training of unified multimodal models with privacy preservation and low communication overhead, providing empirical experience for future research in federated UMMs.

Abstract: Unified multimodal models (UMMs) are emerging as strong foundation models that can do both generation and understanding tasks in a single architecture. However, they are typically trained in centralized settings where all training and downstream datasets are gathered in a central server, limiting the deployment in privacy-sensitive and geographically distributed scenarios. In this paper, we present FedUMM, a general federated learning framework for UMMs under non-IID multimodal data with low communication cost. Built on NVIDIA FLARE, FedUMM instantiates federation for a BLIP3o backbone via parameter-efficient fine-tuning: clients train lightweight LoRA adapters while freezing the foundation models, and the server aggregates only adapter updates. We evaluate on VQA v2 and the GenEval compositional generation benchmarks under Dirichlet-controlled heterogeneity with up to 16 clients. Results show slight degradation as client count and heterogeneity increase, while remaining competitive with centralized training. We further analyze computation--communication trade-offs and demonstrate that adapter-only federation reduces per-round communication by over an order of magnitude compared to full fine-tuning, enabling practical federated UMM training. This work provides empirical experience for future research on privacy-preserving federated unified multimodal models.

</details>


### [74] [Attention-Informed Surrogates for Navigating Power-Performance Trade-offs in HPC](https://arxiv.org/abs/2601.15399)
*Ashna Nawar Ahmed,Banooqa Banday,Terry Jones,Tanzima Z. Islam*

Main category: cs.LG

TL;DR: A multi-objective Bayesian optimization framework using attention-based embeddings to optimize HPC job scheduling for runtime-power trade-offs, achieving better Pareto fronts with data-efficient sampling.


<details>
  <summary>Details</summary>
Motivation: HPC schedulers need to balance user performance with resource constraints, requiring optimal node allocation decisions that are complex and currently not automated effectively.

Method: Surrogate-assisted multi-objective Bayesian optimization (MOBO) with attention-based embeddings of job telemetry to capture performance dynamics, paired with intelligent sample acquisition for data efficiency.

Result: On two production HPC datasets, the embedding-informed method consistently identified higher-quality Pareto fronts of runtime-power trade-offs compared to baselines, with drastically reduced training costs and improved stability.

Conclusion: First successful application of embedding-informed surrogates in MOBO framework for HPC scheduling, jointly optimizing performance and power on production workloads.

Abstract: High-Performance Computing (HPC) schedulers must balance user performance with facility-wide resource constraints. The task boils down to selecting the optimal number of nodes for a given job. We present a surrogate-assisted multi-objective Bayesian optimization (MOBO) framework to automate this complex decision. Our core hypothesis is that surrogate models informed by attention-based embeddings of job telemetry can capture performance dynamics more effectively than standard regression techniques. We pair this with an intelligent sample acquisition strategy to ensure the approach is data-efficient. On two production HPC datasets, our embedding-informed method consistently identified higher-quality Pareto fronts of runtime-power trade-offs compared to baselines. Furthermore, our intelligent data sampling strategy drastically reduced training costs while improving the stability of the results. To our knowledge, this is the first work to successfully apply embedding-informed surrogates in a MOBO framework to the HPC scheduling problem, jointly optimizing for performance and power on production workloads.

</details>


### [75] [Ambient Dataloops: Generative Models for Dataset Refinement](https://arxiv.org/abs/2601.15417)
*Adrián Rodríguez-Muñoz,William Daspit,Adam Klivans,Antonio Torralba,Constantinos Daskalakis,Giannis Daras*

Main category: cs.LG

TL;DR: Ambient Dataloops is an iterative dataset refinement framework that improves diffusion model training by progressively enhancing dataset quality through a dataset-model co-evolution process, preventing destructive self-consuming loops with Ambient Diffusion techniques.


<details>
  <summary>Details</summary>
Motivation: Modern datasets contain samples of varying quality, and training directly on such heterogeneous data yields suboptimal models. There's a need for a method to systematically improve dataset quality to enhance model performance.

Method: An iterative dataset-model co-evolution framework where: 1) At each iteration, synthetically improved samples are treated as noisy but at slightly lower noise levels than previous iterations, 2) Ambient Diffusion techniques are used for learning under corruption, 3) This prevents destructive self-consuming loops while progressively improving both dataset and model quality.

Result: Achieves state-of-the-art performance in unconditional and text-conditional image generation and de novo protein design. The framework also provides theoretical justification for the data looping procedure.

Conclusion: Ambient Dataloops provides an effective framework for dataset refinement that enables diffusion models to better learn underlying data distributions through iterative dataset-model co-evolution, with applications across multiple domains including image generation and protein design.

Abstract: We propose Ambient Dataloops, an iterative framework for refining datasets that makes it easier for diffusion models to learn the underlying data distribution. Modern datasets contain samples of highly varying quality, and training directly on such heterogeneous data often yields suboptimal models. We propose a dataset-model co-evolution process; at each iteration of our method, the dataset becomes progressively higher quality, and the model improves accordingly. To avoid destructive self-consuming loops, at each generation, we treat the synthetically improved samples as noisy, but at a slightly lower noisy level than the previous iteration, and we use Ambient Diffusion techniques for learning under corruption. Empirically, Ambient Dataloops achieve state-of-the-art performance in unconditional and text-conditional image generation and de novo protein design. We further provide a theoretical justification for the proposed framework that captures the benefits of the data looping procedure.

</details>


### [76] [Lattice: A Confidence-Gated Hybrid System for Uncertainty-Aware Sequential Prediction with Behavioral Archetypes](https://arxiv.org/abs/2601.15423)
*Lorian Bannis*

Main category: cs.LG

TL;DR: Lattice is a hybrid sequential prediction system that uses binary confidence gating to conditionally activate learned behavioral archetypes, falling back to baseline predictions when uncertain.


<details>
  <summary>Details</summary>
Motivation: To create a system that can manage epistemic uncertainty in safety-critical applications by only activating learned behavioral structure when confident, preventing false activation during distribution shifts or when structure is already present.

Method: Clusters behavior windows into behavioral archetypes and uses binary confidence gating to activate archetype-based scoring only when confidence exceeds a threshold, otherwise falls back to baseline predictions. Validated on recommendation systems (MovieLens), scientific time-series (LIGO), and financial markets using LSTM and transformer backbones.

Result: On MovieLens with LSTM: +31.9% improvement over LSTM baseline in HR@10, outperforming transformer baselines by 109.4% over SASRec and 218.6% over BERT4Rec. On LIGO/financial data: correctly refuses archetype activation during distribution shift. On transformer backbones: 0.0% improvement (neutral, no degradation).

Conclusion: Confidence gating is a promising architectural principle for managing epistemic uncertainty in safety-critical applications, as it activates when patterns apply, refuses when they don't, and defers when redundant.

Abstract: We introduce Lattice, a hybrid sequential prediction system that conditionally activates learned behavioral structure using binary confidence gating. The system clusters behavior windows into behavioral archetypes and uses binary confidence gating to activate archetype-based scoring only when confidence exceeds a threshold, falling back to baseline predictions when uncertain. We validate Lattice on recommendation systems (MovieLens), scientific time-series (LIGO), and financial markets, using LSTM and transformer backbones. On MovieLens with LSTM, Lattice achieves +31.9% improvement over LSTM baseline in HR@10 (p < 3.29 x 10^-25, 30 seeds), outperforming transformer baselines by 109.4% over SASRec and 218.6% over BERT4Rec. On LIGO and financial data, the system correctly refuses archetype activation when distribution shift occurs - a successful outcome demonstrating confidence gating prevents false activation. On transformer backbones, Lattice provides 0.0% improvement (neutral, no degradation), gracefully deferring when structure is already present. This bidirectional validation - activating when patterns apply, refusing when they don't, and deferring when redundant - supports confidence gating as a promising architectural principle for managing epistemic uncertainty in safety-critical applications.

</details>


### [77] [CASL: Concept-Aligned Sparse Latents for Interpreting Diffusion Models](https://arxiv.org/abs/2601.15441)
*Zhenghao He,Guangzhi Xiong,Boyang Wang,Sanchit Sinha,Aidong Zhang*

Main category: cs.LG

TL;DR: CASL introduces supervised concept alignment for diffusion models using sparse autoencoders and linear mapping, enabling precise semantic control and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised SAE methods fail to align sparse features with human-understandable concepts, limiting reliable semantic control over generated images. There's a need for supervised alignment between latent representations and semantic concepts in diffusion models.

Method: CASL trains an SAE on frozen U-Net activations for disentangled latents, then learns a lightweight linear mapping associating each concept with relevant latent dimensions. CASL-Steer is introduced as a controlled latent intervention for causal probing.

Result: The method achieves superior editing precision and interpretability compared to existing approaches, with the Editing Precision Ratio (EPR) metric showing improved concept specificity and preservation of unrelated attributes.

Conclusion: CASL is the first supervised framework to align sparse latent dimensions of diffusion models with semantic concepts, enabling reliable semantic control and better understanding of diffusion model representations.

Abstract: Internal activations of diffusion models encode rich semantic information, but interpreting such representations remains challenging. While Sparse Autoencoders (SAEs) have shown promise in disentangling latent representations, existing SAE-based methods for diffusion model understanding rely on unsupervised approaches that fail to align sparse features with human-understandable concepts. This limits their ability to provide reliable semantic control over generated images. We introduce CASL (Concept-Aligned Sparse Latents), a supervised framework that aligns sparse latent dimensions of diffusion models with semantic concepts. CASL first trains an SAE on frozen U-Net activations to obtain disentangled latent representations, and then learns a lightweight linear mapping that associates each concept with a small set of relevant latent dimensions. To validate the semantic meaning of these aligned directions, we propose CASL-Steer, a controlled latent intervention that shifts activations along the learned concept axis. Unlike editing methods, CASL-Steer is used solely as a causal probe to reveal how concept-aligned latents influence generated content. We further introduce the Editing Precision Ratio (EPR), a metric that jointly measures concept specificity and the preservation of unrelated attributes. Experiments show that our method achieves superior editing precision and interpretability compared to existing approaches. To the best of our knowledge, this is the first work to achieve supervised alignment between latent representations and semantic concepts in diffusion models.

</details>


### [78] [Learning from Synthetic Data: Limitations of ERM](https://arxiv.org/abs/2601.15468)
*Kareem Amin,Alex Bie,Weiwei Kong,Umar Syed,Sergei Vassilvitskii*

Main category: cs.LG

TL;DR: The paper analyzes learning from mixed natural and synthetic data, showing ERM has limitations while proposing better algorithms for mean estimation and PAC learning.


<details>
  <summary>Details</summary>
Motivation: The rise of LLM-generated synthetic content contaminating natural data creates a fundamental learning theory challenge where algorithms must learn from mixed data without knowing which examples are synthetic.

Method: Model learning as sequence of tasks with mixed natural/synthetic data, analyze ERM performance, propose alternative algorithms with non-uniform weighting across data generations for mean estimation and PAC learning.

Result: ERM converges to true mean but is outperformed by weighted algorithms; for PAC learning, ERM may not converge to true concept (echoing model collapse), but algorithms exist that can learn correct hypothesis for arbitrary VC classes and contamination levels.

Conclusion: Standard ERM is insufficient for learning from mixed natural/synthetic data; new algorithms with strategic weighting across data generations are needed to overcome contamination and achieve reliable learning.

Abstract: The prevalence and low cost of LLMs have led to a rise of synthetic content. From review sites to court documents, ``natural'' content has been contaminated by data points that appear similar to natural data, but are in fact LLM-generated. In this work we revisit fundamental learning theory questions in this, now ubiquitous, setting. We model this scenario as a sequence of learning tasks where the input is a mix of natural and synthetic data, and the learning algorithms are oblivious to the origin of any individual example.
  We study the possibilities and limitations of ERM in this setting. For the problem of estimating the mean of an arbitrary $d$-dimensional distribution, we find that while ERM converges to the true mean, it is outperformed by an algorithm that assigns non-uniform weights to examples from different generations of data. For the PAC learning setting, the disparity is even more stark. We find that ERM does not always converge to the true concept, echoing the model collapse literature. However, we show there are algorithms capable of learning the correct hypothesis for arbitrary VC classes and arbitrary amounts of contamination.

</details>


### [79] [Panther: Faster and Cheaper Computations with Randomized Numerical Linear Algebra](https://arxiv.org/abs/2601.15473)
*Fahd Seddik,Abdulrahman Elbedewy,Gaser Sami,Mohamed Abdelmoniem,Yahia Zakaria*

Main category: cs.LG

TL;DR: Panther is a PyTorch-compatible library that implements Randomized Numerical Linear Algebra (RandNLA) algorithms for efficient deep learning model compression, achieving up to 75% memory savings on BERT with minimal code changes.


<details>
  <summary>Details</summary>
Motivation: Training modern deep learning models is constrained by GPU memory and compute limits. While RandNLA offers proven compression techniques, there's no unified, production-grade library to widely adopt these methods.

Method: Panther consolidates established RandNLA algorithms into a single high-performance framework with efficient drop-in replacements for standard components: sketched linear layers, 2D convolution, multi-head attention, and randomized matrix decompositions. It uses a custom C++/CUDA backend (pawX) optimized for both CPUs and GPUs.

Result: By replacing standard PyTorch linear layers with Panther layers (requiring only a few lines of code), significant memory savings (up to 75%) are achieved on BERT while maintaining comparable loss.

Conclusion: Panther demonstrates the effectiveness of RandNLA techniques and provides an easy-to-adopt solution for model compression, addressing memory constraints in deep learning training with minimal code changes.

Abstract: Training modern deep learning models is increasingly constrained by GPU memory and compute limits. While Randomized Numerical Linear Algebra (RandNLA) offers proven techniques to compress these models, the lack of a unified, production-grade library prevents widely adopting these methods. We present Panther, a PyTorch-compatible library that consolidates established RandNLA algorithms into a single high-performance framework. Panther engineers efficient, drop-in replacements for standard components including sketched linear layers, 2D convolution, multi-head attention, and randomized matrix decompositions (such as pivoted CholeskyQR). By implementing a custom C++/CUDA backend (pawX), Panther provides an optimized implementation that can run on both CPUs and GPUs. We demonstrate the effectiveness of RandNLA techniques and Panther's ease of adoption. By replacing standard PyTorch linear layers with Panther layers (requiring only a few lines of code) we achieve significant memory savings (up to 75%) on BERT while maintaining comparable loss. Source code is available (MIT License) at https://github.com/FahdSeddik/panther, along with demonstration video at https://youtu.be/7M3RQb4KWxs.

</details>


### [80] [Multi-Targeted Graph Backdoor Attack](https://arxiv.org/abs/2601.15474)
*Md Nabi Newaz Khan,Abdullah Arafat Miah,Yu Bi*

Main category: cs.LG

TL;DR: First multi-targeted backdoor attack for graph classification using subgraph injection instead of replacement, achieving high attack success rates across multiple targets with minimal clean accuracy impact.


<details>
  <summary>Details</summary>
Motivation: Existing backdoor attacks for graph classification are limited to single-target attacks using subgraph replacement, which only implants one trigger. There's a need for more sophisticated multi-target attacks that can redirect predictions to different target labels simultaneously.

Method: Proposes subgraph injection attack that preserves original graph structure while poisoning clean graphs. Uses multiple triggers simultaneously to redirect predictions to different target labels, unlike conventional subgraph replacement methods.

Result: Achieves high attack success rates for all target labels with minimal impact on clean accuracy across five datasets. Attack generalizes across four different GNN models regardless of architecture and training parameters. Robust against state-of-the-art defenses like randomized smoothing and fine-pruning.

Conclusion: Demonstrates GNN vulnerability to multi-targeted backdoor attacks in graph classification tasks. The proposed subgraph injection approach outperforms conventional subgraph replacement methods and highlights significant security concerns for GNN applications.

Abstract: Graph neural network (GNN) have demonstrated exceptional performance in solving critical problems across diverse domains yet remain susceptible to backdoor attacks. Existing studies on backdoor attack for graph classification are limited to single target attack using subgraph replacement based mechanism where the attacker implants only one trigger into the GNN model. In this paper, we introduce the first multi-targeted backdoor attack for graph classification task, where multiple triggers simultaneously redirect predictions to different target labels. Instead of subgraph replacement, we propose subgraph injection which preserves the structure of the original graphs while poisoning the clean graphs. Extensive experiments demonstrate the efficacy of our approach, where our attack achieves high attack success rates for all target labels with minimal impact on the clean accuracy. Experimental results on five dataset demonstrate the superior performance of our attack framework compared to the conventional subgraph replacement-based attack. Our analysis on four GNN models confirms the generalization capability of our attack which is effective regardless of the GNN model architectures and training parameters settings. We further investigate the impact of the attack design parameters including injection methods, number of connections, trigger sizes, trigger edge density and poisoning ratios. Additionally, our evaluation against state-of-the-art defenses (randomized smoothing and fine-pruning) demonstrates the robustness of our proposed multi-target attacks. This work highlights the GNN vulnerability against multi-targeted backdoor attack in graph classification task. Our source codes will be available at https://github.com/SiSL-URI/Multi-Targeted-Graph-Backdoor-Attack.

</details>


### [81] [Early predicting of hospital admission using machine learning algorithms: Priority queues approach](https://arxiv.org/abs/2601.15481)
*Jakub Antczak,James Montgomery,Małgorzata O'Reilly,Zbigniew Palmowski,Richard Turner*

Main category: cs.LG

TL;DR: This study compares SARIMAX, XGBoost, and LSTM models for forecasting daily emergency department arrivals, with XGBoost performing best for total admissions and SARIMAX for complex cases, though all underestimate sudden surges.


<details>
  <summary>Details</summary>
Motivation: Emergency Department overcrowding compromises patient safety and operational efficiency, requiring accurate demand forecasting for effective resource allocation in healthcare settings.

Method: The study evaluates SARIMAX, XGBoost, and LSTM models for 7-day ED arrival forecasting using Australian hospital data (2017-2021). It decomposes demand into 8 ward categories and stratifies by clinical complexity, using Prophet model to generate synthetic counterfactual values for COVID-19 anomalies.

Result: All three models outperformed seasonal naive baseline. XGBoost achieved best accuracy for total daily admissions (MAE=6.63), while SARIMAX performed marginally better for major complexity cases (MAE=3.77). All models successfully reproduced regular patterns but underestimated sudden patient surges.

Conclusion: While the predictive techniques effectively forecast regular ED patterns, they share a limitation in underestimating infrequent, sudden surges in patient volume, highlighting an area for future improvement in emergency department demand forecasting.

Abstract: Emergency Department overcrowding is a critical issue that compromises patient safety and operational efficiency, necessitating accurate demand forecasting for effective resource allocation. This study evaluates and compares three distinct predictive models: Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors (SARIMAX), EXtreme Gradient Boosting (XGBoost) and Long Short-Term Memory (LSTM) networks for forecasting daily ED arrivals over a seven-day horizon. Utilizing data from an Australian tertiary referral hospital spanning January 2017 to December 2021, this research distinguishes itself by decomposing demand into eight specific ward categories and stratifying patients by clinical complexity. To address data distortions caused by the COVID-19 pandemic, the study employs the Prophet model to generate synthetic counterfactual values for the anomalous period. Experimental results demonstrate that all three proposed models consistently outperform a seasonal naive baseline. XGBoost demonstrated the highest accuracy for predicting total daily admissions with a Mean Absolute Error of 6.63, while the statistical SARIMAX model proved marginally superior for forecasting major complexity cases with an MAE of 3.77. The study concludes that while these techniques successfully reproduce regular day-to-day patterns, they share a common limitation in underestimating sudden, infrequent surges in patient volume.

</details>


### [82] [Martingale Foresight Sampling: A Principled Approach to Inference-Time LLM Decoding](https://arxiv.org/abs/2601.15482)
*Huayu Li,ZhengXiao He,Siyuan Tian,Jinghao Wen,Ao Li*

Main category: cs.LG

TL;DR: MFS introduces a principled framework using Martingale theory to improve LLM decoding by modeling reasoning paths as stochastic processes, replacing heuristic methods with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Standard autoregressive decoding in LLMs is short-sighted and fails to find globally optimal reasoning paths. Existing inference-time strategies rely on ad-hoc heuristics for path valuation and search space pruning, lacking theoretical foundations.

Method: Reformulates LLM decoding as identifying an optimal stochastic process. Uses Martingale theory: step valuation via Doob Decomposition Theorem, path selection via Optional Stopping Theory, and adaptive stopping via Martingale Convergence Theorem.

Result: Experiments on six reasoning benchmarks show MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency.

Conclusion: MFS provides a theoretically-grounded alternative to heuristic-based decoding strategies, demonstrating both improved performance and efficiency through principled application of probability theory.

Abstract: Standard autoregressive decoding in large language models (LLMs) is inherently short-sighted, often failing to find globally optimal reasoning paths due to its token-by-token generation process. While inference-time strategies like foresight sampling attempt to mitigate this by simulating future steps, they typically rely on ad-hoc heuristics for valuing paths and pruning the search space. This paper introduces Martingale Foresight Sampling (MFS), a principled framework that reformulates LLM decoding as a problem of identifying an optimal stochastic process. By modeling the quality of a reasoning path as a stochastic process, we leverage Martingale theory to design a theoretically-grounded algorithm. Our approach replaces heuristic mechanisms with principles from probability theory: step valuation is derived from the Doob Decomposition Theorem to measure a path's predictable advantage, path selection uses Optional Stopping Theory for principled pruning of suboptimal candidates, and an adaptive stopping rule based on the Martingale Convergence Theorem terminates exploration once a path's quality has provably converged. Experiments on six reasoning benchmarks demonstrate that MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency. Code will be released at https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Sampling.

</details>


### [83] [MARS: Unleashing the Power of Speculative Decoding via Margin-Aware Verification](https://arxiv.org/abs/2601.15498)
*Jingwei Song,Xinyu Wang,Hanbin Wang,Xiaoxuan Lei,Bill Shi,Shixin Han,Eric Yang,Xiao-Wen Chang,Lynn Ai*

Main category: cs.LG

TL;DR: Margin-Aware Speculative Verification improves speculative decoding by adapting verification to the target model's decisiveness, relaxing strict token rejection when it provides minimal benefit, achieving consistent speedups across model scales.


<details>
  <summary>Details</summary>
Motivation: Current speculative decoding verification relies on strict token-level rejection sampling, which is inefficient in low-margin regimes where target models have weak preference among top candidates. Rejecting plausible runner-up tokens yields negligible information gain while incurring substantial rollback costs.

Method: Proposes Margin-Aware Speculative Verification, a training-free and domain-agnostic verification strategy that conditions verification on decision stability measured directly from target model logits. It relaxes rejection only when strict verification provides minimal benefit, modifying only the verification rule while being fully compatible with existing target-coupled speculative decoding frameworks.

Result: Extensive experiments across model scales from 8B to 235B demonstrate consistent and significant inference speedups over state-of-the-art baselines while preserving generation quality across diverse benchmarks.

Conclusion: Margin-Aware Speculative Verification addresses fundamental inefficiency in speculative decoding verification by adapting to target model's local decisiveness, providing a practical improvement that works with existing frameworks without requiring retraining.

Abstract: Speculative Decoding (SD) accelerates autoregressive large language model (LLM) inference by decoupling generation and verification. While recent methods improve draft quality by tightly coupling the drafter with the target model, the verification mechanism itself remains largely unchanged, relying on strict token-level rejection sampling. In practice, modern LLMs frequently operate in low-margin regimes where the target model exhibits weak preference among top candidates. In such cases, rejecting plausible runner-up tokens yields negligible information gain while incurring substantial rollback cost, leading to a fundamental inefficiency in verification. We propose Margin-Aware Speculative Verification, a training-free and domain-agnostic verification strategy that adapts to the target model's local decisiveness. Our method conditions verification on decision stability measured directly from the target logits and relaxes rejection only when strict verification provides minimal benefit. Importantly, the approach modifies only the verification rule and is fully compatible with existing target-coupled speculative decoding frameworks. Extensive experiments across model scales ranging from 8B to 235B demonstrate that our method delivers consistent and significant inference speedups over state-of-the-art baselines while preserving generation quality across diverse benchmarks.

</details>


### [84] [Data-driven Lake Water Quality Forecasting for Time Series with Missing Data using Machine Learning](https://arxiv.org/abs/2601.15503)
*Rishit Chatterjee,Tahiya Chowdhury*

Main category: cs.LG

TL;DR: Ridge regression is optimal for forecasting Secchi Disk Depth in lakes, requiring only ~64 recent samples and 1 predictor to stay within 5% of full-history, full-feature accuracy, enabling efficient lake monitoring.


<details>
  <summary>Details</summary>
Motivation: Volunteer-led lake monitoring produces irregular, seasonal time series with many gaps due to ice cover, weather constraints, and human errors, making harmful algal bloom forecasting and early warning difficult.

Method: Used 30-lake dataset from three decades of Maine lake records; handled missing data with MICE; evaluated six models with nMAE metric; ridge regression performed best; determined minimal sample size and feature set needed to stay within 5% of baseline accuracy.

Result: Ridge regression provided best mean test performance; model reaches within 5% of full-history accuracy with ~176 training samples; compact 4-feature subset matches 13-feature baseline; joint feasibility analysis shows only ~64 recent samples and 1 predictor needed per lake.

Conclusion: The joint feasibility strategy unifies recent-history length and feature choice under fixed accuracy targets, providing simple, efficient rules for setting sampling effort and measurement priorities for lake researchers, making targeted monitoring practical.

Abstract: Volunteer-led lake monitoring yields irregular, seasonal time series with many gaps arising from ice cover, weather-related access constraints, and occasional human errors, complicating forecasting and early warning of harmful algal blooms. We study Secchi Disk Depth (SDD) forecasting on a 30-lake, data-rich subset drawn from three decades of in situ records collected across Maine lakes. Missingness is handled via Multiple Imputation by Chained Equations (MICE), and we evaluate performance with a normalized Mean Absolute Error (nMAE) metric for cross-lake comparability. Among six candidates, ridge regression provides the best mean test performance. Using ridge regression, we then quantify the minimal sample size, showing that under a backward, recent-history protocol, the model reaches within 5% of full-history accuracy with approximately 176 training samples per lake on average. We also identify a minimal feature set, where a compact four-feature subset matches the thirteen-feature baseline within the same 5% tolerance. Bringing these results together, we introduce a joint feasibility function that identifies the minimal training history and fewest predictors sufficient to achieve the target of staying within 5% of the complete-history, full-feature baseline. In our study, meeting the 5% accuracy target required about 64 recent samples and just one predictor per lake, highlighting the practicality of targeted monitoring. Hence, our joint feasibility strategy unifies recent-history length and feature choice under a fixed accuracy target, yielding a simple, efficient rule for setting sampling effort and measurement priorities for lake researchers.

</details>


### [85] [SAGE-FM: A lightweight and interpretable spatial transcriptomics foundation model](https://arxiv.org/abs/2601.15504)
*Xianghao Zhan,Jingyu Xu,Yuanning Zheng,Zinaida Good,Olivier Gevaert*

Main category: cs.LG

TL;DR: SAGE-FM is a lightweight spatial transcriptomics foundation model using graph convolutional networks that learns spatially coherent embeddings and generalizes well to downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Spatial transcriptomics enables spatial gene expression profiling, motivating computational models that capture spatially conditioned regulatory relationships.

Method: SAGE-FM uses graph convolutional networks (GCNs) trained with masked central spot prediction objective on 416 human Visium samples spanning 15 organs.

Result: The model learns spatially coherent embeddings that robustly recover masked genes (91% significant correlations), outperforms existing methods in clustering and heterogeneity preservation, achieves 81% accuracy in spot annotation, and captures directional regulatory effects.

Conclusion: Simple, parameter-efficient GCNs can serve as biologically interpretable and spatially aware foundation models for large-scale spatial transcriptomics.

Abstract: Spatial transcriptomics enables spatial gene expression profiling, motivating computational models that capture spatially conditioned regulatory relationships. We introduce SAGE-FM, a lightweight spatial transcriptomics foundation model based on graph convolutional networks (GCNs) trained with a masked central spot prediction objective. Trained on 416 human Visium samples spanning 15 organs, SAGE-FM learns spatially coherent embeddings that robustly recover masked genes, with 91% of masked genes showing significant correlations (p < 0.05). The embeddings generated by SAGE-FM outperform MOFA and existing spatial transcriptomics methods in unsupervised clustering and preservation of biological heterogeneity. SAGE-FM generalizes to downstream tasks, enabling 81% accuracy in pathologist-defined spot annotation in oropharyngeal squamous cell carcinoma and improving glioblastoma subtype prediction relative to MOFA. In silico perturbation experiments further demonstrate that the model captures directional ligand-receptor and upstream-downstream regulatory effects consistent with ground truth. These results demonstrate that simple, parameter-efficient GCNs can serve as biologically interpretable and spatially aware foundation models for large-scale spatial transcriptomics.

</details>


### [86] [Machine learning-enhanced non-amnestic Alzheimer's disease diagnosis from MRI and clinical features](https://arxiv.org/abs/2601.15530)
*Megan A. Witherow,Michael L. Evans,Ahmed Temtam,Hamid Okhravi,Khan M. Iftekharuddin*

Main category: cs.LG

TL;DR: ML approach improves diagnosis of atypical Alzheimer's disease using clinical tests and MRI features, boosting recall from 34-52% to 69-77%.


<details>
  <summary>Details</summary>
Motivation: Atypical Alzheimer's disease (atAD) patients are often misdiagnosed because current clinical methods (cognitive tests and hippocampal volume on MRI) work well for typical AD but not for atypical presentations. There's a need for better diagnostic tools using standard clinical data.

Method: Machine learning approach using clinical testing battery and MRI data from 1410 subjects across four groups. Multiple classification experiments comparing atAD vs. non-AD using clinical features, hippocampal volume, and comprehensive MRI features across the brain. Boruta statistical approach used to identify significant brain regions.

Result: Best performance achieved by incorporating additional important MRI features beyond just hippocampal volume. Improved recall for atAD diagnosis from 52% to 69% for NACC data and from 34% to 77% for ADNI data, while maintaining high precision.

Conclusion: The ML approach significantly improves diagnostic accuracy for atypical Alzheimer's disease using only standard clinical testing and MRI data, with important implications for clinical practice where invasive biomarker collection is not feasible.

Abstract: Alzheimer's disease (AD), defined as an abnormal buildup of amyloid plaques and tau tangles in the brain can be diagnosed with high accuracy based on protein biomarkers via PET or CSF analysis. However, due to the invasive nature of biomarker collection, most AD diagnoses are made in memory clinics using cognitive tests and evaluation of hippocampal atrophy based on MRI. While clinical assessment and hippocampal volume show high diagnostic accuracy for amnestic or typical AD (tAD), a substantial subgroup of AD patients with atypical presentation (atAD) are routinely misdiagnosed. To improve diagnosis of atAD patients, we propose a machine learning approach to distinguish between atAD and non-AD cognitive impairment using clinical testing battery and MRI data collected as standard-of-care. We develop and evaluate our approach using 1410 subjects across four groups (273 tAD, 184 atAD, 235 non-AD, and 685 cognitively normal) collected from one private data set and two public data sets from the National Alzheimer's Coordinating Center (NACC) and the Alzheimer's Disease Neuroimaging Initiative (ADNI). We perform multiple atAD vs. non-AD classification experiments using clinical features and hippocampal volume as well as a comprehensive set of MRI features from across the brain. The best performance is achieved by incorporating additional important MRI features, which outperforms using hippocampal volume alone. Furthermore, we use the Boruta statistical approach to identify and visualize significant brain regions distinguishing between diagnostic groups. Our ML approach improves the percentage of correctly diagnosed atAD cases (the recall) from 52% to 69% for NACC and from 34% to 77% for ADNI, while achieving high precision. The proposed approach has important implications for improving diagnostic accuracy for non-amnestic atAD in clinical settings using only clinical testing battery and MRI.

</details>


### [87] [QUAIL: Quantization Aware Unlearning for Mitigating Misinformation in LLMs](https://arxiv.org/abs/2601.15538)
*Himanshu Mishra,Kanwal Mehreen*

Main category: cs.LG

TL;DR: Quantization can catastrophically restore forgotten information in machine unlearning; proposed method uses logits space hinge loss to preserve forgetting under 4-bit quantization.


<details>
  <summary>Details</summary>
Motivation: Machine unlearning aims to remove specific knowledge from trained models, but quantization (e.g., 4-bit) used in deployment can restore forgotten information, undermining unlearning effectiveness.

Method: Analyze weight-change statistics and bucket overlaps in quantization to show unlearning updates are too small to cross quantization thresholds. Introduce logits space hinge loss that forces output logits of unlearned model to differ from original by at least half the quantization step margin.

Result: Method preserves forgetting under 4-bit quantization on language and classification tasks (including Twitter misinformation dataset), while existing methods almost entirely recover forgotten knowledge.

Conclusion: Quantization-aware unlearning with logits space hinge loss effectively maintains forgetting under low-bit quantization, addressing a critical deployment issue for unlearned models.

Abstract: Machine unlearning aims to remove specific knowledge (e.g., copyrighted or private data) from a trained model without full retraining. In practice, models are often quantized (e.g., 4-bit) for deployment, but we find that quantization can catastrophically restore forgotten information [1]. In this paper, we (1) analyze why low-bit quantization undermines unlearning, and (2) propose a quantization-aware unlearning method to mitigate this. We first compute weight-change statistics and bucket overlaps in quantization to show that typical unlearning updates are too small to cross quantization thresholds. Building on this insight, we introduce a logits space hinge loss: for each forget example, we force the output logits of the unlearned model to differ from the original model by at least a margin (half the quantization step). This ensures forgotten examples remain distinguishable even after quantization. We evaluate on language and classification tasks (including a Twitter misinformation dataset) and show our method preserves forgetting under 4-bit quantization, whereas existing methods almost entirely recover the forgotten knowledge.

</details>


### [88] [PRISM: Deriving the Transformer as a Signal-Denoising Operator via Maximum Coding Rate Reduction](https://arxiv.org/abs/2601.15540)
*Dongchen Huang*

Main category: cs.LG

TL;DR: Prism is a white-box attention architecture based on MCR² principles that uses geometric constraints to achieve unsupervised functional disentanglement, showing interpretability and performance are not mutually exclusive.


<details>
  <summary>Details</summary>
Motivation: Transformers are criticized as black boxes lacking interpretability. The authors aim to create a more interpretable architecture that maintains performance while providing transparency into how attention mechanisms work.

Method: Propose Prism architecture derived from MCR² principles, modeling attention as gradient ascent on signal-noise manifold. Introduce two physical constraints: overcomplete dictionary for expanded phase space and irrational frequency separation (π-RoPE) to enforce incoherence between signal and noise subspaces.

Result: Prism spontaneously specializes attention heads into spectrally distinct regimes: low-frequency heads capture long-range causal dependencies (signal) while high-frequency heads handle local syntactic constraints (noise). This functional disentanglement emerges unsupervised.

Conclusion: Interpretability and performance are not a trade-off but can be unified through principled geometric construction. Geometric inductive biases can induce unsupervised functional disentanglement in attention mechanisms.

Abstract: Deep learning models, particularly Transformers, are often criticized as "black boxes" and lack interpretability. We propose Prism, a white-box attention-based architecture derived from the principles of Maximizing Coding Rate Reduction ($\text{MCR}^2$). By modeling the attention mechanism as a gradient ascent process on a distinct signal-noise manifold, we introduce two physical constraints: an overcomplete dictionary to expand the representational phase space, and an irrational frequency separation ($π$-RoPE) to enforce incoherence between signal and noise subspaces. We demonstrate that these geometric inductive biases can be viewed as a physical constraint and they are sufficient to induce unsupervised functional disentanglement alone. Using TinyStories as a controlled testbed for verifying spectral dynamics, we observe that Prism spontaneously specializes its attention heads into spectrally distinct regimes: low-frequency heads capturing long-range causal dependencies (signal) and high-frequency heads handling local syntactic constraints (noise). Our results suggest that interpretability and performance are not a trade-off, but can be unified through principled geometric construction.

</details>


### [89] [RDumb++: Drift-Aware Continual Test-Time Adaptation](https://arxiv.org/abs/2601.15544)
*Himanshu Mishra*

Main category: cs.LG

TL;DR: RDumb++ improves continual test-time adaptation by adding drift detection mechanisms (entropy and KL-divergence scoring) with adaptive reset strategies to prevent prediction collapse during long-term deployment with rapidly changing distributions.


<details>
  <summary>Details</summary>
Motivation: Existing CTTA methods like Tent and EATA struggle with rapidly changing test distributions over extremely long horizons, as demonstrated by the CCC benchmark with 7.5M samples and continually changing corruption types and severities.

Method: RDumb++ extends RDumb with two drift-detection mechanisms: entropy-based drift scoring and KL-divergence drift scoring, combined with adaptive reset strategies that detect when accumulated adaptation becomes harmful and recover before prediction collapse occurs.

Result: Across CCC-medium with three speeds and three seeds (nine runs, each containing one million samples), RDumb++ consistently surpasses RDumb, achieving approximately 3% absolute accuracy gains while maintaining stable adaptation throughout the entire stream.

Conclusion: Drift-aware resetting is essential for preventing collapse and achieving reliable long-horizon CTTA, as demonstrated by ablation experiments on drift thresholds and reset strengths.

Abstract: Continual Test-Time Adaptation (CTTA) seeks to update a pretrained model during deployment using only the incoming, unlabeled data stream. Although prior approaches such as Tent, EATA etc. provide meaningful improvements under short evolving shifts, they struggle when the test distribution changes rapidly or over extremely long horizons. This challenge is exemplified by the CCC benchmark, where models operate over streams of 7.5M samples with continually changing corruption types and severities. We propose RDumb++, a principled extension of RDumb that introduces two drift-detection mechanisms i.e entropy-based drift scoring and KL-divergence drift scoring, together with adaptive reset strategies. These mechanisms allow the model to detect when accumulated adaptation becomes harmful and to recover before prediction collapse occurs. Across CCC-medium with three speeds and three seeds (nine runs, each containing one million samples), RDumb++ consistently surpasses RDumb, yielding approx 3% absolute accuracy gains while maintaining stable adaptation throughout the entire stream. Ablation experiments on drift thresholds and reset strengths further show that drift-aware resetting is essential for preventing collapse and achieving reliable long-horizon CTTA.

</details>


### [90] [Beyond validation loss: Clinically-tailored optimization metrics improve a model's clinical performance](https://arxiv.org/abs/2601.15546)
*Charles B. Delahunt,Courosh Mehanian,Daniel E. Shea,Matthew P. Horning*

Main category: cs.LG

TL;DR: Using clinically-tailored metrics instead of validation loss for model optimization in healthcare ML leads to better clinical performance.


<details>
  <summary>Details</summary>
Motivation: Traditional ML uses validation loss for optimization, but healthcare ML has distinct goals - models must meet specific clinical requirements rather than just minimize training loss. Clinical requirements are better captured by tailored metrics.

Method: Conducted two controlled experiments comparing model optimization using clinically-tailored metrics versus traditional validation loss. These metrics don't need to be differentiable since many optimization tasks don't require differentiability.

Result: Clinically-tailored metrics provide superior model optimization compared to validation loss, resulting in better performance on the clinical task.

Conclusion: While using clinically-relevant metrics requires extra effort for definition and implementation, it yields models that better meet the central goal of healthcare ML: strong clinical performance.

Abstract: A key task in ML is to optimize models at various stages, e.g. by choosing hyperparameters or picking a stopping point. A traditional ML approach is to use validation loss, i.e. to apply the training loss function on a validation set to guide these optimizations. However, ML for healthcare has a distinct goal from traditional ML: Models must perform well relative to specific clinical requirements, vs. relative to the loss function used for training. These clinical requirements can be captured more precisely by tailored metrics. Since many optimization tasks do not require the driving metric to be differentiable, they allow a wider range of options, including the use of metrics tailored to be clinically-relevant. In this paper we describe two controlled experiments which show how the use of clinically-tailored metrics provide superior model optimization compared to validation loss, in the sense of better performance on the clinical task. The use of clinically-relevant metrics for optimization entails some extra effort, to define the metrics and to code them into the pipeline. But it can yield models that better meet the central goal of ML for healthcare: strong performance in the clinic.

</details>


### [91] [Learning Neural Operators from Partial Observations via Latent Autoregressive Modeling](https://arxiv.org/abs/2601.15547)
*Jingren Hou,Hong Wang,Pengyu Xu,Chang Gao,Huafeng Liu,Liping Jing*

Main category: cs.LG

TL;DR: LANO introduces a framework for learning neural operators from partial observational data, addressing supervision gaps and spatial mismatches through mask-to-predict training and physics-aware latent propagation.


<details>
  <summary>Details</summary>
Motivation: Real-world scientific applications often have incomplete observational data due to sensor limitations, geographic constraints, or measurement costs. Current neural operators assume fully-observed spatial inputs, which severely restricts their applicability to practical scenarios with partial observations.

Method: Proposes Latent Autoregressive Neural Operator (LANO) with two key components: 1) mask-to-predict training strategy that creates artificial supervision by strategically masking observed regions, and 2) Physics-Aware Latent Propagator that reconstructs solutions through boundary-first autoregressive generation in latent space. Also introduces POBench-PDE benchmark for evaluating neural operators under partial observation conditions.

Result: Achieves state-of-the-art performance with 18-69% relative L2 error reduction across all benchmarks under patch-wise missingness with less than 50% missing rate, including real-world climate prediction. Effectively handles scenarios with up to 75% missing rate.

Conclusion: LANO bridges the gap between idealized research settings and real-world scientific computing by enabling neural operators to work effectively with partial observational data, addressing fundamental obstacles of supervision gaps and spatial mismatches in unobserved regions.

Abstract: Real-world scientific applications frequently encounter incomplete observational data due to sensor limitations, geographic constraints, or measurement costs. Although neural operators significantly advanced PDE solving in terms of computational efficiency and accuracy, their underlying assumption of fully-observed spatial inputs severely restricts applicability in real-world applications. We introduce the first systematic framework for learning neural operators from partial observation. We identify and formalize two fundamental obstacles: (i) the supervision gap in unobserved regions that prevents effective learning of physical correlations, and (ii) the dynamic spatial mismatch between incomplete inputs and complete solution fields. Specifically, our proposed Latent Autoregressive Neural Operator~(\ours) introduces two novel components designed explicitly to address the core difficulties of partial observations: (i) a mask-to-predict training strategy that creates artificial supervision by strategically masking observed regions, and (ii) a Physics-Aware Latent Propagator that reconstructs solutions through boundary-first autoregressive generation in latent space. Additionally, we develop POBench-PDE, a dedicated and comprehensive benchmark designed specifically for evaluating neural operators under partial observation conditions across three PDE-governed tasks. \ours achieves state-of-the-art performance with 18--69$\%$ relative L2 error reduction across all benchmarks under patch-wise missingness with less than 50$\%$ missing rate, including real-world climate prediction. Our approach effectively addresses practical scenarios involving up to 75$\%$ missing rate, to some extent bridging the existing gap between idealized research settings and the complexities of real-world scientific computing.

</details>


### [92] [Neural Nonlinear Shrinkage of Covariance Matrices for Minimum Variance Portfolio Optimization](https://arxiv.org/abs/2601.15597)
*Liusha Yang,Siqi Zhao,Shuqi Chai*

Main category: cs.LG

TL;DR: Neural network-based nonlinear shrinkage estimator for covariance matrices that combines Ledoit-Wolf shrinkage with transformer networks to optimize minimum variance portfolios.


<details>
  <summary>Details</summary>
Motivation: To improve minimum variance portfolio optimization by developing a covariance matrix estimator that directly targets portfolio risk minimization, overcoming limitations of traditional statistical estimators.

Method: Hybrid approach starting from Ledoit-Wolf shrinkage estimator, decomposing covariance matrix into eigenvalues/eigenvectors, applying lightweight transformer-based neural network to learn nonlinear eigenvalue shrinkage function, trained with portfolio risk as loss function.

Result: Method consistently achieves lower out-of-sample realized risk than benchmark approaches on S&P500 stock daily returns, demonstrating scalability across different sample sizes and asset universes.

Conclusion: Shows promise of integrating structural statistical models with data-driven learning for improved covariance estimation and portfolio optimization.

Abstract: This paper introduces a neural network-based nonlinear shrinkage estimator of covariance matrices for the purpose of minimum variance portfolio optimization. It is a hybrid approach that integrates statistical estimation with machine learning. Starting from the Ledoit-Wolf (LW) shrinkage estimator, we decompose the LW covariance matrix into its eigenvalues and eigenvectors, and apply a lightweight transformer-based neural network to learn a nonlinear eigenvalue shrinkage function. Trained with portfolio risk as the loss function, the resulting precision matrix (the inverse covariance matrix) estimator directly targets portfolio risk minimization. By conditioning on the sample-to-dimension ratio, the approach remains scalable across different sample sizes and asset universes. Empirical results on stock daily returns from Standard & Poor's 500 Index (S&P500) demonstrate that the proposed method consistently achieves lower out-of-sample realized risk than benchmark approaches. This highlights the promise of integrating structural statistical models with data-driven learning.

</details>


### [93] [BanditLP: Large-Scale Stochastic Optimization for Personalized Recommendations](https://arxiv.org/abs/2601.15552)
*Phuc Nguyen,Benjamin Zelditch,Joyce Chen,Rohit Patra,Changshuai Wei*

Main category: cs.LG

TL;DR: BanditLP is a scalable multi-stakeholder contextual bandit framework combining neural Thompson Sampling for outcome learning with large-scale linear programming for constrained action selection, achieving business wins in LinkedIn's email marketing system.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for scalable multi-stakeholder decision-making systems that can handle exploration (learning) and constrained optimization simultaneously in production environments, particularly for applications like email marketing where multiple objectives and constraints must be balanced.

Method: BanditLP unifies neural Thompson Sampling for learning objective-specific outcomes with a large-scale linear program for constrained action selection. The framework is application-agnostic, compatible with arbitrary neural architectures, and uses an LP solver capable of handling billions of variables for web-scale deployment.

Result: Experiments on public benchmarks and synthetic data show consistent gains over strong baselines. Application in LinkedIn's email marketing system demonstrates business wins, illustrating the value of integrated exploration and constrained optimization in production.

Conclusion: BanditLP successfully integrates neural Thompson Sampling with large-scale linear programming to create a scalable multi-stakeholder contextual bandit framework that delivers practical business value in production systems, highlighting the importance of combining exploration with constrained optimization.

Abstract: We present BanditLP, a scalable multi-stakeholder contextual bandit framework that unifies neural Thompson Sampling for learning objective-specific outcomes with a large-scale linear program for constrained action selection at serving time. The methodology is application-agnostic, compatible with arbitrary neural architectures, and deployable at web scale, with an LP solver capable of handling billions of variables. Experiments on public benchmarks and synthetic data show consistent gains over strong baselines. We apply this approach in LinkedIn's email marketing system and demonstrate business win, illustrating the value of integrated exploration and constrained optimization in production.

</details>


### [94] [Deep Learning for Perishable Inventory Systems with Human Knowledge](https://arxiv.org/abs/2601.15589)
*Xuan Liao,Zhenkang Peng,Ying Rong*

Main category: cs.LG

TL;DR: Deep learning approach for perishable inventory management with unknown demand and lead times, using end-to-end learning with structure-guided policies to improve performance over black-box methods.


<details>
  <summary>Details</summary>
Motivation: Managing perishable products with limited lifetimes is challenging due to stockouts or waste. Existing systems face difficulties when both demand process and lead time distribution are unknown, especially with limited historical data.

Method: Developed end-to-end deep learning policies using marginal cost accounting scheme. Created three variants: E2E-BB (black-box), E2E-PIL (structure-guided with projected inventory level policy), and E2E-BPIL (boosted version using homogeneity property).

Result: Experiments show robust performance ordering: E2E-BB < E2E-PIL < E2E-BPIL. Structure-guided approaches reduce effective model complexity and improve learning efficiency with modest flexibility loss.

Conclusion: Deep learning-based decision tools are more effective and robust when guided by human knowledge, highlighting the value of integrating advanced analytics with inventory theory.

Abstract: Managing perishable products with limited lifetimes is a fundamental challenge in inventory management, as poor ordering decisions can quickly lead to stockouts or excessive waste. We study a perishable inventory system with random lead times in which both the demand process and the lead time distribution are unknown. We consider a practical setting where orders are placed using limited historical data together with observed covariates and current system states. To improve learning efficiency under limited data, we adopt a marginal cost accounting scheme that assigns each order a single lifetime cost and yields a unified loss function for end-to-end learning. This enables training a deep learning-based policy that maps observed covariates and system states directly to order quantities. We develop two end-to-end variants: a purely black-box approach that outputs order quantities directly (E2E-BB), and a structure-guided approach that embeds the projected inventory level (PIL) policy, capturing inventory effects through explicit computation rather than additional learning (E2E-PIL). We further show that the objective induced by E2E-PIL is homogeneous of degree one, enabling a boosting technique from operational data analytics (ODA) that yields an enhanced policy (E2E-BPIL). Experiments on synthetic and real data establish a robust performance ordering: E2E-BB is dominated by E2E-PIL, which is further improved by E2E-BPIL. Using an excess-risk decomposition, we show that embedding heuristic policy structure reduces effective model complexity and improves learning efficiency with only a modest loss of flexibility. More broadly, our results suggest that deep learning-based decision tools are more effective and robust when guided by human knowledge, highlighting the value of integrating advanced analytics with inventory theory.

</details>


### [95] [When Sharpening Becomes Collapse: Sampling Bias and Semantic Coupling in RL with Verifiable Rewards](https://arxiv.org/abs/2601.15609)
*Mingyuan Fan,Weiguang Han,Daixin Wang,Cen Chen,Zhiqiang Zhang,Jun Zhou*

Main category: cs.LG

TL;DR: RLVR can cause over-sharpening where policies collapse onto limited modes, suppressing valid alternatives. The paper proposes calibration methods to improve generalization.


<details>
  <summary>Details</summary>
Motivation: To understand whether RLVR elicits novel capabilities or merely sharpens existing knowledge, and to address the over-sharpening phenomenon where policies collapse onto limited modes, suppressing valid alternatives.

Method: Proposes inverse-success advantage calibration to prioritize difficult queries and distribution-level calibration to diversify sampling via a memory network.

Result: Empirical evaluations validate that the proposed strategies can effectively improve generalization in RLVR systems.

Conclusion: RLVR suffers from over-sharpening that biases learning toward sampled modes, but this can be mitigated through proper calibration techniques to improve generalization.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a central paradigm for turning large language models (LLMs) into reliable problem solvers, especially in logic-heavy domains. Despite its empirical success, it remains unclear whether RLVR elicits novel capabilities or merely sharpens the distribution over existing knowledge. We study this by formalizing over-sharpening, a phenomenon where the policy collapses onto limited modes, suppressing valid alternatives. At a high level, we discover finite-batch updates intrinsically bias learning toward sampled modes, triggering a collapse that propagates globally via semantic coupling. To mitigate this, we propose inverse-success advantage calibration to prioritize difficult queries and distribution-level calibration to diversify sampling via a memory network. Empirical evaluations validate that our strategies can effectively improve generalization.

</details>


### [96] [Closing the Gap on the Sample Complexity of 1-Identification](https://arxiv.org/abs/2601.15620)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: The paper addresses the 1-identification problem in multi-armed bandits, providing new lower bounds and an algorithm with tight upper bounds for identifying qualified arms above a threshold.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the 1-identification problem where an agent needs to determine if any arm has mean reward ≥ μ₀, with correctness guarantee 1-δ while minimizing expected pulling times. Previous literature left open the analysis of expected pulling times when multiple qualified arms exist.

Method: The authors use an optimization formulation to derive a new lower bound for expected pulling times when at least one qualified arm exists. They also design a new algorithm that achieves tight upper bounds with logarithmic gaps to the lower bounds.

Result: The paper provides a complete analysis of expected pulling times for the 1-identification problem, including cases with multiple qualified arms. The algorithm's upper bounds are tight up to polynomial logarithmic factors compared to the derived lower bounds.

Conclusion: This work solves the open problem of analyzing expected pulling times in 1-identification when multiple qualified arms exist, providing both theoretical lower bounds and practical algorithms with near-optimal performance guarantees.

Abstract: 1-identification is a fundamental multi-armed bandit formulation on pure exploration. An agent aims to determine whether there exists a qualified arm whose mean reward is not less than a known threshold $μ_0$, or to output \textsf{None} if it believes such an arm does not exist. The agent needs to guarantee its output is correct with probability at least $1-δ$, while making expected total pulling times $\mathbb{E}τ$ as small as possible. We work on 1-identification with two main contributions. (1) We utilize an optimization formulation to derive a new lower bound of $\mathbb{E}τ$, when there is at least one qualified arm. (2) We design a new algorithm, deriving tight upper bounds whose gap to lower bounds are up to a polynomial of logarithm factor across all problem instance. Our result complements the analysis of $\mathbb{E}τ$ when there are multiple qualified arms, which is an open problem left by history literature.

</details>


### [97] [Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors](https://arxiv.org/abs/2601.15625)
*Zhiwei Zhang,Fei Zhao,Rui Wang,Zezhong Wang,Bin Liang,Jiakang Wang,Yao Hu,Shaosheng Cao,Kam-Fai Wong*

Main category: cs.LG

TL;DR: Fission-GRPO improves LLM error recovery in multi-turn tool execution by converting execution errors into corrective supervision during RL training, using diagnostic feedback from an Error Simulator to resample recovery rollouts.


<details>
  <summary>Details</summary>
Motivation: Current LLMs are brittle in multi-turn tool execution - they fail to interpret error feedback and self-correct after tool call errors, hindering reliable real-world deployment where execution errors are inevitable.

Method: Proposes Fission-GRPO framework that converts execution errors into corrective supervision within RL training loop. Core mechanism: fissions failed trajectories into new training instances by augmenting with diagnostic feedback from a finetuned Error Simulator, then resamples recovery rollouts on-policy.

Result: On BFCL v4 Multi-Turn, Fission-GRPO improves error recovery rate of Qwen3-8B by 5.7% absolute, yielding 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.

Conclusion: Fission-GRPO enables LLMs to learn from their own execution errors during exploration rather than static pre-collected error cases, significantly improving error recovery and overall tool-use performance in multi-turn scenarios.

Abstract: Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.

</details>


### [98] [An Empirical Study on Ensemble-Based Transfer Learning Bayesian Optimisation with Mixed Variable Types](https://arxiv.org/abs/2601.15640)
*Natasha Trinkle,Huong Ha,Jeffrey Chan*

Main category: cs.LG

TL;DR: Empirical analysis of ensemble-based transfer learning Bayesian optimization methods, proposing new pipeline components and benchmarks, with findings that warm start initialization and positive weight constraints improve performance.


<details>
  <summary>Details</summary>
Motivation: Bayesian optimization is sample-efficient for expensive black-box functions, and leveraging historic datasets from related problems through transfer learning can enhance its performance. The study aims to empirically analyze various ensemble-based transfer learning methods and pipeline components.

Method: The study performs empirical analysis of ensemble-based transfer learning Bayesian optimization methods, expands on previous work by contributing specific pipeline components, and introduces three new real-time transfer learning benchmarks. Key contributions include a weighting strategy for ensemble surrogate models using regularized regression with positive weight constraints, and a component for handling cases where transfer learning doesn't improve performance.

Result: The analysis finds that two components significantly improve transfer learning Bayesian optimization performance: warm start initialization and constraining ensemble surrogate model weights to be positive.

Conclusion: The study provides empirical evidence that specific pipeline components, particularly warm start initialization and positive weight constraints for ensemble models, enhance transfer learning Bayesian optimization performance, contributing to more effective exploitation of historic datasets for optimization tasks.

Abstract: Bayesian optimisation is a sample efficient method for finding a global optimum of expensive black-box objective functions. Historic datasets from related problems can be exploited to help improve performance of Bayesian optimisation by adapting transfer learning methods to various components of the Bayesian optimisation pipeline. In this study we perform an empirical analysis of various ensemble-based transfer learning Bayesian optimisation methods and pipeline components. We expand on previous work in the literature by contributing some specific pipeline components, and three new real-time transfer learning Bayesian optimisation benchmarks. In particular we propose to use a weighting strategy for ensemble surrogate model predictions based on regularised regression with weights constrained to be positive, and a related component for handling the case when transfer learning is not improving Bayesian optimisation performance. We find that in general, two components that help improve transfer learning Bayesian optimisation performance are warm start initialisation and constraining weights used with ensemble surrogate model to be positive.

</details>


### [99] [Integrating Knowledge Distillation Methods: A Sequential Multi-Stage Framework](https://arxiv.org/abs/2601.15657)
*Yinxi Tian,Changwu Huang,Ke Tang,Xin Yao*

Main category: cs.LG

TL;DR: SMSKD is a sequential multi-stage knowledge distillation framework that integrates heterogeneous KD methods stage-by-stage while using frozen reference models to prevent forgetting, with adaptive TCP-based weighting for better knowledge retention.


<details>
  <summary>Details</summary>
Motivation: Current knowledge distillation methods face challenges when integrating multiple approaches: complex implementation, inflexible combinations, and catastrophic forgetting that limit practical effectiveness despite the promise of combining different knowledge sources.

Method: Sequential Multi-Stage Knowledge Distillation (SMSKD) trains students in stages with different distillation methods, using frozen reference models from previous stages to anchor learned knowledge and prevent forgetting, plus adaptive weighting based on teacher true class probability (TCP) to balance knowledge retention and integration.

Result: SMSKD consistently improves student accuracy across diverse teacher-student architectures and method combinations, outperforming existing baselines with negligible computational overhead. Ablation studies show stage-wise distillation and reference model supervision are primary contributors to gains, with TCP-based adaptive weighting providing complementary benefits.

Conclusion: SMSKD provides a practical and resource-efficient solution for integrating heterogeneous KD methods, supporting arbitrary method combinations and stage counts while effectively mitigating catastrophic forgetting through sequential training with reference model anchoring.

Abstract: Knowledge distillation (KD) transfers knowledge from large teacher models to compact student models, enabling efficient deployment on resource constrained devices. While diverse KD methods, including response based, feature based, and relation based approaches, capture different aspects of teacher knowledge, integrating multiple methods or knowledge sources is promising but often hampered by complex implementation, inflexible combinations, and catastrophic forgetting, which limits practical effectiveness.
  This work proposes SMSKD (Sequential Multi Stage Knowledge Distillation), a flexible framework that sequentially integrates heterogeneous KD methods. At each stage, the student is trained with a specific distillation method, while a frozen reference model from the previous stage anchors learned knowledge to mitigate forgetting. In addition, we introduce an adaptive weighting mechanism based on the teacher true class probability (TCP) that dynamically adjusts the reference loss per sample to balance knowledge retention and integration.
  By design, SMSKD supports arbitrary method combinations and stage counts with negligible computational overhead. Extensive experiments show that SMSKD consistently improves student accuracy across diverse teacher student architectures and method combinations, outperforming existing baselines. Ablation studies confirm that stage wise distillation and reference model supervision are primary contributors to performance gains, with TCP based adaptive weighting providing complementary benefits. Overall, SMSKD is a practical and resource efficient solution for integrating heterogeneous KD methods.

</details>


### [100] [Dualformer: Time-Frequency Dual Domain Learning for Long-term Time Series Forecasting](https://arxiv.org/abs/2601.15669)
*Jingjing Bai,Yoshinobu Kawahara*

Main category: cs.LG

TL;DR: Dualformer is a transformer-based framework for long-term time series forecasting that addresses the low-pass filtering issue by introducing dual-domain modeling with hierarchical frequency sampling and periodicity-aware weighting.


<details>
  <summary>Details</summary>
Motivation: Transformer models for long-term time series forecasting suffer from inherent low-pass filtering effects that progressively attenuate high-frequency information, limiting their ability to capture fine-grained temporal variations.

Method: Dualformer introduces: 1) dual-branch architecture for concurrent time and frequency domain modeling, 2) hierarchical frequency sampling that allocates distinct frequency bands to different layers, and 3) periodicity-aware weighting mechanism that dynamically balances dual branches based on harmonic energy ratio.

Result: Extensive experiments on eight benchmarks demonstrate Dualformer's robustness and superior performance, particularly on heterogeneous or weakly periodic data, effectively preserving high-frequency information and enhancing generalization.

Conclusion: Dualformer provides a principled dual-domain framework that enables structured frequency modeling and adaptive integration of time-frequency features, addressing transformer limitations in long-term time series forecasting.

Abstract: Transformer-based models, despite their promise for long-term time series forecasting (LTSF), suffer from an inherent low-pass filtering effect that limits their effectiveness. This issue arises due to undifferentiated propagation of frequency components across layers, causing a progressive attenuation of high-frequency information crucial for capturing fine-grained temporal variations. To address this limitation, we propose Dualformer, a principled dual-domain framework that rethinks frequency modeling from a layer-wise perspective. Dualformer introduces three key components: (1) a dual-branch architecture that concurrently models complementary temporal patterns in both time and frequency domains; (2) a hierarchical frequency sampling module that allocates distinct frequency bands to different layers, preserving high-frequency details in lower layers while modeling low-frequency trends in deeper layers; and (3) a periodicity-aware weighting mechanism that dynamically balances contributions from the dual branches based on the harmonic energy ratio of inputs, supported theoretically by a derived lower bound. This design enables structured frequency modeling and adaptive integration of time-frequency features, effectively preserving high-frequency information and enhancing generalization. Extensive experiments conducted on eight widely used benchmarks demonstrate Dualformer's robustness and superior performance, particularly on heterogeneous or weakly periodic data. Our code is publicly available at https://github.com/Akira-221/Dualformer.

</details>


### [101] [Beyond Hard Writes and Rigid Preservation: Soft Recursive Least-Squares for Lifelong LLM Editing](https://arxiv.org/abs/2601.15686)
*Xinyu Wang,Sicheng Lyu,Yu Gu,Jerry Huang,Peng Lu,Yufei Cui,Xiao-Wen Chang*

Main category: cs.LG

TL;DR: RLSEdit is a recursive least-squares editor for long sequential editing of LLMs that solves the plasticity-stability dilemma by formulating editing as online quadratic optimization with soft constraints, enabling stable scaling to 10K edits.


<details>
  <summary>Details</summary>
Motivation: Existing model editing methods face a plasticity-stability dilemma in real deployment where edits arrive as long streams: hard writes accumulate interference over time, while hard preservation only preserves explicitly constrained behaviors, allowing past edits to be overwritten and unconstrained behaviors to deviate, degrading general capabilities.

Method: RLSEdit formulates editing as an online quadratic optimization with soft constraints, minimizing a cumulative key-value fitting objective with two regularizers that control deviation from pre-trained weights and from a designated anchor mapping. The update uses an efficient online recursion via the Woodbury identity, with per-edit cost independent of history length.

Result: Experiments on multiple model families demonstrate stable scaling to 10K edits, outperforming strong baselines in both edit success and holistic stability - crucially retaining early edits and preserving general capabilities on GLUE and held-out reasoning/code benchmarks.

Conclusion: RLSEdit provides an effective solution for long sequential model editing that balances edit adherence with preservation of general capabilities, with theoretical guarantees on deviation bounds and asymptotic characterization of the trade-off in the many-edits regime.

Abstract: Model editing updates a pre-trained LLM with new facts or rules without re-training, while preserving unrelated behavior. In real deployment, edits arrive as long streams, and existing editors often face a plasticity-stability dilemma: locate-then-edit "hard writes" can accumulate interference over time, while null-space-style "hard preservation" preserves only what is explicitly constrained, so past edits can be overwritten and unconstrained behaviors may deviate, degrading general capabilities in the many-edits regime. We propose RLSEdit, a recursive least-squares editor for long sequential editing. RLSEdit formulates editing as an online quadratic optimization with soft constraints, minimizing a cumulative key-value fitting objective with two regularizers that control for both deviation from the pre-trained weights and from a designated anchor mapping. The resulting update admits an efficient online recursion via the Woodbury identity, with per-edit cost independent of history length and scaling only with the current edit size. We further provide deviation bounds and an asymptotic characterization of the adherence-preservation trade-off in the many-edits regime. Experiments on multiple model families demonstrate stable scaling to 10K edits, outperforming strong baselines in both edit success and holistic stability -- crucially retaining early edits, and preserving general capabilities on GLUE and held-out reasoning/code benchmarks.

</details>


### [102] [Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in Trustworthy LLMs](https://arxiv.org/abs/2601.15714)
*Ryoma Sato*

Main category: cs.LG

TL;DR: The paper proposes Zero-Error Horizon (ZEH) as a metric to measure the maximum error-free range of LLMs, revealing surprising failures in state-of-the-art models on simple tasks and providing insights into algorithmic capability emergence.


<details>
  <summary>Details</summary>
Motivation: To develop a trustworthy evaluation metric for LLMs that reveals their fundamental limitations, particularly for safety-critical applications where error-free performance is essential. The motivation stems from the need to understand when LLMs fail on seemingly simple problems despite their advanced capabilities.

Method: Proposes Zero-Error Horizon (ZEH) - a metric that measures the maximum range/size of problems a model can solve without any errors. The method involves systematically testing LLMs on increasingly complex instances of problems until the first error occurs. The paper also discusses computational optimization techniques using tree structures and online softmax to reduce evaluation costs.

Result: Surprising findings: GPT-5.2 fails on basic tasks like computing parity of short strings (e.g., 11000) and determining balanced parentheses. ZEH correlates with accuracy but reveals different behavioral patterns. Analysis of Qwen2.5 shows ZEH provides insights into the emergence of algorithmic capabilities. Computational optimizations achieve up to 10x speedup in ZEH evaluation.

Conclusion: ZEH is a valuable metric for assessing LLM trustworthiness, revealing critical limitations in state-of-the-art models. The surprising failures on simple tasks highlight the importance of rigorous testing before deploying LLMs in safety-critical domains. The computational optimizations make ZEH evaluation more practical for broader adoption.

Abstract: We propose Zero-Error Horizon (ZEH) for trustworthy LLMs, which represents the maximum range that a model can solve without any errors. While ZEH itself is simple, we demonstrate that evaluating the ZEH of state-of-the-art LLMs yields abundant insights. For example, by evaluating the ZEH of GPT-5.2, we found that GPT-5.2 cannot even compute the parity of a short string like 11000, and GPT-5.2 cannot determine whether the parentheses in ((((()))))) are balanced. This is surprising given the excellent capabilities of GPT-5.2. The fact that LLMs make mistakes on such simple problems serves as an important lesson when applying LLMs to safety-critical domains. By applying ZEH to Qwen2.5 and conducting detailed analysis, we found that while ZEH correlates with accuracy, the detailed behaviors differ, and ZEH provides clues about the emergence of algorithmic capabilities. Finally, while computing ZEH incurs significant computational cost, we discuss how to mitigate this cost by achieving up to one order of magnitude speedup using tree structures and online softmax.

</details>


### [103] [Communication-efficient Federated Graph Classification via Generative Diffusion Modeling](https://arxiv.org/abs/2601.15722)
*Xiuling Wang,Xin Huang,Haibo Hu,Jianliang Xu*

Main category: cs.LG

TL;DR: CeFGC is a novel federated GNN paradigm that reduces communication rounds to only three by using generative diffusion models to capture local graph distributions instead of exchanging parameters, addressing high communication overhead and non-IID data challenges.


<details>
  <summary>Details</summary>
Motivation: Federated GNNs face two major challenges: 1) high communication overhead from multiple rounds of parameter exchanges between server and clients, and 2) non-IID data characteristics across clients that degrade model performance.

Method: CeFGC limits communication to only three rounds: 1) Each client trains a generative diffusion model on local graph data and shares it with server, 2) Server redistributes all generative models to all clients, 3) Clients generate synthetic graphs using these models, combine with local graphs to train local GNNs, then upload weights to server for aggregation into global model.

Result: Theoretical analysis shows CeFGC reduces communication volume to constant three rounds only. Extensive experiments on real graph datasets demonstrate superior performance against state-of-the-art competitors, especially on non-IID graphs, by aligning local/global objectives and enriching training data with diverse synthetic graphs.

Conclusion: CeFGC provides an efficient federated GNN paradigm that effectively addresses communication overhead and non-IID data challenges through generative diffusion models, enabling practical deployment of federated GNNs in real-world scenarios with limited communication bandwidth.

Abstract: Graph Neural Networks (GNNs) unlock new ways of learning from graph-structured data, proving highly effective in capturing complex relationships and patterns. Federated GNNs (FGNNs) have emerged as a prominent distributed learning paradigm for training GNNs over decentralized data. However, FGNNs face two significant challenges: high communication overhead from multiple rounds of parameter exchanges and non-IID data characteristics across clients. To address these issues, we introduce CeFGC, a novel FGNN paradigm that facilitates efficient GNN training over non-IID data by limiting communication between the server and clients to three rounds only. The core idea of CeFGC is to leverage generative diffusion models to minimize direct client-server communication. Each client trains a generative diffusion model that captures its local graph distribution and shares this model with the server, which then redistributes it back to all clients. Using these generative models, clients generate synthetic graphs combined with their local graphs to train local GNN models. Finally, clients upload their model weights to the server for aggregation into a global GNN model. We theoretically analyze the I/O complexity of communication volume to show that CeFGC reduces to a constant of three communication rounds only. Extensive experiments on several real graph datasets demonstrate the effectiveness and efficiency of CeFGC against state-of-the-art competitors, reflecting our superior performance on non-IID graphs by aligning local and global model objectives and enriching the training set with diverse graphs.

</details>


### [104] [Towards Automated Kernel Generation in the Era of LLMs](https://arxiv.org/abs/2601.15727)
*Yang Yu,Peiyu Zang,Chi Hsu Tsai,Haiming Wu,Yixin Shen,Jialing Zhang,Haoyu Wang,Zhiyou Xiao,Jingze Shi,Yuyu Luo,Wentao Zhang,Chunlei Men,Guang Liu,Yonghua Lin*

Main category: cs.LG

TL;DR: Survey paper on LLM-driven kernel generation and optimization, providing structured overview of approaches, datasets, benchmarks, and future research directions.


<details>
  <summary>Details</summary>
Motivation: Kernel engineering is critical but time-consuming and non-scalable, requiring expert hardware knowledge. LLMs and agentic systems offer new possibilities for automating kernel generation and optimization, but the field lacks systematic organization.

Method: Survey methodology: structured overview of existing approaches including LLM-based methods and agentic optimization workflows, systematic compilation of datasets and benchmarks, and analysis of key challenges.

Result: Comprehensive reference framework for LLM-driven kernel generation, organized approaches, identified datasets/benchmarks, outlined open challenges, and maintained open-source repository for tracking the field.

Conclusion: LLM-driven kernel generation represents a promising direction for automating critical but complex kernel engineering, with significant progress made but requiring systematic organization and addressing key challenges for next-generation automated kernel optimization.

Abstract: The performance of modern AI systems is fundamentally constrained by the quality of their underlying kernels, which translate high-level algorithmic semantics into low-level hardware operations. Achieving near-optimal kernels requires expert-level understanding of hardware architectures and programming models, making kernel engineering a critical but notoriously time-consuming and non-scalable process. Recent advances in large language models (LLMs) and LLM-based agents have opened new possibilities for automating kernel generation and optimization. LLMs are well-suited to compress expert-level kernel knowledge that is difficult to formalize, while agentic systems further enable scalable optimization by casting kernel development as an iterative, feedback-driven loop. Rapid progress has been made in this area. However, the field remains fragmented, lacking a systematic perspective for LLM-driven kernel generation. This survey addresses this gap by providing a structured overview of existing approaches, spanning LLM-based approaches and agentic optimization workflows, and systematically compiling the datasets and benchmarks that underpin learning and evaluation in this domain. Moreover, key open challenges and future research directions are further outlined, aiming to establish a comprehensive reference for the next generation of automated kernel optimization. To keep track of this field, we maintain an open-source GitHub repository at https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation.

</details>


### [105] [Rethinking Drug-Drug Interaction Modeling as Generalizable Relation Learning](https://arxiv.org/abs/2601.15771)
*Dong Xu,Jiantao Wu,Qihua Pan,Sisi Yuan,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: GenRel-DDI reformulates DDI prediction as relation-centric learning to improve generalization to unseen drugs, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current DDI prediction methods fail to generalize to realistic scenarios where most candidate drug pairs involve previously unseen drugs and validated interactions are scarce. Molecule-centric approaches don't reliably correlate embedding proximity with interaction labels, and scaling model capacity doesn't solve the generalization problem.

Method: Proposes GenRel-DDI, a generalizable relation learning framework that reformulates DDI prediction as relation-centric learning. Instead of learning drug-specific representations, it learns interaction representations independently of drug identities, capturing transferable interaction patterns that generalize to unseen drugs and novel drug pairs.

Result: Extensive experiments across multiple benchmarks show GenRel-DDI consistently and significantly outperforms state-of-the-art methods, with particularly large gains on strict entity-disjoint evaluations where drugs are completely unseen during training.

Conclusion: Relation-centric learning provides an effective and practical approach for robust DDI prediction, addressing the generalization limitations of molecule-centric models and enabling better performance in realistic deployment scenarios with unseen drugs.

Abstract: Drug-drug interaction (DDI) prediction is central to drug discovery and clinical development, particularly in the context of increasingly prevalent polypharmacy. Although existing computational methods achieve strong performance on standard benchmarks, they often fail to generalize to realistic deployment scenarios, where most candidate drug pairs involve previously unseen drugs and validated interactions are scarce. We demonstrate that proximity in the embedding spaces of prevailing molecule-centric DDI models does not reliably correspond to interaction labels, and that simply scaling up model capacity therefore fails to improve generalization. To address these limitations, we propose GenRel-DDI, a generalizable relation learning framework that reformulates DDI prediction as a relation-centric learning problem, in which interaction representations are learned independently of drug identities. This relation-level abstraction enables the capture of transferable interaction patterns that generalize to unseen drugs and novel drug pairs. Extensive experiments across multiple benchmark demonstrate that GenRel-DDI consistently and significantly outperforms state-of-the-art methods, with particularly large gains on strict entity-disjoint evaluations, highlighting the effectiveness and practical utility of relation learning for robust DDI prediction. The code is available at https://github.com/SZU-ADDG/GenRel-DDI.

</details>


### [106] [Next Generation Active Learning: Mixture of LLMs in the Loop](https://arxiv.org/abs/2601.15773)
*Yuanyuan Qi,Xiaohao Yang,Jueqing Lu,Guoxiang Guo,Joanne Enticott,Gang Liu,Lan Du*

Main category: cs.LG

TL;DR: Proposes Mixture of LLMs in the Loop Active Learning framework that uses multiple LLMs as annotators instead of humans, with techniques to handle noisy labels and achieve performance comparable to human annotation.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used as annotators in active learning to reduce costs, but their annotation quality often falls short of real-world applicability. Need to enhance robustness of LLM-based annotations while maintaining cost efficiency.

Method: 1) Mixture-of-LLMs-based annotation model aggregates strengths of multiple LLMs. 2) Uses annotation discrepancy to identify unreliable annotations. 3) Employs negative learning to enhance learning effectiveness with noisy labels.

Result: Framework achieves performance comparable to human annotation, consistently outperforms single-LLM baselines and other LLM-ensemble approaches. Can operate fully on local machines using lightweight LLMs.

Conclusion: Proposed framework effectively addresses LLM annotation quality issues in active learning, providing robust, cost-effective alternative to human annotation that works well in real-world applications.

Abstract: With the rapid advancement and strong generalization capabilities of large language models (LLMs), they have been increasingly incorporated into the active learning pipelines as annotators to reduce annotation costs. However, considering the annotation quality, labels generated by LLMs often fall short of real-world applicability. To address this, we propose a novel active learning framework, Mixture of LLMs in the Loop Active Learning, replacing human annotators with labels generated through a Mixture-of-LLMs-based annotation model, aimed at enhancing LLM-based annotation robustness by aggregating the strengths of multiple LLMs. To further mitigate the impact of the noisy labels, we introduce annotation discrepancy and negative learning to identify the unreliable annotations and enhance learning effectiveness. Extensive experiments demonstrate that our framework achieves performance comparable to human annotation and consistently outperforms single-LLM baselines and other LLM-ensemble-based approaches. Moreover, our framework is built on lightweight LLMs, enabling it to operate fully on local machines in real-world applications.

</details>


### [107] [Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models](https://arxiv.org/abs/2601.15801)
*Fengheng Chu,Jiahao Chen,Yuhong Wang,Jun Wang,Zhihui Fu,Shouling Ji,Songze Li*

Main category: cs.LG

TL;DR: GOSV framework uses global optimization to identify safety-critical attention heads in LLMs, revealing separate malicious and safety suppression vectors, enabling a novel white-box jailbreak attack.


<details>
  <summary>Details</summary>
Motivation: Current LLM safety guardrails are fragile against jailbreak attacks, and existing attribution methods use local, greedy approaches that overlook cooperative interactions between attention heads in safety mechanisms.

Method: Proposed GOSV (Global Optimization for Safety Vector Extraction) identifies safety-critical attention heads through global optimization over all heads simultaneously, using two complementary activation repatching strategies: Harmful Patching and Zero Ablation.

Result: Identified two spatially distinct sets of safety vectors (Malicious Injection Vectors and Safety Suppression Vectors) with low overlap, showing aligned LLMs maintain separate functional pathways for safety. Found complete safety breakdown occurs at ~30% of total heads repatched. Developed a novel white-box jailbreak attack that outperforms existing methods.

Conclusion: GOSV provides an effective framework for LLM safety interpretability by revealing how safety mechanisms work through cooperative attention head interactions, enabling both better understanding and demonstration of vulnerabilities through improved jailbreak attacks.

Abstract: While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \textbf{G}lobal \textbf{O}ptimization for \textbf{S}afety \textbf{V}ector Extraction (GOSV), a framework that identifies safety-critical attention heads through global optimization over all heads simultaneously. We employ two complementary activation repatching strategies: Harmful Patching and Zero Ablation. These strategies identify two spatially distinct sets of safety vectors with consistently low overlap, termed Malicious Injection Vectors and Safety Suppression Vectors, demonstrating that aligned LLMs maintain separate functional pathways for safety purposes. Through systematic analyses, we find that complete safety breakdown occurs when approximately 30\% of total heads are repatched across all models. Building on these insights, we develop a novel inference-time white-box jailbreak method that exploits the identified safety vectors through activation repatching. Our attack substantially outperforms existing white-box attacks across all test models, providing strong evidence for the effectiveness of the proposed GOSV framework on LLM safety interpretability.

</details>


### [108] [Uncertainty-guided Generation of Dark-field Radiographs](https://arxiv.org/abs/2601.15859)
*Lina Felsner,Henriette Bast,Tina Dorosti,Florian Schaff,Franz Pfeiffer,Daniela Pfeiffer,Julia Schnabel*

Main category: cs.LG

TL;DR: First framework to generate X-ray dark-field images from standard chest X-rays using uncertainty-guided GAN, achieving high structural fidelity and good generalization.


<details>
  <summary>Details</summary>
Motivation: X-ray dark-field radiography provides complementary diagnostic information but has limited data availability, making it challenging to develop robust deep learning models for clinical applications.

Method: Uncertainty-Guided Progressive Generative Adversarial Network that incorporates both aleatoric and epistemic uncertainty to improve interpretability and reliability of generated dark-field images from standard attenuation chest X-rays.

Result: High structural fidelity of generated images with consistent improvement of quantitative metrics across stages. Out-of-distribution evaluation confirms good generalization of the proposed model.

Conclusion: Uncertainty-guided generative modeling enables realistic dark-field image synthesis and provides a reliable foundation for future clinical applications in X-ray diagnostics.

Abstract: X-ray dark-field radiography provides complementary diagnostic information to conventional attenuation imaging by visualizing microstructural tissue changes through small-angle scattering. However, the limited availability of such data poses challenges for developing robust deep learning models. In this work, we present the first framework for generating dark-field images directly from standard attenuation chest X-rays using an Uncertainty-Guided Progressive Generative Adversarial Network. The model incorporates both aleatoric and epistemic uncertainty to improve interpretability and reliability. Experiments demonstrate high structural fidelity of the generated images, with consistent improvement of quantitative metrics across stages. Furthermore, out-of-distribution evaluation confirms that the proposed model generalizes well. Our results indicate that uncertainty-guided generative modeling enables realistic dark-field image synthesis and provides a reliable foundation for future clinical applications.

</details>


### [109] [Why Inference in Large Models Becomes Decomposable After Training](https://arxiv.org/abs/2601.15871)
*Jidong Jin*

Main category: cs.LG

TL;DR: Post-training inference systems can be decomposed into independent substructures by removing statistically unsupported parameter dependencies, enabling parallel inference without changing model functionality.


<details>
  <summary>Details</summary>
Motivation: Current inference in large AI models uses dense parameter matrices, leading to unsustainable scaling of inference cost and system complexity. This isn't due to insufficient model capacity, but from treating inference systems as monolithic operators while ignoring internal structures formed during learning.

Method: Introduce a post-training statistical criterion and structural annealing procedure that removes unsupported parameter dependencies (those statistically indistinguishable from initialization distribution) to reveal stable, independent substructures.

Result: Shows that gradient update events in large models are highly localized and selective, leaving many parameter dependencies unchanged from initialization. This enables decomposition of inference systems into independent substructures.

Conclusion: Establishes a post-training, model-agnostic structural view of inference systems that enables structured, parallel inference without modifying model functionality or interfaces, addressing scalability issues in large AI models.

Abstract: Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.

</details>


### [110] [SoK: Challenges in Tabular Membership Inference Attacks](https://arxiv.org/abs/2601.15874)
*Cristina Pêra,Tânia Carvalho,Maxime Cordy,Luís Antunes*

Main category: cs.LG

TL;DR: MIAs show poor performance on tabular data but effectively expose single-out records; different surrogate models improve attack effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address unexplored concerns about MIAs on tabular data, analyze their efficacy across centralized/federated learning, and examine threats from outsider adversaries in federated settings.

Method: Extensive review and refined taxonomy of MIAs; evaluation of multiple attack strategies on tabular data with defenses; analysis of outsider adversaries in federated learning; investigation of single-out vulnerability and model architecture transferability.

Result: MIAs perform poorly on tabular data overall but successfully expose many single-out records; different surrogate models enhance attack effectiveness; outsider adversaries pose significant threats in federated learning.

Conclusion: Tabular data shows resilience to MIAs but single-outs remain highly vulnerable; model architecture diversity improves attack success; comprehensive evaluation reveals limitations of current MIA approaches for tabular data privacy assessment.

Abstract: Membership Inference Attacks (MIAs) are currently a dominant approach for evaluating privacy in machine learning applications. Despite their significance in identifying records belonging to the training dataset, several concerns remain unexplored, particularly with regard to tabular data. In this paper, first, we provide an extensive review and analysis of MIAs considering two main learning paradigms: centralized and federated learning. We extend and refine the taxonomy for both. Second, we demonstrate the efficacy of MIAs in tabular data using several attack strategies, also including defenses. Furthermore, in a federated learning scenario, we consider the threat posed by an outsider adversary, which is often neglected. Third, we demonstrate the high vulnerability of single-outs (records with a unique signature) to MIAs. Lastly, we explore how MIAs transfer across model architectures. Our results point towards a general poor performance of these attacks in tabular data which contrasts with previous state-of-the-art. Notably, even attacks with limited attack performance can still successfully expose a large portion of single-outs. Moreover, our findings suggest that using different surrogate models makes MIAs more effective.

</details>


### [111] [Iterative Amortized Hierarchical VAE](https://arxiv.org/abs/2601.15894)
*Simon W. Penninga,Ruud J. G. van Sloun*

Main category: cs.LG

TL;DR: IA-HVAE combines amortized inference with iterative refinement using decoder gradients, achieving 35x speed-up over traditional HVAE while improving reconstruction quality for inverse problems.


<details>
  <summary>Details</summary>
Motivation: Traditional hierarchical variational autoencoders (HVAEs) face computational bottlenecks in iterative inference. The paper aims to develop a more efficient hybrid approach that combines the speed of amortized inference with the accuracy of iterative refinement for real-time applications.

Method: Proposes Iterative Amortized Hierarchical VAE (IA-HVAE) with a hybrid scheme: initial amortized guess followed by iterative refinement using decoder gradients. Key innovation is creating a linearly separable decoder in transform domain (e.g., Fourier space) to enable real-time applications with high model depths.

Result: Achieves 35x speed-up for iterative inference compared to traditional HVAE. The hybrid approach outperforms fully amortized and fully iterative methods in accuracy and speed respectively. Shows improved reconstruction quality over vanilla HVAE for inverse problems like deblurring and denoising.

Conclusion: IA-HVAE successfully combines amortized and iterative inference, offering significant computational advantages while maintaining or improving reconstruction quality, making it suitable for real-time applications and inverse problem solving.

Abstract: In this paper we propose the Iterative Amortized Hierarchical Variational Autoencoder (IA-HVAE), which expands on amortized inference with a hybrid scheme containing an initial amortized guess and iterative refinement with decoder gradients. We achieve this by creating a linearly separable decoder in a transform domain (e.g. Fourier space), enabling real-time applications with very high model depths. The architectural change leads to a 35x speed-up for iterative inference with respect to the traditional HVAE. We show that our hybrid approach outperforms fully amortized and fully iterative equivalents in accuracy and speed respectively. Moreover, the IAHVAE shows improved reconstruction quality over a vanilla HVAE in inverse problems such as deblurring and denoising.

</details>


### [112] [Predicting Healthcare System Visitation Flow by Integrating Hospital Attributes and Population Socioeconomics with Human Mobility Data](https://arxiv.org/abs/2601.15977)
*Binbin Lin,Lei Zou,Hao Tian,Heng Cai,Yifan Yang,Bing Zhou*

Main category: cs.LG

TL;DR: Study integrates hospital attributes, population SES, and spatial factors to predict healthcare visitation patterns using mobility data and Google reviews, finding Deep Gravity model performs best and revealing distance-dependent influences of hospital characteristics.


<details>
  <summary>Details</summary>
Motivation: Existing research examines healthcare visitation determinants in isolation, lacking integrated analysis of hospital attributes, population socioeconomics, and spatial factors together.

Method: Used 4 years of SafeGraph mobility data and Google Maps Reviews to train five flow prediction models (Naive Regression, Gradient Boosting, MLPs, Deep Gravity, HGNN) for Houston, Texas, with SHAP and PDP analysis to examine factor impacts.

Result: Deep Gravity outperformed other models. Hospital capacities, ICU occupancy, ratings, and popularity significantly influence visitation patterns differently across travel distances. Short-distance visits driven by convenience, long-distance by hospital ratings. SES factors affect visitation frequency and rating sensitivity.

Conclusion: Integrated approach reveals complex, distance-dependent influences on healthcare visitation patterns, with hospital characteristics and population SES interacting to shape healthcare access decisions.

Abstract: Healthcare visitation patterns are influenced by a complex interplay of hospital attributes, population socioeconomics, and spatial factors. However, existing research often adopts a fragmented approach, examining these determinants in isolation. This study addresses this gap by integrating hospital capacities, occupancy rates, reputation, and popularity with population SES and spatial mobility patterns to predict visitation flows and analyze influencing factors. Utilizing four years of SafeGraph mobility data and user experience data from Google Maps Reviews, five flow prediction models, Naive Regression, Gradient Boosting, Multilayer Perceptrons (MLPs), Deep Gravity, and Heterogeneous Graph Neural Networks (HGNN),were trained and applied to simulate visitation flows in Houston, Texas, U.S. The Shapley additive explanation (SHAP) analysis and the Partial Dependence Plot (PDP) method were employed to examine the combined impacts of different factors on visitation patterns. The findings reveal that Deep Gravity outperformed other models. Hospital capacities, ICU occupancy rates, ratings, and popularity significantly influence visitation patterns, with their effects varying across different travel distances. Short-distance visits are primarily driven by convenience, whereas long-distance visits are influenced by hospital ratings. White-majority areas exhibited lower sensitivity to hospital ratings for short-distance visits, while Asian populations and those with higher education levels prioritized hospital rating in their visitation decisions. SES further influence these patterns, as areas with higher proportions of Hispanic, Black, under-18, and over-65 populations tend to have more frequent hospital visits, potentially reflecting greater healthcare needs or limited access to alternative medical services.

</details>


### [113] [Partially Lazy Gradient Descent for Smoothed Online Learning](https://arxiv.org/abs/2601.15984)
*Naram Mhaisen,George Iosifidis*

Main category: cs.LG

TL;DR: k-lazyGD bridges greedy OGD and lazy GD, achieving optimal dynamic regret with laziness up to Θ(√(T/P_T)) without sacrificing hitting performance.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between reactive (greedy OGD) and stable (lazy GD) updates in Smoothed Online Convex Optimization, creating a spectrum that allows laziness without compromising tracking ability.

Method: Introduces k-lazyGD algorithm based on Follow the Regularized Leader (FTRL) framework, with laziness parameter k creating a spectrum between OGD (k=1) and lazy GD (k=T). Uses an ensemble of learners with various slacks to adapt to comparator path length.

Result: Proves k-lazyGD achieves optimal dynamic regret O(√((P_T+1)T)) for any laziness slack k up to Θ(√(T/P_T)), where P_T is comparator path length. Derives matching lower bound and shows ensemble method is provably stable when possible and agile when necessary.

Conclusion: Laziness is possible without sacrificing hitting performance; allowable laziness connects to comparator's shifts, enabling retention of small movements from lazy methods while maintaining tracking ability.

Abstract: We introduce $k$-lazyGD, an online learning algorithm that bridges the gap between greedy Online Gradient Descent (OGD, for $k=1$) and lazy GD/dual-averaging (for $k=T$), creating a spectrum between reactive and stable updates. We analyze this spectrum in Smoothed Online Convex Optimization (SOCO), where the learner incurs both hitting and movement costs. Our main contribution is establishing that laziness is possible without sacrificing hitting performance: we prove that $k$-lazyGD achieves the optimal dynamic regret $\mathcal{O}(\sqrt{(P_T+1)T})$ for any laziness slack $k$ up to $Θ(\sqrt{T/P_T})$, where $P_T$ is the comparator path length. This result formally connects the allowable laziness to the comparator's shifts, showing that $k$-lazyGD can retain the inherently small movements of lazy methods without compromising tracking ability. We base our analysis on the Follow the Regularized Leader (FTRL) framework, and derive a matching lower bound. Since the slack depends on $P_T$, an ensemble of learners with various slacks is used, yielding a method that is provably stable when it can be, and agile when it must be.

</details>


### [114] [Data-Driven Conditional Flexibility Index](https://arxiv.org/abs/2601.16028)
*Moritz Wedemeyer,Eike Cramer,Alexander Mitsos,Manuel Dahmen*

Main category: cs.LG

TL;DR: The paper proposes a Conditional Flexibility Index (CFI) that extends traditional flexibility analysis by learning data-driven admissible uncertainty sets from historical data and making them conditional on contextual information using normalizing flows.


<details>
  <summary>Details</summary>
Motivation: Traditional flexibility index uses simple admissible uncertainty sets (like hypercubes) and doesn't incorporate available contextual information (forecasts) when determining flexibility. This leads to less informative flexibility estimates that don't consider which uncertainty regions are more likely under given conditions.

Method: Uses normalizing flow to learn bijective mapping from Gaussian base distribution to data distribution. Constructs admissible latent uncertainty set as hypersphere in latent space and maps it to data space. Incorporates contextual information to make admissible uncertainty sets conditional on specific conditions.

Result: CFI provides more informative flexibility estimates by focusing on relevant uncertainty regions. No general guarantee that data-driven sets outperform simple sets or conditional sets outperform unconditional ones, but both ensure only uncertainty regions containing actual realizations are considered. Applied to security-constrained unit commitment, CFI improves scheduling quality by incorporating temporal information.

Conclusion: The Conditional Flexibility Index successfully extends traditional flexibility analysis by incorporating data-driven learning and contextual conditioning, leading to more relevant and practical flexibility assessments for robust scheduling decisions.

Abstract: With the increasing flexibilization of processes, determining robust scheduling decisions has become an important goal. Traditionally, the flexibility index has been used to identify safe operating schedules by approximating the admissible uncertainty region using simple admissible uncertainty sets, such as hypercubes. Presently, available contextual information, such as forecasts, has not been considered to define the admissible uncertainty set when determining the flexibility index. We propose the conditional flexibility index (CFI), which extends the traditional flexibility index in two ways: by learning the parametrized admissible uncertainty set from historical data and by using contextual information to make the admissible uncertainty set conditional. This is achieved using a normalizing flow that learns a bijective mapping from a Gaussian base distribution to the data distribution. The admissible latent uncertainty set is constructed as a hypersphere in the latent space and mapped to the data space. By incorporating contextual information, the CFI provides a more informative estimate of flexibility by defining admissible uncertainty sets in regions that are more likely to be relevant under given conditions. Using an illustrative example, we show that no general statement can be made about data-driven admissible uncertainty sets outperforming simple sets, or conditional sets outperforming unconditional ones. However, both data-driven and conditional admissible uncertainty sets ensure that only regions of the uncertain parameter space containing realizations are considered. We apply the CFI to a security-constrained unit commitment example and demonstrate that the CFI can improve scheduling quality by incorporating temporal information.

</details>


### [115] [CLASP: An online learning algorithm for Convex Losses And Squared Penalties](https://arxiv.org/abs/2601.16072)
*Ricardo N. Ferreira,Cláudia Soares,João Xavier*

Main category: cs.LG

TL;DR: CLASP algorithm for Constrained Online Convex Optimization achieves optimal regret and penalty bounds: O(T^max{β,1-β}) regret and O(T^{1-β}) squared penalties for convex losses, and first-ever logarithmic guarantees (O(log T)) for both metrics in strongly convex cases.


<details>
  <summary>Details</summary>
Motivation: Prior work on Constrained Online Convex Optimization (COCO) lacked optimal algorithms that could simultaneously minimize both cumulative loss and constraint violations, especially for strongly convex problems where logarithmic guarantees were missing.

Method: Introduces CLASP (Convex Losses And Squared Penalties), an algorithm that minimizes cumulative loss together with squared constraint violations using a novel analysis that fully leverages the firm non-expansiveness of convex projectors.

Result: For convex losses: regret O(T^max{β,1-β}) and cumulative squared penalty O(T^{1-β}) for any β∈(0,1). For strongly convex problems: first logarithmic guarantees with both regret and cumulative squared penalty bounded by O(log T).

Conclusion: CLASP provides state-of-the-art performance for COCO, achieving optimal trade-offs between regret and constraint violations, with particularly groundbreaking logarithmic guarantees for strongly convex problems using novel proof techniques.

Abstract: We study Constrained Online Convex Optimization (COCO), where a learner chooses actions iteratively, observes both unanticipated convex loss and convex constraint, and accumulates loss while incurring penalties for constraint violations. We introduce CLASP (Convex Losses And Squared Penalties), an algorithm that minimizes cumulative loss together with squared constraint violations. Our analysis departs from prior work by fully leveraging the firm non-expansiveness of convex projectors, a proof strategy not previously applied in this setting. For convex losses, CLASP achieves regret $O\left(T^{\max\{β,1-β\}}\right)$ and cumulative squared penalty $O\left(T^{1-β}\right)$ for any $β\in (0,1)$. Most importantly, for strongly convex problems, CLASP provides the first logarithmic guarantees on both regret and cumulative squared penalty. In the strongly convex case, the regret is upper bounded by $O( \log T )$ and the cumulative squared penalty is also upper bounded by $O( \log T )$.

</details>


### [116] [Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical Systems](https://arxiv.org/abs/2601.16074)
*Annemarie Jutte,Uraz Odyurt*

Main category: cs.LG

TL;DR: Using XAI to analyze time-series decomposition components reveals insufficient contextual information in ML models for industrial CPS, leading to improved performance through increased window sizes.


<details>
  <summary>Details</summary>
Motivation: Industrial CPS are critical infrastructure requiring high reliability. ML models integrated into CPS are complex and non-transparent, needing rigorous evaluation to prevent unexpected behavior on unseen data. XAI can uncover model reasoning for better analysis.

Method: Apply XAI to improve ML predictive performance for industrial CPS. Analyze effects of time-series data decomposition components on model predictions using SHAP values. Use XAI findings to identify lack of contextual information and increase data window size.

Result: XAI analysis revealed insufficient contextual information during model training. By increasing the window size of data instances based on XAI findings, model performance was improved.

Conclusion: XAI can effectively identify limitations in ML models for industrial CPS, such as insufficient contextual information. Adjusting model inputs based on XAI insights (like increasing window size) can enhance predictive performance and reliability.

Abstract: Industrial Cyber-Physical Systems (CPS) are sensitive infrastructure from both safety and economics perspectives, making their reliability critically important. Machine Learning (ML), specifically deep learning, is increasingly integrated in industrial CPS, but the inherent complexity of ML models results in non-transparent operation. Rigorous evaluation is needed to prevent models from exhibiting unexpected behaviour on future, unseen data. Explainable AI (XAI) can be used to uncover model reasoning, allowing a more extensive analysis of behaviour. We apply XAI to to improve predictive performance of ML models intended for industrial CPS. We analyse the effects of components from time-series data decomposition on model predictions using SHAP values. Through this method, we observe evidence on the lack of sufficient contextual information during model training. By increasing the window size of data instances, informed by the XAI findings, we are able to improve model performance.

</details>


### [117] [Probably Approximately Correct Maximum A Posteriori Inference](https://arxiv.org/abs/2601.16083)
*Matthew Shorvon,Frederik Mallmann-Trenn,David S. Watson*

Main category: cs.LG

TL;DR: PAC algorithms for MAP inference with provable optimality guarantees under computational budgets, using information-theoretic measures and probabilistic circuits.


<details>
  <summary>Details</summary>
Motivation: MAP inference is fundamental but generally intractable, even with structural constraints and approximations. There's a need for methods that provide provable guarantees while working within computational budgets.

Method: Introduces PAC-MAP algorithms using information-theoretic measures estimated from finite samples. Implements solvers using probabilistic circuits with appropriate architectures. Develops randomization strategies that can be used standalone or to improve existing heuristics.

Result: Provides provably optimal solutions under variable and fixed computational budgets. Characterizes tractability conditions using information-theoretic measures. Experiments confirm benefits across a range of benchmarks.

Conclusion: PAC-MAP offers a principled approach to MAP inference with rigorous guarantees, bridging the gap between theoretical optimality and practical computational constraints through information-theoretic characterization and probabilistic circuit implementation.

Abstract: Computing the conditional mode of a distribution, better known as the $\mathit{maximum\ a\ posteriori}$ (MAP) assignment, is a fundamental task in probabilistic inference. However, MAP estimation is generally intractable, and remains hard even under many common structural constraints and approximation schemes. We introduce $\mathit{probably\ approximately\ correct}$ (PAC) algorithms for MAP inference that provide provably optimal solutions under variable and fixed computational budgets. We characterize tractability conditions for PAC-MAP using information theoretic measures that can be estimated from finite samples. Our PAC-MAP solvers are efficiently implemented using probabilistic circuits with appropriate architectures. The randomization strategies we develop can be used either as standalone MAP inference techniques or to improve on popular heuristics, fortifying their solutions with rigorous guarantees. Experiments confirm the benefits of our method in a range of benchmarks.

</details>


### [118] [Benchmarking Deep Learning Models for Raman Spectroscopy Across Open-Source Datasets](https://arxiv.org/abs/2601.16107)
*Adithya Sineesh,Akshita Kamsali*

Main category: cs.LG

TL;DR: Systematic benchmark of five Raman-specific deep learning classifiers across three open-source datasets with unified training protocols, providing fair comparisons of classification performance.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning evaluations for Raman spectroscopy often lack direct comparisons between Raman-specific models on shared datasets, making it difficult to assess true performance advantages over classical methods.

Method: Evaluated five representative Raman-specific deep learning architectures using unified training and hyperparameter tuning protocols across three open-source Raman datasets designed for standard evaluation, fine-tuning, and distribution-shift testing.

Result: Reported classification accuracies and macro-averaged F1 scores to provide reproducible performance comparisons of deep learning models for Raman spectral classification.

Conclusion: This study provides one of the first systematic benchmarks comparing multiple Raman-specific deep learning classifiers, establishing a foundation for fair evaluation and advancing the field of deep learning for Raman spectroscopy.

Abstract: Deep learning classifiers for Raman spectroscopy are increasingly reported to outperform classical chemometric approaches. However their evaluations are often conducted in isolation or compared against traditional machine learning methods or trivially adapted vision-based architectures that were not originally proposed for Raman spectroscopy. As a result, direct comparisons between existing deep learning models developed specifically for Raman spectral analysis on shared open-source datasets remain scarce. To the best of our knowledge, this study presents one of the first systematic benchmarks comparing three or more published Raman-specific deep learning classifiers across multiple open-source Raman datasets. We evaluate five representative deep learning architectures under a unified training and hyperparameter tuning protocol across three open-source Raman datasets selected to support standard evaluation, fine-tuning, and explicit distribution-shift testing. We report classification accuracies and macro-averaged F1 scores to provide a fair and reproducible comparison of deep learning models for Raman spectra based classification.

</details>


### [119] [Variable Splitting Binary Tree Models Based on Bayesian Context Tree Models for Time Series Segmentation](https://arxiv.org/abs/2601.16112)
*Yuta Nakahara,Shota Saito,Kohei Horinouchi,Koshi Shimada,Naoki Ichijo,Manabu Kobayashi,Toshiyasu Matsushima*

Main category: cs.LG

TL;DR: VSBT model combines Bayesian context trees with variable splitting for time series segmentation using recursive logistic regression for flexible interval partitioning.


<details>
  <summary>Details</summary>
Motivation: To develop a more flexible time series segmentation model that allows split positions at arbitrary locations within intervals, enabling more compact tree representations compared to previous BCT applications.

Method: Variable splitting binary tree (VSBT) model based on Bayesian context tree models with recursive logistic regression for interval partitioning, combining local variational approximation for logistic regression with context tree weighting (CTW) algorithm.

Result: The model enables simultaneous estimation of split positions and tree depth, with numerical examples on synthetic data demonstrating effectiveness.

Conclusion: VSBT provides a flexible framework for time series segmentation with arbitrary split positions and compact tree representations through recursive logistic regression and effective inference algorithms.

Abstract: We propose a variable splitting binary tree (VSBT) model based on Bayesian context tree (BCT) models for time series segmentation. Unlike previous applications of BCT models, the tree structure in our model represents interval partitioning on the time domain. Moreover, interval partitioning is represented by recursive logistic regression models. By adjusting logistic regression coefficients, our model can represent split positions at arbitrary locations within each interval. This enables more compact tree representations. For simultaneous estimation of both split positions and tree depth, we develop an effective inference algorithm that combines local variational approximation for logistic regression with the context tree weighting (CTW) algorithm. We present numerical examples on synthetic data demonstrating the effectiveness of our model and algorithm.

</details>


### [120] [On the Intrinsic Dimensions of Data in Kernel Learning](https://arxiv.org/abs/2601.16139)
*Rustem Takhanov*

Main category: cs.LG

TL;DR: The paper analyzes kernel ridge regression generalization using two intrinsic dimension measures: Minkowski dimension d_ρ and effective dimension d_K derived from Kolmogorov n-widths. It shows d_K can be smaller than d_ρ for fractal domains, leading to improved error bounds O(n^{-(2+d_K)/(2+2d_K)+ε}), and provides algorithms to estimate n-widths from finite samples.


<details>
  <summary>Details</summary>
Motivation: The manifold hypothesis suggests better generalization when data lies on low-dimensional manifolds. The paper aims to understand how different notions of intrinsic dimension affect kernel ridge regression generalization, particularly comparing Minkowski dimension vs. effective dimension from Kolmogorov n-widths, especially for irregular/fractal domains.

Method: 1) Define two intrinsic dimensions: d_ρ (Minkowski dimension via kernel metric) and d_K (effective dimension from Kolmogorov n-width decay). 2) Analyze relationship between n-widths and eigenvalues of kernel integral operator. 3) Show n-widths characterize worst-case eigenvalue decay across probability measures. 4) Derive generalization error bounds using d_K. 5) Propose algorithm to estimate n-width bounds from finite samples with sample complexity O(ε^{-d_ρ}log(1/ε)). 6) Compute d_K for fractal sets and conduct numerical experiments.

Result: 1) Kolmogorov n-widths characterize worst-case eigenvalue decay. 2) Excess error bound O(n^{-(2+d_K)/(2+2d_K)+ε}) for large n. 3) For Laplace kernel, d_K can be significantly smaller than d_ρ on fractal domains (though equal on regular domains). 4) Algorithm can estimate n-width bounds with O(ε^{-d_ρ}log(1/ε)) samples for near-uniform distributions. 5) Numerical experiments validate theoretical findings.

Conclusion: Effective dimension d_K from Kolmogorov n-widths provides tighter generalization bounds than Minkowski dimension d_ρ for kernel ridge regression, especially on fractal domains. The theoretical framework connects n-widths to eigenvalue decay, enabling improved error analysis and practical estimation algorithms for intrinsic dimensionality.

Abstract: The manifold hypothesis suggests that the generalization performance of machine learning methods improves significantly when the intrinsic dimension of the input distribution's support is low. In the context of KRR, we investigate two alternative notions of intrinsic dimension. The first, denoted $d_ρ$, is the upper Minkowski dimension defined with respect to the canonical metric induced by a kernel function $K$ on a domain $Ω$. The second, denoted $d_K$, is the effective dimension, derived from the decay rate of Kolmogorov $n$-widths associated with $K$ on $Ω$. Given a probability measure $μ$ on $Ω$, we analyze the relationship between these $n$-widths and eigenvalues of the integral operator $φ\to \int_ΩK(\cdot,x)φ(x)dμ(x)$. We show that, for a fixed domain $Ω$, the Kolmogorov $n$-widths characterize the worst-case eigenvalue decay across all probability measures $μ$ supported on $Ω$. These eigenvalues are central to understanding the generalization behavior of constrained KRR, enabling us to derive an excess error bound of order $O(n^{-\frac{2+d_K}{2+2d_K} + ε})$ for any $ε> 0$, when the training set size $n$ is large. We also propose an algorithm that estimates upper bounds on the $n$-widths using only a finite sample from $μ$. For distributions close to uniform, we prove that $ε$-accurate upper bounds on all $n$-widths can be computed with high probability using at most $O\left(ε^{-d_ρ}\log\frac{1}ε\right)$ samples, with fewer required for small $n$. Finally, we compute the effective dimension $d_K$ for various fractal sets and present additional numerical experiments. Our results show that, for kernels such as the Laplace kernel, the effective dimension $d_K$ can be significantly smaller than the Minkowski dimension $d_ρ$, even though $d_K = d_ρ$ provably holds on regular domains.

</details>


### [121] [Beat-ssl: Capturing Local ECG Morphology through Heartbeat-level Contrastive Learning with Soft Targets](https://arxiv.org/abs/2601.16147)
*Muhammad Ilham Rizqyawan,Peter Macfarlane,Stathis Hadjidemetriou,Fani Deligianni*

Main category: cs.LG

TL;DR: Beat-SSL is a contrastive learning framework for ECG analysis that uses dual-context learning (rhythm-level and heartbeat-level) with soft targets to address limited labelled data challenges.


<details>
  <summary>Details</summary>
Motivation: Obtaining labelled ECG data is difficult, and existing contrastive learning methods either focus only on global context or fail to exploit ECG-specific characteristics. Current methods also use hard contrastive targets that don't adequately capture the continuous nature of ECG feature similarity.

Method: Beat-SSL performs dual-context contrastive learning through both rhythm-level and heartbeat-level contrasting with soft targets, allowing the model to learn representations across different ECG contexts.

Result: The pretrained model achieved 93% of an ECG foundation model's performance in multilabel classification for global rhythm assessment, and surpassed all other methods (including the foundation model) by 4% in ECG segmentation tasks.

Conclusion: Beat-SSL effectively addresses ECG-specific learning challenges through dual-context contrastive learning with soft targets, achieving strong performance on both global rhythm assessment and detailed segmentation tasks despite limited labelled data.

Abstract: Obtaining labelled ECG data for developing supervised models is challenging. Contrastive learning (CL) has emerged as a promising pretraining approach that enables effective transfer learning with limited labelled data. However, existing CL frameworks either focus solely on global context or fail to exploit ECG-specific characteristics. Furthermore, these methods rely on hard contrastive targets, which may not adequately capture the continuous nature of feature similarity in ECG signals. In this paper, we propose Beat-SSL, a contrastive learning framework that performs dual-context learning through both rhythm-level and heartbeat-level contrasting with soft targets. We evaluated our pretrained model on two downstream tasks: 1) multilabel classification for global rhythm assessment, and 2) ECG segmentation to assess its capacity to learn representations across both contexts. We conducted an ablation study and compared the best configuration with three other methods, including one ECG foundation model. Despite the foundation model's broader pretraining, Beat-SSL reached 93% of its performance in multilabel classification task and surpassed all other methods in the segmentation task by 4%.

</details>


### [122] [Learning to Discover at Test Time](https://arxiv.org/abs/2601.16175)
*Mert Yuksekgonul,Daniel Koceja,Xinhao Li,Federico Bianchi,Jed McCaleb,Xiaolong Wang,Jan Kautz,Yejin Choi,James Zou,Carlos Guestrin,Yu Sun*

Main category: cs.LG

TL;DR: TTT-Discover uses test-time reinforcement learning to discover state-of-the-art solutions for scientific problems by continuously training LLMs on specific test problems rather than using frozen models.


<details>
  <summary>Details</summary>
Motivation: Prior test-time scaling methods use frozen LLMs, but the authors want to enable LLMs to continue training during testing with problem-specific experience to discover optimal solutions rather than average performance.

Method: Test-Time Training to Discover (TTT-Discover) performs reinforcement learning at test time with learning objectives and search subroutines designed to prioritize the most promising solutions for specific problems.

Result: TTT-Discover sets new state-of-the-art results across multiple domains: Erdős' minimum overlap problem, autocorrelation inequality, GPUMode kernel competition (2× faster), AtCoder algorithm competitions, and single-cell analysis denoising.

Conclusion: The method demonstrates that test-time training with open models can achieve superior results compared to closed frontier models, with reproducible results at low cost, advancing AI's ability to discover optimal solutions for scientific problems.

Abstract: How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.

</details>


### [123] [Provable Robustness in Multimodal Large Language Models via Feature Space Smoothing](https://arxiv.org/abs/2601.16200)
*Song Xia,Meiwen Ding,Chenqi Kong,Wenhan Yang,Xudong Jiang*

Main category: cs.LG

TL;DR: Feature-space Smoothing (FS) with Purifier and Smoothness Mapper (PSM) provides certified robustness guarantees for multimodal LLMs against adversarial attacks by maintaining feature cosine similarity bounds.


<details>
  <summary>Details</summary>
Motivation: Multimodal large language models (MLLMs) are powerful but vulnerable to adversarial perturbations that distort feature representations and cause erroneous predictions, creating a need for robust defense mechanisms.

Method: Proposes Feature-space Smoothing (FS) that transforms feature encoders into smoothed variants with certified robustness guarantees. Introduces Purifier and Smoothness Mapper (PSM) as a plug-and-play module to improve Gaussian robustness scores and enhance certified robustness without retraining MLLMs.

Result: FS-PSM reduces Attack Success Rate (ASR) of various white-box attacks from nearly 90% to about 1% across diverse MLLMs and downstream tasks, outperforming adversarial training while providing theoretical robustness guarantees.

Conclusion: The FS-PSM framework offers both strong theoretical certified robustness and superior empirical performance against adversarial attacks on MLLMs, providing an effective defense mechanism without requiring model retraining.

Abstract: Multimodal large language models (MLLMs) exhibit strong capabilities across diverse applications, yet remain vulnerable to adversarial perturbations that distort their feature representations and induce erroneous predictions. To address this vulnerability, we propose the Feature-space Smoothing (FS) and theoretically prove that FS offers certified robustness on the feature representations of MLLMs. Specifically, FS transforms any feature encoder into a smoothed variant that is guaranteed to maintain a certified lower bound on the feature cosine similarity between clean and adversarial representations under $\ell_2$-bounded attacks. Moreover, we indicate that the value of this Feature Cosine Similarity Bound (FCSB) derived from FS can be improved by enlarging the defined Gaussian robustness score on the vanilla encoder. Building upon this, we introduce the Purifier and Smoothness Mapper (PSM), a plug-and-play module that improves the Gaussian robustness score of MLLMs and thus enhances their certified robustness under FS, without requiring any retraining on MLLMs. We demonstrate that the FS with PSM not only provides a strong theoretical robustness guarantee but also exhibits superior empirical performance compared to adversarial training. Extensive experiments across diverse MLLMs and downstream tasks indicate the effectiveness of the FS-PSM, reducing the Attack Success Rate (ASR) of various white-box attacks from nearly 90\% to about 1\%.

</details>


### [124] [Counterfactual Training: Teaching Models Plausible and Actionable Explanations](https://arxiv.org/abs/2601.16205)
*Patrick Altmeyer,Aleksander Buszydlik,Arie van Deursen,Cynthia C. S. Liem*

Main category: cs.LG

TL;DR: Counterfactual training uses counterfactual explanations during model training to improve explanatory capacity and adversarial robustness.


<details>
  <summary>Details</summary>
Motivation: Current counterfactual explanations are post-hoc methods applied after training; the paper aims to make models inherently produce better counterfactuals by incorporating them directly into the training process.

Method: Proposes counterfactual training that minimizes divergence between learned representations and plausible, actionable counterfactual explanations during the training phase.

Result: Empirical and theoretical demonstration that the method produces models with inherently desirable counterfactual explanations and improved adversarial robustness.

Conclusion: Counterfactual training is an effective approach to enhance model explainability and robustness by directly optimizing for desirable counterfactual properties during training.

Abstract: We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable counterfactual explanations and additionally exhibit improved adversarial robustness.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [125] [Q-Probe: Scaling Image Quality Assessment to High Resolution via Context-Aware Agentic Probing](https://arxiv.org/abs/2601.15356)
*Xiang Li,XueHeng Li,Yu Wang,XuanHua He,ZhangChi Hu,WeiWei Yu,ChengJun Xie*

Main category: eess.IV

TL;DR: Q-Probe is a new agentic IQA framework that addresses limitations of existing RL-based models in high-resolution scenarios by using context-aware probing to capture subtle local degradations without introducing cropping biases.


<details>
  <summary>Details</summary>
Motivation: Existing RL-based IQA models fail to capture subtle local degradations in high-resolution scenarios because they rely on coarse-grained global views. While "Thinking with Images" paradigms enable multi-scale perception, they introduce spurious "cropping-implies-degradation" biases and misinterpret natural depth-of-field as artifacts when directly adapted to IQA.

Method: Q-Probe uses a context-aware probing framework with three key components: 1) Vista-Bench benchmark for fine-grained local degradation analysis in high-resolution IQA, 2) Three-stage training paradigm that progressively aligns models with human preferences, and 3) Novel context-aware cropping strategy to eliminate causal biases.

Result: Extensive experiments show Q-Probe achieves state-of-the-art performance in high-resolution settings while maintaining superior efficacy across resolution scales.

Conclusion: Q-Probe successfully addresses the limitations of existing RL-based IQA models by providing a scalable framework for high-resolution image quality assessment that captures local degradations without introducing biases, representing a significant advancement in multimodal LLM-based IQA.

Abstract: Reinforcement Learning (RL) has empowered Multimodal Large Language Models (MLLMs) to achieve superior human preference alignment in Image Quality Assessment (IQA). However, existing RL-based IQA models typically rely on coarse-grained global views, failing to capture subtle local degradations in high-resolution scenarios. While emerging "Thinking with Images" paradigms enable multi-scale visual perception via zoom-in mechanisms, their direct adaptation to IQA induces spurious "cropping-implies-degradation" biases and misinterprets natural depth-of-field as artifacts. To address these challenges, we propose Q-Probe, the first agentic IQA framework designed to scale IQA to high resolution via context-aware probing. First, we construct Vista-Bench, a pioneering benchmark tailored for fine-grained local degradation analysis in high-resolution IQA settings. Furthermore, we propose a three-stage training paradigm that progressively aligns the model with human preferences, while simultaneously eliminating causal bias through a novel context-aware cropping strategy. Extensive experiments demonstrate that Q-Probe achieves state-of-the-art performance in high-resolution settings while maintaining superior efficacy across resolution scales.

</details>


### [126] [High-Fidelity 3D Tooth Reconstruction by Fusing Intraoral Scans and CBCT Data via a Deep Implicit Representation](https://arxiv.org/abs/2601.15358)
*Yi Zhu,Razmig Kechichian,Raphaël Richert,Satoshi Ikehata,Sébastien Valette*

Main category: eess.IV

TL;DR: A deep learning pipeline fuses CBCT (root) and IOS (crown) dental scans using DeepSDF to create seamless 3D tooth models without artifacts.


<details>
  <summary>Details</summary>
Motivation: Clinical dental imaging has limitations: CBCT captures roots but has noisy crowns, while IOS provides high-fidelity crowns but no root information. Naive fusion creates unnatural seams and artifacts.

Method: 1) Segment and register tooth instances from CBCT and IOS data; 2) Create hybrid proxy mesh combining IOS crown and CBCT root; 3) Use this proxy to guide a class-specific DeepSDF network that projects onto learned manifold of ideal tooth shapes.

Result: Qualitative and quantitative evaluations show the method uniquely preserves both high-fidelity crown from IOS and patient-specific root morphology from CBCT, overcoming limitations of each modality and naive stitching.

Conclusion: The proposed fully-automated pipeline generates seamless, watertight, and anatomically coherent 3D tooth models by fusing CBCT and IOS data using deep implicit representations, addressing a critical need in digital dentistry.

Abstract: High-fidelity 3D tooth models are essential for digital dentistry, but must capture both the detailed crown and the complete root. Clinical imaging modalities are limited: Cone-Beam Computed Tomography (CBCT) captures the root but has a noisy, low-resolution crown, while Intraoral Scanners (IOS) provide a high-fidelity crown but no root information. A naive fusion of these sources results in unnatural seams and artifacts. We propose a novel, fully-automated pipeline that fuses CBCT and IOS data using a deep implicit representation. Our method first segments and robustly registers the tooth instances, then creates a hybrid proxy mesh combining the IOS crown and the CBCT root. The core of our approach is to use this noisy proxy to guide a class-specific DeepSDF network. This optimization process projects the input onto a learned manifold of ideal tooth shapes, generating a seamless, watertight, and anatomically coherent model. Qualitative and quantitative evaluations show our method uniquely preserves both the high-fidelity crown from IOS and the patient-specific root morphology from CBCT, overcoming the limitations of each modality and naive stitching.

</details>


### [127] [Aligned Stable Inpainting: Mitigating Unwanted Object Insertion and Preserving Color Consistency](https://arxiv.org/abs/2601.15368)
*Yikai Wang,Junqiu Yu,Chenjie Cao,Xiangyang Xue,Yanwei Fu*

Main category: eess.IV

TL;DR: ASUKA framework addresses two key issues in generative image inpainting: unwanted object hallucination and color inconsistency, using reconstruction-based priors and specialized VAE decoder for local harmonization.


<details>
  <summary>Details</summary>
Motivation: Existing generative inpainting methods produce unnatural results due to two main problems: (1) hallucination of arbitrary objects that don't match context, and (2) color inconsistency leading to smeared textures and degraded quality.

Method: Proposes ASUKA framework with two components: reconstruction-based priors to suppress unwanted object insertion while preserving generative flexibility, and a specialized VAE decoder that treats latent-to-image decoding as local harmonization task to reduce color shifts.

Result: ASUKA effectively suppresses object hallucination and improves color consistency, outperforming standard diffusion models, rectified flow models, and other inpainting methods on Places2 dataset and proposed MISATO benchmark.

Conclusion: ASUKA provides efficient post-hoc solutions for pre-trained inpainting models that address key unnatural artifacts while maintaining generative capacity, with lightweight injection strategies for U-Net and DiT-based architectures.

Abstract: Generative image inpainting can produce realistic, high-fidelity results even with large, irregular masks. However, existing methods still face key issues that make inpainted images look unnatural. In this paper, we identify two main problems: (1) Unwanted object insertion: generative models may hallucinate arbitrary objects in the masked region that do not match the surrounding context. (2) Color inconsistency: inpainted regions often exhibit noticeable color shifts, leading to smeared textures and degraded image quality. We analyze the underlying causes of these issues and propose efficient post-hoc solutions for pre-trained inpainting models. Specifically, we introduce the principled framework of Aligned Stable inpainting with UnKnown Areas prior (ASUKA). To reduce unwanted object insertion, we use reconstruction-based priors to guide the generative model, suppressing hallucinated objects while preserving generative flexibility. To address color inconsistency, we design a specialized VAE decoder that formulates latent-to-image decoding as a local harmonization task. This design significantly reduces color shifts and produces more color-consistent results. We implement ASUKA on two representative inpainting architectures: a U-Net-based model and a DiT-based model. We analyze and propose lightweight injection strategies that minimize interference with the model's original generation capacity while ensuring the mitigation of the two issues. We evaluate ASUKA using the Places2 dataset and MISATO, our proposed diverse benchmark. Experiments show that ASUKA effectively suppresses object hallucination and improves color consistency, outperforming standard diffusion, rectified flow models, and other inpainting methods. Dataset, models and codes will be released in github.

</details>


### [128] [OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation](https://arxiv.org/abs/2601.15369)
*Letian Zhang,Sucheng Ren,Yanqing Liu,Xianhang Li,Zeyu Wang,Yuyin Zhou,Huaxiu Yao,Zeyu Zheng,Weili Nie,Guilin Liu,Zhiding Yu,Cihang Xie*

Main category: eess.IV

TL;DR: OpenVision 3 is a unified vision encoder that learns a single representation for both image understanding and generation by combining reconstruction and semantic objectives in a shared latent space.


<details>
  <summary>Details</summary>
Motivation: Current vision models typically have separate architectures for understanding (like CLIP) and generation (like VAE encoders). The authors aim to create a single unified visual representation that can serve both purposes effectively.

Method: Uses a ViT encoder that processes VAE-compressed image latents. The encoder output serves two roles: 1) passed to ViT-VAE decoder for image reconstruction (generative structure), and 2) optimized with contrastive learning and image-captioning objectives (semantic features). Joint optimization of reconstruction and semantic signals in shared latent space.

Result: For multimodal understanding (plugged into LLaVA-1.5): performs comparably with standard CLIP (62.4 vs 62.2 on SeedBench, 83.7 vs 82.9 on POPE). For generation (under RAE framework): substantially surpasses CLIP-based encoder (gFID: 1.89 vs 2.54 on ImageNet).

Conclusion: The unified design successfully creates a single visual representation that works well for both understanding and generation tasks, demonstrating the feasibility and benefits of unified modeling approaches in vision.

Abstract: This paper presents a family of advanced vision encoder, named OpenVision 3, that learns a single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to a ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstruction- and semantics-driven signals in a shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validate this unified design through extensive downstream evaluations with the encoder frozen. For multimodal understanding, we plug the encoder into the LLaVA-1.5 framework: it performs comparably with a standard CLIP vision encoder (e.g., 62.4 vs 62.2 on SeedBench, and 83.7 vs 82.9 on POPE). For generation, we test it under the RAE framework: ours substantially surpasses the standard CLIP-based encoder (e.g., gFID: 1.89 vs 2.54 on ImageNet). We hope this work can spur future research on unified modeling.

</details>


### [129] [A Machine Vision Approach to Preliminary Skin Lesion Assessments](https://arxiv.org/abs/2601.15539)
*Ali Khreis,Ro'Yah Radaideh,Quinn McGill*

Main category: eess.IV

TL;DR: A system combining ABCD dermoscopy rules with ML for skin lesion assessment shows custom CNNs outperform both rule-based approaches and transfer learning on small medical datasets.


<details>
  <summary>Details</summary>
Motivation: Early detection of malignant skin lesions is critical for improving patient outcomes in aggressive, metastatic skin cancers. There's a need for automated assessment systems that can combine clinical interpretability with high accuracy.

Method: The study evaluates a comprehensive system combining ABCD rule-based TDS scoring with machine learning. Uses 1,000-image subset of HAM10000 dataset. Compares: 1) Automated rule-based pipeline computing TDS, 2) Traditional classifiers (Logistic Regression, Random Forest, SVM), 3) Transfer learning with EfficientNet-B0, 4) Custom three-layer CNN trained from scratch on median-filtered images.

Result: Rule-based system provides high interpretability but has performance bottleneck. Transfer learning with EfficientNet-B0 failed significantly due to domain shift. Custom CNN achieved 78.5% accuracy and 86.5% recall, representing 19-point accuracy improvement over traditional methods. Direct pixel-level learning captures diagnostic patterns beyond handcrafted features.

Conclusion: Purpose-built lightweight architectures can outperform large pretrained models for small, domain-specific medical datasets. Custom CNNs trained from scratch are more effective than transfer learning when dealing with domain shift between natural and medical images.

Abstract: Early detection of malignant skin lesions is critical for improving patient outcomes in aggressive, metastatic skin cancers. This study evaluates a comprehensive system for preliminary skin lesion assessment that combines the clinically established ABCD rule of dermoscopy (analyzing Asymmetry, Borders, Color, and Dermoscopic Structures) with machine learning classification. Using a 1,000-image subset of the HAM10000 dataset, the system implements an automated, rule-based pipeline to compute a Total Dermoscopy Score (TDS) for each lesion. This handcrafted approach is compared against various machine learning solutions, including traditional classifiers (Logistic Regression, Random Forest, and SVM) and deep learning models. While the rule-based system provides high clinical interpretability, results indicate a performance bottleneck when reducing complex morphology to five numerical features. Experimental findings show that transfer learning with EfficientNet-B0 failed significantly due to domain shift between natural and medical images. In contrast, a custom three-layer Convolutional Neural Network (CNN) trained from scratch achieved 78.5% accuracy and 86.5% recall on median-filtered images, representing a 19-point accuracy improvement over traditional methods. The results demonstrate that direct pixel-level learning captures diagnostic patterns beyond handcrafted features and that purpose-built lightweight architectures can outperform large pretrained models for small, domain-specific medical datasets.

</details>


### [130] [FUGC: Benchmarking Semi-Supervised Learning Methods for Cervical Segmentation](https://arxiv.org/abs/2601.15572)
*Jieyun Bai,Yitong Tang,Zihao Zhou,Mahdi Islam,Musarrat Tabassum,Enrique Almar-Munoz,Hongyu Liu,Hui Meng,Nianjiang Lv,Bo Deng,Yu Chen,Zilun Peng,Yusong Xiao,Li Xiao,Nam-Khanh Tran,Dac-Phu Phan-Le,Hai-Dang Nguyen,Xiao Liu,Jiale Hu,Mingxu Huang,Jitao Liang,Chaolu Feng,Xuezhi Zhang,Lyuyang Tong,Bo Du,Ha-Hieu Pham,Thanh-Huy Nguyen,Min Xu,Juntao Jiang,Jiangning Zhang,Yong Liu,Md. Kamrul Hasan,Jie Gan,Zhuonan Liang,Weidong Cai,Yuxin Huang,Gongning Luo,Mohammad Yaqub,Karim Lekadir*

Main category: eess.IV

TL;DR: FUGC is the first benchmark for semi-supervised cervical segmentation in TVS images, addressing preterm birth risk assessment with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Accurate cervical segmentation in transvaginal ultrasound is crucial for preterm birth risk assessment, but supervised learning approaches are limited by scarce labeled data, necessitating semi-supervised methods.

Method: Created the FUGC benchmark with 890 TVS images (500 training, 90 validation, 300 test) and evaluated methods using weighted combination of Dice Similarity Coefficient (0.4), Hausdorff Distance (0.4), and runtime (0.2).

Result: 10 teams with 82 participants submitted solutions; best methods achieved 90.26% mDSC, 38.88 mHD, and 32.85 ms RT, demonstrating efficacy of semi-supervised approaches with limited labeled data.

Conclusion: FUGC establishes a standardized benchmark for cervical segmentation, validates semi-supervised methods for limited labeled data scenarios, and provides foundation for AI-assisted clinical preterm birth risk assessment.

Abstract: Accurate segmentation of cervical structures in transvaginal ultrasound (TVS) is critical for assessing the risk of spontaneous preterm birth (PTB), yet the scarcity of labeled data limits the performance of supervised learning approaches. This paper introduces the Fetal Ultrasound Grand Challenge (FUGC), the first benchmark for semi-supervised learning in cervical segmentation, hosted at ISBI 2025. FUGC provides a dataset of 890 TVS images, including 500 training images, 90 validation images, and 300 test images. Methods were evaluated using the Dice Similarity Coefficient (DSC), Hausdorff Distance (HD), and runtime (RT), with a weighted combination of 0.4/0.4/0.2. The challenge attracted 10 teams with 82 participants submitting innovative solutions. The best-performing methods for each individual metric achieved 90.26\% mDSC, 38.88 mHD, and 32.85 ms RT, respectively. FUGC establishes a standardized benchmark for cervical segmentation, demonstrates the efficacy of semi-supervised methods with limited labeled data, and provides a foundation for AI-assisted clinical PTB risk assessment.

</details>


### [131] [THOR: A Versatile Foundation Model for Earth Observation Climate and Society Applications](https://arxiv.org/abs/2601.16011)
*Theodor Forgaard,Jarle H. Reksten,Anders U. Waldeland,Valerio Marsocci,Nicolas Longépé,Michael Kampffmeyer,Arnt-Børre Salberg*

Main category: eess.IV

TL;DR: THOR is a compute-adaptive Earth observation foundation model that unifies data from multiple Sentinel satellites and allows flexible patch sizes at inference without retraining.


<details>
  <summary>Details</summary>
Motivation: Current Earth observation foundation models are architecturally rigid, struggle with heterogeneous sensors, and are constrained to fixed patch sizes, limiting their deployment in real-world scenarios requiring flexible compute-accuracy trade-offs.

Method: THOR unifies data from Copernicus Sentinel-1, -2, and -3 satellites, processes their native 10m to 1000m resolutions in a single model, and uses a novel randomized patch and input image size strategy during pre-training.

Result: THOR achieves state-of-the-art performance on downstream benchmarks, particularly in data-limited regimes like the PANGAEA 10% split, validating its flexible feature generation for diverse climate and society applications.

Conclusion: THOR solves both input heterogeneity and deployment rigidity in Earth observation models, enabling dynamic trade-offs between computational cost and feature resolution without retraining.

Abstract: Current Earth observation foundation models are architecturally rigid, struggle with heterogeneous sensors and are constrained to fixed patch sizes. This limits their deployment in real-world scenarios requiring flexible computeaccuracy trade-offs. We propose THOR, a "computeadaptive" foundation model that solves both input heterogeneity and deployment rigidity. THOR is the first architecture to unify data from Copernicus Sentinel-1, -2, and -3 (OLCI & SLSTR) satellites, processing their native 10 m to 1000 m resolutions in a single model. We pre-train THOR with a novel randomized patch and input image size strategy. This allows a single set of pre-trained weights to be deployed at inference with any patch size, enabling a dynamic trade-off between computational cost and feature resolution without retraining. We pre-train THOR on THOR Pretrain, a new, large-scale multi-sensor dataset and demonstrate state-of-the-art performance on downstream benchmarks, particularly in data-limited regimes like the PANGAEA 10% split, validating that THOR's flexible feature generation excels for diverse climate and society applications.

</details>


### [132] [Phi-SegNet: Phase-Integrated Supervision for Medical Image Segmentation](https://arxiv.org/abs/2601.16064)
*Shams Nafisa Ali,Taufiq Hasan*

Main category: eess.IV

TL;DR: Phi-SegNet introduces a CNN-based medical image segmentation architecture that incorporates phase-aware frequency-domain information at both architectural and optimization levels, achieving state-of-the-art performance across diverse imaging modalities with improved generalization.


<details>
  <summary>Details</summary>
Motivation: Current deep learning segmentation models primarily encode spatial information while overlooking frequency-domain representations that capture rich structural and textural cues. Although some recent studies explore spectral information at the feature level, supervision-level integration of frequency cues for fine-grained object localization remains largely untapped.

Method: Proposes Phi-SegNet with Bi-Feature Mask Former (BFMF) modules that blend neighboring encoder features to reduce semantic gaps, and Reverse Fourier Attention (RFA) blocks that refine decoder outputs using phase-regularized features. Includes a dedicated phase-aware loss that aligns features with structural priors, forming a closed feedback loop emphasizing boundary precision.

Result: Evaluated on five public datasets spanning X-ray, US, histopathology, MRI, and colonoscopy, Phi-SegNet consistently achieved state-of-the-art performance with average relative improvement of 1.54±1.26% in IoU and 0.98±0.71% in F1-score over next best model. Also exhibits robust cross-dataset generalization in unseen datasets from known domains.

Conclusion: The work demonstrates the potential of leveraging spectral priors in both feature representation and supervision, paving the way for generalized segmentation frameworks that excel in fine-grained object localization across diverse medical imaging modalities.

Abstract: Deep learning has substantially advanced medical image segmentation, yet achieving robust generalization across diverse imaging modalities and anatomical structures remains a major challenge. A key contributor to this limitation lies in how existing architectures, ranging from CNNs to Transformers and their hybrids, primarily encode spatial information while overlooking frequency-domain representations that capture rich structural and textural cues. Although few recent studies have begun exploring spectral information at the feature level, supervision-level integration of frequency cues-crucial for fine-grained object localization-remains largely untapped. To this end, we propose Phi-SegNet, a CNN-based architecture that incorporates phase-aware information at both architectural and optimization levels. The network integrates Bi-Feature Mask Former (BFMF) modules that blend neighboring encoder features to reduce semantic gaps, and Reverse Fourier Attention (RFA) blocks that refine decoder outputs using phase-regularized features. A dedicated phase-aware loss aligns these features with structural priors, forming a closed feedback loop that emphasizes boundary precision. Evaluated on five public datasets spanning X-ray, US, histopathology, MRI, and colonoscopy, Phi-SegNet consistently achieved state-of-the-art performance, with an average relative improvement of 1.54+/-1.26% in IoU and 0.98+/-0.71% in F1-score over the next best-performing model. In cross-dataset generalization scenarios involving unseen datasets from the known domain, Phi-SegNet also exhibits robust and superior performance, highlighting its adaptability and modality-agnostic design. These findings demonstrate the potential of leveraging spectral priors in both feature representation and supervision, paving the way for generalized segmentation frameworks that excel in fine-grained object localization.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [133] [Gated Sparse Attention: Combining Computational Efficiency with Training Stability for Long-Context Language Models](https://arxiv.org/abs/2601.15305)
*Alfred Shen,Aaron Shen*

Main category: cs.AI

TL;DR: Gated Sparse Attention (GSA) combines sparse attention efficiency with gated attention stability, achieving 12-16x speedup at 128K context while improving perplexity from 6.03 to 5.70 and reducing attention sink phenomena.


<details>
  <summary>Details</summary>
Motivation: Address the computational burden of attention in long-context language models by combining the complementary strengths of sparse attention (efficiency) and gated attention (training stability, mitigating attention sinks).

Method: Gated Sparse Attention (GSA) with three key components: 1) gated lightning indexer with sigmoid activations for bounded, interpretable selection scores, 2) adaptive sparsity controller that modulates attended tokens based on local uncertainty, and 3) dual gating at value and output stages.

Result: In 1.7B parameter models trained on 400B tokens: 12-16x speedup at 128K context, perplexity improves from 6.03 to 5.70, RULER scores nearly double at 128K context, attention to first token (attention sink proxy) drops from 47% to under 4%, and loss spikes reduced by 98%.

Conclusion: GSA successfully combines sparse and gated attention approaches, achieving both computational efficiency and quality improvements while significantly enhancing training stability and mitigating attention sink phenomena in long-context language models.

Abstract: The computational burden of attention in long-context language models has motivated two largely independent lines of work: sparse attention mechanisms that reduce complexity by attending to selected tokens, and gated attention variants that improve training sta-bility while mitigating the attention sink phenomenon. We observe that these approaches address complementary weaknesses and propose Gated Sparse Attention (GSA), an architecture that realizes the benefits of both. GSA incorporates a gated lightning indexer with sigmoid activations that produce bounded, interpretable selection scores, an adaptive sparsity controller that modulates the number of attended tokens based on local uncertainty, and dual gating at the value and output stages. We establish theoretical foundations for the approach, including complexity analysis, expressiveness results, and convergence guarantees. In experiments with 1.7B parameter models trained on 400B tokens, GSA matches the efficiency of sparse-only baselines (12-16x speedup at 128K context) while achieving the quality gains associated with gated attention: perplexity improves from 6.03 to 5.70, RULER scores at 128K context nearly double, and attention to the first token, a proxy for attention sinks, drops from 47% to under 4%. Training stability improves markedly, with loss spikes reduced by 98%.

</details>


### [134] [Uncovering Latent Bias in LLM-Based Emergency Department Triage Through Proxy Variables](https://arxiv.org/abs/2601.15306)
*Ethan Zhang*

Main category: cs.AI

TL;DR: LLMs in emergency department triage show hidden biases through proxy variables, with systematic severity modifications based on input tokens regardless of framing, revealing imperfect training on noisy signals.


<details>
  <summary>Details</summary>
Motivation: Despite advances in LLM integration into clinical decision-making, hidden biases against patients across racial, social, economic, and clinical backgrounds persist, requiring investigation of bias in LLM-based medical AI systems for emergency department triage.

Method: Used 32 patient-level proxy variables with paired positive/negative qualifiers, evaluated on both public (MIMIC-IV-ED Demo, MIMIC-IV Demo) and restricted-access credentialed (MIMIC-IV-ED and MIMIC-IV) datasets.

Result: Revealed discriminatory behavior mediated through proxy variables in ED triage scenarios, and systematic tendency for LLMs to modify perceived patient severity when specific tokens appear in input context, regardless of positive/negative framing.

Conclusion: AI systems are imperfectly trained on noisy, sometimes non-causal signals that don't reliably reflect true patient acuity, requiring more work to ensure safe and responsible deployment of AI technologies in clinical settings.

Abstract: Recent advances in large language models (LLMs) have enabled their integration into clinical decision-making; however, hidden biases against patients across racial, social, economic, and clinical backgrounds persist. In this study, we investigate bias in LLM-based medical AI systems applied to emergency department (ED) triage. We employ 32 patient-level proxy variables, each represented by paired positive and negative qualifiers, and evaluate their effects using both public (MIMIC-IV-ED Demo, MIMIC-IV Demo) and restricted-access credentialed (MIMIC-IV-ED and MIMIC-IV) datasets as appropriate~\cite{mimiciv_ed_demo,mimiciv_ed,mimiciv}. Our results reveal discriminatory behavior mediated through proxy variables in ED triage scenarios, as well as a systematic tendency for LLMs to modify perceived patient severity when specific tokens appear in the input context, regardless of whether they are framed positively or negatively. These findings indicate that AI systems is still imperfectly trained on noisy, sometimes non-causal signals that do not reliably reflect true patient acuity. Consequently, more needs to be done to ensure the safe and responsible deployment of AI technologies in clinical settings.

</details>


### [135] [DeepSurvey-Bench: Evaluating Academic Value of Automatically Generated Scientific Survey](https://arxiv.org/abs/2601.15307)
*Guo-Biao Zhang,Ding-Yuan Liu,Da-Yi Wu,Tian Lan,Heyan Huang,Zhijing Wu,Xian-Ling Mao*

Main category: cs.AI

TL;DR: DeepSurvey-Bench is a new benchmark for evaluating the academic value of AI-generated scientific surveys, addressing limitations of existing benchmarks that focus only on surface-level metrics.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for evaluating generated scientific surveys have two key problems: (1) they use unreliable ground truth datasets without academic dimension annotations, and (2) they only assess surface-level qualities like structural coherence rather than deep academic value. This prevents proper evaluation of core research objectives and critical analysis in surveys.

Method: Proposes DeepSurvey-Bench with comprehensive academic value evaluation criteria covering three dimensions: informational value, scholarly communication value, and research guidance value. Constructs a reliable dataset with academic value annotations and uses this to evaluate the deep academic value of generated surveys.

Result: Extensive experimental results show that DeepSurvey-Bench is highly consistent with human performance in assessing the academic value of generated surveys.

Conclusion: DeepSurvey-Bench provides a more comprehensive and reliable benchmark for evaluating the true academic value of AI-generated scientific surveys, addressing the limitations of existing evaluation approaches.

Abstract: The rapid development of automated scientific survey generation technology has made it increasingly important to establish a comprehensive benchmark to evaluate the quality of generated surveys.Nearly all existing evaluation benchmarks rely on flawed selection criteria such as citation counts and structural coherence to select human-written surveys as the ground truth survey datasets, and then use surface-level metrics such as structural quality and reference relevance to evaluate generated surveys.However, these benchmarks have two key issues: (1) the ground truth survey datasets are unreliable because of a lack academic dimension annotations; (2) the evaluation metrics only focus on the surface quality of the survey such as logical coherence. Both issues lead to existing benchmarks cannot assess to evaluate their deep "academic value", such as the core research objectives and the critical analysis of different studies. To address the above problems, we propose DeepSurvey-Bench, a novel benchmark designed to comprehensively evaluate the academic value of generated surveys. Specifically, our benchmark propose a comprehensive academic value evaluation criteria covering three dimensions: informational value, scholarly communication value, and research guidance value. Based on this criteria, we construct a reliable dataset with academic value annotations, and evaluate the deep academic value of the generated surveys. Extensive experimental results demonstrate that our benchmark is highly consistent with human performance in assessing the academic value of generated surveys.

</details>


### [136] [Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.15311)
*Mustafa Arslan*

Main category: cs.AI

TL;DR: Aeon is a neuro-symbolic cognitive OS that solves LLM memory limitations with a Memory Palace spatial index and episodic graph, achieving sub-millisecond retrieval via predictive caching.


<details>
  <summary>Details</summary>
Motivation: LLMs face quadratic computational costs in self-attention and "Lost in the Middle" degradation with long contexts. Current "Flat RAG" architectures treat memory as unstructured embeddings, creating "Vector Haze" - disjointed facts lacking episodic continuity.

Method: Aeon structures memory into: 1) Memory Palace (spatial index via Atlas - SIMD-accelerated Page-Clustered Vector Index combining small-world graphs with B+ Tree disk locality), and 2) Trace (neuro-symbolic episodic graph). Uses Semantic Lookaside Buffer (SLB) for predictive caching exploiting conversational locality.

Result: Achieves < 1ms retrieval latency on conversational workloads while ensuring state consistency via zero-copy C++/Python bridge. Enables persistent, structured memory for autonomous agents.

Conclusion: Aeon redefines memory as a managed OS resource rather than static store, effectively solving LLM memory limitations through neuro-symbolic architecture and efficient indexing/caching mechanisms.

Abstract: Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the "Lost in the Middle" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily "Flat RAG" architectures relying on vector databases, treat memory as an unstructured bag of embeddings. This approach fails to capture the hierarchical and temporal structure of long-horizon interactions, leading to "Vector Haze", the retrieval of disjointed facts lacking episodic continuity. We propose Aeon, a Neuro-Symbolic Cognitive Operating System that redefines memory not as a static store, but as a managed OS resource. Aeon structures memory into a Memory Palace (a spatial index implemented via Atlas, a SIMD-accelerated Page-Clustered Vector Index that combines small-world graph navigation with B+ Tree-style disk locality to minimize read amplification) and a Trace (a neuro-symbolic episodic graph). We introduce the Semantic Lookaside Buffer (SLB), a predictive caching mechanism that exploits conversational locality to achieve sub-millisecond retrieval latencies. Benchmarks demonstrate that Aeon achieves < 1ms retrieval latency on conversational workloads while ensuring state consistency via a zero-copy C++/Python bridge, effectively enabling persistent, structured memory for autonomous agents.

</details>


### [137] [The Paradigm Shift: A Comprehensive Survey on Large Vision Language Models for Multimodal Fake News Detection](https://arxiv.org/abs/2601.15316)
*Wei Ai,Yilong Tan,Yuntao Shou,Tao Meng,Haowen Chen,Zhixiong He,Keqin Li*

Main category: cs.AI

TL;DR: This is a comprehensive survey paper reviewing the paradigm shift in multimodal fake news detection from traditional feature engineering to large vision-language model-driven approaches.


<details>
  <summary>Details</summary>
Motivation: The field lacks a systematic survey documenting the transition from traditional multimodal fake news detection methods to the new paradigm driven by large vision-language models, despite rapid evolution and significant advances in the area.

Method: The paper provides a structured review with historical perspective, taxonomy covering model architectures, datasets, and performance benchmarks, analysis of technical challenges, and outlines future research directions.

Result: This is the first comprehensive survey to systematically document and analyze the transformative role of large vision-language models in combating multimodal fake news, with a GitHub repository summarizing existing methods.

Conclusion: The survey traces the paradigm shift in multimodal fake news detection and provides guidance for future research directions in this rapidly evolving field.

Abstract: In recent years, the rapid evolution of large vision-language models (LVLMs) has driven a paradigm shift in multimodal fake news detection (MFND), transforming it from traditional feature-engineering approaches to unified, end-to-end multimodal reasoning frameworks. Early methods primarily relied on shallow fusion techniques to capture correlations between text and images, but they struggled with high-level semantic understanding and complex cross-modal interactions. The emergence of LVLMs has fundamentally changed this landscape by enabling joint modeling of vision and language with powerful representation learning, thereby enhancing the ability to detect misinformation that leverages both textual narratives and visual content. Despite these advances, the field lacks a systematic survey that traces this transition and consolidates recent developments. To address this gap, this paper provides a comprehensive review of MFND through the lens of LVLMs. We first present a historical perspective, mapping the evolution from conventional multimodal detection pipelines to foundation model-driven paradigms. Next, we establish a structured taxonomy covering model architectures, datasets, and performance benchmarks. Furthermore, we analyze the remaining technical challenges, including interpretability, temporal reasoning, and domain generalization. Finally, we outline future research directions to guide the next stage of this paradigm shift. To the best of our knowledge, this is the first comprehensive survey to systematically document and analyze the transformative role of LVLMs in combating multimodal fake news. The summary of existing methods mentioned is in our Github: \href{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}.

</details>


### [138] [Replayable Financial Agents: A Determinism-Faithfulness Assurance Harness for Tool-Using LLM Agents](https://arxiv.org/abs/2601.15322)
*Raffi Khatchadourian*

Main category: cs.AI

TL;DR: DFAH framework measures LLM agent determinism and faithfulness for financial regulatory audits, finding positive correlation between consistency and evidence-alignment, with schema-first models meeting audit requirements.


<details>
  <summary>Details</summary>
Motivation: LLM agents in financial services struggle with regulatory audit replay - they fail to produce consistent results when reproducing flagged transaction decisions with identical inputs, creating compliance risks.

Method: Introduces Determinism-Faithfulness Assurance Harness (DFAH) framework to measure trajectory determinism and evidence-conditioned faithfulness. Tests 74 configurations across 12 models, 4 providers with 8-24 runs each at T=0.0. Provides three financial benchmarks (compliance triage, portfolio constraints, DataOps exceptions) and open-source stress-test harness.

Result: 7-20B parameter models achieved 100% determinism, while 120B+ models needed 3.7x larger validation samples. Agentic tool-use introduces additional variance. Positive correlation (r=0.45, p<0.01) between determinism and faithfulness - consistent models tend to be more evidence-aligned. Tier 1 schema-first models achieved determinism levels meeting audit requirements.

Conclusion: DFAH enables reliable measurement of LLM agent determinism for financial regulatory compliance. Schema-first architectures show promise for audit replay requirements, and contrary to assumptions, determinism correlates positively with faithfulness rather than trading off with capability.

Abstract: LLM agents struggle with regulatory audit replay: when asked to reproduce a flagged transaction decision with identical inputs, most deployments fail to return consistent results. This paper introduces the Determinism-Faithfulness Assurance Harness (DFAH), a framework for measuring trajectory determinism and evidence-conditioned faithfulness in tool-using agents deployed in financial services.
  Across 74 configurations (12 models, 4 providers, 8-24 runs each at T=0.0) in non-agentic baseline experiments, 7-20B parameter models achieved 100% determinism, while 120B+ models required 3.7x larger validation samples to achieve equivalent statistical reliability. Agentic tool-use introduces additional variance (see Tables 4-7). Contrary to the assumed reliability-capability trade-off, a positive Pearson correlation emerged (r = 0.45, p < 0.01, n = 51 at T=0.0) between determinism and faithfulness; models producing consistent outputs also tended to be more evidence-aligned.
  Three financial benchmarks are provided (compliance triage, portfolio constraints, DataOps exceptions; 50 cases each) along with an open-source stress-test harness. In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements.

</details>


### [139] [Prometheus Mind: Retrofitting Memory to Frozen Language Models](https://arxiv.org/abs/2601.15324)
*Mark Wind*

Main category: cs.AI

TL;DR: Prometheus Mind adds memory to frozen Qwen3-4B using 11 modular adapters (530MB, 7% overhead) that are fully reversible. It solves four key problems: extraction via Contrastive Direction Discovery, stage-wise training, injection using existing lm_head weights, and recovering hidden state collapse.


<details>
  <summary>Details</summary>
Motivation: Adding memory to pretrained language models typically requires architectural changes or weight modification, which can be intrusive and irreversible. The authors aim to develop a reversible memory system that can be added to frozen models without permanent changes.

Method: Four-part approach: (1) Contrastive Direction Discovery for semantic extraction without labeled data, (2) Stage-wise training of 11 modular adapters on simple proxy tasks, (3) Injection using existing lm_head.weight rows instead of training new encoders, (4) Training projections to recover distinctions lost in hidden state collapse.

Result: Achieves 94.4% retrieval accuracy on clean inputs (n=54, 95% CI: [84.9%, 98.1%]) but degrades to 19.4% on informal inputs with ellipsis, filler words, or implicit subjects (n=36). Primary bottleneck is relation classification at 47.3% accuracy.

Conclusion: The system successfully adds reversible memory to frozen models with minimal overhead, but performance degrades significantly on informal language. Relation classification remains the main challenge for extraction accuracy.

Abstract: Adding memory to pretrained language models typically requires architectural changes or weight modification. We present Prometheus Mind, which retrofits memory to a frozen Qwen3-4B using 11 modular adapters (530MB, 7% overhead) -- fully reversible by removing the adapters. Building this system required solving four problems: (1) Extraction -- we develop Contrastive Direction Discovery (CDD), which finds semantic directions via minimal pairs without labeled data. (2) Training -- end-to-end optimization collapses; stage-wise training of each adapter on simple proxy tasks succeeds. (3) Injection -- learned encoders fail to generalize; we find that lm_head.weight rows already provide the mapping we need, requiring no training. (4) Hidden state collapse -- transformers make ``wife'' and ``brother'' 0.98+ similar; we train projections to recover distinction (0.98 $\rightarrow$ 0.09). On PrometheusExtract-132 (132 cases), the system achieves 94.4% retrieval on clean inputs (n=54, 95% CI: [84.9%, 98.1%]), degrading to 19.4% on informal inputs with ellipsis, filler words, or implicit subjects (n=36). The primary bottleneck is relation classification (47.3% accuracy), responsible for most extraction errors.

</details>


### [140] [Logic Programming on Knowledge Graph Networks And its Application in Medical Domain](https://arxiv.org/abs/2601.15347)
*Chuanqing Wang,Zhenmin Zhao,Shanshan Du,Chaoqun Fei,Songmao Zhang,Ruqian Lu*

Main category: cs.AI

TL;DR: This paper introduces the concept of 'knowledge graph network' to address gaps in current knowledge graph research, particularly in medical/healthcare applications, by developing systematic theory and techniques for multi-KG cooperation under various conditions.


<details>
  <summary>Details</summary>
Motivation: Current knowledge graph research has advanced rapidly but lacks sufficient application of advanced techniques like logic reasoning, AI methods, specialized programming languages, and probabilistic theories. Particularly, multiple knowledge graphs cooperation and competition techniques have been neglected, creating a gap in comprehensive KG development and application.

Method: The paper develops a systematic theory, technique, and application framework for 'knowledge graph network' concept. It covers definition, development, reasoning, computing, and application under various conditions including unsharp, uncertain, multi-modal, vectorized, distributed, and federated settings, with real data examples and experiments.

Result: The research provides comprehensive examples and experiment results for each condition (unsharp, uncertain, multi-modal, etc.) using real data, demonstrating the practical application of knowledge graph network theory in medical and healthcare domains.

Conclusion: The paper concludes with an innovation summary, presenting knowledge graph network as a novel approach to address current limitations in KG research and application, particularly for enabling effective cooperation and competition among multiple knowledge graphs in complex domains like healthcare.

Abstract: The rash development of knowledge graph research has brought big driving force to its application in many areas, including the medicine and healthcare domain. However, we have found that the application of some major information processing techniques on knowledge graph still lags behind. This defect includes the failure to make sufficient use of advanced logic reasoning, advanced artificial intelligence techniques, special-purpose programming languages, modern probabilistic and statistic theories et al. on knowledge graphs development and application. In particular, the multiple knowledge graphs cooperation and competition techniques have not got enough attention from researchers. This paper develops a systematic theory, technique and application of the concept 'knowledge graph network' and its application in medical and healthcare domain. Our research covers its definition, development, reasoning, computing and application under different conditions such as unsharp, uncertain, multi-modal, vectorized, distributed, federated. Almost in each case we provide (real data) examples and experiment results. Finally, a conclusion of innovation is provided.

</details>


### [141] [GeMM-GAN: A Multimodal Generative Model Conditioned on Histopathology Images and Clinical Descriptions for Gene Expression Profile Generation](https://arxiv.org/abs/2601.15392)
*Francesca Pia Panaccione,Carlo Sgaravatti,Pietro Pinoli*

Main category: cs.AI

TL;DR: GeMM-GAN is a GAN that generates realistic gene expression profiles from histopathology images and clinical metadata, overcoming privacy and cost barriers in biomedical research.


<details>
  <summary>Details</summary>
Motivation: Gene expression data is crucial for biomedical research but faces privacy regulations and high experimental costs, while medical images and clinical metadata are more readily available. There's a need to generate realistic gene expression profiles from these accessible modalities.

Method: GeMM-GAN uses a Transformer Encoder for histopathology image patches with a Cross Attention mechanism between patches and text tokens to create a conditioning vector. This guides a generative model to produce biologically coherent gene expression profiles.

Result: On TCGA dataset, GeMM-GAN outperforms standard generative models, producing more realistic and functionally meaningful gene expression profiles. It improves disease type prediction accuracy by over 11% compared to state-of-the-art generative models.

Conclusion: GeMM-GAN successfully generates realistic gene expression profiles from histopathology images and clinical metadata, offering a practical solution to overcome privacy and cost limitations in biomedical research while maintaining biological coherence.

Abstract: Biomedical research increasingly relies on integrating diverse data modalities, including gene expression profiles, medical images, and clinical metadata. While medical images and clinical metadata are routinely collected in clinical practice, gene expression data presents unique challenges for widespread research use, mainly due to stringent privacy regulations and costly laboratory experiments. To address these limitations, we present GeMM-GAN, a novel Generative Adversarial Network conditioned on histopathology tissue slides and clinical metadata, designed to synthesize realistic gene expression profiles. GeMM-GAN combines a Transformer Encoder for image patches with a final Cross Attention mechanism between patches and text tokens, producing a conditioning vector to guide a generative model in generating biologically coherent gene expression profiles. We evaluate our approach on the TCGA dataset and demonstrate that our framework outperforms standard generative models and generates more realistic and functionally meaningful gene expression profiles, improving by more than 11\% the accuracy on downstream disease type prediction compared to current state-of-the-art generative models. Code will be available at: https://github.com/francescapia/GeMM-GAN

</details>


### [142] [Beyond Prompting: Efficient and Robust Contextual Biasing for Speech LLMs via Logit-Space Integration (LOGIC)](https://arxiv.org/abs/2601.15397)
*Peidong Wang*

Main category: cs.AI

TL;DR: LOGIC is a framework that integrates contextual biasing directly in the decoding layer of Speech LLMs to handle new entities without prompting limitations.


<details>
  <summary>Details</summary>
Motivation: Speech LLMs struggle with domain-specific terms (contact names, playlists, jargon) due to static training knowledge. Prompting solutions have scalability issues (context window limits, latency, lost-in-the-middle), while GEC suffers from over-correction hallucinations.

Method: LOGIC (Logit-Space Integration for Contextual Biasing) operates directly in the decoding layer, decoupling context injection from input processing to achieve constant-time complexity relative to prompt length.

Result: Experiments with Phi-4-MM model across 11 multilingual locales show LOGIC achieves 9% relative reduction in Entity WER with only 0.30% increase in False Alarm Rate.

Conclusion: LOGIC provides an efficient and robust alternative to prompting for contextual biasing in Speech LLMs, effectively handling new entities while maintaining performance.

Abstract: The rapid emergence of new entities -- driven by cultural shifts, evolving trends, and personalized user data -- poses a significant challenge for existing Speech Large Language Models (Speech LLMs). While these models excel at general conversational tasks, their static training knowledge limits their ability to recognize domain-specific terms such as contact names, playlists, or technical jargon. Existing solutions primarily rely on prompting, which suffers from poor scalability: as the entity list grows, prompting encounters context window limitations, increased inference latency, and the "lost-in-the-middle" phenomenon. An alternative approach, Generative Error Correction (GEC), attempts to rewrite transcripts via post-processing but frequently suffers from "over-correction", introducing hallucinations of entities that were never spoken.
  In this work, we introduce LOGIC (Logit-Space Integration for Contextual Biasing), an efficient and robust framework that operates directly in the decoding layer. Unlike prompting, LOGIC decouples context injection from input processing, ensuring constant-time complexity relative to prompt length. Extensive experiments using the Phi-4-MM model across 11 multilingual locales demonstrate that LOGIC achieves an average 9% relative reduction in Entity WER with a negligible 0.30% increase in False Alarm Rate.

</details>


### [143] [Not Your Typical Sycophant: The Elusive Nature of Sycophancy in Large Language Models](https://arxiv.org/abs/2601.15436)
*Shahar Ben Natan,Oren Tsur*

Main category: cs.AI

TL;DR: Researchers propose a novel zero-sum game framework using LLM-as-a-judge to evaluate sycophancy in LLMs, finding that while all tested models show sycophantic tendencies, Claude and Mistral exhibit "moral remorse" when sycophancy harms others, and all models show recency bias that interacts with sycophancy.


<details>
  <summary>Details</summary>
Motivation: Prior methods for evaluating LLM sycophancy suffer from uncontrolled bias, noise, or manipulative language in prompts. There's a need for a more direct and neutral evaluation approach that can better capture sycophantic tendencies without these confounding factors.

Method: The researchers use LLM-as-a-judge to evaluate sycophancy as a zero-sum game in a bet setting, where sycophancy serves one individual while explicitly incurring cost on another. They compare four leading models: Gemini 2.5 Pro, ChatGPT 4o, Mistral-Large-Instruct-2411, and Claude Sonnet 3.7.

Result: All models exhibit sycophantic tendencies in self-serving scenarios, but Claude and Mistral show "moral remorse" and over-compensate when sycophancy harms a third party. All models display recency bias, and crucially, sycophancy and recency bias interact to produce a "constructive interference" effect where agreement with the user is exacerbated when the user's opinion is presented last.

Conclusion: The novel zero-sum game framework provides a more direct evaluation of sycophancy, revealing that sycophancy is not uniform across models and interacts with recency bias. Some models exhibit ethical considerations when sycophancy harms others, suggesting varying levels of moral reasoning in different LLMs.

Abstract: We propose a novel way to evaluate sycophancy of LLMs in a direct and neutral way, mitigating various forms of uncontrolled bias, noise, or manipulative language, deliberately injected to prompts in prior works. A key novelty in our approach is the use of LLM-as-a-judge, evaluation of sycophancy as a zero-sum game in a bet setting. Under this framework, sycophancy serves one individual (the user) while explicitly incurring cost on another. Comparing four leading models - Gemini 2.5 Pro, ChatGpt 4o, Mistral-Large-Instruct-2411, and Claude Sonnet 3.7 - we find that while all models exhibit sycophantic tendencies in the common setting, in which sycophancy is self-serving to the user and incurs no cost on others, Claude and Mistral exhibit "moral remorse" and over-compensate for their sycophancy in case it explicitly harms a third party. Additionally, we observed that all models are biased toward the answer proposed last. Crucially, we find that these two phenomena are not independent; sycophancy and recency bias interact to produce `constructive interference' effect, where the tendency to agree with the user is exacerbated when the user's opinion is presented last.

</details>


### [144] [A tensor network formalism for neuro-symbolic AI](https://arxiv.org/abs/2601.15442)
*Alex Goessmann,Janina Schütte,Maximilian Fröhlich,Martin Eigel*

Main category: cs.AI

TL;DR: Tensor network formalism unifies neural and symbolic AI by representing logical formulas and probability distributions as structured tensor decompositions, enabling hybrid logical-probabilistic models.


<details>
  <summary>Details</summary>
Motivation: The unification of neural and symbolic approaches to artificial intelligence remains a central open challenge. The paper aims to bridge this gap by developing a mathematical framework that can represent both logical reasoning and probabilistic inference.

Method: Introduces a tensor network formalism with basis encoding for functions, models neural decompositions as tensor decompositions, and represents logical formulas and probability distributions as structured tensor decompositions. Identifies tensor network contractions as fundamental inference class and formulates reasoning algorithms as contraction message passing schemes.

Result: Develops a framework that enables definition and training of hybrid logical and probabilistic models called Hybrid Logic Networks. Provides theoretical concepts accompanied by the python library tnreason for practical implementation.

Conclusion: The tensor network approach provides a unified mathematical foundation for combining neural and symbolic AI, offering a practical framework for hybrid reasoning systems with both theoretical foundations and practical implementation tools.

Abstract: The unification of neural and symbolic approaches to artificial intelligence remains a central open challenge. In this work, we introduce a tensor network formalism, which captures sparsity principles originating in the different approaches in tensor decompositions. In particular, we describe a basis encoding scheme for functions and model neural decompositions as tensor decompositions. The proposed formalism can be applied to represent logical formulas and probability distributions as structured tensor decompositions. This unified treatment identifies tensor network contractions as a fundamental inference class and formulates efficiently scaling reasoning algorithms, originating from probability theory and propositional logic, as contraction message passing schemes. The framework enables the definition and training of hybrid logical and probabilistic models, which we call Hybrid Logic Network. The theoretical concepts are accompanied by the python library tnreason, which enables the implementation and practical use of the proposed architectures.

</details>


### [145] [Reliability by design: quantifying and eliminating fabrication risk in LLMs. From generative to consultative AI: a comparative analysis in the legal domain and lessons for high-stakes knowledge bases](https://arxiv.org/abs/2601.15476)
*Alex Dantart*

Main category: cs.AI

TL;DR: Advanced RAG systems with end-to-end optimization can reduce legal hallucinations to negligible levels (below 0.2%), making LLMs reliable for high-stakes legal work, while standalone models are unsuitable with false citation rates above 30%.


<details>
  <summary>Details</summary>
Motivation: To address the critical problem of hallucinations in large language models when applied to high-stakes legal work, where accuracy and reliability are essential for professional use.

Method: The study distinguishes three AI paradigms: standalone generative models, basic RAG systems, and advanced end-to-end optimized RAG systems. It introduces two reliability metrics (False Citation Rate and Fabricated Fact Rate) and evaluates 2,700 judicial-style answers from 12 LLMs across 75 legal tasks using expert double-blind review.

Result: Standalone models are unsuitable for professional use (FCR above 30%), basic RAG greatly reduces errors but still has notable misgrounding, while advanced RAG with embedding fine-tuning, re-ranking, and self-correction reduces fabrication to negligible levels (below 0.2%).

Conclusion: Trustworthy legal AI requires rigor-focused, retrieval-based architectures that emphasize verification and traceability. The evaluation framework developed is applicable to other high-risk domains beyond law.

Abstract: This paper examines how to make large language models reliable for high-stakes legal work by reducing hallucinations. It distinguishes three AI paradigms: (1) standalone generative models ("creative oracle"), (2) basic retrieval-augmented systems ("expert archivist"), and (3) an advanced, end-to-end optimized RAG system ("rigorous archivist"). The authors introduce two reliability metrics -False Citation Rate (FCR) and Fabricated Fact Rate (FFR)- and evaluate 2,700 judicial-style answers from 12 LLMs across 75 legal tasks using expert, double-blind review. Results show that standalone models are unsuitable for professional use (FCR above 30%), while basic RAG greatly reduces errors but still leaves notable misgrounding. Advanced RAG, using techniques such as embedding fine-tuning, re-ranking, and self-correction, reduces fabrication to negligible levels (below 0.2%). The study concludes that trustworthy legal AI requires rigor-focused, retrieval-based architectures emphasizing verification and traceability, and provides an evaluation framework applicable to other high-risk domains.

</details>


### [146] [MiRAGE: A Multiagent Framework for Generating Multimodal Multihop Question-Answer Dataset for RAG Evaluation](https://arxiv.org/abs/2601.15487)
*Chandan Kumar Sahu,Premith Kumar Chilukuri,Matthew Hetrich*

Main category: cs.AI

TL;DR: MiRAGE is a multiagent framework that automatically generates verified, domain-specific, multimodal, multi-hop QA datasets for evaluating RAG systems in specialized domains.


<details>
  <summary>Details</summary>
Motivation: Existing RAG evaluation benchmarks fail to capture the complexity of specialized technical documents where information is multimodal and reasoning requires synthesizing disjoint evidence across domains like regulations, finance, and quantitative biology.

Method: MiRAGE uses a collaborative swarm of specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent for factual grounding, and an agent to recognize expert personas and domains to mimic expert cognitive workflows.

Result: MiRAGE generates datasets with significantly higher reasoning complexity (>2.3 average hops) and factual faithfulness across four domains (regulations, finance, quantitative biology, and journalism). Ablation studies show it works with LLMs if image descriptions are available, though visual grounding remains a challenge.

Conclusion: MiRAGE automates creation of gold-standard evaluation datasets that reflect proprietary corpora's thematic structure, providing necessary infrastructure to rigorously benchmark next-generation multimodal RAG systems in enterprise applications.

Abstract: The rapid evolution of Retrieval-Augmented Generation (RAG) toward multimodal, high-stakes enterprise applications has outpaced the development of domain specific evaluation benchmarks. Existing datasets often rely on general-domain corpora or purely textual retrieval, failing to capture the complexity of specialized technical documents where information is inextricably multimodal and reasoning requires synthesizing disjoint evidence. We address this gap by introducing MiRAGE, a Multiagent framework for RAG systems Evaluation, that leverages a collaborative swarm of specialized agents to generate verified, domain-specific, multimodal, and multi-hop Question-Answer datasets. MiRAGE orchestrates a swarm of specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent to guarantee factual grounding, and an agent to recognize the expert persona and the relevant domain to mimic expert cognitive workflows. Extensive empirical evaluation across four distinct domains (regulations, finance, quantitative biology, and journalism) demonstrates that MiRAGE generates datasets with significantly higher reasoning complexity (>2.3 average hops) and factual faithfulness. Our ablation studies point that MiRAGE can be powered by LLMs if textual descriptions of the images are available. Visual grounding still remains a frontier. By automating the creation of gold standard evaluation datasets that reflect the latent thematic structure of proprietary corpora, MiRAGE provides the necessary infrastructure to rigorously benchmark the next generation information retrieval systems.

</details>


### [147] [Tracking the Limits of Knowledge Propagation: How LLMs Fail at Multi-Step Reasoning with Conflicting Knowledge](https://arxiv.org/abs/2601.15495)
*Yiyang Feng,Zeming Chen,Haotian Wu,Jiawei Zhou,Antoine Bosselut*

Main category: cs.AI

TL;DR: TRACK is a new benchmark for evaluating how LLMs propagate conflicting knowledge through multi-step reasoning, showing that providing updated facts can actually worsen performance compared to no updates.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for knowledge conflicts in LLMs focus only on single knowledge updates and fact recall, without evaluating how these updates affect downstream reasoning. There's a need to study how LLMs propagate new knowledge through multi-step reasoning when it conflicts with their parametric knowledge.

Method: Introduces TRACK benchmark with three reasoning-intensive scenarios (WIKI, CODE, and MATH) that introduce multiple, realistic conflicts to mirror real-world complexity. The benchmark tests how LLMs handle conflicting knowledge during multi-step reasoning.

Result: Providing updated facts to models for reasoning can worsen performance compared to providing no updated facts, and this performance degradation exacerbates as more updated facts are provided. The failure stems from both inability to faithfully integrate updated facts and flawed reasoning even when knowledge is integrated.

Conclusion: TRACK provides a rigorous new benchmark to measure and guide future progress on propagating conflicting knowledge in multi-step reasoning, revealing significant challenges in how LLMs handle knowledge conflicts during complex reasoning tasks.

Abstract: A common solution for mitigating outdated or incorrect information in Large Language Models (LLMs) is to provide updated facts in-context or through knowledge editing. However, these methods introduce knowledge conflicts when the knowledge update fails to overwrite the model's parametric knowledge, which propagate to faulty reasoning. Current benchmarks for this problem, however, largely focus only on single knowledge updates and fact recall without evaluating how these updates affect downstream reasoning. In this work, we introduce TRACK (Testing Reasoning Amid Conflicting Knowledge), a new benchmark for studying how LLMs propagate new knowledge through multi-step reasoning when it conflicts with the model's initial parametric knowledge. Spanning three reasoning-intensive scenarios (WIKI, CODE, and MATH), TRACK introduces multiple, realistic conflicts to mirror real-world complexity. Our results on TRACK reveal that providing updated facts to models for reasoning can worsen performance compared to providing no updated facts to a model, and that this performance degradation exacerbates as more updated facts are provided. We show this failure stems from both inability to faithfully integrate updated facts, but also flawed reasoning even when knowledge is integrated. TRACK provides a rigorous new benchmark to measure and guide future progress on propagating conflicting knowledge in multi-step reasoning.

</details>


### [148] [The Dark Side of AI Transformers: Sentiment Polarization & the Loss of Business Neutrality by NLP Transformers](https://arxiv.org/abs/2601.15509)
*Prasanna Kumar*

Main category: cs.AI

TL;DR: Transformer-based sentiment analysis improves accuracy but reduces neutrality, causing polarization between sentiment classes and harming reliability for industry applications.


<details>
  <summary>Details</summary>
Motivation: The paper addresses a critical problem in Applied AI Analytics: while transformers improve sentiment analysis accuracy, they often do so by polarizing sentiment classifications and failing to maintain neutrality, which undermines reliability for real-world industry tasks.

Method: The research appears to be based on experimental observations of transformer-based sentiment analysis models, likely comparing their performance across different sentiment classes and examining how accuracy improvements in one class affect others.

Result: Experiments show that transformer-driven accuracy improvements in sentiment analysis come at the cost of polarization between sentiment classes and loss of neutrality, creating reliability issues for practical applications.

Conclusion: The dark side of transformer-based sentiment analysis is the trade-off between improved accuracy and reduced neutrality, posing significant challenges for reliable industry deployment of NLP systems that depend on balanced sentiment analytics.

Abstract: The use of Transfer Learning & Transformers has steadily improved accuracy and has significantly contributed in solving complex computation problems. However, this transformer led accuracy improvement in Applied AI Analytics specifically in sentiment analytics comes with the dark side. It is observed during experiments that a lot of these improvements in transformer led accuracy of one class of sentiment has been at the cost of polarization of another class of sentiment and the failing of neutrality. This lack of neutrality poses an acute problem in the Applied NLP space, which relies heavily on the computational outputs of sentiment analytics for reliable industry ready tasks.

</details>


### [149] [TransportAgents: a multi-agents LLM framework for traffic accident severity prediction](https://arxiv.org/abs/2601.15519)
*Zhichao Yang,Jiashu He,Jinxuan Fan,Cirillo Cinzia*

Main category: cs.AI

TL;DR: TransportAgents: A hybrid multi-agent LLM framework for traffic crash severity prediction that outperforms traditional ML and single-agent LLMs by using specialized agents for different data categories and an MLP integration module.


<details>
  <summary>Details</summary>
Motivation: Single-agent LLMs struggle with heterogeneous, domain-specific crash data and tend to generate biased or unstable predictions for traffic crash severity, which is critical for emergency response and public safety planning.

Method: Proposes TransportAgents, a hybrid multi-agent framework integrating category-specific LLM reasoning with an MLP integration module. Specialized agents focus on subsets of traffic information (demographics, environmental context, incident details) to produce intermediate assessments that are fused into unified predictions.

Result: Outperforms traditional ML and advanced LLM-based baselines on two U.S. datasets (CPSRMS and NEISS). Shows strong robustness, scalability, and cross-dataset generalizability across three backbones (GPT-3.5, GPT-4o, LLaMA-3.3). Produces more balanced and well-calibrated severity predictions than single-agent LLM approaches.

Conclusion: TransportAgents demonstrates superior interpretability and reliability for safety-critical decision support applications, offering a robust solution for traffic crash severity prediction that addresses limitations of single-agent LLM architectures.

Abstract: Accurate prediction of traffic crash severity is critical for improving emergency response and public safety planning. Although recent large language models (LLMs) exhibit strong reasoning capabilities, their single-agent architectures often struggle with heterogeneous, domain-specific crash data and tend to generate biased or unstable predictions. To address these limitations, this paper proposes TransportAgents, a hybrid multi-agent framework that integrates category-specific LLM reasoning with a multilayer perceptron (MLP) integration module. Each specialized agent focuses on a particular subset of traffic information, such as demographics, environmental context, or incident details, to produce intermediate severity assessments that are subsequently fused into a unified prediction. Extensive experiments on two complementary U.S. datasets, the Consumer Product Safety Risk Management System (CPSRMS) and the National Electronic Injury Surveillance System (NEISS), demonstrate that TransportAgents consistently outperforms both traditional machine learning and advanced LLM-based baselines. Across three representative backbones, including closed-source models such as GPT-3.5 and GPT-4o, as well as open-source models such as LLaMA-3.3, the framework exhibits strong robustness, scalability, and cross-dataset generalizability. A supplementary distributional analysis further shows that TransportAgents produces more balanced and well-calibrated severity predictions than standard single-agent LLM approaches, highlighting its interpretability and reliability for safety-critical decision support applications.

</details>


### [150] [From Generative Engines to Actionable Simulators: The Imperative of Physical Grounding in World Models](https://arxiv.org/abs/2601.15533)
*Zhikang Chen,Tingting Zhu*

Main category: cs.AI

TL;DR: World models need to focus on causal understanding and constraint satisfaction rather than visual realism, especially for safety-critical domains like medicine.


<details>
  <summary>Details</summary>
Motivation: Current world models prioritize visual fidelity over understanding physical and causal dynamics, leading to failures in safety-critical decision-making where trial-and-error is impossible and errors are irreversible.

Method: Proposes reframing world models as actionable simulators with structured 4D interfaces, constraint-aware dynamics, and closed-loop evaluation, using medical decision-making as an epistemic stress test.

Result: Shows that visual realism is an unreliable proxy for world understanding, and that effective world models must encode causal structure, respect domain-specific constraints, and remain stable over long horizons.

Conclusion: World models should be evaluated by their ability to support counterfactual reasoning, intervention planning, and robust long-horizon foresight rather than visual realism, especially in safety-critical domains.

Abstract: A world model is an AI system that simulates how an environment evolves under actions, enabling planning through imagined futures rather than reactive perception. Current world models, however, suffer from visual conflation: the mistaken assumption that high-fidelity video generation implies an understanding of physical and causal dynamics. We show that while modern models excel at predicting pixels, they frequently violate invariant constraints, fail under intervention, and break down in safety-critical decision-making. This survey argues that visual realism is an unreliable proxy for world understanding. Instead, effective world models must encode causal structure, respect domain-specific constraints, and remain stable over long horizons. We propose a reframing of world models as actionable simulators rather than visual engines, emphasizing structured 4D interfaces, constraint-aware dynamics, and closed-loop evaluation. Using medical decision-making as an epistemic stress test, where trial-and-error is impossible and errors are irreversible, we demonstrate that a world model's value is determined not by how realistic its rollouts appear, but by its ability to support counterfactual reasoning, intervention planning, and robust long-horizon foresight.

</details>


### [151] [ALIGNAgent: Adaptive Learner Intelligence for Gap Identification and Next-step guidance](https://arxiv.org/abs/2601.15551)
*Bismack Tokoli,Luis Jaimes,Ayesha S. Dina*

Main category: cs.AI

TL;DR: ALIGNAgent is a multi-agent educational framework that integrates knowledge estimation, skill-gap identification, and personalized resource recommendation into a cohesive adaptive learning cycle, achieving high precision and F1 scores in empirical evaluations.


<details>
  <summary>Details</summary>
Motivation: Existing personalized learning systems are fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation separately, but rarely integrating these components into a cohesive adaptive cycle that can provide comprehensive personalized learning.

Method: ALIGNAgent uses a multi-agent framework with: 1) Skill Gap Agent that processes student quiz performance, gradebook data, and preferences to generate topic-level proficiency estimates using concept-level diagnostic reasoning; 2) Recommender Agent that retrieves preference-aware learning materials aligned with diagnosed deficiencies; 3) A continuous feedback loop where interventions occur before advancing to subsequent topics.

Result: Extensive evaluation on authentic datasets from two undergraduate computer science courses shows ALIGNAgent's effectiveness, with GPT-4o-based agents achieving precision of 0.87-0.90 and F1 scores of 0.84-0.87 in knowledge proficiency estimation validated against actual exam performance.

Conclusion: ALIGNAgent successfully integrates multiple personalized learning components into a cohesive adaptive framework, demonstrating that multi-agent systems can effectively deliver comprehensive personalized education through integrated knowledge estimation, skill-gap identification, and targeted resource recommendation.

Abstract: Personalized learning systems have emerged as a promising approach to enhance student outcomes by tailoring educational content, pacing, and feedback to individual needs. However, most existing systems remain fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation, but rarely integrating these components into a cohesive adaptive cycle. In this paper, we propose ALIGNAgent (Adaptive Learner Intelligence for Gap Identification and Next-step guidance), a multi-agent educational framework designed to deliver personalized learning through integrated knowledge estimation, skill-gap identification, and targeted resource recommendation.ALIGNAgent begins by processing student quiz performance, gradebook data, and learner preferences to generate topic-level proficiency estimates using a Skill Gap Agent that employs concept-level diagnostic reasoning to identify specific misconceptions and knowledge deficiencies. After identifying skill gaps, the Recommender Agent retrieves preference-aware learning materials aligned with diagnosed deficiencies, implementing a continuous feedback loop where interventions occur before advancing to subsequent topics. Extensive empirical evaluation on authentic datasets from two undergraduate computer science courses demonstrates ALIGNAgent's effectiveness, with GPT-4o-based agents achieving precision of 0.87-0.90 and F1 scores of 0.84-0.87 in knowledge proficiency estimation validated against actual exam performance.

</details>


### [152] [Autonomous Business System via Neuro-symbolic AI](https://arxiv.org/abs/2601.15599)
*Cecil Pang,Hiroki Sayama*

Main category: cs.AI

TL;DR: AUTOBUS is a neuro-symbolic AI system that combines LLM-based AI agents with predicate logic programming to orchestrate end-to-end business initiatives, bridging the gap between flexible natural language understanding and deterministic business logic execution.


<details>
  <summary>Details</summary>
Motivation: Current enterprise systems are siloed, rigid, and hard-coded, making it difficult to reconfigure cross-functional processes. While LLMs excel at natural language understanding, they lack deterministic execution of complex business logic. There's a need to integrate these capabilities for flexible yet verifiable business process orchestration.

Method: AUTOBUS uses a neuro-symbolic architecture combining LLM-based AI agents, predicate-logic programming, and business-semantics-centric knowledge graphs. Initiatives are modeled as task networks with explicit conditions, data requirements, evaluation rules, and API actions. Enterprise data is organized as a knowledge graph translated into logic facts and rules. AI agents synthesize task instructions, semantics, and tools into task-specific logic programs executed by a logic engine.

Result: The paper introduces the AUTOBUS architecture that enables deterministic, verifiable execution of complex business initiatives while maintaining flexibility through LLM-based natural language interpretation. It provides a framework where humans define semantics and policies while AI agents generate and execute logic programs.

Conclusion: AUTOBUS represents a novel neuro-symbolic approach to business process orchestration that combines the strengths of LLMs (natural language understanding) with symbolic AI (deterministic logic execution), enabling adaptable yet accountable business initiative management with human oversight.

Abstract: Current business environments require organizations to continuously reconfigure cross-functional processes, yet enterprise systems are still organized around siloed departments, rigid workflows, and hard-coded automation. Meanwhile large language models (LLMs) excel at interpreting natural language and unstructured data but lack deterministic, verifiable execution of complex business logic. To address this gap, here we introduce AUTOBUS, an Autonomous Business System that integrates LLM-based AI agents, predicate-logic programming, and business-semantics-centric enterprise data into a coherent neuro-symbolic AI architecture for orchestrating end-to-end business initiatives. AUTOBUS models an initiative as a network of tasks with explicit pre/post conditions, required data, evaluation rules, and API-level actions. Enterprise data is organized as a knowledge graph whose entities, relationships, and constraints are translated into logic facts and foundational rules, providing the semantic grounding for task reasoning. Core AI agents synthesize task instructions, enterprise semantics, and available tools into task-specific logic programs, which are executed by a logic engine that enforces constraints, coordinates auxiliary tools, and orchestrate execution of actions and outcomes. Humans define and maintain the semantics, policies and task instructions, curate tools, and supervise high-impact or ambiguous decisions, ensuring accountability and adaptability. We detail the AUTOBUS architecture, the anatomy of the AI agent generated logic programs, and the role of humans and auxiliary tools in the lifecycle of a business initiative.

</details>


### [153] [CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human Cognition for Large Language Models](https://arxiv.org/abs/2601.15628)
*Haibo Tong,Zeyang Yue,Feifei Zhao,Erliang Lin,Lu Jia,Ruolin Chen,Yinqian Sun,Qian Zhang,Yi Zeng*

Main category: cs.AI

TL;DR: CogToM is a comprehensive bilingual benchmark with 8000+ instances across 46 paradigms for evaluating LLM Theory of Mind capabilities, revealing performance gaps and cognitive divergences from humans.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for evaluating Theory of Mind (ToM) in LLMs are too narrow, focusing mainly on false belief tasks and failing to capture the full spectrum of human cognitive mechanisms.

Method: Created CogToM benchmark with over 8000 bilingual instances across 46 paradigms, validated by 49 human annotators. Systematically evaluated 22 representative models including frontier models like GPT-5.1 and Qwen3-Max.

Result: Revealed significant performance heterogeneities among models, identified persistent bottlenecks in specific dimensions, and found potential divergences between LLM and human cognitive structures based on human cognitive patterns.

Conclusion: CogToM provides a robust instrument and perspective for investigating the evolving cognitive boundaries of LLMs, highlighting that current LLMs still lack comprehensive human-like Theory of Mind capabilities.

Abstract: Whether Large Language Models (LLMs) truly possess human-like Theory of Mind (ToM) capabilities has garnered increasing attention. However, existing benchmarks remain largely restricted to narrow paradigms like false belief tasks, failing to capture the full spectrum of human cognitive mechanisms. We introduce CogToM, a comprehensive, theoretically grounded benchmark comprising over 8000 bilingual instances across 46 paradigms, validated by 49 human annotator.A systematic evaluation of 22 representative models, including frontier models like GPT-5.1 and Qwen3-Max, reveals significant performance heterogeneities and highlights persistent bottlenecks in specific dimensions. Further analysis based on human cognitive patterns suggests potential divergences between LLM and human cognitive structures. CogToM offers a robust instrument and perspective for investigating the evolving cognitive boundaries of LLMs.

</details>


### [154] [Agentic AI Governance and Lifecycle Management in Healthcare](https://arxiv.org/abs/2601.15630)
*Chandra Prakash,Mary Lind,Avneesh Sisodia*

Main category: cs.AI

TL;DR: UALM blueprint for managing AI agent sprawl in healthcare with five control layers and maturity model for audit-ready oversight.


<details>
  <summary>Details</summary>
Motivation: Healthcare faces agent sprawl from embedded AI agents across workflows, causing duplication, unclear accountability, inconsistent controls, and persistent permissions beyond original use cases. Existing governance frameworks lack guidance for day-to-day agent fleet operations.

Method: Proposed Unified Agent Lifecycle Management (UALM) blueprint derived from rapid synthesis of governance standards, agent security literature, and healthcare compliance requirements. Maps gaps to five control-plane layers: identity/persona registry, orchestration/mediation, PHI-bounded context/memory, runtime policy enforcement with kill-switches, and lifecycle management with credential revocation.

Result: UALM provides implementable pattern for healthcare leaders with companion maturity model for staged adoption, enabling audit-ready oversight while preserving local innovation and safer scaling across clinical/administrative domains.

Conclusion: UALM addresses critical governance gap for AI agent fleets in healthcare, offering structured approach to manage agent sprawl with practical controls that balance innovation with compliance and security requirements.

Abstract: Healthcare organizations are beginning to embed agentic AI into routine workflows, including clinical documentation support and early-warning monitoring. As these capabilities diffuse across departments and vendors, health systems face agent sprawl, causing duplicated agents, unclear accountability, inconsistent controls, and tool permissions that persist beyond the original use case. Existing AI governance frameworks emphasize lifecycle risk management but provide limited guidance for the day-to-day operations of agent fleets. We propose a Unified Agent Lifecycle Management (UALM) blueprint derived from a rapid, practice-oriented synthesis of governance standards, agent security literature, and healthcare compliance requirements. UALM maps recurring gaps onto five control-plane layers: (1) an identity and persona registry, (2) orchestration and cross-domain mediation, (3) PHI-bounded context and memory, (4) runtime policy enforcement with kill-switch triggers, and (5) lifecycle management and decommissioning linked to credential revocation and audit logging. A companion maturity model supports staged adoption. UALM offers healthcare CIOs, CISOs, and clinical leaders an implementable pattern for audit-ready oversight that preserves local innovation and enables safer scaling across clinical and administrative domains.

</details>


### [155] [Predictive Coding and Information Bottleneck for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2601.15652)
*Manish Bhatt*

Main category: cs.AI

TL;DR: A hybrid hallucination detection framework combining neuroscience-inspired signals (Predictive Coding and Information Bottleneck) with supervised ML achieves competitive performance with 75x less data, 1000x faster inference, and full interpretability.


<details>
  <summary>Details</summary>
Motivation: Current hallucination detection methods rely on expensive external retrieval loops or opaque black-box LLM judges with 70B+ parameters, creating barriers for high-stakes deployment. There's a need for efficient, interpretable detection.

Method: Hybrid framework combining neuroscience-inspired signal design with supervised ML. Extracts interpretable signals from Predictive Coding (quantifying surprise against internal priors) and Information Bottleneck (measuring signal retention under perturbation). Includes Entity-Focused Uptake, Context Adherence, and Falsifiability Score features.

Result: Achieves 0.8669 AUROC on HaluBench (4.95% gain over baseline), with 75x less training data (200 vs 15,000 samples), 1000x faster inference (5ms vs 5s), and full interpretability. Negative finding: Rationalization signal fails to distinguish hallucinations.

Conclusion: Domain knowledge encoded in signal architecture provides superior data efficiency compared to scaling LLM judges, enabling strong performance with lightweight (<1M parameter), explainable models suitable for production deployment.

Abstract: Hallucinations in Large Language Models (LLMs) -- generations that are plausible but factually unfaithful -- remain a critical barrier to high-stakes deployment. Current detection methods typically rely on computationally expensive external retrieval loops or opaque black-box LLM judges requiring 70B+ parameters. In this work, we introduce [Model Name], a hybrid detection framework that combines neuroscience-inspired signal design with supervised machine learning. We extract interpretable signals grounded in Predictive Coding (quantifying surprise against internal priors) and the Information Bottleneck (measuring signal retention under perturbation). Through systematic ablation, we demonstrate three key enhancements: Entity-Focused Uptake (concentrating on high-value tokens), Context Adherence (measuring grounding strength), and Falsifiability Score (detecting confident but contradictory claims).
  Evaluating on HaluBench (n=200, perfectly balanced), our theory-guided baseline achieves 0.8017 AUROC. BASE supervised models reach 0.8274 AUROC, while IMPROVED features boost performance to 0.8669 AUROC (4.95% gain), demonstrating consistent improvements across architectures. This competitive performance is achieved while using 75x less training data than Lynx (200 vs 15,000 samples), 1000x faster inference (5ms vs 5s), and remaining fully interpretable. Crucially, we report a negative result: the Rationalization signal fails to distinguish hallucinations, suggesting that LLMs generate coherent reasoning for false premises ("Sycophancy").
  This work demonstrates that domain knowledge encoded in signal architecture provides superior data efficiency compared to scaling LLM judges, achieving strong performance with lightweight (less than 1M parameter), explainable models suitable for production deployment.

</details>


### [156] [Improving Methodologies for Agentic Evaluations Across Domains: Leakage of Sensitive Information, Fraud and Cybersecurity Threats](https://arxiv.org/abs/2601.15679)
*Ee Wei Seah,Yongsen Zheng,Naga Nikshith,Mahran Morsidi,Gabriel Waikin Loh Matienzo,Nigel Gay,Akriti Vij,Benjamin Chua,En Qi Ng,Sharmini Johnson,Vanessa Wilfred,Wan Sie Lee,Anna Davidson,Catherine Devine,Erin Zorer,Gareth Holvey,Harry Coppock,James Walpole,Jerome Wynee,Magda Dubois,Michael Schmatz,Patrick Keane,Sam Deverett,Bill Black,Bo Yan,Bushra Sabir,Frank Sun,Hao Zhang,Harriet Farlow,Helen Zhou,Lingming Dong,Qinghua Lu,Seung Jang,Sharif Abuadbba,Simon O'Callaghan,Suyu Ma,Tom Howroyd,Cyrus Fung,Fatemeh Azadi,Isar Nejadgholi,Krishnapriya Vishnubhotla,Pulei Xiong,Saeedeh Lohrasbi,Scott Buffett,Shahrear Iqbal,Sowmya Vajjala,Anna Safont-Andreu,Luca Massarelli,Oskar van der Wal,Simon Möller,Agnes Delaborde,Joris Duguépéroux,Nicolas Rolin,Romane Gallienne,Sarah Behanzin,Tom Seimandi,Akiko Murakami,Takayuki Semitsu,Teresa Tsukiji,Angela Kinuthia,Michael Michie,Stephanie Kasaon,Jean Wangari,Hankyul Baek,Jaewon Noh,Kihyuk Nam,Sang Seo,Sungpil Shin,Taewhi Lee,Yongsu Kim*

Main category: cs.AI

TL;DR: International collaboration develops agent testing methodologies focusing on multilingual/cultural safety, cybersecurity, and information risks.


<details>
  <summary>Details</summary>
Motivation: As AI agents become globally deployed with reduced human oversight, there's an urgent need to ensure they handle different languages and cultures accurately and securely. Current agent testing is still a developing science, requiring international alignment on evaluation approaches.

Method: Third international exercise involving participants from multiple countries, split into two strands: (1) common risks (information leakage, fraud) led by Singapore, and (2) cybersecurity led by UK. Evaluated both open and closed-weight models using public agentic benchmarks, with focus on methodological issues rather than model capabilities.

Result: The collaboration represents an important step forward in advancing the science of agentic evaluations, though specific test results are not the primary focus. The exercise builds on insights from two previous joint testing exercises in November 2024 and February 2025.

Conclusion: International cooperation is essential for developing robust agent testing methodologies that address global deployment challenges, particularly around multilingual/cultural safety, cybersecurity, and information security risks. This collaborative approach helps refine best practices for testing advanced AI systems.

Abstract: The rapid rise of autonomous AI systems and advancements in agent capabilities are introducing new risks due to reduced oversight of real-world interactions. Yet agent testing remains nascent and is still a developing science. As AI agents begin to be deployed globally, it is important that they handle different languages and cultures accurately and securely.
  To address this, participants from The International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the European Commission, France, Kenya, South Korea, and the United Kingdom have come together to align approaches to agentic evaluations.
  This is the third exercise, building on insights from two earlier joint testing exercises conducted by the Network in November 2024 and February 2025. The objective is to further refine best practices for testing advanced AI systems.
  The exercise was split into two strands: (1) common risks, including leakage of sensitive information and fraud, led by Singapore AISI; and (2) cybersecurity, led by UK AISI. A mix of open and closed-weight models were evaluated against tasks from various public agentic benchmarks. Given the nascency of agentic testing, our primary focus was on understanding methodological issues in conducting such tests, rather than examining test results or model capabilities. This collaboration marks an important step forward as participants work together to advance the science of agentic evaluations.

</details>


### [157] [From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2601.15690)
*Jiaxin Zhang,Wendi Cui,Zhuohang Li,Lifu Huang,Bradley Malin,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: Survey on using uncertainty as an active control signal in LLMs to improve reliability in high-stakes domains, covering reasoning, autonomous agents, and reinforcement learning applications.


<details>
  <summary>Details</summary>
Motivation: LLMs show remarkable capabilities but their unreliability remains a critical barrier to deployment in high-stakes domains. There's a need to transform uncertainty from a passive diagnostic metric into an active control signal for real-time model behavior guidance.

Method: The survey analyzes how uncertainty is leveraged as an active control signal across three frontiers: 1) advanced reasoning (optimizing computation and triggering self-correction), 2) autonomous agents (governing metacognitive decisions about tool use and information seeking), and 3) reinforcement learning (mitigating reward hacking and enabling self-improvement via intrinsic rewards). Grounds these advancements in theoretical frameworks like Bayesian methods and Conformal Prediction.

Result: Provides a comprehensive overview, critical analysis, and practical design patterns for using uncertainty as an active control mechanism. Demonstrates how this approach enables more reliable AI systems across multiple application domains.

Conclusion: Mastering the new trend of uncertainty as an active control signal is essential for building the next generation of scalable, reliable, and trustworthy AI systems, representing a functional evolution in addressing LLM unreliability.

Abstract: While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \textbf{advanced reasoning} to optimize computation and trigger self-correction; in \textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.

</details>


### [158] [Agentic Uncertainty Quantification](https://arxiv.org/abs/2601.15703)
*Jiaxin Zhang,Prafulla Kumar Choubey,Kung-Hsiang Huang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: Proposes Dual-Process Agentic UQ framework that transforms verbalized uncertainty into active control signals to prevent hallucination spirals in AI agents.


<details>
  <summary>Details</summary>
Motivation: AI agents suffer from "Spiral of Hallucination" where early errors propagate irreversibly. Existing methods either only diagnose risks (UQ methods) or make aimless corrections (self-reflection), creating a gap between detection and resolution.

Method: Dual-Process Agentic UQ framework with two mechanisms: System 1 (Uncertainty-Aware Memory) implicitly propagates confidence and explanations to prevent blind decisions; System 2 (Uncertainty-Aware Reflection) uses explanations as rational cues to trigger targeted inference-time resolution only when needed.

Result: Training-free approach achieves superior performance and trajectory-level calibration on closed-loop benchmarks and open-ended deep research tasks.

Conclusion: The AUQ framework represents a significant step towards reliable agents by dynamically balancing efficient execution and deep deliberation through active uncertainty management.

Abstract: Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.

</details>


### [159] [Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning](https://arxiv.org/abs/2601.16163)
*Moo Jin Kim,Yihuai Gao,Tsung-Yi Lin,Yen-Chen Lin,Yunhao Ge,Grace Lam,Percy Liang,Shuran Song,Ming-Yu Liu,Chelsea Finn,Jinwei Gu*

Main category: cs.AI

TL;DR: Cosmos Policy adapts pretrained video models into robot policies through single-stage post-training, achieving SOTA performance on robotics benchmarks by generating actions, states, and values within the video model's latent diffusion process.


<details>
  <summary>Details</summary>
Motivation: Existing video model-based robotics approaches require complex multi-stage post-training and architectural modifications for action generation. The authors aim to simplify this process while leveraging the strong spatiotemporal priors of pretrained video models.

Method: Single-stage post-training of pretrained video model (Cosmos-Predict2) on robot demonstrations without architectural changes. The model generates robot actions encoded as latent frames within its latent diffusion process, along with future state images and values for test-time planning.

Result: Achieves SOTA performance on LIBERO (98.5%) and RoboCasa (67.1%) simulation benchmarks, and highest average score in real-world bimanual manipulation tasks. Outperforms diffusion policies trained from scratch, video model-based policies, and fine-tuned vision-language-action models.

Conclusion: Cosmos Policy demonstrates that pretrained video models can be effectively adapted into robot policies through simple post-training, leveraging their existing priors and learning algorithms without architectural modifications, enabling both direct action generation and planning capabilities.

Abstract: Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/

</details>


### [160] [Improving Methodologies for LLM Evaluations Across Global Languages](https://arxiv.org/abs/2601.15706)
*Akriti Vij,Benjamin Chua,Darshini Ramiah,En Qi Ng,Mahran Morsidi,Naga Nikshith Gangarapu,Sharmini Johnson,Vanessa Wilfred,Vikneswaran Kumaran,Wan Sie Lee,Wenzhuo Yang,Yongsen Zheng,Bill Black,Boming Xia,Frank Sun,Hao Zhang,Qinghua Lu,Suyu Ma,Yue Liu,Chi-kiu Lo,Fatemeh Azadi,Isar Nejadgholi,Sowmya Vajjala,Agnes Delaborde,Nicolas Rolin,Tom Seimandi,Akiko Murakami,Haruto Ishi,Satoshi Sekine,Takayuki Semitsu,Tasuku Sasaki,Angela Kinuthia,Jean Wangari,Michael Michie,Stephanie Kasaon,Hankyul Baek,Jaewon Noh,Kihyuk Nam,Sang Seo,Sungpil Shin,Taewhi Lee,Yongsu Kim,Daisy Newbold-Harrop,Jessica Wang,Mahmoud Ghanem,Vy Hong*

Main category: cs.AI

TL;DR: Multilingual safety evaluation of AI models across 10 languages reveals varying safeguard robustness, evaluator reliability differences, and methodological insights for improved cross-cultural AI safety testing.


<details>
  <summary>Details</summary>
Motivation: To examine how AI model safety safeguards hold up across diverse linguistic and cultural contexts as frontier AI models are deployed globally, ensuring safe and reliable behavior worldwide.

Method: International collaborative evaluation testing two open-weight models across 10 languages (Cantonese, English, Farsi, French, Japanese, Korean, Kiswahili, Malay, Mandarin Chinese, Telugu) using over 6,000 newly translated prompts across five harm categories, with both LLM-as-a-judge and human annotation.

Result: Safety behaviors vary significantly across languages, including differences in safeguard robustness across languages and harm types, and variation in evaluator reliability between LLM-as-judge and human review.

Conclusion: This work establishes an initial framework for multilingual safety testing, highlighting the need for culturally contextualized translations, stress-tested evaluator prompts, clearer human annotation guidelines, and calls for continued international collaboration.

Abstract: As frontier AI models are deployed globally, it is essential that their behaviour remains safe and reliable across diverse linguistic and cultural contexts. To examine how current model safeguards hold up in such settings, participants from the International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the EU, France, Kenya, South Korea and the UK conducted a joint multilingual evaluation exercise. Led by Singapore AISI, two open-weight models were tested across ten languages spanning high and low resourced groups: Cantonese English, Farsi, French, Japanese, Korean, Kiswahili, Malay, Mandarin Chinese and Telugu. Over 6,000 newly translated prompts were evaluated across five harm categories (privacy, non-violent crime, violent crime, intellectual property and jailbreak robustness), using both LLM-as-a-judge and human annotation.
  The exercise shows how safety behaviours can vary across languages. These include differences in safeguard robustness across languages and harm types and variation in evaluator reliability (LLM-as-judge vs. human review). Further, it also generated methodological insights for improving multilingual safety evaluations, such as the need for culturally contextualised translations, stress-tested evaluator prompts and clearer human annotation guidelines. This work represents an initial step toward a shared framework for multilingual safety testing of advanced AI systems and calls for continued collaboration with the wider research community and industry.

</details>


### [161] [AgentSM: Semantic Memory for Agentic Text-to-SQL](https://arxiv.org/abs/2601.15709)
*Asim Biswal,Chuan Lei,Xiao Qin,Aodong Li,Balakrishnan Narayanaswamy,Tim Kraska*

Main category: cs.AI

TL;DR: AgentSM is an agentic Text-to-SQL framework that uses structured semantic memory from execution traces to improve efficiency and accuracy on complex enterprise schemas.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based Text-to-SQL systems struggle with large enterprise schemas, diverse SQL dialects, and expensive multi-step reasoning. Agentic approaches show promise but suffer from inefficiency, instability, and inconsistent outputs.

Method: AgentSM builds interpretable semantic memory by capturing prior execution traces (or synthesizing curated ones) as structured programs that directly guide future reasoning, enabling systematic reuse of reasoning paths.

Result: AgentSM reduces average token usage by 25% and trajectory length by 35% on Spider 2.0 benchmark, and achieves state-of-the-art accuracy of 44.8% on Spider 2.0 Lite benchmark.

Conclusion: The structured semantic memory approach enables Text-to-SQL agents to scale efficiently and reliably to larger schemas, more complex questions, and longer reasoning trajectories compared to existing methods.

Abstract: Recent advances in LLM-based Text-to-SQL have achieved remarkable gains on public benchmarks such as BIRD and Spider. Yet, these systems struggle to scale in realistic enterprise settings with large, complex schemas, diverse SQL dialects, and expensive multi-step reasoning. Emerging agentic approaches show potential for adaptive reasoning but often suffer from inefficiency and instability-repeating interactions with databases, producing inconsistent outputs, and occasionally failing to generate valid answers. To address these challenges, we introduce Agent Semantic Memory (AgentSM), an agentic framework for Text-to-SQL that builds and leverages interpretable semantic memory. Instead of relying on raw scratchpads or vector retrieval, AgentSM captures prior execution traces-or synthesizes curated ones-as structured programs that directly guide future reasoning. This design enables systematic reuse of reasoning paths, which allows agents to scale to larger schemas, more complex questions, and longer trajectories efficiently and reliably. Compared to state-of-the-art systems, AgentSM achieves higher efficiency by reducing average token usage and trajectory length by 25% and 35%, respectively, on the Spider 2.0 benchmark. It also improves execution accuracy, reaching a state-of-the-art accuracy of 44.8% on the Spider 2.0 Lite benchmark.

</details>


### [162] [Investigation of the Generalisation Ability of Genetic Programming-evolved Scheduling Rules in Dynamic Flexible Job Shop Scheduling](https://arxiv.org/abs/2601.15717)
*Luyao Zhu,Fangfang Zhang,Yi Mei,Mengjie Zhang*

Main category: cs.AI

TL;DR: GP-evolved scheduling rules for DFJSS show good cross-type generalization when training instances have more jobs than test instances with fixed machines, and when training/test instances have similar scales or parameters.


<details>
  <summary>Details</summary>
Motivation: Existing GP studies for DFJSS typically train and test on instances that differ only by random seeds, leaving cross-type generalization ability largely unexplored. The paper aims to systematically investigate how GP-evolved scheduling rules generalize across diverse DFJSS conditions.

Method: Conducted systematic experiments across multiple dimensions: problem scale (number of machines/jobs), key job shop parameters (utilization level), and data distributions. Analyzed how these factors influence GP performance on unseen instance types.

Result: Good generalization occurs when training instances contain more jobs than test instances while keeping machines fixed, and when training/test instances have similar scales or parameters. Decision point number and distribution in DFJSS instances play crucial role - similar distributions lead to better generalization, while significant discrepancies cause performance degradation.

Conclusion: The study provides new insights into GP generalization in DFJSS and highlights the need to evolve more generalizable GP rules capable of handling heterogeneous DFJSS instances effectively. Decision point distribution is key factor explaining generalization performance differences.

Abstract: Dynamic Flexible Job Shop Scheduling (DFJSS) is a complex combinatorial optimisation problem that requires simultaneous machine assignment and operation sequencing decisions in dynamic production environments. Genetic Programming (GP) has been widely applied to automatically evolve scheduling rules for DFJSS. However, existing studies typically train and test GP-evolved rules on DFJSS instances of the same type, which differ only by random seeds rather than by structural characteristics, leaving their cross-type generalisation ability largely unexplored. To address this gap, this paper systematically investigates the generalisation ability of GP-evolved scheduling rules under diverse DFJSS conditions. A series of experiments are conducted across multiple dimensions, including problem scale (i.e., the number of machines and jobs), key job shop parameters (e.g., utilisation level), and data distributions, to analyse how these factors influence GP performance on unseen instance types. The results show that good generalisation occurs when the training instances contain more jobs than the test instances while keeping the number of machines fixed, and when both training and test instances have similar scales or job shop parameters. Further analysis reveals that the number and distribution of decision points in DFJSS instances play a crucial role in explaining these performance differences. Similar decision point distributions lead to better generalisation, whereas significant discrepancies result in a marked degradation of performance. Overall, this study provides new insights into the generalisation ability of GP in DFJSS and highlights the necessity of evolving more generalisable GP rules capable of handling heterogeneous DFJSS instances effectively.

</details>


### [163] [Benchmarking Text-to-Python against Text-to-SQL: The Impact of Explicit Logic and Ambiguity](https://arxiv.org/abs/2601.15728)
*Hangle Hu,Chenyu Hou,Bin Cao,Ruizhe Li*

Main category: cs.AI

TL;DR: BIRD-Python benchmark shows Text-to-Python achieves parity with Text-to-SQL when domain knowledge gaps are addressed, establishing Python as viable for analytical agents.


<details>
  <summary>Details</summary>
Motivation: Real-world analytics increasingly require Python/Pandas for file-based data and complex workflows, but Text-to-Python reliability remains underexplored compared to mature SQL ecosystem.

Method: Introduced BIRD-Python benchmark with refined dataset to reduce noise and align semantics; proposed Logic Completion Framework (LCF) to resolve ambiguity by incorporating latent domain knowledge.

Result: Performance differences stem from missing domain context, not inherent code generation limitations; when gaps are addressed, Text-to-Python achieves performance parity with Text-to-SQL.

Conclusion: Python is viable foundation for analytical agents if systems effectively ground ambiguous natural language inputs in executable logical specifications.

Abstract: While Text-to-SQL remains the dominant approach for database interaction, real-world analytics increasingly require the flexibility of general-purpose programming languages such as Python or Pandas to manage file-based data and complex analytical workflows. Despite this growing need, the reliability of Text-to-Python in core data retrieval remains underexplored relative to the mature SQL ecosystem. To address this gap, we introduce BIRD-Python, a benchmark designed for cross-paradigm evaluation. We systematically refined the original dataset to reduce annotation noise and align execution semantics, thereby establishing a consistent and standardized baseline for comparison. Our analysis reveals a fundamental paradigmatic divergence: whereas SQL leverages implicit DBMS behaviors through its declarative structure, Python requires explicit procedural logic, making it highly sensitive to underspecified user intent. To mitigate this challenge, we propose the Logic Completion Framework (LCF), which resolves ambiguity by incorporating latent domain knowledge into the generation process. Experimental results show that (1) performance differences primarily stem from missing domain context rather than inherent limitations in code generation, and (2) when these gaps are addressed, Text-to-Python achieves performance parity with Text-to-SQL. These findings establish Python as a viable foundation for analytical agents-provided that systems effectively ground ambiguous natural language inputs in executable logical specifications. Resources are available at https://anonymous.4open.science/r/Bird-Python-43B7/.

</details>


### [164] [PhysProver: Advancing Automatic Theorem Proving for Physics](https://arxiv.org/abs/2601.15737)
*Hanning Zhang,Ruida Wang,Rui Pan,Wenyuan Wang,Bingxu Meng,Tong Zhang*

Main category: cs.AI

TL;DR: First approach to enhance formal theorem proving in physics using RLVR training on PhysLeanData, achieving 2.4% improvement in physics domains and 1.3% gains on math benchmarks.


<details>
  <summary>Details</summary>
Motivation: While verifiable languages and LLMs have advanced mathematical theorem proving, formal physics reasoning has been neglected despite relying on similar problem-solving frameworks. There's a need to extend formal theorem proving capabilities to physics domains.

Method: Created PhysLeanData dataset from PhysLean theorems and conjecture-based formal data generation. Used DeepSeek-Prover-V2-7B as base model and applied Reinforcement Learning with Verifiable Rewards (RLVR) to train PhysProver.

Result: With only ~5K training samples, PhysProver achieved 2.4% overall improvement across multiple physics sub-domains. Also showed 1.3% gains on MiniF2F-Test benchmark, demonstrating generalization beyond physics and enhancement of formal math capability.

Conclusion: The approach is effective and efficient for extending formal provers outside mathematical domains, providing a paradigm for domain-specific formal reasoning enhancement. Dataset and model will be released to community.

Abstract: The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science communities because it provides a rigorous foundation for theorem proving. Recent advancements in the field provide foundation models and sophisticated agentic systems pushing the boundaries of formal mathematical reasoning to approach the natural language capability of LLMs. However, little attention has been given to the formal physics reasoning, which also heavily relies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowledge, the first approach to enhance formal theorem proving in the physics domain. We compose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean and data generated by a conjecture-based formal data generation pipeline. In the training pipeline, we leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to train our model PhysProver. Comprehensive experiments demonstrate that, using only $\sim$5K training samples, PhysProver achieves an overall 2.4\% improvement in multiple sub-domains. Furthermore, after formal physics training, we observe 1.3\% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers outside mathematical domains. To foster further research, we will release both our dataset and model to the community.

</details>


### [165] [Tabular Incremental Inference](https://arxiv.org/abs/2601.15751)
*Xinda Chen,Xing Zhen,Hanyu Zhang,Weimin Tan,Bo Yan*

Main category: cs.AI

TL;DR: Tabular Incremental Inference (TabII) enables AI models to handle dynamically changing table columns during inference by incorporating new columns without retraining, using LLM placeholders and incremental sample condensation.


<details>
  <summary>Details</summary>
Motivation: Traditional AI models trained on fixed-column tables cannot handle dynamically changing tables in real-world scenarios where columns evolve due to technological advancements, changing needs, and data integration. A new unsupervised approach is needed for practical table analysis.

Method: Frames TabII as an optimization problem using information bottleneck theory. Uses Large Language Model placeholders and Pretrained TabAdapter for external knowledge, with Incremental Sample Condensation blocks to condense task-relevant information from incremental column attributes.

Result: Experimental results on eight public datasets show TabII effectively utilizes incremental attributes and achieves state-of-the-art performance in handling dynamically changed tables.

Conclusion: TabII addresses the practical challenge of dynamically changing tables by enabling incremental column incorporation during inference, providing a more flexible and practical solution for real-world tabular data analysis.

Abstract: Tabular data is a fundamental form of data structure. The evolution of table analysis tools reflects humanity's continuous progress in data acquisition, management, and processing. The dynamic changes in table columns arise from technological advancements, changing needs, data integration, etc. However, the standard process of training AI models on tables with fixed columns and then performing inference is not suitable for handling dynamically changed tables. Therefore, new methods are needed for efficiently handling such tables in an unsupervised manner. In this paper, we introduce a new task, Tabular Incremental Inference (TabII), which aims to enable trained models to incorporate new columns during the inference stage, enhancing the practicality of AI models in scenarios where tables are dynamically changed. Furthermore, we demonstrate that this new task can be framed as an optimization problem based on the information bottleneck theory, which emphasizes that the key to an ideal tabular incremental inference approach lies in minimizing mutual information between tabular data and representation while maximizing between representation and task labels. Under this guidance, we design a TabII method with Large Language Model placeholders and Pretrained TabAdapter to provide external knowledge and Incremental Sample Condensation blocks to condense the task-relevant information given by incremental column attributes. Experimental results across eight public datasets show that TabII effectively utilizes incremental attributes, achieving state-of-the-art performance.

</details>


### [166] [Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning](https://arxiv.org/abs/2601.15761)
*Xiefeng Wu,Mingyu Hu,Shu Zhang*

Main category: cs.AI

TL;DR: SigEnt-SAC is an off-policy actor-critic RL method that learns from a single expert trajectory using a sigmoid-bounded entropy term to prevent negative-entropy-driven optimization and reduce Q-function oscillations.


<details>
  <summary>Details</summary>
Motivation: Real-world RL deployment faces challenges: sample inefficiency, sparse rewards, noisy visual observations. Existing methods need large datasets (offline-to-online) or extensive pretraining (VLA-assisted RL), making them costly and impractical for real-world applications.

Method: SigEnt-SAC introduces a sigmoid-bounded entropy term in the actor-critic framework that prevents optimization toward out-of-distribution actions and reduces Q-function oscillations. It learns from scratch using only a single expert trajectory.

Result: On D4RL benchmarks, SigEnt-SAC substantially alleviates Q-function oscillations and reaches 100% success rate faster than prior methods. On four real-world robotic tasks with raw images and sparse rewards, it learns successful policies with minimal real-world interactions.

Conclusion: SigEnt-SAC provides a low-cost, practical pathway for real-world RL deployment by enabling learning from minimal data (single expert trajectory) while maintaining stability and efficiency.

Abstract: Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.

</details>


### [167] [Agentic Confidence Calibration](https://arxiv.org/abs/2601.15778)
*Jiaxin Zhang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: The paper introduces Agentic Confidence Calibration to address AI agents' overconfidence in failure during multi-step tasks, proposing Holistic Trajectory Calibration (HTC) framework that analyzes process-level features across entire trajectories for better calibration and interpretability.


<details>
  <summary>Details</summary>
Motivation: AI agents are advancing from passive language models to autonomous systems executing complex multi-step tasks, but their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods designed for static single-turn outputs cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes.

Method: The paper introduces Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. The method uses a simple, interpretable model to analyze agent behavior throughout task execution rather than just final outputs.

Result: HTC consistently surpasses strong baselines in both calibration and discrimination across eight benchmarks, multiple LLMs, and diverse agent frameworks. It provides interpretability by revealing signals behind failure, enables transferability across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark.

Conclusion: The paper establishes a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents. The contributions move beyond traditional output-only calibration to address the unique challenges of autonomous agent systems in high-stakes deployment scenarios.

Abstract: AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.

</details>


### [168] [Creativity in the Age of AI: Rethinking the Role of Intentional Agency](https://arxiv.org/abs/2601.15797)
*James S. Pearson,Matthew J. Dennis,Marc Cheong*

Main category: cs.AI

TL;DR: The paper argues against requiring intentional agency for creativity, showing how generative AI challenges this traditional view and proposing a consistency requirement instead.


<details>
  <summary>Details</summary>
Motivation: To challenge the traditional Intentional Agency Condition (IAC) for creativity, which requires intentional agency, in light of advances in generative AI that can produce creative outputs without intentional agency.

Method: 1) Corpus analysis showing increasing acceptance of AI creativity despite lack of intentional agency; 2) Conceptual engineering approach to analyze the social function of the IAC concept.

Result: The IAC is problematic both descriptively (language use shows acceptance of AI creativity) and functionally (it distorts assessment of AI outputs). A consistency requirement that tracks reliable generation of novel and valuable products is proposed as replacement.

Conclusion: The Intentional Agency Condition should be rejected as a general condition for creativity but retained in specific local domains, replaced by a consistency requirement that better serves the social function of identifying reliable sources of novel and valuable products.

Abstract: Many theorists of creativity maintain that intentional agency is a necessary condition of creativity. We argue that this requirement, which we call the Intentional Agency Condition (IAC), should be rejected as a general condition of creativity, while retaining its relevance in specific contexts. We show that recent advances in generative AI have rendered the IAC increasingly problematic, both descriptively and functionally. We offer two reasons for abandoning it at the general level. First, we present corpus evidence indicating that authors and journalists are increasingly comfortable ascribing creativity to generative AI, despite its lack of intentional agency. This development places pressure on the linguistic intuitions that have traditionally been taken to support the IAC. Second, drawing on the method of conceptual engineering, we argue that the IAC no longer fulfils its core social function. Rather than facilitating the identification and encouragement of reliable sources of novel and valuable products, it now feeds into biases that distort our assessments of AI-generated outputs. We therefore propose replacing the IAC with a consistency requirement, according to which creativity tracks the reliable generation of novel and valuable products. Nonetheless, we explain why the IAC should be retained in specific local domains.

</details>


### [169] [VitalDiagnosis: AI-Driven Ecosystem for 24/7 Vital Monitoring and Chronic Disease Management](https://arxiv.org/abs/2601.15798)
*Zhikai Xue,Tianqianjin Lin,Pengwei Yan,Ruichun Wang,Yuxin Liu,Zhuoren Jiang,Xiaozhong Liu*

Main category: cs.AI

TL;DR: VitalDiagnosis is an LLM-driven ecosystem that transforms chronic disease management from passive monitoring to proactive engagement by integrating wearable data with LLM reasoning for anomaly detection and personalized guidance.


<details>
  <summary>Details</summary>
Motivation: Chronic diseases are the leading cause of death worldwide, exacerbated by strained medical resources and aging populations. Patients struggle with interpreting early deterioration signs and maintaining care plan adherence, creating a need for more proactive management solutions.

Method: VitalDiagnosis integrates continuous data from wearable devices with LLM reasoning capabilities. The system analyzes health triggers through context-aware inquiries, produces provisional insights within a collaborative patient-clinician workflow, and offers personalized guidance.

Result: The system addresses both acute health anomalies and routine adherence challenges, shifting chronic disease management from passive monitoring to proactive, interactive engagement.

Conclusion: VitalDiagnosis promotes a more proactive and cooperative care paradigm with potential to enhance patient self-management and reduce avoidable clinical workload through LLM-driven personalized guidance.

Abstract: Chronic diseases have become the leading cause of death worldwide, a challenge intensified by strained medical resources and an aging population. Individually, patients often struggle to interpret early signs of deterioration or maintain adherence to care plans. In this paper, we introduce VitalDiagnosis, an LLM-driven ecosystem designed to shift chronic disease management from passive monitoring to proactive, interactive engagement. By integrating continuous data from wearable devices with the reasoning capabilities of LLMs, the system addresses both acute health anomalies and routine adherence. It analyzes triggers through context-aware inquiries, produces provisional insights within a collaborative patient-clinician workflow, and offers personalized guidance. This approach aims to promote a more proactive and cooperative care paradigm, with the potential to enhance patient self-management and reduce avoidable clinical workload.

</details>


### [170] [Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification](https://arxiv.org/abs/2601.15808)
*Yuxuan Wan,Tianqing Fang,Zaitang Li,Yintong Huo,Wenxuan Wang,Haitao Mi,Dong Yu,Michael R. Lyu*

Main category: cs.AI

TL;DR: DeepVerifier: A rubrics-based verification system that enables Deep Research Agents to self-evolve at inference time by evaluating their own outputs against a failure taxonomy, improving accuracy without additional training.


<details>
  <summary>Details</summary>
Motivation: Most existing Deep Research Agent improvements focus on post-training enhancements, but there's a need for alternative paradigms that enable agents to self-improve during inference without additional training.

Method: Proposes DeepVerifier, a rubrics-based outcome reward verifier that uses an automatically constructed DRA Failure Taxonomy (5 major categories, 13 sub-categories) to evaluate agent outputs. The system leverages verification asymmetry and provides detailed rubric-based feedback for iterative bootstrapping during test-time inference.

Result: DeepVerifier outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. Test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch. Releases DeepVerifier-4K dataset with 4,646 high-quality agent steps for open-source advancement.

Conclusion: The paper introduces a novel self-evolution paradigm for Deep Research Agents through inference-time verification scaling, demonstrating significant performance improvements without additional training and contributing open-source resources for community advancement.

Abstract: Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.

</details>


### [171] [ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models](https://arxiv.org/abs/2601.15812)
*Shir Ashury-Tahan,Yifan Mai,Elron Bandel,Michal Shmueli-Scheuer,Leshem Choshen*

Main category: cs.AI

TL;DR: ErrorMap is a method to identify why LLMs fail on benchmarks, not just when they fail, by extracting failure signatures and creating ErrorAtlas - a taxonomy of model errors across 35 datasets and 83 models.


<details>
  <summary>Details</summary>
Motivation: Current LLM benchmarks only show when models fail but not why they fail, making it difficult to distinguish between formatting issues, calculation errors, dataset noise, and actual reasoning weaknesses. This incomplete evaluation cannot reliably guide model improvement.

Method: ErrorMap extracts a model's unique "failure signature" by analyzing errors across datasets. It works on any model or dataset with the same logic, disentangling different failure causes to clarify what benchmarks actually measure.

Result: Applied to 35 datasets and 83 models, ErrorMap generated ErrorAtlas - a taxonomy of model errors revealing recurring failure patterns. It identified underexplored error types like omissions of required details and question misinterpretation.

Conclusion: ErrorMap and ErrorAtlas enable advanced evaluation that exposes hidden weaknesses and directs progress, shifting focus from where models succeed to why they fail. This provides richer insights into model behavior and limitations beyond traditional task-level metrics.

Abstract: Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique "failure signature", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.

</details>


### [172] [EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience](https://arxiv.org/abs/2601.15876)
*Taofeng Xue,Chong Peng,Mianqiu Huang,Linsen Guo,Tiancheng Han,Haozhe Wang,Jianing Wang,Xiaocheng Zhang,Xin Yang,Dengchang Zhao,Jinrui Ding,Xiandi Ma,Yuchen Xie,Peng Pei,Xunliang Cai,Xipeng Qiu*

Main category: cs.AI

TL;DR: EvoCUA introduces an evolutionary learning paradigm for computer-use agents that cycles between data generation and policy optimization, achieving state-of-the-art performance on OSWorld benchmark.


<details>
  <summary>Details</summary>
Motivation: Current computer-use agents are bottlenecked by static data scaling limitations. Existing approaches relying on passive imitation of static datasets fail to capture the complex causal dynamics of long-horizon computer tasks, necessitating a more dynamic learning paradigm.

Method: EvoCUA integrates data generation and policy optimization in an evolutionary cycle: 1) verifiable synthesis engine generates diverse tasks with executable validators, 2) scalable infrastructure orchestrates thousands of asynchronous sandbox rollouts for experience acquisition, 3) iterative evolving learning strategy dynamically regulates policy updates by identifying capability boundaries and transforming failures into supervision.

Result: Achieves 56.7% success rate on OSWorld benchmark, establishing new open-source SOTA. Outperforms previous best open-source model OpenCUA-72B (45.0%) and closed-weights model UI-TARS-2 (53.1%). Shows consistent performance gains across foundation models of varying scales.

Conclusion: The evolutionary paradigm driven by learning from experience provides a robust and scalable path for advancing native agent capabilities, demonstrating generalizability across different model scales and establishing a new approach beyond static imitation learning.

Abstract: The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.

</details>


### [173] [ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search](https://arxiv.org/abs/2601.15931)
*Xiangyu Wang,Zhixin Lv,Yongjiao Sun,Anrui Han,Ye Yuan,Hangxu Ji*

Main category: cs.AI

TL;DR: ICON is a causal framework for Text-Based Person Search that uses neuro-symbolic priors to achieve geometric invariance and environmental independence by severing location shortcuts and background interference.


<details>
  <summary>Details</summary>
Motivation: Current TBPS models relying on pre-training suffer from spurious correlations and spatial semantic misalignment when transferred to complex open-world scenarios, lacking robustness against distribution shifts due to "Passive Observation" approaches.

Method: Four key components: 1) Rule-Guided Spatial Intervention to penalize bounding box noise sensitivity, 2) Counterfactual Context Disentanglement via semantic background transplantation, 3) Saliency-Driven Semantic Regularization with adaptive masking, and 4) Neuro-Symbolic Topological Alignment for structurally consistent feature matching.

Result: ICON maintains leading performance on standard benchmarks while showing exceptional robustness against occlusion, background interference, and localization noise.

Conclusion: The framework advances TBPS by shifting from fitting statistical co-occurrences to learning causal invariance through neuro-symbolic priors and topological constraints.

Abstract: Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on "Passive Observation" leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.

</details>


### [174] [Natural Language-Driven Global Mapping of Martian Landforms](https://arxiv.org/abs/2601.15949)
*Yiran Wang,Shuoyuan Wang,Zhaoran Wei,Jiannan Zhao,Zhonghua Yao,Zejian Xie,Songxin Zhang,Jun Huang,Bingyi Jing,Hongxin Wei*

Main category: cs.AI

TL;DR: MarScope is a planetary-scale vision-language framework that enables natural language-driven, label-free mapping of Martian landforms by aligning planetary images and text in a shared semantic space.


<details>
  <summary>Details</summary>
Motivation: There's a mismatch between how planetary surfaces are analyzed (using high-level semantic concepts in natural language) and how orbital image archives are organized (at the pixel level), which limits scalable, open-ended exploration of planetary surfaces.

Method: MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs, enabling natural language-driven, label-free mapping of Martian landforms.

Result: The framework enables arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978, transforming global geomorphic mapping by replacing pre-defined classifications with flexible semantic retrieval.

Conclusion: MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets, extending beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at planetary scale.

Abstract: Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.

</details>


### [175] [Decoupling Return-to-Go for Efficient Decision Transformer](https://arxiv.org/abs/2601.15953)
*Yongyi Wang,Hanyu Liu,Lingfeng Li,Bozhou Chen,Ang Li,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: DDT simplifies Decision Transformer by removing redundant RTG sequence input, using only latest RTG for action guidance, improving performance and reducing computation.


<details>
  <summary>Details</summary>
Motivation: The authors identify a critical redundancy in Decision Transformer's design: feeding the entire sequence of Return-to-Go (RTG) values into the Transformer is theoretically unnecessary since only the most recent RTG affects action prediction. This redundancy can impair DT's performance.

Method: Propose Decoupled DT (DDT) which simplifies architecture by processing only observation and action sequences through the Transformer, while using only the latest RTG to guide action prediction. This decouples the RTG from the sequence modeling.

Result: DDT significantly outperforms original DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks. The streamlined approach also reduces computational cost.

Conclusion: The redundancy in DT's RTG sequence input can impair performance, and DDT's simplified architecture with decoupled RTG guidance improves both performance and efficiency in offline reinforcement learning.

Abstract: The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT's performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.

</details>


### [176] [Deja Vu in Plots: Leveraging Cross-Session Evidence with Retrieval-Augmented LLMs for Live Streaming Risk Assessment](https://arxiv.org/abs/2601.16027)
*Yiran Qiao,Xiang Ao,Jing Chen,Yang Liu,Qiwei Zhong,Qing He*

Main category: cs.AI

TL;DR: CS-VAR is a cross-session evidence-aware retrieval-augmented detector for live streaming risk assessment that combines a lightweight domain-specific model with LLM guidance to detect recurring malicious patterns across streams while maintaining real-time efficiency.


<details>
  <summary>Details</summary>
Motivation: Live streaming platforms face complex risks like scams and coordinated malicious behaviors that accumulate gradually and recur across seemingly unrelated streams, making detection challenging due to the need to identify cross-session patterns while maintaining real-time performance.

Method: CS-VAR uses a lightweight domain-specific model for fast session-level risk inference, guided during training by an LLM that reasons over retrieved cross-session behavioral evidence. The LLM transfers its local-to-global insights to the small model, enabling it to recognize recurring patterns across streams and perform structured risk assessment.

Result: Extensive offline experiments on large-scale industrial datasets combined with online validation demonstrate state-of-the-art performance. CS-VAR provides interpretable, localized signals that effectively empower real-world moderation for live streaming.

Conclusion: CS-VAR successfully addresses live streaming risk assessment by enabling efficient cross-session pattern recognition through LLM-guided training of lightweight models, achieving both high performance and real-time deployment capabilities with interpretable results for practical moderation.

Abstract: The rise of live streaming has transformed online interaction, enabling massive real-time engagement but also exposing platforms to complex risks such as scams and coordinated malicious behaviors. Detecting these risks is challenging because harmful actions often accumulate gradually and recur across seemingly unrelated streams. To address this, we propose CS-VAR (Cross-Session Evidence-Aware Retrieval-Augmented Detector) for live streaming risk assessment. In CS-VAR, a lightweight, domain-specific model performs fast session-level risk inference, guided during training by a Large Language Model (LLM) that reasons over retrieved cross-session behavioral evidence and transfers its local-to-global insights to the small model. This design enables the small model to recognize recurring patterns across streams, perform structured risk assessment, and maintain efficiency for real-time deployment. Extensive offline experiments on large-scale industrial datasets, combined with online validation, demonstrate the state-of-the-art performance of CS-VAR. Furthermore, CS-VAR provides interpretable, localized signals that effectively empower real-world moderation for live streaming.

</details>


### [177] [Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval](https://arxiv.org/abs/2601.16038)
*Olga Bunkova,Lorenzo Di Fruscia,Sophia Rupprecht,Artur M. Schweidtmann,Marcel J. T. Reinders,Jana M. Weber*

Main category: cs.AI

TL;DR: LLMs for chemical synthesis planning often hallucinate outdated suggestions. This paper studies LLM interactions with reaction knowledge graphs via Text2Cypher generation, comparing prompting methods and a self-correction loop for reaction path retrieval.


<details>
  <summary>Details</summary>
Motivation: Standard LLM prompting for chemical synthesis planning often produces hallucinated or outdated suggestions, necessitating better methods to ground LLMs in reliable reaction knowledge graphs for accurate synthesis planning.

Method: Cast reaction path retrieval as Text2Cypher generation, define single- and multi-step retrieval tasks, compare zero-shot prompting to one-shot variants with different exemplar selection methods (static, random, embedding-based), and assess a checklist-driven validator/corrector loop.

Result: One-shot prompting with aligned exemplars consistently performs best. The checklist-style self-correction loop mainly improves executability in zero-shot settings but offers limited additional retrieval gains when good exemplars are already present.

Conclusion: The study demonstrates that one-shot prompting with aligned exemplars is most effective for KG-grounded LLM synthesis planning, and provides a reproducible Text2Cypher evaluation setup to facilitate further research in this area.

Abstract: Large Language Models (LLMs) can aid synthesis planning in chemistry, but standard prompting methods often yield hallucinated or outdated suggestions. We study LLM interactions with a reaction knowledge graph by casting reaction path retrieval as a Text2Cypher (natural language to graph query) generation problem, and define single- and multi-step retrieval tasks. We compare zero-shot prompting to one-shot variants using static, random, and embedding-based exemplar selection, and assess a checklist-driven validator/corrector loop. To evaluate our framework, we consider query validity and retrieval accuracy. We find that one-shot prompting with aligned exemplars consistently performs best. Our checklist-style self-correction loop mainly improves executability in zero-shot settings and offers limited additional retrieval gains once a good exemplar is present. We provide a reproducible Text2Cypher evaluation setup to facilitate further work on KG-grounded LLMs for synthesis planning. Code is available at https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.

</details>


### [178] [AgriPINN: A Process-Informed Neural Network for Interpretable and Scalable Crop Biomass Prediction Under Water Stress](https://arxiv.org/abs/2601.16045)
*Yue Shi,Liangxiu Han,Xin Zhang,Tam Sobeih,Thomas Gaiser,Nguyen Huu Thuy,Dominik Behrend,Amit Kumar Srivastava,Krishnagopal Halder,Frank Ewert*

Main category: cs.AI

TL;DR: AgriPINN integrates biophysical crop-growth differential equations into deep learning to predict above-ground biomass under water stress, outperforming both pure data-driven and process-based models.


<details>
  <summary>Details</summary>
Motivation: Data-driven models lack interpretability and degrade under distribution shift, while process-based models require extensive calibration and are difficult to deploy at scale. Need a hybrid approach for accurate, interpretable, and scalable crop biomass prediction.

Method: AgriPINN (process-informed neural network) integrates biophysical crop-growth differential equations as differentiable constraints within a deep learning backbone. Uses pretraining on 60 years of historical data across 397 German regions, then fine-tuning on 3 years of field experiments with controlled water treatments. Recovers latent physiological variables (LAI, PAR, RUE, water-stress factors) without direct supervision.

Result: Outperforms state-of-the-art deep learning baselines (ConvLSTM-ViT, SLTF, CNN-Transformer) and process-based LINTUL5 model with RMSE reductions up to 43%. Achieves better computational efficiency while maintaining physiological consistency.

Conclusion: AgriPINN combines scalability of deep learning with biophysical rigor of process-based modeling, providing robust and interpretable framework for spatio-temporal AGB prediction with practical applications in irrigation planning, yield forecasting, and climate adaptation.

Abstract: Accurate prediction of crop above-ground biomass (AGB) under water stress is critical for monitoring crop productivity, guiding irrigation, and supporting climate-resilient agriculture. Data-driven models scale well but often lack interpretability and degrade under distribution shift, whereas process-based crop models (e.g. DSSAT, APSIM, LINTUL5) require extensive calibration and are difficult to deploy over large spatial domains. To address these limitations, we propose AgriPINN, a process-informed neural network that integrates a biophysical crop-growth differential equation as a differentiable constraint within a deep learning backbone. This design encourages physiologically consistent biomass dynamics under water-stress conditions while preserving model scalability for spatially distributed AGB prediction. AgriPINN recovers latent physiological variables, including leaf area index (LAI), absorbed photosynthetically active radiation (PAR), radiation use efficiency (RUE), and water-stress factors, without requiring direct supervision. We pretrain AgriPINN on 60 years of historical data across 397 regions in Germany and fine-tune it on three years of field experiments under controlled water treatments. Results show that AgriPINN consistently outperforms state-of-the-art deep-learning baselines (ConvLSTM-ViT, SLTF, CNN-Transformer) and the process-based LINTUL5 model in terms of accuracy (RMSE reductions up to $43\%$) and computational efficiency. By combining the scalability of deep learning with the biophysical rigor of process-based modeling, AgriPINN provides a robust and interpretable framework for spatio-temporal AGB prediction, offering practical value for planning of irrigation infrastructure, yield forecasting, and climate-adaptation planning.

</details>


### [179] [Designing faster mixed integer linear programming algorithm via learning the optimal path](https://arxiv.org/abs/2601.16056)
*Ruizhi Liu,Liming Xu,Xulin Huang,Jingyan Sui,Shizhe Ding,Boyang Xia,Chungong Yu,Dongbo Bu*

Main category: cs.AI

TL;DR: DeepBound is a deep learning-based node selection algorithm for Mixed-Integer Linear Programming that learns to prioritize nodes containing optimal solutions, improving solving efficiency over traditional heuristic methods.


<details>
  <summary>Details</summary>
Motivation: Traditional MILP solving relies on hand-crafted heuristic strategies for node selection in branch-and-bound algorithms, which suffer from unstable and unpredictable performance across different problem instances. There's a need for more robust, automated approaches that can learn effective selection strategies from data.

Method: DeepBound uses a multi-level feature fusion network to capture node representations and employs a pairwise training paradigm to address node imbalance in branch-and-bound trees. The model learns to prioritize nodes containing optimal solutions by discriminating between nodes effectively.

Result: Extensive experiments on three NP-hard MILP benchmarks show DeepBound achieves superior solving efficiency over conventional heuristic rules and existing learning-based approaches, obtaining optimal solutions with significantly reduced computation time. It also demonstrates strong generalization capability on large, complex instances.

Conclusion: DeepBound can automatically discover more flexible and robust feature selection strategies that may effectively improve and potentially replace human-designed heuristic rules in MILP solving, offering a promising direction for automated optimization algorithm design.

Abstract: Designing faster algorithms for solving Mixed-Integer Linear Programming (MILP) problems is highly desired across numerous practical domains, as a vast array of complex real-world challenges can be effectively modeled as MILP formulations. Solving these problems typically employs the branch-and-bound algorithm, the core of which can be conceived as searching for a path of nodes (or sub-problems) that contains the optimal solution to the original MILP problem. Traditional approaches to finding this path rely heavily on hand-crafted, intuition-based heuristic strategies, which often suffer from unstable and unpredictable performance across different MILP problem instances. To address this limitation, we introduce DeepBound, a deep learning-based node selection algorithm that automates the learning of such human intuition from data. The core of DeepBound lies in learning to prioritize nodes containing the optimal solution, thereby improving solving efficiency. DeepBound introduces a multi-level feature fusion network to capture the node representations. To tackle the inherent node imbalance in branch-and-bound trees, DeepBound employs a pairwise training paradigm that enhances the model's ability to discriminate between nodes. Extensive experiments on three NP-hard MILP benchmarks demonstrate that DeepBound achieves superior solving efficiency over conventional heuristic rules and existing learning-based approaches, obtaining optimal feasible solutions with significantly reduced computation time. Moreover, DeepBound demonstrates strong generalization capability on large and complex instances. The analysis of its learned features reveals that the method can automatically discover more flexible and robust feature selection, which may effectively improve and potentially replace human-designed heuristic rules.

</details>


### [180] [Controlling Long-Horizon Behavior in Language Model Agents with Explicit State Dynamics](https://arxiv.org/abs/2601.16087)
*Sukesh Subaharan*

Main category: cs.AI

TL;DR: This paper introduces an affective subsystem with Valence-Arousal-Dominance (VAD) dynamics to improve temporal coherence in LLM agents during multi-turn dialogues.


<details>
  <summary>Details</summary>
Motivation: LLM agents often show abrupt shifts in tone and persona during extended interactions due to lack of explicit temporal structure governing agent-level state. Prior work focuses on turn-local sentiment or static emotion classification, leaving affective dynamics in long-horizon behavior underexplored.

Method: Introduces an agent-level affective subsystem maintaining continuous VAD state external to the LLM, governed by first- and second-order update rules. Affective signals are extracted using a fixed memoryless estimator and integrated via exponential smoothing or momentum-based dynamics, then injected back into generation without modifying model parameters.

Result: Stateless agents fail to show coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.

Conclusion: Imposing dynamical structure on external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue, with different dynamic models offering trade-offs between stability and responsiveness.

Abstract: Large language model (LLM) agents often exhibit abrupt shifts in tone and persona during extended interaction, reflecting the absence of explicit temporal structure governing agent-level state. While prior work emphasizes turn-local sentiment or static emotion classification, the role of explicit affective dynamics in shaping long-horizon agent behavior remains underexplored. This work investigates whether imposing dynamical structure on an external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue. We introduce an agent-level affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state external to the language model and governed by first- and second-order update rules. Instantaneous affective signals are extracted using a fixed, memoryless estimator and integrated over time via exponential smoothing or momentum-based dynamics. The resulting affective state is injected back into generation without modifying model parameters. Using a fixed 25-turn dialogue protocol, we compare stateless, first-order, and second-order affective dynamics. Stateless agents fail to exhibit coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.

</details>


### [181] [Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources](https://arxiv.org/abs/2601.16108)
*Marzieh Adeli Shamsabad,Hamed Ghodrati*

Main category: cs.AI

TL;DR: This paper addresses climate disinformation by enhancing vision-language models with external knowledge retrieval to better detect misleading images and claims.


<details>
  <summary>Details</summary>
Motivation: Climate disinformation is a growing problem with convincing misleading images/videos on social media that delay climate action. Current VLMs are limited by their training data and cannot reason about recent events or updates.

Method: Combines vision-language models with external knowledge retrieval including reverse image search results, online fact-checks, and trusted expert content to assess image-claim accuracy.

Result: The approach improves the model's ability to handle real-world climate disinformation by better assessing whether images and claims are accurate, misleading, false, or unverifiable.

Conclusion: Integrating VLMs with external knowledge sources enhances climate disinformation detection and supports efforts to protect public understanding of science in a rapidly changing information landscape.

Abstract: Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.

</details>


### [182] [LLM Prompt Evaluation for Educational Applications](https://arxiv.org/abs/2601.16134)
*Langdon Holmes,Adam Coscia,Scott Crossley,Joon Suh Choi,Wesley Morris*

Main category: cs.AI

TL;DR: Researchers developed a systematic tournament-style evaluation framework using Glicko2 ratings to compare six LLM prompt templates for generating follow-up questions in educational dialogues, finding one prompt focused on strategic reading significantly outperformed others.


<details>
  <summary>Details</summary>
Motivation: As LLMs become more common in education, there's a need for evidence-based methods to design and evaluate prompts that produce personalized, pedagogically aligned outputs, moving beyond ad-hoc prompt engineering.

Method: Created six prompt templates incorporating established prompt engineering patterns with distinct pedagogical strategies. Used tournament-style evaluation with Glicko2 rating system, eight judges evaluating question pairs across format, dialogue support, and appropriateness dimensions. Data from 120 authentic user interactions across three educational deployments.

Result: A single prompt related to strategic reading outperformed all others with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager patterns and was designed to support metacognitive learning strategies like self-directed learning.

Conclusion: The methodology demonstrates how educational technology researchers can systematically evaluate and improve prompt designs, advancing from ad-hoc prompt engineering toward evidence-based prompt development for educational applications.

Abstract: As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.

</details>


### [183] [Structured Hints for Sample-Efficient Lean Theorem Proving](https://arxiv.org/abs/2601.16172)
*Zachary Burton*

Main category: cs.AI

TL;DR: Simple inference-time prompting with tactic skeletons boosts theorem prover performance by 43% on miniF2F benchmark.


<details>
  <summary>Details</summary>
Motivation: To investigate whether highly-trained neural theorem provers like DeepSeek-Prover-V1.5 still benefit from simple structural guidance at inference time, despite their sophisticated RL training.

Method: A lightweight intervention using a fixed prompt schedule over 15 common tactic skeletons, evaluated on the miniF2F benchmark with standard sampling as baseline.

Result: 21.7% pass@16 with prompting vs 15.2% for standard sampling (43% relative improvement) using same number of samples (k=16) and same maximum generation length (1024 tokens).

Conclusion: Even capable RL-trained provers underutilize structural priors available in tactic language, and simple inference-time guidance remains a cheap, complementary boost to performance.

Abstract: State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.

</details>


### [184] [Scalable Board Expansion within a General Game System](https://arxiv.org/abs/2601.16216)
*Clémentine Sacré*

Main category: cs.AI

TL;DR: A General Game System with dynamic board expansion that grows automatically during play instead of using oversized static boards.


<details>
  <summary>Details</summary>
Motivation: Traditional boardless games use oversized static boards from the start, even though large portions may never be used, leading to unnecessary complexity.

Method: Proposes a dynamic board expansion mechanism within a General Game System where the game board grows automatically during gameplay.

Result: Not specified in the abstract (thesis work in progress).

Conclusion: Dynamic board expansion addresses the inefficiency of static oversized boards in boardless games.

Abstract: This thesis explores the use of a General Game System (GGS) to support the automatic expansion of game boards in boardless games. Traditional implementations of such games often rely on oversized static boards defined from the start, even though large portions of these boards may never be used during gameplay. This approach leads to unnecessary complexity. To address this issue, this thesis propose a dynamic board expansion mechanism in which the game board grows automatically during play.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [185] [Partially Polarized Polar Codes: A New Design for 6G Control Channels](https://arxiv.org/abs/2601.15404)
*Arman Fazeli,Mohammad M. Mansour,Ziyuan Zhu,Louay Jalloul*

Main category: cs.IT

TL;DR: PPP codes are a new polar-like code family created by selectively pruning polarization kernels from conventional polar codes to enable early access to information bits, improving early termination efficiency for blind decoding in control channels.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve blind decoding efficiency in downlink control channels where user equipment must process many candidates (most carrying no valid information). Conventional polar codes lack early-access information bits for effective early termination, especially at larger block lengths where hardware scaling is limited.

Method: PPP codes are constructed from conventional polar codes by selectively pruning polarization kernels. This modifies synthesized bit-channel capacities to guarantee a certain number of non-frozen bits available early in decoding. The paper also proposes several frozen-bitmap design strategies specifically tailored for PPP codes.

Result: PPP codes offer substantial performance gains over conventional polar codes, particularly at larger block lengths where hardware limitations restrict straightforward scaling. Compared to existing methods like aggregation or segmentation, PPP codes achieve higher efficiency without requiring additional hardware support.

Conclusion: PPP codes represent an effective solution for blind decoding scenarios in control channels, providing better early termination capabilities through selective kernel pruning and modified frozen-bitmap designs, outperforming conventional approaches while maintaining hardware efficiency.

Abstract: We introduce a new family of polar-like codes, called Partially Polarized Polar (PPP) codes. PPP codes are constructed from conventional polar codes by selectively pruning polarization kernels, thereby modifying the synthesized bit-channel capacities to ensure a guaranteed number of non-frozen bits available early in decoding. These early-access information bits enable more effective early termination, which is particularly valuable for blind decoding in downlink control channels, where user equipment (UE) must process multiple candidates, many of which carry no valid control information. Our results show that PPP codes offer substantial performance gains over conventional polar codes, particularly at larger block lengths where hardware limitations restrict straightforward scaling. Compared with existing methods such as aggregation or segmentation, PPP codes achieve higher efficiency without the need for additional hardware support. Finally, we propose several frozen-bitmap design strategies tailored to PPP codes.

</details>


### [186] [Rank-metric codes over arbitrary fields: Bounds and constructions](https://arxiv.org/abs/2601.15464)
*Alessandro Neri,Ferdinando Zullo*

Main category: cs.IT

TL;DR: Survey paper on rank-metric codes covering their mathematical foundations, bounds, constructions, and extensions beyond finite fields, with focus on MRD codes and future research directions.


<details>
  <summary>Details</summary>
Motivation: Rank-metric codes have gained importance due to applications in network coding and connections to diverse mathematical areas. The paper aims to survey their development, mathematical foundations, and extensions beyond traditional finite field settings.

Method: Survey methodology examining: (1) Singleton-like bounds on code parameters and their sharpness, (2) constructions of Maximum Rank Distance (MRD) codes over fields with cyclic Galois extensions, (3) relationship between linear rank-metric codes with systems and evasive subspaces, (4) results for algebraically closed fields and real numbers from topology and measure theory perspectives.

Result: Demonstrates sharpness of Singleton bounds in finite field cases versus contexts where bounds are not tight. Reviews existing constructions and relationships, particularly focusing on MRD codes. Synthesizes results from different mathematical contexts including topology and measure theory for algebraically closed fields and real numbers.

Conclusion: Proposes future research directions including conjectures on MRD code existence and exploration of rank-metric codes over various field extensions. Highlights the need for further investigation beyond traditional finite field settings.

Abstract: Rank-metric codes, defined as sets of matrices over a finite field with the rank distance, have gained significant attention due to their applications in network coding and connections to diverse mathematical areas. Initially studied by Delsarte in 1978 and later rediscovered by Gabidulin, these codes have become a central topic in coding theory. This paper surveys the development and mathematical foundations, in particular, regarding bounds and constructions of rank-metric codes, emphasizing their extension beyond finite fields to more general settings. We examine Singleton-like bounds on code parameters, demonstrating their sharpness in finite field cases and contrasting this with contexts where the bounds are not tight. Furthermore, we discuss constructions of Maximum Rank Distance (MRD) codes over fields with cyclic Galois extensions and the relationship between linear rank-metric codes with systems and evasive subspaces. The paper also reviews results for algebraically closed fields and real numbers, previously appearing in the context of topology and measure theory. We conclude by proposing future research directions, including conjectures on MRD code existence and the exploration of rank-metric codes over various field extensions.

</details>


### [187] [Stabilizer-Code Channel Transforms Beyond Repetition Codes for Improved Hashing Bounds](https://arxiv.org/abs/2601.15505)
*Tyler Kann,Matthieu R. Bloch,Shrinivas Kudekar,Ruediger Urbanke*

Main category: cs.IT

TL;DR: The paper presents a method to improve achievable quantum communication rates by using stabilizer codes as channel transforms, computing induced logical noise distributions, and applying hashing bound with decoder side information.


<details>
  <summary>Details</summary>
Motivation: The quantum hashing bound for memoryless Pauli channels is not generally tight, and there's a need to improve achievable rates for asymmetric Pauli channels beyond the baseline hashing bound.

Method: Generalize the induced-channel viewpoint to arbitrary stabilizer codes used as channel transforms. Construct full symplectic tableau, compute induced joint distribution of logical Pauli errors and syndromes under physical Pauli channel, and obtain achievable rate via hashing bound with decoder side information.

Result: Performed structured search over small transforms and found instances that improve the baseline hashing bound for a family of Pauli channels with skewed and independent errors studied in prior work.

Conclusion: Stabilizer codes can be effectively used as channel transforms to beat the baseline quantum hashing bound for asymmetric Pauli channels by computing induced logical noise distributions and applying hashing bound with side information.

Abstract: The quantum hashing bound guarantees that rates up to $1-H(p_I, p_X, p_Y, p_Z)$ are achievable for memoryless Pauli channels, but it is not generally tight. A known way to improve achievable rates for certain asymmetric Pauli channels is to apply a small inner stabilizer code to a few channel uses, decode, and treat the resulting logical noise as an induced Pauli channel; reapplying the hashing argument to this induced channel can beat the baseline hashing bound. We generalize this induced-channel viewpoint to arbitrary stabilizer codes used purely as channel transforms. Given any $ [\![ n, k ]\!] $ stabilizer generator set, we construct a full symplectic tableau, compute the induced joint distribution of logical Pauli errors and syndromes under the physical Pauli channel, and obtain an achievable rate via a hashing bound with decoder side information. We perform a structured search over small transforms and report instances that improve the baseline hashing bound for a family of Pauli channels with skewed and independent errors studied in prior work.

</details>


### [188] [A Class of Subadditive Information Measures and their Applications](https://arxiv.org/abs/2601.15639)
*Hamidreza Abin,Mahdi Zinati,Amin Gohari,Mohammad Hossein Yassaee,Mohammad Mahdi Mojahedian*

Main category: cs.IT

TL;DR: The paper introduces (G,f)-divergences, a two-parameter family of discrepancy measures combining a non-decreasing function G with an f-divergence, and studies their subadditivity properties with applications to information theory.


<details>
  <summary>Details</summary>
Motivation: To develop a flexible framework for information measures that extends classical f-divergences and mutual information, enabling analysis of subadditivity properties that are crucial for applications in channel coding and hypothesis testing.

Method: Define (G,f)-divergences by applying non-decreasing function G to f-divergences, then develop reduction principles showing subadditivity verification can be simplified to binary alphabets. Derive tractable sufficient conditions for subadditivity for specific G functions covering standard f-divergences.

Result: Established reduction principles for verifying subadditivity, derived sufficient conditions for subadditivity for key G functions, and demonstrated applications to finite-blocklength channel coding converses, binary hypothesis testing bounds, and extension of sphere-packing exponent framework.

Conclusion: The (G,f)-divergence framework provides a powerful generalization of classical information measures with verifiable subadditivity properties, enabling new applications in information theory including improved converses for channel coding and hypothesis testing.

Abstract: We introduce a two-parameter family of discrepancy measures, termed \emph{$(G,f)$-divergences}, obtained by applying a non-decreasing function $G$ to an $f$-divergence $D_f$. Building on Csiszár's formulation of mutual $f$-information, we define a corresponding $(G,f)$-information measure $
I_{G,f}(X;Y)$. A central theme of the paper is subadditivity over product distributions and product channels. We develop reduction principles showing that, for broad classes of $G$, it suffices to verify divergence subadditivity on binary alphabets. Specializing to the functions $G(x)\in\{x,\log(1+x),-\log(1-x)\}$, we derive tractable sufficient conditions on $f$ that guarantee subadditivity, covering many standard $f$-divergences. Finally, we present applications to finite-blocklength converses for channel coding, bounds in binary hypothesis testing, and an extension of the Shannon--Gallager--Berlekamp sphere-packing exponent framework to subadditive $(G,f)$-divergences.

</details>


### [189] [Generative AI-Empowered Semantic Twin Channel Model for ISAC](https://arxiv.org/abs/2601.15642)
*Yi Chen,Yatao Hu,Ming Li,Chong Han*

Main category: cs.IT

TL;DR: The paper proposes environmental semantics as a unifying abstraction for ISAC channel modeling, bridging the gap between communication-centric statistical models and computationally expensive deterministic models, and introduces a generative AI-powered semantic twin channel model.


<details>
  <summary>Details</summary>
Motivation: Current ISAC channel modeling has a gap: statistical models miss weak multipath signatures needed for sensing, while deterministic models are too computationally expensive for system-level evaluation. There's a need for a unifying abstraction that connects environmental meaning for sensing with channel behavior for communication.

Method: The paper establishes a semantics-oriented channel modeling principle that preserves environmental semantics while abstracting unnecessary details. It introduces a generative AI-empowered semantic twin channel model (STCM) that generates physically plausible channel realizations representative of semantic conditions.

Result: Case studies show semantic consistency under challenging multi-view settings, demonstrating practical applications for controllable simulation, dataset generation, and reproducible ISAC benchmarking.

Conclusion: Environmental semantics provides a crucial abstraction for ISAC channel modeling, balancing accuracy and complexity. The proposed semantic twin channel model offers a practical path toward future ISAC design and standardization through controllable simulation and reproducible benchmarking.

Abstract: Integrated sensing and communication (ISAC) increasingly exposes a gap in today's channel modeling. Efficient statistical models focus on coarse communication-centric metrics, and therefore miss the weak but critical multipath signatures for sensing, whereas deterministic models are computationally inefficient to scale for system-level ISAC evaluation. This gap calls for a unifying abstraction that can couple what the environment means for sensing with how the channel behaves for communication, namely, environmental semantics. This article clarifies the meaning and essentiality of environmental semantics in ISAC channel modeling and establishes how semantics is connected to observable channel structures across multiple semantic levels. Based on this perspective, a semantics-oriented channel modeling principle was advocated, which preserves environmental semantics while abstracting unnecessary detail to balance accuracy and complexity. Then, a generative AI-empowered semantic twin channel model (STCM) was introduced to generate a family of physically plausible channel realizations representative of a semantic condition. Case studies further show semantic consistency under challenging multi-view settings, suggesting a practical path to controllable simulation, dataset generation, and reproducible ISAC benchmarking toward future design and standardization.

</details>


### [190] [Generalized Information Inequalities via Submodularity, and Two Combinatorial Problems](https://arxiv.org/abs/2601.15723)
*Gunank Jakhar,Gowtham R. Kurri,Suryajith Chillara,Vinod M. Prabhakaran*

Main category: cs.IT

TL;DR: The paper extends submodularity-based information inequality frameworks, establishing convex-functional generalizations of Madiman-Tetali inequalities, deriving improved Loomis-Whitney-type projection bounds, and solving extremal graph problems using Shearer's lemma.


<details>
  <summary>Details</summary>
Motivation: To build upon existing frameworks connecting entropy inequalities and submodularity, particularly the works of Madiman & Tetali (2010) and Sason (2022), by extending their results and applying them to new domains including geometric projection inequalities and extremal graph theory.

Method: 1) Develop convex-functional generalizations of strong and weak Madiman-Tetali inequalities for submodular functions. 2) Apply a special case of the strong Madiman-Tetali inequality to derive improved Loomis-Whitney-type projection inequalities for finite point sets in ℝ^d. 3) Use Shearer's lemma (instead of Han's inequality) to study extremal graph theory problems, recovering and extending previous results.

Result: 1) New convex-functional generalizations of Madiman-Tetali inequalities. 2) Improved Loomis-Whitney-type projection inequality that incorporates slice-level structural information. 3) Solutions to extremal graph theory problems that extend previous results by Sason (2022) and Boucheron et al.

Conclusion: The work successfully extends the submodularity-based information inequality frameworks, demonstrating their applicability to diverse areas including geometric combinatorics and extremal graph theory, while providing improved bounds and new theoretical connections.

Abstract: It is well known that there is a strong connection between entropy inequalities and submodularity, since the entropy of a collection of random variables is a submodular function. Unifying frameworks for information inequalities arising from submodularity were developed by Madiman and Tetali (2010) and Sason (2022). Madiman and Tetali (2010) established strong and weak fractional inequalities that subsume classical results such as Han's inequality and Shearer's lemma. Sason (2022) introduced a convex-functional framework for generalizing Han's inequality, and derived unified inequalities for submodular and supermodular functions. In this work, we build on these frameworks and make three contributions. First, we establish convex-functional generalizations of the strong and weak Madiman and Tetali inequalities for submodular functions. Second, using a special case of the strong Madiman-Tetali inequality, we derive a new Loomis-Whitney-type projection inequality for finite point sets in $\mathbb{R}^d$, which improves upon the classical Loomis-Whitney bound by incorporating slice-level structural information. Finally, we study an extremal graph theory problem that recovers and extends the previously known results of Sason (2022) and Boucheron et al., employing Shearer's lemma in contrast to the use of Han's inequality in those works.

</details>


### [191] [Recursive Flow: A Generative Framework for MIMO Channel Estimation](https://arxiv.org/abs/2601.15767)
*Zehua Jiang,Fenghao Zhu,Chongwen Huang,Richeng Jin,Zhaohui Yang,Xiaoming Chen,Zhaoyang Zhang,Mérouane Debbah*

Main category: cs.IT

TL;DR: RC-Flow is a novel recursive flow matching method for robust channel estimation in massive MIMO systems, achieving state-of-the-art performance with 2.7 dB gain in low SNR regimes and 100x faster inference.


<details>
  <summary>Details</summary>
Motivation: Channel estimation is critical for massive MIMO system performance, but existing methods struggle with noisy, under-determined measurements, especially in noise-dominated scenarios where conventional generative models have limitations.

Method: RC-Flow combines flow matching priors with closed-loop refinement via serial restart mechanism and anchored trajectory rectification. It synergizes flow-consistent prior directions with data-fidelity proximal projections, enhanced by adaptive dual-scheduling strategy for trade-off management.

Result: RC-Flow achieves 2.7 dB performance gain in low SNR regimes compared to score-based baselines, reduces inference latency by two orders of magnitude (100x faster), and demonstrates robust channel reconstruction across diverse noise levels.

Conclusion: RC-Flow establishes a superior closed-loop framework for channel estimation that outperforms conventional generative approaches, offering both theoretical stability guarantees and practical performance improvements in challenging noise-dominated scenarios.

Abstract: Channel estimation is a fundamental challenge in massive multiple-input multiple-output systems, where estimation accuracy governs the spectral efficiency and link reliability. In this work, we introduce Recursive Flow (RC-Flow), a novel solver that leverages pre-trained flow matching priors to robustly recover channel state information from noisy, under-determined measurements. Different from conventional open-loop generative models, our approach establishes a closed-loop refinement framework via a serial restart mechanism and anchored trajectory rectification. By synergizing flow-consistent prior directions with data-fidelity proximal projections, the proposed RC-Flow achieves robust channel reconstruction and delivers state-of-the-art performance across diverse noise levels, particularly in noise-dominated scenarios. The framework is further augmented by an adaptive dual-scheduling strategy, offering flexible management of the trade-off between convergence speed and reconstruction accuracy. Theoretically, we analyze the Jacobian spectral radius of the recursive operator to prove its global asymptotic stability. Numerical results demonstrate that RC-Flow reduces inference latency by two orders of magnitude while achieving a 2.7 dB performance gain in low signal-to-noise ratio regimes compared to the score-based baseline.

</details>


### [192] [Practical applications of Set Shaping Theory to Non-Uniform Sequences](https://arxiv.org/abs/2601.15853)
*A. Schmidt,A. Vdberg,A. Petit*

Main category: cs.IT

TL;DR: SST enables data compression by mapping sequences to structured subsets with lower average information content, overcoming exponential complexity through approximate ordering for non-uniform distributions.


<details>
  <summary>Details</summary>
Motivation: Previous experimental applications of Set Shaping Theory were limited to uniformly distributed sequences due to the exponential complexity of exact ordering required for non-uniform sequences.

Method: Developed an approximate but informative ordering approach that preserves SST's structural requirements while avoiding exponential complexity, enabling application to non-uniform sequences.

Result: Successfully extended SST to non-uniform sequences, demonstrating that the shaping advantage persists beyond uniform distributions, with publicly available software for verification.

Conclusion: The approximate ordering method overcomes the practical implementation barrier for SST on non-uniform sequences, making the theory's compression benefits accessible for real-world applications.

Abstract: Set Shaping Theory (SST) moves beyond the classical fixed-space model by constructing bijective mappings the original sequence set into structured regions of a larger sequence space. These shaped subsets are characterized by a reduced average information content, measured by the product of the empirical entropy and the length, yielding (N +k)H0(f(s)) < NH0(s), which represents the universal coding limit when the source distribution is unknown. The principal experimental difficulty in applying Set Shaping Theory to non-uniform sequences arises from the need to order the sequences of both the original and transformed sets according to their information content. An exact ordering of these sets entails exponential complexity, rendering a direct implementation impractical. In this article, we show that this obstacle can be overcome by performing an approximate but informative ordering that preserves the structural requirements of SST while achieving the shaping gain predicted by the theory. This result extends previous experimental findings obtained for uniformly distributed sequences and demonstrates that the shaping advantage of SST persists for non-uniform sequences. Finally, to ensure full reproducibility, the software implementing the proposed method has been made publicly available on GitHub, enabling independent verification of the results reported in this work

</details>


### [193] [Blind Identification of Channel Codes: A Subspace-Coding Approach](https://arxiv.org/abs/2601.15903)
*Pramod Singh,Prasad Krishnan,Arti Yardi*

Main category: cs.IT

TL;DR: New method for blind channel code identification on BSC using subspace codes framework with theoretical guarantees and improved performance over existing techniques.


<details>
  <summary>Details</summary>
Motivation: Existing code identification methods require special code structures, are computationally expensive, and lack rigorous analytical performance guarantees.

Method: Minimum denoised subspace discrepancy decoder combining hamming-metric and subspace-metric decoding principles, inspired by subspace codes for operator channels.

Result: Theoretical guarantees for bounded-weight errors, error probability bound on BSC, and simulations showing improved performance for random linear codes across most channel conditions with limited received vectors.

Conclusion: Proposed decoder provides effective blind code identification with theoretical foundations and practical performance advantages over existing general-purpose techniques.

Abstract: The problem of blind identification of channel codes at a receiver involves identifying a code chosen by a transmitter from a known code-family, by observing the transmitted codewords through the channel. Most existing approaches for code-identification are contingent upon the codes in the family having some special structure, and are often computationally expensive otherwise. Further, rigorous analytical guarantees on the performance of these existing techniques are largely absent. This work presents a new method for code-identification on the binary symmetric channel (BSC), inspired by the framework of subspace codes for operator channels, carefully combining principles of hamming-metric and subspace-metric decoding. We refer to this method as the minimum denoised subspace discrepancy decoder. We present theoretical guarantees for code-identification using this decoder, for bounded-weight errors, and also present a bound on the probability of error when used on the BSC. Simulations demonstrate the improved performance of our decoder for random linear codes beyond existing general-purpose techniques, across most channel conditions and even with a limited number of received vectors.

</details>


### [194] [A Remark on Downlink Massive Random Access](https://arxiv.org/abs/2601.15928)
*Yuchen Liao,Wenyi Zhang*

Main category: cs.IT

TL;DR: Deterministic variable-length codes for downlink massive random access achieve overhead ≤ 1 + log₂e bits, improving on random coding bounds.


<details>
  <summary>Details</summary>
Motivation: In downlink massive random access (DMRA), explicitly encoding active user identities incurs significant logarithmic overhead. Recent random coding showed overhead can be bounded irrespective of total users, but deterministic constructions are needed.

Method: Recognize DMRA code design as an instance of covering arrays in combinatorics, then construct deterministic variable-length codes using this combinatorial framework.

Result: Existence proof of deterministic variable-length codes with overhead no greater than 1 + log₂e bits (approximately 2.4427 bits), improving on previous random coding bounds.

Conclusion: Deterministic code constructions for DMRA can achieve bounded overhead independent of total users, with explicit bound of 1 + log₂e bits, leveraging covering array theory from combinatorics.

Abstract: In downlink massive random access (DMRA), a base station transmits messages to a typically small subset of active users, selected randomly from a massive number of total users. Explicitly encoding the identities of active users would incur a significant overhead scaling logarithmically with the number of total users. Recently, via a random coding argument, Song, Attiah and Yu have shown that the overhead can be reduced to within some upper bound irrespective of the number of total users. In this remark, recognizing that the code design for DMRA is an instance of covering arrays in combinatorics, we show that there exists deterministic construction of variable-length codes that incur an overhead no greater than $1 + log_2 e$ bits.

</details>


### [195] [Stacked Intelligent Metasurface-Aided Wave-Domain Signal Processing: From Communications to Sensing and Computing](https://arxiv.org/abs/2601.16030)
*Jiancheng An,Chau Yuen,Marco Di Renzo,Mehdi Bennis,Merouane Debbah,Lajos Hanzo*

Main category: cs.IT

TL;DR: A comprehensive overview of Stacked Intelligent Metasurface (SIM) technology that combines neural networks, electromagnetic computing, and metasurfaces to create physical neural networks for high-speed, parallel, low-power electromagnetic signal processing.


<details>
  <summary>Details</summary>
Motivation: To explore the intersection of three cutting-edge technologies: neural networks (for feature extraction), electromagnetic computing (using wave propagation), and metasurfaces (for engineering electromagnetic waves). The goal is to create physical neural networks that can directly process electromagnetic waves for various computational tasks.

Method: The article provides a comprehensive overview of SIM technology, including: 1) Evolutionary development of SIMs, 2) Theoretical foundations and existing prototypes, 3) Optimization/training strategies from two perspectives to configure SIMs for desired functionalities, 4) Experimental exploration of applications across communication, sensing, and computing domains.

Result: SIM technology enables high-speed, massively parallel, and low-power signal processing in the electromagnetic domain. Experimental evidence shows SIMs can support multiple functions within a single device across communication, sensing, and computing applications.

Conclusion: SIMs represent an exciting avenue for electromagnetic signal processing, but critical technical challenges must be addressed for deployment in next-generation wireless networks. The article identifies these challenges and suggests promising research directions to unlock SIMs' full potential.

Abstract: Neural networks possess incredible capabilities for extracting abstract features from data. Electromagnetic computing harnesses wave propagation to execute computational operations. Metasurfaces, composed of subwavelength meta-atoms, are capable of engineering electromagnetic waves in unprecedented ways. What happens when combining these three cutting-edge technologies? This question has sparked a surge of interest in designing physical neural networks using stacked intelligent metasurface (SIM) technology, with the aim of implementing various computational tasks by directly processing electromagnetic waves. SIMs open up an exciting avenue toward high-speed, massively parallel, and low-power signal processing in the electromagnetic domain. This article provides a comprehensive overview of SIM technology, commencing with its evolutionary development. We subsequently examine its theoretical foundations and existing SIM prototypes in depth. Furthermore, the optimization/training strategies conceived to configure SIMs for achieving the desired functionalities are discussed from two different perspectives. Additionally, we explore the diverse applications of SIM technology across the communication, sensing, and computing domains, presenting experimental evidence that highlights its distinctive advantages in supporting multiple functions within a single device. Finally, we identify critical technical challenges that must be addressed to deploy SIMs in next-generation wireless networks and shed light on promising research directions to unlock their full potential.

</details>


### [196] [RIS-Aided Cooperative ISAC Network for Imaging-Based Low-Altitude Surveillance](https://arxiv.org/abs/2601.16033)
*Zhixin Chen,Yixuan Huang,Zhengze Ji,Jie Yang,Shi Jin*

Main category: cs.IT

TL;DR: Proposes RIS-aided cooperative ISAC network for low-altitude surveillance using active RIS to amplify signals and compressed sensing imaging approach.


<details>
  <summary>Details</summary>
Motivation: Low-altitude economy requires advanced surveillance, but conventional methods have high deployment costs and low signal strength limitations.

Method: Uses reconfigurable intelligent surface (RIS)-aided cooperative integrated sensing and communication (ISAC) network with active RIS for signal amplification. Models surveillance as compressed sensing imaging problem solved via subspace pursuit algorithm.

Result: Derives CRLB for the imaging system, analyzes parameter impacts, and shows active RIS outperforms passive RIS under same power constraints, achieving effective imaging up to 300 meters altitude.

Conclusion: Proposed RIS-aided ISAC network with active RIS and compressed sensing imaging provides effective low-altitude surveillance solution with improved signal strength and performance.

Abstract: The low-altitude economy is integral to the advancement of numerous sectors, necessitating the development of advanced low-altitude surveillance techniques. Nevertheless, conventional methods encounter limitations of high deployment costs and low signal strength. This study proposes a reconfigurable intelligent surface (RIS)-aided cooperative integrated sensing and communication (ISAC) network for low-altitude surveillance. This network employs RISs to reflect ISAC signals into low-altitude space for sensing. To enhance signal strength, we employ active RIS (ARIS) to amplify the signals. Moreover, in order to avoid error propagation and data association in traditional sensing methods, we model low-altitude surveillance as an imaging problem based on compressed sensing theory, which can be solved through the subspace pursuit algorithm. We derive the Cramer-Rao lower bound (CRLB) of the proposed RIS-aided low-altitude imaging system and analyze the impacts of various system parameters on sensing performance, providing guidance for ISAC system configuration. Numerical results show that ARIS outperforms passive RIS under identical power constraints, achieving effective imaging and target detection at altitudes up to 300 meters.

</details>


### [197] [Tri-Hybrid Beamforming Design for integrated Sensing and Communications](https://arxiv.org/abs/2601.16036)
*Tianyu Fang,Mengyuan Ma,Markku Juntti,Nhan Thanh Nguyen*

Main category: cs.IT

TL;DR: Tri-hybrid beamforming for ISAC improves communications SNR and sensing power with energy efficiency, using low-cost metasurface antennas in extra-large arrays.


<details>
  <summary>Details</summary>
Motivation: Enable energy-efficient communications in extra-large-scale antenna arrays using low-cost programmable metasurface antennas, while improving both communications and sensing performances for integrated sensing and communications (ISAC) applications.

Method: Formulate multi-objective optimization balancing communications SNR and sensing power at target direction, subject to power consumption and physical constraints. Develop efficient iterative algorithm with closed-form variable updates for low-complexity design.

Result: Tri-hybrid architecture improves spatial gain and energy efficiency compared to conventional hybrid beamforming, though with reduced beam alignment capability.

Conclusion: Tri-hybrid beamforming provides effective solution for ISAC systems with enhanced performance and energy efficiency, despite some trade-offs in beam alignment capability.

Abstract: Tri-hybrid beamforming architectures have been proposed to enable energy-efficient communications systems in extra-largescale antenna arrays using low-cost programmable metasurface antennas. We study the tri-hybrid beamforming design for integrated sensing and communications (ISAC) to improve both communications and sensing performances. Specifically, we formulate a multi-objective optimization problem that balances communications signal-to-noise ratio (SNR) and the sensing power at a target direction, subject to constraints on the total power consumption and physical limitations inherent to the trihybrid beamforming architecture. We develop an efficient iterative algorithm in which the variables are updated in a closed form at each iteration, leading to a low-complexity and fast-execution design. Numerical results show that the tri-hybrid architecture improves spatial gain and energy efficiency, though with reduced beam alignment capability compared to conventional hybrid beamforming architectures.

</details>


### [198] [Tensor Reed-Muller Codes: Achieving Capacity with Quasilinear Decoding Time](https://arxiv.org/abs/2601.16164)
*Emmanuel Abbe,Colin Sandon,Oscar Sprumont*

Main category: cs.IT

TL;DR: Tensor Reed-Muller codes achieve near-capacity rates with quasilinear-time decoding and exponentially small error probabilities.


<details>
  <summary>Details</summary>
Motivation: To construct error-correcting codes that combine high rates (near capacity) with efficient decoding algorithms, addressing the trade-off between rate and decoding complexity in coding theory.

Method: Define Tensor Reed-Muller codes as tensor products of Reed-Muller codes, then develop two constructions: one with t=3 achieving error probability n^{-ω(log n)} and O(n log log n) decoding, and another with t≥4 achieving error probability 2^{-n^{1/2-1/(2(t-2))-o(1)}} and O(n log n) decoding. Key tool is a polynomial-time algorithm for decoding arbitrary tensor codes from adversarial errors.

Result: For any constant rate R below capacity, Tensor Reed-Muller codes can be constructed with quasilinear decoding time. First construction (t=3) has super-polynomially small error probability n^{-ω(log n)} and O(n log log n) decoding. Second construction (t≥4) has exponentially small error probability 2^{-n^{1/2-1/(2(t-2))-o(1)}} and O(n log n) decoding.

Conclusion: Tensor Reed-Muller codes provide a practical solution to achieve near-capacity rates with efficient quasilinear-time decoding, offering a favorable trade-off between rate, error probability, and decoding complexity.

Abstract: Define the codewords of the Tensor Reed-Muller code $\mathsf{TRM}(r_1,m_1;r_2,m_2;\dots;r_t,m_t)$ to be the evaluation vectors of all multivariate polynomials in the variables $\left\{x_{ij}\right\}_{i=1,\dots,t}^{j=1,\dots m_i}$ with degree at most $r_i$ in the variables $x_{i1},x_{i2},\dots,x_{im_i}$. The generator matrix of $\mathsf{TRM}(r_1,m_1;\dots;r_t,m_t)$ is thus the tensor product of the generator matrices of the Reed-Muller codes $\mathsf{RM}(r_1,m_1),\dots, \mathsf{RM}(r_t,m_t)$.
  We show that for any constant rate $R$ below capacity, one can construct a Tensor Reed-Muller code $\mathsf{TRM}(r_1,m_1;\dotsc;r_t,m_t)$ of rate $R$ that is decodable in quasilinear time. For any blocklength $n$, we provide two constructions of such codes:
  1) Our first construction (with $t=3$) has error probability $n^{-ω(\log n)}$ and decoding time $O(n\log\log n)$.
  2) Our second construction, for any $t\geq 4$, has error probability $2^{-n^{\frac{1}{2}-\frac{1}{2(t-2)}-o(1)}}$ and decoding time $O(n\log n)$.
  One of our main tools is a polynomial-time algorithm for decoding an arbitrary tensor code $C=C_1\otimes\dotsc\otimes C_t$ from $\frac{d_{\min}(C)}{2\max\{d_{\min}(C_1),\dotsc,d_{\min}(C_t) \}}-1$ adversarial errors. Crucially, this algorithm does not require the codes $C_1,\dotsc,C_t$ to themselves be decodable in polynomial time.

</details>


### [199] [Non-Linearly Separable Distributed Computing: A Sparse Tensor Factorization Approach](https://arxiv.org/abs/2601.16171)
*Ali Khalesi,Ahmad Tanha,Derya Malak,Petros Elia*

Main category: cs.IT

TL;DR: Novel tensor-theoretic approach for distributed computing with polynomial function evaluations using tensor decomposition and multi-dimensional tiling to reduce computation and communication costs.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of efficiently computing arbitrary multi-variable polynomial evaluations of basis subfunctions in N-server distributed systems with K users, seeking to reduce both computation and communication costs.

Method: Tensor-theoretic approach representing requested functions as tensor $\bar{\mathcal{F}}$, sparse decomposition into tensor $\bar{\mathcal{E}}$ and matrix $\mathbf{D}$, using fixed-support SVD-based tensor factorization and multi-dimensional tiling of subtensors.

Result: Developed achievable scheme with derived computation and communication costs that substantially outperform state-of-the-art methods.

Conclusion: The tensor decomposition approach provides efficient task allocation and data communication protocols for distributed polynomial function evaluation, offering significant improvements over existing methods.

Abstract: The work considers the $N$-server distributed computing setting with $K$ users requesting functions that are arbitrary multi-variable polynomial evaluations of $L$ real (potentially non-linear) basis subfunctions. Our aim is to seek efficient task-allocation and data-communication techniques that reduce computation and communication costs. Towards this, we take a tensor-theoretic approach, in which we represent the requested non-linearly decomposable functions using a properly designed tensor $\bar{\mathcal{F}}$, whose sparse decomposition into a tensor $\bar{\mathcal{E}}$ and matrix $\mathbf{D}$ directly defines the task assignment, connectivity, and communication patterns. We here design an achievable scheme, employing novel fixed-support SVD-based tensor factorization methods and careful multi-dimensional tiling of subtensors, yielding computation and communication protocols whose costs are derived here, and which are shown to perform substantially better than the state of art.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [200] [Designing Persuasive Social Robots for Health Behavior Change: A Systematic Review of Behavior Change Strategies and Evaluation Methods](https://arxiv.org/abs/2601.15309)
*Jiaxin Xu,Chao Zhang,Raymond H. Cuijpers,Wijnand A. IJsselsteijn*

Main category: cs.RO

TL;DR: Systematic review analyzes behavior change strategies and evaluation methods in social robot health interventions, identifying four strategy categories and proposing future research directions.


<details>
  <summary>Details</summary>
Motivation: Social robots are increasingly used for health behavior change interventions, but there's limited actionable knowledge to guide their design and evaluation. The field needs systematic synthesis of existing approaches to inform better design and assessment practices.

Method: Conducted systematic review of HRI studies using social robots for health behavior change. Identified literature through systematic database searches and hand searches. Analyzed 39 studies to categorize behavior change strategies and evaluation methods.

Result: Identified four categories of behavior change strategies: coaching strategies, counseling strategies, social influence strategies, and persuasion-enhancing strategies. These highlight unique affordances of social robots. Also identified current evaluation practices including study designs, settings, durations, and outcome measures.

Conclusion: The review provides valuable design heuristics for social robot interventions and proposes several directions for future HRI research based on current evaluation practices. The synthesized knowledge helps advance the field of social robots in health behavior change.

Abstract: Social robots are increasingly applied as health behavior change interventions, yet actionable knowledge to guide their design and evaluation remains limited. This systematic review synthesizes (1) the behavior change strategies used in existing HRI studies employing social robots to promote health behavior change, and (2) the evaluation methods applied to assess behavior change outcomes. Relevant literature was identified through systematic database searches and hand searches. Analysis of 39 studies revealed four overarching categories of behavior change strategies: coaching strategies, counseling strategies, social influence strategies, and persuasion-enhancing strategies. These strategies highlight the unique affordances of social robots as behavior change interventions and offer valuable design heuristics. The review also identified key characteristics of current evaluation practices, including study designs, settings, durations, and outcome measures, on the basis of which we propose several directions for future HRI research.

</details>


### [201] [Preparation and Motion Study of Magnetically Driven Micro Soft Robot Mimicking the Cownose Ray](https://arxiv.org/abs/2601.15349)
*Jiaqing Chang,Song Gao,Chaowei Dong,zhaobang Li,Yang Liu*

Main category: cs.RO

TL;DR: Researchers developed a magnetically-driven micro soft robot inspired by cownose rays for underwater narrow spaces, achieving 5.25 mm/s swimming speed with wireless magnetic actuation.


<details>
  <summary>Details</summary>
Motivation: Micro soft robots have advantages in narrow underwater environments like environmental monitoring and medical procedures, but face power limitations due to miniaturization. Bionic design can improve swimming performance, and wireless magnetic actuation offers a solution for powering these small robots.

Method: Designed and fabricated a cownose ray-inspired micro soft robot using NdFeB and PDMS materials. Used a 3D Helmholtz coil to generate oscillating harmonic magnetic fields for wireless actuation. Conducted swimming experiments to explore magnetic field parameter effects, and implemented stepwise adjustment to reduce response errors.

Result: Achieved fastest swimming speed of 5.25 mm/s (0.5 body lengths per second) at B = 5 mT and f = 11 Hz. Demonstrated multiple swimming modes including straight, turning, and directional swimming through magnetic field control. Stepwise adjustment effectively reduced trajectory errors.

Conclusion: Successfully demonstrated a magnetically-driven micro soft robot method that enables wireless operation in underwater narrow spaces, providing foundation for applications in environmental monitoring and minimally invasive medical procedures.

Abstract: In narrow, unstructured underwater environments such as environmental monitoring and minimally invasive medical procedures, micro soft robots exhibit unique advantages due to their flexible movement capabilities and small size. At the same time, applying bionic technology to the structural design of micro soft robots can significantly improve their swimming performance. However, limited by their miniaturization, these robots are difficult to power internally and usually adopt a wireless power supply method. This study designs and fabricates a magnetically responsive, cownose ray-inspired micro soft robot based on the swimming principle of the cownose ray. The robot is made of a certain proportion of NdFeB and PDMS. Then, a three-dimensional Helmholtz coil is used to generate an oscillating harmonic magnetic field to conduct swimming experiments on the robot, exploring the influence of magnetic field parameters on the robot's swimming performance. The experimental results show that the swimming speed is the fastest at B = 5 mT and f = 11 Hz, reaching 5.25 mm/s, which is about 0.5 body lengths per second. In addition, by adjusting the current direction and frequency of the coil, the robot can perform different swimming modes such as straight swimming, turning swimming, and directional swimming. By employing a stepwise adjustment method, the impact of response errors on the robot's trajectory can be effectively reduced. This study demonstrates a method for magnetically driven micro soft robots, laying a foundation for the application of wireless-driven robots in underwater narrow spaces.

</details>


### [202] [Learning a Unified Latent Space for Cross-Embodiment Robot Control](https://arxiv.org/abs/2601.15419)
*Yashuai Yan,Dongheui Lee*

Main category: cs.RO

TL;DR: Cross-embodiment humanoid robot control framework using shared latent representation that unifies motion across humans and diverse humanoid platforms, enabling direct policy deployment without adaptation.


<details>
  <summary>Details</summary>
Motivation: To create a scalable framework for controlling diverse humanoid robots (single-arm, dual-arm, legged) using human motion data, overcoming challenges of different morphologies and enabling embodiment-agnostic control.

Method: Two-stage approach: 1) Build decoupled latent space using contrastive learning with tailored similarity metrics (joint rotation + end-effector positioning) for accurate motion retargeting; 2) Train goal-conditioned control policy in latent space using human data via conditional variational autoencoder that predicts latent space displacements.

Result: Policy can be directly deployed on multiple robots without adaptation, supports efficient addition of new robots via lightweight embedding layers, and enables robust, scalable, embodiment-agnostic control across diverse humanoid platforms.

Conclusion: The framework successfully enables cross-embodiment humanoid robot control through shared latent representation, demonstrating scalability, flexibility, and direct policy transfer across diverse robot morphologies.

Abstract: We present a scalable framework for cross-embodiment humanoid robot control by learning a shared latent representation that unifies motion across humans and diverse humanoid platforms, including single-arm, dual-arm, and legged humanoid robots. Our method proceeds in two stages: first, we construct a decoupled latent space that captures localized motion patterns across different body parts using contrastive learning, enabling accurate and flexible motion retargeting even across robots with diverse morphologies. To enhance alignment between embodiments, we introduce tailored similarity metrics that combine joint rotation and end-effector positioning for critical segments, such as arms. Then, we train a goal-conditioned control policy directly within this latent space using only human data. Leveraging a conditional variational autoencoder, our policy learns to predict latent space displacements guided by intended goal directions. We show that the trained policy can be directly deployed on multiple robots without any adaptation. Furthermore, our method supports the efficient addition of new robots to the latent space by learning only a lightweight, robot-specific embedding layer. The learned latent policies can also be directly applied to the new robots. Experimental results demonstrate that our approach enables robust, scalable, and embodiment-agnostic robot control across a wide range of humanoid platforms.

</details>


### [203] [Neural Collision Detection for Multi-arm Laparoscopy Surgical Robots Through Learning-from-Simulation](https://arxiv.org/abs/2601.15459)
*Sarvin Ghiasi,Majid Roshanfar,Jake Barralet,Liane S. Feldman,Amir Hooshiar*

Main category: cs.RO

TL;DR: Integrated framework combining analytical modeling, 3D simulation, and deep learning for collision detection and distance estimation in laparoscopic robotic arms.


<details>
  <summary>Details</summary>
Motivation: To enhance safety and operational efficiency of robotic arms in laparoscopic surgery by addressing challenges in collision detection and minimum distance estimation.

Method: Developed analytical model for theoretical distance calculations, created 3D simulation environment with two 7-DOF Kinova robotic arms, and trained deep neural network using joint actuators and relative positions as inputs.

Result: Deep neural network achieved mean absolute error of 282.2 mm and R-squared value of 0.85, showing close alignment between predicted and actual distances.

Conclusion: Combining analytical precision with machine learning algorithms effectively enhances precision and reliability of robotic systems for surgical applications.

Abstract: This study presents an integrated framework for enhancing the safety and operational efficiency of robotic arms in laparoscopic surgery by addressing key challenges in collision detection and minimum distance estimation. By combining analytical modeling, real-time simulation, and machine learning, the framework offers a robust solution for ensuring safe robotic operations. An analytical model was developed to estimate the minimum distances between robotic arms based on their joint configurations, offering precise theoretical calculations that serve as both a validation tool and a benchmark. To complement this, a 3D simulation environment was created to model two 7-DOF Kinova robotic arms, generating a diverse dataset of configurations for collision detection and distance estimation. Using these insights, a deep neural network model was trained with joint actuators of robot arms and relative positions as inputs, achieving a mean absolute error of 282.2 mm and an R-squared value of 0.85. The close alignment between predicted and actual distances highlights the network's accuracy and its ability to generalize spatial relationships. This work demonstrates the effectiveness of combining analytical precision with machine learning algorithms to enhance the precision and reliability of robotic systems.

</details>


### [204] [A Universal Large Language Model -- Drone Command and Control Interface](https://arxiv.org/abs/2601.15486)
*Javier N. Ramos-Silva,Peter J. Burke*

Main category: cs.RO

TL;DR: Universal LLM-drone interface using Model Context Protocol (MCP) enables natural language drone control with real-time data integration.


<details>
  <summary>Details</summary>
Motivation: Current LLM-drone interfaces require tedious, application-specific integration efforts, limiting the transformative potential of AI for drone control despite the availability of comprehensive training data (maps, weather, etc.).

Method: Developed a universal interface using the Model Context Protocol (MCP) standard with a cloud-based Linux server supporting Mavlink protocol for drone control, integrated with Google Maps MCP server for real-time navigation.

Result: Successfully demonstrated flight control of a real UAV and extensive flight planning/control capabilities in simulated drones with real-time navigation data integration.

Conclusion: Created the first universal, versatile, comprehensive, and easy-to-use drone control interface that translates natural language to drone commands, leveraging modern AI industry capabilities with drone technology.

Abstract: The use of artificial intelligence (AI) for drone control can have a transformative impact on drone capabilities, especially when real world information can be integrated with drone sensing, command, and control, part of a growing field of physical AI. Large language models (LLMs) can be advantageous if trained at scale on general knowledge, but especially and in particular when the training data includes information such as detailed map geography topology of the entire planet, as well as the ability to access real time situational data such as weather. However, challenges remain in the interface between drones and LLMs in general, with each application requiring a tedious, labor intensive effort to connect the LLM trained knowledge to drone command and control. Here, we solve that problem, using an interface strategy that is LLM agnostic and drone agnostic, providing the first universal, versatile, comprehensive and easy to use drone control interface. We do this using the new model context protocol (MCP) standard, an open standard that provides a universal way for AI systems to access external data, tools, and services. We develop and deploy a cloud based Linux machine hosting an MCP server that supports the Mavlink protocol, an ubiquitous drone control language used almost universally by millions of drones including Ardupilot and PX4 framework.We demonstrate flight control of a real unmanned aerial vehicle. In further testing, we demonstrate extensive flight planning and control capability in a simulated drone, integrated with a Google Maps MCP server for up to date, real time navigation information. This demonstrates a universal approach to integration of LLMs with drone command and control, a paradigm that leverages and exploits virtually all of modern AI industry with drone technology in an easy to use interface that translates natural language to drone control.

</details>


### [205] [CompliantVLA-adaptor: VLM-Guided Variable Impedance Action for Safe Contact-Rich Manipulation](https://arxiv.org/abs/2601.15541)
*Heng Zhang,Wei-Hsing Huang,Qiyi Tong,Gokhan Solak,Puze Liu,Sheng Liu,Jan Peters,Arash Ajoudani*

Main category: cs.RO

TL;DR: CompliantVLA-adaptor enhances VLA models with VLM-informed variable impedance control for safer contact-rich robotic manipulation by adapting stiffness/damping parameters based on task context and real-time force feedback.


<details>
  <summary>Details</summary>
Motivation: Existing VLA systems output only position commands without force-aware adaptation, leading to unsafe or failed interactions in contact-rich tasks involving compliance or uncertainty. There's a need for safer manipulation in physical tasks requiring contact.

Method: Proposes a CompliantVLA-adaptor that uses a Vision-Language Model to interpret task context from images and natural language, then adapts stiffness and damping parameters of a variable impedance controller. Real-time force/torque feedback further regulates these parameters to keep interaction forces within safe thresholds.

Result: Outperforms VLA baselines on complex contact-rich tasks in simulation and real hardware, with improved success rates and reduced force violations. Overall success rate increases from 9.86% to 17.29% across all tasks.

Conclusion: The approach presents a promising path towards safe contact-rich manipulation using VLAs by integrating vision-language context understanding with variable impedance control and force feedback regulation.

Abstract: We propose a CompliantVLA-adaptor that augments the state-of-the-art Vision-Language-Action (VLA) models with vision-language model (VLM)-informed context-aware variable impedance control (VIC) to improve the safety and effectiveness of contact-rich robotic manipulation tasks. Existing VLA systems (e.g., RDT, Pi0, OpenVLA-oft) typically output position, but lack force-aware adaptation, leading to unsafe or failed interactions in physical tasks involving contact, compliance, or uncertainty. In the proposed CompliantVLA-adaptor, a VLM interprets task context from images and natural language to adapt the stiffness and damping parameters of a VIC controller. These parameters are further regulated using real-time force/torque feedback to ensure interaction forces remain within safe thresholds. We demonstrate that our method outperforms the VLA baselines on a suite of complex contact-rich tasks, both in simulation and on real hardware, with improved success rates and reduced force violations. The overall success rate across all tasks increases from 9.86\% to 17.29\%, presenting a promising path towards safe contact-rich manipulation using VLAs. We release our code, prompts, and force-torque-impedance-scenario context datasets at https://sites.google.com/view/compliantvla.

</details>


### [206] [A Mobile Magnetic Manipulation Platform for Gastrointestinal Navigation with Deep Reinforcement Learning Control](https://arxiv.org/abs/2601.15545)
*Zhifan Yan,Chang Liu,Yiyang Jiang,Wenxuan Zheng,Xinhao Chen,Axel Krieger*

Main category: cs.RO

TL;DR: A compact mobile magnetic manipulation platform using Deep Reinforcement Learning for precise control of magnetic robots in GI drug delivery, eliminating the need for complex pre-calibrated models.


<details>
  <summary>Details</summary>
Motivation: Targeted drug delivery in the GI tract using magnetic robots is promising but faces control challenges. Stationary systems have limited workspace, while mobile systems suffer from a "model-calibration bottleneck" requiring complex, time-consuming physical models.

Method: A compact four-electromagnet array mounted on a UR5 collaborative robot controlled by a Soft Actor-Critic (SAC)-based DRL strategy trained through sim-to-real pipeline, enabling rapid deployment within 15 minutes.

Result: Achieved RMSE of 1.18 mm for square path and 1.50 mm for circular path with a 7-mm magnetic capsule. Demonstrated successful tracking over a clinically relevant 30 cm × 20 cm workspace using a 2D GI phantom.

Conclusion: The work presents a rapidly deployable, model-free control framework capable of precise magnetic manipulation in large workspaces, overcoming the model-calibration bottleneck in mobile magnetic manipulation systems.

Abstract: Targeted drug delivery in the gastrointestinal (GI) tract using magnetic robots offers a promising alternative to systemic treatments. However, controlling these robots is a major challenge. Stationary magnetic systems have a limited workspace, while mobile systems (e.g., coils on a robotic arm) suffer from a "model-calibration bottleneck", requiring complex, pre-calibrated physical models that are time-consuming to create and computationally expensive. This paper presents a compact, low-cost mobile magnetic manipulation platform that overcomes this limitation using Deep Reinforcement Learning (DRL). Our system features a compact four-electromagnet array mounted on a UR5 collaborative robot. A Soft Actor-Critic (SAC)-based control strategy is trained through a sim-to-real pipeline, enabling effective policy deployment within 15 minutes and significantly reducing setup time. We validated the platform by controlling a 7-mm magnetic capsule along 2D trajectories. Our DRL-based controller achieved a root-mean-square error (RMSE) of 1.18~mm for a square path and 1.50~mm for a circular path. We also demonstrated successful tracking over a clinically relevant, 30 cm * 20 cm workspace. This work demonstrates a rapidly deployable, model-free control framework capable of precise magnetic manipulation in a large workspace,validated using a 2D GI phantom.

</details>


### [207] [Airflow Source Seeking on Small Quadrotors Using a Single Flow Sensor](https://arxiv.org/abs/2601.15607)
*Lenworth Thomas,Tjaden Bridges,Sarah Bergbreiter*

Main category: cs.RO

TL;DR: A quadrotor system using custom flow sensors for chemical plume tracking and airflow source-seeking in confined spaces.


<details>
  <summary>Details</summary>
Motivation: Environmental disasters require better pollutant source detection. Small quadrotors can operate in human spaces but face challenges with gas sensor limitations (poor sensitivity, slow response). Need to complement chemical sensing with airflow-based source-seeking.

Method: Developed custom flow sensor that measures both airflow magnitude and direction for small quadrotors (<100g). Implemented modified 'Cast and Surge' algorithm that uses flow direction sensing to locate and navigate toward flow sources.

Result: Characterization experiments confirmed in-flight airflow detection and quadrotor reorientation toward airflow. Multiple trials with random starting positions/orientations demonstrated reliable flow source finding.

Conclusion: Provides foundation for future platforms combining flow sensors with other sensors for enhanced plume tracking data collection and source-seeking capabilities.

Abstract: As environmental disasters happen more frequently and severely, seeking the source of pollutants or harmful particulates using plume tracking becomes even more important. Plume tracking on small quadrotors would allow these systems to operate around humans and fly in more confined spaces, but can be challenging due to poor sensitivity and long response times from gas sensors that fit on small quadrotors. In this work, we present an approach to complement chemical plume tracking with airflow source-seeking behavior using a custom flow sensor that can sense both airflow magnitude and direction on small quadrotors < 100 g. We use this sensor to implement a modified version of the `Cast and Surge' algorithm that takes advantage of flow direction sensing to find and navigate towards flow sources. A series of characterization experiments verified that the system can detect airflow while in flight and reorient the quadrotor toward the airflow. Several trials with random starting locations and orientations were used to show that our source-seeking algorithm can reliably find a flow source. This work aims to provide a foundation for future platforms that can use flow sensors in concert with other sensors to enable richer plume tracking data collection and source-seeking.

</details>


### [208] [AION: Aerial Indoor Object-Goal Navigation Using Dual-Policy Reinforcement Learning](https://arxiv.org/abs/2601.15614)
*Zichen Yan,Yuchen Hou,Shenao Wang,Yichao Gao,Rui Huang,Lin Zhao*

Main category: cs.RO

TL;DR: AION is an end-to-end dual-policy RL framework for vision-based aerial ObjectNav that decouples exploration and goal-reaching behaviors, achieving superior performance in exploration, navigation efficiency, and safety without external localization or global maps.


<details>
  <summary>Details</summary>
Motivation: While ObjectNav has been studied for 2D locomotion, extending it to aerial platforms with 3D locomotion remains underexplored. Aerial robots offer superior maneuverability and search efficiency but introduce new challenges in spatial perception, dynamic control, and safety assurance.

Method: AION is an end-to-end dual-policy reinforcement learning framework that decouples exploration and goal-reaching behaviors into two specialized policies. It operates without relying on external localization or global maps, using only vision-based inputs.

Result: AION achieves superior performance across comprehensive evaluation metrics in exploration, navigation efficiency, and safety on the AI2-THOR benchmark and in real-time performance assessment using high-fidelity drone models in IsaacSim.

Conclusion: The proposed AION framework successfully addresses the challenges of aerial ObjectNav by decoupling exploration and goal-reaching behaviors, demonstrating effective performance in both simulation environments and with realistic drone models.

Abstract: Object-Goal Navigation (ObjectNav) requires an agent to autonomously explore an unknown environment and navigate toward target objects specified by a semantic label. While prior work has primarily studied zero-shot ObjectNav under 2D locomotion, extending it to aerial platforms with 3D locomotion capability remains underexplored. Aerial robots offer superior maneuverability and search efficiency, but they also introduce new challenges in spatial perception, dynamic control, and safety assurance. In this paper, we propose AION for vision-based aerial ObjectNav without relying on external localization or global maps. AION is an end-to-end dual-policy reinforcement learning (RL) framework that decouples exploration and goal-reaching behaviors into two specialized policies. We evaluate AION on the AI2-THOR benchmark and further assess its real-time performance in IsaacSim using high-fidelity drone models. Experimental results show that AION achieves superior performance across comprehensive evaluation metrics in exploration, navigation efficiency, and safety. The video can be found at https://youtu.be/TgsUm6bb7zg.

</details>


### [209] [D-Optimality-Guided Reinforcement Learning for Efficient Open-Loop Calibration of a 3-DOF Ankle Rehabilitation Robot](https://arxiv.org/abs/2601.15707)
*Qifan Hu,Branko Celler,Weidong Mu,Steven W. Su*

Main category: cs.RO

TL;DR: A two-stage calibration framework using D-optimal posture selection via PPO for efficient alignment of 3-DOF ankle rehabilitation robots.


<details>
  <summary>Details</summary>
Motivation: Accurate alignment of multi-DOF rehabilitation robots is crucial for safe and effective patient training, requiring efficient calibration methods that minimize experimental burden while maintaining precision.

Method: Two-stage framework: 1) Kronecker-product-based open-loop calibration to linearize parameter identification, 2) D-optimal posture selection using PPO agent trained in simulation to choose 4 informative postures from 50 candidates.

Result: PPO selection yields information matrix determinants >100x higher than random selection with reduced variance. Real-world tests show 4 D-optimal postures provide better cross-episode prediction consistency than 50 unstructured postures.

Conclusion: The framework improves calibration efficiency while maintaining robust parameter estimation, offering practical guidance for high-precision alignment of multi-DOF rehabilitation robots.

Abstract: Accurate alignment of multi-degree-of-freedom rehabilitation robots is essential for safe and effective patient training. This paper proposes a two-stage calibration framework for a self-designed three-degree-of-freedom (3-DOF) ankle rehabilitation robot. First, a Kronecker-product-based open-loop calibration method is developed to cast the input-output alignment into a linear parameter identification problem, which in turn defines the associated experimental design objective through the resulting information matrix. Building on this formulation, calibration posture selection is posed as a combinatorial design-of-experiments problem guided by a D-optimality criterion, i.e., selecting a small subset of postures that maximises the determinant of the information matrix. To enable practical selection under constraints, a Proximal Policy Optimization (PPO) agent is trained in simulation to choose 4 informative postures from a candidate set of 50. Across simulation and real-robot evaluations, the learned policy consistently yields substantially more informative posture combinations than random selection: the mean determinant of the information matrix achieved by PPO is reported to be more than two orders of magnitude higher with reduced variance. In addition, real-world results indicate that a parameter vector identified from only four D-optimality-guided postures provides stronger cross-episode prediction consistency than estimates obtained from a larger but unstructured set of 50 postures. The proposed framework therefore improves calibration efficiency while maintaining robust parameter estimation, offering practical guidance for high-precision alignment of multi-DOF rehabilitation robots.

</details>


### [210] [DualShield: Safe Model Predictive Diffusion via Reachability Analysis for Interactive Autonomous Driving](https://arxiv.org/abs/2601.15729)
*Rui Yang,Lei Zheng,Ruoyu Yao,Jun Ma*

Main category: cs.RO

TL;DR: DualShield is a safe planning framework that combines diffusion models with Hamilton-Jacobi reachability analysis for autonomous driving, using value functions both proactively to guide planning and reactively as a safety shield.


<details>
  <summary>Details</summary>
Motivation: Diffusion models for motion planning face practical deployment challenges: difficulty enforcing vehicle dynamics and reliance on accurate predictions of other agents, making them prone to safety issues under uncertain interactions.

Method: DualShield leverages Hamilton-Jacobi reachability value functions in two ways: 1) as proactive guidance to steer diffusion denoising toward safe/dynamically feasible regions, and 2) as reactive safety shield using control barrier-value functions to modify executed actions.

Result: Simulations in challenging unprotected U-turn scenarios show DualShield significantly improves both safety and task efficiency compared to leading methods from different planning paradigms under uncertainty.

Conclusion: The dual mechanism preserves diffusion models' rich exploration capabilities while providing principled safety assurance under uncertain and even adversarial interactions.

Abstract: Diffusion models have emerged as a powerful approach for multimodal motion planning in autonomous driving. However, their practical deployment is typically hindered by the inherent difficulty in enforcing vehicle dynamics and a critical reliance on accurate predictions of other agents, making them prone to safety issues under uncertain interactions. To address these limitations, we introduce DualShield, a planning and control framework that leverages Hamilton-Jacobi (HJ) reachability value functions in a dual capacity. First, the value functions act as proactive guidance, steering the diffusion denoising process towards safe and dynamically feasible regions. Second, they form a reactive safety shield using control barrier-value functions (CBVFs) to modify the executed actions and ensure safety. This dual mechanism preserves the rich exploration capabilities of diffusion models while providing principled safety assurance under uncertain and even adversarial interactions. Simulations in challenging unprotected U-turn scenarios demonstrate that DualShield significantly improves both safety and task efficiency compared to leading methods from different planning paradigms under uncertainty.

</details>


### [211] [Glove2UAV: A Wearable IMU-Based Glove for Intuitive Control of UAV](https://arxiv.org/abs/2601.15775)
*Amir Habel,Ivan Snegirev,Elizaveta Semenyakina,Miguel Altamirano Cabrera,Jeffrin Sam,Fawad Mehboob,Roohan Ahmed Khan,Muhammad Ahsan Mustafa,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: Glove2UAV is a wearable IMU-glove for intuitive UAV control via hand gestures with vibrotactile warnings for speed limits, validated in simulation and real flights.


<details>
  <summary>Details</summary>
Motivation: To create a safer, more intuitive UAV control interface that enables predictable interaction in dynamic flight environments through wearable technology.

Method: A lightweight wearable IMU-glove that streams inertial data in real-time, uses median-based outlier suppression and Madgwick-based orientation estimation to track hand/finger motions, maps gestures to flight controls, and provides vibrotactile warnings when speed thresholds are exceeded.

Result: The system demonstrated fast gesture-based command execution, stable coupling between gestures and UAV motion, correct operation of core command set, and timely delivery of vibrotactile warnings in both simulation and real-world flight trials.

Conclusion: Glove2UAV provides an effective, intuitive wearable interface for UAV control with integrated safety warnings, showing real-time feasibility and promising performance for gesture-based drone operation.

Abstract: This paper presents Glove2UAV, a wearable IMU-glove interface for intuitive UAV control through hand and finger gestures, augmented with vibrotactile warnings for exceeding predefined speed thresholds. To promote safer and more predictable interaction in dynamic flight, Glove2UAV is designed as a lightweight and easily deployable wearable interface intended for real-time operation. Glove2UAV streams inertial measurements in real time and estimates palm and finger orientations using a compact processing pipeline that combines median-based outlier suppression with Madgwick-based orientation estimation. The resulting motion estimations are mapped to a small set of control primitives for directional flight (forward/backward and lateral motion) and, when supported by the platform, to object-interaction commands. Vibrotactile feedback is triggered when flight speed exceeds predefined threshold values, providing an additional alert channel during operation. We validate real-time feasibility by synchronizing glove signals with UAV telemetry in both simulation and real-world flights. The results show fast gesture-based command execution, stable coupling between gesture dynamics and platform motion, correct operation of the core command set in our trials, and timely delivery of vibratile warning cues.

</details>


### [212] [A Beacon Based Solution for Autonomous UUVs GNSS-Denied Stealthy Navigation](https://arxiv.org/abs/2601.15802)
*Alexandre Albore,Humbert Fiorino,Damien Pellier*

Main category: cs.RO

TL;DR: UUVs use acoustic beacon networks for GNSS-denied covert navigation in coastal areas, with hierarchical planning for adaptive route optimization.


<details>
  <summary>Details</summary>
Motivation: Enable military and civilian covert operations in coastal areas without GNSS or support vessels, maintaining stealth in restricted environments where surfacing would expose UUVs to detection.

Method: Deploy constellation of acoustic beacons via aerial/surface drones to create synthetic landmark network; use hierarchical planner to generate adaptive routes with continuous monitoring and replanning.

Result: Precise fleet positioning and navigation from continental shelf to shore in GNSS-denied environments using acoustic localization.

Conclusion: Acoustic beacon networks with hierarchical planning enable effective covert UUV operations in GNSS-denied coastal environments.

Abstract: Autonomous Unmanned Underwater Vehicles (UUVs) enable military and civilian covert operations in coastal areas without relying on support vessels or Global Navigation Satellite Systems (GNSS). Such operations are critical when surface access is not possible and stealthy navigation is required in restricted environments such as protected zones or dangerous areas under access ban. GNSS denied navigation is then essential to maintaining concealment as surfacing could expose UUVs to detection. To ensure a precise fleet positioning a constellation of beacons deployed by aerial or surface drones establish a synthetic landmark network that will guide the fleet of UUVs along an optimized path from the continental shelf to the goal on the shore. These beacons either submerged or floating emit acoustic signals for UUV localisation and navigation. A hierarchical planner generates an adaptive route for the drones executing primitive actions while continuously monitoring and replanning as needed to maintain trajectory accuracy.

</details>


### [213] [TeNet: Text-to-Network for Compact Policy Synthesis](https://arxiv.org/abs/2601.15912)
*Ariyan Bighashdel,Kevin Sebastian Luck*

Main category: cs.RO

TL;DR: TeNet is a framework that uses text-conditioned hypernetworks to generate compact, executable robot policies from natural language instructions, enabling efficient real-time control without large end-to-end models.


<details>
  <summary>Details</summary>
Motivation: Current approaches for language-driven robot control either use high-level planning with hand-designed interfaces (limited flexibility) or large end-to-end models (difficult to deploy for real-time control). There's a need for compact, efficient policies that can follow natural language instructions while maintaining real-time performance.

Method: TeNet uses a hypernetwork conditioned on text embeddings from a pretrained LLM to generate fully executable policies. The language is used only once at policy instantiation, then the generated policy operates on low-dimensional state inputs at high frequencies. Optional grounding during training aligns text embeddings with demonstrated actions without needing demonstrations at inference.

Result: Experiments on MuJoCo and Meta-World benchmarks show TeNet produces policies orders of magnitude smaller than sequence-based baselines while achieving strong performance in both multi-task and meta-learning settings. It supports high-frequency control and demonstrates practical viability for resource-constrained robot tasks.

Conclusion: Text-conditioned hypernetworks provide a practical approach to building compact, language-driven controllers for resource-constrained robot control tasks with real-time requirements, combining LLM knowledge with efficient execution.

Abstract: Robots that follow natural-language instructions often either plan at a high level using hand-designed interfaces or rely on large end-to-end models that are difficult to deploy for real-time control. We propose TeNet (Text-to-Network), a framework for instantiating compact, task-specific robot policies directly from natural language descriptions. TeNet conditions a hypernetwork on text embeddings produced by a pretrained large language model (LLM) to generate a fully executable policy, which then operates solely on low-dimensional state inputs at high control frequencies. By using the language only once at the policy instantiation time, TeNet inherits the general knowledge and paraphrasing robustness of pretrained LLMs while remaining lightweight and efficient at execution time. To improve generalization, we optionally ground language in behavior during training by aligning text embeddings with demonstrated actions, while requiring no demonstrations at inference time. Experiments on MuJoCo and Meta-World benchmarks show that TeNet produces policies that are orders of magnitude smaller than sequence-based baselines, while achieving strong performance in both multi-task and meta-learning settings and supporting high-frequency control. These results show that text-conditioned hypernetworks offer a practical way to build compact, language-driven controllers for ressource-constrained robot control tasks with real-time requirements.

</details>


### [214] [Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems](https://arxiv.org/abs/2601.15946)
*Zijie Chen,Xiaowei Liu,Yong Xu,Shenghai Yuan,Jianping Li,Lihua Xie*

Main category: cs.RO

TL;DR: LM-Calibr enables targetless calibration for spinning LiDAR-motor systems using Denavit-Hartenberg convention, while EVA-LIO provides adaptive LiDAR-inertial odometry that adjusts to environmental features for robust localization.


<details>
  <summary>Details</summary>
Motivation: Existing methods require parameterization based on specific mounting configurations, limiting generalizability. Spinning actuated LiDAR inevitably scans featureless regions, creating challenges in balancing scanning coverage with localization robustness.

Method: Two components: 1) LM-Calibr uses Denavit-Hartenberg convention for targetless calibration supporting various mounting configurations; 2) EVA-LIO adaptively selects downsample rates and map resolutions based on spatial scale to handle featureless areas.

Result: LM-Calibr demonstrates accuracy and convergence across different scenarios, mounting angles, and initial values. EVA-LIO enables actuators to operate at maximum speed while ensuring robust localization even when scanning featureless areas.

Conclusion: The proposed methods provide generalizable calibration and robust localization for spinning actuated LiDAR systems, addressing limitations of existing approaches and improving scanning completeness while maintaining localization accuracy.

Abstract: Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \textcolor{blue}{\href{https://github.com/zijiechenrobotics/lm_calibr}{github.com/zijiechenrobotics/lm\_calibr}}. The video is available at \textcolor{blue}{\href{https://youtu.be/cZyyrkmeoSk}{youtu.be/cZyyrkmeoSk}}

</details>


### [215] [PUMA: Perception-driven Unified Foothold Prior for Mobility Augmented Quadruped Parkour](https://arxiv.org/abs/2601.15995)
*Liang Wang,Kanzhong Yao,Yang Liu,Weikai Qin,Jun Wu,Zhe Sun,Qiuguo Zhu*

Main category: cs.RO

TL;DR: PUMA is an end-to-end learning framework that integrates visual perception and foothold priors for agile quadruped parkour, enabling real-time adaptability without hierarchical controllers.


<details>
  <summary>Details</summary>
Motivation: Human athletes can perceive environmental characteristics to select appropriate footholds for obstacle traversal, but endowing legged robots with similar perceptual reasoning remains challenging. Existing methods rely on hierarchical controllers with pre-computed footholds, limiting real-time adaptability and reinforcement learning exploration.

Method: PUMA integrates visual perception and foothold priors into a single-stage training process. It leverages terrain features to estimate egocentric polar foothold priors (relative distance and heading), guiding the robot in active posture adaptation for parkour tasks.

Result: Extensive experiments in simulation and real-world environments across various discrete complex terrains demonstrate PUMA's exceptional agility and robustness in challenging parkour scenarios.

Conclusion: PUMA presents an effective end-to-end learning framework that overcomes limitations of hierarchical controllers, enabling quadruped robots to achieve human-like perceptual reasoning and adaptability for agile parkour locomotion.

Abstract: Parkour tasks for quadrupeds have emerged as a promising benchmark for agile locomotion. While human athletes can effectively perceive environmental characteristics to select appropriate footholds for obstacle traversal, endowing legged robots with similar perceptual reasoning remains a significant challenge. Existing methods often rely on hierarchical controllers that follow pre-computed footholds, thereby constraining the robot's real-time adaptability and the exploratory potential of reinforcement learning. To overcome these challenges, we present PUMA, an end-to-end learning framework that integrates visual perception and foothold priors into a single-stage training process. This approach leverages terrain features to estimate egocentric polar foothold priors, composed of relative distance and heading, guiding the robot in active posture adaptation for parkour tasks. Extensive experiments conducted in simulation and real-world environments across various discrete complex terrains, demonstrate PUMA's exceptional agility and robustness in challenging scenarios.

</details>


### [216] [Collision-Free Humanoid Traversal in Cluttered Indoor Scenes](https://arxiv.org/abs/2601.16035)
*Han Xue,Sikai Liang,Zhikai Zhang,Zicheng Zeng,Yun Liu,Yunrui Lian,Jilong Wang,Qingtao Liu,Xuesong Shi,Li Yi*

Main category: cs.RO

TL;DR: HumanoidPF enables collision-free humanoid navigation in cluttered indoor scenes by encoding obstacle relationships as motion directions, with minimal sim-to-real gap and real-world deployment via teleoperation.


<details>
  <summary>Details</summary>
Motivation: Humanoids need to traverse cluttered indoor scenes (hurdling, crouching, squeezing) but lack effective representations for humanoid-obstacle relationships during collision avoidance, making direct learning difficult.

Method: Propose Humanoid Potential Field (HumanoidPF) encoding obstacle relationships as collision-free motion directions to facilitate RL-based skill learning. Use hybrid scene generation combining realistic 3D scene crops with procedurally synthesized obstacles for training.

Result: Successfully transfer policies to real world with minimal sim-to-real gap. Develop teleoperation system allowing single-click traversal commands. Extensive experiments validate effectiveness in simulation and real-world deployment.

Conclusion: HumanoidPF effectively enables humanoid traversal in diverse cluttered indoor scenes by providing an efficient representation for obstacle relationships, demonstrating practical real-world applicability with teleoperation capabilities.

Abstract: We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: https://axian12138.github.io/CAT/.

</details>


### [217] [DextER: Language-driven Dexterous Grasp Generation with Embodied Reasoning](https://arxiv.org/abs/2601.16046)
*Junha Lee,Eunha Park,Minsu Cho*

Main category: cs.RO

TL;DR: DextER introduces contact-based embodied reasoning for dexterous grasp generation, using contact tokens as intermediate representation to bridge task semantics with physical constraints.


<details>
  <summary>Details</summary>
Motivation: Existing approaches directly map observations to grasp parameters without intermediate reasoning about physical interactions, lacking embodiment-aware understanding of hand-object contact dynamics.

Method: DextER autoregressively generates embodied contact tokens specifying which finger links contact where on object surface, followed by grasp tokens encoding hand configuration.

Result: Achieves 67.14% success rate on DexGYS, outperforming state-of-the-art by 3.83%p with 96.4% improvement in intention alignment; enables steerable generation through partial contact specification.

Conclusion: Contact-based embodied reasoning provides effective intermediate representation for dexterous grasp generation, enabling better physical understanding and fine-grained control over grasp synthesis.

Abstract: Language-driven dexterous grasp generation requires the models to understand task semantics, 3D geometry, and complex hand-object interactions. While vision-language models have been applied to this problem, existing approaches directly map observations to grasp parameters without intermediate reasoning about physical interactions. We present DextER, Dexterous Grasp Generation with Embodied Reasoning, which introduces contact-based embodied reasoning for multi-finger manipulation. Our key insight is that predicting which hand links contact where on the object surface provides an embodiment-aware intermediate representation bridging task semantics with physical constraints. DextER autoregressively generates embodied contact tokens specifying which finger links contact where on the object surface, followed by grasp tokens encoding the hand configuration. On DexGYS, DextER achieves 67.14% success rate, outperforming state-of-the-art by 3.83%p with 96.4% improvement in intention alignment. We also demonstrate steerable generation through partial contact specification, providing fine-grained control over grasp synthesis.

</details>


### [218] [Improve the autonomy of the SE2(3) group based Extended Kalman Filter for Integrated Navigation: Theoretical Analysis](https://arxiv.org/abs/2601.16062)
*Jiarui Cui,Maosong Wang,Wenqi Wu,Peiqi Li,Xianfei Pan*

Main category: cs.RO

TL;DR: The paper analyzes autonomy limitations in SE2(3) Lie group navigation models for high-precision applications, identifies Coriolis force terms as the main obstacle, and proposes a new construction method to achieve better autonomy.


<details>
  <summary>Details</summary>
Motivation: Current SE2(3) Lie group navigation models maintain error propagation autonomy only in low-precision applications (like MEMS navigation without earth rotation and biases). However, in high-precision navigation with earth rotation and inertial device biases, maintaining autonomy becomes extremely difficult, which motivates theoretical analysis and improved modeling approaches.

Method: The paper conducts theoretical analysis of SE2(3) group based high-precision navigation models under inertial, earth, and world frames. It identifies that traditional SE2(3) group navigation modeling fails due to Coriolis force terms introduced by velocity in non-inertial frames. Based on this analysis, the authors propose a new construction method for SE2(3) group navigation models that brings the models closer to full autonomy.

Result: The theoretical analysis reveals that the limitation of traditional SE2(3) group navigation modeling is the presence of Coriolis force terms caused by velocity in non-inertial frames. The proposed construction method addresses this limitation and enables navigation models to achieve better autonomy in high-precision applications.

Conclusion: The paper successfully identifies the root cause of autonomy limitations in high-precision SE2(3) navigation models (Coriolis force terms) and provides a constructive solution that improves autonomy, making SE2(3) Lie group frameworks more suitable for high-precision navigation applications with earth rotation and inertial device biases.

Abstract: One of core advantages of the SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. Current research on Lie group based extended Kalman filters has demonstrated that error propagation autonomy holds in low-precision applications, such as in micro electromechanical system (MEMS) based integrated navigation without considering earth rotation and inertial device biases. However, in high-precision navigation state estimation, maintaining autonomy is extremely difficult when considering with earth rotation and inertial device biases. This paper presents the theoretical analysis on the autonomy of SE2(3) group based high-precision navigation models under inertial, earth and world frame respectively. Through theoretical analysis, we find that the limitation of the traditional, trivial SE2(3) group navigation modeling method is that the presence of Coriolis force terms introduced by velocity in non-inertial frame. Therefore, a construction method for SE2(3) group navigation models is proposed, which brings the navigation models closer to full autonomy.

</details>


### [219] [Improve the autonomy of the SE2(3) group based Extended Kalman Filter for Integrated Navigation: Application](https://arxiv.org/abs/2601.16078)
*Jiarui Cui,Maosong Wang,Wenqi Wu,Peiqi Li,Xianfei Pan*

Main category: cs.RO

TL;DR: Experimental validation of improved SE2(3) group navigation models through real-world SINS/odometer tests and Monte-Carlo simulations


<details>
  <summary>Details</summary>
Motivation: To validate the theoretical autonomy properties of SE2(3) Lie group navigation models through practical experiments, complementing previous theoretical analysis

Method: Conducted real-world strapdown inertial navigation system (SINS)/odometer experiments and Monte-Carlo simulations to test the improved SE2(3) group navigation models

Result: Demonstrated the performance of improved SE2(3) group based high-precision navigation models in practical scenarios

Conclusion: The experimental results validate the theoretical autonomy properties and practical effectiveness of SE2(3) group navigation models for high-precision navigation applications

Abstract: One of the core advantages of SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. In the previous paper, the theoretical analysis of autonomy property of navigation model in inertial, earth and world frames was given. A construction method for SE2(3) group navigation model is proposed to improve the non-inertial navigation model toward full autonomy. This paper serves as a counterpart to previous paper and conducts the real-world strapdown inertial navigation system (SINS)/odometer(ODO) experiments as well as Monte-Carlo simulations to demonstrate the performance of improved SE2(3) group based high-precision navigation models.

</details>


### [220] [Efficiently Learning Robust Torque-based Locomotion Through Reinforcement with Model-Based Supervision](https://arxiv.org/abs/2601.16109)
*Yashuai Yan,Tobias Egle,Christian Ott,Dongheui Lee*

Main category: cs.RO

TL;DR: Integrates model-based bipedal control with residual RL for robust walking under real-world uncertainties, using a DCM planner as base policy and RL-trained residual policy supervised by privileged model-based oracle.


<details>
  <summary>Details</summary>
Motivation: Achieve robust and adaptive bipedal locomotion in real-world environments despite uncertainties like inaccurate dynamics modeling and sensor noise, enabling reliable sim-to-real transfer.

Method: Combines model-based controller (DCM trajectory planner + whole-body controller) as base policy with residual RL policy. Uses domain randomization and novel supervision from privileged model-based oracle policy (with ground-truth dynamics access) to train residual policy efficiently without extensive reward shaping.

Result: Demonstrates improved robustness and generalization across randomized conditions, offering scalable solution for sim-to-real transfer in bipedal locomotion.

Conclusion: The hybrid approach of model-based control with supervised residual RL enables efficient learning of corrective behaviors for unmodeled effects, providing robust and adaptive walking capabilities for real-world deployment.

Abstract: We propose a control framework that integrates model-based bipedal locomotion with residual reinforcement learning (RL) to achieve robust and adaptive walking in the presence of real-world uncertainties. Our approach leverages a model-based controller, comprising a Divergent Component of Motion (DCM) trajectory planner and a whole-body controller, as a reliable base policy. To address the uncertainties of inaccurate dynamics modeling and sensor noise, we introduce a residual policy trained through RL with domain randomization. Crucially, we employ a model-based oracle policy, which has privileged access to ground-truth dynamics during training, to supervise the residual policy via a novel supervised loss. This supervision enables the policy to efficiently learn corrective behaviors that compensate for unmodeled effects without extensive reward shaping. Our method demonstrates improved robustness and generalization across a range of randomized conditions, offering a scalable solution for sim-to-real transfer in bipedal locomotion.

</details>


### [221] [IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance](https://arxiv.org/abs/2601.16207)
*Jongwoo Park,Kanchana Ranasinghe,Jinhyeok Jang,Cristina Mata,Yoo Sung Jang,Michael S Ryoo*

Main category: cs.RO

TL;DR: IVRA is a training-free method that improves spatial understanding in Vision-Language-Action models by injecting affinity hints from the vision encoder into language-model layers, preserving geometric structure without retraining.


<details>
  <summary>Details</summary>
Motivation: Current VLA models flatten image patches into 1D token sequences, which weakens 2D spatial cues needed for precise manipulation tasks. This spatial information loss limits performance in manipulation tasks requiring geometric understanding.

Method: IVRA exploits affinity hints already available in the model's built-in vision encoder and selectively injects these affinity signals into language-model layers where instance-level features reside. This inference-time intervention realigns visual-token interactions to better preserve geometric structure while keeping all model parameters fixed.

Result: IVRA improves performance across diverse VLA architectures (LLaRA, OpenVLA, FLOWER) on both 2D and 3D manipulation benchmarks. On 2D VIMA, it improves average success by +4.2% over baseline LLaRA in low-data regime. On 3D LIBERO, it yields consistent gains including improvement from 96.3% to 97.1% when baseline accuracy is near saturation.

Conclusion: IVRA demonstrates that spatial understanding in VLA models can be significantly improved through lightweight, training-free interventions that leverage existing affinity information in vision encoders, enabling better geometric structure preservation for manipulation tasks.

Abstract: Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening the 2D spatial cues needed for precise manipulation. We introduce IVRA, a lightweight, training-free method that improves spatial understanding by exploiting affinity hints already available in the model's built-in vision encoder, without requiring any external encoder or retraining. IVRA selectively injects these affinity signals into a language-model layer in which instance-level features reside. This inference-time intervention realigns visual-token interactions and better preserves geometric structure while keeping all model parameters fixed. We demonstrate the generality of IVRA by applying it to diverse VLA architectures (LLaRA, OpenVLA, and FLOWER) across simulated benchmarks spanning both 2D and 3D manipulation (VIMA and LIBERO) and on various real-robot tasks. On 2D VIMA, IVRA improves average success by +4.2% over the baseline LLaRA in a low-data regime. On 3D LIBERO, it yields consistent gains over the OpenVLA and FLOWER baselines, including improvements when baseline accuracy is near saturation (96.3% to 97.1%). All code and models will be released publicly. Visualizations are available at: jongwoopark7978.github.io/IVRA

</details>


### [222] [Point Bridge: 3D Representations for Cross Domain Policy Learning](https://arxiv.org/abs/2601.16212)
*Siddhant Haldar,Lars Johannsmeier,Lerrel Pinto,Abhishek Gupta,Dieter Fox,Yashraj Narang,Ajay Mandlekar*

Main category: cs.RO

TL;DR: Point Bridge uses domain-agnostic point representations to enable zero-shot sim-to-real policy transfer without visual alignment, achieving significant performance gains with synthetic data and limited real demonstrations.


<details>
  <summary>Details</summary>
Motivation: Progress in robot foundation models is constrained by scarcity of large-scale real-world manipulation datasets. Simulation offers scalability but suffers from visual domain gap between simulation and reality.

Method: Framework combines automated point-based representation extraction via Vision-Language Models (VLMs), transformer-based policy learning, and efficient inference-time pipelines to train real-world manipulation agents using only synthetic data.

Result: Achieves up to 44% gains in zero-shot sim-to-real transfer and up to 66% with limited real data across both single-task and multitask settings, substantially outperforming prior vision-based methods.

Conclusion: Point Bridge demonstrates that unified point-based representations can effectively bridge the sim-to-real gap without explicit visual alignment, enabling scalable training of capable real-world manipulation agents.

Abstract: Robot foundation models are beginning to deliver on the promise of generalist robotic agents, yet progress remains constrained by the scarcity of large-scale real-world manipulation datasets. Simulation and synthetic data generation offer a scalable alternative, but their usefulness is limited by the visual domain gap between simulation and reality. In this work, we present Point Bridge, a framework that leverages unified, domain-agnostic point-based representations to unlock synthetic datasets for zero-shot sim-to-real policy transfer, without explicit visual or object-level alignment. Point Bridge combines automated point-based representation extraction via Vision-Language Models (VLMs), transformer-based policy learning, and efficient inference-time pipelines to train capable real-world manipulation agents using only synthetic data. With additional co-training on small sets of real demonstrations, Point Bridge further improves performance, substantially outperforming prior vision-based sim-and-real co-training methods. It achieves up to 44% gains in zero-shot sim-to-real transfer and up to 66% with limited real data across both single-task and multitask settings. Videos of the robot are best viewed at: https://pointbridge3d.github.io/

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [223] [ISAC-over-NTN: HAPS-UAV Framework for Post-Disaster Responsive 6G Networks](https://arxiv.org/abs/2601.15422)
*Berk Ciloglu,Ozgun Ersoy,Metin Ozturk,Ali Gorcin*

Main category: eess.SP

TL;DR: ISAC-over-NTN integrates UAVs and HAPS for disaster response, combining reliable communication with Doppler-based user detection using MU-MIMO beamforming.


<details>
  <summary>Details</summary>
Motivation: Terrestrial networks often fail in disasters, creating critical need for both reliable communication and situational awareness for search-and-rescue operations.

Method: Integrated sensing and communication architecture using UAVs and HAPS with innovative beamforming that combines MU-MIMO communication and monostatic sensing for simultaneous data transmission and Doppler-based mobility detection.

Result: The framework maintains reliable connectivity and achieves 90% motion detection sensitivity and 88% detection accuracy for users in critical locations.

Conclusion: ISAC-over-NTN provides an effective solution for post-disaster scenarios by integrating communication and sensing capabilities to support both connectivity and user detection needs.

Abstract: In disaster scenarios, ensuring both reliable communication and situational awareness becomes a critical challenge due to the partial or complete collapse of terrestrial networks. This paper proposes an integrated sensing and communication (ISAC) over non-terrestrial networks (NTN) architecture referred to as ISAC-over-NTN that integrates multiple uncrewed aerial vehicles (UAVs) and a high-altitude platform station (HAPS) to maintain resilient and reliable network operations in post-disaster conditions. We aim to achieve two main objectives: i) provide a reliable communication infrastructure, thereby ensuring the continuity of search-and-rescue activities and connecting people to their loved ones, and ii) detect users, such as those trapped under rubble or those who are mobile, using a Doppler-based mobility detection model. We employ an innovative beamforming method that simultaneously transmits data and detects Doppler-based mobility by integrating multi-user multiple-input multiple-output (MU-MIMO) communication and monostatic sensing within the same transmission chain. The results show that the proposed framework maintains reliable connectivity and achieves high detection accuracy of users in critical locations, reaching 90% motion detection sensitivity and 88% detection accuracy.

</details>


### [224] [Achievable Rate Optimization for Large Flexible Intelligent Metasurface Assisted Downlink MISO under Statistical CSI](https://arxiv.org/abs/2601.15471)
*Ling He,Vaibhav Kumar,Anastasios Papazafeiropoulos,Miaowen Wen,Le-Nam Tran,Marwa Chafii*

Main category: eess.SP

TL;DR: Statistical-CSI-based optimization framework for FIM-aided MISO systems outperforms rigid antenna arrays without requiring perfect instantaneous CSI.


<details>
  <summary>Details</summary>
Motivation: Existing FIM-aided systems assume perfect instantaneous CSI, which is impractical in large-scale networks due to high training overhead and complicated channel estimation. Need robust approach using statistical CSI instead.

Method: Proposed robust statistical-CSI-based optimization framework for downlink MISO systems with FIM-assisted transmitters. Developed block coordinate ascent (BCA)-based iterative algorithm to jointly optimize power allocation and FIM morphing.

Result: Simulation results show the proposed statistical-CSI-driven FIM design significantly outperforms conventional rigid antenna arrays (RAAs), validating effectiveness and practicality.

Conclusion: Statistical CSI approach enables practical FIM deployment in large-scale networks by overcoming limitations of perfect instantaneous CSI assumption, offering superior performance over traditional antenna arrays.

Abstract: The integration of electromagnetic metasurfaces into wireless communications enables intelligent control of the propagation environment. Recently, flexible intelligent metasurfaces (FIMs) have evolved beyond conventional reconfigurable intelligent surfaces (RISs), enabling three-dimensional surface deformation for adaptive wave manipulation. However, most existing FIM-aided system designs assume perfect instantaneous channel state information (CSI), which is impractical in large-scale networks due to the high training overhead and complicated channel estimation. To overcome this limitation, we propose a robust statistical-CSI-based optimization framework for downlink multiple-input single-output (MISO) systems with FIM-assisted transmitters. A block coordinate ascent (BCA)-based iterative algorithm is developed to jointly optimize power allocation and FIM morphing, maximizing the average achievable sum rate. Simulation results show that the proposed statistical-CSI-driven FIM design significantly outperforms conventional rigid antenna arrays (RAAs), validating its effectiveness and practicality.

</details>


### [225] [Applicability and Limitation Analysis of PMU Data and Phasor Concept for Low- and High- Frequency Oscillations](https://arxiv.org/abs/2601.15529)
*Bowen Ou,Bin Wang,Slava Maslennikov,Hanchao Liu,Jim Follum*

Main category: eess.SP

TL;DR: PMU phasors fail for high-frequency oscillations; proposed new signal model and estimation method using DFT, Matrix Pencil, and Least Squares; phasor concept itself invalid for asymmetric high-frequency oscillations.


<details>
  <summary>Details</summary>
Motivation: PMUs are crucial for power system monitoring but only effective for low-frequency oscillations. Current DFT-based PMU processing introduces errors, anti-aliasing filters cause attenuation, and low reporting rates limit high-frequency oscillation analysis.

Method: Proposed a more general signal model and multi-step estimation method combining one-cycle DFT, Matrix Pencil Method, and Least Squares Method to better represent and estimate waveform signals with oscillations.

Result: Numerical experiments show superior performance of proposed model and method. Revealed that phasor concept becomes invalid for high-frequency oscillations with asymmetric sub- and super-synchronous components.

Conclusion: PMU data and phasor concept have fundamental limitations for high-frequency oscillations. Need to rely on waveform data instead of phasors for analyzing high-frequency oscillations in modern power systems.

Abstract: Phasor Measurement Units (PMUs) convert high-speed waveform data into low-speed phasor data, which are fundamental to wide-area monitoring and control in power systems, with oscillation detection and localization among their most prominent applications. However, representing electrical waveform signals with oscillations using PMU phasors is effective only for low-frequency oscillations. This paper investigates the root causes of this limitation, focusing on errors introduced by Discrete Fourier Transform (DFT)-based signal processing, in addition to the attenuation effects of anti-aliasing filters, and the impact of low reporting rates. To better represent and estimate waveform signals with oscillations, we propose a more general signal model and a multi-step estimation method that leverages one-cycle DFT, the Matrix Pencil Method, and the Least Squares Method. Numerical experiments demonstrate the superior performance of the proposed signal model and estimation method. Furthermore, this paper reveals that the phasor concept, let alone PMU phasors, can become invalid for waveform signals with high-frequency oscillations characterized by asymmetric sub- and super-synchronous components. These findings highlight the fundamental limitations of PMU data and phasor concept, and emphasize the need to rely on waveform data for analyzing high-frequency oscillations in modern power systems.

</details>


### [226] [Distributed Uplink Anti-Jamming in LEO Mega-Constellations via Game-Theoretic Beamforming](https://arxiv.org/abs/2601.15557)
*Shizhen Jia,Mingjun Ying,Marco Mezzavilla,Theodore S. Rappaport,Sundeep Rangan*

Main category: eess.SP

TL;DR: Distributed multi-satellite anti-jamming strategy for LEO constellations using game theory and cooperative beamforming to counter ground-based jamming.


<details>
  <summary>Details</summary>
Motivation: LEO satellite constellations are vulnerable to ground-based jamming due to predictable orbits and exposed geometries. Traditional single-satellite interference mitigation fails against nearby jammers, requiring new approaches for NTN resilience.

Method: Model uplink interference as convex-concave game between transmitter and jammer optimizing spatial covariance matrices. Propose min-max solver with alternating best-response updates and projected gradient descent to find Nash equilibrium beamforming strategies.

Result: Distributed satellite cooperation significantly enhances resilience against jamming. While close-proximity jammers cripple single-satellite links, cooperative approach shifts capacity distribution upward under strong interference.

Conclusion: Leveraging dense connectivity and inter-satellite links of modern LEO mega-constellations enables effective distributed anti-jamming strategies, providing enhanced resilience against ground-based interference.

Abstract: Low-Earth-Orbit (LEO) satellite constellations have become vital in emerging commercial and defense Non-Terrestrial Networks (NTNs). However, their predictable orbital dynamics and exposed geometries make them highly susceptible to ground-based jamming. Traditional single-satellite interference mitigation techniques struggle to spatially separate desired uplink signals from nearby jammers, even with large antenna arrays. This paper explores a distributed multi-satellite anti-jamming strategy leveraging the dense connectivity and high-speed inter-satellite links of modern LEO mega-constellations. We model the uplink interference scenario as a convex-concave game between a desired terrestrial transmitter and a jammer, each optimizing their spatial covariance matrices to maximize or minimize achievable rate. We propose an efficient min-max solver combining alternating best-response updates with projected gradient descent, achieving fast convergence of the beamforming strategy to the Nash equilibrium. Using realistic Starlink orbital geometries and Sionna ray-tracing simulations, we demonstrate that while close-proximity jammers can cripple single-satellite links, distributed satellite cooperation significantly enhances resilience, shifting the capacity distribution upward under strong interference.

</details>


### [227] [An Iterated Hybrid Fast Parallel FIR Filter](https://arxiv.org/abs/2601.15582)
*Keshab K. Parhi*

Main category: eess.SP

TL;DR: The paper presents a novel hybrid fast parallel FIR filter design that combines transposed and direct-form 2-parallel filters in an iterated structure to reduce hardware complexity compared to prior approaches.


<details>
  <summary>Details</summary>
Motivation: To enhance computational efficiency and throughput in DSP applications by developing improved parallel FIR filter architectures that reduce hardware complexity while maintaining performance.

Method: Uses polyphase decomposition and iterated fast FIR algorithms (FFAs) to create a hybrid filter that iterates a transposed 2-parallel fast FIR filter in inner layers and a direct-form 2-parallel fast FIR filter in the outermost layer.

Result: The hybrid fast parallel filters require fewer additions compared to prior approaches, demonstrating reduced hardware complexity.

Conclusion: The novel hybrid iterated approach provides an efficient parallel FIR filter design with lower hardware requirements than existing methods, offering improved performance for DSP applications.

Abstract: This paper revisits the design and optimization of parallel fast finite impulse response (FIR) filters using polyphase decomposition and iterated fast FIR algorithms (FFAs). Parallel FIR filtering enhances computational efficiency and throughput in digital signal processing (DSP) applications by enabling the simultaneous processing of multiple input samples. We revisit a prior approach to design of fast parallel filter architectures by using the iterated FFA approach where the same primitive filter, such as 2-parallel, is iterated to design the fast parallel filter. In this paper, we present yet another novel iterated fast parallel FIR filter, referred to as the fast hybrid filter. The hybrid filter iterates a transposed 2-parallel fast FIR filter in all the inner layers and a direct-form 2-parallel fast FIR filter in the outermost layer, resulting in reduced hardware complexity. Such an iterated hybrid approach has not been presented before. We show that the hybrid fast parallel filters require less number of additions compared to prior approaches.

</details>


### [228] [Amalgamated CHIRP and OFDM for ISAC](https://arxiv.org/abs/2601.15584)
*Pankaj Kumar,Mohammed El-Hajjar,Ibrahim A. Hemadeh,Yasser Mestrah,Suraj Srivastava,Aditya K. Jagannatham,Lajos Hanzo*

Main category: eess.SP

TL;DR: Proposes a novel OFDM-chirp hybrid waveform for ISAC that reduces PAPR while improving sensing accuracy without consuming communication resources.


<details>
  <summary>Details</summary>
Motivation: ISAC needs waveforms that efficiently support both communication and sensing. Traditional OFDM has high PAPR and requires resource allocation for sensing, which reduces communication performance.

Method: Affine addition of OFDM and chirp signals creates near constant-envelope waveform. Novel slot-level integration of chirp into OFDM framework enhances range estimation accuracy without consuming communication resources.

Result: The hybrid waveform shows better autocorrelation properties, improved RMSE for range and velocity estimation, lower PAPR, and preserves communication efficiency while enabling sensing.

Conclusion: The proposed OFDM-chirp hybrid effectively addresses key ISAC challenges by reducing PAPR, improving sensing accuracy without resource overhead, and characterizing the communication-sensing trade-off.

Abstract: Integrated Sensing and Communication (ISAC) requires the development of a waveform capable of efficiently supporting both communication and sensing functionalities. This paper proposes a novel waveform that combines the benefits of both the orthogonal frequency division multiplexing (OFDM) and the chirp waveforms to improve both the communication and sensing performance within an ISAC framework. Hence, a new architecture is proposed that utilizes the conventional communication framework while leveraging the parameters sensed at the receiver (Rx) for enhancing the communication performance. We demonstrate that the affine addition of OFDM and chirp signals results in a near constant-envelope OFDM waveform, which effectively reduces the peak-to-average power ratio (PAPR), a key limitation of traditional OFDM systems. Using the OFDM framework for sensing in the conventional fashion requires the allocation of some resources for sensing, which in turn reduces communication performance. As a remedy, the proposed affine amalgam facilitates sensing through the chirp waveform without consuming communication resources, thereby preserving communication efficiency. Furthermore, a novel technique of integrating the chirp signal into the OFDM framework at the slot-level is proposed to enhance the accuracy of range estimation. The results show that the OFDM signal incorporated with chirp has better autocorrelation properties, improved root mean square error (RMSE) of range and velocity, and lower PAPR. Finally, we characterize the trade-off between communications and sensing performance.

</details>


### [229] [Does 6G Need a New Waveform: Comparing Zak-OTFS with CP-OFDM](https://arxiv.org/abs/2601.15602)
*Imran Ali Khan,Saif Khan Mohammed,Ronny Hadani,Ananthanarayanan Chockalingam,Robert Calderbank,Anton Monk,Shachar Kons,Shlomo Rakib,Yoav Hebron*

Main category: eess.SP

TL;DR: Comprehensive performance comparison between CP-OFDM and Zak-OTFS waveforms across 6G environments, highlighting architectural trade-offs between preventing vs. embracing inter-carrier interference.


<details>
  <summary>Details</summary>
Motivation: With growing interest in new waveforms like Zak-OTFS and emerging over-the-air implementations, there's a need to understand the fundamental architectural choice between OFDM (which prevents ICI) and Zak-OTFS (which embraces ICI), particularly for 6G use cases with high delay/Doppler spreads.

Method: The paper conducts a comprehensive performance comparison of cyclic prefix OFDM (CP-OFDM) and Zak-OTFS across the full range of 6G propagation environments, analyzing their characteristics in different channel conditions.

Result: Zak-OTFS exhibits superior performance in doubly-spread 6G use cases with high delay/Doppler channel spreads (high mobility and/or large cells), while the architectural choice depends on typical use cases which vary geographically based on cell sizes and propagation conditions.

Conclusion: The choice between OFDM and Zak-OTFS represents a fundamental architectural decision between preventing inter-carrier interference (OFDM) versus embracing it (Zak-OTFS), with Zak-OTFS being particularly advantageous for 6G scenarios with high mobility and large cells.

Abstract: Across the world, there is growing interest in new waveforms, Zak-OTFS in particular, and over-the-air implementations are starting to appear. The choice between OFDM and Zak-OTFS is not so much a choice between waveforms as it is an architectural choice between preventing inter-carrier interference (ICI) and embracing ICI. In OFDM, once the Input-Output (I/O) relation is known, equalization is relatively simple, at least when there is no ICI. However, in the presence of ICI the I/O relation is non-predictable and its acquisition is non-trivial. In contrast, equalization is more involved in Zak-OTFS due to inter-symbol-interference (ISI), however the I/O relation is predictable and its acquisition is simple. {Zak-OTFS exhibits superior performance in doubly-spread 6G use cases with high delay/Doppler channel spreads (i.e., high mobility and/or large cells), but architectural choice is governed by the typical use case, today and in the future. What is typical depends to some degree on geography, since large delay spread is a characteristic of large cells which are the rule rather than the exception in many important wireless markets.} This paper provides a comprehensive performance comparison of cyclic prefix OFDM (CP-OFDM) and Zak-OTFS across the full range of 6G propagation environments. The performance results provide insights into the fundamental architectural choice.

</details>


### [230] [Bistatic ISAC: Practical Challenges and Solutions](https://arxiv.org/abs/2601.15733)
*Lucas Giroto,Marcus Henninger,Alexander Felix,Maximilian Bauhofer,Taewon Jeong,Umut Utku Erdem,Stephan ten Brink,Thomas Zwick,Benjamin Nuss,Silvio Mandelli*

Main category: eess.SP

TL;DR: This paper addresses practical challenges and solutions for bistatic ISAC in 6G networks using OFDM waveforms, focusing on system design, hardware impairment mitigation, synchronization techniques, and multi-dimensional sensing parameter estimation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address practical implementation challenges in bistatic integrated sensing and communication systems for 6G networks, particularly dealing with hardware impairments while achieving both communication and sensing performance requirements.

Method: The paper uses orthogonal frequency-division multiplexing (OFDM) waveforms and discusses system design approaches to balance sensing KPIs with hardware impairment limitations. It also presents signal processing techniques for over-the-air synchronization and generation of periodograms with range, Doppler shift, and angular information.

Result: Simulation results are presented for a cellular-based ISAC scenario using parameters compliant with current 5G standards, demonstrating the feasibility of the proposed approaches. The paper also identifies open challenges for future deployments.

Conclusion: The paper concludes that practical bistatic ISAC in 6G networks is feasible with proper system design and signal processing techniques, but identifies several open challenges that need to be addressed for future deployments.

Abstract: This article presents and discusses challenges and solutions for practical issues in bistatic integrated sensing and communication (ISAC) in 6G networks. Considering orthogonal frequency-division multiplexing as the adopted waveform, a discussion on system design aiming to achieve both a desired sensing key performance indicators and limit the impact of hardware impairments is presented. In addition, signal processing techniques to enable over-the-air synchronization and generation of periodograms with range, Doppler shift, and angular information are discussed. Simulation results are then presented for a cellular-based ISAC scenario considering system parameterization compliant to current 5G and, finally, a discussion on open challenges for future deployments is presented.

</details>


### [231] [Joint Pilot and Unknown Data-based Localization for OFDM Opportunistic Radar Systems](https://arxiv.org/abs/2601.15785)
*Mathieu Reniers,Martin Willame,Jérôme Louveaux,Luc Vandendorpe*

Main category: eess.SP

TL;DR: Novel method extracts positioning info from data payloads without decoding, using FFT for efficient implementation in ISAC systems.


<details>
  <summary>Details</summary>
Motivation: Existing ISAC approaches either use only pilots (ignoring data payloads) or rely on data decisions (limiting positioning to communication performance). Need to extract positioning info from data without decoding.

Method: Proposed method extracts positioning information from data payloads without decoding them. Uses opportunistic scenario where communication signals from user are captured by radar with Uniform Linear Array. Efficiently implemented using Fast Fourier Transforms.

Result: Demonstrates superior localization performance compared to existing methods through numerical simulations.

Conclusion: Novel approach overcomes limitations of existing ISAC positioning methods by extracting positioning info from data payloads without decoding, enabling better localization performance.

Abstract: Integrated Sensing and Communications (ISAC) has emerged as a promising paradigm for Sixth Generation (6G) and Wi-Fi 7 networks, with the communication-centric approach being particularly attractive due to its compatibility with current standards. Typical communication signals comprise both deterministic known pilot signals and random unknown data payloads. Most existing approaches either rely solely on pilots for positioning, thereby ignoring the radar information present in the received data symbols that constitute the majority of each frame, or rely on data decisions, which bounds positioning performance to that of the communication system. To overcome these limitations, we propose a novel method that extracts positioning information from data payloads without decoding them. We consider an opportunistic scenario in which communication signals from a user are captured by an opportunistic radar equipped with a Uniform Linear Arrays of antennas. We show that, in this setting, the estimation can be efficiently implemented using Fast Fourier Transforms. Finally, we demonstrate superior localization performance compared to existing methods in the literature through numerical simulations.

</details>


### [232] [Adaptive Non-Uniform Sampling of Bandlimited Signals via Algorithm-Encoder Co-Design](https://arxiv.org/abs/2601.15790)
*Kaluguri Yashaswini,Anshu Arora,Satish Mulleti*

Main category: eess.SP

TL;DR: Adaptive non-uniform sampling framework for bandlimited signals using algorithm-encoder co-design with variable-bias integrate-and-fire time encoding machine (VBT-IF-TEM) that enables sparse sampling in slowly varying regions while maintaining reconstruction accuracy.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of classical uniform sampling (Nyquist rate) by enabling adaptive sampling density based on local signal characteristics, allowing sparse sampling in slowly varying regions while maintaining reconstruction accuracy.

Method: Algorithm-encoder co-design with local energy-based convergence condition; variable-bias, variable-threshold integrate-and-fire time encoding machine (VBT-IF-TEM) with shifted-signal formulation for robustness; iterative reconstruction algorithm.

Result: Substantial reduction in sampling density compared to uniform sampling and conventional IF-TEMs while maintaining accurate reconstruction; demonstrated on synthetic, ultrasonic guided-wave, and ECG signals; controllable tradeoff between sampling density, accuracy, and convergence.

Conclusion: Proposed adaptive non-uniform sampling framework enables efficient signal acquisition by adaptively adjusting sampling density based on local signal variation, achieving significant sampling reduction while preserving reconstruction quality through algorithm-encoder co-design.

Abstract: We propose an adaptive non-uniform sampling framework for bandlimited signals based on an algorithm-encoder co-design perspective. By revisiting the convergence analysis of iterative reconstruction algorithms for non-uniform measurements, we derive a local, energy-based sufficient condition that governs reconstruction behavior as a function of the signal and derivative energies within each sampling interval. Unlike classical approaches that impose a global Nyquist-type bound on the inter-sample spacing, the proposed condition permits large gaps in slowly varying regions while enforcing denser sampling only where the signal exhibits rapid temporal variation. Building on this theoretical insight, we design a variable-bias, variable-threshold integrate-and-fire time encoding machine (VBT-IF-TEM) whose firing mechanism is explicitly shaped to enforce the derived local convergence condition. To ensure robustness, a shifted-signal formulation is introduced to suppress excessive firing in regions where the magnitude of the signal amplitude is close to zero or the local signal energy approaches zero. Using the proposed encoder, an analog signal is discretely represented by time encodings and signal averages, enabling perfect reconstruction via a standard iterative algorithm even when the local sampling rate falls below the Nyquist rate. Simulation results on synthetic signals and experiments on ultrasonic guided-wave and ECG signals demonstrate that the proposed framework achieves substantial reductions in sampling density compared to uniform sampling and conventional IF-TEMs, while maintaining accurate reconstruction. The results further highlight a controllable tradeoff between sampling density, reconstruction accuracy, and convergence behavior, which can be navigated through adaptive parameter selection.

</details>


### [233] [Dual-Mapping Sparse Vector Transmission for Short Packet URLLC](https://arxiv.org/abs/2601.15819)
*Yanfeng Zhang,Xu Zhu,Jinkai Zheng,Weiwei Yang,Xianhua Yu,Haiyong Zeng,Yujie Liu,Yong Liang Guan*

Main category: eess.SP

TL;DR: DM-SVC improves SVC performance for URLLC by using dual mapping (block + single-element sparse patterns) and two-stage decoding.


<details>
  <summary>Details</summary>
Motivation: SVC is promising for URLLC short-packet transmission but needs further performance enhancement for next-generation systems.

Method: Proposes dual-mapping SVC with block sparse mapping (concentrates power) and single-element sparse mapping (controls code length), plus two-stage decoding algorithm.

Result: DM-SVC outperforms existing SVC schemes in block error rate and spectral efficiency based on extensive simulations.

Conclusion: DM-SVC enhances SVC transmission performance for URLLC applications through efficient dual-mapping approach.

Abstract: Sparse vector coding (SVC) is a promising short-packet transmission method for ultra reliable low latency communication (URLLC) in next generation communication systems. In this paper, a dual-mapping SVC (DM-SVC) based short packet transmission scheme is proposed to further enhance the transmission performance of SVC. The core idea behind the proposed scheme lies in mapping the transmitted information bits onto sparse vectors via block and single-element sparse mappings. The block sparse mapping pattern is able to concentrate the transmit power in a small number of non-zero blocks thus improving the decoding accuracy, while the single-element sparse mapping pattern ensures that the code length does not increase dramatically with the number of transmitted information bits. At the receiver, a two-stage decoding algorithm is proposed to sequentially identify non-zero block indexes and single-element non-zero indexes. Extensive simulation results verify that proposed DM-SVC scheme outperforms the existing SVC schemes in terms of block error rate and spectral efficiency.

</details>


### [234] [Separable Delay And Doppler Estimation In Passive Radar](https://arxiv.org/abs/2601.15821)
*Mats Viberg,Daniele Gerosa,Tomas McKelvey,Patrik Dammert,Thomas Eriksson*

Main category: eess.SP

TL;DR: Proposes separable time-delay and Doppler estimation for passive radar to reduce computational complexity and communication overhead in distributed networks.


<details>
  <summary>Details</summary>
Motivation: Passive radar using distributed sensors faces computational challenges with joint 2-D parameter estimation (time-delay and Doppler), especially in distributed settings where communication overhead is significant.

Method: Separable approach: first estimate time-delay alone (avoiding 2-D search), then restore batch coherency to estimate Doppler. Designed for slowly moving targets.

Result: Time-delay accuracy similar to full 2-D method, superior Doppler estimates over wide parameter range, reduced computational complexity, and significantly lower communication overhead in distributed radar.

Conclusion: Separable estimation technique offers efficient alternative to joint 2-D parameter estimation for passive radar, particularly beneficial for distributed sensor networks with communication constraints.

Abstract: In passive radar, a network of distributed sensors exploit signals from so-called Illuminators-of-Opportunity to detect and localize targets. We consider the case where the IO signal is available at each receiver node through a reference channel, whereas target returns corrupted by interference are collected in a separate surveillance channel. The problem formulation is similar to an active radar that uses a noise-like waveform, or an integrated sensing and communication application. The available data is first split into batches of manageable size. In the direct approach, the target's time-delay and Doppler parameters are estimated jointly by incoherently combining the batch-wise data. We propose a new method to estimate the time-delay separately, thus avoiding a costly 2-D search. Our approach is designed for slowly moving targets, and the accuracy of the time-delay estimate is similar to that of the full batch-wise 2-D method. Given the time-delay, the coherency between batches can be restored when estimating the Doppler parameter. Thereby, the separable approach is found to yield superior Doppler estimates over a wide parameter range. In addition to reducing computational complexity, the proposed separable estimation technique also significantly reduces the communication overhead in a distributed radar setting.

</details>


### [235] [Performance Analysis of Digital Beamforming mmWave MIMO with Low-Resolution DACs/ADCs](https://arxiv.org/abs/2601.15831)
*Faruk Pasic,Mariam Mussbah,Stefan Schwarz,Markus Rupp,Fredrik Tufvesson,Christoph F. Mecklenbräuker*

Main category: eess.SP

TL;DR: 4-bit DAC/ADC quantization offers optimal trade-off between energy efficiency and spectral efficiency in fully digital mmWave MIMO systems with channel estimation.


<details>
  <summary>Details</summary>
Motivation: Future wireless needs mmWave MIMO with fully digital beamforming for low latency, but power efficiency requires low-resolution DACs/ADCs which cause distortion and degrade performance.

Method: Investigates channel estimation performance of fully digital mmWave MIMO with low-resolution quantization under practical constraints, evaluating spectral efficiency and energy efficiency.

Result: Simulation shows 4-bit per DAC/ADC provides favorable trade-off between energy consumption and achievable data rate.

Conclusion: Moderate 4-bit quantization resolution is optimal for balancing energy efficiency and spectral efficiency in practical mmWave MIMO systems with fully digital beamforming.

Abstract: Future wireless communications will rely on multiple-input multiple-output (MIMO) beamforming operating at millimeter wave (mmWave) frequency bands to deliver high data rates. To support flexible spatial processing and meet the demands of latency critical applications, it is essential to use fully digital mmWave MIMO beamforming, which relies on accurate channel estimation. However, ensuring power efficiency in fully digital mmWave MIMO systems requires the use of low-resolution digital-to-analog converters (DACs) and analog-to-digital converters (ADCs). The reduced resolution of these quantizers introduces distortion in both transmitted and received signals, ultimately degrading system performance. In this paper, we investigate the channel estimation performance of mmWave MIMO systems employing fully digital beamforming with low-resolution quantization, under practical system constraints. We evaluate the system performance in terms of spectral efficiency (SE) and energy efficiency (EE). Simulation results demonstrate that a moderate quantization resolutions of 4-bit per DAC/ADC offers a favorable trade-off between energy consumption and achievable data rate.

</details>


### [236] [Time-Varying Rician K-factor in Measured Vehicular Channels at cmWave and mmWave Bands](https://arxiv.org/abs/2601.15863)
*Faruk Pasic,Markus Hofer,Thomas Zemen,Andreas F. Molisch,Christoph F. Mecklenbräuker*

Main category: eess.SP

TL;DR: Multi-band V2I channel measurements show similar Rician K-factors across 3.2 GHz, 34.3 GHz, and 62.35 GHz bands, with correlation to RMS delay spread.


<details>
  <summary>Details</summary>
Motivation: Future vehicular systems will use mmWave for higher data rates, requiring understanding of propagation differences between mmWave and cmWave bands. Multi-band channel measurements are needed to investigate small-scale fading characteristics like the Rician K-factor.

Method: Conducted multi-band V2I channel measurements in urban street environment across three frequency bands (3.2 GHz, 34.3 GHz, 62.35 GHz) with 155.5 MHz bandwidth and 31.25 μs sounding repetition rate. Analyzed time-varying K-factor and its relationship with RMS delay spread.

Result: Rician K-factor is similar across different frequency bands (3.2 GHz, 34.3 GHz, 62.35 GHz). K-factor shows correlation with RMS delay spread in V2I channels.

Conclusion: The similarity of K-factors across frequency bands suggests consistent small-scale fading characteristics, while the correlation with delay spread provides insights for channel modeling in future multi-band vehicular communication systems.

Abstract: Future vehicular communication systems will integrate millimeter wave (mmWave) technology to enhance data transmission rates. To investigate the propagation effects and small-scale fading differences between mmWave and conventional centimeter wave (cmWave) bands, multi-band channel measurements have to be conducted. One key parameter to characterize small-scale fading is the Rician K-factor. In this paper, we analyze the time-varying K-factor of vehicle-to-infrastructure (V2I) channels across multiple frequency bands, measured in an urban street environment. Specifically, we investigate three frequency bands with center frequencies of 3.2 GHz, 34.3 GHz and 62.35 GHz using measurement data with 155.5 MHz bandwidth and a sounding repetition rate of 31.25 μs. Furthermore, we analyze the relationship between K-factor and root-mean-square (RMS) delay spread. We show that the Ricean K-factor is similar at different frequency bands and that is correlated with the RMS delay spread.

</details>


### [237] [Reconstructing Patched or Partial Holograms to allow for Whole Slide Imaging with a Self-Referencing Holographic Microscope](https://arxiv.org/abs/2601.15952)
*Philip Groult,Julia D. Sistermanns,Ellen Emken,Oliver Hayden,Wolfgang Utschick*

Main category: eess.SP

TL;DR: Researchers developed a reconstruction algorithm that enables whole slide imaging of cervical smears using a self-referencing three-wave digital holographic microscope, bridging WSI and QPI technologies.


<details>
  <summary>Details</summary>
Motivation: While WSI and deep learning have advanced computer-aided cytological diagnostics, and QPI offers richer cell information with less preparation, these two worlds haven't been combined. The authors aim to bridge this gap by enabling WSI using QPI techniques.

Method: Developed an adaptive reconstruction algorithm for whole slide imaging using a self-referencing three-wave digital holographic microscope. The algorithm works with single-shot holograms and is flexible enough to handle partial holograms and patched holograms, making it suitable for constructing WSIs from multiple patches.

Result: The algorithm performs well for tested epithelial cells, demonstrating successful integration of WSI and QPI technologies for cervical smear analysis.

Conclusion: This work successfully bridges WSI and QPI by developing an adaptive reconstruction algorithm that enables whole slide imaging of cervical smears using digital holographic microscopy, representing a significant step forward in computer-aided cytological diagnostics.

Abstract: The last decade has seen significant advances in computer-aided diagnostics for cytological screening, mainly through the improvement and integration of scanning techniques such as whole slide imaging (WSI) and the combination with deep learning. Simultaneously, new imaging techniques such as quantitative phase imaging (QPI) are being developed to capture richer cell information with less sample preparation. So far, the two worlds of WSI and QPI have not been combined. In this work, we present a reconstruction algorithm which makes whole slide imaging of cervical smears possible by using a self-referencing three-wave digital holographic microscope. Since a WSI is constructed by combining multiple patches, the algorithm is adaptive and can be used on partial holograms and patched holograms. We present the algorithm for a single shot hologram, the adaptations to make it flexible to various inputs and show that the algorithm performs well for the tested epithelial cells. This is a preprint of our paper, which has been accepted for publication in 2026 IEEE International Symposium on Biomedical Imaging (ISBI).

</details>


### [238] [Performance Scaling Laws for PD Array-based Receivers in IM/DD Optical Wireless Communication Systems](https://arxiv.org/abs/2601.15973)
*Aravindh Krishnamoorthy,Robert Schober,Harald Haas*

Main category: eess.SP

TL;DR: PD array receivers can outperform single PD receivers in SNR and rate for narrow beams above an SNR threshold, but simply adding more PDs isn't enough - joint optimization of beam pattern, mode, power, and PD positions is required.


<details>
  <summary>Details</summary>
Motivation: To understand the performance scaling laws and design trade-offs for photodetector array receivers using intensity modulation/direct detection, comparing them to single PD receivers for next-generation high-bandwidth systems.

Method: Analytical modeling and numerical analysis of PD array-based receivers, considering the square-law relationship between optical and electrical powers, comparing SNR and achievable rate to single PD reference receivers.

Result: PD arrays provide performance gains only for sufficiently narrow beams and above an SNR threshold. Increasing PD count alone doesn't improve performance - joint optimization of beam pattern, transverse electromagnetic mode, received power, and PD positions is necessary.

Conclusion: The study provides practical design guidelines and highlights trade-offs for next-generation high-bandwidth PD array receivers, emphasizing the need for holistic optimization rather than simply increasing PD count.

Abstract: We study the performance scaling laws for electrical-domain combining in photodetector (PD) array-based receivers employing intensity modulation and direct detection, taking into account the inherent square-law relationship between the optical and electrical received powers. The performance of PD array-based systems is compared, in terms of signal-to-noise ratio (SNR) and achievable rate, to that of a reference receiver employing a single PD. Analytical and numerical results show that PD arrays provide performance gains for sufficiently narrow beams and above an SNR threshold. Furthermore, increasing the number of PDs alone does not enhance performance, and joint optimization of beam pattern, transverse electromagnetic mode, received power, and PD positions is necessary. Our model and derived insights provide practical guidelines and highlight the trade-offs for the design of next-generation high-bandwidth PD array receivers.

</details>


### [239] [Graph Topology Identification Based on Covariance Matching](https://arxiv.org/abs/2601.15999)
*Yongsheng Han,Raj Thilak Rajan,Geert Leus*

Main category: eess.SP

TL;DR: CovMatch: A covariance matching framework for graph topology identification that aligns empirical data covariance with theoretical graph covariance, handling both undirected and directed graphs without restrictive assumptions.


<details>
  <summary>Details</summary>
Motivation: Graph topology identification is challenging because underlying structures are hidden while nodal data are available. Existing methods rely on probabilistic models or complex optimization with non-convexity issues, often requiring restrictive assumptions like acyclicity or positivity.

Method: Proposes CovMatch framework that directly matches empirical covariance of observed data with theoretical covariance implied by underlying graph. Works for any data-generating process with explicit covariance expression. For linear SEMs, simplifies to conic mixed integer program for undirected graphs or orthogonal matrix optimization for directed graphs.

Result: CovMatch efficiently recovers true topology even for relatively large graphs and outperforms standard baselines in accuracy. Handles both undirected and general sparse directed graphs (acyclic or positively weighted) without explicit knowledge of structural constraints.

Conclusion: CovMatch provides a powerful alternative to log-determinant or Bayesian methods for GTI, enabling learning of complex network topologies with minimal assumptions and paving way for broader research in network inference.

Abstract: Graph topology identification (GTI) is a central challenge in networked systems, where the underlying structure is often hidden, yet nodal data are available. Conventional solutions to address these challenges rely on probabilistic models or complex optimization formulations, commonly suffering from non-convexity or requiring restrictive assumptions on acyclicity or positivity. In this paper, we propose a novel covariance matching (CovMatch) framework that directly aligns the empirical covariance of the observed data with the theoretical covariance implied by an underlying graph. We show that as long as the data-generating process permits an explicit covariance expression, CovMatch offers a unified route to topology inference.
  We showcase our methodology on linear structural equation models (SEMs), showing that CovMatch naturally handles both undirected and general sparse directed graphs - whether acyclic or positively weighted - without explicit knowledge of these structural constraints. Through appropriate reparameterizations, CovMatch simplifies the graph learning problem to either a conic mixed integer program for undirected graphs or an orthogonal matrix optimization for directed graphs. Numerical results confirm that, even for relatively large graphs, our approach efficiently recovers the true topology and outperforms standard baselines in accuracy. These findings highlight CovMatch as a powerful alternative to log-determinant or Bayesian methods for GTI, paving the way for broader research on learning complex network topologies with minimal assumptions.

</details>


### [240] [Low-Complexity Sparse Superimposed Coding for Ultra Reliable Low Latency Communications](https://arxiv.org/abs/2601.16012)
*Yanfeng Zhang,Xi'an Fan,Xu Zhu,Jinkai Zheng,Hui Liang,Weiwei Yang,Tom H. Luan*

Main category: eess.SP

TL;DR: Proposes a low-complexity sparse superimposed coding scheme using sparse codebook design to reduce computational complexity while maintaining performance for short-packet URLLC.


<details>
  <summary>Details</summary>
Motivation: Conventional SSC schemes suffer from high encoding/decoding complexity due to dense codebook matrices, which is problematic for ultra-reliable low-latency communication requiring efficient short-packet transmission.

Method: Designs a sparse codebook structure where each codeword contains only a small number of non-zero elements, and uses multipath matching pursuit algorithm for decoding to exploit codebook sparsity.

Result: Achieves favorable trade-off between BLER performance and computational complexity, with strong robustness across different transmission block lengths.

Conclusion: The proposed sparse codebook SSC scheme significantly reduces complexity while maintaining performance, making it suitable for URLLC short-packet transmission scenarios.

Abstract: Sparse superimposed coding (SSC) has emerged as a promising technique for short-packet transmission in ultra-reliable low-latency communication scenarios. However, conventional SSC schemes often suffer from high encoding and decoding complexity due to the use of dense codebook matrices. In this paper, we propose a low-complexity SSC scheme by designing a sparse codebook structure, where each codeword contains only a small number of non-zero elements. The decoding is performed using the traditional multipath matching pursuit algorithm, and the overall complexity is significantly reduced by exploiting the sparsity of the codebook. Simulation results show that the proposed scheme achieves a favorable trade-off between BLER performance and computational complexity, and exhibits strong robustness across different transmission block lengths.

</details>


### [241] [Hybrid Channel Estimation with Quantized Phase Feedback for Over-the-Air Computation](https://arxiv.org/abs/2601.16054)
*Martin Dahl,Erik G. Larsson*

Main category: eess.SP

TL;DR: Hybrid channel estimation scheme combines reciprocity-based and feedback-based methods to reduce signaling overhead in over-the-air computation, with separate amplitude/phase estimation precision selection.


<details>
  <summary>Details</summary>
Motivation: To reduce signaling overhead in over-the-air computation systems by developing a more efficient channel estimation approach that minimizes communication overhead while maintaining performance.

Method: Proposes hybrid channel estimation combining reciprocity-based and feedback-based methods, with quantized phase-feedback (amplitude assumed exact). Two variants: one with phase feedback only, and another with reciprocity-based phase estimation plus optimal quantization of phase feedback.

Result: Through simulations and theoretical analysis, the second variant (reciprocity-based phase estimation with optimal quantization) outperforms the first variant (phase feedback only).

Conclusion: Hybrid channel estimation with separate amplitude/phase precision selection and optimal quantization strategy provides better performance than pure feedback-based approaches for over-the-air computation systems.

Abstract: To reduce the signaling overhead of over-the-air computation, a hybrid channel estimation scheme is proposed, where reciprocity-based and feedback-based channel estimation are combined. In particular, the impact of quantized phase-feedback is studied while the amplitude is assumed estimated exactly. The scheme enables selecting the estimation precision of amplitude and phase separately, depending on the importance of each. Two variants of the scheme are proposed: As shown through simulations and theory, the second variant with reciprocity-based estimation of the channel phase, and optimal quantization of phase feedback, can outperform the first variant estimating the phase by feedback only.

</details>
