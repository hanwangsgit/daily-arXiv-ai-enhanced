<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 85]
- [cs.IT](#cs.IT) [Total: 4]
- [cs.AI](#cs.AI) [Total: 17]
- [eess.SP](#eess.SP) [Total: 8]
- [cs.LG](#cs.LG) [Total: 54]
- [eess.IV](#eess.IV) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Understanding Virality: A Rubric based Vision-Language Model Framework for Short-Form Edutainment Evaluation](https://arxiv.org/abs/2512.21402)
*Arnav Gupta,Gurekas Singh Sahney,Hardik Rathi,Abhishek Chandwani,Ishaan Gupta,Pratik Narang,Dhruv Kumar*

Main category: cs.CV

TL;DR: Proposes a data-driven framework using Vision-Language Models to extract audiovisual features, cluster them into interpretable factors, and train a regression model to predict engagement on short-form edutainment videos.


<details>
  <summary>Details</summary>
Motivation: Existing video evaluation frameworks focus on visual/semantic fidelity but fail to capture how specific audiovisual attributes drive real audience engagement. Need for human-aligned, multimodal reasoning that connects technical features to actual viewer behavior.

Method: Uses Vision-Language Models (VLMs) to extract unsupervised audiovisual features from short-form videos, clusters features into interpretable factors, trains regression-based evaluator to predict engagement metrics using curated YouTube Shorts dataset.

Result: Strong correlations between predicted and actual engagement metrics. Lightweight feature-based evaluator outperforms traditional metrics (SSIM, FID) while providing interpretable and scalable assessments.

Conclusion: The framework advances video understanding by grounding evaluation in both multimodal feature importance and human-centered engagement signals, enabling robust and explainable assessment of short-form video content.

Abstract: Evaluating short-form video content requires moving beyond surface-level quality metrics toward human-aligned, multimodal reasoning. While existing frameworks like VideoScore-2 assess visual and semantic fidelity, they do not capture how specific audiovisual attributes drive real audience engagement. In this work, we propose a data-driven evaluation framework that uses Vision-Language Models (VLMs) to extract unsupervised audiovisual features, clusters them into interpretable factors, and trains a regression-based evaluator to predict engagement on short-form edutainment videos. Our curated YouTube Shorts dataset enables systematic analysis of how VLM-derived features relate to human engagement behavior. Experiments show strong correlations between predicted and actual engagement, demonstrating that our lightweight, feature-based evaluator provides interpretable and scalable assessments compared to traditional metrics (e.g., SSIM, FID). By grounding evaluation in both multimodal feature importance and human-centered engagement signals, our approach advances toward robust and explainable video understanding.

</details>


### [2] [A Tool Bottleneck Framework for Clinically-Informed and Interpretable Medical Image Understanding](https://arxiv.org/abs/2512.21414)
*Christina Liu,Alan Q. Wang,Joy Hsu,Jiajun Wu,Ehsan Adeli*

Main category: cs.CV

TL;DR: Tool Bottleneck Framework (TBF) improves medical image understanding by using a learned neural network to compose specialized tools selected by VLMs, outperforming text-based composition methods.


<details>
  <summary>Details</summary>
Motivation: Existing tool-use frameworks for vision-language models rely on text-based composition (code or natural language) which performs poorly on medical images where spatially-localized features are crucial but difficult to compose via text alone.

Method: Proposes Tool Bottleneck Framework (TBF) with Tool Bottleneck Model (TBM): uses off-the-shelf medical VLM to select clinically-relevant tools from toolbox, then composes tool outputs via learned neural network (TBM) that fuses features before final prediction.

Result: TBF performs on par or better than deep learning classifiers, VLMs, and state-of-the-art tool-use frameworks on histopathology and dermatology tasks, with particular gains in data-limited regimes, while providing more interpretable, clinically-grounded predictions.

Conclusion: The Tool Bottleneck Framework effectively addresses limitations of text-based tool composition in medical imaging by using learned neural fusion, improving performance and interpretability in clinical contexts.

Abstract: Recent tool-use frameworks powered by vision-language models (VLMs) improve image understanding by grounding model predictions with specialized tools. Broadly, these frameworks leverage VLMs and a pre-specified toolbox to decompose the prediction task into multiple tool calls (often deep learning models) which are composed to make a prediction. The dominant approach to composing tools is using text, via function calls embedded in VLM-generated code or natural language. However, these methods often perform poorly on medical image understanding, where salient information is encoded as spatially-localized features that are difficult to compose or fuse via text alone. To address this, we propose a tool-use framework for medical image understanding called the Tool Bottleneck Framework (TBF), which composes VLM-selected tools using a learned Tool Bottleneck Model (TBM). For a given image and task, TBF leverages an off-the-shelf medical VLM to select tools from a toolbox that each extract clinically-relevant features. Instead of text-based composition, these tools are composed by the TBM, which computes and fuses the tool outputs using a neural network before outputting the final prediction. We propose a simple and effective strategy for TBMs to make predictions with any arbitrary VLM tool selection. Overall, our framework not only improves tool-use in medical imaging contexts, but also yields more interpretable, clinically-grounded predictors. We evaluate TBF on tasks in histopathology and dermatology and find that these advantages enable our framework to perform on par with or better than deep learning-based classifiers, VLMs, and state-of-the-art tool-use frameworks, with particular gains in data-limited regimes. Our code is available at https://github.com/christinaliu2020/tool-bottleneck-framework.

</details>


### [3] [Scalable Deep Subspace Clustering Network](https://arxiv.org/abs/2512.21434)
*Nairouz Mrabah,Mohamed Bouguessa,Sihem Sami*

Main category: cs.CV

TL;DR: SDSNet is a scalable deep subspace clustering framework that achieves linear O(n) complexity by using landmark-based approximation instead of full affinity matrices, avoiding the O(n³) bottleneck of traditional methods.


<details>
  <summary>Details</summary>
Motivation: Subspace clustering methods face scalability limits due to O(n³) computational costs from constructing full affinity matrices and spectral decomposition. Deep learning approaches still maintain this bottleneck through exhaustive pairwise similarity computations.

Method: SDSNet uses: (1) landmark-based approximation to avoid full affinity matrices, (2) joint optimization of auto-encoder reconstruction with self-expression objectives, and (3) direct spectral clustering on factorized representations. It combines convolutional auto-encoders with subspace-preserving constraints.

Result: Experimental results show SDSNet achieves comparable clustering quality to state-of-the-art methods with significantly improved computational efficiency, demonstrating the effectiveness of the linear complexity approach.

Conclusion: SDSNet provides a scalable deep subspace clustering framework that overcomes the O(n³) computational bottleneck through landmark-based approximation and joint optimization, enabling efficient clustering at linear O(n) complexity while maintaining competitive performance.

Abstract: Subspace clustering methods face inherent scalability limits due to the $O(n^3)$ cost (with $n$ denoting the number of data samples) of constructing full $n\times n$ affinities and performing spectral decomposition. While deep learning-based approaches improve feature extraction, they maintain this computational bottleneck through exhaustive pairwise similarity computations. We propose SDSNet (Scalable Deep Subspace Network), a deep subspace clustering framework that achieves $\mathcal{O}(n)$ complexity through (1) landmark-based approximation, avoiding full affinity matrices, (2) joint optimization of auto-encoder reconstruction with self-expression objectives, and (3) direct spectral clustering on factorized representations. The framework combines convolutional auto-encoders with subspace-preserving constraints. Experimental results demonstrate that SDSNet achieves comparable clustering quality to state-of-the-art methods with significantly improved computational efficiency.

</details>


### [4] [Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism](https://arxiv.org/abs/2512.21452)
*Haotian Lv,Yuhui Zhang,Jiangbo Dai,Hanli Wu,Jiaji Wang,Dawei Wang*

Main category: cs.CV

TL;DR: A deep learning framework using DCGAN data augmentation and MCGA-Net architecture achieves high-accuracy automated defect detection in GPR images, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Conventional GPR image interpretation relies on subjective expertise, leading to inefficiencies and inaccuracies in subsurface road defect detection. There's a need for automated, objective analysis methods.

Method: Three-part framework: 1) DCGAN-based data augmentation to synthesize high-fidelity GPR images, 2) Novel MCGA-Net architecture with Multi-modal Chain Feature Fusion and Global Attention Mechanism, 3) MS COCO transfer learning for backbone network fine-tuning.

Result: MCGA-Net achieves Precision (92.8%), Recall (92.5%), and mAP@50 (95.9%). Shows robustness against Gaussian noise, weak signals, and small targets, outperforming other models.

Conclusion: The framework establishes a new paradigm for automated GPR-based defect detection, balancing computational efficiency with high accuracy in complex subsurface environments.

Abstract: Ground Penetrating Radar (GPR) has emerged as a pivotal tool for non-destructive evaluation of subsurface road defects. However, conventional GPR image interpretation remains heavily reliant on subjective expertise, introducing inefficiencies and inaccuracies. This study introduces a comprehensive framework to address these limitations: (1) A DCGAN-based data augmentation strategy synthesizes high-fidelity GPR images to mitigate data scarcity while preserving defect morphology under complex backgrounds; (2) A novel Multi-modal Chain and Global Attention Network (MCGA-Net) is proposed, integrating Multi-modal Chain Feature Fusion (MCFF) for hierarchical multi-scale defect representation and Global Attention Mechanism (GAM) for context-aware feature enhancement; (3) MS COCO transfer learning fine-tunes the backbone network, accelerating convergence and improving generalization. Ablation and comparison experiments validate the framework's efficacy. MCGA-Net achieves Precision (92.8%), Recall (92.5%), and mAP@50 (95.9%). In the detection of Gaussian noise, weak signals and small targets, MCGA-Net maintains robustness and outperforms other models. This work establishes a new paradigm for automated GPR-based defect detection, balancing computational efficiency with high accuracy in complex subsurface environments.

</details>


### [5] [CCAD: Compressed Global Feature Conditioned Anomaly Detection](https://arxiv.org/abs/2512.21459)
*Xiao Jin,Liang Diao,Qixin Xiao,Yifan Hu,Ziqi Zhang,Yuchen Liu,Haisong Gu*

Main category: cs.CV

TL;DR: CCAD combines reconstruction-based and unsupervised representation-based anomaly detection by using compressed global features as a condition for reconstruction, improving performance and training efficiency.


<details>
  <summary>Details</summary>
Motivation: Current anomaly detection methods have limitations: unsupervised representation-based methods struggle with domain shift, while reconstruction-based methods suffer from low training efficiency and performance degradation due to insufficient constraints.

Method: CCAD synergizes both paradigms by adapting global features as a new modality condition for reconstruction models, with an adaptive compression mechanism to enhance generalization and training efficiency.

Result: Extensive experiments show CCAD consistently outperforms state-of-the-art methods in AUC while achieving faster convergence. The authors also contribute a reorganized DAGM 2007 dataset with new annotations.

Conclusion: CCAD effectively addresses limitations of existing anomaly detection methods by combining their strengths, demonstrating superior performance and efficiency across various benchmarks.

Abstract: Anomaly detection holds considerable industrial significance, especially in scenarios with limited anomalous data. Currently, reconstruction-based and unsupervised representation-based approaches are the primary focus. However, unsupervised representation-based methods struggle to extract robust features under domain shift, whereas reconstruction-based methods often suffer from low training efficiency and performance degradation due to insufficient constraints. To address these challenges, we propose a novel method named Compressed Global Feature Conditioned Anomaly Detection (CCAD). CCAD synergizes the strengths of both paradigms by adapting global features as a new modality condition for the reconstruction model. Furthermore, we design an adaptive compression mechanism to enhance both generalization and training efficiency. Extensive experiments demonstrate that CCAD consistently outperforms state-of-the-art methods in terms of AUC while achieving faster convergence. In addition, we contribute a reorganized and re-annotated version of the DAGM 2007 dataset with new annotations to further validate our method's effectiveness. The code for reproducing main results is available at https://github.com/chloeqxq/CCAD.

</details>


### [6] [IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset](https://arxiv.org/abs/2512.21472)
*Kumar Abhishek,Jeremy Kawahara,Ghassan Hamarneh*

Main category: cs.CV

TL;DR: ISIC MultiAnnot++ is the largest publicly available multi-annotator skin lesion segmentation dataset with 17,684 masks across 14,967 dermoscopic images, including annotator metadata.


<details>
  <summary>Details</summary>
Motivation: There are no large-scale publicly available multi-annotator skin lesion segmentation datasets for dermoscopic imaging, which is needed for research on annotator-specific modeling and segmentation analysis.

Method: Created ISIC MultiAnnot++ by collecting and organizing segmentation masks from the ISIC Archive, including 2,394 images with 2-5 segmentations per image, and adding annotator metadata (skill level, segmentation tool).

Result: Final dataset contains 17,684 segmentation masks spanning 14,967 dermoscopic images, making it the largest publicly available SLS dataset with comprehensive annotator metadata.

Conclusion: ISIC MultiAnnot++ enables research on annotator-specific preference modeling, segmentation analysis, and provides curated data partitions and consensus masks for the research community.

Abstract: Multi-annotator medical image segmentation is an important research problem, but requires annotated datasets that are expensive to collect. Dermoscopic skin lesion imaging allows human experts and AI systems to observe morphological structures otherwise not discernable from regular clinical photographs. However, currently there are no large-scale publicly available multi-annotator skin lesion segmentation (SLS) datasets with annotator-labels for dermoscopic skin lesion imaging. We introduce ISIC MultiAnnot++, a large public multi-annotator skin lesion segmentation dataset for images from the ISIC Archive. The final dataset contains 17,684 segmentation masks spanning 14,967 dermoscopic images, where 2,394 dermoscopic images have 2-5 segmentations per image, making it the largest publicly available SLS dataset. Further, metadata about the segmentation, including the annotators' skill level and segmentation tool, is included, enabling research on topics such as annotator-specific preference modeling for segmentation and annotator metadata analysis. We provide an analysis on the characteristics of this dataset, curated data partitions, and consensus segmentation masks.

</details>


### [7] [GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification](https://arxiv.org/abs/2512.21476)
*Suncheng Xiang,Xiaoyang Wang,Junjie Jiang,Hejia Wang,Dahong Qian*

Main category: cs.CV

TL;DR: Proposes Gated Progressive Fusion network for colonoscopic polyp re-identification, using multi-level feature fusion with gating mechanisms to improve matching accuracy.


<details>
  <summary>Details</summary>
Motivation: Colonoscopic polyp re-identification is crucial for colorectal cancer prevention, but current methods using coarse high-level features perform poorly on small polyps where detailed information matters.

Method: Gated Progressive Fusion network that selectively fuses features from multiple levels using gates in a fully connected way, with gated progressive fusion strategy for layer-wise refinement of semantic information through multi-level feature interactions.

Result: Experiments on standard benchmarks show benefits over state-of-the-art unimodal ReID models, especially when combined with specialized multimodal fusion strategy.

Conclusion: The proposed architecture effectively addresses the challenge of polyp re-identification by leveraging multi-level feature fusion with gating mechanisms, improving performance particularly for small polyps where detailed information is critical.

Abstract: Colonoscopic Polyp Re-Identification aims to match the same polyp from a large gallery with images from different views taken using different cameras, which plays an important role in the prevention and treatment of colorectal cancer in computer-aided diagnosis. However, the coarse resolution of high-level features of a specific polyp often leads to inferior results for small objects where detailed information is important. To address this challenge, we propose a novel architecture, named Gated Progressive Fusion network, to selectively fuse features from multiple levels using gates in a fully connected way for polyp ReID. On the basis of it, a gated progressive fusion strategy is introduced to achieve layer-wise refinement of semantic information through multi-level feature interactions. Experiments on standard benchmarks show the benefits of the multimodal setting over state-of-the-art unimodal ReID models, especially when combined with the specialized multimodal fusion strategy.

</details>


### [8] [Generative Multi-Focus Image Fusion](https://arxiv.org/abs/2512.21495)
*Xinzhe Xie,Buyu Guo,Bolin Li,Shuangyan He,Yanzhen Gu,Qingyan Jiang,Peiliang Li*

Main category: cs.CV

TL;DR: GMFF is a two-stage generative framework for multi-focus image fusion that first performs deterministic fusion using StackMFF V4, then uses IFControlNet (latent diffusion model) to restore missing content and eliminate artifacts.


<details>
  <summary>Details</summary>
Motivation: Existing fusion algorithms assume at least one input image has each spatial location in focus, and suffer from edge artifacts in complex real-world scenarios due to uncertain focus estimation or hard-selection operations.

Method: Two-stage cascaded framework: 1) Deterministic fusion using StackMFF V4 with focal plane information, 2) Generative restoration using IFControlNet (latent diffusion model) to reconstruct missing focal plane content, restore fine details, and eliminate edge artifacts.

Result: GMFF achieves state-of-the-art fusion performance and shows significant potential for practical applications, especially with complex multi-focal content.

Conclusion: The proposed GMFF framework effectively addresses limitations of existing methods by combining deterministic fusion with generative restoration, producing high-quality all-in-focus images even in challenging scenarios.

Abstract: Multi-focus image fusion aims to generate an all-in-focus image from a sequence of partially focused input images. Existing fusion algorithms generally assume that, for every spatial location in the scene, there is at least one input image in which that location is in focus. Furthermore, current fusion models often suffer from edge artifacts caused by uncertain focus estimation or hard-selection operations in complex real-world scenarios. To address these limitations, we propose a generative multi-focus image fusion framework, termed GMFF, which operates in two sequential stages. In the first stage, deterministic fusion is implemented using StackMFF V4, the latest version of the StackMFF series, and integrates the available focal plane information to produce an initial fused image. The second stage, generative restoration, is realized through IFControlNet, which leverages the generative capabilities of latent diffusion models to reconstruct content from missing focal planes, restore fine details, and eliminate edge artifacts. Each stage is independently developed and functions seamlessly in a cascaded manner. Extensive experiments demonstrate that GMFF achieves state-of-the-art fusion performance and exhibits significant potential for practical applications, particularly in scenarios involving complex multi-focal content. The implementation is publicly available at https://github.com/Xinzhe99/StackMFF-Series.

</details>


### [9] [BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization](https://arxiv.org/abs/2512.21769)
*Evgeny Alves Limarenko,Anastasiia Studenikina*

Main category: cs.CV

TL;DR: BertsWin: A hybrid SSL architecture combining BERT-style masking with Swin Transformer windows for 3D medical imaging, achieving 5.8x faster convergence and 15x reduction in training epochs compared to standard ViT-MAE.


<details>
  <summary>Details</summary>
Motivation: Standard Masked Autoencoders (MAE) struggle with 3D volumetric medical images due to difficulty capturing three-dimensional spatial relationships when discarding 75% of tokens. There's a need for better SSL methods that preserve 3D spatial topology while maintaining computational efficiency.

Method: Proposes BertsWin - a hybrid architecture combining full BERT-style token masking with Swin Transformer windows. Unlike classic MAE that processes only visible areas, BertsWin uses a complete 3D grid of tokens (masked and visible) to preserve spatial topology. Uses single-level local Swin windows to handle quadratic complexity of ViT. Introduces structural priority loss function and GradientConductor optimizer.

Result: Achieves 5.8x faster semantic convergence compared to standard ViT-MAE baselines. Full framework with GradientConductor achieves 15-fold reduction in training epochs (44 vs 660) to reach state-of-the-art reconstruction fidelity. Maintains theoretical FLOP parity with sparse ViT baselines at canonical input resolutions, resulting in significant computational resource reduction.

Conclusion: BertsWin enables efficient SSL for 3D medical imaging by preserving complete 3D spatial topology while maintaining computational efficiency, accelerating convergence without computational penalty typically associated with dense volumetric processing.

Abstract: The application of self-supervised learning (SSL) and Vision Transformers (ViTs) approaches demonstrates promising results in the field of 2D medical imaging, but the use of these methods on 3D volumetric images is fraught with difficulties. Standard Masked Autoencoders (MAE), which are state-of-the-art solution for 2D, have a hard time capturing three-dimensional spatial relationships, especially when 75% of tokens are discarded during pre-training. We propose BertsWin, a hybrid architecture combining full BERT-style token masking using Swin Transformer windows, to enhance spatial context learning in 3D during SSL pre-training. Unlike the classic MAE, which processes only visible areas, BertsWin introduces a complete 3D grid of tokens (masked and visible), preserving the spatial topology. And to smooth out the quadratic complexity of ViT, single-level local Swin windows are used. We introduce a structural priority loss function and evaluate the results of cone beam computed tomography of the temporomandibular joints. The subsequent assessment includes TMJ segmentation on 3D CT scans. We demonstrate that the BertsWin architecture, by maintaining a complete three-dimensional spatial topology, inherently accelerates semantic convergence by a factor of 5.8x compared to standard ViT-MAE baselines. Furthermore, when coupled with our proposed GradientConductor optimizer, the full BertsWin framework achieves a 15-fold reduction in training epochs (44 vs 660) required to reach state-of-the-art reconstruction fidelity. Analysis reveals that BertsWin achieves this acceleration without the computational penalty typically associated with dense volumetric processing. At canonical input resolutions, the architecture maintains theoretical FLOP parity with sparse ViT baselines, resulting in a significant net reduction in total computational resources due to faster convergence.

</details>


### [10] [SVBench: Evaluation of Video Generation Models on Social Reasoning](https://arxiv.org/abs/2512.21507)
*Wenshuo Peng,Gongxuan Wang,Tianmeng Yang,Chuanhao Li,Xiaojie Xu,Hui He,Kaipeng Zhang*

Main category: cs.CV

TL;DR: Researchers introduce the first benchmark for evaluating social reasoning in video generation models, revealing that current models fail at understanding social cognition despite good visual quality.


<details>
  <summary>Details</summary>
Motivation: Current text-to-video models generate visually realistic videos but lack social reasoning capabilities - they can't infer intentions, beliefs, emotions, or social norms like humans do. There's a need to systematically evaluate this gap in social cognition.

Method: Created a benchmark grounded in developmental and social psychology with 30 classic social cognition paradigms across 7 dimensions. Developed a training-free agent-based pipeline that distills reasoning mechanisms, synthesizes video scenarios, enforces conceptual neutrality, and evaluates using a VLM judge across 5 interpretable dimensions.

Result: Large-scale evaluation of 7 state-of-the-art video generation systems shows substantial performance gaps. While models excel in surface-level plausibility, they systematically fail in intention recognition, belief reasoning, joint attention, and prosocial inference.

Conclusion: Current video generation models have significant limitations in social reasoning despite advances in visual quality. The benchmark provides a systematic way to evaluate and improve social cognition capabilities in AI video generation systems.

Abstract: Recent text-to-video generation models exhibit remarkable progress in visual realism, motion fidelity, and text-video alignment, yet they remain fundamentally limited in their ability to generate socially coherent behavior. Unlike humans, who effortlessly infer intentions, beliefs, emotions, and social norms from brief visual cues, current models tend to render literal scenes without capturing the underlying causal or psychological logic. To systematically evaluate this gap, we introduce the first benchmark for social reasoning in video generation. Grounded in findings from developmental and social psychology, our benchmark organizes thirty classic social cognition paradigms into seven core dimensions, including mental-state inference, goal-directed action, joint attention, social coordination, prosocial behavior, social norms, and multi-agent strategy. To operationalize these paradigms, we develop a fully training-free agent-based pipeline that (i) distills the reasoning mechanism of each experiment, (ii) synthesizes diverse video-ready scenarios, (iii) enforces conceptual neutrality and difficulty control through cue-based critique, and (iv) evaluates generated videos using a high-capacity VLM judge across five interpretable dimensions of social reasoning. Using this framework, we conduct the first large-scale study across seven state-of-the-art video generation systems. Our results reveal substantial performance gaps: while modern models excel in surface-level plausibility, they systematically fail in intention recognition, belief reasoning, joint attention, and prosocial inference.

</details>


### [11] [Fixed-Budget Parameter-Efficient Training with Frozen Encoders Improves Multimodal Chest X-Ray Classification](https://arxiv.org/abs/2512.21508)
*Md Ashik Khan,Md Nahid Siddique*

Main category: cs.CV

TL;DR: Parameter-efficient training methods (frozen encoders, adapters, LoRA) achieve better chest X-ray classification performance than full fine-tuning with 40x fewer parameters, but have calibration issues.


<details>
  <summary>Details</summary>
Motivation: To reduce the computational cost of fine-tuning large vision-language models for multimodal chest X-ray analysis while maintaining or improving performance.

Method: Study parameter-efficient training strategies including frozen encoders, BitFit, LoRA, and adapters for multi-label classification on chest X-ray datasets. Use redacted pathology terms to prevent data leakage while retaining clinical context. Compare under fixed parameter budgets.

Result: All PET methods (2.37M parameters, 2.51% of total) achieve AUROC 0.892-0.908 vs full fine-tuning's 0.770 (94.3M parameters). External validation on CheXpert shows adapters achieve best performance (0.7214 AUROC). Vision-only models outperform budget-matched multimodal models, suggesting improvements come from parameter allocation rather than cross-modal synergy. PET methods have degraded calibration (ECE: 0.29-0.34).

Conclusion: Frozen encoder strategies provide superior discrimination at substantially reduced computational cost, but require calibration correction for clinical deployment. Parameter allocation matters more than cross-modal synergy for performance gains.

Abstract: Multimodal chest X-Ray analysis often fine-tunes large vision-language models, which is computationally costly. We study parameter-efficient training (PET) strategies, including frozen encoders, BitFit, LoRA, and adapters for multi-label classification on the Indiana University Chest X-Ray dataset (3,851 image-report pairs; 579 test samples). To mitigate data leakage, we redact pathology terms from reports used as text inputs while retaining clinical context. Under a fixed parameter budget (2.37M parameters, 2.51% of total), all PET variants achieve AUROC between 0.892 and 0.908, outperforming full fine-tuning (0.770 AUROC), which uses 94.3M trainable parameters, a 40x reduction. External validation on CheXpert (224,316 images, 58x larger) confirms scalability: all PET methods achieve >0.69 AUROC with <9% trainable parameters, with Adapter achieving best performance (0.7214 AUROC). Budget-matched comparisons reveal that vision-only models (0.653 AUROC, 1.06M parameters) outperform budget-matched multimodal models (0.641 AUROC, 1.06M parameters), indicating improvements arise primarily from parameter allocation rather than cross-modal synergy. While PET methods show degraded calibration (ECE: 0.29-0.34) compared to simpler models (ECE: 0.049), this represents a tractable limitation addressable through post-hoc calibration methods. These findings demonstrate that frozen encoder strategies provide superior discrimination at substantially reduced computational cost, though calibration correction is essential for clinical deployment.

</details>


### [12] [Fixed-Threshold Evaluation of a Hybrid CNN-ViT for AI-Generated Image Detection Across Photos and Art](https://arxiv.org/abs/2512.21512)
*Md Ashik Khan,Arafat Alam Jion*

Main category: cs.CV

TL;DR: This paper introduces a fixed-threshold evaluation protocol for AI-generated image detectors to address misleading robustness estimates, revealing that ViTs maintain performance under compression while CNNs collapse, and that semantic patterns in artistic content provide more reliable detection cues than forensic artifacts in photorealistic images.


<details>
  <summary>Details</summary>
Motivation: Existing AI-generated image detectors optimize single metrics without addressing deployment-critical factors like operating point selection and fixed-threshold robustness. Traditional methods retune thresholds per condition, artificially inflating robustness estimates and masking deployment failures.

Method: Introduced fixed-threshold evaluation protocol that holds decision thresholds (selected once on clean validation data) fixed across all post-processing transformations. Used a lightweight CNN-ViT hybrid with gated fusion and optional frequency enhancement, evaluated at three operating points (Low-FPR, ROC-optimal, Best-F1) under systematic degradation testing.

Result: Exposed forensic-semantic spectrum: frequency-aided CNNs excel on pristine photos (93.33%) but collapse under compression (61.49%), while ViTs degrade minimally (92.86% to 88.36%). All architectures achieve 15% higher AUROC on artistic content (0.901-0.907) vs photorealistic images (0.747-0.759). Hybrid approach achieves balanced cross-domain performance: 91.4% on photos, 89.7% on art/graphics, and 98.3% on CIFAKE.

Conclusion: Fixed-threshold evaluation eliminates retuning inflation, reveals genuine robustness gaps, and yields actionable deployment guidance: prefer CNNs for clean photo verification, ViTs for compressed content, and hybrids for art/graphics screening.

Abstract: AI image generators create both photorealistic images and stylized art, necessitating robust detectors that maintain performance under common post-processing transformations (JPEG compression, blur, downscaling). Existing methods optimize single metrics without addressing deployment-critical factors such as operating point selection and fixed-threshold robustness. This work addresses misleading robustness estimates by introducing a fixed-threshold evaluation protocol that holds decision thresholds, selected once on clean validation data, fixed across all post-processing transformations. Traditional methods retune thresholds per condition, artificially inflating robustness estimates and masking deployment failures. We report deployment-relevant performance at three operating points (Low-FPR, ROC-optimal, Best-F1) under systematic degradation testing using a lightweight CNN-ViT hybrid with gated fusion and optional frequency enhancement. Our evaluation exposes a statistically validated forensic-semantic spectrum: frequency-aided CNNs excel on pristine photos but collapse under compression (93.33% to 61.49%), whereas ViTs degrade minimally (92.86% to 88.36%) through robust semantic pattern recognition. Multi-seed experiments demonstrate that all architectures achieve 15% higher AUROC on artistic content (0.901-0.907) versus photorealistic images (0.747-0.759), confirming that semantic patterns provide fundamentally more reliable detection cues than forensic artifacts. Our hybrid approach achieves balanced cross-domain performance: 91.4% accuracy on tiny-genimage photos, 89.7% on AiArtData art/graphics, and 98.3% (competitive) on CIFAKE. Fixed-threshold evaluation eliminates retuning inflation, reveals genuine robustness gaps, and yields actionable deployment guidance: prefer CNNs for clean photo verification, ViTs for compressed content, and hybrids for art/graphics screening.

</details>


### [13] [MuS-Polar3D: A Benchmark Dataset for Computational Polarimetric 3D Imaging under Multi-Scattering Conditions](https://arxiv.org/abs/2512.21513)
*Puyun Wang,Kaimin Yu,Huayang He,Xianyu Wu*

Main category: cs.CV

TL;DR: MuS-Polar3D is the first public benchmark dataset for polarization-based underwater 3D imaging, featuring 42 objects captured under controlled scattering conditions with multiple viewpoints, enabling fair evaluation of reconstruction methods.


<details>
  <summary>Details</summary>
Motivation: Existing polarization-based underwater 3D imaging datasets lack diversity in scattering and observation conditions, hindering fair comparisons between different reconstruction approaches (single-view vs multi-view).

Method: Constructed MuS-Polar3D dataset with 42 objects captured under 7 controlled scattering conditions and 5 viewpoints, providing polarization images, high-precision 3D models, normal maps, and foreground masks. Proposed a two-stage pipeline (descattering + 3D reconstruction) from computational imaging perspective.

Result: Achieved best mean angular error of 15.49 degrees in extensive evaluations under complex scattering conditions. Dataset supports multiple vision tasks including normal estimation, segmentation, descattering, and 3D reconstruction.

Conclusion: MuS-Polar3D enables accurate 3D reconstruction and fair algorithm evaluation under controllable scattering conditions, addressing the lack of diverse public datasets for polarization-based underwater 3D imaging.

Abstract: Polarization-based underwater 3D imaging exploits polarization cues to suppress background scattering, exhibiting distinct advantages in turbid water. Although data-driven polarization-based underwater 3D reconstruction methods show great potential, existing public datasets lack sufficient diversity in scattering and observation conditions, hindering fair comparisons among different approaches, including single-view and multi-view polarization imaging methods.
  To address this limitation, we construct MuS-Polar3D, a benchmark dataset comprising polarization images of 42 objects captured under seven quantitatively controlled scattering conditions and five viewpoints, together with high-precision 3D models (+/- 0.05 mm accuracy), normal maps, and foreground masks. The dataset supports multiple vision tasks, including normal estimation, object segmentation, descattering, and 3D reconstruction.
  Inspired by computational imaging, we further decouple underwater 3D reconstruction under scattering into a two-stage pipeline, namely descattering followed by 3D reconstruction, from an imaging-chain perspective. Extensive evaluations using multiple baseline methods under complex scattering conditions demonstrate the effectiveness of the proposed benchmark, achieving a best mean angular error of 15.49 degrees. To the best of our knowledge, MuS-Polar3D is the first publicly available benchmark dataset for quantitative turbidity underwater polarization-based 3D imaging, enabling accurate reconstruction and fair algorithm evaluation under controllable scattering conditions. The dataset and code are publicly available at https://github.com/WangPuyun/MuS-Polar3D.

</details>


### [14] [DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO](https://arxiv.org/abs/2512.21514)
*Henglin Liu,Huijuan Huang,Jing Wang,Chang Liu,Xiu Li,Xiangyang Ji*

Main category: cs.CV

TL;DR: The paper addresses diversity degradation in GRPO-based image generation by proposing a distributional creativity bonus and structure-aware regularization to improve semantic diversity while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Traditional GRPO methods suffer from homogenized outputs in later training stages, lacking creativity and visual diversity due to single-sample reward signals and misaligned regularization that neglects early-stage denoising's role in preserving diversity.

Method: Two key innovations: 1) Distributional creativity bonus using spectral clustering over semantically grouped samples to allocate exploratory rewards based on group sizes, encouraging novel visual modes. 2) Structure-aware regularization that enforces stronger early-stage constraints to preserve diversity without compromising reward optimization efficiency.

Result: The method achieves 13-18% improvement in semantic diversity under matched quality scores, establishing a new Pareto frontier between image quality and diversity for GRPO-based image generation.

Conclusion: By addressing both reward modeling and generation dynamics, the proposed approach effectively mitigates diversity degradation in GRPO-based image generation, enabling better quality-diversity trade-offs and expanding application scenarios.

Abstract: Reinforcement learning (RL), particularly GRPO, improves image generation quality significantly by comparing the relative performance of images generated within the same group. However, in the later stages of training, the model tends to produce homogenized outputs, lacking creativity and visual diversity, which restricts its application scenarios. This issue can be analyzed from both reward modeling and generation dynamics perspectives. First, traditional GRPO relies on single-sample quality as the reward signal, driving the model to converge toward a few high-reward generation modes while neglecting distribution-level diversity. Second, conventional GRPO regularization neglects the dominant role of early-stage denoising in preserving diversity, causing a misaligned regularization budget that limits the achievable quality--diversity trade-off. Motivated by these insights, we revisit the diversity degradation problem from both reward modeling and generation dynamics. At the reward level, we propose a distributional creativity bonus based on semantic grouping. Specifically, we construct a distribution-level representation via spectral clustering over samples generated from the same caption, and adaptively allocate exploratory rewards according to group sizes to encourage the discovery of novel visual modes. At the generation level, we introduce a structure-aware regularization, which enforces stronger early-stage constraints to preserve diversity without compromising reward optimization efficiency. Experiments demonstrate that our method achieves a 13\%--18\% improvement in semantic diversity under matched quality scores, establishing a new Pareto frontier between image quality and diversity for GRPO-based image generation.

</details>


### [15] [Hierarchy-Aware Fine-Tuning of Vision-Language Models](https://arxiv.org/abs/2512.21529)
*Jiayu Li,Rajesh Gangireddy,Samet Akcay,Wei Cheng,Juhua Hu*

Main category: cs.CV

TL;DR: Efficient hierarchy-aware fine-tuning framework for Vision-Language Models that enforces structural consistency in hierarchical classification with minimal parameter updates.


<details>
  <summary>Details</summary>
Motivation: Standard approaches treat labels as flat categories and require expensive full fine-tuning, producing inconsistent predictions across taxonomy levels in hierarchical classification tasks.

Method: Combines two objectives: Tree-Path KL Divergence (TP-KL) for vertical coherence along ground-truth label paths, and Hierarchy-Sibling Smoothed Cross-Entropy (HiSCE) for consistency among sibling classes. Both work in VLM's shared embedding space with lightweight LoRA adaptation.

Result: Experiments across multiple benchmarks show consistent improvements in Full-Path Accuracy and Tree-based Inconsistency Error with minimal parameter overhead.

Conclusion: The approach provides an efficient strategy for adapting VLMs to structured taxonomies while maintaining hierarchical consistency.

Abstract: Vision-Language Models (VLMs) learn powerful multimodal representations through large-scale image-text pretraining, but adapting them to hierarchical classification is underexplored. Standard approaches treat labels as flat categories and require full fine-tuning, which is expensive and produces inconsistent predictions across taxonomy levels. We propose an efficient hierarchy-aware fine-tuning framework that updates a few parameters while enforcing structural consistency. We combine two objectives: Tree-Path KL Divergence (TP-KL) aligns predictions along the ground-truth label path for vertical coherence, while Hierarchy-Sibling Smoothed Cross-Entropy (HiSCE) encourages consistent predictions among sibling classes. Both losses work in the VLM's shared embedding space and integrate with lightweight LoRA adaptation. Experiments across multiple benchmarks show consistent improvements in Full-Path Accuracy and Tree-based Inconsistency Error with minimal parameter overhead. Our approach provides an efficient strategy for adapting VLMs to structured taxonomies.

</details>


### [16] [Vision Transformers are Circulant Attention Learners](https://arxiv.org/abs/2512.21542)
*Dongchen Han,Tianyu Li,Ziyi Wang,Gao Huang*

Main category: cs.CV

TL;DR: Circulant Attention reduces self-attention's quadratic complexity to O(N log N) by modeling attention maps as Block Circulant matrices with Circulant Blocks (BCCB), maintaining model capacity while enabling efficient computation for vision Transformers.


<details>
  <summary>Details</summary>
Motivation: Self-attention's quadratic complexity creates heavy computational burden in high-resolution vision scenarios, limiting practical applications. Previous methods that introduce handcrafted patterns like locality or sparsity compromise model capacity, so there's a need for efficient attention that preserves capacity.

Method: The authors identify that self-attention matrices in vision Transformers often approximate Block Circulant matrices with Circulant Blocks (BCCB). They explicitly model attention maps as their nearest BCCB matrices and develop an efficient computation algorithm that enables O(N log N) complexity while closely mirroring vanilla self-attention.

Result: Extensive experiments on diverse visual tasks demonstrate the effectiveness of Circulant Attention, establishing it as a promising alternative to self-attention for vision Transformer architectures with significantly reduced computational complexity.

Conclusion: Circulant Attention leverages the inherent efficient pattern of self-attention to achieve O(N log N) computation complexity while largely maintaining the capacity of standard self-attention, making it a practical solution for high-resolution vision applications.

Abstract: The self-attention mechanism has been a key factor in the advancement of vision Transformers. However, its quadratic complexity imposes a heavy computational burden in high-resolution scenarios, restricting the practical application. Previous methods attempt to mitigate this issue by introducing handcrafted patterns such as locality or sparsity, which inevitably compromise model capacity. In this paper, we present a novel attention paradigm termed \textbf{Circulant Attention} by exploiting the inherent efficient pattern of self-attention. Specifically, we first identify that the self-attention matrix in vision Transformers often approximates the Block Circulant matrix with Circulant Blocks (BCCB), a kind of structured matrix whose multiplication with other matrices can be performed in $\mathcal{O}(N\log N)$ time. Leveraging this interesting pattern, we explicitly model the attention map as its nearest BCCB matrix and propose an efficient computation algorithm for fast calculation. The resulting approach closely mirrors vanilla self-attention, differing only in its use of BCCB matrices. Since our design is inspired by the inherent efficient paradigm, it not only delivers $\mathcal{O}(N\log N)$ computation complexity, but also largely maintains the capacity of standard self-attention. Extensive experiments on diverse visual tasks demonstrate the effectiveness of our approach, establishing circulant attention as a promising alternative to self-attention for vision Transformer architectures. Code is available at https://github.com/LeapLabTHU/Circulant-Attention.

</details>


### [17] [EraseLoRA: MLLM-Driven Foreground Exclusion and Background Subtype Aggregation for Dataset-Free Object Removal](https://arxiv.org/abs/2512.21545)
*Sanghyun Jo,Donghwan Lee,Eunji Jung,Seong Je Oh,Kyungsu Kim*

Main category: cs.CV

TL;DR: EraseLoRA: A dataset-free framework for object removal that uses background-aware reasoning and test-time adaptation instead of attention manipulation, achieving better results by separating foreground/background and enforcing consistent integration.


<details>
  <summary>Details</summary>
Motivation: Object removal requires preventing target reappearance and reconstructing occluded background with structural fidelity, not just plausible hole filling. Existing dataset-free approaches that manipulate self-attention fail by misinterpreting non-target foregrounds as background (causing unwanted regeneration) and disrupting fine details.

Method: Two-stage approach: 1) Background-aware Foreground Exclusion (BFE) uses multimodal LLMs to separate target foreground, non-target foregrounds, and clean background from single image-mask pairs without supervision. 2) Background-aware Reconstruction with Subtype Aggregation (BRSA) performs test-time optimization treating background subtypes as complementary pieces, enforcing consistent integration through reconstruction and alignment objectives without explicit attention intervention.

Result: EraseLoRA works as a plug-in to pretrained diffusion models, showing consistent improvements over dataset-free baselines and competitive results against dataset-driven methods across object removal benchmarks.

Conclusion: The proposed framework successfully addresses limitations of attention-based approaches by replacing attention surgery with background-aware reasoning and test-time adaptation, achieving reliable object removal without requiring paired supervision datasets.

Abstract: Object removal differs from common inpainting, since it must prevent the masked target from reappearing and reconstruct the occluded background with structural and contextual fidelity, rather than merely filling a hole plausibly. Recent dataset-free approaches that redirect self-attention inside the mask fail in two ways: non-target foregrounds are often misinterpreted as background, which regenerates unwanted objects, and direct attention manipulation disrupts fine details and hinders coherent integration of background cues. We propose EraseLoRA, a novel dataset-free framework that replaces attention surgery with background-aware reasoning and test-time adaptation. First, Background-aware Foreground Exclusion (BFE), uses a multimodal large-language models to separate target foreground, non-target foregrounds, and clean background from a single image-mask pair without paired supervision, producing reliable background cues while excluding distractors. Second, Background-aware Reconstruction with Subtype Aggregation (BRSA), performs test-time optimization that treats inferred background subtypes as complementary pieces and enforces their consistent integration through reconstruction and alignment objectives, preserving local detail and global structure without explicit attention intervention. We validate EraseLoRA as a plug-in to pretrained diffusion models and across benchmarks for object removal, demonstrating consistent improvements over dataset-free baselines and competitive results against dataset-driven methods. The code will be made available upon publication.

</details>


### [18] [Toward Intelligent Scene Augmentation for Context-Aware Object Placement and Sponsor-Logo Integration](https://arxiv.org/abs/2512.21560)
*Unnati Saraswat,Tarun Rao,Namah Gupta,Shweta Swami,Shikhar Sharma,Prateek Narang,Dhruv Kumar*

Main category: cs.CV

TL;DR: The paper introduces two new tasks for advertising/digital media: context-aware object insertion and sponsor-product logo augmentation, along with corresponding datasets.


<details>
  <summary>Details</summary>
Motivation: Existing image editing methods using VLMs and diffusion models often fail to ensure contextual appropriateness of inserted objects, particularly for advertising and digital media applications.

Method: Introduces two new tasks and builds two new datasets with category annotations, placement regions, and sponsor-product labels to support these tasks.

Result: Two new tasks defined: (1) context-aware object insertion requiring category prediction, generation, and plausible placement; (2) sponsor-product logo augmentation involving product detection and correct brand logo insertion.

Conclusion: The paper addresses the gap in contextual appropriateness for image editing in advertising/digital media by introducing specialized tasks and datasets to enable more intelligent and contextually-aware visual manipulation.

Abstract: Intelligent image editing increasingly relies on advances in computer vision, multimodal reasoning, and generative modeling. While vision-language models (VLMs) and diffusion models enable guided visual manipulation, existing work rarely ensures that inserted objects are \emph{contextually appropriate}. We introduce two new tasks for advertising and digital media: (1) \emph{context-aware object insertion}, which requires predicting suitable object categories, generating them, and placing them plausibly within the scene; and (2) \emph{sponsor-product logo augmentation}, which involves detecting products and inserting correct brand logos, even when items are unbranded or incorrectly branded. To support these tasks, we build two new datasets with category annotations, placement regions, and sponsor-product labels.

</details>


### [19] [Exploration of Reproducible Generated Image Detection](https://arxiv.org/abs/2512.21562)
*Yihang Duan*

Main category: cs.CV

TL;DR: This paper analyzes reproducibility and generalizability issues in AIGC image detection, finding that omitted experimental details and overfitting to specific generator features are the main problems.


<details>
  <summary>Details</summary>
Motivation: The field of AI-Generated Content (AIGC) image detection faces two core problems: poor reproducibility and insufficient generalizability, which hinder practical application of these technologies.

Method: The study reviews 7 key papers on AIGC detection, constructs a lightweight test dataset, and reproduces a representative detection method to analyze reproducibility issues.

Result: Basic performance can be reproduced when strictly following core procedures, but detection performance drops sharply when preprocessing disrupts key features or when testing across different generators. The root causes are omitted implicit details and overfitting to exclusive generator features.

Conclusion: The research provides empirical evidence for improving AIGC detection reproducibility and offers reference directions for researchers to disclose experimental details more comprehensively and verify method generalizability.

Abstract: While the technology for detecting AI-Generated Content (AIGC) images has advanced rapidly, the field still faces two core issues: poor reproducibility and insufficient gen eralizability, which hinder the practical application of such technologies. This study addresses these challenges by re viewing 7 key papers on AIGC detection, constructing a lightweight test dataset, and reproducing a representative detection method. Through this process, we identify the root causes of the reproducibility dilemma in the field: firstly, papers often omit implicit details such as prepro cessing steps and parameter settings; secondly, most detec tion methods overfit to exclusive features of specific gener ators rather than learning universal intrinsic features of AIGC images. Experimental results show that basic perfor mance can be reproduced when strictly following the core procedures described in the original papers. However, de tection performance drops sharply when preprocessing dis rupts key features or when testing across different genera tors. This research provides empirical evidence for improv ing the reproducibility of AIGC detection technologies and offers reference directions for researchers to disclose ex perimental details more comprehensively and verify the generalizability of their proposed methods.

</details>


### [20] [Towards Long-window Anchoring in Vision-Language Model Distillation](https://arxiv.org/abs/2512.21576)
*Haoyi Zhou,Shuo Li,Tianyu Chen,Qi Song,Chonghan Gao,Jianxin Li*

Main category: cs.CV

TL;DR: LAid is a knowledge distillation method that transfers long-range attention mechanisms from large to small vision-language models, enabling 3.2x longer effective context windows while maintaining performance on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: Small vision-language models struggle with long-context understanding due to limited window sizes and poor linguistics-photography alignment. While large VLMs have strong long-context capabilities, their small branches fail to capture this ability effectively.

Method: LAid uses two complementary components: (1) progressive distance-weighted attention matching that dynamically emphasizes longer position differences during training, and (2) learnable RoPE response gain modulation that selectively amplifies position sensitivity where needed.

Result: LAid-distilled models achieve up to 3.2 times longer effective context windows compared to baseline small models while maintaining or improving performance on standard VL benchmarks. Spectral analysis shows LAid preserves crucial low-frequency attention components that conventional methods fail to transfer.

Conclusion: LAid provides practical techniques for building more efficient long-context VLMs and offers theoretical insights into how positional understanding emerges and transfers during distillation, addressing the limitation of small models in handling long-context vision-language tasks.

Abstract: While large vision-language models (VLMs) demonstrate strong long-context understanding, their prevalent small branches fail on linguistics-photography alignment for a limited window size. We discover that knowledge distillation improves students' capability as a complement to Rotary Position Embeddings (RoPE) on window sizes (anchored from large models). Building on this insight, we propose LAid, which directly aims at the transfer of long-range attention mechanisms through two complementary components: (1) a progressive distance-weighted attention matching that dynamically emphasizes longer position differences during training, and (2) a learnable RoPE response gain modulation that selectively amplifies position sensitivity where needed. Extensive experiments across multiple model families demonstrate that LAid-distilled models achieve up to 3.2 times longer effective context windows compared to baseline small models, while maintaining or improving performance on standard VL benchmarks. Spectral analysis also suggests that LAid successfully preserves crucial low-frequency attention components that conventional methods fail to transfer. Our work not only provides practical techniques for building more efficient long-context VLMs but also offers theoretical insights into how positional understanding emerges and transfers during distillation.

</details>


### [21] [LLM-Free Image Captioning Evaluation in Reference-Flexible Settings](https://arxiv.org/abs/2512.21582)
*Shinnosuke Hirano,Yuiga Wada,Kazuki Matsuda,Seitaro Otsuki,Komei Sugiura*

Main category: cs.CV

TL;DR: Pearl is an LLM-free supervised metric for image caption evaluation that works in both reference-based and reference-free settings, outperforming existing LLM-free metrics on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based metrics favor their own generations (lack neutrality), while most LLM-free metrics don't always show high performance. There's a need for a neutral yet high-performing metric for image caption evaluation.

Method: Proposes Pearl, an LLM-free supervised metric that learns representations of image-caption and caption-caption similarities. Also constructs a large human-annotated dataset (~333k judgments from 2,360 annotators across 75k+ images).

Result: Pearl outperformed other existing LLM-free metrics on Composite, Flickr8K-Expert, Flickr8K-CF, Nebula, and FOIL datasets in both reference-based and reference-free settings.

Conclusion: Pearl provides an effective LLM-free solution for image caption evaluation that addresses neutrality concerns while maintaining high performance across multiple benchmark datasets.

Abstract: We focus on the automatic evaluation of image captions in both reference-based and reference-free settings. Existing metrics based on large language models (LLMs) favor their own generations; therefore, the neutrality is in question. Most LLM-free metrics do not suffer from such an issue, whereas they do not always demonstrate high performance. To address these issues, we propose Pearl, an LLM-free supervised metric for image captioning, which is applicable to both reference-based and reference-free settings. We introduce a novel mechanism that learns the representations of image--caption and caption--caption similarities. Furthermore, we construct a human-annotated dataset for image captioning metrics, that comprises approximately 333k human judgments collected from 2,360 annotators across over 75k images. Pearl outperformed other existing LLM-free metrics on the Composite, Flickr8K-Expert, Flickr8K-CF, Nebula, and FOIL datasets in both reference-based and reference-free settings. Our project page is available at https://pearl.kinsta.page/.

</details>


### [22] [UltraLBM-UNet: Ultralight Bidirectional Mamba-based Model for Skin Lesion Segmentation](https://arxiv.org/abs/2512.21584)
*Linxuan Fan,Juntao Jiang,Weixuan Liu,Zhucun Xue,Jiajun Lv,Jiangning Zhang,Yong Liu*

Main category: cs.CV

TL;DR: UltraLBM-UNet: A lightweight U-Net variant using bidirectional Mamba for skin lesion segmentation with only 0.034M parameters, achieving SOTA performance and enabling point-of-care deployment.


<details>
  <summary>Details</summary>
Motivation: Existing skin lesion segmentation methods have limitations in accuracy, robustness, and computational efficiency, making them unsuitable for point-of-care clinical deployment where resource constraints exist.

Method: Proposes UltraLBM-UNet, a lightweight U-Net variant integrating bidirectional Mamba-based global modeling with multi-branch local feature perception. Uses efficient local feature injection with bidirectional state-space modeling for rich contextual interactions while maintaining computational compactness. Also introduces hybrid knowledge distillation to train an ultra-compact student model (UltraLBM-UNet-T).

Result: Achieves state-of-the-art segmentation accuracy on ISIC 2017, ISIC 2018, and PH2 datasets with only 0.034M parameters and 0.060 GFLOPs. The distilled variant UltraLBM-UNet-T achieves competitive performance with only 0.011M parameters and 0.019 GFLOPs.

Conclusion: UltraLBM-UNet provides an accurate, robust, and resource-efficient solution for skin lesion segmentation, making it highly suitable for point-of-care clinical deployment where computational resources are limited but accurate lesion analysis is essential.

Abstract: Skin lesion segmentation is a crucial step in dermatology for guiding clinical decision-making. However, existing methods for accurate, robust, and resource-efficient lesion analysis have limitations, including low performance and high computational complexity. To address these limitations, we propose UltraLBM-UNet, a lightweight U-Net variant that integrates a bidirectional Mamba-based global modeling mechanism with multi-branch local feature perception. The proposed architecture integrates efficient local feature injection with bidirectional state-space modeling, enabling richer contextual interaction across spatial dimensions while maintaining computational compactness suitable for point-of-care deployment. Extensive experiments on the ISIC 2017, ISIC 2018, and PH2 datasets demonstrate that our model consistently achieves state-of-the-art segmentation accuracy, outperforming existing lightweight and Mamba counterparts with only 0.034M parameters and 0.060 GFLOPs. In addition, we introduce a hybrid knowledge distillation strategy to train an ultra-compact student model, where the distilled variant UltraLBM-UNet-T, with only 0.011M parameters and 0.019 GFLOPs, achieves competitive segmentation performance. These results highlight the suitability of UltraLBM-UNet for point-of-care deployment, where accurate and robust lesion analyses are essential. The source code is publicly available at https://github.com/LinLinLin-X/UltraLBM-UNet.

</details>


### [23] [From Shallow Humor to Metaphor: Towards Label-Free Harmful Meme Detection via LMM Agent Self-Improvement](https://arxiv.org/abs/2512.21598)
*Jian Lang,Rongpei Hong,Ting Zhong,Leiting Chen,Qiang Gao,Fan Zhou*

Main category: cs.CV

TL;DR: ALARM is a label-free harmful meme detection framework using Large Multimodal Model agents that self-improve by learning from explicit memes to handle complex ones.


<details>
  <summary>Details</summary>
Motivation: Harmful meme detection traditionally requires large labeled datasets, which are expensive to create and can't adapt quickly to evolving online content. Current methods lack scalability for dynamic online environments.

Method: ALARM uses a Confidence-based Explicit Meme Identification mechanism to isolate explicit memes and assign pseudo-labels, then employs Pairwise Learning Guided Agent Self-Improvement where explicit memes are organized into contrastive pairs to train a learner LMM agent that autonomously derives detection cues.

Result: Experiments on three diverse datasets show superior performance and strong adaptability to newly evolved memes. The method even outperforms label-driven approaches.

Conclusion: ALARM demonstrates the potential of label-free frameworks as scalable solutions for adapting to novel forms of harmful memes in dynamic online environments, offering a promising alternative to traditional supervised methods.

Abstract: The proliferation of harmful memes on online media poses significant risks to public health and stability. Existing detection methods heavily rely on large-scale labeled data for training, which necessitates substantial manual annotation efforts and limits their adaptability to the continually evolving nature of harmful content. To address these challenges, we present ALARM, the first lAbeL-free hARmful Meme detection framework powered by Large Multimodal Model (LMM) agent self-improvement. The core innovation of ALARM lies in exploiting the expressive information from "shallow" memes to iteratively enhance its ability to tackle more complex and subtle ones. ALARM consists of a novel Confidence-based Explicit Meme Identification mechanism that isolates the explicit memes from the original dataset and assigns them pseudo-labels. Besides, a new Pairwise Learning Guided Agent Self-Improvement paradigm is introduced, where the explicit memes are reorganized into contrastive pairs (positive vs. negative) to refine a learner LMM agent. This agent autonomously derives high-level detection cues from these pairs, which in turn empower the agent itself to handle complex and challenging memes effectively. Experiments on three diverse datasets demonstrate the superior performance and strong adaptability of ALARM to newly evolved memes. Notably, our method even outperforms label-driven methods. These results highlight the potential of label-free frameworks as a scalable and promising solution for adapting to novel forms and topics of harmful memes in dynamic online environments.

</details>


### [24] [GaussianEM: Model compositional and conformational heterogeneity using 3D Gaussians](https://arxiv.org/abs/2512.21599)
*Bintao He,Yiran Cheng,Hongjia Li,Xiang Gao,Xin Gao,Fa Zhang,Renmin Han*

Main category: cs.CV

TL;DR: GaussianEM: A Gaussian pseudo-atomic framework for analyzing cryo-EM datasets with continuous motions and discrete states, using Gaussian components to model structural heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Understanding protein flexibility and dynamic interactions is essential for protein function, but analyzing cryo-EM datasets containing both continuous motions and discrete states remains challenging.

Method: GaussianEM uses a Gaussian pseudo-atomic framework with a two-encoder-one-decoder architecture to map cryo-EM images to individual Gaussian components, representing structural variability through changes in Gaussian parameters.

Result: The approach provides intuitive description of conformational changes, preserves local structural consistency along transition trajectories, and bridges gap between density-based and atomic models. Demonstrated effectiveness on both simulated and experimental datasets.

Conclusion: GaussianEM offers a powerful framework for simultaneously modeling compositional and conformational heterogeneity in cryo-EM data, enabling better understanding of protein dynamics and flexibility.

Abstract: Understanding protein flexibility and its dynamic interactions with other molecules is essential for protein function study. Cryogenic electron microscopy (cryo-EM) provides an opportunity to directly observe macromolecular dynamics. However, analyzing datasets that contain both continuous motions and discrete states remains highly challenging. Here we present GaussianEM, a Gaussian pseudo-atomic framework that simultaneously models compositional and conformational heterogeneity from experimental cryo-EM images. GaussianEM employs a two-encoder-one-decoder architecture to map an image to its individual Gaussian components, and represent structural variability through changes in Gaussian parameters. This approach provides an intuitive and interpretable description of conformational changes, preserves local structural consistency along the transition trajectories, and naturally bridges the gap between density-based models and corresponding atomic models. We demonstrate the effectiveness of GaussianEM on both simulated and experimental datasets.

</details>


### [25] [TAMEing Long Contexts in Personalization: Towards Training-Free and State-Aware MLLM Personalized Assistant](https://arxiv.org/abs/2512.21616)
*Rongpei Hong,Jian Lang,Ting Zhong,Yong Wang,Fan Zhou*

Main category: cs.CV

TL;DR: LCMP is the first benchmark for evaluating long-context multimodal LLM personalization, and TAME is a training-free framework that achieves state-of-the-art performance on it.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM personalization methods only handle simple visual identification and textual replacement, lacking support for long-context conversations where personalized concepts evolve over time. There's a need for benchmarks and methods that can handle continuous learning from dialogue histories.

Method: Proposes LCMP benchmark for evaluating long-context personalization, and TAME framework with double memories (temporal & persistent) to manage concept variations, plus RA2G (Retrieve-then-Align Augmented Generation) paradigm for contextually fitting retrieved knowledge to current queries.

Result: TAME achieves best performance on LCMP benchmark, demonstrating remarkable and evolving interaction experiences in long-context scenarios.

Conclusion: LCMP addresses the gap in long-context MLLM personalization evaluation, and TAME provides an effective training-free solution with state-aware memory management and context alignment capabilities.

Abstract: Multimodal Large Language Model (MLLM) Personalization is a critical research problem that facilitates personalized dialogues with MLLMs targeting specific entities (known as personalized concepts). However, existing methods and benchmarks focus on the simple, context-agnostic visual identification and textual replacement of the personalized concept (e.g., "A yellow puppy" -> "Your puppy Mochi"), overlooking the ability to support long-context conversations. An ideal personalized MLLM assistant is capable of engaging in long-context dialogues with humans and continually improving its experience quality by learning from past dialogue histories. To bridge this gap, we propose LCMP, the first Long-Context MLLM Personalization evaluation benchmark. LCMP assesses the capability of MLLMs in perceiving variations of personalized concepts and generating contextually appropriate personalized responses that reflect these variations. As a strong baseline for LCMP, we introduce a novel training-free and state-aware framework TAME. TAME endows MLLMs with double memories to manage the temporal and persistent variations of each personalized concept in a differentiated manner. In addition, TAME incorporates a new training-free Retrieve-then-Align Augmented Generation (RA2G) paradigm. RA2G introduces an alignment step to extract the contextually fitted information from the multi-memory retrieved knowledge to the current questions, enabling better interactions for complex real-world user queries. Experiments on LCMP demonstrate that TAME achieves the best performance, showcasing remarkable and evolving interaction experiences in long-context scenarios.

</details>


### [26] [CausalFSFG: Rethinking Few-Shot Fine-Grained Visual Categorization from Causal Perspective](https://arxiv.org/abs/2512.21617)
*Zhiwen Yang,Jinglin Xu,Yuxin Pen*

Main category: cs.CV

TL;DR: Proposes CausalFSFG, a causal inference approach for few-shot fine-grained visual categorization that addresses biased data distributions through sample-level and feature-level interventions.


<details>
  <summary>Details</summary>
Motivation: Existing FS-FGVC methods overlook that support samples act as confounding variables, introducing biased data distributions and misleading discriminative feature extraction, which hampers performance.

Method: Uses structural causal modeling to identify spurious correlations, then implements two interventions: Interventional multi-scale encoder (IMSE) for sample-level interventions and Interventional masked feature reconstruction (IMFR) for feature-level interventions.

Result: Achieves state-of-the-art performance on CUB-200-2011, Stanford Dogs, and Stanford Cars datasets through extensive experiments and thorough analyses.

Conclusion: Causal inference effectively addresses biased distributions in FS-FGVC, with sample-level and feature-level interventions revealing real causalities from inputs to subcategories, leading to superior performance.

Abstract: Few-shot fine-grained visual categorization (FS-FGVC) focuses on identifying various subcategories within a common superclass given just one or few support examples. Most existing methods aim to boost classification accuracy by enriching the extracted features with discriminative part-level details. However, they often overlook the fact that the set of support samples acts as a confounding variable, which hampers the FS-FGVC performance by introducing biased data distribution and misguiding the extraction of discriminative features. To address this issue, we propose a new causal FS-FGVC (CausalFSFG) approach inspired by causal inference for addressing biased data distributions through causal intervention. Specifically, based on the structural causal model (SCM), we argue that FS-FGVC infers the subcategories (i.e., effect) from the inputs (i.e., cause), whereas both the few-shot condition disturbance and the inherent fine-grained nature (i.e., large intra-class variance and small inter-class variance) lead to unobservable variables that bring spurious correlations, compromising the final classification performance. To further eliminate the spurious correlations, our CausalFSFG approach incorporates two key components: (1) Interventional multi-scale encoder (IMSE) conducts sample-level interventions, (2) Interventional masked feature reconstruction (IMFR) conducts feature-level interventions, which together reveal real causalities from inputs to subcategories. Extensive experiments and thorough analyses on the widely-used public datasets, including CUB-200-2011, Stanford Dogs, and Stanford Cars, demonstrate that our CausalFSFG achieves new state-of-the-art performance. The code is available at https://github.com/PKU-ICST-MIPL/CausalFSFG_TMM.

</details>


### [27] [SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration](https://arxiv.org/abs/2512.21618)
*Zhiyuan Liu,Daocheng Fu,Pinlong Cai,Lening Wang,Ying Liu,Yilong Ren,Botian Shi,Jianqiang Wang*

Main category: cs.CV

TL;DR: SymDrive: A unified diffusion framework for high-quality 3D scene rendering and interactive traffic editing in autonomous driving simulation.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D autonomous driving simulation struggle to achieve both photorealistic rendering and interactive traffic editing simultaneously, often suffering from large-angle view synthesis issues and geometric/lighting artifacts during asset manipulation.

Method: Proposes SymDrive with Symmetric Auto-regressive Online Restoration paradigm that constructs paired symmetric views for detail recovery using ground-truth-guided dual-view formulation, and auto-regressive strategy for consistent lateral view generation. Also introduces training-free harmonization mechanism treating vehicle insertion as context-aware inpainting.

Result: Extensive experiments show SymDrive achieves state-of-the-art performance in both novel-view enhancement and realistic 3D vehicle insertion.

Conclusion: SymDrive provides a unified solution for high-fidelity, controllable 3D simulation that addresses data scarcity in autonomous driving through simultaneous high-quality rendering and scene editing capabilities.

Abstract: High-fidelity and controllable 3D simulation is essential for addressing the long-tail data scarcity in Autonomous Driving (AD), yet existing methods struggle to simultaneously achieve photorealistic rendering and interactive traffic editing. Current approaches often falter in large-angle novel view synthesis and suffer from geometric or lighting artifacts during asset manipulation. To address these challenges, we propose SymDrive, a unified diffusion-based framework capable of joint high-quality rendering and scene editing. We introduce a Symmetric Auto-regressive Online Restoration paradigm, which constructs paired symmetric views to recover fine-grained details via a ground-truth-guided dual-view formulation and utilizes an auto-regressive strategy for consistent lateral view generation. Furthermore, we leverage this restoration capability to enable a training-free harmonization mechanism, treating vehicle insertion as context-aware inpainting to ensure seamless lighting and shadow consistency. Extensive experiments demonstrate that SymDrive achieves state-of-the-art performance in both novel-view enhancement and realistic 3D vehicle insertion.

</details>


### [28] [Training-Free Disentangled Text-Guided Image Editing via Sparse Latent Constraints](https://arxiv.org/abs/2512.21637)
*Mutiara Shabrina,Nova Kurnia Putri,Jefri Satria Ferdiansyah,Sabita Khansa Dewi,Novanto Yudistira*

Main category: cs.CV

TL;DR: The paper introduces a sparsity-based constraint using L1 regularization to improve disentangled image editing in the PPE framework, reducing unintended attribute changes while preserving identity.


<details>
  <summary>Details</summary>
Motivation: Text-driven image manipulation often suffers from attribute entanglement, where modifying one attribute unintentionally alters other semantic properties. The original PPE framework's regularization strategy has limitations with dense latent updates prone to semantic leakage.

Method: Analyzes the PPE framework's architecture (BERT-based attribute prediction, StyleGAN2-based generation on CelebA-HQ), identifies limitations in original regularization, and proposes a sparsity-based constraint using L1 regularization on latent space manipulation.

Result: Experimental results show the proposed approach enforces more focused and controlled edits, effectively reducing unintended changes in non-target attributes while preserving facial identity.

Conclusion: L1 regularization on latent space manipulation improves disentangled editing by enforcing sparsity, addressing semantic leakage issues in the original PPE framework for more precise text-driven image manipulation.

Abstract: Text-driven image manipulation often suffers from attribute entanglement, where modifying a target attribute (e.g., adding bangs) unintentionally alters other semantic properties such as identity or appearance. The Predict, Prevent, and Evaluate (PPE) framework addresses this issue by leveraging pre-trained vision-language models for disentangled editing. In this work, we analyze the PPE framework, focusing on its architectural components, including BERT-based attribute prediction and StyleGAN2-based image generation on the CelebA-HQ dataset. Through empirical analysis, we identify a limitation in the original regularization strategy, where latent updates remain dense and prone to semantic leakage. To mitigate this issue, we introduce a sparsity-based constraint using L1 regularization on latent space manipulation. Experimental results demonstrate that the proposed approach enforces more focused and controlled edits, effectively reducing unintended changes in non-target attributes while preserving facial identity.

</details>


### [29] [TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References](https://arxiv.org/abs/2512.21641)
*Jiahong Yu,Ziqi Wang,Hailiang Zhao,Wei Zhai,Xueqiang Yan,Shuiguang Deng*

Main category: cs.CV

TL;DR: TrackTeller is a temporal multimodal framework for 3D object grounding that uses LiDAR-image fusion and temporal reasoning to understand language references in dynamic driving scenes, achieving significant improvements in tracking accuracy.


<details>
  <summary>Details</summary>
Motivation: Many referring expressions in driving scenes describe objects through recent motion or short-term interactions, which cannot be resolved from static appearance or geometry alone. Current methods fail to effectively leverage temporal information for language-based 3D grounding.

Method: TrackTeller integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning in a unified architecture. It constructs a shared UniScene representation aligned with textual semantics, generates language-aware 3D proposals, and refines grounding decisions using motion history and short-term dynamics.

Result: On the NuPrompt benchmark, TrackTeller consistently improves language-grounded tracking performance, outperforming strong baselines with a 70% relative improvement in Average Multi-Object Tracking Accuracy and a 3.15-3.4 times reduction in False Alarm Frequency.

Conclusion: Temporal reasoning is crucial for understanding language references in dynamic 3D scenes, and TrackTeller's unified multimodal approach effectively leverages motion history and short-term dynamics to improve 3D object grounding performance.

Abstract: Understanding natural-language references to objects in dynamic 3D driving scenes is essential for interactive autonomous systems. In practice, many referring expressions describe targets through recent motion or short-term interactions, which cannot be resolved from static appearance or geometry alone. We study temporal language-based 3D grounding, where the objective is to identify the referred object in the current frame by leveraging multi-frame observations. We propose TrackTeller, a temporal multimodal grounding framework that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning in a unified architecture. TrackTeller constructs a shared UniScene representation aligned with textual semantics, generates language-aware 3D proposals, and refines grounding decisions using motion history and short-term dynamics. Experiments on the NuPrompt benchmark demonstrate that TrackTeller consistently improves language-grounded tracking performance, outperforming strong baselines with a 70% relative improvement in Average Multi-Object Tracking Accuracy and a 3.15-3.4 times reduction in False Alarm Frequency.

</details>


### [30] [Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding](https://arxiv.org/abs/2512.21643)
*Zhiwang Zhou,Yuandong Pu,Xuming He,Yidi Liu,Yixin Chen,Junchao Gong,Xiang Zhuang,Wanghan Xu,Qinglong Cao,Shixiang Tang,Yihao Liu,Wenlong Zhang,Lei Bai*

Main category: cs.CV

TL;DR: Omni-Weather is the first multimodal foundation model that unifies weather generation and understanding in a single architecture, achieving SOTA performance in both tasks through shared self-attention and Chain-of-Thought reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing weather modeling methods treat accurate prediction and mechanistic interpretation in isolation, separating generation from understanding. There's a need to bridge this gap by creating a unified approach that can both generate weather data and understand weather mechanisms.

Method: Omni-Weather uses a multimodal foundation model with a radar encoder for weather generation tasks, followed by unified processing using a shared self-attention mechanism. The authors also construct a Chain-of-Thought dataset for causal reasoning in weather generation to enable interpretable outputs and improved perceptual quality.

Result: Extensive experiments show Omni-Weather achieves state-of-the-art performance in both weather generation and understanding. The model demonstrates that generative and understanding tasks in the weather domain can mutually enhance each other.

Conclusion: Omni-Weather demonstrates the feasibility and value of unifying weather generation and understanding within a single architecture, offering both accurate prediction and mechanistic interpretation capabilities.

Abstract: Weather modeling requires both accurate prediction and mechanistic interpretation, yet existing methods treat these goals in isolation, separating generation from understanding. To address this gap, we present Omni-Weather, the first multimodal foundation model that unifies weather generation and understanding within a single architecture. Omni-Weather integrates a radar encoder for weather generation tasks, followed by unified processing using a shared self-attention mechanism. Moreover, we construct a Chain-of-Thought dataset for causal reasoning in weather generation, enabling interpretable outputs and improved perceptual quality. Extensive experiments show Omni-Weather achieves state-of-the-art performance in both weather generation and understanding. Our findings further indicate that generative and understanding tasks in the weather domain can mutually enhance each other. Omni-Weather also demonstrates the feasibility and value of unifying weather generation and understanding.

</details>


### [31] [The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds](https://arxiv.org/abs/2512.21670)
*Subramanyam Sahoo,Jared Junkin*

Main category: cs.CV

TL;DR: A mechanistic interpretability framework for deepfake detection that combines sparse autoencoder analysis with forensic manifold analysis to understand model decision processes.


<details>
  <summary>Details</summary>
Motivation: Deepfake detection models achieve high accuracy but remain opaque "black boxes" - there's a need to understand their internal decision processes and which features they use to identify synthetic media.

Method: Combines sparse autoencoder (SAE) analysis of internal network representations with novel forensic manifold analysis that probes how model features respond to controlled forensic artifact manipulations.

Result: Only a small fraction of latent features are actively used in each layer, and geometric properties of the model's feature manifold (intrinsic dimensionality, curvature, feature selectivity) vary systematically with different deepfake artifacts.

Conclusion: Provides first steps toward opening the "black box" of deepfake detectors, enabling identification of learned features corresponding to specific forensic artifacts and guiding development of more interpretable and robust models.

Abstract: Deepfake detection models have achieved high accuracy in identifying synthetic media, but their decision processes remain largely opaque. In this paper we present a mechanistic interpretability framework for deepfake detection applied to a vision-language model. Our approach combines a sparse autoencoder (SAE) analysis of internal network representations with a novel forensic manifold analysis that probes how the model's features respond to controlled forensic artifact manipulations. We demonstrate that only a small fraction of latent features are actively used in each layer, and that the geometric properties of the model's feature manifold, including intrinsic dimensionality, curvature, and feature selectivity, vary systematically with different types of deepfake artifacts. These insights provide a first step toward opening the "black box" of deepfake detectors, allowing us to identify which learned features correspond to specific forensic artifacts and to guide the development of more interpretable and robust models.

</details>


### [32] [Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles](https://arxiv.org/abs/2512.21673)
*Jalal Khan*

Main category: cs.CV

TL;DR: This paper compares YOLOv8s and YOLO-NAS for autonomous vehicle perception tasks, finding YOLOv8s achieves 83% accuracy vs 81% for YOLO-NAS while saving 75% training time.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles require efficient, safe, and reliable perception systems for detecting objects like vehicles, pedestrians, and road signs. With many ML/DL algorithms emerging, there's a need to compare their performance in real-world scenarios to inform the research community.

Method: The researchers captured a custom dataset and conducted comparative experiments with two deep learning models: YOLO-NAS and YOLOv8. They evaluated both models on detection-based perception tasks using their custom dataset.

Result: YOLOv8s outperformed YOLO-NAS in both efficiency and accuracy: it saved 75% of training time compared to YOLO-NAS, and achieved 83% object detection accuracy versus 81% for YOLO-NAS.

Conclusion: YOLOv8s is more efficient and accurate than YOLO-NAS for autonomous vehicle perception tasks. This comparative analysis provides valuable insights for the research community on model performance in real-world scenarios.

Abstract: Recently, a plethora of machine learning (ML) and deep learning (DL) algorithms have been proposed to achieve the efficiency, safety, and reliability of autonomous vehicles (AVs). The AVs use a perception system to detect, localize, and identify other vehicles, pedestrians, and road signs to perform safe navigation and decision-making. In this paper, we compare the performance of DL models, including YOLO-NAS and YOLOv8, for a detection-based perception task. We capture a custom dataset and experiment with both DL models using our custom dataset. Our analysis reveals that the YOLOv8s model saves 75% of training time compared to the YOLO-NAS model. In addition, the YOLOv8s model (83%) outperforms the YOLO-NAS model (81%) when the target is to achieve the highest object detection accuracy. These comparative analyses of these new emerging DL models will allow the relevant research community to understand the models' performance under real-world use case scenarios.

</details>


### [33] [UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture](https://arxiv.org/abs/2512.21675)
*Shuo Cao,Jiayang Li,Xiaohui Li,Yuandong Pu,Kaiwen Zhu,Yuanting Gao,Siqi Luo,Yi Xin,Qi Qin,Yu Zhou,Xiangyu Chen,Wenlong Zhang,Bin Fu,Yu Qiao,Yihao Liu*

Main category: cs.CV

TL;DR: UniPercept-Bench is a unified framework for evaluating perceptual-level image understanding across aesthetics, quality, structure, and texture domains, with a strong baseline model that outperforms existing MLLMs.


<details>
  <summary>Details</summary>
Motivation: Current multimodal large language models (MLLMs) have made significant progress in visual understanding tasks but remain limited in their ability to perceive perceptual-level image features like aesthetics, quality, structure, and texture.

Method: The authors establish a hierarchical definition system and construct large-scale datasets for perceptual-level image understanding. They develop UniPercept using Domain-Adaptive Pre-Training and Task-Aligned Reinforcement Learning, creating a unified framework that handles both Visual Rating and Visual Question Answering tasks.

Result: UniPercept outperforms existing MLLMs on perceptual-level image understanding tasks and can serve as a plug-and-play reward model for text-to-image generation.

Conclusion: This work defines Perceptual-Level Image Understanding in the MLLM era and provides a comprehensive benchmark with a strong baseline, establishing a solid foundation for advancing perceptual-level multimodal image understanding.

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding.

</details>


### [34] [Contrastive Graph Modeling for Cross-Domain Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2512.21683)
*Yuntian Bo,Tao Zhou,Zechao Li,Haofeng Zhang,Ling Shao*

Main category: cs.CV

TL;DR: C-Graph: A contrastive graph modeling framework for cross-domain few-shot medical image segmentation that leverages structural consistency as domain-transferable prior, outperforming existing methods while preserving source-domain accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing cross-domain few-shot medical image segmentation methods filter out domain-specific information to improve generalization, but this limits cross-domain performance and degrades source-domain accuracy. There's a need for a method that can effectively transfer knowledge across domains without sacrificing source-domain performance.

Method: Proposes Contrastive Graph Modeling (C-Graph) framework that represents image features as graphs (pixels as nodes, semantic affinities as edges). Includes: 1) Structural Prior Graph (SPG) layer to capture target-category node dependencies and enable global structure modeling, 2) Subgraph Matching Decoding (SMD) mechanism that exploits semantic relations among nodes to guide prediction, and 3) Confusion-minimizing Node Contrast (CNC) loss to mitigate node ambiguity and subgraph heterogeneity through contrastive learning.

Result: Significantly outperforms prior CD-FSMIS approaches across multiple cross-domain benchmarks, achieving state-of-the-art performance while simultaneously preserving strong segmentation accuracy on the source domain.

Conclusion: The C-Graph framework successfully leverages structural consistency of medical images as a reliable domain-transferable prior, addressing limitations of existing methods by preserving both cross-domain generalization and source-domain accuracy through graph-based modeling and contrastive learning techniques.

Abstract: Cross-domain few-shot medical image segmentation (CD-FSMIS) offers a promising and data-efficient solution for medical applications where annotations are severely scarce and multimodal analysis is required. However, existing methods typically filter out domain-specific information to improve generalization, which inadvertently limits cross-domain performance and degrades source-domain accuracy. To address this, we present Contrastive Graph Modeling (C-Graph), a framework that leverages the structural consistency of medical images as a reliable domain-transferable prior. We represent image features as graphs, with pixels as nodes and semantic affinities as edges. A Structural Prior Graph (SPG) layer is proposed to capture and transfer target-category node dependencies and enable global structure modeling through explicit node interactions. Building upon SPG layers, we introduce a Subgraph Matching Decoding (SMD) mechanism that exploits semantic relations among nodes to guide prediction. Furthermore, we design a Confusion-minimizing Node Contrast (CNC) loss to mitigate node ambiguity and subgraph heterogeneity by contrastively enhancing node discriminability in the graph space. Our method significantly outperforms prior CD-FSMIS approaches across multiple cross-domain benchmarks, achieving state-of-the-art performance while simultaneously preserving strong segmentation accuracy on the source domain.

</details>


### [35] [SlideChain: Semantic Provenance for Lecture Understanding via Blockchain Registration](https://arxiv.org/abs/2512.21684)
*Md Motaleb Hossen Manik,Md Zabirul Islam,Ge Wang*

Main category: cs.CV

TL;DR: SlideChain is a blockchain-based framework for verifiable integrity of multimodal semantic extraction from educational content, tested on medical lecture slides.


<details>
  <summary>Details</summary>
Motivation: VLMs are increasingly used for educational content but their semantic outputs are hard to verify, reproduce, and audit, especially in high-stakes STEM domains where inconsistencies across models and environments undermine reliability.

Method: Developed SlideChain framework that extracts concepts and relational triples from VLMs using a curated dataset of 1,117 medical imaging lecture slides, then anchors cryptographic hashes of structured provenance records on a local EVM-compatible blockchain for tamper-evident auditability.

Result: Revealed significant cross-model discrepancies (low concept overlap, near-zero agreement in relational triples), demonstrated perfect tamper detection, deterministic reproducibility, and evaluated gas usage/throughput/scalability under simulated deployment conditions.

Conclusion: SlideChain provides a practical, scalable solution for trustworthy, verifiable multimodal educational pipelines with long-term auditability, reproducibility, and integrity for AI-assisted instructional systems.

Abstract: Modern vision--language models (VLMs) are increasingly used to interpret and generate educational content, yet their semantic outputs remain challenging to verify, reproduce, and audit over time. Inconsistencies across model families, inference settings, and computing environments undermine the reliability of AI-generated instructional material, particularly in high-stakes and quantitative STEM domains. This work introduces SlideChain, a blockchain-backed provenance framework designed to provide verifiable integrity for multimodal semantic extraction at scale. Using the SlideChain Slides Dataset-a curated corpus of 1,117 medical imaging lecture slides from a university course-we extract concepts and relational triples from four state-of-the-art VLMs and construct structured provenance records for every slide. SlideChain anchors cryptographic hashes of these records on a local EVM (Ethereum Virtual Machine)-compatible blockchain, providing tamper-evident auditability and persistent semantic baselines. Through the first systematic analysis of semantic disagreement, cross-model similarity, and lecture-level variability in multimodal educational content, we reveal pronounced cross-model discrepancies, including low concept overlap and near-zero agreement in relational triples on many slides. We further evaluate gas usage, throughput, and scalability under simulated deployment conditions, and demonstrate perfect tamper detection along with deterministic reproducibility across independent extraction runs. Together, these results show that SlideChain provides a practical and scalable step toward trustworthy, verifiable multimodal educational pipelines, supporting long-term auditability, reproducibility, and integrity for AI-assisted instructional systems.

</details>


### [36] [BeHGAN: Bengali Handwritten Word Generation from Plain Text Using Generative Adversarial Networks](https://arxiv.org/abs/2512.21694)
*Md. Rakibul Islam,Md. Kamrozzaman Bhuiyan,Safwan Muntasir,Arifur Rahman Jawad,Most. Sharmin Sultana Samu*

Main category: cs.CV

TL;DR: Proposes a method for generating Bengali handwritten words using a self-collected dataset to address the lack of research in Bengali HTG compared to other languages.


<details>
  <summary>Details</summary>
Motivation: Handwritten Text Generation (HTG) is an emerging field with significant potential, but Bengali HTG has received little attention despite being the fifth most spoken language. Existing datasets are difficult to collect and not readily available, creating a research gap.

Method: Developed a self-collected dataset of Bengali handwriting samples from approximately 500 individuals across different ages and genders. All images were pre-processed for consistency and quality. Proposed a method for generating Bengali handwritten words from input plain text.

Result: The approach demonstrates the ability to produce diverse handwritten outputs from input plain text, showing promising results for Bengali handwriting generation.

Conclusion: This work contributes to advancing Bengali handwriting generation and can support further research in this area, addressing the significant gap in Bengali HTG research compared to languages like English and Arabic.

Abstract: Handwritten Text Recognition (HTR) is a well-established research area. In contrast, Handwritten Text Generation (HTG) is an emerging field with significant potential. This task is challenging due to the variation in individual handwriting styles. A large and diverse dataset is required to generate realistic handwritten text. However, such datasets are difficult to collect and are not readily available. Bengali is the fifth most spoken language in the world. While several studies exist for languages such as English and Arabic, Bengali handwritten text generation has received little attention. To address this gap, we propose a method for generating Bengali handwritten words. We developed and used a self-collected dataset of Bengali handwriting samples. The dataset includes contributions from approximately five hundred individuals across different ages and genders. All images were pre-processed to ensure consistency and quality. Our approach demonstrates the ability to produce diverse handwritten outputs from input plain text. We believe this work contributes to the advancement of Bengali handwriting generation and can support further research in this area.

</details>


### [37] [Analyzing the Mechanism of Attention Collapse in VGGT from a Dynamics Perspective](https://arxiv.org/abs/2512.21691)
*Huan Li,Longjun Luo,Yuling Shi,Xiaodong Gu*

Main category: cs.CV

TL;DR: VGGT's global self-attention suffers from rank collapse with long sequences; analysis shows it's a degenerate diffusion process converging to Dirac measure at O(1/L) rate, explaining token-merging remedy.


<details>
  <summary>Details</summary>
Motivation: To mathematically explain the collapse phenomenon in VGGT's global self-attention when processing long sequences (hundreds of frames), where attention matrices become near rank-one and reconstruction error accumulates super-linearly.

Method: View global-attention iteration as a degenerate diffusion process, prove token-feature flow converges to Dirac-type measure at O(1/L) rate, derive closed-form mean-field partial differential equation predicting rank profile.

Result: Theory quantitatively matches attention-heat-map evolution and experimental outcomes, explains why token-merging (removing redundant tokens) slows diffusion coefficient and delays collapse without additional training.

Conclusion: Analysis provides principled framework for interpreting scalable 3D-vision transformers, with potential for multi-modal generalization, offering mathematical understanding of attention collapse phenomena.

Abstract: Visual Geometry Grounded Transformer (VGGT) delivers state-of-the-art feed-forward 3D reconstruction, yet its global self-attention layer suffers from a drastic collapse phenomenon when the input sequence exceeds a few hundred frames: attention matrices rapidly become near rank-one, token geometry degenerates to an almost one-dimensional subspace, and reconstruction error accumulates super-linearly.In this report,we establish a rigorous mathematical explanation of the collapse by viewing the global-attention iteration as a degenerate diffusion process.We prove that,in VGGT, the token-feature flow converges toward a Dirac-type measure at a $O(1/L)$ rate, where $L$ is the layer index, yielding a closed-form mean-field partial differential equation that precisely predicts the empirically observed rank profile.The theory quantitatively matches the attention-heat-map evolution and a series of experiments outcomes reported in relevant works and explains why its token-merging remedy -- which periodically removes redundant tokens -- slows the effective diffusion coefficient and thereby delays collapse without additional training.We believe the analysis provides a principled lens for interpreting future scalable 3D-vision transformers,and we highlight its potential for multi-modal generalization.

</details>


### [38] [ShinyNeRF: Digitizing Anisotropic Appearance in Neural Radiance Fields](https://arxiv.org/abs/2512.21692)
*Albert Barreiro,Roger Marí,Rafael Redondo,Gloria Haro,Carles Bosch*

Main category: cs.CV

TL;DR: ShinyNeRF: A novel NeRF framework that accurately models both isotropic and anisotropic specular reflections, enabling realistic digitization of materials like brushed metals with physical interpretability and editing capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing NeRF methods struggle to accurately model anisotropic specular surfaces commonly found in cultural heritage artifacts like brushed metals, limiting the quality of 3D digitization for reflective materials.

Method: ShinyNeRF jointly estimates surface normals, tangents, specular concentration, and anisotropy magnitudes using an Anisotropic Spherical Gaussian (ASG) distribution, approximating outgoing radiance as an encoded mixture of isotropic von Mises-Fisher distributions.

Result: ShinyNeRF achieves state-of-the-art performance in digitizing anisotropic specular reflections while providing plausible physical interpretations and enabling material property editing compared to existing methods.

Conclusion: ShinyNeRF successfully addresses the limitation of existing NeRF methods in modeling anisotropic specular surfaces, advancing 3D digitization capabilities for cultural heritage preservation with improved realism and editability.

Abstract: Recent advances in digitization technologies have transformed the preservation and dissemination of cultural heritage. In this vein, Neural Radiance Fields (NeRF) have emerged as a leading technology for 3D digitization, delivering representations with exceptional realism. However, existing methods struggle to accurately model anisotropic specular surfaces, typically observed, for example, on brushed metals. In this work, we introduce ShinyNeRF, a novel framework capable of handling both isotropic and anisotropic reflections. Our method is capable of jointly estimating surface normals, tangents, specular concentration, and anisotropy magnitudes of an Anisotropic Spherical Gaussian (ASG) distribution, by learning an approximation of the outgoing radiance as an encoded mixture of isotropic von Mises-Fisher (vMF) distributions. Experimental results show that ShinyNeRF not only achieves state-of-the-art performance on digitizing anisotropic specular reflections, but also offers plausible physical interpretations and editing of material properties compared to existing methods.

</details>


### [39] [Prior-AttUNet: Retinal OCT Fluid Segmentation Based on Normal Anatomical Priors and Attention Gating](https://arxiv.org/abs/2512.21693)
*Li Yang,Yuting Liu*

Main category: cs.CV

TL;DR: Prior-AttUNet: A hybrid dual-path segmentation model using generative anatomical priors and triple-attention mechanism for accurate macular edema segmentation in OCT images across multiple devices.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation of macular edema is essential for clinical diagnosis and management, but faces challenges including ambiguous boundaries and cross-device heterogeneity in OCT images.

Method: Hybrid dual-path architecture integrating generative prior pathway (using variational autoencoder for multi-scale normative anatomical priors) with segmentation backbone (densely connected blocks and spatial pyramid pooling). Novel triple-attention mechanism guided by anatomical priors dynamically modulates feature importance across decoding stages.

Result: Achieved excellent performance on RETOUCH benchmark across three OCT devices: mean Dice scores of 93.93% (Cirrus), 95.18% (Spectralis), and 93.47% (Topcon). Maintained low computational cost of 0.37 TFLOPs.

Conclusion: Prior-AttUNet demonstrates potential as a reliable tool for automated clinical analysis, effectively balancing segmentation precision and inference efficiency while handling cross-device heterogeneity.

Abstract: Accurate segmentation of macular edema, a hallmark pathological feature in vision-threatening conditions such as age-related macular degeneration and diabetic macular edema, is essential for clinical diagnosis and management. To overcome the challenges of segmenting fluid regions in optical coherence tomography (OCT) images-notably ambiguous boundaries and cross-device heterogeneity-this study introduces Prior-AttUNet, a segmentation model augmented with generative anatomical priors. The framework adopts a hybrid dual-path architecture that integrates a generative prior pathway with a segmentation network. A variational autoencoder supplies multi-scale normative anatomical priors, while the segmentation backbone incorporates densely connected blocks and spatial pyramid pooling modules to capture richer contextual information. Additionally, a novel triple-attention mechanism, guided by anatomical priors, dynamically modulates feature importance across decoding stages, substantially enhancing boundary delineation. Evaluated on the public RETOUCH benchmark, Prior-AttUNet achieves excellent performance across three OCT imaging devices (Cirrus, Spectralis, and Topcon), with mean Dice similarity coefficients of 93.93%, 95.18%, and 93.47%, respectively. The model maintains a low computational cost of 0.37 TFLOPs, striking an effective balance between segmentation precision and inference efficiency. These results demonstrate its potential as a reliable tool for automated clinical analysis.

</details>


### [40] [A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network for Multimodal Liver Tumor Segmentation from Unpaired Datasets](https://arxiv.org/abs/2512.21760)
*Arunkumar V,Firos V M,Senthilkumar S,Gangadharan G R*

Main category: cs.CV

TL;DR: A-QCF-Net learns unified segmentation from unpaired CT/MRI using quaternion networks and adaptive cross-fusion for bidirectional knowledge transfer, achieving significant improvements over unimodal baselines.


<details>
  <summary>Details</summary>
Motivation: Multimodal medical imaging provides complementary information but is limited by scarcity of large paired/aligned datasets. Need to leverage separate unpaired CT and MRI cohorts that are common in healthcare.

Method: Adaptive Quaternion Cross-Fusion Network (A-QCF-Net) with quaternion neural networks for shared feature space and A-QCF block for data-driven attention enabling bidirectional knowledge transfer between CT and MRI streams.

Result: Joint training on unpaired LiTS (CT) and ATLAS (MRI) achieves Tumor Dice scores of 76.7% on CT and 78.3% on MRI, exceeding unimodal nnU-Net baseline by 5.4% and 4.7% respectively. Explainability analysis confirms clinically meaningful representations.

Conclusion: Provides robust paradigm for leveraging large unpaired imaging archives in healthcare through unified model learning from separate modalities with bidirectional knowledge transfer.

Abstract: Multimodal medical imaging provides complementary information that is crucial for accurate delineation of pathology, but the development of deep learning models is limited by the scarcity of large datasets in which different modalities are paired and spatially aligned. This paper addresses this fundamental limitation by proposing an Adaptive Quaternion Cross-Fusion Network (A-QCF-Net) that learns a single unified segmentation model from completely separate and unpaired CT and MRI cohorts. The architecture exploits the parameter efficiency and expressive power of Quaternion Neural Networks to construct a shared feature space. At its core is the Adaptive Quaternion Cross-Fusion (A-QCF) block, a data driven attention module that enables bidirectional knowledge transfer between the two streams. By learning to modulate the flow of information dynamically, the A-QCF block allows the network to exchange abstract modality specific expertise, such as the sharp anatomical boundary information available in CT and the subtle soft tissue contrast provided by MRI. This mutual exchange regularizes and enriches the feature representations of both streams. We validate the framework by jointly training a single model on the unpaired LiTS (CT) and ATLAS (MRI) datasets. The jointly trained model achieves Tumor Dice scores of 76.7% on CT and 78.3% on MRI, significantly exceeding the strong unimodal nnU-Net baseline by margins of 5.4% and 4.7% respectively. Furthermore, comprehensive explainability analysis using Grad-CAM and Grad-CAM++ confirms that the model correctly focuses on relevant pathological structures, ensuring the learned representations are clinically meaningful. This provides a robust and clinically viable paradigm for unlocking the large unpaired imaging archives that are common in healthcare.

</details>


### [41] [Inference-based GAN Video Generation](https://arxiv.org/abs/2512.21776)
*Jingbo Yang,Adrian G. Bors*

Main category: cs.CV

TL;DR: Proposes a VAE-GAN hybrid video generator with variational encoder for inference capabilities, then extends it with Markov chain framework to generate long videos (hundreds/thousands of frames) while maintaining quality and temporal consistency.


<details>
  <summary>Details</summary>
Motivation: Existing video generation models (GANs, VAEs, Diffusion) struggle with generating long sequences - quality degrades when scaling beyond short clips (typically 16 frames). Need to generate meaningful long videos with temporal continuity and consistency.

Method: 1) First propose VAE-GAN hybrid with variational encoder for inference capabilities; 2) Extend with Markov chain framework where each state is a VAE-GAN short-video generator, using recall mechanism to sequentially connect generated sub-sequences for temporal dependencies.

Result: Enables generation of long videos (hundreds/thousands of frames) with maintained quality, temporal continuity, consistency and dynamics, overcoming the temporal scaling limitation of existing approaches.

Conclusion: The proposed memory-efficient approach successfully generates long video sequences by combining VAE-GAN architecture with Markov chain framework and recall mechanism, addressing the temporal scaling problem in video generation.

Abstract: Video generation has seen remarkable progresses thanks to advancements in generative deep learning. Generated videos should not only display coherent and continuous movement but also meaningful movement in successions of scenes. Generating models such as Generative Adversarial Networks (GANs) or Variational Autoencoders (VAEs) and more recently Diffusion Networks have been used for generating short video sequences, usually of up to 16 frames. In this paper, we first propose a new type of video generator by enabling adversarial-based unconditional video generators with a variational encoder, akin to a VAE-GAN hybrid structure, in order to enable the generation process with inference capabilities. The proposed model, as in other video deep learning-based processing frameworks, incorporates two processing branches, one for content and another for movement. However, existing models struggle with the temporal scaling of the generated videos. In classical approaches when aiming to increase the generated video length, the resulting video quality degrades, particularly when considering generating significantly long sequences. To overcome this limitation, our research study extends the initially proposed VAE-GAN video generation model by employing a novel, memory-efficient approach to generate long videos composed of hundreds or thousands of frames ensuring their temporal continuity, consistency and dynamics. Our approach leverages a Markov chain framework with a recall mechanism, with each state representing a VAE-GAN short-length video generator. This setup allows for the sequential connection of generated video sub-sequences, enabling temporal dependencies, resulting in meaningful long video sequences.

</details>


### [42] [FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection](https://arxiv.org/abs/2512.21695)
*Md. Zahid Hossain,Most. Sharmin Sultana Samu,Md. Kamrozzaman Bhuiyan,Farhad Uz Zaman,Md. Rakibul Islam*

Main category: cs.CV

TL;DR: FUSE is a hybrid AI-generated image detection system that combines spectral (FFT) and semantic (CLIP) features in a two-stage progressive training approach, achieving state-of-the-art performance across multiple datasets and generators.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of generative AI models has created an urgent need for reliable detection methods to identify AI-generated images, as existing approaches often fail to generalize well across different generators and struggle with high-fidelity images.

Method: FUSE extracts spectral features using Fast Fourier Transform and semantic features from CLIP's Vision encoder, fuses them into a joint representation, and trains progressively in two stages to enhance detection capabilities.

Result: Achieves state-of-the-art results on Chameleon benchmark, 91.36% mean accuracy on GenImage, 88.71% accuracy across all tested generators, and 94.96% mean Average Precision. Stage 2 training further improves performance for most generators.

Conclusion: The integration of spectral and semantic features provides robust generalization across diverse AI image generators, demonstrating superior performance compared to existing methods, particularly on high-fidelity images.

Abstract: The fast evolution of generative models has heightened the demand for reliable detection of AI-generated images. To tackle this challenge, we introduce FUSE, a hybrid system that combines spectral features extracted through Fast Fourier Transform with semantic features obtained from the CLIP's Vision encoder. The features are fused into a joint representation and trained progressively in two stages. Evaluations on GenImage, WildFake, DiTFake, GPT-ImgEval and Chameleon datasets demonstrate strong generalization across multiple generators. Our FUSE (Stage 1) model demonstrates state-of-the-art results on the Chameleon benchmark. It also attains 91.36% mean accuracy on the GenImage dataset, 88.71% accuracy across all tested generators, and a mean Average Precision of 94.96%. Stage 2 training further improves performance for most generators. Unlike existing methods, which often perform poorly on high-fidelity images in Chameleon, our approach maintains robustness across diverse generators. These findings highlight the benefits of integrating spectral and semantic features for generalized detection of images generated by AI.

</details>


### [43] [InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation](https://arxiv.org/abs/2512.21788)
*Jinqi Xiao,Qing Yan,Liming Jiang,Zichuan Liu,Hao Kang,Shen Sang,Tiancheng Zhi,Jing Liu,Cheng Yang,Xin Lu,Bo Yuan*

Main category: cs.CV

TL;DR: InstructMoLE introduces instruction-guided routing for diffusion transformers to prevent task interference in multi-conditional generation, replacing token-level routing with global instruction-based expert selection.


<details>
  <summary>Details</summary>
Motivation: Current parameter-efficient fine-tuning methods for Diffusion Transformers (DiTs) suffer from task interference when using monolithic adapters like LoRA. Mixture of Low-rank Experts (MoLE) helps but its token-level routing conflicts with global user instructions, causing spatial fragmentation and semantic drift in complex image generation.

Method: InstructMoLE uses Instruction-Guided Routing (IGR) that derives a global routing signal from user instructions to select a coherent expert council applied uniformly across all input tokens. It also introduces an output-space orthogonality loss to promote expert functional diversity and prevent representational collapse.

Result: InstructMoLE significantly outperforms existing LoRA adapters and MoLE variants across challenging multi-conditional generation benchmarks, demonstrating superior compositional control and fidelity to user intent.

Conclusion: The work presents a robust and generalizable framework for instruction-driven fine-tuning of generative models, enabling better preservation of global semantics and structural integrity in complex image generation tasks.

Abstract: Parameter-Efficient Fine-Tuning of Diffusion Transformers (DiTs) for diverse, multi-conditional tasks often suffers from task interference when using monolithic adapters like LoRA. The Mixture of Low-rank Experts (MoLE) architecture offers a modular solution, but its potential is usually limited by routing policies that operate at a token level. Such local routing can conflict with the global nature of user instructions, leading to artifacts like spatial fragmentation and semantic drift in complex image generation tasks. To address these limitations, we introduce InstructMoLE, a novel framework that employs an Instruction-Guided Mixture of Low-Rank Experts. Instead of per-token routing, InstructMoLE utilizes a global routing signal, Instruction-Guided Routing (IGR), derived from the user's comprehensive instruction. This ensures that a single, coherently chosen expert council is applied uniformly across all input tokens, preserving the global semantics and structural integrity of the generation process. To complement this, we introduce an output-space orthogonality loss, which promotes expert functional diversity and mitigates representational collapse. Extensive experiments demonstrate that InstructMoLE significantly outperforms existing LoRA adapters and MoLE variants across challenging multi-conditional generation benchmarks. Our work presents a robust and generalizable framework for instruction-driven fine-tuning of generative models, enabling superior compositional control and fidelity to user intent.

</details>


### [44] [Spatiotemporal-Untrammelled Mixture of Experts for Multi-Person Motion Prediction](https://arxiv.org/abs/2512.21707)
*Zheng Yin,Chengjian Li,Xiangbo Shu,Meiqi Cao,Rui Yan,Jinhui Tang*

Main category: cs.CV

TL;DR: ST-MoE: A novel spatiotemporal mixture of experts model for multi-person motion prediction that uses bidirectional Mamba experts to capture complex dependencies while reducing computational costs and parameters.


<details>
  <summary>Details</summary>
Motivation: Existing methods for multi-person motion prediction have two main limitations: 1) inflexible spatiotemporal representation due to reliance on positional encodings, and 2) high computational costs from quadratic time complexity of attention mechanisms.

Method: Proposes Spatiotemporal-Untrammelled Mixture of Experts (ST-MoE) with four distinct types of spatiotemporal experts, each specializing in different spatial/temporal dependencies. Uses bidirectional spatiotemporal Mamba as experts that share bidirectional temporal and spatial Mamba in distinct combinations for efficiency.

Result: Outperforms state-of-the-art in accuracy on four multi-person benchmark datasets while reducing model parameters by 41.38% and achieving 3.6x speedup in training.

Conclusion: ST-MoE effectively captures complex spatio-temporal dependencies in human motion while significantly improving computational efficiency and reducing model size, making it a practical solution for multi-person motion prediction.

Abstract: Comprehensively and flexibly capturing the complex spatio-temporal dependencies of human motion is critical for multi-person motion prediction. Existing methods grapple with two primary limitations: i) Inflexible spatiotemporal representation due to reliance on positional encodings for capturing spatiotemporal information. ii) High computational costs stemming from the quadratic time complexity of conventional attention mechanisms. To overcome these limitations, we propose the Spatiotemporal-Untrammelled Mixture of Experts (ST-MoE), which flexibly explores complex spatio-temporal dependencies in human motion and significantly reduces computational cost. To adaptively mine complex spatio-temporal patterns from human motion, our model incorporates four distinct types of spatiotemporal experts, each specializing in capturing different spatial or temporal dependencies. To reduce the potential computational overhead while integrating multiple experts, we introduce bidirectional spatiotemporal Mamba as experts, each sharing bidirectional temporal and spatial Mamba in distinct combinations to achieve model efficiency and parameter economy. Extensive experiments on four multi-person benchmark datasets demonstrate that our approach not only outperforms state-of-art in accuracy but also reduces model parameter by 41.38% and achieves a 3.6x speedup in training. The code is available at https://github.com/alanyz106/ST-MoE.

</details>


### [45] [CellMamba: Adaptive Mamba for Accurate and Efficient Cell Detection](https://arxiv.org/abs/2512.21803)
*Ruochen Liu,Yi Tian,Jiahao Wang,Hongbin Liu,Xianxu Hou,Jingxin Liu*

Main category: cs.CV

TL;DR: CellMamba: A lightweight one-stage detector for biomedical cell detection using Mamba blocks with triple-mapping attention and adaptive multi-scale fusion.


<details>
  <summary>Details</summary>
Motivation: Cell detection in pathological images is challenging due to densely packed objects, subtle inter-class differences, and severe background clutter, requiring specialized solutions.

Method: Built on VSSD backbone with CellMamba Blocks integrating NC-Mamba or Multi-Head Self-Attention with Triple-Mapping Adaptive Coupling (TMAC) module. TMAC splits channels into parallel branches with dual idiosyncratic and consensus attention maps. Adaptive Mamba Head fuses multi-scale features via learnable weights.

Result: Outperforms CNN-based, Transformer-based, and Mamba-based baselines on CoNSeP and CytoDArk0 datasets in accuracy while significantly reducing model size and inference latency.

Conclusion: CellMamba provides an efficient and effective solution for high-resolution cell detection, offering superior performance with reduced computational requirements.

Abstract: Cell detection in pathological images presents unique challenges due to densely packed objects, subtle inter-class differences, and severe background clutter. In this paper, we propose CellMamba, a lightweight and accurate one-stage detector tailored for fine-grained biomedical instance detection. Built upon a VSSD backbone, CellMamba integrates CellMamba Blocks, which couple either NC-Mamba or Multi-Head Self-Attention (MSA) with a novel Triple-Mapping Adaptive Coupling (TMAC) module. TMAC enhances spatial discriminability by splitting channels into two parallel branches, equipped with dual idiosyncratic and one consensus attention map, adaptively fused to preserve local sensitivity and global consistency. Furthermore, we design an Adaptive Mamba Head that fuses multi-scale features via learnable weights for robust detection under varying object sizes. Extensive experiments on two public datasets-CoNSeP and CytoDArk0-demonstrate that CellMamba outperforms both CNN-based, Transformer-based, and Mamba-based baselines in accuracy, while significantly reducing model size and inference latency. Our results validate CellMamba as an efficient and effective solution for high-resolution cell detection.

</details>


### [46] [RAPTOR: Real-Time High-Resolution UAV Video Prediction with Efficient Video Attention](https://arxiv.org/abs/2512.21710)
*Zhan Chen,Zile Guo,Enze Zhu,Peirong Zhang,Xiaoxuan Liu,Lei Wang,Yidan Zhang*

Main category: cs.CV

TL;DR: RAPTOR is a real-time high-resolution video prediction architecture that breaks the traditional trade-off between quality and speed using efficient spatiotemporal factorization, achieving >30 FPS at 512² resolution on edge hardware.


<details>
  <summary>Details</summary>
Motivation: Video prediction faces a fundamental trilemma: high-resolution and perceptual quality typically come at the cost of real-time speed, which is critical for latency-sensitive applications like autonomous UAVs in dense urban environments. Existing methods (diffusion, autoregressive models, quadratic-complexity attention) fail to meet these stringent demands on edge hardware.

Method: RAPTOR introduces Efficient Video Attention (EVA), a novel translator module that factorizes spatiotemporal modeling by alternating operations along spatial (S) and temporal (T) axes instead of processing flattened spacetime tokens. This reduces time complexity to O(S+T) and memory to O(max(S,T)). The architecture uses a single-pass design to avoid error accumulation, with a 3-stage training curriculum that progressively refines predictions from coarse structure to sharp details.

Result: RAPTOR is the first predictor to exceed 30 FPS on a Jetson AGX Orin for 512² video, setting new SOTA on UAVid, KTH, and custom high-resolution datasets in PSNR, SSIM, and LPIPS. It boosts mission success rate in real-world UAV navigation by 18%.

Conclusion: RAPTOR breaks the long-standing trade-off between video prediction quality and speed, enabling real-time high-resolution forecasting that is crucial for safety-critical applications like autonomous UAVs, paving the way for safer and more anticipatory embodied agents.

Abstract: Video prediction is plagued by a fundamental trilemma: achieving high-resolution and perceptual quality typically comes at the cost of real-time speed, hindering its use in latency-critical applications. This challenge is most acute for autonomous UAVs in dense urban environments, where foreseeing events from high-resolution imagery is non-negotiable for safety. Existing methods, reliant on iterative generation (diffusion, autoregressive models) or quadratic-complexity attention, fail to meet these stringent demands on edge hardware. To break this long-standing trade-off, we introduce RAPTOR, a video prediction architecture that achieves real-time, high-resolution performance. RAPTOR's single-pass design avoids the error accumulation and latency of iterative approaches. Its core innovation is Efficient Video Attention (EVA), a novel translator module that factorizes spatiotemporal modeling. Instead of processing flattened spacetime tokens with $O((ST)^2)$ or $O(ST)$ complexity, EVA alternates operations along the spatial (S) and temporal (T) axes. This factorization reduces the time complexity to $O(S + T)$ and memory complexity to $O(max(S, T))$, enabling global context modeling at $512^2$ resolution and beyond, operating directly on dense feature maps with a patch-free design. Complementing this architecture is a 3-stage training curriculum that progressively refines predictions from coarse structure to sharp, temporally coherent details. Experiments show RAPTOR is the first predictor to exceed 30 FPS on a Jetson AGX Orin for $512^2$ video, setting a new state-of-the-art on UAVid, KTH, and a custom high-resolution dataset in PSNR, SSIM, and LPIPS. Critically, RAPTOR boosts the mission success rate in a real-world UAV navigation task by 18/%, paving the way for safer and more anticipatory embodied agents.

</details>


### [47] [S&P 500 Stock's Movement Prediction using CNN](https://arxiv.org/abs/2512.21804)
*Rahul Gupta*

Main category: cs.CV

TL;DR: Using CNN on multivariate raw market data (including stock splits/dividends) to predict S&P 500 stock movements, treating historical data matrices as images for classification.


<details>
  <summary>Details</summary>
Motivation: Traditional mathematical approaches dominate algorithmic trading, but recent neural network advances show promise. Most existing research uses simplified single-dimension data and doesn't handle real-world complexities like stock splits/dividends. There's a need to apply cutting-edge deep learning to raw, multivariate financial data.

Method: Uses Convolutional Neural Networks (CNN) on multivariate raw market data including stock split/dividend events. Treats historical stock data as multi-dimensional matrices (like images) - essentially vector of historical data matrices. Can predict at individual stock, sector, or portfolio levels.

Result: The model achieves promising results in predicting stock movements, demonstrating CNN's effectiveness on raw financial data without feature engineering.

Conclusion: CNN can successfully process raw multivariate financial data (including complex events) for stock prediction, providing a foundation for future research in applying deep learning to real-world financial markets.

Abstract: This paper is about predicting the movement of stock consist of S&P 500 index. Historically there are many approaches have been tried using various methods to predict the stock movement and being used in the market currently for algorithm trading and alpha generating systems using traditional mathematical approaches [1, 2].
  The success of artificial neural network recently created a lot of interest and paved the way to enable prediction using cutting-edge research in the machine learning and deep learning. Some of these papers have done a great job in implementing and explaining benefits of these new technologies. Although most these papers do not go into the complexity of the financial data and mostly utilize single dimension data, still most of these papers were successful in creating the ground for future research in this comparatively new phenomenon. In this paper, I am trying to use multivariate raw data including stock split/dividend events (as-is) present in real-world market data instead of engineered financial data. Convolution Neural Network (CNN), the best-known tool so far for image classification, is used on the multi-dimensional stock numbers taken from the market mimicking them as a vector of historical data matrices (read images) and the model achieves promising results. The predictions can be made stock by stock, i.e., a single stock, sector-wise or for the portfolio of stocks.

</details>


### [48] [AstraNav-World: World Model for Foresight Control and Consistency](https://arxiv.org/abs/2512.21714)
*Junjun Hu,Jintao Chen,Haochen Bai,Minghua Luo,Shichao Xie,Ziyi Chen,Fei Liu,Zedong Chu,Xinda Xue,Botao Ren,Xiaolong Wu,Mu Xu,Shanghang Zhang*

Main category: cs.CV

TL;DR: AstraNav-World is an end-to-end world model that jointly predicts future visual states and action sequences using a unified probabilistic framework with diffusion-based video generation and vision-language policy integration.


<details>
  <summary>Details</summary>
Motivation: Embodied navigation in open, dynamic environments requires accurate foresight of world evolution and action outcomes. Current approaches often use decoupled "envision-then-plan" pipelines that suffer from cumulative errors due to lack of synchronization between visual predictions and action planning.

Method: Integrates diffusion-based video generator with vision-language policy in a unified probabilistic framework. Uses synchronized rollouts where predicted scenes and planned actions update simultaneously. Training optimizes two complementary objectives: generating action-conditioned multi-step visual predictions and deriving trajectories conditioned on those predicted visuals.

Result: Improved trajectory accuracy and higher success rates across diverse embodied navigation benchmarks. Demonstrated exceptional zero-shot capabilities in real-world testing, adapting to unseen scenarios without fine-tuning. Ablations confirmed necessity of tight vision-action coupling and unified training.

Conclusion: AstraNav-World captures transferable spatial understanding and planning-relevant navigation dynamics rather than overfitting to simulation data. Unifying foresight vision and control within a single generative model moves toward reliable, interpretable, and general-purpose embodied agents for open-ended real-world settings.

Abstract: Embodied navigation in open, dynamic environments demands accurate foresight of how the world will evolve and how actions will unfold over time. We propose AstraNav-World, an end-to-end world model that jointly reasons about future visual states and action sequences within a unified probabilistic framework. Our framework integrates a diffusion-based video generator with a vision-language policy, enabling synchronized rollouts where predicted scenes and planned actions are updated simultaneously. Training optimizes two complementary objectives: generating action-conditioned multi-step visual predictions and deriving trajectories conditioned on those predicted visuals. This bidirectional constraint makes visual predictions executable and keeps decisions grounded in physically consistent, task-relevant futures, mitigating cumulative errors common in decoupled "envision-then-plan" pipelines. Experiments across diverse embodied navigation benchmarks show improved trajectory accuracy and higher success rates. Ablations confirm the necessity of tight vision-action coupling and unified training, with either branch removal degrading both prediction quality and policy reliability. In real-world testing, AstraNav-World demonstrated exceptional zero-shot capabilities, adapting to previously unseen scenarios without any real-world fine-tuning. These results suggest that AstraNav-World captures transferable spatial understanding and planning-relevant navigation dynamics, rather than merely overfitting to simulation-specific data distribution. Overall, by unifying foresight vision and control within a single generative model, we move closer to reliable, interpretable, and general-purpose embodied agents that operate robustly in open-ended real-world settings.

</details>


### [49] [Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation](https://arxiv.org/abs/2512.21734)
*Steven Xiao,XIndi Zhang,Dechao Meng,Qi Wang,Peng Zhang,Bang Zhang*

Main category: cs.CV

TL;DR: Knot Forcing is a streaming framework for real-time portrait animation that enables high-fidelity, temporally consistent generation over infinite sequences with ultra-low latency, addressing limitations of diffusion-based and autoregressive approaches.


<details>
  <summary>Details</summary>
Motivation: Real-time portrait animation needs high visual fidelity, temporal coherence, ultra-low latency, and responsive control for interactive applications like virtual assistants and live avatars. Current approaches have limitations: diffusion models are non-causal (not suitable for streaming), while autoregressive methods suffer from error accumulation, motion discontinuities at chunk boundaries, and degraded long-term consistency.

Method: Three key designs: (1) Chunk-wise generation with global identity preservation via cached KV states of reference image and local temporal modeling using sliding window attention; (2) Temporal knot module that overlaps adjacent chunks and propagates spatio-temporal cues via image-to-video conditioning to smooth inter-chunk motion transitions; (3) "Running ahead" mechanism that dynamically updates reference frame's temporal coordinate during inference to keep semantic context ahead of current rollout frame for long-term coherence.

Result: Enables high-fidelity, temporally consistent, and interactive portrait animation over infinite sequences, achieving real-time performance with strong visual stability on consumer-grade GPUs.

Conclusion: Knot Forcing addresses key challenges in streaming portrait animation by combining chunk-wise generation with temporal smoothing mechanisms and dynamic reference updating, providing a practical solution for real-time interactive applications requiring both quality and low latency.

Abstract: Real-time portrait animation is essential for interactive applications such as virtual assistants and live avatars, requiring high visual fidelity, temporal coherence, ultra-low latency, and responsive control from dynamic inputs like reference images and driving signals. While diffusion-based models achieve strong quality, their non-causal nature hinders streaming deployment. Causal autoregressive video generation approaches enable efficient frame-by-frame generation but suffer from error accumulation, motion discontinuities at chunk boundaries, and degraded long-term consistency. In this work, we present a novel streaming framework named Knot Forcing for real-time portrait animation that addresses these challenges through three key designs: (1) a chunk-wise generation strategy with global identity preservation via cached KV states of the reference image and local temporal modeling using sliding window attention; (2) a temporal knot module that overlaps adjacent chunks and propagates spatio-temporal cues via image-to-video conditioning to smooth inter-chunk motion transitions; and (3) A "running ahead" mechanism that dynamically updates the reference frame's temporal coordinate during inference, keeping its semantic context ahead of the current rollout frame to support long-term coherence. Knot Forcing enables high-fidelity, temporally consistent, and interactive portrait animation over infinite sequences, achieving real-time performance with strong visual stability on consumer-grade GPUs.

</details>


### [50] [Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening](https://arxiv.org/abs/2512.21861)
*Md Rafid Islam,Rafsan Jany,Akib Ahmed,Mohammad Ashrafuzzaman Khan*

Main category: cs.CV

TL;DR: Feature-level fusion of CNN backbones (EfficientNet-B0 + DenseNet121) outperforms single models for binary diabetic retinopathy screening on heterogeneous fundus images, achieving 82.89% accuracy with good accuracy-latency balance.


<details>
  <summary>Details</summary>
Motivation: Diabetic retinopathy screening is limited by specialist availability and variable image quality across devices/populations. Need for accurate, efficient screening methods that generalize across diverse datasets.

Method: Used 11,156 fundus images from 5 public datasets (APTOS, EyePACS, IDRiD, Messidor, ODIR). Compared three pretrained CNN models (ResNet50, EfficientNet-B0, DenseNet121) against pairwise and tri-fusion variants for binary DR classification. Conducted 5 independent runs for evaluation.

Result: Fusion consistently outperformed single backbones. EfficientNet-B0 + DenseNet121 fusion achieved best mean accuracy (82.89%) with balanced F1-scores (normal: 83.60%, diabetic: 82.60%). EfficientNet-B0 was fastest (1.16 ms/image), while fusion offered best accuracy-latency trade-off.

Conclusion: Lightweight feature fusion enhances generalization across heterogeneous datasets, supporting scalable binary DR screening workflows where both accuracy and throughput are critical.

Abstract: Diabetic retinopathy (DR) remains a leading cause of preventable blindness, yet large-scale screening is constrained by limited specialist availability and variable image quality across devices and populations. This work investigates whether feature-level fusion of complementary convolutional neural network (CNN) backbones can deliver accurate and efficient binary DR screening on globally sourced fundus images. Using 11,156 images pooled from five public datasets (APTOS, EyePACS, IDRiD, Messidor, and ODIR), we frame DR detection as a binary classification task and compare three pretrained models (ResNet50, EfficientNet-B0, and DenseNet121) against pairwise and tri-fusion variants. Across five independent runs, fusion consistently outperforms single backbones. The EfficientNet-B0 + DenseNet121 (Eff+Den) fusion model achieves the best overall mean performance (accuracy: 82.89\%) with balanced class-wise F1-scores for normal (83.60\%) and diabetic (82.60\%) cases. While the tri-fusion is competitive, it incurs a substantially higher computational cost. Inference profiling highlights a practical trade-off: EfficientNet-B0 is the fastest (approximately 1.16 ms/image at batch size 1000), whereas the Eff+Den fusion offers a favorable accuracy--latency balance. These findings indicate that lightweight feature fusion can enhance generalization across heterogeneous datasets, supporting scalable binary DR screening workflows where both accuracy and throughput are critical.

</details>


### [51] [SyncAnyone: Implicit Disentanglement via Progressive Self-Correction for Lip-Syncing in the wild](https://arxiv.org/abs/2512.21736)
*Xindi Zhang,Dechao Meng,Steven Xiao,Qi Wang,Peng Zhang,Bang Zhang*

Main category: cs.CV

TL;DR: SyncAnyone is a two-stage framework for high-quality AI video dubbing that achieves accurate lip-sync while maintaining facial structure and background consistency, overcoming limitations of mask-based approaches.


<details>
  <summary>Details</summary>
Motivation: Existing mask-based video dubbing methods disrupt spatiotemporal context, causing instability in facial structure and background consistency despite achieving lip-sync accuracy.

Method: Two-stage framework: Stage 1 trains diffusion-based video transformer for masked mouth inpainting; Stage 2 uses mask-free tuning with pseudo-paired training samples from synthesized lip-synced videos.

Result: Achieves state-of-the-art results in visual quality, temporal coherence, and identity preservation in in-the-wild lip-syncing scenarios.

Conclusion: SyncAnyone successfully overcomes mask-induced artifacts by combining accurate motion modeling with high visual fidelity through a novel two-stage learning approach.

Abstract: High-quality AI-powered video dubbing demands precise audio-lip synchronization, high-fidelity visual generation, and faithful preservation of identity and background. Most existing methods rely on a mask-based training strategy, where the mouth region is masked in talking-head videos, and the model learns to synthesize lip movements from corrupted inputs and target audios. While this facilitates lip-sync accuracy, it disrupts spatiotemporal context, impairing performance on dynamic facial motions and causing instability in facial structure and background consistency. To overcome this limitation, we propose SyncAnyone, a novel two-stage learning framework that achieves accurate motion modeling and high visual fidelity simultaneously. In Stage 1, we train a diffusion-based video transformer for masked mouth inpainting, leveraging its strong spatiotemporal modeling to generate accurate, audio-driven lip movements. However, due to input corruption, minor artifacts may arise in the surrounding facial regions and the background. In Stage 2, we develop a mask-free tuning pipeline to address mask-induced artifacts. Specifically, on the basis of the Stage 1 model, we develop a data generation pipeline that creates pseudo-paired training samples by synthesizing lip-synced videos from the source video and random sampled audio. We further tune the stage 2 model on this synthetic data, achieving precise lip editing and better background consistency. Extensive experiments show that our method achieves state-of-the-art results in visual quality, temporal coherence, and identity preservation under in-the wild lip-syncing scenarios.

</details>


### [52] [Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning](https://arxiv.org/abs/2512.21924)
*Tao Yang,Xiuying Wang,Hao Liu,Guanzhong Gong,Lian-Ming Wu,Yu-Ping Wang,Lisheng Wang*

Main category: cs.CV

TL;DR: A novel unsupervised brain MRI lesion detection framework using disentangled representation and edge-to-image restoration to improve generalizability across multi-modality/multi-center data and suppress abnormal residuals in reconstructed pseudo-healthy images.


<details>
  <summary>Details</summary>
Motivation: Current unsupervised anomaly detection methods for brain MRI have two key limitations: 1) poor generalizability to multi-modality and multi-center data due to reliance on specific imaging information in training data, and 2) constrained performance due to abnormal residuals propagating from input images to reconstructed pseudo-healthy images.

Method: Two novel modules: 1) Disentangled representation module that decouples brain MRI into imaging information and imaging-invariant anatomical images using brain anatomical priors and differentiable one-hot encoding for stability; 2) Edge-to-image restoration module that reconstructs high-quality pseudo-healthy images by restoring anatomical representation from high-frequency edge information and recoupling imaging information.

Result: Outperformed 17 state-of-the-art methods on nine public datasets (4,443 patients' MRIs from multiple centers), achieving absolute improvements of +18.32% in AP (Average Precision) and +13.64% in DSC (Dice Similarity Coefficient).

Conclusion: The proposed framework effectively addresses limitations of current unsupervised methods by improving generalizability across diverse imaging conditions and suppressing abnormal residuals, leading to superior lesion detection performance in brain MRI.

Abstract: Detection of various lesions in brain MRI is clinically critical, but challenging due to the diversity of lesions and variability in imaging conditions. Current unsupervised learning methods detect anomalies mainly through reconstructing abnormal images into pseudo-healthy images (PHIs) by normal samples learning and then analyzing differences between images. However, these unsupervised models face two significant limitations: restricted generalizability to multi-modality and multi-center MRIs due to their reliance on the specific imaging information in normal training data, and constrained performance due to abnormal residuals propagated from input images to reconstructed PHIs. To address these limitations, two novel modules are proposed, forming a new PHI reconstruction framework. Firstly, the disentangled representation module is proposed to improve generalizability by decoupling brain MRI into imaging information and essential imaging-invariant anatomical images, ensuring that the reconstruction focuses on the anatomy. Specifically, brain anatomical priors and a differentiable one-hot encoding operator are introduced to constrain the disentanglement results and enhance the disentanglement stability. Secondly, the edge-to-image restoration module is designed to reconstruct high-quality PHIs by restoring the anatomical representation from the high-frequency edge information of anatomical images, and then recoupling the disentangled imaging information. This module not only suppresses abnormal residuals in PHI by reducing abnormal pixels input through edge-only input, but also effectively reconstructs normal regions using the preserved structural details in the edges. Evaluated on nine public datasets (4,443 patients' MRIs from multiple centers), our method outperforms 17 SOTA methods, achieving absolute improvements of +18.32% in AP and +13.64% in DSC.

</details>


### [53] [Scene-VLM: Multimodal Video Scene Segmentation via Vision-Language Models](https://arxiv.org/abs/2512.21778)
*Nimrod Berman,Adam Botach,Emanuel Ben-Baruch,Shunit Haviv Hakimi,Asaf Gendler,Ilan Naiman,Erez Yosef,Igor Kviatkovsky*

Main category: cs.CV

TL;DR: Scene-VLM: First fine-tuned vision-language model for video scene segmentation using multimodal reasoning across frames, text, and metadata with sequential processing and explainable rationales.


<details>
  <summary>Details</summary>
Motivation: Existing encoder-based methods have visual-centric biases, process shots in isolation without sequential dependencies, lack narrative understanding, and are not explainable.

Method: Fine-tuned VLM framework that jointly processes visual and textual cues (frames, transcriptions, metadata) with sequential predictions using causal dependencies and context-focus windows. Extracts confidence scores from token-level logits and aligns model to generate natural-language rationales.

Result: Achieves state-of-the-art performance on standard benchmarks. On MovieNet: +6 AP and +13.7 F1 improvements over previous leading method.

Conclusion: Scene-VLM enables multimodal reasoning for video scene segmentation with sequential dependencies, controllable precision-recall trade-offs, and explainable rationales, outperforming previous methods.

Abstract: Segmenting long-form videos into semantically coherent scenes is a fundamental task in large-scale video understanding. Existing encoder-based methods are limited by visual-centric biases, classify each shot in isolation without leveraging sequential dependencies, and lack both narrative understanding and explainability. In this paper, we present Scene-VLM, the first fine-tuned vision-language model (VLM) framework for video scene segmentation. Scene-VLM jointly processes visual and textual cues including frames, transcriptions, and optional metadata to enable multimodal reasoning across consecutive shots. The model generates predictions sequentially with causal dependencies among shots and introduces a context-focus window mechanism to ensure sufficient temporal context for each shot-level decision. In addition, we propose a scheme to extract confidence scores from the token-level logits of the VLM, enabling controllable precision-recall trade-offs that were previously limited to encoder-based methods. Furthermore, we demonstrate that our model can be aligned to generate coherent natural-language rationales for its boundary decisions through minimal targeted supervision. Our approach achieves state-of-the-art performance on standard scene segmentation benchmarks. On MovieNet, for example, Scene-VLM yields significant improvements of +6 AP and +13.7 F1 over the previous leading method.

</details>


### [54] [LVLM-Aided Alignment of Task-Specific Vision Models](https://arxiv.org/abs/2512.21985)
*Alexander Koebler,Lukas Kuhn,Ingo Thon,Florian Buettner*

Main category: cs.CV

TL;DR: LVLM-VA: A method using Large Vision Language Models to align small vision models with human domain knowledge, reducing reliance on spurious correlations without fine-grained feedback.


<details>
  <summary>Details</summary>
Motivation: Small task-specific vision models are computationally efficient but often rely on spurious correlations rather than meaningful domain knowledge, leading to brittle real-world performance. Current explanation methods reveal this misalignment but don't solve it.

Method: LVLM-Aided Visual Alignment (LVLM-VA) creates a bidirectional interface using a Large Vision Language Model. It translates model behavior into natural language and maps human class-level specifications to image-level critiques, enabling effective interaction between domain experts and the model.

Result: The method demonstrates substantial improvement in aligning model behavior with human specifications on both synthetic and real-world datasets. It effectively reduces the model's dependence on spurious features and group-specific biases.

Conclusion: LVLM-VA provides an efficient way to align small vision models with human domain knowledge using LVLMs, addressing the brittleness caused by spurious correlations without requiring fine-grained feedback from experts.

Abstract: In high-stakes domains, small task-specific vision models are crucial due to their low computational requirements and the availability of numerous methods to explain their results. However, these explanations often reveal that the models do not align well with human domain knowledge, relying instead on spurious correlations. This might result in brittle behavior once deployed in the real-world. To address this issue, we introduce a novel and efficient method for aligning small task-specific vision models with human domain knowledge by leveraging the generalization capabilities of a Large Vision Language Model (LVLM). Our LVLM-Aided Visual Alignment (LVLM-VA) method provides a bidirectional interface that translates model behavior into natural language and maps human class-level specifications to image-level critiques, enabling effective interaction between domain experts and the model. Our method demonstrates substantial improvement in aligning model behavior with human specifications, as validated on both synthetic and real-world datasets. We show that it effectively reduces the model's dependence on spurious features and on group-specific biases, without requiring fine-grained feedback.

</details>


### [55] [LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration](https://arxiv.org/abs/2512.22010)
*Wen Jiang,Li Wang,Kangyao Huang,Wei Fan,Jinyuan Liu,Shaoyu Liu,Hongwei Duan,Bin Xu,Xiangyang Ji*

Main category: cs.CV

TL;DR: LongFly is a spatiotemporal context modeling framework for UAV vision-and-language navigation that improves long-horizon navigation in complex post-disaster environments by better modeling historical context and trajectory dynamics.


<details>
  <summary>Details</summary>
Motivation: Current UAV vision-and-language navigation methods struggle with long-horizon spatiotemporal context modeling in complex environments like post-disaster search and rescue, leading to inaccurate semantic alignment and unstable path planning.

Method: Proposes three modules: 1) Slot-based historical image compression to distill multi-view historical observations into fixed-length contextual representations, 2) Spatiotemporal trajectory encoding to capture temporal dynamics and spatial structure, and 3) Prompt-guided multimodal integration to combine spatiotemporal context with current observations for time-based reasoning.

Result: Outperforms state-of-the-art UAV VLN baselines by 7.89% in success rate and 6.33% in success weighted by path length, with consistent performance across both seen and unseen environments.

Conclusion: LongFly effectively addresses the challenges of long-horizon spatiotemporal context modeling in UAV VLN, providing more accurate semantic alignment and stable path planning for complex post-disaster search and rescue missions.

Abstract: Unmanned aerial vehicles (UAVs) are crucial tools for post-disaster search and rescue, facing challenges such as high information density, rapid changes in viewpoint, and dynamic structures, especially in long-horizon navigation. However, current UAV vision-and-language navigation(VLN) methods struggle to model long-horizon spatiotemporal context in complex environments, resulting in inaccurate semantic alignment and unstable path planning. To this end, we propose LongFly, a spatiotemporal context modeling framework for long-horizon UAV VLN. LongFly proposes a history-aware spatiotemporal modeling strategy that transforms fragmented and redundant historical data into structured, compact, and expressive representations. First, we propose the slot-based historical image compression module, which dynamically distills multi-view historical observations into fixed-length contextual representations. Then, the spatiotemporal trajectory encoding module is introduced to capture the temporal dynamics and spatial structure of UAV trajectories. Finally, to integrate existing spatiotemporal context with current observations, we design the prompt-guided multimodal integration module to support time-based reasoning and robust waypoint prediction. Experimental results demonstrate that LongFly outperforms state-of-the-art UAV VLN baselines by 7.89\% in success rate and 6.33\% in success weighted by path length, consistently across both seen and unseen environments.

</details>


### [56] [AI for Mycetoma Diagnosis in Histopathological Images: The MICCAI 2024 Challenge](https://arxiv.org/abs/2512.21792)
*Hyam Omar Ali,Sahar Alhesseen,Lamis Elkhair,Adrian Galdran,Ming Feng,Zhixiang Xiong,Zengming Lin,Kele Xu,Liang Hu,Benjamin Keel,Oliver Mills,James Battye,Akshay Kumar,Asra Aslam,Prasad Dutande,Ujjwal Baid,Bhakti Baheti,Suhas Gajre,Aravind Shrenivas Murali,Eung-Joo Lee,Ahmed Fahal,Rachid Jennane*

Main category: cs.CV

TL;DR: The paper presents the mAIcetoma challenge which aimed to develop AI models for automated segmentation and classification of mycetoma grains from histopathological images to address diagnostic challenges in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: Mycetoma is a neglected tropical disease causing severe tissue damage, affecting poor rural communities. Diagnosis is particularly challenging in low-resource settings where expert pathologists are limited, creating a need for automated AI solutions.

Method: Organized the Mycetoma MicroImage: Detect and Classify Challenge (mAIcetoma) where teams developed deep learning models for segmenting mycetoma grains and classifying mycetoma types from histopathological images. Provided the Mycetoma database (MyData) as a standardized dataset for model development and evaluation.

Result: Five finalist teams participated with various deep learning architectures. All models achieved high segmentation accuracy, emphasizing grain detection as critical for diagnosis. Top-performing models showed significant performance in classifying mycetoma types.

Conclusion: The mAIcetoma challenge successfully advanced mycetoma diagnosis through AI solutions, demonstrating that automated models can effectively segment grains and classify mycetoma types, potentially addressing diagnostic challenges in resource-limited settings.

Abstract: Mycetoma is a neglected tropical disease caused by fungi or bacteria leading to severe tissue damage and disabilities. It affects poor and rural communities and presents medical challenges and socioeconomic burdens on patients and healthcare systems in endemic regions worldwide. Mycetoma diagnosis is a major challenge in mycetoma management, particularly in low-resource settings where expert pathologists are limited. To address this challenge, this paper presents an overview of the Mycetoma MicroImage: Detect and Classify Challenge (mAIcetoma) which was organized to advance mycetoma diagnosis through AI solutions. mAIcetoma focused on developing automated models for segmenting mycetoma grains and classifying mycetoma types from histopathological images. The challenge attracted the attention of several teams worldwide to participate and five finalist teams fulfilled the challenge objectives. The teams proposed various deep learning architectures for the ultimate goal of this challenge. Mycetoma database (MyData) was provided to participants as a standardized dataset to run the proposed models. Those models were evaluated using evaluation metrics. Results showed that all the models achieved high segmentation accuracy, emphasizing the necessitate of grain detection as a critical step in mycetoma diagnosis. In addition, the top-performing models show a significant performance in classifying mycetoma types.

</details>


### [57] [Diffusion Posterior Sampling for Super-Resolution under Gaussian Measurement Noise](https://arxiv.org/abs/2512.21797)
*Abu Hanif Muhammad Syarubany*

Main category: cs.CV

TL;DR: Diffusion posterior sampling (DPS) for single-image super-resolution achieves best results with PS scale 0.95 and noise σ=0.01, balancing diffusion priors with measurement consistency.


<details>
  <summary>Details</summary>
Motivation: To study diffusion posterior sampling for single-image super-resolution under known degradation models without retraining diffusion models for each operator, aiming to balance diffusion priors with measurement consistency for high-quality reconstructions.

Method: Implements likelihood-guided sampling combining unconditional diffusion prior with gradient-based conditioning to enforce measurement consistency for 4× super-resolution with additive Gaussian noise. Evaluates posterior sampling conditioning across guidance scales and noise levels using PSNR, SSIM, and a combined selection score.

Result: Best configuration achieved at PS scale 0.95 and noise standard deviation σ=0.01 with score 1.45231. Moderate guidance improves reconstruction quality, restoring sharper edges and more coherent facial details compared to downsampled inputs. Alternative conditioning strategies (MCG and PS-annealed) show different texture fidelity trade-offs.

Conclusion: Balancing diffusion priors and measurement-gradient strength is crucial for stable, high-quality reconstructions without retraining diffusion models for each operator. The selected PS setting effectively restores image details while maintaining measurement consistency.

Abstract: This report studies diffusion posterior sampling (DPS) for single-image super-resolution (SISR) under a known degradation model. We implement a likelihood-guided sampling procedure that combines an unconditional diffusion prior with gradient-based conditioning to enforce measurement consistency for $4\times$ super-resolution with additive Gaussian noise. We evaluate posterior sampling (PS) conditioning across guidance scales and noise levels, using PSNR and SSIM as fidelity metrics and a combined selection score $(\mathrm{PSNR}/40)+\mathrm{SSIM}$. Our ablation shows that moderate guidance improves reconstruction quality, with the best configuration achieved at PS scale $0.95$ and noise standard deviation $σ=0.01$ (score $1.45231$). Qualitative results confirm that the selected PS setting restores sharper edges and more coherent facial details compared to the downsampled inputs, while alternative conditioning strategies (e.g., MCG and PS-annealed) exhibit different texture fidelity trade-offs. These findings highlight the importance of balancing diffusion priors and measurement-gradient strength to obtain stable, high-quality reconstructions without retraining the diffusion model for each operator.

</details>


### [58] [StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars](https://arxiv.org/abs/2512.22065)
*Zhiyao Sun,Ziqiao Peng,Yifeng Ma,Yi Chen,Zhengguang Zhou,Zixiang Zhou,Guozhen Zhang,Youliang Zhang,Yuan Zhou,Qinglin Lu,Yong-Jin Liu*

Main category: cs.CV

TL;DR: Two-stage autoregressive framework adapts diffusion models for real-time interactive avatars with full-body gestures, achieving SOTA quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based avatar methods are non-causal and computationally expensive, unsuitable for streaming. Current interactive approaches are limited to head-and-shoulder regions without full-body gestures.

Method: Two-stage autoregressive adaptation with distillation and adversarial refinement. Key components: Reference Sink, Reference-Anchored Positional Re-encoding (RAPR), and Consistency-Aware Discriminator for long-term stability.

Result: Achieves state-of-the-art performance in generation quality, real-time efficiency, and interaction naturalness. Capable of generating both talking and listening behaviors with coherent gestures.

Conclusion: Proposed framework successfully enables real-time, interactive streaming avatars with full-body gestures, overcoming limitations of existing diffusion-based methods.

Abstract: Real-time, streaming interactive avatars represent a critical yet challenging goal in digital human research. Although diffusion-based human avatar generation methods achieve remarkable success, their non-causal architecture and high computational costs make them unsuitable for streaming. Moreover, existing interactive approaches are typically limited to head-and-shoulder region, limiting their ability to produce gestures and body motions. To address these challenges, we propose a two-stage autoregressive adaptation and acceleration framework that applies autoregressive distillation and adversarial refinement to adapt a high-fidelity human video diffusion model for real-time, interactive streaming. To ensure long-term stability and consistency, we introduce three key components: a Reference Sink, a Reference-Anchored Positional Re-encoding (RAPR) strategy, and a Consistency-Aware Discriminator. Building on this framework, we develop a one-shot, interactive, human avatar model capable of generating both natural talking and listening behaviors with coherent gestures. Extensive experiments demonstrate that our method achieves state-of-the-art performance, surpassing existing approaches in generation quality, real-time efficiency, and interaction naturalness. Project page: https://streamavatar.github.io .

</details>


### [59] [Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models](https://arxiv.org/abs/2512.21815)
*Mengqi He,Xinyu Tian,Xin Shen,Jinhong Ni,Shu Zou,Zhaoyuan Yang,Jing Zhang*

Main category: cs.CV

TL;DR: Selective adversarial attacks targeting only high-entropy tokens (critical decision points) in VLMs achieve comparable semantic degradation to global attacks with smaller budgets, while exposing significant safety risks by converting benign outputs to harmful ones.


<details>
  <summary>Details</summary>
Motivation: Current entropy-based attacks maximize uncertainty at all decoding steps, assuming equal token contribution to instability. The authors identify that only a small fraction of high-entropy tokens disproportionately govern output trajectories, suggesting more efficient and dangerous attack strategies.

Method: Propose Entropy-bank Guided Adversarial attacks (EGA) that concentrate adversarial perturbations on critical high-entropy positions (about 20% of tokens) rather than all tokens, exploiting the observation that vulnerable decision points recur across diverse VLMs.

Result: Selective attacks achieve semantic degradation comparable to global methods with smaller budgets, convert 35-49% of benign outputs into harmful ones across multiple VLMs, and show transferability (17-26% harmful rates on unseen targets). EGA achieves 93-95% attack success rates with high harmful conversion.

Conclusion: Critical decision points in autoregressive generation are vulnerable to targeted attacks, exposing significant safety risks in current VLMs. The recurrence of high-entropy forks across diverse architectures enables transferable attacks, revealing weaknesses in current safety mechanisms.

Abstract: Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.

</details>


### [60] [End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration](https://arxiv.org/abs/2512.21831)
*Zhenwei Yang,Yibo Ai,Weidong Zhang*

Main category: cs.CV

TL;DR: XET-V2X is a multi-modal fused end-to-end tracking framework for V2X collaboration that unifies multi-view sensing in shared spatiotemporal representation, improving perception under communication delays.


<details>
  <summary>Details</summary>
Motivation: Multi-view cooperative perception and multimodal fusion are essential for reliable 3D spatiotemporal understanding in autonomous driving, especially under occlusions, limited viewpoints, and communication delays in V2X scenarios.

Method: XET-V2X introduces a dual-layer spatial cross-attention module based on multi-scale deformable attention. It first aggregates multi-view image features for semantic consistency, then fuses point clouds guided by updated spatial queries, enabling effective cross-modal interaction while reducing computational overhead.

Result: Experiments on V2X-Seq-SPD, V2X-Sim-V2V, and V2X-Sim-V2I benchmarks demonstrate consistent improvements in detection and tracking performance under varying communication delays. Both quantitative results and qualitative visualizations show robust and temporally stable perception in complex traffic scenarios.

Conclusion: XET-V2X achieves robust and temporally stable perception in complex traffic scenarios through unified multi-view multimodal sensing within shared spatiotemporal representation, effectively addressing challenges of occlusions, limited viewpoints, and communication delays in V2X collaboration.

Abstract: Multi-view cooperative perception and multimodal fusion are essential for reliable 3D spatiotemporal understanding in autonomous driving, especially under occlusions, limited viewpoints, and communication delays in V2X scenarios. This paper proposes XET-V2X, a multi-modal fused end-to-end tracking framework for v2x collaboration that unifies multi-view multimodal sensing within a shared spatiotemporal representation. To efficiently align heterogeneous viewpoints and modalities, XET-V2X introduces a dual-layer spatial cross-attention module based on multi-scale deformable attention. Multi-view image features are first aggregated to enhance semantic consistency, followed by point cloud fusion guided by the updated spatial queries, enabling effective cross-modal interaction while reducing computational overhead. Experiments on the real-world V2X-Seq-SPD dataset and the simulated V2X-Sim-V2V and V2X-Sim-V2I benchmarks demonstrate consistent improvements in detection and tracking performance under varying communication delays. Both quantitative results and qualitative visualizations indicate that XET-V2X achieves robust and temporally stable perception in complex traffic scenarios.

</details>


### [61] [Scalable Class-Incremental Learning Based on Parametric Neural Collapse](https://arxiv.org/abs/2512.21845)
*Chuangxin Zhang,Guangfeng Lin,Enhui Zhao,Kaiyang Liao,Yajun Chen*

Main category: cs.CV

TL;DR: SCL-PNC is a scalable class-incremental learning method that uses parametric neural collapse to enable demand-driven backbone expansion and dynamic ETF classifiers to handle evolving class distributions while maintaining feature consistency.


<details>
  <summary>Details</summary>
Motivation: Existing incremental learning methods freeze old model parameters but ignore structural efficiency, leading to feature differences between modules and class misalignment due to evolving class distributions.

Method: Proposes SCL-PNC with: 1) adapt-layer for demand-driven minimal-cost backbone expansion, 2) dynamic parametric Equiangular Tight Frame (ETF) framework that adapts to incremental classes, 3) parallel expansion framework with knowledge distillation to align features across modules and prevent feature drift.

Result: Experiments on standard benchmarks demonstrate the effectiveness and efficiency of the proposed method in handling model expansion with increasing categories in real-world scenarios.

Conclusion: SCL-PNC successfully addresses class misalignment from evolving distributions and ensures feature consistency through neural collapse principles, combining expandable backbone, adapt-layer, and parametric ETF classifier for scalable incremental learning.

Abstract: Incremental learning often encounter challenges such as overfitting to new data and catastrophic forgetting of old data. Existing methods can effectively extend the model for new tasks while freezing the parameters of the old model, but ignore the necessity of structural efficiency to lead to the feature difference between modules and the class misalignment due to evolving class distributions. To address these issues, we propose scalable class-incremental learning based on parametric neural collapse (SCL-PNC) that enables demand-driven, minimal-cost backbone expansion by adapt-layer and refines the static into a dynamic parametric Equiangular Tight Frame (ETF) framework according to incremental class. This method can efficiently handle the model expansion question with the increasing number of categories in real-world scenarios. Additionally, to counteract feature drift in serial expansion models, the parallel expansion framework is presented with a knowledge distillation algorithm to align features across expansion modules. Therefore, SCL-PNC can not only design a dynamic and extensible ETF classifier to address class misalignment due to evolving class distributions, but also ensure feature consistency by an adapt-layer with knowledge distillation between extended modules. By leveraging neural collapse, SCL-PNC induces the convergence of the incremental expansion model through a structured combination of the expandable backbone, adapt-layer, and the parametric ETF classifier. Experiments on standard benchmarks demonstrate the effectiveness and efficiency of our proposed method. Our code is available at https://github.com/zhangchuangxin71-cyber/dynamic_ ETF2. Keywords: Class incremental learning; Catastrophic forgetting; Neural collapse;Knowledge distillation; Expanded model.

</details>


### [62] [Breaking Alignment Barriers: TPS-Driven Semantic Correlation Learning for Alignment-Free RGB-T Salient Object Detection](https://arxiv.org/abs/2512.21856)
*Lupiao Hu,Fasheng Wang,Fangmei Chen,Fuming Sun,Haojie Li*

Main category: cs.CV

TL;DR: Proposes TPS-SCL, a lightweight RGB-T salient object detection method for real-world unaligned image pairs using MobileViT encoder, Mamba scanning, and thin-plate spline alignment.


<details>
  <summary>Details</summary>
Motivation: Existing RGB-T SOD methods rely on manually aligned datasets and struggle with real-world unaligned image pairs due to spatial misalignment, scale variations, and viewpoint shifts, causing performance degradation.

Method: Uses dual-stream MobileViT encoder with Mamba scanning mechanisms; includes Semantic Correlation Constraint Module (SCCM) to suppress background interference, Thin-Plate Spline Alignment Module (TPSAM) to mitigate spatial discrepancies, and Cross-Modal Correlation Module (CMCM) to integrate inter-modal dependencies.

Result: Extensive experiments show TPS-SCL achieves state-of-the-art performance among lightweight SOD methods and outperforms mainstream RGB-T SOD approaches on various datasets.

Conclusion: TPS-SCL effectively addresses real-world unaligned RGB-T SOD challenges with efficient architecture and alignment mechanisms, demonstrating superior performance over existing methods.

Abstract: Existing RGB-T salient object detection methods predominantly rely on manually aligned and annotated datasets, struggling to handle real-world scenarios with raw, unaligned RGB-T image pairs. In practical applications, due to significant cross-modal disparities such as spatial misalignment, scale variations, and viewpoint shifts, the performance of current methods drastically deteriorates on unaligned datasets. To address this issue, we propose an efficient RGB-T SOD method for real-world unaligned image pairs, termed Thin-Plate Spline-driven Semantic Correlation Learning Network (TPS-SCL). We employ a dual-stream MobileViT as the encoder, combined with efficient Mamba scanning mechanisms, to effectively model correlations between the two modalities while maintaining low parameter counts and computational overhead. To suppress interference from redundant background information during alignment, we design a Semantic Correlation Constraint Module (SCCM) to hierarchically constrain salient features. Furthermore, we introduce a Thin-Plate Spline Alignment Module (TPSAM) to mitigate spatial discrepancies between modalities. Additionally, a Cross-Modal Correlation Module (CMCM) is incorporated to fully explore and integrate inter-modal dependencies, enhancing detection performance. Extensive experiments on various datasets demonstrate that TPS-SCL attains state-of-the-art (SOTA) performance among existing lightweight SOD methods and outperforms mainstream RGB-T SOD approaches.

</details>


### [63] [Fast Inference of Visual Autoregressive Model with Adjacency-Adaptive Dynamical Draft Trees](https://arxiv.org/abs/2512.21857)
*Haodong Lei,Hongsong Wang,Xin Geng,Liang Wang,Pan Zhou*

Main category: cs.CV

TL;DR: ADT-Tree accelerates autoregressive image generation by dynamically adjusting draft tree structure based on spatial prediction difficulty, achieving 3x speedup.


<details>
  <summary>Details</summary>
Motivation: Autoregressive image models produce high-quality images but suffer from slow sequential inference (2000 steps for 576x576). Existing speculative decoding methods from LLMs underperform on visual AR models due to spatially varying token prediction difficulty and inconsistent acceptance rates across draft trees.

Method: Proposes Adjacency-Adaptive Dynamical Draft Trees (ADT-Tree) that dynamically adjusts draft tree depth and width using adjacent token states and prior acceptance rates. Initializes via horizontal adjacency, then refines depth/width via bisectional adaptation - creating deeper trees in simple regions and wider trees in complex ones.

Result: Empirical evaluations on MS-COCO 2017 and PartiPrompts show speedups of 3.13x and 3.05x respectively. Integrates seamlessly with relaxed sampling methods like LANTERN for further acceleration.

Conclusion: ADT-Tree effectively addresses the spatial prediction difficulty challenge in visual AR models, achieving significant inference acceleration while maintaining quality, with potential for further improvements through integration with relaxed sampling techniques.

Abstract: Autoregressive (AR) image models achieve diffusion-level quality but suffer from sequential inference, requiring approximately 2,000 steps for a 576x576 image. Speculative decoding with draft trees accelerates LLMs yet underperforms on visual AR models due to spatially varying token prediction difficulty. We identify a key obstacle in applying speculative decoding to visual AR models: inconsistent acceptance rates across draft trees due to varying prediction difficulties in different image regions. We propose Adjacency-Adaptive Dynamical Draft Trees (ADT-Tree), an adjacency-adaptive dynamic draft tree that dynamically adjusts draft tree depth and width by leveraging adjacent token states and prior acceptance rates. ADT-Tree initializes via horizontal adjacency, then refines depth/width via bisectional adaptation, yielding deeper trees in simple regions and wider trees in complex ones. The empirical evaluations on MS-COCO 2017 and PartiPrompts demonstrate that ADT-Tree achieves speedups of 3.13xand 3.05x, respectively. Moreover, it integrates seamlessly with relaxed sampling methods such as LANTERN, enabling further acceleration. Code is available at https://github.com/Haodong-Lei-Ray/ADT-Tree.

</details>


### [64] [Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models](https://arxiv.org/abs/2512.21860)
*Masayuki Kawarada,Kosuke Yamada,Antonio Tejero-de-Pablos,Naoto Inoue*

Main category: cs.CV

TL;DR: DIOR is a training-free method that uses large vision-language models to generate conditional image embeddings by prompting them to describe images with single words related to specific conditions.


<details>
  <summary>Details</summary>
Motivation: Existing vision foundation models like CLIP provide rich image representations but aren't designed to focus on specific conditional aspects (e.g., color, genre) indicated by textual conditions. There's a need for methods that can generate embeddings that emphasize particular conditional attributes without requiring additional training.

Method: DIOR prompts a large vision-language model (LVLM) to describe an image with a single word related to a given condition, then extracts the hidden state vector of the LVLM's last token as the conditional image embedding. The approach is training-free and doesn't require task-specific priors.

Result: DIOR outperforms existing training-free baselines including CLIP on conditional image similarity tasks. It also achieves superior performance compared to methods that require additional training across multiple settings.

Conclusion: DIOR provides a versatile, training-free solution for generating conditional image embeddings that can focus on specific aspects indicated by textual conditions, demonstrating strong performance across various settings without requiring additional training.

Abstract: Conditional image embeddings are feature representations that focus on specific aspects of an image indicated by a given textual condition (e.g., color, genre), which has been a challenging problem. Although recent vision foundation models, such as CLIP, offer rich representations of images, they are not designed to focus on a specified condition. In this paper, we propose DIOR, a method that leverages a large vision-language model (LVLM) to generate conditional image embeddings. DIOR is a training-free approach that prompts the LVLM to describe an image with a single word related to a given condition. The hidden state vector of the LVLM's last token is then extracted as the conditional image embedding. DIOR provides a versatile solution that can be applied to any image and condition without additional training or task-specific priors. Comprehensive experimental results on conditional image similarity tasks demonstrate that DIOR outperforms existing training-free baselines, including CLIP. Furthermore, DIOR achieves superior performance compared to methods that require additional training across multiple settings.

</details>


### [65] [EasyOmnimatte: Taming Pretrained Inpainting Diffusion Models for End-to-End Video Layered Decomposition](https://arxiv.org/abs/2512.21865)
*Yihan Hu,Xuelin Chen,Xiaodong Cun*

Main category: cs.CV

TL;DR: EasyOmnimatte: A unified end-to-end video omnimatte method that finetunes a video inpainting diffusion model with dual complementary experts (Effect Expert and Quality Expert) for high-quality foreground layer decomposition with associated effects.


<details>
  <summary>Details</summary>
Motivation: Existing video omnimatte methods are slow, multi-stage, or use inference-time optimization that fails to fully exploit generative priors, producing suboptimal decompositions. The authors recognize that if a video inpainting model can be finetuned to remove foreground effects, it should also be capable of perceiving and decomposing them.

Method: Finetune a pretrained video inpainting diffusion model with dual experts: 1) Effect Expert with LoRA applied only to effect-sensitive DiT blocks to capture foreground and associated effects, and 2) Quality Expert with full LoRA finetuning to refine alpha mattes. During sampling, Effect Expert handles early high-noise steps while Quality Expert takes over at later low-noise steps.

Result: EasyOmnimatte sets new state-of-the-art for video omnimatte, significantly outperforming baselines in both quality and efficiency. It enables various downstream tasks and eliminates the need for two full diffusion passes, reducing computational cost without compromising output quality.

Conclusion: The proposed Dual-Expert strategy effectively addresses the limitations of existing methods by leveraging specialized experts for different aspects of the decomposition task, resulting in a unified, efficient, and high-quality video omnimatte solution.

Abstract: Existing video omnimatte methods typically rely on slow, multi-stage, or inference-time optimization pipelines that fail to fully exploit powerful generative priors, producing suboptimal decompositions. Our key insight is that, if a video inpainting model can be finetuned to remove the foreground-associated effects, then it must be inherently capable of perceiving these effects, and hence can also be finetuned for the complementary task: foreground layer decomposition with associated effects. However, although naïvely finetuning the inpainting model with LoRA applied to all blocks can produce high-quality alpha mattes, it fails to capture associated effects. Our systematic analysis reveals this arises because effect-related cues are primarily encoded in specific DiT blocks and become suppressed when LoRA is applied across all blocks. To address this, we introduce EasyOmnimatte, the first unified, end-to-end video omnimatte method. Concretely, we finetune a pretrained video inpainting diffusion model to learn dual complementary experts while keeping its original weights intact: an Effect Expert, where LoRA is applied only to effect-sensitive DiT blocks to capture the coarse structure of the foreground and associated effects, and a fully LoRA-finetuned Quality Expert learns to refine the alpha matte. During sampling, Effect Expert is used for denoising at early, high-noise steps, while Quality Expert takes over at later, low-noise steps. This design eliminates the need for two full diffusion passes, significantly reducing computational cost without compromising output quality. Ablation studies validate the effectiveness of this Dual-Expert strategy. Experiments demonstrate that EasyOmnimatte sets a new state-of-the-art for video omnimatte and enables various downstream tasks, significantly outperforming baselines in both quality and efficiency.

</details>


### [66] [DPAR: Dynamic Patchification for Efficient Autoregressive Visual Generation](https://arxiv.org/abs/2512.21867)
*Divyansh Srivastava,Akshay Mehra,Pranav Maneriker,Debopam Sanyal,Vishnu Raj,Vijay Kamarshi,Fan Du,Joshua Kimball*

Main category: cs.CV

TL;DR: DPAR is a decoder-only autoregressive model that dynamically aggregates image tokens into variable-sized patches using next-token prediction entropy, reducing token counts by 1.8-2x and FLOPs by up to 40% while improving image generation quality.


<details>
  <summary>Details</summary>
Motivation: Standard decoder-only autoregressive image generation suffers from quadratic growth in token counts with resolution, leading to high computational and memory demands for attention mechanisms. Fixed-length tokenization schemes are inefficient as they treat all image regions equally regardless of information content.

Method: DPAR uses next-token prediction entropy from a lightweight unsupervised autoregressive model as a criterion to dynamically merge tokens into larger patches based on information content. It makes minimal modifications to standard decoder architecture, allowing variable patch sizes during training and inference, with more compute allocated to high-information regions.

Result: DPAR reduces token count by 1.81x on ImageNet 256 and 2.06x on ImageNet 384 resolution, leading to up to 40% reduction in training FLOPs. The method exhibits faster convergence and improves FID by up to 27.1% relative to baseline models.

Conclusion: Dynamic patch aggregation based on information content enables efficient decoder-only autoregressive image generation with significant computational savings and quality improvements. The approach maintains compatibility with multimodal frameworks and scales effectively to larger resolutions.

Abstract: Decoder-only autoregressive image generation typically relies on fixed-length tokenization schemes whose token counts grow quadratically with resolution, substantially increasing the computational and memory demands of attention. We present DPAR, a novel decoder-only autoregressive model that dynamically aggregates image tokens into a variable number of patches for efficient image generation. Our work is the first to demonstrate that next-token prediction entropy from a lightweight and unsupervised autoregressive model provides a reliable criterion for merging tokens into larger patches based on information content. DPAR makes minimal modifications to the standard decoder architecture, ensuring compatibility with multimodal generation frameworks and allocating more compute to generation of high-information image regions. Further, we demonstrate that training with dynamically sized patches yields representations that are robust to patch boundaries, allowing DPAR to scale to larger patch sizes at inference. DPAR reduces token count by 1.81x and 2.06x on Imagenet 256 and 384 generation resolution respectively, leading to a reduction of up to 40% FLOPs in training costs. Further, our method exhibits faster convergence and improves FID by up to 27.1% relative to baseline models.

</details>


### [67] [SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis](https://arxiv.org/abs/2512.21881)
*Mo Wang,Junfeng Xia,Wenhao Ye,Enyu Liu,Kaining Peng,Jianfeng Feng,Quanying Liu,Hongkai Wen*

Main category: cs.CV

TL;DR: SLIM-Brain is a new fMRI foundation model that improves both data- and training-efficiency through a two-stage adaptive design with temporal saliency selection and hierarchical 4D encoding, achieving SOTA performance with only 4k pre-training sessions and 30% GPU memory usage.


<details>
  <summary>Details</summary>
Motivation: Current fMRI foundation models face a dual bottleneck: atlas-based methods lose spatial details and need huge datasets, while atlas-free methods preserve spatial fidelity but are computationally prohibitive for large-scale pre-training.

Method: Two-stage adaptive design: (1) lightweight temporal extractor captures global context and ranks data windows by saliency, (2) 4D hierarchical encoder (Hiera-JEPA) learns fine-grained voxel-level representations only from top-k selected windows while deleting ~70% masked patches.

Result: Establishes new state-of-the-art performance across seven public benchmarks while requiring only 4 thousand pre-training sessions and approximately 30% of GPU memory compared to traditional voxel-level methods.

Conclusion: SLIM-Brain successfully addresses the dual efficiency bottleneck in fMRI foundation models, providing an atlas-free solution that maintains spatial fidelity while being both data- and training-efficient.

Abstract: Foundation models are emerging as a powerful paradigm for fMRI analysis, but current approaches face a dual bottleneck of data- and training-efficiency. Atlas-based methods aggregate voxel signals into fixed regions of interest, reducing data dimensionality but discarding fine-grained spatial details, and requiring extremely large cohorts to train effectively as general-purpose foundation models. Atlas-free methods, on the other hand, operate directly on voxel-level information - preserving spatial fidelity but are prohibitively memory- and compute-intensive, making large-scale pre-training infeasible. We introduce SLIM-Brain (Sample-efficient, Low-memory fMRI Foundation Model for Human Brain), a new atlas-free foundation model that simultaneously improves both data- and training-efficiency. SLIM-Brain adopts a two-stage adaptive design: (i) a lightweight temporal extractor captures global context across full sequences and ranks data windows by saliency, and (ii) a 4D hierarchical encoder (Hiera-JEPA) learns fine-grained voxel-level representations only from the top-$k$ selected windows, while deleting about 70% masked patches. Extensive experiments across seven public benchmarks show that SLIM-Brain establishes new state-of-the-art performance on diverse tasks, while requiring only 4 thousand pre-training sessions and approximately 30% of GPU memory comparing to traditional voxel-level methods.

</details>


### [68] [Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer](https://arxiv.org/abs/2512.21883)
*Tianchen Deng,Wenhua Wu,Kunzhen Wu,Guangming Wang,Siting Zhu,Shenghai Yuan,Xun Chen,Guole Shen,Zhe Liu,Hesheng Wang*

Main category: cs.CV

TL;DR: Reloc-VGGT is a novel visual localization framework that uses early-fusion multi-view spatial integration instead of traditional late-fusion pairwise pose regression, achieving robust real-time performance in diverse environments.


<details>
  <summary>Details</summary>
Motivation: Traditional visual localization uses pairwise pose regression with late-fusion strategies that are insufficient for effective spatial integration and degrade in complex environments. There's a need for more robust approaches that can handle both structured and unstructured environments.

Method: Built on VGGT backbone for multi-view 3D geometry encoding, with pose tokenizer and projection module to exploit spatial relationships from multiple database views. Introduces sparse mask attention strategy to reduce computational cost from quadratic to linear complexity, enabling real-time performance.

Result: Trained on ~8 million posed image pairs, Reloc-VGGT demonstrates strong accuracy and remarkable generalization ability. Extensive experiments across diverse public datasets validate effectiveness and efficiency, delivering high-quality camera pose estimates in real time with robustness to unseen environments.

Conclusion: The proposed early-fusion multi-view spatial integration framework outperforms traditional late-fusion approaches, providing robust real-time visual localization that generalizes well to diverse environments. The sparse mask attention enables scalability while maintaining accuracy.

Abstract: Visual localization has traditionally been formulated as a pair-wise pose regression problem. Existing approaches mainly estimate relative poses between two images and employ a late-fusion strategy to obtain absolute pose estimates. However, the late motion average is often insufficient for effectively integrating spatial information, and its accuracy degrades in complex environments. In this paper, we present the first visual localization framework that performs multi-view spatial integration through an early-fusion mechanism, enabling robust operation in both structured and unstructured environments. Our framework is built upon the VGGT backbone, which encodes multi-view 3D geometry, and we introduce a pose tokenizer and projection module to more effectively exploit spatial relationships from multiple database views. Furthermore, we propose a novel sparse mask attention strategy that reduces computational cost by avoiding the quadratic complexity of global attention, thereby enabling real-time performance at scale. Trained on approximately eight million posed image pairs, Reloc-VGGT demonstrates strong accuracy and remarkable generalization ability. Extensive experiments across diverse public datasets consistently validate the effectiveness and efficiency of our approach, delivering high-quality camera pose estimates in real time while maintaining robustness to unseen environments. Our code and models will be publicly released upon acceptance.https://github.com/dtc111111/Reloc-VGGT.

</details>


### [69] [CrownGen: Patient-customized Crown Generation via Point Diffusion Model](https://arxiv.org/abs/2512.21890)
*Juyoung Bae,Moo Hyun Son,Jiale Peng,Wanting Qu,Wener Chen,Zelin Qiu,Kaixin Li,Xiaojuan Chen,Yifan Lin,Hao Chen*

Main category: cs.CV

TL;DR: CrownGen is a generative AI framework that automates patient-customized dental crown design using diffusion models on tooth-level point clouds, reducing manual design time while maintaining clinical quality.


<details>
  <summary>Details</summary>
Motivation: Digital crown design is labor-intensive and creates a bottleneck in restorative dentistry, limiting scalability and increasing costs for dental care.

Method: Uses a denoising diffusion model on tooth-level point cloud representation with two core components: boundary prediction module for spatial priors and diffusion-based generative module to synthesize high-fidelity morphology for multiple teeth in single inference.

Result: Outperforms state-of-the-art models in geometric fidelity, significantly reduces active design time, and clinical assessments show CrownGen-assisted crowns are statistically non-inferior to expert technician manual workflows.

Conclusion: CrownGen offers a scalable solution to automate complex prosthetic modeling, lowering costs, shortening turnaround times, and enhancing patient access to high-quality dental care.

Abstract: Digital crown design remains a labor-intensive bottleneck in restorative dentistry. We present \textbf{CrownGen}, a generative framework that automates patient-customized crown design using a denoising diffusion model on a novel tooth-level point cloud representation. The system employs two core components: a boundary prediction module to establish spatial priors and a diffusion-based generative module to synthesize high-fidelity morphology for multiple teeth in a single inference pass. We validated CrownGen through a quantitative benchmark on 496 external scans and a clinical study of 26 restoration cases. Results demonstrate that CrownGen surpasses state-of-the-art models in geometric fidelity and significantly reduces active design time. Clinical assessments by trained dentists confirmed that CrownGen-assisted crowns are statistically non-inferior in quality to those produced by expert technicians using manual workflows. By automating complex prosthetic modeling, CrownGen offers a scalable solution to lower costs, shorten turnaround times, and enhance patient access to high-quality dental care.

</details>


### [70] [High-Fidelity and Long-Duration Human Image Animation with Diffusion Transformer](https://arxiv.org/abs/2512.21905)
*Shen Zheng,Jiaran Cai,Yuansheng Guan,Shenneng Huang,Xingpei Ma,Junjie Cao,Hanfeng Zhao,Qiang Zhang,Shunsi Zhang,Xiao-Ping Zhang*

Main category: cs.CV

TL;DR: A DiT-based framework for high-fidelity, long-duration human animation with improved facial/hand details and arbitrary-length video generation.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models struggle with long-duration video generation and lack fine-grained facial/hand details, limiting real-world high-quality applications.

Method: Proposes a DiT-based framework with: 1) hybrid implicit guidance signals + sharpness guidance for facial/hand details, 2) Position Shift Adaptive Module for arbitrary-length videos, 3) data augmentation + skeleton alignment for identity shape variations.

Result: Outperforms state-of-the-art approaches in both high-fidelity and long-duration human image animation.

Conclusion: The proposed framework effectively addresses limitations in long-duration video generation and fine-grained detail synthesis for human animation.

Abstract: Recent progress in diffusion models has significantly advanced the field of human image animation. While existing methods can generate temporally consistent results for short or regular motions, significant challenges remain, particularly in generating long-duration videos. Furthermore, the synthesis of fine-grained facial and hand details remains under-explored, limiting the applicability of current approaches in real-world, high-quality applications. To address these limitations, we propose a diffusion transformer (DiT)-based framework which focuses on generating high-fidelity and long-duration human animation videos. First, we design a set of hybrid implicit guidance signals and a sharpness guidance factor, enabling our framework to additionally incorporate detailed facial and hand features as guidance. Next, we incorporate the time-aware position shift fusion module, modify the input format within the DiT backbone, and refer to this mechanism as the Position Shift Adaptive Module, which enables video generation of arbitrary length. Finally, we introduce a novel data augmentation strategy and a skeleton alignment model to reduce the impact of human shape variations across different identities. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches, achieving superior performance in both high-fidelity and long-duration human image animation.

</details>


### [71] [Patch as Node: Human-Centric Graph Representation Learning for Multimodal Action Recognition](https://arxiv.org/abs/2512.21916)
*Zeyu Liang,Hailun Xia,Naichuan Zheng*

Main category: cs.CV

TL;DR: PAN: Human-centric graph representation learning framework for multimodal action recognition that fuses RGB and skeleton data by representing RGB patches containing human joints as spatiotemporal graphs.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal methods fusing RGB and skeleton data suffer from inherent heterogeneity between modalities and fail to fully exploit their complementary potential. There's a need for more effective fusion that aligns better with skeleton-based approaches.

Method: Proposes PAN framework with human-centric graph representation learning where token embeddings of RGB patches containing human joints are represented as spatiotemporal graphs. Includes attention-based post calibration to reduce dependency on high-quality skeletal data. Two variants: PAN-Ensemble (dual-path GCNs with late fusion) and PAN-Unified (unified graph representation learning in single network).

Result: Both PAN-Ensemble and PAN-Unified achieve state-of-the-art performance on three widely used multimodal action recognition datasets in their respective settings: separate modeling (PAN-Ensemble) and unified modeling (PAN-Unified).

Conclusion: PAN provides an effective human-centric graph modeling approach that suppresses RGB redundancy and enables semantically coherent multimodal fusion, achieving SOTA performance while reducing dependency on high-quality skeletal data.

Abstract: While human action recognition has witnessed notable achievements, multimodal methods fusing RGB and skeleton modalities still suffer from their inherent heterogeneity and fail to fully exploit the complementary potential between them. In this paper, we propose PAN, the first human-centric graph representation learning framework for multimodal action recognition, in which token embeddings of RGB patches containing human joints are represented as spatiotemporal graphs. The human-centric graph modeling paradigm suppresses the redundancy in RGB frames and aligns well with skeleton-based methods, thus enabling a more effective and semantically coherent fusion of multimodal features. Since the sampling of token embeddings heavily relies on 2D skeletal data, we further propose attention-based post calibration to reduce the dependency on high-quality skeletal data at a minimal cost interms of model performance. To explore the potential of PAN in integrating with skeleton-based methods, we present two variants: PAN-Ensemble, which employs dual-path graph convolution networks followed by late fusion, and PAN-Unified, which performs unified graph representation learning within a single network. On three widely used multimodal action recognition datasets, both PAN-Ensemble and PAN-Unified achieve state-of-the-art (SOTA) performance in their respective settings of multimodal fusion: separate and unified modeling, respectively.

</details>


### [72] [AutoPP: Towards Automated Product Poster Generation and Optimization](https://arxiv.org/abs/2512.21921)
*Jiahao Fan,Yuxin Qin,Wei Feng,Yanyin Chen,Yaoyu Li,Ao Ma,Yixiu Li,Li Zhuang,Haoyi Bian,Zheng Zhang,Jingjing Lv,Junjie Shen,Ching Law*

Main category: cs.CV

TL;DR: AutoPP is an automated pipeline for product poster generation and optimization that eliminates human intervention by combining AI-generated posters with online CTR optimization using feedback data.


<details>
  <summary>Details</summary>
Motivation: Manual creation and optimization of product posters is labor-intensive and resource-consuming. Current approaches require human designers to craft appealing posters and manually optimize them based on online performance metrics.

Method: Two-stage pipeline: 1) Generator uses basic product info with unified design module (background, text, layout) and element rendering module to create posters; 2) Optimizer enhances CTR using online feedback via systematic element replacement and Isolated Direct Preference Optimization (IDPO) to attribute CTR gains to isolated elements.

Result: AutoPP achieves state-of-the-art results in both offline and online settings, supported by AutoPP1M dataset containing 1M high-quality posters and user feedback from over 1M users.

Conclusion: AutoPP provides a fully automated solution for product poster generation and optimization that outperforms existing methods, with publicly available code and dataset to advance research in this area.

Abstract: Product posters blend striking visuals with informative text to highlight the product and capture customer attention. However, crafting appealing posters and manually optimizing them based on online performance is laborious and resource-consuming. To address this, we introduce AutoPP, an automated pipeline for product poster generation and optimization that eliminates the need for human intervention. Specifically, the generator, relying solely on basic product information, first uses a unified design module to integrate the three key elements of a poster (background, text, and layout) into a cohesive output. Then, an element rendering module encodes these elements into condition tokens, efficiently and controllably generating the product poster. Based on the generated poster, the optimizer enhances its Click-Through Rate (CTR) by leveraging online feedback. It systematically replaces elements to gather fine-grained CTR comparisons and utilizes Isolated Direct Preference Optimization (IDPO) to attribute CTR gains to isolated elements. Our work is supported by AutoPP1M, the largest dataset specifically designed for product poster generation and optimization, which contains one million high-quality posters and feedback collected from over one million users. Experiments demonstrate that AutoPP achieves state-of-the-art results in both offline and online settings. Our code and dataset are publicly available at: https://github.com/JD-GenX/AutoPP

</details>


### [73] [Data relativistic uncertainty framework for low-illumination anime scenery image enhancement](https://arxiv.org/abs/2512.21944)
*Yiquan Gao,John See*

Main category: cs.CV

TL;DR: Proposes DRU framework for low-light enhancement in anime scenery images using data relativistic uncertainty to handle illumination diversity, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Addresses domain gap in low-light enhancement for anime scenery images, which is underexplored compared to natural images/videos, and tackles data scarcity in this niche domain.

Method: Curates unpaired anime scenery dataset, proposes Data Relativistic Uncertainty (DRU) framework inspired by Relativistic GAN, quantifies illumination uncertainty using wave-particle duality analogy, and dynamically adjusts objective functions based on uncertainty.

Result: DRU framework demonstrates superior perceptual and aesthetic qualities over state-of-the-art methods, showing effectiveness by training multiple EnlightenGAN versions that learn from data uncertainty perspective.

Conclusion: DRU framework exposes novel data-centric learning paradigm for visual and language domains, effectively handling illumination uncertainty in anime scenery enhancement with available code.

Abstract: By contrast with the prevailing works of low-light enhancement in natural images and videos, this study copes with the low-illumination quality degradation in anime scenery images to bridge the domain gap. For such an underexplored enhancement task, we first curate images from various sources and construct an unpaired anime scenery dataset with diverse environments and illumination conditions to address the data scarcity. To exploit the power of uncertainty information inherent with the diverse illumination conditions, we propose a Data Relativistic Uncertainty (DRU) framework, motivated by the idea from Relativistic GAN. By analogy with the wave-particle duality of light, our framework interpretably defines and quantifies the illumination uncertainty of dark/bright samples, which is leveraged to dynamically adjust the objective functions to recalibrate the model learning under data uncertainty. Extensive experiments demonstrate the effectiveness of DRU framework by training several versions of EnlightenGANs, yielding superior perceptual and aesthetic qualities beyond the state-of-the-art methods that are incapable of learning from data uncertainty perspective. We hope our framework can expose a novel paradigm of data-centric learning for potential visual and language domains. Code is available.

</details>


### [74] [Automated Discovery of Parsimonious Spectral Indices via Normalized Difference Polynomials](https://arxiv.org/abs/2512.21948)
*Ali Lotfi,Adam Carter,Thuan Ha,Mohammad Meysami,Kwabena Nketia,Steve Shirtliffe*

Main category: cs.CV

TL;DR: Automated method finds compact spectral indices for vegetation classification by generating polynomial combinations of normalized band differences and selecting optimal subsets via feature selection.


<details>
  <summary>Details</summary>
Motivation: To automate the discovery of simple, interpretable spectral indices for vegetation classification that maintain illumination invariance and can be easily deployed in platforms like Google Earth Engine.

Method: Generate all pairwise normalized differences from spectral bands, create polynomial combinations up to degree 2, then apply feature selection methods (ANOVA filtering, recursive elimination, L1-regularized SVM) to select compact index sets.

Result: For Kochia detection using Sentinel-2 imagery, a single degree-2 index (product of two normalized differences from red-edge bands) achieved 96.26% accuracy; using eight indices increased accuracy to 97.70%.

Conclusion: The method successfully finds compact, interpretable spectral indices that capture discriminative signals through spectral interactions rather than individual band ratios, with open-source implementation available for broader application.

Abstract: We introduce an automated way to find compact spectral indices for vegetation classification. The idea is to take all pairwise normalized differences from the spectral bands and then build polynomial combinations up to a fixed degree, which gives a structured search space that still keeps the illumination invariance needed in remote sensing. For a sensor with $n$ bands this produces $\binom{n}{2}$ base normalized differences, and the degree-2 polynomial expansion gives 1,080 candidate features for the 10-band Sentinel-2 configuration we use here. Feature selection methods (ANOVA filtering, recursive elimination, and $L_1$-regularized SVM) then pick out small sets of indices that reach the desired accuracy, so the final models stay simple and easy to interpret. We test the framework on Kochia (\textit{Bassia scoparia}) detection using Sentinel-2 imagery from Saskatchewan, Canada ($N = 2{,}318$ samples, 2022--2024). A single degree-2 index, the product of two normalized differences from the red-edge bands, already reaches 96.26\% accuracy, and using eight indices only raises this to 97.70\%. In every case the chosen features are degree-2 products built from bands $b_4$ through $b_8$, which suggests that the discriminative signal comes from spectral \emph{interactions} rather than individual band ratios. Because the indices involve only simple arithmetic, they can be deployed directly in platforms like Google Earth Engine. The same approach works for other sensors and classification tasks, and an open-source implementation (\texttt{ndindex}) is available.

</details>


### [75] [Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs](https://arxiv.org/abs/2512.21999)
*Jiayu Hu,Beibei Li,Jiangwei Xia,Yanjun Qin,Bing Ji,Zhongshi He*

Main category: cs.CV

TL;DR: ALEAHallu is an adversarial parametric editing framework that mitigates hallucinations in Vision-Language Models by identifying and fine-tuning hallucination-prone parameter clusters using adversarial prompts.


<details>
  <summary>Details</summary>
Motivation: VLMs suffer from persistent hallucination issues due to over-reliance on linguistic priors and insufficient visual feature integration. Existing heuristic decoding calibration strategies are non-trainable and have limited optimization potential.

Method: Proposes an Activate-Locate-Edit Adversarially paradigm: 1) Construct activation dataset with grounded (positive) and hallucinatory (negative) responses, 2) Identify critical hallucination-prone parameter clusters by analyzing differential hidden states, 3) Fine-tune these clusters using prompts with adversarial prefixes optimized to maximize visual neglect.

Result: Evaluations on both generative and discriminative VLM tasks demonstrate significant effectiveness in alleviating hallucinations.

Conclusion: ALEAHallu provides an effective trainable framework for mitigating VLM hallucinations by forcing models to prioritize visual evidence over inherent parametric biases through adversarial parametric editing.

Abstract: While Vision-Language Models (VLMs) have garnered increasing attention in the AI community due to their promising practical applications, they exhibit persistent hallucination issues, generating outputs misaligned with visual inputs. Recent studies attribute these hallucinations to VLMs' over-reliance on linguistic priors and insufficient visual feature integration, proposing heuristic decoding calibration strategies to mitigate them. However, the non-trainable nature of these strategies inherently limits their optimization potential. To this end, we propose an adversarial parametric editing framework for Hallucination mitigation in VLMs, which follows an \textbf{A}ctivate-\textbf{L}ocate-\textbf{E}dit \textbf{A}dversarially paradigm. Specifically, we first construct an activation dataset that comprises grounded responses (positive samples attentively anchored in visual features) and hallucinatory responses (negative samples reflecting LLM prior bias and internal knowledge artifacts). Next, we identify critical hallucination-prone parameter clusters by analyzing differential hidden states of response pairs. Then, these clusters are fine-tuned using prompts injected with adversarial tuned prefixes that are optimized to maximize visual neglect, thereby forcing the model to prioritize visual evidence over inherent parametric biases. Evaluations on both generative and discriminative VLM tasks demonstrate the significant effectiveness of ALEAHallu in alleviating hallucinations. Our code is available at https://github.com/hujiayu1223/ALEAHallu.

</details>


### [76] [Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models](https://arxiv.org/abs/2512.21964)
*Dunyuan XU,Xikai Yang,Yaoqian Li,Juzheng Miao,Jinpeng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: Medical MLLMs are sensitive to real-world noise like imaging artifacts and text errors, undermining clinical use. This paper introduces a training-free framework (IMC) that leverages MLLMs' inherent denoising capabilities to enhance robustness across visual and textual modalities without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Medical MLLMs show promising clinical performance but are vulnerable to real-world input perturbations (imaging artifacts, textual errors), which critically undermines their clinical applicability. Existing robustness studies focus mainly on text modality and require costly fine-tuning, failing to address complex medical noise patterns and strict safety standards.

Method: Proposes Inherent-enhanced Multi-modal Calibration (IMC) framework: 1) For visual modality: Perturbation-aware Denoising Calibration (PDC) uses MLLMs' vision encoder to identify noise patterns and perform prototype-guided feature calibration. 2) For text modality: Self-instantiated Multi-agent System (SMS) leverages MLLMs' self-assessment capabilities to refine noisy text through a cooperative hierarchy of agents. The approach is training-free and follows perceive-and-calibrate principle.

Result: Constructed benchmark with 11 noise types across image and text modalities on 2 datasets. Experimental results demonstrate state-of-the-art performance across multiple modalities, showing potential to enhance MLLMs' robustness in real clinical scenarios.

Conclusion: The proposed training-free IMC framework effectively enhances medical MLLMs' robustness to real-world perturbations by leveraging their inherent denoising capabilities, addressing critical safety concerns for clinical deployment without requiring costly fine-tuning.

Abstract: Medical Multi-modal Large Language Models (MLLMs) have shown promising clinical performance. However, their sensitivity to real-world input perturbations, such as imaging artifacts and textual errors, critically undermines their clinical applicability. Systematic analysis of such noise impact on medical MLLMs remains largely unexplored. Furthermore, while several works have investigated the MLLMs' robustness in general domains, they primarily focus on text modality and rely on costly fine-tuning. They are inadequate to address the complex noise patterns and fulfill the strict safety standards in medicine. To bridge this gap, this work systematically analyzes the impact of various perturbations on medical MLLMs across both visual and textual modalities. Building on our findings, we introduce a training-free Inherent-enhanced Multi-modal Calibration (IMC) framework that leverages MLLMs' inherent denoising capabilities following the perceive-and-calibrate principle for cross-modal robustness enhancement. For the visual modality, we propose a Perturbation-aware Denoising Calibration (PDC) which leverages MLLMs' own vision encoder to identify noise patterns and perform prototype-guided feature calibration. For text denoising, we design a Self-instantiated Multi-agent System (SMS) that exploits the MLLMs' self-assessment capabilities to refine noisy text through a cooperative hierarchy of agents. We construct a benchmark containing 11 types of noise across both image and text modalities on 2 datasets. Experimental results demonstrate our method achieves the state-of-the-art performance across multiple modalities, showing potential to enhance MLLMs' robustness in real clinical scenarios.

</details>


### [77] [A Lightweight Multi-Scale Attention Framework for Real-Time Spinal Endoscopic Instance Segmentation](https://arxiv.org/abs/2512.21984)
*Qi Lai,JunYan Li,Qiang Cai,Lei Wang,Tao Yan,XiaoKun Liang*

Main category: cs.CV

TL;DR: LMSF-A is a lightweight multi-scale attention framework for real-time spinal endoscopy instance segmentation that balances accuracy and speed with only 1.8M parameters, addressing challenges like narrow FOV, specular highlights, and batch-1 training constraints.


<details>
  <summary>Details</summary>
Motivation: Real-time instance segmentation in spinal endoscopy is crucial for surgical safety but challenging due to narrow field of view, specular highlights, smoke/bleeding, unclear boundaries, and scale changes. Deployment is constrained by limited surgical hardware requiring models to balance accuracy/speed and remain stable under small-batch training.

Method: Co-designed lightweight multi-scale attention framework with three components: 1) Backbone uses C2f-Pro module combining RepViT-style re-parameterized convolution with efficient multi-scale attention for multi-branch training collapsing to single fast path; 2) Neck improves cross-scale consistency and boundaries using Scale-Sequence Feature Fusion and Triple Feature Encoding; 3) Head adopts Lightweight Multi-task Shared Head with shared convolutions and GroupNorm for parameter reduction and batch-1 stability.

Result: LMSF-A achieves competitive or better performance across all evaluation metrics with only 1.8M parameters and 8.8 GFLOPs. It generalizes well to public teeth benchmark. Authors release clinically reviewed PELD dataset (61 patients, 610 images) with instance masks for adipose tissue, bone, ligamentum flavum, and nerve.

Conclusion: The proposed LMSF-A framework effectively addresses real-time instance segmentation challenges in spinal endoscopy through lightweight multi-scale attention design, achieving excellent accuracy-speed balance with minimal parameters while maintaining stability under surgical deployment constraints.

Abstract: Real-time instance segmentation for spinal endoscopy is important for identifying and protecting critical anatomy during surgery, but it is difficult because of the narrow field of view, specular highlights, smoke/bleeding, unclear boundaries, and large scale changes. Deployment is also constrained by limited surgical hardware, so the model must balance accuracy and speed and remain stable under small-batch (even batch-1) training. We propose LMSF-A, a lightweight multi-scale attention framework co-designed across backbone, neck, and head. The backbone uses a C2f-Pro module that combines RepViT-style re-parameterized convolution (RVB) with efficient multi-scale attention (EMA), enabling multi-branch training while collapsing into a single fast path for inference. The neck improves cross-scale consistency and boundary detail using Scale-Sequence Feature Fusion (SSFF) and Triple Feature Encoding (TFE), which strengthens high-resolution features. The head adopts a Lightweight Multi-task Shared Head (LMSH) with shared convolutions and GroupNorm to reduce parameters and support batch-1 stability. We also release the clinically reviewed PELD dataset (61 patients, 610 images) with instance masks for adipose tissue, bone, ligamentum flavum, and nerve. Experiments show that LMSF-A is highly competitive (or even better than) in all evaluation metrics and much lighter than most instance segmentation methods requiring only 1.8M parameters and 8.8 GFLOPs, and it generalizes well to a public teeth benchmark. Code and dataset: https://github.com/hhwmortal/PELD-Instance-segmentation.

</details>


### [78] [iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception](https://arxiv.org/abs/2512.22009)
*Sarthak Mehrotra,Sairam V C Rebbapragada,Mani Hemanth Reddy Bonthu,Vineeth N Balasubramanian*

Main category: cs.CV

TL;DR: iSHIFT is a lightweight 2.5B parameter GUI agent that combines implicit chain-of-thought reasoning with perception control to dynamically switch between slow (detailed visual grounding) and fast (global cues) modes for efficient and precise interface interactions.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with balancing efficiency for routine GUI tasks and precision for fine-grained interactions requiring exact visual grounding. They also remain large and lack adaptive reasoning depth based on task complexity.

Method: iSHIFT integrates latent thinking (implicit chain-of-thought) with a perception control module using special perception tokens. This allows the model to dynamically switch between slow mode (detailed visual grounding for precision) and fast mode (global cues for efficiency), while guiding attention to relevant screen regions.

Result: Despite its compact 2.5B parameter size, iSHIFT matches state-of-the-art performance on multiple benchmark datasets, demonstrating effective balance between efficiency and precision in GUI interactions.

Conclusion: iSHIFT successfully addresses the efficiency-precision tradeoff in GUI agents through adaptive reasoning modes and visual attention control, achieving strong performance with a lightweight architecture.

Abstract: Multimodal Large Language Models (MLLMs) show strong potential for interpreting and interacting with complex, pixel-rich Graphical User Interface (GUI) environments. However, building agents that are both efficient for high-level tasks and precise for fine-grained interactions remains challenging. GUI agents must perform routine actions efficiently while also handling tasks that demand exact visual grounding, yet existing approaches struggle when accuracy depends on identifying specific interface elements. These MLLMs also remain large and cannot adapt their reasoning depth to the task at hand. In this work, we introduce iSHIFT: Implicit Slow-fast Hybrid Inference with Flexible Tokens, a lightweight agent that integrates latent thinking (implicit chain-of-thought) with a perception control module. iSHIFT enables an MLLM to switch between a slow mode, which leverages detailed visual grounding for high precision and a fast mode that uses global cues for efficiency. Special perception tokens guide attention to relevant screen regions, allowing the model to decide both how to reason and where to focus. Despite its compact 2.5B size, iSHIFT matches state-of-the-art performance on multiple benchmark datasets.

</details>


### [79] [Patch-Discontinuity Mining for Generalized Deepfake Detection](https://arxiv.org/abs/2512.22027)
*Huanhuan Yuan,Yang Ping,Zhengqin Xu,Junyi Cao,Shuai Jia,Chao Ma*

Main category: cs.CV

TL;DR: GenDF: A lightweight framework using large vision models for deepfake detection with strong cross-domain generalization using only 0.28M parameters.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods struggle with unseen forgery patterns and suffer performance degradation in cross-domain settings, while often relying on complex architectures and handcrafted forensic cues.

Method: Transfers large-scale vision model to deepfake detection with compact design, incorporating deepfake-specific representation learning, feature space redistribution to address distribution mismatch, and classification-invariant feature augmentation for generalization without extra parameters.

Result: Achieves state-of-the-art generalization performance in cross-domain and cross-manipulation settings while requiring only 0.28M trainable parameters, demonstrating both effectiveness and efficiency.

Conclusion: GenDF provides a simple yet effective framework for deepfake detection that addresses generalization challenges through transfer learning and specialized techniques, offering strong performance with minimal parameters.

Abstract: The rapid advancement of generative artificial intelligence has enabled the creation of highly realistic fake facial images, posing serious threats to personal privacy and the integrity of online information. Existing deepfake detection methods often rely on handcrafted forensic cues and complex architectures, achieving strong performance in intra-domain settings but suffering significant degradation when confronted with unseen forgery patterns. In this paper, we propose GenDF, a simple yet effective framework that transfers a powerful large-scale vision model to the deepfake detection task with a compact and neat network design. GenDF incorporates deepfake-specific representation learning to capture discriminative patterns between real and fake facial images, feature space redistribution to mitigate distribution mismatch, and a classification-invariant feature augmentation strategy to enhance generalization without introducing additional trainable parameters. Extensive experiments demonstrate that GenDF achieves state-of-the-art generalization performance in cross-domain and cross-manipulation settings while requiring only 0.28M trainable parameters, validating the effectiveness and efficiency of the proposed framework.

</details>


### [80] [Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models](https://arxiv.org/abs/2512.22046)
*Zongmin Zhang,Zhen Sun,Yifan Liao,Wenhan Dong,Xinlei He,Xingshuo Han,Shengmin Xu,Xinyi Huang*

Main category: cs.CV

TL;DR: BadVSFM is a novel backdoor attack framework specifically designed for prompt-driven video segmentation foundation models, achieving high attack success rates while maintaining clean performance, unlike traditional backdoor attacks which fail on these models.


<details>
  <summary>Details</summary>
Motivation: Prompt-driven Video Segmentation Foundation Models (VSFMs) are increasingly used in critical applications, raising security concerns. Traditional backdoor attacks are ineffective on VSFMs (ASR <5%), creating a need for specialized attacks to understand and address this vulnerability.

Method: BadVSFM uses a two-stage strategy: (1) Steer the image encoder so triggered frames map to target embeddings while clean frames align with a reference encoder; (2) Train the mask decoder so triggered frame-prompt pairs produce a shared target mask across prompt types, while clean outputs stay close to a reference decoder.

Result: Extensive experiments on two datasets and five VSFMs show BadVSFM achieves strong, controllable backdoor effects under diverse triggers and prompts while preserving clean segmentation quality. The attack remains robust to hyperparameter changes and defeats four representative defenses.

Conclusion: BadVSFM reveals a significant vulnerability in current VSFMs by successfully separating triggered and clean representations and shifting attention to trigger regions, demonstrating that specialized attacks are needed for prompt-driven foundation models and highlighting an underexplored security threat.

Abstract: Prompt-driven Video Segmentation Foundation Models (VSFMs) such as SAM2 are increasingly deployed in applications like autonomous driving and digital pathology, raising concerns about backdoor threats. Surprisingly, we find that directly transferring classic backdoor attacks (e.g., BadNet) to VSFMs is almost ineffective, with ASR below 5\%. To understand this, we study encoder gradients and attention maps and observe that conventional training keeps gradients for clean and triggered samples largely aligned, while attention still focuses on the true object, preventing the encoder from learning a distinct trigger-related representation. To address this challenge, we propose BadVSFM, the first backdoor framework tailored to prompt-driven VSFMs. BadVSFM uses a two-stage strategy: (1) steer the image encoder so triggered frames map to a designated target embedding while clean frames remain aligned with a clean reference encoder; (2) train the mask decoder so that, across prompt types, triggered frame-prompt pairs produce a shared target mask, while clean outputs stay close to a reference decoder. Extensive experiments on two datasets and five VSFMs show that BadVSFM achieves strong, controllable backdoor effects under diverse triggers and prompts while preserving clean segmentation quality. Ablations over losses, stages, targets, trigger settings, and poisoning rates demonstrate robustness to reasonable hyperparameter changes and confirm the necessity of the two-stage design. Finally, gradient-conflict analysis and attention visualizations show that BadVSFM separates triggered and clean representations and shifts attention to trigger regions, while four representative defenses remain largely ineffective, revealing an underexplored vulnerability in current VSFMs.

</details>


### [81] [MAI-UI Technical Report: Real-World Centric Foundation GUI Agents](https://arxiv.org/abs/2512.22047)
*Hanzhang Zhou,Xu Zhang,Panrong Tong,Jianan Zhang,Liangyu Chen,Quyu Kong,Chenglin Cai,Chen Liu,Yue Wang,Jingren Zhou,Steven Hoi*

Main category: cs.CV

TL;DR: MAI-UI is a family of foundation GUI agents that addresses key deployment challenges through self-evolving data, device-cloud collaboration, and online RL, achieving SOTA results on GUI grounding and mobile navigation benchmarks.


<details>
  <summary>Details</summary>
Motivation: GUI agents could revolutionize human-computer interaction, but face four key deployment challenges: lack of native agent-user interaction, limits of UI-only operation, absence of practical deployment architecture, and brittleness in dynamic environments.

Method: Unified methodology with: 1) self-evolving data pipeline expanding navigation data to include user interaction and MCP tool calls, 2) native device-cloud collaboration system routing execution by task state, and 3) online RL framework with advanced optimizations for scaling parallel environments and context length.

Result: Achieves SOTA across benchmarks: 73.5% on ScreenSpot-Pro, 91.3% on MMBench GUI L2, 70.9% on OSWorld-G, 49.2% on UI-Vision; 76.7% on AndroidWorld; 41.7% on MobileWorld. Online RL scaling shows +5.2 points from 32 to 512 parallel environments and +4.3 points from 15 to 50 environment steps. Device-cloud system improves on-device performance by 33% and reduces cloud calls by 40%.

Conclusion: MAI-UI successfully addresses key GUI agent deployment challenges through its unified methodology, establishing new state-of-the-art performance while enabling practical deployment with privacy-preserving device-cloud collaboration.

Abstract: The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent-user interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI addresses these issues with a unified methodology: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device-cloud collaboration system routes execution by task state, and an online RL framework with advanced optimizations to scale parallel environments and context length. MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation. On grounding benchmarks, it reaches 73.5% on ScreenSpot-Pro, 91.3% on MMBench GUI L2, 70.9% on OSWorld-G, and 49.2% on UI-Vision, surpassing Gemini-3-Pro and Seed1.8 on ScreenSpot-Pro. On mobile GUI navigation, it sets a new SOTA of 76.7% on AndroidWorld, surpassing UI-Tars-2, Gemini-2.5-Pro and Seed1.8. On MobileWorld, MAI-UI obtains 41.7% success rate, significantly outperforming end-to-end GUI models and competitive with Gemini-3-Pro based agentic frameworks. Our online RL experiments show significant gains from scaling parallel environments from 32 to 512 (+5.2 points) and increasing environment step budget from 15 to 50 (+4.3 points). Finally, the native device-cloud collaboration system improves on-device performance by 33%, reduces cloud model calls by over 40%, and preserves user privacy.

</details>


### [82] [Yume-1.5: A Text-Controlled Interactive World Generation Model](https://arxiv.org/abs/2512.22096)
*Xiaofeng Mao,Zhen Li,Chuanhao Li,Xiaojie Xu,Kaining Ying,Tong He,Jiangmiao Pang,Yu Qiao,Kaipeng Zhang*

Main category: cs.CV

TL;DR: A novel framework called \method generates interactive, continuous worlds from single images or text prompts using diffusion models with real-time performance and text control.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based world generation methods suffer from large parameter sizes, slow inference, growing historical context, and lack text control, limiting real-time performance.

Method: Three core components: 1) long-video generation with unified context compression and linear attention, 2) real-time streaming acceleration via bidirectional attention distillation and enhanced text embedding, 3) text-controlled world event generation.

Result: The framework generates realistic, interactive, continuous worlds from single images or text prompts with keyboard-based exploration capabilities.

Conclusion: \method addresses key limitations of previous approaches by enabling real-time, text-controlled generation of explorable worlds with efficient architecture and acceleration strategies.

Abstract: Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.

</details>


### [83] [Learning Association via Track-Detection Matching for Multi-Object Tracking](https://arxiv.org/abs/2512.22105)
*Momir Adžemović*

Main category: cs.CV

TL;DR: TDLP is a tracking-by-detection method that uses link prediction between tracks and detections for per-frame association, learning from data without handcrafted rules while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing multi-object tracking methods have trade-offs: tracking-by-detection is efficient but uses handcrafted heuristics, while end-to-end methods learn association but are computationally expensive. There's a need for a method that learns association from data while remaining efficient.

Method: Track-Detection Link Prediction (TDLP) performs per-frame association via link prediction between tracks and detections. It predicts the correct continuation of each track at every frame, primarily using geometric features like bounding boxes, with optional incorporation of pose and appearance cues.

Result: Extensive experiments show TDLP consistently surpasses state-of-the-art performance across both tracking-by-detection and end-to-end methods on multiple benchmarks. Analysis shows link prediction is more effective than metric learning-based association, especially with heterogeneous features like detection bounding boxes.

Conclusion: TDLP offers a novel approach that combines the computational efficiency of tracking-by-detection with the learning capability of end-to-end methods, providing superior performance while maintaining modularity and efficiency.

Abstract: Multi-object tracking aims to maintain object identities over time by associating detections across video frames. Two dominant paradigms exist in literature: tracking-by-detection methods, which are computationally efficient but rely on handcrafted association heuristics, and end-to-end approaches, which learn association from data at the cost of higher computational complexity. We propose Track-Detection Link Prediction (TDLP), a tracking-by-detection method that performs per-frame association via link prediction between tracks and detections, i.e., by predicting the correct continuation of each track at every frame. TDLP is architecturally designed primarily for geometric features such as bounding boxes, while optionally incorporating additional cues, including pose and appearance. Unlike heuristic-based methods, TDLP learns association directly from data without handcrafted rules, while remaining modular and computationally efficient compared to end-to-end trackers. Extensive experiments on multiple benchmarks demonstrate that TDLP consistently surpasses state-of-the-art performance across both tracking-by-detection and end-to-end methods. Finally, we provide a detailed analysis comparing link prediction with metric learning-based association and show that link prediction is more effective, particularly when handling heterogeneous features such as detection bounding boxes. Our code is available at \href{https://github.com/Robotmurlock/TDLP}{https://github.com/Robotmurlock/TDLP}.

</details>


### [84] [ProEdit: Inversion-based Editing From Prompts Done Right](https://arxiv.org/abs/2512.22118)
*Zhi Ouyang,Dian Zheng,Xiao-Ming Wu,Jian-Jian Jiang,Kun-Yu Lin,Jingke Meng,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: ProEdit improves inversion-based visual editing by addressing source information over-reliance through KV-mix (attention mixing) and Latents-Shift (latent perturbation), achieving better attribute changes while maintaining consistency.


<details>
  <summary>Details</summary>
Motivation: Existing inversion-based editing methods overly rely on source image information during sampling, which negatively affects target edits - often failing to properly change subject attributes like pose, number, or color as instructed.

Method: Two-pronged approach: 1) KV-mix mixes KV features of source and target in edited regions to mitigate source influence while maintaining background consistency; 2) Latents-Shift perturbs edited regions of source latent to eliminate inverted latent influence on sampling.

Result: Extensive experiments on image and video editing benchmarks demonstrate state-of-the-art performance. The method is plug-and-play and can be seamlessly integrated with existing inversion and editing methods like RF-Solver, FireFlow, and UniEdit.

Conclusion: ProEdit effectively addresses the source information over-reliance problem in inversion-based editing through attention and latent modifications, enabling better attribute changes while maintaining editing consistency.

Abstract: Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on source information, which negatively affects the edits in the target image (e.g., failing to change the subject's atributes like pose, number, or color as instructed). In this work, we propose ProEdit to address this issue both in the attention and the latent aspects. In the attention aspect, we introduce KV-mix, which mixes KV features of the source and the target in the edited region, mitigating the influence of the source image on the editing region while maintaining background consistency. In the latent aspect, we propose Latents-Shift, which perturbs the edited region of the source latent, eliminating the influence of the inverted latent on the sampling. Extensive experiments on several image and video editing benchmarks demonstrate that our method achieves SOTA performance. In addition, our design is plug-and-play, which can be seamlessly integrated into existing inversion and editing methods, such as RF-Solver, FireFlow and UniEdit.

</details>


### [85] [See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning](https://arxiv.org/abs/2512.22120)
*Shuoshuo Zhang,Yizhen Zhang,Jingjing Fu,Lei Song,Jiang Bian,Yujiu Yang,Rui Wang*

Main category: cs.CV

TL;DR: Bi-directional Perceptual Shaping (BiPS) improves vision-language models by using bidirectional where-to-look signals to shape perception during training, enhancing visual reliance and reducing text-only shortcuts.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models often rely on intermediate visual cues that overlook fine-grained visual evidence, generalize poorly across domains, and incur high inference-time costs. There's a need for better mechanisms that enforce visual reliance and prevent text-only shortcuts.

Method: BiPS transforms question-conditioned masked views into bidirectional where-to-look signals. It uses two constraints: 1) KL-consistency between original image and evidence-preserving view (keeping only question-relevant regions) for coarse but complete coverage, and 2) KL-separation between original and evidence-ablated view (masking critical pixels) to discourage text-only shortcuts and enforce fine-grained visual reliance.

Result: Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.

Conclusion: BiPS effectively shapes visual perception during training, improving model performance and generalization while reducing reliance on text-only shortcuts through bidirectional perceptual constraints.

Abstract: Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [86] [Learning to Reconfigure: Using Device Status to Select the Right Constrained Coding Scheme](https://arxiv.org/abs/2512.21396)
*Doğukan Özbayrak,Ahmed Hareedy*

Main category: cs.IT

TL;DR: Proposes offline and online learning methods to optimize reconfiguration of LOCO codes in TDMR systems based on device status, using polynomial models and linear programming to maximize capacity/minimize complexity.


<details>
  <summary>Details</summary>
Motivation: Modern storage systems need different protection levels at different device ages, but current industry practice uses predetermined time stamps that neglect actual device status. There's a need for intelligent reconfiguration methods based on device condition.

Method: Uses LOCO codes which are naturally reconfigurable. Proposes offline learning (using all training data) and online learning (using data at specific intervals) to fit polynomial equations modeling bit error rate vs. TD density. Designs optimization problem reduced to linear programming to make optimal reconfiguration decisions.

Result: The solution is proven to be globally optimal based on problem characteristics. Experimental results demonstrate effectiveness in TDMR systems for maximizing storage capacity and/or minimizing decoding complexity.

Conclusion: The proposed learning-based reconfiguration approach outperforms industry's predetermined time stamp method by adapting to actual device status, extending device lifetime through intelligent switching between coding schemes.

Abstract: In the age of data revolution, a modern storage~or transmission system typically requires different levels of protection. For example, the coding technique used to fortify data in a modern storage system when the device is fresh cannot be the same as that used when the device ages. Therefore, providing reconfigurable coding schemes and devising an effective way to perform this reconfiguration are key to extending the device lifetime. We focus on constrained coding schemes for the emerging two-dimensional magnetic recording (TDMR) technology. Recently, we have designed efficient lexicographically-ordered constrained (LOCO) coding schemes for various stages of the TDMR device lifetime, focusing on the elimination of isolation patterns, and demonstrated remarkable gains by using them. LOCO codes are naturally reconfigurable, and we exploit this feature in our work. Reconfiguration based on predetermined time stamps, which is what the industry adopts, neglects the actual device status. Instead, we propose offline and online learning methods to perform this task based on the device status. In offline learning, training data is assumed to be available throughout the time span of interest, while in online learning, we only use training data at specific time intervals to make consequential decisions. We fit the training data to polynomial equations that give the bit error rate in terms of TD density, then design an optimization problem in order to reach the optimal reconfiguration decisions to switch from a coding scheme to another. The objective is to maximize the storage capacity and/or minimize the decoding complexity. The problem reduces to a linear programming problem. We show that our solution is the global optimal based on problem characteristics, and we offer various experimental results that demonstrate the effectiveness of our approach in TDMR systems.

</details>


### [87] [Near-Field Communication with Massive Movable Antennas: An Electrostatic Equilibrium Perspective](https://arxiv.org/abs/2512.21660)
*Shicong Liu,Xianghao Yu,Shenghui Song,Khaled B. Letaief*

Main category: cs.IT

TL;DR: Proposes an ODE-based antenna placement optimization for near-field massive MIMO systems that transforms the problem into an electrostatic equilibrium problem solved via eigenvalue decomposition.


<details>
  <summary>Details</summary>
Motivation: Existing antenna placement schemes have limited scalability and ignore near-field effects in large-scale systems, hindering effective utilization of spatial degrees of freedom in wireless channels.

Method: Reformulates antenna placement as a weighted Fekete problem in angular domain, reveals it's an electrostatic equilibrium problem, proposes ODE-based framework with polynomial solutions, and uses two-step eigenvalue decomposition to efficiently obtain optimal positions.

Result: The scheme efficiently harnesses spatial DoFs of near-field channels with significant spectral efficiency gains, maintains robustness against parameter mismatches, and asymptotic closed-form solution approaches theoretical optimum across practical scenarios.

Conclusion: Proposed approach provides scalable, efficient antenna placement optimization for near-field massive MIMO systems by transforming the problem into solvable electrostatic equilibrium and ODE frameworks with practical computational advantages.

Abstract: Recent advancements in large-scale position-reconfigurable antennas have opened up new dimensions to effectively utilize the spatial degrees of freedom (DoFs) of wireless channels. However, the deployment of existing antenna placement schemes is primarily hindered by their limited scalability and frequently overlooked near-field effects in large-scale antenna systems. In this paper, we propose a novel antenna placement approach tailored for near-field massive multiple-input multiple-output systems, which effectively exploits the spatial DoFs to enhance spectral efficiency. For that purpose, we first reformulate the antenna placement problem in the angular domain, resulting in a weighted Fekete problem. We then derive the optimality condition and reveal that the {optimal} antenna placement is in principle an electrostatic equilibrium problem. To further reduce the computational complexity of numerical optimization, we propose an ordinary differential equation (ODE)-based framework to efficiently solve the equilibrium problem. In particular, the optimal antenna positions are characterized by the roots of the polynomial solutions to specific ODEs in the normalized angular domain. By simply adopting a two-step eigenvalue decomposition (EVD) approach, the optimal antenna positions can be efficiently obtained. Furthermore, we perform an asymptotic analysis when the antenna size tends to infinity, which yields a closed-form solution. Simulation results demonstrate that the proposed scheme efficiently harnesses the spatial DoFs of near-field channels with prominent gains in spectral efficiency and maintains robustness against system parameter mismatches. In addition, the derived asymptotic closed-form {solution} closely approaches the theoretical optimum across a wide range of practical scenarios.

</details>


### [88] [Latency-Optimal Cache-aided Multicast Streaming via Forward-Backward Reinforcement Learning](https://arxiv.org/abs/2512.21954)
*Mohsen Amidzadeh*

Main category: cs.IT

TL;DR: A forward-backward Markov decision process (FB-MDP) framework is proposed for cache-enabled cellular networks using orthogonal multipoint multicast streaming, with a forward-backward multi-objective reinforcement learning (FB-MORL) algorithm to optimize latency, outage probability, and resource consumption.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of designing latency-optimal streaming policies in cache-enabled cellular networks where unsatisfied users remain interested in their content preferences, creating forward dynamics that need to be combined with backward latency dynamics for proper optimization.

Method: Developed a forward-backward Markov decision process (FB-MDP) that captures both forward user preference dynamics (when users remain unsatisfied) and backward latency dynamics. Used a forward-backward multi-objective reinforcement learning (FB-MORL) algorithm to optimize multiple performance metrics simultaneously.

Result: Simulation results demonstrate the effectiveness of the proposed FB-MORL algorithm in finding promising dynamic cache policies that balance latency optimization with other performance metrics like outage probability and resource consumption.

Conclusion: The FB-MDP framework successfully captures the complex temporal dynamics of cache-enabled cellular networks, and the FB-MORL algorithm provides an effective solution for multi-objective optimization of streaming policies in such dynamic environments.

Abstract: We consider a cellular network equipped with cache-enabled base-stations (BSs) leveraging an orthogonal multipoint multicast (OMPMC) streaming scheme. The network operates in a time-slotted fashion to serve content-requesting users by streaming cached files. The users being unsatisfied by the multicat streaming face a delivery outage, implying that they will remain interested in their preference at the next time-slot, which leads to a forward dynamics on the user preference. To design a latency-optimal streaming policy, the dynamics of latency is properly modeled and included in the learning procedure. We show that this dynamics surprisingly represents a backward dynamics. The combination of problem's forward and backward dynamics then develops a forward-backward Markov decision process (FB-MDP) that fully captures the network evolution across time. This FB-MDP necessitates usage of a forward-backward multi-objective reinforcement learning (FB-MORL) algorithm to optimize the expected latency as well as other performance metrics of interest including the overall outage probability and total resource consumption. Simulation results show the merit of proposed FB-MORL algorithm in finding a promising dynamic cache policy.

</details>


### [89] [On the Ergodic Capacity for SIM-Aided Holographic MIMO Communications](https://arxiv.org/abs/2512.22068)
*Anastasios Papazafeiropoulos,Ioannis Bartsiokas,Dimitra I. Kaklamani,Iakovos S. Venieris*

Main category: cs.IT

TL;DR: Novel closed-form lower bound for ergodic capacity of holographic MIMO systems with stacked intelligent metasurfaces under Rayleigh fading.


<details>
  <summary>Details</summary>
Motivation: To provide analytical insights into the capacity performance of HMIMO systems enhanced by SIMs, which currently lack closed-form expressions for ergodic capacity under practical finite-antenna scenarios.

Method: Derived a novel closed-form lower bound expression for ergodic capacity under Rayleigh fading conditions, valid for finite antenna and SIM element systems, and conducted comprehensive low-SNR analysis.

Result: The proposed lower bound exhibits tightness throughout the whole SNR range and provides meaningful observations on how key system parameters influence capacity performance.

Conclusion: The derived closed-form lower bound offers valuable analytical tools for understanding and optimizing HMIMO-SIM systems, with particular insights from low-SNR regime analysis.

Abstract: We derive a novel closed-form lower bound on the ergodic capacity of holographic multiple-input multiple-output (HMIMO) systems enhanced by stacked intelligent metasurfaces (SIMs) under Rayleigh fading conditions. The proposed expression is valid for systems with a finite number of antennas and SIM elements and exhibits tightness throughout the whole signal-to-noise ratio (SNR) range. Furthermore, we conduct a comprehensive low-SNR analysis, offering meaningful observations on how key system parameters influence the capacity performance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [90] [From Visual Perception to Deep Empathy: An Automated Assessment Framework for House-Tree-Person Drawings Using Multimodal LLMs and Multi-Agent Collaboration](https://arxiv.org/abs/2512.21360)
*Shuide Wen,Yu Sun,Beier Ku,Zhi Gao,Lijun Ma,Yang Yang,Can Jiao*

Main category: cs.AI

TL;DR: MLLM-based multi-agent system achieves expert-level performance in interpreting HTP drawings, offering standardized projective assessment with high ecological validity.


<details>
  <summary>Details</summary>
Motivation: To address long-standing challenges in the HTP drawing test including heterogeneous scoring standards, subjective examiner reliance, and lack of unified quantitative coding systems.

Method: Developed a multi-agent framework using Multimodal Large Language Models (MLLMs) that integrates social-psychological perspectives and destigmatizing narratives, decoupling feature recognition from psychological inference.

Result: Mean semantic similarity of 0.75 between MLLM and human expert interpretations (SD≈0.05), rising to 0.85 in structured expert datasets; system effectively corrected visual hallucinations and produced psychologically coherent reports.

Conclusion: MLLMs show potential as standardized tools for projective assessment, with the multi-agent framework offering a new paradigm for digital mental-health services by separating feature recognition from psychological inference.

Abstract: Background: The House-Tree-Person (HTP) drawing test, introduced by John Buck in 1948, remains a widely used projective technique in clinical psychology. However, it has long faced challenges such as heterogeneous scoring standards, reliance on examiners subjective experience, and a lack of a unified quantitative coding system.
  Results: Quantitative experiments showed that the mean semantic similarity between Multimodal Large Language Model (MLLM) interpretations and human expert interpretations was approximately 0.75 (standard deviation about 0.05). In structurally oriented expert data sets, this similarity rose to 0.85, indicating expert-level baseline comprehension. Qualitative analyses demonstrated that the multi-agent system, by integrating social-psychological perspectives and destigmatizing narratives, effectively corrected visual hallucinations and produced psychological reports with high ecological validity and internal coherence.
  Conclusions: The findings confirm the potential of multimodal large models as standardized tools for projective assessment. The proposed multi-agent framework, by dividing roles, decouples feature recognition from psychological inference and offers a new paradigm for digital mental-health services.
  Keywords: House-Tree-Person test; multimodal large language model; multi-agent collaboration; cosine similarity; computational psychology; artificial intelligence

</details>


### [91] [A Study of Solving Life-and-Death Problems in Go Using Relevance-Zone Based Solvers](https://arxiv.org/abs/2512.21365)
*Chung-Chin Shih,Ti-Rong Wu,Ting Han Wei,Yu-Shan Hsu,Hung Guei,I-Chen Wu*

Main category: cs.AI

TL;DR: Analysis of Life-and-Death Go problems using relevance-zone based solvers reveals critical areas, discovers rare patterns, and finds alternative solutions, but shows limitations in pattern valuation and strategic preferences compared to human players.


<details>
  <summary>Details</summary>
Motivation: To understand how state-of-the-art computer Go solvers with relevance-zone techniques perform on classic Life-and-Death problems, comparing their solutions to established human expert answers from Cho Chikun's authoritative book.

Method: Used Relevance-Zone Based Search (RZS) and relevance-zone pattern table techniques on seven L&D problems from Cho Chikun's "Life and Death Dictionary," analyzing the solvers' identified critical areas, discovered patterns, and solution variations.

Result: Solvers successfully identified relevance-zones highlighting critical areas, discovered rare patterns, and found different answers for two problems compared to the book's solutions. However, they misjudged values of rare patterns and prioritized direct survival over territory maximization unlike human players.

Conclusion: While relevance-zone based solvers show promise in analyzing L&D problems, they have limitations in pattern valuation and strategic decision-making compared to human experts. Future work should address these issues to better align with human Go playing strategies.

Abstract: This paper analyzes the behavior of solving Life-and-Death (L&D) problems in the game of Go using current state-of-the-art computer Go solvers with two techniques: the Relevance-Zone Based Search (RZS) and the relevance-zone pattern table. We examined the solutions derived by relevance-zone based solvers on seven L&D problems from the renowned book "Life and Death Dictionary" written by Cho Chikun, a Go grandmaster, and found several interesting results. First, for each problem, the solvers identify a relevance-zone that highlights the critical areas for solving. Second, the solvers discover a series of patterns, including some that are rare. Finally, the solvers even find different answers compared to the given solutions for two problems. We also identified two issues with the solver: (a) it misjudges values of rare patterns, and (b) it tends to prioritize living directly rather than maximizing territory, which differs from the behavior of human Go players. We suggest possible approaches to address these issues in future work. Our code and data are available at https://rlg.iis.sinica.edu.tw/papers/study-LD-RZ.

</details>


### [92] [Three-way conflict analysis based on alliance and conflict functions](https://arxiv.org/abs/2512.21419)
*Junfang Luo,Mengjun Hu,Guangming Lang,Xin Yang,Keyun Qin*

Main category: cs.AI

TL;DR: The paper proposes separating alliance and conflict functions in three-way conflict analysis to clarify semantics, addressing issues with standard aggregation methods that obscure meaningful differences between relationships.


<details>
  <summary>Details</summary>
Motivation: Current three-way conflict analysis uses single functions that combine opposite aspects (alliance/conflict), making aggregations ambiguous - for example, averaging +1 and -1 gives same result as two 0s, even though they represent very different attitudes.

Method: Separate alliance and conflict functions instead of using a single auxiliary function; use these separate functions to trisect agents, issues, and agent pairs; explore alliance sets and strategies concepts.

Result: Developed clearer semantic framework for conflict analysis; demonstrated applications in solving crucial questions in conflict analysis; provided real-world application to illustrate proposed models.

Conclusion: Separating alliance and conflict functions provides better semantic clarity in three-way conflict analysis, enabling more meaningful analysis of agent relationships and attitudes across multiple issues.

Abstract: Trisecting agents, issues, and agent pairs are essential topics of three-way conflict analysis. They have been commonly studied based on either a rating or an auxiliary function. A rating function defines the positive, negative, or neutral ratings of agents on issues. An auxiliary function defines the alliance, conflict, and neutrality relations between agents. These functions measure two opposite aspects in a single function, leading to challenges in interpreting their aggregations over a group of issues or agents. For example, when studying agent relations regarding a set of issues, a standard aggregation takes the average of an auxiliary function concerning single issues. Therefore, a pair of alliance +1 and conflict -1 relations will produce the same result as a pair of neutrality 0 relations, although the attitudes represented by the two pairs are very different. To clarify semantics, we separate the two opposite aspects in an auxiliary function into a pair of alliance and conflict functions. Accordingly, we trisect the agents, issues, and agent pairs and investigate their applications in solving a few crucial questions in conflict analysis. Particularly, we explore the concepts of alliance sets and strategies. A real-world application is given to illustrate the proposed models.

</details>


### [93] [Feasible strategies in three-way conflict analysis with three-valued ratings](https://arxiv.org/abs/2512.21420)
*Jing Liu,Mengjun Hu,Guangming Lang*

Main category: cs.AI

TL;DR: This paper proposes a new approach to conflict resolution in three-way conflict analysis by developing weighted consistency and non-consistency measures to identify feasible and optimal strategies, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing three-way conflict analysis focuses on understanding conflicts (trisecting agent pairs, agents, or issues) but lacks practical resolution methods. There's insufficient attention to formulating feasible strategies, which are essential for actual conflict resolution and mitigation.

Method: 1) Compute overall rating of agent cliques using positive/negative similarity degrees. 2) Propose weighted consistency and non-consistency measures considering both agent and issue weights. 3) Develop algorithms to identify feasible strategies, L-order feasible strategies, and optimal ones. 4) Apply models to NBA labor negotiations and Gansu Province development case studies with sensitivity and comparative analysis.

Result: The proposed conflict resolution models outperform conventional approaches by unifying weighted agent-issue evaluation with consistency/non-consistency measures. This enables systematic identification of both feasible strategies and optimal solutions, demonstrated through practical case studies.

Conclusion: The paper successfully addresses the gap in conflict resolution by developing a systematic approach that identifies feasible and optimal strategies through weighted consistency/non-consistency measures, providing practical tools for real-world conflict analysis and resolution.

Abstract: Most existing work on three-way conflict analysis has focused on trisecting agent pairs, agents, or issues, which contributes to understanding the nature of conflicts but falls short in addressing their resolution. Specifically, the formulation of feasible strategies, as an essential component of conflict resolution and mitigation, has received insufficient scholarly attention. Therefore, this paper aims to investigate feasible strategies from two perspectives of consistency and non-consistency. Particularly, we begin with computing the overall rating of a clique of agents based on positive and negative similarity degrees. Afterwards, considering the weights of both agents and issues, we propose weighted consistency and non-consistency measures, which are respectively used to identify the feasible strategies for a clique of agents. Algorithms are developed to identify feasible strategies, $L$-order feasible strategies, and the corresponding optimal ones. Finally, to demonstrate the practicality, effectiveness, and superiority of the proposed models, we apply them to two commonly used case studies on NBA labor negotiations and development plans for Gansu Province and conduct a sensitivity analysis on parameters and a comparative analysis with existing state-of-the-art conflict analysis approaches. The comparison results demonstrate that our conflict resolution models outperform the conventional approaches by unifying weighted agent-issue evaluation with consistency and non-consistency measures to enable the systematic identification of not only feasible strategies but also optimal solutions.

</details>


### [94] [Three-way decision with incomplete information based on similarity and satisfiability](https://arxiv.org/abs/2512.21421)
*Junfang Luo,Mengjun Hu,Keyun Qin*

Main category: cs.AI

TL;DR: This paper generalizes three-way decision from complete to incomplete information by extending both computational (equivalence relations) and conceptual (logic satisfiability) formulations with similarity-based and satisfiability-degree approaches.


<details>
  <summary>Details</summary>
Motivation: Three-way decision with rough sets is well-established for complete information, but real-world applications often involve incomplete information. The paper aims to extend both computational and conceptual formulations to handle incomplete data.

Method: For computational formulation: proposes similarity degree measure (generalizing equivalence relations) and uses alpha-similarity classes and approximability of objects. For conceptual formulation: proposes satisfiability degree measure (quantitative generalization of satisfiability) and uses alpha-meaning sets and confidence of formulas.

Result: The paper develops two approaches for each formulation: computational (alpha-similarity classes and approximability) and conceptual (alpha-meaning sets and confidence of formulas). While similarity classes are common, approximability and conceptual approaches offer new directions.

Conclusion: The paper successfully generalizes three-way decision to incomplete information through both computational and conceptual formulations, with approximability and conceptual approaches providing promising new research directions beyond traditional similarity-based methods.

Abstract: Three-way decision is widely applied with rough set theory to learn classification or decision rules. The approaches dealing with complete information are well established in the literature, including the two complementary computational and conceptual formulations. The computational formulation uses equivalence relations, and the conceptual formulation uses satisfiability of logic formulas. In this paper, based on a briefly review of these two formulations, we generalize both formulations into three-way decision with incomplete information that is more practical in real-world applications. For the computational formulation, we propose a new measure of similarity degree of objects as a generalization of equivalence relations. Based on it, we discuss two approaches to three-way decision using alpha-similarity classes and approximability of objects, respectively. For the conceptual formulation, we propose a measure of satisfiability degree of formulas as a quantitative generalization of satisfiability with complete information. Based on it, we study two approaches to three-way decision using alpha-meaning sets of formulas and confidence of formulas, respectively. While using similarity classes is a common method of analyzing incomplete information in the literature, the proposed concept of approximability and the two approaches in conceptual formulation point out new promising directions.

</details>


### [95] [LogicLens: Visual-Logical Co-Reasoning for Text-Centric Forgery Analysis](https://arxiv.org/abs/2512.21482)
*Fanwei Zeng,Changtao Miao,Jing Huang,Zhiya Tan,Shutao Gong,Xiaoming Yu,Yang Wang,Huazhe Tan,Weibin Yao,Jianshu Li*

Main category: cs.AI

TL;DR: LogicLens: A unified visual-textual co-reasoning framework for text-centric forgery analysis that jointly handles detection, grounding, and explanation through cross-cues-aware reasoning.


<details>
  <summary>Details</summary>
Motivation: Current methods for text-centric forgery analysis are limited to coarse-grained visual analysis, lack sophisticated reasoning capabilities, and treat detection, grounding, and explanation as separate tasks, missing opportunities for holistic performance improvement.

Method: Introduces LogicLens with Cross-Cues-aware Chain of Thought (CCT) mechanism for iterative visual-textual cross-validation, weighted multi-task reward function for GRPO optimization, and PR² pipeline (Perceiver, Reasoner, Reviewer) for generating high-quality annotations.

Result: LogicLens achieves superior performance: 41.4% improvement over specialized framework and 23.4% over GPT-4o on T-IC13 in zero-shot evaluation, and establishes significant leads on T-SROIE dataset across multiple metrics (mF1, CSS, macro-average F1).

Conclusion: LogicLens provides a unified framework for text-centric forgery analysis that enables sophisticated reasoning and joint task optimization, demonstrating state-of-the-art performance across multiple benchmarks while addressing limitations of current approaches.

Abstract: Sophisticated text-centric forgeries, fueled by rapid AIGC advancements, pose a significant threat to societal security and information authenticity. Current methods for text-centric forgery analysis are often limited to coarse-grained visual analysis and lack the capacity for sophisticated reasoning. Moreover, they typically treat detection, grounding, and explanation as discrete sub-tasks, overlooking their intrinsic relationships for holistic performance enhancement. To address these challenges, we introduce LogicLens, a unified framework for Visual-Textual Co-reasoning that reformulates these objectives into a joint task. The deep reasoning of LogicLens is powered by our novel Cross-Cues-aware Chain of Thought (CCT) mechanism, which iteratively cross-validates visual cues against textual logic. To ensure robust alignment across all tasks, we further propose a weighted multi-task reward function for GRPO-based optimization. Complementing this framework, we first designed the PR$^2$ (Perceiver, Reasoner, Reviewer) pipeline, a hierarchical and iterative multi-agent system that generates high-quality, cognitively-aligned annotations. Then, we constructed RealText, a diverse dataset comprising 5,397 images with fine-grained annotations, including textual explanations, pixel-level segmentation, and authenticity labels for model training. Extensive experiments demonstrate the superiority of LogicLens across multiple benchmarks. In a zero-shot evaluation on T-IC13, it surpasses the specialized framework by 41.4% and GPT-4o by 23.4% in macro-average F1 score. Moreover, on the challenging dense-text T-SROIE dataset, it establishes a significant lead over other MLLM-based methods in mF1, CSS, and the macro-average F1. Our dataset, model, and code will be made publicly available.

</details>


### [96] [Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model](https://arxiv.org/abs/2512.21540)
*Yanhao Li,Lu Ma,Jiaran Zhang,Lexiang Tang,Wentao Zhang,Guibo Luo*

Main category: cs.AI

TL;DR: Leash is a reinforcement learning framework that uses adaptive length penalties to make LLM reasoning more concise while maintaining accuracy, reducing reasoning length by 60% across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Existing fixed length penalties are hard to tune and fail to adapt to LLMs' evolving reasoning abilities, leading to suboptimal trade-offs between accuracy and conciseness in reasoning tasks.

Method: Formulates length control as constrained optimization problem, employs Lagrangian primal-dual method to dynamically adjust penalty coefficient - intensifying penalty when generations exceed target length, relaxing when shorter.

Result: Reduces average reasoning length by 60% across diverse tasks (in-distribution mathematical reasoning and out-of-distribution domains like coding and instruction following) while maintaining competitive performance.

Conclusion: Presents a practical and effective paradigm for developing controllable and efficient LLMs that balance reasoning capabilities with computational budgets through adaptive length control.

Abstract: Existing approaches typically rely on fixed length penalties, but such penalties are hard to tune and fail to adapt to the evolving reasoning abilities of LLMs, leading to suboptimal trade-offs between accuracy and conciseness. To address this challenge, we propose Leash (adaptive LEngth penAlty and reward SHaping), a reinforcement learning framework for efficient reasoning in LLMs. We formulate length control as a constrained optimization problem and employ a Lagrangian primal-dual method to dynamically adjust the penalty coefficient. When generations exceed the target length, the penalty is intensified; when they are shorter, it is relaxed. This adaptive mechanism guides models toward producing concise reasoning without sacrificing task performance. Experiments on Deepseek-R1-Distill-Qwen-1.5B and Qwen3-4B-Thinking-2507 show that Leash reduces the average reasoning length by 60% across diverse tasks - including in-distribution mathematical reasoning and out-of-distribution domains such as coding and instruction following - while maintaining competitive performance. Our work thus presents a practical and effective paradigm for developing controllable and efficient LLMs that balance reasoning capabilities with computational budgets.

</details>


### [97] [NEMO-4-PAYPAL: Leveraging NVIDIA's Nemo Framework for empowering PayPal's Commerce Agent](https://arxiv.org/abs/2512.21578)
*Ali Sahami,Sudhanshu Garg,Andrew Wang,Chaitanya Kulkarni,Farhad Farahani,Sean Yun-Shiuan Chuang,Jian Wan,Srinivasan Manoharan,Uma Kona,Nitin Sharma,Linsey Pang,Prakhar Mehrotra,Jessica Clark,Mark Moyou*

Main category: cs.AI

TL;DR: PayPal developed a commerce agent using NVIDIA's NeMo Framework, fine-tuning a Nemotron SLM to optimize search and discovery, achieving significant latency and cost improvements while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: To revolutionize agentic commerce on PayPal's platform by optimizing agent performance, particularly addressing the retrieval component which accounts for over 50% of total agent response time.

Method: Used NVIDIA's NeMo Framework for LLM fine-tuning, specifically optimizing the Search and Discovery agent by replacing the base model with a fine-tuned Nemotron small language model. Conducted systematic hyperparameter sweeps across learning rates, optimizers (Adam, AdamW), cosine annealing schedules, and LoRA ranks using llama3.1-nemotron-nano-8B-v1 architecture.

Result: The fine-tuned Nemotron SLM effectively resolved key performance issues in the retrieval component while maintaining or enhancing overall system performance, achieving significant improvements in latency and cost.

Conclusion: Successfully demonstrated the first application of NVIDIA's NeMo Framework to commerce-specific agent optimization, creating a scalable framework for multi-agent system optimization in production e-commerce environments.

Abstract: We present the development and optimization of PayPal's Commerce Agent, powered by NEMO-4-PAYPAL, a multi-agent system designed to revolutionize agentic commerce on the PayPal platform. Through our strategic partnership with NVIDIA, we leveraged the NeMo Framework for LLM model fine-tuning to enhance agent performance. Specifically, we optimized the Search and Discovery agent by replacing our base model with a fine-tuned Nemotron small language model (SLM).
  We conducted comprehensive experiments using the llama3.1-nemotron-nano-8B-v1 architecture, training LoRA-based models through systematic hyperparameter sweeps across learning rates, optimizers (Adam, AdamW), cosine annealing schedules, and LoRA ranks. Our contributions include: (1) the first application of NVIDIA's NeMo Framework to commerce-specific agent optimization, (2) LLM powered fine-tuning strategy for retrieval-focused commerce tasks, (3) demonstration of significant improvements in latency and cost while maintaining agent quality, and (4) a scalable framework for multi-agent system optimization in production e-commerce environments. Our results demonstrate that the fine-tuned Nemotron SLM effectively resolves the key performance issue in the retrieval component, which represents over 50\% of total agent response time, while maintaining or enhancing overall system performance.

</details>


### [98] [A Medical Multimodal Diagnostic Framework Integrating Vision-Language Models and Logic Tree Reasoning](https://arxiv.org/abs/2512.21583)
*Zelin Zang,Wenyi Gu,Siqi Ma,Dan Yang,Yue Shen,Zhu Zhang,Guohui Fan,Wing-Kuen Ling,Fuji Yang*

Main category: cs.AI

TL;DR: LLaVA-based diagnostic framework combines vision-language alignment with logic-regularized reasoning to improve reliability and interpretability in medical multimodal AI.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal medical models often produce hallucinations or inconsistent reasoning chains, limiting clinical trust despite the growth of LLMs and VLMs in medicine.

Method: Built upon LLaVA, includes input encoder for text/images, projection module for cross-modal alignment, reasoning controller for task decomposition, and logic tree generator for assembling verifiable conclusions.

Result: Improves diagnostic accuracy and yields more interpretable reasoning traces on multimodal tasks (MedXpertQA benchmarks), while remaining competitive on text-only settings.

Conclusion: The framework represents a promising step toward trustworthy multimodal medical AI by addressing reliability and interpretability issues.

Abstract: With the rapid growth of large language models (LLMs) and vision-language models (VLMs) in medicine, simply integrating clinical text and medical imaging does not guarantee reliable reasoning. Existing multimodal models often produce hallucinations or inconsistent chains of thought, limiting clinical trust. We propose a diagnostic framework built upon LLaVA that combines vision-language alignment with logic-regularized reasoning. The system includes an input encoder for text and images, a projection module for cross-modal alignment, a reasoning controller that decomposes diagnostic tasks into steps, and a logic tree generator that assembles stepwise premises into verifiable conclusions. Evaluations on MedXpertQA and other benchmarks show that our method improves diagnostic accuracy and yields more interpretable reasoning traces on multimodal tasks, while remaining competitive on text-only settings. These results suggest a promising step toward trustworthy multimodal medical AI.

</details>


### [99] [AMS-IO-Bench and AMS-IO-Agent: Benchmarking and Structured Reasoning for Analog and Mixed-Signal Integrated Circuit Input/Output Design](https://arxiv.org/abs/2512.21613)
*Zhishuai Zhang,Xintian Li,Shilong Liu,Aodong Zhang,Lu Jie,Nan Sun*

Main category: cs.AI

TL;DR: AMS-IO-Agent is an LLM-based agent for automated I/O subsystem generation in analog/mixed-signal ICs, achieving 70% DRC+LVS pass rate and reducing design time from hours to minutes.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between natural language design intent and industrial-level AMS IC design deliverables, automating the traditionally time-consuming I/O subsystem generation process.

Method: Framework with structured domain knowledge base for reusable constraints, and design intent structuring that converts ambiguous user intent into verifiable logic steps using JSON/Python as intermediate formats.

Result: Over 70% DRC+LVS pass rate on AMS-IO-Bench benchmark, reduces design turnaround from hours to minutes, and successfully fabricated/validated in 28nm CMOS tape-out.

Conclusion: First reported human-agent collaborative AMS IC design where LLM-based agent completes nontrivial subtask with outputs directly used in silicon, demonstrating practical effectiveness in real design flows.

Abstract: In this paper, we propose AMS-IO-Agent, a domain-specialized LLM-based agent for structure-aware input/output (I/O) subsystem generation in analog and mixed-signal (AMS) integrated circuits (ICs). The central contribution of this work is a framework that connects natural language design intent with industrial-level AMS IC design deliverables. AMS-IO-Agent integrates two key capabilities: (1) a structured domain knowledge base that captures reusable constraints and design conventions; (2) design intent structuring, which converts ambiguous user intent into verifiable logic steps using JSON and Python as intermediate formats. We further introduce AMS-IO-Bench, a benchmark for wirebond-packaged AMS I/O ring automation. On this benchmark, AMS-IO-Agent achieves over 70\% DRC+LVS pass rate and reduces design turnaround time from hours to minutes, outperforming the baseline LLM. Furthermore, an agent-generated I/O ring was fabricated and validated in a 28 nm CMOS tape-out, demonstrating the practical effectiveness of the approach in real AMS IC design flows. To our knowledge, this is the first reported human-agent collaborative AMS IC design in which an LLM-based agent completes a nontrivial subtask with outputs directly used in silicon.

</details>


### [100] [Democratizing Drug Discovery with an Orchestrated, Knowledge-Driven Multi-Agent Team for User-Guided Therapeutic Design](https://arxiv.org/abs/2512.21623)
*Takahide Suzuki,Kazuki Nakanishi,Takashi Fujiwara,Hideyuki Shimizu*

Main category: cs.AI

TL;DR: OrchestRA is an AI-powered multi-agent platform that autonomously executes therapeutic discovery by integrating biology, chemistry, and pharmacology into a unified feedback loop, transforming drug discovery from stochastic search to programmable engineering.


<details>
  <summary>Details</summary>
Motivation: Current therapeutic discovery faces challenges from domain fragmentation and the gap between computational design and physiological validation. Existing generative AI models often serve as passive assistants rather than autonomous executors, limiting their effectiveness in drug discovery.

Method: OrchestRA uses a human-in-the-loop multi-agent platform with three specialized agents: 1) Biologist Agent that reasons over a massive knowledge graph (>10M associations) to identify targets, 2) Chemist Agent that autonomously detects structural pockets for de novo design or drug repositioning, and 3) Pharmacologist Agent that evaluates candidates via PBPK simulations. An Orchestrator coordinates these agents to create a dynamic feedback loop.

Result: The platform establishes a dynamic feedback loop where pharmacokinetic and toxicity profiles directly trigger structural reoptimization, enabling iterative optimization of therapeutic candidates through autonomous execution and reasoning.

Conclusion: OrchestRA democratizes therapeutic design by seamlessly integrating autonomous execution with human guidance, transforming drug discovery from a stochastic search to a programmable evidence-based engineering discipline.

Abstract: Therapeutic discovery remains a formidable challenge, impeded by the fragmentation of specialized domains and the execution gap between computational design and physiological validation. Although generative AI offers promise, current models often function as passive assistants rather than as autonomous executors. Here, we introduce OrchestRA, a human-in-the-loop multi-agent platform that unifies biology, chemistry, and pharmacology into an autonomous discovery engine. Unlike static code generators, our agents actively execute simulations and reason the results to drive iterative optimization. Governed by an Orchestrator, a Biologist Agent leverages deep reasoning over a massive knowledge graph (>10 million associations) to pinpoint high-confidence targets; a Chemist Agent autonomously detects structural pockets for de novo design or drug repositioning; and a Pharmacologist Agent evaluates candidates via rigorous physiologically based pharmacokinetic (PBPK) simulations. This architecture establishes a dynamic feedback loop where pharmacokinetic and toxicity profiles directly trigger structural reoptimization. By seamlessly integrating autonomous execution with human guidance, OrchestRA democratizes therapeutic design, transforming drug discovery from a stochastic search to a programmable evidence-based engineering discipline.

</details>


### [101] [Multiple-play Stochastic Bandits with Prioritized Arm Capacity Sharing](https://arxiv.org/abs/2512.21626)
*Hong Xie,Haoran Gu,Yanying Huang,Tao Tan,Defu Lian*

Main category: cs.AI

TL;DR: A variant of multiple-play stochastic bandits for resource allocation in LLM/edge intelligence applications, featuring prioritized resource sharing with capacity constraints and priority weights.


<details>
  <summary>Details</summary>
Motivation: Address resource allocation problems in emerging applications like LLM serving and edge intelligence, where resources have capacity constraints and requests have priority weights, requiring prioritized sharing mechanisms.

Method: Propose MSB-PRS-OffOpt algorithm to find optimal play allocation policy with O(MK³) complexity, then design approximate UCB-based algorithm using it as subroutine for online learning.

Result: Prove instance independent regret lower bound Ω(α₁σ√(KMT)) and instance dependent lower bound Ω(α₁σ²(M/Δ)lnT). Achieve matching upper bounds up to factors of √(KlnKT) and α₁K² respectively.

Conclusion: Successfully addresses technical challenges in optimizing/learning under nonlinear combinatorial utility from prioritized resource sharing, providing theoretical guarantees for practical resource allocation problems.

Abstract: This paper proposes a variant of multiple-play stochastic bandits tailored to resource allocation problems arising from LLM applications, edge intelligence, etc. The model is composed of $M$ arms and $K$ plays. Each arm has a stochastic number of capacities, and each unit of capacity is associated with a reward function. Each play is associated with a priority weight. When multiple plays compete for the arm capacity, the arm capacity is allocated in a larger priority weight first manner. Instance independent and instance dependent regret lower bounds of $Ω( α_1 σ\sqrt{KM T} )$ and $Ω(α_1 σ^2 \frac{M}Δ \ln T)$ are proved, where $α_1$ is the largest priority weight and $σ$ characterizes the reward tail. When model parameters are given, we design an algorithm named \texttt{MSB-PRS-OffOpt} to locate the optimal play allocation policy with a computational complexity of $O(MK^3)$. Utilizing \texttt{MSB-PRS-OffOpt} as a subroutine, an approximate upper confidence bound (UCB) based algorithm is designed, which has instance independent and instance dependent regret upper bounds matching the corresponding lower bound up to factors of $ \sqrt{K \ln KT }$ and $α_1 K^2$ respectively. To this end, we address nontrivial technical challenges arising from optimizing and learning under a special nonlinear combinatorial utility function induced by the prioritized resource sharing mechanism.

</details>


### [102] [Towards Responsible and Explainable AI Agents with Consensus-Driven Reasoning](https://arxiv.org/abs/2512.21699)
*Eranga Bandara,Tharaka Hewa,Ross Gore,Sachin Shetty,Ravi Mukkamala,Peter Foytik,Abdul Rahman,Safdar H. Bouk,Xueping Liang,Amin Hass,Sachini Rajapakse,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.AI

TL;DR: A Responsible and Explainable AI Agent Architecture using multi-model consensus and reasoning-layer governance to address challenges in autonomous agentic AI systems.


<details>
  <summary>Details</summary>
Motivation: Agentic AI enables powerful autonomous capabilities but introduces critical challenges in explainability, accountability, robustness, and governance, especially when agent outputs influence real-world decisions. Existing implementations focus on functionality and scalability but lack mechanisms for understanding decision rationale or enforcing responsibility.

Method: Proposes a Responsible and Explainable AI Agent Architecture based on multi-model consensus and reasoning-layer governance. Uses a consortium of heterogeneous LLM and VLM agents to independently generate candidate outputs from shared input context, exposing uncertainty and alternative interpretations. A dedicated reasoning agent performs structured consolidation across outputs, enforcing safety/policy constraints, mitigating hallucinations/bias, and producing auditable decisions.

Result: Evaluated across multiple real-world agentic AI workflows, demonstrating that consensus-driven reasoning improves robustness, transparency, and operational trust across diverse application domains.

Conclusion: Provides practical guidance for designing agentic AI systems that are both autonomous/scalable and responsible/explainable by construction, addressing critical governance challenges in production-grade agentic workflows.

Abstract: Agentic AI represents a major shift in how autonomous systems reason, plan, and execute multi-step tasks through the coordination of Large Language Models (LLMs), Vision Language Models (VLMs), tools, and external services. While these systems enable powerful new capabilities, increasing autonomy introduces critical challenges related to explainability, accountability, robustness, and governance, especially when agent outputs influence downstream actions or decisions. Existing agentic AI implementations often emphasize functionality and scalability, yet provide limited mechanisms for understanding decision rationale or enforcing responsibility across agent interactions. This paper presents a Responsible(RAI) and Explainable(XAI) AI Agent Architecture for production-grade agentic workflows based on multi-model consensus and reasoning-layer governance. In the proposed design, a consortium of heterogeneous LLM and VLM agents independently generates candidate outputs from a shared input context, explicitly exposing uncertainty, disagreement, and alternative interpretations. A dedicated reasoning agent then performs structured consolidation across these outputs, enforcing safety and policy constraints, mitigating hallucinations and bias, and producing auditable, evidence-backed decisions. Explainability is achieved through explicit cross-model comparison and preserved intermediate outputs, while responsibility is enforced through centralized reasoning-layer control and agent-level constraints. We evaluate the architecture across multiple real-world agentic AI workflows, demonstrating that consensus-driven reasoning improves robustness, transparency, and operational trust across diverse application domains. This work provides practical guidance for designing agentic AI systems that are autonomous and scalable, yet responsible and explainable by construction.

</details>


### [103] [Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets](https://arxiv.org/abs/2512.21775)
*Matyas Bohacek,Ignacio Vilanova Echavarri*

Main category: cs.AI

TL;DR: The paper introduces a Compliance Rating Scheme (CRS) framework and open-source library to evaluate dataset compliance with transparency, accountability, and security principles, addressing ethical gaps in GAI dataset creation.


<details>
  <summary>Details</summary>
Motivation: GAI datasets are often built using unrestricted, opaque data collection practices, with ethical/legal considerations neglected. As datasets are shared and reproduced online, information about their origin, legitimacy, and safety gets lost.

Method: Introduces Compliance Rating Scheme (CRS) framework to evaluate dataset compliance with transparency, accountability, and security principles. Releases open-source Python library built around data provenance technology for seamless integration into existing pipelines.

Result: Developed a framework and library that is both reactive (evaluating existing datasets' CRS) and proactive (informing responsible scraping and construction of new datasets).

Conclusion: The CRS framework and accompanying library address critical gaps in dataset compliance evaluation, promoting more ethical and transparent GAI dataset development practices.

Abstract: Generative Artificial Intelligence (GAI) has experienced exponential growth in recent years, partly facilitated by the abundance of large-scale open-source datasets. These datasets are often built using unrestricted and opaque data collection practices. While most literature focuses on the development and applications of GAI models, the ethical and legal considerations surrounding the creation of these datasets are often neglected. In addition, as datasets are shared, edited, and further reproduced online, information about their origin, legitimacy, and safety often gets lost. To address this gap, we introduce the Compliance Rating Scheme (CRS), a framework designed to evaluate dataset compliance with critical transparency, accountability, and security principles. We also release an open-source Python library built around data provenance technology to implement this framework, allowing for seamless integration into existing dataset-processing and AI training pipelines. The library is simultaneously reactive and proactive, as in addition to evaluating the CRS of existing datasets, it equally informs responsible scraping and construction of new datasets.

</details>


### [104] [Accelerating Scientific Discovery with Autonomous Goal-evolving Agents](https://arxiv.org/abs/2512.21782)
*Yuanqi Du,Botao Yu,Tianyu Liu,Tony Shen,Junwu Chen,Jan G. Rittig,Kunyang Sun,Yikun Zhang,Zhangde Song,Bo Zhou,Cassandra Masschelein,Yingze Wang,Haorui Wang,Haojun Jia,Chao Zhang,Hongyu Zhao,Martin Ester,Teresa Head-Gordon,Carla P. Gomes,Huan Sun,Chenru Duan,Philippe Schwaller,Wengong Jin*

Main category: cs.AI

TL;DR: SAGA introduces a bi-level autonomous agent that automatically designs and evolves objective functions for scientific discovery, rather than relying on fixed human-specified objectives.


<details>
  <summary>Details</summary>
Motivation: Current scientific discovery agents rely on imperfect human-specified objective functions as proxies for complex scientific goals. There's a need to automate objective function design to better address grand scientific challenges.

Method: SAGA uses a bi-level architecture: outer loop LLM agents analyze optimization outcomes, propose new objectives, and convert them to computable scoring functions; inner loop performs solution optimization under current objectives.

Result: Demonstrated across diverse applications including antibiotic design, inorganic materials design, functional DNA sequence design, and chemical process design, showing substantial improvements in scientific discovery effectiveness.

Conclusion: Automating objective formulation is a central requirement for scientific discovery agents, and SAGA's bi-level design enables systematic exploration of objective spaces and trade-offs, significantly enhancing discovery capabilities.

Abstract: There has been unprecedented interest in developing agents that expand the boundary of scientific discovery, primarily by optimizing quantitative objective functions specified by scientists. However, for grand challenges in science , these objectives are only imperfect proxies. We argue that automating objective function design is a central, yet unmet requirement for scientific discovery agents. In this work, we introduce the Scientific Autonomous Goal-evolving Agent (SAGA) to amend this challenge. SAGA employs a bi-level architecture in which an outer loop of LLM agents analyzes optimization outcomes, proposes new objectives, and converts them into computable scoring functions, while an inner loop performs solution optimization under the current objectives. This bi-level design enables systematic exploration of the space of objectives and their trade-offs, rather than treating them as fixed inputs. We demonstrate the framework through a broad spectrum of applications, including antibiotic design, inorganic materials design, functional DNA sequence design, and chemical process design, showing that automating objective formulation can substantially improve the effectiveness of scientific discovery agents.

</details>


### [105] [SpatialBench: Can Agents Analyze Real-World Spatial Biology Data?](https://arxiv.org/abs/2512.21907)
*Kenny Workman,Zhen Yang,Harihara Muralidharan,Hannah Le*

Main category: cs.AI

TL;DR: SpatialBench is a benchmark of 146 verifiable problems from real spatial transcriptomics workflows to evaluate AI agents' ability to extract biological insights from messy spatial datasets.


<details>
  <summary>Details</summary>
Motivation: Spatial transcriptomics assays are increasing in scale and complexity, creating computational bottlenecks. While AI agents have improved at software engineering and general data analysis, it's unclear if they can extract biological insights from messy, real-world spatial datasets.

Method: Created SpatialBench benchmark with 146 verifiable problems derived from practical spatial analysis workflows across five spatial technologies and seven task categories. Each problem provides experimental data snapshots and deterministic graders to evaluate recovery of key biological results.

Result: Base model accuracy remains low (20-38% across model families) with strong model-task and model-platform interactions. Harness design (tools, prompts, control flow, execution environment) has large empirical effect on performance.

Conclusion: SpatialBench serves as both a measurement tool and diagnostic lens for developing agents that can interact with real spatial datasets faithfully, transparently, and reproducibly. Tools, prompts, and execution environment should be evaluated as first-class objects.

Abstract: Spatial transcriptomics assays are rapidly increasing in scale and complexity, making computational analysis a major bottleneck in biological discovery. Although frontier AI agents have improved dramatically at software engineering and general data analysis, it remains unclear whether they can extract biological insight from messy, real-world spatial datasets. We introduce SpatialBench, a benchmark of 146 verifiable problems derived from practical spatial analysis workflows spanning five spatial technologies and seven task categories. Each problem provides a snapshot of experimental data immediately prior to an analysis step and a deterministic grader that evaluates recovery of a key biological result. Benchmark data on frontier models shows that base model accuracy remains low (20-38% across model families), with strong model-task and model-platform interactions. Harness design has a large empirical effect on performance, indicating that tools, prompts, control flow, and execution environment should be evaluated and improved as first-class objects. SpatialBench serves both as a measurement tool and a diagnostic lens for developing agents that can interact with real spatial datasets faithfully, transparently, and reproducibly.

</details>


### [106] [Pruning as a Game: Equilibrium-Driven Sparsification of Neural Networks](https://arxiv.org/abs/2512.22106)
*Zubair Shah,Noaman Khan*

Main category: cs.AI

TL;DR: Pruning as equilibrium outcome of strategic interaction among model components, where sparsity emerges naturally when participation becomes dominated strategy.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods treat sparsity as externally imposed constraint using heuristic importance scores or regularization. Need more principled, theory-grounded approach.

Method: Model parameter groups (weights, neurons, filters) as players in continuous non-cooperative game. Each player selects participation level balancing contribution vs redundancy/competition. Derive equilibrium-driven pruning algorithm updating network parameters and participation variables jointly without explicit importance scores.

Result: Shows dominated players collapse to zero participation at equilibrium under mild conditions. Experiments on standard benchmarks demonstrate competitive sparsity-accuracy trade-offs.

Conclusion: Provides principled formulation of pruning as equilibrium phenomenon, offering interpretable, theory-grounded alternative to existing heuristic pruning methods.

Abstract: Neural network pruning is widely used to reduce model size and computational cost. Yet, most existing methods treat sparsity as an externally imposed constraint, enforced through heuristic importance scores or training-time regularization. In this work, we propose a fundamentally different perspective: pruning as an equilibrium outcome of strategic interaction among model components. We model parameter groups such as weights, neurons, or filters as players in a continuous non-cooperative game, where each player selects its level of participation in the network to balance contribution against redundancy and competition. Within this formulation, sparsity emerges naturally when continued participation becomes a dominated strategy at equilibrium. We analyze the resulting game and show that dominated players collapse to zero participation under mild conditions, providing a principled explanation for pruning behavior. Building on this insight, we derive a simple equilibrium-driven pruning algorithm that jointly updates network parameters and participation variables without relying on explicit importance scores. This work focuses on establishing a principled formulation and empirical validation of pruning as an equilibrium phenomenon, rather than exhaustive architectural or large-scale benchmarking. Experiments on standard benchmarks demonstrate that the proposed approach achieves competitive sparsity-accuracy trade-offs while offering an interpretable, theory-grounded alternative to existing pruning methods.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [107] [Near-field Target Localization: Effect of Hardware Impairments](https://arxiv.org/abs/2512.21480)
*Jiapeng Li,Changsheng You,Chao Zhou,Yong Zeng,Zhiyong Feng*

Main category: eess.SP

TL;DR: Proposes a three-phase HI-aware near-field localization method for XL-arrays that detects faulty antennas, performs phase calibration, and accurately estimates target positions, outperforming existing methods especially for short-range targets.


<details>
  <summary>Details</summary>
Motivation: Existing near-field localization methods assume ideal hardware and suffer from two practical limitations: 1) XL-arrays face hardware impairments causing unknown phase/amplitude errors, and 2) BCD-based methods have high localization errors for targets close to XL-arrays.

Method: Three-phase approach: 1) Detect faulty antennas using compressed sensing with coarse target localization, 2) Design phase calibration method to correct errors from detected faulty antennas, 3) Devise efficient near-field localization method using full XL-array with phase calibration. Also uses MCRB to quantify HI performance loss.

Result: Numerical results show the proposed method significantly reduces localization errors compared to benchmark schemes, especially for cases with short target range and/or high fault probability.

Conclusion: The proposed HI-aware near-field localization method effectively addresses hardware impairment challenges in XL-arrays, providing accurate target localization even for short-range scenarios through systematic faulty antenna detection and phase calibration.

Abstract: The prior works on near-field target localization have mostly assumed ideal hardware models and thus suffer from two limitations in practice. First, extremely large-scale arrays (XL-arrays) usually face a variety of hardware impairments (HIs) that may introduce unknown phase and/or amplitude errors. Second, the existing block coordinate descent (BCD)-based methods for joint estimation of the HI indicator, channel gain, angle, and range may induce considerable target localization error when the target is very close to the XL-array. To address these issues, we propose in this paper a new three-phase HI-aware near-field localization method, by efficiently detecting faulty antennas and estimating the positions of targets. Specifically, we first determine faulty antennas by using compressed sensing (CS) methods and improve detection accuracy based on coarse target localization. Then, a dedicated phase calibration method is designed to correct phase errors induced by detected faulty antennas. Subsequently, an efficient near-field localization method is devised to accurately estimate the positions of targets based on the full XL-array with phase calibration. Additionally, we resort to the misspecified Cramer-Rao bound (MCRB) to quantify the performance loss caused by HIs. Last, numerical results demonstrate that our proposed method significantly reduces the localization errors as compared to various benchmark schemes, especially for the case with a short target range and/or a high fault probability.

</details>


### [108] [When the Base Station Flies: Rethinking Security for UAV-Based 6G Networks](https://arxiv.org/abs/2512.21574)
*Ammar El Falou*

Main category: eess.SP

TL;DR: This paper analyzes security vulnerabilities in UAV-based base stations for 6G non-terrestrial networks and proposes mitigation principles.


<details>
  <summary>Details</summary>
Motivation: UAVs are crucial for 6G global coverage but introduce unique security challenges due to their mobile, wireless, energy-constrained nature, making them vulnerable to various attacks in emergency scenarios.

Method: The paper identifies attack surfaces of UAV-BS systems and outlines principles for threat mitigation, focusing on security vulnerabilities specific to UAV-based communication infrastructure.

Result: The analysis reveals multiple attack vectors including emergency alert spoofing, DoS attacks, jamming, interception, spoofing of wireless backhaul/GNSS, and malicious handover manipulation due to UAV mobility.

Conclusion: UAV-BS systems in 6G NTNs require specialized security measures to address their unique vulnerabilities, and the paper provides foundational principles for securing these critical emergency communication platforms.

Abstract: The integration of non-terrestrial networks (NTNs) into 6G systems is crucial for achieving seamless global coverage, particularly in underserved and disaster-prone regions. Among NTN platforms, unmanned aerial vehicles (UAVs) are especially promising due to their rapid deployability. However, this shift from fixed, wired base stations (BSs) to mobile, wireless, energy-constrained UAV-BSs introduces unique security challenges. Their central role in emergency communications makes them attractive candidates for emergency alert spoofing. Their limited computing and energy resources make them more vulnerable to denial-of-service (DoS) attacks, and their dependence on wireless backhaul links and GNSS navigation exposes them to jamming, interception, and spoofing. Furthermore, UAV mobility opens new attack vectors such as malicious handover manipulation. This paper identifies several attack surfaces of UAV-BS systems and outlines principles for mitigating their threats.

</details>


### [109] [Pinching Antenna-aided NOMA Systems with Internal Eavesdropping](https://arxiv.org/abs/2512.21601)
*Haolian Chi,Kunrui Cao,Zhou Su,Lei Zhou,Panagiotis D. Diamantoulakis,Yuanwei Liu,George K. Karagiannidis*

Main category: eess.SP

TL;DR: This paper investigates physical layer security in PA-aided NOMA systems with internal eavesdropping, proposing flexible power strategies to enhance security and deriving closed-form SOP expressions.


<details>
  <summary>Details</summary>
Motivation: The integration of pinching antennas (PA) with NOMA offers synergistic benefits, but SIC in power-domain NOMA introduces security risks when internal eavesdropping exists. The paper addresses the security vulnerabilities of PA-aided NOMA systems where nearby users act as internal eavesdroppers.

Method: The authors enhance NOMA system security by optimizing PA radiated power and deriving closed-form expressions for secrecy outage probability (SOP). They extend PA flexibility to include coupling length regulation and propose a flexible power strategy based on two conventional PA power models (equal and proportional power models).

Result: The results demonstrate the potential of PA-aided NOMA systems in mitigating internal eavesdropping risks and provide effective strategies for optimizing power allocation and user activity cell range.

Conclusion: The proposed flexible power strategy effectively enhances physical layer security in PA-aided NOMA systems against internal eavesdropping, offering practical solutions for secure next-generation communication systems.

Abstract: As a novel member of flexible antennas, the pinching antenna (PA) is realized by integrating small dielectric particles on a waveguide, offering unique regulatory capabilities on constructing line-of-sight (LoS) links and enhancing transceiver channels, reducing path loss and signal blockage. Meanwhile, non-orthogonal multiple access (NOMA) has become a potential technology of next-generation communications due to its remarkable advantages in spectrum efficiency and user access capability. The integration of PA and NOMA enables synergistic leveraging of PA's channel regulation capability and NOMA's multi-user multiplexing advantage, forming a complementary technical framework to deliver high-performance communication solutions. However, the use of successive interference cancellation (SIC) introduces significant security risks to power-domain NOMA systems when internal eavesdropping is present. To this end, this paper investigates the physical layer security of a PA-aided NOMA system where a nearby user is considered as an internal eavesdropper. We enhance the security of the NOMA system through optimizing the radiated power of PAs and analyze the secrecy performance by deriving the closed-form expressions for the secrecy outage probability (SOP). Furthermore, we extend the characterization of PA flexibility beyond deployment and scale adjustment to include flexible regulation of PA coupling length. Based on two conventional PA power models, i.e., the equal power model and the proportional power model, we propose a flexible power strategy to achieve secure transmission. The results highlight the potential of the PA-aided NOMA system in mitigating internal eavesdropping risks, and provide an effective strategy for optimizing power allocation and cell range of user activity.

</details>


### [110] [Integrating Low-Altitude SAR Imaging into UAV Data Backhaul](https://arxiv.org/abs/2512.21937)
*Zhen Du,Fan Liu,Jie Yang,Yifeng Xiong,Yuanhao Cui,Weijie Yuan,Zenghui Zhang,Shi Jin*

Main category: eess.SP

TL;DR: UAV-based SAR imaging using OFDM communication waveforms with data symbols outperforms pilot-only sensing for low-altitude wireless networks.


<details>
  <summary>Details</summary>
Motivation: Current SAR systems use deterministic waveforms that conflict with ISAC paradigms and underutilize data symbols, similar to 5G NR's uplink pilot sensing with SRS. There's untapped potential in data-aided imaging for UAV-based low-altitude wireless networks.

Method: Developed a low-altitude SAR imaging framework using native OFDM communication waveforms with data symbols. Incorporated time-frequency domain filtering schemes within a range-Doppler imaging framework to mitigate quality degradation from random modulated data (QAM). Proposed NMSE of reference point target profile as imaging performance metric.

Result: Simulation results with 5G NR parameters show data-aided imaging substantially outperforms pilot-only sensing, validating the effectiveness of the proposed OFDM-SAR imaging approach.

Conclusion: Data-aided SAR imaging using OFDM communication waveforms is effective for low-altitude wireless networks, offering better performance than conventional pilot-only approaches and enabling better integrated sensing and communications.

Abstract: Synthetic aperture radar (SAR) deployed on unmanned aerial vehicles (UAVs) is expected to provide burgeoning imaging services for low-altitude wireless networks (LAWNs), thereby enabling large-scale environmental sensing and timely situational awareness. Conventional SAR systems typically leverages a deterministic radar waveform, while it conflicts with the integrated sensing and communications (ISAC) paradigm by discarding signaling randomness, in whole or in part. In fact, this approach reduces to the uplink pilot sensing in 5G New Radio (NR) with sounding reference signals (SRS), underutilizing data symbols. To explore the potential of data-aided imaging, we develop a low-altitude SAR imaging framework that sufficiently leverages data symbols carried by the native orthogonal frequency division multiplexing (OFDM) communication waveform. The randomness of modulated data in the temporal-frequency (TF) domain, introduced by non-constant modulus constellations such as quadrature amplitude modulation (QAM), may however severely degrade the imaging quality. To mitigate this effect, we incorporate several TF-domain filtering schemes within a rangeDoppler (RD) imaging framework and evaluate their impact. We further propose using the normalized mean square error (NMSE) of a reference point target's profile as an imaging performance metric. Simulation results with 5G NR parameters demonstrate that data-aided imaging substantially outperforms pilot-only counterpart, accordingly validating the effectiveness of the proposed OFDM-SAR imaging approach in LAWNs.

</details>


### [111] [A Light Weight Neural Network for Automatic Modulation Classification in OFDM Systems](https://arxiv.org/abs/2512.21941)
*Indiwara Nanayakkara,Dehan Jayawickrama,Dasuni Jayawardena,Vijitha R. Herath,Arjuna Madanayake*

Main category: eess.SP

TL;DR: Lightweight subcarrier-based AMC method for OFDM using LWNN for initial subcarrier classification and RNN for remaining subcarrier prediction.


<details>
  <summary>Details</summary>
Motivation: Existing blind AMC methods for OFDM lack accuracy, while deep learning approaches have high computational complexity. Need for lightweight yet accurate modulation classification.

Method: Two-stage approach: 1) LWNN classifies selected subcarriers first, 2) RNN uses LWNN output as embedded vector to predict modulation schemes for remaining subcarriers in OFDM frame.

Result: Proposed method achieves accurate modulation classification with reduced computational complexity compared to existing deep learning approaches.

Conclusion: Lightweight subcarrier-based approach provides effective AMC for OFDM systems, balancing accuracy and computational efficiency for future wireless systems.

Abstract: Automatic Modulation Classification (AMC) is a vital component in the development of intelligent and adaptive transceivers for future wireless communication systems. Existing statistically-based blind modulation classification methods for Orthogonal Frequency Division Multiplexing (OFDM) often fail to achieve the required accuracy and performance. Consequently, the modulation classification research community has shifted its focus toward deep learning techniques, which demonstrate promising performance, but come with increased computational complexity. In this paper, we propose a lightweight subcarrier-based modulation classification method for OFDM systems. In the proposed approach, a selected set of subcarriers in an OFDM frame is classified first, followed by the prediction of the modulation types for the remaining subcarriers based on the initial results. A Lightweight Neural Network (LWNN) is employed to identify the initially selected set of subcarriers, and its output is fed into a Recurrent Neural Network (RNN) as an embedded vector to predict the modulation schemes of the remaining subcarriers in the OFDM frame.

</details>


### [112] [Phase-Coherent D-MIMO ISAC: Multi-Target Estimation and Spectral Efficiency Trade-Offs](https://arxiv.org/abs/2512.21953)
*Venkatesh Tentu,Henk Wymeersch,Musa Furkan Keskin,Sauradeep Dey,Tommy Svensson*

Main category: eess.SP

TL;DR: Two-stage sensing framework for D-MIMO ISAC systems combining non-coherent and coherent ML estimation with adaptive AP mode-selection strategies to balance communication and sensing performance.


<details>
  <summary>Details</summary>
Motivation: To achieve high-accuracy multi-target estimation in distributed MIMO ISAC systems while balancing communication and sensing performance, addressing the trade-off between spectral efficiency and sensing precision.

Method: Proposed a two-stage sensing framework: 1) non-coherent ML estimation followed by 2) coherent ML estimation. Introduced adaptive AP mode-selection strategies: communication-centric (maximizes downlink SE) and sensing-centric (selects geometrically diverse receive APs to enhance sensing coverage).

Result: Simulation results confirm SE-sensing trade-off, showing that appropriate power allocation and larger array apertures alleviate performance degradation, achieving high SE with millimeter-level sensing precision. The AP-selection strategy reveals an optimal number of receive APs that maximizes sensing coverage without significantly sacrificing SE.

Conclusion: The proposed framework successfully balances communication and sensing performance in D-MIMO ISAC systems, demonstrating that careful AP selection and resource allocation can achieve both high spectral efficiency and millimeter-level sensing precision.

Abstract: We investigate distributed multiple-input multiple-output (D-MIMO) integrated sensing and communication (ISAC) systems, in which multiple phase-synchronized access points (APs) jointly serve user equipments (UEs) while cooperatively detecting and estimating multiple static targets. To achieve high-accuracy multi-target estimation, we propose a two-stage sensing framework combining non-coherent and coherent maximum-likelihood (ML) estimation. In parallel, adaptive AP mode-selection strategies are introduced to balance communication and sensing performance: a communication-centric scheme that maximizes downlink spectral efficiency (SE) and a sensing-centric scheme that selects geometrically diverse receive APs to enhance sensing coverage. Simulation results confirm the SE-sensing trade-off, where appropriate power allocation between communication and sensing and larger array apertures alleviate performance degradation, achieving high SE with millimeter-level sensing precision. We further demonstrate that the proposed AP-selection strategy reveals an optimal number of receive APs that maximizes sensing coverage without significantly sacrificing SE.

</details>


### [113] [Multi-Satellite Multi-Stream Beamspace Massive MIMO Transmission](https://arxiv.org/abs/2512.21998)
*Yafei Wang,Yiming Zhu,Vu Nguyen Ha,Wenjin Wang,Rui Ding,Symeon Chatzinotas,Björn Ottersten*

Main category: eess.SP

TL;DR: This paper proposes beamspace MIMO transmission for multi-satellite multi-stream systems, developing optimization methods for satellite clustering, beam selection, and precoding under statistical CSI to approach conventional MIMO performance with lower complexity.


<details>
  <summary>Details</summary>
Motivation: To enable efficient multi-satellite cooperative transmission where multiple satellites form distributed MIMO systems to deliver multiple data streams to multi-antenna users, addressing synchronization errors and complexity challenges while maintaining performance gains.

Method: Formulates MSMS beamspace MIMO signal model, proposes sCSI-based optimization for satellite clustering, beam selection, and transmit precoding using sum-rate upper-bound approximation. Develops CDWMMSE problem for precoder design, closed-form covariance decomposition, iterative beam-domain precoder, and heuristic closed-form precoders. Enhances competition-based satellite clustering algorithm and designs two-stage low-complexity beam selection algorithm.

Result: Simulations validate proposed methods across various configurations (data streams, receive antennas, serving satellites, active beams) showing beamspace transmission approaches conventional MIMO performance at lower complexity.

Conclusion: Beamspace MIMO transmission effectively combines earth-moving beamforming with beam-domain precoding, enabling multi-satellite cooperative systems to achieve near-conventional MIMO performance with reduced complexity through the proposed optimization framework.

Abstract: This paper studies multi-satellite multi-stream (MSMS) beamspace transmission, where multiple satellites cooperate to form a distributed multiple-input multiple-output (MIMO) system and jointly deliver multiple data streams to multi-antenna user terminals (UTs), and beamspace transmission combines earth-moving beamforming with beam-domain precoding. For the first time, we formulate the signal model for MSMS beamspace MIMO transmission. Under synchronization errors, multi-antenna UTs enable the distributed MIMO channel to exhibit higher rank, supporting multiple data streams. Beamspace MIMO retains conventional codebook based beamforming while providing the performance gains of precoding. Based on the signal model, we propose statistical channel state information (sCSI)-based optimization of satellite clustering, beam selection, and transmit precoding, using a sum-rate upper-bound approximation. With given satellite clustering and beam selection, we cast precoder design as an equivalent covariance decomposition-based weighted minimum mean square error (CDWMMSE) problem. To obtain tractable algorithms, we develop a closed-form covariance decomposition required by CDWMMSE and derive an iterative MSMS beam-domain precoder under sCSI. Following this, we further propose several heuristic closed-form precoders to avoid iterative cost. For satellite clustering, we enhance a competition-based algorithm by introducing a mechanism to regulate the number of satellites serving certain UT. Furthermore, we design a two-stage low-complexity beam selection algorithm focused on enhancing the effective channel power. Simulations under practical configurations validate the proposed methods across the number of data streams, receive antennas, serving satellites, and active beams, and show that beamspace transmission approaches conventional MIMO performance at lower complexity.

</details>


### [114] [Hybrid Deep Reinforcement Learning for Joint Resource Allocation in Multi-Active RIS-Aided Uplink Communications](https://arxiv.org/abs/2512.22107)
*Mohamed Shalma,Engy Aly Maher,Ahmed El-Mahdy*

Main category: eess.SP

TL;DR: Hybrid DRL framework for multi-user uplink with multiple active RISs, using SAC, DDPG, TD3 to optimize user powers, RIS configurations, and BS beamforming to maximize minimum user rate.


<details>
  <summary>Details</summary>
Motivation: Active RIS technology is promising for 6G networks, but resource allocation in multi-user uplink systems with multiple active RISs presents a high-dimensional, non-convex optimization problem that needs efficient solutions.

Method: Proposes hybrid DRL framework combining closed-form optimal beamforming solution with DRL algorithms (SAC, DDPG, TD3) to jointly optimize user transmit powers and active RIS configurations for maximizing minimum user rate.

Result: SAC achieves superior performance with high learning rate, faster convergence, and lower computational cost compared to DDPG and TD3. The closed-form optimal beamforming effectively enhances the minimum rate.

Conclusion: The hybrid DRL framework with SAC and closed-form beamforming provides an efficient solution for resource allocation in active RIS-assisted 6G networks, demonstrating practical advantages over other DRL approaches.

Abstract: Active Reconfigurable Intelligent Surfaces (RIS) are a promising technology for 6G wireless networks. This paper investigates a novel hybrid deep reinforcement learning (DRL) framework for resource allocation in a multi-user uplink system assisted by multiple active RISs. The objective is to maximize the minimum user rate by jointly optimizing user transmit powers, active RIS configurations, and base station (BS) beamforming. We derive a closed-form solution for optimal beamforming and employ DRL algorithms: Soft actor-critic (SAC), deep deterministic policy gradient (DDPG), and twin delayed DDPG (TD3) to solve the high-dimensional, non-convex power and RIS optimization problem. Simulation results demonstrate that SAC achieves superior performance with high learning rate leading to faster convergence and lower computational cost compared to DDPG and TD3. Furthermore, the closed-form of optimally beamforming enhances the minimum rate effectively.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [115] [Physics-Informed Neural Solvers for Periodic Quantum Eigenproblems](https://arxiv.org/abs/2512.21349)
*Haaris Mian*

Main category: cs.LG

TL;DR: Physics-informed neural network framework solves Floquet-Bloch eigenvalue problems for 2D periodic potentials, focusing on honeycomb lattices with Dirac points, using mesh-free training over Brillouin zone.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a physics-informed machine learning approach for solving quantum eigenproblems in periodic potentials, particularly honeycomb lattices (like graphene) that exhibit distinctive band topology with Dirac points, which are important for materials science and quantum physics applications.

Method: Uses neural networks to simultaneously learn complex Bloch functions and eigenvalues through a composite loss function that enforces the Schrödinger equation, Bloch periodicity, and normalization constraints without supervision. The model is trained over the Brillouin zone, and transfer learning is employed to adapt from nearly-free to strongly varying potentials.

Result: The framework successfully recovers band structures and Bloch modes, validated numerically against traditional plane-wave expansion methods. It demonstrates the ability to capture changes in band structure topology through transfer learning between different potential regimes.

Conclusion: This work contributes to physics-informed machine learning for quantum eigenproblems, providing insights into the relationship between symmetry, band structure, and neural architectures, offering a mesh-free alternative to traditional computational methods for periodic quantum systems.

Abstract: This thesis presents a physics-informed machine learning framework for solving the Floquet-Bloch eigenvalue problem associated with particles in two-dimensional periodic potentials, with a focus on honeycomb lattice geometry, due to its distinctive band topology featuring Dirac points and its relevance to materials such as graphene. By leveraging neural networks to learn complex Bloch functions and their associated eigenvalues (energies) simultaneously, we develop a mesh-free solver enforcing the governing Schrödinger equation, Bloch periodicity, and normalization constraints through a composite loss function without supervision. The model is trained over the Brillouin zone to recover band structures and Bloch modes, with numerical validation against traditional plane-wave expansion methods. We further explore transfer learning techniques to adapt the solver from nearly-free electron potentials to strongly varying potentials, demonstrating its ability to capture changes in band structure topology. This work contributes to the growing field of physics-informed machine learning for quantum eigenproblems, providing insights into the interplay between symmetry, band structure, and neural architectures.

</details>


### [116] [A Reinforcement Learning Approach to Synthetic Data Generation](https://arxiv.org/abs/2512.21395)
*Natalia Espinosa-Dice,Nicholas J. Jackson,Chao Yan,Aaron Lee,Bradley A. Malin*

Main category: cs.LG

TL;DR: RLSyn reframes synthetic data generation as a reinforcement learning problem, using PPO with discriminator rewards for stable training, outperforming GANs and matching diffusion models on biomedical datasets.


<details>
  <summary>Details</summary>
Motivation: Current synthetic data generation methods require large datasets and complex training, limiting their use in small-sample biomedical settings where privacy-preserving data sharing is needed.

Method: RLSyn models data generation as a reinforcement learning problem with a stochastic policy over patient records, optimized using Proximal Policy Optimization with discriminator-derived rewards for stable and data-efficient training.

Result: RLSyn performs comparably to diffusion models and outperforms GANs on MIMIC-IV, while outperforming both diffusion models and GANs on the smaller AI-READI dataset, demonstrating effectiveness in data-scarce regimes.

Conclusion: Reinforcement learning provides a principled and effective alternative for synthetic biomedical data generation, particularly in data-scarce settings, offering stable training and competitive performance.

Abstract: Synthetic data generation (SDG) is a promising approach for enabling data sharing in biomedical studies while preserving patient privacy. Yet, state-of-the-art generative models often require large datasets and complex training procedures, limiting their applicability in small-sample settings. In this work, we reframe SDG as a reinforcement learning (RL) problem and introduce RLSyn, a novel framework that models the data generator as a stochastic policy over patient records and optimizes it using Proximal Policy Optimization with discriminator-derived rewards, yielding more stable and data-efficient training. We evaluate RLSyn on two biomedical datasets - AI-READI and MIMIC-IV- and benchmark it against state-of-the-art generative adversarial networks (GANs) and diffusion-based methods across extensive privacy, utility, and fidelity evaluations. RL-Syn performs comparably to diffusion models and outperforms GANs on MIMIC-IV, while outperforming both diffusion models and GANs on the smaller AI-READI dataset. These results demonstrate that reinforcement learning provides a principled and effective alternative for synthetic biomedical data generation, particularly in data-scarce regimes.

</details>


### [117] [kooplearn: A Scikit-Learn Compatible Library of Algorithms for Evolution Operator Learning](https://arxiv.org/abs/2512.21409)
*Giacomo Turri,Grégoire Pacreau,Giacomo Meanti,Timothée Devergne,Daniel Ordonez,Erfan Mirzaei,Bruno Belucci,Karim Lounici,Vladimir Kostic,Massimiliano Pontil,Pietro Novelli*

Main category: cs.LG

TL;DR: kooplearn is an ML library for learning dynamical operators (Koopman/Transfer operators and generators) with scikit-learn API compliance for spectral analysis, reduced-order modeling, and forecasting of dynamical systems.


<details>
  <summary>Details</summary>
Motivation: To provide a unified, user-friendly library for learning dynamical operators from data, enabling spectral analysis, reduced-order modeling, and forecasting of dynamical systems with scikit-learn compatibility.

Method: Implements linear, kernel, and deep-learning estimators for discrete-time evolution operators (Koopman/Transfer) and continuous-time infinitesimal generators with scikit-learn API compliance.

Result: A comprehensive ML library with curated benchmark datasets that supports spectral methods, data-driven reduced-order models, forecasting, and integration into existing ML workflows.

Conclusion: kooplearn provides an accessible, standardized tool for dynamical systems analysis through learned operators, facilitating research reproducibility and algorithm comparison in the field.

Abstract: kooplearn is a machine-learning library that implements linear, kernel, and deep-learning estimators of dynamical operators and their spectral decompositions. kooplearn can model both discrete-time evolution operators (Koopman/Transfer) and continuous-time infinitesimal generators. By learning these operators, users can analyze dynamical systems via spectral methods, derive data-driven reduced-order models, and forecast future states and observables. kooplearn's interface is compliant with the scikit-learn API, facilitating its integration into existing machine learning and data science workflows. Additionally, kooplearn includes curated benchmark datasets to support experimentation, reproducibility, and the fair comparison of learning algorithms. The software is available at https://github.com/Machine-Learning-Dynamical-Systems/kooplearn.

</details>


### [118] [A Survey of Freshness-Aware Wireless Networking with Reinforcement Learning](https://arxiv.org/abs/2512.21412)
*Alimu Alibotaiken,Suyang Wang,Oluwaseun T. Ajayi,Yu Cheng*

Main category: cs.LG

TL;DR: This survey paper examines reinforcement learning (RL) specifically for Age of Information (AoI) and freshness optimization in wireless networks, providing a unified framework and taxonomy for learning-based freshness control in B5G/6G systems.


<details>
  <summary>Details</summary>
Motivation: Existing surveys either focus on classical AoI formulations or provide broad RL discussions without addressing freshness as a unified learning problem. There's a gap in understanding how RL specifically applies to AoI and freshness optimization in next-generation wireless systems.

Method: The paper organizes AoI variants into three families (native, function-based, application-oriented) and introduces a policy-centric taxonomy with four RL categories: update-control RL, medium-access RL, risk-sensitive RL, and multi-agent RL. This provides a framework for understanding learning applications to sampling, scheduling, trajectory planning, and distributed coordination.

Result: The survey synthesizes recent progress in RL-driven freshness control and establishes a unified foundation for learning-based freshness optimization. It provides a coherent framework for how RL can support various freshness-related decisions in wireless networks.

Conclusion: The paper aims to establish a unified foundation for learning-based freshness optimization in next-generation wireless networks (B5G/6G), while highlighting open challenges related to delayed decision processes, stochastic variability, and cross-layer design.

Abstract: The age of information (AoI) has become a central measure of data freshness in modern wireless systems, yet existing surveys either focus on classical AoI formulations or provide broad discussions of reinforcement learning (RL) in wireless networks without addressing freshness as a unified learning problem. Motivated by this gap, this survey examines RL specifically through the lens of AoI and generalized freshness optimization. We organize AoI and its variants into native, function-based, and application-oriented families, providing a clearer view of how freshness should be modeled in B5G and 6G systems. Building on this foundation, we introduce a policy-centric taxonomy that reflects the decisions most relevant to freshness, consisting of update-control RL, medium-access RL, risk-sensitive RL, and multi-agent RL. This structure provides a coherent framework for understanding how learning can support sampling, scheduling, trajectory planning, medium access, and distributed coordination. We further synthesize recent progress in RL-driven freshness control and highlight open challenges related to delayed decision processes, stochastic variability, and cross-layer design. The goal is to establish a unified foundation for learning-based freshness optimization in next-generation wireless networks.

</details>


### [119] [DeepCQ: General-Purpose Deep-Surrogate Framework for Lossy Compression Quality Prediction](https://arxiv.org/abs/2512.21433)
*Khondoker Mirazul Mumenin,Robert Underwood,Dong Dai,Jinzhen Wang,Sheng Di,Zarija Lukić,Franck Cappello*

Main category: cs.LG

TL;DR: DeepCQ is a deep-surrogate framework for predicting compression quality that works across different compressors, metrics, and datasets, using a two-stage design with mixture-of-experts for time-evolving data.


<details>
  <summary>Details</summary>
Motivation: Error-bounded lossy compression is essential for managing large scientific data, but assessing post-compression quality is computationally expensive due to intensive metric calculations.

Method: 1) Generalizable surrogate model for compression quality prediction across compressors, metrics, and datasets; 2) Two-stage design separating expensive feature extraction from lightweight metrics prediction; 3) Mixture-of-experts design optimized for time-evolving data to handle variations across simulation timesteps.

Result: Validated on four real-world scientific applications, achieving exceptional predictive accuracy with errors generally under 10%, significantly outperforming existing methods.

Conclusion: DeepCQ empowers scientific users to make informed compression decisions based on preferred data quality, significantly reducing I/O and computational overhead in scientific data analysis.

Abstract: Error-bounded lossy compression techniques have become vital for scientific data management and analytics, given the ever-increasing volume of data generated by modern scientific simulations and instruments. Nevertheless, assessing data quality post-compression remains computationally expensive due to the intensive nature of metric calculations. In this work, we present a general-purpose deep-surrogate framework for lossy compression quality prediction (DeepCQ), with the following key contributions: 1) We develop a surrogate model for compression quality prediction that is generalizable to different error-bounded lossy compressors, quality metrics, and input datasets; 2) We adopt a novel two-stage design that decouples the computationally expensive feature-extraction stage from the light-weight metrics prediction, enabling efficient training and modular inference; 3) We optimize the model performance on time-evolving data using a mixture-of-experts design. Such a design enhances the robustness when predicting across simulation timesteps, especially when the training and test data exhibit significant variation. We validate the effectiveness of DeepCQ on four real-world scientific applications. Our results highlight the framework's exceptional predictive accuracy, with prediction errors generally under 10\% across most settings, significantly outperforming existing methods. Our framework empowers scientific users to make informed decisions about data compression based on their preferred data quality, thereby significantly reducing I/O and computational overhead in scientific data analysis.

</details>


### [120] [An Information Theoretic Perspective on Agentic System Design](https://arxiv.org/abs/2512.21720)
*Shizhe He,Avanika Narayan,Ishan S. Khare,Scott W. Linderman,Christopher Ré,Dan Biderman*

Main category: cs.LG

TL;DR: Agentic LM systems use smaller "compressor" LMs to distill context into compact text for larger "predictor" LMs. The paper introduces an information-theoretic framework using mutual information to quantify compression quality, showing it strongly predicts downstream performance independent of specific tasks.


<details>
  <summary>Details</summary>
Motivation: Current compressor-predictor system designs are ad hoc with little guidance on how compressor and predictor choices affect performance. There's a need for task-independent metrics to evaluate compression quality and optimize system design without costly task-specific sweeps.

Method: View compressor LM as a noisy channel and introduce mutual information estimator between context and compression to quantify compression quality. Conduct comprehensive empirical analysis across five datasets and three model families using this information-theoretic framework.

Result: Mutual information strongly predicts downstream performance. Larger compressors are more accurate, token-efficient, and convey more bits of information per token. Scaling compressors is more effective than scaling predictors, enabling local compressors to pair with smaller cloud predictors. A 7B Qwen-2.5 compressor is 1.6× more accurate, 4.6× more concise, and conveys 5.5× more bits per token than its 1.5B sibling.

Conclusion: Information-theoretic approach provides principled guidance for designing agentic LM systems. Local compressors as small as 3B parameters can recover 99% of frontier-LM accuracy at 26% of API costs, demonstrating practical benefits of the framework.

Abstract: Agentic language model (LM) systems power modern applications like "Deep Research" and "Claude Code," and leverage multi-LM architectures to overcome context limitations. Beneath their apparent diversity lies a recurring pattern: smaller "compressor" LMs (that can even run locally) distill raw context into compact text that is then consumed by larger "predictor" LMs. Despite their popularity, the design of compressor-predictor systems remains largely ad hoc, with little guidance on how compressor and predictor choices shape downstream performance. In practice, attributing gains to compression versus prediction requires costly, task-specific pairwise sweeps. We argue that these agentic system design questions are, at root, information-theoretic. Viewing the compressor LM as a noisy channel, we introduce a simple estimator of mutual information between the context and its compression to quantify compression quality in a task-independent way. We show that mutual information strongly predicts downstream performance, independent of any specific task. Through an information-theoretic framework, we perform a comprehensive empirical analysis across five datasets and three model families. Results reveal that larger compressors not only are more accurate, but also more token-efficient, conveying more bits of information per token. A 7B Qwen-2.5 compressor, for instance, is $1.6\times$ more accurate, $4.6\times$ more concise, and conveys $5.5\times$ more bits of mutual information per token than its 1.5B sibling. Across datasets, scaling compressors is substantially more effective than scaling predictors, enabling larger on-device compressors to pair with smaller cloud predictors. Applied to a Deep Research system, these principles enable local compressors as small as 3B parameters to recover $99\%$ of frontier-LM accuracy at $26\%$ of API costs.

</details>


### [121] [dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning](https://arxiv.org/abs/2512.21446)
*Shirui Chen,Jiantao Jiao,Lillian J. Ratliff,Banghua Zhu*

Main category: cs.LG

TL;DR: dUltra is an RL framework that improves parallel token generation in masked diffusion language models by learning optimal unmasking strategies, achieving better accuracy-efficiency trade-offs than existing methods.


<details>
  <summary>Details</summary>
Motivation: Current masked diffusion language models have limited parallel decoding capability (less than 5 tokens per forward pass), making their sampling speeds comparable to autoregressive models with speculative decoding. Existing distillation-based accelerators suffer from off-policy training and are limited by base model quality.

Method: Proposes dUltra, an on-policy reinforcement learning framework using Group Relative Policy Optimization (GRPO). It introduces an unmasking planner head that predicts per-token unmasking likelihoods under independent Bernoulli distributions. Jointly optimizes base diffusion LLM and unmasking order planner using reward signals combining verifiable reward, distillation reward, and number of unmasking steps.

Result: Across mathematical reasoning and code generation tasks, dUltra improves the accuracy-efficiency trade-off over state-of-the-art heuristic and distillation baselines, moving toward achieving "diffusion supremacy" over autoregressive models.

Conclusion: dUltra demonstrates that on-policy reinforcement learning can effectively learn unmasking strategies for efficient parallel decoding in masked diffusion language models, overcoming limitations of existing approaches and advancing the potential for diffusion models to surpass autoregressive models in both quality and speed.

Abstract: Masked diffusion language models (MDLMs) offer the potential for parallel token generation, but most open-source MDLMs decode fewer than 5 tokens per model forward pass even with sophisticated sampling strategies. As a result, their sampling speeds are often comparable to AR + speculative decoding schemes, limiting their advantage over mainstream autoregressive approaches. Existing distillation-based accelerators (dParallel, d3LLM) finetune MDLMs on trajectories generated by a base model, which can become off-policy during finetuning and restrict performance to the quality of the base model's samples. We propose \texttt{dUltra}, an on-policy reinforcement learning framework based on Group Relative Policy Optimization (GRPO) that learns unmasking strategies for efficient parallel decoding. dUltra introduces an unmasking planner head that predicts per-token unmasking likelihoods under independent Bernoulli distributions. We jointly optimize the base diffusion LLM and the unmasking order planner using reward signals combining verifiable reward, distillation reward, and the number of unmasking steps. Across mathematical reasoning and code generation tasks, dUltra improves the accuracy--efficiency trade-off over state-of-the-art heuristic and distillation baselines, moving towards achieving ``diffusion supremacy'' over autoregressive models.

</details>


### [122] [An Equivariance Toolbox for Learning Dynamics](https://arxiv.org/abs/2512.21447)
*Yongyi Yang,Liu Ziyin*

Main category: cs.LG

TL;DR: A general equivariance framework for deep learning that extends Noether-type analyses to second-order constraints, connecting transformation symmetries to optimization geometry.


<details>
  <summary>Details</summary>
Motivation: Existing analyses of symmetry/equivariance in neural networks are problem-specific and focus mainly on first-order consequences (conservation laws), while second-order implications remain poorly understood. There's a need for a unified framework that captures both first- and second-order constraints from transformation symmetries.

Method: Developed a general equivariance toolbox that yields coupled first- and second-order constraints on learning dynamics. Extended classical Noether-type analyses in three directions: 1) from gradient constraints to Hessian constraints, 2) from symmetry to general equivariance, and 3) from continuous to discrete transformations.

Result: At first order, the framework unifies conservation laws and implicit-bias relations as special cases of a single identity. At second order, it provides structural predictions about curvature: identifies flat/sharp directions, gradient-Hessian eigenspace alignment, and how loss landscape geometry reflects underlying transformation structure.

Conclusion: The framework connects transformation structure to modern empirical observations about optimization geometry, recovering known results while deriving new characterizations that link symmetry/equivariance properties to second-order learning dynamics and loss landscape geometry.

Abstract: Many theoretical results in deep learning can be traced to symmetry or equivariance of neural networks under parameter transformations. However, existing analyses are typically problem-specific and focus on first-order consequences such as conservation laws, while the implications for second-order structure remain less understood. We develop a general equivariance toolbox that yields coupled first- and second-order constraints on learning dynamics. The framework extends classical Noether-type analyses in three directions: from gradient constraints to Hessian constraints, from symmetry to general equivariance, and from continuous to discrete transformations. At the first order, our framework unifies conservation laws and implicit-bias relations as special cases of a single identity. At the second order, it provides structural predictions about curvature: which directions are flat or sharp, how the gradient aligns with Hessian eigenspaces, and how the loss landscape geometry reflects the underlying transformation structure. We illustrate the framework through several applications, recovering known results while also deriving new characterizations that connect transformation structure to modern empirical observations about optimization geometry.

</details>


### [123] [RLLaVA: An RL-central Framework for Language and Vision Assistants](https://arxiv.org/abs/2512.21450)
*Lei Zhao,Zihao Ma,Boyu Lin,Yuhe Liu,Wenjun Wu,Lei Huang*

Main category: cs.LG

TL;DR: RLLaVA is an RL framework for vision-language assistants that decouples RL logic from model architecture, enabling efficient training of 1B-7B models on common GPUs with minimal code changes.


<details>
  <summary>Details</summary>
Motivation: To create a flexible RL framework for vision-language assistants that supports easy implementation of new RL algorithms, works with various vision-language models, and enables resource-efficient training on accessible hardware.

Method: Formulates vision-language assistants as Markov decision processes, decouples RL algorithmic logic from model architecture and distributed execution, and supports plug-and-play integration of RL methods and VLMs while remaining agnostic to specific training/inference engines.

Result: Enables training of 4B-scale models end-to-end with full-parameter updates on a single 24GB GPU. Models trained with RLLaVA consistently outperform base models on multi-modal and agentic tasks, competitive with specially engineered RL frameworks.

Conclusion: RLLaVA provides an efficient, extensible RL framework for vision-language assistants that lowers the barrier to RL research while maintaining competitive performance, making large-scale VLM training accessible on common hardware.

Abstract: We present an RL-central framework for Language and Vision Assistants (RLLaVA) with its formulation of Markov decision process (MDP). RLLaVA decouples RL algorithmic logic from model architecture and distributed execution, supporting researchers in implementing new RL algorithms with minimal code, and to plug in a broad family of RL methods and vision-language models (VLMs) while remaining agnostic to specific training and inference engines. RLLaVA makes resource-efficient training of 1B--7B models feasible on common GPUs; notably, 4B-scale models can be trained end-to-end with full-parameter updates on a single 24GB GPU. Experiments on multi-modal and agentic tasks demonstrate that RLLaVA has task extensibility, and the models trained with it consistently improve performance over base models, competitive with other specially engineered RL frameworks. The code is available at https://github.com/TinyLoopX/RLLaVA.

</details>


### [124] [Statistical vs. Deep Learning Models for Estimating Substance Overdose Excess Mortality in the US](https://arxiv.org/abs/2512.21456)
*Sukanya Krishna,Marie-Laure Charpignon,Maimuna Majumder*

Main category: cs.LG

TL;DR: DL models (especially LSTM) outperform traditional SARIMA for estimating excess substance overdose mortality during COVID-19, providing more accurate counterfactual projections under regime changes.


<details>
  <summary>Details</summary>
Motivation: Traditional statistical methods like SARIMA assume linearity, stationarity, and fixed seasonality, which may not hold under structural disruptions like the COVID-19 pandemic that exacerbated substance overdose mortality trends.

Method: Systematic comparison of SARIMA against three deep learning architectures (LSTM, Seq2Seq, Transformer) using national CDC data (2015-2019 for training/validation, 2020-2023 for projection), incorporating conformal prediction intervals and convergence analysis across 60+ trials per configuration.

Result: LSTM achieves superior point estimation (17.08% MAPE vs. 23.88% for SARIMA) and better-calibrated uncertainty (68.8% vs. 47.9% prediction interval coverage). Attention-based models (Seq2Seq, Transformer) underperform due to overfitting to historical means.

Conclusion: Carefully validated DL models can provide more reliable counterfactual estimates than traditional methods for public health planning, while highlighting the need for calibration techniques when deploying neural forecasting in high-stakes domains.

Abstract: Substance overdose mortality in the United States claimed over 80,000 lives in 2023, with the COVID-19 pandemic exacerbating existing trends through healthcare disruptions and behavioral changes. Estimating excess mortality, defined as deaths beyond expected levels based on pre-pandemic patterns, is essential for understanding pandemic impacts and informing intervention strategies. However, traditional statistical methods like SARIMA assume linearity, stationarity, and fixed seasonality, which may not hold under structural disruptions. We present a systematic comparison of SARIMA against three deep learning (DL) architectures (LSTM, Seq2Seq, and Transformer) for counterfactual mortality estimation using national CDC data (2015-2019 for training/validation, 2020-2023 for projection). We contribute empirical evidence that LSTM achieves superior point estimation (17.08% MAPE vs. 23.88% for SARIMA) and better-calibrated uncertainty (68.8% vs. 47.9% prediction interval coverage) when projecting under regime change. We also demonstrate that attention-based models (Seq2Seq, Transformer) underperform due to overfitting to historical means rather than capturing emergent trends. Ourreproducible pipeline incorporates conformal prediction intervals and convergence analysis across 60+ trials per configuration, and we provide an open-source framework deployable with 15 state health departments. Our findings establish that carefully validated DL models can provide more reliable counterfactual estimates than traditional methods for public health planning, while highlighting the need for calibration techniques when deploying neural forecasting in high-stakes domains.

</details>


### [125] [When Bayesian Tensor Completion Meets Multioutput Gaussian Processes: Functional Universality and Rank Learning](https://arxiv.org/abs/2512.21486)
*Siyuan Li,Shikai Fang,Lei Cheng,Feng Yin,Yik-Chung Wu,Peter Gerstoft,Sergios Theodoridis*

Main category: cs.LG

TL;DR: RR-FBTC is a Bayesian tensor completion method that automatically determines tensor rank while handling continuous-valued indices, with proven universal approximation capability.


<details>
  <summary>Details</summary>
Motivation: Existing functional tensor decomposition methods require pre-specification of tensor rank, which is NP-hard to determine optimally, and lack theoretical understanding of expressive power for continuous signals.

Method: Proposes rank-revealing functional Bayesian tensor completion (RR-FBTC) using multioutput Gaussian processes to model latent functions, enabling automatic rank determination through variational inference with closed-form updates.

Result: Establishes universal approximation property for continuous multi-dimensional signals, and experimental results on synthetic and real-world datasets show superiority over state-of-the-art approaches.

Conclusion: RR-FBTC effectively addresses the rank determination problem in functional tensor decomposition while maintaining strong expressive power for continuous signals, with practical implementation available.

Abstract: Functional tensor decomposition can analyze multi-dimensional data with real-valued indices, paving the path for applications in machine learning and signal processing. A limitation of existing approaches is the assumption that the tensor rank-a critical parameter governing model complexity-is known. However, determining the optimal rank is a non-deterministic polynomial-time hard (NP-hard) task and there is a limited understanding regarding the expressive power of functional low-rank tensor models for continuous signals. We propose a rank-revealing functional Bayesian tensor completion (RR-FBTC) method. Modeling the latent functions through carefully designed multioutput Gaussian processes, RR-FBTC handles tensors with real-valued indices while enabling automatic tensor rank determination during the inference process. We establish the universal approximation property of the model for continuous multi-dimensional signals, demonstrating its expressive power in a concise format. To learn this model, we employ the variational inference framework and derive an efficient algorithm with closed-form updates. Experiments on both synthetic and real-world datasets demonstrate the effectiveness and superiority of the RR-FBTC over state-of-the-art approaches. The code is available at https://github.com/OceanSTARLab/RR-FBTC.

</details>


### [126] [RefineBridge: Generative Bridge Models Improve Financial Forecasting by Foundation Models](https://arxiv.org/abs/2512.21572)
*Anthony Bolton,Wuyang Zhou,Zehua Chen,Giorgos Iacovides,Danilo Mandic*

Main category: cs.LG

TL;DR: RefineBridge enhances transformer-based time series foundation models for financial forecasting using a Schrödinger Bridge framework to refine predictions toward ground truths.


<details>
  <summary>Details</summary>
Motivation: Financial time series forecasting is challenging for transformer-based TSFMs due to non-stationarity, heavy-tailed distributions, and high-frequency noise. Existing methods like LoRA underperform because they preserve the original TSFM architecture rather than complementing it.

Method: Proposes RefineBridge, a refinement module built on a tractable Schrödinger Bridge generative framework. It uses TSFM forecasts as generative priors and observed ground truths as targets, learning context-conditioned stochastic transport maps to iteratively improve predictions.

Result: Simulations on multiple financial benchmarks show RefineBridge consistently improves performance of state-of-the-art TSFMs across different prediction horizons.

Conclusion: RefineBridge effectively enhances TSFMs for financial forecasting by learning to bridge predictions to ground truths through a generative Schrödinger Bridge framework, outperforming parameter-efficient adaptation methods like LoRA.

Abstract: Financial time series forecasting is particularly challenging for transformer-based time series foundation models (TSFMs) due to non-stationarity, heavy-tailed distributions, and high-frequency noise present in data. Low-rank adaptation (LoRA) has become a popular parameter-efficient method for adapting pre-trained TSFMs to downstream data domains. However, it still underperforms in financial data, as it preserves the network architecture and training objective of TSFMs rather than complementing the foundation model. To further enhance TSFMs, we propose a novel refinement module, RefineBridge, built upon a tractable Schrödinger Bridge (SB) generative framework. Given the forecasts of TSFM as generative prior and the observed ground truths as targets, RefineBridge learns context-conditioned stochastic transport maps to improve TSFM predictions, iteratively approaching the ground-truth target from even a low-quality prior. Simulations on multiple financial benchmarks demonstrate that RefineBridge consistently improves the performance of state-of-the-art TSFMs across different prediction horizons.

</details>


### [127] [MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding](https://arxiv.org/abs/2512.21506)
*Aiwei Zhang,Arvind Pillai,Andrew Campbell,Nicholas C. Jacobson*

Main category: cs.LG

TL;DR: MotionTeller is a framework that converts minute-level wearable activity data into natural language summaries using LLMs, achieving high semantic and lexical accuracy on real-world data.


<details>
  <summary>Details</summary>
Motivation: As wearable sensing becomes pervasive, there's a need to transform raw physiological signals (like actigraphy) into natural language summaries for better human interpretation and application in behavioral monitoring and healthcare.

Method: Combines a pretrained actigraphy encoder with a lightweight projection module that maps behavioral embeddings into the token space of a frozen decoder-only LLM, enabling autoregressive text generation. Trained on 54,383 real-world (actigraphy, text) pairs from NHANES using cross-entropy loss with supervision only on language tokens.

Result: Achieves high semantic fidelity (BERTScore-F1 = 0.924) and lexical accuracy (ROUGE-1 = 0.722), outperforming prompt-based baselines by 7% in ROUGE-1. Training loss converges to 0.38 by epoch 15. Qualitative analysis shows it captures circadian structure and behavioral transitions, with PCA revealing enhanced cluster alignment in embedding space.

Conclusion: MotionTeller is a scalable, interpretable system for transforming wearable sensor data into fluent, human-centered descriptions, enabling new pathways for behavioral monitoring, clinical review, and personalized health interventions.

Abstract: As wearable sensing becomes increasingly pervasive, a key challenge remains: how can we generate natural language summaries from raw physiological signals such as actigraphy - minute-level movement data collected via accelerometers? In this work, we introduce MotionTeller, a generative framework that natively integrates minute-level wearable activity data with large language models (LLMs). MotionTeller combines a pretrained actigraphy encoder with a lightweight projection module that maps behavioral embeddings into the token space of a frozen decoder-only LLM, enabling free-text, autoregressive generation of daily behavioral summaries. We construct a novel dataset of 54383 (actigraphy, text) pairs derived from real-world NHANES recordings, and train the model using cross-entropy loss with supervision only on the language tokens. MotionTeller achieves high semantic fidelity (BERTScore-F1 = 0.924) and lexical accuracy (ROUGE-1 = 0.722), outperforming prompt-based baselines by 7 percent in ROUGE-1. The average training loss converges to 0.38 by epoch 15, indicating stable optimization. Qualitative analysis confirms that MotionTeller captures circadian structure and behavioral transitions, while PCA plots reveal enhanced cluster alignment in embedding space post-training. Together, these results position MotionTeller as a scalable, interpretable system for transforming wearable sensor data into fluent, human-centered descriptions, introducing new pathways for behavioral monitoring, clinical review, and personalized health interventions.

</details>


### [128] [Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering](https://arxiv.org/abs/2512.21510)
*Wenyuan Yang,Jie Xu,Hongqing He,Jiangzhang Gan,Xiaofeng Zhu*

Main category: cs.LG

TL;DR: TreeEIC: A novel incomplete multi-view clustering framework using missing-pattern trees to fully utilize available multi-view pairs, with decision ensemble and knowledge distillation for robust clustering under highly inconsistent missing patterns.


<details>
  <summary>Details</summary>
Motivation: Real-world multi-view data has highly inconsistent missing patterns that challenge incomplete multi-view clustering. Existing methods overlook the "pair under-utilization issue" where incomplete but available multi-view pairs cannot be fully utilized due to inconsistent missing patterns, limiting model performance.

Method: 1) Defines missing-pattern tree model to group data into decision sets based on missing patterns, 2) Performs multi-view clustering within each set, 3) Uses multi-view decision ensemble module with uncertainty-based weights to aggregate results and suppress unreliable decisions, 4) Implements ensemble-to-individual knowledge distillation to transfer ensemble knowledge to view-specific models, optimizing cross-view consistency and inter-cluster discrimination losses.

Result: Extensive experiments on multiple benchmark datasets demonstrate that TreeEIC achieves state-of-the-art incomplete multi-view clustering performance and exhibits superior robustness under highly inconsistent missing patterns.

Conclusion: TreeEIC effectively addresses the pair under-utilization issue in incomplete multi-view clustering by fully exploiting available multi-view pairs through missing-pattern tree grouping, decision ensemble, and knowledge distillation, achieving robust performance even with highly inconsistent missing patterns.

Abstract: Real-world multi-view data usually exhibits highly inconsistent missing patterns which challenges the effectiveness of incomplete multi-view clustering (IMVC). Although existing IMVC methods have made progress from both imputation-based and imputation-free routes, they have overlooked the pair under-utilization issue, i.e., inconsistent missing patterns make the incomplete but available multi-view pairs unable to be fully utilized, thereby limiting the model performance. To address this, we propose a novel missing-pattern tree based IMVC framework entitled TreeEIC. Specifically, to achieve full exploitation of available multi-view pairs, TreeEIC first defines the missing-pattern tree model to group data into multiple decision sets according to different missing patterns, and then performs multi-view clustering within each set. Furthermore, a multi-view decision ensemble module is proposed to aggregate clustering results from all decision sets, which infers uncertainty-based weights to suppress unreliable clustering decisions and produce robust decisions. Finally, an ensemble-to-individual knowledge distillation module transfers the ensemble knowledge to view-specific clustering models, which enables ensemble and individual modules to promote each other by optimizing cross-view consistency and inter-cluster discrimination losses. Extensive experiments on multiple benchmark datasets demonstrate that our TreeEIC achieves state-of-the-art IMVC performance and exhibits superior robustness under highly inconsistent missing patterns.

</details>


### [129] [Perplexity-Aware Data Scaling Law: Perplexity Landscapes Predict Performance for Continual Pre-training](https://arxiv.org/abs/2512.21515)
*Lei Liu,Hao Zhu,Yue Shen,Zhixuan Chu,Jian Wang,Jinjie Gu,Kui Ren*

Main category: cs.LG

TL;DR: Proposes a perplexity-aware data scaling law for continual pre-training that uses model perplexity on domain data to identify high-utility subsets, improving data efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Standard scaling laws show diminishing returns when simply increasing data for continual pre-training, leading to suboptimal data utilization and inefficient training. There's a need for better methods to select high-value domain-specific data.

Method: Develops a novel perplexity-aware data scaling law that establishes a predictive relationship between the perplexity landscape of domain-specific data and test loss. Uses pre-trained model perplexity on domain data as a proxy for estimating knowledge gaps, quantifying the informational perplexity landscape of candidate training samples.

Result: Extensive experiments show the method consistently identifies near-optimal training subsets and achieves superior performance on both medical and general-domain benchmarks.

Conclusion: The proposed perplexity-aware scaling law enables adaptive selection of high-utility data subsets for continual pre-training, prioritizing content that maximizes knowledge absorption while minimizing redundancy and noise, leading to more efficient training and better performance.

Abstract: Continual Pre-training (CPT) serves as a fundamental approach for adapting foundation models to domain-specific applications. Scaling laws for pre-training define a power-law relationship between dataset size and the test loss of an LLM. However, the marginal gains from simply increasing data for CPT diminish rapidly, yielding suboptimal data utilization and inefficient training. To address this challenge, we propose a novel perplexity-aware data scaling law to establish a predictive relationship between the perplexity landscape of domain-specific data and the test loss. Our approach leverages the perplexity derived from the pre-trained model on domain data as a proxy for estimating the knowledge gap, effectively quantifying the informational perplexity landscape of candidate training samples. By fitting this scaling law across diverse perplexity regimes, we enable adaptive selection of high-utility data subsets, prioritizing content that maximizes knowledge absorption while minimizing redundancy and noise. Extensive experiments demonstrate that our method consistently identifies near-optimal training subsets and achieves superior performance on both medical and general-domain benchmarks.

</details>


### [130] [Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data](https://arxiv.org/abs/2512.21516)
*Hongqing He,Jie Xu,Wenyuan Yang,Yonghua Zhu,Guoqiu Wen,Xiaofeng Zhu*

Main category: cs.LG

TL;DR: A unified contrastive learning framework for multi-view clustering that addresses rare-paired and mis-paired issues in incomplete/noisy multi-view data through global-graph and local-graph guided contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Real-world multi-view data often suffers from incompleteness and noise, leading to rare-paired samples (insufficient complementary information) and mis-paired samples (wrong optimization direction) that challenge contrastive learning effectiveness in multi-view clustering.

Method: 1) Global-graph guided contrastive learning: constructs a global-view affinity graph from all view samples to form new sample pairs for exploring complementary information. 2) Local-graph weighted contrastive learning: uses local neighbors to generate pair-wise weights to adaptively strengthen or weaken contrastive learning. The method is imputation-free and integrates both approaches into a unified framework.

Result: Extensive experiments on both incomplete and noise settings of multi-view data demonstrate superior performance compared with state-of-the-art approaches.

Conclusion: The proposed unified global-local graph-guided contrastive learning framework effectively addresses rare-paired and mis-paired issues in incomplete and noisy multi-view data, enhancing clustering effectiveness without requiring data imputation.

Abstract: Recently, contrastive learning (CL) plays an important role in exploring complementary information for multi-view clustering (MVC) and has attracted increasing attention. Nevertheless, real-world multi-view data suffer from data incompleteness or noise, resulting in rare-paired samples or mis-paired samples which significantly challenges the effectiveness of CL-based MVC. That is, rare-paired issue prevents MVC from extracting sufficient multi-view complementary information, and mis-paired issue causes contrastive learning to optimize the model in the wrong direction. To address these issues, we propose a unified CL-based MVC framework for enhancing clustering effectiveness on incomplete and noise multi-view data. First, to overcome the rare-paired issue, we design a global-graph guided contrastive learning, where all view samples construct a global-view affinity graph to form new sample pairs for fully exploring complementary information. Second, to mitigate the mis-paired issue, we propose a local-graph weighted contrastive learning, which leverages local neighbors to generate pair-wise weights to adaptively strength or weaken the pair-wise contrastive learning. Our method is imputation-free and can be integrated into a unified global-local graph-guided contrastive learning framework. Extensive experiments on both incomplete and noise settings of multi-view data demonstrate that our method achieves superior performance compared with state-of-the-art approaches.

</details>


### [131] [First Provable Guarantees for Practical Private FL: Beyond Restrictive Assumptions](https://arxiv.org/abs/2512.21521)
*Egor Shulgin,Grigory Malinovsky,Sarit Khirirat,Peter Richtárik*

Main category: cs.LG

TL;DR: Fed-α-NormEC is the first differentially private FL framework with provable convergence and DP guarantees under standard assumptions, supporting practical features like multiple local updates and partial client participation.


<details>
  <summary>Details</summary>
Motivation: Current differentially private federated learning methods rely on unrealistic assumptions (bounded gradients or heterogeneity) and neglect practical FL features like multiple local updates and partial client participation, hindering real-world application.

Method: Fed-α-NormEC integrates local updates (full and incremental gradient steps), separate server and client stepsizes, and crucially supports partial client participation, which is essential for real-world deployment and provides privacy amplification.

Result: The framework provides provable convergence and differential privacy guarantees under standard assumptions, with theoretical guarantees corroborated by experiments on private deep learning tasks.

Conclusion: Fed-α-NormEC is the first differentially private FL framework that addresses practical deployment needs while maintaining strong theoretical guarantees, making differentially private FL more applicable to real-world scenarios.

Abstract: Federated Learning (FL) enables collaborative training on decentralized data. Differential privacy (DP) is crucial for FL, but current private methods often rely on unrealistic assumptions (e.g., bounded gradients or heterogeneity), hindering practical application. Existing works that relax these assumptions typically neglect practical FL features, including multiple local updates and partial client participation. We introduce Fed-$α$-NormEC, the first differentially private FL framework providing provable convergence and DP guarantees under standard assumptions while fully supporting these practical features. Fed-$α$-NormE integrates local updates (full and incremental gradient steps), separate server and client stepsizes, and, crucially, partial client participation, which is essential for real-world deployment and vital for privacy amplification. Our theoretical guarantees are corroborated by experiments on private deep learning tasks.

</details>


### [132] [Generative Actor Critic](https://arxiv.org/abs/2512.21527)
*Aoyang Qin,Deqian Kong,Wei Wang,Ying Nian Wu,Song-Chun Zhu,Sirui Xie*

Main category: cs.LG

TL;DR: GAC is a novel RL framework that decouples policy evaluation as learning a generative model of trajectories and returns, and policy improvement as performing inference on this model, enabling effective offline-to-online refinement.


<details>
  <summary>Details</summary>
Motivation: Conventional RL algorithms focused on expected returns struggle with refining offline pretrained models using online experiences, creating a need for a framework that better handles offline-to-online transfer.

Method: GAC reframes policy evaluation as learning p(τ, y) - a generative model of joint distribution over trajectories and returns. Policy improvement becomes versatile inference on this model. Implementation uses latent variable model with continuous latent plan vectors, with separate inference strategies for exploitation (optimizing latent plans to maximize returns) and exploration (sampling latent plans conditioned on dynamically adjusted target returns).

Result: Experiments on Gym-MuJoCo and Maze2D benchmarks show GAC achieves strong offline performance and significantly enhanced offline-to-online improvement compared to state-of-the-art methods, even without step-wise rewards.

Conclusion: GAC provides an effective framework for refining offline pretrained models with online experiences by decoupling sequential decision-making through generative modeling and versatile inference strategies.

Abstract: Conventional Reinforcement Learning (RL) algorithms, typically focused on estimating or maximizing expected returns, face challenges when refining offline pretrained models with online experiences. This paper introduces Generative Actor Critic (GAC), a novel framework that decouples sequential decision-making by reframing \textit{policy evaluation} as learning a generative model of the joint distribution over trajectories and returns, $p(τ, y)$, and \textit{policy improvement} as performing versatile inference on this learned model. To operationalize GAC, we introduce a specific instantiation based on a latent variable model that features continuous latent plan vectors. We develop novel inference strategies for both \textit{exploitation}, by optimizing latent plans to maximize expected returns, and \textit{exploration}, by sampling latent plans conditioned on dynamically adjusted target returns. Experiments on Gym-MuJoCo and Maze2D benchmarks demonstrate GAC's strong offline performance and significantly enhanced offline-to-online improvement compared to state-of-the-art methods, even in absence of step-wise rewards.

</details>


### [133] [AVP-Fusion: Adaptive Multi-Modal Fusion and Contrastive Learning for Two-Stage Antiviral Peptide Identification](https://arxiv.org/abs/2512.21544)
*Xinru Wen,Weizhong Lin,Xuan Xiao*

Main category: cs.LG

TL;DR: AVP-Fusion is a two-stage deep learning framework for antiviral peptide identification that integrates adaptive feature fusion and contrastive learning, achieving state-of-the-art performance and enabling precise viral family/subtype prediction.


<details>
  <summary>Details</summary>
Motivation: Current computational methods for antiviral peptide (AVP) identification struggle to capture intricate sequence dependencies and effectively handle ambiguous, hard-to-classify samples, which hinders novel drug development.

Method: Two-stage deep learning framework with: 1) Adaptive Gating Mechanism that dynamically regulates weights of local motifs (CNNs) and global dependencies (BiLSTMs) based on sequence context, 2) Contrastive learning with Online Hard Example Mining and BLOSUM62-based data augmentation, 3) Transfer learning for viral family/subtype prediction.

Result: Achieves accuracy of 0.9531 and MCC of 0.9064 on benchmark Set 1 dataset, significantly outperforming state-of-the-art methods. Enables precise subclass prediction for six viral families and eight specific viruses even with limited samples.

Conclusion: AVP-Fusion serves as a robust and interpretable tool for high-throughput antiviral drug screening, addressing key challenges in AVP identification through adaptive feature fusion and contrastive learning.

Abstract: Accurate identification of antiviral peptides (AVPs) is critical for accelerating novel drug development. However, current computational methods struggle to capture intricate sequence dependencies and effectively handle ambiguous, hard-to-classify samples. To address these challenges, we propose AVP-Fusion, a novel two-stage deep learning framework integrating adaptive feature fusion and contrastive learning. Unlike traditional static feature concatenation, we construct a panoramic feature space using 10 distinct descriptors and introduce an Adaptive Gating Mechanism.This mechanism dynamically regulates the weights of local motifs extracted by CNNs and global dependencies captured by BiLSTMs based on sequence context. Furthermore, to address data distribution challenges, we employ a contrastive learning strategy driven by Online Hard Example Mining (OHEM) and BLOSUM62-based data augmentation, which significantly sharpens the model's decision boundaries. Experimental results on the benchmark Set 1 dataset demonstrate that AVP-Fusion achieves an accuracy of 0.9531 and an MCC of 0.9064, significantly outperforming state-of-the-art methods. In the second stage, leveraging transfer learning, the model enables precise subclass prediction for six viral families and eight specific viruses, even under limited sample sizes. In summary, AVP-Fusion serves as a robust and interpretable tool for high-throughput antiviral drug screening.

</details>


### [134] [Discovering Sparse Recovery Algorithms Using Neural Architecture Search](https://arxiv.org/abs/2512.21563)
*Patrick Yubeaton,Sarthak Gupta,M. Salman Asif,Chinmay Hegde*

Main category: cs.LG

TL;DR: Meta-learning framework using Neural Architecture Search (NAS) to automatically discover/rediscover signal processing algorithms like ISTA/FISTA from large search spaces.


<details>
  <summary>Details</summary>
Motivation: Manual design of inverse problem algorithms is difficult, heuristic-driven, and time-consuming. The paper aims to automate algorithm discovery through meta-learning to reduce human effort and potentially discover novel algorithms.

Method: Developed a meta-learning framework using Neural Architecture Search (NAS) tools. Applied it to rediscover Iterative Shrinkage Thresholding Algorithm (ISTA) and Fast ISTA (FISTA) from a search space of over 50,000 variables. The framework can handle various data distributions and different algorithms beyond ISTA/FISTA.

Result: Successfully rediscovered several key elements of ISTA and FISTA algorithms. Demonstrated the framework's applicability to various data distributions and other algorithms besides ISTA/FISTA.

Conclusion: Meta-learning and NAS can effectively automate algorithm discovery in signal processing, potentially reducing manual design effort and enabling discovery of novel algorithms for inverse problems.

Abstract: The design of novel algorithms for solving inverse problems in signal processing is an incredibly difficult, heuristic-driven, and time-consuming task. In this short paper, we the idea of automated algorithm discovery in the signal processing context through meta-learning tools such as Neural Architecture Search (NAS). Specifically, we examine the Iterative Shrinkage Thresholding Algorithm (ISTA) and its accelerated Fast ISTA (FISTA) variant as candidates for algorithm rediscovery. We develop a meta-learning framework which is capable of rediscovering (several key elements of) the two aforementioned algorithms when given a search space of over 50,000 variables. We then show how our framework can apply to various data distributions and algorithms besides ISTA/FISTA.

</details>


### [135] [AnchorGK: Anchor-based Incremental and Stratified Graph Learning Framework for Inductive Spatio-Temporal Kriging](https://arxiv.org/abs/2512.21569)
*Xiaobin Ren,Kaiqi Zhao,Katerina Taškova,Patricia Riddle*

Main category: cs.LG

TL;DR: AnchorGK: Anchor-based incremental stratified graph learning framework for inductive spatio-temporal kriging that addresses sparse sensor deployments and heterogeneous feature availability.


<details>
  <summary>Details</summary>
Motivation: Existing spatio-temporal kriging methods under-exploit two practical characteristics: sparse spatial distribution of sensor locations and heterogeneous availability of auxiliary features across locations. This leads to suboptimal performance in real-world deployments.

Method: Introduces anchor locations to stratify data based on feature availability, forms strata around anchors, uses incremental representation mechanism to handle feature incompleteness, and employs dual-view graph learning layer to jointly aggregate feature-relevant and location-relevant information.

Result: Extensive experiments on multiple benchmark datasets show AnchorGK consistently outperforms state-of-the-art baselines for spatio-temporal kriging.

Conclusion: AnchorGK effectively addresses sparse spatial distributions and heterogeneous feature availability through principled stratification and incremental learning, enabling accurate inference in inductive spatio-temporal kriging settings.

Abstract: Spatio-temporal kriging is a fundamental problem in sensor networks, driven by the sparsity of deployed sensors and the resulting missing observations. Although recent approaches model spatial and temporal correlations, they often under-exploit two practical characteristics of real deployments: the sparse spatial distribution of locations and the heterogeneous availability of auxiliary features across locations. To address these challenges, we propose AnchorGK, an Anchor-based Incremental and Stratified Graph Learning framework for inductive spatio-temporal kriging. AnchorGK introduces anchor locations to stratify the data in a principled manner. Anchors are constructed according to feature availability, and strata are then formed around these anchors. This stratification serves two complementary roles. First, it explicitly represents and continuously updates correlations between unobserved regions and surrounding observed locations within a graph learning framework. Second, it enables the systematic use of all available features across strata via an incremental representation mechanism, mitigating feature incompleteness without discarding informative signals. Building on the stratified structure, we design a dual-view graph learning layer that jointly aggregates feature-relevant and location-relevant information, learning stratum-specific representations that support accurate inference under inductive settings. Extensive experiments on multiple benchmark datasets demonstrate that AnchorGK consistently outperforms state-of-the-art baselines for spatio-temporal kriging.

</details>


### [136] [Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations](https://arxiv.org/abs/2512.21586)
*Xin Liu,Haoran Li,Dongbin Zhao*

Main category: cs.LG

TL;DR: BCV-LR enables sample-efficient imitation learning from videos by extracting latent actions through self-supervision and dynamics prediction, then aligning them to real actions online.


<details>
  <summary>Details</summary>
Motivation: Humans learn efficiently from videos, but replicating this for autonomous agents is challenging due to visual complexity, lack of action/reward signals, and limited interaction steps.

Method: Extracts action-related latent features via self-supervised tasks, predicts latent actions between frames using dynamics-based unsupervised objective, then fine-tunes and aligns latent actions to real action space online for policy behavior cloning.

Result: Achieves expert-level performance on some tasks with few interactions, surpassing state-of-the-art ILV baselines and RL methods in sample efficiency across 24/28 tasks.

Conclusion: First work demonstrating that videos alone can support extremely sample-efficient visual policy learning without expert supervision.

Abstract: Humans can efficiently extract knowledge and learn skills from the videos within only a few trials and errors. However, it poses a big challenge to replicate this learning process for autonomous agents, due to the complexity of visual input, the absence of action or reward signals, and the limitations of interaction steps. In this paper, we propose a novel, unsupervised, and sample-efficient framework to achieve imitation learning from videos (ILV), named Behavior Cloning from Videos via Latent Representations (BCV-LR). BCV-LR extracts action-related latent features from high-dimensional video inputs through self-supervised tasks, and then leverages a dynamics-based unsupervised objective to predict latent actions between consecutive frames. The pre-trained latent actions are fine-tuned and efficiently aligned to the real action space online (with collected interactions) for policy behavior cloning. The cloned policy in turn enriches the agent experience for further latent action finetuning, resulting in an iterative policy improvement that is highly sample-efficient.
  We conduct extensive experiments on a set of challenging visual tasks, including both discrete control and continuous control. BCV-LR enables effective (even expert-level on some tasks) policy performance with only a few interactions, surpassing state-of-the-art ILV baselines and reinforcement learning methods (provided with environmental rewards) in terms of sample efficiency across 24/28 tasks. To the best of our knowledge, this work for the first time demonstrates that videos can support extremely sample-efficient visual policy learning, without the need to access any other expert supervision.

</details>


### [137] [Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data in Emergency and Critical Care](https://arxiv.org/abs/2512.21602)
*Yusuf Brima,Marcellin Atemkeng*

Main category: cs.LG

TL;DR: Tree-based models like XGBoost outperform deep learning approaches on imbalanced clinical data, offering better robustness and computational efficiency for emergency care applications.


<details>
  <summary>Details</summary>
Motivation: Clinical data in emergency and intensive care settings is often severely imbalanced, undermining model reliability for rare but crucial outcomes. There's a need for models that are both accurate and computationally efficient for real-world clinical use.

Method: Systematic evaluation of classical ML models on imbalanced tabular data from MIMIC-IV-ED and eICU. Compared tree-based methods, TabNet deep learning model, and custom lightweight TabResNet. Used complementary metrics to quantify class imbalance, Bayesian hyperparameter optimization, and assessed performance, robustness to imbalance, and computational scalability across seven clinical predictive tasks.

Result: Tree-based methods, particularly XGBoost, consistently achieved the most stable performance across imbalance levels and scaled efficiently with sample size. Deep tabular models degraded more sharply under imbalance and incurred higher computational costs. TabResNet provided a lighter alternative to TabNet but did not surpass ensemble benchmarks.

Conclusion: In emergency and critical care, robustness to imbalance and computational scalability outweigh architectural complexity. Tree-based ensemble methods currently offer the most practical and clinically feasible choice for high-stakes, time-sensitive environments.

Abstract: Emergency and intensive care environments require predictive models that are both accurate and computationally efficient, yet clinical data in these settings are often severely imbalanced. Such skewness undermines model reliability, particularly for rare but clinically crucial outcomes, making robustness and scalability essential for real-world usage. In this paper, we systematically evaluate the robustness and scalability of classical machine learning models on imbalanced tabular data from MIMIC-IV-ED and eICU. Class imbalance was quantified using complementary metrics, and we compared the performance of tree-based methods, the state-of-the-art TabNet deep learning model, and a custom lightweight residual network. TabResNet was designed as a computationally efficient alternative to TabNet, replacing its complex attention mechanisms with a streamlined residual architecture to maintain representational capacity for real-time clinical use. All models were optimized via a Bayesian hyperparameter search and assessed on predictive performance, robustness to increasing imbalance, and computational scalability. Our results, on seven clinically vital predictive tasks, show that tree-based methods, particularly XGBoost, consistently achieved the most stable performance across imbalance levels and scaled efficiently with sample size. Deep tabular models degraded more sharply under imbalance and incurred higher computational costs, while TabResNet provided a lighter alternative to TabNet but did not surpass ensemble benchmarks. These findings indicate that in emergency and critical care, robustness to imbalance and computational scalability could outweigh architectural complexity. Tree-based ensemble methods currently offer the most practical and clinically feasible choice, equipping practitioners with a framework for selecting models suited to high-stakes, time-sensitive environments.

</details>


### [138] [A Data-Driven Multi-Objective Approach for Predicting Mechanical Performance, Flowability, and Porosity in Ultra-High-Performance Concrete (UHPC)](https://arxiv.org/abs/2512.21610)
*Jagaran Chakma,Zhiguang Zhou,Jyoti Chakma,Cao YuSen*

Main category: cs.LG

TL;DR: A data-driven framework using XGBoost with data cleaning and feature selection achieves high accuracy in predicting UHPC mechanical performance, flow ability, and porosity, reducing experimental testing needs.


<details>
  <summary>Details</summary>
Motivation: To reduce extensive experimental testing in UHPC mix design by developing an accurate predictive framework for mechanical performance, flow ability, and porosity.

Method: Two-stage process: 1) Test 21 ML algorithms, select XGBoost after hyperparameter tuning; 2) Clean data (remove multicollinear features, detect outliers with Isolation Forest, select features with SHAP), retrain XGBoost. Develop GUI for material designers.

Result: XGBoost achieves high prediction accuracy across all outputs (mechanical performance, flow ability, porosity) after data refinement. The framework significantly improves prediction accuracy.

Conclusion: The proposed data-driven framework effectively predicts UHPC properties with high accuracy, minimizing the need for extensive experimental testing in mix design optimization.

Abstract: This study presents a data-driven, multi-objective approach to predict the mechanical performance, flow ability, and porosity of Ultra-High-Performance Concrete (UHPC). Out of 21 machine learning algorithms tested, five high-performing models are selected, with XGBoost showing the best accuracy after hyperparameter tuning using Random Search and K-Fold Cross-Validation. The framework follows a two-stage process: the initial XGBoost model is built using raw data, and once selected as the final model, the dataset is cleaned by (1) removing multicollinear features, (2) identifying outliers with Isolation Forest, and (3) selecting important features using SHAP analysis. The refined dataset as model 2 is then used to retrain XGBoost, which achieves high prediction accuracy across all outputs. A graphical user interface (GUI) is also developed to support material designers. Overall, the proposed framework significantly improves the prediction accuracy and minimizes the need for extensive experimental testing in UHPC mix design.

</details>


### [139] [MAD-NG: Meta-Auto-Decoder Neural Galerkin Method for Solving Parametric Partial Differential Equations](https://arxiv.org/abs/2512.21633)
*Qiuqi Li,Yiting Liu,Jin Zhao,Wencan Zhu*

Main category: cs.LG

TL;DR: A novel framework combining Neural Galerkin Method with Meta-Auto-Decoder paradigm for efficient and accurate solution of parametric PDEs, featuring space-time decoupling, meta-learning adaptation, and randomized sparse updates.


<details>
  <summary>Details</summary>
Motivation: Parametric PDEs are crucial for modeling physical/engineering systems with uncertain/varying parameters, but traditional neural network solvers (PINNs, Deep Galerkin Methods) struggle with generalization and long-time prediction efficiency due to full space-time approximations.

Method: Proposes an enhanced Neural Galerkin Method framework incorporating Meta-Auto-Decoder paradigm with three key innovations: 1) space-time decoupling for stable/efficient time integration, 2) meta-learning-driven adaptation for rapid generalization to unseen parameters, and 3) randomized sparse updates to reduce computational costs.

Result: The method achieves physically consistent, long-horizon predictions for complex parameterized evolution equations with significantly lower computational overhead. Numerical experiments on benchmark problems demonstrate comparable performance in accuracy, robustness, and adaptability.

Conclusion: The proposed framework successfully addresses limitations of traditional neural PDE solvers by combining NGM with MAD paradigm, enabling efficient, accurate, and generalizable solutions for parametric PDEs with reduced computational costs.

Abstract: Parametric partial differential equations (PDEs) are fundamental for modeling a wide range of physical and engineering systems influenced by uncertain or varying parameters. Traditional neural network-based solvers, such as Physics-Informed Neural Networks (PINNs) and Deep Galerkin Methods, often face challenges in generalization and long-time prediction efficiency due to their dependence on full space-time approximations. To address these issues, we propose a novel and scalable framework that significantly enhances the Neural Galerkin Method (NGM) by incorporating the Meta-Auto-Decoder (MAD) paradigm. Our approach leverages space-time decoupling to enable more stable and efficient time integration, while meta-learning-driven adaptation allows rapid generalization to unseen parameter configurations with minimal retraining. Furthermore, randomized sparse updates effectively reduce computational costs without compromising accuracy. Together, these advancements enable our method to achieve physically consistent, long-horizon predictions for complex parameterized evolution equations with significantly lower computational overhead. Numerical experiments on benchmark problems demonstrate that our methods performs comparatively well in terms of accuracy, robustness, and adaptability.

</details>


### [140] [Mechanical Strength Prediction of Steel-Polypropylene Fiber-based High-Performance Concrete Using Hybrid Machine Learning Algorithms](https://arxiv.org/abs/2512.21638)
*Jagaran Chakma,Zhiguang Zhou,Badhan Chakma*

Main category: cs.LG

TL;DR: ML models (ET-XGB, RF-LGBM, Transformer-XGB) predict mechanical properties of fiber-reinforced HPC with high accuracy; ET-XGB best overall, RF-LGBM most stable for flexural strength, SHAP identifies key influencing factors.


<details>
  <summary>Details</summary>
Motivation: To develop accurate and interpretable machine learning models for predicting mechanical properties of steel-polypropylene fiber-reinforced high-performance concrete, enabling better mix design optimization and structural performance evaluation.

Method: Three ensemble ML approaches: Extra Trees with XGBoost (ET-XGB), Random Forest with LightGBM (RF-LGBM), and Transformer with XGBoost (Transformer-XGB). Used extensive experimental dataset, k-fold cross-validation, hyperparameter optimization, SHAP analysis, and uncertainty analysis to predict compressive, flexural, and tensile strength.

Result: ET-XGB achieved highest overall accuracy (R²: 0.994 CS, 0.944 FS, 0.978 TS) with lowest uncertainty for CS and TS. RF-LGBM provided most stable FS predictions (R² 0.977) with lowest FS uncertainty. Transformer-XGB showed strong predictive capability but highest uncertainty. SHAP identified fiber aspect ratios, silica fume, and steel fiber content as most influential positive predictors, while water content and water-binder ratio had negative effects.

Conclusion: Machine learning models can provide accurate, interpretable, and generalizable predictions for HPC mechanical properties, offering valuable tools for concrete mix optimization and structural performance evaluation in engineering applications.

Abstract: This research develops and evaluates machine learning models to predict the mechanical properties of steel-polypropylene fiber-reinforced high-performance concrete (HPC). Three model families were investigated: Extra Trees with XGBoost (ET-XGB), Random Forest with LightGBM (RF-LGBM), and Transformer with XGBoost (Transformer-XGB). The target properties included compressive strength (CS), flexural strength (FS), and tensile strength (TS), based on an extensive dataset compiled from published experimental studies. Model training involved k-fold cross-validation, hyperparameter optimization, Shapley additive explanations (SHAP), and uncertainty analysis to ensure both robustness and interpretability. Among the tested approaches, the ET-XGB model achieved the highest overall accuracy, with testing R^2 values of 0.994 for CS, 0.944 for FS, and 0.978 for TS and exhibited lowest uncertainty for CS and TS (approximately 13-16% and 30.4%, respectively). The RF-LGBM model provided the most stable and reliable predictions for FS (R^2 0.977), yielding the lowest uncertainty for FS (approximately 5-33%). The Transformer-XGB model demonstrated strong predictive capability (R^2 0.978 for TS and 0.967 for FS) but consistently showed the highest uncertainty, indicating reduced generalization reliability. SHAP analysis further indicated that fiber aspect ratios (AR1 and AR2), silica fume (Sfu), and steel fiber content (SF) were the most influential predictors of strength, whereas water content (W) and the water-binder ratio (w/b) consistently had negative effects. The findings confirm that machine learning models can provide accurate, interpretable, and generalizable predictions of HPC mechanical properties. These models offer valuable tools for optimizing concrete mix design and enhancing structural performance evaluation in engineering applications.

</details>


### [141] [Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search](https://arxiv.org/abs/2512.21648)
*Maximilian Weichart*

Main category: cs.LG

TL;DR: The paper introduces Inverse-RPO, a method to systematically derive prior-based UCTs from any prior-free UCB, creating variance-aware tree policies that outperform PUCT.


<details>
  <summary>Details</summary>
Motivation: While PUCT (used in AlphaZero) improves exploration efficiency with prior terms, it was derived empirically. Existing UCB alternatives with stronger theoretical guarantees haven't been extended to prior-based UCTs. The recent RPO framework provides theoretical justification for PUCT, enabling systematic derivation of prior-based UCTs.

Method: Introduces Inverse-RPO methodology that systematically derives prior-based UCTs from any prior-free UCB. Applies this to variance-aware UCB-V to create two new prior-based tree policies incorporating variance estimates into search.

Result: Variance-aware prior-based UCTs outperform PUCT across multiple benchmarks without additional computational cost. Extension to mctx library shows minimal code changes needed.

Conclusion: Inverse-RPO provides a principled framework for deriving prior-based UCTs from prior-free UCBs, enabling better tree policies like variance-aware variants that improve upon PUCT.

Abstract: Monte Carlo Tree Search (MCTS) has profoundly influenced reinforcement learning (RL) by integrating planning and learning in tasks requiring long-horizon reasoning, exemplified by the AlphaZero family of algorithms. Central to MCTS is the search strategy, governed by a tree policy based on an upper confidence bound (UCB) applied to trees (UCT). A key factor in the success of AlphaZero is the introduction of a prior term in the UCB1-based tree policy PUCT, which improves exploration efficiency and thus accelerates training. While many alternative UCBs with stronger theoretical guarantees than UCB1 exist, extending them to prior-based UCTs has been challenging, since PUCT was derived empirically rather than from first principles. Recent work retrospectively justified PUCT by framing MCTS as a regularized policy optimization (RPO) problem. Building on this perspective, we introduce Inverse-RPO, a general methodology that systematically derives prior-based UCTs from any prior-free UCB. Applying this method to the variance-aware UCB-V, we obtain two new prior-based tree policies that incorporate variance estimates into the search. Experiments indicate that these variance-aware prior-based UCTs outperform PUCT across multiple benchmarks without incurring additional computational cost. We also provide an extension of the mctx library supporting variance-aware UCTs, showing that the required code changes are minimal and intended to facilitate further research on principled prior-based UCTs. Code: github.com/Max-We/inverse-rpo.

</details>


### [142] [Causal-HM: Restoring Physical Generative Logic in Multimodal Anomaly Detection via Hierarchical Modulation](https://arxiv.org/abs/2512.21650)
*Xiao Liu,Junchen Jin,Yanjie Zhao,Zhixuan Xing*

Main category: cs.LG

TL;DR: Causal-HM is a multimodal anomaly detection framework for robotic welding that models physical process-to-result dependencies using sensor-guided modulation and causal-hierarchical architecture.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal anomaly detection methods suffer from causal blindness by treating all modalities equally and ignoring the physical generative logic between process modalities (video, audio, sensors) and result modalities (post-weld images). Additionally, the heterogeneity gap between high-dimensional visual data and low-dimensional sensor signals causes critical process context to be lost.

Method: Causal-HM incorporates two key innovations: 1) Sensor-Guided CHM Modulation that uses low-dimensional sensor signals as context to guide high-dimensional audio-visual feature extraction, and 2) Causal-Hierarchical Architecture that enforces unidirectional generative mapping to identify anomalies violating physical consistency.

Result: Extensive experiments on the newly constructed Weld-4M benchmark across four modalities show that Causal-HM achieves state-of-the-art I-AUROC of 90.7%.

Conclusion: Causal-HM effectively addresses causal blindness and heterogeneity gaps in multimodal anomaly detection for robotic welding by explicitly modeling physical process-to-result dependencies, achieving superior performance on the Weld-4M benchmark.

Abstract: Multimodal Unsupervised Anomaly Detection (UAD) is critical for quality assurance in smart manufacturing, particularly in complex processes like robotic welding. However, existing methods often suffer from causal blindness, treating process modalities (e.g., real-time video, audio, and sensors) and result modalities (e.g., post-weld images) as equal feature sources, thereby ignoring the inherent physical generative logic. Furthermore, the heterogeneity gap between high-dimensional visual data and low-dimensional sensor signals frequently leads to critical process context being drowned out. In this paper, we propose Causal-HM, a unified multimodal UAD framework that explicitly models the physical Process to Result dependency. Specifically, our framework incorporates two key innovations: a Sensor-Guided CHM Modulation mechanism that utilizes low-dimensional sensor signals as context to guide high-dimensional audio-visual feature extraction , and a Causal-Hierarchical Architecture that enforces a unidirectional generative mapping to identify anomalies that violate physical consistency. Extensive experiments on our newly constructed Weld-4M benchmark across four modalities demonstrate that Causal-HM achieves a state-of-the-art (SOTA) I-AUROC of 90.7%. Code will be released after the paper is accepted.

</details>


### [143] [Rethinking Output Alignment For 1-bit Post-Training Quantization of Large Language Models](https://arxiv.org/abs/2512.21651)
*Dung Anh Hoang,Cuong Pham,Cuong Nguyen,Trung le,Jianfei Cai,Thanh-Toan Do*

Main category: cs.LG

TL;DR: This paper proposes a novel data-aware post-training quantization approach for 1-bit LLMs that addresses activation error accumulation, outperforming existing 1-bit PTQ methods with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: While 1-bit quantization (converting weights to ±1) offers maximum compression for LLM deployment on resource-constrained devices, existing 1-bit PTQ methods suffer from significant performance degradation. Most current approaches focus on weight alignment rather than output alignment, and naive output-matching approaches fail in 1-bit LLM quantization.

Method: The paper investigates why output-matching fails in 1-bit LLM quantization and proposes a novel data-aware PTQ approach that explicitly accounts for activation error accumulation while maintaining optimization efficiency. The method addresses the specific challenges of 1-bit quantization rather than focusing on weight alignment.

Result: Empirical experiments demonstrate that the proposed solution consistently outperforms existing 1-bit PTQ methods with minimal computational overhead, achieving better performance while maintaining the efficiency benefits of post-training quantization.

Conclusion: The paper successfully addresses the challenges of 1-bit LLM quantization by developing a data-aware approach that considers activation error accumulation, enabling more effective 1-bit quantization with better performance preservation compared to existing methods.

Abstract: Large Language Models (LLMs) deliver strong performance across a wide range of NLP tasks, but their massive sizes hinder deployment on resource-constrained devices. To reduce their computational and memory burden, various compression techniques have been proposed, including quantization, pruning, and knowledge distillation. Among these, post-training quantization (PTQ) is widely adopted for its efficiency, as it requires no retraining and only a small dataset for calibration, enabling low-cost deployment. Recent advances for post-training quantization have demonstrated that even sub-4-bit methods can maintain most of the original model performance. However, 1-bit quantization that converts floating-point weights to \(\pm\)1, remains particularly challenging, as existing 1-bit PTQ methods often suffer from significant performance degradation compared to the full-precision models. Specifically, most of existing 1-bit PTQ approaches focus on weight alignment, aligning the full-precision model weights with those of the quantized models, rather than directly aligning their outputs. Although the output-matching approach objective is more intuitive and aligns with the quantization goal, naively applying it in 1-bit LLMs often leads to notable performance degradation. In this paper, we investigate why and under what conditions output-matching fails, in the context of 1-bit LLM quantization. Based on our findings, we propose a novel data-aware PTQ approach for 1-bit LLMs that explicitly accounts for activation error accumulation while keeping optimization efficient. Empirical experiments demonstrate that our solution consistently outperforms existing 1-bit PTQ methods with minimal overhead.

</details>


### [144] [Dictionary-Transform Generative Adversarial Networks](https://arxiv.org/abs/2512.21677)
*Angshul Majumdar*

Main category: cs.LG

TL;DR: DT-GAN is a model-based adversarial framework using sparse dictionaries and analysis transforms, providing theoretical guarantees for stability, identifiability, and convergence that neural GANs lack.


<details>
  <summary>Details</summary>
Motivation: Classical GANs have ill-posed objectives, unstable training dynamics, and limited interpretability. The authors aim to create a theoretically sound adversarial framework with rigorous guarantees.

Method: DT-GAN uses a sparse synthesis dictionary as the generator and an analysis transform as the discriminator (energy model). Both are linear operators with explicit constraints, departing from neural architectures for theoretical tractability.

Result: DT-GAN has well-posed adversarial games with Nash equilibria, provable identifiability up to permutation/sign ambiguities, geometric alignment between operators, finite-sample stability, and robust convergence even in heavy-tailed regimes.

Conclusion: DT-GAN demonstrates that adversarial learning can be interpretable, stable, and provably correct when grounded in sparse modeling, offering a principled alternative for data with sparse synthesis structure.

Abstract: Generative adversarial networks (GANs) are widely used for distribution learning, yet their classical formulations remain theoretically fragile, with ill-posed objectives, unstable training dynamics, and limited interpretability. In this work, we introduce \emph{Dictionary-Transform Generative Adversarial Networks} (DT-GAN), a fully model-based adversarial framework in which the generator is a sparse synthesis dictionary and the discriminator is an analysis transform acting as an energy model. By restricting both players to linear operators with explicit constraints, DT-GAN departs fundamentally from neural GAN architectures and admits rigorous theoretical analysis.
  We show that the DT-GAN adversarial game is well posed and admits at least one Nash equilibrium. Under a sparse generative model, equilibrium solutions are provably identifiable up to standard permutation and sign ambiguities and exhibit a precise geometric alignment between synthesis and analysis operators. We further establish finite-sample stability and consistency of empirical equilibria, demonstrating that DT-GAN training converges reliably under standard sampling assumptions and remains robust in heavy-tailed regimes.
  Experiments on mixture-structured synthetic data validate the theoretical predictions, showing that DT-GAN consistently recovers underlying structure and exhibits stable behavior under identical optimization budgets where a standard GAN degrades. DT-GAN is not proposed as a universal replacement for neural GANs, but as a principled adversarial alternative for data distributions that admit sparse synthesis structure. The results demonstrate that adversarial learning can be made interpretable, stable, and provably correct when grounded in classical sparse modeling.

</details>


### [145] [RIPCN: A Road Impedance Principal Component Network for Probabilistic Traffic Flow Forecasting](https://arxiv.org/abs/2512.21685)
*Haochen Lv,Yan Lin,Shengnan Guo,Xiaowei Mao,Hong Nie,Letian Gong,Youfang Lin,Huaiyu Wan*

Main category: cs.LG

TL;DR: RIPCN is a probabilistic traffic flow forecasting model that integrates transportation theory with spatiotemporal learning to address uncertainty estimation challenges by modeling road impedance dynamics and capturing uncertainty correlations.


<details>
  <summary>Details</summary>
Motivation: Probabilistic traffic flow forecasting is crucial for intelligent transportation services, but existing approaches struggle with two key challenges: (1) uncovering and modeling the causes of traffic flow uncertainty for reliable forecasting, and (2) capturing spatiotemporal correlations of uncertainty for accurate prediction.

Method: RIPCN (Road Impedance Principal Component Network) integrates domain-specific transportation theory with spatiotemporal principal component learning. It features: (1) a dynamic impedance evolution network that captures directional traffic transfer patterns driven by road congestion and flow variability, revealing uncertainty causes; (2) a principal component network that forecasts dominant eigenvectors of future flow covariance to capture spatiotemporal uncertainty correlations.

Result: Experimental results on real-world datasets show that RIPCN outperforms existing probabilistic forecasting methods, providing both accurate uncertainty estimation and improved point prediction performance.

Conclusion: RIPCN successfully addresses key challenges in probabilistic traffic flow forecasting by integrating transportation theory with machine learning, enabling reliable uncertainty estimation while enhancing both interpretability and prediction accuracy.

Abstract: Accurate traffic flow forecasting is crucial for intelligent transportation services such as navigation and ride-hailing. In such applications, uncertainty estimation in forecasting is important because it helps evaluate traffic risk levels, assess forecast reliability, and provide timely warnings. As a result, probabilistic traffic flow forecasting (PTFF) has gained significant attention, as it produces both point forecasts and uncertainty estimates. However, existing PTFF approaches still face two key challenges: (1) how to uncover and model the causes of traffic flow uncertainty for reliable forecasting, and (2) how to capture the spatiotemporal correlations of uncertainty for accurate prediction.
  To address these challenges, we propose RIPCN, a Road Impedance Principal Component Network that integrates domain-specific transportation theory with spatiotemporal principal component learning for PTFF. RIPCN introduces a dynamic impedance evolution network that captures directional traffic transfer patterns driven by road congestion level and flow variability, revealing the direct causes of uncertainty and enhancing both reliability and interpretability. In addition, a principal component network is designed to forecast the dominant eigenvectors of future flow covariance, enabling the model to capture spatiotemporal uncertainty correlations. This design allows for accurate and efficient uncertainty estimation while also improving point prediction performance. Experimental results on real-world datasets show that our approach outperforms existing probabilistic forecasting methods.

</details>


### [146] [Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning](https://arxiv.org/abs/2512.21743)
*Hengyi Wu,Zhenyi Wang,Heng Huang*

Main category: cs.LG

TL;DR: Entropy-aware continual learning method uses dynamic feedback to regulate layer entropy - reducing entropy in high-entropy layers to prevent underfitting and increasing entropy in low-entropy layers to prevent overfitting, leading to better generalization.


<details>
  <summary>Details</summary>
Motivation: Most continual learning methods treat all layers uniformly, trading stability for plasticity or vice versa. Different layers naturally exhibit varying uncertainty (entropy) when classifying tasks, with high-entropy layers tending to underfit and low-entropy layers risking overfitting.

Method: Proposes an entropy-aware continual learning method with dynamic feedback mechanism that regulates each layer based on its entropy. Reduces entropy in high-entropy layers to mitigate underfitting and increases entropy in overly confident layers to alleviate overfitting, encouraging convergence to wider local minima for better generalization.

Result: Experiments on various datasets demonstrate substantial performance gains over state-of-the-art continual learning baselines.

Conclusion: The proposed entropy-aware method with dynamic layer regulation effectively addresses the imbalance between underfitting and overfitting in continual learning, improving generalization and can be seamlessly integrated with both replay- and regularization-based approaches.

Abstract: Continual learning aims to acquire new tasks while preserving performance on previously learned ones, but most methods struggle with catastrophic forgetting. Existing approaches typically treat all layers uniformly, often trading stability for plasticity or vice versa. However, different layers naturally exhibit varying levels of uncertainty (entropy) when classifying tasks. High-entropy layers tend to underfit by failing to capture task-specific patterns, while low-entropy layers risk overfitting by becoming overly confident and specialized. To address this imbalance, we propose an entropy-aware continual learning method that employs a dynamic feedback mechanism to regulate each layer based on its entropy. Specifically, our approach reduces entropy in high-entropy layers to mitigate underfitting and increases entropy in overly confident layers to alleviate overfitting. This adaptive regulation encourages the model to converge to wider local minima, which have been shown to improve generalization. Our method is general and can be seamlessly integrated with both replay- and regularization-based approaches. Experiments on various datasets demonstrate substantial performance gains over state-of-the-art continual learning baselines.

</details>


### [147] [A Model of Causal Explanation on Neural Networks for Tabular Data](https://arxiv.org/abs/2512.21746)
*Takashi Isozaki,Masahiro Yamamoto,Atsushi Noda*

Main category: cs.LG

TL;DR: CENNET is a causal explanation method for neural network predictions on tabular data that addresses pseudo-correlation and causality issues using structural causal models combined with neural networks, with a new entropy-based explanation power index.


<details>
  <summary>Details</summary>
Motivation: There's a need to explain machine learning predictions, especially for neural networks on tabular data where issues like pseudo-correlation and causality arise. Current methods lack proper causal explanations for neural network predictions.

Method: CENNET combines structural causal models (SCMs) with neural networks to provide causal explanations, despite SCMs not typically being used as predictive models due to accuracy limitations. The method includes a new explanation power index based on entropy.

Result: CENNET successfully provides causal explanations for neural network predictions through comparative experiments with existing methods on both synthetic and quasi-real data in classification tasks.

Conclusion: CENNET effectively addresses the challenges of providing causal explanations for neural network predictions on tabular data by integrating structural causal models with neural networks and introducing an entropy-based explanation power metric.

Abstract: The problem of explaining the results produced by machine learning methods continues to attract attention. Neural network (NN) models, along with gradient boosting machines, are expected to be utilized even in tabular data with high prediction accuracy. This study addresses the related issues of pseudo-correlation, causality, and combinatorial reasons for tabular data in NN predictors. We propose a causal explanation method, CENNET, and a new explanation power index using entropy for the method. CENNET provides causal explanations for predictions by NNs and uses structural causal models (SCMs) effectively combined with the NNs although SCMs are usually not used as predictive models on their own in terms of predictive accuracy. We show that CEN-NET provides such explanations through comparative experiments with existing methods on both synthetic and quasi-real data in classification tasks.

</details>


### [148] [Approximation Capabilities of Feedforward Neural Networks with GELU Activations](https://arxiv.org/abs/2512.21749)
*Konstantin Yakovlev,Nikita Puchkin*

Main category: cs.LG

TL;DR: The paper provides simultaneous approximation error bounds for functions and all their derivatives using GELU neural networks, covering polynomials, exponentials, and reciprocals with global derivative bounds.


<details>
  <summary>Details</summary>
Motivation: To develop neural network approximation theory that simultaneously bounds approximation errors for functions and all their higher-order derivatives, which is important for applications requiring accurate derivative information like physics-informed neural networks and numerical analysis.

Method: Constructive approximation using feedforward neural networks with GELU activation. The analysis starts with approximating multiplication, then extends to division and exponential functions. The approach ensures simultaneous error bounds over expanding domains.

Result: Derived simultaneous approximation error bounds for functions and all derivatives up to any prescribed order. The bounds apply to multivariate polynomials, exponential functions, and reciprocal functions. Also reported network size, weight magnitudes, and asymptotic behavior.

Conclusion: GELU neural networks can simultaneously approximate functions and all their derivatives with provable error bounds, enabling applications where accurate derivative information is crucial. The constructive approach provides practical network architectures with controlled weight magnitudes.

Abstract: We derive an approximation error bound that holds simultaneously for a function and all its derivatives up to any prescribed order. The bounds apply to elementary functions, including multivariate polynomials, the exponential function, and the reciprocal function, and are obtained using feedforward neural networks with the Gaussian Error Linear Unit (GELU) activation. In addition, we report the network size, weight magnitudes, and behavior at infinity. Our analysis begins with a constructive approximation of multiplication, where we prove the simultaneous validity of error bounds over domains of increasing size for a given approximator. Leveraging this result, we obtain approximation guarantees for division and the exponential function, ensuring that all higher-order derivatives of the resulting approximators remain globally bounded.

</details>


### [149] [VAMP-Net: An Interpretable Multi-Path Framework of Genomic Permutation-Invariant Set Attention and Quality-Aware 1D-CNN for MTB Drug Resistance](https://arxiv.org/abs/2512.21786)
*Aicha Boutorh,Kamar Hibatallah Baghdadi,Anais Daoud*

Main category: cs.LG

TL;DR: VAMP-Net: A dual-path neural network combining Set Attention Transformer for epistatic interactions and CNN for sequencing quality analysis to predict tuberculosis drug resistance with >95% accuracy and dual interpretability.


<details>
  <summary>Details</summary>
Motivation: Current genomic prediction of Mycobacterium tuberculosis drug resistance faces challenges from complex epistatic interactions and variable sequencing data quality, limiting clinical utility.

Method: Two-path architecture: Path-1 uses Set Attention Transformer for permutation-invariant variant sets to capture epistatic interactions; Path-2 uses 1D CNN to analyze VCF quality metrics for adaptive confidence scoring; fusion module combines both pathways.

Result: Superior performance over baseline models with >95% accuracy and ~97% AUC for Rifampicin and Rifabutin resistance prediction; comparative evaluation shows benefits of unmasked vs padding-masked Set Attention Blocks.

Conclusion: VAMP-Net advances clinical genomics by providing state-of-the-art predictive performance with dual-layer interpretability (epistatic networks and quality metric dependencies), establishing a new paradigm for robust, clinically-actionable resistance prediction.

Abstract: Genomic prediction of drug resistance in Mycobacterium tuberculosis remains challenging due to complex epistatic interactions and highly variable sequencing data quality. We present a novel Interpretable Variant-Aware Multi-Path Network (VAMP-Net) that addresses both challenges through complementary machine learning pathways. Path-1 employs a Set Attention Transformer processing permutation-invariant variant sets to capture epistatic interactions between genomic loci. Path-2 utilizes a 1D Convolutional Neural Network that analyzes Variant Call Format quality metrics to learn adaptive confidence scores. A fusion module combines both pathways for final resistance classification. We conduct comparative evaluations of unmasked versus padding-masked Set Attention Blocks, and demonstrate that our multi-path architecture achieves superior performance over baseline CNN and MLP models, with accuracy exceeding 95% and AUC around 97% for Rifampicin (RIF) and Rifabutin (RFB) resistance prediction. The framework provides dual-layer interpretability: Attention Weight Analysis reveals Epistatic networks, and Integrated Gradients (IG) was applied for critical resistance loci (notably rpoB), while gradient-based feature importance from the CNN pathway uncovers drug-specific dependencies on data quality metrics. This architecture advances clinical genomics by delivering state-of-the-art predictive performance alongside auditable interpretability at two distinct levels, genetic causality of mutation sets and technical confidence of sequencing evidence, establishing a new paradigm for robust, clinically-actionable resistance prediction.

</details>


### [150] [Synthetic Financial Data Generation for Enhanced Financial Modelling](https://arxiv.org/abs/2512.21791)
*Christophe D. Hounwanou,Yae Ulrich Gaba,Pierre Ntakirutimana*

Main category: cs.LG

TL;DR: This paper presents a unified evaluation framework for synthetic financial data, comparing ARIMA-GARCH, VAEs, and TimeGAN on S&P 500 data across fidelity, temporal structure, and downstream utility metrics.


<details>
  <summary>Details</summary>
Motivation: Data scarcity and confidentiality in finance hinder model development and testing, creating a need for reliable synthetic data generation methods that can overcome these limitations while maintaining realistic financial characteristics.

Method: Developed a multi-criteria evaluation framework assessing fidelity (MMD), temporal structure (autocorrelation, volatility clustering), and practical utility in downstream tasks (portfolio optimization, volatility forecasting). Applied to three generative paradigms using historical S&P 500 data.

Result: ARIMA-GARCH captures linear trends and conditional volatility but fails at nonlinear dynamics; VAEs produce smooth trajectories underestimating extreme events; TimeGAN achieves best trade-off between realism and temporal coherence (lowest MMD: 1.84e-3).

Conclusion: The paper provides practical guidelines for selecting generative models based on application needs and computational constraints, and establishes a unified evaluation protocol with reproducible codebase to standardize benchmarking in synthetic financial data research.

Abstract: Data scarcity and confidentiality in finance often impede model development and robust testing. This paper presents a unified multi-criteria evaluation framework for synthetic financial data and applies it to three representative generative paradigms: the statistical ARIMA-GARCH baseline, Variational Autoencoders (VAEs), and Time-series Generative Adversarial Networks (TimeGAN). Using historical S and P 500 daily data, we evaluate fidelity (Maximum Mean Discrepancy, MMD), temporal structure (autocorrelation and volatility clustering), and practical utility in downstream tasks, specifically mean-variance portfolio optimization and volatility forecasting. Empirical results indicate that ARIMA-GARCH captures linear trends and conditional volatility but fails to reproduce nonlinear dynamics; VAEs produce smooth trajectories that underestimate extreme events; and TimeGAN achieves the best trade-off between realism and temporal coherence (e.g., TimeGAN attained the lowest MMD: 1.84e-3, average over 5 seeds). Finally, we articulate practical guidelines for selecting generative models according to application needs and computational constraints. Our unified evaluation protocol and reproducible codebase aim to standardize benchmarking in synthetic financial data research.

</details>


### [151] [Smart IoT-Based Leak Forecasting and Detection for Energy-Efficient Liquid Cooling in AI Data Centers](https://arxiv.org/abs/2512.21801)
*Krishna Chaitanya Sunkara,Rambabu Konakanchi*

Main category: cs.LG

TL;DR: Smart IoT system using LSTM for leak forecasting and Random Forest for detection achieves 96.5% detection accuracy and 87% forecasting accuracy, preventing ~1,500 kWh annual energy waste in liquid-cooled AI data centers.


<details>
  <summary>Details</summary>
Motivation: Liquid-cooled AI data centers face substantial energy loss from coolant leaks causing unplanned shutdowns and extended repairs. Current systems lack proactive leak monitoring capabilities.

Method: Combines LSTM neural networks for probabilistic leak forecasting with Random Forest classifiers for instant detection. Uses MQTT streaming, InfluxDB storage, and Streamlit dashboards. Tested on synthetic data aligned with ASHRAE 2021 standards.

Result: Achieves 96.5% detection accuracy and 87% forecasting accuracy at 90% probability within ±30-minute windows. Identifies sudden leaks within 1 minute and forecasts 2-4 hours ahead. Humidity, pressure, and flow rate provide strong predictive signals while temperature shows minimal immediate response.

Conclusion: The system could prevent ~1,500 kWh annual energy waste in a typical 47-rack facility through proactive maintenance. While validation is synthetic-only, results establish feasibility for future operational deployment in sustainable data center operations.

Abstract: AI data centers which are GPU centric, have adopted liquid cooling to handle extreme heat loads, but coolant leaks result in substantial energy loss through unplanned shutdowns and extended repair periods. We present a proof-of-concept smart IoT monitoring system combining LSTM neural networks for probabilistic leak forecasting with Random Forest classifiers for instant detection. Testing on synthetic data aligned with ASHRAE 2021 standards, our approach achieves 96.5% detection accuracy and 87% forecasting accuracy at 90% probability within plus or minus 30-minute windows. Analysis demonstrates that humidity, pressure, and flow rate deliver strong predictive signals, while temperature exhibits minimal immediate response due to thermal inertia in server hardware. The system employs MQTT streaming, InfluxDB storage, and Streamlit dashboards, forecasting leaks 2-4 hours ahead while identifying sudden events within 1 minute. For a typical 47-rack facility, this approach could prevent roughly 1,500 kWh annual energy waste through proactive maintenance rather than reactive emergency procedures. While validation remains synthetic-only, results establish feasibility for future operational deployment in sustainable data center operations.

</details>


### [152] [A Comedy of Estimators: On KL Regularization in RL Training of LLMs](https://arxiv.org/abs/2512.21852)
*Vedant Shah,Johan Obando-Ceron,Vineet Jain,Brian Bartoldson,Bhavya Kailkhura,Sarthak Mittal,Glen Berseth,Pablo Samuel Castro,Yoshua Bengio,Nikolay Malkin,Moksh Jain,Siddarth Venkatraman,Aaron Courville*

Main category: cs.LG

TL;DR: This paper analyzes how different KL divergence estimator configurations affect RL fine-tuning of LLMs, finding that unbiased gradient estimators lead to better performance and stability compared to biased ones.


<details>
  <summary>Details</summary>
Motivation: There's a discrepancy between stated RL objectives for LLM training and their implementation - prevailing practices for KL regularization don't provide correct gradients, but there's no systematic study analyzing how different KL estimator configurations affect downstream performance.

Method: The authors analyze gradient biases of various KL estimator configurations and empirically test them by RL fine-tuning Qwen2.5-7B, Llama-3.1-8B-Instruct, and Qwen3-4B-Instruct-2507 models with different configurations, evaluating on both in-distribution and out-of-distribution tasks.

Result: In on-policy settings: (1) biased gradient estimators cause training instabilities, (2) unbiased gradient estimators lead to better performance on both in-domain and out-of-domain tasks. In off-policy settings, KL regularization helps stabilize training in asynchronous setups.

Conclusion: Careful selection of KL estimator configurations with unbiased gradients is crucial for stable and effective RL fine-tuning of LLMs, improving both in-distribution and out-of-distribution performance while stabilizing off-policy training.

Abstract: The reasoning performance of large language models (LLMs) can be substantially improved by training them with reinforcement learning (RL). The RL objective for LLM training involves a regularization term, which is the reverse Kullback-Leibler (KL) divergence between the trained policy and the reference policy. Since computing the KL divergence exactly is intractable, various estimators are used in practice to estimate it from on-policy samples. Despite its wide adoption, including in several open-source libraries, there is no systematic study analyzing the numerous ways of incorporating KL estimators in the objective and their effect on the downstream performance of RL-trained models. Recent works show that prevailing practices for incorporating KL regularization do not provide correct gradients for stated objectives, creating a discrepancy between the objective and its implementation. In this paper, we further analyze these practices and study the gradients of several estimators configurations, revealing how design choices shape gradient bias. We substantiate these findings with empirical observations by RL fine-tuning \texttt{Qwen2.5-7B}, \texttt{Llama-3.1-8B-Instruct} and \texttt{Qwen3-4B-Instruct-2507} with different configurations and evaluating their performance on both in- and out-of-distribution tasks. Through our analysis, we observe that, in on-policy settings: (1) estimator configurations with biased gradients can result in training instabilities; and (2) using estimator configurations resulting in unbiased gradients leads to better performance on in-domain as well as out-of-domain tasks. We also investigate the performance resulting from different KL configurations in off-policy settings and observe that KL regularization can help stabilize off-policy RL training resulting from asynchronous setups.

</details>


### [153] [Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation](https://arxiv.org/abs/2512.21866)
*Yiming Qian,Thorsten Neumann,Xueyining Huang,David Hardoon,Fei Gao,Yong Liu,Siow Mong Rick Goh*

Main category: cs.LG

TL;DR: Privacy-preserving dataset distillation using random forest leaf regions to generate synthetic fraud detection data that maintains performance while enabling explainability and protecting privacy.


<details>
  <summary>Details</summary>
Motivation: Need for collaborative fraud detection across institutions while preserving privacy, enabling explainability, and reducing data volume for regulatory compliance and auditability.

Method: Convert trained random forest into transparent axis-aligned rule regions (leaf hyperrectangles), then uniformly sample within each region to generate synthetic transactions, creating a compact surrogate dataset.

Result: 85-93% data reduction while maintaining competitive performance; improved cross-institution detection; strong privacy (membership inference at chance level); high structural similarity (93%+); uncertainty filtering boosts AUC to 0.687.

Conclusion: Tree-region distillation enables trustworthy, deployable fraud analytics with interpretable rules, per-case rationales with uncertainty quantification, and strong privacy suitable for multi-institution collaboration and regulatory audit.

Abstract: We propose an explainable, privacy-preserving dataset distillation framework for collaborative financial fraud detection. A trained random forest is converted into transparent, axis-aligned rule regions (leaf hyperrectangles), and synthetic transactions are generated by uniformly sampling within each region. This produces a compact, auditable surrogate dataset that preserves local feature interactions without exposing sensitive original records. The rule regions also support explainability: aggregated rule statistics (for example, support and lift) describe global patterns, while assigning each case to its generating region gives concise human-readable rationales and calibrated uncertainty based on tree-vote disagreement.
  On the IEEE-CIS fraud dataset (590k transactions across three institution-like clusters), distilled datasets reduce data volume by 85% to 93% (often under 15% of the original) while maintaining competitive precision and micro-F1, with only a modest AUC drop. Sharing and augmenting with synthesized data across institutions improves cross-cluster precision, recall, and AUC. Real vs. synthesized structure remains highly similar (over 93% by nearest-neighbor cosine analysis). Membership-inference attacks perform at chance level (about 0.50) when distinguishing training from hold-out records, suggesting low memorization risk. Removing high-uncertainty synthetic points using disagreement scores further boosts AUC (up to 0.687) and improves calibration. Sensitivity tests show weak dependence on the distillation ratio (AUC about 0.641 to 0.645 from 6% to 60%).
  Overall, tree-region distillation enables trustworthy, deployable fraud analytics with interpretable global rules, per-case rationales with quantified uncertainty, and strong privacy properties suitable for multi-institution settings and regulatory audit.

</details>


### [154] [MMCTOP: A Multimodal Textualization and Mixture-of-Experts Framework for Clinical Trial Outcome Prediction](https://arxiv.org/abs/2512.21897)
*Carolina Aparício,Qi Shi,Bo Wen,Tesfaye Yadete,Qiwei Han*

Main category: cs.LG

TL;DR: MMCTOP is a multimodal framework for clinical trial outcome prediction that integrates molecular structures, protocol metadata, eligibility narratives, and disease ontologies using schema-guided textualization and a drug-disease-conditioned sparse Mixture-of-Experts transformer.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of multimodal data fusion in high-dimensional biomedical informatics for clinical trial outcome prediction, where heterogeneous biomedical signals need to be effectively integrated.

Method: Proposes MMCTOP framework with schema-guided textualization, input-fidelity validation, modality-aware representation learning using domain-specific encoders, and a transformer backbone with drug-disease-conditioned sparse Mixture-of-Experts (SMoE) for fusion with top-k routing.

Result: Achieves consistent improvements in precision, F1, and AUC over unimodal and multimodal baselines on benchmark datasets. Ablations show schema-guided textualization and selective expert routing contribute materially to performance and stability. Temperature scaling provides calibrated probabilities.

Conclusion: MMCTOP advances multimodal trial modeling by combining controlled narrative normalization, context-conditioned expert fusion, and operational safeguards for auditability and reproducibility in biomedical informatics.

Abstract: Addressing the challenge of multimodal data fusion in high-dimensional biomedical informatics, we propose MMCTOP, a MultiModal Clinical-Trial Outcome Prediction framework that integrates heterogeneous biomedical signals spanning (i) molecular structure representations, (ii) protocol metadata and long-form eligibility narratives, and (iii) disease ontologies. MMCTOP couples schema-guided textualization and input-fidelity validation with modality-aware representation learning, in which domain-specific encoders generate aligned embeddings that are fused by a transformer backbone augmented with a drug-disease-conditioned sparse Mixture-of-Experts (SMoE). This design explicitly supports specialization across therapeutic and design subspaces while maintaining scalable computation through top-k routing. MMCTOP achieves consistent improvements in precision, F1, and AUC over unimodal and multimodal baselines on benchmark datasets, and ablations show that schema-guided textualization and selective expert routing contribute materially to performance and stability. We additionally apply temperature scaling to obtain calibrated probabilities, ensuring reliable risk estimation for downstream decision support. Overall, MMCTOP advances multimodal trial modeling by combining controlled narrative normalization, context-conditioned expert fusion, and operational safeguards aimed at auditability and reproducibility in biomedical informatics.

</details>


### [155] [GQ-VAE: A gated quantized VAE for learning variable length tokens](https://arxiv.org/abs/2512.21913)
*Theo Datta,Kayla Huang,Sham Kakade,David Brandfonbrener*

Main category: cs.LG

TL;DR: GQ-VAE is a neural tokenizer that can replace BPE without major architecture changes, offering better compression and language modeling performance than standard VQ-VAE, and improving downstream LM learning when compression rates match BPE.


<details>
  <summary>Details</summary>
Motivation: Existing neural tokenizers add complexity and require major architecture changes, making them difficult to implement at scale. Current deterministic tokenizers like BPE dominate despite limitations.

Method: Proposed Gated Quantized Variational Autoencoder (GQ-VAE) that learns to encode variable-length discrete tokens and can be independently pre-trained as a drop-in replacement for existing tokenizers.

Result: GQ-VAE improves compression and language modeling performance over standard VQ-VAE, approaches BPE's compression and LM performance, and when compression rates match, improves downstream language model learning compared to BPE.

Conclusion: GQ-VAE offers a practical neural tokenizer alternative to BPE without major architectural changes, with promising results and several exciting avenues for future work.

Abstract: While most frontier models still use deterministic frequency-based tokenization algorithms such as byte-pair encoding (BPE), there has been significant recent work to design learned neural tokenizers. However, these schemes generally add to underlying language model complexity and force large changes to architecture, making them hard to implement at large scales. To overcome these challenges, we propose the gated quantized variational autoencoder (GQ-VAE), a novel architecture that can be independently pre-trained to serve as a drop-in replacement for existing tokenizers. The key innovation of the architecture is to learn to encode variable-length discrete tokens. GQ-VAE improves compression and language modeling performance over a standard VQ-VAE tokenizer, and approaches the compression rate and language modeling performance of BPE. Interestingly, if we use BPE with a smaller vocabulary, such that the compression is equivalent between GQ-VAE and BPE, we find that GQ-VAE improves downstream language model learning. We conclude with a discussion of several exciting avenues for future work. Code can be found at https://github.com/Theo-Datta-115/gq-vae.

</details>


### [156] [Exploring the Heterogeneity of Tabular Data: A Diversity-aware Data Generator via LLMs](https://arxiv.org/abs/2512.21915)
*Yafeng Tang,Xiaoou Ding,Jianzhuo Du,Zishuo Yan,Zhuang Ma,Zheng Liang,Zekai Qian,Hongzhi Wang*

Main category: cs.LG

TL;DR: DATE is a framework for generating diverse, high-quality tabular data by partitioning heterogeneous data into subsets, using LLMs with decision tree feedback, and employing Multi-Arm Bandit sampling to balance diversity and quality.


<details>
  <summary>Details</summary>
Motivation: Real-world tabular data is heterogeneous with diverse distributions, making it challenging to create a single generative model that works well across all data types. Existing methods struggle to generate universally good data for diverse distributions.

Method: DATE partitions heterogeneous data into diverse subsets, uses LLMs with decision tree reasoning as feedback to generate labeled data for each subset, and employs a Multi-Arm Bandit-based sampling algorithm to balance diversity and quality in the generated data.

Result: DATE outperforms state-of-the-art GAN-based and LLM-based methods, achieving 23.75% average error rate reduction with just 100 generated data. It also improves DPO accuracy and enhances LLM reasoning capability on target data.

Conclusion: DATE effectively addresses the challenge of generating diverse, high-quality tabular data from heterogeneous distributions by combining intelligent partitioning, LLM-based generation with feedback, and optimal sampling strategies.

Abstract: Tabular data generation has become increasingly essential for enabling robust machine learning applications, which require large-scale, high-quality data. Existing solutions leverage generative models to learn original data distributions. However, real-world data are naturally heterogeneous with diverse distributions, making it challenging to obtain a universally good model for diverse data generation. To address this limitation, we introduce Diversity-Aware Tabular data gEnerator (DATE), a framework that (i) prepares high-quality and distributionally distinct examples for in-context learning by effectively partitioning the original heterogeneous data into multiple diverse subsets; (ii) harnesses Large Language Models (LLMs) to explore the diversity of the partitioned distribution with decision tree reasoning as feedback, generating high-quality labeled data for each subset. However, the massive generated data inherently involves a trade-off between diversity and quality. To integrate this issue, existing solutions greedily select the validation-best data. However, we prove that the selection in heterogeneous settings does not possess the greedy-choice property, and design a Multi-Arm Bandit-based sampling algorithm that balances the diversity and quality of generated data. Extensive experiments on tabular classification and regression benchmarks demonstrate that DATE consistently outperforms state-of-the-art GAN-based and LLM-based methods. On average, DATE achieves a 23.75% reduction in error rate with just 100 generated data. Empirically, we demonstrate that data generated by DATE can improve the accuracy of Direct Preference Optimization (DPO) and enhance the reasoning capability of LLMs on the target data. Code is available at https://github.com/windblow32/DATE.

</details>


### [157] [Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model](https://arxiv.org/abs/2512.21917)
*Nathan Kallus*

Main category: cs.LG

TL;DR: The paper proposes robust policy alignment methods for LLMs that work with unknown preference noise distributions, avoiding bias from misspecified link functions.


<details>
  <summary>Details</summary>
Motivation: Current preference alignment methods assume known link functions (like logistic in Bradley-Terry models), but if the link is wrong, inferred rewards become biased and policies misaligned. There's a need for methods robust to unknown preference noise distributions.

Method: Formulates policy alignment as f-divergence-constrained reward maximization, showing realizability implies a semiparametric single-index binary choice model. Develops three types of policy learners: 1) profiling the link function, 2) orthogonalizing the link function, and 3) using link-agnostic bipartite ranking objectives. Provides finite-sample policy error bounds and practical implementations using first-order optimization for neural networks.

Result: Theoretical analysis shows the methods are robust to unknown preference noise distribution and scale while preserving direct policy optimization without explicitly fitting rewards. Finite-sample policy error bounds depend on functional complexity measures of the index class.

Conclusion: The proposed methods enable robust policy alignment to preferences under unknown and unrestricted link functions, addressing the limitations of traditional approaches that assume specific link functions like logistic.

Abstract: Aligning large language models to preference data is commonly implemented by assuming a known link function between the distribution of observed preferences and the unobserved rewards (e.g., a logistic link as in Bradley-Terry). If the link is wrong, however, inferred rewards can be biased and policies be misaligned. We study policy alignment to preferences under an unknown and unrestricted link. We consider an $f$-divergence-constrained reward maximization problem and show that realizability of the solution in a policy class implies a semiparametric single-index binary choice model, where a scalar-valued index determined by a policy captures the dependence on demonstrations and the rest of the preference distribution is an unrestricted function thereof. Rather than focus on estimation of identifiable finite-dimensional structural parameters in the index as in econometrics, we focus on policy learning, focusing on error to the optimal policy and allowing unidentifiable and nonparametric indices. We develop a variety of policy learners based on profiling the link function, orthogonalizing the link function, and using link-agnostic bipartite ranking objectives. We analyze these and provide finite-sample policy error bounds that depend on generic functional complexity measures of the index class. We further consider practical implementations using first-order optimization suited to neural networks and batched data. The resulting methods are robust to unknown preference noise distribution and scale, while preserving the direct optimization of policies without explicitly fitting rewards.

</details>


### [158] [Hybrid Combinatorial Multi-armed Bandits with Probabilistically Triggered Arms](https://arxiv.org/abs/2512.21925)
*Kongchang Zhou,Tingyu Zhang,Wei Chen,Fang Kong*

Main category: cs.LG

TL;DR: Hybrid CMAB-T framework combines offline data with online interaction to overcome limitations of purely online or offline combinatorial multi-armed bandits with triggered arms.


<details>
  <summary>Details</summary>
Motivation: Online CMAB-T suffers from high interaction costs and slow adaptation, while offline methods are constrained by dataset quality and lack exploration capabilities. The authors aim to address these complementary weaknesses by integrating both paradigms.

Method: Propose hybrid CMAB-T framework and hybrid CUCB algorithm that leverages offline data to guide exploration and accelerate convergence, while strategically incorporating online interactions to mitigate insufficient coverage or distributional bias in offline data.

Result: Theoretical guarantees show hybrid CUCB significantly outperforms purely online approaches when high-quality offline data is available, and effectively corrects bias in offline-only methods when data is limited or misaligned. Empirical results demonstrate consistent advantages.

Conclusion: Hybrid CMAB-T framework successfully integrates offline and online learning, overcoming limitations of each approach individually and providing practical benefits in combinatorial bandit settings with triggered arms.

Abstract: The problem of combinatorial multi-armed bandits with probabilistically triggered arms (CMAB-T) has been extensively studied. Prior work primarily focuses on either the online setting where an agent learns about the unknown environment through iterative interactions, or the offline setting where a policy is learned solely from logged data. However, each of these paradigms has inherent limitations: online algorithms suffer from high interaction costs and slow adaptation, while offline methods are constrained by dataset quality and lack of exploration capabilities. To address these complementary weaknesses, we propose hybrid CMAB-T, a new framework that integrates offline data with online interaction in a principled manner. Our proposed hybrid CUCB algorithm leverages offline data to guide exploration and accelerate convergence, while strategically incorporating online interactions to mitigate the insufficient coverage or distributional bias of the offline dataset. We provide theoretical guarantees on the algorithm's regret, demonstrating that hybrid CUCB significantly outperforms purely online approaches when high-quality offline data is available, and effectively corrects the bias inherent in offline-only methods when the data is limited or misaligned. Empirical results further demonstrate the consistent advantage of our algorithm.

</details>


### [159] [DuaDeep-SeqAffinity: Dual-Stream Deep Learning Framework for Sequence-Only Antigen-Antibody Affinity Prediction](https://arxiv.org/abs/2512.22007)
*Aicha Boutorh,Soumia Bouyahiaoui,Sara Belhadj,Nour El Yakine Guendouz,Manel Kara Laouar*

Main category: cs.LG

TL;DR: DuaDeep-SeqAffinity is a sequence-only deep learning framework that predicts antibody-antigen binding affinity using ESM-2 embeddings with dual-stream CNN-Transformer architecture, outperforming both sequence-only and structure-sequence hybrid methods.


<details>
  <summary>Details</summary>
Motivation: Traditional affinity prediction methods rely on scarce and expensive 3D structural data. There's a need for scalable, structure-free approaches that can handle large sequence libraries for drug and vaccine discovery.

Method: Uses pre-trained ESM-2 protein language model embeddings with dual-stream architecture: 1D CNNs for local motif detection and Transformer encoders for global context. Features are fused through a fusion module and passed to fully connected network for regression.

Result: Achieved Pearson correlation 0.688, R² 0.460, RMSE 0.737, and AUC 0.890. Outperformed single-branch variants (ESM-CNN, ESM-Transformer) and existing SOTA methods, including structure-sequence hybrid models.

Conclusion: High-fidelity sequence embeddings can capture binding patterns traditionally requiring structural modeling. The framework provides scalable, efficient solution for high-throughput screening, accelerating therapeutic discovery without 3D structure dependency.

Abstract: Predicting the binding affinity between antigens and antibodies is fundamental to drug discovery and vaccine development. Traditional computational approaches often rely on experimentally determined 3D structures, which are scarce and computationally expensive to obtain. This paper introduces DuaDeep-SeqAffinity, a novel sequence-only deep learning framework that predicts affinity scores solely from their amino acid sequences using a dual-stream hybrid architecture. Our approach leverages pre-trained ESM-2 protein language model embeddings, combining 1D Convolutional Neural Networks (CNNs) for local motif detection with Transformer encoders for global contextual representation. A subsequent fusion module integrates these multi-faceted features, which are then passed to a fully connected network for final score regression. Experimental results demonstrate that DuaDeep-SeqAffinity significantly outperforms individual architectural components and existing state-of-the-art (SOTA) methods. DuaDeep achieved a superior Pearson correlation of 0.688, an R^2 of 0.460, and a Root Mean Square Error (RMSE) of 0.737, surpassing single-branch variants ESM-CNN and ESM-Transformer. Notably, the model achieved an Area Under the Curve (AUC) of 0.890, outperforming sequence-only benchmarks and even surpassing structure-sequence hybrid models. These findings prove that high-fidelity sequence embeddings can capture essential binding patterns typically reserved for structural modeling. By eliminating the reliance on 3D structures, DuaDeep-SeqAffinity provides a highly scalable and efficient solution for high-throughput screening of vast sequence libraries, significantly accelerating the therapeutic discovery pipeline.

</details>


### [160] [HWL-HIN: A Hypergraph-Level Hypergraph Isomorphism Network as Powerful as the Hypergraph Weisfeiler-Lehman Test with Application to Higher-Order Network Robustness](https://arxiv.org/abs/2512.22014)
*Chengyu Tian,Wenbin Pei*

Main category: cs.LG

TL;DR: Proposes Hypergraph Isomorphism Network (HIN) framework for hypergraph robustness prediction with theoretical expressive power equivalent to Hypergraph Weisfeiler-Lehman test, outperforming existing graph-based models and HGNNs.


<details>
  <summary>Details</summary>
Motivation: Conventional attack-based robustness assessments are computationally expensive, while existing deep learning methods (CNNs, GNNs) neglect higher-order correlations in real-world systems modeled as hypergraphs. Current Hypergraph Neural Networks (HGNNs) don't reach theoretical expressive power limits.

Method: Proposes hypergraph-level Hypergraph Isomorphism Network framework inspired by Graph Isomorphism Networks, with theoretical expressive power equivalent to Hypergraph Weisfeiler-Lehman test, applied to predict hypergraph robustness.

Result: Experimental results show the method maintains superior training/prediction efficiency while outperforming existing graph-based models and significantly surpassing conventional HGNNs in tasks prioritizing topological structure representation.

Conclusion: The proposed Hypergraph Isomorphism Network framework effectively addresses limitations of existing methods for hypergraph robustness prediction, achieving both theoretical expressive power guarantees and practical performance improvements.

Abstract: Robustness in complex systems is of significant engineering and economic importance. However, conventional attack-based a posteriori robustness assessments incur prohibitive computational overhead. Recently, deep learning methods, such as Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs), have been widely employed as surrogates for rapid robustness prediction. Nevertheless, these methods neglect the complex higher-order correlations prevalent in real-world systems, which are naturally modeled as hypergraphs. Although Hypergraph Neural Networks (HGNNs) have been widely adopted for hypergraph learning, their topological expressive power has not yet reached the theoretical upper bound. To address this limitation, inspired by Graph Isomorphism Networks, this paper proposes a hypergraph-level Hypergraph Isomorphism Network framework. Theoretically, this approach is proven to possess an expressive power strictly equivalent to the Hypergraph Weisfeiler-Lehman test and is applied to predict hypergraph robustness. Experimental results demonstrate that while maintaining superior efficiency in training and prediction, the proposed method not only outperforms existing graph-based models but also significantly surpasses conventional HGNNs in tasks that prioritize topological structure representation.

</details>


### [161] [LibContinual: A Comprehensive Library towards Realistic Continual Learning](https://arxiv.org/abs/2512.22029)
*Wenbin Li,Shangge Liu,Borui Kang,Yiyang Chen,KaXuan Lew,Yang Chen,Yinghuan Shi,Lei Wang,Yang Gao,Jiebo Luo*

Main category: cs.LG

TL;DR: LibContinual is a unified continual learning library addressing fragmentation in the field by standardizing implementations and evaluation, while revealing performance drops of existing methods under realistic constraints.


<details>
  <summary>Details</summary>
Motivation: The continual learning field suffers from fragmentation with inconsistent implementations, conflicting dependencies, and varying evaluation protocols, making fair comparisons and reproducible research difficult. Existing evaluations often rely on unrealistic assumptions that overestimate real-world applicability.

Method: Developed LibContinual, a comprehensive library with high-cohesion, low-coupling modular architecture integrating 19 representative algorithms across five methodological categories. Used this framework to systematically investigate three implicit assumptions in mainstream evaluation and proposed new evaluation protocols including strict online CL settings, unified memory budget protocol, and category-randomized settings.

Result: Significant performance drops were revealed in many representative CL methods when subjected to realistic constraints. The study shows that existing methods often fail under strict online settings, regulated memory resources, and intra-task semantic heterogeneity.

Conclusion: Resource-aware and semantically robust CL strategies are necessary for real-world applications. LibContinual serves as a foundational toolkit for future research in realistic continual learning, addressing the field's fragmentation and enabling standardized, reproducible evaluations.

Abstract: A fundamental challenge in Continual Learning (CL) is catastrophic forgetting, where adapting to new tasks degrades the performance on previous ones. While the field has evolved with diverse methods, this rapid surge in diverse methodologies has culminated in a fragmented research landscape. The lack of a unified framework, including inconsistent implementations, conflicting dependencies, and varying evaluation protocols, makes fair comparison and reproducible research increasingly difficult. To address this challenge, we propose LibContinual, a comprehensive and reproducible library designed to serve as a foundational platform for realistic CL. Built upon a high-cohesion, low-coupling modular architecture, LibContinual integrates 19 representative algorithms across five major methodological categories, providing a standardized execution environment. Meanwhile, leveraging this unified framework, we systematically identify and investigate three implicit assumptions prevalent in mainstream evaluation: (1) offline data accessibility, (2) unregulated memory resources, and (3) intra-task semantic homogeneity. We argue that these assumptions often overestimate the real-world applicability of CL methods. Through our comprehensive analysis using strict online CL settings, a novel unified memory budget protocol, and a proposed category-randomized setting, we reveal significant performance drops in many representative CL methods when subjected to these real-world constraints. Our study underscores the necessity of resource-aware and semantically robust CL strategies, and offers LibContinual as a foundational toolkit for future research in realistic continual learning. The source code is available from \href{https://github.com/RL-VIG/LibContinual}{https://github.com/RL-VIG/LibContinual}.

</details>


### [162] [Direction Finding with Sparse Arrays Based on Variable Window Size Spatial Smoothing](https://arxiv.org/abs/2512.22024)
*Wesley S. Leite,Rodrigo C. de Lamare,Yuriy Zakharov,Wei Liu,Martin Haardt*

Main category: cs.LG

TL;DR: VWS spatial smoothing framework improves DOA estimation for sparse arrays by compressing smoothing aperture, replacing perturbed terms with unperturbed low-rank terms to enhance signal-noise separation while preserving signal subspace.


<details>
  <summary>Details</summary>
Motivation: Coarray-based DOA estimation for sparse linear arrays suffers from performance limitations due to perturbed rank-one outer products in smoothed coarray data, which reduces signal-noise subspace separation.

Method: Proposed VWS-CA-MUSIC and VWS-CA-rMUSIC algorithms use variable window size spatial smoothing to compress smoothing aperture, replacing perturbed rank-one outer products with unperturbed low-rank additional terms while preserving signal subspace span.

Result: Simulations with sparse geometries show significant performance improvements and complexity savings compared to fixed-window coarray MUSIC method, with derived bounds guaranteeing identifiability by limiting compression parameter values.

Conclusion: VWS spatial smoothing framework effectively enhances coarray-based DOA estimation for sparse linear arrays by improving signal-noise separation while reducing computational complexity.

Abstract: In this work, we introduce a variable window size (VWS) spatial smoothing framework that enhances coarray-based direction of arrival (DOA) estimation for sparse linear arrays. By compressing the smoothing aperture, the proposed VWS Coarray MUSIC (VWS-CA-MUSIC) and VWS Coarray root-MUSIC (VWS-CA-rMUSIC) algorithms replace part of the perturbed rank-one outer products in the smoothed coarray data with unperturbed low-rank additional terms, increasing the separation between signal and noise subspaces, while preserving the signal subspace span. We also derive the bounds that guarantees identifiability, by limiting the values that can be assumed by the compression parameter. Simulations with sparse geometries reveal significant performance improvements and complexity savings relative to the fixed-window coarray MUSIC method.

</details>


### [163] [From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation](https://arxiv.org/abs/2512.22031)
*Nagham Osman,Vittorio Lembo,Giovanni Bottegoni,Laura Toni*

Main category: cs.LG

TL;DR: This paper investigates whether generative AI models can replace traditional hit identification in drug discovery by generating hit-like molecules, proposing a specialized evaluation framework and showing promising results with synthesized active compounds.


<details>
  <summary>Details</summary>
Motivation: Traditional hit identification in drug discovery is resource-intensive and costly. While deep learning has enabled generative models for molecular design, replacing the entire drug discovery pipeline remains challenging. The authors aim to test if generative models can specifically replace the hit-like molecule generation step as a standalone task.

Method: The study frames hit-like molecule generation as a standalone task and proposes a tailored evaluation framework with physicochemical, structural, and bioactivity criteria. They benchmark two autoregressive and one diffusion-based generative models across various datasets and training settings, using standard metrics and target-specific docking scores within a multi-stage filtering pipeline.

Result: The models successfully generated valid, diverse, and biologically relevant compounds across multiple targets. Selected GSK-3β hits were synthesized and confirmed active in vitro, demonstrating practical utility. The study also identified limitations in current evaluation metrics and available training data.

Conclusion: Generative models show promise for replacing traditional hit identification workflows by generating hit-like molecules, though current evaluation metrics and training data limitations need addressing. This represents a practical intermediate approach rather than attempting to replace the entire drug discovery pipeline.

Abstract: Hit identification is a critical yet resource-intensive step in the drug discovery pipeline, traditionally relying on high-throughput screening of large compound libraries. Despite advancements in virtual screening, these methods remain time-consuming and costly. Recent progress in deep learning has enabled the development of generative models capable of learning complex molecular representations and generating novel compounds de novo. However, using ML to replace the entire drug-discovery pipeline is highly challenging. In this work, we rather investigate whether generative models can replace one step of the pipeline: hit-like molecule generation. To the best of our knowledge, this is the first study to explicitly frame hit-like molecule generation as a standalone task and empirically test whether generative models can directly support this stage of the drug discovery pipeline. Specifically, we investigate if such models can be trained to generate hit-like molecules, enabling direct incorporation into, or even substitution of, traditional hit identification workflows. We propose an evaluation framework tailored to this task, integrating physicochemical, structural, and bioactivity-related criteria within a multi-stage filtering pipeline that defines the hit-like chemical space. Two autoregressive and one diffusion-based generative models were benchmarked across various datasets and training settings, with outputs assessed using standard metrics and target-specific docking scores. Our results show that these models can generate valid, diverse, and biologically relevant compounds across multiple targets, with a few selected GSK-3$β$ hits synthesized and confirmed active in vitro. We also identify key limitations in current evaluation metrics and available training data.

</details>


### [164] [Unifying Learning Dynamics and Generalization in Transformers Scaling Law](https://arxiv.org/abs/2512.22088)
*Chiwun Yang*

Main category: cs.LG

TL;DR: The paper provides a theoretical foundation for LLM scaling laws by analyzing transformer training dynamics as an ODE system, revealing a phase transition in generalization error decay from exponential to power-law scaling as computational resources increase.


<details>
  <summary>Details</summary>
Motivation: While scaling laws are empirically validated for LLMs, their theoretical foundations remain poorly understood. The paper aims to provide rigorous theoretical analysis of transformer training dynamics under realistic conditions, moving beyond toy models to understand how computational resources affect generalization performance.

Method: Formalizes transformer learning dynamics as an ordinary differential equation (ODE) system and approximates it to kernel behaviors. Analyzes stochastic gradient descent (SGD) training for multi-layer transformers on sequence-to-sequence data with arbitrary data distribution, closely mirroring real-world conditions.

Result: Establishes a theoretical upper bound on excess risk with a distinct phase transition: initial optimization phase shows exponential decay relative to computational cost, while after crossing a resource threshold, generalization error follows power-law decay of Θ(C^{-1/6}). Also derives isolated scaling laws for model size, training time, and dataset size.

Conclusion: The work provides a unified theoretical framework for understanding LLM scaling laws, revealing the phase transition behavior in generalization error decay and offering rigorous analysis of how different computational resources independently govern performance bounds.

Abstract: The scaling law, a cornerstone of Large Language Model (LLM) development, predicts improvements in model performance with increasing computational resources. Yet, while empirically validated, its theoretical underpinnings remain poorly understood. This work formalizes the learning dynamics of transformer-based language models as an ordinary differential equation (ODE) system, then approximates this process to kernel behaviors. Departing from prior toy-model analyses, we rigorously analyze stochastic gradient descent (SGD) training for multi-layer transformers on sequence-to-sequence data with arbitrary data distribution, closely mirroring real-world conditions. Our analysis characterizes the convergence of generalization error to the irreducible risk as computational resources scale with data, especially during the optimization process.
  We establish a theoretical upper bound on excess risk characterized by a distinct phase transition. In the initial optimization phase, the excess risk decays exponentially relative to the computational cost ${\sf C}$. However, once a specific resource allocation threshold is crossed, the system enters a statistical phase, where the generalization error follows a power-law decay of $Θ(\mathsf{C}^{-1/6})$. Beyond this unified framework, our theory derives isolated scaling laws for model size, training time, and dataset size, elucidating how each variable independently governs the upper bounds of generalization.

</details>


### [165] [Why Smooth Stability Assumptions Fail for ReLU Learning](https://arxiv.org/abs/2512.22055)
*Ronald Katende*

Main category: cs.LG

TL;DR: No uniform smoothness-based stability bounds (gradient Lipschitzness, Hessian control) can hold globally for ReLU networks, even in empirically stable settings, requiring nonsmooth-aware stability frameworks.


<details>
  <summary>Details</summary>
Motivation: Modern learning systems stability analyses rely on smoothness assumptions violated by ReLU-type nonlinearities, creating a gap between theoretical stability guarantees and empirical observations of stable training trajectories.

Method: Provide concrete counterexample demonstrating failure of classical stability bounds for ReLU networks, identify minimal generalized derivative condition for meaningful stability restoration, and analyze why smooth ReLU approximations are misleading.

Result: Shows that no uniform smoothness-based stability proxy can hold globally for ReLU networks, even in simple empirically stable settings, necessitating nonsmooth-aware stability frameworks.

Conclusion: Smooth approximations of ReLU are misleading for stability analysis; nonsmooth-aware frameworks are needed to properly analyze stability of ReLU networks and bridge the gap between theory and empirical observations.

Abstract: Stability analyses of modern learning systems are frequently derived under smoothness assumptions that are violated by ReLU-type nonlinearities. In this note, we isolate a minimal obstruction by showing that no uniform smoothness-based stability proxy such as gradient Lipschitzness or Hessian control can hold globally for ReLU networks, even in simple settings where training trajectories appear empirically stable. We give a concrete counterexample demonstrating the failure of classical stability bounds and identify a minimal generalized derivative condition under which stability statements can be meaningfully restored. The result clarifies why smooth approximations of ReLU can be misleading and motivates nonsmooth-aware stability frameworks.

</details>


### [166] [A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting](https://arxiv.org/abs/2512.22101)
*Shuyu Gan,Renxiang Wang,James Mooney,Dongyeop Kang*

Main category: cs.LG

TL;DR: A2P-Vis is a two-part multi-agent pipeline that automates end-to-end data science reporting by generating diverse visualizations and assembling them into coherent, professional reports.


<details>
  <summary>Details</summary>
Motivation: Current AI agents for data science pipelines struggle with generating insightful, diverse visual evidence and assembling it into coherent, professional reports, limiting their real-world usefulness for practitioners.

Method: A two-part multi-agent pipeline: 1) Data Analyzer orchestrates profiling, proposes visualization directions, generates/executes code, filters low-quality figures, and scores insights for depth, correctness, specificity, and actionability. 2) Presenter orders topics, composes chart-grounded narratives from top insights, writes transitions, and revises for clarity.

Result: The system converts raw data into curated materials (charts + vetted insights) and readable narratives without manual intervention, producing publication-ready reports.

Conclusion: By coupling quality-assured analysis with narrative presentation, A2P-Vis operationalizes co-analysis end-to-end, improving the practical usefulness of automated data analysis for real-world applications.

Abstract: Automating end-to-end data science pipeline with AI agents still stalls on two gaps: generating insightful, diverse visual evidence and assembling it into a coherent, professional report. We present A2P-Vis, a two-part, multi-agent pipeline that turns raw datasets into a high-quality data-visualization report. The Data Analyzer orchestrates profiling, proposes diverse visualization directions, generates and executes plotting code, filters low-quality figures with a legibility checker, and elicits candidate insights that are automatically scored for depth, correctness, specificity, depth and actionability. The Presenter then orders topics, composes chart-grounded narratives from the top-ranked insights, writes justified transitions, and revises the document for clarity and consistency, yielding a coherent, publication-ready report. Together, these agents convert raw data into curated materials (charts + vetted insights) and into a readable narrative without manual glue work. We claim that by coupling a quality-assured Analyzer with a narrative Presenter, A2P-Vis operationalizes co-analysis end-to-end, improving the real-world usefulness of automated data analysis for practitioners. For the complete dataset report, please see: https://www.visagent.org/api/output/f2a3486d-2c3b-4825-98d4-5af25a819f56.

</details>


### [167] [Scaling Adversarial Training via Data Selection](https://arxiv.org/abs/2512.22069)
*Youran Ye,Dejin Wang,Ajinkya Bhandare*

Main category: cs.LG

TL;DR: Selective Adversarial Training reduces computational cost by perturbing only critical samples in each minibatch, achieving comparable robustness to full PGD adversarial training with up to 50% less computation.


<details>
  <summary>Details</summary>
Motivation: PGD adversarial training is computationally expensive because all training samples undergo identical iterative optimization, despite contributing unequally to robustness. This inefficiency motivates a more selective approach.

Method: Proposes Selective Adversarial Training with two sample selection criteria: (1) margin-based sampling (prioritizes samples near decision boundaries) and (2) gradient-matching sampling (selects samples whose gradients align with batch optimization direction). Only selected samples are adversarially perturbed, while others are trained cleanly with a mixed objective.

Result: On MNIST and CIFAR-10, achieves robustness comparable to or exceeding full PGD adversarial training while reducing adversarial computation by up to 50%.

Conclusion: Informed sample selection is sufficient for scalable adversarial robustness, demonstrating that not all samples need adversarial perturbation during training to achieve strong robustness.

Abstract: Projected Gradient Descent (PGD) is a strong and widely used first-order adversarial attack, yet its computational cost scales poorly, as all training samples undergo identical iterative inner-loop optimization despite contributing unequally to robustness. Motivated by this inefficiency, we propose \emph{Selective Adversarial Training}, which perturbs only a subset of critical samples in each minibatch. Specifically, we introduce two principled selection criteria: (1) margin-based sampling, which prioritizes samples near the decision boundary, and (2) gradient-matching sampling, which selects samples whose gradients align with the dominant batch optimization direction. Adversarial examples are generated only for the selected subset, while the remaining samples are trained cleanly using a mixed objective. Experiments on MNIST and CIFAR-10 show that the proposed methods achieve robustness comparable to, or even exceeding, full PGD adversarial training, while reducing adversarial computation by up to $50\%$, demonstrating that informed sample selection is sufficient for scalable adversarial robustness.

</details>


### [168] [Explainable Multimodal Regression via Information Decomposition](https://arxiv.org/abs/2512.22102)
*Zhaozhao Ma,Shujian Yu*

Main category: cs.LG

TL;DR: Proposes a multimodal regression framework using Partial Information Decomposition (PID) to quantify modality-specific contributions (unique, redundant, synergistic) with Gaussianity assumptions for analytical computation and conditional independence regularization.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal regression methods lack principled tools to disentangle and quantify individual modality contributions and their interactions, limiting interpretability of multimodal fusion.

Method: Uses Partial Information Decomposition (PID) to decompose modality representations into unique, redundant, and synergistic components. Introduces Gaussianity assumption in latent representations and transformed response variable for analytical PID computation. Derives closed-form conditional independence regularizer to isolate unique information in each modality.

Result: Outperforms state-of-the-art methods on six real-world datasets in both predictive accuracy and interpretability. Enables informed modality selection for efficient inference, demonstrated in brain age prediction from multimodal neuroimaging data.

Conclusion: The proposed PID-based framework provides a principled approach for interpretable multimodal regression with superior performance and practical utility for modality selection.

Abstract: Multimodal regression aims to predict a continuous target from heterogeneous input sources and typically relies on fusion strategies such as early or late fusion. However, existing methods lack principled tools to disentangle and quantify the individual contributions of each modality and their interactions, limiting the interpretability of multimodal fusion. We propose a novel multimodal regression framework grounded in Partial Information Decomposition (PID), which decomposes modality-specific representations into unique, redundant, and synergistic components. The basic PID framework is inherently underdetermined. To resolve this, we introduce inductive bias by enforcing Gaussianity in the joint distribution of latent representations and the transformed response variable (after inverse normal transformation), thereby enabling analytical computation of the PID terms. Additionally, we derive a closed-form conditional independence regularizer to promote the isolation of unique information within each modality. Experiments on six real-world datasets, including a case study on large-scale brain age prediction from multimodal neuroimaging data, demonstrate that our framework outperforms state-of-the-art methods in both predictive accuracy and interpretability, while also enabling informed modality selection for efficient inference. Implementation is available at https://github.com/zhaozhaoma/PIDReg.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [169] [A Graph-Augmented knowledge Distillation based Dual-Stream Vision Transformer with Region-Aware Attention for Gastrointestinal Disease Classification with Explainable AI](https://arxiv.org/abs/2512.21372)
*Md Assaduzzaman,Nushrat Jahan Oyshi,Eram Mahamud*

Main category: eess.IV

TL;DR: Hybrid dual-stream teacher-student framework using Swin Transformer and Vision Transformer achieves near-perfect GI disease classification with interpretable, efficient Tiny-ViT student model.


<details>
  <summary>Details</summary>
Motivation: Accurate GI disease classification from endoscopic/histopathological imagery is challenging due to large data volumes and subtle visual variations between disease classes. Need for efficient yet accurate diagnostic tools suitable for clinical settings.

Method: Teacher-student knowledge distillation framework: high-capacity teacher combines Swin Transformer (global context) and Vision Transformer (local features). Student is compact Tiny-ViT that inherits teacher's knowledge via soft-label distillation. Uses two balanced Wireless Capsule Endoscopy datasets.

Result: Achieved remarkable performance: 0.9978 and 0.9928 accuracy on two datasets, average AUC of 1.0000 (near-perfect discrimination). Tiny-ViT maintained comparable performance to teacher with reduced computational complexity and faster inference.

Conclusion: Framework provides robust, interpretable, scalable solution for AI-assisted GI disease diagnosis. Suitable for resource-constrained clinical environments and paves way for intelligent endoscopic screening compatible with clinical practicality.

Abstract: The accurate classification of gastrointestinal diseases from endoscopic and histopathological imagery remains a significant challenge in medical diagnostics, mainly due to the vast data volume and subtle variation in inter-class visuals. This study presents a hybrid dual-stream deep learning framework built on teacher-student knowledge distillation, where a high-capacity teacher model integrates the global contextual reasoning of a Swin Transformer with the local fine-grained feature extraction of a Vision Transformer. The student network was implemented as a compact Tiny-ViT structure that inherits the teacher's semantic and morphological knowledge via soft-label distillation, achieving a balance between efficiency and diagnostic accuracy. Two carefully curated Wireless Capsule Endoscopy datasets, encompassing major GI disease classes, were employed to ensure balanced representation and prevent inter-sample bias. The proposed framework achieved remarkable performance with accuracies of 0.9978 and 0.9928 on Dataset 1 and Dataset 2 respectively, and an average AUC of 1.0000, signifying near-perfect discriminative capability. Interpretability analyses using Grad-CAM, LIME, and Score-CAM confirmed that the model's predictions were grounded in clinically significant tissue regions and pathologically relevant morphological cues, validating the framework's transparency and reliability. The Tiny-ViT demonstrated diagnostic performance with reduced computational complexity comparable to its transformer-based teacher while delivering faster inference, making it suitable for resource-constrained clinical environments. Overall, the proposed framework provides a robust, interpretable, and scalable solution for AI-assisted GI disease diagnosis, paving the way toward future intelligent endoscopic screening that is compatible with clinical practicality.

</details>


### [170] [Enabling Ultra-Fast Cardiovascular Imaging Across Heterogeneous Clinical Environments with a Generalist Foundation Model and Multimodal Database](https://arxiv.org/abs/2512.21652)
*Zi Wang,Mingkai Huang,Zhang Shi,Hongjie Hu,Lan Lan,Hui Zhang,Yan Li,Xi Hu,Qing Lu,Zongming Zhu,Qiong Yao,Yuxiang Dai,Fanwen Wang,Yinzhe Wu,Jun Lyu,Qianqian Gao,Guangming Xu,Zhenxuan Zhang,Haosen Zhang,Qing Li,Guangming Wang,Tianxing He,Lizhen Lan,Siyue Li,Le Xue,Mengting Sun,Yuntong Lyu,Junpu Hu,Jiayu Zhu,Rizwan Ahmad,Zhengyu Bu,Xianling Qian,Guanke Cai,Ruiyu Cao,Weirui Cai,Chang Xu,Yuyang Ren,Feidan Yu,Siying Ma,Ziqiang Xu,Xinran Chen,Sha Hua,Daniel Kim,Yajing Zhang,Chen Ouyang,Wenjia Bai,Jing Qin,Yucheng Yang,Daniel Rueckert,He Wang,Qian Tao,Claudia Prieto,Michael Markl,Alistair Young,Lianming Wu,Shuo Wang,Chen Qin,Mengsu Zeng,Xihong Hu,Haibo Xu,Xiaobo Qu,Hao Li,Guang Yang,Chengyan Wang*

Main category: eess.IV

TL;DR: CardioMM is a generalist reconstruction foundation model for ultra-fast cardiovascular MRI that achieves 24x acceleration while preserving diagnostic quality, trained on the largest multimodal CMR database (MMCMR-427K) with 427K k-space samples.


<details>
  <summary>Details</summary>
Motivation: Clinical adoption of cardiovascular MRI is limited by long scan times and heterogeneity across medical environments, creating an urgent need for a generalist reconstruction model that can adapt across diverse imaging scenarios and serve as foundation for downstream analyses.

Method: Created MMCMR-427K database (427,465 multi-coil k-space samples with metadata from 13 international centers), then developed CardioMM foundation model that unifies semantic contextual understanding with physics-informed data consistency for robust reconstructions across varied scanners, protocols, and patient presentations.

Result: CardioMM achieves state-of-the-art performance in internal centers and strong zero-shot generalization to unseen external settings, reliably preserving key cardiac phenotypes, quantitative biomarkers, and diagnostic quality even at 24x acceleration, enabling substantial increase in CMR throughput without compromising clinical integrity.

Conclusion: The open-access MMCMR-427K database and CardioMM framework establish a scalable pathway toward high-throughput, high-quality, and clinically accessible cardiovascular imaging by addressing key bottlenecks in CMR adoption.

Abstract: Multimodal cardiovascular magnetic resonance (CMR) imaging provides comprehensive and non-invasive insights into cardiovascular disease (CVD) diagnosis and underlying mechanisms. Despite decades of advancements, its widespread clinical adoption remains constrained by prolonged scan times and heterogeneity across medical environments. This underscores the urgent need for a generalist reconstruction foundation model for ultra-fast CMR imaging, one capable of adapting across diverse imaging scenarios and serving as the essential substrate for all downstream analyses. To enable this goal, we curate MMCMR-427K, the largest and most comprehensive multimodal CMR k-space database to date, comprising 427,465 multi-coil k-space data paired with structured metadata across 13 international centers, 12 CMR modalities, 15 scanners, and 17 CVD categories in populations across three continents. Building on this unprecedented resource, we introduce CardioMM, a generalist reconstruction foundation model capable of dynamically adapting to heterogeneous fast CMR imaging scenarios. CardioMM unifies semantic contextual understanding with physics-informed data consistency to deliver robust reconstructions across varied scanners, protocols, and patient presentations. Comprehensive evaluations demonstrate that CardioMM achieves state-of-the-art performance in the internal centers and exhibits strong zero-shot generalization to unseen external settings. Even at imaging acceleration up to 24x, CardioMM reliably preserves key cardiac phenotypes, quantitative myocardial biomarkers, and diagnostic image quality, enabling a substantial increase in CMR examination throughput without compromising clinical integrity. Together, our open-access MMCMR-427K database and CardioMM framework establish a scalable pathway toward high-throughput, high-quality, and clinically accessible cardiovascular imaging.

</details>


### [171] [RT-Focuser: A Real-Time Lightweight Model for Edge-side Image Deblurring](https://arxiv.org/abs/2512.21975)
*Zhuoyu Wu,Wenhui Ou,Qiawei Zheng,Jiayan Yang,Quanjun Wang,Wenqi Fang,Zheng Wang,Yongkui Yang,Heshan Li*

Main category: eess.IV

TL;DR: RT-Focuser is a lightweight U-shaped network for real-time motion deblurring that achieves 30.67 dB PSNR with only 5.85M parameters and 15.76 GMACs, running at 140+ FPS on GPU and mobile devices.


<details>
  <summary>Details</summary>
Motivation: Motion blur from camera or object movement degrades image quality and poses challenges for real-time applications like autonomous driving, UAV perception, and medical imaging that require fast, efficient deblurring solutions.

Method: U-shaped network with three key components: Lightweight Deblurring Block (LD) for edge-aware feature extraction, Multi-Level Integrated Aggregation module (MLIA) for encoder integration, and Cross-source Fusion Block (X-Fuse) for progressive decoder refinement.

Result: Achieves 30.67 dB PSNR with only 5.85M parameters and 15.76 GMACs, runs 6ms per frame on GPU and mobile devices, exceeding 140 FPS on both platforms.

Conclusion: RT-Focuser demonstrates strong potential for edge deployment with excellent speed-accuracy balance, making it suitable for real-time applications requiring efficient motion deblurring.

Abstract: Motion blur caused by camera or object movement severely degrades image quality and poses challenges for real-time applications such as autonomous driving, UAV perception, and medical imaging. In this paper, a lightweight U-shaped network tailored for real-time deblurring is presented and named RT-Focuser. To balance speed and accuracy, we design three key components: Lightweight Deblurring Block (LD) for edge-aware feature extraction, Multi-Level Integrated Aggregation module (MLIA) for encoder integration, and Cross-source Fusion Block (X-Fuse) for progressive decoder refinement. Trained on a single blurred input, RT-Focuser achieves 30.67 dB PSNR with only 5.85M parameters and 15.76 GMACs. It runs 6ms per frame on GPU and mobile, exceeds 140 FPS on both, showing strong potential for deployment on the edge. The official code and usage are available on: https://github.com/ReaganWu/RT-Focuser.

</details>


### [172] [The Color-Clinical Decoupling: Why Perceptual Calibration Fails Clinical Biomarkers in Smartphone Dermatology](https://arxiv.org/abs/2512.21988)
*Sungwoo Kang*

Main category: eess.IV

TL;DR: Color calibration reduces perceptual color error but fails to ensure reliable clinical biomarker measurements across devices, especially for underrepresented skin phototypes, due to "color-clinical decoupling" where anatomical variance outweighs device effects.


<details>
  <summary>Details</summary>
Motivation: Smartphone-based tele-dermatology assumes colorimetric calibration ensures clinical reliability, but this remains untested for underrepresented skin phototypes (Fitzpatrick III-IV). The study investigates whether standard calibration translates to reliable clinical biomarkers.

Method: Analyzed 43,425 images from 965 Korean subjects (Fitzpatrick III-IV) across DSLR, tablet, and smartphone devices. Used Linear Color Correction Matrix (CCM) normalization and evaluated color error (Delta E) and biomarker reliability (Individual Typology Angle - ITA and Melanin Index) using inter-device agreement (ICC).

Result: Color calibration reduced color error by 67-77% (Delta E < 2.3), achieving near-clinical accuracy. However, biomarker reliability was poor: ITA showed poor inter-device agreement (ICC = 0.40) while Melanin Index achieved good agreement (ICC = 0.77). Facial region accounted for 25.2% of color variance (3.6x greater than device effects at 7.0%).

Conclusion: Current colorimetric standards are insufficient for clinical-grade biomarker extraction due to "color-clinical decoupling" phenomenon. Single-patch calibration is inadequate; region-aware protocols are necessary for mobile dermatology, especially given anatomical variance outweighs device effects.

Abstract: Smartphone-based tele-dermatology assumes that colorimetric calibration ensures clinical reliability, yet this remains untested for underrepresented skin phototypes. We investigated whether standard calibration translates to reliable clinical biomarkers using 43,425 images from 965 Korean subjects (Fitzpatrick III-IV) across DSLR, tablet, and smartphone devices. While Linear Color Correction Matrix (CCM) normalization reduced color error by 67-77% -- achieving near-clinical accuracy (Delta E < 2.3) -- this success did not translate to biomarker reliability.
  We identify a phenomenon termed "color-clinical decoupling": despite perceptual accuracy, the Individual Typology Angle (ITA) showed poor inter-device agreement (ICC = 0.40), while the Melanin Index achieved good agreement (ICC = 0.77). This decoupling is driven by the ITA formula's sensitivity to b* channel noise and is further compounded by anatomical variance. Facial region accounts for 25.2% of color variance -- 3.6x greater than device effects (7.0%) -- challenging the efficacy of single-patch calibration. Our results demonstrate that current colorimetric standards are insufficient for clinical-grade biomarker extraction, necessitating region-aware protocols for mobile dermatology.

</details>
