<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 53]
- [eess.SP](#eess.SP) [Total: 15]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.LG](#cs.LG) [Total: 51]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.RO](#cs.RO) [Total: 15]
- [cs.IT](#cs.IT) [Total: 16]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GR3EN: Generative Relighting for 3D Environments](https://arxiv.org/abs/2601.16272)
*Xiaoyan Xing,Philipp Henzler,Junhwa Hur,Runze Li,Jonathan T. Barron,Pratul P. Srinivasan,Dor Verbin*

Main category: cs.CV

TL;DR: A method for relighting 3D reconstructions of room-scale environments by distilling outputs from a video-to-video relighting diffusion model into 3D reconstructions, avoiding difficult inverse rendering problems.


<details>
  <summary>Details</summary>
Motivation: Existing 3D scene relighting solutions often require solving under-determined or ill-conditioned inverse rendering problems, making them unable to produce high-quality results on complex real-world scenes. Current generative diffusion approaches are limited to 2D image/video relighting or individual 3D objects.

Method: The approach distills outputs from a video-to-video relighting diffusion model into 3D reconstructions, sidestepping the need to solve difficult inverse rendering problems. This enables controllable 3D relighting of room-scale scenes.

Result: The method produces a flexible system that can relight 3D reconstructions of complex real-world scenes. Validation on both synthetic and real-world datasets shows it can faithfully render novel views of scenes under new lighting conditions.

Conclusion: The approach successfully enables high-quality 3D relighting of room-scale environments by leveraging diffusion models while avoiding traditional inverse rendering challenges, representing an advancement over existing 2D and object-level relighting techniques.

Abstract: We present a method for relighting 3D reconstructions of large room-scale environments. Existing solutions for 3D scene relighting often require solving under-determined or ill-conditioned inverse rendering problems, and are as such unable to produce high-quality results on complex real-world scenes. Though recent progress in using generative image and video diffusion models for relighting has been promising, these techniques are either limited to 2D image and video relighting or 3D relighting of individual objects. Our approach enables controllable 3D relighting of room-scale scenes by distilling the outputs of a video-to-video relighting diffusion model into a 3D reconstruction. This side-steps the need to solve a difficult inverse rendering problem, and results in a flexible system that can relight 3D reconstructions of complex real-world scenes. We validate our approach on both synthetic and real-world datasets to show that it can faithfully render novel views of scenes under new lighting conditions.

</details>


### [2] [Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory](https://arxiv.org/abs/2601.16296)
*Dohun Lee,Chun-Hao Paul Huang,Xuelin Chen,Jong Chul Ye,Duygu Ceylan,Hyeonho Jeong*

Main category: cs.CV

TL;DR: Memory-V2V is a framework that adds explicit memory to video-to-video diffusion models to maintain cross-consistency across multiple rounds of iterative video editing.


<details>
  <summary>Details</summary>
Motivation: Current video editors struggle to maintain consistency across sequential edits in multi-turn video editing scenarios, where users refine results through multiple rounds of interaction.

Method: Augments existing video-to-video models with explicit memory using accurate retrieval and dynamic tokenization strategies from a cache of previously edited videos. Includes a learnable token compressor within the DiT backbone to reduce redundancy and computational overhead.

Result: Achieves significantly better cross-consistency with minimal computational overhead (30% speedup), while maintaining or improving task-specific performance on video novel view synthesis and text-conditioned long video editing tasks.

Conclusion: Memory-V2V effectively addresses cross-consistency in multi-turn video editing through explicit memory mechanisms, making iterative video editing more practical and efficient.

Abstract: Recent foundational video-to-video diffusion models have achieved impressive results in editing user provided videos by modifying appearance, motion, or camera movement. However, real-world video editing is often an iterative process, where users refine results across multiple rounds of interaction. In this multi-turn setting, current video editors struggle to maintain cross-consistency across sequential edits. In this work, we tackle, for the first time, the problem of cross-consistency in multi-turn video editing and introduce Memory-V2V, a simple, yet effective framework that augments existing video-to-video models with explicit memory. Given an external cache of previously edited videos, Memory-V2V employs accurate retrieval and dynamic tokenization strategies to condition the current editing step on prior results. To further mitigate redundancy and computational overhead, we propose a learnable token compressor within the DiT backbone that compresses redundant conditioning tokens while preserving essential visual cues, achieving an overall speedup of 30%. We validate Memory-V2V on challenging tasks including video novel view synthesis and text-conditioned long video editing. Extensive experiments show that Memory-V2V produces videos that are significantly more cross-consistent with minimal computational overhead, while maintaining or even improving task-specific performance over state-of-the-art baselines. Project page: https://dohunlee1.github.io/MemoryV2V

</details>


### [3] [FeTTL: Federated Template and Task Learning for Multi-Institutional Medical Imaging](https://arxiv.org/abs/2601.16302)
*Abhijeet Parida,Antonia Alomar,Zhifan Jiang,Pooneh Roshanitabrizi,Austin Tapp,Ziyue Xu,Syed Muhammad Anwar,Maria J. Ledesma-Carbayo,Holger R. Roth,Marius George Linguraru*

Main category: cs.CV

TL;DR: FeTTL is a federated learning framework that learns a global template and task model to align data distributions across medical institutions, significantly outperforming state-of-the-art methods on retinal and histopathology tasks.


<details>
  <summary>Details</summary>
Motivation: Federated learning in medical imaging suffers from domain shifts and data heterogeneity due to variations in acquisition protocols, scanner types, and patient populations across institutions, leading to degraded model performance.

Method: Federated Template and Task Learning (FeTTL) framework that jointly learns a global template together with a task model to align data distributions among clients in federated environments.

Result: FeTTL significantly outperforms state-of-the-art federated learning baselines (p-values <0.002) for optical disc segmentation and metastasis classification from multi-institutional data, demonstrating the importance of jointly learning template and task.

Conclusion: FeTTL offers a principled and extensible solution for mitigating distribution shifts in federated learning, supporting robust model deployment in real-world, multi-institutional medical environments.

Abstract: Federated learning enables collaborative model training across geographically distributed medical centers while preserving data privacy. However, domain shifts and heterogeneity in data often lead to a degradation in model performance. Medical imaging applications are particularly affected by variations in acquisition protocols, scanner types, and patient populations. To address these issues, we introduce Federated Template and Task Learning (FeTTL), a novel framework designed to harmonize multi-institutional medical imaging data in federated environments. FeTTL learns a global template together with a task model to align data distributions among clients. We evaluated FeTTL on two challenging and diverse multi-institutional medical imaging tasks: retinal fundus optical disc segmentation and histopathological metastasis classification. Experimental results show that FeTTL significantly outperforms the state-of-the-art federated learning baselines (p-values <0.002) for optical disc segmentation and classification of metastases from multi-institutional data. Our experiments further highlight the importance of jointly learning the template and the task. These findings suggest that FeTTL offers a principled and extensible solution for mitigating distribution shifts in federated learning, supporting robust model deployment in real-world, multi-institutional environments.

</details>


### [4] [Where is the multimodal goal post? On the Ability of Foundation Models to Recognize Contextually Important Moments](https://arxiv.org/abs/2601.16333)
*Aditya K Surikuchi,Raquel Fernández,Sandro Pezzelle*

Main category: cs.CV

TL;DR: Models struggle to identify important sub-events in football videos, performing near chance level despite using state-of-the-art multimodal architectures.


<details>
  <summary>Details</summary>
Motivation: Foundation models are widely used for generating language from temporally-ordered multimodal events, but their ability to identify important sub-events - a fundamental prerequisite for narration and summarization - remains unstudied, particularly in complex domains like football games.

Method: Constructed a new dataset using human preferences implicit in football game highlight reels (no additional annotation costs), evaluated several state-of-the-art multimodal models on their ability to distinguish between important and non-important sub-events, and conducted analyses beyond standard metrics.

Result: Models perform not far from chance level, tend to rely on a single dominant modality, and are ineffective at synthesizing necessary information from multiple sources.

Conclusion: Highlights the need for modular architectures that handle sample-level heterogeneity in multimodal data and complementary training procedures that maximize cross-modal synergy.

Abstract: Foundation models are used for many real-world applications involving language generation from temporally-ordered multimodal events. In this work, we study the ability of models to identify the most important sub-events in a video, which is a fundamental prerequisite for narrating or summarizing multimodal events. Specifically, we focus on football games and evaluate models on their ability to distinguish between important and non-important sub-events in a game. To this end, we construct a new dataset by leveraging human preferences for importance implicit in football game highlight reels, without any additional annotation costs. Using our dataset, which we will publicly release to the community, we compare several state-of-the-art multimodal models and show that they are not far from chance level performance. Analyses of models beyond standard evaluation metrics reveal their tendency to rely on a single dominant modality and their ineffectiveness in synthesizing necessary information from multiple sources. Our findings underline the importance of modular architectures that can handle sample-level heterogeneity in multimodal data and the need for complementary training procedures that can maximize cross-modal synergy.

</details>


### [5] [Coarse-to-Fine Non-rigid Multi-modal Image Registration for Historical Panel Paintings based on Crack Structures](https://arxiv.org/abs/2601.16348)
*Aline Sindel,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: A coarse-to-fine non-rigid registration method using craquelure patterns for aligning multi-modal images of historical panel paintings, combining CNN keypoint detection with graph neural network matching and multi-level refinement.


<details>
  <summary>Details</summary>
Motivation: Manual pixel-wise alignment of multi-modal images (visual light, infrared, UV, x-ray, macro) for art analysis is laborious and imprecise. Automated registration is needed to handle challenges like varying resolutions, large sizes, non-rigid distortions, and modality-dependent content.

Method: Coarse-to-fine non-rigid registration using craquelure patterns as features. CNN for joint keypoint detection/description, graph neural network for patch-based descriptor matching, homography reprojection error filtering. Novel multi-level keypoint refinement for mixed-resolution images up to highest resolution.

Result: Created multi-modal dataset with extensive keypoint annotations and large test set across five domains. Ablation study confirms effectiveness of all refinement modules. Proposed method achieves best registration results compared to competing keypoint/dense matching and refinement approaches.

Conclusion: The method successfully automates multi-modal image registration for historical paintings using craquelure patterns, overcoming challenges of varying resolutions and non-rigid distortions while outperforming existing techniques.

Abstract: Art technological investigations of historical panel paintings rely on acquiring multi-modal image data, including visual light photography, infrared reflectography, ultraviolet fluorescence photography, x-radiography, and macro photography. For a comprehensive analysis, the multi-modal images require pixel-wise alignment, which is still often performed manually. Multi-modal image registration can reduce this laborious manual work, is substantially faster, and enables higher precision. Due to varying image resolutions, huge image sizes, non-rigid distortions, and modality-dependent image content, registration is challenging. Therefore, we propose a coarse-to-fine non-rigid multi-modal registration method efficiently relying on sparse keypoints and thin-plate-splines. Historical paintings exhibit a fine crack pattern, called craquelure, on the paint layer, which is captured by all image systems and is well-suited as a feature for registration. In our one-stage non-rigid registration approach, we employ a convolutional neural network for joint keypoint detection and description based on the craquelure and a graph neural network for descriptor matching in a patch-based manner, and filter matches based on homography reprojection errors in local areas. For coarse-to-fine registration, we introduce a novel multi-level keypoint refinement approach to register mixed-resolution images up to the highest resolution. We created a multi-modal dataset of panel paintings with a high number of keypoint annotations, and a large test set comprising five multi-modal domains and varying image resolutions. The ablation study demonstrates the effectiveness of all modules of our refinement method. Our proposed approaches achieve the best registration results compared to competing keypoint and dense matching methods and refinement methods.

</details>


### [6] [Cognitively-Inspired Tokens Overcome Egocentric Bias in Multimodal Models](https://arxiv.org/abs/2601.16378)
*Bridget Leonard,Scott O. Murray*

Main category: cs.CV

TL;DR: Perspective tokens improve multimodal models' spatial reasoning by encoding orientation information, enabling allocentric perspective-taking that reduces egocentric bias.


<details>
  <summary>Details</summary>
Motivation: Current multimodal language models (MLMs) perform well on semantic vision-language tasks but fail at spatial reasoning requiring perspective-taking, showing persistent egocentric bias. This raises questions about whether models support allocentric reasoning.

Method: Introduce perspective tokens - specialized embeddings that encode orientation through either (1) embodied body-keypoint cues or (2) abstract representations supporting mental rotation. Integrate these tokens into LLaVA-1.5-13B model.

Result: Perspective tokens improve accuracy on level-2 visual perspective-taking tasks across synthetic and naturalistic benchmarks (Isle Bricks V2, COCO, 3DSRBench). Rotation-based tokens generalize to non-human reference agents. Fine-tuning enhances latent orientation sensitivity already present in base models.

Conclusion: Embedding cognitively grounded spatial structure directly into token space provides lightweight, model-agnostic mechanism for perspective-taking and more human-like spatial reasoning. MLMs contain precursors of allocentric reasoning but lack appropriate internal structure.

Abstract: Multimodal language models (MLMs) perform well on semantic vision-language tasks but fail at spatial reasoning that requires adopting another agent's visual perspective. These errors reflect a persistent egocentric bias and raise questions about whether current models support allocentric reasoning. Inspired by human spatial cognition, we introduce perspective tokens, specialized embeddings that encode orientation through either (1) embodied body-keypoint cues or (2) abstract representations supporting mental rotation. Integrating these tokens into LLaVA-1.5-13B yields performance on level-2 visual perspective-taking tasks. Across synthetic and naturalistic benchmarks (Isle Bricks V2, COCO, 3DSRBench), perspective tokens improve accuracy, with rotation-based tokens generalizing to non-human reference agents. Representational analyses reveal that fine-tuning enhances latent orientation sensitivity already present in the base model, suggesting that MLMs contain precursors of allocentric reasoning but lack appropriate internal structure. Overall, embedding cognitively grounded spatial structure directly into token space provides a lightweight, model-agnostic mechanism for perspective-taking and more human-like spatial reasoning.

</details>


### [7] [VTFusion: A Vision-Text Multimodal Fusion Network for Few-Shot Anomaly Detection](https://arxiv.org/abs/2601.16381)
*Yuxin Jiang,Yunkang Cao,Yuqi Cheng,Yiheng Zhang,Weiming Shen*

Main category: cs.CV

TL;DR: VTFusion is a vision-text multimodal framework for Few-Shot Anomaly Detection that addresses domain gaps and semantic misalignment between modalities through adaptive feature extractors and dedicated fusion modules.


<details>
  <summary>Details</summary>
Motivation: Current FSAD methods rely on pre-trained features from natural scenes, missing domain-specific industrial semantics, and use superficial fusion strategies that don't address semantic misalignment between visual and textual modalities, compromising robustness.

Method: 1) Adaptive feature extractors for both image and text modalities to learn task-specific representations and bridge domain gaps, augmented by synthetic anomaly generation. 2) Multimodal prediction fusion module with cross-modal information exchange and segmentation network for pixel-level anomaly maps.

Result: Achieved image-level AUROCs of 96.8% (MVTec AD) and 86.2% (VisA) in 2-shot scenarios, plus 93.5% AUPRO on a real-world industrial automotive plastic parts dataset.

Conclusion: VTFusion significantly advances FSAD performance by effectively bridging domain gaps and addressing semantic misalignment between modalities, demonstrating strong practical applicability in demanding industrial inspection scenarios.

Abstract: Few-Shot Anomaly Detection (FSAD) has emerged as a critical paradigm for identifying irregularities using scarce normal references. While recent methods have integrated textual semantics to complement visual data, they predominantly rely on features pre-trained on natural scenes, thereby neglecting the granular, domain-specific semantics essential for industrial inspection. Furthermore, prevalent fusion strategies often resort to superficial concatenation, failing to address the inherent semantic misalignment between visual and textual modalities, which compromises robustness against cross-modal interference. To bridge these gaps, this study proposes VTFusion, a vision-text multimodal fusion framework tailored for FSAD. The framework rests on two core designs. First, adaptive feature extractors for both image and text modalities are introduced to learn task-specific representations, bridging the domain gap between pre-trained models and industrial data; this is further augmented by generating diverse synthetic anomalies to enhance feature discriminability. Second, a dedicated multimodal prediction fusion module is developed, comprising a fusion block that facilitates rich cross-modal information exchange and a segmentation network that generates refined pixel-level anomaly maps under multimodal guidance. VTFusion significantly advances FSAD performance, achieving image-level AUROCs of 96.8% and 86.2% in the 2-shot scenario on the MVTec AD and VisA datasets, respectively. Furthermore, VTFusion achieves an AUPRO of 93.5% on a real-world dataset of industrial automotive plastic parts introduced in this paper, further demonstrating its practical applicability in demanding industrial scenarios.

</details>


### [8] [ResAgent: Entropy-based Prior Point Discovery and Visual Reasoning for Referring Expression Segmentation](https://arxiv.org/abs/2601.16394)
*Yihao Wang,Jusheng Zhang,Ziyi Tang,Keze Wang,Meng Yang*

Main category: cs.CV

TL;DR: EBD-VBR framework improves Referring Expression Segmentation by using entropy-based point discovery and vision-based reasoning to address limitations of MLLM-based approaches.


<details>
  <summary>Details</summary>
Motivation: Existing RES methods suffer from two key limitations: 1) coarse bounding boxes from MLLMs lead to redundant/non-discriminative point prompts, and 2) textual coordinate reasoning is unreliable for distinguishing targets from visually similar distractors.

Method: Proposes EBD-VBR framework with two components: Entropy-Based Point Discovery (EBD) that identifies high-information candidate points by modeling spatial uncertainty within bounding boxes, and Vision-Based Reasoning (VBR) that verifies point correctness through joint visual-semantic alignment instead of text-only coordinate inference. Uses coarse-to-fine workflow: bounding box initialization, entropy-guided point discovery, vision-based validation, and mask decoding.

Result: Achieves new state-of-the-art performance across all four benchmark datasets: RefCOCO, RefCOCO+, RefCOCOg, and ReasonSeg.

Conclusion: EBD-VBR effectively generates accurate and semantically grounded segmentation masks with minimal prompts, addressing key limitations of existing MLLM-based RES approaches through information-theoretic point selection and robust vision-based validation.

Abstract: Referring Expression Segmentation (RES) is a core vision-language segmentation task that enables pixel-level understanding of targets via free-form linguistic expressions, supporting critical applications such as human-robot interaction and augmented reality. Despite the progress of Multimodal Large Language Model (MLLM)-based approaches, existing RES methods still suffer from two key limitations: first, the coarse bounding boxes from MLLMs lead to redundant or non-discriminative point prompts; second, the prevalent reliance on textual coordinate reasoning is unreliable, as it fails to distinguish targets from visually similar distractors. To address these issues, we propose \textbf{\model}, a novel RES framework integrating \textbf{E}ntropy-\textbf{B}ased Point \textbf{D}iscovery (\textbf{EBD}) and \textbf{V}ision-\textbf{B}ased \textbf{R}easoning (\textbf{VBR}). Specifically, EBD identifies high-information candidate points by modeling spatial uncertainty within coarse bounding boxes, treating point selection as an information maximization process. VBR verifies point correctness through joint visual-semantic alignment, abandoning text-only coordinate inference for more robust validation. Built on these components, \model implements a coarse-to-fine workflow: bounding box initialization, entropy-guided point discovery, vision-based validation, and mask decoding. Extensive evaluations on four benchmark datasets (RefCOCO, RefCOCO+, RefCOCOg, and ReasonSeg) demonstrate that \model achieves new state-of-the-art performance across all four benchmarks, highlighting its effectiveness in generating accurate and semantically grounded segmentation masks with minimal prompts.

</details>


### [9] [A Cosine Network for Image Super-Resolution](https://arxiv.org/abs/2601.16413)
*Chunwei Tian,Chengyuan Zhang,Bob Zhang,Zhiwu Li,C. L. Philip Chen,David Zhang*

Main category: cs.CV

TL;DR: CSRNet: A cosine network for image super-resolution using odd/even heterogeneous blocks and cosine annealing for improved structural information extraction and training optimization.


<details>
  <summary>Details</summary>
Motivation: While deep CNNs can extract hierarchical structural information for image super-resolution, preserving the effectiveness of this structural information is crucial. The paper aims to address the limitations of homologous structural information and improve training optimization.

Method: Proposes CSRNet with: 1) Odd and even heterogeneous blocks to extract complementary structural information by enlarging architectural differences; 2) Combination of linear and non-linear structural information to enhance robustness; 3) Cosine annealing mechanism with warm restarts to optimize training and avoid local minima.

Result: Experimental results show that CSRNet is competitive with state-of-the-art methods in image super-resolution.

Conclusion: The proposed CSRNet effectively extracts complementary structural information through heterogeneous blocks and optimizes training with cosine annealing, achieving competitive performance in image super-resolution tasks.

Abstract: Deep convolutional neural networks can use hierarchical information to progressively extract structural information to recover high-quality images. However, preserving the effectiveness of the obtained structural information is important in image super-resolution. In this paper, we propose a cosine network for image super-resolution (CSRNet) by improving a network architecture and optimizing the training strategy. To extract complementary homologous structural information, odd and even heterogeneous blocks are designed to enlarge the architectural differences and improve the performance of image super-resolution. Combining linear and non-linear structural information can overcome the drawback of homologous information and enhance the robustness of the obtained structural information in image super-resolution. Taking into account the local minimum of gradient descent, a cosine annealing mechanism is used to optimize the training procedure by performing warm restarts and adjusting the learning rate. Experimental results illustrate that the proposed CSRNet is competitive with state-of-the-art methods in image super-resolution.

</details>


### [10] [DCCS-Det: Directional Context and Cross-Scale-Aware Detector for Infrared Small Target](https://arxiv.org/abs/2601.16428)
*Shuying Li,Qiang Ma,San Zhang,Chuang Yang*

Main category: cs.CV

TL;DR: DCCS-Det is a new infrared small target detector that uses dual-stream saliency enhancement and latent-aware semantic extraction to improve target-background discrimination and feature quality.


<details>
  <summary>Details</summary>
Motivation: Existing IRSTD methods have two main problems: 1) inadequate joint modeling of local-global features, harming target-background discrimination, and 2) feature redundancy and semantic dilution, degrading target representation quality.

Method: Proposes DCCS-Det with two key components: 1) Dual-stream Saliency Enhancement (DSE) block that integrates localized perception with direction-aware context aggregation, and 2) Latent-aware Semantic Extraction and Aggregation (LaSEA) module that uses cross-scale feature extraction and random pooling sampling to mitigate feature degradation.

Result: Extensive experiments show DCCS-Det achieves state-of-the-art detection accuracy with competitive efficiency across multiple datasets. Ablation studies validate the contributions of DSE and LaSEA in improving target perception and feature representation.

Conclusion: DCCS-Det effectively addresses the limitations of existing IRSTD methods by better capturing spatial dependencies and enhancing discriminative features while suppressing noise in complex scenarios.

Abstract: Infrared small target detection (IRSTD) is critical for applications like remote sensing and surveillance, which aims to identify small, low-contrast targets against complex backgrounds. However, existing methods often struggle with inadequate joint modeling of local-global features (harming target-background discrimination) or feature redundancy and semantic dilution (degrading target representation quality). To tackle these issues, we propose DCCS-Det (Directional Context and Cross-Scale Aware Detector for Infrared Small Target), a novel detector that incorporates a Dual-stream Saliency Enhancement (DSE) block and a Latent-aware Semantic Extraction and Aggregation (LaSEA) module. The DSE block integrates localized perception with direction-aware context aggregation to help capture long-range spatial dependencies and local details. On this basis, the LaSEA module mitigates feature degradation via cross-scale feature extraction and random pooling sampling strategies, enhancing discriminative features and suppressing noise. Extensive experiments show that DCCS-Det achieves state-of-the-art detection accuracy with competitive efficiency across multiple datasets. Ablation studies further validate the contributions of DSE and LaSEA in improving target perception and feature representation under complex scenarios. \href{https://huggingface.co/InPeerReview/InfraredSmallTargetDetection-IRSTD.DCCS}{DCCS-Det Official Code is Available Here!}

</details>


### [11] [AlphaFace: High Fidelity and Real-time Face Swapper Robust to Facial Pose](https://arxiv.org/abs/2601.16429)
*Jongmin Yu,Hyeontaek Oh,Zhongtian Sun,Angelica I Aviles-Rivero,Moongu Jeon,Jinhong Yang*

Main category: cs.CV

TL;DR: AlphaFace is a real-time face-swapping method that uses vision-language models and CLIP embeddings with contrastive losses to handle extreme facial poses while maintaining identity representation and attribute preservation.


<details>
  <summary>Details</summary>
Motivation: Existing face-swapping methods degrade in quality with extreme facial poses, geometric features add dependencies and computational cost, and diffusion-based methods are too slow for real-time processing.

Method: Leverages open-source vision-language model and CLIP image/text embeddings with novel visual and textual semantic contrastive losses for stronger identity representation and precise attribute preservation.

Result: Surpasses state-of-the-art methods in pose-challenging cases across FF++, MPIE, and LPFF datasets while maintaining real-time performance.

Conclusion: AlphaFace effectively addresses extreme pose challenges in face-swapping with better identity preservation and real-time capability, making it practical for real-world applications.

Abstract: Existing face-swapping methods often deliver competitive results in constrained settings but exhibit substantial quality degradation when handling extreme facial poses. To improve facial pose robustness, explicit geometric features are applied, but this approach remains problematic since it introduces additional dependencies and increases computational cost. Diffusion-based methods have achieved remarkable results; however, they are impractical for real-time processing. We introduce AlphaFace, which leverages an open-source vision-language model and CLIP image and text embeddings to apply novel visual and textual semantic contrastive losses. AlphaFace enables stronger identity representation and more precise attribute preservation, all while maintaining real-time performance. Comprehensive experiments across FF++, MPIE, and LPFF demonstrate that AlphaFace surpasses state-of-the-art methods in pose-challenging cases. The project is publicly available on `https://github.com/andrewyu90/Alphaface_Official.git'.

</details>


### [12] [MDAFNet: Multiscale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection](https://arxiv.org/abs/2601.16434)
*Shuying Li,Qiang Ma,San Zhang,Wuwei Wang,Chuang Yang*

Main category: cs.CV

TL;DR: MDAFNet is a new infrared small target detection network that addresses edge degradation and frequency interference issues through multi-scale differential edge enhancement and dual-domain adaptive feature processing.


<details>
  <summary>Details</summary>
Motivation: Existing IRSTD methods suffer from gradual degradation of target edge pixels as network depth increases, and traditional convolution struggles to differentiate frequency components, leading to background interference and false detections from noise.

Method: Proposes MDAFNet with two key modules: 1) Multi-Scale Differential Edge (MSDE) module for edge extraction and enhancement to compensate for edge information loss during downsampling, and 2) Dual-Domain Adaptive Feature Enhancement (DAFE) module combining frequency domain processing with simulated frequency decomposition/fusion in spatial domain to enhance high-frequency targets while suppressing noise.

Result: Experimental results on multiple datasets demonstrate superior detection performance compared to existing methods.

Conclusion: MDAFNet effectively addresses edge degradation and frequency interference problems in infrared small target detection through innovative multi-scale edge enhancement and dual-domain adaptive feature processing.

Abstract: Infrared small target detection (IRSTD) plays a crucial role in numerous military and civilian applications. However, existing methods often face the gradual degradation of target edge pixels as the number of network layers increases, and traditional convolution struggles to differentiate between frequency components during feature extraction, leading to low-frequency backgrounds interfering with high-frequency targets and high-frequency noise triggering false detections. To address these limitations, we propose MDAFNet (Multi-scale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection), which integrates the Multi-Scale Differential Edge (MSDE) module and Dual-Domain Adaptive Feature Enhancement (DAFE) module. The MSDE module, through a multi-scale edge extraction and enhancement mechanism, effectively compensates for the cumulative loss of target edge information during downsampling. The DAFE module combines frequency domain processing mechanisms with simulated frequency decomposition and fusion mechanisms in the spatial domain to effectively improve the network's capability to adaptively enhance high-frequency targets and selectively suppress high-frequency noise. Experimental results on multiple datasets demonstrate the superior detection performance of MDAFNet.

</details>


### [13] [Masked Face Recognition under Different Backbones](https://arxiv.org/abs/2601.16440)
*Bo Zhang,Ming Zhang,Kun Wu,Lei Bian,Yi Lin*

Main category: cs.CV

TL;DR: Paper evaluates backbone networks for face recognition in post-pandemic era where many passengers wear masks, comparing performance on standard vs. masked face recognition tasks.


<details>
  <summary>Details</summary>
Motivation: In post-pandemic era, high proportion of civil aviation passengers wear masks during security checks, posing significant challenges to traditional face recognition models that weren't designed for masked faces.

Method: Conducted extensive comparative experiments evaluating several core backbone networks (r100 series, r50, r34_mask_v1, r100_mask_v2, r50_mask_v3, Vit-Small/Tiny) on both standard and masked face recognition tests.

Result: Standard tests: r100 series excelled (98%+ accuracy at 0.01% FAR), r50 ranked second, r34_mask_v1 lagged. Masked tests: r100_mask_v2 led (90.07% accuracy), r50_mask_v3 performed best among r50 but trailed r100. Vit-Small/Tiny showed strong masked performance with gains in effectiveness.

Conclusion: Provides comprehensive evaluation revealing impacts of different backbone models on face recognition with and without masks, offering specific deployment recommendations for civil aviation security applications.

Abstract: Erratum to the paper (Zhang et al., 2025): corrections to Table IV and the data in Page 3, Section A. In the post-pandemic era, a high proportion of civil aviation passengers wear masks during security checks, posing significant challenges to traditional face recognition models. The backbone network serves as the core component of face recognition models. In standard tests, r100 series models excelled (98%+ accuracy at 0.01% FAR in face comparison, high top1/top5 in search). r50 ranked second, r34_mask_v1 lagged. In masked tests, r100_mask_v2 led (90.07% accuracy), r50_mask_v3 performed best among r50 but trailed r100. Vit-Small/Tiny showed strong masked performance with gains in effectiveness. Through extensive comparative experiments, this paper conducts a comprehensive evaluation of several core backbone networks, aiming to reveal the impacts of different models on face recognition with and without masks, and provide specific deployment recommendations.

</details>


### [14] [Emotion-LLaMAv2 and MMEVerse: A New Framework and Benchmark for Multimodal Emotion Understanding](https://arxiv.org/abs/2601.16449)
*Xiaojiang Peng,Jingyi Chen,Zebang Cheng,Bao Peng,Fengyi Wu,Yifei Dong,Shuyuan Tu,Qiyu Hu,Huiting Huang,Yuxiang Lin,Jun-Yan He,Kai Wang,Zheng Lian,Zhi-Qi Cheng*

Main category: cs.CV

TL;DR: Emotion-LLaMAv2 introduces an end-to-end multimodal emotion reasoning framework with multiview encoding, Conv Attention pre-fusion, and curriculum instruction tuning, accompanied by the MMEVerse benchmark with 130k training clips across 12 datasets.


<details>
  <summary>Details</summary>
Motivation: Current multimodal LLMs have limited emotional reasoning capabilities due to scarcity of large-scale emotion datasets with quality annotations, lack of standardized benchmarks, and limitations of previous approaches that relied on explicit face detectors and implicit fusion strategies.

Method: 1) End-to-end multiview encoder eliminating external face detection and capturing emotional cues via spatial/temporal multiview tokens; 2) Conv Attention pre-fusion module for simultaneous local/global multimodal feature interactions; 3) Perception-to-cognition curriculum instruction tuning within LLaMA2 backbone unifying emotion recognition and reasoning.

Result: Created MMEVerse benchmark aggregating 12 emotion datasets (IEMOCAP, MELD, DFEW, MAFW, etc.) into unified multimodal instruction format with 130k training clips and 36k testing clips across 18 evaluation benchmarks, re-annotated via multi-agent pipeline.

Conclusion: Establishes comprehensive end-to-end pipeline and standardized evaluation setting for emotion recognition and reasoning, addressing previous limitations through architectural improvements and large-scale benchmark creation.

Abstract: Understanding human emotions from multimodal signals poses a significant challenge in affective computing and human-robot interaction. While multimodal large language models (MLLMs) have excelled in general vision-language tasks, their capabilities in emotional reasoning remain limited. The field currently suffers from a scarcity of large-scale datasets with high-quality, descriptive emotion annotations and lacks standardized benchmarks for evaluation. Our preliminary framework, Emotion-LLaMA, pioneered instruction-tuned multimodal learning for emotion reasoning but was restricted by explicit face detectors, implicit fusion strategies, and low-quality training data with limited scale. To address these limitations, we present Emotion-LLaMAv2 and the MMEVerse benchmark, establishing an end-to-end pipeline together with a standardized evaluation setting for emotion recognition and reasoning. Emotion-LLaMAv2 introduces three key advances. First, an end-to-end multiview encoder eliminates external face detection and captures nuanced emotional cues via richer spatial and temporal multiview tokens. Second, a Conv Attention pre-fusion module is designed to enable simultaneous local and global multimodal feature interactions external to the LLM backbone. Third, a perception-to-cognition curriculum instruction tuning scheme within the LLaMA2 backbone unifies emotion recognition and free-form emotion reasoning. To support large-scale training and reproducible evaluation, MMEVerse aggregates twelve publicly available emotion datasets, including IEMOCAP, MELD, DFEW, and MAFW, into a unified multimodal instruction format. The data are re-annotated via a multi-agent pipeline involving Qwen2 Audio, Qwen2.5 VL, and GPT 4o, producing 130k training clips and 36k testing clips across 18 evaluation benchmarks.

</details>


### [15] [VISTA-PATH: An interactive foundation model for pathology image segmentation and quantitative analysis in computational pathology](https://arxiv.org/abs/2601.16451)
*Peixian Liang,Songhao Li,Shunsuke Koga,Yutong Li,Zahra Alipour,Yucheng Tang,Daguang Xu,Zhi Huang*

Main category: cs.CV

TL;DR: VISTA-PATH is an interactive pathology segmentation foundation model that combines visual context, semantic descriptions, and expert feedback for precise multi-class segmentation, outperforming existing models and enabling clinically meaningful tissue analysis.


<details>
  <summary>Details</summary>
Motivation: Current segmentation foundation models treat segmentation as static visual prediction tasks, lacking alignment with pathology needs for handling heterogeneous structures, incorporating expert feedback, and producing clinically meaningful pixel-level segmentation.

Method: VISTA-PATH jointly conditions segmentation on visual context, semantic tissue descriptions, and optional expert spatial prompts. It uses a large-scale pathology segmentation corpus (VISTA-PATH Data) with 1.6M image-mask-text triplets across 9 organs and 93 tissue classes, and supports human-in-the-loop refinement through sparse bounding-box annotation propagation.

Result: VISTA-PATH consistently outperforms existing segmentation foundation models across held-out and external benchmarks. It enables dynamic refinement and produces high-fidelity segmentation that improves tissue microenvironment analysis through Tumor Interaction Score (TIS), showing strong associations with patient survival.

Conclusion: VISTA-PATH elevates pathology segmentation from static prediction to interactive, clinically grounded representation, establishing itself as a preferred foundation model for computational pathology with demonstrated clinical relevance.

Abstract: Accurate semantic segmentation for histopathology image is crucial for quantitative tissue analysis and downstream clinical modeling. Recent segmentation foundation models have improved generalization through large-scale pretraining, yet remain poorly aligned with pathology because they treat segmentation as a static visual prediction task. Here we present VISTA-PATH, an interactive, class-aware pathology segmentation foundation model designed to resolve heterogeneous structures, incorporate expert feedback, and produce pixel-level segmentation that are directly meaningful for clinical interpretation. VISTA-PATH jointly conditions segmentation on visual context, semantic tissue descriptions, and optional expert-provided spatial prompts, enabling precise multi-class segmentation across heterogeneous pathology images. To support this paradigm, we curate VISTA-PATH Data, a large-scale pathology segmentation corpus comprising over 1.6 million image-mask-text triplets spanning 9 organs and 93 tissue classes. Across extensive held-out and external benchmarks, VISTA-PATH consistently outperforms existing segmentation foundation models. Importantly, VISTA-PATH supports dynamic human-in-the-loop refinement by propagating sparse, patch-level bounding-box annotation feedback into whole-slide segmentation. Finally, we show that the high-fidelity, class-aware segmentation produced by VISTA-PATH is a preferred model for computational pathology. It improve tissue microenvironment analysis through proposed Tumor Interaction Score (TIS), which exhibits strong and significant associations with patient survival. Together, these results establish VISTA-PATH as a foundation model that elevates pathology image segmentation from a static prediction to an interactive and clinically grounded representation for digital pathology. Source code and demo can be found at https://github.com/zhihuanglab/VISTA-PATH.

</details>


### [16] [Order from Chaos: Physical World Understanding from Glitchy Gameplay Videos](https://arxiv.org/abs/2601.16471)
*Meng Cao,Haoran Tang,Haoze Zhao,Mingfei Han,Ruyang Liu,Qiang Sun,Xiaojun Chang,Ian Reid,Xiaodan Liang*

Main category: cs.CV

TL;DR: The paper introduces PhysGame, a dataset using gameplay video glitches as supervision for physical reasoning, and GameBench, an evaluation benchmark, showing improved physical understanding in multimodal models.


<details>
  <summary>Details</summary>
Motivation: Current multimodal LLMs lack human-level physical understanding. Existing datasets are either expensive (real videos) or unrealistic (synthetic simulations). Gameplay video glitches offer a scalable, cost-effective source of physical anomaly supervision.

Method: Created PhysGame dataset with 140K QA pairs from gameplay glitches across 5 physical domains using metadata-guided prompting. Built GameBench benchmark with 880 expert-annotated glitch videos for evaluation.

Result: PhysGame improves Qwen2.5VL by 2.5% on real-world PhysBench, 1.9% on general MVBench, and 3.7% on GameBench, demonstrating enhanced physical reasoning and anomaly detection capabilities.

Conclusion: Learning from gameplay anomalies provides scalable, effective pathway for advancing physical world understanding in multimodal AI systems, bridging simulation-reality gap.

Abstract: Understanding the physical world, including object dynamics, material properties, and causal interactions, remains a core challenge in artificial intelligence. Although recent multi-modal large language models (MLLMs) have demonstrated impressive general reasoning capabilities, they still fall short of achieving human-level understanding of physical principles. Existing datasets for physical reasoning either rely on real-world videos, which incur high annotation costs, or on synthetic simulations, which suffer from limited realism and diversity. In this paper, we propose a novel paradigm that leverages glitches in gameplay videos, referring to visual anomalies that violate predefined physical laws, as a rich and scalable supervision source for physical world understanding. We introduce PhysGame, an meta information guided instruction-tuning dataset containing 140,057 glitch-centric question-answer pairs across five physical domains and sixteen fine-grained categories. To ensure data accuracy, we design a prompting strategy that utilizes gameplay metadata such as titles and descriptions to guide high-quality QA generation. Complementing PhysGame, we construct GameBench, an expert-annotated benchmark with 880 glitch-identified gameplay videos designed to evaluate physical reasoning capabilities. Extensive experiments show that PhysGame significantly enhances both Game2Real transferability, improving the real world physical reasoning performance of Qwen2.5VL by 2.5% on PhysBench, and Game2General transferability, yielding a 1.9% gain on the MVBench benchmark. Moreover, PhysGame-tuned models achieve a 3.7% absolute improvement on GameBench, demonstrating enhanced robustness in detecting physical implausibilities. These results indicate that learning from gameplay anomalies offers a scalable and effective pathway toward advancing physical world understanding in multimodal intelligence.

</details>


### [17] [Multi-View Consistent Wound Segmentation With Neural Fields](https://arxiv.org/abs/2601.16487)
*Remi Chierchia,Léo Lebrat,David Ahmedt-Aristizabal,Yulia Arzhaeva,Olivier Salvado,Clinton Fookes,Rodrigo Santa Cruz*

Main category: cs.CV

TL;DR: WoundNeRF uses Neural Radiance Fields (NeRF) with Signed Distance Functions (SDF) to create 3D wound segmentations from 2D images, outperforming Vision Transformers and conventional methods.


<details>
  <summary>Details</summary>
Motivation: Wound care faces economic and logistical challenges worldwide. While computer vision has helped with wound segmentation from RGB images, existing 2D approaches lack the completeness and precision needed for accurate healing progress tracking. The key challenge is inferring multi-view consistent 3D structures from 2D images.

Method: WoundNeRF, a NeRF SDF-based method that estimates robust wound segmentations from automatically generated annotations. It uses Neural Radiance Fields with Signed Distance Functions to create 3D reconstructions from 2D wound images.

Result: The method demonstrates potential in recovering accurate segmentations, outperforming state-of-the-art Vision Transformer networks and conventional rasterisation-based algorithms in comparative evaluation.

Conclusion: WoundNeRF represents a promising paradigm for 3D wound segmentation that enables more complete and precise healing progress tracking. The code will be released to facilitate further development in this area.

Abstract: Wound care is often challenged by the economic and logistical burdens that consistently afflict patients and hospitals worldwide. In recent decades, healthcare professionals have sought support from computer vision and machine learning algorithms. In particular, wound segmentation has gained interest due to its ability to provide professionals with fast, automatic tissue assessment from standard RGB images. Some approaches have extended segmentation to 3D, enabling more complete and precise healing progress tracking. However, inferring multi-view consistent 3D structures from 2D images remains a challenge. In this paper, we evaluate WoundNeRF, a NeRF SDF-based method for estimating robust wound segmentations from automatically generated annotations. We demonstrate the potential of this paradigm in recovering accurate segmentations by comparing it against state-of-the-art Vision Transformer networks and conventional rasterisation-based algorithms. The code will be released to facilitate further development in this promising paradigm.

</details>


### [18] [Expert Knowledge-Guided Decision Calibration for Accurate Fine-Grained Tree Species Classification](https://arxiv.org/abs/2601.16498)
*Chen Long,Dian Chen,Ruifei Ding,Zhe Chen,Zhen Dong,Bisheng Yang*

Main category: cs.CV

TL;DR: EKDC-Net is a lightweight plug-and-play module that uses external domain expert knowledge to improve fine-grained tree species classification, addressing long-tailed distributions and high inter-class similarity issues.


<details>
  <summary>Details</summary>
Motivation: Existing tree species classification methods struggle with long-tailed distributions and high inter-class similarity in limited data, particularly for few-shot or confusing categories. Inspired by how humans seek expert assistance to overcome local thinking limitations, the authors propose incorporating external domain expert knowledge.

Method: Two core modules: 1) Local Prior Guided Knowledge Extraction Module (LPKEM) uses Class Activation Map analysis to guide domain experts to focus on discriminative features; 2) Uncertainty-Guided Decision Calibration Module (UDCM) dynamically corrects local model decisions based on overall category and instance-level prediction uncertainty. Also introduces CU-Tree102 dataset with 102 tree species.

Result: Achieves state-of-the-art performance on three benchmark datasets. As a lightweight plug-and-play module, EKDC-Net improves backbone accuracy by 6.42% and precision by 11.46% with only 0.08M additional parameters.

Conclusion: The proposed expert knowledge-guided approach effectively addresses challenges in fine-grained tree species classification, offering significant performance improvements with minimal parameter overhead. The method and new dataset advance the field of forest inventory and biodiversity monitoring.

Abstract: Accurate fine-grained tree species classification is critical for forest inventory and biodiversity monitoring. Existing methods predominantly focus on designing complex architectures to fit local data distributions. However, they often overlook the long-tailed distributions and high inter-class similarity inherent in limited data, thereby struggling to distinguish between few-shot or confusing categories. In the process of knowledge dissemination in the human world, individuals will actively seek expert assistance to transcend the limitations of local thinking. Inspired by this, we introduce an external "Domain Expert" and propose an Expert Knowledge-Guided Classification Decision Calibration Network (EKDC-Net) to overcome these challenges. Our framework addresses two core issues: expert knowledge extraction and utilization. Specifically, we first develop a Local Prior Guided Knowledge Extraction Module (LPKEM). By leveraging Class Activation Map (CAM) analysis, LPKEM guides the domain expert to focus exclusively on discriminative features essential for classification. Subsequently, to effectively integrate this knowledge, we design an Uncertainty-Guided Decision Calibration Module (UDCM). This module dynamically corrects the local model's decisions by considering both overall category uncertainty and instance-level prediction uncertainty. Furthermore, we present a large-scale classification dataset covering 102 tree species, named CU-Tree102 to address the issue of scarce diversity in current benchmarks. Experiments on three benchmark datasets demonstrate that our approach achieves state-of-the-art performance. Crucially, as a lightweight plug-and-play module, EKDC-Net improves backbone accuracy by 6.42% and precision by 11.46% using only 0.08M additional learnable parameters. The dataset, code, and pre-trained models are available at https://github.com/WHU-USI3DV/TreeCLS.

</details>


### [19] [SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer](https://arxiv.org/abs/2601.16515)
*Tongcheng Fang,Hanling Zhang,Ruiqi Xie,Zhuo Han,Xin Tao,Tianchen Zhao,Pengfei Wan,Wenbo Ding,Wanli Ouyang,Xuefei Ning,Yu Wang*

Main category: cs.CV

TL;DR: SALAD introduces a lightweight linear attention branch with input-dependent gating to achieve 90% sparsity and 1.72x inference speedup for Diffusion Transformers in video generation, while maintaining quality comparable to full attention.


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers for video generation suffer from high computational latency due to quadratic complexity of full attention. Existing sparse attention methods have limitations: training-free methods offer modest acceleration with limited sparsity, while training-based methods require substantial data and computation.

Method: SALAD proposes a lightweight linear attention branch parallel to sparse attention, with an input-dependent gating mechanism to balance the two branches. The approach achieves 90% sparsity through efficient finetuning requiring only 2,000 video samples and 1,600 training steps.

Result: Achieves 90% sparsity and 1.72x inference speedup while maintaining generation quality comparable to full attention baseline. The finetuning process is highly efficient, requiring minimal data and computation.

Conclusion: SALAD provides an effective solution to the computational bottleneck in video diffusion transformers, achieving high sparsity and significant speedup with minimal training overhead while preserving generation quality.

Abstract: Diffusion Transformers have recently demonstrated remarkable performance in video generation. However, the long input sequences result in high computational latency due to the quadratic complexity of full attention. Various sparse attention mechanisms have been proposed. Training-free sparse attention is constrained by limited sparsity and thus offers modest acceleration, whereas training-based methods can reach much higher sparsity but demand substantial data and computation for training. In this work, we propose SALAD, introducing a lightweight linear attention branch in parallel with the sparse attention. By incorporating an input-dependent gating mechanism to finely balance the two branches, our method attains 90% sparsity and 1.72x inference speedup, while maintaining generation quality comparable to the full attention baseline. Moreover, our finetuning process is highly efficient, requiring only 2,000 video samples and 1,600 training steps with a batch size of 8.

</details>


### [20] [TangramPuzzle: Evaluating Multimodal Large Language Models with Compositional Spatial Reasoning](https://arxiv.org/abs/2601.16520)
*Daixian Liu,Jiayi Kuang,Yinghui Li,Yangning Li,Di Yin,Haoyu Cao,Xing Sun,Ying Shen,Hai-Tao Zheng,Liang Lin,Philip S. Yu*

Main category: cs.CV

TL;DR: TangramPuzzle benchmark evaluates MLLMs' compositional spatial reasoning using Tangram game geometry with precise coordinate specifications, revealing models prioritize silhouette matching over geometric constraints.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs lack rigorous evaluation for precise compositional spatial reasoning, with existing benchmarks using simple tasks, semantic approximations, and coarse positioning without mathematical rigor.

Method: Introduces TangramPuzzle benchmark with Tangram Construction Expression (TCE) - a symbolic geometric framework for exact coordinate specifications. Designs two tasks: Outline Prediction (inferring global shapes from components) and End-to-End Code Generation (solving inverse geometric assembly).

Result: Evaluation of advanced MLLMs reveals they tend to prioritize matching target silhouettes while neglecting geometric constraints, leading to piece distortions and deformations.

Conclusion: The TangramPuzzle benchmark provides rigorous evaluation for compositional spatial reasoning, exposing limitations in current MLLMs' geometric understanding and highlighting the need for improved spatial reasoning capabilities.

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual recognition and semantic understanding. Nevertheless, their ability to perform precise compositional spatial reasoning remains largely unexplored. Existing benchmarks often involve relatively simple tasks and rely on semantic approximations or coarse relative positioning, while their evaluation metrics are typically limited and lack rigorous mathematical formulations. To bridge this gap, we introduce TangramPuzzle, a geometry-grounded benchmark designed to evaluate compositional spatial reasoning through the lens of the classic Tangram game. We propose the Tangram Construction Expression (TCE), a symbolic geometric framework that grounds tangram assemblies in exact, machine-verifiable coordinate specifications, to mitigate the ambiguity of visual approximation. We design two complementary tasks: Outline Prediction, which demands inferring global shapes from local components, and End-to-End Code Generation, which requires solving inverse geometric assembly problems. We conduct extensive evaluation experiments on advanced open-source and proprietary models, revealing an interesting insight: MLLMs tend to prioritize matching the target silhouette while neglecting geometric constraints, leading to distortions or deformations of the pieces.

</details>


### [21] [GPA-VGGT:Adapting VGGT to Large scale Localization by self-Supervised learning with Geometry and Physics Aware loss](https://arxiv.org/abs/2601.16885)
*Yangfan Xu,Lilian Zhang,Xiaofeng He,Pengdong Wu,Wenqi Wu,Jun Mao*

Main category: cs.CV

TL;DR: Self-supervised training of Visual Geometry Grounded Transformers for camera pose estimation using sequence-wise geometric constraints without ground truth labels.


<details>
  <summary>Details</summary>
Motivation: Existing VGGT models require ground truth labels for training, limiting their adaptability to unlabeled and unseen scenes in large-scale environments.

Method: Extends pair-wise relations to sequence-wise geometric constraints, sampling multiple source frames and projecting them onto different target frames. Uses joint optimization loss combining photometric consistency and geometric constraints without hard labels.

Result: Model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Both attention layers and camera/depth heads effectively capture multi-view geometry.

Conclusion: Proposed self-supervised framework enables VGGT training with unlabeled data, enhancing localization capability in large-scale environments without requiring ground truth labels.

Abstract: Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unlabeled and unseen scenes. In this paper, we propose a self-supervised framework to train VGGT with unlabeled data, thereby enhancing its localization capability in large-scale environments. To achieve this, we extend conventional pair-wise relations to sequence-wise geometric constraints for self-supervised learning. Specifically, in each sequence, we sample multiple source frames and geometrically project them onto different target frames, which improves temporal feature consistency. We formulate physical photometric consistency and geometric constraints as a joint optimization loss to circumvent the requirement for hard labels. By training the model with this proposed method, not only the local and global cross-view attention layers but also the camera and depth heads can effectively capture the underlying multi-view geometry. Experiments demonstrate that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Our code will be released at https://github.com/X-yangfan/GPA-VGGT.

</details>


### [22] [AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding](https://arxiv.org/abs/2601.16532)
*Runmao Yao,Junsheng Zhou,Zhen Dong,Yu-Shen Liu*

Main category: cs.CV

TL;DR: AnchoredDream: A zero-shot pipeline for generating complete 360° indoor scenes from single images using appearance-geometry mutual boosting and geometric anchoring.


<details>
  <summary>Details</summary>
Motivation: Single-view indoor scene generation is crucial for real-world applications but remains challenging due to difficulty maintaining appearance consistency and geometric plausibility under large viewpoint changes. Existing methods using diffusion models and depth estimation still struggle with full-scene generation.

Method: Proposes AnchoredDream with appearance-geometry mutual boosting: 1) appearance-guided geometry generation for reliable 3D layout, 2) progressive scene generation through warp-and-inpaint, warp-and-refine, post-optimization modules, and 3) novel Grouting Block for seamless transitions between input and generated regions.

Result: Extensive experiments show AnchoredDream outperforms existing methods by large margin in both appearance consistency and geometric plausibility, achieving high-quality results in zero-shot manner.

Conclusion: Demonstrates potential of geometric grounding for high-quality, zero-shot single-view scene generation, with AnchoredDream effectively addressing appearance consistency and geometric plausibility challenges.

Abstract: Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation.

</details>


### [23] [AnyView: Synthesizing Any Novel View in Dynamic Scenes](https://arxiv.org/abs/2601.16982)
*Basile Van Hoorick,Dian Chen,Shun Iwase,Pavel Tokmakov,Muhammad Zubair Irshad,Igor Vasiljevic,Swati Gupta,Fangzhou Cheng,Sergey Zakharov,Vitor Campagnolo Guizilini*

Main category: cs.CV

TL;DR: AnyView is a diffusion-based video generation framework for dynamic view synthesis that uses multi-source data to create spatiotemporally consistent videos from arbitrary viewpoints, outperforming baselines on extreme dynamic scenarios.


<details>
  <summary>Details</summary>
Motivation: Current generative video models struggle with maintaining multi-view and spatiotemporal consistency in highly dynamic real-world environments, especially when viewpoints have minimal overlap.

Method: A diffusion-based framework leveraging multiple data sources (monocular 2D, multi-view static 3D, and multi-view dynamic 4D datasets) to train a generalist spatiotemporal implicit representation for zero-shot novel video generation from arbitrary camera trajectories.

Result: Competitive performance on standard benchmarks and superior results on the proposed AnyViewBench for extreme dynamic view synthesis, where baselines degrade significantly while AnyView maintains realistic, plausible, and consistent videos.

Conclusion: AnyView demonstrates effective dynamic view synthesis with minimal geometric assumptions, handling extreme viewpoint changes in diverse real-world scenarios where existing methods fail.

Abstract: Modern generative video models excel at producing convincing, high-quality outputs, but struggle to maintain multi-view and spatiotemporal consistency in highly dynamic real-world environments. In this work, we introduce \textbf{AnyView}, a diffusion-based video generation framework for \emph{dynamic view synthesis} with minimal inductive biases or geometric assumptions. We leverage multiple data sources with various levels of supervision, including monocular (2D), multi-view static (3D) and multi-view dynamic (4D) datasets, to train a generalist spatiotemporal implicit representation capable of producing zero-shot novel videos from arbitrary camera locations and trajectories. We evaluate AnyView on standard benchmarks, showing competitive results with the current state of the art, and propose \textbf{AnyViewBench}, a challenging new benchmark tailored towards \emph{extreme} dynamic view synthesis in diverse real-world scenarios. In this more dramatic setting, we find that most baselines drastically degrade in performance, as they require significant overlap between viewpoints, while AnyView maintains the ability to produce realistic, plausible, and spatiotemporally consistent videos when prompted from \emph{any} viewpoint. Results, data, code, and models can be viewed at: https://tri-ml.github.io/AnyView/

</details>


### [24] [OnlineSI: Taming Large Language Model for Online 3D Understanding and Grounding](https://arxiv.org/abs/2601.16538)
*Zixian Liu,Zhaoxi Chen,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: OnlineSI is a framework that enables Multimodal Large Language Models to continuously improve spatial understanding from video streams using finite spatial memory and 3D point cloud integration.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM methods lack continuous spatial understanding capabilities for changing environments and cannot be deployed on real-world embodied systems. There's a need for methods that can work with accumulating video input without increasing computational requirements.

Method: Proposes OnlineSI framework with finite spatial memory to retain past observations, ensuring constant computation per inference. Integrates 3D point cloud information with semantic data to help MLLMs better locate and identify objects in scenes.

Result: Introduced Fuzzy F1-Score to mitigate ambiguity in evaluation. Tested on two representative datasets, demonstrating effectiveness of the method for continuous spatial understanding.

Conclusion: The framework enables continuous spatial understanding from video streams, paving the way for deployment on real-world embodied systems with practical computational constraints.

Abstract: In recent years, researchers have increasingly been interested in how to enable Multimodal Large Language Models (MLLM) to possess spatial understanding and reasoning capabilities. However, most existing methods overlook the importance of the ability to continuously work in an ever-changing world, and lack the possibility of deployment on embodied systems in real-world environments. In this work, we introduce OnlineSI, a framework that can continuously improve its spatial understanding of its surroundings given a video stream. Our core idea is to maintain a finite spatial memory to retain past observations, ensuring the computation required for each inference does not increase as the input accumulates. We further integrate 3D point cloud information with semantic information, helping MLLM to better locate and identify objects in the scene. To evaluate our method, we introduce the Fuzzy $F_1$-Score to mitigate ambiguity, and test our method on two representative datasets. Experiments demonstrate the effectiveness of our method, paving the way towards real-world embodied systems.

</details>


### [25] [Semi-Supervised Hierarchical Open-Set Classification](https://arxiv.org/abs/2601.16541)
*Erik Wallin,Fredrik Kahl,Lars Hammarstrand*

Main category: cs.CV

TL;DR: Semi-supervised hierarchical open-set classification using teacher-student framework with subtree pseudo-labels and age-gating to handle unknown classes in uncurated datasets.


<details>
  <summary>Details</summary>
Motivation: Extend hierarchical open-set classification to semi-supervised setting to leverage large-scale uncurated datasets containing both known and unknown classes, improving performance with limited labeled data.

Method: Propose teacher-student framework with pseudo-labeling, featuring two key components: 1) subtree pseudo-labels for reliable supervision with unknown data, and 2) age-gating mechanism to mitigate pseudo-label overconfidence.

Result: Outperforms self-supervised pretraining followed by supervised adaptation, and matches fully supervised counterpart with only 20 labeled samples per class on iNaturalist19 benchmark.

Conclusion: The proposed semi-supervised framework effectively leverages uncurated datasets for hierarchical open-set classification, achieving strong performance with minimal labeled data.

Abstract: Hierarchical open-set classification handles previously unseen classes by assigning them to the most appropriate high-level category in a class taxonomy. We extend this paradigm to the semi-supervised setting, enabling the use of large-scale, uncurated datasets containing a mixture of known and unknown classes to improve the hierarchical open-set performance. To this end, we propose a teacher-student framework based on pseudo-labeling. Two key components are introduced: 1) subtree pseudo-labels, which provide reliable supervision in the presence of unknown data, and 2) age-gating, a mechanism that mitigates overconfidence in pseudo-labels. Experiments show that our framework outperforms self-supervised pretraining followed by supervised adaptation, and even matches the fully supervised counterpart when using only 20 labeled samples per class on the iNaturalist19 benchmark. Our code is available at https://github.com/walline/semihoc.

</details>


### [26] [Using Shadows in Circular Synthetic Aperture Sonar Imaging for Target Analysis](https://arxiv.org/abs/2601.16733)
*Yann Le Gall,Nicolas Burlet,Mathieu Simon,Fabien Novella,Samantha Dugelay,Jean-Philippe Malkasse*

Main category: cs.CV

TL;DR: CSAS provides 360° seabed views but loses shadow information crucial for target recognition. This paper proposes using sub-aperture filtering and FFSE to retrieve shadows from CSAS data for improved target analysis and 3D reconstruction.


<details>
  <summary>Details</summary>
Motivation: While CSAS offers superior 360° azimuth coverage compared to conventional side-scan SAS, it loses valuable shadow information due to parallax effects. Shadows provide complementary shape information essential for target recognition in mine warfare, particularly for reducing false alarms. The paper aims to recover this lost shadow data to enhance target analysis.

Method: The approach uses sub-aperture filtering to generate multiple images from different viewpoints along the circular trajectory. Fixed Focus Shadow Enhancement (FFSE) is applied to obtain sharp shadows. An interactive interface allows human operators to visualize shadows along the circular path. Finally, space-carving reconstruction methods are used to infer 3D shapes from segmented shadows.

Result: The method successfully retrieves shadow information from CSAS data that was previously lost. The results demonstrate the potential of using shadows in circular SAS for improving target analysis and enabling 3D reconstruction of objects on the seabed.

Conclusion: Shadows in circular SAS provide valuable complementary information for target recognition and 3D reconstruction. The proposed method effectively recovers shadow data, enhancing the utility of CSAS for mine warfare applications where reducing false alarms is critical.

Abstract: Circular Synthetic Aperture Sonar (CSAS) provides a 360° azimuth view of the seabed, surpassing the limited aperture and mono-view image of conventional side-scan SAS. This makes CSAS a valuable tool for target recognition in mine warfare where the diversity of point of view is essential for reducing false alarms. CSAS processing typically produces a very high-resolution two-dimensional image. However, the parallax introduced by the circular displacement of the illuminator fill-in the shadow regions, and the shadow cast by an object on the seafloor is lost in favor of azimuth coverage and resolution. Yet the shadows provide complementary information on target shape useful for target recognition. In this paper, we explore a way to retrieve shadow information from CSAS data to improve target analysis and carry 3D reconstruction. Sub-aperture filtering is used to get a collection of images at various points of view along the circular trajectory and fixed focus shadow enhancement (FFSE) is applied to obtain sharp shadows. An interactive interface is also proposed to allow human operators to visualize these shadows along the circular trajectory. A space-carving reconstruction method is applied to infer the 3D shape of the object from the segmented shadows. The results demonstrate the potential of shadows in circular SAS for improving target analysis and 3D reconstruction.

</details>


### [27] [HA2F: Dual-module Collaboration-Guided Hierarchical Adaptive Aggregation Framework for Remote Sensing Change Detection](https://arxiv.org/abs/2601.16573)
*Shuying Li,Yuchen Wang,San Zhang,Chuang Yang*

Main category: cs.CV

TL;DR: HA2F is a hierarchical adaptive aggregation framework for remote sensing change detection that addresses feature alignment deviations and noise sensitivity through dual modules for dynamic feature calibration and noise-adaptive refinement.


<details>
  <summary>Details</summary>
Motivation: Existing RSCD methods suffer from cross-temporal feature matching deviations and sensitivity to radiometric/geometric noise, as they either focus on localized patches or process entire images holistically without proper feature alignment and noise handling.

Method: Proposes HA2F with two modules: 1) Dynamic Hierarchical Feature Calibration Module (DHFCM) that fuses adjacent-level features through perceptual feature selection to suppress irrelevant discrepancies, and 2) Noise-Adaptive Feature Refinement Module (NAFRM) that uses dual feature selection to highlight change-sensitive regions and generate spatial masks to suppress irrelevant areas or shadows.

Result: Achieves state-of-the-art performance on LEVIR-CD, WHU-CD, and SYSU-CD datasets, surpassing existing methods in both precision metrics and computational efficiency. Ablation experiments confirm the effectiveness of both modules.

Conclusion: HA2F effectively addresses feature alignment and noise sensitivity issues in remote sensing change detection through its hierarchical adaptive aggregation framework, demonstrating superior performance and efficiency across multiple benchmark datasets.

Abstract: Remote sensing change detection (RSCD) aims to identify the spatio-temporal changes of land cover, providing critical support for multi-disciplinary applications (e.g., environmental monitoring, disaster assessment, and climate change studies). Existing methods focus either on extracting features from localized patches, or pursue processing entire images holistically, which leads to the cross temporal feature matching deviation and exhibiting sensitivity to radiometric and geometric noise. Following the above issues, we propose a dual-module collaboration guided hierarchical adaptive aggregation framework, namely HA2F, which consists of dynamic hierarchical feature calibration module (DHFCM) and noise-adaptive feature refinement module (NAFRM). The former dynamically fuses adjacent-level features through perceptual feature selection, suppressing irrelevant discrepancies to address multi-temporal feature alignment deviations. The NAFRM utilizes the dual feature selection mechanism to highlight the change sensitive regions and generate spatial masks, suppressing the interference of irrelevant regions or shadows. Extensive experiments verify the effectiveness of the proposed HA2F, which achieves state-of-the-art performance on LEVIR-CD, WHU-CD, and SYSU-CD datasets, surpassing existing comparative methods in terms of both precision metrics and computational efficiency. In addition, ablation experiments show that DHFCM and NAFRM are effective. \href{https://huggingface.co/InPeerReview/RemoteSensingChangeDetection-RSCD.HA2F}{HA2F Official Code is Available Here!}

</details>


### [28] [X-Aligner: Composed Visual Retrieval without the Bells and Whistles](https://arxiv.org/abs/2601.16582)
*Yuqian Zheng,Mariana-Iuliana Georgescu*

Main category: cs.CV

TL;DR: A novel two-stage CoVR framework using Vision Language Models with X-Aligner cross-attention module and visual query captions achieves SOTA performance on Webvid-CoVR and strong zero-shot generalization on CIR tasks.


<details>
  <summary>Details</summary>
Motivation: Existing CoVR frameworks fuse multimodal inputs in a single stage with marginal gains, lacking effective progressive fusion and alignment of multimodal representations with target videos.

Method: Proposes a CoVR framework with X-Aligner cross-attention module for progressive visual-textual fusion and alignment, incorporates visual query captions, uses two-stage training (first stage trains only X-Aligner, second stage fine-tunes textual encoder), implemented on BLIP-family architectures.

Result: Achieves state-of-the-art Recall@1 of 63.93% on Webvid-CoVR-Test and demonstrates strong zero-shot generalization on CIRCO and Fashion-IQ datasets.

Conclusion: The proposed framework effectively leverages VLM representations through progressive multimodal fusion and two-stage training, achieving superior CoVR performance and generalization capabilities.

Abstract: Composed Video Retrieval (CoVR) facilitates video retrieval by combining visual and textual queries. However, existing CoVR frameworks typically fuse multimodal inputs in a single stage, achieving only marginal gains over initial baseline. To address this, we propose a novel CoVR framework that leverages the representational power of Vision Language Models (VLMs). Our framework incorporates a novel cross-attention module X-Aligner, composed of cross-attention layers that progressively fuse visual and textual inputs and align their multimodal representation with that of the target video. To further enhance the representation of the multimodal query, we incorporate the caption of the visual query as an additional input. The framework is trained in two stages to preserve the pretrained VLM representation. In the first stage, only the newly introduced module is trained, while in the second stage, the textual query encoder is also fine-tuned. We implement our framework on top of BLIP-family architecture, namely BLIP and BLIP-2, and train it on the Webvid-CoVR data set. In addition to in-domain evaluation on Webvid-CoVR-Test, we perform zero-shot evaluations on the Composed Image Retrieval (CIR) data sets CIRCO and Fashion-IQ. Our framework achieves state-of-the-art performance on CoVR obtaining a Recall@1 of 63.93% on Webvid-CoVR-Test, and demonstrates strong zero-shot generalization on CIR tasks.

</details>


### [29] [A Lightweight Medical Image Classification Framework via Self-Supervised Contrastive Learning and Quantum-Enhanced Feature Modeling](https://arxiv.org/abs/2601.16608)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: A lightweight medical image classification framework combining self-supervised contrastive learning with quantum-enhanced feature modeling, achieving superior performance with minimal parameters and computational cost.


<details>
  <summary>Details</summary>
Motivation: Address challenges in medical image analysis: scarce annotations, constrained computational resources, and poor model generalization. Need for practical solutions in resource-constrained clinical settings.

Method: Uses MobileNetV2 as compact backbone with SimCLR-style self-supervised pretraining on unlabeled images. Integrates lightweight parameterized quantum circuit (PQC) as quantum feature enhancement module, creating hybrid classical-quantum architecture fine-tuned on limited labeled data.

Result: Method outperforms classical baselines without self-supervised learning or quantum enhancement in Accuracy, AUC, and F1-score. Achieves this with only ~2-3 million parameters and low computational cost. Feature visualization shows improved discriminability and representation stability.

Conclusion: Provides practical and forward-looking solution for high-performance medical AI under resource-constrained settings, demonstrating viability of quantum-enhanced approaches in medical imaging.

Abstract: Intelligent medical image analysis is essential for clinical decision support but is often limited by scarce annotations, constrained computational resources, and suboptimal model generalization. To address these challenges, we propose a lightweight medical image classification framework that integrates self-supervised contrastive learning with quantum-enhanced feature modeling. MobileNetV2 is employed as a compact backbone and pretrained using a SimCLR-style self-supervised paradigm on unlabeled images. A lightweight parameterized quantum circuit (PQC) is embedded as a quantum feature enhancement module, forming a hybrid classical-quantum architecture, which is subsequently fine-tuned on limited labeled data. Experimental results demonstrate that, with only approximately 2-3 million parameters and low computational cost, the proposed method consistently outperforms classical baselines without self-supervised learning or quantum enhancement in terms of Accuracy, AUC, and F1-score. Feature visualization further indicates improved discriminability and representation stability. Overall, this work provides a practical and forward-looking solution for high-performance medical artificial intelligence under resource-constrained settings.

</details>


### [30] [Boundary and Position Information Mining for Aerial Small Object Detection](https://arxiv.org/abs/2601.16617)
*Rongxin Huang,Guangfeng Lin,Wenbo Zhou,Zhirong Li,Wenhuan Wu*

Main category: cs.CV

TL;DR: BPIM framework improves small object detection in UAV imagery by integrating boundary, position, and scale information through attention mechanisms and cross-scale feature fusion.


<details>
  <summary>Details</summary>
Motivation: UAV applications face challenges in accurately detecting small objects due to imbalanced scales and blurred edges, requiring better integration of boundary and position information.

Method: Proposes BPIM framework with five modules: PIG for location information, BIG for object edges, CSF for shallow feature assembly, TFF for combining position/boundary info, and AWF for deep semantic feature fusion.

Result: BPIM outperforms baseline Yolov5-P2 on VisDrone2021, DOTA1.0, and WiderPerson datasets, achieving state-of-the-art performance with comparable computation load.

Conclusion: BPIM effectively addresses small object detection challenges in UAV imagery by mining boundary and position information, demonstrating superior performance through integrated attention and fusion strategies.

Abstract: Unmanned Aerial Vehicle (UAV) applications have become increasingly prevalent in aerial photography and object recognition. However, there are major challenges to accurately capturing small targets in object detection due to the imbalanced scale and the blurred edges. To address these issues, boundary and position information mining (BPIM) framework is proposed for capturing object edge and location cues. The proposed BPIM includes position information guidance (PIG) module for obtaining location information, boundary information guidance (BIG) module for extracting object edge, cross scale fusion (CSF) module for gradually assembling the shallow layer image feature, three feature fusion (TFF) module for progressively combining position and boundary information, and adaptive weight fusion (AWF) module for flexibly merging the deep layer semantic feature. Therefore, BPIM can integrate boundary, position, and scale information in image for small object detection using attention mechanisms and cross-scale feature fusion strategies. Furthermore, BPIM not only improves the discrimination of the contextual feature by adaptive weight fusion with boundary, but also enhances small object perceptions by cross-scale position fusion. On the VisDrone2021, DOTA1.0, and WiderPerson datasets, experimental results show the better performances of BPIM compared to the baseline Yolov5-P2, and obtains the promising performance in the state-of-the-art methods with comparable computation load.

</details>


### [31] [SCHIGAND: A Synthetic Facial Generation Mode Pipeline](https://arxiv.org/abs/2601.16627)
*Ananya Kadali,Sunnie Jehan-Morrison,Orasiki Wellington,Barney Evans,Precious Durojaiye,Richard Guest*

Main category: cs.CV

TL;DR: SCHIGAND is a synthetic face generation pipeline combining StyleCLIP, HyperStyle, InterfaceGAN, and Diffusion models to create realistic, diverse facial datasets for biometric testing while preserving identity and addressing privacy concerns.


<details>
  <summary>Details</summary>
Motivation: Growing demand for diverse facial datasets faces challenges from privacy regulations, data scarcity, and ethical concerns. Existing generative models struggle to balance realism, diversity, and identity preservation for biometric applications.

Method: SCHIGAND integrates StyleCLIP, HyperStyle, InterfaceGAN, and Diffusion models in a pipeline to generate synthetic facial images with enhanced identity preservation, realistic intra-class variations, and inter-class distinctiveness.

Result: Experimental evaluation using ArcFace facial verification model shows SCHIGAND achieves balance between image quality and diversity, addressing limitations of prior generative models and performing comparably to real-world datasets.

Conclusion: SCHIGAND demonstrates potential to supplement or replace real data for facial biometric applications, offering privacy-compliant and scalable solutions for synthetic dataset generation in biometric testing.

Abstract: The growing demand for diverse and high-quality facial datasets for training and testing biometric systems is challenged by privacy regulations, data scarcity, and ethical concerns. Synthetic facial images offer a potential solution, yet existing generative models often struggle to balance realism, diversity, and identity preservation. This paper presents SCHIGAND, a novel synthetic face generation pipeline integrating StyleCLIP, HyperStyle, InterfaceGAN, and Diffusion models to produce highly realistic and controllable facial datasets. SCHIGAND enhances identity preservation while generating realistic intra-class variations and maintaining inter-class distinctiveness, making it suitable for biometric testing. The generated datasets were evaluated using ArcFace, a leading facial verification model, to assess their effectiveness in comparison to real-world facial datasets. Experimental results demonstrate that SCHIGAND achieves a balance between image quality and diversity, addressing key limitations of prior generative models. This research highlights the potential of SCHIGAND to supplement and, in some cases, replace real data for facial biometric applications, paving the way for privacy-compliant and scalable solutions in synthetic dataset generation.

</details>


### [32] [Edge-Aware Image Manipulation via Diffusion Models with a Novel Structure-Preservation Loss](https://arxiv.org/abs/2601.16645)
*Minsu Gong,Nuri Ryu,Jungseul Ok,Sunghyun Cho*

Main category: cs.CV

TL;DR: Proposes Structure Preservation Loss (SPL) using local linear models to maintain edge structures in latent-diffusion-based image editing, with training-free integration and additional techniques for distortion mitigation.


<details>
  <summary>Details</summary>
Motivation: Current latent diffusion models for image editing struggle to maintain pixel-level edge structures, which is crucial for photorealistic style transfer and image tone adjustment tasks.

Method: Introduces Structure Preservation Loss (SPL) that uses local linear models to quantify structural differences, integrates it training-free into diffusion process, adds post-processing for LDM decoding distortions, masking for edit localization, and color preservation loss.

Result: SPL enhances structural fidelity and achieves state-of-the-art performance in latent-diffusion-based image editing.

Conclusion: The proposed SPL effectively addresses structural preservation challenges in latent-diffusion-based editing through a novel loss function and complementary techniques.

Abstract: Recent advances in image editing leverage latent diffusion models (LDMs) for versatile, text-prompt-driven edits across diverse tasks. Yet, maintaining pixel-level edge structures-crucial for tasks such as photorealistic style transfer or image tone adjustment-remains as a challenge for latent-diffusion-based editing. To overcome this limitation, we propose a novel Structure Preservation Loss (SPL) that leverages local linear models to quantify structural differences between input and edited images. Our training-free approach integrates SPL directly into the diffusion model's generative process to ensure structural fidelity. This core mechanism is complemented by a post-processing step to mitigate LDM decoding distortions, a masking strategy for precise edit localization, and a color preservation loss to preserve hues in unedited areas. Experiments confirm SPL enhances structural fidelity, delivering state-of-the-art performance in latent-diffusion-based image editing. Our code will be publicly released at https://github.com/gongms00/SPL.

</details>


### [33] [Reliable Brain Tumor Segmentation Based on Spiking Neural Networks with Efficient Training](https://arxiv.org/abs/2601.16652)
*Aurora Pia Ghiardelli,Guangzhi Tang,Tao Sun*

Main category: cs.CV

TL;DR: SNN-based 3D brain tumor segmentation framework with multi-view ensemble for uncertainty estimation and FPTT training for computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To develop a reliable and energy-efficient brain tumor segmentation method suitable for low-power medical IoT and Point-of-Care systems, addressing the computational challenges of SNN training.

Method: Multi-view ensemble of sagittal, coronal, and axial SNN models for voxel-wise uncertainty estimation, combined with Forward Propagation Through Time (FPTT) for efficient SNN training with reduced computational cost.

Result: Competitive accuracy on BraTS 2017 and 2023 datasets, well-calibrated uncertainty estimation, and 87% reduction in FLOPs compared to conventional approaches.

Conclusion: The framework demonstrates SNNs' potential for reliable, low-power medical applications, offering both computational efficiency and uncertainty-aware segmentation for clinical deployment.

Abstract: We propose a reliable and energy-efficient framework for 3D brain tumor segmentation using spiking neural networks (SNNs). A multi-view ensemble of sagittal, coronal, and axial SNN models provides voxel-wise uncertainty estimation and enhances segmentation robustness. To address the high computational cost in training SNN models for semantic image segmentation, we employ Forward Propagation Through Time (FPTT), which maintains temporal learning efficiency with significantly reduced computational cost. Experiments on the Multimodal Brain Tumor Segmentation Challenges (BraTS 2017 and BraTS 2023) demonstrate competitive accuracy, well-calibrated uncertainty, and an 87% reduction in FLOPs, underscoring the potential of SNNs for reliable, low-power medical IoT and Point-of-Care systems.

</details>


### [34] [ReWeaver: Towards Simulation-Ready and Topology-Accurate Garment Reconstruction](https://arxiv.org/abs/2601.16672)
*Ming Li,Hui Shan,Kai Zheng,Chentao Shen,Siyu Liu,Yanwei Fu,Zhen Chen,Xiangru Huang*

Main category: cs.CV

TL;DR: ReWeaver: A framework for topology-accurate 3D garment and sewing pattern reconstruction from sparse multi-view RGB images, enabling structured 2D-3D representations suitable for physical simulation.


<details>
  <summary>Details</summary>
Motivation: Existing garment reconstruction methods use unstructured representations (like 3D Gaussian Splats) that struggle with accurate garment topology and sewing structures, making them unsuitable for high-fidelity physical simulation needed in digital avatars, virtual try-on, and robotic manipulation.

Method: ReWeaver reconstructs garments from as few as four input views by predicting seams and panels with their connectivities in both 2D UV space and 3D space. Uses a large-scale synthetic dataset GCD-TS with over 100,000 samples containing multi-view RGB images, 3D geometries, textured human bodies, and annotated sewing patterns for training.

Result: ReWeaver consistently outperforms existing methods in topology accuracy, geometry alignment, and seam-panel consistency. The reconstructed outputs align precisely with multi-view images and provide structured representations suitable for 3D perception, physical simulation, and robotic manipulation.

Conclusion: ReWeaver addresses the sim-to-real gap by providing topology-accurate 3D garment reconstruction with sewing patterns, enabling high-fidelity applications that require accurate garment structure and physical properties.

Abstract: High-quality 3D garment reconstruction plays a crucial role in mitigating the sim-to-real gap in applications such as digital avatars, virtual try-on and robotic manipulation. However, existing garment reconstruction methods typically rely on unstructured representations, such as 3D Gaussian Splats, struggling to provide accurate reconstructions of garment topology and sewing structures. As a result, the reconstructed outputs are often unsuitable for high-fidelity physical simulation. We propose ReWeaver, a novel framework for topology-accurate 3D garment and sewing pattern reconstruction from sparse multi-view RGB images. Given as few as four input views, ReWeaver predicts seams and panels as well as their connectivities in both the 2D UV space and the 3D space. The predicted seams and panels align precisely with the multi-view images, yielding structured 2D--3D garment representations suitable for 3D perception, high-fidelity physical simulation, and robotic manipulation. To enable effective training, we construct a large-scale dataset GCD-TS, comprising multi-view RGB images, 3D garment geometries, textured human body meshes and annotated sewing patterns. The dataset contains over 100,000 synthetic samples covering a wide range of complex geometries and topologies. Extensive experiments show that ReWeaver consistently outperforms existing methods in terms of topology accuracy, geometry alignment and seam-panel consistency.

</details>


### [35] [Affinity Contrastive Learning for Skeleton-based Human Activity Understanding](https://arxiv.org/abs/2601.16694)
*Hongda Liu,Yunfan Liu,Min Ren,Lin Sui,Yunlong Wang,Zhenan Sun*

Main category: cs.CV

TL;DR: ACLNet introduces affinity contrastive learning for skeleton-based activity understanding, using affinity metrics to form activity superclasses, dynamic temperature scheduling, and margin-based contrastive strategy to improve feature discrimination.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive learning methods for skeleton-based activity understanding fail to exploit structural inter-class similarities and overlook the impact of anomalous positive samples, limiting feature discrimination.

Method: Proposes ACLNet with: 1) affinity metric to refine similarity measurements and form activity superclasses, 2) dynamic temperature schedule to adaptively adjust penalty strength for different superclasses, 3) margin-based contrastive strategy to improve separation of hard positive and negative samples.

Result: Extensive experiments on NTU RGB+D 60, NTU RGB+D 120, Kinetics-Skeleton, PKU-MMD, FineGYM, and CASIA-B demonstrate superiority in skeleton-based action recognition, gait recognition, and person re-identification.

Conclusion: ACLNet effectively addresses limitations of existing contrastive learning approaches by exploiting structural inter-class relationships and handling anomalous samples, leading to improved performance across multiple skeleton-based activity understanding tasks.

Abstract: In skeleton-based human activity understanding, existing methods often adopt the contrastive learning paradigm to construct a discriminative feature space. However, many of these approaches fail to exploit the structural inter-class similarities and overlook the impact of anomalous positive samples. In this study, we introduce ACLNet, an Affinity Contrastive Learning Network that explores the intricate clustering relationships among human activity classes to improve feature discrimination. Specifically, we propose an affinity metric to refine similarity measurements, thereby forming activity superclasses that provide more informative contrastive signals. A dynamic temperature schedule is also introduced to adaptively adjust the penalty strength for various superclasses. In addition, we employ a margin-based contrastive strategy to improve the separation of hard positive and negative samples within classes. Extensive experiments on NTU RGB+D 60, NTU RGB+D 120, Kinetics-Skeleton, PKU-MMD, FineGYM, and CASIA-B demonstrate the superiority of our method in skeleton-based action recognition, gait recognition, and person re-identification. The source code is available at https://github.com/firework8/ACLNet.

</details>


### [36] [CER-HV: A CER-Based Human-in-the-Loop Framework for Cleaning Datasets Applied to Arabic-Script HTR](https://arxiv.org/abs/2601.16713)
*Sana Al-azzawi,Elisa Barney,Marcus Liwicki*

Main category: cs.CV

TL;DR: CER-HV framework improves Arabic-script HTR by detecting and cleaning label errors in datasets using CER-based ranking with human verification, achieving state-of-the-art performance across multiple languages.


<details>
  <summary>Details</summary>
Motivation: Arabic-script handwritten text recognition lags behind Latin-script HTR despite advances, with data quality being a significant limiting factor in many published datasets.

Method: CER-HV framework combines CER-based noise detector using carefully configured CRNN with early stopping to avoid overfitting, plus human-in-the-loop verification of high-ranking samples.

Result: Framework identifies various dataset errors with up to 90% precision; CRNN achieves state-of-the-art performance across 5/6 datasets; CER-HV improves evaluation CER by 0.3-1.8% depending on dataset noise level.

Conclusion: Data quality is crucial for Arabic-script HTR improvement; CER-HV effectively detects and cleans label errors; framework is generalizable to other text recognition datasets beyond Arabic-script languages.

Abstract: Handwritten text recognition (HTR) for Arabic-script languages still lags behind Latin-script HTR, despite recent advances in model architectures, datasets, and benchmarks. We show that data quality is a significant limiting factor in many published datasets and propose CER-HV (CER-based Ranking with Human Verification) as a framework to detect and clean label errors. CER-HV combines a CER-based noise detector, built on a carefully configured Convolutional Recurrent Neural Network (CRNN) with early stopping to avoid overfitting noisy samples, and a human-in-the-loop (HITL) step that verifies high-ranking samples. The framework reveals that several existing datasets contain previously underreported problems, including transcription, segmentation, orientation, and non-text content errors. These have been identified with up to 90 percent precision in the Muharaf and 80-86 percent in the PHTI datasets.
  We also show that our CRNN achieves state-of-the-art performance across five of the six evaluated datasets, reaching 8.45 percent Character Error Rate (CER) on KHATT (Arabic), 8.26 percent on PHTI (Pashto), 10.66 percent on Ajami, and 10.11 percent on Muharaf (Arabic), all without any data cleaning. We establish a new baseline of 11.3 percent CER on the PHTD (Persian) dataset. Applying CER-HV improves the evaluation CER by 0.3-0.6 percent on the cleaner datasets and 1.0-1.8 percent on the noisier ones. Although our experiments focus on documents written in an Arabic-script language, including Arabic, Persian, Urdu, Ajami, and Pashto, the framework is general and can be applied to other text recognition datasets.

</details>


### [37] [A Step to Decouple Optimization in 3DGS](https://arxiv.org/abs/2601.16736)
*Renjie Ding,Yaonan Wang,Min Liu,Jialin Zhu,Jiazheng Wang,Jiahao Zhao,Wenting Shen,Feixiang He,Xiang Che*

Main category: cs.CV

TL;DR: The paper identifies optimization issues in 3D Gaussian Splatting (3DGS) and proposes AdamW-GS, a decoupled optimization method that improves efficiency and representation quality.


<details>
  <summary>Details</summary>
Motivation: Current 3DGS optimization inherits DNN practices but overlooks two critical coupling issues: (1) update step coupling causing optimizer state rescaling and costly attribute updates, and (2) gradient coupling in the moment leading to ineffective regularization. These under-explored problems limit optimization efficiency.

Method: The authors decouple the 3DGS optimization process into three components: Sparse Adam, Re-State Regularization, and Decoupled Attribute Regularization. They then re-couple beneficial components to create AdamW-GS, which is evaluated under both 3DGS and 3DGS-MCMC frameworks.

Result: Experimental analysis provides deeper understanding of optimization components. AdamW-GS achieves simultaneous improvements in both optimization efficiency and representation effectiveness compared to standard approaches.

Conclusion: The proposed AdamW-GS optimization method addresses overlooked coupling issues in 3DGS, offering better performance through careful decoupling and re-coupling of optimization components.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time novel view synthesis. As an explicit representation optimized through gradient propagation among primitives, optimization widely accepted in deep neural networks (DNNs) is actually adopted in 3DGS, such as synchronous weight updating and Adam with the adaptive gradient. However, considering the physical significance and specific design in 3DGS, there are two overlooked details in the optimization of 3DGS: (i) update step coupling, which induces optimizer state rescaling and costly attribute updates outside the viewpoints, and (ii) gradient coupling in the moment, which may lead to under- or over-effective regularization. Nevertheless, such a complex coupling is under-explored. After revisiting the optimization of 3DGS, we take a step to decouple it and recompose the process into: Sparse Adam, Re-State Regularization and Decoupled Attribute Regularization. Taking a large number of experiments under the 3DGS and 3DGS-MCMC frameworks, our work provides a deeper understanding of these components. Finally, based on the empirical analysis, we re-design the optimization and propose AdamW-GS by re-coupling the beneficial components, under which better optimization efficiency and representation effectiveness are achieved simultaneously.

</details>


### [38] [Automated Road Crack Localization to Guide Highway Maintenance](https://arxiv.org/abs/2601.16737)
*Steffen Knoblauch,Ram Kumar Muthusamy,Pedram Ghamisi,Alexander Zipf*

Main category: cs.CV

TL;DR: This paper proposes a framework using open-source data (airborne imagery + OSM) with fine-tuned YOLOv11 for highway crack detection, creating a Swiss Relative Highway Crack Density index to guide maintenance decisions.


<details>
  <summary>Details</summary>
Motivation: Climate change-induced temperature fluctuations are increasing stress on road pavements, leading to higher maintenance costs. There's a need for targeted, efficient maintenance strategies for highway infrastructure.

Method: Integrates airborne imagery and OpenStreetMap to fine-tune YOLOv11 for highway crack localization. Creates a Swiss Relative Highway Crack Density (RHCD) index to inform nationwide highway maintenance decisions.

Result: Crack classification model achieved F1-scores of 0.84 (crack) and 0.97 (no crack). Swiss RHCD index showed weak correlations with temperature amplitudes (r = -0.05) and traffic volume (r = 0.17). High RHCD values observed near urban centers and intersections.

Conclusion: Open-source data can drive innovation for efficient public sector solutions. The RHCD index provides added value over traditional data sources for guiding highway maintenance, with practical validation from observed patterns near urban areas.

Abstract: Highway networks are crucial for economic prosperity. Climate change-induced temperature fluctuations are exacerbating stress on road pavements, resulting in elevated maintenance costs. This underscores the need for targeted and efficient maintenance strategies. This study investigates the potential of open-source data to guide highway infrastructure maintenance. The proposed framework integrates airborne imagery and OpenStreetMap (OSM) to fine-tune YOLOv11 for highway crack localization. To demonstrate the framework's real-world applicability, a Swiss Relative Highway Crack Density (RHCD) index was calculated to inform nationwide highway maintenance. The crack classification model achieved an F1-score of $0.84$ for the positive class (crack) and $0.97$ for the negative class (no crack). The Swiss RHCD index exhibited weak correlations with Long-term Land Surface Temperature Amplitudes (LT-LST-A) (Pearson's $r\ = -0.05$) and Traffic Volume (TV) (Pearson's $r\ = 0.17$), underlining the added value of this novel index for guiding maintenance over other data. Significantly high RHCD values were observed near urban centers and intersections, providing contextual validation for the predictions. These findings highlight the value of open-source data sharing to drive innovation, ultimately enabling more efficient solutions in the public sector.

</details>


### [39] [Curated endoscopic retrograde cholangiopancreatography images dataset](https://arxiv.org/abs/2601.16759)
*Alda João Andrade,Mónica Martins,André Ferreira,Tarcísio Araújo,Luís Lopes,Victor Alves*

Main category: cs.CV

TL;DR: Researchers created a large, curated ERCP dataset to address the scarcity of public datasets for AI-based endoscopic diagnosis, providing over 38,000 images with expert annotations.


<details>
  <summary>Details</summary>
Motivation: The scarcity of public ERCP datasets limits the development and application of AI-based automated diagnosis systems for biliary and pancreatic diseases, creating a need for high-quality, annotated medical imaging data.

Method: Collected 19,018 raw images and 19,317 processed images from 1,602 patients, with 5,519 images manually annotated by experienced gastroenterologists (two with 5+ years experience, reviewed by one with 20+ years experience, all performing 400+ ERCP procedures annually).

Result: Created a comprehensive, ready-to-use ERCP dataset with expert annotations, validated through a classification experiment to demonstrate its utility and validity for AI applications.

Conclusion: This curated dataset addresses the critical gap in public ERCP data and aims to serve as a benchmark for developing and evaluating AI systems for automatic ERCP analysis and diagnosis of biliary and pancreatic diseases.

Abstract: Endoscopic Retrograde Cholangiopancreatography (ERCP) is a key procedure in the diagnosis and treatment of biliary and pancreatic diseases. Artificial intelligence has been pointed as one solution to automatize diagnosis. However, public ERCP datasets are scarce, which limits the use of such approach. Therefore, this study aims to help fill this gap by providing a large and curated dataset. The collection is composed of 19.018 raw images and 19.317 processed from 1.602 patients. 5.519 images are labeled, which provides a ready to use dataset. All images were manually inspected and annotated by two gastroenterologist with more than 5 years of experience and reviewed by another gastroenterologist with more than 20 years of experience, all with more than 400 ERCP procedures annually. The utility and validity of the dataset is proven by a classification experiment. This collection aims to provide or contribute for a benchmark in automatic ERCP analysis and diagnosis of biliary and pancreatic diseases.

</details>


### [40] [Flow Matching for Probabilistic Monocular 3D Human Pose Estimation](https://arxiv.org/abs/2601.16763)
*Cuong Le,Pavló Melnyk,Bastian Wandt,Mårten Wadenbäck*

Main category: cs.CV

TL;DR: FMPose: A probabilistic 3D human pose estimation method using flow matching generative approach to lift 2D poses to 3D with uncertainty modeling, outperforming diffusion methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: 3D human pose estimation from monocular views is ill-posed due to depth ambiguity. Existing methods often produce incorrect but overconfident 3D estimations. Probabilistic approaches that treat 3D estimations as distributions with uncertainty measurements are needed to address this problem.

Method: FMPose uses flow matching generative approach with optimal transport. It learns optimal transport from simple source distribution to plausible 3D human pose distribution via continuous normalizing flows, conditioned on 2D cues. Graph convolutional networks model 2D lifting conditions using learnable connections between human body joints as graph structure for feature aggregation.

Result: FMPose produces faster and more accurate 3D pose generations compared to diffusion-based methods. Shows major improvements over current state-of-the-art methods on three benchmarks: Human3.6M, MPI-INF-3DHP, and 3DPW.

Conclusion: FMPose effectively addresses the ill-posed nature of 3D human pose estimation from monocular views by combining probabilistic modeling with flow matching and graph convolutional networks, achieving superior performance on standard benchmarks.

Abstract: Recovering 3D human poses from a monocular camera view is a highly ill-posed problem due to the depth ambiguity. Earlier studies on 3D human pose lifting from 2D often contain incorrect-yet-overconfident 3D estimations. To mitigate the problem, emerging probabilistic approaches treat the 3D estimations as a distribution, taking into account the uncertainty measurement of the poses. Falling in a similar category, we proposed FMPose, a probabilistic 3D human pose estimation method based on the flow matching generative approach. Conditioned on the 2D cues, the flow matching scheme learns the optimal transport from a simple source distribution to the plausible 3D human pose distribution via continuous normalizing flows. The 2D lifting condition is modeled via graph convolutional networks, leveraging the learnable connections between human body joints as the graph structure for feature aggregation. Compared to diffusion-based methods, the FMPose with optimal transport produces faster and more accurate 3D pose generations. Experimental results show major improvements of our FMPose over current state-of-the-art methods on three common benchmarks for 3D human pose estimation, namely Human3.6M, MPI-INF-3DHP and 3DPW.

</details>


### [41] [AutoRegressive Generation with B-rep Holistic Token Sequence Representation](https://arxiv.org/abs/2601.16771)
*Jiahao Li,Yunpeng Bai,Yongkang Dai,Hao Guo,Hongping Gan,Yilei Shi*

Main category: cs.CV

TL;DR: BrepARG is the first method to encode B-rep geometry and topology into holistic token sequences, enabling sequence-based B-rep generation using autoregressive transformers.


<details>
  <summary>Details</summary>
Motivation: Previous graph-based B-rep approaches disentangle geometric and topological features through decoupled pipelines, preventing the use of sequence-based generative frameworks like transformers that have shown remarkable performance.

Method: Encodes B-rep into three token types: geometry tokens, position tokens, and face index tokens. Constructs holistic token sequences hierarchically by first building geometry blocks (faces/edges), then sequencing these blocks, and finally assembling the complete B-rep sequence. Uses a transformer-based autoregressive model with multi-layer decoder-only architecture and causal masking for next-token prediction.

Result: Achieves state-of-the-art (SOTA) performance in B-rep generation experiments.

Conclusion: BrepARG validates the feasibility of representing B-rep as holistic token sequences, opening new directions for B-rep generation research.

Abstract: Previous representation and generation approaches for the B-rep relied on graph-based representations that disentangle geometric and topological features through decoupled computational pipelines, thereby precluding the application of sequence-based generative frameworks, such as transformer architectures that have demonstrated remarkable performance. In this paper, we propose BrepARG, the first attempt to encode B-rep's geometry and topology into a holistic token sequence representation, enabling sequence-based B-rep generation with an autoregressive architecture. Specifically, BrepARG encodes B-rep into 3 types of tokens: geometry and position tokens representing geometric features, and face index tokens representing topology. Then the holistic token sequence is constructed hierarchically, starting with constructing the geometry blocks (i.e., faces and edges) using the above tokens, followed by geometry block sequencing. Finally, we assemble the holistic sequence representation for the entire B-rep. We also construct a transformer-based autoregressive model that learns the distribution over holistic token sequences via next-token prediction, using a multi-layer decoder-only architecture with causal masking. Experiments demonstrate that BrepARG achieves state-of-the-art (SOTA) performance. BrepARG validates the feasibility of representing B-rep as holistic token sequences, opening new directions for B-rep generation.

</details>


### [42] [CASP: Few-Shot Class-Incremental Learning with CLS Token Attention Steering Prompts](https://arxiv.org/abs/2601.16773)
*Shuai Huang,Xuhan Lin,Yuwu Lu*

Main category: cs.CV

TL;DR: CASP introduces CLS token attention steering prompts with bias parameters to modulate self-attention weights, plus attention perturbation and token mixup for better generalization in few-shot class-incremental learning.


<details>
  <summary>Details</summary>
Motivation: FSCIL requires models to adapt to new classes with limited samples while preventing catastrophic forgetting. Existing prompt-based methods need improvement for extreme few-shot settings where transfer and generalization capabilities are critical.

Method: Proposes CLS Token Attention Steering Prompts (CASP) with class-shared trainable bias parameters in query/key/value projections of CLS token to modulate self-attention weights. Also uses attention perturbation strategy and Manifold Token Mixup in shallow feature space to synthesize new class features.

Result: Outperforms state-of-the-art methods on CUB200, CIFAR100, and ImageNet-R datasets in both standard and fine-grained FSCIL settings, without fine-tuning during incremental phases and with significantly reduced parameter overhead.

Conclusion: CASP effectively leverages pretrained knowledge through CLS token attention steering and feature synthesis techniques, achieving superior performance in few-shot class-incremental learning with minimal parameter requirements.

Abstract: Few-shot class-incremental learning (FSCIL) presents a core challenge in continual learning, requiring models to rapidly adapt to new classes with very limited samples while mitigating catastrophic forgetting. Recent prompt-based methods, which integrate pretrained backbones with task-specific prompts, have made notable progress. However, under extreme few-shot incremental settings, the model's ability to transfer and generalize becomes critical, and it is thus essential to leverage pretrained knowledge to learn feature representations that can be shared across future categories during the base session. Inspired by the mechanism of the CLS token, which is similar to human attention and progressively filters out task-irrelevant information, we propose the CLS Token Attention Steering Prompts (CASP). This approach introduces class-shared trainable bias parameters into the query, key, and value projections of the CLS token to explicitly modulate the self-attention weights. To further enhance generalization, we also design an attention perturbation strategy and perform Manifold Token Mixup in the shallow feature space, synthesizing potential new class features to improve generalization and reserve the representation capacity for upcoming tasks. Experiments on the CUB200, CIFAR100, and ImageNet-R datasets demonstrate that CASP outperforms state-of-the-art methods in both standard and fine-grained FSCIL settings without requiring fine-tuning during incremental phases and while significantly reducing the parameter overhead.

</details>


### [43] [SLD: Segmentation-Based Landmark Detection for Spinal Ligaments](https://arxiv.org/abs/2601.16782)
*Lara Blomenkamp,Ivanna Kramer,Sabine Bauer,Theresa Schöche*

Main category: cs.CV

TL;DR: Novel automated method for detecting spinal ligament landmarks using shape-based segmentation and domain-specific rules, achieving high accuracy across all spinal regions.


<details>
  <summary>Details</summary>
Motivation: Precise identification of ligament attachment points is crucial for realistic biomechanical spine modeling, but existing automated methods are either region-limited or insufficiently accurate.

Method: Two-stage approach: first performs shape-based segmentation of 3D vertebrae, then applies domain-specific rules to identify different types of ligament attachment points.

Result: Outperforms existing approaches with high accuracy and strong generalization across all spinal regions. Validation on two independent datasets showed MAE of 0.7 mm and RMSE of 1.1 mm.

Conclusion: The proposed method provides a reliable, accurate, and generalizable solution for automated detection of spinal ligament landmarks, enabling more realistic biomechanical spine simulations.

Abstract: In biomechanical modeling, the representation of ligament attachments is crucial for a realistic simulation of the forces acting between the vertebrae. These forces are typically modeled as vectors connecting ligament landmarks on adjacent vertebrae, making precise identification of these landmarks a key requirement for constructing reliable spine models. Existing automated detection methods are either limited to specific spinal regions or lack sufficient accuracy. This work presents a novel approach for detecting spinal ligament landmarks, which first performs shape-based segmentation of 3D vertebrae and subsequently applies domain-specific rules to identify different types of attachment points. The proposed method outperforms existing approaches by achieving high accuracy and demonstrating strong generalization across all spinal regions. Validation on two independent spinal datasets from multiple patients yielded a mean absolute error (MAE) of 0.7 mm and a root mean square error (RMSE) of 1.1 mm.

</details>


### [44] [REL-SF4PASS: Panoramic Semantic Segmentation with REL Depth Representation and Spherical Fusion](https://arxiv.org/abs/2601.16788)
*Xuewei Li,Xinghan Bao,Zhimin Chen,Xi Li*

Main category: cs.CV

TL;DR: REL-SF4PASS improves panoramic semantic segmentation using novel REL depth representation in cylindrical coordinates and Spherical-dynamic Multi-Modal Fusion (SMMF) for better geometry utilization and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing PASS methods don't fully utilize panoramic image geometry - they either focus on spherical geometry with RGB input or use depth in original/HHA format, missing comprehensive 3D spatial representation.

Method: Proposes REL depth representation (Rectified Depth, Elevation-Gained Vertical Inclination Angle, Lateral Orientation Angle) in cylindrical coordinates, plus Spherical-dynamic Multi-Modal Fusion (SMMF) that uses different fusion strategies for different panoramic regions to reduce ERP projection distortion.

Result: Achieves 2.35% average mIoU improvement on all 3 folds of Stanford2D3D Panoramic datasets and reduces performance variance by ~70% when facing 3D disturbances.

Conclusion: REL-SF4PASS significantly improves panoramic semantic segmentation performance and robustness by better utilizing panoramic geometry through cylindrical coordinate representation and adaptive multi-modal fusion.

Abstract: As an important and challenging problem in computer vision, Panoramic Semantic Segmentation (PASS) aims to give complete scene perception based on an ultra-wide angle of view. Most PASS methods often focus on spherical geometry with RGB input or using the depth information in original or HHA format, which does not make full use of panoramic image geometry. To address these shortcomings, we propose REL-SF4PASS with our REL depth representation based on cylindrical coordinate and Spherical-dynamic Multi-Modal Fusion SMMF. REL is made up of Rectified Depth, Elevation-Gained Vertical Inclination Angle, and Lateral Orientation Angle, which fully represents 3D space in cylindrical coordinate style and the surface normal direction. SMMF aims to ensure the diversity of fusion for different panoramic image regions and reduce the breakage of cylinder side surface expansion in ERP projection, which uses different fusion strategies to match the different regions in panoramic images. Experimental results show that REL-SF4PASS considerably improves performance and robustness on popular benchmark, Stanford2D3D Panoramic datasets. It gains 2.35% average mIoU improvement on all 3 folds and reduces the performance variance by approximately 70% when facing 3D disturbance.

</details>


### [45] [Incorporating Eye-Tracking Signals Into Multimodal Deep Visual Models For Predicting User Aesthetic Experience In Residential Interiors](https://arxiv.org/abs/2601.16811)
*Chen-Ying Chien,Po-Chih Kuo*

Main category: cs.CV

TL;DR: Dual-branch CNN-LSTM framework fuses visual features with eye-tracking signals to predict aesthetic evaluations of interior spaces, achieving 72.2% accuracy on objective dimensions and 66.8% on subjective dimensions.


<details>
  <summary>Details</summary>
Motivation: Predicting aesthetic experiences in interior design is challenging due to subjective perception and complex visual responses. Current methods struggle to capture how people perceive and evaluate interior spaces for well-being promotion.

Method: Developed a dual-branch CNN-LSTM framework that combines visual features from interior design videos with synchronized eye-tracking signals (gaze data and pupil responses). Collected dataset of 224 interior design videos with gaze data from 28 participants who rated 15 aesthetic dimensions.

Result: Model achieved 72.2% accuracy on objective dimensions (e.g., light) and 66.8% on subjective dimensions (e.g., relaxation), outperforming state-of-the-art video baselines. Models trained with eye-tracking retain comparable performance when deployed with visual input alone. Pupil responses contribute most to objective assessments, while gaze+visual combination enhances subjective evaluations.

Conclusion: Eye-tracking serves as valuable privileged information during training, enabling more practical tools for aesthetic assessment in interior design. The framework demonstrates that incorporating physiological signals improves prediction of both objective and subjective aesthetic evaluations.

Abstract: Understanding how people perceive and evaluate interior spaces is essential for designing environments that promote well-being. However, predicting aesthetic experiences remains difficult due to the subjective nature of perception and the complexity of visual responses. This study introduces a dual-branch CNN-LSTM framework that fuses visual features with eye-tracking signals to predict aesthetic evaluations of residential interiors. We collected a dataset of 224 interior design videos paired with synchronized gaze data from 28 participants who rated 15 aesthetic dimensions. The proposed model attains 72.2% accuracy on objective dimensions (e.g., light) and 66.8% on subjective dimensions (e.g., relaxation), outperforming state-of-the-art video baselines and showing clear gains on subjective evaluation tasks. Notably, models trained with eye-tracking retain comparable performance when deployed with visual input alone. Ablation experiments further reveal that pupil responses contribute most to objective assessments, while the combination of gaze and visual cues enhances subjective evaluations. These findings highlight the value of incorporating eye-tracking as privileged information during training, enabling more practical tools for aesthetic assessment in interior design.

</details>


### [46] [ColorConceptBench: A Benchmark for Probabilistic Color-Concept Understanding in Text-to-Image Models](https://arxiv.org/abs/2601.16836)
*Chenxi Ruan,Yu Xiao,Yihan Hou,Guosheng Hu,Wei Zeng*

Main category: cs.CV

TL;DR: ColorConceptBench is a new benchmark for evaluating how text-to-image models associate colors with implicit concepts, revealing current models lack sensitivity to abstract color semantics that resists standard scaling interventions.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models have advanced considerably, but their capability to associate colors with implicit concepts remains underexplored. There's a need to systematically evaluate how models translate abstract color concepts beyond explicit color names or codes.

Method: The authors introduce ColorConceptBench, a human-annotated benchmark with 1,281 implicit color concepts and 6,369 human annotations. It evaluates color-concept associations through probabilistic color distributions rather than explicit color specifications. They tested seven leading T2I models and examined standard interventions like scaling and guidance.

Result: Evaluation reveals current T2I models lack sensitivity to abstract color semantics. Crucially, this limitation appears resistant to standard interventions like model scaling and guidance techniques. The findings suggest current approaches cannot achieve human-like color understanding through simple scaling alone.

Conclusion: Achieving human-like color semantics in T2I models requires more than larger models or standard interventions. It demands a fundamental shift in how models learn and represent implicit meaning, moving beyond current architectural and training paradigms.

Abstract: While text-to-image (T2I) models have advanced considerably, their capability to associate colors with implicit concepts remains underexplored. To address the gap, we introduce ColorConceptBench, a new human-annotated benchmark to systematically evaluate color-concept associations through the lens of probabilistic color distributions. ColorConceptBench moves beyond explicit color names or codes by probing how models translate 1,281 implicit color concepts using a foundation of 6,369 human annotations. Our evaluation of seven leading T2I models reveals that current models lack sensitivity to abstract semantics, and crucially, this limitation appears resistant to standard interventions (e.g., scaling and guidance). This demonstrates that achieving human-like color semantics requires more than larger models, but demands a fundamental shift in how models learn and represent implicit meaning.

</details>


### [47] [No Validation, No Problem: Predicting Model Performance from a Single Gradient](https://arxiv.org/abs/2601.16874)
*Fangzheng Wu,Brian Summa*

Main category: cs.CV

TL;DR: A validation-free checkpoint selection method using classifier-head gradient norm as a proxy metric that works across CNNs, Transformers, and diffusion models with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Traditional checkpoint selection requires validation sets and labels, which can be expensive and sometimes unavailable. The paper aims to develop a lightweight, label-free method for selecting the best checkpoint during training without needing validation data.

Method: Proposes using the Frobenius norm of the classifier-head gradient from a single forward-backward pass on one detached-feature batch as a proxy metric. The method computes ||g||_F = ||dL/dW||_F and selects checkpoints with minimum gradient norm in a tail window. Different normalization schemes are used for different architectures: head-scale normalization for classic CNNs and feature-scale normalization for Transformers and modern CNNs.

Result: The method achieves 4.24% +/- 2.00% gap to oracle with universal setup, reducing to about 1.12% with per-family tuning. It works across ImageNet-1k CNNs and Transformers, predicts COCO detection/segmentation mAP, and tracks progress in diffusion models (UNet/DDPM on CIFAR-10). The probe adds less than 0.1% of an epoch overhead and enables near-oracle checkpoint selection.

Conclusion: The classifier-head gradient norm provides an effective, lightweight, and validation-free proxy for checkpoint selection across diverse architectures, closing most of the gap to oracle performance with minimal computational cost.

Abstract: We propose a validation-free checkpointing signal from a single forward-backward pass: the Frobenius norm of the classifier-head gradient on one detached-feature batch, ||g||_F = ||dL/dW||_F. Across ImageNet-1k CNNs and Transformers, this proxy is strongly negative with Top-1 and positive with loss. Selecting the checkpoint with the minimum head gradient in a short tail window closes most of the gap to the oracle (4.24% +/- 2.00% with a universal setup, about 1.12% with light per-family tuning). For practical deployment, a head-scale normalization is more stable within classic CNN families (e.g., ResNets), while a feature-scale normalization works well for Transformers and modern CNNs. The same one-batch probe also predicts COCO detection/segmentation mAP. In diffusion (UNet/DDPM on CIFAR-10), it tracks progress and enables near-oracle tail-window selection; it is positively correlated with same-distribution probe MSE and negatively with FID (lower is better), so it can be used as a lightweight, label-free monitor. Validation labels are never used beyond reporting. The probe adds much less than 0.1% of an epoch and works as a drop-in for validation-free checkpoint selection and early stopping.

</details>


### [48] [Evaluating Large Vision-language Models for Surgical Tool Detection](https://arxiv.org/abs/2601.16895)
*Nakul Poudel,Richard Simon,Cristian A. Linte*

Main category: cs.CV

TL;DR: Large vision-language models (VLMs) show strong potential for surgical tool detection, with Qwen2.5 outperforming other VLMs and demonstrating competitive performance against specialized detection models.


<details>
  <summary>Details</summary>
Motivation: Current AI systems in surgery are mostly unimodal, limiting holistic understanding of surgical workflows. There's a need for general-purpose surgical AI systems that can comprehensively model interrelated surgical scene components. Recent advances in large VLMs offer potential for modeling surgical tasks with human-like reasoning.

Method: Evaluated three state-of-the-art VLMs (Qwen2.5, LLaVA1.5, InternVL3.5) on the GraSP robotic surgery dataset for surgical tool detection. Tested both zero-shot and parameter-efficient LoRA fine-tuning settings. Compared performance with open-set detection baseline Grounding DINO.

Result: Qwen2.5 consistently achieved superior detection performance among VLMs in both zero-shot and fine-tuned configurations. Compared to Grounding DINO, Qwen2.5 showed stronger zero-shot generalization and comparable fine-tuned performance. Qwen2.5 demonstrated superior instrument recognition, while Grounding DINO showed stronger localization capabilities.

Conclusion: Large VLMs, particularly Qwen2.5, show promising capabilities for surgical tool detection, offering strong zero-shot generalization and competitive performance with specialized models. This suggests VLMs could be valuable for developing more comprehensive surgical AI systems with multimodal understanding.

Abstract: Surgery is a highly complex process, and artificial intelligence has emerged as a transformative force in supporting surgical guidance and decision-making. However, the unimodal nature of most current AI systems limits their ability to achieve a holistic understanding of surgical workflows. This highlights the need for general-purpose surgical AI systems capable of comprehensively modeling the interrelated components of surgical scenes. Recent advances in large vision-language models that integrate multimodal data processing offer strong potential for modeling surgical tasks and providing human-like scene reasoning and understanding. Despite their promise, systematic investigations of VLMs in surgical applications remain limited. In this study, we evaluate the effectiveness of large VLMs for the fundamental surgical vision task of detecting surgical tools. Specifically, we investigate three state-of-the-art VLMs, Qwen2.5, LLaVA1.5, and InternVL3.5, on the GraSP robotic surgery dataset under both zero-shot and parameter-efficient LoRA fine-tuning settings. Our results demonstrate that Qwen2.5 consistently achieves superior detection performance in both configurations among the evaluated VLMs. Furthermore, compared with the open-set detection baseline Grounding DINO, Qwen2.5 exhibits stronger zero-shot generalization and comparable fine-tuned performance. Notably, Qwen2.5 shows superior instrument recognition, while Grounding DINO demonstrates stronger localization.

</details>


### [49] [LoL: Longer than Longer, Scaling Video Generation to Hour](https://arxiv.org/abs/2601.16914)
*Justin Cui,Jie Wu,Ming Li,Tao Yang,Xiaojie Li,Rui Wang,Andrew Bai,Yuanhao Ban,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: The paper addresses sink-collapse in long-form video generation by proposing a training-free multi-head RoPE jitter method to break attention homogenization, enabling real-time streaming infinite-length video generation with minimal quality decay.


<details>
  <summary>Details</summary>
Motivation: Current autoregressive video generation models suffer from error accumulation and loss of long-term coherence. While attention sink frames help, they cause sink-collapse where generated content repeatedly reverts to sink frames, creating abrupt scene resets and cyclic motion patterns.

Method: Proposes a lightweight, training-free approach using multi-head RoPE (Rotary Position Embedding) jitter to break inter-head attention homogenization and mitigate long-horizon collapse. This addresses the inherent conflict between RoPE's periodic structure and multi-head attention mechanisms.

Result: The method successfully alleviates sink-collapse while preserving generation quality. Achieves real-time, streaming, infinite-length video generation with little quality decay. Demonstrates continuous videos up to 12 hours in length, among the longest publicly demonstrated results in streaming video generation.

Conclusion: The proposed training-free multi-head RoPE jitter effectively suppresses sink-collapse in long-form video generation, enabling robust infinite-length streaming video generation with maintained quality, representing a significant advancement in long-horizon video synthesis.

Abstract: Recent research in long-form video generation has shifted from bidirectional to autoregressive models, yet these methods commonly suffer from error accumulation and a loss of long-term coherence. While attention sink frames have been introduced to mitigate this performance decay, they often induce a critical failure mode we term sink-collapse: the generated content repeatedly reverts to the sink frame, resulting in abrupt scene resets and cyclic motion patterns. Our analysis reveals that sink-collapse originates from an inherent conflict between the periodic structure of Rotary Position Embedding (RoPE) and the multi-head attention mechanisms prevalent in current generative models. To address it, we propose a lightweight, training-free approach that effectively suppresses this behavior by introducing multi-head RoPE jitter that breaks inter-head attention homogenization and mitigates long-horizon collapse. Extensive experiments show that our method successfully alleviates sink-collapse while preserving generation quality. To the best of our knowledge, this work achieves the first demonstration of real-time, streaming, and infinite-length video generation with little quality decay. As an illustration of this robustness, we generate continuous videos up to 12 hours in length, which, to our knowledge, is among the longest publicly demonstrated results in streaming video generation.

</details>


### [50] [Reward-Forcing: Autoregressive Video Generation with Reward Feedback](https://arxiv.org/abs/2601.16933)
*Jingran Zhang,Ning Li,Yuanhao Ban,Andrew Bai,Justin Cui*

Main category: cs.CV

TL;DR: The paper proposes using reward signals instead of teacher models to guide autoregressive video generation, achieving comparable performance to state-of-the-art methods while simplifying training.


<details>
  <summary>Details</summary>
Motivation: Prior autoregressive video generation models heavily depend on teacher models, which limit performance when strong autoregressive teachers are unavailable, resulting in output quality that lags behind bidirectional models.

Method: The authors use reward signals to guide the autoregressive generation process, which simplifies training while maintaining high visual fidelity and temporal consistency without relying on teacher architectures.

Result: On VBench, the method achieves a total score of 84.92, closely matching state-of-the-art autoregressive methods (84.31) that require significant heterogeneous distillation, and in some cases surpasses similarly sized bidirectional models.

Conclusion: Reward-guided autoregressive generation provides an effective alternative to teacher-dependent approaches, enabling efficient and scalable video generation with competitive performance.

Abstract: While most prior work in video generation relies on bidirectional architectures, recent efforts have sought to adapt these models into autoregressive variants to support near real-time generation. However, such adaptations often depend heavily on teacher models, which can limit performance, particularly in the absence of a strong autoregressive teacher, resulting in output quality that typically lags behind their bidirectional counterparts. In this paper, we explore an alternative approach that uses reward signals to guide the generation process, enabling more efficient and scalable autoregressive generation. By using reward signals to guide the model, our method simplifies training while preserving high visual fidelity and temporal consistency. Through extensive experiments on standard benchmarks, we find that our approach performs comparably to existing autoregressive models and, in some cases, surpasses similarly sized bidirectional models by avoiding constraints imposed by teacher architectures. For example, on VBench, our method achieves a total score of 84.92, closely matching state-of-the-art autoregressive methods that score 84.31 but require significant heterogeneous distillation.

</details>


### [51] [Domain-invariant Mixed-domain Semi-supervised Medical Image Segmentation with Clustered Maximum Mean Discrepancy Alignment](https://arxiv.org/abs/2601.16954)
*Ba-Thinh Lam,Thanh-Huy Nguyen,Hoang-Thien Nguyen,Quang-Khai Bui-Tran,Nguyen Lan Vi Vu,Phat K. Huynh,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: Proposes a domain-invariant mixed-domain semi-supervised segmentation framework for medical images that handles unknown domain labels and severe domain gaps with few labeled examples.


<details>
  <summary>Details</summary>
Motivation: Deep learning for medical image segmentation requires large expert annotations and consistent data, but real-world scenarios have scarce annotations and images from multiple scanners/centers with unknown domain labels and severe domain gaps. Existing methods assume single domain shifts or explicit domain indices, which don't match real-world deployment.

Method: Proposes a framework with two key components: 1) Copy-Paste Mechanism (CPM) to augment training by transferring informative regions across domains, 2) Cluster Maximum Mean Discrepancy (CMMD) block that clusters unlabeled features and aligns them with labeled anchors via MMD objective to encourage domain-invariant representations. Integrated within a teacher-student framework.

Result: Experiments on Fundus and M&Ms benchmarks show the approach consistently surpasses semi-supervised and domain adaptation methods, achieving robust and precise segmentation with very few labeled examples and multiple unknown domain discrepancies.

Conclusion: Establishes a potential solution for mixed-domain semi-supervised medical image segmentation that addresses real-world challenges of scarce annotations and unknown domain shifts.

Abstract: Deep learning has shown remarkable progress in medical image semantic segmentation, yet its success heavily depends on large-scale expert annotations and consistent data distributions. In practice, annotations are scarce, and images are collected from multiple scanners or centers, leading to mixed-domain settings with unknown domain labels and severe domain gaps. Existing semi-supervised or domain adaptation approaches typically assume either a single domain shift or access to explicit domain indices, which rarely hold in real-world deployment. In this paper, we propose a domain-invariant mixed-domain semi-supervised segmentation framework that jointly enhances data diversity and mitigates domain bias. A Copy-Paste Mechanism (CPM) augments the training set by transferring informative regions across domains, while a Cluster Maximum Mean Discrepancy (CMMD) block clusters unlabeled features and aligns them with labeled anchors via an MMD objective, encouraging domain-invariant representations. Integrated within a teacher-student framework, our method achieves robust and precise segmentation even with very few labeled examples and multiple unknown domain discrepancies. Experiments on Fundus and M&Ms benchmarks demonstrate that our approach consistently surpasses semi-supervised and domain adaptation methods, establishing a potential solution for mixed-domain semi-supervised medical image segmentation.

</details>


### [52] [VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents](https://arxiv.org/abs/2601.16973)
*Zirui Wang,Junyi Zhang,Jiaxin Ge,Long Lian,Letian Fu,Lisa Dunlap,Ken Goldberg,XuDong Wang,Ion Stoica,David M. Chan,Sewon Min,Joseph E. Gonzalez*

Main category: cs.CV

TL;DR: VisGym introduces a comprehensive evaluation suite for Vision-Language Models in multi-step visual interactions, revealing significant limitations in frontier models' ability to integrate perception, memory, and action over long horizons.


<details>
  <summary>Details</summary>
Motivation: Modern VLMs lack proper characterization in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. There's a need for systematic evaluation of their capabilities in interactive settings.

Method: Created VisGym - a gymnasium of 17 environments spanning symbolic puzzles, real-image understanding, navigation, and manipulation. Provides flexible controls over difficulty, input representation, planning horizon, and feedback. Includes multi-step solvers that generate structured demonstrations for supervised finetuning.

Result: Frontier models struggle significantly in interactive settings, achieving only 46.6% success in easy configurations and 26.0% in hard configurations. Key limitations: models struggle with long context (perform worse with unbounded history), text-based tasks become harder when rendered visually, and they have difficulty in partially observable settings.

Conclusion: Explicit goal observations, textual feedback, and exploratory demonstrations yield consistent gains, highlighting concrete failure modes and pathways for improving multi-step visual decision-making in VLMs. The VisGym suite provides a valuable benchmark for future development.

Abstract: Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over difficulty, input representation, planning horizon, and feedback. We also provide multi-step solvers that generate structured demonstrations, enabling supervised finetuning. Our evaluations show that all frontier models struggle in interactive settings, achieving low success rates in both the easy (46.6%) and hard (26.0%) configurations. Our experiments reveal notable limitations: models struggle to effectively leverage long context, performing worse with an unbounded history than with truncated windows. Furthermore, we find that several text-based symbolic tasks become substantially harder once rendered visually. However, explicit goal observations, textual feedback, and exploratory demonstrations in partially observable or unknown-dynamics settings for supervised finetuning yield consistent gains, highlighting concrete failure modes and pathways for improving multi-step visual decision-making. Code, data, and models can be found at: https://visgym.github.io/.

</details>


### [53] [SyncLight: Controllable and Consistent Multi-View Relighting](https://arxiv.org/abs/2601.16981)
*David Serrano-Lozano,Anand Bhattad,Luis Herranz,Jean-François Lalonde,Javier Vazquez-Corral*

Main category: cs.CV

TL;DR: SyncLight is the first method for consistent parametric relighting across multiple uncalibrated views of static scenes, enabling precise lighting control across multi-view captures with a single reference edit.


<details>
  <summary>Details</summary>
Motivation: Existing single-view relighting methods fail to maintain lighting consistency across multiple views, which is essential for multi-camera broadcasts, stereoscopic cinema, and virtual production workflows.

Method: Uses a multi-view diffusion transformer trained with latent bridge matching formulation, enabling high-fidelity relighting of entire image sets in one inference step. Trained on a large-scale hybrid dataset of synthetic environments and real-world multi-view captures under calibrated illumination.

Result: Achieves consistent relighting across multiple views with precise control over light intensity and color. Surprisingly generalizes zero-shot to arbitrary numbers of viewpoints without requiring camera pose information, effectively propagating lighting changes across all views.

Conclusion: SyncLight enables practical relighting workflows for multi-view capture systems, addressing the critical need for lighting consistency in professional multi-camera production environments.

Abstract: We present SyncLight, the first method to enable consistent, parametric relighting across multiple uncalibrated views of a static scene. While single-view relighting has advanced significantly, existing generative approaches struggle to maintain the rigorous lighting consistency essential for multi-camera broadcasts, stereoscopic cinema, and virtual production. SyncLight addresses this by enabling precise control over light intensity and color across a multi-view capture of a scene, conditioned on a single reference edit. Our method leverages a multi-view diffusion transformer trained using a latent bridge matching formulation, achieving high-fidelity relighting of the entire image set in a single inference step. To facilitate training, we introduce a large-scale hybrid dataset comprising diverse synthetic environments -- curated from existing sources and newly designed scenes -- alongside high-fidelity, real-world multi-view captures under calibrated illumination. Surprisingly, though trained only on image pairs, SyncLight generalizes zero-shot to an arbitrary number of viewpoints, effectively propagating lighting changes across all views, without requiring camera pose information. SyncLight enables practical relighting workflows for multi-view capture systems.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [54] [Gesture Recognition from body-Worn RFID under Missing Data](https://arxiv.org/abs/2601.16301)
*Sahar Golipoor,Richard T. Brophy,Ying Liu,Reza Ghazalian,Stephan Sigg*

Main category: eess.SP

TL;DR: Hand-gesture recognition using passive body-worn reflective tags with a data processing pipeline for missing data recovery and a graph-based CNN achieving 98.13% accuracy for 21 gestures.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate hand-gesture recognition system using passive body-worn reflective tags, addressing the challenge of missing data in real-world scenarios and exploring optimal tag placement for gesture recognition.

Method: Proposed a data processing pipeline with linear/exponential interpolation and extrapolation for missing data recovery, plus imputation and proximity-based inference. Represented tags as nodes in a temporal graph with edges based on RSS and phase correlations, and trained a graph-based CNN with graph-based self-attention.

Result: Achieved 98.13% accuracy for recognizing 21 gestures, outperforming state-of-the-art methods. Under leave-one-person-out cross-validation, achieved 89.28% accuracy. Found that arm tag placements are more expressive than wrist tags - removing arm tags reduces accuracy by >10%, while removing wrist tag only reduces by ~2%.

Conclusion: Passive body-worn reflective tags with the proposed data processing pipeline and graph-based CNN architecture provide highly accurate gesture recognition. Arm tag placements are more critical for gesture recognition than wrist placements, suggesting optimal tag positioning strategies for practical applications.

Abstract: We explore hand-gesture recognition through the use of passive body-worn reflective tags. A data processing pipeline is proposed to address the issue of missing data. Specifically, missing information is recovered through linear and exponential interpolation and extrapolation. Furthermore, imputation and proximity-based inference are employed. We represent tags as nodes in a temporal graph, with edges formed based on correlations between received signal strength (RSS) and phase values across successive timestamps, and we train a graph-based convolutional neural network that exploits graph-based self-attention. The system outperforms state-of-the-art methods with an accuracy of 98.13% for the recognition of 21 gestures. We achieve 89.28% accuracy under leave-one-person-out cross-validation. We further investigate the contribution of various body locations on the recognition accuracy. Removing tags from the arms reduces accuracy by more than 10%, while removing the wrist tag only reduces accuracy by around 2%. Therefore, tag placements on the arms are more expressive for gesture recognition than on the wrist.

</details>


### [55] [Angle of Arrival Estimation for Gesture Recognition from reflective body-worn tags](https://arxiv.org/abs/2601.16303)
*Sahar Golipoor,Reza Ghazalian,Ines Lobato Mesquita,Stephan Sigg*

Main category: eess.SP

TL;DR: Using passive reflective tags for hand gesture recognition, the paper shows that Angle of Arrival (AoA) tracking outperforms traditional RSS and phase features, improving gesture recognition accuracy by up to 15%.


<details>
  <summary>Details</summary>
Motivation: Traditional hand gesture recognition using backscattered RSS and phase signals struggles with distinguishing similar gestures due to feature similarities. AoA varies characteristically during body motion and could provide better distinguishing features.

Method: First validates AoA estimation using MUSIC algorithm with fixed tags, then proposes AoA tracking method based on Kalman smoothing. Implements gesture recognition benchmarks comparing RSS/phase features with AoA-enhanced features.

Result: AoA tracking effectively differentiates gestures that RSS and phase alone cannot distinguish. Incorporating AoA features significantly boosts gesture recognition performance with improvements up to 15%.

Conclusion: AoA tracking provides superior distinguishing capability for hand gesture recognition compared to traditional RSS and phase features, offering substantial performance improvements for passive tag-based gesture recognition systems.

Abstract: We investigate hand gesture recognition by leveraging passive reflective tags worn on the body. Considering a large set of gestures, distinct patterns are difficult to be captured by learning algorithms using backscattered received signal strength (RSS) and phase signals. This is because these features often exhibit similarities across signals from different gestures. To address this limitation, we explore the estimation of Angle of Arrival (AoA) as a distinguishing feature, since AoA characteristically varies during body motion. To ensure reliable estimation in our system, which employs Smart Antenna Switching (SAS), we first validate AoA estimation using the Multiple SIgnal Classification (MUSIC) algorithm while the tags are fixed at specific angles. Building on this, we propose an AoA tracking method based on Kalman smoothing. Our analysis demonstrates that, while RSS and phase alone are insufficient for distinguishing certain gesture data, AoA tracking can effectively differentiate them. To evaluate the effectiveness of AoA tracking, we implement gesture recognition system benchmarks and show that incorporating AoA features significantly boosts their performance. Improvements of up to 15% confirm the value of AoA-based enhancement.

</details>


### [56] [TransfoREM: Transformer aided 3D Radio Environment Mapping](https://arxiv.org/abs/2601.16421)
*Gautham Reddy,Ismail Guvenc,Mihail L. Sichitiu,Arupjyoti Bhuyan,Bryton Petersen,Jason Abrahamson*

Main category: eess.SP

TL;DR: TransfoREM is a transformer-based 3D Radio Environment Map generation method that combines deterministic models and real-world data to provide cellular coverage mapping for UAVs at higher altitudes.


<details>
  <summary>Details</summary>
Motivation: Existing terrestrial cellular networks are optimized for ground-level coverage, providing poor and unreliable connectivity for UAVs due to limited side lobe coverage and flight dynamics. There's a need for accurate 3D coverage mapping at higher altitudes to support UAV operations.

Method: TransfoREM uses a transformer model that translates radio propagation mapping into a sequence prediction task. It combines deterministic channel models with real-world data to construct 3D Radio Environment Maps (REMs). The method is designed for integration at the base station level.

Result: TransfoREM demonstrates improved interpolation capability on real-world data compared to conventional Kriging and other machine learning techniques. It can build accurate REMs for higher altitudes.

Conclusion: TransfoREM provides an effective solution for 3D cellular coverage mapping that can be integrated into cellular networks at the BS level, enabling enhanced resource allocation, interference management, and spatial spectrum utilization for UAV connectivity.

Abstract: Providing reliable cellular connectivity to Unmanned Aerial Vehicles (UAV) is a key challenge, as existing terrestrial networks are deployed mainly for ground-level coverage. The cellular network coverage may be available for a limited range from the antenna side lobes, with poor connectivity further exacerbated by UAV flight dynamics. In this work, we propose TransfoREM, a 3D Radio Environment Map (REM) generation method that combines deterministic channel models and real-world data to map terrestrial network coverage at higher altitudes. At the core of our solution is a transformer model that translates radio propagation mapping into a sequence prediction task to construct REMs. Our results demonstrate that TransfoREM offers improved interpolation capability on real-world data compared against conventional Kriging and other machine learning (ML) techniques. Furthermore, TransfoREM is designed for holistic integration into cellular networks at the base station (BS) level, where it can build REMs, which can then be leveraged for enhanced resource allocation, interference management, and spatial spectrum utilization.

</details>


### [57] [Auditory Attention Decoding without Spatial Information: A Diotic EEG Study](https://arxiv.org/abs/2601.16442)
*Masahiro Yoshino,Haruki Yokota,Junya Hara,Yuichi Tanaka,Hiroshi Higashi*

Main category: eess.SP

TL;DR: Proposes an auditory attention decoding framework for diotic environments that maps EEG and speech signals to a shared latent space, achieving 72.70% accuracy (22.58% higher than state-of-the-art direction-based methods).


<details>
  <summary>Details</summary>
Motivation: Existing AAD research relies on dichotic environments with spatial cues, limiting applicability to real-world scenarios like cocktail parties where speakers overlap or move dynamically. There's a need for AAD systems that work in diotic environments without spatial cues.

Method: Proposes a framework that maps EEG and speech signals into a shared latent space using independent encoders. Uses wav2vec 2.0 for speech feature extraction with 2-layer 1D CNN encoding, and BrainNetwork architecture for EEG encoding. Identifies attended speech by calculating cosine similarity between EEG and speech representations.

Result: Achieves 72.70% accuracy on a diotic EEG dataset, which is 22.58% higher than the state-of-the-art direction-based AAD method.

Conclusion: The proposed diotic AAD framework effectively addresses limitations of spatial-reliant methods and shows promising performance for real-world applications like smart hearing aids and objective audiometry systems in complex auditory environments.

Abstract: Auditory attention decoding (AAD) identifies the attended speech stream in multi-speaker environments by decoding brain signals such as electroencephalography (EEG). This technology is essential for realizing smart hearing aids that address the cocktail party problem and for facilitating objective audiometry systems. Existing AAD research mainly utilizes dichotic environments where different speech signals are presented to the left and right ears, enabling models to classify directional attention rather than speech content. However, this spatial reliance limits applicability to real-world scenarios, such as the "cocktail party" situation, where speakers overlap or move dynamically. To address this challenge, we propose an AAD framework for diotic environments where identical speech mixtures are presented to both ears, eliminating spatial cues. Our approach maps EEG and speech signals into a shared latent space using independent encoders. We extract speech features using wav2vec 2.0 and encode them with a 2-layer 1D convolutional neural network (CNN), while employing the BrainNetwork architecture for EEG encoding. The model identifies the attended speech by calculating the cosine similarity between EEG and speech representations. We evaluate our method on a diotic EEG dataset and achieve 72.70% accuracy, which is 22.58% higher than the state-of-the-art direction-based AAD method.

</details>


### [58] [Cell-Free MIMO with Rotatable Antennas: When Macro-Diversity Meets Antenna Directivity](https://arxiv.org/abs/2601.16543)
*Xingxiang Peng,Qingqing Wu,Ziyuan Zheng,Yanze Zhu,Wen Chen,Penghui Huang,Ying Gao,Honghao Wang*

Main category: eess.SP

TL;DR: Cell-free networks with rotatable antennas improve worst-user rates by jointly optimizing beamforming and antenna orientations to enhance macro-diversity.


<details>
  <summary>Details</summary>
Motivation: Cell-free networks suffer from performance disparities due to user geometry and blockages, limiting macro-diversity benefits. Rotatable antennas provide a lightweight hardware solution to strengthen unfavorable links and improve fairness.

Method: Two approaches: 1) Alternating optimization algorithm iteratively updates beamformers via SOCP and optimizes orientations using successive convex approximation. 2) Two-stage scheme first designs orientations via manifold-aware Frank-Wolfe updates maximizing proportional-fair log-utility, then computes beamformers using SOCP.

Result: Proposed orientation-aware designs achieve substantially higher worst-user rates than conventional beamforming-only benchmarks. Larger antenna directivity enhances fairness with proper orientation but can degrade worst-user performance otherwise.

Conclusion: Rotatable antennas effectively address channel quality disparities in cell-free networks, enabling better exploitation of macro-diversity for higher and more uniform performance through joint optimization of beamforming and antenna orientations.

Abstract: Cell-free networks leverage distributed access points (APs) to achieve macro-diversity, yet their performance is often constrained by large disparities in channel quality arising from user geometry and blockages. To address this, rotatable antennas (RAs) add a lightweight hardware degree of freedom by steering the antenna boresight toward dominant propagation directions to strengthen unfavorable links, thereby enabling the network to better exploit macro-diversity for higher and more uniform performance. This paper investigates an RA-enabled cell-free downlink network and formulates a max-min rate problem that jointly optimizes transmit beamforming and antenna orientations. To tackle this challenging problem, we develop an alternating-optimization-based algorithm that iteratively updates the beamformers via a second-order cone program (SOCP) and optimizes the antenna orientations using successive convex approximation. To reduce complexity, we further propose an efficient two-stage scheme that first designs orientations by maximizing a proportional-fair log-utility using manifold-aware Frank-Wolfe updates, and then computes the beamformers using an SOCP-based design. Simulation results demonstrate that the proposed orientation-aware designs achieve a substantially higher worst-user rate than conventional beamforming-only benchmarks. Furthermore, larger antenna directivity enhances fairness with proper orientation but can degrade the worst-user performance otherwise.

</details>


### [59] [Spiking Neural Networks for Communication Systems: Encoding Schemes, Learning Algorithms, and Equalization~Techniques](https://arxiv.org/abs/2601.16550)
*Eike-Manuel Edelmann*

Main category: eess.SP

TL;DR: SNN-based receivers outperform ANN counterparts for nonlinear channels using novel encoding and optimization methods, achieving energy efficiency with low spike counts.


<details>
  <summary>Details</summary>
Motivation: Address the growing power consumption of ANN-based communication systems by leveraging SNNs' brain-inspired energy efficiency for signal processing in nonlinear time-invariant frequency-selective channels.

Method: Investigated SNN receiver design using backpropagation through time with surrogate gradients and novel quantization encoding (QE). Compared two receiver architectures, introduced policy gradient-based update (PGU) for encoding optimization without backpropagation.

Result: SNN-based receivers significantly outperform ANN-based counterparts. Decision feedback with QE achieves strong performance and low spike counts. PGU reduces runtime, complexity, and spikes per inference while maintaining performance.

Conclusion: Successfully developed a design and optimization framework for SNN-based receivers that addresses key SNN challenges, enabling future advances in energy-efficient communication systems.

Abstract: Machine learning with artificial neural networks (ANNs), provides solutions for the growing complexity of modern communication systems. This complexity, however, increases power consumption, making the systems energy-intensive. Spiking neural networks (SNNs) represent a novel generation of neural networks inspired by the highly efficient human brain. By emulating its event-driven and energy-efficient mechanisms, SNNs enable low-power, real-time signal processing. They differ from ANNs in two key ways: they exhibit inherent temporal dynamics and process and transmit information as short binary signals called spikes. Despite their promise, major challenges remain, e.g., identifying optimal learning rules and effective neural encoding. This thesis investigates the design of SNN-based receivers for nonlinear time-invariant frequency-selective channels. Backpropagation through time with surrogate gradients is identified as a promising update rule and the novel quantization encoding (QE) as promising neural encoding. Given the model of the intensity modulation with direct detection link, we compare two different receiver architectures based on equalization performance and spike count. Using decision feedback and QE achieves both strong performance and low spike counts. Notably, SNN-based receivers significantly outperform ANN-based counterparts. We furthermore introduce policy gradient-based update (PGU), an reinforcement learning-based update algorithm that requires no backpropagation. Using PGU, encoding parameters are optimized, drastically reducing runtime, complexity, and spikes per inference while maintaining performance. This thesis contributes a successful design and optimization framework for SNN-based receivers. By addressing key challenges in SNN optimization, it facilitates future advances in the design and deployment of energy-efficient SNN receivers.

</details>


### [60] [OFDM-Based ISAC Imaging of Extended Targets via Inverse Virtual Aperture Processing](https://arxiv.org/abs/2601.16664)
*Michael Negosanti,Lorenzo Pucci,Andrea Giorgetti*

Main category: eess.SP

TL;DR: ISAC system using IVA for imaging moving vehicles with MIMO-OFDM waveforms, evaluating performance via image contrast and range estimation accuracy, and analyzing sensing-communication trade-offs.


<details>
  <summary>Details</summary>
Motivation: To develop effective integrated sensing and communication (ISAC) systems for vehicular scenarios that can image moving extended targets using radio networks, addressing the need for next-generation radio networks with sensing capabilities.

Method: Uses inverse virtual aperture (IVA) imaging with MIMO-OFDM waveforms from a base station operating as monostatic sensor. Processes echoes with motion-compensation techniques to create range-Doppler images. Evaluates using 5G NR waveform in upper mid-band with 3GPP Release 19 vehicle model as distributed scatterers.

Result: Performance evaluated via image contrast (IC) and RMSE of estimated target-centroid range. Analyzes trade-off between sensing accuracy and communication efficiency by varying subcarrier allocation for IVA imaging.

Conclusion: Provides design insights for effective sensing strategies in next-generation radio networks by demonstrating IVA-based ISAC system performance and sensing-communication trade-offs for vehicular applications.

Abstract: This work investigates the performance of an integrated sensing and communication (ISAC) system exploiting inverse virtual aperture (IVA) for imaging moving extended targets in vehicular scenarios. A base station (BS) operates as a monostatic sensor using MIMO-OFDM waveforms. Echoes reflected by the target are processed through motion-compensation techniques to form an IVA range-Doppler (cross-range) image. A case study considers a 5G NR waveform in the upper mid-band, with the target model defined in 3GPP Release 19, representing a vehicle as a set of spatially distributed scatterers. Performance is evaluated in terms of image contrast (IC) and the root mean squared error (RMSE) of the estimated target-centroid range. Finally, the trade-off between sensing accuracy and communication efficiency is examined by varying the subcarrier allocation for IVA imaging. The results provide insights for designing effective sensing strategies in next-generation radio networks.

</details>


### [61] [Real-Time Evaluation of an Ultra-Tight GNSS/INS Integration Based on Adaptive PLL Bandwidth](https://arxiv.org/abs/2601.16577)
*Gaël Pages,Priot Benoît,Guillaume Beaugendre*

Main category: eess.SP

TL;DR: Proposes an ultra-tight GNSS/INS coupling using vector tracking loops with adaptive PLL bandwidth based on INS data, implemented efficiently on FPGA without extra resources.


<details>
  <summary>Details</summary>
Motivation: To create a more efficient GNSS/INS integration that is easily implementable on FPGA hardware without increasing resource usage or requiring parallel scalar loops or pre-stored ephemeris data.

Method: Uses vector tracking loop architecture with adaptive phase lock loop bandwidth controlled by inertial navigation system information. Decodes navigation messages within the loop, eliminating need for parallel scalar loops or pre-downloaded ephemeris. Implemented on Zynq-Ultrascale FPGA with one acquisition module and 16 tracking channels (8 GPS L1/C, 8 Galileo E1).

Result: The architecture is FPGA-implementable with minimal modifications to existing GNSS receivers, doesn't increase FPGA area usage, avoids additional storage resources, and maintains full functionality for GPS and Galileo signals.

Conclusion: The proposed ultra-tight GNSS/INS coupling with vector tracking and adaptive PLL bandwidth offers an efficient, resource-conscious solution for FPGA implementation that improves integration without hardware overhead.

Abstract: In this contribution, we propose a GNSS/INS ultra-tight coupling in which the GNSS receiver architecture is based on a vector tracking loop type architecture. In the proposed approach, the phase lock loop bandwidth is adapted according to the inertial navigation system information. The latter has the advantage to be easily implementable on a System-on-Chip component such as an FPGA (Field-Programmable Gate Arrays), and can be implemented with minor modifications on an existing GNSS receiver platform. Moreover, compared to classical vector-based solutions, the proposed architecture decodes the navigation message in the loop, without the need to run scalar loops in parallel or having to store pre-downloaded ephemeris data. This architecture therefore does not increase the area occupied on the FPGA and does not use additional resources for storage. The proposed GNSS receiver architecture uses GPS L1/C and Galileo E1 signals and is composed of one acquisition module and 16 tracking channels (8 GPS and 8 Galileo) which are implemented within a FPGA (Zynq-Ultrascale).

</details>


### [62] [Learning Successive Interference Cancellation for Low-Complexity Soft-Output MIMO Detection](https://arxiv.org/abs/2601.16586)
*Benedikt Fesl,Fatih Capar*

Main category: eess.SP

TL;DR: recurSIC: A lightweight learning-based MIMO detection framework inspired by SIC that generates reliable soft information with low complexity, suitable for edge devices like 5G RedCap and IoT.


<details>
  <summary>Details</summary>
Motivation: Low-complexity MIMO detection is crucial for resource-constrained edge devices (5G RedCap, IoT) that need to balance machine learning deployment with computational/memory constraints while supporting high-order modulation and providing reliable soft information for channel decoding.

Method: Proposes recurSIC, a lightweight learning-based MIMO detection framework structurally inspired by successive interference cancellation (SIC) with learned processing stages. Uses multi-path hypothesis tracking with tunable complexity parameter, requiring only single forward pass and minimal parameters.

Result: Numerical results in realistic wireless scenarios show recurSIC achieves strong hard- and soft-detection performance at very low complexity.

Conclusion: recurSIC is well suited for edge-constrained MIMO receivers, providing accurate detection with reliable soft information while meeting strict computational and memory constraints.

Abstract: Low-complexity multiple-input multiple-output (MIMO) detection remains a key challenge in modern wireless systems, particularly for 5G reduced capability (RedCap) and internet-of-things (IoT) devices. In this context, the growing interest in deploying machine learning on edge devices must be balanced against stringent constraints on computational complexity and memory while supporting high-order modulation. Beyond accurate hard detection, reliable soft information is equally critical, as modern receivers rely on soft-input channel decoding, imposing additional requirements on the detector design. In this work, we propose recurSIC, a lightweight learning-based MIMO detection framework that is structurally inspired by successive interference cancellation (SIC) and incorporates learned processing stages. It generates reliable soft information via multi-path hypothesis tracking with a tunable complexity parameter while requiring only a single forward pass and a minimal parameter count. Numerical results in realistic wireless scenarios show that recurSIC achieves strong hard- and soft-detection performance at very low complexity, making it well suited for edge-constrained MIMO receivers.

</details>


### [63] [Assessment of Errors of Fundamental Frequency Estimation Methods in the Presence of Voltage Fluctuations and Distortions](https://arxiv.org/abs/2601.16606)
*Antonio Bracale,Pasquale De Falco,Piotr Kuwałek,Grzegorz Wiczyński*

Main category: eess.SP

TL;DR: Numerical simulation study comparing fundamental frequency estimation methods' accuracy under modern power grid conditions with voltage fluctuations and distortions.


<details>
  <summary>Details</summary>
Motivation: Fundamental frequency is crucial for power quality assessment, and accurate estimation under modern grid conditions (with voltage fluctuations and distortions) is needed for diagnostic purposes within short time windows.

Method: Numerical simulation studies using test signals that recreate power grid states with simultaneous voltage fluctuations and distortions. Multiple fundamental frequency estimation methods were evaluated, including the standard IEC 61000-4-30 method.

Result: Assessment of errors in various fundamental frequency estimation methods when analyzing signals similar to those found in modern power grids. The study provides comparative error analysis across different methods.

Conclusion: Conclusions are presented based on the conducted research, though specific findings about which methods perform best under modern grid conditions are not detailed in the abstract.

Abstract: The fundamental frequency is one of the parameters that define power quality. Correctly determining this parameter under the conditions that prevail in modern power grids is crucial. Diagnostic purposes often require an efficient estimation of this parameter within short time windows. Therefore, this article presents the results of numerical simulation studies that allow the assessment of errors in various fundamental frequency estimation methods, including the standard IEC 61000-4-30 method, when the analyzed signal has a form similar to that found in modern power grids. For the purposes of this study, a test signal was adopted recreating the states of the power grid, including the simultaneous occurrence of voltage fluctuations and distortions. Conclusions are presented based on conducted research.

</details>


### [64] [Low-Power On-Device Gesture Recognition with Einsum Networks](https://arxiv.org/abs/2601.16662)
*Sahar Golipoor,Lingyun Yao,Martin Andraud,Stephan Sigg*

Main category: eess.SP

TL;DR: Gesture recognition pipeline using Einsum Networks for distributed, resource-constrained devices, validated with RFID-based gesture recognition.


<details>
  <summary>Details</summary>
Motivation: Need for gesture recognition on distributed, resource-constrained devices with requirements for tractable inference, explainability, and energy efficiency.

Method: Distributed pipeline using Einsum Networks (probabilistic circuits) with task-specific processing units for RSS/phase/AoA processing and feature extraction, plus dedicated Einsum hardware, with decision aggregation for fusion.

Result: System outperforms benchmark models in low-power, body-worn, passive RFID-based gesture recognition scenario.

Conclusion: Einsum Networks provide effective solution for distributed gesture recognition on resource-constrained devices with advantages in inference, explainability, and energy efficiency.

Abstract: We design a gesture-recognition pipeline for networks of distributed, resource constrained devices utilising Einsum Networks. Einsum Networks are probabilistic circuits that feature a tractable inference, explainability, and energy efficiency. The system is validated in a scenario of low-power, body-worn, passive Radio Frequency Identification-based gesture recognition. Each constrained device includes task-specific processing units responsible for Received Signal Strength (RSS) and phase processing or Angle of Arrival (AoA) estimation, along with feature extraction, as well as dedicated Einsum hardware that processes the extracted features. The output of all constrained devices is then fused in a decision aggregation module to predict gestures. Experimental results demonstrate that the method outperforms the benchmark models.

</details>


### [65] [Precise Low-Current Measurement Techniques for IoT Devices: A Case Study on MoleNet](https://arxiv.org/abs/2601.16727)
*Julian Block,Andreas Könsgen,Jens Dede,Anna Förster*

Main category: eess.SP

TL;DR: Comparison of source measurement units (SMUs) for precise measurement of very small currents in IoT devices, demonstrated on MoleNet sensor board.


<details>
  <summary>Details</summary>
Motivation: IoT devices often run on batteries for extended periods, requiring accurate power consumption measurements, especially for very small sleep mode currents that standard multimeters and oscilloscopes cannot measure effectively.

Method: Comparison of dedicated source measurement units (SMUs) that can measure very small currents with high precision, using the MoleNet IoT sensor board as an application example for current measurements.

Result: The report demonstrates that SMUs are suitable for measuring the very small currents in IoT devices, particularly during sleep modes, where traditional measurement tools are inadequate.

Conclusion: Dedicated SMUs are essential tools for accurately measuring power consumption in battery-powered IoT devices, enabling proper assessment before field deployment.

Abstract: Power consumption is a crucial aspect of IoT devices which often have to run on a battery for an extended period of time. Therefore, supply current measurements are crucial before deploying a device in the field. Multimeters and oscilloscopes are not well suited when it comes to measuring very small currents which occur e.g. when an IoT device is in sleep mode. In this report, we compare dedicated source measurement units (SMUs) which allow to measure very small currents with high precision. As an application example, we demonstrate current measurements on our MoleNet IoT sensor board.

</details>


### [66] [A Dynamic Parametric Simulator for Fetal Heart Sounds](https://arxiv.org/abs/2601.16792)
*Yingtong Zhou,Yiang Zhou,Zhengxian Qu,Kang Liu,Ting Tan*

Main category: eess.SP

TL;DR: Researchers developed an open-source parametric simulator for generating realistic fetal phonocardiogram signals to address challenges in fPCG research like limited data, maternal interference, and signal attenuation.


<details>
  <summary>Details</summary>
Motivation: Fetal phonocardiogram research faces three main challenges: limited abdominal recordings, substantial maternal interference, and transmission-induced signal attenuation, making reproducible benchmarking difficult.

Method: Created a dynamic parametric simulator that combines cycle-level fetal S1/S2 event synthesis with convolutional transmission module, configurable interference/noise, and model parameters calibrated from real abdominal recordings to capture beat-to-beat variability.

Result: Generated signals were validated against real recordings in terms of envelope-based temporal structure and frequency-domain characteristics, showing realistic simulation of fPCG signals.

Conclusion: The open-source simulator enables rapid, reproducible evaluation of fPCG processing methods under controlled acquisition conditions, addressing key challenges in fetal heart monitoring research.

Abstract: Research on fetal phonocardiogram (fPCG) is challenged by the limited number of abdominal recordings, substantial maternal interference, and marked transmissioninduced signal attenuation that complicate reproducible benchmarking. We present a reproducible dynamic parametric simulator that generates long abdominal fPCG sequences by combining cycle-level fetal S1/S2 event synthesis with a convolutional transmission module and configurable interference and background noise. Model parameters are calibrated cyclewise from real abdominal recordings to capture beat-to-beat variability and to define data-driven admissible ranges for controllable synthesis. The generated signals are validated against real recordings in terms of envelope-based temporal structure and frequency-domain characteristics. The simulator is released as open software to support rapid, reproducible evaluation of fPCG processing methods under controlled acquisition conditions.

</details>


### [67] [Hierarchical Distribution Matcher Design for Probabilistic Constellation Shaping Based on a Novel Semi-Analytical Optimization Approach](https://arxiv.org/abs/2601.16847)
*Pantea Nadimi Goki,Luca Potì*

Main category: eess.SP

TL;DR: A novel design procedure for hierarchical distribution matchers (HiDMs) in probabilistically shaped systems with analytical optimization of parameters, validated on 16QAM showing 2.8% shaping gain improvement.


<details>
  <summary>Details</summary>
Motivation: To develop practical hierarchical distribution matchers that are compatible with hardware constraints (ASICs/FPGAs) while optimizing performance metrics like energy loss, rate loss, and memory requirements for probabilistically shaped constellation systems.

Method: A semi-analytical optimization framework that analytically estimates lower bounds on energy loss, rate loss, and memory requirements for HiDMs approximating Maxwell Boltzmann distribution. The approach jointly optimizes rate and energy loss to select optimal hierarchical layers, memory size, and block length.

Result: The model shows good agreement between analytical predictions and simulated results for PAS 16QAM. At 200 Gbps with 25% FEC overhead, the optimized HiDM achieves 2.8% shaping gain improvement over previous solutions, evaluated via NGMI vs OSNR over AWGN channel.

Conclusion: The proposed analytical tool enables practical HiDM design compatible with hardware constraints while achieving significant performance improvements, making it suitable for implementation in modern communication systems using ASICs and FPGAs.

Abstract: A novel design procedure for practical hierarchical distribution matchers (HiDMs) in probabilistically shaped constellation systems is presented. The proposed approach enables the determination of optimal parameters for any target distribution matcher rate. Specifically, lower bounds on energy loss, rate loss, and memory requirements are analytically estimated for HiDM architectures approximating the Maxwell Boltzmann (MB) distribution. A semi analytical optimization framework is employed to jointly optimize rate and energy loss, allowing the selection of the number of hierarchical layers, memory size, and block length required to optimize channel capacity. The accuracy of the proposed model is validated through probabilistic amplitude shaping of 16QAM (PAS 16QAM), showing good agreement between analytical predictions and simulated results. The proposed analytical tool facilitates the design of HiDM structures that are compatible with practical hardware and implementation constraints, such as those imposed by state-of-the-art application-specific integrated circuits (ASICs) and field-programmable gate arrays (FPGAs). Furthermore, the performance of the optimized HiDM structure, incorporating layer selection based on lower-bound energy loss, is evaluated over the AWGN channel in terms of normalized generalized mutual information (NGMI) as a function of the optical signal-to-noise ratio (OSNR). At a net data rate of 200 Gbps with 25% forward error correction (FEC) overhead, the proposed scheme achieves a shaping gain improvement of 2.8% compared to previously reported solutions.

</details>


### [68] [IRS Compensation of Hyper-Rayleigh Fading: How Many Elements Are Needed?](https://arxiv.org/abs/2601.16915)
*Aleksey S. Gvozdarev*

Main category: eess.SP

TL;DR: The paper determines the minimum number of IRS elements needed to overcome severe fading conditions in multipath channels, finding that at least 6 elements are needed to escape worst-case fading and 14 elements to achieve no severe fading.


<details>
  <summary>Details</summary>
Motivation: To address the practical challenge of deploying Intelligent Reflecting Surfaces (IRS) in real-world scenarios by determining the minimum number of IRS elements required to compensate for severe fading conditions in multipath channels, particularly in Hyper-Rayleigh Regimes (HRRs) which represent worst-case fading conditions.

Method: Uses Inverse Power Lomax (IPL) channel model to quantify fading severity across different HRRs (full-HRR, strong-, weak-, and no-HRR). Derives closed-form channel coefficient envelope statistics for single IRS-element channels and provides tight approximations for channel coefficient and instantaneous SNR statistics for total IRS-assisted channels. Uses these expressions to estimate channel parameters corresponding to specific HRRs.

Result: When both source-IRS and IRS-destination links are in full-HRR (worst-case conditions), the minimum number of IRS elements needed to bring the total IRS-assisted link out of full-HRR is at least 6. To bring the total link into no-HRR (no severe fading), a minimum of 14 IRS elements is required, both valid across the entire IPL scale parameter range corresponding to full-HRR.

Conclusion: The study provides concrete guidelines for IRS deployment by establishing minimum element counts needed to overcome severe fading conditions. The findings show that relatively small IRS arrays (6-14 elements) can effectively mitigate worst-case fading scenarios, offering practical insights for IRS system design in challenging wireless environments.

Abstract: The letter introduces and studies the problem of defining the minimum number of Intelligent Reflecting Surface (IRS) elements needed to compensate for heavy fading conditions in multipath fading channels. The fading severity is quantified in terms of Hyper-Rayleigh Regimes (HRRs) (i.e., full-HRR (worst-case conditions), strong-, weak-, and no-HRR), and the channel model used (Inverse Power Lomax (IPL)) was chosen since it can account for all HRRs. The research presents the derived closed-form channel coefficient envelope statistics for the single IRS-element channel with IPL statistics in both subchannels and total IRS-assisted channel, as well as tight approximations for the channel coefficient and instantaneous signal-to-noise ratio (SNR) statistics for the latter. The derived expressions helped estimate channel parameters corresponding to the specific HRRs of the total channel and demonstrate that while both single links (i.e., ''source-IRS'' and ''IRS-destination'') are in full-HRR, the minimum number of IRS elements needed to bring the total IRS-assisted link (''source-IRS-destination'') out of full-HRR is no less than $6$ (for the whole range on the IPL scale parameter corresponding full-HRR). Furthermore, the minimum number of IRS elements required to bring the total IRS-assisted link into no-HRR is $14$ (under the same conditions).

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [69] [When Agents Fail to Act: A Diagnostic Framework for Tool Invocation Reliability in Multi-Agent LLM Systems](https://arxiv.org/abs/2601.16280)
*Donghao Huang,Gauri Malwe,Zhaoxia Wang*

Main category: cs.AI

TL;DR: A diagnostic framework for evaluating tool-use reliability in LLM-powered multi-agent systems, featuring a 12-category error taxonomy and systematic testing of 1,980 instances across various models and hardware configurations.


<details>
  <summary>Details</summary>
Motivation: Multi-agent LLM systems are transforming enterprise automation, but there's a lack of systematic evaluation methodologies for assessing tool-use reliability, especially for SME deployment in privacy-sensitive environments.

Method: Developed a comprehensive diagnostic framework with 12-category error taxonomy covering tool initialization, parameter handling, execution, and result interpretation. Systematically evaluated 1,980 deterministic test instances across open-weight models (Qwen2.5 series, Functionary) and proprietary models (GPT-4, Claude 3.5/3.7) on diverse edge hardware configurations.

Result: Procedural reliability, especially tool initialization failures, is the primary bottleneck for smaller models. Qwen2.5:32b achieves flawless performance matching GPT-4.1. Mid-sized models (qwen2.5:14b) offer practical trade-offs with 96.6% success rate and 7.3s latency on commodity hardware.

Conclusion: The framework establishes foundational infrastructure for systematic reliability evaluation of tool-augmented multi-agent AI systems, enabling cost-effective deployment for resource-constrained organizations with actionable reliability thresholds for production.

Abstract: Multi-agent systems powered by large language models (LLMs) are transforming enterprise automation, yet systematic evaluation methodologies for assessing tool-use reliability remain underdeveloped. We introduce a comprehensive diagnostic framework that leverages big data analytics to evaluate procedural reliability in intelligent agent systems, addressing critical needs for SME-centric deployment in privacy-sensitive environments. Our approach features a 12-category error taxonomy capturing failure modes across tool initialization, parameter handling, execution, and result interpretation. Through systematic evaluation of 1,980 deterministic test instances spanning both open-weight models (Qwen2.5 series, Functionary) and proprietary alternatives (GPT-4, Claude 3.5/3.7) across diverse edge hardware configurations, we identify actionable reliability thresholds for production deployment. Our analysis reveals that procedural reliability, particularly tool initialization failures, constitutes the primary bottleneck for smaller models, while qwen2.5:32b achieves flawless performance matching GPT-4.1. The framework demonstrates that mid-sized models (qwen2.5:14b) offer practical accuracy-efficiency trade-offs on commodity hardware (96.6\% success rate, 7.3 s latency), enabling cost-effective intelligent agent deployment for resource-constrained organizations. This work establishes foundational infrastructure for systematic reliability evaluation of tool-augmented multi-agent AI systems.

</details>


### [70] [SemanticALLI: Caching Reasoning, Not Just Responses, in Agentic Systems](https://arxiv.org/abs/2601.16286)
*Varun Chillara,Dylan Kline,Christopher Alvares,Evan Wooten,Huan Yang,Shlok Khetan,Cade Bauer,Tré Guillory,Tanishka Shah,Yashodhara Dhariwal,Volodymyr Pavlov,George Popstefanov*

Main category: cs.AI

TL;DR: SemanticALLI is a pipeline-aware architecture that caches structured intermediate representations in AI pipelines, achieving 83.10% cache hit rate by decomposing generation into analytic intent resolution and visualization synthesis stages.


<details>
  <summary>Details</summary>
Motivation: Agentic AI pipelines inefficiently reconstruct identical intermediate logic (like metric normalization or chart scaffolding) even with novel natural language inputs. Conventional monolithic caching fails because it treats inference as a black box and can't capture semantic redundancy.

Method: Introduces SemanticALLI architecture that decomposes generation into two stages: Analytic Intent Resolution (AIR) and Visualization Synthesis (VS). Elevates structured intermediate representations (IRs) to first-class, cacheable artifacts within the pipeline-aware design.

Result: Baseline monolithic caching achieves only 38.7% hit rate due to linguistic variance. SemanticALLI's structured approach enables Visualization Synthesis stage to achieve 83.10% hit rate, bypassing 4,023 LLM calls with median latency of 2.66 ms, reducing total token consumption.

Conclusion: Even when users rarely repeat themselves, AI pipelines often repeat work at stable, structured checkpoints where caching is most reliable. Pipeline-aware caching of intermediate representations offers substantial efficiency gains for AI system design.

Abstract: Agentic AI pipelines suffer from a hidden inefficiency: they frequently reconstruct identical intermediate logic, such as metric normalization or chart scaffolding, even when the user's natural language phrasing is entirely novel. Conventional boundary caching fails to capture this inefficiency because it treats inference as a monolithic black box.
  We introduce SemanticALLI, a pipeline-aware architecture within Alli (PMG's marketing intelligence platform), designed to operationalize redundant reasoning. By decomposing generation into Analytic Intent Resolution (AIR) and Visualization Synthesis (VS), SemanticALLI elevates structured intermediate representations (IRs) to first-class, cacheable artifacts.
  The impact of caching within the agentic loop is substantial. In our evaluation, baseline monolithic caching caps at a 38.7% hit rate due to linguistic variance. In contrast, our structured approach allows for an additional stage, the Visualization Synthesis stage, to achieve an 83.10% hit rate, bypassing 4,023 LLM calls with a median latency of just 2.66 ms. This internal reuse reduces total token consumption, offering a practical lesson for AI system design: even when users rarely repeat themselves, the pipeline often does, at stable, structured checkpoints where caching is most reliable.

</details>


### [71] [DSGym: A Holistic Framework for Evaluating and Training Data Science Agents](https://arxiv.org/abs/2601.16344)
*Fan Nie,Junlin Wang,Harper Hua,Federico Bianchi,Yongchan Kwon,Zhenting Qi,Owen Queen,Shang Zhu,James Zou*

Main category: cs.AI

TL;DR: DSGym is a standardized framework for evaluating and training data science agents with realistic data grounding, addressing limitations of existing fragmented benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing data science benchmarks have fragmented evaluation interfaces, narrow task coverage, and lack rigorous data grounding - many tasks can be solved without using actual data, which doesn't reflect real-world data science challenges.

Method: Introduces DSGym: a modular framework with self-contained execution environments, DSGym-Tasks suite (standardized existing benchmarks + new tasks), DSBio (bioinformatics tasks), DSPredict (prediction tasks), and an execution-verified data synthesis pipeline for training.

Result: Built a 2,000-example training set and trained a 4B model that outperforms GPT-4o on standardized analysis benchmarks, demonstrating DSGym's effectiveness for both evaluation and training.

Conclusion: DSGym enables rigorous end-to-end measurement of whether agents can plan, implement, and validate data analyses in realistic scientific contexts, serving as an extensible testbed for data science agent development.

Abstract: Data science agents promise to accelerate discovery and insight-generation by turning data into executable analyses and findings. Yet existing data science benchmarks fall short due to fragmented evaluation interfaces that make cross-benchmark comparison difficult, narrow task coverage and a lack of rigorous data grounding. In particular, we show that a substantial portion of tasks in current benchmarks can be solved without using the actual data. To address these limitations, we introduce DSGym, a standardized framework for evaluating and training data science agents in self-contained execution environments. Unlike static benchmarks, DSGym provides a modular architecture that makes it easy to add tasks, agent scaffolds, and tools, positioning it as a live, extensible testbed. We curate DSGym-Tasks, a holistic task suite that standardizes and refines existing benchmarks via quality and shortcut solvability filtering. We further expand coverage with (1) DSBio: expert-derived bioinformatics tasks grounded in literature and (2) DSPredict: challenging prediction tasks spanning domains such as computer vision, molecular prediction, and single-cell perturbation. Beyond evaluation, DSGym enables agent training via execution-verified data synthesis pipeline. As a case study, we build a 2,000-example training set and trained a 4B model in DSGym that outperforms GPT-4o on standardized analysis benchmarks. Overall, DSGym enables rigorous end-to-end measurement of whether agents can plan, implement, and validate data analyses in realistic scientific context.

</details>


### [72] [Doc2AHP: Inferring Structured Multi-Criteria Decision Models via Semantic Trees with LLMs](https://arxiv.org/abs/2601.16479)
*Hongjia Wu,Shuai Zhou,Hongxin Zhang,Wei Chen*

Main category: cs.AI

TL;DR: Doc2AHP bridges LLMs' generalization with AHP's rigor by using AHP principles as constraints to guide LLMs in structured decision-making from unstructured documents, eliminating expert bottlenecks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with structural consistency in complex decision-making, while classical decision theories like AHP require labor-intensive domain expertise, creating an "expert bottleneck" that limits scalability.

Method: Doc2AHP uses AHP structural principles as constraints to guide LLMs in constrained search within unstructured document space, enforcing logical entailment between parent-child nodes. Includes multi-agent weighting mechanism with adaptive consistency optimization for numerical consistency.

Result: Empirical results show Doc2AHP enables non-experts to construct high-quality decision models from scratch and significantly outperforms direct generative baselines in both logical completeness and downstream task accuracy.

Conclusion: Doc2AHP successfully bridges the gap between LLM generalization capabilities and decision theory rigor, providing a scalable framework for structured decision-making without requiring extensive annotated data or manual intervention.

Abstract: While Large Language Models (LLMs) demonstrate remarkable proficiency in semantic understanding, they often struggle to ensure structural consistency and reasoning reliability in complex decision-making tasks that demand rigorous logic. Although classical decision theories, such as the Analytic Hierarchy Process (AHP), offer systematic rational frameworks, their construction relies heavily on labor-intensive domain expertise, creating an "expert bottleneck" that hinders scalability in general scenarios. To bridge the gap between the generalization capabilities of LLMs and the rigor of decision theory, we propose Doc2AHP, a novel structured inference framework guided by AHP principles. Eliminating the need for extensive annotated data or manual intervention, our approach leverages the structural principles of AHP as constraints to direct the LLM in a constrained search within the unstructured document space, thereby enforcing the logical entailment between parent and child nodes. Furthermore, we introduce a multi-agent weighting mechanism coupled with an adaptive consistency optimization strategy to ensure the numerical consistency of weight allocation. Empirical results demonstrate that Doc2AHP not only empowers non-expert users to construct high-quality decision models from scratch but also significantly outperforms direct generative baselines in both logical completeness and downstream task accuracy.

</details>


### [73] [SycoEval-EM: Sycophancy Evaluation of Large Language Models in Simulated Clinical Encounters for Emergency Care](https://arxiv.org/abs/2601.16529)
*Dongshen Peng,Yi Wang,Carl Preiksaitis,Christian Rose*

Main category: cs.AI

TL;DR: LLMs in clinical settings are vulnerable to patient persuasion for inappropriate care, with SycoEval-EM framework showing 0-100% acquiescence rates across models, highlighting need for adversarial testing.


<details>
  <summary>Details</summary>
Motivation: LLMs show promise for clinical decision support but risk acquiescing to patient pressure for inappropriate care, creating safety concerns that static benchmarks fail to capture.

Method: SycoEval-EM multi-agent simulation framework evaluates LLM robustness through adversarial patient persuasion in emergency medicine across 20 LLMs and 1,875 encounters spanning three Choosing Wisely scenarios.

Result: Acquiescence rates ranged 0-100% across models, with higher vulnerability to imaging requests (38.8%) than opioid prescriptions (25.0%). All persuasion tactics were equally effective (30.0-36.0%), indicating general susceptibility rather than tactic-specific weakness.

Conclusion: Static benchmarks inadequately predict safety under social pressure, necessitating multi-turn adversarial testing for clinical AI certification to ensure robustness against patient persuasion.

Abstract: Large language models (LLMs) show promise in clinical decision support yet risk acquiescing to patient pressure for inappropriate care. We introduce SycoEval-EM, a multi-agent simulation framework evaluating LLM robustness through adversarial patient persuasion in emergency medicine. Across 20 LLMs and 1,875 encounters spanning three Choosing Wisely scenarios, acquiescence rates ranged from 0-100\%. Models showed higher vulnerability to imaging requests (38.8\%) than opioid prescriptions (25.0\%), with model capability poorly predicting robustness. All persuasion tactics proved equally effective (30.0-36.0\%), indicating general susceptibility rather than tactic-specific weakness. Our findings demonstrate that static benchmarks inadequately predict safety under social pressure, necessitating multi-turn adversarial testing for clinical AI certification.

</details>


### [74] [LLM is Not All You Need: A Systematic Evaluation of ML vs. Foundation Models for text and image based Medical Classification](https://arxiv.org/abs/2601.16549)
*Meet Raval,Tejul Pandit,Dhvani Upadhyay*

Main category: cs.AI

TL;DR: Traditional ML models outperform modern transformer-based approaches in medical classification tasks, with PEFT fine-tuning performing worst and zero-shot VLMs showing mixed results.


<details>
  <summary>Details</summary>
Motivation: To provide a rigorous benchmark comparing traditional ML methods with contemporary transformer-based approaches (prompt-based LLMs/VLMs and fine-tuned PEFT models) for medical classification across text and image modalities.

Method: Used four public datasets covering text and image modalities with binary and multiclass complexity. Evaluated three model classes: Classical ML (LR, LightGBM, ResNet-50), Prompt-Based LLMs/VLMs (Gemini 2.5), and Fine-Tuned PEFT Models (LoRA-adapted Gemma3 variants). All experiments used consistent data splits and aligned metrics.

Result: Traditional ML models achieved best overall performance across most tasks, especially on structured text-based datasets. LoRA-tuned Gemma variants performed worst across all experiments. Zero-shot Gemini 2.5 performed poorly on text tasks but showed competitive performance on multiclass image tasks, matching ResNet-50 baseline.

Conclusion: Established ML models remain most reliable for medical categorization. Foundation models are not universally superior, and PEFT effectiveness depends heavily on adaptation strategy - minimal fine-tuning proved detrimental in this study.

Abstract: The combination of multimodal Vision-Language Models (VLMs) and Large Language Models (LLMs) opens up new possibilities for medical classification. This work offers a rigorous, unified benchmark by using four publicly available datasets covering text and image modalities (binary and multiclass complexity) that contrasts traditional Machine Learning (ML) with contemporary transformer-based techniques. We evaluated three model classes for each task: Classical ML (LR, LightGBM, ResNet-50), Prompt-Based LLMs/VLMs (Gemini 2.5), and Fine-Tuned PEFT Models (LoRA-adapted Gemma3 variants). All experiments used consistent data splits and aligned metrics. According to our results, traditional machine learning (ML) models set a high standard by consistently achieving the best overall performance across most medical categorization tasks. This was especially true for structured text-based datasets, where the classical models performed exceptionally well. In stark contrast, the LoRA-tuned Gemma variants consistently showed the worst performance across all text and image experiments, failing to generalize from the minimal fine-tuning provided. However, the zero-shot LLM/VLM pipelines (Gemini 2.5) had mixed results; they performed poorly on text-based tasks, but demonstrated competitive performance on the multiclass image task, matching the classical ResNet-50 baseline. These results demonstrate that in many medical categorization scenarios, established machine learning models continue to be the most reliable option. The experiment suggests that foundation models are not universally superior and that the effectiveness of Parameter-Efficient Fine-Tuning (PEFT) is highly dependent on the adaptation strategy, as minimal fine-tuning proved detrimental in this study.

</details>


### [75] [LUMINA: Long-horizon Understanding for Multi-turn Interactive Agents](https://arxiv.org/abs/2601.16649)
*Amin Rakhsha,Thomas Hehn,Pietro Mazzaglia,Fabio Valerio Massoli,Arash Behboodi,Tribhuvanesh Orekondy*

Main category: cs.AI

TL;DR: The paper introduces an oracle counterfactual framework to measure the importance of different AI agent capabilities (planning, state tracking, long context processing) in multi-turn tasks, using procedurally generated environments to isolate each skill's contribution.


<details>
  <summary>Details</summary>
Motivation: While LLMs perform well on isolated tasks, they struggle with multi-turn, long-horizon agentic problems requiring planning, state tracking, and long context processing. The paper aims to understand which of these underlying capabilities is most critical for advancing AI agents.

Method: Developed an oracle counterfactual framework that measures how agent performance changes when given perfect assistance on specific skills. Created procedurally generated, game-like tasks with tunable complexity to provide precise oracle interventions (perfect planning, flawless state tracking) and isolate each skill's contribution without confounding effects.

Result: Planning interventions consistently improve performance across settings, while the usefulness of other skills (like state tracking) depends on environment properties and language model characteristics.

Conclusion: The work provides insights into the challenges of multi-turn agentic environments and guides future development of AI agents and language models by identifying which capabilities are most critical for advancement.

Abstract: Large language models can perform well on many isolated tasks, yet they continue to struggle on multi-turn, long-horizon agentic problems that require skills such as planning, state tracking, and long context processing. In this work, we aim to better understand the relative importance of advancing these underlying capabilities for success on such tasks. We develop an oracle counterfactual framework for multi-turn problems that asks: how would an agent perform if it could leverage an oracle to perfectly perform a specific task? The change in the agent's performance due to this oracle assistance allows us to measure the criticality of such oracle skill in the future advancement of AI agents. We introduce a suite of procedurally generated, game-like tasks with tunable complexity. These controlled environments allow us to provide precise oracle interventions, such as perfect planning or flawless state tracking, and make it possible to isolate the contribution of each oracle without confounding effects present in real-world benchmarks. Our results show that while some interventions (e.g., planning) consistently improve performance across settings, the usefulness of other skills is dependent on the properties of the environment and language model. Our work sheds light on the challenges of multi-turn agentic environments to guide the future efforts in the development of AI agents and language models.

</details>


### [76] [AgentsEval: Clinically Faithful Evaluation of Medical Imaging Reports via Multi-Agent Reasoning](https://arxiv.org/abs/2601.16685)
*Suzhong Fu,Jingqi Dong,Xuan Ding,Rui Sun,Yiming Yang,Shuguang Cui,Zhen Li*

Main category: cs.AI

TL;DR: AgentsEval: A multi-agent framework for evaluating medical imaging reports by emulating radiologists' diagnostic workflow, providing structured clinical feedback and explicit reasoning traces.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for medical imaging reports fail to capture structured diagnostic logic, resulting in unreliable judgments and limited clinical relevance. There's a need for clinically grounded assessment that reflects radiological interpretation workflows.

Method: AgentsEval uses a multi-agent stream reasoning framework that divides evaluation into interpretable steps: criteria definition, evidence extraction, alignment, and consistency scoring. It emulates collaborative diagnostic workflows of radiologists and provides explicit reasoning traces.

Result: AgentsEval delivers clinically aligned, semantically faithful, and interpretable evaluations that remain robust under paraphrastic, semantic, and stylistic perturbations. Experimental validation uses a multi-domain perturbation-based benchmark covering five medical report datasets with diverse imaging modalities.

Conclusion: The framework represents progress toward transparent and clinically grounded assessment of medical report generation systems, supporting trustworthy integration of large language models into clinical practice by providing structured clinical feedback.

Abstract: Evaluating the clinical correctness and reasoning fidelity of automatically generated medical imaging reports remains a critical yet unresolved challenge. Existing evaluation methods often fail to capture the structured diagnostic logic that underlies radiological interpretation, resulting in unreliable judgments and limited clinical relevance. We introduce AgentsEval, a multi-agent stream reasoning framework that emulates the collaborative diagnostic workflow of radiologists. By dividing the evaluation process into interpretable steps including criteria definition, evidence extraction, alignment, and consistency scoring, AgentsEval provides explicit reasoning traces and structured clinical feedback. We also construct a multi-domain perturbation-based benchmark covering five medical report datasets with diverse imaging modalities and controlled semantic variations. Experimental results demonstrate that AgentsEval delivers clinically aligned, semantically faithful, and interpretable evaluations that remain robust under paraphrastic, semantic, and stylistic perturbations. This framework represents a step toward transparent and clinically grounded assessment of medical report generation systems, fostering trustworthy integration of large language models into clinical practice.

</details>


### [77] [LongCat-Flash-Thinking-2601 Technical Report](https://arxiv.org/abs/2601.16725)
*Meituan LongCat Team,Anchun Gui,Bei Li,Bingyang Tao,Bole Zhou,Borun Chen,Chao Zhang,Chao Zhang,Chen Gao,Chen Zhang,Chengcheng Han,Chenhui Yang,Chuyu Zhang,Cong Chen,Cunguang Wang,Daoru Pan,Defei Bu,Dengchang Zhao,Di Xiu,Dishan Liu,Dongyu Ru,Dunwei Tu,Fan Wu,Fengcheng Yuan,Fengcun Li,Gang Xu,Guanyu Wu,Guoyuan Lin,Haibin Wang,Hansi Yang,Hao Yang,Haonan Yan,Haoxiang Ma,Haoxing Wen,Hongyan Hao,Hongyin Tang,Hongyu Zang,Hongzhi Ni,Hui Su,Jiacheng Zhang,Jiahong Zhou,Jiahuan Li,Jiaming Wang,Jian Yang,Jianfei Zhang,Jianhao Xu,Jianing Wang,Jiapeng Zhu,Jiaqi Sun,Jiarong Shi,Jiarui Zhao,Jingang Wang,Jinluan Yang,Jinrui Ding,Jinwei Xiao,Jiyuan He,Juncan Xu,Kefeng Zhang,Keheng Wang,Li Wei,Lianhui Ma,Lin Qiu,Lingbing Kong,Lingchuan Liu,Linsen Guo,Mengshen Zhu,Mengxia Shen,Mingyang Zhu,Peiguang Li,Peng Pei,Pengcheng Jia,Pengtao Zhang,Peng Zhao,Qi Gu,Qiong Huang,Qiyuan Duan,Quanchi Weng,Rongxiang Weng,Rongzhi Zhang,Rumei Li,Shanglin Lei,Shengnan An,Shijun Dai,Shuaikang Liu,Shuang Zhou,Shuo Wang,Songyuan Zhao,Tao Liang,Tianhao Hu,Tianze Chen,Wei Liu,Wei Shi,Wei Wang,Weifeng Tang,Wenjie Shi,Wenlong Zhu,Wentao Chen,Wentao Shi,Xi Su,Xiangcheng Liu,Xiandi Ma,Xiangyu Xi,Xiangyuan Liu,Xiangzhou Huang,Xiao Liu,Xiaodong Cai,Xiaolong Chen,Xiaowei Shi,Xiaoyu Li,Xin Chen,Xingchen Liu,Xuan Huang,Xuezhi Cao,Xunliang Cai,Yan Chen,Yang Bai,Yang Liu,Yang Yang,Yang Zheng,Yaoming Wang,Yaoming Zhu,Yaqi Huo,Yanyu Chen,Yaorui Shi,Yerui Sun,Yi Zhang,Yihao Chen,Yi-Kai Zhang,Yifan Lu,Yifan Zhao,Yitao Zhai,Yongjing Yin,Yongwei Zhou,Youshao Xiao,Yuchuan Dai,Yuchen Xie,Yuchen Yu,Yufei Zhang,Yuhuai Wei,Yulei Qian,Yunfan Liang,Yunke Zhao,Yuwei Jiang,Yuxin Bian,Yuxin Chen,Yuxin Liu,Yue Xu,Yueqing Sun,Zeyang Yu,Zhao Yang,Zhengsheng Huang,Zhengyu Chen,Zhijian Liu,Zhikang Xia,Zhimin Lin,Zhiyuan Yao,Zhuofan Chen,Zhuowen Han,Zijian Zhang,Ziran Li,Ziwen Wang,Ziyuan Zhuang*

Main category: cs.AI

TL;DR: LongCat-Flash-Thinking-2601 is a 560B parameter open-source MoE reasoning model with state-of-the-art agentic reasoning capabilities, featuring a unified training framework, environment scaling, noise robustness, and a Heavy Thinking mode for complex reasoning.


<details>
  <summary>Details</summary>
Motivation: To develop an open-source reasoning model with superior agentic capabilities that can handle complex tool interactions, noisy real-world environments, and demonstrate strong generalization across diverse domains and tasks.

Method: Uses a unified training framework combining domain-parallel expert training with fusion, systematic environment scaling (10k+ environments across 20+ domains), extended DORA asynchronous RL framework for stable multi-environment training, noise pattern analysis for robustness, and Heavy Thinking mode for test-time scaling of reasoning depth and width.

Result: Achieves state-of-the-art performance among open-source models on agentic benchmarks (search, tool use, tool-integrated reasoning), demonstrates strong generalization to complex tool interactions, and shows robust behavior in noisy real-world environments.

Conclusion: LongCat-Flash-Thinking-2601 represents a significant advancement in open-source agentic reasoning models, with comprehensive co-design of data, environments, algorithms, and infrastructure enabling superior performance and real-world applicability.

Abstract: We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.

</details>


### [78] [An Efficient Insect-inspired Approach for Visual Point-goal Navigation](https://arxiv.org/abs/2601.16806)
*Lu Yihe,Barbara Webb*

Main category: cs.AI

TL;DR: Insect-inspired agent for visual point-goal navigation achieves SOTA performance with dramatically lower computational cost.


<details>
  <summary>Details</summary>
Motivation: To develop efficient visual navigation agents inspired by insect brains, drawing analogy between Habitat benchmark and insect navigation between food sources and nests.

Method: Combines abstracted models of two insect brain structures: one for associative learning and one for path integration, creating a simple insect-inspired agent.

Result: Agent achieves performance comparable to recent SOTA models with many orders of magnitude less computational cost, and shows robustness to perturbations in realistic simulations.

Conclusion: Insect-inspired approaches offer efficient and robust solutions for visual navigation tasks, demonstrating biological inspiration can lead to computationally efficient AI systems.

Abstract: In this work we develop a novel insect-inspired agent for visual point-goal navigation. This combines abstracted models of two insect brain structures that have been implicated, respectively, in associative learning and path integration. We draw an analogy between the formal benchmark of the Habitat point-goal navigation task and the ability of insects to learn and refine visually guided paths around obstacles between a discovered food location and their nest. We demonstrate that the simple insect-inspired agent exhibits performance comparable to recent SOTA models at many orders of magnitude less computational cost. Testing in a more realistic simulated environment shows the approach is robust to perturbations.

</details>


### [79] [Reasoning Promotes Robustness in Theory of Mind Tasks](https://arxiv.org/abs/2601.16853)
*Ian B. de Haan,Peter van der Putten,Max van Duijn*

Main category: cs.AI

TL;DR: Reasoning-oriented LLMs trained with RLVR show improved robustness on Theory of Mind tasks, but gains appear to stem from better solution-finding rather than fundamentally new reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: To investigate whether reasoning-oriented LLMs trained with reinforcement learning with verifiable rewards (RLVR) exhibit fundamentally new Theory of Mind capabilities or simply improved robustness in existing reasoning approaches.

Method: Used novel adaptations of machine psychological experiments and established benchmarks to test reasoning models on Theory of Mind tasks, analyzing their behavior under various prompt variations and task perturbations.

Result: Reasoning models consistently showed increased robustness to prompt variations and task perturbations compared to standard LLMs, but analysis suggests gains are due to improved solution-finding robustness rather than fundamentally new ToM reasoning forms.

Conclusion: The observed improvements in reasoning models on ToM tasks likely reflect enhanced robustness in applying existing reasoning strategies rather than development of fundamentally new social-cognitive capabilities, with implications for how we evaluate LLM social cognition.

Abstract: Large language models (LLMs) have recently shown strong performance on Theory of Mind (ToM) tests, prompting debate about the nature and true performance of the underlying capabilities. At the same time, reasoning-oriented LLMs trained via reinforcement learning with verifiable rewards (RLVR) have achieved notable improvements across a range of benchmarks. This paper examines the behavior of such reasoning models in ToM tasks, using novel adaptations of machine psychological experiments and results from established benchmarks. We observe that reasoning models consistently exhibit increased robustness to prompt variations and task perturbations. Our analysis indicates that the observed gains are more plausibly attributed to increased robustness in finding the correct solution, rather than to fundamentally new forms of ToM reasoning. We discuss the implications of this interpretation for evaluating social-cognitive behavior in LLMs.

</details>


### [80] [Mixture-of-Models: Unifying Heterogeneous Agents via N-Way Self-Evaluating Deliberation](https://arxiv.org/abs/2601.16863)
*Tims Pecerskis,Aivars Smirnovs*

Main category: cs.AI

TL;DR: NSED protocol enables small model ensembles to match/exceed large SOTA models via dynamic expert selection and iterative deliberation architecture.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of static Mixture-of-Experts (MoE) systems and enable small consumer-grade models to achieve performance comparable to much larger models through dynamic, cost-aware expert selection and collaborative deliberation.

Method: Uses Dynamic Expertise Broker treating model selection as Knapsack Problem, formalizes deliberation as Macro-Scale RNN with semantic forget gate, includes orchestration fabric for peer review, Quadratic Voting activation, and feedback-driven state updates.

Result: Ensembles of small (<20B) models match/exceed 100B+ parameter models on AIME 2025 and LiveCodeBench benchmarks, with improved safety (reduced sycophancy) on DarkBench suite.

Conclusion: NSED establishes new hardware efficiency frontier and demonstrates intrinsic alignment properties through peer-mediated correction, enabling high performance with smaller models.

Abstract: This paper introduces the N-Way Self-Evaluating Deliberation (NSED) protocol, a Runtime Mixture-of-Models (MoM) architecture that constructs emergent composite models from a plurality of distinct expert agents. Unlike traditional Mixture-of-Experts (MoE) which rely on static gating networks, NSED employs a Dynamic Expertise Broker - a runtime optimization engine that treats model selection as a variation of the Knapsack Problem, binding heterogeneous checkpoints to functional roles based on live telemetry and cost constraints. At the execution layer, we formalize deliberation as a Macro-Scale Recurrent Neural Network (RNN), where the consensus state loops back through a semantic forget gate to enable iterative refinement without proportional VRAM scaling. Key components include an orchestration fabric for trustless N-to-N peer review, a Quadratic Voting activation function for non-linear consensus, and a feedback-driven state update. Empirical validation on challenging benchmarks (AIME 2025, LiveCodeBench) demonstrates that this topology allows ensembles of small (less than 20B) consumer-grade models to match or exceed the performance of state-of-the-art 100B+ parameter models, establishing a new hardware arbitrage efficiency frontier. Furthermore, testing on the DarkBench safety suite reveals intrinsic alignment properties, with peer-mediated correction reducing sycophancy scores below that of any individual agent.

</details>


### [81] [MAGE-KT: Multi-Agent Graph-Enhanced Knowledge Tracing with Subgraph Retrieval and Asymmetric Fusion](https://arxiv.org/abs/2601.16886)
*Chi Yu,Hongyu Yuan,Zhiyi Duan*

Main category: cs.AI

TL;DR: MAGE-KT is a novel graph-based knowledge tracing framework that uses multi-agent KC relation extraction and compact subgraph retrieval to improve prediction accuracy while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based KT methods inadequately explore inter-concept relations (often inferred only from interaction sequences) and suffer from computational inefficiency and noise in full-graph encoding, leading to attention diffusion and degraded relation fidelity.

Method: Constructs a multi-view heterogeneous graph combining multi-agent KC relation extractor and student-question interaction graph; retrieves compact subgraphs conditioned on target student's history; integrates them using Asymmetric Cross-attention Fusion Module.

Result: Experiments on three widely used KT datasets show substantial improvements in KC-relation accuracy and clear gains in next-question prediction over existing methods.

Conclusion: MAGE-KT effectively addresses limitations of existing graph-based KT methods by better capturing inter-concept relations while avoiding computational inefficiency and attention diffusion through selective subgraph retrieval and fusion.

Abstract: Knowledge Tracing (KT) aims to model a student's learning trajectory and predict performance on the next question. A key challenge is how to better represent the relationships among students, questions, and knowledge concepts (KCs). Recently, graph-based KT paradigms have shown promise for this problem. However, existing methods have not sufficiently explored inter-concept relations, often inferred solely from interaction sequences. In addition, the scale and heterogeneity of KT graphs make full-graph encoding both computationally both costly and noise-prone, causing attention to bleed into student-irrelevant regions and degrading the fidelity of inter-KC relations. To address these issues, we propose a novel framework: Multi-Agent Graph-Enhanced Knowledge Tracing (MAGE-KT). It constructs a multi-view heterogeneous graph by combining a multi-agent KC relation extractor and a student-question interaction graph, capturing complementary semantic and behavioral signals. Conditioned on the target student's history, it retrieves compact, high-value subgraphs and integrates them using an Asymmetric Cross-attention Fusion Module to enhance prediction while avoiding attention diffusion and irrelevant computation. Experiments on three widely used KT datasets show substantial improvements in KC-relation accuracy and clear gains in next-question prediction over existing methods.

</details>


### [82] [Preventing the Collapse of Peer Review Requires Verification-First AI](https://arxiv.org/abs/2601.16909)
*Lei You,Lele Cao,Iryna Gurevych*

Main category: cs.AI

TL;DR: AI peer review should prioritize verification over mimicking human review, focusing on truth-coupling to prevent proxy-sovereign evaluation collapse.


<details>
  <summary>Details</summary>
Motivation: Current AI-assisted peer review risks amplifying claim inflation by mimicking human scoring rather than verifying scientific truth, leading to proxy optimization over truth-seeking.

Method: Formalizes truth-coupling concept, models verification pressure and signal shrinkage, derives coupling law and incentive-collapse condition in a minimal model mixing high-fidelity checks with proxy judgment.

Result: Identifies phase transition toward proxy-sovereign evaluation where rational effort shifts from truth-seeking to proxy optimization, even when decisions appear reliable.

Conclusion: AI should be deployed as adversarial auditor generating verification artifacts to expand verification bandwidth, not as score predictor that amplifies claim inflation.

Abstract: This paper argues that AI-assisted peer review should be verification-first rather than review-mimicking. We propose truth-coupling, i.e. how tightly venue scores track latent scientific truth, as the right objective for review tools. We formalize two forces that drive a phase transition toward proxy-sovereign evaluation: verification pressure, when claims outpace verification capacity, and signal shrinkage, when real improvements become hard to separate from noise. In a minimal model that mixes occasional high-fidelity checks with frequent proxy judgment, we derive an explicit coupling law and an incentive-collapse condition under which rational effort shifts from truth-seeking to proxy optimization, even when current decisions still appear reliable. These results motivate actions for tool builders and program chairs: deploy AI as an adversarial auditor that generates auditable verification artifacts and expands effective verification bandwidth, rather than as a score predictor that amplifies claim inflation.

</details>


### [83] [AgentDrive: An Open Benchmark Dataset for Agentic AI Reasoning with LLM-Generated Scenarios in Autonomous Systems](https://arxiv.org/abs/2601.16964)
*Mohamed Amine Ferrag,Abderrahmane Lakas,Merouane Debbah*

Main category: cs.AI

TL;DR: AgentDrive is a large-scale benchmark dataset with 300K LLM-generated driving scenarios and a 100K-question MCQ test for evaluating autonomous AI agents across diverse conditions and reasoning dimensions.


<details>
  <summary>Details</summary>
Motivation: There's a lack of large-scale, structured, and safety-critical benchmarks for evaluating and training LLM-integrated autonomous systems for reasoning-driven perception, planning, and decision-making in driving contexts.

Method: Created AgentDrive with 300K LLM-generated driving scenarios across 7 orthogonal axes using an LLM-driven prompt-to-JSON pipeline, validated against physical/schema constraints, with simulation rollouts and safety metrics. Also developed AgentDrive-MCQ with 100K questions across 5 reasoning dimensions.

Result: Evaluation of 50 leading LLMs shows proprietary frontier models excel in contextual/policy reasoning, while advanced open models are catching up in structured/physics-grounded reasoning. The benchmark reveals current capabilities and gaps.

Conclusion: AgentDrive provides a comprehensive benchmark for autonomous agent development, enabling systematic evaluation, training, and comparison of LLM-based driving agents across diverse scenarios and reasoning tasks.

Abstract: The rapid advancement of large language models (LLMs) has sparked growing interest in their integration into autonomous systems for reasoning-driven perception, planning, and decision-making. However, evaluating and training such agentic AI models remains challenging due to the lack of large-scale, structured, and safety-critical benchmarks. This paper introduces AgentDrive, an open benchmark dataset containing 300,000 LLM-generated driving scenarios designed for training, fine-tuning, and evaluating autonomous agents under diverse conditions. AgentDrive formalizes a factorized scenario space across seven orthogonal axes: scenario type, driver behavior, environment, road layout, objective, difficulty, and traffic density. An LLM-driven prompt-to-JSON pipeline generates semantically rich, simulation-ready specifications that are validated against physical and schema constraints. Each scenario undergoes simulation rollouts, surrogate safety metric computation, and rule-based outcome labeling. To complement simulation-based evaluation, we introduce AgentDrive-MCQ, a 100,000-question multiple-choice benchmark spanning five reasoning dimensions: physics, policy, hybrid, scenario, and comparative reasoning. We conduct a large-scale evaluation of fifty leading LLMs on AgentDrive-MCQ. Results show that while proprietary frontier models perform best in contextual and policy reasoning, advanced open models are rapidly closing the gap in structured and physics-grounded reasoning. We release the AgentDrive dataset, AgentDrive-MCQ benchmark, evaluation code, and related materials at https://github.com/maferrag/AgentDrive

</details>


### [84] [Spatial-Agent: Agentic Geo-spatial Reasoning with Scientific Core Concepts](https://arxiv.org/abs/2601.16965)
*Riyang Bao,Cheng Yang,Dazhou Yu,Zhexiang Tang,Gengchen Mai,Liang Zhao*

Main category: cs.AI

TL;DR: Spatial-Agent: An AI agent for geospatial reasoning that formalizes geo-analytical questions as concept transformation problems using GeoFlow Graphs, outperforming existing LLM-based approaches.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based agents fail at genuine geospatial computation, relying on web search or pattern matching while hallucinating spatial relationships, despite geospatial reasoning being essential for real-world applications like urban analytics, transportation planning, and disaster response.

Method: Formalizes geo-analytical question answering as a concept transformation problem, parsing natural-language questions into executable workflows represented as GeoFlow Graphs (directed acyclic graphs with nodes as spatial concepts and edges as transformations). Uses spatial information theory to extract spatial concepts, assign functional roles with principled ordering constraints, and compose transformation sequences through template-based generation.

Result: Extensive experiments on MapEval-API and MapQA benchmarks demonstrate that Spatial-Agent significantly outperforms existing baselines including ReAct and Reflexion, while producing interpretable and executable geospatial workflows.

Conclusion: Spatial-Agent provides a principled approach to geospatial reasoning grounded in spatial information science, addressing the limitations of current LLM-based agents and enabling genuine geospatial computation with interpretable workflows.

Abstract: Geospatial reasoning is essential for real-world applications such as urban analytics, transportation planning, and disaster response. However, existing LLM-based agents often fail at genuine geospatial computation, relying instead on web search or pattern matching while hallucinating spatial relationships. We present Spatial-Agent, an AI agent grounded in foundational theories of spatial information science. Our approach formalizes geo-analytical question answering as a concept transformation problem, where natural-language questions are parsed into executable workflows represented as GeoFlow Graphs -- directed acyclic graphs with nodes corresponding to spatial concepts and edges representing transformations. Drawing on spatial information theory, Spatial-Agent extracts spatial concepts, assigns functional roles with principled ordering constraints, and composes transformation sequences through template-based generation. Extensive experiments on MapEval-API and MapQA benchmarks demonstrate that Spatial-Agent significantly outperforms existing baselines including ReAct and Reflexion, while producing interpretable and executable geospatial workflows.

</details>


### [85] [Empowering Medical Equipment Sustainability in Low-Resource Settings: An AI-Powered Diagnostic and Support Platform for Biomedical Technicians](https://arxiv.org/abs/2601.16967)
*Bernes Lorier Atabonfack,Ahmed Tahiru Issah,Mohammed Hardi Abdul Baaki,Clemence Ingabire,Tolulope Olusuyi,Maruf Adewole,Udunna C. Anazodo,Timothy X Brown*

Main category: cs.AI

TL;DR: AI-powered platform helps biomedical technicians in LMICs diagnose and repair medical devices using LLM-based troubleshooting and peer-to-peer forums, achieving high accuracy in proof-of-concept with ultrasound machine.


<details>
  <summary>Details</summary>
Motivation: Medical diagnostic equipment in LMICs suffers from underutilization and non-functionality due to lack of timely maintenance, limited technical expertise, and minimal manufacturer support, leading to increased downtime, delayed diagnoses, and compromised patient care.

Method: Developed an AI-powered support platform integrating a large language model (LLM) with a web interface that allows technicians to input error codes or symptoms and receive step-by-step troubleshooting guidance. Includes a global peer-to-peer discussion forum for knowledge exchange. Proof of concept implemented using Philips HDI 5000 ultrasound machine.

Result: The proof-of-concept achieved 100% precision in error code interpretation and 80% accuracy in suggesting corrective actions for the ultrasound machine.

Conclusion: The study demonstrates the feasibility and potential of AI-driven systems to support medical device maintenance in resource-constrained environments, with the aim of reducing equipment downtime and improving healthcare delivery.

Abstract: In low- and middle-income countries (LMICs), a significant proportion of medical diagnostic equipment remains underutilized or non-functional due to a lack of timely maintenance, limited access to technical expertise, and minimal support from manufacturers, particularly for devices acquired through third-party vendors or donations. This challenge contributes to increased equipment downtime, delayed diagnoses, and compromised patient care. This research explores the development and validation of an AI-powered support platform designed to assist biomedical technicians in diagnosing and repairing medical devices in real-time. The system integrates a large language model (LLM) with a user-friendly web interface, enabling imaging technologists/radiographers and biomedical technicians to input error codes or device symptoms and receive accurate, step-by-step troubleshooting guidance. The platform also includes a global peer-to-peer discussion forum to support knowledge exchange and provide additional context for rare or undocumented issues. A proof of concept was developed using the Philips HDI 5000 ultrasound machine, achieving 100% precision in error code interpretation and 80% accuracy in suggesting corrective actions. This study demonstrates the feasibility and potential of AI-driven systems to support medical device maintenance, with the aim of reducing equipment downtime to improve healthcare delivery in resource-constrained environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [86] [Ordering-based Causal Discovery via Generalized Score Matching](https://arxiv.org/abs/2601.16249)
*Vy Vo,He Zhao,Trung Le,Edwin V. Bonilla,Dinh Phung*

Main category: cs.LG

TL;DR: Extends score matching framework for causal discovery to discrete data, introducing novel leaf discriminant criterion based on discrete score function to infer causal orders from observed discrete data.


<details>
  <summary>Details</summary>
Motivation: Learning DAG structures from purely observational data remains challenging across scientific domains, especially for discrete data where existing score matching frameworks are primarily designed for continuous data.

Method: Extends score matching framework for causal discovery to discrete data, introduces novel leaf discriminant criterion based on discrete score function, uses leaf node detection to identify topological order, followed by edge pruning for graph recovery.

Result: Demonstrates accurate inference of true causal orders from observed discrete data through simulated and real-world experiments; identified ordering significantly boosts accuracy of existing causal discovery baselines on nearly all settings.

Conclusion: The proposed discrete score matching framework enables effective causal discovery from discrete observational data, providing a significant improvement over existing methods through better topological order identification.

Abstract: Learning DAG structures from purely observational data remains a long-standing challenge across scientific domains. An emerging line of research leverages the score of the data distribution to initially identify a topological order of the underlying DAG via leaf node detection and subsequently performs edge pruning for graph recovery. This paper extends the score matching framework for causal discovery, which is originally designated for continuous data, and introduces a novel leaf discriminant criterion based on the discrete score function. Through simulated and real-world experiments, we demonstrate that our theory enables accurate inference of true causal orders from observed discrete data and the identified ordering can significantly boost the accuracy of existing causal discovery baselines on nearly all of the settings.

</details>


### [87] [Student Mental Health Screening via Fitbit Data Collected During the COVID-19 Pandemic](https://arxiv.org/abs/2601.16324)
*Rebecca Lopez,Avantika Shrestha,ML Tlachac,Kevin Hickey,Xingtong Guo,Shichao Liu,Elke Rundensteiner*

Main category: cs.LG

TL;DR: Using Fitbit data from college students during the pandemic, machine learning models achieved F1 scores up to 0.79 for anxiety, 0.77 for stress, and 0.78 for depression screening, demonstrating wearable technology's potential for mental health monitoring.


<details>
  <summary>Details</summary>
Motivation: College students face high levels of anxiety and depression, but current research on wearable technology for mental health detection is limited in psychological instruments, physiological modalities, and time series parameters. There's a need for comprehensive assessment of wearable devices for mental illness screening.

Method: Collected the Student Mental and Environmental Health (StudentMEH) Fitbit dataset from college students during the pandemic. Used predictive machine learning models to screen for depression, anxiety, and stress using different Fitbit modalities (heart rate, sleep data, etc.).

Result: Models showed strong performance with F1 scores as high as 0.79 for anxiety screening, 0.77 for stress screening (using heart rate), and 0.78 for depression screening (using sleep data). Different modalities performed best for different mental health conditions.

Conclusion: Wearable devices show significant potential for continuous mental health monitoring. The research highlights the importance of identifying optimal data aggregation levels and appropriate physiological modalities for screening different mental health conditions.

Abstract: College students experience many stressors, resulting in high levels of anxiety and depression. Wearable technology provides unobtrusive sensor data that can be used for the early detection of mental illness. However, current research is limited concerning the variety of psychological instruments administered, physiological modalities, and time series parameters. In this research, we collect the Student Mental and Environmental Health (StudentMEH) Fitbit dataset from students at our institution during the pandemic. We provide a comprehensive assessment of the ability of predictive machine learning models to screen for depression, anxiety, and stress using different Fitbit modalities. Our findings indicate potential in physiological modalities such as heart rate and sleep to screen for mental illness with the F1 scores as high as 0.79 for anxiety, the former modality reaching 0.77 for stress screening, and the latter modality achieving 0.78 for depression. This research highlights the potential of wearable devices to support continuous mental health monitoring, the importance of identifying best data aggregation levels and appropriate modalities for screening for different mental ailments.

</details>


### [88] [Efficient Gaussian process learning via subspace projections](https://arxiv.org/abs/2601.16332)
*Felipe Tobar,Elsa Cazelles*

Main category: cs.LG

TL;DR: Proposed projected likelihood (PL) training for GPs using lower-dimensional linear projections, showing improved accuracy and computational efficiency over exact GP and variational methods.


<details>
  <summary>Details</summary>
Motivation: To address computational challenges in Gaussian Process training for moderately large datasets by developing a more efficient training objective that reduces information loss while maintaining accuracy.

Method: Introduces projected likelihood (PL) using lower-dimensional linear projections of data, provides closed-form expression for information loss, and uses random projections on unit sphere to reduce this loss.

Result: PL demonstrates superiority over exact GP training and variational free energy approach to sparse GPs in terms of both accuracy and computational efficiency across different optimizers, kernels, and datasets.

Conclusion: Projected likelihood offers an effective alternative for GP training that balances computational efficiency with accuracy, particularly suitable for moderately large datasets.

Abstract: We propose a novel training objective for GPs constructed using lower-dimensional linear projections of the data, referred to as \emph{projected likelihood} (PL). We provide a closed-form expression for the information loss related to the PL and empirically show that it can be reduced with random projections on the unit sphere. We show the superiority of the PL, in terms of accuracy and computational efficiency, over the exact GP training and the variational free energy approach to sparse GPs over different optimisers, kernels and datasets of moderately large sizes.

</details>


### [89] [Analyzing Neural Network Information Flow Using Differential Geometry](https://arxiv.org/abs/2601.16366)
*Shuhang Tan,Jayson Sia,Paul Bogdan,Radoslav Ivanov*

Main category: cs.LG

TL;DR: This paper introduces a novel approach to neural network data flow analysis using graph curvature theory, specifically Ollivier-Ricci curvature, to identify important connections in neural networks for pruning and model analysis.


<details>
  <summary>Details</summary>
Motivation: The paper aims to provide a fresh perspective on neural network data flow analysis by applying graph theory concepts rather than traditional information theory approaches. Understanding which neural connections are most critical can enable better symbolic analysis, robustness assessment, and model repair.

Method: The method constructs a graph from the neural network structure and introduces neural curvature (NC) based on Ollivier-Ricci curvature. It calculates curvatures using activation patterns from input examples, then uses negative curvature edges (bottlenecks) to identify critical connections and positive curvature edges to find unimportant ones for pruning.

Result: The method successfully identifies important neural connections through pruning experiments. Removing negative-ORC edges quickly degrades network performance, while removing positive-ORC edges has little impact. The approach outperforms state-of-the-art pruning methods by identifying more unimportant edges across models trained on MNIST, CIFAR-10, and CIFAR-100 datasets.

Conclusion: Graph curvature theory, specifically Ollivier-Ricci curvature, provides an effective framework for analyzing neural network data flow and identifying critical connections. The neural curvature approach offers a promising alternative to information-theoretic methods for model analysis and pruning.

Abstract: This paper provides a fresh view of the neural network (NN) data flow problem, i.e., identifying the NN connections that are most important for the performance of the full model, through the lens of graph theory. Understanding the NN data flow provides a tool for symbolic NN analysis, e.g.,~robustness analysis or model repair. Unlike the standard approach to NN data flow analysis, which is based on information theory, we employ the notion of graph curvature, specifically Ollivier-Ricci curvature (ORC). The ORC has been successfully used to identify important graph edges in various domains such as road traffic analysis, biological and social networks. In particular, edges with negative ORC are considered bottlenecks and as such are critical to the graph's overall connectivity, whereas positive-ORC edges are not essential. We use this intuition for the case of NNs as well: we 1)~construct a graph induced by the NN structure and introduce the notion of neural curvature (NC) based on the ORC; 2)~calculate curvatures based on activation patterns for a set of input examples; 3)~aim to demonstrate that NC can indeed be used to rank edges according to their importance for the overall NN functionality. We evaluate our method through pruning experiments and show that removing negative-ORC edges quickly degrades the overall NN performance, whereas positive-ORC edges have little impact. The proposed method is evaluated on a variety of models trained on three image datasets, namely MNIST, CIFAR-10 and CIFAR-100. The results indicate that our method can identify a larger number of unimportant edges as compared to state-of-the-art pruning methods.

</details>


### [90] [A Regularized Actor-Critic Algorithm for Bi-Level Reinforcement Learning](https://arxiv.org/abs/2601.16399)
*Sihan Zeng,Sujay Bhatt,Sumitra Ganesh,Alec Koppel*

Main category: cs.LG

TL;DR: Single-loop first-order actor-critic algorithm for bi-level optimization with MDP lower-level, using attenuating entropy regularization for asymptotically unbiased hyper-gradient estimation.


<details>
  <summary>Details</summary>
Motivation: Existing bi-level optimization and RL methods require second-order information, impose strong regularization, or use inefficient nested-loop procedures. Need efficient single-loop method for bi-level problems where upper-level parameterizes MDP reward and depends on optimal policy.

Method: Propose single-loop first-order actor-critic algorithm with penalty-based reformulation. Introduce attenuating entropy regularization into lower-level RL objective to enable asymptotically unbiased upper-level hyper-gradient estimation without solving unregularized RL exactly.

Result: Establish finite-time and finite-sample convergence to stationary point of original unregularized bi-level problem through novel lower-level residual analysis under special Polyak-Lojasiewicz condition. Validate on GridWorld goal position and RLHF for happy tweet generation.

Conclusion: Proposed method provides efficient single-loop solution for bi-level MDP optimization with theoretical guarantees and practical effectiveness on RL applications including RLHF.

Abstract: We study a structured bi-level optimization problem where the upper-level objective is a smooth function and the lower-level problem is policy optimization in a Markov decision process (MDP). The upper-level decision variable parameterizes the reward of the lower-level MDP, and the upper-level objective depends on the optimal induced policy. Existing methods for bi-level optimization and RL often require second-order information, impose strong regularization at the lower level, or inefficiently use samples through nested-loop procedures. In this work, we propose a single-loop, first-order actor-critic algorithm that optimizes the bi-level objective via a penalty-based reformulation. We introduce into the lower-level RL objective an attenuating entropy regularization, which enables asymptotically unbiased upper-level hyper-gradient estimation without solving the unregularized RL problem exactly. We establish the finite-time and finite-sample convergence of the proposed algorithm to a stationary point of the original, unregularized bi-level optimization problem through a novel lower-level residual analysis under a special type of Polyak-Lojasiewicz condition. We validate the performance of our method through experiments on a GridWorld goal position problem and on happy tweet generation through reinforcement learning from human feedback (RLHF).

</details>


### [91] [Towards a Theoretical Understanding to the Generalization of RLHF](https://arxiv.org/abs/2601.16403)
*Zhaochun Li,Mingyang Yi,Yue Wang,Shisheng Cui,Yong Liu*

Main category: cs.LG

TL;DR: The paper develops generalization theory for RLHF in LLMs using algorithmic stability framework, proving O(n^{-1/2}) generalization bounds under feature coverage conditions.


<details>
  <summary>Details</summary>
Motivation: While RLHF is empirically effective for aligning LLMs with human intent, its theoretical generalization properties in high-dimensional settings remain unexplored. The paper aims to bridge this gap between empirical success and theoretical understanding.

Method: The authors build generalization theory for RLHF of LLMs under linear reward models using algorithmic stability framework. They analyze under an end-to-end learning framework (consistent with practice) rather than maximum likelihood estimation consistency. They prove generalization bounds under feature coverage conditions.

Result: The paper proves that under feature coverage conditions, empirical optima of policy models have generalization bounds of order O(n^{-1/2}). The results extend to parameters obtained by gradient-based algorithms (Gradient Ascent and Stochastic Gradient Ascent).

Conclusion: The theoretical results provide new evidence for the empirically observed generalization of LLMs after RLHF, bridging the gap between practical effectiveness and theoretical understanding of RLHF alignment methods.

Abstract: Reinforcement Learning from Human Feedback (RLHF) and its variants have emerged as the dominant approaches for aligning Large Language Models with human intent. While empirically effective, the theoretical generalization properties of these methods in high-dimensional settings remain to be explored. To this end, we build the generalization theory on RLHF of LLMs under the linear reward model, through the framework of algorithmic stability. In contrast to the existing works built upon the consistency of maximum likelihood estimations on reward model, our analysis is presented under an end-to-end learning framework, which is consistent with practice. Concretely, we prove that under a key \textbf{feature coverage} condition, the empirical optima of policy model have a generalization bound of order $\mathcal{O}(n^{-\frac{1}{2}})$. Moreover, the results can be extrapolated to parameters obtained by gradient-based learning algorithms, i.e., Gradient Ascent (GA) and Stochastic Gradient Ascent (SGA). Thus, we argue that our results provide new theoretical evidence for the empirically observed generalization of LLMs after RLHF.

</details>


### [92] [Reasoning-Enhanced Rare-Event Prediction with Balanced Outcome Correction](https://arxiv.org/abs/2601.16406)
*Vitaly Bulgakov,Alexander Turchin*

Main category: cs.LG

TL;DR: LPCORP is a two-stage framework for rare-event prediction that combines reasoning-enhanced prediction with confidence-based correction to address extreme class imbalance without resampling.


<details>
  <summary>Details</summary>
Motivation: Rare-event prediction is critical in high-stakes domains like healthcare and finance, but extreme class imbalance biases conventional models toward majority-class predictions, limiting recall, calibration, and operational usefulness.

Method: Two-stage framework: 1) Reasoning model produces enriched predictions from narrative inputs, 2) Lightweight logistic-regression classifier evaluates and selectively corrects these outputs to mitigate prevalence-driven bias.

Result: Transforms highly imbalanced settings into well-balanced ones while preserving original sample count without resampling. Substantially improves performance, particularly precision. Cost-reduction analysis shows >50% reduction in some cases compared to damage control without preventive measures.

Conclusion: LPCORP effectively addresses extreme class imbalance in rare-event prediction through reasoning-enhanced prediction with confidence-based correction, improving operational usefulness and reducing costs in critical domains.

Abstract: Rare-event prediction is critical in domains such as healthcare, finance, reliability engineering, customer support, aviation safety, where positive outcomes are infrequent yet potentially catastrophic. Extreme class imbalance biases conventional models toward majority-class predictions, limiting recall, calibration, and operational usefulness. We propose LPCORP (Low-Prevalence CORrector for Prediction)*, a two-stage framework that combines reasoningenhanced prediction with confidence-based outcome correction. A reasoning model first produces enriched predictions from narrative inputs, after which a lightweight logistic-regression classifier evaluates and selectively corrects these outputs to mitigate prevalence-driven bias. We evaluate LPCORP on real-world datasets from medical and consumer service domains. The results show that this method transforms a highly imbalanced setting into a well-balanced one while preserving the original number of samples and without applying any resampling strategies. Test-set evaluation demonstrates substantially improved performance, particularly in precision, which is a known weakness in low-prevalence data. We further provide a costreduction analysis comparing the expenses associated with rare-event damage control without preventive measures to those incurred when low-cost, prediction-based preventive interventions are applied that showed more than 50% reduction in some cases. * Patent pending: U.S. Provisional 63/933,518, filed 8 December 2025.

</details>


### [93] [A Refinement of Vapnik--Chervonenkis' Theorem](https://arxiv.org/abs/2601.16411)
*A. Iosevich,A. Vagharshakyan,E. Wyman*

Main category: cs.LG

TL;DR: The paper revisits the probabilistic component of the VC theorem, replacing Hoeffding's inequality with normal approximation and Berry-Esseen error control to obtain sharper moderate-deviation bounds.


<details>
  <summary>Details</summary>
Motivation: To improve upon the classical VC theorem's convergence rate estimates by using more refined probabilistic techniques that yield sharper bounds, particularly in moderate-deviation regimes.

Method: Replaces the final step of Hoeffding's inequality in the classical VC argument with normal approximation using explicit Berry-Esseen error control, focusing on moderate-deviation regimes.

Result: Obtains a moderate-deviation sharpening of the usual VC estimate, with an additional factor of order (ε√n)^{-1} in the leading exponential term when ε√n is large.

Conclusion: The refined probabilistic approach using normal approximation with Berry-Esseen error control provides sharper convergence rate estimates for the VC theorem in moderate-deviation scenarios.

Abstract: Vapnik--Chervonenkis' theorem is a seminal result in machine learning. It establishes sufficient conditions for empirical probabilities to converge to theoretical probabilities, uniformly over families of events. It also provides an estimate for the rate of such uniform convergence.
  We revisit the probabilistic component of the classical argument. Instead of applying Hoeffding's inequality at the final step, we use a normal approximation with explicit Berry--Esseen error control. This yields a moderate-deviation sharpening of the usual VC estimate, with an additional factor of order $(\varepsilon\sqrt{n})^{-1}$ in the leading exponential term when $\varepsilon\sqrt{n}$ is large.

</details>


### [94] [PyHealth 2.0: A Comprehensive Open-Source Toolkit for Accessible and Reproducible Clinical Deep Learning](https://arxiv.org/abs/2601.16414)
*John Wu,Yongda Fan,Zhenbang Wu,Paul Landes,Eric Schrock,Sayeed Sajjad Razin,Arjun Chatterjee,Naveen Baskaran,Joshua Steier,Andrea Fitzpatrick,Bilal Arif,Rian Atri,Jathurshan Pradeepkumar,Siddhartha Laghuvarapu,Junyi Gao,Adam R. Cross,Jimeng Sun*

Main category: cs.LG

TL;DR: PyHealth 2.0 is an enhanced clinical AI toolkit that enables predictive modeling in just 7 lines of code, addressing reproducibility, computational cost, and domain expertise barriers in healthcare AI research.


<details>
  <summary>Details</summary>
Motivation: The paper addresses persistent barriers in clinical AI research: difficulty replicating baselines, high computational costs, and required domain expertise that limit accessibility and reproducibility in healthcare AI.

Method: Developed PyHealth 2.0 as a comprehensive clinical deep learning toolkit that unifies 15+ datasets, 20+ clinical tasks, 25+ models, 5+ interpretability methods, and uncertainty quantification including conformal prediction. Supports diverse clinical data modalities (signals, imaging, EHRs) with translation of 5+ medical coding standards, and features accessibility-focused design for multimodal data and diverse computational resources.

Result: PyHealth 2.0 achieves up to 39x faster processing and 20x lower memory usage, enabling work from 16GB laptops to production systems. It has built an active open-source community of 400+ members with extensive documentation and multi-language support via RHealth, lowering domain expertise barriers.

Conclusion: PyHealth 2.0 establishes an open-source foundation and community that advances accessible, reproducible healthcare AI by addressing key barriers in clinical AI research through a unified framework that supports diverse data modalities and computational environments.

Abstract: Difficulty replicating baselines, high computational costs, and required domain expertise create persistent barriers to clinical AI research. To address these challenges, we introduce PyHealth 2.0, an enhanced clinical deep learning toolkit that enables predictive modeling in as few as 7 lines of code. PyHealth 2.0 offers three key contributions: (1) a comprehensive toolkit addressing reproducibility and compatibility challenges by unifying 15+ datasets, 20+ clinical tasks, 25+ models, 5+ interpretability methods, and uncertainty quantification including conformal prediction within a single framework that supports diverse clinical data modalities - signals, imaging, and electronic health records - with translation of 5+ medical coding standards; (2) accessibility-focused design accommodating multimodal data and diverse computational resources with up to 39x faster processing and 20x lower memory usage, enabling work from 16GB laptops to production systems; and (3) an active open-source community of 400+ members lowering domain expertise barriers through extensive documentation, reproducible research contributions, and collaborations with academic health systems and industry partners, including multi-language support via RHealth. PyHealth 2.0 establishes an open-source foundation and community advancing accessible, reproducible healthcare AI. Available at pip install pyhealth.

</details>


### [95] [Sample-wise Constrained Learning via a Sequential Penalty Approach with Applications in Image Processing](https://arxiv.org/abs/2601.16812)
*Francesca Lanzillotta,Chiara Albisani,Davide Pucci,Daniele Baracchi,Alessandro Piva,Matteo Lapucci*

Main category: cs.LG

TL;DR: A sequential penalty method for deep learning that handles strict constraints on individual data samples rather than arbitrary penalties, with convergence guarantees and practical viability.


<details>
  <summary>Details</summary>
Motivation: In many learning tasks, requirements on processing individual data samples should be formalized as strict constraints rather than arbitrary penalties in the optimization problem.

Method: A sequential penalty method that properly deals with constraints in learning scenarios, designed to work with deep learning frameworks.

Result: The algorithm possesses convergence guarantees under reasonable deep learning assumptions, and experimental results on image processing tasks show practical viability.

Conclusion: The proposed sequential penalty method effectively handles strict constraints in learning tasks, offers theoretical convergence guarantees, and demonstrates practical applicability in real-world scenarios like image processing.

Abstract: In many learning tasks, certain requirements on the processing of individual data samples should arguably be formalized as strict constraints in the underlying optimization problem, rather than by means of arbitrary penalties. We show that, in these scenarios, learning can be carried out exploiting a sequential penalty method that allows to properly deal with constraints. The proposed algorithm is shown to possess convergence guarantees under assumptions that are reasonable in deep learning scenarios. Moreover, the results of experiments on image processing tasks show that the method is indeed viable to be used in practice.

</details>


### [96] [Bayesian Experimental Design for Model Discrepancy Calibration: A Rivalry between Kullback--Leibler Divergence and Wasserstein Distance](https://arxiv.org/abs/2601.16425)
*Huchen Yang,Xinghao Dong,Jin-Long Wu*

Main category: cs.LG

TL;DR: This paper compares KL divergence vs Wasserstein distance as utility functions in Bayesian experimental design, showing KL works better without model errors but Wasserstein is more robust with model discrepancies.


<details>
  <summary>Details</summary>
Motivation: The selection of utility functions in Bayesian experimental design (BED) is an active research area, with different criteria emphasizing different notions of information. While KL divergence is common, recent studies propose Wasserstein distance as an alternative, but systematic comparisons are needed to understand their trade-offs in practical applications.

Method: The authors first use a toy example to illustrate issues with Wasserstein distance, showing its value depends on posterior position within support and can give false rewards. They then conduct systematic comparison through a classical source inversion problem in BED literature, analyzing performance with and without model discrepancy.

Result: KL divergence leads to faster convergence in the absence of model discrepancy, while Wasserstein metrics provide more robust sequential BED results when model discrepancy is non-negligible. The toy example reveals Wasserstein distance can exhibit false rewards unrelated to information gain, especially with non-informative priors.

Conclusion: The findings clarify trade-offs between KL divergence and Wasserstein metrics for utility functions in BED. KL divergence is preferable when model accuracy is high, while Wasserstein distance offers robustness when model discrepancies exist, providing practical guidelines for selecting appropriate criteria in different experimental design scenarios.

Abstract: Designing experiments that systematically gather data from complex physical systems is central to accelerating scientific discovery. While Bayesian experimental design (BED) provides a principled, information-based framework that integrates experimental planning with probabilistic inference, the selection of utility functions in BED is a long-standing and active topic, where different criteria emphasize different notions of information. Although Kullback--Leibler (KL) divergence has been one of the most common choices, recent studies have proposed Wasserstein distance as an alternative. In this work, we first employ a toy example to illustrate an issue of Wasserstein distance - the value of Wasserstein distance of a fixed-shape posterior depends on the relative position of its main mass within the support and can exhibit false rewards unrelated to information gain, especially with a non-informative prior (e.g., uniform distribution). We then further provide a systematic comparison between these two criteria through a classical source inversion problem in the BED literature, revealing that the KL divergence tends to lead to faster convergence in the absence of model discrepancy, while Wasserstein metrics provide more robust sequential BED results if model discrepancy is non-negligible. These findings clarify the trade-offs between KL divergence and Wasserstein metrics for the utility function and provide guidelines for selecting suitable criteria in practical BED applications.

</details>


### [97] [Safe Multitask Molecular Graph Networks for Vapor Pressure and Odor Threshold Prediction](https://arxiv.org/abs/2601.16426)
*Shuang Wu,Meijie Wang,Lun Yu*

Main category: cs.LG

TL;DR: The paper investigates vapor pressure and odor threshold modeling using molecular graph features with GINE and PNA backbones, introduces a "safe multitask" approach for joint training, and provides comprehensive experimental analysis.


<details>
  <summary>Details</summary>
Motivation: To develop robust models for two important odor-related properties (vapor pressure and odor threshold) with strong out-of-distribution generalization capabilities, addressing challenges in multitask learning where auxiliary tasks might harm primary task performance.

Method: Uses Bemis-Murcko scaffold split for OOD evaluation, A20/E17 molecular graph features (20D atom + 17D bond features), compares GINE and PNA backbones, and introduces "safe multitask" approach with VP as primary task, OP as auxiliary using delayed activation, gradient clipping, and small weights.

Result: PNA achieves Val MSE ≈0.21 for VP; A20/E17 with robust training achieves Val MSE ≈0.60-0.61 for OP; "safe multitask" approach yields best VP generalization without harming primary task performance while benefiting from auxiliary OP data.

Conclusion: The proposed methods effectively model odor-related properties with good OOD generalization, and the "safe multitask" strategy successfully leverages auxiliary tasks without compromising primary task performance, providing a reproducible framework for molecular property prediction.

Abstract: We investigate two important tasks in odor-related property modeling: Vapor Pressure (VP) and Odor Threshold (OP). To evaluate the model's out-of-distribution (OOD) capability, we adopt the Bemis-Murcko scaffold split. In terms of features, we introduce the rich A20/E17 molecular graph features (20-dimensional atom features + 17-dimensional bond features) and systematically compare GINE and PNA backbones. The results show: for VP, PNA with a simple regression head achieves Val MSE $\approx$ 0.21 (normalized space); for the OP single task under the same scaffold split, using A20/E17 with robust training (Huber/winsor) achieves Val MSE $\approx$ 0.60-0.61. For multitask training, we propose a **"safe multitask"** approach: VP as the primary task and OP as the auxiliary task, using delayed activation + gradient clipping + small weight, which avoids harming the primary task and simultaneously yields the best VP generalization performance. This paper provides complete reproducible experiments, ablation studies, and error-similarity analysis while discussing the impact of data noise and method limitations.

</details>


### [98] [Endless Terminals: Scaling RL Environments for Terminal Agents](https://arxiv.org/abs/2601.16443)
*Kanishk Gandhi,Shivam Garg,Noah D. Goodman,Dimitris Papailiopoulos*

Main category: cs.LG

TL;DR: Endless Terminals: A fully autonomous pipeline for procedurally generating terminal-use tasks enables effective RL training, showing substantial improvements across models despite simple PPO implementation.


<details>
  <summary>Details</summary>
Motivation: Current terminal benchmarks are designed for evaluation, not training. RL requires scalable environments, not just datasets. There's a need for autonomous generation of diverse terminal tasks without human annotation.

Method: Four-stage pipeline: 1) Generate diverse task descriptions, 2) Build and validate containerized environments, 3) Produce completion tests, 4) Filter for solvability. Training uses vanilla PPO with binary episode-level rewards and minimal interaction loop (no retrieval, multi-agent coordination, or specialized tools).

Result: Generated 3255 tasks spanning file operations, log management, data processing, scripting, and database operations. Models show substantial gains: Llama-3.2-3B improves from 4.0% to 18.2%, Qwen2.5-7B from 10.7% to 53.3%, Qwen3-8B-openthinker-sft from 42.6% to 59.0% on held-out dev set. Improvements transfer to human-curated benchmarks (TerminalBench 2.0).

Conclusion: Simple RL succeeds when environments scale. The Endless Terminals pipeline demonstrates that procedurally generated training environments enable substantial agent improvement without complex agentic scaffolds, highlighting the importance of scalable environment generation for self-improving agents.

Abstract: Environments are the bottleneck for self-improving agents. Current terminal benchmarks were built for evaluation, not training; reinforcement learning requires a scalable pipeline, not just a dataset. We introduce Endless Terminals, a fully autonomous pipeline that procedurally generates terminal-use tasks without human annotation. The pipeline has four stages: generating diverse task descriptions, building and validating containerized environments, producing completion tests, and filtering for solvability. From this pipeline we obtain 3255 tasks spanning file operations, log management, data processing, scripting, and database operations. We train agents using vanilla PPO with binary episode level rewards and a minimal interaction loop: no retrieval, multi-agent coordination, or specialized tools. Despite this simplicity, models trained on Endless Terminals show substantial gains: on our held-out dev set, Llama-3.2-3B improves from 4.0% to 18.2%, Qwen2.5-7B from 10.7% to 53.3%, and Qwen3-8B-openthinker-sft from 42.6% to 59.0%. These improvements transfer to human-curated benchmarks: models trained on Endless Terminals show substantial gains on held out human curated benchmarks: on TerminalBench 2.0, Llama-3.2-3B improves from 0.0% to 2.2%, Qwen2.5-7B from 2.2% to 3.4%, and Qwen3-8B-openthinker-sft from 1.1% to 6.7%, in each case outperforming alternative approaches including models with more complex agentic scaffolds. These results demonstrate that simple RL succeeds when environments scale.

</details>


### [99] [Brownian ReLU(Br-ReLU): A New Activation Function for a Long-Short Term Memory (LSTM) Network](https://arxiv.org/abs/2601.16446)
*George Awiakye-Marfo,Elijah Agbosu,Victoria Mawuena Barns,Samuel Asante Gyamerah*

Main category: cs.LG

TL;DR: Introduces BrownianReLU, a stochastic activation function based on Brownian motion that improves gradient stability in LSTM networks for financial time series analysis.


<details>
  <summary>Details</summary>
Motivation: Standard activation functions (ReLU, LeakyReLU, PReLU) suffer from gradient instability when applied to noisy, non-stationary financial time series, leading to poor learning performance.

Method: Proposes BrownianReLU, a stochastic activation function induced by Brownian motion, implemented using Monte Carlo simulation to provide smooth, adaptive responses for negative inputs and mitigate the dying ReLU problem.

Result: Evaluation on financial time series (Apple, GCB, S&P 500) and LendingClub loan data shows consistently lower Mean Squared Error and higher R² values, indicating improved predictive accuracy and generalization.

Conclusion: BrownianReLU enhances gradient propagation and learning stability in LSTMs for financial applications, with activation choice significantly affecting the trade-off between accuracy and sensitivity, yielding practically meaningful performance improvements.

Abstract: Deep learning models are effective for sequential data modeling, yet commonly used activation functions such as ReLU, LeakyReLU, and PReLU often exhibit gradient instability when applied to noisy, non-stationary financial time series. This study introduces BrownianReLU, a stochastic activation function induced by Brownian motion that enhances gradient propagation and learning stability in Long Short-Term Memory (LSTM) networks. Using Monte Carlo simulation, BrownianReLU provides a smooth, adaptive response for negative inputs, mitigating the dying ReLU problem. The proposed activation is evaluated on financial time series from Apple, GCB, and the S&P 500, as well as LendingClub loan data for classification. Results show consistently lower Mean Squared Error and higher $R^2$ values, indicating improved predictive accuracy and generalization. Although ROC-AUC metric is limited in classification tasks, activation choice significantly affects the trade-off between accuracy and sensitivity, with Brownian ReLU and the selected activation functions yielding practically meaningful performance.

</details>


### [100] [On the Expressive Power of Floating-Point Transformers](https://arxiv.org/abs/2601.16450)
*Sejun Park,Yeachan Park,Geonho Hwang*

Main category: cs.LG

TL;DR: Floating-point transformers can represent non-permutation-equivariant functions without positional encoding, can represent all permutation-equivariant functions only for bounded sequence lengths, and non-trivial positional encoding harms their representability.


<details>
  <summary>Details</summary>
Motivation: Existing theoretical results on transformers assume real parameters and exact operations, but real implementations use floating-point numbers with round-off errors. This work investigates how these practical limitations affect transformers' representational capabilities.

Method: Analyze floating-point transformers that use floating-point parameters and operations with round-off errors, examining their representability properties compared to ideal transformers with exact operations.

Result: 1) Floating-point transformers can represent non-permutation-equivariant functions even without positional encoding. 2) They can represent all permutation-equivariant functions only when sequence length is bounded, but fail for large sequences. 3) Found minimal equivariance structure in floating-point transformers. 4) Non-trivial additive positional encoding harms representability.

Conclusion: Practical floating-point implementations of transformers have different representational properties than ideal theoretical models, with limitations on permutation-equivariance and negative effects of positional encoding due to numerical precision constraints.

Abstract: The study on the expressive power of transformers shows that transformers are permutation equivariant, and they can approximate all permutation-equivariant continuous functions on a compact domain. However, these results are derived under real parameters and exact operations, while real implementations on computers can only use a finite set of numbers and inexact machine operations with round-off errors. In this work, we investigate the representability of floating-point transformers that use floating-point parameters and floating-point operations. Unlike existing results under exact operations, we first show that floating-point transformers can represent a class of non-permutation-equivariant functions even without positional encoding. Furthermore, we prove that floating-point transformers can represent all permutation-equivariant functions when the sequence length is bounded, but they cannot when the sequence length is large. We also found the minimal equivariance structure in floating-point transformers, and show that all non-trivial additive positional encoding can harm the representability of floating-point transformers.

</details>


### [101] [On the Effects of Adversarial Perturbations on Distribution Robustness](https://arxiv.org/abs/2601.16464)
*Yipei Wang,Zhaoying Pan,Xiaoqian Wang*

Main category: cs.LG

TL;DR: The paper analyzes the tradeoff between adversarial robustness and distribution robustness, finding that while adversarial training can harm distribution robustness by increasing reliance on spurious features, moderate ℓ∞ perturbations on moderately biased data can actually improve distribution robustness when feature separability is high.


<details>
  <summary>Details</summary>
Motivation: To understand the complex relationship between adversarial robustness (resistance to input perturbations) and distribution robustness (performance under data shifts), particularly how adversarial training affects reliance on spurious features and impacts underrepresented subgroups.

Method: Theoretical analysis using tractable surrogate for per-step adversarial training by studying models trained on perturbed data, examining ℓ∞ perturbations on data with varying degrees of bias and feature separability.

Result: Identified both a tradeoff (adversarial training can harm distribution robustness) and a nuanced phenomenon where moderate ℓ∞ perturbations on moderately biased data can increase distribution robustness, especially when simplicity bias induces reliance on core features with high separability.

Conclusion: The tradeoff between adversarial and distribution robustness is nuanced and depends on feature separability; overlooking feature separability can lead to misleading conclusions about robustness relationships.

Abstract: Adversarial robustness refers to a model's ability to resist perturbation of inputs, while distribution robustness evaluates the performance of the model under data shifts. Although both aim to ensure reliable performance, prior work has revealed a tradeoff in distribution and adversarial robustness. Specifically, adversarial training might increase reliance on spurious features, which can harm distribution robustness, especially the performance on some underrepresented subgroups. We present a theoretical analysis of adversarial and distribution robustness that provides a tractable surrogate for per-step adversarial training by studying models trained on perturbed data. In addition to the tradeoff, our work further identified a nuanced phenomenon that $\ell_\infty$ perturbations on data with moderate bias can yield an increase in distribution robustness. Moreover, the gain in distribution robustness remains on highly skewed data when simplicity bias induces reliance on the core feature, characterized as greater feature separability. Our theoretical analysis extends the understanding of the tradeoff by highlighting the interplay of the tradeoff and the feature separability. Despite the tradeoff that persists in many cases, overlooking the role of feature separability may lead to misleading conclusions about robustness.

</details>


### [102] [A Cautionary Tale of Self-Supervised Learning for Imaging Biomarkers: Alzheimer's Disease Case Study](https://arxiv.org/abs/2601.16467)
*Maxwell Reynolds,Chaitanya Srinivasan,Vijay Cherupally,Michael Leone,Ke Yu,Li Sun,Tigmanshu Chaudhary,Andreas Pfenning,Kayhan Batmanghelich*

Main category: cs.LG

TL;DR: R-NCE, a new self-supervised learning framework for Alzheimer's disease biomarkers, outperforms traditional FreeSurfer features and existing SSL methods in disease classification and prediction tasks while showing biological relevance through genetic associations.


<details>
  <summary>Details</summary>
Motivation: Current structural MRI biomarkers for Alzheimer's disease rely on hand-crafted features like cortical thickness or volume, which may not capture the full biological complexity. Self-supervised learning has potential to uncover more powerful biomarkers from the same data, but existing SSL methods underperform traditional FreeSurfer features.

Method: Residual Noise Contrastive Estimation (R-NCE), a new SSL framework that integrates auxiliary FreeSurfer features while maximizing additional augmentation-invariant information from structural MRI data.

Result: R-NCE outperforms traditional FreeSurfer features and existing SSL methods across multiple benchmarks including AD classification, conversion prediction, and amyloid status prediction. R-NCE-derived Brain Age Gap measures show high heritability and genetic associations with MAPT and IRAG1 genes, with enrichment in astrocytes and oligodendrocytes.

Conclusion: R-NCE successfully uncovers more powerful and biologically relevant biomarkers from structural MRI data than traditional methods, demonstrating sensitivity to both neurodegenerative and cerebrovascular processes in Alzheimer's disease.

Abstract: Discovery of sensitive and biologically grounded biomarkers is essential for early detection and monitoring of Alzheimer's disease (AD). Structural MRI is widely available but typically relies on hand-crafted features such as cortical thickness or volume. We ask whether self-supervised learning (SSL) can uncover more powerful biomarkers from the same data. Existing SSL methods underperform FreeSurfer-derived features in disease classification, conversion prediction, and amyloid status prediction. We introduce Residual Noise Contrastive Estimation (R-NCE), a new SSL framework that integrates auxiliary FreeSurfer features while maximizing additional augmentation-invariant information. R-NCE outperforms traditional features and existing SSL methods across multiple benchmarks, including AD conversion prediction. To assess biological relevance, we derive Brain Age Gap (BAG) measures and perform genome-wide association studies. R-NCE-BAG shows high heritability and associations with MAPT and IRAG1, with enrichment in astrocytes and oligodendrocytes, indicating sensitivity to neurodegenerative and cerebrovascular processes.

</details>


### [103] [Robust Categorical Data Clustering Guided by Multi-Granular Competitive Learning](https://arxiv.org/abs/2601.16491)
*Shenghong Cai,Yiqun Zhang,Xiaopeng Luo,Yiu-Ming Cheung,Hong Jia,Peng Liu*

Main category: cs.LG

TL;DR: MGCPL algorithm for categorical data clustering that handles nested granular clusters through competitive penalization learning and encoding-based aggregation.


<details>
  <summary>Details</summary>
Motivation: Categorical data is common in big data but challenging for clustering due to undefined distance spaces and prevalent nested granular cluster effects where small clusters form larger ones.

Method: Proposes Multi-Granular Competitive Penalization Learning (MGCPL) algorithm for interactive cluster tuning, and Cluster Aggregation based on MGCPL Encoding (CAME) to encode data objects before final clustering.

Result: MCDC approach effectively explores nested multi-granular clusters, is robust across domains, has linear time complexity, scalable to large datasets, and superior to state-of-the-art methods.

Conclusion: The proposed MCDC approach is competent for categorical data clustering, automatically handles nested granular distributions, and shows promise for distributed computing applications.

Abstract: Data set composed of categorical features is very common in big data analysis tasks. Since categorical features are usually with a limited number of qualitative possible values, the nested granular cluster effect is prevalent in the implicit discrete distance space of categorical data. That is, data objects frequently overlap in space or subspace to form small compact clusters, and similar small clusters often form larger clusters. However, the distance space cannot be well-defined like the Euclidean distance due to the qualitative categorical data values, which brings great challenges to the cluster analysis of categorical data. In view of this, we design a Multi-Granular Competitive Penalization Learning (MGCPL) algorithm to allow potential clusters to interactively tune themselves and converge in stages with different numbers of naturally compact clusters. To leverage MGCPL, we also propose a Cluster Aggregation strategy based on MGCPL Encoding (CAME) to first encode the data objects according to the learned multi-granular distributions, and then perform final clustering on the embeddings. It turns out that the proposed MGCPL-guided Categorical Data Clustering (MCDC) approach is competent in automatically exploring the nested distribution of multi-granular clusters and highly robust to categorical data sets from various domains. Benefiting from its linear time complexity, MCDC is scalable to large-scale data sets and promising in pre-partitioning data sets or compute nodes for boosting distributed computing. Extensive experiments with statistical evidence demonstrate its superiority compared to state-of-the-art counterparts on various real public data sets.

</details>


### [104] [BoostFGL: Boosting Fairness in Federated Graph Learning](https://arxiv.org/abs/2601.16496)
*Zekai Chen,Kairui Yang,Xunkai Li,Henan Sun,Zhihan Zhang,Jia Li,Qiangqiang Dai,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: BoostFGL is a fairness-aware federated graph learning framework that addresses performance disparities across disadvantaged node groups through coordinated client-side node boosting, topology boosting, and server-side model boosting mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing federated graph learning methods achieve high overall accuracy but conceal severe degradation on disadvantaged node groups due to three systematic sources of disparity: label skew toward majority patterns, topology confounding in message propagation, and aggregation dilution of updates from hard clients.

Method: BoostFGL introduces three coordinated mechanisms: 1) Client-side node boosting that reshapes local training signals to emphasize under-served nodes, 2) Client-side topology boosting that reallocates propagation emphasis toward reliable yet underused structures while attenuating misleading neighborhoods, and 3) Server-side model boosting that performs difficulty- and reliability-aware aggregation to preserve informative updates from hard clients while stabilizing the global model.

Result: Extensive experiments on 9 datasets show that BoostFGL delivers substantial fairness gains, improving Overall-F1 by 8.43% while preserving competitive overall performance against strong FGL baselines.

Conclusion: BoostFGL effectively addresses fairness issues in federated graph learning by systematically tackling the three sources of disparity through a coordinated boosting framework, achieving both fairness improvements and competitive overall performance.

Abstract: Federated graph learning (FGL) enables collaborative training of graph neural networks (GNNs) across decentralized subgraphs without exposing raw data. While existing FGL methods often achieve high overall accuracy, we show that this average performance can conceal severe degradation on disadvantaged node groups. From a fairness perspective, these disparities arise systematically from three coupled sources: label skew toward majority patterns, topology confounding in message propagation, and aggregation dilution of updates from hard clients. To address this, we propose \textbf{BoostFGL}, a boosting-style framework for fairness-aware FGL. BoostFGL introduces three coordinated mechanisms: \ding{182} \emph{Client-side node boosting}, which reshapes local training signals to emphasize systematically under-served nodes; \ding{183} \emph{Client-side topology boosting}, which reallocates propagation emphasis toward reliable yet underused structures and attenuates misleading neighborhoods; and \ding{184} \emph{Server-side model boosting}, which performs difficulty- and reliability-aware aggregation to preserve informative updates from hard clients while stabilizing the global model. Extensive experiments on 9 datasets show that BoostFGL delivers substantial fairness gains, improving Overall-F1 by 8.43\%, while preserving competitive overall performance against strong FGL baselines.

</details>


### [105] [kNN-Graph: An adaptive graph model for $k$-nearest neighbors](https://arxiv.org/abs/2601.16509)
*Jiaye Li,Gang Chen,Hang Xu,Shichao Zhang*

Main category: cs.LG

TL;DR: The paper presents an adaptive graph model combining HNSW with pre-computed voting to decouple inference latency from computational complexity, enabling real-time kNN classification without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: kNN faces computational trade-offs between inference speed and accuracy in large-scale applications. Existing approximate nearest neighbor solutions accelerate retrieval but degrade precision and lack adaptability in selecting optimal neighborhood size (k).

Method: Integrates Hierarchical Navigable Small World (HNSW) graph with pre-computed voting mechanism, transferring computational burden of neighbor selection and weighting to training phase. Higher graph layers enable rapid navigation while lower layers encode precise node-specific decision boundaries with adaptive neighbor counts.

Result: Benchmarked against eight state-of-the-art baselines across six diverse datasets, demonstrating significantly accelerated inference speeds achieving real-time performance without compromising classification accuracy.

Conclusion: Offers scalable, robust solution to long-standing inference bottleneck of kNN, establishing new structural paradigm for graph-based nonparametric learning.

Abstract: The k-nearest neighbors (kNN) algorithm is a cornerstone of non-parametric classification in artificial intelligence, yet its deployment in large-scale applications is persistently constrained by the computational trade-off between inference speed and accuracy. Existing approximate nearest neighbor solutions accelerate retrieval but often degrade classification precision and lack adaptability in selecting the optimal neighborhood size (k). Here, we present an adaptive graph model that decouples inference latency from computational complexity. By integrating a Hierarchical Navigable Small World (HNSW) graph with a pre-computed voting mechanism, our framework completely transfers the computational burden of neighbor selection and weighting to the training phase. Within this topological structure, higher graph layers enable rapid navigation, while lower layers encode precise, node-specific decision boundaries with adaptive neighbor counts. Benchmarking against eight state-of-the-art baselines across six diverse datasets, we demonstrate that this architecture significantly accelerates inference speeds, achieving real-time performance, without compromising classification accuracy. These findings offer a scalable, robust solution to the long-standing inference bottleneck of kNN, establishing a new structural paradigm for graph-based nonparametric learning.

</details>


### [106] [Interpretable Fine-Gray Deep Survival Model for Competing Risks: Predicting Post-Discharge Foot Complications for Diabetic Patients in Ontario](https://arxiv.org/abs/2511.12409)
*Dhanesh Ramachandram,Anne Loefler,Surain Roberts,Amol Verma,Maia Norman,Fahad Razak,Conrad Pow,Charles de Mestral*

Main category: cs.LG

TL;DR: CRISPNAM-FG is an interpretable deep learning model for competing risks survival analysis that combines Neural Additive Models with Fine-Gray formulation to provide transparent predictions while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: Current deep learning survival models have good predictive performance but lack interpretability, which hinders their clinical adoption. There's a need for intrinsically interpretable models that clinicians can trust for medical applications like competing risks survival analysis.

Method: Proposes CRISPNAM-FG, an intrinsically interpretable survival model that leverages Neural Additive Models (NAMs) structure with separate projection vectors for each risk. It predicts the Cumulative Incidence Function using the Fine-Gray formulation, providing transparent and auditable predictions through shape functions and feature importance plots.

Result: The model achieves competitive performance compared to other deep survival models on benchmark datasets. It was successfully applied to predict future foot complications in diabetic patients across 29 Ontario hospitals (2016-2023), demonstrating both predictive power and interpretability.

Conclusion: CRISPNAM-FG provides a solution to the interpretability gap in deep survival models, offering both high predictive performance and transparency through intrinsic interpretability, making it suitable for clinical adoption where trust and auditability are crucial.

Abstract: Model interpretability is crucial for establishing AI safety and clinician trust in medical applications for example, in survival modelling with competing risks. Recent deep learning models have attained very good predictive performance but their limited transparency, being black-box models, hinders their integration into clinical practice. To address this gap, we propose an intrinsically interpretable survival model called CRISPNAM-FG. Leveraging the structure of Neural Additive Models (NAMs) with separate projection vectors for each risk, our approach predicts the Cumulative Incidence Function using the Fine-Gray formulation, achieving high predictive power with intrinsically transparent and auditable predictions. We validated the model on several benchmark datasets and applied our model to predict future foot complications in diabetic patients across 29 Ontario hospitals (2016-2023). Our method achieves competitive performance compared to other deep survival models while providing transparency through shape functions and feature importance plots.

</details>


### [107] [Theory of Minimal Weight Perturbations in Deep Networks and its Applications for Low-Rank Activated Backdoor Attacks](https://arxiv.org/abs/2601.16880)
*Bethan Evans,Jared Tanner*

Main category: cs.LG

TL;DR: The paper derives exact formulas for minimal weight perturbations needed to change DNN outputs, analyzes layer sensitivity, and applies these to backdoor attacks and compression thresholds.


<details>
  <summary>Details</summary>
Motivation: To understand the minimal parameter changes required to alter DNN outputs, analyze layer-wise sensitivity, and provide certifiable guarantees for robustness against attacks like precision-modification-activated backdoors.

Method: Derives exact single-layer formulas for minimal norm weight perturbations, contrasts with multi-layer Lipschitz constant guarantees, applies to backdoor attacks, establishes provable compression thresholds, and analyzes back-propagated margins.

Result: Single-layer exact formulas and multi-layer Lipschitz guarantees are of the same order, indicating similar efficacy. Provable compression thresholds established below which backdoor attacks cannot succeed. Low-rank compression can activate latent backdoors while preserving accuracy.

Conclusion: The derived expressions reveal how back-propagated margins govern layer sensitivity and provide certifiable guarantees on smallest parameter updates for desired output shifts, with applications to robustness analysis and backdoor attack detection.

Abstract: The minimal norm weight perturbations of DNNs required to achieve a specified change in output are derived and the factors determining its size are discussed. These single-layer exact formulae are contrasted with more generic multi-layer Lipschitz constant based robustness guarantees; both are observed to be of the same order which indicates similar efficacy in their guarantees. These results are applied to precision-modification-activated backdoor attacks, establishing provable compression thresholds below which such attacks cannot succeed, and show empirically that low-rank compression can reliably activate latent backdoors while preserving full-precision accuracy. These expressions reveal how back-propagated margins govern layer-wise sensitivity and provide certifiable guarantees on the smallest parameter updates consistent with a desired output shift.

</details>


### [108] [Finite-Time Analysis of Gradient Descent for Shallow Transformers](https://arxiv.org/abs/2601.16514)
*Enes Arda,Semih Cayci,Atilla Eryilmaz*

Main category: cs.LG

TL;DR: Shallow Transformers in kernel regime need only logarithmic width scaling with sample size and have optimization error independent of sequence length, unlike RNNs where error grows exponentially with sequence length.


<details>
  <summary>Details</summary>
Motivation: To understand why Transformers perform so well despite their non-convex optimization landscape, and to analyze their optimization properties compared to recurrent architectures.

Method: Analyze a shallow Transformer with m independent heads trained by projected gradient descent in the kernel regime, examining width scaling and optimization error dependencies.

Result: Two key findings: (1) width required for nonasymptotic guarantees scales only logarithmically with sample size n, (2) optimization error is independent of sequence length T (unlike RNNs where error grows exponentially with T).

Conclusion: Transformers have favorable optimization properties with logarithmic width scaling and sequence-length-independent optimization error, though they require memory that grows with sequence length to maintain full context.

Abstract: Understanding why Transformers perform so well remains challenging due to their non-convex optimization landscape. In this work, we analyze a shallow Transformer with $m$ independent heads trained by projected gradient descent in the kernel regime. Our analysis reveals two main findings: (i) the width required for nonasymptotic guarantees scales only logarithmically with the sample size $n$, and (ii) the optimization error is independent of the sequence length $T$. This contrasts sharply with recurrent architectures, where the optimization error can grow exponentially with $T$. The trade-off is memory: to keep the full context, the Transformer's memory requirement grows with the sequence length. We validate our theoretical results numerically in a teacher-student setting and confirm the predicted scaling laws for Transformers.

</details>


### [109] [Rethinking Large Language Models For Irregular Time Series Classification In Critical Care](https://arxiv.org/abs/2601.16516)
*Feixiang Zheng,Yu Wu,Cecilia Mascolo,Ting Dang*

Main category: cs.LG

TL;DR: LLMs for ICU time series show promise but have limitations: encoder design is crucial (12.8% AUPRC improvement), alignment matters less (2.9% improvement), but LLMs require 10× longer training than supervised models with comparable performance and underperform in few-shot settings.


<details>
  <summary>Details</summary>
Motivation: To investigate how well Large Language Models (LLMs) perform on irregular ICU time series data with high missing values, focusing on two key components: time series encoder design and multimodal alignment strategy.

Method: Established a systematic testbed to evaluate LLM-based methods on benchmark ICU datasets against strong supervised and self-supervised baselines, examining different encoder designs and alignment strategies.

Result: Encoder design is critical - irregularity-aware encoders achieved 12.8% AUPRC improvement over vanilla Transformer. Alignment strategy had modest impact (2.9% improvement). LLMs require 10× longer training than best supervised models with comparable performance, and underperform in few-shot learning.

Conclusion: LLMs show promise for irregular ICU time series but have current limitations: encoder design matters most, alignment matters less, training is much slower than supervised methods, and performance is poor in data-scarce settings.

Abstract: Time series data from the Intensive Care Unit (ICU) provides critical information for patient monitoring. While recent advancements in applying Large Language Models (LLMs) to time series modeling (TSM) have shown great promise, their effectiveness on the irregular ICU data, characterized by particularly high rates of missing values, remains largely unexplored. This work investigates two key components underlying the success of LLMs for TSM: the time series encoder and the multimodal alignment strategy. To this end, we establish a systematic testbed to evaluate their impact across various state-of-the-art LLM-based methods on benchmark ICU datasets against strong supervised and self-supervised baselines. Results reveal that the encoder design is more critical than the alignment strategy. Encoders that explicitly model irregularity achieve substantial performance gains, yielding an average AUPRC increase of $12.8\%$ over the vanilla Transformer. While less impactful, the alignment strategy is also noteworthy, with the best-performing semantically rich, fusion-based strategy achieving a modest $2.9\%$ improvement over cross-attention. However, LLM-based methods require at least 10$\times$ longer training than the best-performing irregular supervised models, while delivering only comparable performance. They also underperform in data-scarce few-shot learning settings. These findings highlight both the promise and current limitations of LLMs for irregular ICU time series. The code is available at https://github.com/mHealthUnimelb/LLMTS.

</details>


### [110] [DANCE: Dynamic, Available, Neighbor-gated Condensation for Federated Text-Attributed Graphs](https://arxiv.org/abs/2601.16519)
*Zekai Chen,Haodong Lu,Xunkai Li,Henan Sun,Jia Li,Hongchao Qin,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: DANCE improves TAG-FGL by using round-wise graph condensation with model-in-the-loop refresh and provenance-preserving evidence packs, achieving better accuracy with fewer tokens.


<details>
  <summary>Details</summary>
Motivation: Current TAG-FGL methods face three challenges: high LLM overhead from processing long texts, suboptimal performance due to non-adaptive graph condensation, and lack of interpretability in LLM-based condensation.

Method: DANCE introduces round-wise, model-in-the-loop condensation refresh using the latest global model to improve performance, and preserves provenance through locally inspectable evidence packs that trace predictions to specific neighbors and source text spans.

Result: Across 8 TAG datasets, DANCE improves accuracy by 2.33% at an 8% condensation ratio, with 33.42% fewer tokens than baselines.

Conclusion: DANCE addresses the key challenges of TAG-FGL by providing an efficient, adaptive, and interpretable solution that balances performance with practical constraints.

Abstract: Federated graph learning (FGL) enables collaborative training on graph data across multiple clients. With the rise of large language models (LLMs), textual attributes in FGL graphs are gaining attention. Text-attributed graph federated learning (TAG-FGL) improves FGL by explicitly leveraging LLMs to process and integrate these textual features. However, current TAG-FGL methods face three main challenges: \textbf{(1) Overhead.} LLMs for processing long texts incur high token and computation costs. To make TAG-FGL practical, we introduce graph condensation (GC) to reduce computation load, but this choice also brings new issues. \textbf{(2) Suboptimal.} To reduce LLM overhead, we introduce GC into TAG-FGL by compressing multi-hop texts/neighborhoods into a condensed core with fixed LLM surrogates. However, this one-shot condensation is often not client-adaptive, leading to suboptimal performance. \textbf{(3) Interpretability.} LLM-based condensation further introduces a black-box bottleneck: summaries lack faithful attribution and clear grounding to specific source spans, making local inspection and auditing difficult. To address the above issues, we propose \textbf{DANCE}, a new TAG-FGL paradigm with GC. To improve \textbf{suboptimal} performance, DANCE performs round-wise, model-in-the-loop condensation refresh using the latest global model. To enhance \textbf{interpretability}, DANCE preserves provenance by storing locally inspectable evidence packs that trace predictions to selected neighbors and source text spans. Across 8 TAG datasets, DANCE improves accuracy by \textbf{2.33\%} at an \textbf{8\%} condensation ratio, with \textbf{33.42\%} fewer tokens than baselines.

</details>


### [111] [Beyond Superficial Unlearning: Sharpness-Aware Robust Erasure of Hallucinations in Multimodal LLMs](https://arxiv.org/abs/2601.16527)
*Xianya Fang,Feiyang Ren,Xiang Chen,Yu Tian,Zhen Bi,Haiyang Yu,Sheng-Jun Huang*

Main category: cs.LG

TL;DR: SARE addresses structural fragility in multimodal LLM hallucination unlearning by using targeted min-max optimization and loss landscape flattening for robust, stable erasure.


<details>
  <summary>Details</summary>
Motivation: Multimodal LLMs suffer from object hallucinations that harm reliability. Current unlearning methods have structural fragility - they achieve only superficial suppression, trapping models in sharp minima where hallucinations catastrophically resurge after lightweight relearning.

Method: Proposes SARE (Structural-Aware Robust Erasure) which casts unlearning as a targeted min-max optimization problem. Uses Targeted-SAM (Sharpness-Aware Minimization) mechanism to explicitly flatten the loss landscape around hallucinated concepts. Suppresses hallucinations under simulated worst-case parameter perturbations to ensure robust removal stable against weight shifts.

Result: SARE significantly outperforms baselines in erasure efficacy while preserving general generation quality. Maintains persistent hallucination suppression against relearning and parameter updates, validating the effectiveness of geometric stabilization.

Conclusion: Geometric stabilization through targeted min-max optimization and loss landscape flattening is crucial for robust hallucination unlearning in multimodal LLMs, addressing the structural fragility of standard erasure methods.

Abstract: Multimodal LLMs are powerful but prone to object hallucinations, which describe non-existent entities and harm reliability. While recent unlearning methods attempt to mitigate this, we identify a critical flaw: structural fragility. We empirically demonstrate that standard erasure achieves only superficial suppression, trapping the model in sharp minima where hallucinations catastrophically resurge after lightweight relearning. To ensure geometric stability, we propose SARE, which casts unlearning as a targeted min-max optimization problem and uses a Targeted-SAM mechanism to explicitly flatten the loss landscape around hallucinated concepts. By suppressing hallucinations under simulated worst-case parameter perturbations, our framework ensures robust removal stable against weight shifts. Extensive experiments demonstrate that SARE significantly outperforms baselines in erasure efficacy while preserving general generation quality. Crucially, it maintains persistent hallucination suppression against relearning and parameter updates, validating the effectiveness of geometric stabilization.

</details>


### [112] [A Collision-Free Hot-Tier Extension for Engram-Style Conditional Memory: A Controlled Study of Training Dynamics](https://arxiv.org/abs/2601.16531)
*Tao Lin*

Main category: cs.LG

TL;DR: Collision-free Engram-Nine doesn't improve validation loss despite eliminating high-frequency key collisions, revealing that collisions may provide beneficial regularization and gating issues are the real bottleneck.


<details>
  <summary>Details</summary>
Motivation: To investigate whether high-frequency key collisions are the primary bottleneck in Engram-style conditional memory systems, and to understand if eliminating collisions would improve training outcomes.

Method: Introduced Engram-Nine, a collision-free hot-tier extension using Minimal Perfect Hash Function (MPHF) for frequent n-grams while keeping original multi-head hashed lookup as cold tier. Used iso-parameter setup and route-stratified evaluation to decompose per-token loss into hot/cold contributions.

Result: Collision-free design didn't consistently improve validation loss. Found "hot-to-cold advantage flip" where cold positions eventually surpass hot positions. Collision-free configurations flipped earlier, suggesting collisions act as implicit regularization. Also identified gating mismatch where gate favors hot positions even after they become worse.

Conclusion: Improving lookup precision alone doesn't guarantee better training outcomes. The dominant limitation may be gating credit assignment rather than index accuracy, and collision-induced noise may provide beneficial regularization that shouldn't be naively eliminated.

Abstract: We investigate whether high-frequency key collisions are a primary bottleneck in Engram-style conditional memory. To isolate the effect of collisions, we introduce Engram-Nine, a collision-free hot-tier extension that maps the most frequent n-grams through a Minimal Perfect Hash Function (MPHF) while retaining the original multi-head hashed lookup as a cold tier. Under a strictly iso-parameter setup, the collision-free design does not consistently improve validation loss.
  Through route-stratified evaluation (decomposing per-token loss into hot/cold contributions), we uncover a consistent "hot-to-cold advantage flip" during training: hot (high-frequency) positions initially have lower loss, but cold positions eventually surpass them. Crucially, collision-free configurations flip earlier than collision-prone baselines, suggesting that collisions act as implicit regularization. We also identify a gating mismatch: the gate learns to favor hot positions early in training, but this preference persists even after the flip, assigning higher weights to positions with higher loss.
  Our findings suggest that improving lookup precision alone does not guarantee better training outcomes. The dominant limitation may lie in gating credit assignment rather than index accuracy, and collision-induced noise may provide beneficial regularization that should not be naively eliminated.

</details>


### [113] [Understanding and Improving UMAP with Geometric and Topological Priors: The JORC-UMAP Algorithm](https://arxiv.org/abs/2601.16552)
*Xiaobin Li,Run Zhang*

Main category: cs.LG

TL;DR: JORC-UMAP improves UMAP by using Ollivier-Ricci curvature and Jaccard similarity to better capture manifold geometry and reduce topological tearing.


<details>
  <summary>Details</summary>
Motivation: UMAP's local Euclidean distance assumption often fails to capture intrinsic manifold geometry, causing topological tearing and structural collapse. The sensitivity to k-nearest neighbor graph is identified as a key problem.

Method: Introduces Ollivier-Ricci curvature as a geometric prior to reinforce edges at geometric bottlenecks and reduce redundant links. Incorporates Jaccard similarity as a topological prior to ensure neighborhood consistency and handle noise sensitivity in curvature estimation.

Result: JORC-UMAP reduces tearing and collapse more effectively than standard UMAP and other dimensionality reduction methods, as measured by SVM accuracy and triplet preservation scores, while maintaining computational efficiency.

Conclusion: JORC-UMAP offers a geometry-aware enhancement to UMAP for more faithful data visualization by better distinguishing true manifold structure from spurious connections.

Abstract: Nonlinear dimensionality reduction techniques, particularly UMAP, are widely used for visualizing high-dimensional data. However, UMAP's local Euclidean distance assumption often fails to capture intrinsic manifold geometry, leading to topological tearing and structural collapse. We identify UMAP's sensitivity to the k-nearest neighbor graph as a key cause. To address this, we introduce Ollivier-Ricci curvature as a geometric prior, reinforcing edges at geometric bottlenecks and reducing redundant links. Since curvature estimation is noise-sensitive, we also incorporate a topological prior using Jaccard similarity to ensure neighborhood consistency. The resulting method, JORC-UMAP, better distinguishes true manifold structure from spurious connections. Experiments on synthetic and real-world datasets show that JORC-UMAP reduces tearing and collapse more effectively than standard UMAP and other DR methods, as measured by SVM accuracy and triplet preservation scores, while maintaining computational efficiency. This work offers a geometry-aware enhancement to UMAP for more faithful data visualization.

</details>


### [114] [Process-Tensor Tomography of SGD: Measuring Non-Markovian Memory via Back-Flow of Distinguishability](https://arxiv.org/abs/2601.16563)
*Vasileios Sevetlidis,George Pavlidis*

Main category: cs.LG

TL;DR: Proposes viewing neural training as a process tensor and introduces a model-agnostic witness of training memory based on back-flow of distinguishability, showing practical SGD deviates from Markov idealization.


<details>
  <summary>Details</summary>
Motivation: To develop a principled diagnostic tool to measure training memory in neural networks and provide empirical evidence that practical SGD training deviates from Markov idealization, enabling testable analysis of data order effects.

Method: Treats training as a process tensor mapping controllable instruments to model observables. Introduces a two-step protocol comparing outcome distributions after one vs two interventions, measuring distinguishability increase (Δ_BF) using TV/JS/Hellinger distances on softmax predictions over a fixed probe set.

Result: Consistent positive back-flow with tight confidence intervals, amplified by higher momentum, larger batch overlap, and more micro-steps. Collapses under causal break (resetting optimizer state), attributing effect to optimizer/data-state memory. Robust across distance metrics and inexpensive to compute.

Conclusion: Provides a measurement framework for training memory that positions SGD as non-Markovian, offers testable operators for data order effects, and creates a common stage to compare optimizers, curricula, and schedules through their induced training memory.

Abstract: This work proposes neural training as a \emph{process tensor}: a multi-time map that takes a sequence of controllable instruments (batch choices, augmentations, optimizer micro-steps) and returns an observable of the trained model. Building on this operational lens, we introduce a simple, model-agnostic witness of training memory based on \emph{back-flow of distinguishability}. In a controlled two-step protocol, we compare outcome distributions after one intervention versus two; the increase $Δ_{\mathrm{BF}} = D_2 - D_1>0$ (with $D\in\{\mathrm{TV}, \mathrm{JS}, \mathrm{H}\}$ measured on softmax predictions over a fixed probe set) certifies non-Markovianity. We observe consistent positive back-flow with tight bootstrap confidence intervals, amplification under higher momentum, larger batch overlap, and more micro-steps, and collapse under a \emph{causal break} (resetting optimizer state), directly attributing the effect to optimizer/data-state memory. The witness is robust across TV/JS/Hellinger, inexpensive to compute, and requires no architectural changes. We position this as a \emph{measurement} contribution: a principled diagnostic and empirical evidence that practical SGD deviates from the Markov idealization. An exploratory case study illustrates how the micro-level signal can inform curriculum orderings. "Data order matters" turns into a testable operator with confidence bounds, our framework offers a common stage to compare optimizers, curricula, and schedules through their induced training memory.

</details>


### [115] [Predicting Startup Success Using Large Language Models: A Novel In-Context Learning Approach](https://arxiv.org/abs/2601.16568)
*Abdurahman Maarouf,Alket Bakiaj,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: Proposes kNN-ICL, a k-nearest-neighbor-based in-context learning framework using LLMs for early-stage startup success prediction with minimal labeled data, outperforming traditional ML methods in data-scarce VC environments.


<details>
  <summary>Details</summary>
Motivation: Predicting early-stage startup success is challenging due to data scarcity in VC firms (often only have data on few dozen startups), limiting traditional ML methods that require large labeled datasets.

Method: Proposes kNN-ICL, a novel k-nearest-neighbor-based in-context learning framework that selects the most relevant past startups as demonstration examples based on similarity, using LLMs without model training.

Result: Using Crunchbase data, kNN-ICL achieves higher prediction accuracy than supervised ML baselines and vanilla in-context learning. High balanced accuracy achieved with as few as 50 examples.

Conclusion: In-context learning can serve as an effective decision-making tool for VC firms operating in data-scarce environments, enabling accurate startup success prediction with minimal labeled data.

Abstract: Venture capital (VC) investments in early-stage startups that end up being successful can yield high returns. However, predicting early-stage startup success remains challenging due to data scarcity (e.g., many VC firms have information about only a few dozen of early-stage startups and whether they were successful). This limits the effectiveness of traditional machine learning methods that rely on large labeled datasets for model training. To address this challenge, we propose an in-context learning framework for startup success prediction using large language models (LLMs) that requires no model training and leverages only a small set of labeled startups as demonstration examples. Specifically, we propose a novel k-nearest-neighbor-based in-context learning framework, called kNN-ICL, which selects the most relevant past startups as examples based on similarity. Using real-world profiles from Crunchbase, we find that the kNN-ICL approach achieves higher prediction accuracy than supervised machine learning baselines and vanilla in-context learning. Further, we study how performance varies with the number of in-context examples and find that a high balanced accuracy can be achieved with as few as 50 examples. Together, we demonstrate that in-context learning can serve as a decision-making tool for VC firms operating in data-scarce environments.

</details>


### [116] [Integrating Meteorological and Operational Data: A Novel Approach to Understanding Railway Delays in Finland](https://arxiv.org/abs/2601.16592)
*Vinicius Pozzobon Borin,Jean Michel de Souza Sant'Ana,Usama Raheel,Nurul Huda Mahmood*

Main category: cs.LG

TL;DR: First public dataset combining Finnish railway operations with synchronized meteorological data (2018-2024), enabling analysis of weather impacts on train delays and supporting ML applications in railway operations research.


<details>
  <summary>Details</summary>
Motivation: Existing datasets rarely integrate meteorological information with operational train data, despite weather being a significant factor in railway reliability, especially in Nordic regions where weather impacts are pronounced.

Method: Integrated operational metrics from Finland Digitraffic Railway Traffic Service with weather measurements from 209 environmental monitoring stations using spatial-temporal alignment via Haversine distance. Preprocessing included strategic missing data handling through spatial fallback algorithms, cyclical encoding of temporal features, and robust scaling of weather data.

Result: Created dataset with 28 engineered features across operational and meteorological variables, covering ~38.5 million observations from Finland's 5,915km rail network. Analysis revealed distinct seasonal patterns (winter delay rates >25%) and geographic clustering of high-delay corridors in central/northern Finland. Baseline XGBoost regression achieved MAE of 2.73 minutes for station-specific delay prediction.

Conclusion: The dataset enables diverse applications including train delay prediction, weather impact assessment, and infrastructure vulnerability mapping, providing researchers with a flexible resource for machine learning applications in railway operations research.

Abstract: Train delays result from complex interactions between operational, technical, and environmental factors. While weather impacts railway reliability, particularly in Nordic regions, existing datasets rarely integrate meteorological information with operational train data. This study presents the first publicly available dataset combining Finnish railway operations with synchronized meteorological observations from 2018-2024. The dataset integrates operational metrics from Finland Digitraffic Railway Traffic Service with weather measurements from 209 environmental monitoring stations, using spatial-temporal alignment via Haversine distance. It encompasses 28 engineered features across operational variables and meteorological measurements, covering approximately 38.5 million observations from Finland's 5,915-kilometer rail network. Preprocessing includes strategic missing data handling through spatial fallback algorithms, cyclical encoding of temporal features, and robust scaling of weather data to address sensor outliers. Analysis reveals distinct seasonal patterns, with winter months exhibiting delay rates exceeding 25\% and geographic clustering of high-delay corridors in central and northern Finland. Furthermore, the work demonstrates applications of the data set in analysing the reliability of railway traffic in Finland. A baseline experiment using XGBoost regression achieved a Mean Absolute Error of 2.73 minutes for predicting station-specific delays, demonstrating the dataset's utility for machine learning applications. The dataset enables diverse applications, including train delay prediction, weather impact assessment, and infrastructure vulnerability mapping, providing researchers with a flexible resource for machine learning applications in railway operations research.

</details>


### [117] [E2Former-V2: On-the-Fly Equivariant Attention with Linear Activation Memory](https://arxiv.org/abs/2601.16622)
*Lin Huang,Chengxiang Huang,Ziang Wang,Yiyue Du,Chu Wang,Haocheng Lu,Yunyang Li,Xiaoli Liu,Arthur Jiang,Jia Zhang*

Main category: cs.LG

TL;DR: E2Former-V2 is a scalable equivariant graph neural network architecture that uses algebraic sparsity and hardware-aware execution to overcome scalability bottlenecks in 3D atomistic modeling.


<details>
  <summary>Details</summary>
Motivation: Mainstream EGNN architectures face critical scalability bottlenecks due to explicit construction of geometric features or dense tensor products on every edge, limiting their efficiency for large-scale 3D atomistic systems.

Method: Introduces Equivariant Axis-Aligned Sparsification (EAAS) that transforms dense tensor contractions into sparse parity re-indexing operations via SO(3)→SO(2) change of basis, and On-the-Fly Equivariant Attention implemented via custom fused Triton kernel that eliminates materialized edge tensors.

Result: Achieves 20× improvement in TFLOPS compared to standard implementations while maintaining comparable predictive performance on SPICE and OMol25 datasets, enabling efficient training of large equivariant transformers on widely accessible GPU platforms.

Conclusion: E2Former-V2 demonstrates that algebraic sparsity combined with hardware-aware execution can overcome scalability bottlenecks in equivariant graph neural networks, making large-scale 3D atomistic modeling more efficient and accessible.

Abstract: Equivariant Graph Neural Networks (EGNNs) have become a widely used approach for modeling 3D atomistic systems. However, mainstream architectures face critical scalability bottlenecks due to the explicit construction of geometric features or dense tensor products on \textit{every} edge. To overcome this, we introduce \textbf{E2Former-V2}, a scalable architecture that integrates algebraic sparsity with hardware-aware execution. We first propose \textbf{E}quivariant \textbf{A}xis-\textbf{A}ligned \textbf{S}parsification (EAAS). EAAS builds on Wigner-$6j$ convolution by exploiting an $\mathrm{SO}(3) \rightarrow \mathrm{SO}(2)$ change of basis to transform computationally expensive dense tensor contractions into efficient, sparse parity re-indexing operations. Building on this representation, we introduce \textbf{On-the-Fly Equivariant Attention}, a fully node-centric mechanism implemented via a custom fused Triton kernel. By eliminating materialized edge tensors and maximizing SRAM utilization, our kernel achieves a \textbf{20$\times$ improvement in TFLOPS} compared to standard implementations. Extensive experiments on the SPICE and OMol25 datasets demonstrate that E2Former-V2 maintains comparable predictive performance while notably accelerating inference. This work demonstrates that large equivariant transformers can be trained efficiently using widely accessible GPU platforms. The code is avalible at https://github.com/IQuestLab/UBio-MolFM/tree/e2formerv2.

</details>


### [118] [Dual-Prototype Disentanglement: A Context-Aware Enhancement Framework for Time Series Forecasting](https://arxiv.org/abs/2601.16632)
*Haonan Yang,Jianchao Tang,Zhuo Li*

Main category: cs.LG

TL;DR: DPAD is a model-agnostic auxiliary framework that enhances time series forecasting models by dynamically disentangling complex temporal patterns through dual prototype banks and context-aware routing.


<details>
  <summary>Details</summary>
Motivation: Existing forecasting approaches often fail to dynamically disentangle complex, intertwined temporal patterns, resulting in static, averaged representations that lack context-aware capabilities.

Method: Proposes DPAD framework with: 1) Dynamic Dual-Prototype bank (DDP) with common pattern bank (strong temporal priors) and rare pattern bank (critical infrequent events), 2) Dual-Path Context-aware routing (DPC) mechanism for selective pattern retrieval, and 3) Disentanglement-Guided Loss (DGLoss) for role specialization and coverage.

Result: Comprehensive experiments show DPAD consistently improves forecasting performance and reliability of state-of-the-art models across diverse real-world benchmarks.

Conclusion: DPAD successfully addresses the limitation of static pattern learning in time series forecasting by providing dynamic pattern disentanglement and context-aware adaptation capabilities to existing models.

Abstract: Time series forecasting has witnessed significant progress with deep learning. While prevailing approaches enhance forecasting performance by modifying architectures or introducing novel enhancement strategies, they often fail to dynamically disentangle and leverage the complex, intertwined temporal patterns inherent in time series, thus resulting in the learning of static, averaged representations that lack context-aware capabilities. To address this, we propose the Dual-Prototype Adaptive Disentanglement framework (DPAD), a model-agnostic auxiliary method that equips forecasting models with the ability of pattern disentanglement and context-aware adaptation. Specifically, we construct a Dynamic Dual-Prototype bank (DDP), comprising a common pattern bank with strong temporal priors to capture prevailing trend or seasonal patterns, and a rare pattern bank dynamically memorizing critical yet infrequent events, and then an Dual-Path Context-aware routing (DPC) mechanism is proposed to enhance outputs with selectively retrieved context-specific pattern representations from the DDP. Additionally, we introduce a Disentanglement-Guided Loss (DGLoss) to ensure that each prototype bank specializes in its designated role while maintaining comprehensive coverage. Comprehensive experiments demonstrate that DPAD consistently improves forecasting performance and reliability of state-of-the-art models across diverse real-world benchmarks.

</details>


### [119] [Provably Robust Bayesian Counterfactual Explanations under Model Changes](https://arxiv.org/abs/2601.16659)
*Jamie Duell,Xiuyi Fan*

Main category: cs.LG

TL;DR: PSCE generates counterfactual explanations with probabilistic safety guarantees (δ-safe for confidence, ε-robust for variance) that remain valid under model updates.


<details>
  <summary>Details</summary>
Motivation: Existing counterfactual explanations become invalid/unreliable when models are frequently updated in real-world settings, creating a need for robust explanations that maintain validity under model changes.

Method: Probabilistically Safe CEs (PSCE) uses Bayesian principles to generate δ-safe (high predictive confidence) and ε-robust (low predictive variance) counterfactual explanations. Uncertainty-aware constraints are integrated into an optimization framework, creating the ⟨δ,ε⟩-set with formal probabilistic guarantees.

Result: Empirical validation across diverse datasets shows PSCE outperforms state-of-the-art Bayesian CE methods, producing more plausible and discriminative explanations that are provably robust under model changes.

Conclusion: PSCE provides a principled approach to generating counterfactual explanations that maintain reliability and validity even when machine learning models are updated, addressing a critical limitation of existing methods in dynamic real-world applications.

Abstract: Counterfactual explanations (CEs) offer interpretable insights into machine learning predictions by answering ``what if?" questions. However, in real-world settings where models are frequently updated, existing counterfactual explanations can quickly become invalid or unreliable. In this paper, we introduce Probabilistically Safe CEs (PSCE), a method for generating counterfactual explanations that are $δ$-safe, to ensure high predictive confidence, and $ε$-robust to ensure low predictive variance. Based on Bayesian principles, PSCE provides formal probabilistic guarantees for CEs under model changes which are adhered to in what we refer to as the $\langle δ, ε\rangle$-set. Uncertainty-aware constraints are integrated into our optimization framework and we validate our method empirically across diverse datasets. We compare our approach against state-of-the-art Bayesian CE methods, where PSCE produces counterfactual explanations that are not only more plausible and discriminative, but also provably robust under model change.

</details>


### [120] [Dynamic Expert-Guided Model Averaging for Causal Discovery](https://arxiv.org/abs/2601.16715)
*Adrick Tench,Thomas Demeester*

Main category: cs.LG

TL;DR: A flexible ensemble method for causal discovery that dynamically incorporates expert knowledge (including LLMs) to combine multiple algorithms, addressing real-world challenges where standard assumptions are violated.


<details>
  <summary>Details</summary>
Motivation: Causal modeling is crucial for healthcare applications like treatment effect estimation and counterfactual reasoning, but practitioners face too many algorithm choices without clear best options. Real-world scenarios often violate standard causal discovery assumptions, forcing heavy reliance on expert knowledge.

Method: A flexible model averaging approach that ensembles diverse causal discovery algorithms by dynamically requesting expert knowledge. Inspired by recent work on LLMs as experts, the method incorporates imperfect expert knowledge (including from LLMs) to guide the ensemble process.

Result: Experiments show the method's efficacy with imperfect experts like LLMs on both clean and noisy data. The analysis examines the impact of different degrees of expert correctness and assesses LLM capabilities for clinical causal discovery.

Conclusion: The proposed ensemble method with dynamically requested expert knowledge provides a practical solution for causal discovery in healthcare, offering valuable insights for practitioners dealing with real-world data where standard assumptions don't hold.

Abstract: Understanding causal relationships is critical for healthcare. Accurate causal models provide a means to enhance the interpretability of predictive models, and furthermore a basis for counterfactual and interventional reasoning and the estimation of treatment effects. However, would-be practitioners of causal discovery face a dizzying array of algorithms without a clear best choice. This abundance of competitive algorithms makes ensembling a natural choice for practical applications. At the same time, real-world use cases frequently face challenges that violate the assumptions of common causal discovery algorithms, forcing heavy reliance on expert knowledge. Inspired by recent work on dynamically requested expert knowledge and LLMs as experts, we present a flexible model averaging method leveraging dynamically requested expert knowledge to ensemble a diverse array of causal discovery algorithms. Experiments demonstrate the efficacy of our method with imperfect experts such as LLMs on both clean and noisy data. We also analyze the impact of different degrees of expert correctness and assess the capabilities of LLMs for clinical causal discovery, providing valuable insights for practitioners.

</details>


### [121] [Uncertainty propagation through trained multi-layer perceptrons: Exact analytical results](https://arxiv.org/abs/2601.16830)
*Andrew Thompson,Miles McCrory*

Main category: cs.LG

TL;DR: Exact analytical expressions for mean and variance of MLP outputs with ReLU activations under Gaussian input uncertainty


<details>
  <summary>Details</summary>
Motivation: Previous methods for uncertainty propagation through neural networks relied on approximations or series expansions. There's a need for exact analytical expressions to better understand how uncertainty propagates through trained MLPs with ReLU activations.

Method: Develop analytical derivations for multi-layer perceptrons with single hidden layer and ReLU activation functions. Derive exact expressions for output mean and variance when input follows multivariate Gaussian distribution, without using series expansions.

Result: Obtained exact closed-form expressions for both mean and variance of MLP outputs given Gaussian input distributions. These results provide precise uncertainty quantification without approximation errors from series expansions.

Conclusion: The paper provides exact analytical solutions for uncertainty propagation through ReLU MLPs, offering improved accuracy over previous approximation-based methods and enabling better uncertainty quantification in neural network applications.

Abstract: We give analytical results for propagation of uncertainty through trained multi-layer perceptrons (MLPs) with a single hidden layer and ReLU activation functions. More precisely, we give expressions for the mean and variance of the output when the input is multivariate Gaussian. In contrast to previous results, we obtain exact expressions without resort to a series expansion.

</details>


### [122] [Calibrated Probabilistic Interpolation for GEDI Biomass](https://arxiv.org/abs/2601.16834)
*Robin Young,Srinivasan Keshav*

Main category: cs.LG

TL;DR: ANPs (Attentive Neural Processes) outperform traditional ML methods for GEDI biomass mapping by providing calibrated uncertainty estimates that adapt to landscape heterogeneity through spatial conditioning and meta-learning.


<details>
  <summary>Details</summary>
Motivation: Standard ML methods (Random Forest, XGBoost) for GEDI biomass mapping fail to produce calibrated prediction intervals because they treat predictions independently, conflate ensemble variance with aleatoric uncertainty, and ignore local spatial context in heterogeneous landscapes.

Method: Introduces Attentive Neural Processes (ANPs), a probabilistic meta-learning framework that explicitly conditions predictions on local observation sets and geospatial foundation model embeddings. ANPs learn a flexible spatial covariance function that adapts uncertainty estimates to landscape complexity.

Result: ANPs achieve competitive accuracy while maintaining near-ideal uncertainty calibration across five distinct biomes (Tropical Amazonian forests to Boreal/Alpine ecosystems). The method demonstrates operational utility through few-shot adaptation, recovering most performance gap in cross-region transfer with minimal local data.

Conclusion: ANPs provide a scalable, theoretically rigorous alternative to ensemble variance for continental-scale earth observation, addressing the critical need for reliable uncertainty quantification in heterogeneous landscape biomass mapping.

Abstract: Reliable wall-to-wall biomass mapping from NASA's GEDI mission requires interpolating sparse LiDAR observations across heterogeneous landscapes. While machine learning approaches like Random Forest and XGBoost are standard for this task, they treat spatial predictions of GEDI observations from multispectral or SAR remote sensing data as independent without adapting to the varying difficulty of heterogeneous landscapes. We demonstrate these approaches generally fail to produce calibrated prediction intervals. We identify that this stems from conflating ensemble variance with aleatoric uncertainty and ignoring local spatial context.
  To resolve this, we introduce Attentive Neural Processes (ANPs), a probabilistic meta-learning framework that explicitly conditions predictions on local observation sets and geospatial foundation model embeddings. Unlike static ensembles, ANPs learn a flexible spatial covariance function, allowing uncertainty estimates to expand in complex landscapes and contract in homogeneous areas. We validate this approach across five distinct biomes ranging from Tropical Amazonian forests to Boreal and Alpine ecosystems, demonstrating that ANPs achieve competitive accuracy while maintaining near-ideal uncertainty calibration. We demonstrate the operational utility of the method through few-shot adaptation, where the model recovers most of the performance gap in cross-region transfer using minimal local data. This work provides a scalable, theoretically rigorous alternative to ensemble variance for continental scale earth observation.

</details>


### [123] [The Art of Being Difficult: Combining Human and AI Strengths to Find Adversarial Instances for Heuristics](https://arxiv.org/abs/2601.16849)
*Henri Nikoleit,Ankit Anand,Anurag Murty Naredla,Heiko Röglin*

Main category: cs.LG

TL;DR: Human-LLM collaboration refines FunSearch outputs to generate adversarial instances, achieving state-of-the-art lower bounds for combinatorial optimization problems including hierarchical k-median, bin packing, knapsack, and Lovász's gasoline problem, breaking decade-old barriers.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the power of human-LLM collaboration in tackling open problems in theoretical computer science, specifically to improve upon existing results in combinatorial optimization where some problems haven't seen significant improvement for over a decade despite intermittent attention.

Method: Refine outputs from the FunSearch algorithm through iterative human-LLM collaboration, focusing on generating adversarial instances where standard heuristics perform poorly. Expert oversight extrapolates algorithmic insights from LLM-based evolutionary methods.

Result: Achieved state-of-the-art lower bounds for hierarchical k-median clustering, bin packing, the knapsack problem, and a generalization of Lovász's gasoline problem, breaking long-standing barriers in these combinatorial optimization problems.

Conclusion: LLMs provide critical initial patterns, but human expertise is essential for transforming these patterns into mathematically rigorous and insightful constructions. LLMs serve as strong collaborative tools in mathematics and computer science research when combined with expert oversight.

Abstract: We demonstrate the power of human-LLM collaboration in tackling open problems in theoretical computer science. Focusing on combinatorial optimization, we refine outputs from the FunSearch algorithm [Romera-Paredes et al., Nature 2023] to derive state-of-the-art lower bounds for standard heuristics. Specifically, we target the generation of adversarial instances where these heuristics perform poorly. By iterating on FunSearch's outputs, we identify improved constructions for hierarchical $k$-median clustering, bin packing, the knapsack problem, and a generalization of Lovász's gasoline problem - some of these have not seen much improvement for over a decade, despite intermittent attention. These results illustrate how expert oversight can effectively extrapolate algorithmic insights from LLM-based evolutionary methods to break long-standing barriers.
  Our findings demonstrate that while LLMs provide critical initial patterns, human expertise is essential for transforming these patterns into mathematically rigorous and insightful constructions. This work highlights that LLMs are a strong collaborative tool in mathematics and computer science research.

</details>


### [124] [Provably Learning Attention with Queries](https://arxiv.org/abs/2601.16873)
*Satwik Bhattamishra,Kulin Shah,Michael Hahn,Varun Kanade*

Main category: cs.LG

TL;DR: The paper studies learning Transformer parameters from black-box output queries, showing efficient algorithms for single-head attention but identifiability issues for multi-head attention.


<details>
  <summary>Details</summary>
Motivation: To understand how Transformer-based sequence models can be learned from black-box access to their outputs, which has implications for model extraction, reverse engineering, and understanding model representations.

Method: Develop query-based learning algorithms: 1) elementary O(d²) algorithm for single-head attention, 2) extension to one-layer Transformers using ReLU FFN learning, 3) randomized O(rd) algorithm via compressed sensing for low head dimension, 4) analysis of noisy oracle access, and 5) identifiability analysis for multi-head attention.

Result: Single-head attention parameters can be learned exactly with O(d²) queries; with low head dimension r≪d, O(rd) queries suffice via compressed sensing; parameters can be estimated under noise with polynomial queries; multi-head attention parameters are not generally identifiable from value queries.

Conclusion: Single-head attention models are efficiently learnable from black-box queries, but multi-head attention lacks identifiability without additional structural assumptions, revealing fundamental differences in their learnability from output observations.

Abstract: We study the problem of learning Transformer-based sequence models with black-box access to their outputs. In this setting, a learner may adaptively query the oracle with any sequence of vectors and observe the corresponding real-valued output. We begin with the simplest case, a single-head softmax-attention regressor. We show that for a model with width $d$, there is an elementary algorithm to learn the parameters of single-head attention exactly with $O(d^2)$ queries. Further, we show that if there exists an algorithm to learn ReLU feedforward networks (FFNs), then the single-head algorithm can be easily adapted to learn one-layer Transformers with single-head attention. Next, motivated by the regime where the head dimension $r \ll d$, we provide a randomised algorithm that learns single-head attention-based models with $O(rd)$ queries via compressed sensing arguments. We also study robustness to noisy oracle access, proving that under mild norm and margin conditions, the parameters can be estimated to $\varepsilon$ accuracy with a polynomial number of queries even when outputs are only provided up to additive tolerance. Finally, we show that multi-head attention parameters are not identifiable from value queries in general -- distinct parameterisations can induce the same input-output map. Hence, guarantees analogous to the single-head setting are impossible without additional structural assumptions.

</details>


### [125] [Multigrade Neural Network Approximation](https://arxiv.org/abs/2601.16884)
*Shijun Zhang,Zuowei Shen,Yuesheng Xu*

Main category: cs.LG

TL;DR: MGDL trains deep networks grade-by-grade, freezing previous layers and training new residual blocks to reduce approximation error, providing theoretical guarantees for vanishing error.


<details>
  <summary>Details</summary>
Motivation: Training very deep neural networks is challenging due to non-convex, ill-conditioned optimization landscapes, while shallow networks (especially one-hidden-layer ReLU models) have convex reformulations with global guarantees. This motivates a structured approach to scale depth while maintaining stability.

Method: Multigrade Deep Learning (MGDL) trains deep networks grade by grade: previously learned grades are frozen, and each new residual block is trained solely to reduce the remaining approximation error. This creates an interpretable hierarchical refinement process with operator-theoretic foundations.

Result: Theoretical proof that for any continuous target function, there exists a fixed-width multigrade ReLU scheme whose residuals decrease strictly across grades and converge uniformly to zero. This is the first rigorous guarantee that grade-wise training yields provable vanishing approximation error in deep networks.

Conclusion: MGDL provides a principled framework for structured error refinement in deep networks with theoretical convergence guarantees, addressing optimization challenges in deep learning while maintaining interpretability and stability through hierarchical training.

Abstract: We study multigrade deep learning (MGDL) as a principled framework for structured error refinement in deep neural networks. While the approximation power of neural networks is now relatively well understood, training very deep architectures remains challenging due to highly non-convex and often ill-conditioned optimization landscapes. In contrast, for relatively shallow networks, most notably one-hidden-layer $\texttt{ReLU}$ models, training admits convex reformulations with global guarantees, motivating learning paradigms that improve stability while scaling to depth. MGDL builds upon this insight by training deep networks grade by grade: previously learned grades are frozen, and each new residual block is trained solely to reduce the remaining approximation error, yielding an interpretable and stable hierarchical refinement process. We develop an operator-theoretic foundation for MGDL and prove that, for any continuous target function, there exists a fixed-width multigrade $\texttt{ReLU}$ scheme whose residuals decrease strictly across grades and converge uniformly to zero. To the best of our knowledge, this work provides the first rigorous theoretical guarantee that grade-wise training yields provable vanishing approximation error in deep networks. Numerical experiments further illustrate the theoretical results.

</details>


### [126] [FedSGM: A Unified Framework for Constraint Aware, Bidirectionally Compressed, Multi-Step Federated Optimization](https://arxiv.org/abs/2601.16897)
*Antesh Upadhyay,Sang Bin Moon,Abolfazl Hashemi*

Main category: cs.LG

TL;DR: FedSGM is a unified federated constrained optimization framework that handles functional constraints, communication bottlenecks, local updates, and partial client participation with projection-free primal-only updates and bi-directional error feedback.


<details>
  <summary>Details</summary>
Motivation: Address four major challenges in federated learning: functional constraints, communication bottlenecks, local updates, and partial client participation. Existing methods lack unified solutions for all these challenges simultaneously.

Method: Based on switching gradient method with projection-free primal-only updates. Incorporates bi-directional error feedback to handle compression bias. Introduces soft switching version for stability near feasibility boundaries.

Result: Achieves canonical O(1/√T) convergence rate with high-probability bounds that decouple optimization progress from sampling noise. Validated on Neyman-Pearson classification and constrained Markov decision process tasks.

Conclusion: FedSGM is the first unified framework for constrained federated learning that simultaneously addresses functional constraints, compression, multiple local updates, and partial client participation with theoretical guarantees.

Abstract: We introduce FedSGM, a unified framework for federated constrained optimization that addresses four major challenges in federated learning (FL): functional constraints, communication bottlenecks, local updates, and partial client participation. Building on the switching gradient method, FedSGM provides projection-free, primal-only updates, avoiding expensive dual-variable tuning or inner solvers. To handle communication limits, FedSGM incorporates bi-directional error feedback, correcting the bias introduced by compression while explicitly understanding the interaction between compression noise and multi-step local updates. We derive convergence guarantees showing that the averaged iterate achieves the canonical $\boldsymbol{\mathcal{O}}(1/\sqrt{T})$ rate, with additional high-probability bounds that decouple optimization progress from sampling noise due to partial participation. Additionally, we introduce a soft switching version of FedSGM to stabilize updates near the feasibility boundary. To our knowledge, FedSGM is the first framework to unify functional constraints, compression, multiple local updates, and partial client participation, establishing a theoretically grounded foundation for constrained federated learning. Finally, we validate the theoretical guarantees of FedSGM via experimentation on Neyman-Pearson classification and constrained Markov decision process (CMDP) tasks.

</details>


### [127] [Embedding -based Crop Type Classification in the Groundnut Basin of Senegal](https://arxiv.org/abs/2601.16900)
*Madeline C. Lisaius,Srinivasan Keshav,Andrew Blake,Clement Atzberger*

Main category: cs.LG

TL;DR: Geospatial foundation model embeddings, particularly TESSERA, outperform baseline methods for crop type mapping in smallholder regions like Senegal's groundnut basin, meeting key criteria for practical application.


<details>
  <summary>Details</summary>
Motivation: Current satellite-based crop mapping methods are not well-suited for smallholder farming conditions, creating a gap in tools needed for food security, livelihood support, and climate change mitigation in these regions.

Method: Established a four-part criteria (performance, plausibility, transferability, accessibility) to evaluate geospatial foundation model embeddings (TESSERA and AlphaEarth) against baseline methods for crop type mapping in Senegal's groundnut basin.

Result: TESSERA-based approach best fulfills all selection criteria, showing 28% higher accuracy than the next best method in one temporal transfer example, making it effective for crop classification in Senegal.

Conclusion: TESSERA embeddings represent an effective approach for crop type classification and mapping in smallholder regions like Senegal, addressing the gap in suitable satellite-based methods for these agricultural systems.

Abstract: Crop type maps from satellite remote sensing are important tools for food security, local livelihood support and climate change mitigation in smallholder regions of the world, but most satellite-based methods are not well suited to smallholder conditions. To address this gap, we establish a four-part criteria for a useful embedding-based approach consisting of 1) performance, 2) plausibility, 3) transferability and 4) accessibility and evaluate geospatial foundation model (FM) embeddings -based approaches using TESSERA and AlphaEarth against current baseline methods for a region in the groundnut basin of Senegal. We find that the TESSERA -based approach to land cover and crop type mapping fulfills the selection criteria best, and in one temporal transfer example shows 28% higher accuracy compared to the next best method. These results indicate that TESSERA embeddings are an effective approach for crop type classification and mapping tasks in Senegal.

</details>


### [128] [GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints](https://arxiv.org/abs/2601.16905)
*Andy Zhu,Rongzhe Wei,Yupu Gu,Pan Li*

Main category: cs.LG

TL;DR: GRIP is a framework that prevents machine unlearning methods from exploiting MoE router vulnerabilities by constraining router updates to preserve routing stability while allowing necessary parameter changes.


<details>
  <summary>Details</summary>
Motivation: Existing machine unlearning methods fail for Mixture-of-Experts (MoE) architectures because they exploit router vulnerabilities - manipulating routers to redirect queries away from knowledgeable experts rather than actually erasing knowledge, causing utility loss and superficial forgetting.

Method: Geometric Routing Invariance Preservation (GRIP) uses a geometric constraint that projects router gradient updates into expert-specific null-spaces. This decouples routing stability from parameter rigidity: discrete expert selections remain stable while continuous router parameters can reconfigure within the null space to satisfy unlearning objectives.

Result: GRIP eliminates expert selection shift (achieving over 95% routing stability) across all tested unlearning methods while preserving model utility. It prevents algorithms from exploiting MoE router vulnerabilities and adapts existing unlearning research to MoE architectures.

Conclusion: GRIP provides an algorithm-agnostic framework that forces unlearning optimization to erase knowledge directly from expert parameters rather than exploiting router manipulation shortcuts, enabling effective machine unlearning for MoE models.

Abstract: Machine unlearning (MU) for large language models has become critical for AI safety, yet existing methods fail to generalize to Mixture-of-Experts (MoE) architectures. We identify that traditional unlearning methods exploit MoE's architectural vulnerability: they manipulate routers to redirect queries away from knowledgeable experts rather than erasing knowledge, causing a loss of model utility and superficial forgetting. We propose Geometric Routing Invariance Preservation (GRIP), an algorithm-agnostic framework for unlearning for MoE. Our core contribution is a geometric constraint, implemented by projecting router gradient updates into an expert-specific null-space. Crucially, this decouples routing stability from parameter rigidity: while discrete expert selections remain stable for retained knowledge, the continuous router parameters remain plastic within the null space, allowing the model to undergo necessary internal reconfiguration to satisfy unlearning objectives. This forces the unlearning optimization to erase knowledge directly from expert parameters rather than exploiting the superficial router manipulation shortcut. GRIP functions as an adapter, constraining router parameter updates without modifying the underlying unlearning algorithm. Extensive experiments on large-scale MoE models demonstrate that our adapter eliminates expert selection shift (achieving over 95% routing stability) across all tested unlearning methods while preserving their utility. By preventing existing algorithms from exploiting MoE model's router vulnerability, GRIP adapts existing unlearning research from dense architectures to MoEs.

</details>


### [129] [The Trajectory Alignment Coefficient in Two Acts: From Reward Tuning to Reward Learning](https://arxiv.org/abs/2601.16906)
*Calarina Muslimani,Yunshu Du,Kenta Kawamoto,Kaushik Subramanian,Peter Stone,Peter Wurman*

Main category: cs.LG

TL;DR: TAC (Trajectory Alignment Coefficient) helps RL practitioners design better reward functions and serves as a differentiable objective (Soft-TAC) for learning reward models from human preferences.


<details>
  <summary>Details</summary>
Motivation: Reward function design in RL is time-consuming and prone to misspecification. The paper aims to support practitioners in reward tuning and automate reward learning from human preferences.

Method: 1) Human study where RL practitioners tuned reward weights for Lunar Lander with/without TAC feedback. 2) Proposed Soft-TAC, a differentiable approximation of TAC used as loss function to train reward models from human preference data.

Result: With TAC, participants produced more performant reward functions and reported lower cognitive workload. Soft-TAC trained reward models captured preference-specific objectives better than Cross-Entropy loss, producing policies with more distinct behaviors in Gran Turismo 7.

Conclusion: TAC serves as both a practical tool for guiding reward tuning and an effective reward learning objective for complex domains, addressing challenges in reward function design.

Abstract: The success of reinforcement learning (RL) is fundamentally tied to having a reward function that accurately reflects the task objective. Yet, designing reward functions is notoriously time-consuming and prone to misspecification. To address this issue, our first goal is to understand how to support RL practitioners in specifying appropriate weights for a reward function. We leverage the Trajectory Alignment Coefficient (TAC), a metric that evaluates how closely a reward function's induced preferences match those of a domain expert. To evaluate whether TAC provides effective support in practice, we conducted a human-subject study in which RL practitioners tuned reward weights for Lunar Lander. We found that providing TAC during reward tuning led participants to produce more performant reward functions and report lower cognitive workload relative to standard tuning without TAC. However, the study also underscored that manual reward design, even with TAC, remains labor-intensive. This limitation motivated our second goal: to learn a reward model that maximizes TAC directly. Specifically, we propose Soft-TAC, a differentiable approximation of TAC that can be used as a loss function to train reward models from human preference data. Validated in the racing simulator Gran Turismo 7, reward models trained using Soft-TAC successfully captured preference-specific objectives, resulting in policies with qualitatively more distinct behaviors than models trained with standard Cross-Entropy loss. This work demonstrates that TAC can serve as both a practical tool for guiding reward tuning and a reward learning objective in complex domains.

</details>


### [130] [Calibrated Similarity for Reliable Geometric Analysis of Embedding Spaces](https://arxiv.org/abs/2601.16907)
*Nicolas Tacheny*

Main category: cs.LG

TL;DR: The paper proposes isotonic regression calibration to fix cosine similarity's miscalibration while preserving rank correlation, without altering embeddings.


<details>
  <summary>Details</summary>
Motivation: Cosine similarity in pretrained embeddings has strong rank correlation but suffers from anisotropy-induced miscalibration - scores concentrate in a narrow high-similarity band regardless of actual semantic relatedness, limiting interpretability as a quantitative measure.

Method: Use isotonic regression trained on human similarity judgments to construct a monotonic transformation that calibrates cosine similarity while preserving rank correlation and local stability (98% across seven perturbation types).

Result: Achieves near-perfect calibration while preserving rank correlation and local stability. Proves that all order-based constructions (angular ordering, nearest neighbors, threshold graphs, quantile-based decisions) are invariant under this monotone transformation.

Conclusion: Isotonic calibration restores interpretability of cosine similarity's absolute values without altering its ranking properties or requiring recomputation of embeddings, unlike prior space-modification approaches.

Abstract: While raw cosine similarity in pretrained embedding spaces exhibits strong rank correlation with human judgments, anisotropy induces systematic miscalibration of absolute values: scores concentrate in a narrow high-similarity band regardless of actual semantic relatedness, limiting interpretability as a quantitative measure. Prior work addresses this by modifying the embedding space (whitening, contrastive fine tuning), but such transformations alter geometric structure and require recomputing all embeddings.
  Using isotonic regression trained on human similarity judgments, we construct a monotonic transformation that achieves near-perfect calibration while preserving rank correlation and local stability(98% across seven perturbation types). Our contribution is not to replace cosine similarity, but to restore interpretability of its absolute values through monotone calibration, without altering its ranking properties.
  We characterize isotonic calibration as an order-preserving reparameterization and prove that all order-based constructions (angular ordering, nearest neighbors, threshold graphs and quantile-based decisions) are invariant under this transformation.

</details>


### [131] [Group-realizable multi-group learning by minimizing empirical risk](https://arxiv.org/abs/2601.16922)
*Navid Ardeshir,Samuel Deng,Daniel Hsu,Jingwen Liu*

Main category: cs.LG

TL;DR: Multi-group learning has better sample complexity in group-realizable setting vs agnostic setting, even with infinite groups of finite VC dimension, but ERM implementation is computationally intractable, suggesting improper learning as alternative.


<details>
  <summary>Details</summary>
Motivation: To understand and improve the sample complexity of multi-group learning, particularly comparing the group-realizable setting (where data satisfies group structure) with the agnostic setting (no assumptions), even when dealing with infinite families of groups.

Method: Empirical risk minimization over group-realizable concepts, analysis of sample complexity improvements, computational complexity analysis of ERM implementation, and proposal of improper learning as alternative approach.

Result: Sample complexity improves in group-realizable setting over agnostic setting, even with infinite groups having finite VC dimension. However, implementing ERM over group-realizable concepts is computationally intractable despite the theoretical sample complexity benefits.

Conclusion: While group-realizable setting offers theoretical sample complexity advantages over agnostic setting, practical implementation via ERM is computationally infeasible, necessitating alternative approaches like improper learning for practical multi-group learning systems.

Abstract: The sample complexity of multi-group learning is shown to improve in the group-realizable setting over the agnostic setting, even when the family of groups is infinite so long as it has finite VC dimension. The improved sample complexity is obtained by empirical risk minimization over the class of group-realizable concepts, which itself could have infinite VC dimension. Implementing this approach is also shown to be computationally intractable, and an alternative approach is suggested based on improper learning.

</details>


### [132] [Is BatchEnsemble a Single Model? On Calibration and Diversity of Efficient Ensembles](https://arxiv.org/abs/2601.16936)
*Anton Zamyatin,Patrick Indri,Sagar Malhotra,Thomas Gärtner*

Main category: cs.LG

TL;DR: BatchEnsemble fails to provide meaningful epistemic uncertainty like Deep Ensembles, performing similarly to a single model baseline across multiple metrics.


<details>
  <summary>Details</summary>
Motivation: Need efficient uncertainty estimation for resource-constrained, low-latency applications where Deep Ensembles are too expensive.

Method: BatchEnsemble uses learned rank-1 perturbations to a shared base network to reduce parameter and memory costs compared to training multiple full models.

Result: BatchEnsemble underperforms Deep Ensembles and closely tracks single model performance in accuracy, calibration, and OOD detection on CIFAR10/10C/SVHN. On MNIST, ensemble members are nearly identical, showing limited capacity for distinct predictive modes.

Conclusion: BatchEnsemble behaves more like a single model than a true ensemble, failing to provide robust epistemic uncertainty despite its efficiency claims.

Abstract: In resource-constrained and low-latency settings, uncertainty estimates must be efficiently obtained. Deep Ensembles provide robust epistemic uncertainty (EU) but require training multiple full-size models. BatchEnsemble aims to deliver ensemble-like EU at far lower parameter and memory cost by applying learned rank-1 perturbations to a shared base network. We show that BatchEnsemble not only underperforms Deep Ensembles but closely tracks a single model baseline in terms of accuracy, calibration and out-of-distribution (OOD) detection on CIFAR10/10C/SVHN. A controlled study on MNIST finds members are near-identical in function and parameter space, indicating limited capacity to realize distinct predictive modes. Thus, BatchEnsemble behaves more like a single model than a true ensemble.

</details>


### [133] [3D Molecule Generation from Rigid Motifs via SE(3) Flows](https://arxiv.org/abs/2601.16955)
*Roman Poletukhin,Marcel Kollovieh,Eike Eberhard,Stephan Günnemann*

Main category: cs.LG

TL;DR: SE(3)-equivariant generative modeling for 3D molecule generation using rigid-body motifs instead of individual atoms, achieving faster generation and more compact representations.


<details>
  <summary>Details</summary>
Motivation: Current 3D molecular structure generation works at the atomic level, while molecular graph generation often uses fragments. The paper aims to bridge this gap by extending fragmentation ideas to 3D space, treating molecules as sets of rigid-body motifs for more efficient generation.

Method: Extends frame-based protein structure generation techniques to general molecules, representing them as sets of rigid-body motifs. Uses SE(3)-equivariant generative modeling for de novo 3D molecule generation from these rigid motifs.

Result: Achieves comparable or superior results to state-of-the-art methods across benchmarks, with better atom stability on GEOM-Drugs. Offers 2x to 10x reduction in generation steps and 3.5x compression in molecular representations compared to standard atom-based methods.

Conclusion: Treating molecules as rigid-body motifs enables more efficient 3D molecular generation with fewer steps and more compact representations while maintaining or improving quality compared to atom-based approaches.

Abstract: Three-dimensional molecular structure generation is typically performed at the level of individual atoms, yet molecular graph generation techniques often consider fragments as their structural units. Building on the advances in frame-based protein structure generation, we extend these fragmentation ideas to 3D, treating general molecules as sets of rigid-body motifs. Utilising this representation, we employ SE(3)-equivariant generative modelling for de novo 3D molecule generation from rigid motifs. In our evaluations, we observe comparable or superior results to state-of-the-art across benchmarks, surpassing it in atom stability on GEOM-Drugs, while yielding a 2x to 10x reduction in generation steps and offering 3.5x compression in molecular representations compared to the standard atom-based methods.

</details>


### [134] [Auto-Regressive Masked Diffusion Models](https://arxiv.org/abs/2601.16971)
*Mahdi Karami,Ali Ghodsi*

Main category: cs.LG

TL;DR: ARMD is a new architecture that combines autoregressive training efficiency with diffusion parallel generation, achieving SOTA performance on language modeling benchmarks with faster inference.


<details>
  <summary>Details</summary>
Motivation: Masked diffusion models have performance gaps compared to autoregressive models and require more training iterations. The authors want to unify the strengths of both approaches.

Method: Reframes masked diffusion as block-wise causal model, creates strictly causal permutation-equivariant architecture that computes all conditional probabilities in parallel, supports progressive permutation training, and introduces strided parallel generation strategy.

Result: Achieves state-of-the-art performance on standard language modeling benchmarks, outperforms diffusion baselines with fewer training steps, and sets new benchmark for parallel text generation.

Conclusion: ARMD effectively bridges the performance gap between parallel and sequential decoding, combining the best of both autoregressive and diffusion approaches.

Abstract: Masked diffusion models (MDMs) have emerged as a promising approach for language modeling, yet they face a performance gap compared to autoregressive models (ARMs) and require more training iterations. In this work, we present the Auto-Regressive Masked Diffusion (ARMD) model, an architecture designed to close this gap by unifying the training efficiency of autoregressive models with the parallel generation capabilities of diffusion-based models. Our key insight is to reframe the masked diffusion process as a block-wise causal model. This perspective allows us to design a strictly causal, permutation-equivariant architecture that computes all conditional probabilities across multiple denoising steps in a single, parallel forward pass. The resulting architecture supports efficient, autoregressive-style decoding and a progressive permutation training scheme, allowing the model to learn both canonical left-to-right and random token orderings. Leveraging this flexibility, we introduce a novel strided parallel generation strategy that accelerates inference by generating tokens in parallel streams while maintaining global coherence. Empirical results demonstrate that ARMD achieves state-of-the-art performance on standard language modeling benchmarks, outperforming established diffusion baselines while requiring significantly fewer training steps. Furthermore, it establishes a new benchmark for parallel text generation, effectively bridging the performance gap between parallel and sequential decoding.

</details>


### [135] [Latent Diffusion for Internet of Things Attack Data Generation in Intrusion Detection](https://arxiv.org/abs/2601.16976)
*Estela Sánchez-Carballo,Francisco M. Melgarejo-Meseguer,José Luis Rojo-Álvarez*

Main category: cs.LG

TL;DR: The paper proposes using Latent Diffusion Models (LDMs) for attack data augmentation in IoT intrusion detection to address class imbalance, showing superior performance over existing methods with high F1-scores and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: ML-based IDSs for IoT suffer from performance degradation due to strong class imbalance between benign and attack traffic. Existing data augmentation approaches struggle with balancing sample fidelity, diversity, and computational efficiency simultaneously.

Method: Proposes using Latent Diffusion Models (LDM) for attack data augmentation in IoT intrusion detection. Conducts comprehensive comparison against state-of-the-art baselines on three IoT attack types (DDoS, Mirai, Man-in-the-Middle), evaluating both downstream IDS performance and intrinsic generative quality using distributional, dependency-based, and diversity metrics.

Result: Balancing training data with LDM-generated samples substantially improves IDS performance, achieving F1-scores up to 0.99 for DDoS and Mirai attacks, consistently outperforming competing methods. LDMs effectively preserve feature dependencies while generating diverse samples and reduce sampling time by approximately 25% compared to diffusion models operating directly in data space.

Conclusion: Latent diffusion models provide an effective and scalable solution for synthetic IoT attack data generation, substantially mitigating the impact of class imbalance in ML-based IDSs for IoT scenarios.

Abstract: Intrusion Detection Systems (IDSs) are a key component for protecting Internet of Things (IoT) environments. However, in Machine Learning-based (ML-based) IDSs, performance is often degraded by the strong class imbalance between benign and attack traffic. Although data augmentation has been widely explored to mitigate this issue, existing approaches typically rely on simple oversampling techniques or generative models that struggle to simultaneously achieve high sample fidelity, diversity, and computational efficiency. To address these limitations, we propose the use of a Latent Diffusion Model (LDM) for attack data augmentation in IoT intrusion detection and provide a comprehensive comparison against state-of-the-art baselines. Experiments were conducted on three representative IoT attack types, specifically Distributed Denial-of-Service (DDoS), Mirai, and Man-in-the-Middle, evaluating both downstream IDS performance and intrinsic generative quality using distributional, dependency-based, and diversity metrics. Results show that balancing the training data with LDM-generated samples substantially improves IDS performance, achieving F1-scores of up to 0.99 for DDoS and Mirai attacks and consistently outperforming competing methods. Additionally, quantitative and qualitative analyses demonstrate that LDMs effectively preserve feature dependencies while generating diverse samples and reduce sampling time by approximately 25\% compared to diffusion models operating directly in data space. These findings highlight latent diffusion as an effective and scalable solution for synthetic IoT attack data generation, substantially mitigating the impact of class imbalance in ML-based IDSs for IoT scenarios.

</details>


### [136] [A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs](https://arxiv.org/abs/2601.16979)
*Dayal Singh Kalra,Jean-Christophe Gagnon-Audet,Andrey Gromov,Ishita Mediratta,Kelvin Niu,Alexander H Miller,Michael Shvartsman*

Main category: cs.LG

TL;DR: Critical sharpness (λ_c) is a computationally efficient alternative to Hessian sharpness that captures curvature dynamics in LLMs up to 7B parameters, enabling analysis of progressive sharpening and Edge of Stability phenomena.


<details>
  <summary>Details</summary>
Motivation: Direct measurement of Hessian sharpness (λ_max^H) is computationally prohibitive for Large Language Models, limiting analysis of curvature evolution and training dynamics despite its fundamental importance for understanding stability and learning rate interactions.

Method: Introduces critical sharpness (λ_c), a computationally efficient measure requiring fewer than 10 forward passes given the update direction Δθ. Also introduces relative critical sharpness (λ_c^{1→2}) to quantify curvature of one loss landscape while optimizing another, enabling analysis of transitions between training phases.

Result: First demonstration of Hessian sharpness phenomena (progressive sharpening and Edge of Stability) at scale up to 7B parameters across pre-training and mid-training of OLMo-2 models. Shows critical sharpness captures well-documented curvature dynamics while being computationally practical.

Conclusion: Critical sharpness provides a practical tool for diagnosing curvature dynamics and informing data composition choices at scale, demonstrating that scalable curvature measures can offer actionable insights for large-scale training of neural networks.

Abstract: Understanding the curvature evolution of the loss landscape is fundamental to analyzing the training dynamics of neural networks. The most commonly studied measure, Hessian sharpness ($λ_{\max}^H$) -- the largest eigenvalue of the loss Hessian -- determines local training stability and interacts with the learning rate throughout training. Despite its significance in analyzing training dynamics, direct measurement of Hessian sharpness remains prohibitive for Large Language Models (LLMs) due to high computational cost. We analyze $\textit{critical sharpness}$ ($λ_c$), a computationally efficient measure requiring fewer than $10$ forward passes given the update direction $Δ\mathbfθ$. Critically, this measure captures well-documented Hessian sharpness phenomena, including progressive sharpening and Edge of Stability. Using this measure, we provide the first demonstration of these sharpness phenomena at scale, up to $7$B parameters, spanning both pre-training and mid-training of OLMo-2 models. We further introduce $\textit{relative critical sharpness}$ ($λ_c^{1\to 2}$), which quantifies the curvature of one loss landscape while optimizing another, to analyze the transition from pre-training to fine-tuning and guide data mixing strategies. Critical sharpness provides practitioners with a practical tool for diagnosing curvature dynamics and informing data composition choices at scale. More broadly, our work shows that scalable curvature measures can provide actionable insights for large-scale training.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [137] [Experience with Single Domain Generalization in Real World Medical Imaging Deployments](https://arxiv.org/abs/2601.16359)
*Ayan Banerjee,Komandoor Srivathsan,Sandeep K. S. Gupta*

Main category: eess.IV

TL;DR: The paper presents DL+EKE, a novel expert knowledge integrated deep learning technique for Single Domain Generalization in medical imaging, addressing limitations of existing SDG methods and demonstrating effectiveness across multiple real-world applications including diabetic retinopathy, stress ECG, and fMRI.


<details>
  <summary>Details</summary>
Motivation: Single Domain Generalization (SDG) is crucial for medical AI deployment but current state-of-the-art methods fail in real-world scenarios due to domain shifts from different scanners and imaging protocols, especially for rare class characteristics in multi-center studies.

Method: Developed DL+EKE (Deep Learning + Expert Knowledge Enhancement), a generic framework integrating expert knowledge into deep learning. First validated on diabetic retinopathy, then deployed on two real-world medical imaging applications: seizure onset zone detection using fMRI and stress ECG-based coronary artery detection.

Result: DL+EKE outperformed state-of-the-art SDG methods on diabetic retinopathy. Successfully deployed on stress ECG and resting-state fMRI applications, addressing domain shift challenges that existing SDG techniques struggled with in real-world deployment scenarios.

Conclusion: Expert knowledge integration (DL+EKE) is essential for effective Single Domain Generalization in medical imaging, overcoming limitations of pure data-driven approaches and enabling robust deployment across diverse clinical domains with varying acquisition conditions.

Abstract: A desirable property of any deployed artificial intelligence is generalization across domains, i.e. data generation distribution under a specific acquisition condition. In medical imagining applications the most coveted property for effective deployment is Single Domain Generalization (SDG), which addresses the challenge of training a model on a single domain to ensure it generalizes well to unseen target domains. In multi-center studies, differences in scanners and imaging protocols introduce domain shifts that exacerbate variability in rare class characteristics. This paper presents our experience on SDG in real life deployment for two exemplary medical imaging case studies on seizure onset zone detection using fMRI data, and stress electrocardiogram based coronary artery detection. Utilizing the commonly used application of diabetic retinopathy, we first demonstrate that state-of-the-art SDG techniques fail to achieve generalized performance across data domains. We then develop a generic expert knowledge integrated deep learning technique DL+EKE and instantiate it for the DR application and show that DL+EKE outperforms SOTA SDG methods on DR. We then deploy instances of DL+EKE technique on the two real world examples of stress ECG and resting state (rs)-fMRI and discuss issues faced with SDG techniques.

</details>


### [138] [On The Robustness of Foundational 3D Medical Image Segmentation Models Against Imprecise Visual Prompts](https://arxiv.org/abs/2601.16383)
*Soumitri Chattopadhyay,Basar Demir,Marc Niethammer*

Main category: eess.IV

TL;DR: The paper systematically studies the robustness of 3D medical segmentation foundational models to imprecise visual prompts, revealing their reliance on shape/spatial cues and resilience patterns.


<details>
  <summary>Details</summary>
Motivation: While 3D foundational models show promise for promptable medical segmentation, their robustness to imprecise prompts (common in real-world scenarios) remains under-explored, creating a gap in understanding their practical reliability.

Method: The authors systematically study controlled perturbations of dense visual prompts that mimic real-world imprecision. They conduct experiments with two recent foundational models on multi-organ abdominal segmentation tasks to analyze prompt robustness.

Result: The study reveals several facets of promptable medical segmentation: models' reliance on visual shape and spatial cues, and the extent of their resilience to certain types of perturbations. The analysis provides insights into how these models handle imprecise prompts.

Conclusion: The work addresses the important gap in understanding prompt robustness for medical segmentation foundational models, providing systematic analysis that can guide future improvements in making these models more reliable for real-world clinical applications where precise prompts may not always be available.

Abstract: While 3D foundational models have shown promise for promptable segmentation of medical volumes, their robustness to imprecise prompts remains under-explored. In this work, we aim to address this gap by systematically studying the effect of various controlled perturbations of dense visual prompts, that closely mimic real-world imprecision. By conducting experiments with two recent foundational models on a multi-organ abdominal segmentation task, we reveal several facets of promptable medical segmentation, especially pertaining to reliance on visual shape and spatial cues, and the extent of resilience of models towards certain perturbations. Codes are available at: https://github.com/ucsdbiag/Prompt-Robustness-MedSegFMs

</details>


### [139] [Unsupervised Super-Resolution of Hyperspectral Remote Sensing Images Using Fully Synthetic Training](https://arxiv.org/abs/2601.16602)
*Xinxin Xu,Yann Gousseau,Christophe Kervazo,Saïd Ladjal*

Main category: eess.IV

TL;DR: Unsupervised hyperspectral image super-resolution using synthetic abundance data generated via dead leaves model to mimic real statistics, avoiding need for ground truth training data.


<details>
  <summary>Details</summary>
Motivation: Most hyperspectral super-resolution methods are supervised and require ground truth data for training, which is often unavailable. Need for unsupervised approach that doesn't rely on paired training data.

Method: 1) Decompose hyperspectral image into abundances and endmembers via unmixing. 2) Train abundance super-resolution neural network using synthetic abundances generated via dead leaves model to mimic real statistics. 3) Increase spatial resolution of abundances using trained network. 4) Recombine with endmembers to obtain high-resolution hyperspectral image.

Result: Experimental results show training potential of synthetic images and demonstrate method effectiveness for unsupervised hyperspectral super-resolution.

Conclusion: Proposed unsupervised training strategy using synthetic abundance data successfully addresses the lack of ground truth training data problem in hyperspectral super-resolution, with synthetic data effectively mimicking real abundance statistics.

Abstract: Considerable work has been dedicated to hyperspectral single image super-resolution to improve the spatial resolution of hyperspectral images and fully exploit their potential. However, most of these methods are supervised and require some data with ground truth for training, which is often non-available. To overcome this problem, we propose a new unsupervised training strategy for the super-resolution of hyperspectral remote sensing images, based on the use of synthetic abundance data. Its first step decomposes the hyperspectral image into abundances and endmembers by unmixing. Then, an abundance super-resolution neural network is trained using synthetic abundances, which are generated using the dead leaves model in such a way as to faithfully mimic real abundance statistics. Next, the spatial resolution of the considered hyperspectral image abundances is increased using this trained network, and the high resolution hyperspectral image is finally obtained by recombination with the endmembers. Experimental results show the training potential of the synthetic images, and demonstrate the method effectiveness.

</details>


### [140] [PanopMamba: Vision State Space Modeling for Nuclei Panoptic Segmentation](https://arxiv.org/abs/2601.16631)
*Ming Kang,Fung Fung Ting,Raphaël C. -W. Phan,Zongyuan Ge,Chee-Ming Ting*

Main category: eess.IV

TL;DR: PanopMamba: A hybrid Mamba-Transformer architecture for nuclei panoptic segmentation with SSM-based feature fusion and new evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Nuclei panoptic segmentation is crucial for cancer diagnostics but faces challenges with small objects, ambiguous boundaries, and class imbalance. Existing methods need better handling of long-range dependencies and multiscale feature integration for densely overlapping nuclei.

Method: Proposes PanopMamba, a hybrid encoder-decoder architecture combining Mamba and Transformer with SSM-based feature fusion. Features multiscale Mamba backbone and SSM-based fusion network for efficient long-range perception and cross-scale information sharing. Also introduces new evaluation metrics: iPQ, wPQ, and fwPQ.

Result: Superior performance on MoNuSAC2020 and NuInsSeg benchmark datasets compared to state-of-the-art methods. Validates robustness across various metrics and demonstrates distinctiveness of proposed PQ variants.

Conclusion: PanopMamba effectively addresses nuclei panoptic segmentation challenges through hybrid architecture and SSM-based fusion. New evaluation metrics provide more comprehensive assessment. First Mamba-based approach for panoptic segmentation, showing promising results for histopathology image analysis.

Abstract: Nuclei panoptic segmentation supports cancer diagnostics by integrating both semantic and instance segmentation of different cell types to analyze overall tissue structure and individual nuclei in histopathology images. Major challenges include detecting small objects, handling ambiguous boundaries, and addressing class imbalance. To address these issues, we propose PanopMamba, a novel hybrid encoder-decoder architecture that integrates Mamba and Transformer with additional feature-enhanced fusion via state space modeling. We design a multiscale Mamba backbone and a State Space Model (SSM)-based fusion network to enable efficient long-range perception in pyramid features, thereby extending the pure encoder-decoder framework while facilitating information sharing across multiscale features of nuclei. The proposed SSM-based feature-enhanced fusion integrates pyramid feature networks and dynamic feature enhancement across different spatial scales, enhancing the feature representation of densely overlapping nuclei in both semantic and spatial dimensions. To the best of our knowledge, this is the first Mamba-based approach for panoptic segmentation. Additionally, we introduce alternative evaluation metrics, including image-level Panoptic Quality ($i$PQ), boundary-weighted PQ ($w$PQ), and frequency-weighted PQ ($fw$PQ), which are specifically designed to address the unique challenges of nuclei segmentation and thereby mitigate the potential bias inherent in vanilla PQ. Experimental evaluations on two multiclass nuclei segmentation benchmark datasets, MoNuSAC2020 and NuInsSeg, demonstrate the superiority of PanopMamba for nuclei panoptic segmentation over state-of-the-art methods. Consequently, the robustness of PanopMamba is validated across various metrics, while the distinctiveness of PQ variants is also demonstrated. Code is available at https://github.com/mkang315/PanopMamba.

</details>


### [141] [Fast, faithful and photorealistic diffusion-based image super-resolution with enhanced Flow Map models](https://arxiv.org/abs/2601.16660)
*Maxence Noble,Gonzalo Iñaki Quintana,Benjamin Aubin,Clément Chadebec*

Main category: eess.IV

TL;DR: FlowMapSR: A novel diffusion-based super-resolution framework using Flow Map self-distillation for efficient inference, achieving better balance between reconstruction faithfulness and photorealism than state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between reconstruction faithfulness and photorealism in diffusion-based super-resolution, while improving inference efficiency. Existing teacher-student distillation approaches suffer from information compression that degrades perceptual cues like textures and depth of field.

Method: Proposes FlowMapSR framework based on Flow Map self-distillation models adapted for super-resolution. Introduces two key enhancements: (1) positive-negative prompting guidance (generalization of classifier-free guidance to Flow Map models), and (2) adversarial fine-tuning using Low-Rank Adaptation (LoRA). Evaluates three Flow Map formulations (Eulerian, Lagrangian, Shortcut) and finds Shortcut variant performs best.

Result: FlowMapSR achieves better balance between reconstruction faithfulness and photorealism than recent state-of-the-art methods for both x4 and x8 upscaling, while maintaining competitive inference time. A single model works for both upscaling factors without scale-specific conditioning or degradation-guided mechanisms.

Conclusion: FlowMapSR successfully addresses the efficiency-quality trade-off in diffusion-based super-resolution by leveraging Flow Map self-distillation with novel enhancements, outperforming existing approaches in balancing reconstruction accuracy and perceptual quality.

Abstract: Diffusion-based image super-resolution (SR) has recently attracted significant attention by leveraging the expressive power of large pre-trained text-to-image diffusion models (DMs). A central practical challenge is resolving the trade-off between reconstruction faithfulness and photorealism. To address inference efficiency, many recent works have explored knowledge distillation strategies specifically tailored to SR, enabling one-step diffusion-based approaches. However, these teacher-student formulations are inherently constrained by information compression, which can degrade perceptual cues such as lifelike textures and depth of field, even with high overall perceptual quality. In parallel, self-distillation DMs, known as Flow Map models, have emerged as a promising alternative for image generation tasks, enabling fast inference while preserving the expressivity and training stability of standard DMs. Building on these developments, we propose FlowMapSR, a novel diffusion-based framework for image super-resolution explicitly designed for efficient inference. Beyond adapting Flow Map models to SR, we introduce two complementary enhancements: (i) positive-negative prompting guidance, based on a generalization of classifier free-guidance paradigm to Flow Map models, and (ii) adversarial fine-tuning using Low-Rank Adaptation (LoRA). Among the considered Flow Map formulations (Eulerian, Lagrangian, and Shortcut), we find that the Shortcut variant consistently achieves the best performance when combined with these enhancements. Extensive experiments show that FlowMapSR achieves a better balance between reconstruction faithfulness and photorealism than recent state-of-the-art methods for both x4 and x8 upscaling, while maintaining competitive inference time. Notably, a single model is used for both upscaling factors, without any scale-specific conditioning or degradation-guided mechanisms.

</details>


### [142] [PocketDVDNet: Realtime Video Denoising for Real Camera Noise](https://arxiv.org/abs/2601.16780)
*Crispian Morris,Imogen Dexter,Fan Zhang,David R. Bull,Nantheera Anantrasirichai*

Main category: eess.IV

TL;DR: PocketDVDNet is a lightweight video denoiser using model compression with sparsity pruning, physics-informed noise modeling, and knowledge distillation to achieve real-time performance with 74% size reduction.


<details>
  <summary>Details</summary>
Motivation: Live video denoising under realistic, multi-component sensor noise remains challenging for real-time applications like autofocus, autonomous driving, and surveillance, requiring both high-quality restoration and efficient resource usage.

Method: Combines sparsity-guided structured pruning, physics-informed noise model, and knowledge distillation. Starts from reference model, induces sparsity, applies targeted channel pruning, retrains teacher on realistic noise, and has student learn implicit noise handling without explicit noise-map inputs.

Result: PocketDVDNet reduces original model size by 74% while improving denoising quality and processing 5-frame patches in real-time, demonstrating aggressive compression with domain-adapted distillation can reconcile performance and efficiency.

Conclusion: The proposed compression framework combining sparsity pruning, noise modeling, and knowledge distillation enables practical, real-time video denoising that balances performance and efficiency for real-world applications.

Abstract: Live video denoising under realistic, multi-component sensor noise remains challenging for applications such as autofocus, autonomous driving, and surveillance. We propose PocketDVDNet, a lightweight video denoiser developed using our model compression framework that combines sparsity-guided structured pruning, a physics-informed noise model, and knowledge distillation to achieve high-quality restoration with reduced resource demands. Starting from a reference model, we induce sparsity, apply targeted channel pruning, and retrain a teacher on realistic multi-component noise. The student network learns implicit noise handling, eliminating the need for explicit noise-map inputs. PocketDVDNet reduces the original model size by 74% while improving denoising quality and processing 5-frame patches in real-time. These results demonstrate that aggressive compression, combined with domain-adapted distillation, can reconcile performance and efficiency for practical, real-time video denoising.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [143] [Scalable Screw-Theoretic Synthesis for PDE-Based Dynamic Modeling of Multibody Flexible Manipulators](https://arxiv.org/abs/2601.16242)
*S. Yaqubi,J. Mattila*

Main category: cs.RO

TL;DR: A screw-theoretic PDE framework for modeling serial robotic manipulators with arbitrary flexible links, enabling scalable multibody dynamics with both local and global dynamics captured.


<details>
  <summary>Details</summary>
Motivation: To develop a systematic, scalable approach for dynamic modeling of serial robotic manipulators with flexible links that can handle arbitrary numbers of links while maintaining mathematical rigor and computational tractability.

Method: Uses screw theory to construct PDE models for individual flexible links with dual screws in body-fixed coordinates, enforces holonomic joint constraints through interaction forces, applies variational principles to derive governing dynamics, and synthesizes link models into a scalable multibody representation.

Result: Creates an infinitely scalable multibody representation that captures both local (subsystem-level) and global (system-level) dynamics, explicitly recovers all dynamic states including motion of body-fixed frames and distributed deformation fields, and formulates the system as a semi-explicit index-1 differential-algebraic system.

Conclusion: The framework provides a mathematically rigorous, scalable approach for PDE-based dynamic modeling of flexible-link robotic manipulators, with well-posedness established through reformulation as an abstract Cauchy problem.

Abstract: This paper presents a novel and scalable screw-theoretic multibody synthesis framework for PDE-based dynamic modeling of serial robotic manipulators with an arbitrary number of flexible links in three-dimensional space. The proposed approach systematically constructs screw-theoretic PDE models for individual flexible links and rigorously enforces holonomic joint constraints through interaction forces. The dynamics of each link are formulated using a set of dual screws expressed in body-fixed coordinates: one describing the motion of the body-fixed frame relative to the inertial frame, a second relating the body-fixed frame to the undeformed configuration, and a third capturing elastic deformations. By expressing the system energy and applying variational principles, the governing dynamics of each link had been previously derived in a unified manner. Synthesizing the individual link models yields an infinitely scalable multibody representation capable of capturing both local (subsystem-level) and global (system-level) dynamics. The framework explicitly recovers all dynamic states, including the motion of each body-fixed frame and the distributed deformation fields of the flexible links. For computational tractability and mathematical rigor, the resulting governing equations are formulated as a semi-explicit index-1 differential-algebraic system. Furthermore, by applying separation of variables, the PDE model is recast as an abstract Cauchy problem, and well-posedness of the resulting system is established.

</details>


### [144] [DMV-AVP: Distributed Multi-Vehicle Autonomous Valet Parking using Autoware](https://arxiv.org/abs/2601.16327)
*Zubair Islam,Mohamed El-Darieby*

Main category: cs.RO

TL;DR: Distributed simulation system for multi-vehicle autonomous valet parking using DMAVA architecture with state coordination and vision-based parking detection.


<details>
  <summary>Details</summary>
Motivation: Existing AVP simulations are centralized/non-distributed, limiting scalability and autonomous control capabilities for multi-vehicle scenarios.

Method: Built two modules on DMAVA: 1) Multi-Vehicle AVP Node for state coordination, queuing, and reservation management; 2) Unity-Integrated YOLOv5 Parking Spot Detection Module for real-time vision perception. Uses Zenoh-based communication for low-latency synchronization.

Result: Experiments on 2-3 host configurations show deterministic coordination, conflict-free parking behavior, and scalable performance across distributed Autoware instances.

Conclusion: The system enables cooperative AVP simulation and provides foundation for real-world and hardware-in-the-loop validation.

Abstract: This paper presents the DMV-AVP System, a distributed simulation of Multi-Vehicle Autonomous Valet Parking (AVP). The system was implemented as an application of the Distributed Multi-Vehicle Architecture (DMAVA) for synchronized multi-host execution. Most existing simulation approaches rely on centralized or non-distributed designs that constrain scalability and limit fully autonomous control. This work introduces two modules built on top of the DMAVA: 1) a Multi-Vehicle AVP Node that performs state-based coordination, queuing, and reservation management across multiple vehicles, and 2) a Unity-Integrated YOLOv5 Parking Spot Detection Module that provides real-time, vision-based perception within AWSIM Labs. Both modules integrate seamlessly with the DMAVA and extend it specifically for multi-vehicle AVP operation, supported by a Zenoh-based communication layer that ensures low-latency topic synchronization and coordinated behavior across hosts. Experiments conducted on two- and three-host configurations demonstrate deterministic coordination, conflict-free parking behavior, and scalable performance across distributed Autoware instances. The results confirm that the proposed Distributed Multi-Vehicle AVP System supports cooperative AVP simulation and establishes a foundation for future real-world and hardware-in-the-loop validation. Demo videos and source code are available at https://github.com/zubxxr/multi-vehicle-avp

</details>


### [145] [DMAVA: Distributed Multi-Autonomous Vehicle Architecture Using Autoware](https://arxiv.org/abs/2601.16336)
*Zubair Islam,Mohamed El-Darieby*

Main category: cs.RO

TL;DR: DMAVA enables synchronized real-time multi-AV simulation across multiple physical hosts with independent AV stacks and low-latency coordination.


<details>
  <summary>Details</summary>
Motivation: Existing simulation architectures are limited to single-vehicle operation or rely on centralized control, making multi-AV coordination simulation challenging.

Method: Distributed Multi-AV Architecture (DMAVA) with independent AV stacks per vehicle, low-latency data-centric communication layer, integrating ROS 2 Humble, Autoware Universe, AWSIM Labs, and Zenoh within a shared Unity-based environment.

Result: Experiments show stable localization, reliable inter-host communication, and fully synchronized closed-loop control across multiple-host configurations.

Conclusion: DMAVA successfully enables distributed multi-AV simulation and serves as foundation for cooperative autonomy applications like Multi-Vehicle Autonomous Valet Parking.

Abstract: Simulating and validating coordination among multiple autonomous vehicles (AVs) is a challenging task as most existing simulation architectures are limited to single-vehicle operation or rely on centralized control. This paper presents a Distributed Multi-AV Architecture (DMAVA) that enables synchronized, real-time autonomous driving simulation across multiple physical hosts. Each vehicle runs its own complete AV stack and operates independently from other AVs. The vehicles in the simulation maintain synchronized coordination through a low-latency data-centric communication layer. The proposed system integrates ROS 2 Humble, Autoware Universe, AWSIM Labs, and Zenoh to support concurrent execution of multiple Autoware stacks within a shared Unity-based environment. Experiments conducted on multiple-host configurations demonstrate stable localization, reliable inter-host communication, and fully synchronized closed-loop control. The DMAVA also serves as a foundation for Multi-Vehicle Autonomous Valet Parking, demonstrating its extensibility toward higher-level cooperative autonomy. Demo videos and source code are available at: https://github.com/zubxxr/distributed-multi-autonomous-vehicle-architecture.

</details>


### [146] [GNSS-based Lunar Orbit and Clock Estimation With Stochastic Cloning UD Filter](https://arxiv.org/abs/2601.16393)
*Keidai Iiyama,Grace Gao*

Main category: cs.RO

TL;DR: Terrestrial GNSS-based orbit/clock estimation framework for lunar navigation satellites using stochastic-cloning UD-factorized filter and TDCP measurements achieves meter-level orbit accuracy.


<details>
  <summary>Details</summary>
Motivation: To enable high-precision orbit and clock estimation for lunar navigation satellites under low-observability conditions at lunar distances, addressing the needs of future Lunar Augmented Navigation Services (LANS).

Method: Developed stochastic-cloning UD-factorized filter and delayed-state smoother for processing precise time-differenced carrier phase (TDCP) measurements. Formulated comprehensive dynamics and measurement models accounting for relativistic coupling, lunar time-scale transformations, and signal propagation delays (ionospheric, plasmaspheric, Shapiro effects).

Result: Combining ionosphere-free pseudorange and TDCP measurements achieves meter-level orbit accuracy and sub-millimeter-per-second velocity accuracy in high-fidelity Monte-Carlo simulations with realistic GNSS geometry, ephemeris errors, and propagation delays.

Conclusion: The proposed approach satisfies stringent signal-in-space error requirements for future Lunar Augmented Navigation Services, demonstrating feasibility of terrestrial GNSS-based precise orbit determination for lunar satellites.

Abstract: This paper presents a terrestrial GNSS-based orbit and clock estimation framework for lunar navigation satellites. To enable high-precision estimation under the low-observability conditions encountered at lunar distances, we develop a stochastic-cloning UD-factorized filter and delayed-state smoother that provide enhanced numerical stability when processing precise time-differenced carrier phase (TDCP) measurements. A comprehensive dynamics and measurement model is formulated, explicitly accounting for relativistic coupling between orbital and clock states, lunar time-scale transformations, and signal propagation delays including ionospheric, plasmaspheric, and Shapiro effects. The proposed approach is evaluated using high-fidelity Monte-Carlo simulations incorporating realistic multi-constellation GNSS geometry, broadcast ephemeris errors, lunar satellite dynamics, and ionospheric and plasmaspheric delay computed from empirical electron density models. Simulation results demonstrate that combining ionosphere-free pseudorange and TDCP measurements achieves meter-level orbit accuracy and sub-millimeter-per-second velocity accuracy, satisfying the stringent signal-in-space error requirements of future Lunar Augmented Navigation Services (LANS).

</details>


### [147] [Reinforcement Learning-Based Energy-Aware Coverage Path Planning for Precision Agriculture](https://arxiv.org/abs/2601.16405)
*Beining Wu,Zihao Ding,Leo Ostigaard,Jun Huang*

Main category: cs.RO

TL;DR: Energy-aware coverage path planning using SAC reinforcement learning with CNN-LSTM architecture for agricultural robots, achieving over 90% coverage while ensuring energy safety.


<details>
  <summary>Details</summary>
Motivation: Existing coverage path planning solutions for agricultural robots often ignore energy constraints, leading to incomplete operations in large-scale or resource-limited environments where robots need to manage battery life and charging requirements.

Method: Proposes an energy-aware CPP framework based on Soft Actor-Critic (SAC) reinforcement learning for grid-based environments with obstacles and charging stations. Integrates CNNs for spatial feature extraction and LSTMs for temporal dynamics. Uses a dedicated reward function to jointly optimize coverage efficiency, energy consumption, and return-to-base constraints.

Result: Achieves over 90% coverage while ensuring energy safety. Outperforms traditional heuristic algorithms (RRT, PSO, ACO) by 13.4-19.5% in coverage and reduces constraint violations by 59.9-88.3%.

Conclusion: The SAC-based framework is validated as an effective and scalable solution for energy-constrained coverage path planning in agricultural robotics, addressing critical energy management challenges in real-world deployments.

Abstract: Coverage Path Planning (CPP) is a fundamental capability for agricultural robots; however, existing solutions often overlook energy constraints, resulting in incomplete operations in large-scale or resource-limited environments. This paper proposes an energy-aware CPP framework grounded in Soft Actor-Critic (SAC) reinforcement learning, designed for grid-based environments with obstacles and charging stations. To enable robust and adaptive decision-making under energy limitations, the framework integrates Convolutional Neural Networks (CNNs) for spatial feature extraction and Long Short-Term Memory (LSTM) networks for temporal dynamics. A dedicated reward function is designed to jointly optimize coverage efficiency, energy consumption, and return-to-base constraints. Experimental results demonstrate that the proposed approach consistently achieves over 90% coverage while ensuring energy safety, outperforming traditional heuristic algorithms such as Rapidly-exploring Random Tree (RRT), Particle Swarm Optimization (PSO), and Ant Colony Optimization (ACO) baselines by 13.4-19.5% in coverage and reducing constraint violations by 59.9-88.3%. These findings validate the proposed SAC-based framework as an effective and scalable solution for energy-constrained CPP in agricultural robotics.

</details>


### [148] [RENEW: Risk- and Energy-Aware Navigation in Dynamic Waterways](https://arxiv.org/abs/2601.16424)
*Mingi Jeong,Alberto Quattrini Li*

Main category: cs.RO

TL;DR: RENEW is a global path planner for Autonomous Surface Vehicles that combines risk- and energy-aware planning with adaptive safety constraints for dynamic maritime environments with water currents.


<details>
  <summary>Details</summary>
Motivation: Current ASV path planners lack robust solutions for dynamic maritime environments with external disturbances like water currents, and fail to jointly address adaptive non-navigability identification and topological path diversity for robust navigation.

Method: Hierarchical architecture with high-level constrained triangulation for topological diversity and low-level trajectory optimization within safe corridors. Uses unified risk- and energy-aware strategy with dynamic non-navigable region identification and adaptive safety constraints inspired by maritime contingency planning.

Result: Validated with real-world ocean data, RENEW is the first framework to jointly address adaptive non-navigability and topological path diversity for robust maritime navigation, ensuring safety through dynamic hazard identification and control maintenance under adverse conditions.

Conclusion: RENEW provides a novel, robust path planning solution for ASVs in dynamic maritime environments by combining risk-aware planning with topological diversity, representing an advancement in maritime autonomous navigation safety and reliability.

Abstract: We present RENEW, a global path planner for Autonomous Surface Vehicle (ASV) in dynamic environments with external disturbances (e.g., water currents). RENEW introduces a unified risk- and energy-aware strategy that ensures safety by dynamically identifying non-navigable regions and enforcing adaptive safety constraints. Inspired by maritime contingency planning, it employs a best-effort strategy to maintain control under adverse conditions. The hierarchical architecture combines high-level constrained triangulation for topological diversity with low-level trajectory optimization within safe corridors. Validated with real-world ocean data, RENEW is the first framework to jointly address adaptive non-navigability and topological path diversity for robust maritime navigation.

</details>


### [149] [Zero-Shot MARL Benchmark in the Cyber-Physical Mobility Lab](https://arxiv.org/abs/2601.16578)
*Julius Beerwerth,Jianye Xu,Simon Schäfer,Fynn Belderink,Bassam Alrifaee*

Main category: cs.RO

TL;DR: A reproducible benchmark for evaluating sim-to-real transfer of MARL policies for CAVs using a platform that integrates simulation, digital twin, and physical testbed for structured zero-shot evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of evaluating sim-to-real transfer in MARL for Connected and Automated Vehicles by creating a structured, reproducible platform that can systematically analyze performance degradation across different domains.

Method: Developed a benchmark platform based on the Cyber-Physical Mobility Lab (CPM Lab) that integrates three domains: simulation, high-fidelity digital twin, and physical testbed. Used this platform to deploy SigmaRL-trained MARL policies for motion planning and evaluate them in a zero-shot transfer setting.

Result: Identified two complementary sources of performance degradation: 1) architectural differences between simulation and hardware control stacks, and 2) the sim-to-real gap induced by increasing environmental realism. The platform enables systematic analysis of these challenges under realistic, reproducible conditions.

Conclusion: The open-source benchmark provides a valuable tool for systematically evaluating sim-to-real transfer in MARL for CAVs, helping researchers understand and address the performance degradation that occurs when moving from simulation to real-world deployment.

Abstract: We present a reproducible benchmark for evaluating sim-to-real transfer of Multi-Agent Reinforcement Learning (MARL) policies for Connected and Automated Vehicles (CAVs). The platform, based on the Cyber-Physical Mobility Lab (CPM Lab) [1], integrates simulation, a high-fidelity digital twin, and a physical testbed, enabling structured zero-shot evaluation of MARL motion-planning policies. We demonstrate its use by deploying a SigmaRL-trained policy [2] across all three domains, revealing two complementary sources of performance degradation: architectural differences between simulation and hardware control stacks, and the sim-to-real gap induced by increasing environmental realism. The open-source setup enables systematic analysis of sim-to-real challenges in MARL under realistic, reproducible conditions.

</details>


### [150] [A Unified Calibration Framework for High-Accuracy Articulated Robot Kinematics](https://arxiv.org/abs/2601.16638)
*Philip Tobuschat,Simon Duenser,Markus Bambach,Ivo Aschwanden*

Main category: cs.RO

TL;DR: Unified static calibration method for industrial robots that identifies both geometric and non-geometric error sources using a single experiment, achieving 26.8 μm mean position error vs 102.3 μm for geometric-only calibration.


<details>
  <summary>Details</summary>
Motivation: Existing robot calibration methods require separate specialized experiments and models for different error sources (geometric, compliant bending, thermal deformation, gear transmission errors), making calibration complex and time-consuming.

Method: Augments kinematic chain with virtual joints for each modeled effect, uses Gauss-Newton optimization with analytic gradients for parameter identification, requires only a single straightforward experiment for data collection.

Result: Achieved mean position error of 26.8 μm on KUKA KR30 industrial robot, significantly better than 102.3 μm from purely geometric calibration. Fisher information spectra show well-conditioned estimation, systematic validation demonstrates robust model identification.

Conclusion: The unified approach provides accurate and robust static calibration by simultaneously identifying multiple error sources with minimal experimental effort, offering practical advantages over existing piecemeal calibration methods.

Abstract: Researchers have identified various sources of tool positioning errors for articulated industrial robots and have proposed dedicated compensation strategies. However, these typically require individual, specialized experiments with separate models and identification procedures. This article presents a unified approach to the static calibration of industrial robots that identifies a robot model, including geometric and non-geometric effects (compliant bending, thermal deformation, gear transmission errors), using only a single, straightforward experiment for data collection. The model augments the kinematic chain with virtual joints for each modeled effect and realizes the identification using Gauss-Newton optimization with analytic gradients. Fisher information spectra show that the estimation is well-conditioned and the parameterization near-minimal, whereas systematic temporal cross-validation and model ablations demonstrate robustness of the model identification. The resulting model is very accurate and its identification robust, achieving a mean position error of 26.8 $μm$ on a KUKA KR30 industrial robot compared to 102.3 $μm$ for purely geometric calibration.

</details>


### [151] [ReViP: Reducing False Completion in Vision-Language-Action Models with Vision-Proprioception Rebalance](https://arxiv.org/abs/2601.16667)
*Zhuohao Li,Yinghao Li,Jian-Jian Jiang,Lang Zhou,Tianyu Zhang,Wei-Shi Zheng*

Main category: cs.RO

TL;DR: ReViP is a Vision-Language-Action framework that rebalances vision-proprioception fusion to reduce false completions in robotic manipulation by using task-aware visual cues to modulate feature coupling.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models suffer from state-dominant bias where policies over-rely on proprioceptive signals while underusing visual evidence, leading to false completions despite visible execution failures.

Method: Introduces Vision-Proprioception Rebalance using an external VLM as task-stage observer to extract real-time task-centric visual cues, which drive Feature-wise Linear Modulation to adaptively couple semantic perception with proprioceptive dynamics.

Result: ReViP effectively reduces false-completion rates and improves success rates over strong VLA baselines on their False-Completion Benchmark Suite, with gains extending to LIBERO, RoboTwin 2.0, and real-world evaluations.

Conclusion: The proposed vision-proprioception rebalancing approach enhances visual grounding and robustness under perturbations, addressing modality imbalance in VLA models for more reliable robotic manipulation.

Abstract: Vision-Language-Action (VLA) models have advanced robotic manipulation by combining vision, language, and proprioception to predict actions. However, previous methods fuse proprioceptive signals directly with VLM-encoded vision-language features, resulting in state-dominant bias and false completions despite visible execution failures. We attribute this to modality imbalance, where policies over-rely on internal state while underusing visual evidence. To address this, we present ReViP, a novel VLA framework with Vision-Proprioception Rebalance to enhance visual grounding and robustness under perturbations. The key insight is to introduce auxiliary task-aware environment priors to adaptively modulate the coupling between semantic perception and proprioceptive dynamics. Specifically, we use an external VLM as a task-stage observer to extract real-time task-centric visual cues from visual observations, which drive a Vision-Proprioception Feature-wise Linear Modulation to enhance environmental awareness and reduce state-driven errors. Moreover, to evaluate false completion, we propose the first False-Completion Benchmark Suite built on LIBERO with controlled settings such as Object-Drop. Extensive experiments show that ReViP effectively reduces false-completion rates and improves success rates over strong VLA baselines on our suite, with gains extending to LIBERO, RoboTwin 2.0, and real-world evaluations.

</details>


### [152] [Sim-to-Real Transfer via a Style-Identified Cycle Consistent Generative Adversarial Network: Zero-Shot Deployment on Robotic Manipulators through Visual Domain Adaptation](https://arxiv.org/abs/2601.16677)
*Lucía Güitta-López,Lionel Güitta-López,Jaime Boal,Álvaro Jesús López-López*

Main category: cs.RO

TL;DR: Proposes SICGAN (StyleID-CycleGAN) for zero-shot sim-to-real transfer in DRL, achieving 95%+ accuracy in real-world robotic pick-and-place tasks without additional real training.


<details>
  <summary>Details</summary>
Motivation: DRL's sample inefficiency hinders industrial adoption due to high real-world training costs. Virtual training is cheaper but suffers from sim-to-real gap. Zero-shot transfer is desirable for practical efficiency.

Method: Uses StyleID-CycleGAN (SICGAN) to translate virtual observations into real-synthetic images, creating hybrid domain for DRL training. Agents train in virtual environments with real-like visuals, then deploy directly to real world.

Result: Virtual training achieves 90-100% success rates. Real-world zero-shot transfer achieves >95% accuracy for most workspace regions. Agents generalize to real objects of varying colors/shapes (LEGO cubes, mug).

Conclusion: Proposed pipeline provides efficient, scalable solution to sim-to-real problem, enabling zero-shot transfer without real-world training, validated on industrial robotic pick-and-place tasks.

Abstract: The sample efficiency challenge in Deep Reinforcement Learning (DRL) compromises its industrial adoption due to the high cost and time demands of real-world training. Virtual environments offer a cost-effective alternative for training DRL agents, but the transfer of learned policies to real setups is hindered by the sim-to-real gap. Achieving zero-shot transfer, where agents perform directly in real environments without additional tuning, is particularly desirable for its efficiency and practical value. This work proposes a novel domain adaptation approach relying on a Style-Identified Cycle Consistent Generative Adversarial Network (StyleID-CycleGAN or SICGAN), an original Cycle Consistent Generative Adversarial Network (CycleGAN) based model. SICGAN translates raw virtual observations into real-synthetic images, creating a hybrid domain for training DRL agents that combines virtual dynamics with real-like visual inputs. Following virtual training, the agent can be directly deployed, bypassing the need for real-world training. The pipeline is validated with two distinct industrial robots in the approaching phase of a pick-and-place operation. In virtual environments agents achieve success rates of 90 to 100\%, and real-world deployment confirms robust zero-shot transfer (i.e., without additional training in the physical environment) with accuracies above 95\% for most workspace regions. We use augmented reality targets to improve the evaluation process efficiency, and experimentally demonstrate that the agent successfully generalizes to real objects of varying colors and shapes, including LEGO\textsuperscript{\textregistered}~cubes and a mug. These results establish the proposed pipeline as an efficient, scalable solution to the sim-to-real problem.

</details>


### [153] [Adaptive Reinforcement and Model Predictive Control Switching for Safe Human-Robot Cooperative Navigation](https://arxiv.org/abs/2601.16686)
*Ning Liu,Sen Shen,Zheng Li,Matthew D'Souza,Jen Jen Chung,Thomas Braunl*

Main category: cs.RO

TL;DR: ARMS is a hybrid learning-control framework for human-guided robot navigation that adaptively switches between RL and MPC controllers based on context, achieving high success rates in cluttered environments with low latency.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of human-guided navigation for mobile collaborative robots that must simultaneously maintain proximity regulation (staying close to humans) and safety constraints (avoiding obstacles), particularly in cluttered environments where traditional methods struggle.

Method: ARMS integrates a PPO-trained RL follower with a one-step MPC safety filter. It uses a decoupled sensing architecture with LSTM temporal encoding for human-robot states and spatial encoding for LiDAR scans. A key innovation is a learned neural switcher that performs context-aware soft action fusion between controllers, favoring MPC in low-risk areas and shifting to RL in cluttered scenarios.

Result: ARMS achieves 82.5% success rate in highly cluttered environments, outperforming DWA by 7.1% and RL-only by 3.1%. It reduces computational latency by 33% to 5.2ms compared to multi-step MPC. Simulation and real-world tests demonstrate practicality for human-robot collaboration.

Conclusion: The ARMS framework effectively combines the strengths of learning-based and model-based approaches for human-guided navigation, providing adaptive, safe, and efficient control that outperforms existing methods in challenging cluttered environments.

Abstract: This paper addresses the challenge of human-guided navigation for mobile collaborative robots under simultaneous proximity regulation and safety constraints. We introduce Adaptive Reinforcement and Model Predictive Control Switching (ARMS), a hybrid learning-control framework that integrates a reinforcement learning follower trained with Proximal Policy Optimization (PPO) and an analytical one-step Model Predictive Control (MPC) formulated as a quadratic program safety filter. To enable robust perception under partial observability and non-stationary human motion, ARMS employs a decoupled sensing architecture with a Long Short-Term Memory (LSTM) temporal encoder for the human-robot relative state and a spatial encoder for 360-degree LiDAR scans. The core contribution is a learned adaptive neural switcher that performs context-aware soft action fusion between the two controllers, favoring conservative, constraint-aware QP-based control in low-risk regions while progressively shifting control authority to the learned follower in highly cluttered or constrained scenarios where maneuverability is critical, and reverting to the follower action when the QP becomes infeasible. Extensive evaluations against Pure Pursuit, Dynamic Window Approach (DWA), and an RL-only baseline demonstrate that ARMS achieves an 82.5 percent success rate in highly cluttered environments, outperforming DWA and RL-only approaches by 7.1 percent and 3.1 percent, respectively, while reducing average computational latency by 33 percent to 5.2 milliseconds compared to a multi-step MPC baseline. Additional simulation transfer in Gazebo and initial real-world deployment results further indicate the practicality and robustness of ARMS for safe and efficient human-robot collaboration. Source code and a demonstration video are available at https://github.com/21ning/ARMS.git.

</details>


### [154] [Creating a biologically more accurate spider robot to study active vibration sensing](https://arxiv.org/abs/2601.16691)
*Siyuan Sun,Eugene H. Lin,Nathan Brown,Hsin-Yi Hung,Andrew Gordus,Jochen Mueller,Chen Li*

Main category: cs.RO

TL;DR: Researchers developed an improved eight-legged spider robot with more biologically accurate leg morphology and deeper crouching capability to study how spiders use leg movements to enhance vibration sensing on webs.


<details>
  <summary>Details</summary>
Motivation: To understand how orb-weaving spiders enhance prey detection through leg crouching behaviors, which is difficult to study in living animals due to challenges in measuring system vibrations during active sensing.

Method: Created a new spider robot with eight legs (vs previous four), each with four joints approximating spider leg morphology. Used 3D-printed exoskeletons with silicone molding for joint stiffness tuning, and tendon-driven actuation for deep leg crouching. Integrated accelerometers at leg joints to record vibrations.

Result: The new spider robot reproduced key vibration features from the previous model while improving biological accuracy, providing a more realistic robophysical platform for studying vibration sensing.

Conclusion: This improved spider robot offers a biologically more accurate robophysical model for investigating how leg behaviors modulate vibration sensing on webs, advancing the study of active sensing strategies in spiders.

Abstract: Orb-weaving spiders detect prey on a web using vibration sensors at leg joints. They often dynamically crouch their legs during prey sensing, likely an active sensing strategy. However, how leg crouching enhances sensing is poorly understood, because measuring system vibrations in behaving animals is difficult. We use robophysical modeling to study this problem. Our previous spider robot had only four legs, simplified leg morphology, and a shallow crouching range of motion. Here, we developed a new spider robot, with eight legs, each with four joints that better approximated spider leg morphology. Leg exoskeletons were 3-D printed and joint stiffness was tuned using integrated silicone molding with variable materials and geometry. Tendon-driven actuation allowed a motor in the body to crouch all eight legs deeply as spiders do, while accelerometers at leg joints record leg vibrations. Experiments showed that our new spider robot reproduced key vibration features observed in the previous robot while improving biological accuracy. Our new robot provides a biologically more accurate robophysical model for studying how leg behaviors modulate vibration sensing on a web.

</details>


### [155] [A Feature Extraction Pipeline for Enhancing Lightweight Neural Networks in sEMG-based Joint Torque Estimation](https://arxiv.org/abs/2601.16712)
*Kartik Chari,Raid Dokhan,Anas Homsi,Niklas Kueper,Elsa Andrea Kirchner*

Main category: cs.RO

TL;DR: Proposed sEMG feature extraction pipeline enables simple MLP to predict joint torques as effectively as temporal networks, beneficial for limited data rehabilitation applications.


<details>
  <summary>Details</summary>
Motivation: Accurate joint torque prediction is essential for personalized robot-assisted rehabilitation, but existing methods may be complex or require large datasets.

Method: Feature extraction pipeline using 8-channel sEMG signals, integrated with MLP and TCN models. Data collected from single subject performing elbow/shoulder movements under three load conditions, with torques estimated from kinematics.

Result: MLP achieved mean RMSE of 0.963 N m (elbow), 1.403 N m (front-shoulder), and 1.434 N m (side-shoulder), comparable to TCN performance despite simpler architecture.

Conclusion: Simple MLP with effective feature extraction can match temporal network performance, making it suitable for rehabilitation applications with limited training data.

Abstract: Robot-assisted rehabilitation offers an effective approach, wherein exoskeletons adapt to users' needs and provide personalized assistance. However, to deliver such assistance, accurate prediction of the user's joint torques is essential. In this work, we propose a feature extraction pipeline using 8-channel surface electromyography (sEMG) signals to predict elbow and shoulder joint torques. For preliminary evaluation, this pipeline was integrated into two neural network models: the Multilayer Perceptron (MLP) and the Temporal Convolutional Network (TCN). Data were collected from a single subject performing elbow and shoulder movements under three load conditions (0 kg, 1.10 kg, and 1.85 kg) using three motion-capture cameras. Reference torques were estimated from center-of-mass kinematics under the assumption of static equilibrium. Our offline analyses showed that, with our feature extraction pipeline, MLP model achieved mean RMSE of 0.963 N m, 1.403 N m, and 1.434 N m (over five seeds) for elbow, front-shoulder, and side-shoulder joints, respectively, which were comparable to the TCN performance. These results demonstrate that the proposed feature extraction pipeline enables a simple MLP to achieve performance comparable to that of a network designed explicitly for temporal dependencies. This finding is particularly relevant for applications with limited training data, a common scenario patient care.

</details>


### [156] [Boosting Deep Reinforcement Learning with Semantic Knowledge for Robotic Manipulators](https://arxiv.org/abs/2601.16866)
*Lucía Güitta-López,Vincenzo Suriani,Jaime Boal,Álvaro J. López-López,Daniele Nardi*

Main category: cs.RO

TL;DR: Integrating Knowledge Graph Embeddings with Deep Reinforcement Learning reduces robotic learning time by 60% and improves accuracy by 15% without extra computational cost.


<details>
  <summary>Details</summary>
Motivation: DRL for robotic control requires substantial experience, leading to high computational and time costs. The paper aims to address this inefficiency by leveraging semantic knowledge to enhance learning.

Method: Proposes integration of DRL with Knowledge Graph Embeddings (KGEs) that combine semantic knowledge with visual observations, allowing agents to exploit environmental knowledge during training.

Result: Achieves up to 60% reduction in learning time and improves task accuracy by approximately 15 percentage points in robotic manipulator experiments with fixed and randomized target attributes.

Conclusion: Semantic knowledge integration through KGEs significantly reduces sample complexity and improves DRL effectiveness in robotic applications without increasing training time or computational complexity.

Abstract: Deep Reinforcement Learning (DRL) is a powerful framework for solving complex sequential decision-making problems, particularly in robotic control. However, its practical deployment is often hindered by the substantial amount of experience required for learning, which results in high computational and time costs. In this work, we propose a novel integration of DRL with semantic knowledge in the form of Knowledge Graph Embeddings (KGEs), aiming to enhance learning efficiency by providing contextual information to the agent. Our architecture combines KGEs with visual observations, enabling the agent to exploit environmental knowledge during training. Experimental validation with robotic manipulators in environments featuring both fixed and randomized target attributes demonstrates that our method achieves up to {60}{\%} reduction in learning time and improves task accuracy by approximately 15 percentage points, without increasing training time or computational complexity. These results highlight the potential of semantic knowledge to reduce sample complexity and improve the effectiveness of DRL in robotic applications.

</details>


### [157] [A Multimodal Data Collection Framework for Dialogue-Driven Assistive Robotics to Clarify Ambiguities: A Wizard-of-Oz Pilot Study](https://arxiv.org/abs/2601.16870)
*Guangping Liu,Nicholas Hawkins,Billy Madden,Tipu Sultan,Flavio Esposito,Madi Babaiasl*

Main category: cs.RO

TL;DR: A multimodal data collection framework for wheelchair-mounted robotic arms using dialogue-based interaction and Wizard-of-Oz setup to capture natural human-robot interaction with conversational ambiguity.


<details>
  <summary>Details</summary>
Motivation: Existing interfaces for integrated wheelchair and robotic arm control lack flexibility for intuitive assistive interaction, and progress in AI methods is limited by the absence of multimodal datasets capturing natural HRI, especially conversational ambiguity in dialogue-driven control.

Method: Proposed a multimodal data collection framework using dialogue-based interaction protocol and two-room Wizard-of-Oz setup to simulate robot autonomy while eliciting natural user behavior. Records five synchronized modalities: RGB-D video, conversational audio, IMU signals, end-effector Cartesian pose, and whole-body joint states across five assistive tasks.

Result: Collected a pilot dataset of 53 trials from five participants. Validated quality through motion smoothness analysis and user feedback. Framework effectively captures diverse ambiguity types and supports natural dialogue-driven interaction.

Conclusion: The framework is suitable for scaling to a larger dataset for learning, benchmarking, and evaluation of ambiguity-aware assistive control, addressing the critical gap in multimodal HRI datasets for wheelchair-mounted robotic arms.

Abstract: Integrated control of wheelchairs and wheelchair-mounted robotic arms (WMRAs) has strong potential to increase independence for users with severe motor limitations, yet existing interfaces often lack the flexibility needed for intuitive assistive interaction. Although data-driven AI methods show promise, progress is limited by the lack of multimodal datasets that capture natural Human-Robot Interaction (HRI), particularly conversational ambiguity in dialogue-driven control. To address this gap, we propose a multimodal data collection framework that employs a dialogue-based interaction protocol and a two-room Wizard-of-Oz (WoZ) setup to simulate robot autonomy while eliciting natural user behavior. The framework records five synchronized modalities: RGB-D video, conversational audio, inertial measurement unit (IMU) signals, end-effector Cartesian pose, and whole-body joint states across five assistive tasks. Using this framework, we collected a pilot dataset of 53 trials from five participants and validated its quality through motion smoothness analysis and user feedback. The results show that the framework effectively captures diverse ambiguity types and supports natural dialogue-driven interaction, demonstrating its suitability for scaling to a larger dataset for learning, benchmarking, and evaluation of ambiguity-aware assistive control.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [158] [Study of Switched Step-size Based Filtered-x NLMS Algorithm for Active Noise Cancellation](https://arxiv.org/abs/2601.16382)
*Zhiyuan Li,Yi Yu,Hongsen He,Yuyu Zhu,Rodrigo C. de Lamare*

Main category: cs.IT

TL;DR: Proposed a switched step-size FxNLMS algorithm with robust variant for impulsive noise to overcome fixed step-size trade-off and noise sensitivity limitations.


<details>
  <summary>Details</summary>
Motivation: The standard FxNLMS algorithm has two critical limitations: 1) fixed step-size creates a trade-off between convergence rate and steady-state error, and 2) performance deteriorates significantly in impulsive noise environments.

Method: 1) Proposed switched step-size FxNLMS (SSS-FxNLMS) by deriving mean-square deviation (MSD) trend and comparing MSD trends for different step-sizes to select optimal step-size each iteration. 2) Integrated robust strategy into SSS-FxNLMS for impulsive noise scenarios.

Result: The proposed algorithms demonstrated effectiveness and superiority through computer simulations in different noise scenarios.

Conclusion: The SSS-FxNLMS algorithm addresses the step-size constraint issue, and its robust variant enhances performance in impulsive noise environments, overcoming key limitations of the standard FxNLMS algorithm.

Abstract: While the filtered-x normalized least mean square (FxNLMS) algorithm is widely applied due to its simple structure and easy implementation for active noise control system, it faces two critical limitations: the fixed step-size causes a trade-off between convergence rate and steady-state residual error, and its performance deteriorates significantly in impulsive noise environments. To address the step-size constraint issue, we propose the switched \mbox{step-size} FxNLMS (SSS-FxNLMS) algorithm. Specifically, we derive the \mbox{mean-square} deviation (MSD) trend of the FxNLMS algorithm, and then by comparing the MSD trends corresponding to different \mbox{step-sizes}, the optimal step-size for each iteration is selected. Furthermore, to enhance the algorithm's robustness in impulsive noise scenarios, we integrate a robust strategy into the SSS-FxNLMS algorithm, resulting in a robust variant of it. The effectiveness and superiority of the proposed algorithms has been confirmed through computer simulations in different noise scenarios.

</details>


### [159] [Two classes of LCD codes derived from $(\mathcal{L},\mathcal{P})$-TGRS codes](https://arxiv.org/abs/2601.16438)
*Ziwei Zhao,Xiaoni DU,Xingbin Qiao*

Main category: cs.IT

TL;DR: The paper constructs two classes of LCD codes from twisted generalized Reed-Solomon (TGRS) codes, derives conditions for them to be AMDS codes, and obtains LCD MDS codes from these constructions.


<details>
  <summary>Details</summary>
Motivation: TGRS codes are an important extension of classical GRS codes with recent attention. The motivation is to construct linear complementary dual (LCD) codes from TGRS codes, which have applications in cryptography and coding theory due to their self-orthogonal properties.

Method: 1. Start with a specific TGRS code structure with length n, dimension k, and twisting parameter h. 2. Derive the parity check matrix for this TGRS code. 3. Establish necessary and sufficient conditions for the code to be an AMDS code. 4. Construct two classes of LCD codes by carefully selecting evaluation points and imposing restrictions on the coefficient of x^{h-1} in the polynomial associated with the twisting term. 5. Further derive LCD MDS codes from the constructed LCD codes.

Result: The paper successfully: 1. Derives the parity check matrix for the specified TGRS code. 2. Provides necessary and sufficient conditions for the code to be an AMDS code. 3. Constructs two classes of LCD codes from the TGRS code structure. 4. Obtains two classes of LCD MDS codes from the LCD constructions. 5. Presents several examples to illustrate the constructions.

Conclusion: The paper provides systematic methods for constructing LCD codes from TGRS codes, with applications to obtaining LCD MDS codes. The results contribute to the theory of twisted generalized Reed-Solomon codes and their applications in constructing codes with desirable properties like linear complementary duality.

Abstract: Twisted generalized Reed-Solomon (TGRS) codes, as a flexible extension of classical generalized Reed-Solomon (GRS) codes, have attracted significant attention in recent years. In this paper, we construct two classes of LCD codes from the $(\mathcal{L},\mathcal{P})$-TGRS code $\mathcal{C}_h$ of length $n$ and dimension $k$, where $\mathcal{L}=\{0,1,\ldots,l\}$ for $l\leq n-k-1$ and $\mathcal{P}=\{h\}$ for $1\leq h\leq k-1$. First, we derive the parity check matrix of $\mathcal{C}_h$ and provide a necessary and sufficient condition for $\mathcal{C}_h$ to be an AMDS code. Then, we construct two classes of LCD codes from $\mathcal{C}_h$ by suitably choosing the evaluation points together with certain restrictions on the coefficient of $x^{h-1}$ in the polynomial associated with the twisting term. From the constructed LCD codes we further obtain two classes of LCD MDS codes. Finally, several examples are presented.

</details>


### [160] [Cramér-Rao Bound Minimization for Flexible Intelligent Metasurface-Enabled ISAC Systems](https://arxiv.org/abs/2601.16455)
*Qian Zhang,Yufei Zhao,Jiancheng An,Zheng Dong,Yong Liang Guan,Ju Liu,Chau Yuen*

Main category: cs.IT

TL;DR: First study on CRB minimization in flexible intelligent metasurface-enabled ISAC systems, showing surface shaping can significantly reduce sensing error bounds while maintaining communication quality.


<details>
  <summary>Details</summary>
Motivation: Integrated sensing and communication (ISAC) is key for future wireless networks, with Cramér-Rao bound (CRB) quantifying sensing accuracy. Flexible intelligent metasurfaces (FIMs) offer array reconfigurability that could substantially enhance sensing performance, but CRB minimization in FIM-enabled ISAC systems hasn't been studied before.

Method: Derived average CRB expression dependent on FIM surface shape. Used average Fisher information maximization as surrogate objective with Gauss-Hermite quadrature for approximation. Decoupled problem into three subproblems: beamforming optimization (using Schur complement and penalty-based SDR), transmit FIM surface optimization (projected gradient algorithm), and receive FIM surface optimization (fixed-point equation method).

Result: Simulation results show that surface shaping of both transmit and receive FIMs significantly reduces average sensing CRB compared to rigid arrays while maintaining communication quality. The approach remains effective even in multi-target scenarios.

Conclusion: This first study on CRB minimization in FIM-enabled ISAC systems demonstrates that flexible surface shaping can substantially enhance sensing performance. The proposed optimization framework effectively reduces sensing error bounds while preserving communication capabilities, making it valuable for future wireless networks.

Abstract: Integrated sensing and communication (ISAC) have been widely recognized as a key enabler for future wireless networks, where the Cramér-Rao bound (CRB) plays a central role in quantifying sensing accuracy.In this paper, we present the first study on CRB minimization in flexible intelligent metasurface (FIM)-enabled ISAC systems.Specifically, we first derive an average CRB expression that explicitly depends on FIM surface shape and demonstrate that array reconfigurability can substantially reduce the CRB, thereby significantly enhancing sensing performance.Moreover, to tackle the challenging CRB minimization problem, we adopt average Fisher information maximization as a surrogate objective and use the Gauss-Hermite quadrature method to obtain an explicit approximation of the objective function.The resulting problem is then decoupled into three subproblem, i.e., beamforming optimization and transmit/receive FIM surface shape optimization.For beamforming optimization, we employ the Schur complement and penalty-based semi-definite relaxation (SDR) technique to solve it.Furthermore, we propose a fixed-point equation method and a projected gradient algorithm to optimize the surface shapes of the receive and transmit FIMs, respectively.Simulation results demonstrate that, compared to rigid arrays, surface shaping of both transmit and receive FIMs can significantly reduce the average sensing CRB while maintaining communication quality, and remains effective even in multi-target scenarios.

</details>


### [161] [Log-Likelihood Loss for Semantic Compression](https://arxiv.org/abs/2601.16461)
*Anuj Kumar Yadav,Dan Song,Yanina Shkel,Ayfer Özgür*

Main category: cs.IT

TL;DR: The paper studies lossy source coding with log-likelihood distortion, where reconstruction is a semantic representation that probabilistically generates the source rather than pointwise approximation.


<details>
  <summary>Details</summary>
Motivation: To model compression scenarios where reconstructions serve as semantic representations that can probabilistically generate the source, rather than providing pointwise approximations. This addresses semantic compression needs beyond traditional distortion measures.

Method: Formulates rate-distortion problem with log-likelihood distortion measure defined by negative log-likelihood induced by prescribed conditional distribution P_{X|U}. Analyzes fundamental properties of resulting rate-distortion function.

Result: Characterizes fundamental properties of the rate-distortion function, establishes connections to lossy compression under log-loss, classical rate-distortion problems with arbitrary distortion measures, and rate-distortion with perfect perception.

Conclusion: The log-likelihood distortion framework provides a principled approach for semantic compression where reconstructions serve as generative representations, bridging connections to existing rate-distortion theories.

Abstract: We study lossy source coding under a distortion measure defined by the negative log-likelihood induced by a prescribed conditional distribution $P_{X|U}$. This \emph{log-likelihood distortion} models compression settings in which the reconstruction is a semantic representation from which the source can be probabilistically generated, rather than a pointwise approximation. We formulate the corresponding rate-distortion problem and characterize fundamental properties of the resulting rate-distortion function, including its connections to lossy compression under log-loss, classical rate-distortion problems with arbitrary distortion measures, and rate-distortion with perfect perception.

</details>


### [162] [Load Balanced ISAC Systems for URLLC Users](https://arxiv.org/abs/2601.16495)
*Shivani Singh,Amudheesan Nakkeeran,Prem Singh,Ekant Sharma,Jyotsna Bapat*

Main category: cs.IT

TL;DR: Energy-efficient downlink cell-free massive MIMO ISAC network for URLLC users and target detection with load balancing algorithm reducing power consumption by 33%.


<details>
  <summary>Details</summary>
Motivation: To design an energy-efficient cell-free massive MIMO ISAC network that simultaneously serves URLLC users and detects targets while minimizing total power consumption without degrading system performance.

Method: Proposed a joint power allocation and AP load balancing (JPALB) algorithm that solves a mixed-integer non-convex optimization problem to minimize total network power (transmit, static, and traffic-dependent fronthaul power) while meeting URLLC QoS and sensing requirements.

Result: Simulation results show approximately 33% reduction in power consumption using JPALB algorithm compared to baseline without load balancing, while maintaining communication and sensing QoS requirements.

Conclusion: The proposed JPALB algorithm effectively reduces power consumption in CF-mMIMO ISAC networks by optimizing power allocation and AP load balancing, achieving significant energy savings without compromising system performance for URLLC and sensing applications.

Abstract: This paper presents an energy-efficient downlink cell-free massive multiple-input multiple-output (CF-mMIMO) integrated sensing and communication (ISAC) network that serves ultra-reliable low-latency communication (URLLC) users while simultaneously detecting a target. We propose a load-balancing algorithm that minimizes the total network power consumption; including transmit power, fixed static power, and traffic-dependent fronthaul power at the access points (APs) without degrading system performance. To this end, we formulate a mixed-integer non-convex optimization problem and introduce an iterative joint power allocation and AP load balancing (JPALB) algorithm. The algorithm aims to reduce total power usage while meeting both the communication quality-of-service (QoS) requirements of URLLC users and the sensing QoS needed for target detection. Proposed JPALB algorithm for ISAC systems was simulated with maximum-ratio transmission (MRT) and regularized zero-forcing (RZF) precoders. Simulation results show approximately 33% reduction in power consumption, using JPALB algorithm compared to a baseline with no load balancing, without compromising communication and sensing QoS requirements.

</details>


### [163] [Noise-immune and AI-enhanced DNA storage via adaptive partition mapping of digital data](https://arxiv.org/abs/2601.16518)
*Zimu Li,Bingyi Liu,Lei Zhao,Qian Zhang,Yang Liu,Jun Liu,Ke Ke,Huating Kong,Xiaolei Zuo,Chunhai Fan,Fei Wang*

Main category: cs.IT

TL;DR: PJ encoding scheme enables robust DNA data storage with exceptional noise resilience, allowing file recovery under any strand loss ratio and extreme environmental conditions.


<details>
  <summary>Details</summary>
Motivation: DNA storage faces practical limitations from errors during synthesis, preservation, and sequencing, with traditional error-correcting codes failing when noise exceeds thresholds. Need for more resilient encoding for archival storage.

Method: Developed Partitioning-mapping with Jump-rotating (PJ) encoding scheme: 1) Partitioning removes cross-strand dependencies, 2) Jump-rotating strategy relaxes sequence constraints with adjustable jump length for tunable density, 3) AI-based inference enables controllable recovery.

Result: Files always decodable under any strand loss ratio with smooth fidelity degradation. 10% strand loss recovery demonstrated. Machine learning datasets retain classification performance. Successful decoding after accelerated aging and high-intensity X-ray irradiation.

Conclusion: PJ eliminates reliance on prior error probabilities, establishing a general framework for robust archival DNA storage capable of withstanding real-world preservation conditions.

Abstract: Encoding digital information into DNA sequences offers an attractive potential solution for storing rapidly growing data under the information age and the rise of artificial intelligence. However, practical implementations of DNA storage are constrained by errors introduced during synthesis, preservation, and sequencing processes, and traditional error-correcting codes remain vulnerable to noise levels that exceed predefined thresholds. Here, we developed a Partitioning-mapping with Jump-rotating (PJ) encoding scheme, which exhibits exceptional noise resilience. PJ removes cross-strand information dependencies so that strand loss manifests as localized gaps rather than catastrophic file failure. It prioritizes file decodability under arbitrary noise conditions and leverages AI-based inference to enable controllable recovery of digital information. For the intra-strand encoding, we develop a jump-rotating strategy that relaxes sequence constraints relative to conventional rotating codes and provides tunable information density via an adjustable jump length. Based on this encoding architecture, the original file information can always be decoded and recovered under any strand loss ratio, with fidelity degrading smoothly as damage increases. We demonstrate that original files can be effectively recovered even with 10% strand loss, and machine learning datasets stored under these conditions retain their classification performance. Experiments further confirmed that PJ successfully decodes image files after extreme environmental disturbance using accelerated aging and high-intensity X-ray irradiation. By eliminating reliance on prior error probabilities, PJ establishes a general framework for robust, archival DNA storage capable of withstanding the rigorous conditions of real-world preservation.

</details>


### [164] [Generalized Forms of the Kraft Inequality for Finite-State Encoders](https://arxiv.org/abs/2601.16594)
*Neri Merhav*

Main category: cs.IT

TL;DR: The paper extends the Kraft inequality for finite-state encoders, introducing the concept of a Kraft matrix and showing that for information lossless encoders, the spectral radius of this matrix cannot exceed one.


<details>
  <summary>Details</summary>
Motivation: To develop generalized versions of the Kraft inequality that apply to finite-state encoders, addressing limitations of the classical Kraft inequality for more complex encoding schemes with state dependencies.

Method: Defines a Kraft matrix for finite-state encoders and establishes that information losslessness requires the spectral radius of this matrix to be ≤1. For irreducible encoders, derives equivalent forms using spectral radius formulas. Extends results to cases with side information and lossy compression.

Result: Shows that for irreducible finite-state encoders, Kraft sums are bounded by a constant independent of block length, preventing any subexponential growth. Provides necessary conditions for information losslessness based on spectral properties of the Kraft matrix.

Conclusion: The generalized Kraft inequality provides fundamental limitations for finite-state encoders, with spectral radius conditions serving as necessary criteria for information losslessness, extending classical coding theory results to more practical encoder structures.

Abstract: We derive a few extended versions of the Kraft inequality for information lossless finite-state encoders. The main basic contribution is in defining a notion of a Kraft matrix and in establishing the fact that a necessary condition for information losslessness of a finite-state encoder is that none of the eigenvalues of this matrix have modulus larger than unity, or equivalently, the generalized Kraft inequality asserts that the spectral radius of the Kraft matrix cannot exceed one. For the important special case where the FS encoder is irreducible, we derive several equivalent forms of this inequality, which are based on well known formulas for spectral radius. It also turns out that in the irreducible case, Kraft sums are bounded by a constant, independent of the block length, and thus cannot grow even in any subexponential rate. Finally, two extensions are outlined - one concerns the case of side information available to both encoder and decoder, and the other is for lossy compression.

</details>


### [165] [An Explicit Upper Bound of Generalized Quadratic Gauss Sums and Its Applications for Asymptotically Optimal Aperiodic Polyphase Sequence Design](https://arxiv.org/abs/2601.16599)
*Huaning Liu,Zilong Liu*

Main category: cs.IT

TL;DR: This paper presents systematic constructions of asymptotically order-optimal aperiodic polyphase sequence sets that approach the Welch bound, solving a 30-year-old open problem.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the long-standing open problem of designing asymptotically order-optimal aperiodic polyphase sequence sets with respect to the Welch bound, which has remained unsolved for over 30 years despite attempts by researchers like Mow.

Method: The method involves: 1) Deriving an explicit upper bound for generalized quadratic Gauss sums using recursive application of Paris' asymptotic expansion and bounding via the fast convergence property of the Fibonacci zeta function; 2) Four systematic constructions using carefully selected Chu sequences and Alltop sequences to create order-optimal sequence sets with low aperiodic correlation and/or ambiguity properties.

Result: Key results include: 1) First proof that the full Alltop sequence set is asymptotically optimal for low aperiodic correlation sidelobes; 2) Introduction of a novel subset of Alltop sequences with both order-optimal aperiodic correlation and ambiguity properties for the entire time-shift window; 3) Four systematic constructions of order-optimal sequence sets.

Conclusion: The paper provides a comprehensive solution to the 30-year-old problem of designing asymptotically order-optimal aperiodic polyphase sequence sets, with novel theoretical contributions on generalized quadratic Gauss sums and practical constructions using Chu and Alltop sequences that achieve optimal performance with respect to the Welch bound.

Abstract: This work is motivated by the long-standing open problem of designing asymptotically order-optimal aperiodic polyphase sequence sets with respect to the celebrated Welch bound. Attempts were made by Mow over 30 years ago, but a comprehensive understanding to this problem is lacking. Our first key contribution is an explicit upper bound of generalized quadratic Gauss sums which is obtained by recursively applying Paris' asymptotic expansion and then bounding it by leveraging the fast convergence property of the Fibonacci zeta function. Building upon this major finding, our second key contribution includes four systematic constructions of order-optimal sequence sets with low aperiodic correlation and/or ambiguity properties via carefully selected Chu sequences and Alltop sequences. For the first time in the literature, we reveal that the full Alltop sequence set is asymptotically optimal for its low aperiodic correlation sidelobes. Besides, we introduce a novel subset of Alltop sequences possessing both order-optimal aperiodic correlation and ambiguity properties for the entire time-shift window.

</details>


### [166] [Term Coding: An Entropic Framework for Extremal Combinatorics and the Guessing--Number Sandwich Theorem](https://arxiv.org/abs/2601.16614)
*Søren Riis*

Main category: cs.IT

TL;DR: Term Coding studies maximum solution set sizes for term identities on n-element alphabets, connecting to extremal combinatorics via guessing-number sandwich theorem.


<details>
  <summary>Details</summary>
Motivation: To transform familiar existence problems for quasigroups, designs, and related combinatorial objects into quantitative extremal questions by studying how large solution sets can be for term identities when function symbols can be freely interpreted.

Method: Prove a guessing-number sandwich theorem connecting term coding to graph guessing numbers (graph entropy). Use explicit normalisation and diversification reductions to obtain canonical directed dependency structures with guessing number α. Apply entropy and polymatroid methods to bound or compute α.

Result: Maximum code size satisfies log_n S_n(Γ) = α + o(1) (equivalently S_n(Γ) = n^{α+o(1)}), where α is the guessing number of the canonical dependency structure. The framework works for examples from extremal combinatorics (Steiner-type identities, self-orthogonal Latin squares) and information-flow/network-coding constraints.

Conclusion: Term coding provides a unified framework connecting combinatorial existence problems to quantitative extremal questions via guessing numbers and entropy methods, with applications across extremal combinatorics and network coding.

Abstract: Term Coding asks: given a finite system of term identities $Γ$ in $v$ variables, how large can its solution set be on an $n$--element alphabet, when we are free to choose the interpretations of the function symbols? This turns familiar existence problems for quasigroups, designs, and related objects into quantitative extremal questions.
  We prove a guessing-number sandwich theorem that connects term coding to graph guessing numbers (graph entropy). After explicit normalisation and diversification reductions, every instance yields a canonical directed dependency structure with guessing number $α$ such that the maximum code size satisfies $\log_n \Sn(Γ)=α+o(1)$ (equivalently, $\Sn(Γ)=n^{α+o(1)}$), and $α$ can be bounded or computed using entropy and polymatroid methods.
  We illustrate the framework with examples from extremal combinatorics (Steiner-type identities, self-orthogonal Latin squares) and from information-flow / network-coding style constraints (including a five-cycle instance with fractional exponent and small storage/relay maps).

</details>


### [167] [Taming the Heavy Tail: Age-Optimal Preemption](https://arxiv.org/abs/2601.16624)
*Aimin Li,Yiğit İnce,Elif Uysal*

Main category: cs.IT

TL;DR: Continuous-time joint sampling-and-preemption problem with sampling/preemption penalties under general service distributions, solved via PDMP formulation and policy iteration, showing 30x cost reduction in heavy-tailed regimes.


<details>
  <summary>Details</summary>
Motivation: To address the joint optimization of sampling and preemption in continuous-time systems with general service-time distributions, incorporating both sampling and preemption penalties, which is important for information freshness applications like status updates.

Method: Formulate as impulse-controlled piecewise-deterministic Markov process (PDMP), derive coupled integral average-cost optimality equations via dynamic programming, reduce preemption control to optimal stopping problem, and develop efficient policy iteration algorithm with heavy-tail acceleration using hybrid action grid and far-field linear closure.

Result: Simulations under Pareto and log-normal service times show substantial improvements over AoI-optimal non-preemptive sampling and zero-wait baselines, achieving up to 30x reduction in average cost in heavy-tailed regimes. Counterintuitively, delay variance can become a strategic advantage for information freshness under preemption.

Conclusion: The proposed approach effectively solves the joint sampling-and-preemption problem under general service distributions, demonstrating significant performance gains especially in heavy-tailed regimes, and reveals the surprising insight that delay variance can be beneficial for information freshness when preemption is allowed.

Abstract: This paper studies a continuous-time joint sampling-and-preemption problem, incorporating sampling and preemption penalties under general service-time distributions. We formulate the system as an impulse-controlled piecewise-deterministic Markov process (PDMP) and derive coupled integral average-cost optimality equations via the dynamic programming principle, thereby avoiding the smoothness assumptions typically required for an average-cost Hamilton-Jacobi-Bellman quasi-variational inequality (HJB-QVI) characterization. A key invariance in the busy phase collapses the dynamics onto a one-dimensional busy-start boundary, reducing preemption control to an optimal stopping problem. Building on this structure, we develop an efficient policy iteration algorithm with heavy-tail acceleration, employing a hybrid (uniform/log-spaced) action grid and a far-field linear closure. Simulations under Pareto and log-normal service times demonstrate substantial improvements over AoI-optimal non-preemptive sampling and zero-wait baselines, achieving up to a 30x reduction in average cost in heavy-tailed regimes. Finally, simulations uncover a counterintuitive insight: under preemption, delay variance, despite typically being a liability, can become a strategic advantage for information freshness.

</details>


### [168] [The Oval Strikes Back](https://arxiv.org/abs/2601.16628)
*Andrea Di Giusto,Alberto Ravagnani,Emina Soljanin*

Main category: cs.IT

TL;DR: Ovals in projective planes applied to distributed storage, yielding non-systematic MDS matrices with many small disjoint recovery sets that outperform systematic matrices in service rate region.


<details>
  <summary>Details</summary>
Motivation: To explore how classical finite geometry objects (ovals in projective planes) can be applied to modern distributed storage problems, specifically the Service Rate Region problem, to improve storage system performance.

Method: Leverage incidence relations between lines and ovals in projective planes to construct a class of non-systematic MDS matrices with many small and disjoint recovery sets.

Result: For certain parameters, the service-rate region of these oval-based matrices contains that of systematic generator matrices for the same code, providing better service performance. The construction also enables analysis of PIR properties and development of a one-step majority-logic decoding algorithm with strong error-correction capability.

Conclusion: Ovals, a classical finite geometry object, re-emerge as a valuable tool in modern coding theory, demonstrating the continued relevance of geometric structures in solving contemporary distributed storage problems.

Abstract: We investigate the applications of ovals in projective planes to distributed storage, with a focus on the Service Rate Region problem. Leveraging the incidence relations between lines and ovals, we describe a class of non-systematic MDS matrices with a large number of small and disjoint recovery sets. For certain parameter choices, the service-rate region of these matrices contains the region of a systematic generator matrix for the same code, yielding better service performance. We further apply our construction to analyze the PIR properties of the considered MDS matrices and present a one-step majority-logic decoding algorithm with strong error-correcting capability. These results highlight how ovals, a classical object in finite geometry, re-emerge as a useful tool in modern coding theory.

</details>


### [169] [Stable Source Coding](https://arxiv.org/abs/2601.16680)
*Zhenduo Wen,Amin Gohari*

Main category: cs.IT

TL;DR: Study of stable lossless source codes where small source changes cause bounded output changes, unlike random binning which is unstable.


<details>
  <summary>Details</summary>
Motivation: Random binning is unstable because nearly identical source sequences can get completely unrelated bin indices. Need to study compression rates for stable codes where small source changes result in bounded output changes.

Method: Using combinatorial arguments to derive information-theoretic limits on achievable rate as function of stability parameters.

Result: Derived fundamental limits on compression rates for stable lossless source codes based on stability parameters.

Conclusion: Established theoretical bounds for stable source coding, showing trade-offs between stability requirements and compression efficiency.

Abstract: A source encoder is stable if a small change in the source sequence (e.g., changing a few symbols) results in a small (or bounded) change in the output codeword. By this definition, the common technique of random binning is unstable; because the mapping is random, two nearly identical source sequences can be assigned to completely unrelated bin indices. We study compression rates of stable lossless source codes. Using combinatorial arguments, we derive information-theoretic limits on the achievable rate as a function of the stability parameters.

</details>


### [170] [Adaptive Beam Alignment using Noisy Twenty Questions Estimation with Trained Questioner](https://arxiv.org/abs/2601.16799)
*Chunsong Sun,Lin Zhou*

Main category: cs.IT

TL;DR: Proposes adaptive beam alignment algorithm using noisy twenty questions estimation with trained questioner to overcome limitations of prior methods in 6G mmWave MIMO systems.


<details>
  <summary>Details</summary>
Motivation: Traditional sector-search beam alignment causes high latency in 6G systems. Recent adaptive methods either lack feasibility due to ideal assumptions or lack interpretability due to black-box neural networks.

Method: Uses noisy twenty questions estimation framework with trained questioner. Two training methods: 1) mapping queries to beamforming vectors via weighted summation of steering vectors, 2) using multi-layer fully connected neural networks only for training the questioner.

Result: Numerical simulations show the proposed algorithms outperform all benchmark algorithms in effectiveness.

Conclusion: The approach avoids ideal assumptions while maintaining interpretability, addressing both feasibility and interpretability issues of prior methods in adaptive beam alignment for 6G systems.

Abstract: The 6G communication systems use mmWave and MIMO technologies to achieve wide bandwidth and high throughout, leading to indispensable need for beam alignment to overcome severe signal attenuation. Traditional sector-search-based beam alignment algorithms rely on sequential sampling to identify the best sector, resulting in a significant latency burden on 6G communication systems. Recently proposed adaptive beam alignment algorithms based on the active learning framework address the problem, aiming to identify the optimal sector with the fewest possible samples under an identical sector partition. Nevertheless, these algorithms either lack feasibility (Chiu, Ronquillo and Javidi, JSAC 2019) due to ideal assumptions or lack interpretability (Sohrabi, Chen and Yu, JSAC 2021) due to the use of end-to-end black-box neural networks. To avoid ideal assumptions and maintain interpretability, we address all above problems by proposing an adaptive beam alignment algorithm using the framework of noisy twenty questions estimation with a trained questioner. Specifically, we use two methods for training the questioner to eliminate reliance on ideal assumptions. The first method maps queries of twenty questions estimation to beamforming vectors via weighted summation of steering vectors, as an initial attempt to address the feasibility problem encountered in prior pioneering study by Chiu, Ronquillo and Javidi (JSAC 2019). The second method uses multi-layer fully connected neural networks to achieve improved performance while only employing them to train the questioner, which can effectively mitigate the interpretability issues in prior study by Sohrabi, Chen and Yu (JSAC 2021). Furthermore, we provide numerical simulations to illustrate the effectiveness of our proposed adaptive beam alignment algorithms and demonstrate that our algorithms outperform all benchmark algorithms.

</details>


### [171] [Privacy-Resolution Tradeoff for Adaptive Noisy Twenty Questions Estimation](https://arxiv.org/abs/2601.16825)
*Chunsong Sun,Lin Zhou*

Main category: cs.IT

TL;DR: The paper studies privacy-resolution tradeoffs in noisy twenty questions estimation with adaptive querying, proposing a two-stage private query procedure that outperforms previous noiseless methods.


<details>
  <summary>Details</summary>
Motivation: Adaptive query procedures in twenty questions estimation provide better performance but raise privacy concerns. Previous work studied privacy in noiseless settings, but real-world scenarios involve noisy responses, creating a need to extend privacy analysis to noisy cases.

Method: Proposes a two-stage private query procedure for noisy twenty questions estimation. Analyzes non-asymptotic and second-order asymptotic achievable performance, and discusses the impact of privacy constraints on estimation accuracy.

Result: The proposed private query procedure generalizes previous noiseless results to noisy settings. When specialized to noiseless cases, it achieves better performance than previous methods from COLT 2018 and AISTATS 2021.

Conclusion: The paper successfully addresses privacy concerns in noisy adaptive querying, providing a framework that balances privacy and resolution tradeoffs while improving upon existing noiseless methods.

Abstract: We revisit noisy twenty questions estimation and study the privacy-resolution tradeoff for adaptive query procedures. Specifically, in twenty questions estimation, there are two players: an oracle and a questioner. The questioner aims to estimate target variables by posing queries to the oracle that knows the variables and using noisy responses to form reliable estimates. Typically, there are adaptive and non-adaptive query procedures. In adaptive querying, one designs the current query using previous queries and their noisy responses while in non-adaptive querying, all queries are posed simultaneously. Generally speaking, adaptive query procedures yield better performance. However, adaptive querying leads to privacy concerns, which were first studied by Tsitsiklis, Xu and Xu (COLT 2018) and by Xu, Xu and Yang (AISTATS 2021) for the noiseless case, where the oracle always provides correct answers to queries. In this paper, we generalize the above results to the more practical noisy case, by proposing a two-stage private query procedure, analyzing its non-asymptotic and second-order asymptotic achievable performance and discussing the impact of privacy concerns. Furthermore, when specialized to the noiseless case, our private query procedure achieves better performance than above-mentioned query procedures (COLT 2018, AISTATS 2021).

</details>


### [172] [Information Contraction under $(\varepsilon,δ)$-Differentially Private Mechanisms](https://arxiv.org/abs/2601.16845)
*Theshani Nuradha,Ian George,Christoph Hirche*

Main category: cs.IT

TL;DR: The paper derives improved strong data-processing inequalities for hockey-stick divergence and f-divergences under (ε,δ)-local differential privacy mechanisms, generalizing previous bounds that only worked for (ε,0)-LDP.


<details>
  <summary>Details</summary>
Motivation: Previous characterizations of information measure contraction were limited to (ε,0)-LDP mechanisms, leaving a gap for the more general (ε,δ)-LDP case where δ≠0. The authors aim to provide bounds that work for all (ε,δ)-LDP mechanisms.

Method: The authors derive both linear and non-linear strong data-processing inequalities (SDPIs) for hockey-stick divergence and f-divergences. These inequalities characterize how these distinguishability measures contract when processed through (ε,δ)-LDP mechanisms.

Result: The paper obtains bounds that either generalize or improve previously known bounds on the contraction of hockey-stick divergence and f-divergences for (ε,δ)-LDP mechanisms, including cases where δ≠0.

Conclusion: The derived SDPIs provide a more complete theoretical foundation for analyzing privacy mechanisms, enabling better characterization of information contraction under the broader class of (ε,δ)-LDP mechanisms.

Abstract: The distinguishability quantified by information measures after being processed by a private mechanism has been a useful tool in studying various statistical and operational tasks while ensuring privacy. To this end, standard data-processing inequalities and strong data-processing inequalities (SDPI) are employed. Most of the previously known and even tight characterizations of contraction of information measures, including total variation distance, hockey-stick divergences, and $f$-divergences, are applicable for $(\varepsilon,0)$-local differential private (LDP) mechanisms. In this work, we derive both linear and non-linear strong data-processing inequalities for hockey-stick divergence and $f$-divergences that are valid for all $(\varepsilon,δ)$-LDP mechanisms even when $δ\neq 0$. Our results either generalize or improve the previously known bounds on the contraction of these distinguishability measures.

</details>


### [173] [Perfect Privacy and Strong Stationary Times for Markovian Sources](https://arxiv.org/abs/2601.16857)
*Fangwei Ye,Zonghong Liu,Parimal Parag,Salim El Rouayheb*

Main category: cs.IT

TL;DR: Perfect privacy-preserving data sharing using redaction mechanisms for Markov chain data, achieving optimal distortion with constant average redactions independent of data length.


<details>
  <summary>Details</summary>
Motivation: To share correlated data while maintaining perfect information-theoretic privacy, focusing on protecting the initial state of Markov chain-generated data while maximizing the amount of shared data.

Method: Uses redaction (erasure) mechanisms where data are either withheld or released unchanged. Studies window-based redaction schemes that erase data up to a strong stationary time, and develops an optimal sequential redaction mechanism with equivalent window interpretation.

Result: Establishes connection between perfect privacy and window-based redaction schemes. Shows both mechanisms achieve optimal distortion while redacting only a constant average number of data points, independent of data length N.

Conclusion: Perfect privacy can be achieved for Markov chain data using redaction mechanisms with constant average redactions, providing efficient privacy-preserving data sharing with optimal utility.

Abstract: We consider the problem of sharing correlated data under a perfect information-theoretic privacy constraint. We focus on redaction (erasure) mechanisms, in which data are either withheld or released unchanged, and measure utility by the average cardinality of the released set, equivalently, the expected Hamming distortion. Assuming the data are generated by a finite time-homogeneous Markov chain, we study the protection of the initial state while maximizing the amount of shared data. We establish a connection between perfect privacy and window-based redaction schemes, showing that erasing data up to a strong stationary time preserves privacy under suitable conditions. We further study an optimal sequential redaction mechanism and prove that it admits an equivalent window interpretation. Interestingly, we show that both mechanisms achieve the optimal distortion while redacting only a constant average number of data points, independent of the data length~$N$.

</details>
