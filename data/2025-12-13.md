<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 8]
- [cs.LG](#cs.LG) [Total: 12]
- [cs.AI](#cs.AI) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence](https://arxiv.org/abs/2512.10863)
*Jingli Lin,Runsen Xu,Shaohao Zhu,Sihan Yang,Peizhou Cao,Yunlong Ran,Miao Hu,Chenming Zhu,Yiman Xie,Yilin Long,Wenbo Hu,Dahua Lin,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: MMSI-Video-Bench is a comprehensive human-annotated benchmark for evaluating video-based spatial intelligence in multimodal large language models, covering perception, planning, prediction, and cross-video reasoning across 1,106 questions from diverse video sources.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs lack comprehensive evaluation for spatial understanding in continuous visual input, which is crucial for them to become effective assistants in physical environments. There's no holistic benchmark to assess progress toward this goal.

Method: Created a fully human-annotated benchmark with 1,106 questions grounded in 1,278 video clips from 25 existing datasets plus in-house videos. Questions follow a four-level framework (Perception, Planning, Prediction, Cross-Video Reasoning) and were designed/reviewed by 3DV experts with explanatory rationales. Also includes three domain-oriented sub-benchmarks for targeted assessment.

Result: Evaluation of 25 strong MLLMs reveals a significant human-AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. Spatially fine-tuned models fail to generalize effectively. Error analysis shows systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence.

Conclusion: The benchmark establishes a solid testbed for advancing video-based spatial intelligence, revealing current limitations in MLLMs and showing that typical frame-sampling strategies, 3D spatial cues, and chain-of-thought prompting don't yield meaningful gains on this reasoning-intensive benchmark.

Abstract: Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.

</details>


### [2] [BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models](https://arxiv.org/abs/2512.10932)
*Shengao Wang,Wenqi Wang,Zecheng Wang,Max Whitton,Michael Wakeham,Arjun Chandra,Joey Huang,Pengyue Zhu,Helen Chen,David Li,Jeffrey Li,Shawn Li,Andrew Zagula,Amy Zhao,Andrew Zhu,Sayaka Nakamura,Yuki Yamamoto,Jerry Jun Yokono,Aaron Mueller,Bryan A. Plummer,Kate Saenko,Venkatesh Saligrama,Boqing Gong*

Main category: cs.CV

TL;DR: BabyVLM-V2 is a developmentally grounded vision-language model framework that improves upon V1 with longitudinal infant-centric pretraining data and DevCV Toolbox for cognitive evaluation, achieving competitive performance on infant-level tasks.


<details>
  <summary>Details</summary>
Motivation: Early children's developmental trajectories provide a natural goal for sample-efficient pretraining of vision foundation models, aiming to create more developmentally plausible AI systems.

Method: Uses a longitudinal, multifaceted pretraining set from infant-centric audiovisual corpus (video-utterance, image-utterance, multi-turn conversations), plus DevCV Toolbox adapting NIH Baby Toolbox measures into 10 multimodal tasks for evaluation.

Result: A compact model pretrained from scratch achieves competitive performance on DevCV Toolbox, outperforming GPT-4o on some spatial reasoning, memory, and vocabulary understanding tasks.

Conclusion: BabyVLM-V2 provides a principled, unified framework to accelerate research in developmentally plausible pretraining of vision foundation models.

Abstract: Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.

</details>


### [3] [Any4D: Unified Feed-Forward Metric 4D Reconstruction](https://arxiv.org/abs/2512.10935)
*Jay Karhade,Nikhil Keetha,Yuchen Zhang,Tanisha Gupta,Akash Sharma,Sebastian Scherer,Deva Ramanan*

Main category: cs.CV

TL;DR: Any4D is a scalable multi-view transformer for dense 4D reconstruction that directly predicts per-pixel motion and geometry across N frames, supporting multiple modalities and achieving 2-3x lower error with 15x faster computation.


<details>
  <summary>Details</summary>
Motivation: Prior work has limitations: either focuses on 2-view dense scene flow or sparse 3D point tracking, and recent 4D reconstruction methods from monocular RGB videos lack flexibility for additional modalities like RGB-D, IMU, and Radar Doppler measurements.

Method: Uses a scalable multi-view transformer architecture with modular 4D scene representation: egocentric factors (depthmaps, camera intrinsics) in local camera coordinates and allocentric factors (camera extrinsics, scene flow) in global world coordinates. Processes N frames directly for per-pixel motion and geometry predictions.

Result: Achieves superior performance with 2-3x lower error and 15x faster computation compared to prior methods across diverse setups, supporting multiple modalities when available.

Conclusion: Any4D provides a flexible, efficient framework for dense 4D reconstruction that opens avenues for multiple downstream applications by supporting various sensor modalities and achieving significant improvements in both accuracy and computational efficiency.

Abstract: We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.

</details>


### [4] [OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis](https://arxiv.org/abs/2512.10940)
*Xiang Fan,Sharath Girish,Vivek Ramanujan,Chaoyang Wang,Ashkan Mirzaei,Petr Sushko,Aliaksandr Siarohin,Sergey Tulyakov,Ranjay Krishna*

Main category: cs.CV

TL;DR: OmniView is a unified diffusion framework for 4D consistency tasks that generalizes across novel view synthesis, text-to-video with camera control, and image-to-video by separately representing space, time, and view conditions.


<details>
  <summary>Details</summary>
Motivation: Prior approaches are fragmented and task-specific, trained on disjoint slices of 3D/4D data, lacking a unified solution for various 4D consistency tasks.

Method: Separately represents space, time, and view conditions to enable flexible combinations of inputs, allowing the model to handle static/dynamic/multiview inputs, trajectory extrapolation, and text/image prompts with camera control.

Result: Competitive with task-specific models, improving image quality scores by up to 33% in multiview NVS, 60% in dynamic NVS, 20% in static camera control, and reducing camera trajectory errors by 4x in text-conditioned video generation.

Conclusion: Demonstrates feasibility of a generalist 4D video model with strong generalizability across diverse 4D consistency tasks in a single unified framework.

Abstract: Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\% in multiview NVS LLFF dataset, 60\% in dynamic NVS Neural 3D Video benchmark, 20\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/

</details>


### [5] [Mull-Tokens: Modality-Agnostic Latent Thinking](https://arxiv.org/abs/2512.10941)
*Arijit Ray,Ahmed Abdelkader,Chengzhi Mao,Bryan A. Plummer,Kate Saenko,Ranjay Krishna,Leonidas Guibas,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: Mull-Tokens introduces modality-agnostic latent tokens that allow models to reason in both text and image modalities without relying on specialist tools or costly image generation, achieving improved performance on spatial reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Real-world reasoning requires multimodal thinking about space, time, and affordances that words alone cannot convey. Existing multimodal models are brittle, don't scale, and rely on costly specialist tools, image generation, or handcrafted data for modality switching.

Method: Mull-Tokens are modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities. They're trained using supervision from interleaved text-image traces, then fine-tuned without supervision using only final answers, inspired by latent reasoning frameworks.

Result: Across four challenging spatial reasoning benchmarks (puzzle solving, perspective-taking), Mull-Tokens outperform baselines using text-only or interleaved image-text reasoning, achieving +3% average improvement and up to +16% on puzzle solving reasoning-heavy split.

Conclusion: Mull-Tokens offer a simple solution for abstract multimodal reasoning, enabling models to think free-form in multiple modalities without costly tool calls or image generation, advancing conversations around grounding textual and visual reasoning.

Abstract: Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.

</details>


### [6] [AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation](https://arxiv.org/abs/2512.10943)
*Sharath Girish,Viacheslav Ivanov,Tsai-Shien Chen,Hao Chen,Aliaksandr Siarohin,Sergey Tulyakov*

Main category: cs.CV

TL;DR: AlcheMinT enables precise temporal control over subject appearance/disappearance in subject-driven video generation using timestamp conditioning and positional encoding.


<details>
  <summary>Details</summary>
Motivation: Existing subject-driven video generation methods lack fine-grained temporal control over subject appearance and disappearance, which is essential for applications like compositional video synthesis, storyboarding, and controllable animation.

Method: Introduces explicit timestamps conditioning with novel positional encoding for temporal intervals, subject-descriptive text tokens for identity binding, and token-wise concatenation without additional cross-attention modules.

Result: Achieves visual quality matching state-of-the-art video personalization methods while enabling precise temporal control over multi-subject generation within videos.

Conclusion: AlcheMinT provides a unified framework for subject-driven video generation with fine-grained temporal control, addressing limitations of existing methods for applications requiring precise timing of subject appearances.

Abstract: Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT

</details>


### [7] [Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation](https://arxiv.org/abs/2512.10949)
*Yiwen Tang,Zoey Guo,Kaixin Zhu,Ray Zhang,Qizhi Chen,Dongzhi Jiang,Junli Liu,Bohan Zeng,Haoming Song,Delin Qu,Tianyi Bai,Dan Xu,Wentao Zhang,Bin Zhao*

Main category: cs.CV

TL;DR: First systematic study of RL for text-to-3D autoregressive generation, introducing AR3D-R1 as the first RL-enhanced text-to-3D model with hierarchical optimization.


<details>
  <summary>Details</summary>
Motivation: RL has been effective for 2D image generation but remains unexplored for 3D due to higher spatial complexity requiring globally consistent geometry and fine-grained local textures, making 3D generation sensitive to reward designs and RL algorithms.

Method: Systematic study across four dimensions: (1) Reward designs evaluating dimensions and model choices, (2) RL algorithms studying GRPO variants and token-level optimization, (3) Introducing MME-3DR benchmark for implicit reasoning abilities, (4) Proposing Hi-GRPO for hierarchical global-to-local optimization with reward ensembles.

Result: Developed AR3D-R1, the first RL-enhanced text-to-3D model that excels from coarse shape to texture refinement, with code released publicly.

Conclusion: This study provides insights into RL-driven reasoning for 3D generation, showing that alignment with human preference is crucial and that general multi-modal models provide robust signal for 3D attributes.

Abstract: Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.

</details>


### [8] [SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model](https://arxiv.org/abs/2512.10957)
*Yukai Shi,Weiyu Li,Zihao Wang,Hongyang Li,Xingyu Chen,Ping Tan,Lei Zhang*

Main category: cs.CV

TL;DR: SceneMaker: A decoupled 3D scene generation framework that separates de-occlusion from object generation and uses unified pose estimation to handle severe occlusion in open-set scenes.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with producing high-quality geometry and accurate poses simultaneously under severe occlusion and open-set settings due to insufficient de-occlusion and pose estimation priors.

Method: 1) Decouple de-occlusion model from 3D object generation, enhanced with image datasets and collected de-occlusion datasets. 2) Propose unified pose estimation model integrating global and local mechanisms for attention. 3) Construct open-set 3D scene dataset to improve generalization.

Result: Comprehensive experiments demonstrate superiority on both indoor and open-set scenes. Framework shows improved performance in handling occlusion and open-set scenarios.

Conclusion: SceneMaker's decoupled approach effectively addresses occlusion challenges in 3D scene generation, with released codes and datasets for community use.

Abstract: We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [UrbanAI 2025 Challenge: Linear vs Transformer Models for Long-Horizon Exogenous Temperature Forecasting](https://arxiv.org/abs/2512.10866)
*Ruslan Gokhman*

Main category: cs.LG

TL;DR: Linear models outperform Transformers for long-horizon temperature forecasting using only past temperature data, with DLinear achieving best accuracy.


<details>
  <summary>Details</summary>
Motivation: To evaluate forecasting performance in challenging exogenous-only settings where only past temperature values are available for prediction, comparing simple linear models against complex Transformer architectures.

Method: Standardized evaluation of Linear, NLinear, DLinear, Transformer, Informer, and Autoformer models using train/validation/test splits on long-horizon temperature forecasting with only past temperature data.

Result: Linear baselines consistently outperform Transformer-family models, with DLinear achieving the best overall accuracy across all splits.

Conclusion: Carefully designed linear models remain strong baselines for time series forecasting in challenging exogenous-only settings, outperforming more complex Transformer architectures.

Abstract: We study long-horizon exogenous-only temperature forecasting - a challenging univariate setting where only the past values of the indoor temperature are used for prediction - using linear and Transformer-family models. We evaluate Linear, NLinear, DLinear, Transformer, Informer, and Autoformer under standardized train, validation, and test splits. Results show that linear baselines (Linear, NLinear, DLinear) consistently outperform more complex Transformer-family architectures, with DLinear achieving the best overall accuracy across all splits. These findings highlight that carefully designed linear models remain strong baselines for time series forecasting in challenging exogenous-only settings.

</details>


### [10] [Guided Transfer Learning for Discrete Diffusion Models](https://arxiv.org/abs/2512.10877)
*Julian Kleutgens,Claudio Battiloro,Lingkai Kong,Benjamin Grewe,Francesca Dominici,Mauricio Tec*

Main category: cs.LG

TL;DR: Guided Transfer Learning (GTL) enables sampling from target distributions using pretrained discrete diffusion models without fine-tuning, with an efficient sampler for large vocabularies and long sequences.


<details>
  <summary>Details</summary>
Motivation: Discrete diffusion models require large training datasets which are costly/risky to obtain for new domains. Transfer learning typically requires fine-tuning large models which is computationally expensive and impractical.

Method: GTL uses ratio-based transfer learning with guidance formulation that works for both discrete-time diffusion and continuous-time score-based discrete diffusion. Includes efficient sampler that concentrates evaluations on planner-selected positions and top candidate tokens.

Result: Enables sampling from target distributions without modifying pretrained denoiser. Makes guided language modeling practical at scale for large vocabularies and long sequences. Evaluated on sequential data including synthetic Markov chains and language modeling.

Conclusion: GTL provides a unified, efficient approach for transfer learning with discrete diffusion models that avoids expensive fine-tuning while maintaining practical performance for large-scale language modeling tasks.

Abstract: Discrete diffusion models achieve strong performance across language and other discrete domains, providing a powerful alternative to autoregressive models. However, their strong performance relies on large training datasets, which are costly or risky to obtain, especially when adapting to new domains. Transfer learning is the natural way to adapt pretrained discrete diffusion models, but current methods require fine-tuning large diffusion models, which is computationally expensive and often impractical. Building on ratio-based transfer learning for continuous diffusion, we provide Guided Transfer Learning for discrete diffusion models (GTL). This enables sampling from a target distribution without modifying the pretrained denoiser. The same guidance formulation applies to both discrete-time diffusion and continuous-time score-based discrete diffusion, yielding a unified treatment. Guided discrete diffusion often requires many forward passes of the guidance network, which becomes impractical for large vocabularies and long sequences. To address this, we further present an efficient guided sampler that concentrates evaluations on planner-selected positions and top candidate tokens, thus lowering sampling time and computation. This makes guided language modeling practical at scale for large vocabularies and long sequences. We evaluate GTL on sequential data, including synthetic Markov chains and language modeling, and provide empirical analyses of its behavior.

</details>


### [11] [Classifier Reconstruction Through Counterfactual-Aware Wasserstein Prototypes](https://arxiv.org/abs/2512.10878)
*Xuan Zhao,Zhuo Cao,Arya Bangun,Hanno Scharr,Ira Assent*

Main category: cs.LG

TL;DR: Counterfactual explanations can improve model reconstruction by treating counterfactuals as informative boundary samples, integrated with original data using Wasserstein barycenters to preserve class distributions and reduce decision boundary shift.


<details>
  <summary>Details</summary>
Motivation: Counterfactual explanations are typically used for interpretability, but they can also enhance model reconstruction (training surrogate models). However, naive use of counterfactuals as training samples can cause decision boundary shift because they lie near decision boundaries and are less representative of class distributions. The paper aims to improve model reconstruction by properly leveraging counterfactuals' unique properties.

Method: The method integrates original data samples with counterfactuals to approximate class prototypes using Wasserstein barycenter. This preserves the underlying distributional structure of each class. Counterfactuals are recognized as informative though less representative samples that lie close to decision boundaries, which is particularly useful in limited labeled data settings. The approach mitigates decision boundary shift that occurs when counterfactuals are naively treated as ordinary training instances.

Result: Empirical results across multiple datasets show improved fidelity between surrogate and target models. The method enhances the quality of surrogate models and effectively mitigates the issue of decision boundary shift.

Conclusion: Counterfactual explanations can significantly improve model reconstruction when properly integrated with original data using distribution-preserving techniques like Wasserstein barycenters. This approach is particularly valuable in settings with limited labeled data and provides a principled way to leverage counterfactuals beyond interpretability for model approximation tasks.

Abstract: Counterfactual explanations provide actionable insights by identifying minimal input changes required to achieve a desired model prediction. Beyond their interpretability benefits, counterfactuals can also be leveraged for model reconstruction, where a surrogate model is trained to replicate the behavior of a target model. In this work, we demonstrate that model reconstruction can be significantly improved by recognizing that counterfactuals, which typically lie close to the decision boundary, can serve as informative though less representative samples for both classes. This is particularly beneficial in settings with limited access to labeled data. We propose a method that integrates original data samples with counterfactuals to approximate class prototypes using the Wasserstein barycenter, thereby preserving the underlying distributional structure of each class. This approach enhances the quality of the surrogate model and mitigates the issue of decision boundary shift, which commonly arises when counterfactuals are naively treated as ordinary training instances. Empirical results across multiple datasets show that our method improves fidelity between the surrogate and target models, validating its effectiveness.

</details>


### [12] [Physics-Informed Learning of Flow Distribution and Receiver Heat Losses in Parabolic Trough Solar Fields](https://arxiv.org/abs/2512.10886)
*Stefan Matthes,Markus Schramm*

Main category: cs.LG

TL;DR: Physics-informed learning framework infers hidden hydraulic imbalances and receiver degradation in CSP plants from operational data using nocturnal homogenization periods and differentiable optimization.


<details>
  <summary>Details</summary>
Motivation: Parabolic trough CSP plants have unobserved loop-level mass flows and receiver heat-loss parameters, making it impossible to diagnose hydraulic imbalances or receiver degradation using standard monitoring tools despite having temperature measurements.

Method: Physics-informed learning framework that exploits nocturnal homogenization periods (hot oil circulation through non-irradiated field) to isolate hydraulic and thermal-loss effects. Uses differentiable conjugate heat-transfer model discretized and embedded into end-to-end learning pipeline optimized with historical plant data from Andasol 3.

Result: Model accurately reconstructs loop temperatures (RMSE <2°C), produces physically meaningful estimates of loop imbalances and receiver heat losses. Comparison against drone-based infrared thermography shows strong correspondence, correctly identifying all areas with high-loss receivers.

Conclusion: Noisy real-world CSP operational data contain enough information to recover latent physical parameters when combined with appropriate modeling and differentiable optimization, enabling diagnosis of hydraulic imbalances and receiver degradation.

Abstract: Parabolic trough Concentrating Solar Power (CSP) plants operate large hydraulic networks of collector loops that must deliver a uniform outlet temperature despite spatially heterogeneous optical performance, heat losses, and pressure drops. While loop temperatures are measured, loop-level mass flows and receiver heat-loss parameters are unobserved, making it impossible to diagnose hydraulic imbalances or receiver degradation using standard monitoring tools.
  We present a physics-informed learning framework that infers (i) loop-level mass-flow ratios and (ii) time-varying receiver heat-transfer coefficients directly from routine operational data. The method exploits nocturnal homogenization periods -- when hot oil is circulated through a non-irradiated field -- to isolate hydraulic and thermal-loss effects. A differentiable conjugate heat-transfer model is discretized and embedded into an end-to-end learning pipeline optimized using historical plant data from the 50 MW Andasol 3 solar field.
  The model accurately reconstructs loop temperatures (RMSE $<2^\circ$C) and produces physically meaningful estimates of loop imbalances and receiver heat losses. Comparison against drone-based infrared thermography (QScan) shows strong correspondence, correctly identifying all areas with high-loss receivers. This demonstrates that noisy real-world CSP operational data contain enough information to recover latent physical parameters when combined with appropriate modeling and differentiable optimization.

</details>


### [13] [SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale](https://arxiv.org/abs/2512.10922)
*Max Zimmer,Christophe Roux,Moritz Wagner,Deborah Hendrych,Sebastian Pokutta*

Main category: cs.LG

TL;DR: A novel 1-swap algorithm for LLM pruning that reduces per-layer pruning error by up to 60% over Wanda, improving perplexity and zero-shot accuracy across GPT architectures.


<details>
  <summary>Details</summary>
Motivation: Traditional pruning methods for LLMs are inefficient - full retraining is prohibitive, global magnitude pruning is suboptimal for Transformers, and exact Integer Programming solutions are computationally infeasible at LLM scale.

Method: Decouple rows by enforcing equal sparsity per row, derive optimal 1-swaps using Gram matrix of calibration data, propose tractable 1-swap algorithm that warm starts from any pruning mask and runs efficiently on GPUs.

Result: Reduces per-layer pruning error by up to 60% over Wanda, consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.

Conclusion: The mask selection problem for LLM pruning can be made drastically more tractable through row decoupling and optimal 1-swaps, enabling efficient GPU-based pruning without the need for full retraining.

Abstract: The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.

</details>


### [14] [Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation](https://arxiv.org/abs/2512.10925)
*Zamirddine Mari,Mohamad Motasem Nawaf,Pierre Drap*

Main category: cs.LG

TL;DR: Deep reinforcement learning (PPO) outperforms traditional DWA planner for underwater robot navigation in cluttered environments, with successful sim-to-real transfer.


<details>
  <summary>Details</summary>
Motivation: Underwater navigation is challenging due to GPS absence, poor visibility, and obstacles. Need for robust autonomous navigation methods for underwater robots like BlueROV2.

Method: Proximal Policy Optimization (PPO) algorithm with observation space combining target navigation info, virtual occupancy grid, and ray-casting. Compared against Dynamic Window Approach (DWA) baseline. Evaluated in simulation and validated on physical BlueROV2 using 3D digital twin.

Result: PPO consistently outperforms DWA in highly cluttered environments with better local adaptation and reduced collisions. Successful transfer from simulation to real world demonstrated.

Conclusion: Deep reinforcement learning is relevant and effective for autonomous underwater navigation, showing superior performance over traditional kinematic planners in complex environments.

Abstract: Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.

</details>


### [15] [Decoupled Q-Chunking](https://arxiv.org/abs/2512.10926)
*Qiyang Li,Seohong Park,Sergey Levine*

Main category: cs.LG

TL;DR: Decoupling critic and policy chunk lengths enables multi-step value propagation without requiring open-loop action chunking, improving performance on long-horizon tasks.


<details>
  <summary>Details</summary>
Motivation: TD methods suffer from bootstrapping bias, and while chunked critics speed up value backup, they force policies to output long action chunks open-loop, which is suboptimal for reactive environments and challenging to model.

Method: Decouple critic and policy chunk lengths, then optimize policy against a distilled critic for partial action chunks. The distilled critic approximates maximum value achievable when partial chunks are extended to complete ones via optimistic backup from original chunked critic.

Result: The method reliably outperforms prior methods on challenging, long-horizon offline goal-conditioned tasks.

Conclusion: Decoupling critic and policy chunk lengths retains multi-step value propagation benefits while avoiding open-loop sub-optimality and long action chunk modeling challenges.

Abstract: Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences ("chunks") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal for environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning action chunking policies for long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned tasks and show that it reliably outperforms prior methods. Code: github.com/ColinQiyangLi/dqc.

</details>


### [16] [Asynchronous Reasoning: Training-Free Interactive Thinking LLMs](https://arxiv.org/abs/2512.10931)
*George Yakushev,Nataliia Babina,Masoud Vahid Dastgerdi,Vyacheslav Zhdanovskiy,Alina Shutova,Denis Kuznedelev*

Main category: cs.LG

TL;DR: Enables LLMs to think, listen, and generate outputs simultaneously using rotary embeddings, reducing response delays by 6-11x for real-time applications.


<details>
  <summary>Details</summary>
Motivation: Current LLMs must think sequentially before responding, making them unsuitable for real-time interactive applications like voice assistants that require simultaneous listening, thinking, and responding.

Method: Uses properties of rotary embeddings to enable LLMs to operate asynchronously without additional training, allowing simultaneous thinking, listening, and output generation.

Result: Reduces time to first non-thinking token from minutes to ≤5 seconds and overall real-time delays by 6-11x across math, commonsense, and safety reasoning tasks.

Conclusion: The approach successfully enables reasoning-capable LLMs to function in real-time interactive scenarios without retraining, bridging the gap between reasoning capabilities and interactive requirements.

Abstract: Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.

</details>


### [17] [Empirical evaluation of the Frank-Wolfe methods for constructing white-box adversarial attacks](https://arxiv.org/abs/2512.10936)
*Kristina Korotkova,Aleksandr Katrutsa*

Main category: cs.LG

TL;DR: The paper proposes using modified Frank-Wolfe (projection-free) methods for efficient white-box adversarial attack construction, comparing them with standard projection-based approaches on MNIST and CIFAR-10 datasets.


<details>
  <summary>Details</summary>
Motivation: Adversarial attack construction is crucial for assessing neural network robustness, but current methods need to be fast and efficient. The optimization-based nature of attack construction suggests that advanced numerical optimization techniques could improve attack efficiency.

Method: The authors propose using modified Frank-Wolfe methods (projection-free optimization) to construct white-box adversarial attacks. They perform theoretical analysis and numerical experiments comparing these methods with standard projection-based approaches and geometrically intuitive methods.

Result: Numerical experiments on MNIST and CIFAR-10 datasets using logistic regression, CNNs, and Vision Transformers show that the modified Frank-Wolfe methods provide effective adversarial attacks, potentially outperforming standard approaches in terms of efficiency and effectiveness.

Conclusion: Projection-free methods based on modified Frank-Wolfe algorithms offer a promising approach for constructing efficient and effective adversarial attacks, providing an alternative to traditional projection-based methods for assessing neural network robustness.

Abstract: The construction of adversarial attacks for neural networks appears to be a crucial challenge for their deployment in various services. To estimate the adversarial robustness of a neural network, a fast and efficient approach is needed to construct adversarial attacks. Since the formalization of adversarial attack construction involves solving a specific optimization problem, we consider the problem of constructing an efficient and effective adversarial attack from a numerical optimization perspective. Specifically, we suggest utilizing advanced projection-free methods, known as modified Frank-Wolfe methods, to construct white-box adversarial attacks on the given input data. We perform a theoretical and numerical evaluation of these methods and compare them with standard approaches based on projection operations or geometrical intuition. Numerical experiments are performed on the MNIST and CIFAR-10 datasets, utilizing a multiclass logistic regression model, the convolutional neural networks (CNNs), and the Vision Transformer (ViT).

</details>


### [18] [Stronger Normalization-Free Transformers](https://arxiv.org/abs/2512.10938)
*Mingzhi Chen,Taiming Lu,Jiachen Zhu,Mingjie Sun,Zhuang Liu*

Main category: cs.LG

TL;DR: Derf (erf-based activation function) outperforms normalization layers and Dynamic Tanh across multiple domains, offering better generalization for normalization-free Transformers.


<details>
  <summary>Details</summary>
Motivation: To find point-wise function designs that can surpass Dynamic Tanh (DyT) and normalization layers, exploring how intrinsic properties of such functions influence training and performance.

Method: First study intrinsic properties of point-wise functions, then conduct large-scale search for more effective designs, identifying Derf(x) = erf(αx + s) as optimal.

Result: Derf outperforms LayerNorm, RMSNorm, and DyT across vision (image recognition/generation), speech representation, and DNA sequence modeling, with gains from improved generalization rather than fitting capacity.

Conclusion: Derf's simplicity and stronger performance make it a practical choice for normalization-free Transformer architectures, suggesting effective alternatives to traditional normalization layers.

Abstract: Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\mathrm{Derf}(x) = \mathrm{erf}(αx + s)$, where $\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.

</details>


### [19] [Hierarchical Dataset Selection for High-Quality Data Sharing](https://arxiv.org/abs/2512.10952)
*Xiaona Zhou,Yingyan Zeng,Ran Jin,Ismini Lourentzou*

Main category: cs.LG

TL;DR: DaSH is a dataset selection method that models utility at both dataset and group levels, outperforming state-of-the-art baselines by up to 26.2% accuracy while requiring fewer exploration steps.


<details>
  <summary>Details</summary>
Motivation: Real-world machine learning often involves data from multiple heterogeneous sources (public repositories, institutions) with varying relevance, quality, and utility. Existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources.

Method: DaSH (Dataset Selection via Hierarchies) formalizes dataset selection as selecting entire datasets from a large, heterogeneous pool. It models utility at both dataset and group levels (e.g., collections, institutions), enabling efficient generalization from limited observations.

Result: Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets.

Conclusion: DaSH is suitable for scalable and adaptive dataset selection in practical multi-source learning workflows, addressing the critical need for efficient dataset selection from heterogeneous sources under resource constraints.

Abstract: The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.

</details>


### [20] [Bidirectional Normalizing Flow: From Data to Noise and Back](https://arxiv.org/abs/2512.10953)
*Yiyang Lu,Qiao Sun,Xianbang Wang,Zhicheng Jiang,Hanhong Zhao,Kaiming He*

Main category: cs.LG

TL;DR: BiFlow introduces a bidirectional normalizing flow framework that removes the need for exact analytic inverses, enabling more flexible architectures and faster sampling while maintaining high generation quality.


<details>
  <summary>Details</summary>
Motivation: Standard normalizing flows require explicit invertibility constraints, and recent Transformer-based autoregressive flows (TARFlow) suffer from causal decoding bottlenecks that slow down sampling. There's a need for more flexible NF frameworks that can overcome these limitations while maintaining generation quality.

Method: BiFlow learns a separate reverse model that approximates the noise-to-data inverse mapping instead of requiring exact analytic inverses. This allows using more flexible loss functions and architectures that aren't constrained by explicit invertibility requirements.

Result: On ImageNet, BiFlow improves generation quality compared to causal decoding counterparts while accelerating sampling by up to two orders of magnitude. It achieves state-of-the-art results among NF-based methods and competitive performance with single-evaluation ("1-NFE") methods.

Conclusion: BiFlow demonstrates that removing the exact analytic inverse requirement enables more practical and efficient normalizing flows, potentially revitalizing interest in this classical generative modeling paradigm.

Abstract: Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow ($\textbf{BiFlow}$), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation ("1-NFE") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [LLMs Can Assist with Proposal Selection at Large User Facilities](https://arxiv.org/abs/2512.10895)
*Lijie Ding,Janell Thomson,Jon Taylor,Changwoo Do*

Main category: cs.AI

TL;DR: LLMs can effectively rank scientific proposals at neutron facilities, matching human performance at much lower cost while enabling advanced analyses like similarity assessment.


<details>
  <summary>Details</summary>
Motivation: Traditional human proposal review suffers from bias, inconsistency, and weak inter-proposal correlations. Pairwise preference ranking is theoretically better but impractical for humans due to quadratic workload.

Method: Used LLMs to rank proposals from three SNS beamlines at ORNL, leveraging well-curated proposals and publication records. Applied pairwise preference-based approach and embedding models for similarity analysis.

Result: LLM rankings strongly correlate with human rankings (Spearman ρ≈0.2-0.8, improving to ≥0.5 after outlier removal). LLMs perform no worse than humans at identifying high-publication-potential proposals while costing 100x less.

Conclusion: LLMs offer a scalable, consistent, cost-effective alternative to human proposal review, enabling advanced analyses like quantitative similarity assessment that benefit review committees.

Abstract: We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $ρ\simeq 0.2-0.8$, improving to $\geq 0.5$ after 10\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.

</details>


### [22] [Multi-Granular Node Pruning for Circuit Discovery](https://arxiv.org/abs/2512.10903)
*Muhammad Umair Haider,Hammad Rizwan,Hassan Sajjad,A. B. Siddique*

Main category: cs.AI

TL;DR: A node-level pruning framework for circuit discovery in LLMs that uses learnable masks with granularity-specific sparsity penalties to identify minimal subnetworks more efficiently and at finer granularity than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing circuit discovery methods are computationally expensive (iterative edge pruning) and limited to coarse-grained units like attention heads or MLP blocks, missing finer structures like individual neurons. There's a need for more scalable and granular approaches.

Method: Proposes a node-level pruning framework with learnable masks across multiple granularity levels (blocks to individual neurons) within a unified optimization objective. Uses granularity-specific sparsity penalties to guide pruning, enabling comprehensive compression in a single fine-tuning run.

Result: Identifies circuits with fewer nodes than prior methods, shows many neurons deemed important by coarse methods are actually irrelevant while maintaining task performance, and achieves 5-10x lower memory footprint by not requiring intermediate activations in memory.

Conclusion: The proposed node-level pruning framework addresses scalability and granularity limitations of existing circuit discovery methods, enabling more efficient and fine-grained identification of minimal subnetworks responsible for specific LLM behaviors.

Abstract: Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.

</details>


### [23] [On Decision-Making Agents and Higher-Order Causal Processes](https://arxiv.org/abs/2512.10937)
*Matt Wilson*

Main category: cs.AI

TL;DR: This paper establishes a formal correspondence between POMDP agents in AI and process functions in quantum physics, showing they can be interpreted as dual perspectives of the same mathematical structure.


<details>
  <summary>Details</summary>
Motivation: The paper aims to bridge the gap between decision-making agents in artificial intelligence (specifically in partially observable Markov decision processes) and quantum process functions, revealing a deep mathematical connection between these seemingly disparate fields.

Method: The authors establish a precise correspondence by identifying an agent's policy and memory update as a process function that interacts with a POMDP environment via the link product. They show this allows for dual interpretations: physics view (process function as environment) and AI view (process function as agent).

Result: The paper demonstrates that decision-making agents in POMDPs correspond exactly to one-input process functions, the classical limit of higher-order quantum operations. This correspondence is extended to multi-agent systems by identifying observation-independent decentralized POMDPs as domains for multi-input process functions.

Conclusion: The work reveals a fundamental mathematical equivalence between AI decision-making frameworks and quantum process theory, suggesting that insights from quantum information theory could inform AI agent design and vice versa, particularly in multi-agent settings.

Abstract: We establish a precise correspondence between decision-making agents in partially observable Markov decision processes (POMDPs) and one-input process functions, the classical limit of higher-order quantum operations. In this identification an agent's policy and memory update combine into a process function w that interacts with a POMDP environment via the link product. This suggests a dual interpretation: in the physics view, the process function acts as the environment into which local operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and the inserted functions represent environments. We extend this perspective to multi-agent systems by identifying observation-independent decentralized POMDPs as natural domains for multi-input process functions.

</details>
