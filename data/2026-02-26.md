<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 106]
- [cs.LG](#cs.LG) [Total: 85]
- [cs.RO](#cs.RO) [Total: 41]
- [eess.IV](#eess.IV) [Total: 7]
- [eess.SP](#eess.SP) [Total: 15]
- [cs.IT](#cs.IT) [Total: 9]
- [cs.AI](#cs.AI) [Total: 14]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [StoryTailor:A Zero-Shot Pipeline for Action-Rich Multi-Subject Visual Narratives](https://arxiv.org/abs/2602.21273)
*Jinghao Hu,Yuhe Zhang,GuoHua Geng,Kang Li,Han Zhang*

Main category: cs.CV

TL;DR: StoryTailor is a zero-shot pipeline for generating multi-frame visual narratives that addresses the tension between action faithfulness, subject identity fidelity, and background continuity without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: There's a threefold tension in generating multi-frame visual narratives: maintaining action text faithfulness, preserving subject identity fidelity, and ensuring cross-frame background continuity. Current methods struggle to balance these aspects without requiring fine-tuning.

Method: Three synergistic modules: 1) Gaussian-Centered Attention (GCA) to dynamically focus on subject cores and handle grounding-box overlaps; 2) Action-Boost Singular Value Reweighting (AB-SVR) to amplify action-related directions in text embeddings; 3) Selective Forgetting Cache (SFC) that retains transferable background cues, forgets nonessential history, and selectively surfaces cues for cross-scene semantic ties.

Result: CLIP-T improves by 10-15% over baselines, DreamSim lower than strong baselines, CLIP-I stays in competitive range. Inference is faster than FluxKontext on 24GB GPU with matched resolution and steps. Qualitatively delivers expressive interactions and evolving yet stable scenes.

Conclusion: StoryTailor successfully addresses the threefold tension in visual narrative generation through its synergistic modules, achieving better performance metrics while running efficiently on consumer hardware without fine-tuning.

Abstract: Generating multi-frame, action-rich visual narratives without fine-tuning faces a threefold tension: action text faithfulness, subject identity fidelity, and cross-frame background continuity. We propose StoryTailor, a zero-shot pipeline that runs on a single RTX 4090 (24 GB) and produces temporally coherent, identity-preserving image sequences from a long narrative prompt, per-subject references, and grounding boxes. Three synergistic modules drive the system: Gaussian-Centered Attention (GCA) to dynamically focus on each subject core and ease grounding-box overlaps; Action-Boost Singular Value Reweighting (AB-SVR) to amplify action-related directions in the text embedding space; and Selective Forgetting Cache (SFC) that retains transferable background cues, forgets nonessential history, and selectively surfaces retained cues to build cross-scene semantic ties. Compared with baseline methods, experiments show that CLIP-T improves by up to 10-15%, with DreamSim lower than strong baselines, while CLIP-I stays in a visually acceptable, competitive range. With matched resolution and steps on a 24 GB GPU, inference is faster than FluxKontext. Qualitatively, StoryTailor delivers expressive interactions and evolving yet stable scenes.

</details>


### [2] [HorizonForge: Driving Scene Editing with Any Trajectories and Any Vehicles](https://arxiv.org/abs/2602.21333)
*Yifan Wang,Francesco Pittaluga,Zaid Tasneem,Chenyu You,Manmohan Chandraker,Ziyu Jiang*

Main category: cs.CV

TL;DR: HorizonForge is a unified framework for controllable driving scene generation that combines editable Gaussian Splats and Meshes with noise-aware video diffusion for photorealistic, temporally consistent scene editing and vehicle insertion.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for controllable driving scene generation struggle to jointly achieve photorealism and precise control, creating a need for a solution that can produce realistic, editable driving scenes with fine-grained manipulation capabilities.

Method: The framework reconstructs scenes as editable Gaussian Splats and Meshes, enabling 3D manipulation and language-driven vehicle insertion. Edits are rendered through a noise-aware video diffusion process that enforces spatial and temporal consistency, producing diverse scene variations in a single feed-forward pass without per-trajectory optimization.

Result: The Gaussian-Mesh representation delivers substantially higher fidelity than alternative 3D representations, and temporal priors from video diffusion are essential for coherent synthesis. HorizonForge achieves an 83.4% user-preference gain and a 25.19% FID improvement over the second best state-of-the-art method.

Conclusion: HorizonForge establishes a simple yet powerful paradigm for photorealistic, controllable driving simulation, combining 3D scene representation with video diffusion to enable realistic scene editing and vehicle manipulation for autonomous driving simulation.

Abstract: Controllable driving scene generation is critical for realistic and scalable autonomous driving simulation, yet existing approaches struggle to jointly achieve photorealism and precise control. We introduce HorizonForge, a unified framework that reconstructs scenes as editable Gaussian Splats and Meshes, enabling fine-grained 3D manipulation and language-driven vehicle insertion. Edits are rendered through a noise-aware video diffusion process that enforces spatial and temporal consistency, producing diverse scene variations in a single feed-forward pass without per-trajectory optimization. To standardize evaluation, we further propose HorizonSuite, a comprehensive benchmark spanning ego- and agent-level editing tasks such as trajectory modifications and object manipulation. Extensive experiments show that Gaussian-Mesh representation delivers substantially higher fidelity than alternative 3D representations, and that temporal priors from video diffusion are essential for coherent synthesis. Combining these findings, HorizonForge establishes a simple yet powerful paradigm for photorealistic, controllable driving simulation, achieving an 83.4% user-preference gain and a 25.19% FID improvement over the second best state-of-the-art method. Project page: https://horizonforge.github.io/ .

</details>


### [3] [Scaling View Synthesis Transformers](https://arxiv.org/abs/2602.21341)
*Evan Kim,Hyunwoo Ryu,Thomas W. Mitchel,Vincent Sitzmann*

Main category: cs.CV

TL;DR: Systematic study of scaling laws for view synthesis transformers shows encoder-decoder architectures can be compute-optimal, achieving state-of-the-art performance with reduced training compute.


<details>
  <summary>Details</summary>
Motivation: Geometry-free view synthesis transformers have achieved SOTA in NVS, but scaling factors with compute remain unclear. Need systematic study of scaling laws to derive design principles for compute-optimal NVS models.

Method: Presents systematic study of scaling laws for view synthesis transformers. Develops encoder-decoder architecture called Scalable View Synthesis Model (SVSM). Shows encoder-decoder can be compute-optimal by addressing suboptimal architectural choices and unequal training budgets.

Result: SVSM scales as effectively as decoder-only models, achieves superior performance-compute Pareto frontier, surpasses previous SOTA on real-world NVS benchmarks with substantially reduced training compute.

Conclusion: Encoder-decoder architectures can be compute-optimal for view synthesis transformers when properly designed, challenging prior negative findings and providing practical design principles for efficient NVS models.

Abstract: Geometry-free view synthesis transformers have recently achieved state-of-the-art performance in Novel View Synthesis (NVS), outperforming traditional approaches that rely on explicit geometry modeling. Yet the factors governing their scaling with compute remain unclear. We present a systematic study of scaling laws for view synthesis transformers and derive design principles for training compute-optimal NVS models. Contrary to prior findings, we show that encoder-decoder architectures can be compute-optimal; we trace earlier negative results to suboptimal architectural choices and comparisons across unequal training compute budgets. Across several compute levels, we demonstrate that our encoder-decoder architecture, which we call the Scalable View Synthesis Model (SVSM), scales as effectively as decoder-only models, achieves a superior performance-compute Pareto frontier, and surpasses the previous state-of-the-art on real-world NVS benchmarks with substantially reduced training compute.

</details>


### [4] [Towards Controllable Video Synthesis of Routine and Rare OR Events](https://arxiv.org/abs/2602.21365)
*Dominik Schneider,Lalithkumar Seenivasan,Sampath Rapuri,Vishalroshan Anil,Aiza Maksutova,Yiqing Shen,Jan Emily Mangulabnan,Hao Ding,Jose L. Porras,Masaru Ishii,Mathias Unberath*

Main category: cs.CV

TL;DR: OR video diffusion framework synthesizes rare surgical events from geometric abstractions to overcome data scarcity for AI training.


<details>
  <summary>Details</summary>
Motivation: Curating real OR datasets with rare/safety-critical events is operationally/ethically challenging, creating a data bottleneck for developing ambient intelligence systems.

Method: OR video diffusion framework with geometric abstraction module, conditioning module, and fine-tuned diffusion model to transform scenes into geometric representations and generate realistic OR event videos.

Result: Outperforms baselines on routine events (lower FVD/LPIPS, higher SSIM/PSNR), enables controlled synthesis of counterfactual events, and synthetic data-trained AI achieves 70.13% recall for near-miss detection.

Conclusion: Framework enables controlled synthesis of routine/rare OR events from geometric representations, demonstrating potential to support ambient intelligence development.

Abstract: Purpose: Curating large-scale datasets of operating room (OR) workflow, encompassing rare, safety-critical, or atypical events, remains operationally and ethically challenging. This data bottleneck complicates the development of ambient intelligence for detecting, understanding, and mitigating rare or safety-critical events in the OR.
  Methods: This work presents an OR video diffusion framework that enables controlled synthesis of rare and safety-critical events. The framework integrates a geometric abstraction module, a conditioning module, and a fine-tuned diffusion model to first transform OR scenes into abstract geometric representations, then condition the synthesis process, and finally generate realistic OR event videos. Using this framework, we also curate a synthetic dataset to train and validate AI models for detecting near-misses of sterile-field violations.
  Results: In synthesizing routine OR events, our method outperforms off-the-shelf video diffusion baselines, achieving lower FVD/LPIPS and higher SSIM/PSNR in both in- and out-of-domain datasets. Through qualitative results, we illustrate its ability for controlled video synthesis of counterfactual events. An AI model trained and validated on the generated synthetic data achieved a RECALL of 70.13% in detecting near safety-critical events. Finally, we conduct an ablation study to quantify performance gains from key design choices.
  Conclusion: Our solution enables controlled synthesis of routine and rare OR events from abstract geometric representations. Beyond demonstrating its capability to generate rare and safety-critical scenarios, we show its potential to support the development of ambient intelligence models.

</details>


### [5] [Momentum Memory for Knowledge Distillation in Computational Pathology](https://arxiv.org/abs/2602.21395)
*Yongxin Guo,Hao Lu,Onur C. Koyun,Zhengjie Zhu,Muhammet Fatih Demir,Metin Nafi Gurcan*

Main category: cs.CV

TL;DR: MoMKD is a cross-modal knowledge distillation framework that uses momentum-updated memory to transfer genomic supervision to histopathology models, enabling accurate cancer diagnosis using histology alone without paired data.


<details>
  <summary>Details</summary>
Motivation: Clinical translation of multimodal learning (genomics + histopathology) is hindered by limited paired data. Knowledge distillation offers a solution but existing methods suffer from instability due to batch-local alignment and performance degradation.

Method: Proposes Momentum Memory Knowledge Distillation (MoMKD) with momentum-updated memory that aggregates genomic and histopathology information across batches. Decouples gradients of genomics and histology branches to prevent genomic signals from dominating and eliminate modality-gap issues.

Result: Outperforms state-of-the-art MIL and multimodal KD baselines on TCGA-BRCA benchmark (HER2, PR, ODX classification) and independent in-house testing dataset. Delivers strong performance and generalization under histology-only inference.

Conclusion: MoMKD establishes a robust and generalizable knowledge distillation paradigm for computational pathology, enabling accurate cancer diagnosis using histology alone by effectively transferring genomic supervision through cross-batch memory aggregation.

Abstract: Multimodal learning that integrates genomics and histopathology has shown strong potential in cancer diagnosis, yet its clinical translation is hindered by the limited availability of paired histology-genomics data. Knowledge distillation (KD) offers a practical solution by transferring genomic supervision into histopathology models, enabling accurate inference using histology alone. However, existing KD methods rely on batch-local alignment, which introduces instability due to limited within-batch comparisons and ultimately degrades performance.
  To address these limitations, we propose Momentum Memory Knowledge Distillation (MoMKD), a cross-modal distillation framework driven by a momentum-updated memory. This memory aggregates genomic and histopathology information across batches, effectively enlarging the supervisory context available to each mini-batch. Furthermore, we decouple the gradients of the genomics and histology branches, preventing genomic signals from dominating histology feature learning during training and eliminating the modality-gap issue at inference time.
  Extensive experiments on the TCGA-BRCA benchmark (HER2, PR, and ODX classification tasks) and an independent in-house testing dataset demonstrate that MoMKD consistently outperforms state-of-the-art MIL and multimodal KD baselines, delivering strong performance and generalization under histology-only inference. Overall, MoMKD establishes a robust and generalizable knowledge distillation paradigm for computational pathology.

</details>


### [6] [MMLoP: Multi-Modal Low-Rank Prompting for Efficient Vision-Language Adaptation](https://arxiv.org/abs/2602.21397)
*Sajjad Ghiasvand,Haniyeh Ehsani Oskouie,Mahnoosh Alizadeh,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: MMLoP is a parameter-efficient multi-modal prompting method that achieves deep vision-language prompting with only 11.5K trainable parameters using low-rank factorization and complementary regularization techniques.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal prompt learning methods significantly increase trainable parameters (millions), abandoning the parameter efficiency that makes prompt tuning attractive. There's a need for methods that maintain deep multi-modal prompting while preserving parameter efficiency.

Method: MMLoP uses low-rank factorization to parameterize vision and text prompts at each transformer layer. It includes three key components: 1) self-regulating consistency loss anchoring prompted representations to frozen CLIP features, 2) uniform drift correction to remove global embedding shift, and 3) shared up-projection coupling vision and text prompts through common low-rank factors.

Result: Achieves highly favorable accuracy-efficiency tradeoff with only 11.5K parameters, outperforming most existing methods including those with orders of magnitude more parameters. Achieves harmonic mean of 79.70% on base-to-novel generalization across 11 diverse datasets.

Conclusion: MMLoP demonstrates that deep multi-modal prompting can be achieved with extreme parameter efficiency comparable to early text-only methods, while maintaining competitive performance through careful regularization and cross-modal alignment techniques.

Abstract: Prompt learning has become a dominant paradigm for adapting vision-language models (VLMs) such as CLIP to downstream tasks without modifying pretrained weights. While extending prompts to both vision and text encoders across multiple transformer layers significantly boosts performance, it dramatically increases the number of trainable parameters, with state-of-the-art methods requiring millions of parameters and abandoning the parameter efficiency that makes prompt tuning attractive. In this work, we propose \textbf{MMLoP} (\textbf{M}ulti-\textbf{M}odal \textbf{Lo}w-Rank \textbf{P}rompting), a framework that achieves deep multi-modal prompting with only \textbf{11.5K trainable parameters}, comparable to early text-only methods like CoOp. MMLoP parameterizes vision and text prompts at each transformer layer through a low-rank factorization, which serves as an implicit regularizer against overfitting on few-shot training data. To further close the accuracy gap with state-of-the-art methods, we introduce three complementary components: a self-regulating consistency loss that anchors prompted representations to frozen zero-shot CLIP features at both the feature and logit levels, a uniform drift correction that removes the global embedding shift induced by prompt tuning to preserve class-discriminative structure, and a shared up-projection that couples vision and text prompts through a common low-rank factor to enforce cross-modal alignment. Extensive experiments across three benchmarks and 11 diverse datasets demonstrate that MMLoP achieves a highly favorable accuracy-efficiency tradeoff, outperforming the majority of existing methods including those with orders of magnitude more parameters, while achieving a harmonic mean of 79.70\% on base-to-novel generalization.

</details>


### [7] [FlowFixer: Towards Detail-Preserving Subject-Driven Generation](https://arxiv.org/abs/2602.21402)
*Jinyoung Jun,Won-Dong Jang,Wenbin Ouyang,Raghudeep Gadde,Jungbeom Lee*

Main category: cs.CV

TL;DR: FlowFixer is an image refinement framework for subject-driven generation that restores lost fine details by using direct image-to-image translation from visual references, avoiding language prompt ambiguities.


<details>
  <summary>Details</summary>
Motivation: Subject-driven generation (SDG) methods often lose fine details when changing subject scale and perspective. Existing approaches rely on language prompts which can be ambiguous and fail to preserve high-fidelity details.

Method: 1) Direct image-to-image translation from visual references, 2) One-step denoising scheme for self-supervised training data generation that removes high-frequency details while preserving global structure, 3) Keypoint matching-based metric for assessing detail fidelity beyond semantic similarities.

Result: FlowFixer outperforms state-of-the-art SDG methods in both qualitative and quantitative evaluations, setting a new benchmark for high-fidelity subject-driven generation.

Conclusion: FlowFixer provides an effective refinement framework for SDG that restores lost fine details through image-to-image translation and self-supervised training, with a novel keypoint-based evaluation metric for proper fidelity assessment.

Abstract: We present FlowFixer, a refinement framework for subject-driven generation (SDG) that restores fine details lost during generation caused by changes in scale and perspective of a subject. FlowFixer proposes direct image-to-image translation from visual references, avoiding ambiguities in language prompts. To enable image-to-image training, we introduce a one-step denoising scheme to generate self-supervised training data, which automatically removes high-frequency details while preserving global structure, effectively simulating real-world SDG errors. We further propose a keypoint matching-based metric to properly assess fidelity in details beyond semantic similarities usually measured by CLIP or DINO. Experimental results demonstrate that FlowFixer outperforms state-of-the-art SDG methods in both qualitative and quantitative evaluations, setting a new benchmark for high-fidelity subject-driven generation.

</details>


### [8] [Exploring Vision-Language Models for Open-Vocabulary Zero-Shot Action Segmentation](https://arxiv.org/abs/2602.21406)
*Asim Unmesh,Kaki Ramesh,Mayank Patel,Rahul Jain,Karthik Ramani*

Main category: cs.CV

TL;DR: Training-free open-vocabulary zero-shot temporal action segmentation using vision-language models without task-specific supervision.


<details>
  <summary>Details</summary>
Motivation: Existing temporal action segmentation methods are limited to closed vocabularies and fixed label sets, making comprehensive dataset collection infeasible due to the vast space of activities and alternative breakdowns.

Method: Proposes a training-free pipeline with two components: 1) Frame-Action Embedding Similarity (FAES) that matches video frames to candidate action labels using VLMs, and 2) Similarity-Matrix Temporal Segmentation (SMTS) that enforces temporal consistency.

Result: OVTAS achieves strong results on standard benchmarks without task-specific supervision. The paper also provides the first broad analysis of 14 diverse VLMs for open-vocabulary action segmentation.

Conclusion: Demonstrates the potential of VLMs for structured temporal understanding in open-vocabulary zero-shot settings, offering a promising direction for temporal action segmentation without closed-vocabulary limitations.

Abstract: Temporal Action Segmentation (TAS) requires dividing videos into action segments, yet the vast space of activities and alternative breakdowns makes collecting comprehensive datasets infeasible. Existing methods remain limited to closed vocabularies and fixed label sets. In this work, we explore the largely unexplored problem of Open-Vocabulary Zero-Shot Temporal Action Segmentation (OVTAS) by leveraging the strong zero-shot capabilities of Vision-Language Models (VLMs). We introduce a training-free pipeline that follows a segmentation-by-classification design: Frame-Action Embedding Similarity (FAES) matches video frames to candidate action labels, and Similarity-Matrix Temporal Segmentation (SMTS) enforces temporal consistency. Beyond proposing OVTAS, we present a systematic study across 14 diverse VLMs, providing the first broad analysis of their suitability for open-vocabulary action segmentation. Experiments on standard benchmarks show that OVTAS achieves strong results without task-specific supervision, underscoring the potential of VLMs for structured temporal understanding.

</details>


### [9] [WildSVG: Towards Reliable SVG Generation Under Real-Word Conditions](https://arxiv.org/abs/2602.21416)
*Marco Terral,Haotian Zhang,Tianyang Zhang,Meng Lin,Xiaoqing Xie,Haoran Dai,Darsh Kaushik,Pai Peng,Nicklas Scharpff,David Vazquez,Joan Rodriguez*

Main category: cs.CV

TL;DR: The paper introduces SVG extraction task and WildSVG Benchmark to address lack of suitable benchmarks for extracting scalable vector graphics from real-world images with noise and clutter.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal models work well on clean renderings or text descriptions but fail in real-world scenarios with noise, clutter, and domain shifts. There's a lack of suitable benchmarks for SVG extraction from natural images.

Method: Introduces WildSVG Benchmark with two datasets: Natural WildSVG (real images with company logos paired with SVG annotations) and Synthetic WildSVG (complex SVG renderings blended into real scenes to simulate difficult conditions).

Result: Current state-of-the-art multimodal models perform well below what's needed for reliable SVG extraction in real scenarios. However, iterative refinement methods show promise and model capabilities are steadily improving.

Conclusion: WildSVG Benchmark provides the first foundation for systematic benchmarking of SVG extraction, highlighting current limitations but pointing to promising directions through iterative refinement methods.

Abstract: We introduce the task of SVG extraction, which consists in translating specific visual inputs from an image into scalable vector graphics. Existing multimodal models achieve strong results when generating SVGs from clean renderings or textual descriptions, but they fall short in real-world scenarios where natural images introduce noise, clutter, and domain shifts. A central challenge in this direction is the lack of suitable benchmarks. To address this need, we introduce the WildSVG Benchmark, formed by two complementary datasets: Natural WildSVG, built from real images containing company logos paired with their SVG annotations, and Synthetic WildSVG, which blends complex SVG renderings into real scenes to simulate difficult conditions. Together, these resources provide the first foundation for systematic benchmarking SVG extraction. We benchmark state-of-the-art multimodal models and find that current approaches perform well below what is needed for reliable SVG extraction in real scenarios. Nonetheless, iterative refinement methods point to a promising path forward, and model capabilities are steadily improving

</details>


### [10] [ECHOSAT: Estimating Canopy Height Over Space And Time](https://arxiv.org/abs/2602.21421)
*Jan Pauls,Karsten Schrödter,Sven Ligensa,Martin Schwartz,Berkant Turan,Max Zimmer,Sassan Saatchi,Sebastian Pokutta,Philippe Ciais,Fabian Gieseke*

Main category: cs.CV

TL;DR: ECHOSAT is a global, temporally consistent tree height mapping system at 10m resolution that captures forest dynamics over multiple years, improving carbon monitoring accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing global tree height maps only provide static snapshots and fail to capture temporal forest dynamics, which are essential for accurate carbon accounting and climate change mitigation efforts.

Method: Uses multi-sensor satellite data to train a specialized vision transformer model for pixel-level temporal regression, with a self-supervised growth loss that regularizes predictions to follow natural tree development patterns including gradual growth and abrupt declines from disturbances.

Result: The model improves state-of-the-art accuracies for single-year predictions and provides the first global-scale height map that accurately quantifies tree growth and disturbances over time.

Conclusion: ECHOSAT advances global efforts in carbon monitoring and disturbance assessment by providing temporally consistent tree height data, with maps publicly accessible for research and monitoring applications.

Abstract: Forest monitoring is critical for climate change mitigation. However, existing global tree height maps provide only static snapshots and do not capture temporal forest dynamics, which are essential for accurate carbon accounting. We introduce ECHOSAT, a global and temporally consistent tree height map at 10 m resolution spanning multiple years. To this end, we resort to multi-sensor satellite data to train a specialized vision transformer model, which performs pixel-level temporal regression. A self-supervised growth loss regularizes the predictions to follow growth curves that are in line with natural tree development, including gradual height increases over time, but also abrupt declines due to forest loss events such as fires. Our experimental evaluation shows that our model improves state-of-the-art accuracies in the context of single-year predictions. We also provide the first global-scale height map that accurately quantifies tree growth and disturbances over time. We expect ECHOSAT to advance global efforts in carbon monitoring and disturbance assessment. The maps can be accessed at https://github.com/ai4forest/echosat.

</details>


### [11] [Automating Timed Up and Go Phase Segmentation and Gait Analysis via the tugturn Markerless 3D Pipeline](https://arxiv.org/abs/2602.21425)
*Abel Gonçalves Chinaglia,Guilherme Manna Cesar,Paulo Roberto Pereira Santiago*

Main category: cs.CV

TL;DR: tugturn.py is a Python workflow for 3D markerless TUG analysis with phase segmentation, gait-event detection, spatiotemporal metrics, coordination analysis, and stability assessment.


<details>
  <summary>Details</summary>
Motivation: Current markerless TUG analysis pipelines lack robustness and reproducibility, limiting clinical and research applications. There's a need for standardized, reproducible workflows for instrumented TUG analysis.

Method: Python-based workflow using spatial thresholds for phase segmentation (stand, first gait, turning, second gait, sit), relative-distance strategy for gait-event detection (heel-strike, toe-off), with TOML configuration files and outputs including Vector Coding, XCoM metrics, HTML reports, CSV tables, and QA visualizations.

Result: Developed tugturn.py software that provides comprehensive TUG analysis including conventional kinematics, intersegmental coordination via Vector Coding, and dynamic stability via Extrapolated Center of Mass metrics with reproducible artifacts.

Conclusion: tugturn.py offers a robust, reproducible markerless TUG analysis pipeline that supports clinical and research decision-making through standardized processing and comprehensive biomechanical outputs.

Abstract: Instrumented Timed Up and Go (TUG) analysis can support clinical and research decision-making, but robust and reproducible markerless pipelines are still limited. We present \textit{tugturn.py}, a Python-based workflow for 3D markerless TUG processing that combines phase segmentation, gait-event detection, spatiotemporal metrics, intersegmental coordination, and dynamic stability analysis. The pipeline uses spatial thresholds to segment each trial into stand, first gait, turning, second gait, and sit phases, and applies a relative-distance strategy to detect heel-strike and toe-off events within valid gait windows. In addition to conventional kinematics, \textit{tugturn} provides Vector Coding outputs and Extrapolated Center of Mass (XCoM)-based metrics. The software is configured through TOML files and produces reproducible artifacts, including HTML reports, CSV tables, and quality-assurance visual outputs. A complete runnable example is provided with test data and command-line instructions. This manuscript describes the implementation, outputs, and reproducibility workflow of \textit{tugturn} as a focused software contribution for markerless biomechanical TUG analysis.

</details>


### [12] [PSF-Med: Measuring and Explaining Paraphrase Sensitivity in Medical Vision Language Models](https://arxiv.org/abs/2602.21428)
*Binesh Sadanandan,Vahid Behzadan*

Main category: cs.CV

TL;DR: Medical VLMs show high sensitivity to question paraphrasing (8-58% flip rates), but low flip rates don't guarantee visual grounding. The paper introduces PSF-Med benchmark, analyzes flip mechanisms via sparse autoencoders, and shows intervention reduces flips by 31% with minimal accuracy cost.


<details>
  <summary>Details</summary>
Motivation: Medical VLMs can change answers when clinicians rephrase the same question, creating deployment risks. Current evaluations don't adequately test both paraphrase stability and visual grounding.

Method: 1) Created PSF-Med benchmark with 19,748 chest X-ray questions and ~92K paraphrases across MIMIC-CXR and PadChest. 2) Measured yes/no flip rates across 6 medical VLMs. 3) Used text-only baselines to assess visual grounding. 4) Applied GemmaScope 2 Sparse Autoencoders to MedGemma 4B to analyze flip mechanisms. 5) Conducted causal patching experiments. 6) Implemented feature clamping at inference.

Result: Flip rates ranged 8-58% across models. Text-only baselines revealed some models rely on language priors rather than images. Identified a sparse feature at layer 17 correlating with prompt framing. Causal patching recovered 45% of logit margin and reversed 15% of flips. Feature clamping reduced flip rates by 31% relative with only 1.3pp accuracy cost.

Conclusion: Flip rate alone is insufficient for robustness evaluation; both paraphrase stability and image reliance must be tested. Sparse autoencoders can identify mechanisms behind flips, and targeted interventions can improve model robustness with minimal performance cost.

Abstract: Medical Vision Language Models (VLMs) can change their answers when clinicians rephrase the same question, which raises deployment risks. We introduce Paraphrase Sensitivity Failure (PSF)-Med, a benchmark of 19,748 chest Xray questions paired with about 92,000 meaningpreserving paraphrases across MIMIC-CXR and PadChest. Across six medical VLMs, we measure yes/no flips for the same image and find flip rates from 8% to 58%. However, low flip rate does not imply visual grounding: text-only baselines show that some models stay consistent even when the image is removed, suggesting they rely on language priors. To study mechanisms in one model, we apply GemmaScope 2 Sparse Autoencoders (SAEs) to MedGemma 4B and analyze FlipBank, a curated set of 158 flip cases. We identify a sparse feature at layer 17 that correlates with prompt framing and predicts decision margin shifts. In causal patching, removing this feature's contribution recovers 45% of the yesminus-no logit margin on average and fully reverses 15% of flips. Acting on this finding, we show that clamping the identified feature at inference reduces flip rates by 31% relative with only a 1.3 percentage-point accuracy cost, while also decreasing text-prior reliance. These results suggest that flip rate alone is not enough; robustness evaluations should test both paraphrase stability and image reliance.

</details>


### [13] [Synergizing Understanding and Generation with Interleaved Analyzing-Drafting Thinking](https://arxiv.org/abs/2602.21435)
*Shengqiong Wu,Bobo Li,Xinkai Wang,Xiangtai Li,Lei Cui,Furu Wei,Shuicheng Yan,Hao Fei,Tat-seng Chua*

Main category: cs.CV

TL;DR: AD-Loop introduces an interleaved Analyzing-Drafting paradigm for UVLMs that dynamically alternates between understanding and generation, enabling synergistic multimodal learning through iterative refinement.


<details>
  <summary>Details</summary>
Motivation: Current UVLMs treat understanding and generation as parallel skills rather than synergistic processes, lacking explicit interaction between these capabilities during task solving. Existing approaches focus on architectural unification but overlook the need for dynamic interplay between comprehension and creation.

Method: Proposes AD-Loop: an interleaved Analyzing-Drafting problem-solving loop that dynamically alternates between analytic and drafting operations. Uses textual and visual thoughts for iterative refinement. Training involves two-stage strategy: supervised learning on interleaved thought data to initialize alternation, followed by reinforcement learning for adaptive autonomous control.

Result: AD-Loop consistently improves performance across standard benchmarks for both understanding and generation tasks. Shows strong transferability to various UVLM architectures. Visual analyses validate effectiveness of implicit visual thoughts.

Conclusion: AD-Loop represents a principled and broadly applicable strategy for synergizing comprehension and creation in multimodal learning, moving beyond architectural unification to achieve genuine capability synergy through dynamic interaction.

Abstract: Unified Vision-Language Models (UVLMs) aim to advance multimodal learning by supporting both understanding and generation within a single framework. However, existing approaches largely focus on architectural unification while overlooking the need for explicit interaction between the two capabilities during task solving. As a result, current models treat understanding and generation as parallel skills rather than synergistic processes. To achieve real synergy, we introduce the interleaved Analyzing-Drafting problem-solving loop (AD-Loop), a new think paradigm that dynamically alternates between analytic and drafting operations. By interleaving textual thoughts with visual thoughts, AD-Loop enables models to iteratively refine both comprehension and outputs, fostering genuine synergy. To train this mechanism, we design a two-stage strategy: supervised learning on interleaved thought data to initialize alternation, followed by reinforcement learning to promote adaptive and autonomous control. Extensive experiments demonstrate that AD-Loop consistently improves performance across standard benchmarks for both understanding and generation, with strong transferability to various UVLMs architectures. Visual analyses further validate the effectiveness of implicit visual thoughts. These results highlight AD-Loop as a principled and broadly applicable strategy for synergizing comprehension and creation. The project page is at https://sqwu.top/AD-Loop.

</details>


### [14] [Adversarial Robustness of Deep Learning-Based Thyroid Nodule Segmentation in Ultrasound](https://arxiv.org/abs/2602.21452)
*Nicholas Dietrich,David McShannon*

Main category: cs.CV

TL;DR: This paper evaluates adversarial attacks and defenses for thyroid nodule segmentation in ultrasound images, finding that spatial-domain attacks are partially mitigatable while frequency-domain attacks remain challenging.


<details>
  <summary>Details</summary>
Motivation: Deep learning segmentation models in clinical imaging need better characterization of adversarial robustness, especially for ultrasound images where such vulnerabilities are not well understood.

Method: Developed two black-box adversarial attacks: Structured Speckle Amplification Attack (SSAA) with boundary-targeted noise, and Frequency-Domain Ultrasound Attack (FDUA) with bandpass-filtered phase perturbations. Evaluated three inference-time defenses: randomized preprocessing, deterministic input denoising, and stochastic ensemble inference.

Result: SSAA reduced DSC by 0.29 with high visual similarity (SSIM=0.94), while FDUA reduced DSC by 0.11 with lower visual fidelity (SSIM=0.82). All three defenses significantly improved DSC against SSAA, with denoising showing best recovery (+0.10). No defense worked against FDUA.

Conclusion: Spatial-domain adversarial perturbations in ultrasound segmentation can be partially mitigated with input preprocessing, but frequency-domain perturbations remain resistant to tested defenses, revealing modality-specific challenges in adversarial robustness.

Abstract: Introduction: Deep learning-based segmentation models are increasingly integrated into clinical imaging workflows, yet their robustness to adversarial perturbations remains incompletely characterized, particularly for ultrasound images. We evaluated adversarial attacks and inference-time defenses for thyroid nodule segmentation in B-mode ultrasound. Methods: Two black-box adversarial attacks were developed: (1) Structured Speckle Amplification Attack (SSAA), which injects boundary-targeted noise, and (2) Frequency-Domain Ultrasound Attack (FDUA), which applies bandpass-filtered phase perturbations in the Fourier domain. Three inference-time mitigations were evaluated on adversarial images: randomized preprocessing with test-time augmentation, deterministic input denoising, and stochastic ensemble inference with consistency-aware aggregation. Experiments were conducted on a U-Net segmentation model trained on cine-clips from a database of 192 thyroid nodules. Results: The baseline model achieved a mean Dice similarity coefficient (DSC) of 0.76 (SD 0.20) on unperturbed images. SSAA reduced DSC by 0.29 (SD 0.20) while maintaining high visual similarity (SSIM = 0.94). FDUA resulted in a smaller DSC reduction of 0.11 (SD 0.09) with lower visual fidelity (SSIM = 0.82). Against SSAA, all three defenses significantly improved DSC after correction, with deterministic denoising showing the largest recovery (+0.10, p < 0.001), followed by randomized preprocessing (+0.09, p < 0.001), and stochastic ensemble inference (+0.08, p = 0.002). No defense achieved statistically significant improvement against FDUA. Conclusion: Spatial-domain adversarial perturbations in ultrasound segmentation showed partial mitigation with input preprocessing, whereas frequency-domain perturbations were not mitigated by the defenses, highlighting modality-specific challenges in adversarial robustness evaluation.

</details>


### [15] [Automatic Map Density Selection for Locally-Performant Visual Place Recognition](https://arxiv.org/abs/2602.21473)
*Somayeh Hussaini,Tobias Fischer,Michael Milford*

Main category: cs.CV

TL;DR: Dynamic VPR mapping approach that automatically selects optimal map density to meet user-specified local recall requirements across different parts of an environment, rather than just global average performance.


<details>
  <summary>Details</summary>
Motivation: Existing VPR systems use fixed, engineering-driven map densities that don't ensure specific performance requirements across different local areas. There's a need to guarantee that systems meet user-defined local performance targets rather than just average global performance.

Method: Uses pairs of reference traverses from target environment to model match patterns across different map densities. Predicts required density to meet two user requirements: target Local Recall@1 level and proportion of environment where this must be met (Recall Achievement Rate - RAR).

Result: System consistently achieves/exceeds specified local recall level over at least user-specified proportion of environment. Reliably selects correct operating point in map density, avoiding unnecessary over-densification. Shows conventional global Recall@1 is poor predictor of operationally meaningful RAR metric.

Conclusion: Proposed dynamic mapping approach enables VPR systems to meet user-defined local performance requirements, addressing critical gap in translating VPR from lab to long-term deployment by ensuring predictable performance across different parts of environment.

Abstract: A key challenge in translating Visual Place Recognition (VPR) from the lab to long-term deployment is ensuring a priori that a system can meet user-specified performance requirements across different parts of an environment, rather than just on average globally. A critical mechanism for controlling local VPR performance is the density of the reference mapping database, yet this factor is largely neglected in existing work, where benchmark datasets with fixed, engineering-driven (sensors, storage, GPS frequency) sampling densities are typically used. In this paper, we propose a dynamic VPR mapping approach that uses pairs of reference traverses from the target environment to automatically select an appropriate map density to satisfy two user-defined requirements: (1) a target Local Recall@1 level, and (2) the proportion of the operational environment over which this requirement must be met or exceeded, which we term the Recall Achievement Rate (RAR). Our approach is based on the hypothesis that match patterns between multiple reference traverses, evaluated across different map densities, can be modelled to predict the density required to meet these performance targets on unseen deployment data. Through extensive experiments across multiple VPR methods and the Nordland and Oxford RobotCar benchmarks, we show that our system consistently achieves or exceeds the specified local recall level over at least the user-specified proportion of the environment. Comparisons with alternative baselines demonstrate that our approach reliably selects the correct operating point in map density, avoiding unnecessary over-densification. Finally, ablation studies and analysis evaluate sensitivity to reference map choice and local space definitions, and reveal that conventional global Recall@1 is a poor predictor of the often more operationally meaningful RAR metric.

</details>


### [16] [Unified Unsupervised and Sparsely-Supervised 3D Object Detection by Semantic Pseudo-Labeling and Prototype Learning](https://arxiv.org/abs/2602.21484)
*Yushen He*

Main category: cs.CV

TL;DR: SPL: A unified framework for unsupervised and sparsely-supervised 3D object detection using semantic pseudo-labeling and prototype learning to reduce annotation dependency.


<details>
  <summary>Details</summary>
Motivation: Current 3D object detection relies heavily on large-scale manual annotations, limiting scalability and adaptability. Existing unsupervised and sparsely-supervised methods suffer from low-quality pseudo-labels, unstable feature mining, and lack of unified training frameworks.

Method: SPL generates high-quality pseudo-labels by integrating image semantics, point cloud geometry, and temporal cues. These pseudo-labels serve as probabilistic priors in a multi-stage prototype learning strategy with memory-based initialization and momentum-based prototype updating to stabilize feature representation learning.

Result: Extensive experiments on KITTI and nuScenes datasets show SPL significantly outperforms state-of-the-art methods in both unsupervised and sparsely-supervised 3D object detection settings.

Conclusion: SPL provides a robust and generalizable solution for learning 3D object detectors with minimal or no manual annotations, addressing key challenges in annotation-efficient 3D perception.

Abstract: 3D object detection is essential for autonomous driving and robotic perception, yet its reliance on large-scale manually annotated data limits scalability and adaptability. To reduce annotation dependency, unsupervised and sparsely-supervised paradigms have emerged. However, they face intertwined challenges: low-quality pseudo-labels, unstable feature mining, and a lack of a unified training framework. This paper proposes SPL, a unified training framework for both Unsupervised and Sparsely-Supervised 3D Object Detection via Semantic Pseudo-labeling and prototype Learning. SPL first generates high-quality pseudo-labels by integrating image semantics, point cloud geometry, and temporal cues, producing both 3D bounding boxes for dense objects and 3D point labels for sparse ones. These pseudo-labels are not used directly but as probabilistic priors within a novel, multi-stage prototype learning strategy. This strategy stabilizes feature representation learning through memory-based initialization and momentum-based prototype updating, effectively mining features from both labeled and unlabeled data. Extensive experiments on KITTI and nuScenes datasets demonstrate that SPL significantly outperforms state-of-the-art methods in both settings. Our work provides a robust and generalizable solution for learning 3D object detectors with minimal or no manual annotations.

</details>


### [17] [See It, Say It, Sorted: An Iterative Training-Free Framework for Visually-Grounded Multimodal Reasoning in LVLMs](https://arxiv.org/abs/2602.21497)
*Yongchang Zhang,Xianzheng Ma,Tianyi Liu,Guangquan Zhou,Yang Chen*

Main category: cs.CV

TL;DR: A training-free framework that reduces visual hallucination in multimodal reasoning by supervising each reasoning step with visual evidence at test time.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought reasoning in multimodal models is vulnerable to visual hallucination propagation, where one incorrect visual interpretation can cascade through subsequent reasoning steps. Existing RL-based solutions are costly, model-specific, and difficult to generalize.

Method: Proposes a lightweight, plug-and-play framework that constructs a textual visual-evidence pool to guide reasoning generation. Uses a visual decider module to dynamically extract additional evidence from images when needed, ensuring each reasoning token is visually justified.

Result: Achieves 16.5%-29.5% improvements on TreeBench and 13.7% RH-AUC gains on RH-Bench, substantially reducing hallucination rates while improving reasoning accuracy without additional training across multiple LVLM backbones.

Conclusion: The training-free approach effectively mitigates visual hallucination propagation in multimodal reasoning, offering a practical, generalizable alternative to costly RL-based methods while maintaining or improving reasoning accuracy.

Abstract: Recent large vision-language models (LVLMs) have demonstrated impressive reasoning ability by generating long chain-of-thought (CoT) responses. However, CoT reasoning in multimodal contexts is highly vulnerable to visual hallucination propagation: once an intermediate reasoning step becomes inconsistent with the visual evidence, subsequent steps-even if logically valid-can still lead to incorrect final answers. Existing solutions attempt to mitigate this issue by training models to "think with images" via reinforcement learning (RL). While effective, these methods are costly, model-specific, and difficult to generalize across architectures. Differently, we present a lightweight method that bypasses RL training and provides an iterative, training-free, plug-and-play framework for visually-grounded multimodal reasoning. Our key idea is to supervise each reasoning step at test time with visual evidence, ensuring that every decoded token is justified by corresponding visual cues. Concretely, we construct a textual visual-evidence pool that guides the model's reasoning generation. When existing evidence is insufficient, a visual decider module dynamically extracts additional relevant evidence from the image based on the ongoing reasoning context, expanding the pool until the model achieves sufficient visual certainty to terminate reasoning and produce the final answer. Extensive experiments on multiple LVLM backbones and benchmarks demonstrate the effectiveness of our approach. Our method achieves 16.5%-29.5% improvements on TreeBench and 13.7% RH-AUC gains on RH-Bench, substantially reducing hallucination rates while improving reasoning accuracy without additional training.

</details>


### [18] [Easy3E: Feed-Forward 3D Asset Editing via Rectified Voxel Flow](https://arxiv.org/abs/2602.21499)
*Shimin Hu,Yuanyi Wei,Fei Zha,Yudong Guo,Juyong Zhang*

Main category: cs.CV

TL;DR: A feedforward 3D editing framework using TRELLIS generative backbone that enables fast, globally consistent 3D model editing from a single view, addressing multi-view inconsistency and appearance fidelity issues.


<details>
  <summary>Details</summary>
Motivation: Existing 3D editing methods suffer from computational intensity (scene-by-scene iterative optimization) and multi-view inconsistency, creating a need for efficient and consistent 3D editing solutions.

Method: Proposes two key innovations: 1) Voxel FlowEdit - edit-driven flow in sparse voxel latent space for globally consistent 3D deformation in single pass; 2) Normal-guided single to multi-view generation module as external appearance prior to restore high-fidelity details and textures.

Result: Experiments demonstrate the method enables fast, globally consistent, and high-fidelity 3D model editing, overcoming limitations of previous approaches.

Conclusion: The proposed framework effectively addresses key challenges in 3D editing by combining geometric consistency through Voxel FlowEdit with appearance fidelity restoration through normal-guided generation, offering a practical solution for efficient 3D model modification.

Abstract: Existing 3D editing methods rely on computationally intensive scene-by-scene iterative optimization and suffer from multi-view inconsistency. We propose an effective and fully feedforward 3D editing framework based on the TRELLIS generative backbone, capable of modifying 3D models from a single editing view. Our framework addresses two key issues: adapting training-free 2D editing to structured 3D representations, and overcoming the bottleneck of appearance fidelity in compressed 3D features. To ensure geometric consistency, we introduce Voxel FlowEdit, an edit-driven flow in the sparse voxel latent space that achieves globally consistent 3D deformation in a single pass. To restore high-fidelity details, we develop a normal-guided single to multi-view generation module as an external appearance prior, successfully recovering high-frequency textures. Experiments demonstrate that our method enables fast, globally consistent, and high-fidelity 3D model editing.

</details>


### [19] [AHAN: Asymmetric Hierarchical Attention Network for Identical Twin Face Verification](https://arxiv.org/abs/2602.21503)
*Hoang-Nhat Nguyen*

Main category: cs.CV

TL;DR: AHAN improves identical twin face verification by 3.4% over SOTA using hierarchical attention and facial asymmetry analysis.


<details>
  <summary>Details</summary>
Motivation: Current face recognition systems fail on identical twins (88.9% accuracy vs 99.8% on standard benchmarks), exposing critical biometric security vulnerabilities due to overwhelming genetic similarity.

Method: Proposes Asymmetric Hierarchical Attention Network (AHAN) with: 1) Hierarchical Cross-Attention for multi-scale semantic region analysis, 2) Facial Asymmetry Attention Module for left-right half cross-attention to capture subtle asymmetric patterns, and 3) Twin-Aware Pair-Wise Cross-Attention training regularization using each subject's twin as hardest distractor.

Result: AHAN achieves 92.3% twin verification accuracy on ND_TWIN dataset, representing a 3.4% improvement over state-of-the-art methods.

Conclusion: The proposed AHAN architecture effectively addresses the extreme fine-grained challenge of identical twin verification through specialized attention mechanisms that capture subtle, non-genetic variations and facial asymmetries unique to individuals.

Abstract: Identical twin face verification represents an extreme fine-grained recognition challenge where even state-of-the-art systems fail due to overwhelming genetic similarity. Current face recognition methods achieve over 99.8% accuracy on standard benchmarks but drop dramatically to 88.9% when distinguishing identical twins, exposing critical vulnerabilities in biometric security systems. The difficulty lies in learning features that capture subtle, non-genetic variations that uniquely identify individuals. We propose the Asymmetric Hierarchical Attention Network (AHAN), a novel architecture specifically designed for this challenge through multi-granularity facial analysis. AHAN introduces a Hierarchical Cross-Attention (HCA) module that performs multi-scale analysis on semantic facial regions, enabling specialized processing at optimal resolutions. We further propose a Facial Asymmetry Attention Module (FAAM) that learns unique biometric signatures by computing cross-attention between left and right facial halves, capturing subtle asymmetric patterns that differ even between twins. To ensure the network learns truly individuating features, we introduce Twin-Aware Pair-Wise Cross-Attention (TA-PWCA), a training-only regularization strategy that uses each subject's own twin as the hardest possible distractor. Extensive experiments on the ND_TWIN dataset demonstrate that AHAN achieves 92.3% twin verification accuracy, representing a 3.4% improvement over state-of-the-art methods.

</details>


### [20] [Which Tool Response Should I Trust? Tool-Expertise-Aware Chest X-ray Agent with Multimodal Agentic Learning](https://arxiv.org/abs/2602.21517)
*Zheang Huai,Honglong Yang,Xiaomeng Li*

Main category: cs.CV

TL;DR: A framework for medical AI agents to learn which error-prone tools to trust via reinforcement learning, specifically applied to chest X-ray analysis with multimodal support.


<details>
  <summary>Details</summary>
Motivation: Medical AI tools are error-prone and can produce contradictory responses, but existing research lacks understanding of tools' realistic reliability and cannot effectively resolve tool conflicts.

Method: Introduces TEA-CXA (Tool-Expertise-Aware Chest X-ray Agent) that uses agentic learning to empirically learn tool trustworthiness across multimodal queries. Extends RL codebases for multi-turn tool-calling to support multimodal contexts, multiple tool calls per turn, parallel inference, and multi-image queries.

Result: TEA-CXA outperforms state-of-the-art methods and comprehensive baselines in experiments.

Conclusion: The framework enables effective resolution of tool conflicts in medical AI by learning practical tool trustworthiness, with code applicable to general medical research on multi-turn tool-calling RL in multimodal settings.

Abstract: AI agents with tool-use capabilities show promise for integrating the domain expertise of various tools. In the medical field, however, tools are usually AI models that are inherently error-prone and can produce contradictory responses. Existing research on medical agents lacks sufficient understanding of the tools' realistic reliability and thus cannot effectively resolve tool conflicts. To address this gap, this paper introduces a framework that enables an agent to interact with tools and empirically learn their practical trustworthiness across different types of multimodal queries via agentic learning. As a concrete instantiation, we focus on chest X-ray analysis and present a tool-expertise-aware chest X-ray agent (TEA-CXA). When tool outputs disagree, the agent experimentally accepts or rejects multimodal tool results, receives rewards, and learns which tool to trust for each query type. Importantly, TEA-CXA extends existing codebases for reinforcement learning with multi-turn tool-calling that focus on textual inputs, to support multimodal contexts effectively. In addition, we enhance the codebase for medical use scenarios by supporting multiple tool calls in one turn, parallel tool inference, and multi-image accommodation within a single user query. Our code framework is applicable to general medical research on multi-turn tool-calling reinforcement learning in multimodal settings. Experiments show that TEA-CXA outperforms the state-of-the-art methods and a comprehensive set of baselines. Code will be released.

</details>


### [21] [Pseudo-View Enhancement via Confidence Fusion for Unposed Sparse-View Reconstruction](https://arxiv.org/abs/2602.21535)
*Beizhen Zhao,Sicheng Yu,Guanzhi Ding,Yu Hu,Hao Wang*

Main category: cs.CV

TL;DR: A novel framework for 3D scene reconstruction from unposed sparse viewpoints in outdoor scenes using bidirectional pseudo frame restoration and scene perception Gaussian management.


<details>
  <summary>Details</summary>
Motivation: 3D scene reconstruction from unposed sparse viewpoints is challenging in outdoor scenes due to complex lighting and scale variation. Direct use of diffusion models for pseudo frame synthesis introduces unreasonable geometry that harms reconstruction quality.

Method: 1) Bidirectional pseudo frame restoration method using diffusion-based synthesis guided by adjacent frames with lightweight pseudo-view deblur model and confidence mask inference algorithm. 2) Scene perception Gaussian management strategy that optimizes Gaussians based on joint depth-density information.

Result: Significantly enhances reconstruction completeness, suppresses floating artifacts, and improves geometric consistency under extreme view sparsity. Experiments on outdoor benchmarks demonstrate substantial gains over existing methods in both fidelity and stability.

Conclusion: The proposed framework effectively addresses the challenges of sparse-view outdoor reconstruction, achieving high-quality results through innovative bidirectional restoration and Gaussian optimization techniques.

Abstract: 3D scene reconstruction under unposed sparse viewpoints is a highly challenging yet practically important problem, especially in outdoor scenes due to complex lighting and scale variation. With extremely limited input views, directly utilizing diffusion model to synthesize pseudo frames will introduce unreasonable geometry, which will harm the final reconstruction quality. To address these issues, we propose a novel framework for sparse-view outdoor reconstruction that achieves high-quality results through bidirectional pseudo frame restoration and scene perception Gaussian management. Specifically, we introduce a bidirectional pseudo frame restoration method that restores missing content by diffusion-based synthesis guided by adjacent frames with a lightweight pseudo-view deblur model and confidence mask inference algorithm. Then we propose a scene perception Gaussian management strategy that optimize Gaussians based on joint depth-density information. These designs significantly enhance reconstruction completeness, suppress floating artifacts and improve overall geometric consistency under extreme view sparsity. Experiments on outdoor benchmarks demonstrate substantial gains over existing methods in both fidelity and stability.

</details>


### [22] [IHF-Harmony: Multi-Modality Magnetic Resonance Images Harmonization using Invertible Hierarchy Flow Model](https://arxiv.org/abs/2602.21536)
*Pengli Zhu,Yitao Zhu,Haowen Pang,Anqi Qiu*

Main category: cs.CV

TL;DR: IHF-Harmony is a unified invertible hierarchy flow framework for multi-modality MRI harmonization using unpaired data, ensuring bijective mapping and lossless reconstruction to prevent anatomical distortion.


<details>
  <summary>Details</summary>
Motivation: Current MRI harmonization methods have poor scalability across modalities and rely on traveling subject datasets, limiting their practical application in large-scale multi-site imaging studies.

Method: Uses invertible hierarchy flow (IHF) with hierarchical subtractive coupling to remove artefact-related features, combined with artefact-aware normalization (AAN) for anatomy-fixed feature modulation, plus anatomy and artefact consistency loss objectives.

Result: Outperforms existing methods in both anatomical fidelity and downstream task performance across multiple MRI modalities, demonstrating robust harmonization capabilities.

Conclusion: IHF-Harmony provides a scalable solution for multi-modality MRI harmonization using unpaired data, facilitating large-scale multi-site imaging studies with preserved anatomical integrity.

Abstract: Retrospective MRI harmonization is limited by poor scalability across modalities and reliance on traveling subject datasets. To address these challenges, we introduce IHF-Harmony, a unified invertible hierarchy flow framework for multi-modality harmonization using unpaired data. By decomposing the translation process into reversible feature transformations, IHF-Harmony guarantees bijective mapping and lossless reconstruction to prevent anatomical distortion. Specifically, an invertible hierarchy flow (IHF) performs hierarchical subtractive coupling to progressively remove artefact-related features, while an artefact-aware normalization (AAN) employs anatomy-fixed feature modulation to accurately transfer target characteristics. Combined with anatomy and artefact consistency loss objectives, IHF-Harmony achieves high-fidelity harmonization that retains source anatomy. Experiments across multiple MRI modalities demonstrate that IHF-Harmony outperforms existing methods in both anatomical fidelity and downstream task performance, facilitating robust harmonization for large-scale multi-site imaging studies. Code will be released upon acceptance.

</details>


### [23] [VasGuideNet: Vascular Topology-Guided Couinaud Liver Segmentation with Structural Contrastive Loss](https://arxiv.org/abs/2602.21539)
*Chaojie Shen,Jingjun Gu,Zihao Zhao,Ruocheng Li,Cunyuan Yang,Jiajun Bu,Lei Wu*

Main category: cs.CV

TL;DR: VasGuideNet: A Couinaud liver segmentation framework using vascular topology guidance via GCNs and cross-attention fusion, achieving state-of-the-art performance on liver vessel datasets.


<details>
  <summary>Details</summary>
Motivation: Existing Couinaud liver segmentation methods rely only on image intensity and spatial location, lacking explicit vascular topology modeling, leading to indistinct boundaries near vessels and poor generalization under anatomical variability.

Method: Proposes VasGuideNet with three key components: 1) Encodes skeletonized vessels, Euclidean distance transform geometry, and kNN connectivity into topology features using Graph Convolutional Networks, 2) Injects these features into a 3D encoder-decoder backbone via cross-attention fusion module, 3) Introduces Structural Contrastive Loss with global memory bank to improve inter-class separability and anatomical consistency.

Result: Achieves Dice scores of 83.68% on Task08_HepaticVessel and 76.65% on private LASSD dataset with RVDs of 1.68 and 7.08 respectively. Consistently outperforms UNETR, Swin UNETR, and G-UNETR++ across datasets with higher Dice/mIoU and lower RVD.

Conclusion: VasGuideNet demonstrates effectiveness for anatomically consistent Couinaud liver segmentation by explicitly modeling vascular topology, addressing limitations of existing intensity-based methods and showing strong generalization across datasets.

Abstract: Accurate Couinaud liver segmentation is critical for preoperative surgical planning and tumor localization.However, existing methods primarily rely on image intensity and spatial location cues, without explicitly modeling vascular topology. As a result, they often produce indistinct boundaries near vessels and show limited generalization under anatomical variability.We propose VasGuideNet, the first Couinaud segmentation framework explicitly guided by vascular topology. Specifically, skeletonized vessels, Euclidean distance transform (EDT)--derived geometry, and k-nearest neighbor (kNN) connectivity are encoded into topology features using Graph Convolutional Networks (GCNs). These features are then injected into a 3D encoder--decoder backbone via a cross-attention fusion module. To further improve inter-class separability and anatomical consistency, we introduce a Structural Contrastive Loss (SCL) with a global memory bank.On Task08_HepaticVessel and our private LASSD dataset, VasGuideNet achieves Dice scores of 83.68% and 76.65% with RVDs of 1.68 and 7.08, respectively. It consistently outperforms representative baselines including UNETR, Swin UNETR, and G-UNETR++, delivering higher Dice/mIoU and lower RVD across datasets, demonstrating its effectiveness for anatomically consistent segmentation. Code is available at https://github.com/Qacket/VasGuideNet.git.

</details>


### [24] [Generalizing Visual Geometry Priors to Sparse Gaussian Occupancy Prediction](https://arxiv.org/abs/2602.21552)
*Changqing Zhou,Yueru Luo,Changhao Chen*

Main category: cs.CV

TL;DR: GPOcc is a framework that leverages visual geometry priors for monocular 3D occupancy prediction, using Gaussian primitives for volumetric inference and incremental updates for streaming input.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for 3D scene understanding rely on depth priors but make limited use of 3D cues, restricting performance. Visual geometry models like VGGT provide rich 3D priors but only operate at visible surfaces rather than volumetric interiors, motivating the need to better leverage geometry priors for occupancy prediction.

Method: Extends surface points inward along camera rays to generate volumetric samples, represented as Gaussian primitives for probabilistic occupancy inference. For streaming input, uses a training-free incremental update strategy that fuses per-frame Gaussians into a unified global representation.

Result: Significant improvements: +9.99 mIoU in monocular setting and +11.79 mIoU in streaming setting over prior SOTA. Under same depth prior, achieves +6.73 mIoU while running 2.65× faster.

Conclusion: GPOcc demonstrates more effective and efficient leveraging of geometry priors for 3D occupancy prediction, with substantial performance gains and faster inference compared to existing methods.

Abstract: Accurate 3D scene understanding is essential for embodied intelligence, with occupancy prediction emerging as a key task for reasoning about both objects and free space. Existing approaches largely rely on depth priors (e.g., DepthAnything) but make only limited use of 3D cues, restricting performance and generalization. Recently, visual geometry models such as VGGT have shown strong capability in providing rich 3D priors, but similar to monocular depth foundation models, they still operate at the level of visible surfaces rather than volumetric interiors, motivating us to explore how to more effectively leverage these increasingly powerful geometry priors for 3D occupancy prediction. We present GPOcc, a framework that leverages generalizable visual geometry priors (GPs) for monocular occupancy prediction. Our method extends surface points inward along camera rays to generate volumetric samples, which are represented as Gaussian primitives for probabilistic occupancy inference. To handle streaming input, we further design a training-free incremental update strategy that fuses per-frame Gaussians into a unified global representation. Experiments on Occ-ScanNet and EmbodiedOcc-ScanNet demonstrate significant gains: GPOcc improves mIoU by +9.99 in the monocular setting and +11.79 in the streaming setting over prior state of the art. Under the same depth prior, it achieves +6.73 mIoU while running 2.65$\times$ faster. These results highlight that GPOcc leverages geometry priors more effectively and efficiently. Code will be released at https://github.com/JuIvyy/GPOcc.

</details>


### [25] [MultiAnimate: Pose-Guided Image Animation Made Extensible](https://arxiv.org/abs/2602.21581)
*Yingcheng Hu,Haowen Gong,Chuanguang Yang,Zhulin An,Yongjun Xu,Songhua Liu*

Main category: cs.CV

TL;DR: A diffusion-based framework for multi-character human image animation that addresses identity confusion and occlusion issues through novel identifier mechanisms and scalable training.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based human animation methods are limited to single characters and fail in multi-character scenarios due to identity confusion and implausible occlusions between characters.

Method: Proposes an extensible framework built on Diffusion Transformers (DiTs) with two novel components: Identifier Assigner (captures per-person positional cues) and Identifier Adapter (handles inter-person spatial relationships), using a mask-driven scheme and scalable training strategy.

Result: Achieves state-of-the-art performance in multi-character image animation, generalizes to scenarios with more characters than seen during training, and maintains compatibility with single-character cases despite being trained only on two-character data.

Conclusion: The proposed framework effectively solves multi-character animation challenges through collaborative identifier mechanisms and scalable training, demonstrating superior performance over existing diffusion-based baselines.

Abstract: Pose-guided human image animation aims to synthesize realistic videos of a reference character driven by a sequence of poses. While diffusion-based methods have achieved remarkable success, most existing approaches are limited to single-character animation. We observe that naively extending these methods to multi-character scenarios often leads to identity confusion and implausible occlusions between characters. To address these challenges, in this paper, we propose an extensible multi-character image animation framework built upon modern Diffusion Transformers (DiTs) for video generation. At its core, our framework introduces two novel components-Identifier Assigner and Identifier Adapter - which collaboratively capture per-person positional cues and inter-person spatial relationships. This mask-driven scheme, along with a scalable training strategy, not only enhances flexibility but also enables generalization to scenarios with more characters than those seen during training. Remarkably, trained on only a two-character dataset, our model generalizes to multi-character animation while maintaining compatibility with single-character cases. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in multi-character image animation, surpassing existing diffusion-based baselines.

</details>


### [26] [SEF-MAP: Subspace-Decomposed Expert Fusion for Robust Multimodal HD Map Prediction](https://arxiv.org/abs/2602.21589)
*Haoxiang Fu,Lingfeng Zhang,Hao Li,Ruibing Hu,Zhengrong Li,Guanjing Liu,Zimu Tan,Long Chen,Hangjun Ye,Xiaoshuai Hao*

Main category: cs.CV

TL;DR: SEFMAP is a Subspace-Expert Fusion framework for robust multimodal HD map prediction that disentangles BEV features into four semantic subspaces with dedicated experts and uses uncertainty-aware gating for adaptive fusion.


<details>
  <summary>Details</summary>
Motivation: HD maps are essential for autonomous driving, but multi-modal fusion suffers from inconsistency between camera and LiDAR modalities, leading to performance degradation under low-light conditions, occlusions, or sparse point clouds.

Method: Disentangles BEV features into four semantic subspaces (LiDAR-private, Image-private, Shared, and Interaction) with dedicated experts. Uses uncertainty-aware gating at BEV-cell level to adaptively combine expert outputs, plus distribution-aware masking during training with EMA-statistical surrogate features and specialization loss.

Result: Achieves state-of-the-art performance on nuScenes and Argoverse2 benchmarks, surpassing prior methods by +4.2% and +4.8% in mAP respectively.

Conclusion: SEFMAP provides a robust and effective solution for multi-modal HD map prediction under diverse and degraded conditions through explicit subspace disentanglement and adaptive expert fusion.

Abstract: High-definition (HD) maps are essential for autonomous driving, yet multi-modal fusion often suffers from inconsistency between camera and LiDAR modalities, leading to performance degradation under low-light conditions, occlusions, or sparse point clouds. To address this, we propose SEFMAP, a Subspace-Expert Fusion framework for robust multimodal HD map prediction. The key idea is to explicitly disentangle BEV features into four semantic subspaces: LiDAR-private, Image-private, Shared, and Interaction. Each subspace is assigned a dedicated expert, thereby preserving modality-specific cues while capturing cross-modal consensus. To adaptively combine expert outputs, we introduce an uncertainty-aware gating mechanism at the BEV-cell level, where unreliable experts are down-weighted based on predictive variance, complemented by a usage balance regularizer to prevent expert collapse. To enhance robustness in degraded conditions and promote role specialization, we further propose distribution-aware masking: during training, modality-drop scenarios are simulated using EMA-statistical surrogate features, and a specialization loss enforces distinct behaviors of private, shared, and interaction experts across complete and masked inputs. Experiments on nuScenes and Argoverse2 benchmarks demonstrate that SEFMAP achieves state-of-the-art performance, surpassing prior methods by +4.2% and +4.8% in mAP, respectively. SEF-MAPprovides a robust and effective solution for multi-modal HD map prediction under diverse and degraded conditions.

</details>


### [27] [CADC: Content Adaptive Diffusion-Based Generative Image Compression](https://arxiv.org/abs/2602.21591)
*Xihua Sheng,Lingyu Zhu,Tianyu Zhang,Dong Liu,Shiqi Wang,Jing Wang*

Main category: cs.CV

TL;DR: A content-adaptive diffusion-based image codec with three innovations: adaptive quantization, information concentration, and bitrate-free textual conditioning to overcome limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based image compression methods lack effective content adaptation due to three critical limitations: isotropic quantization fails to adapt to spatial complexity, information concentration bottleneck prevents adaptive semantic preservation, and textual conditioning strategies are inefficient with bitrate overhead or generic prompts.

Method: Three technical innovations: 1) Uncertainty-Guided Adaptive Quantization that learns spatial uncertainty maps to align quantization distortion with content characteristics; 2) Auxiliary Decoder-Guided Information Concentration using a lightweight auxiliary decoder to enforce content-aware information preservation; 3) Bitrate-Free Adaptive Textual Conditioning that derives content-aware textual descriptions from auxiliary reconstructed images without bitrate cost.

Result: The proposed method achieves content-adaptive compression by dynamically aligning encoder representation and decoder generative prior with semantic and structural characteristics of input images, overcoming the three identified limitations of existing approaches.

Conclusion: The proposed content-adaptive diffusion-based image codec effectively addresses the key limitations in existing methods by introducing adaptive quantization, information concentration, and efficient textual conditioning, enabling realistic reconstruction at ultra-low bitrates through better content adaptation.

Abstract: Diffusion-based generative image compression has demonstrated remarkable potential for achieving realistic reconstruction at ultra-low bitrates. The key to unlocking this potential lies in making the entire compression process content-adaptive, ensuring that the encoder's representation and the decoder's generative prior are dynamically aligned with the semantic and structural characteristics of the input image. However, existing methods suffer from three critical limitations that prevent effective content adaptation. First, isotropic quantization applies a uniform quantization step, failing to adapt to the spatially varying complexity of image content and creating a misalignment with the diffusion model's noise-dependent prior. Second, the information concentration bottleneck -- arising from the dimensional mismatch between the high-dimensional noisy latent and the diffusion decoder's fixed input -- prevents the model from adaptively preserving essential semantic information in the primary channels. Third, existing textual conditioning strategies either need significant textual bitrate overhead or rely on generic, content-agnostic textual prompts, thereby failing to provide adaptive semantic guidance efficiently. To overcome these limitations, we propose a content-adaptive diffusion-based image codec with three technical innovations: 1) an Uncertainty-Guided Adaptive Quantization method that learns spatial uncertainty maps to adaptively align quantization distortion with content characteristics; 2) an Auxiliary Decoder-Guided Information Concentration method that uses a lightweight auxiliary decoder to enforce content-aware information preservation in the primary latent channels; and 3) a Bitrate-Free Adaptive Textual Conditioning method that derives content-aware textual descriptions from the auxiliary reconstructed image, enabling semantic guidance without bitrate cost.

</details>


### [28] [A Hidden Semantic Bottleneck in Conditional Embeddings of Diffusion Transformers](https://arxiv.org/abs/2602.21596)
*Trung X. Pham,Kang Zhang,Ji Woo Hong,Chang D. Yoo*

Main category: cs.CV

TL;DR: Diffusion Transformers have redundant conditional embeddings with extreme angular similarity (>99%), where semantic information concentrates in a small subset of dimensions. Pruning low-magnitude dimensions (up to 2/3 of embedding space) doesn't harm generation quality.


<details>
  <summary>Details</summary>
Motivation: Despite Diffusion Transformers achieving SOTA performance, the structure of their learned conditional embeddings remains poorly understood. The paper aims to systematically study these embeddings to uncover their properties and potential inefficiencies.

Method: Systematic study of conditional embeddings in Diffusion Transformers, analyzing angular similarity of class-conditioned embeddings, examining semantic concentration across dimensions, and testing pruning of low-magnitude dimensions to evaluate redundancy.

Result: Found extreme angular similarity (>99% for ImageNet-1K, >99.9% for continuous-condition tasks), semantic information concentrated in head dimensions, and that pruning up to two-thirds of embedding space doesn't affect generation quality (sometimes improves it).

Conclusion: Reveals a semantic bottleneck in Transformer-based diffusion models, providing insights into how semantics are encoded and suggesting opportunities for more efficient conditioning mechanisms through dimension pruning.

Abstract: Diffusion Transformers have achieved state-of-the-art performance in class-conditional and multimodal generation, yet the structure of their learned conditional embeddings remains poorly understood. In this work, we present the first systematic study of these embeddings and uncover a notable redundancy: class-conditioned embeddings exhibit extreme angular similarity, exceeding 99\% on ImageNet-1K, while continuous-condition tasks such as pose-guided image generation and video-to-audio generation reach over 99.9\%. We further find that semantic information is concentrated in a small subset of dimensions, with head dimensions carrying the dominant signal and tail dimensions contributing minimally. By pruning low-magnitude dimensions--removing up to two-thirds of the embedding space--we show that generation quality and fidelity remain largely unaffected, and in some cases improve. These results reveal a semantic bottleneck in Transformer-based diffusion models, providing new insights into how semantics are encoded and suggesting opportunities for more efficient conditioning mechanisms.

</details>


### [29] [Virtual Biopsy for Intracranial Tumors Diagnosis on MRI](https://arxiv.org/abs/2602.21613)
*Xinzhe Luo,Shuai Shao,Yan Wang,Jiangtao Wang,Yutong Bai,Jianguo Zhang*

Main category: cs.CV

TL;DR: A Virtual Biopsy framework using MRI and vision-language models achieves over 90% accuracy for non-invasive pathology prediction of deep intracranial tumors, addressing data scarcity and sampling bias issues in traditional biopsy methods.


<details>
  <summary>Details</summary>
Motivation: Deep intracranial tumors in eloquent brain regions present diagnostic challenges. Stereotactic biopsy carries risks of hemorrhage, neurological deficits, and sampling bias due to tumor heterogeneity. Non-invasive MRI-based pathology prediction is needed for holistic tumor assessment and clinical decision-making.

Method: Proposes Virtual Biopsy framework with three components: MRI-Processor for standardization, Tumor-Localizer using vision-language models for coarse-to-fine localization via weak supervision, and Adaptive-Diagnoser with Masked Channel Attention mechanism fusing local discriminative features with global contexts. Uses ICT-MRI dataset with 249 biopsy-verified cases.

Result: Achieves over 90% accuracy, outperforming baseline methods by more than 20%.

Conclusion: The Virtual Biopsy framework provides accurate non-invasive pathology prediction for deep intracranial tumors, addressing data scarcity and biopsy limitations while enabling holistic tumor assessment for clinical decision-making.

Abstract: Deep intracranial tumors situated in eloquent brain regions controlling vital functions present critical diagnostic challenges. Clinical practice has shifted toward stereotactic biopsy for pathological confirmation before treatment. Yet biopsy carries inherent risks of hemorrhage and neurological deficits and struggles with sampling bias due to tumor spatial heterogeneity, because pathological changes are typically region-selective rather than tumor-wide. Therefore, advancing non-invasive MRI-based pathology prediction is essential for holistic tumor assessment and modern clinical decision-making.
  The primary challenge lies in data scarcity: low tumor incidence requires long collection cycles, and annotation demands biopsy-verified pathology from neurosurgical experts. Additionally, tiny lesion volumes lacking segmentation masks cause critical features to be overwhelmed by background noise. To address these challenges, we construct the ICT-MRI dataset - the first public biopsy-verified benchmark with 249 cases across four categories. We propose a Virtual Biopsy framework comprising: MRI-Processor for standardization; Tumor-Localizer employing vision-language models for coarse-to-fine localization via weak supervision; and Adaptive-Diagnoser with a Masked Channel Attention mechanism fusing local discriminative features with global contexts. Experiments demonstrate over 90% accuracy, outperforming baselines by more than 20%.

</details>


### [30] [Tokenizing Semantic Segmentation with RLE](https://arxiv.org/abs/2602.21627)
*Abhineet Singh,Justin Rozeboom,Nilanjan Ray*

Main category: cs.CV

TL;DR: A unified approach for semantic segmentation in images and videos using language modeling to output masks as discrete RLE tokens, with novel tokenization strategies for video and panoptic segmentation.


<details>
  <summary>Details</summary>
Motivation: To create a unified framework for semantic segmentation that works for both images and videos using language modeling techniques, addressing the challenge of handling video segmentation and panoptic segmentation within the same approach.

Method: Uses run length encoding (RLE) to discretize segmentation masks into tokens, trains a modified Pix2Seq model to autoregressively output RLE tokens, develops novel tokenization strategies to compress token sequences for video applications, and incorporates instance information for panoptic segmentation.

Result: The proposed models achieve competitive performance with state-of-the-art methods on two datasets, despite being limited by computational resources.

Conclusion: Language modeling with RLE tokenization provides an effective unified approach for semantic and panoptic segmentation in both images and videos, demonstrating promising results even with resource constraints.

Abstract: This paper presents a new unified approach to semantic segmentation in both images and videos by using language modeling to output the masks as sequences of discrete tokens. We use run length encoding (RLE) to discretize the segmentation masks and then train a modified version of Pix2Seq \cite{p2s} to output these RLE tokens through autoregression. We propose novel tokenization strategies to compress the length of the token sequence to make it practicable to extend this approach to videos. We also show how instance information can be incorporated into the tokenization process to perform panoptic segmentation. We evaluate our proposed models on two datasets to show that they are competitive with the state of the art in spite of being bottlenecked by our limited computational resources.

</details>


### [31] [UniHand: A Unified Model for Diverse Controlled 4D Hand Motion Modeling](https://arxiv.org/abs/2602.21631)
*Zhihao Sun,Tong Wu,Ruirui Tu,Daoguo Dong,Zuxuan Wu*

Main category: cs.CV

TL;DR: UniHand is a unified diffusion framework that combines hand motion estimation and generation into a single conditional synthesis model, handling heterogeneous inputs like visual observations, MANO parameters, and 2D skeletons through a shared latent space.


<details>
  <summary>Details</summary>
Motivation: Current hand motion research is divided into separate estimation and generation tasks, limiting the use of heterogeneous condition signals and preventing knowledge transfer between tasks. This separation fails to handle practical scenarios with occlusions or incomplete inputs effectively.

Method: UniHand uses a unified diffusion-based framework with: 1) a joint variational autoencoder to embed structured signals into a shared latent space, 2) frozen vision backbone for visual encoding, 3) hand perceptron for hand-specific feature extraction, and 4) latent diffusion model for motion synthesis from diverse conditions.

Result: Extensive experiments across multiple benchmarks show UniHand delivers robust and accurate hand motion modeling, maintaining performance under severe occlusions and temporally incomplete inputs.

Conclusion: UniHand successfully unifies hand motion estimation and generation into a single framework, enabling better handling of heterogeneous inputs and knowledge transfer between tasks, while maintaining robustness to challenging real-world conditions.

Abstract: Hand motion plays a central role in human interaction, yet modeling realistic 4D hand motion (i.e., 3D hand pose sequences over time) remains challenging. Research in this area is typically divided into two tasks: (1) Estimation approaches reconstruct precise motion from visual observations, but often fail under hand occlusion or absence; (2) Generation approaches focus on synthesizing hand poses by exploiting generative priors under multi-modal structured inputs and infilling motion from incomplete sequences. However, this separation not only limits the effective use of heterogeneous condition signals that frequently arise in practice, but also prevents knowledge transfer between the two tasks. We present UniHand, a unified diffusion-based framework that formulates both estimation and generation as conditional motion synthesis. UniHand integrates heterogeneous inputs by embedding structured signals into a shared latent space through a joint variational autoencoder, which aligns conditions such as MANO parameters and 2D skeletons. Visual observations are encoded with a frozen vision backbone, while a dedicated hand perceptron extracts hand-specific cues directly from image features, removing the need for complex detection and cropping pipelines. A latent diffusion model then synthesizes consistent motion sequences from these diverse conditions. Extensive experiments across multiple benchmarks demonstrate that UniHand delivers robust and accurate hand motion modeling, maintaining performance under severe occlusions and temporally incomplete inputs.

</details>


### [32] [Axial-Centric Cross-Plane Attention for 3D Medical Image Classification](https://arxiv.org/abs/2602.21636)
*Doyoung Park,Jinsoo Kim,Lohendran Baskaran*

Main category: cs.CV

TL;DR: Proposes an axial-centric cross-plane attention architecture for 3D medical image classification that mimics clinical interpretation workflow where axial plane is primary and other planes provide complementary information.


<details>
  <summary>Details</summary>
Motivation: Existing 3D deep learning methods either process volumetric data holistically or assign equal importance to all anatomical planes, failing to reflect the clinical workflow where radiologists primarily use axial plane as diagnostic reference with coronal/sagittal planes as complementary views.

Method: Uses MedDINOv3 (medical vision foundation model pretrained on axial CT images) as frozen feature extractor for all three planes. Incorporates RICA blocks and intra-plane transformers for plane-specific features, and axial-centric cross-plane transformers that condition axial features on auxiliary plane information.

Result: Outperforms existing 3D and multi-plane models on six MedMNIST3D benchmark datasets in accuracy and AUC. Ablation studies confirm importance of axial-centric query-key-value allocation and directional cross-plane fusion.

Conclusion: Aligning architectural design with clinical interpretation workflows leads to more robust and data-efficient 3D medical image analysis, demonstrating the value of axial-centric modeling over symmetric plane treatment.

Abstract: Clinicians commonly interpret three-dimensional (3D) medical images, such as computed tomography (CT) scans, using multiple anatomical planes rather than as a single volumetric representation. In this multi-planar approach, the axial plane typically serves as the primary acquisition and diagnostic reference, while the coronal and sagittal planes provide complementary spatial information to increase diagnostic confidence. However, many existing 3D deep learning methods either process volumetric data holistically or assign equal importance to all planes, failing to reflect the axial-centric clinical interpretation workflow. To address this gap, we propose an axial-centric cross-plane attention architecture for 3D medical image classification that captures the inherent asymmetric dependencies between different anatomical planes. Our architecture incorporates MedDINOv3, a medical vision foundation model pretrained via self-supervised learning on large-scale axial CT images, as a frozen feature extractor for the axial, coronal, and sagittal planes. RICA blocks and intra-plane transformer encoders capture plane-specific positional and contextual information within each anatomical plane, while axial-centric cross-plane transformer encoders condition axial features on complementary information from auxiliary planes. Experimental results on six datasets from the MedMNIST3D benchmark demonstrate that the proposed architecture consistently outperforms existing 3D and multi-plane models in terms of accuracy and AUC. Ablation studies further confirm the importance of axial-centric query-key-value allocation and directional cross-plane fusion. These results highlight the importance of aligning architectural design with clinical interpretation workflows for robust and data-efficient 3D medical image analysis.

</details>


### [33] [CARE: A Molecular-Guided Foundation Model with Adaptive Region Modeling for Whole Slide Image Analysis](https://arxiv.org/abs/2602.21637)
*Di Zhang,Zhangpeng Gong,Xiaobo Pang,Jiashuai Liu,Junbo Lu,Hao Cui,Jiusong Ge,Zhi Zeng,Kai Yi,Yinghua Li,Si Liu,Tingsong Yu,Haoran Wang,Mireia Crispin-Ortuzar,eimiao Yu,Chen Li,Zeyu Gao*

Main category: cs.CV

TL;DR: CARE is a pathology foundation model that automatically partitions whole-slide images into morphologically relevant regions using molecular guidance, achieving superior performance with less pretraining data.


<details>
  <summary>Details</summary>
Motivation: Existing pathology foundation models rely on natural image backbones not tailored for tissue morphology, failing to capture coherent tissue architecture and limiting interpretability and clinical relevance. They overlook heterogeneous organization of pathological regions of interest.

Method: Two-stage pretraining: (1) self-supervised unimodal pretraining learns morphological representations from 34,277 WSIs without annotations, (2) cross-modal alignment uses RNA and protein profiles to refine adaptive region construction and representation, enabling identification of biologically relevant patterns and irregular coherent tissue regions.

Result: Using only one-tenth of typical pretraining data, CARE achieves superior average performance across 33 downstream benchmarks including morphological classification, molecular prediction, and survival analysis, outperforming other foundation model baselines overall.

Conclusion: CARE successfully addresses limitations of existing pathology foundation models by incorporating molecular guidance to identify biologically relevant tissue regions, demonstrating strong generalization with efficient data usage and supporting diverse pathology tasks through adaptive region features.

Abstract: Foundation models have recently achieved impressive success in computational pathology, demonstrating strong generalization across diverse histopathology tasks. However, existing models overlook the heterogeneous and non-uniform organization of pathological regions of interest (ROIs) because they rely on natural image backbones not tailored for tissue morphology. Consequently, they often fail to capture the coherent tissue architecture beyond isolated patches, limiting interpretability and clinical relevance. To address these challenges, we present Cross-modal Adaptive Region Encoder (CARE), a foundation model for pathology that automatically partitions WSIs into several morphologically relevant regions. Specifically, CARE employs a two-stage pretraining strategy: (1) a self-supervised unimodal pretraining stage that learns morphological representations from 34,277 whole-slide images (WSIs) without segmentation annotations, and (2) a cross-modal alignment stage that leverages RNA and protein profiles to refine the construction and representation of adaptive regions. This molecular guidance enables CARE to identify biologically relevant patterns and generate irregular yet coherent tissue regions, selecting the most representative area as ROI. CARE supports a broad range of pathology-related tasks, using either the ROI feature or the slide-level feature obtained by aggregating adaptive regions. Based on only one-tenth of the pretraining data typically used by mainstream foundation models, CARE achieves superior average performance across 33 downstream benchmarks, including morphological classification, molecular prediction, and survival analysis, and outperforms other foundation model baselines overall.

</details>


### [34] [Lie Flow: Video Dynamic Fields Modeling and Predicting with Lie Algebra as Geometric Physics Principle](https://arxiv.org/abs/2602.21645)
*Weidong Qiao,Wangmeng Zuo,Hui Li*

Main category: cs.CV

TL;DR: LieFlow introduces a dynamic radiance representation framework using SE(3) Lie group transformations to model both translation and rotation coherently for 4D scene reconstruction, improving physical realism over existing NeRF-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing 4D scene modeling approaches mainly rely on translational displacements, which struggle to represent rotations and articulated transformations, leading to spatial inconsistency and physically implausible motion. There's a need for physically consistent representations of complex rigid and non-rigid motions.

Method: LieFlow is a dynamic radiance representation framework that explicitly models motion within the SE(3) Lie group, enabling coherent learning of translation and rotation in a unified geometric space. The SE(3) transformation field enforces physically inspired constraints to maintain motion continuity and geometric consistency.

Result: Evaluation on synthetic datasets with rigid-body trajectories and two real-world datasets capturing complex motion under natural lighting and occlusions shows that LieFlow consistently improves view-synthesis fidelity, temporal coherence, and physical realism over NeRF-based baselines.

Conclusion: SE(3)-based motion modeling offers a robust and physically grounded framework for representing dynamic 4D scenes, confirming that explicit geometric modeling of both translation and rotation leads to better physical consistency and realism.

Abstract: Modeling 4D scenes requires capturing both spatial structure and temporal motion, which is challenging due to the need for physically consistent representations of complex rigid and non-rigid motions. Existing approaches mainly rely on translational displacements, which struggle to represent rotations, articulated transformations, often leading to spatial inconsistency and physically implausible motion. LieFlow, a dynamic radiance representation framework that explicitly models motion within the SE(3) Lie group, enabling coherent learning of translation and rotation in a unified geometric space. The SE(3) transformation field enforces physically inspired constraints to maintain motion continuity and geometric consistency. The evaluation includes a synthetic dataset with rigid-body trajectories and two real-world datasets capturing complex motion under natural lighting and occlusions. Across all datasets, LieFlow consistently improves view-synthesis fidelity, temporal coherence, and physical realism over NeRF-based baselines. These results confirm that SE(3)-based motion modeling offers a robust and physically grounded framework for representing dynamic 4D scenes.

</details>


### [35] [CCCaption: Dual-Reward Reinforcement Learning for Complete and Correct Image Captioning](https://arxiv.org/abs/2602.21655)
*Zhijiang Tang,Linhua Wang,Jiaxin Qi,Weihao Jiang,Peng Hou,Anxiang Zeng,Jianqiang Huang*

Main category: cs.CV

TL;DR: CCCaption introduces a dual-reward RL framework to generate Complete and Correct Captions by optimizing for completeness (covering all salient visual facts) and correctness (avoiding hallucinations), moving beyond subjective human annotations.


<details>
  <summary>Details</summary>
Motivation: Human-annotated captions are often incomplete or incorrect due to subjective preferences and expertise, which limits caption model performance. The paper argues caption quality should be assessed by objective criteria: completeness (covering all salient visual facts) and correctness (descriptions being true to the image).

Method: CCCaption uses a dual-reward reinforcement learning framework with dedicated fine-tuning corpus. For completeness: uses diverse LVLMs to disentangle images into visual queries, rewarding captions that answer more queries with dynamic query sampling. For correctness: penalizes hallucinations by validating authenticity of sub-caption queries derived from caption decomposition. Uses symmetric dual-reward optimization to jointly maximize both properties.

Result: Extensive experiments across standard captioning benchmarks show consistent improvements, demonstrating that optimizing for objective completeness and correctness criteria leads to better caption generation beyond simply imitating human annotations.

Conclusion: CCCaption offers a principled path to training caption models by explicitly optimizing for objective completeness and correctness criteria rather than just imitating subjective human annotations, leading to more reliable and comprehensive image descriptions.

Abstract: Image captioning remains a fundamental task for vision language understanding, yet ground-truth supervision still relies predominantly on human-annotated references. Because human annotations reflect subjective preferences and expertise, ground-truth captions are often incomplete or even incorrect, which in turn limits caption models. We argue that caption quality should be assessed by two objective aspects: completeness (does the caption cover all salient visual facts?) and correctness (are the descriptions true with respect to the image?). To this end, we introduce CCCaption: a dual-reward reinforcement learning framework with a dedicated fine-tuning corpus that explicitly optimizes these properties to generate \textbf{C}omplete and \textbf{C}orrect \textbf{Captions}. For completeness, we use diverse LVLMs to disentangle the image into a set of visual queries, and reward captions that answer more of these queries, with a dynamic query sampling strategy to improve training efficiency. For correctness, we penalize captions that contain hallucinations by validating the authenticity of sub-caption queries, which are derived from the caption decomposition. Our symmetric dual-reward optimization jointly maximizes completeness and correctness, guiding models toward captions that better satisfy these objective criteria. Extensive experiments across standard captioning benchmarks show consistent improvements, offering a principled path to training caption models beyond human-annotation imitation.

</details>


### [36] [Following the Diagnostic Trace: Visual Cognition-guided Cooperative Network for Chest X-Ray Diagnosis](https://arxiv.org/abs/2602.21657)
*Shaoxuan Wu,Jingkun Chen,Chong Ma,Cong Shen,Xiao Zhang,Jun Feng*

Main category: cs.CV

TL;DR: VCC-Net is a visual cognition-guided collaborative network that integrates radiologists' visual search patterns with AI models for more reliable and interpretable chest X-ray diagnosis.


<details>
  <summary>Details</summary>
Motivation: Current CAD systems are isolated from clinical workflows, lack reliable decision support and interpretability, and have a semantic gap between radiologists' decision-making patterns and model representations, limiting clinical adoption.

Method: Proposes VCC-Net that captures radiologists' visual search traces and attention patterns using eye-tracking or mouse interfaces, uses visual cognition as spatial guidance to learn hierarchical search strategies, and employs a cognition-graph co-editing module to integrate radiologist VC with model inference.

Result: Achieved classification accuracies of 88.40% on SIIM-ACR, 85.05% on EGD-CXR, and 92.41% on TB-Mouse dataset. Attention maps show strong concordance with radiologists' gaze distributions, demonstrating mutual reinforcement.

Conclusion: VCC-Net successfully bridges the gap between radiologists and AI models, enabling complementary and transparent decision-making through visual cognition-guided collaboration, with potential for improved clinical adoption.

Abstract: Computer-aided diagnosis (CAD) has significantly advanced automated chest X-ray diagnosis but remains isolated from clinical workflows and lacks reliable decision support and interpretability. Human-AI collaboration seeks to enhance the reliability of diagnostic models by integrating the behaviors of controllable radiologists. However, the absence of interactive tools seamlessly embedded within diagnostic routines impedes collaboration, while the semantic gap between radiologists' decision-making patterns and model representations further limits clinical adoption. To overcome these limitations, we propose a visual cognition-guided collaborative network (VCC-Net) to achieve the cooperative diagnostic paradigm. VCC-Net centers on visual cognition (VC) and employs clinically compatible interfaces, such as eye-tracking or the mouse, to capture radiologists' visual search traces and attention patterns during diagnosis. VCC-Net employs VC as a spatial cognition guide, learning hierarchical visual search strategies to localize diagnostically key regions. A cognition-graph co-editing module subsequently integrates radiologist VC with model inference to construct a disease-aware graph. The module captures dependencies among anatomical regions and aligns model representations with VC-driven features, mitigating radiologist bias and facilitating complementary, transparent decision-making. Experiments on the public datasets SIIM-ACR, EGD-CXR, and self-constructed TB-Mouse dataset achieved classification accuracies of 88.40%, 85.05%, and 92.41%, respectively. The attention maps produced by VCC-Net exhibit strong concordance with radiologists' gaze distributions, demonstrating a mutual reinforcement of radiologist and model inference. The code is available at https://github.com/IPMI-NWU/VCC-Net.

</details>


### [37] [HybridINR-PCGC: Hybrid Lossless Point Cloud Geometry Compression Bridging Pretrained Model and Implicit Neural Representation](https://arxiv.org/abs/2602.21662)
*Wenjie Huang,Qi Yang,Shuting Xia,He Huang,Zhu Li,Yiling Xu*

Main category: cs.CV

TL;DR: HybridINR-PCGC: A hybrid point cloud compression framework combining pretrained networks with implicit neural representations to achieve distribution-agnostic compression with faster convergence and reduced model overhead.


<details>
  <summary>Details</summary>
Motivation: Existing learning-based point cloud compression methods have limitations: pretrained methods suffer from training data dependency and poor generalization, while INR-based methods require time-consuming online training and have bitstream overhead from overfitted models.

Method: Proposes a hybrid framework with Pretrained Prior Network (PPN) for fast inference and stable performance, and Distribution Agnostic Refiner (DAR) decomposed into base and enhancement layers. Only enhancement layer parameters are packed into bitstream, with supervised model compression to minimize bitrate.

Result: Achieves 20.43% Bpp reduction vs G-PCC on 8iVFB, 57.85% Bpp reduction vs UniPCGC on out-of-distribution Cat1B, and 15.193% average Bpp reduction vs LINR-PCGC on 8iVFB with superior time-rate trade-off.

Conclusion: HybridINR-PCGC successfully bridges pretrained models and INR, achieving distribution-agnostic compression with improved efficiency, faster convergence, and reduced model overhead compared to existing methods.

Abstract: Learning-based point cloud compression presents superior performance to handcrafted codecs. However, pretrained-based methods, which are based on end-to-end training and expected to generalize to all the potential samples, suffer from training data dependency. Implicit neural representation (INR) based methods are distribution-agnostic and more robust, but they require time-consuming online training and suffer from the bitstream overhead from the overfitted model. To address these limitations, we propose HybridINR-PCGC, a novel hybrid framework that bridges the pretrained model and INR. Our framework retains distribution-agnostic properties while leveraging a pretrained network to accelerate convergence and reduce model overhead, which consists of two parts: the Pretrained Prior Network (PPN) and the Distribution Agnostic Refiner (DAR). We leverage the PPN, designed for fast inference and stable performance, to generate a robust prior for accelerating the DAR's convergence. The DAR is decomposed into a base layer and an enhancement layer, and only the enhancement layer needed to be packed into the bitstream. Finally, we propose a supervised model compression module to further supervise and minimize the bitrate of the enhancement layer parameters. Based on experiment results, HybridINR-PCGC achieves a significantly improved compression rate and encoding efficiency. Specifically, our method achieves a Bpp reduction of approximately 20.43% compared to G-PCC on 8iVFB. In the challenging out-of-distribution scenario Cat1B, our method achieves a Bpp reduction of approximately 57.85% compared to UniPCGC. And our method exhibits a superior time-rate trade-off, achieving an average Bpp reduction of 15.193% relative to the LINR-PCGC on 8iVFB.

</details>


### [38] [Send Less, Perceive More: Masked Quantized Point Cloud Communication for Loss-Tolerant Collaborative Perception](https://arxiv.org/abs/2602.21667)
*Sheng Xu,Enshu Wang,Hongfei Xue,Jian Teng,Bingyi Liu,Yi Zhu,Pu Wang,Libing Wu,Chunming Qiao*

Main category: cs.CV

TL;DR: QPoint2Comm is a quantized point-cloud communication framework for collaborative perception that reduces bandwidth while maintaining high accuracy and robustness to packet loss.


<details>
  <summary>Details</summary>
Motivation: Existing collaborative perception approaches struggle with high accuracy under strict bandwidth constraints and are vulnerable to random transmission packet loss, limiting practical deployment in connected vehicle systems.

Method: The framework uses quantized point-cloud indices with a shared codebook instead of transmitting intermediate features, employs masked training to simulate packet loss for robustness, and incorporates a cascade attention fusion module for multi-vehicle information integration.

Result: Extensive experiments on simulated and real-world datasets show QPoint2Comm achieves state-of-the-art performance in accuracy, communication efficiency, and resilience to packet loss.

Conclusion: QPoint2Comm provides an effective solution for bandwidth-efficient and robust collaborative perception in connected vehicles by combining quantization, robust training, and advanced fusion techniques.

Abstract: Collaborative perception allows connected vehicles to overcome occlusions and limited viewpoints by sharing sensory information. However, existing approaches struggle to achieve high accuracy under strict bandwidth constraints and remain highly vulnerable to random transmission packet loss. We introduce QPoint2Comm, a quantized point-cloud communication framework that dramatically reduces bandwidth while preserving high-fidelity 3D information. Instead of transmitting intermediate features, QPoint2Comm directly communicates quantized point-cloud indices using a shared codebook, enabling efficient reconstruction with lower bandwidth than feature-based methods. To ensure robustness to possible communication packet loss, we employ a masked training strategy that simulates random packet loss, allowing the model to maintain strong performance even under severe transmission failures. In addition, a cascade attention fusion module is proposed to enhance multi-vehicle information integration. Extensive experiments on both simulated and real-world datasets demonstrate that QPoint2Comm sets a new state of the art in accuracy, communication efficiency, and resilience to packet loss.

</details>


### [39] [Space-Time Forecasting of Dynamic Scenes with Motion-aware Gaussian Grouping](https://arxiv.org/abs/2602.21668)
*Junmyeong Lee,Hoseung Choi,Minsu Cho*

Main category: cs.CV

TL;DR: MoGaF is a framework for long-term scene extrapolation using 4D Gaussian Splatting with motion-aware grouping and forecasting to generate realistic dynamic scene evolution.


<details>
  <summary>Details</summary>
Motivation: Forecasting dynamic scenes is challenging due to limited observations making it difficult to capture coherent object-level motion and long-term temporal evolution.

Method: Uses 4D Gaussian Splatting representation with motion-aware Gaussian grouping and group-wise optimization for physically consistent motion. A lightweight forecasting module predicts future motion.

Result: Outperforms existing baselines on synthetic and real-world datasets in rendering quality, motion plausibility, and long-term forecasting stability.

Conclusion: MoGaF provides an effective framework for long-term scene extrapolation with coherent motion and stable temporal evolution.

Abstract: Forecasting dynamic scenes remains a fundamental challenge in computer vision, as limited observations make it difficult to capture coherent object-level motion and long-term temporal evolution. We present Motion Group-aware Gaussian Forecasting (MoGaF), a framework for long-term scene extrapolation built upon the 4D Gaussian Splatting representation. MoGaF introduces motion-aware Gaussian grouping and group-wise optimization to enforce physically consistent motion across both rigid and non-rigid regions, yielding spatially coherent dynamic representations. Leveraging this structured space-time representation, a lightweight forecasting module predicts future motion, enabling realistic and temporally stable scene evolution. Experiments on synthetic and real-world datasets demonstrate that MoGaF consistently outperforms existing baselines in rendering quality, motion plausibility, and long-term forecasting stability. Our project page is available at https://slime0519.github.io/mogaf

</details>


### [40] [E-comIQ-ZH: A Human-Aligned Dataset and Benchmark for Fine-Grained Evaluation of E-commerce Posters with Chain-of-Thought](https://arxiv.org/abs/2602.21698)
*Meiqi Sun,Mingyu Li,Junxiong Zhu*

Main category: cs.CV

TL;DR: E-comIQ-ZH is a framework for evaluating Chinese e-commerce posters, addressing the gap in automated quality assessment for generative AI commercial content, especially for complex Chinese characters.


<details>
  <summary>Details</summary>
Motivation: Generative AI is widely used for commercial posters but automated quality assessment lags behind. Existing models focus on generic aesthetics or low-level distortions, lacking functional criteria for e-commerce design. Chinese content presents additional challenges with complex characters producing subtle but critical textual artifacts that current methods overlook.

Method: 1. Built E-comIQ-18k dataset featuring multi-dimensional scores and expert-calibrated Chain of Thought rationales. 2. Trained E-comIQ-M, a specialized evaluation model that aligns with human expert judgment. 3. Created E-comIQ-Bench, the first automated and scalable benchmark for Chinese e-commerce poster generation.

Result: Extensive experiments show E-comIQ-M aligns more closely with expert standards than existing methods and enables scalable automated assessment of e-commerce posters. The framework successfully addresses the gap in Chinese e-commerce poster evaluation.

Conclusion: E-comIQ-ZH provides a comprehensive framework for evaluating Chinese e-commerce posters, filling a critical gap in automated quality assessment for generative AI commercial content. The release of datasets, models, and evaluation tools will support future research in this area.

Abstract: Generative AI is widely used to create commercial posters. However, rapid advances in generation have outpaced automated quality assessment. Existing models emphasize generic esthetics or low level distortions and lack the functional criteria required for e-commerce design. It is especially challenging for Chinese content, where complex characters often produce subtle but critical textual artifacts that are overlooked by existing methods. To address this, we introduce E-comIQ-ZH, a framework for evaluating Chinese e-commerce posters. We build the first dataset E-comIQ-18k to feature multi dimensional scores and expert calibrated Chain of Thought (CoT) rationales. Using this dataset, we train E-comIQ-M, a specialized evaluation model that aligns with human expert judgment. Our framework enables E-comIQ-Bench, the first automated and scalable benchmark for the generation of Chinese e-commerce posters. Extensive experiments show our E-comIQ-M aligns more closely with expert standards and enables scalable automated assessment of e-commerce posters. All datasets, models, and evaluation tools will be released to support future research in this area.Code will be available at https://github.com/4mm7/E-comIQ-ZH.

</details>


### [41] [SF3D-RGB: Scene Flow Estimation from Monocular Camera and Sparse LiDAR](https://arxiv.org/abs/2602.21699)
*Rajai Alhimdiat,Ramy Battrawy,René Schuster,Didier Stricker,Wesam Ashour*

Main category: cs.CV

TL;DR: SF3D-RGB: A deep learning architecture for sparse scene flow estimation that fuses 2D monocular images and 3D point clouds to achieve better accuracy and efficiency than single-modality methods.


<details>
  <summary>Details</summary>
Motivation: Existing scene flow estimation methods focus on single modalities (either image-based or LiDAR-based), which limits robustness. There's a need for multimodal approaches that can leverage complementary information from both 2D images and 3D point clouds for more accurate and robust scene flow estimation.

Method: An end-to-end deep learning architecture that: 1) Encodes information from both 2D monocular images and 3D point clouds into features, 2) Fuses these multimodal features, 3) Uses fused features to enhance graph matching for initial scene flow generation, and 4) Refines the initial flow with a residual scene flow module. The model is designed to balance accuracy and efficiency.

Result: Outperforms single-modality methods and achieves better scene flow accuracy on real-world datasets while using fewer parameters compared to other state-of-the-art fusion methods.

Conclusion: The proposed SF3D-RGB architecture successfully demonstrates that multimodal fusion of 2D images and 3D point clouds leads to more accurate and robust scene flow estimation with improved efficiency, addressing limitations of single-modality approaches.

Abstract: Scene flow estimation is an extremely important task in computer vision to support the perception of dynamic changes in the scene. For robust scene flow, learning-based approaches have recently achieved impressive results using either image-based or LiDAR-based modalities. However, these methods have tended to focus on the use of a single modality. To tackle these problems, we present a deep learning architecture, SF3D-RGB, that enables sparse scene flow estimation using 2D monocular images and 3D point clouds (e.g., acquired by LiDAR) as inputs. Our architecture is an end-to-end model that first encodes information from each modality into features and fuses them together. Then, the fused features enhance a graph matching module for better and more robust mapping matrix computation to generate an initial scene flow. Finally, a residual scene flow module further refines the initial scene flow. Our model is designed to strike a balance between accuracy and efficiency. Furthermore, experiments show that our proposed method outperforms single-modality methods and achieves better scene flow accuracy on real-world datasets while using fewer parameters compared to other state-of-the-art methods with fusion.

</details>


### [42] [Brain Tumor Segmentation with Special Emphasis on the Non-Enhancing Brain Tumor Compartment](https://arxiv.org/abs/2602.21703)
*T. Schaffer,A. Brawanski,S. Wein,A. M. Tomé,E. W. Lang*

Main category: cs.CV

TL;DR: U-Net architecture for brain tumor segmentation in MRI, with special focus on non-enhancing tumor compartment which is clinically important but often overlooked in recent challenges.


<details>
  <summary>Details</summary>
Motivation: The non-enhancing tumor compartment is clinically significant for predicting patient survival and identifying areas of potential tumor growth, yet it has been neglected in recent brain tumor segmentation challenges like MICCAI. There's a need for automated segmentation tools to delineate this important region.

Method: A U-Net based deep learning architecture designed specifically for segmenting brain tumors across various MRI modalities, with special emphasis on accurately identifying and delineating the non-enhancing tumor compartment.

Result: The paper presents a specialized segmentation method that addresses the gap in current challenge evaluations by focusing on the clinically important non-enhancing tumor region, though specific quantitative results are not provided in the abstract.

Conclusion: The proposed U-Net architecture provides essential automated means to segment the non-enhancing tumor compartment, which is crucial for clinical assessment of patient survival and tumor progression, filling an important gap in current brain tumor segmentation research.

Abstract: A U-Net based deep learning architecture is designed to segment brain tumors as they appear on various MRI modalities. Special emphasis is lent to the non-enhancing tumor compartment. The latter has not been considered anymore in recent brain tumor segmentation challenges like the MICCAI challenges. However, it is considered to be indicative of the survival time of the patient as well as of areas of further tumor growth. Hence it deems essential to have means to automatically delineate its extension within the tumor.

</details>


### [43] [UNet-Based Keypoint Regression for 3D Cone Localization in Autonomous Racing](https://arxiv.org/abs/2602.21904)
*Mariia Baidachna,James Carty,Aidan Ferguson,Joseph Agrane,Varad Kulkarni,Aubrey Agub,Michael Baxendale,Aaron David,Rachel Horton,Elliott Atkinson*

Main category: cs.CV

TL;DR: UNet-based neural network for cone keypoint detection in autonomous racing, trained on largest custom-labeled dataset, achieving improved accuracy over traditional methods and enabling real-time performance.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D cone localization is essential for autonomous racing navigation. Traditional computer vision methods are sensitive to environmental variations, while existing neural networks are often trained on limited data and cannot run in real-time.

Method: UNet-based neural network for keypoint detection on cones, leveraging the largest custom-labeled dataset assembled. The approach enables accurate cone position estimation and potential color prediction.

Result: Substantial improvements in keypoint accuracy over conventional methods. High-quality performance across all metrics when integrated into the perception pipeline and evaluated in end-to-end autonomous system.

Conclusion: The approach is effective and has potential for adoption in competitive autonomous racing systems, demonstrating accurate cone localization and real-time feasibility.

Abstract: Accurate cone localization in 3D space is essential in autonomous racing for precise navigation around the track. Approaches that rely on traditional computer vision algorithms are sensitive to environmental variations, and neural networks are often trained on limited data and are infeasible to run in real time. We present a UNet-based neural network for keypoint detection on cones, leveraging the largest custom-labeled dataset we have assembled. Our approach enables accurate cone position estimation and the potential for color prediction. Our model achieves substantial improvements in keypoint accuracy over conventional methods. Furthermore, we leverage our predicted keypoints in the perception pipeline and evaluate the end-to-end autonomous system. Our results show high-quality performance across all metrics, highlighting the effectiveness of this approach and its potential for adoption in competitive autonomous racing systems.

</details>


### [44] [Dynamic Multimodal Activation Steering for Hallucination Mitigation in Large Vision-Language Models](https://arxiv.org/abs/2602.21704)
*Jianghao Yin,Qin Chen,Kedi Chen,Jie Zhou,Xingjiao Wu,Liang He*

Main category: cs.CV

TL;DR: Dynamic Multimodal Activation Steering (DMAS) - a training-free method that mitigates hallucinations in Large Vision-Language Models by dynamically applying context-aware steering vectors to specific attention heads based on semantic similarity.


<details>
  <summary>Details</summary>
Motivation: Large Vision-Language Models (LVLMs) suffer from hallucination problems despite strong performance on vision-language tasks. The authors discovered that truthfulness and visual perception capabilities engage different subsets of attention heads, and truthfulness steering vectors vary across semantic contexts.

Method: Proposes Dynamic Multimodal Activation Steering (DMAS): 1) Constructs a semantic-based truthfulness steering vector database, 2) Computes visual perception steering vectors, 3) During inference, dynamically selects relevant steering vectors based on input semantic similarity, 4) Applies them to the most influential attention heads for context-aware interventions.

Result: Comprehensive experiments across multiple models and datasets show that DMAS significantly enhances model performance and outperforms existing state-of-the-art methods for hallucination mitigation.

Conclusion: The training-free DMAS approach effectively reduces hallucinations in LVLMs by leveraging insights about attention head specialization and semantic context variability, offering a practical solution for improving model truthfulness without retraining.

Abstract: Large Vision-Language Models (LVLMs) exhibit outstanding performance on vision-language tasks but struggle with hallucination problems. Through in-depth analysis of LVLM activation patterns, we reveal two key findings: 1) truthfulness and visual perception capabilities predominantly engage different subsets of attention heads within the model architecture; and 2) truthfulness steering vectors vary significantly across different semantic contexts. Based on these observations, we propose Dynamic Multimodal Activation Steering, a training-free approach for hallucination mitigation. Our method constructs a semantic-based truthfulness steering vector database and computes visual perception steering vectors, enabling context-aware interventions during inference by dynamically selecting the most relevant steering vectors based on input semantic similarity and applying them to the most influential attention heads. We conduct comprehensive experiments across multiple models and datasets, demonstrating that our approach significantly enhances model performance, outperforming existing state-of-the-art methods.

</details>


### [45] [SurGo-R1: Benchmarking and Modeling Contextual Reasoning for Operative Zone in Surgical Video](https://arxiv.org/abs/2602.21706)
*Guanyi Qin,Xiaozhen Wang,Zhu Zhuo,Chang Han Low,Yuancan Xiao,Yibing Fu,Haofeng Liu,Kai Wang,Chunjiang Li,Yueming Jin*

Main category: cs.CV

TL;DR: ResGo benchmark for laparoscopic surgery with Go Zone annotations and clinician rationales, plus SurGo-R1 model using RLHF with phase-then-go architecture for safe operative zone identification.


<details>
  <summary>Details</summary>
Motivation: Current AI systems for minimally invasive surgery offer binary safety verification or static detection, ignoring the phase-dependent nature of intraoperative reasoning that surgeons must perform under high cognitive load.

Method: 1) Created ResGo benchmark with laparoscopic frames annotated with Go Zone bounding boxes and clinician-authored rationales. 2) Developed SurGo-R1 model using RLHF with multi-turn phase-then-go architecture where the model first identifies surgical phase, then generates reasoning and Go Zone coordinates conditioned on that context.

Result: On unseen procedures, SurGo-R1 achieves 76.6% phase accuracy, 32.7 mIoU, and 54.8% hardcore accuracy, representing a 6.6× improvement over mainstream generalist vision-language models.

Conclusion: The proposed benchmark and model demonstrate that phase-aware reasoning is crucial for safe operative zone identification in surgery, and that specialized surgical AI systems significantly outperform generalist VLMs on this complex task.

Abstract: Minimally invasive surgery has dramatically improved patient operative outcomes, yet identifying safe operative zones remains challenging in critical phases, requiring surgeons to integrate visual cues, procedural phase, and anatomical context under high cognitive load. Existing AI systems offer binary safety verification or static detection, ignoring the phase-dependent nature of intraoperative reasoning. We introduce ResGo, a benchmark of laparoscopic frames annotated with Go Zone bounding boxes and clinician-authored rationales covering phase, exposure quality reasoning, next action and risk reminder. We introduce evaluation metrics that treat correct grounding under incorrect phase as failures, revealing that most vision-language models cannot handle such tasks and perform poorly. We then present SurGo-R1, a model optimized via RLHF with a multi-turn phase-then-go architecture where the model first identifies the surgical phase, then generates reasoning and Go Zone coordinates conditioned on that context. On unseen procedures, SurGo-R1 achieves 76.6% phase accuracy, 32.7 mIoU, and 54.8% hardcore accuracy, a 6.6$\times$ improvement over the mainstream generalist VLMs. Code, model and benchmark will be available at https://github.com/jinlab-imvr/SurGo-R1

</details>


### [46] [Assessing airborne laser scanning and aerial photogrammetry for deep learning-based stand delineation](https://arxiv.org/abs/2602.21709)
*Håkon Næss Sandum,Hans Ole Ørka,Oliver Tomic,Terje Gobakken*

Main category: cs.CV

TL;DR: Deep learning for forest stand delineation works equally well with ALS or DAP-derived canopy height models, showing resilience to data variations and enabling use of temporally aligned datasets.


<details>
  <summary>Details</summary>
Motivation: Current forest stand delineation is manual and subjective. While deep learning with ALS data shows promise, temporal misalignment limits scalability. Need to test if DAP-derived CHMs (better temporal alignment) and DTMs can provide comparable performance.

Method: Used U-Net semantic segmentation with municipality-level cross-validation across six municipalities in Norway. Compared three data combinations: (1) aerial imagery + ALS-CHM, (2) aerial imagery + DAP-CHM, (3) aerial imagery + DAP-CHM + DTM. Expert-delineated stands as reference.

Result: All three data combinations achieved similar overall accuracy (0.90-0.91). Model predictions showed high consistency with each other, more than with reference data. DAP-CHMs performed similarly despite reduced structural detail. DTM inclusion provided no improvement.

Conclusion: Deep learning framework is resilient to input data variations. DAP-derived CHMs can reliably replace ALS-CHMs, enabling use of temporally aligned datasets. Subjectivity in reference data is a key factor in performance assessment.

Abstract: Accurate forest stand delineation is essential for forest inventory and management but remains a largely manual and subjective process. A recent study has shown that deep learning can produce stand delineations comparable to expert interpreters when combining aerial imagery and airborne laser scanning (ALS) data. However, temporal misalignment between data sources limits operational scalability. Canopy height models (CHMs) derived from digital photogrammetry (DAP) offer better temporal alignment but may smoothen canopy surface and canopy gaps, raising the question of whether they can reliably replace ALS-derived CHMs. Similarly, the inclusion of a digital terrain model (DTM) has been suggested to improve delineation performance, but has remained untested in published literature. Using expert-delineated forest stands as reference data, we assessed a U-Net-based semantic segmentation framework with municipality-level cross-validation across six municipalities in southeastern Norway. We compared multispectral aerial imagery combined with (i) an ALS-derived CHM, (ii) a DAP-derived CHM, and (iii) a DAP-derived CHM in combination with a DTM. Results showed comparable performance across all data combinations, reaching overall accuracy values between 0.90-0.91. Agreement between model predictions was substantially larger than agreement with the reference data, highlighting both model consistency and the inherent subjectivity of stand delineation. The similar performance of DAP-CHMs, despite the reduced structural detail, and the lack of improvements of the DTM indicate that the framework is resilient to variations in input data. These findings indicate that large datasets for deep learning-based stand delineations can be assembled using projects including temporally aligned ALS data and DAP point clouds.

</details>


### [47] [Innovative Tooth Segmentation Using Hierarchical Features and Bidirectional Sequence Modeling](https://arxiv.org/abs/2602.21712)
*Xinxin Zhao,Jian Jiang,Yan Tian,Liqin Wu,Zhaocheng Xu,Teddy Yang,Yunuo Zou,Xun Wang*

Main category: cs.CV

TL;DR: Proposes a three-stage encoder with hierarchical feature representation for tooth image segmentation, addressing fixed-resolution limitations and computational inefficiency of transformers.


<details>
  <summary>Details</summary>
Motivation: Traditional image encoders with fixed-resolution feature maps cause discontinuous segmentation and poor discrimination between teeth and background due to insufficient environmental and global context modeling. Transformer-based approaches have quadratic complexity O(n²), making them inefficient for high-resolution dental images.

Method: Three-stage encoder with hierarchical feature representation to capture scale-adaptive information. Uses cross-scale feature fusion to combine low-level details and high-level semantics. Incorporates bidirectional sequence modeling strategy for enhanced global spatial context without high computational cost.

Result: Validated on two dental datasets, achieving superiority over existing approaches. On OralVision dataset, achieves 1.1% improvement in mean intersection over union (mIoU).

Conclusion: The proposed method effectively addresses limitations of traditional encoders and transformer-based approaches by providing hierarchical feature representation with efficient global context modeling, leading to improved tooth segmentation performance.

Abstract: Tooth image segmentation is a cornerstone of dental digitization. However, traditional image encoders relying on fixed-resolution feature maps often lead to discontinuous segmentation and poor discrimination between target regions and background, due to insufficient modeling of environmental and global context. Moreover, transformer-based self-attention introduces substantial computational overhead because of its quadratic complexity (O(n^2)), making it inefficient for high-resolution dental images. To address these challenges, we introduce a three-stage encoder with hierarchical feature representation to capture scale-adaptive information in dental images. By jointly leveraging low-level details and high-level semantics through cross-scale feature fusion, the model effectively preserves fine structural information while maintaining strong contextual awareness. Furthermore, a bidirectional sequence modeling strategy is incorporated to enhance global spatial context understanding without incurring high computational cost.
  We validate our method on two dental datasets, with experimental results demonstrating its superiority over existing approaches. On the OralVision dataset, our model achieves a 1.1% improvement in mean intersection over union (mIoU).

</details>


### [48] [Beyond Static Artifacts: A Forensic Benchmark for Video Deepfake Reasoning in Vision Language Models](https://arxiv.org/abs/2602.21779)
*Zheyuan Gu,Qingsong Zhao,Yusong Wang,Zhaohong Huang,Xinqi Li,Cheng Yuan,Jiaowei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: FAQ benchmark transforms temporal deepfake detection into multiple-choice QA, enabling VLMs to learn forensic reasoning across three hierarchical levels.


<details>
  <summary>Details</summary>
Motivation: Current VLMs focus on spatial artifacts but ignore temporal inconsistencies in video forgeries, creating a gap in dynamic cue reasoning for deepfake detection.

Method: Proposes Forensic Answer-Questioning (FAQ) benchmark with three-level hierarchy: Facial Perception, Temporal Deepfake Grounding, and Forensic Reasoning. Also creates FAQ-IT instruction-tuning dataset for fine-tuning VLMs.

Result: Models fine-tuned on FAQ-IT achieve advanced performance on both in-domain and cross-dataset detection benchmarks, with ablation studies confirming FAQ drives temporal reasoning capabilities.

Conclusion: FAQ effectively bridges the temporal reasoning gap in VLMs for deepfake detection, enabling them to analyze dynamic forgery artifacts through structured forensic questioning.

Abstract: Current Vision-Language Models (VLMs) for deepfake detection excel at identifying spatial artifacts but overlook a critical dimension: temporal inconsistencies in video forgeries. Adapting VLMs to reason about these dynamic cues remains a distinct challenge. To bridge this gap, we propose Forensic Answer-Questioning (FAQ), a large-scale benchmark that formulates temporal deepfake analysis as a multiple-choice task. FAQ introduces a three-level hierarchy to progressively evaluate and equip VLMs with forensic capabilities: (1) Facial Perception, testing the ability to identify static visual artifacts; (2) Temporal Deepfake Grounding, requiring the localization of dynamic forgery artifacts across frames; and (3) Forensic Reasoning, challenging models to synthesize evidence for final authenticity verdicts. We evaluate a range of VLMs on FAQ and generate a corresponding instruction-tuning set, FAQ-IT. Extensive experiments show that models fine-tuned on FAQ-IT achieve advanced performance on both in-domain and cross-dataset detection benchmarks. Ablation studies further validate the impact of our key design choices, confirming that FAQ is the driving force behind the temporal reasoning capabilities of these VLMs.

</details>


### [49] [TranX-Adapter: Bridging Artifacts and Semantics within MLLMs for Robust AI-generated Image Detection](https://arxiv.org/abs/2602.21716)
*Wenbin Wang,Yuge Huang,Jianqing Xu,Yue Yu,Jiangtao Yan,Shouhong Ding,Pan Zhou,Yong Luo*

Main category: cs.CV

TL;DR: Proposes TranX-Adapter, a lightweight fusion adapter that improves AI-generated image detection in multimodal LLMs by addressing attention dilution between semantic and artifact features through optimal transport and cross-attention fusion.


<details>
  <summary>Details</summary>
Motivation: AI-generated images threaten information integrity. While multimodal LLMs can use both semantic and texture-level artifact features for detection, artifact features have high intra-feature similarity causing attention dilution, hindering effective feature fusion.

Method: TranX-Adapter with two components: 1) Task-aware Optimal-Transport Fusion using Jensen-Shannon divergence between artifact and semantic prediction probabilities as cost matrix to transfer artifact info into semantic features, 2) X-Fusion using cross-attention to transfer semantic info into artifact features.

Result: Experiments on standard AIGI detection benchmarks show consistent and significant improvements (up to +6% accuracy) across several advanced multimodal LLMs.

Conclusion: The proposed lightweight fusion adapter effectively addresses attention dilution in multimodal LLMs for AIGI detection, achieving substantial performance gains through improved feature fusion between semantic and artifact information.

Abstract: Rapid advances in AI-generated image (AIGI) technology enable highly realistic synthesis, threatening public information integrity and security. Recent studies have demonstrated that incorporating texture-level artifact features alongside semantic features into multimodal large language models (MLLMs) can enhance their AIGI detection capability. However, our preliminary analyses reveal that artifact features exhibit high intra-feature similarity, leading to an almost uniform attention map after the softmax operation. This phenomenon causes attention dilution, thereby hindering effective fusion between semantic and artifact features. To overcome this limitation, we propose a lightweight fusion adapter, TranX-Adapter, which integrates a Task-aware Optimal-Transport Fusion that leverages the Jensen-Shannon divergence between artifact and semantic prediction probabilities as a cost matrix to transfer artifact information into semantic features, and an X-Fusion that employs cross-attention to transfer semantic information into artifact features. Experiments on standard AIGI detection benchmarks upon several advanced MLLMs, show that our TranX-Adapter brings consistent and significant improvements (up to +6% accuracy).

</details>


### [50] [SigVLP: Sigmoid Volume-Language Pre-Training for Self-Supervised CT-Volume Adaptive Representation Learning](https://arxiv.org/abs/2602.21735)
*Jiayi Wang,Hadrien Reynaud,Ibrahim Ethem Hamamci,Sezgin Er,Suprosanna Shit,Bjoern Menze,Bernhard Kainz*

Main category: cs.CV

TL;DR: SigVLP is a vision-language model for medical imaging that uses Rotary Position Embeddings to handle variable-resolution CT volumes as sequences of 3D chunks, enabling chunkwise alignment with localized textual observations for improved text-to-volume correlation.


<details>
  <summary>Details</summary>
Motivation: Medical imaging datasets have highly variable resolution, slice thicknesses, and slice counts across different vendors/devices. Traditional approaches require cropping or interpolating to fixed sizes, causing information loss. There's a need for models that can handle variable input sizes while maintaining fine-grained text-image alignment.

Method: Proposes SigVLP with Rotary Position Embeddings applied within attention operations, treating CT volumes as sequences of 3D chunks along the z-axis. Uses chunkwise sampling during training with localized organ-wise textual observations instead of entire reports. Trained with Muon optimizer.

Result: The model enables handling of variable input sizes without information loss from cropping/interpolation. Chunkwise alignment provides finer-grained supervision, establishing stronger correlations between text and volume representations for improved text-to-volume alignment precision.

Conclusion: SigVLP's approach of treating volumes as chunk sequences with Rotary Position Embeddings and localized text alignment effectively addresses variable-resolution medical imaging challenges, enabling better representation learning for downstream tasks like classification, segmentation, and retrieval.

Abstract: Large-scale, volumetric medical imaging datasets typically aggregate scans from different vendors and devices, resulting in highly variable resolution, slice thicknesses, and numbers of slices per study. Consequently, training representation models usually requires cropping or interpolating along the z-axis to obtain fixed-size blocks, which inevitably causes information loss. We propose a new training approach to overcome this limitation. Instead of absolute position embeddings, we interpret volumes as sequences of 3D chunks and adopt Rotary Position Embeddings, allowing us to treat the z-axis as an unconstrained temporal dimensions. Building on this idea, we introduce a new vision-language model: SigVLP. In SigVLP, we implement Rotary Position Embedding as the positional encoding method, which is applied directly within the attention operation, generating input-conditioned sine and cosine weights on the fly. This design ensures consistent alignment between query and key projections and adapts to any input sizes. To allow for variable input size during training, we sample Computed Tomography volumes in chunks and pair them with localized organ-wise textual observations. Compared to using entire reports for conditioning, chunkwise alignment provides finer-grained supervision, enabling the model to establish stronger correlations between the text and volume representations, thereby improving the precision of text-to-volume alignment. Our models are trained with the Muon optimizer and evaluated on a diverse set of downstream tasks, including zero-shot abnormality and organ classification, segmentation, and retrieval tasks.

</details>


### [51] [SemVideo: Reconstructs What You Watch from Brain Activity via Hierarchical Semantic Guidance](https://arxiv.org/abs/2602.21819)
*Minghan Yang,Lan Yang,Ke Li,Honggang Zhang,Kaiyue Pang,Yizhe Song*

Main category: cs.CV

TL;DR: SemVideo: A novel fMRI-to-video reconstruction framework using hierarchical semantic guidance to address appearance mismatches and poor temporal coherence in brain activity-based video reconstruction.


<details>
  <summary>Details</summary>
Motivation: Current fMRI-to-video reconstruction approaches suffer from two major shortcomings: (1) inconsistent visual representations of salient objects across frames leading to appearance mismatches, and (2) poor temporal coherence resulting in motion misalignment or abrupt frame transitions. These limitations hinder accurate reconstruction of dynamic visual experiences from brain activity.

Method: SemVideo introduces hierarchical semantic guidance through SemMiner module that extracts three levels of semantic cues: static anchor descriptions, motion-oriented narratives, and holistic summaries. The framework has three key components: Semantic Alignment Decoder (aligns fMRI signals with CLIP-style embeddings), Motion Adaptation Decoder (reconstructs dynamic motion patterns using tripartite attention fusion), and Conditional Video Render (uses hierarchical semantic guidance for video reconstruction).

Result: Experiments on CC2017 and HCP datasets demonstrate that SemVideo achieves superior performance in both semantic alignment and temporal consistency, setting a new state-of-the-art in fMRI-to-video reconstruction.

Conclusion: SemVideo successfully addresses the key challenges in fMRI-to-video reconstruction by leveraging hierarchical semantic guidance, resulting in improved visual consistency and temporal coherence for reconstructing dynamic visual experiences from brain activity.

Abstract: Reconstructing dynamic visual experiences from brain activity provides a compelling avenue for exploring the neural mechanisms of human visual perception. While recent progress in fMRI-based image reconstruction has been notable, extending this success to video reconstruction remains a significant challenge. Current fMRI-to-video reconstruction approaches consistently encounter two major shortcomings: (i) inconsistent visual representations of salient objects across frames, leading to appearance mismatches; (ii) poor temporal coherence, resulting in motion misalignment or abrupt frame transitions. To address these limitations, we introduce SemVideo, a novel fMRI-to-video reconstruction framework guided by hierarchical semantic information. At the core of SemVideo is SemMiner, a hierarchical guidance module that constructs three levels of semantic cues from the original video stimulus: static anchor descriptions, motion-oriented narratives, and holistic summaries. Leveraging this semantic guidance, SemVideo comprises three key components: a Semantic Alignment Decoder that aligns fMRI signals with CLIP-style embeddings derived from SemMiner, a Motion Adaptation Decoder that reconstructs dynamic motion patterns using a novel tripartite attention fusion architecture, and a Conditional Video Render that leverages hierarchical semantic guidance for video reconstruction. Experiments conducted on the CC2017 and HCP datasets demonstrate that SemVideo achieves superior performance in both semantic alignment and temporal consistency, setting a new state-of-the-art in fMRI-to-video reconstruction.

</details>


### [52] [Structure-to-Image: Zero-Shot Depth Estimation in Colonoscopy via High-Fidelity Sim-to-Real Adaptation](https://arxiv.org/abs/2602.21740)
*Juan Yang,Yuyan Zhang,Han Jia,Bing Hu,Wanzhong Song*

Main category: cs.CV

TL;DR: Proposes Structure-to-Image paradigm for colonoscopy depth estimation, using depth as active generative foundation instead of passive constraint, achieving 44.18% RMSE reduction.


<details>
  <summary>Details</summary>
Motivation: Monocular depth estimation for colonoscopy suffers from domain gap between simulated and real images. Existing image-to-image translation methods cause structural distortions and specular highlights by failing to balance realism with structure consistency.

Method: Structure-to-Image paradigm transforms depth map from passive constraint to active generative foundation. Introduces phase congruency to colonoscopic domain adaptation and designs cross-level structure constraint to co-optimize geometric structures and fine-grained details like vascular textures.

Result: In zero-shot evaluations on publicly available phantom dataset, MDE model fine-tuned on generated data achieved maximum reduction of 44.18% in RMSE compared to competing methods.

Conclusion: The proposed Structure-to-Image approach effectively addresses domain adaptation challenges in colonoscopic depth estimation by prioritizing structural consistency while maintaining realism, significantly outperforming existing methods.

Abstract: Monocular depth estimation (MDE) for colonoscopy is hampered by the domain gap between simulated and real-world images. Existing image-to-image translation methods, which use depth as a posterior constraint, often produce structural distortions and specular highlights by failing to balance realism with structure consistency. To address this, we propose a Structure-to-Image paradigm that transforms the depth map from a passive constraint into an active generative foundation. We are the first to introduce phase congruency to colonoscopic domain adaptation and design a cross-level structure constraint to co-optimize geometric structures and fine-grained details like vascular textures. In zero-shot evaluations conducted on a publicly available phantom dataset, the MDE model that was fine-tuned on our generated data achieved a maximum reduction of 44.18% in RMSE compared to competing methods. Our code is available at https://github.com/YyangJJuan/PC-S2I.git.

</details>


### [53] [StoryMovie: A Dataset for Semantic Alignment of Visual Stories with Movie Scripts and Subtitles](https://arxiv.org/abs/2602.21829)
*Daniel Oliveira,David Martins de Matos*

Main category: cs.CV

TL;DR: StoryMovie dataset aligns movie scripts with subtitles to improve visual storytelling by enabling accurate dialogue attribution and character relationships, with fine-tuned model showing significant improvements.


<details>
  <summary>Details</summary>
Motivation: Existing visual storytelling models often hallucinate semantic relationships even when correctly grounding entities in images, leading to incorrect dialogue attribution, character interactions, or emotional states.

Method: Created StoryMovie dataset (1,757 stories) by aligning movie scripts with subtitles using LCS matching. Pipeline synchronizes screenplay dialogue with subtitle timestamps to link character names to temporal positions. Fine-tuned Qwen Storyteller3 on this dataset, building on visual grounding and entity re-identification.

Result: Storyteller3 achieves 89.9% win rate against base Qwen2.5-VL 7B on subtitle alignment. Compared to Storyteller (without script grounding), Storyteller3 achieves 48.5% vs 38.0%, showing semantic alignment progressively improves dialogue attribution beyond visual grounding alone.

Conclusion: Aligning movie scripts with subtitles enables more accurate dialogue attribution and character relationship modeling in visual storytelling, with semantic alignment providing significant improvements over visual grounding alone.

Abstract: Visual storytelling models that correctly ground entities in images may still hallucinate semantic relationships, generating incorrect dialogue attribution, character interactions, or emotional states. We introduce StoryMovie, a dataset of 1,757 stories aligned with movie scripts and subtitles through LCS matching. Our alignment pipeline synchronizes screenplay dialogue with subtitle timestamps, enabling dialogue attribution by linking character names from scripts to temporal positions from subtitles. Using this aligned content, we generate stories that maintain visual grounding tags while incorporating authentic character names, dialogue, and relationship dynamics. We fine-tune Qwen Storyteller3 on this dataset, building on prior work in visual grounding and entity re-identification. Evaluation using DeepSeek V3 as judge shows that Storyteller3 achieves an 89.9% win rate against base Qwen2.5-VL 7B on subtitle alignment. Compared to Storyteller, trained without script grounding,
  Storyteller3 achieves 48.5% versus 38.0%, confirming that semantic alignment progressively improves dialogue attribution beyond visual grounding alone.

</details>


### [54] [Enhancing Multi-Modal LLMs Reasoning via Difficulty-Aware Group Normalization](https://arxiv.org/abs/2602.21743)
*Jinghan Li,Junfeng Fang,Jinda Lu,Yuan Wang,Xiaoyan Guo,Tianyu Zhang,Xiang Wang,Xiangnan He*

Main category: cs.CV

TL;DR: Proposes difficulty-aware group normalization (Durian) for multimodal reinforcement learning, addressing instability of std-based normalization by grouping samples by difficulty levels based on perceptual complexity and reasoning uncertainty.


<details>
  <summary>Details</summary>
Motivation: RLVR and GRPO have advanced LLM reasoning, but extending them to multimodal settings faces instability from std-based normalization that's easily distorted by extreme reward samples. Multimodal models are particularly sensitive to such distortions due to both perceptual and reasoning errors influencing responses.

Method: Characterizes samples by difficulty using perceptual complexity (measured via visual entropy) and reasoning uncertainty (captured by model confidence). Proposes difficulty-aware group normalization (Durian) that re-groups samples by difficulty levels and shares standard deviation within each group.

Result: Significant performance gains across multiple multimodal reasoning benchmarks, preserving GRPO's intra-group distinctions while eliminating sensitivity to extreme cases.

Conclusion: Durian effectively addresses the normalization instability in multimodal reinforcement learning by grouping samples based on difficulty, enabling more stable and effective application of RLVR and GRPO methods to multimodal reasoning tasks.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) and Group Relative Policy Optimization (GRPO) have significantly advanced the reasoning capabilities of large language models. Extending these methods to multimodal settings, however, faces a critical challenge: the instability of std-based normalization, which is easily distorted by extreme samples with nearly positive or negative rewards. Unlike pure-text LLMs, multimodal models are particularly sensitive to such distortions, as both perceptual and reasoning errors influence their responses. To address this, we characterize each sample by its difficulty, defined through perceptual complexity (measured via visual entropy) and reasoning uncertainty (captured by model confidence). Building on this characterization, we propose difficulty-aware group normalization (Durian), which re-groups samples by difficulty levels and shares the std within each group. Our approach preserves GRPO's intra-group distinctions while eliminating sensitivity to extreme cases, yielding significant performance gains across multiple multimodal reasoning benchmarks.

</details>


### [55] [LiREC-Net: A Target-Free and Learning-Based Network for LiDAR, RGB, and Event Calibration](https://arxiv.org/abs/2602.21754)
*Aditya Ranjan Dash,Ramy Battrawy,René Schuster,Didier Stricker*

Main category: cs.CV

TL;DR: LiREC-Net: A unified learning-based framework for target-free calibration of multiple sensor modalities (LiDAR, RGB, event data) simultaneously, outperforming single-pair methods and establishing new baseline for tri-modal calibration.


<details>
  <summary>Details</summary>
Motivation: Advanced autonomous systems need accurate multi-sensor fusion for safe perception. Existing learning-based calibration methods only handle single sensor pairs, creating inefficiency and suboptimal performance for systems with multiple sensor types.

Method: Proposes LiREC-Net, a target-free learning-based calibration network that jointly calibrates multiple sensor pairs (LiDAR-RGB, LiDAR-event, etc.) in unified framework. Introduces shared LiDAR representation using both 3D features and projected depth maps to reduce redundant computation and improve cross-modal consistency.

Result: Achieves competitive performance compared to bi-modal models on established datasets (KITTI, DSEC). Sets new strong baseline for tri-modal calibration use case, demonstrating effectiveness of unified multi-modal approach.

Conclusion: LiREC-Net provides an efficient, unified solution for multi-sensor calibration that outperforms single-pair methods and establishes foundation for more robust multi-modal perception in autonomous systems.

Abstract: Advanced autonomous systems rely on multi-sensor fusion for safer and more robust perception. To enable effective fusion, calibrating directly from natural driving scenes (i.e., target-free) with high accuracy is crucial for precise multi-sensor alignment. Existing learning-based calibration methods are typically designed for only a single pair of sensor modalities (i.e., a bi-modal setup). Unlike these methods, we propose LiREC-Net, a target-free, learning-based calibration network that jointly calibrates multiple sensor modality pairs, including LiDAR, RGB, and event data, within a unified framework. To reduce redundant computation and improve efficiency, we introduce a shared LiDAR representation that leverages features from both its 3D nature and projected depth map, ensuring better consistency across modalities. Trained and evaluated on established datasets, such as KITTI and DSEC, our LiREC-Net achieves competitive performance to bi-modal models and sets a new strong baseline for the tri-modal use case.

</details>


### [56] [Understanding Annotation Error Propagation and Learning an Adaptive Policy for Expert Intervention in Barrett's Video Segmentation](https://arxiv.org/abs/2602.21855)
*Lokesha Rasanjalee,Jin Lin Tan,Dileepa Pitawela,Rajvinder Singh,Hsiang-Ting Chen*

Main category: cs.CV

TL;DR: L2RP framework learns when to seek expert input to balance annotation effort and segmentation accuracy in endoscopic video annotation, improving over SAM2 propagation methods.


<details>
  <summary>Details</summary>
Motivation: Endoscopic video annotation is time-consuming, especially for challenging datasets like Barrett's esophagus dysplasia. Semi-automatic tools like SAM2 help but suffer from error accumulation requiring expert correction.

Method: Systematically study annotation error propagation across different prompt types (masks, boxes, points) and propose Learning-to-Re-Prompt (L2RP), a cost-aware framework that learns when and where to seek expert input.

Result: Experiments on private Barrett's dysplasia dataset and public SUN-SEG benchmark show improved temporal consistency and superior performance over baseline strategies.

Conclusion: L2RP effectively balances annotation effort and segmentation accuracy by intelligently determining when to request expert input, addressing error accumulation in semi-automatic video annotation.

Abstract: Accurate annotation of endoscopic videos is essential yet time-consuming, particularly for challenging datasets such as dysplasia in Barrett's esophagus, where the affected regions are irregular and lack clear boundaries. Semi-automatic tools like Segment Anything Model 2 (SAM2) can ease this process by propagating annotations across frames, but small errors often accumulate and reduce accuracy, requiring expert review and correction. To address this, we systematically study how annotation errors propagate across different prompt types, namely masks, boxes, and points, and propose Learning-to-Re-Prompt (L2RP), a cost-aware framework that learns when and where to seek expert input. By tuning a human-cost parameter, our method balances annotation effort and segmentation accuracy. Experiments on a private Barrett's dysplasia dataset and the public SUN-SEG benchmark demonstrate improved temporal consistency and superior performance over baseline strategies.

</details>


### [57] [Accelerating Diffusion via Hybrid Data-Pipeline Parallelism Based on Conditional Guidance Scheduling](https://arxiv.org/abs/2602.21760)
*Euisoo Jung,Byunghyun Kim,Hyunjin Kim,Seonghye Cho,Jae-Gil Lee*

Main category: cs.CV

TL;DR: Hybrid parallelism framework combining condition-based partitioning and adaptive parallelism switching to accelerate diffusion model inference while preserving quality.


<details>
  <summary>Details</summary>
Motivation: Current diffusion acceleration methods using distributed parallelism suffer from generation artifacts and fail to achieve acceleration proportional to GPU count, despite diffusion models' remarkable progress in image/video/audio generation.

Method: Proposes hybrid parallelism framework with: (1) condition-based partitioning - novel data parallel strategy leveraging conditional/unconditional denoising paths as partitioning perspective; (2) adaptive parallelism switching - optimal pipeline scheduling method that adaptively enables pipeline parallelism based on denoising discrepancy between paths.

Result: Achieves 2.31× latency reduction on SDXL and 2.07× on SD3 using two NVIDIA RTX 3090 GPUs while preserving image quality. Outperforms existing methods in high-resolution synthesis settings. Works across both U-Net-based diffusion models and DiT-based flow-matching architectures.

Conclusion: The hybrid parallelism framework effectively accelerates diffusion model inference with quality preservation, demonstrating generality across different architectures and superiority in high-resolution settings.

Abstract: Diffusion models have achieved remarkable progress in high-fidelity image, video, and audio generation, yet inference remains computationally expensive. Nevertheless, current diffusion acceleration methods based on distributed parallelism suffer from noticeable generation artifacts and fail to achieve substantial acceleration proportional to the number of GPUs. Therefore, we propose a hybrid parallelism framework that combines a novel data parallel strategy, condition-based partitioning, with an optimal pipeline scheduling method, adaptive parallelism switching, to reduce generation latency and achieve high generation quality in conditional diffusion models. The key ideas are to (i) leverage the conditional and unconditional denoising paths as a new data-partitioning perspective and (ii) adaptively enable optimal pipeline parallelism according to the denoising discrepancy between these two paths. Our framework achieves $2.31\times$ and $2.07\times$ latency reductions on SDXL and SD3, respectively, using two NVIDIA RTX~3090 GPUs, while preserving image quality. This result confirms the generality of our approach across U-Net-based diffusion models and DiT-based flow-matching architectures. Our approach also outperforms existing methods in acceleration under high-resolution synthesis settings. Code is available at https://github.com/kaist-dmlab/Hybridiff.

</details>


### [58] [DynamicGTR: Leveraging Graph Topology Representation Preferences to Boost VLM Capabilities on Graph QAs](https://arxiv.org/abs/2602.21864)
*Yanbin Wei,Jiangyue Yan,Chun Kang,Yang Chen,Hua Liu,James Kwok,Yu Zhang*

Main category: cs.CV

TL;DR: DynamicGTR framework enhances VLMs for graph QA by dynamically selecting optimal graph topology representations per query, improving accuracy and brevity trade-offs without additional training.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs struggle with graph QA because they use single, fixed graph topology representations that ignore model-specific and task-specific preferences, leading to inaccurate or verbose responses.

Method: Proposes DynamicGTR framework that dynamically selects the optimal graph topology representation for each query during inference, enabling customizable accuracy-brevity trade-offs for VLMs.

Result: Improves VLM-based graph algorithm QA performance, successfully transfers experience from synthetic tasks to real-world applications (link prediction, node classification) without additional training, and shows strong cross-task/domain/model transferability.

Conclusion: DynamicGTR offers a flexible solution for enhancing VLMs' zero-shot graph QA capabilities across diverse graph scenarios by adapting representations to specific queries and preferences.

Abstract: Vision-Language Models (VLMs) have emerged as versatile solutions for zero-shot question answering (QA) across various domains. However, enabling VLMs to effectively comprehend structured graphs and perform accurate, efficient QA remains challenging. Existing approaches typically rely on one single graph topology representation (GTR), such as fixed-style visual images or unified text descriptions. This ``one-size-fits-all'' strategy often neglects model-specific and task-specific preferences, resulting in inaccurate or over-lengthy responses to graph-related queries. To address this, we propose the $\mbox{DynamicGTR}$ framework, which dynamically selects the optimal GTR for each query during inference, thereby enhancing the zero-shot graph QA capabilities of VLMs with a customizable accuracy and brevity trade-off. Extensive experiments show that DynamicGTR not only improves VLM-based graph algorithm QA performance but also successfully transfers the experience trained from synthetic graph algorithm tasks to real-world applications like link prediction and node classification, without any additional training. Additionally, DynamicGTR demonstrates strong transferability across tasks, domains, and models, suggesting its potential as a flexible solution for broad graph scenarios.

</details>


### [59] [SAPNet++: Evolving Point-Prompted Instance Segmentation with Semantic and Spatial Awareness](https://arxiv.org/abs/2602.21762)
*Zhaoyang Wei,Xumeng Han,Xuehui Yu,Xue Yang,Guorong Li,Zhenjun Han,Jianbin Jiao*

Main category: cs.CV

TL;DR: SAPNet++ is a novel point-prompted instance segmentation network that addresses granularity ambiguity and boundary uncertainty in single-point annotation tasks through semantic-aware modules and multi-level refinement.


<details>
  <summary>Details</summary>
Motivation: Single-point annotation reduces labeling costs but introduces challenges in high-precision tasks like instance segmentation. Point annotations cause granularity ambiguity (difficulty distinguishing object vs. parts) and boundary uncertainty (imprecise delineation). Existing methods using proposal selection with only category information fail to resolve granularity issues, and mask generators provide limited discrete solutions that deviate from actual boundaries.

Method: Proposes SAPNet++ with three key components: 1) Point Distance Guidance and Box Mining Strategy to address granularity ambiguity at group and local levels, 2) S-MIL (spatial granularity-aware MIL) incorporating completeness scores in proposals for better instance learning, and 3) Multi-level Affinity Refinement using pixel and semantic clues to reduce boundary uncertainty during mask refinement.

Result: Extensive experiments on four challenging datasets validate the effectiveness of SAPNet++, showing significant improvement in segmentation performance by mitigating granularity ambiguity and boundary uncertainty issues inherent in point-prompted instance segmentation.

Conclusion: SAPNet++ successfully addresses the core challenges of point-prompted instance segmentation - granularity ambiguity and boundary uncertainty - through semantic-aware modules and multi-level refinement, demonstrating strong potential to advance PPIS technology while maintaining labeling efficiency.

Abstract: Single-point annotation is increasingly prominent in visual tasks for labeling cost reduction. However, it challenges tasks requiring high precision, such as the point-prompted instance segmentation (PPIS) task, which aims to estimate precise masks using single-point prompts to train a segmentation network. Due to the constraints of point annotations, granularity ambiguity and boundary uncertainty arise the difficulty distinguishing between different levels of detail (eg. whole object vs. parts) and the challenge of precisely delineating object boundaries. Previous works have usually inherited the paradigm of mask generation along with proposal selection to achieve PPIS. However, proposal selection relies solely on category information, failing to resolve the ambiguity of different granularity. Furthermore, mask generators offer only finite discrete solutions that often deviate from actual masks, particularly at boundaries. To address these issues, we propose the Semantic-Aware Point-Prompted Instance Segmentation Network (SAPNet). It integrates Point Distance Guidance and Box Mining Strategy to tackle group and local issues caused by the point's granularity ambiguity. Additionally, we incorporate completeness scores within proposals to add spatial granularity awareness, enhancing multiple instance learning (MIL) in proposal selection termed S-MIL. The Multi-level Affinity Refinement conveys pixel and semantic clues, narrowing boundary uncertainty during mask refinement. These modules culminate in SAPNet++, mitigating point prompt's granularity ambiguity and boundary uncertainty and significantly improving segmentation performance. Extensive experiments on four challenging datasets validate the effectiveness of our methods, highlighting the potential to advance PPIS.

</details>


### [60] [A Framework for Cross-Domain Generalization in Coronary Artery Calcium Scoring Across Gated and Non-Gated Computed Tomography](https://arxiv.org/abs/2602.21935)
*Mahmut S. Gokmen,Moneera N. Haque,Steve W. Leung,Caroline N. Leach,Seth Parker,Stephen B. Hobbs,Vincent L. Sorrell,W. Brent Seales,V. K. Cody Bumgardner*

Main category: cs.CV

TL;DR: Automated framework for coronary artery calcium detection and scoring that works on both ECG-gated and non-gated CT scans using self-supervised Vision Transformer trained only on gated data.


<details>
  <summary>Details</summary>
Motivation: CAC scoring is crucial for cardiovascular risk assessment but currently requires specialized ECG-gated CT scans, limiting its use to cardiac imaging settings. There's a need to enable CAC scoring from routine chest CT scans without requiring additional specialized scans.

Method: Developed CARD-ViT, a self-supervised Vision Transformer trained exclusively on gated CT data using DINO framework. The model performs automated CAC detection and lesion-specific Agatston scoring across both gated and non-gated CT domains without any non-gated training data.

Result: On non-gated Stanford dataset: 0.707 accuracy, Cohen's kappa 0.528 (matching models trained directly on non-gated scans). On gated test sets: 0.910 accuracy with Cohen's kappa scores of 0.871 and 0.874 across independent datasets, demonstrating robust risk stratification.

Conclusion: The framework demonstrates feasibility of cross-domain CAC scoring from gated to non-gated domains, enabling scalable cardiovascular screening in routine chest imaging without requiring additional specialized scans or annotations.

Abstract: Coronary artery calcium (CAC) scoring is a key predictor of cardiovascular risk, but it relies on ECG-gated CT scans, restricting its use to specialized cardiac imaging settings. We introduce an automated framework for CAC detection and lesion-specific Agatston scoring that operates across both gated and non-gated CT scans. At its core is CARD-ViT, a self-supervised Vision Transformer trained exclusively on gated CT data using DINO. Without any non-gated training data, our framework achieves 0.707 accuracy and a Cohen's kappa of 0.528 on the Stanford non-gated dataset, matching models trained directly on non-gated scans. On gated test sets, the framework achieves 0.910 accuracy with Cohen's kappa scores of 0.871 and 0.874 across independent datasets, demonstrating robust risk stratification. These results demonstrate the feasibility of cross-domain CAC scoring from gated to non-gated domains, supporting scalable cardiovascular screening in routine chest imaging without additional scans or annotations.

</details>


### [61] [From Statics to Dynamics: Physics-Aware Image Editing with Latent Transition Priors](https://arxiv.org/abs/2602.21778)
*Liangbing Zhao,Le Zhuo,Sayak Paul,Hongsheng Li,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: PhysicEdit: A physics-aware image editing framework that addresses limitations in handling complex causal dynamics like refraction and material deformation by reformulating editing as predictive physical state transitions, using a large video dataset and dual-thinking mechanism.


<details>
  <summary>Details</summary>
Motivation: Current instruction-based image editing models fail to produce physically plausible results when editing involves complex causal dynamics (refraction, material deformation). The dominant paradigm treats editing as discrete image-to-image mapping, which only provides boundary conditions and leaves transition dynamics underspecified.

Method: 1) Reformulate physics-aware editing as predictive physical state transitions; 2) Introduce PhysicTran38K dataset with 38K transition trajectories across 5 physical domains using two-stage filtering and constraint-aware annotation; 3) Propose PhysicEdit framework with textual-visual dual-thinking mechanism combining frozen Qwen2.5-VL for physically grounded reasoning with learnable transition queries for timestep-adaptive visual guidance to diffusion backbone.

Result: PhysicEdit improves over Qwen-Image-Edit by 5.9% in physical realism and 10.1% in knowledge-grounded editing, setting new SOTA for open-source methods while remaining competitive with leading proprietary models.

Conclusion: The paper successfully addresses the physics-aware image editing challenge by shifting from discrete mapping to predictive state transitions, demonstrating significant improvements in physical realism through the proposed dataset and dual-thinking framework.

Abstract: Instruction-based image editing has achieved remarkable success in semantic alignment, yet state-of-the-art models frequently fail to render physically plausible results when editing involves complex causal dynamics, such as refraction or material deformation. We attribute this limitation to the dominant paradigm that treats editing as a discrete mapping between image pairs, which provides only boundary conditions and leaves transition dynamics underspecified. To address this, we reformulate physics-aware editing as predictive physical state transitions and introduce PhysicTran38K, a large-scale video-based dataset comprising 38K transition trajectories across five physical domains, constructed via a two-stage filtering and constraint-aware annotation pipeline. Building on this supervision, we propose PhysicEdit, an end-to-end framework equipped with a textual-visual dual-thinking mechanism. It combines a frozen Qwen2.5-VL for physically grounded reasoning with learnable transition queries that provide timestep-adaptive visual guidance to a diffusion backbone. Experiments show that PhysicEdit improves over Qwen-Image-Edit by 5.9% in physical realism and 10.1% in knowledge-grounded editing, setting a new state-of-the-art for open-source methods, while remaining competitive with leading proprietary models.

</details>


### [62] [PatchDenoiser: Parameter-efficient multi-scale patch learning and fusion denoiser for medical images](https://arxiv.org/abs/2602.21987)
*Jitindra Fartiyal,Pedro Freire,Sergei K. Turitsyn,Sergei G. Solovski*

Main category: cs.CV

TL;DR: PatchDenoiser is a lightweight multi-scale patch-based framework for medical image denoising that effectively removes noise while preserving fine anatomical details, with significantly fewer parameters and lower computational cost than existing deep learning methods.


<details>
  <summary>Details</summary>
Motivation: Medical image quality is often degraded by noise from various sources (low-dose acquisition, patient motion, scanner limitations), affecting clinical interpretation and downstream analysis. Traditional filtering methods over-smooth and lose details, while deep learning approaches (CNNs, GANs, transformers) struggle with detail preservation or require large, computationally expensive models that limit clinical practicality.

Method: PatchDenoiser is a lightweight, energy-efficient multi-scale patch-based denoising framework that decomposes denoising into local texture extraction and global context aggregation, fused via a spatially aware patch fusion strategy. This design enables effective noise suppression while preserving fine structural details.

Result: On the 2016 Mayo Low-Dose CT dataset, PatchDenoiser consistently outperforms state-of-the-art CNN- and GAN-based methods in PSNR and SSIM. It shows robustness to variations in slice thickness, reconstruction kernels, and HU windows, generalizes across scanners without fine-tuning, and reduces parameters by ~9x and energy consumption per inference by ~27x compared with conventional CNN denoisers.

Conclusion: PatchDenoiser provides a practical, scalable, and computationally efficient solution for medical image denoising that balances performance, robustness, and clinical deployability, addressing the limitations of both traditional filtering and existing deep learning approaches.

Abstract: Medical images are essential for diagnosis, treatment planning, and research, but their quality is often degraded by noise from low-dose acquisition, patient motion, or scanner limitations, affecting both clinical interpretation and downstream analysis. Traditional filtering approaches often over-smooth and lose fine anatomical details, while deep learning methods, including CNNs, GANs, and transformers, may struggle to preserve such details or require large, computationally expensive models, limiting clinical practicality.
  We propose PatchDenoiser, a lightweight, energy-efficient multi-scale patch-based denoising framework. It decomposes denoising into local texture extraction and global context aggregation, fused via a spatially aware patch fusion strategy. This design enables effective noise suppression while preserving fine structural and anatomical details. PatchDenoiser is ultra-lightweight, with far fewer parameters and lower computational complexity than CNN-, GAN-, and transformer-based denoisers.
  On the 2016 Mayo Low-Dose CT dataset, PatchDenoiser consistently outperforms state-of-the-art CNN- and GAN-based methods in PSNR and SSIM. It is robust to variations in slice thickness, reconstruction kernels, and HU windows, generalizes across scanners without fine-tuning, and reduces parameters by ~9x and energy consumption per inference by ~27x compared with conventional CNN denoisers.
  PatchDenoiser thus provides a practical, scalable, and computationally efficient solution for medical image denoising, balancing performance, robustness, and clinical deployability.

</details>


### [63] [RGB-Event HyperGraph Prompt for Kilometer Marker Recognition based on Pre-trained Foundation Models](https://arxiv.org/abs/2602.22026)
*Xiaoyu Xian,Shiao Wang,Xiao Wang,Daxin Tian,Yan Tian*

Main category: cs.CV

TL;DR: The paper proposes a multi-modal approach combining RGB and event cameras for kilometer marker recognition in metro systems, addressing challenges like illumination variations and high-speed motion. It introduces a new dataset EvMetro5K and shows improved performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Metro train perception systems face significant challenges due to illumination variations, high-speed motion, and adverse weather conditions. Conventional RGB cameras struggle in these complex environments, especially for critical tasks like kilometer marker recognition needed for autonomous localization in GNSS-denied conditions.

Method: The authors propose a multi-modal adaptation approach that integrates event cameras with RGB cameras. They use a pre-trained RGB OCR foundation model enhanced through multi-modal adaptation to leverage event camera advantages in low-light, high-speed scenarios. They also construct EvMetro5K, the first large-scale RGB-Event dataset with 5,599 synchronized sample pairs.

Result: Extensive experiments on EvMetro5K and other benchmarks demonstrate the effectiveness of the proposed approach for kilometer marker recognition. The method shows improved robustness in challenging metro environments compared to conventional RGB-only approaches.

Conclusion: Integrating event cameras with RGB systems provides a robust solution for metro perception challenges. The proposed multi-modal adaptation approach and the new EvMetro5K dataset advance kilometer marker recognition for autonomous metro localization in GNSS-denied conditions.

Abstract: Metro trains often operate in highly complex environments, characterized by illumination variations, high-speed motion, and adverse weather conditions. These factors pose significant challenges for visual perception systems, especially those relying solely on conventional RGB cameras. To tackle these difficulties, we explore the integration of event cameras into the perception system, leveraging their advantages in low-light conditions, high-speed scenarios, and low power consumption. Specifically, we focus on Kilometer Marker Recognition (KMR), a critical task for autonomous metro localization under GNSS-denied conditions. In this context, we propose a robust baseline method based on a pre-trained RGB OCR foundation model, enhanced through multi-modal adaptation. Furthermore, we construct the first large-scale RGB-Event dataset, EvMetro5K, containing 5,599 pairs of synchronized RGB-Event samples, split into 4,479 training and 1,120 testing samples. Extensive experiments on EvMetro5K and other widely used benchmarks demonstrate the effectiveness of our approach for KMR. Both the dataset and source code will be released on https://github.com/Event-AHU/EvMetro5K_benchmark

</details>


### [64] [XStreamVGGT: Extremely Memory-Efficient Streaming Vision Geometry Grounded Transformer with KV Cache Compression](https://arxiv.org/abs/2602.21780)
*Zunhai Su,Weihao Ye,Hansen Feng,Keyu Fan,Jing Zhang,Dahai Yu,Zhengwu Liu,Ngai Wong*

Main category: cs.CV

TL;DR: XStreamVGGT introduces a tuning-free KV cache compression method combining pruning and quantization to enable memory-efficient streaming 3D reconstruction, reducing memory usage 4.42× and accelerating inference 5.48× with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: StreamVGGT's frame-wise causal attention for 3D reconstruction suffers from unbounded KV cache growth with multi-image/long-video inputs, leading to high memory consumption and latency that limits scalability for long-horizon applications.

Method: Proposes XStreamVGGT with two-stage compression: 1) Prunes redundant KVs using token-importance identification to fit fixed memory budget while maintaining FlashAttention compatibility; 2) Applies dimension-adaptive KV quantization leveraging inherent distribution patterns to further reduce memory overhead.

Result: Achieves mostly negligible performance degradation while reducing memory usage by 4.42× and accelerating inference by 5.48×, enabling practical and scalable streaming 3D applications.

Conclusion: XStreamVGGT provides an effective tuning-free solution for KV cache compression in streaming 3D reconstruction, addressing scalability limitations of previous methods through systematic pruning and quantization while maintaining performance.

Abstract: Learning-based 3D visual geometry models have significantly advanced with the advent of large-scale transformers. Among these, StreamVGGT leverages frame-wise causal attention to deliver robust and efficient streaming 3D reconstruction. However, it suffers from unbounded growth in the Key-Value (KV) cache due to the massive influx of vision tokens from multi-image and long-video inputs, leading to increased memory consumption and inference latency as input frames accumulate. This ultimately limits its scalability for long-horizon applications. To address this gap, we propose XStreamVGGT, a tuning-free approach that seamlessly integrates pruning and quantization to systematically compress the KV cache, enabling extremely memory-efficient streaming inference. Specifically, redundant KVs generated from multi-frame inputs are initially pruned to conform to a fixed KV memory budget using an efficient token-importance identification mechanism that maintains full compatibility with high-performance attention kernels (e.g., FlashAttention). Additionally, leveraging the inherent distribution patterns of KV tensors, we apply dimension-adaptive KV quantization within the pruning pipeline to further minimize memory overhead while preserving numerical accuracy. Extensive evaluations show that XStreamVGGT achieves mostly negligible performance degradation while substantially reducing memory usage by 4.42$\times$ and accelerating inference by 5.48$\times$, enabling practical and scalable streaming 3D applications. The code is available at https://github.com/ywh187/XStreamVGGT/.

</details>


### [65] [GeoMotion: Rethinking Motion Segmentation via Latent 4D Geometry](https://arxiv.org/abs/2602.21810)
*Xiankang He,Peile Lin,Ying Cui,Dongyan Guo,Chunhua Shen,Xiaoqin Zhang*

Main category: cs.CV

TL;DR: A fully learning-based motion segmentation method that bypasses explicit correspondence estimation and uses attention mechanisms to directly infer moving objects from latent features, achieving state-of-the-art performance with high efficiency.


<details>
  <summary>Details</summary>
Motivation: Conventional motion segmentation methods rely on noisy motion cues, camera pose estimation, and point correspondences, leading to cumulative errors in multi-stage pipelines with limited performance or high computational cost.

Method: End-to-end feed-forward approach using attention mechanisms to directly infer moving objects from latent feature representations, implicitly disentangling object and camera motion without explicit correspondence estimation. Leverages reliable camera poses and spatial-temporal priors from 4D scene geometry reconstruction.

Result: Extensive experiments show state-of-the-art motion segmentation performance with high efficiency by eliminating complex pre-processing and iterative refinement.

Conclusion: The proposed fully learning-based approach demonstrates that bypassing explicit correspondence estimation and leveraging modern 4D reconstruction techniques enables efficient and robust motion segmentation.

Abstract: Motion segmentation in dynamic scenes is highly challenging, as conventional methods heavily rely on estimating camera poses and point correspondences from inherently noisy motion cues. Existing statistical inference or iterative optimization techniques that struggle to mitigate the cumulative errors in multi-stage pipelines often lead to limited performance or high computational cost. In contrast, we propose a fully learning-based approach that directly infers moving objects from latent feature representations via attention mechanisms, thus enabling end-to-end feed-forward motion segmentation. Our key insight is to bypass explicit correspondence estimation and instead let the model learn to implicitly disentangle object and camera motion. Supported by recent advances in 4D scene geometry reconstruction (e.g., $π^3$), the proposed method leverages reliable camera poses and rich spatial-temporal priors, which ensure stable training and robust inference for the model. Extensive experiments demonstrate that by eliminating complex pre-processing and iterative refinement, our approach achieves state-of-the-art motion segmentation performance with high efficiency. The code is available at:https://github.com/zjutcvg/GeoMotion.

</details>


### [66] [NESTOR: A Nested MOE-based Neural Operator for Large-Scale PDE Pre-Training](https://arxiv.org/abs/2602.22059)
*Dengdi Sun,Xiaoya Zhou,Xiao Wang,Hao Si,Wanli Lyu,Jin Tang,Bin Luo*

Main category: cs.CV

TL;DR: Proposes a large-scale PDE pre-trained neural operator using nested Mixture-of-Experts (MoE) framework to capture both global and local dependencies for improved generalization.


<details>
  <summary>Details</summary>
Motivation: Existing neural operators rely on single network architectures, limiting their ability to capture heterogeneous features and complex system dependencies in diverse PDE systems, creating a bottleneck for large-scale PDE pre-training.

Method: A nested Mixture-of-Experts (MoE) framework with image-level MoE for global dependencies and token-level Sub-MoE for local dependencies, enabling selective activation of expert networks for given inputs.

Result: Conducted large-scale pre-training on twelve diverse PDE datasets and successfully transferred the model to downstream tasks, with extensive experiments demonstrating effectiveness.

Conclusion: The proposed nested MoE framework enhances neural operator capacity for capturing heterogeneous PDE features, improves generalization and transferability, and enables effective large-scale PDE pre-training.

Abstract: Neural operators have emerged as an efficient paradigm for solving PDEs, overcoming the limitations of traditional numerical methods and significantly improving computational efficiency. However, due to the diversity and complexity of PDE systems, existing neural operators typically rely on a single network architecture, which limits their capacity to fully capture heterogeneous features and complex system dependencies. This constraint poses a bottleneck for large-scale PDE pre-training based on neural operators. To address these challenges, we propose a large-scale PDE pre-trained neural operator based on a nested Mixture-of-Experts (MoE) framework. In particular, the image-level MoE is designed to capture global dependencies, while the token-level Sub-MoE focuses on local dependencies. Our model can selectively activate the most suitable expert networks for a given input, thereby enhancing generalization and transferability. We conduct large-scale pre-training on twelve PDE datasets from diverse sources and successfully transfer the model to downstream tasks. Extensive experiments demonstrate the effectiveness of our approach.

</details>


### [67] [SkyReels-V4: Multi-modal Video-Audio Generation, Inpainting and Editing model](https://arxiv.org/abs/2602.21818)
*Guibin Chen,Dixuan Lin,Jiangping Yang,Youqiang Zhang,Zhengcong Fei,Debang Li,Sheng Chen,Chaofeng Ao,Nuo Pang,Yiming Wang,Yikun Dou,Zheng Chen,Mingyuan Fan,Tuanhui Li,Mingshan Chang,Hao Zhang,Xiaopeng Sun,Jingtao Xu,Yuqiang Xie,Jiahua Wang,Zhiheng Xu,Weiming Xiong,Yuzhe Jin,Baoxuan Gu,Binjie Mao,Yunjie Yu,Jujie He,Yuhao Feng,Shiwen Tu,Chaojie Wang,Rui Yan,Wei Shen,Jingchen Wu,Peng Zhao,Xuanyue Zhong,Zhuangzhuang Liu,Kaifei Wang,Fuxiang Zhang,Weikai Xu,Wenyan Liu,Binglu Zhang,Yu Shen,Tianhui Xiong,Bin Peng,Liang Zeng,Xuchen Song,Haoxiang Guo,Peiyu Wang,Yahui Zhou*

Main category: cs.CV

TL;DR: SkyReels V4 is a unified multimodal video foundation model that jointly generates video and audio, supports inpainting/editing, handles multiple input modalities, and achieves cinema-quality 1080p resolution with synchronized audio.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive video foundation model that unifies multiple video tasks (generation, inpainting, editing) with synchronized audio generation, supporting rich multimodal inputs while maintaining efficiency for high-resolution, long-duration content.

Method: Dual-stream Multimodal Diffusion Transformer (MMDiT) architecture with separate branches for video and audio synthesis, sharing a multimodal LLM text encoder. Uses channel concatenation for unified inpainting tasks and joint low-res/high-res generation strategy for efficiency.

Result: Achieves 1080p resolution, 32 FPS, 15-second duration video generation with synchronized audio. Supports complex multimodal conditioning and unified treatment of generation, inpainting, and editing tasks with computational efficiency.

Conclusion: SkyReels V4 is the first video foundation model to simultaneously support multimodal input, joint video-audio generation, and unified generation/inpainting/editing, enabling high-quality cinematic content creation with synchronized sound.

Abstract: SkyReels V4 is a unified multi modal video foundation model for joint video audio generation, inpainting, and editing. The model adopts a dual stream Multimodal Diffusion Transformer (MMDiT) architecture, where one branch synthesizes video and the other generates temporally aligned audio, while sharing a powerful text encoder based on the Multimodal Large Language Models (MMLM). SkyReels V4 accepts rich multi modal instructions, including text, images, video clips, masks, and audio references. By combining the MMLMs multi modal instruction following capability with in context learning in the video branch MMDiT, the model can inject fine grained visual guidance under complex conditioning, while the audio branch MMDiT simultaneously leverages audio references to guide sound generation. On the video side, we adopt a channel concatenation formulation that unifies a wide range of inpainting style tasks, such as image to video, video extension, and video editing under a single interface, and naturally extends to vision referenced inpainting and editing via multi modal prompts. SkyReels V4 supports up to 1080p resolution, 32 FPS, and 15 second duration, enabling high fidelity, multi shot, cinema level video generation with synchronized audio. To make such high resolution, long-duration generation computationally feasible, we introduce an efficiency strategy: Joint generation of low resolution full sequences and high-resolution keyframes, followed by dedicated super-resolution and frame interpolation models. To our knowledge, SkyReels V4 is the first video foundation model that simultaneously supports multi-modal input, joint video audio generation, and a unified treatment of generation, inpainting, and editing, while maintaining strong efficiency and quality at cinematic resolutions and durations.

</details>


### [68] [Joint Shadow Generation and Relighting via Light-Geometry Interaction Maps](https://arxiv.org/abs/2602.21820)
*Shan Wang,Peixia Li,Chenchen Xu,Ziang Cheng,Jiayu Yang,Hongdong Li,Pulak Purkait*

Main category: cs.CV

TL;DR: LGI maps encode light-aware occlusion from monocular depth, providing physics-inspired priors for joint shadow generation and relighting, enabling physically consistent results without full 3D reconstruction.


<details>
  <summary>Details</summary>
Motivation: Current generative models often produce floating shadows, inconsistent illumination, and implausible shadow geometry due to lack of physics-based constraints. Prior methods treat shadow generation and relighting as disjoint tasks, missing their intrinsic coupling essential for modeling indirect lighting effects.

Method: Propose Light-Geometry Interaction (LGI) maps computed from 2.5D depth maps, explicitly tying illumination direction to geometry. Embed LGI into a bridge-matching generative backbone for joint shadow generation and relighting. Create first large-scale benchmark dataset covering reflections, transparency, and complex interreflections.

Result: Significant gains in realism and consistency across synthetic and real images. LGI reduces ambiguity and enforces physically consistent light-shadow reasoning, bridging geometry-inspired rendering with generative modeling.

Conclusion: LGI enables efficient, physically consistent shadow generation and relighting by providing essential light-shadow interaction constraints without requiring full 3D reconstruction, outperforming prior disjoint approaches.

Abstract: We propose Light-Geometry Interaction (LGI) maps, a novel representation that encodes light-aware occlusion from monocular depth. Unlike ray tracing, which requires full 3D reconstruction, LGI captures essential light-shadow interactions reliably and accurately, computed from off-the-shelf 2.5D depth map predictions. LGI explicitly ties illumination direction to geometry, providing a physics-inspired prior that constrains generative models. Without such prior, these models often produce floating shadows, inconsistent illumination, and implausible shadow geometry. Building on this representation, we propose a unified pipeline for joint shadow generation and relighting - unlike prior methods that treat them as disjoint tasks - capturing the intrinsic coupling of illumination and shadowing essential for modeling indirect effects. By embedding LGI into a bridge-matching generative backbone, we reduce ambiguity and enforce physically consistent light-shadow reasoning. To enable effective training, we curated the first large-scale benchmark dataset for joint shadow and relighting, covering reflections, transparency, and complex interreflections. Experiments show significant gains in realism and consistency across synthetic and real images. LGI thus bridges geometry-inspired rendering with generative modeling, enabling efficient, physically consistent shadow generation and relighting.

</details>


### [69] [NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors](https://arxiv.org/abs/2602.22144)
*Lingfeng Ren,Weihao Yu,Runpeng Yu,Xinchao Wang*

Main category: cs.CV

TL;DR: NoLan is a training-free framework that reduces object hallucinations in LVLMs by dynamically suppressing language decoder priors based on output distribution differences between multimodal and text-only inputs.


<details>
  <summary>Details</summary>
Motivation: To determine which component of LVLMs (vision encoder vs language decoder) primarily causes object hallucinations, and develop a solution based on the findings.

Method: Systematic experiments to analyze hallucination sources, revealing language decoder priors as main cause. Proposed NoLan framework refines output distribution by dynamically suppressing language priors using output distribution differences between multimodal and text-only inputs.

Result: NoLan effectively reduces object hallucinations across various LVLMs, achieving substantial improvements on POPE benchmark (e.g., +6.45 accuracy for LLaVA-1.5 7B, +7.21 for Qwen-VL 7B).

Conclusion: Object hallucinations in LVLMs are primarily caused by language decoder priors, and NoLan provides an effective training-free solution to mitigate this issue.

Abstract: Object hallucination is a critical issue in Large Vision-Language Models (LVLMs), where outputs include objects that do not appear in the input image. A natural question arises from this phenomenon: Which component of the LVLM pipeline primarily contributes to object hallucinations? The vision encoder to perceive visual information, or the language decoder to generate text responses? In this work, we strive to answer this question through designing a systematic experiment to analyze the roles of the vision encoder and the language decoder in hallucination generation. Our observations reveal that object hallucinations are predominantly associated with the strong priors from the language decoder. Based on this finding, we propose a simple and training-free framework, No-Language-Hallucination Decoding, NoLan, which refines the output distribution by dynamically suppressing language priors, modulated based on the output distribution difference between multimodal and text-only inputs. Experimental results demonstrate that NoLan effectively reduces object hallucinations across various LVLMs on different tasks. For instance, NoLan achieves substantial improvements on POPE, enhancing the accuracy of LLaVA-1.5 7B and Qwen-VL 7B by up to 6.45 and 7.21, respectively. The code is publicly available at: https://github.com/lingfengren/NoLan.

</details>


### [70] [UniVBench: Towards Unified Evaluation for Video Foundation Models](https://arxiv.org/abs/2602.21835)
*Jianhui Wei,Xiaotian Zhang,Yichen Li,Yuan Wang,Yan Zhang,Ziyi Chen,Zhihang Tang,Wei Xu,Zuozhu Liu*

Main category: cs.CV

TL;DR: UniVBench is a comprehensive benchmark for evaluating video foundation models across four core abilities (understanding, generation, editing, reconstruction) using 200 high-quality multi-shot videos and a unified evaluation system.


<details>
  <summary>Details</summary>
Motivation: Existing video model benchmarks are fragmented, task-specific, and use simple videos, failing to capture the unified capabilities that video foundation models are designed to deliver.

Method: Created UniVBench with 200 diverse, human-created multi-shot videos with detailed captions and editing instructions, plus UniV-Eval - a unified agentic evaluation system for standardized prompting, instruction parsing, and scoring across all tasks.

Result: Provides the first framework for measuring integrated video foundation model capabilities, with extensive human annotations ensuring evaluation aligns with human judgment.

Conclusion: UniVBench enables rigorous, fair, and reproducible assessment of video foundation models, accelerating progress toward robust video intelligence by addressing the fragmentation in current evaluation approaches.

Abstract: Video foundation models aim to integrate video understanding, generation, editing, and instruction following within a single framework, making them a central direction for next-generation multimodal systems. However, existing evaluation benchmarks remain fragmented and limited in scope, as they each target a single task, rely on task-specific metrics, and typically use short or simple video clips. As a result, they do not capture the unified capabilities that these models are designed to deliver. To address this gap, we introduce UniVBench, a benchmark purpose-built for evaluating video foundation models across four core abilities: video understanding, video generation, video editing, and a newly proposed task, video reconstruction, which assesses how faithfully a model can reproduce video content it has encountered. Our benchmark substantially expands the complexity of evaluation by incorporating 200 high-quality, diverse and multi-shot videos, each paired with detailed captions, multi-format editing instructions, and reference images. All videos are human-created and carefully validated, offering richer cinematic information than prior benchmarks. In addition, we develop a unified agentic evaluation system (UniV-Eval) that standardizes prompting, instruction parsing, and scoring across all tasks, enabling fair, scalable, and reproducible comparisons of unified video models. By grounding evaluation in instruction-based multi-shot video tasks, UniVBench provides the first framework for measuring the integrated capabilities that video foundation models aim to achieve. Extensive human annotations ensure our evaluation aligns with human judgment, enabling rigorous assessment and accelerating progress toward robust video intelligence.

</details>


### [71] [Meta-FC: Meta-Learning with Feature Consistency for Robust and Generalizable Watermarking](https://arxiv.org/abs/2602.21849)
*Yuheng Li,Weitong Chen,Chengcheng Zhu,Jiale Zhang,Chunpeng Ge,Di Wu,Guodong Long*

Main category: cs.CV

TL;DR: Meta-FC training strategy improves watermarking robustness via meta-learning and feature consistency, outperforming single random distortion training.


<details>
  <summary>Details</summary>
Motivation: Current watermarking methods use single random distortion (SRD) training which treats distortions independently, causing optimization conflicts and limiting robustness/generalization across different distortion types.

Method: Proposes Meta-FC: meta-learning with feature consistency. Randomly samples multiple distortions for meta-training while holding out one as "unknown" for meta-testing. Uses feature consistency loss to maintain decoded feature consistency across different distortions applied to the same image.

Result: Meta-FC improves robustness and generalization by average of 1.59% (high-intensity distortions), 4.71% (combined distortions), and 2.38% (unknown distortions) compared to SRD training across various watermarking models.

Conclusion: Meta-FC effectively addresses optimization conflicts in SRD training by promoting stable neuron activations and distortion-invariant representations, significantly enhancing watermarking robustness and generalization.

Abstract: Deep learning-based watermarking has made remarkable progress in recent years. To achieve robustness against various distortions, current methods commonly adopt a training strategy where a \underline{\textbf{s}}ingle \underline{\textbf{r}}andom \underline{\textbf{d}}istortion (SRD) is chosen as the noise layer in each training batch. However, the SRD strategy treats distortions independently within each batch, neglecting the inherent relationships among different types of distortions and causing optimization conflicts across batches. As a result, the robustness and generalizability of the watermarking model are limited. To address this issue, we propose a novel training strategy that enhances robustness and generalization via \underline{\textbf{meta}}-learning with \underline{\textbf{f}}eature \underline{\textbf{c}}onsistency (Meta-FC). Specifically, we randomly sample multiple distortions from the noise pool to construct a meta-training task, while holding out one distortion as a simulated ``unknown'' distortion for the meta-testing phase. Through meta-learning, the model is encouraged to identify and utilize neurons that exhibit stable activations across different types of distortions, mitigating the optimization conflicts caused by the random sampling of diverse distortions in each batch. To further promote the transformation of stable activations into distortion-invariant representations, we introduce a feature consistency loss that constrains the decoded features of the same image subjected to different distortions to remain consistent. Extensive experiments demonstrate that, compared to the SRD training strategy, Meta-FC improves the robustness and generalization of various watermarking models by an average of 1.59\%, 4.71\%, and 2.38\% under high-intensity, combined, and unknown distortions.

</details>


### [72] [Off-The-Shelf Image-to-Image Models Are All You Need To Defeat Image Protection Schemes](https://arxiv.org/abs/2602.22197)
*Xavier Pleimling,Sifat Muhammad Abdullah,Gunjan Balde,Peng Gao,Mainack Mondal,Murtuza Jadliwala,Bimal Viswanath*

Main category: cs.CV

TL;DR: Off-the-shelf image-to-image GenAI models can be repurposed as generic denoisers to effectively remove protective perturbations from images, exposing vulnerabilities in current image protection schemes.


<details>
  <summary>Details</summary>
Motivation: Current image protection methods rely on adding imperceptible perturbations to prevent unauthorized use, but existing attacks require specialized methods. The authors aim to demonstrate that generic attacks using readily available tools are now possible.

Method: Repurpose off-the-shelf image-to-image GenAI models as generic denoisers using simple text prompts to remove protective perturbations from images across diverse protection schemes.

Result: The general-purpose attack successfully circumvents 6 diverse protection schemes across 8 case studies, outperforming existing specialized attacks while preserving image utility for adversaries.

Conclusion: Current image protection schemes provide a false sense of security, revealing a critical vulnerability. There's urgent need for robust defenses and benchmarking future protections against attacks from off-the-shelf GenAI models.

Abstract: Advances in Generative AI (GenAI) have led to the development of various protection strategies to prevent the unauthorized use of images. These methods rely on adding imperceptible protective perturbations to images to thwart misuse such as style mimicry or deepfake manipulations. Although previous attacks on these protections required specialized, purpose-built methods, we demonstrate that this is no longer necessary. We show that off-the-shelf image-to-image GenAI models can be repurposed as generic ``denoisers" using a simple text prompt, effectively removing a wide range of protective perturbations. Across 8 case studies spanning 6 diverse protection schemes, our general-purpose attack not only circumvents these defenses but also outperforms existing specialized attacks while preserving the image's utility for the adversary. Our findings reveal a critical and widespread vulnerability in the current landscape of image protection, indicating that many schemes provide a false sense of security. We stress the urgent need to develop robust defenses and establish that any future protection mechanism must be benchmarked against attacks from off-the-shelf GenAI models. Code is available in this repository: https://github.com/mlsecviswanath/img2imgdenoiser

</details>


### [73] [GFPL: Generative Federated Prototype Learning for Resource-Constrained and Data-Imbalanced Vision Task](https://arxiv.org/abs/2602.21873)
*Shiwei Lu,Yuhang He,Jiashuo Li,Qiang Wang,Yihong Gong*

Main category: cs.CV

TL;DR: GFPL is a federated learning framework that uses generative prototypes and dual-classifier architecture to address class imbalance and communication overhead, achieving 3.6% accuracy improvement with low communication cost.


<details>
  <summary>Details</summary>
Motivation: Conventional federated learning faces two critical challenges: ineffective knowledge fusion due to model updates biased toward majority-class features, and prohibitive communication overhead from frequent transmissions of high-dimensional model parameters.

Method: Proposes Generative Federated Prototype Learning (GFPL) with: 1) GMM-based prototype generation capturing class-wise feature statistics, 2) Bhattacharyya distance-based prototype aggregation for semantic knowledge fusion, 3) pseudo-feature generation to mitigate feature distribution imbalance, and 4) dual-classifier architecture optimized via hybrid Dot Regression and Cross-Entropy loss.

Result: Extensive experiments show GFPL improves model accuracy by 3.6% under imbalanced data settings while maintaining low communication cost compared to conventional federated learning approaches.

Conclusion: GFPL effectively addresses class imbalance and communication overhead in federated learning through generative prototype learning and dual-classifier optimization, demonstrating practical value for real-world deployment in applications like medical imaging and autonomous driving.

Abstract: Federated learning (FL) facilitates the secure utilization of decentralized images, advancing applications in medical image recognition and autonomous driving. However, conventional FL faces two critical challenges in real-world deployment: ineffective knowledge fusion caused by model updates biased toward majority-class features, and prohibitive communication overhead due to frequent transmissions of high-dimensional model parameters. Inspired by the human brain's efficiency in knowledge integration, we propose a novel Generative Federated Prototype Learning (GFPL) framework to address these issues. Within this framework, a prototype generation method based on Gaussian Mixture Model (GMM) captures the statistical information of class-wise features, while a prototype aggregation strategy using Bhattacharyya distance effectively fuses semantically similar knowledge across clients. In addition, these fused prototypes are leveraged to generate pseudo-features, thereby mitigating feature distribution imbalance across clients. To further enhance feature alignment during local training, we devise a dual-classifier architecture, optimized via a hybrid loss combining Dot Regression and Cross-Entropy. Extensive experiments on benchmarks show that GFPL improves model accuracy by 3.6% under imbalanced data settings while maintaining low communication cost.

</details>


### [74] [How to Take a Memorable Picture? Empowering Users with Actionable Feedback](https://arxiv.org/abs/2602.21877)
*Francesco Laiti,Davide Talon,Jacopo Staiano,Elisa Ricci*

Main category: cs.CV

TL;DR: MemCoach: A training-free MLLM-based system that provides natural language feedback to improve image memorability at capture time, shifting from passive prediction to actionable guidance.


<details>
  <summary>Details</summary>
Motivation: Current image memorability research focuses on passive prediction or post-hoc generative enhancement, but lacks support for users at the crucial capture moment when they need guidance to improve photo memorability.

Method: MemCoach uses Multimodal Large Language Models (MLLMs) with a teacher-student steering strategy that aligns model activations toward memorable patterns learned from a teacher model trained on least-to-most memorable samples. The approach is training-free.

Result: MemCoach consistently outperforms zero-shot models on the new MemBench benchmark, demonstrating that memorability can be taught and instructed through actionable feedback rather than just predicted.

Conclusion: The work introduces Memorability Feedback as a new task, showing that memorability guidance can be provided at capture time through natural language suggestions, shifting the paradigm from prediction to actionable human-interpretable feedback.

Abstract: Image memorability, i.e., how likely an image is to be remembered, has traditionally been studied in computer vision either as a passive prediction task, with models regressing a scalar score, or with generative methods altering the visual input to boost the image likelihood of being remembered. Yet, none of these paradigms supports users at capture time, when the crucial question is how to improve a photo memorability. We introduce the task of Memorability Feedback (MemFeed), where an automated model should provide actionable, human-interpretable guidance to users with the goal to enhance an image future recall. We also present MemCoach, the first approach designed to provide concrete suggestions in natural language for memorability improvement (e.g., "emphasize facial expression," "bring the subject forward"). Our method, based on Multimodal Large Language Models (MLLMs), is training-free and employs a teacher-student steering strategy, aligning the model internal activations toward more memorable patterns learned from a teacher model progressing along least-to-most memorable samples. To enable systematic evaluation on this novel task, we further introduce MemBench, a new benchmark featuring sequence-aligned photoshoots with annotated memorability scores. Our experiments, considering multiple MLLMs, demonstrate the effectiveness of MemCoach, showing consistently improved performance over several zero-shot models. The results indicate that memorability can not only be predicted but also taught and instructed, shifting the focus from mere prediction to actionable feedback for human creators.

</details>


### [75] [EndoDDC: Learning Sparse to Dense Reconstruction for Endoscopic Robotic Navigation via Diffusion Depth Completion](https://arxiv.org/abs/2602.21893)
*Yinheng Lin,Yiming Huang,Beilei Cui,Long Bai,Huxin Gao,Hongliang Ren,Jiewen Lai*

Main category: cs.CV

TL;DR: EndoDDC is a depth completion method for endoscopic surgery that combines images, sparse depth, and depth gradient features with a diffusion model to improve depth estimation in challenging endoscopic environments with weak textures and variable lighting.


<details>
  <summary>Details</summary>
Motivation: Accurate depth estimation is critical for endoscopic surgical robot navigation, but existing methods struggle in endoscopic environments with weak textures and variable lighting. While self-supervised methods avoid needing precise depth annotations, they produce sparse reconstructions with invalid depth. Depth completion can help, but current techniques haven't been well adapted to endoscopy.

Method: EndoDDC integrates images, sparse depth information, and depth gradient features, then optimizes depth maps through a diffusion model to address weak texture and light reflection issues in endoscopic environments.

Result: Extensive experiments on two publicly available endoscopy datasets show EndoDDC outperforms state-of-the-art models in both depth accuracy and robustness.

Conclusion: The method demonstrates potential to reduce visual errors in complex endoscopic environments, with code to be released publicly.

Abstract: Accurate depth estimation plays a critical role in the navigation of endoscopic surgical robots, forming the foundation for 3D reconstruction and safe instrument guidance. Fine-tuning pretrained models heavily relies on endoscopic surgical datasets with precise depth annotations. While existing self-supervised depth estimation techniques eliminate the need for accurate depth annotations, their performance degrades in environments with weak textures and variable lighting, leading to sparse reconstruction with invalid depth estimation. Depth completion using sparse depth maps can mitigate these issues and improve accuracy. Despite the advances in depth completion techniques in general fields, their application in endoscopy remains limited. To overcome these limitations, we propose EndoDDC, an endoscopy depth completion method that integrates images, sparse depth information with depth gradient features, and optimizes depth maps through a diffusion model, addressing the issues of weak texture and light reflection in endoscopic environments. Extensive experiments on two publicly available endoscopy datasets show that our approach outperforms state-of-the-art models in both depth accuracy and robustness. This demonstrates the potential of our method to reduce visual errors in complex endoscopic environments. Our code will be released at https://github.com/yinheng-lin/EndoDDC.

</details>


### [76] [TIRAuxCloud: A Thermal Infrared Dataset for Day and Night Cloud Detection](https://arxiv.org/abs/2602.21905)
*Alexis Apostolakis,Vasileios Botsos,Niklas Wölki,Andrea Spichtinger,Nikolaos Ioannis Bountos,Ioannis Papoutsis,Panayiotis Tsanakas*

Main category: cs.CV

TL;DR: TIRAuxCloud: A multi-modal thermal infrared dataset with auxiliary layers for day/night cloud segmentation, addressing limitations of optical bands at night.


<details>
  <summary>Details</summary>
Motivation: Clouds obstruct Earth observation applications like disaster response and climate monitoring. Optical bands fail at night due to lack of sunlight, while thermal infrared works but faces challenges from limited spectral info and low resolution.

Method: Created TIRAuxCloud dataset combining multispectral TIR, optical, and NIR data from Landsat/VIIRS with auxiliary layers (elevation, land cover, weather, cloud-free references). Includes automated cloud masks plus manual annotations for evaluation.

Result: Dataset enables comprehensive benchmarks through supervised and transfer learning, establishing performance baselines for day/night cloud detection models.

Conclusion: TIRAuxCloud advances cloud segmentation research by providing multi-modal thermal data with auxiliary information to reduce surface-cloud ambiguity, supporting development of innovative methods for 24/7 cloud detection.

Abstract: Clouds are a major obstacle in Earth observation, limiting the usability and reliability of critical remote sensing applications such as fire disaster response, urban heat island monitoring, and snow and ice cover mapping. Therefore, the ability to detect clouds 24/7 is of paramount importance. While visible and near-infrared bands are effective for daytime cloud detection, their dependence on solar illumination makes them unsuitable for nighttime monitoring. In contrast, thermal infrared (TIR) imagery plays a crucial role in detecting clouds at night, when sunlight is absent. Due to their generally lower temperatures, clouds emit distinct thermal signatures that are detectable in TIR bands. Despite this, accurate nighttime cloud detection remains challenging due to limited spectral information and the typically lower spatial resolution of TIR imagery. To address these challenges, we present TIRAuxCloud, a multi-modal dataset centered around thermal spectral data to facilitate cloud segmentation under both daytime and nighttime conditions. The dataset comprises a unique combination of multispectral data (TIR, optical, and near-infrared bands) from Landsat and VIIRS, aligned with auxiliary information layers. Elevation, land cover, meteorological variables, and cloud-free reference images are included to help reduce surface-cloud ambiguity and cloud formation uncertainty. To overcome the scarcity of manual cloud labels, we include a large set of samples with automated cloud masks and a smaller manually annotated subset to further evaluate and improve models. Comprehensive benchmarks are presented to establish performance baselines through supervised and transfer learning, demonstrating the dataset's value in advancing the development of innovative methods for day and night time cloud detection.

</details>


### [77] [Protein Graph Neural Networks for Heterogeneous Cryo-EM Reconstruction](https://arxiv.org/abs/2602.21915)
*Jonathan Krook,Axel Janson,Joakim andén,Melanie Weber,Ozan Öktem*

Main category: cs.CV

TL;DR: A geometry-aware GNN method for heterogeneous cryo-EM reconstruction that predicts atomic backbone conformations using graph neural networks and differentiable cryo-EM forward modeling.


<details>
  <summary>Details</summary>
Motivation: To incorporate protein-structure priors into cryo-EM reconstruction and better handle structural heterogeneity by leveraging geometric information in protein backbones.

Method: Represents protein backbone as a graph, uses GNN autodecoder with per-image latent variables to predict 3D displacements from template conformation, combines differentiable cryo-EM forward model with geometric regularization, and uses ellipsoidal support lifting for pose estimation.

Result: On synthetic datasets from molecular dynamics trajectories, the GNN achieves higher accuracy compared to a comparable-size MLP, demonstrating benefits of geometry-informed inductive bias.

Conclusion: Graph neural networks with geometric priors outperform traditional MLPs for heterogeneous cryo-EM reconstruction, showing the importance of incorporating structural knowledge into deep learning approaches for structural biology.

Abstract: We present a geometry-aware method for heterogeneous single-particle cryogenic electron microscopy (cryo-EM) reconstruction that predicts atomic backbone conformations. To incorporate protein-structure priors, we represent the backbone as a graph and use a graph neural network (GNN) autodecoder that maps per-image latent variables to 3D displacements of a template conformation. The objective combines a data-discrepancy term based on a differentiable cryo-EM forward model with geometric regularization, and it supports unknown orientations via ellipsoidal support lifting (ESL) pose estimation. On synthetic datasets derived from molecular dynamics trajectories, the proposed GNN achieves higher accuracy compared to a multilayer perceptron (MLP) of comparable size, highlighting the benefits of a geometry-informed inductive bias.

</details>


### [78] [Scan Clusters, Not Pixels: A Cluster-Centric Paradigm for Efficient Ultra-high-definition Image Restoration](https://arxiv.org/abs/2602.21917)
*Chen Wu,Ling Wang,Zhuoran Zheng,Yuning Cui,Zhixiong Yang,Xiangyu Chen,Yue Zhang,Weidong Jiang,Jingyuan Xia*

Main category: cs.CV

TL;DR: C²SSM introduces a cluster-serial scanning approach for UHD image restoration, replacing pixel-serial processing with semantic centroids to dramatically reduce computational complexity while achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Existing UHD image restoration models suffer from unsustainable computational demands due to pixel-wise operations, and while SSMs like Mamba offer linear complexity, their pixel-serial scanning remains a bottleneck for millions of pixels in UHD content.

Method: C²SSM shifts from pixel-serial to cluster-serial scanning by distilling UHD image features into sparse semantic centroids via a neural-parameterized mixture model. It uses a dual-path process: scanning cluster centers for global reasoning, then diffusing context back to pixels through similarity distribution, with a lightweight modulator preserving fine details.

Result: Achieves decisive efficiency improvements, slashing computational costs while establishing new state-of-the-art results across five UHD restoration tasks.

Conclusion: C²SSM charts a new course for efficient large-scale vision by demonstrating that scanning clusters, not pixels, is the key to scalable UHD image restoration.

Abstract: Ultra-High-Definition (UHD) image restoration is trapped in a scalability crisis: existing models, bound to pixel-wise operations, demand unsustainable computation. While state space models (SSMs) like Mamba promise linear complexity, their pixel-serial scanning remains a fundamental bottleneck for the millions of pixels in UHD content. We ask: must we process every pixel to understand the image? This paper introduces C$^2$SSM, a visual state space model that breaks this taboo by shifting from pixel-serial to cluster-serial scanning. Our core discovery is that the rich feature distribution of a UHD image can be distilled into a sparse set of semantic centroids via a neural-parameterized mixture model. C$^2$SSM leverages this to reformulate global modeling into a novel dual-path process: it scans and reasons over a handful of cluster centers, then diffuses the global context back to all pixels through a principled similarity distribution, all while a lightweight modulator preserves fine details. This cluster-centric paradigm achieves a decisive leap in efficiency, slashing computational costs while establishing new state-of-the-art results across five UHD restoration tasks. More than a solution, C$^2$SSM charts a new course for efficient large-scale vision: scan clusters, not pixels.

</details>


### [79] [Geometry-as-context: Modulating Explicit 3D in Scene-consistent Video Generation to Geometry Context](https://arxiv.org/abs/2602.21929)
*JiaKui Hu,Jialun Liu,Liying Yang,Xinliang Zhang,Kaiwen Li,Shuang Zeng,Yuanwei Li,Haibin Huang,Chi Zhang,Yanye Lu*

Main category: cs.CV

TL;DR: A novel "geometry-as-context" approach for scene-consistent video generation that uses an autoregressive camera-controlled model to estimate geometry and restore novel views, overcoming limitations of previous methods.


<details>
  <summary>Details</summary>
Motivation: Previous methods for scene-consistent video generation rely on external memory or iterative 3D reconstruction with inpainting, which accumulate errors due to incorrect intermediary outputs, non-differentiable processes, and separate models.

Method: Introduces "geometry-as-context" using an autoregressive camera-controlled video generation model that iteratively: (1) estimates geometry for 3D reconstruction, and (2) simulates/restores novel view images. Uses camera gated attention module for better camera pose utilization and random geometry context dropping during training for RGB-only inference.

Result: The method shows superiority over previous approaches in maintaining scene consistency and camera control when tested on scene video generation with one-direction and forth-and-back trajectories.

Conclusion: The proposed "geometry-as-context" framework effectively addresses error accumulation issues in scene-consistent video generation by integrating geometry estimation and novel view restoration within a unified model, achieving better scene consistency and camera control.

Abstract: Scene-consistent video generation aims to create videos that explore 3D scenes based on a camera trajectory. Previous methods rely on video generation models with external memory for consistency, or iterative 3D reconstruction and inpainting, which accumulate errors during inference due to incorrect intermediary outputs, non-differentiable processes, and separate models. To overcome these limitations, we introduce ``geometry-as-context". It iteratively completes the following steps using an autoregressive camera-controlled video generation model: (1) estimates the geometry of the current view necessary for 3D reconstruction, and (2) simulates and restores novel view images rendered by the 3D scene. Under this multi-task framework, we develop the camera gated attention module to enhance the model's capability to effectively leverage camera poses. During the training phase, text contexts are utilized to ascertain whether geometric or RGB images should be generated. To ensure that the model can generate RGB-only outputs during inference, the geometry context is randomly dropped from the interleaved text-image-geometry training sequence. The method has been tested on scene video generation with one-direction and forth-and-back trajectories. The results show its superiority over previous approaches in maintaining scene consistency and camera control.

</details>


### [80] [Directed Ordinal Diffusion Regularization for Progression-Aware Diabetic Retinopathy Grading](https://arxiv.org/abs/2602.21942)
*Huangwei Chen,Junhao Jia,Ruocheng Li,Cunyuan Yang,Wu Li,Xiaotao Pang,Yifei Chen,Haishuai Wang,Jiajun Bu,Lei Wu*

Main category: cs.CV

TL;DR: D-ODR introduces directed ordinal diffusion regularization to model diabetic retinopathy progression as a directed flow, preventing biologically implausible reverse transitions in feature representations.


<details>
  <summary>Details</summary>
Motivation: Existing ordinal regression methods treat DR severity as static symmetric ranks, ignoring the unidirectional nature of disease progression, which can lead to biologically implausible feature representations that allow reverse transitions between stages.

Method: Proposes Directed Ordinal Diffusion Regularization (D-ODR) that constructs a progression-constrained directed graph to enforce forward disease evolution, then performs multi-scale diffusion on this directed structure to penalize score inversions along valid progression paths.

Result: Extensive experiments show D-ODR achieves superior grading performance compared to state-of-the-art ordinal regression and DR-specific grading methods, providing more clinically reliable severity assessment.

Conclusion: D-ODR successfully models DR progression as a directed flow, aligning feature representations with natural disease trajectory and preventing biologically inconsistent reverse transitions, offering improved clinical reliability.

Abstract: Diabetic Retinopathy (DR) progresses as a continuous and irreversible deterioration of the retina, following a well-defined clinical trajectory from mild to severe stages. However, most existing ordinal regression approaches model DR severity as a set of static, symmetric ranks, capturing relative order while ignoring the inherent unidirectional nature of disease progression. As a result, the learned feature representations may violate biological plausibility, allowing implausible proximity between non-consecutive stages or even reverse transitions. To bridge this gap, we propose Directed Ordinal Diffusion Regularization (D-ODR), which explicitly models the feature space as a directed flow by constructing a progression-constrained directed graph that strictly enforces forward disease evolution. By performing multi-scale diffusion on this directed structure, D-ODR imposes penalties on score inversions along valid progression paths, thereby effectively preventing the model from learning biologically inconsistent reverse transitions. This mechanism aligns the feature representation with the natural trajectory of DR worsening. Extensive experiments demonstrate that D-ODR yields superior grading performance compared to state-of-the-art ordinal regression and DR-specific grading methods, offering a more clinically reliable assessment of disease severity. Our code is available on https://github.com/HovChen/D-ODR.

</details>


### [81] [Mobile-Ready Automated Triage of Diabetic Retinopathy Using Digital Fundus Images](https://arxiv.org/abs/2602.21943)
*Aadi Joshi,Manav S. Sharma,Vijay Uttam Rathod,Ashlesha Sawant,Prajakta Musale,Asmita B. Kalamkar*

Main category: cs.CV

TL;DR: Lightweight MobileNetV3 with CORAL head achieves 0.9019 QWK score for automated diabetic retinopathy severity assessment from fundus images, optimized for mobile deployment.


<details>
  <summary>Details</summary>
Motivation: Manual DR diagnosis is time-consuming, error-prone, and causes screening delays. There's a need for automated, efficient solutions suitable for resource-constrained environments.

Method: MobileNetV3 architecture with Consistent Rank Logits (CORAL) head to model ordered disease progression. Preprocessing includes circular cropping and illumination normalization. Trained on combined APTOS 2019 and IDRiD datasets with 3-fold cross-validation and ablation studies.

Result: Achieves Quadratic Weighted Kappa (QWK) score of 0.9019 and accuracy of 80.03%. Model calibration reduces overconfidence, and optimization enables mobile deployment.

Conclusion: The lightweight framework provides a scalable, practical tool for early-stage diabetic retinopathy screening in resource-constrained environments.

Abstract: Diabetic Retinopathy (DR) is a major cause of vision impairment worldwide. However, manual diagnosis is often time-consuming and prone to errors, leading to delays in screening. This paper presents a lightweight automated deep learning framework for efficient assessment of DR severity from digital fundus images. We use a MobileNetV3 architecture with a Consistent Rank Logits (CORAL) head to model the ordered progression of disease while maintaining computational efficiency for resource-constrained environments. The model is trained and validated on a combined dataset of APTOS 2019 and IDRiD images using a preprocessing pipeline including circular cropping and illumination normalization. Extensive experiments including 3-fold cross-validation and ablation studies demonstrate strong performance. The model achieves a Quadratic Weighted Kappa (QWK) score of 0.9019 and an accuracy of 80.03 percent. Additionally, we address real-world deployment challenges through model calibration to reduce overconfidence and optimization for mobile devices. The proposed system provides a scalable and practical tool for early-stage diabetic retinopathy screening.

</details>


### [82] [Learning to Fuse and Reconstruct Multi-View Graphs for Diabetic Retinopathy Grading](https://arxiv.org/abs/2602.21944)
*Haoran Li,Yuxin Lin,Huan Wang,Xiaoling Luo,Qi Zhu,Jiahua Shi,Huaming Chen,Bo Du,Johan Barthelemy,Zongyan Xue,Jun Shen,Yong Xu*

Main category: cs.CV

TL;DR: MVGFDR: A multi-view graph fusion framework for diabetic retinopathy grading that disentangles shared and view-specific features using frequency-domain graph initialization and cross-view reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing multi-view DR grading methods overlook inter-view correlations and fail to exploit inherent consistency across views from the same patient, limiting their ability to fully leverage multi-view information.

Method: Proposes MVGFDR with three key components: 1) Multi-view Graph Initialization using residual-guided connections and DCT coefficients as frequency-domain anchors, 2) Multi-view Graph Fusion integrating selective nodes based on frequency-domain relevance, and 3) Masked Cross-view Reconstruction for view-invariant representation learning.

Result: Extensive experiments on MFIDDR (largest multi-view fundus image dataset) demonstrate superiority over existing state-of-the-art approaches in diabetic retinopathy grading.

Conclusion: MVGFDR effectively captures both shared and view-specific information across multi-view fundus images, providing a more comprehensive approach for DR grading by explicitly modeling inter-view correlations.

Abstract: Diabetic retinopathy (DR) is one of the leading causes of vision loss worldwide, making early and accurate DR grading critical for timely intervention. Recent clinical practices leverage multi-view fundus images for DR detection with a wide coverage of the field of view (FOV), motivating deep learning methods to explore the potential of multi-view learning for DR grading. However, existing methods often overlook the inter-view correlations when fusing multi-view fundus images, failing to fully exploit the inherent consistency across views originating from the same patient. In this work, we present MVGFDR, an end-to-end Multi-View Graph Fusion framework for DR grading. Different from existing methods that directly fuse visual features from multiple views, MVGFDR is equipped with a novel Multi-View Graph Fusion (MVGF) module to explicitly disentangle the shared and view-specific visual features. Specifically, MVGF comprises three key components: (1) Multi-view Graph Initialization, which constructs visual graphs via residual-guided connections and employs Discrete Cosine Transform (DCT) coefficients as frequency-domain anchors; (2) Multi-view Graph Fusion, which integrates selective nodes across multi-view graphs based on frequency-domain relevance to capture complementary view-specific information; and (3) Masked Cross-view Reconstruction, which leverages masked reconstruction of shared information across views to facilitate view-invariant representation learning. Extensive experimental results on MFIDDR, by far the largest multi-view fundus image dataset, demonstrate the superiority of our proposed approach over existing state-of-the-art approaches in diabetic retinopathy grading.

</details>


### [83] [MindDriver: Introducing Progressive Multimodal Reasoning for Autonomous Driving](https://arxiv.org/abs/2602.21952)
*Lingjun Zhang,Yujian Yuan,Changjie Wu,Xinyuan Chang,Xin Cai,Shuang Zeng,Linzhe Shi,Sijin Wang,Hang Zhang,Mu Xu*

Main category: cs.CV

TL;DR: MindDriver is a progressive multimodal reasoning framework that enables Vision-Language Models to imitate human-like thinking for autonomous driving by bridging semantic understanding, imagination, and trajectory planning.


<details>
  <summary>Details</summary>
Motivation: Current Chain-of-Thought approaches for VLMs in autonomous driving have limitations: textual CoT creates a gap between text semantics and physical trajectory space, while image-based CoT lacks planning-oriented guidance for accurate scene evolution.

Method: MindDriver uses a progressive multimodal reasoning framework with three stages: semantic understanding, semantic-to-physical space imagination, and physical-space trajectory planning. It employs a feedback-guided automatic data annotation pipeline for aligned multimodal reasoning training data, and progressive reinforcement fine-tuning with high-level reward-based learning.

Result: MindDriver demonstrates superior performance in both nuScenes open-loop and Bench2Drive closed-loop evaluations, showing improved autonomous driving capabilities.

Conclusion: MindDriver successfully addresses the limitations of existing CoT approaches by enabling VLMs to perform progressive multimodal reasoning that bridges semantic understanding with physical trajectory planning for autonomous driving.

Abstract: Vision-Language Models (VLM) exhibit strong reasoning capabilities, showing promise for end-to-end autonomous driving systems. Chain-of-Thought (CoT), as VLM's widely used reasoning strategy, is facing critical challenges. Existing textual CoT has a large gap between text semantic space and trajectory physical space. Although the recent approach utilizes future image to replace text as CoT process, it lacks clear planning-oriented objective guidance to generate images with accurate scene evolution. To address these, we innovatively propose MindDriver, a progressive multimodal reasoning framework that enables VLM to imitate human-like progressive thinking for autonomous driving. MindDriver presents semantic understanding, semantic-to-physical space imagination, and physical-space trajectory planning. To achieve aligned reasoning processes in MindDriver, we develop a feedback-guided automatic data annotation pipeline to generate aligned multimodal reasoning training data. Furthermore, we develop a progressive reinforcement fine-tuning method to optimize the alignment through progressive high- level reward-based learning. MindDriver demonstrates superior performance in both nuScences open-loop and Bench2Drive closed-loop evaluation. Codes are available at https://github.com/hotdogcheesewhite/MindDriver.

</details>


### [84] [Global-Local Dual Perception for MLLMs in High-Resolution Text-Rich Image Translation](https://arxiv.org/abs/2602.21956)
*Junxin Lu,Tengfei Song,Zhanglin Wu,Pengfei Li,Xiaowei Liang,Hui Yang,Kun Chen,Ning Xie,Yunfei Lu,Jing Zhao,Shiliang Sun,Daimeng Wei*

Main category: cs.CV

TL;DR: GLoTran is a global-local dual visual perception framework for text image machine translation that addresses challenges in high-resolution text-rich images by integrating low-resolution global context with multi-scale text slices.


<details>
  <summary>Details</summary>
Motivation: Existing TIMT methods struggle with high-resolution text-rich images due to cluttered layouts, diverse fonts, and non-textual distractions, leading to text omission, semantic drift, and contextual inconsistency.

Method: GLoTran integrates low-resolution global images with multi-scale region-level text image slices using instruction-guided alignment strategy, conditioning MLLMs to maintain scene-level context while capturing fine-grained textual details. Also created GLoD dataset with 510K high-resolution global-local image-text pairs.

Result: Extensive experiments show GLoTran substantially improves translation completeness and accuracy over state-of-the-art MLLMs.

Conclusion: GLoTran offers a new paradigm for fine-grained TIMT under high-resolution and text-rich conditions by addressing visual perception challenges through global-local dual perception framework.

Abstract: Text Image Machine Translation (TIMT) aims to translate text embedded in images in the source-language into target-language, requiring synergistic integration of visual perception and linguistic understanding. Existing TIMT methods, whether cascaded pipelines or end-to-end multimodal large language models (MLLMs),struggle with high-resolution text-rich images due to cluttered layouts, diverse fonts, and non-textual distractions, resulting in text omission, semantic drift, and contextual inconsistency. To address these challenges, we propose GLoTran, a global-local dual visual perception framework for MLLM-based TIMT. GLoTran integrates a low-resolution global image with multi-scale region-level text image slices under an instruction-guided alignment strategy, conditioning MLLMs to maintain scene-level contextual consistency while faithfully capturing fine-grained textual details. Moreover, to realize this dual-perception paradigm, we construct GLoD, a large-scale text-rich TIMT dataset comprising 510K high-resolution global-local image-text pairs covering diverse real-world scenarios. Extensive experiments demonstrate that GLoTran substantially improves translation completeness and accuracy over state-of-the-art MLLMs, offering a new paradigm for fine-grained TIMT under high-resolution and text-rich conditions.

</details>


### [85] [Global-Aware Edge Prioritization for Pose Graph Initialization](https://arxiv.org/abs/2602.21963)
*Tong Wei,Giorgos Tolias,Jiri Matas,Daniel Barath*

Main category: cs.CV

TL;DR: A new method for pose graph initialization in SfM that uses edge prioritization with GNN-based reliability prediction and connectivity-aware construction to create more reliable and compact pose graphs.


<details>
  <summary>Details</summary>
Motivation: Current SfM pipelines rely on image retrieval for pose graph initialization, treating image pairs independently and ignoring global consistency, which limits reconstruction accuracy especially in sparse and ambiguous scenes.

Method: Three components: (1) GNN trained with SfM-derived supervision to predict globally consistent edge reliability, (2) multi-minimal-spanning-tree-based pose graph construction guided by edge ranks, and (3) connectivity-aware score modulation to reinforce weak regions and reduce graph diameter.

Result: The method produces more reliable and compact pose graphs, improving reconstruction accuracy in sparse and high-speed settings, and outperforms state-of-the-art retrieval methods on ambiguous scenes.

Conclusion: Globally informed initialization through edge prioritization addresses limitations of independent pair-wise approaches, yielding better SfM performance with available code and trained models.

Abstract: The pose graph is a core component of Structure-from-Motion (SfM), where images act as nodes and edges encode relative poses. Since geometric verification is expensive, SfM pipelines restrict the pose graph to a sparse set of candidate edges, making initialization critical. Existing methods rely on image retrieval to connect each image to its $k$ nearest neighbors, treating pairs independently and ignoring global consistency. We address this limitation through the concept of edge prioritization, ranking candidate edges by their utility for SfM. Our approach has three components: (1) a GNN trained with SfM-derived supervision to predict globally consistent edge reliability; (2) multi-minimal-spanning-tree-based pose graph construction guided by these ranks; and (3) connectivity-aware score modulation that reinforces weak regions and reduces graph diameter. This globally informed initialization yields more reliable and compact pose graphs, improving reconstruction accuracy in sparse and high-speed settings and outperforming SOTA retrieval methods on ambiguous scenes. The ode and trained models are available at https://github.com/weitong8591/global_edge_prior.

</details>


### [86] [When LoRA Betrays: Backdooring Text-to-Image Models by Masquerading as Benign Adapters](https://arxiv.org/abs/2602.21977)
*Liangwei Lyu,Jiaqi Xu,Jianwei Ding,Qiyao Deng*

Main category: cs.CV

TL;DR: MasqLoRA is a backdoor attack framework that uses LoRA modules to stealthily inject malicious behavior into text-to-image diffusion models, achieving 99.8% attack success rate with minimal resources.


<details>
  <summary>Details</summary>
Motivation: While LoRA's modular flexibility enables efficient fine-tuning and sharing of diffusion models, this same flexibility creates a broader attack surface that can be exploited in the AI supply chain.

Method: The attack freezes base model parameters and trains a standalone LoRA module using "trigger word-target image" pairs, embedding a hidden cross-modal mapping that activates only when specific triggers are provided.

Result: MasqLoRA achieves 99.8% attack success rate with minimal resource overhead, demonstrating that LoRA modules can be effectively weaponized for stealthy backdoor attacks.

Conclusion: The work reveals a severe threat in the AI supply chain and underscores the urgent need for dedicated defense mechanisms for the LoRA-centric sharing ecosystem.

Abstract: Low-Rank Adaptation (LoRA) has emerged as a leading technique for efficiently fine-tuning text-to-image diffusion models, and its widespread adoption on open-source platforms has fostered a vibrant culture of model sharing and customization. However, the same modular and plug-and-play flexibility that makes LoRA appealing also introduces a broader attack surface. To highlight this risk, we propose Masquerade-LoRA (MasqLoRA), the first systematic attack framework that leverages an independent LoRA module as the attack vehicle to stealthily inject malicious behavior into text-to-image diffusion models. MasqLoRA operates by freezing the base model parameters and updating only the low-rank adapter weights using a small number of "trigger word-target image" pairs. This enables the attacker to train a standalone backdoor LoRA module that embeds a hidden cross-modal mapping: when the module is loaded and a specific textual trigger is provided, the model produces a predefined visual output; otherwise, it behaves indistinguishably from the benign model, ensuring the stealthiness of the attack. Experimental results demonstrate that MasqLoRA can be trained with minimal resource overhead and achieves a high attack success rate of 99.8%. MasqLoRA reveals a severe and unique threat in the AI supply chain, underscoring the urgent need for dedicated defense mechanisms for the LoRA-centric sharing ecosystem.

</details>


### [87] [PanoEnv: Exploring 3D Spatial Intelligence in Panoramic Environments with Reinforcement Learning](https://arxiv.org/abs/2602.21992)
*Zekai Lin,Xu Zheng*

Main category: cs.CV

TL;DR: PanoEnv is a new VQA benchmark for 360° panoramic images with 14.8K questions and 3D annotations. Current VLMs perform poorly on 3D spatial reasoning (49.34% accuracy). The authors propose a reinforcement learning framework with curriculum training that improves a 7B model to 52.93% accuracy, outperforming larger 32B models.


<details>
  <summary>Details</summary>
Motivation: 360° panoramic images are crucial for VR, autonomous driving, and robotics, but current Vision-Language Models struggle with 3D spatial reasoning on equirectangular projections due to geometric distortion and limited 3D supervision.

Method: 1) Created PanoEnv benchmark with 14.8K questions across 5 categories grounded in accurate 3D annotations. 2) Proposed reinforcement learning post-training framework using Group Relative Policy Optimization with ground-truth-guided reward and geometry-aware strategies. 3) Implemented two-stage curriculum: Stage 1 trains on structured tasks, Stage 2 fine-tunes on mixed open-ended data.

Result: Benchmarked 14 state-of-the-art VLMs showing limited 3D understanding (49.34% overall accuracy, 8.36% on open-ended). Their 7B model achieved new SOTA: 52.93% overall accuracy (+3.59%), 14.83% open-ended accuracy, and top semantic evaluation scores (Q-Score 6.24, P-Score 5.95), surpassing 32B models.

Conclusion: PanoEnv-QA benchmark and the curriculum-based RL framework effectively instill 3D spatial intelligence in VLMs for omnidirectional perception, demonstrating that targeted training can overcome limitations in geometric distortion and 3D reasoning.

Abstract: 360 panoramic images are increasingly used in virtual reality, autonomous driving, and robotics for holistic scene understanding. However, current Vision-Language Models (VLMs) struggle with 3D spatial reasoning on Equirectangular Projection (ERP) images due to geometric distortion and limited 3D supervision. We introduce PanoEnv, a large-scale VQA benchmark built from synthetic 3D environments, containing 14.8K questions across five categories (e.g., relative position, volume comparison) grounded in accurate 3D annotations including depth, segmentation, and bounding boxes. Benchmarking 14 state-of-the-art VLMs reveals limited 3D understanding, achieving only 49.34% overall accuracy and 8.36% on open-ended (OE) questions. To enhance 3D reasoning, we propose a reinforcement learning post-training framework based on Group Relative Policy Optimization (GRPO) with a ground-truth-guided reward that incorporates five geometry-aware strategies such as distance tolerance and spatial consistency. A two-stage curriculum further mitigates catastrophic forgetting: Stage 1 trains on structured tasks (true/false and multiple choice), and Stage 2 fine-tunes on mixed open-ended data to improve generalization. Our 7B model achieves new state-of-the-art performance, improving overall accuracy to 52.93% (+3.59%) and open-ended accuracy to 14.83% while maintaining structured-task performance. It also achieves top semantic evaluation scores (Q-Score 6.24, P-Score 5.95), surpassing 32B models. These results demonstrate that PanoEnv-QA and our curriculum-based RL framework effectively instill 3D spatial intelligence in VLMs for omnidirectional perception.

</details>


### [88] [RobustVisRAG: Causality-Aware Vision-Based Retrieval-Augmented Generation under Visual Degradations](https://arxiv.org/abs/2602.22013)
*I-Hsiang Chen,Yu-Wei Liu,Tse-Yu Wu,Yu-Chien Chiang,Jen-Chien Yang,Wei-Ting Chen*

Main category: cs.CV

TL;DR: RobustVisRAG is a causality-guided dual-path framework that improves vision-based retrieval-augmented generation robustness against visual degradations like blur, noise, and low light, while maintaining efficiency and zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Existing VisRAG models degrade when visual inputs suffer from distortions (blur, noise, low light, shadow) because semantic and degradation factors become entangled in pretrained visual encoders, causing errors in both retrieval and generation stages.

Method: A causality-guided dual-path framework with: 1) non-causal path to capture degradation signals via unidirectional attention, 2) causal path to learn purified semantics guided by degradation signals, 3) Non-Causal Distortion Modeling and Causal Semantic Alignment objectives to separate semantics from degradations.

Result: Improves retrieval by 7.35%, generation by 6.35%, and end-to-end performance by 12.40% on real-world degradations while maintaining comparable accuracy on clean inputs. Introduces Distortion-VisRAG benchmark with synthetic and real-world degraded documents across 7 domains and 17 distortion types.

Conclusion: RobustVisRAG effectively separates semantics from degradations, enabling stable retrieval and generation under challenging visual conditions while preserving efficiency and zero-shot generalization capabilities.

Abstract: Vision-based Retrieval-Augmented Generation (VisRAG) leverages vision-language models (VLMs) to jointly retrieve relevant visual documents and generate grounded answers based on multimodal evidence. However, existing VisRAG models degrade in performance when visual inputs suffer from distortions such as blur, noise, low light, or shadow, where semantic and degradation factors become entangled within pretrained visual encoders, leading to errors in both retrieval and generation stages. To address this limitation, we introduce RobustVisRAG, a causality-guided dual-path framework that improves VisRAG robustness while preserving efficiency and zero-shot generalization. RobustVisRAG uses a non-causal path to capture degradation signals through unidirectional attention and a causal path to learn purified semantics guided by these signals. Together with the proposed Non-Causal Distortion Modeling and Causal Semantic Alignment objectives, the framework enforces a clear separation between semantics and degradations, enabling stable retrieval and generation under challenging visual conditions. To evaluate robustness under realistic conditions, we introduce the Distortion-VisRAG dataset, a large-scale benchmark containing both synthetic and real-world degraded documents across seven domains, with 12 synthetic and 5 real distortion types that comprehensively reflect practical visual degradations. Experimental results show that RobustVisRAG improves retrieval, generation, and end-to-end performance by 7.35%, 6.35%, and 12.40%, respectively, on real-world degradations, while maintaining comparable accuracy on clean inputs.

</details>


### [89] [Olbedo: An Albedo and Shading Aerial Dataset for Large-Scale Outdoor Environments](https://arxiv.org/abs/2602.22025)
*Shuang Song,Debao Huang,Deyan Deng,Haolin Xiong,Yang Tang,Yajie Zhao,Rongjun Qin*

Main category: cs.CV

TL;DR: Olbedo is a large-scale aerial dataset for outdoor albedo-shading decomposition, enabling diffusion-based models to generalize from synthetic indoor data to real outdoor imagery with applications in 3D relighting and urban digital twins.


<details>
  <summary>Details</summary>
Motivation: Intrinsic image decomposition (IID) for outdoor scenes is crucial for relighting, editing, and understanding large-scale environments, but progress has been limited by the lack of real-world datasets with reliable albedo and shading supervision.

Method: Created Olbedo dataset with 5,664 UAV images across diverse conditions, using an inverse-rendering refinement pipeline over multi-view stereo reconstructions and calibrated sky illumination to generate multi-view consistent annotations including albedo/shading maps, depth, normals, and sky domes.

Result: Fine-tuning on Olbedo significantly improves single-view outdoor albedo prediction on the MatrixCity benchmark, enabling state-of-the-art diffusion-based IID models to generalize to real outdoor imagery. Also enables multi-view consistent relighting, material editing, and scene change analysis.

Conclusion: Olbedo addresses the critical data gap for outdoor IID research, enabling generalization of advanced models to real outdoor scenes and supporting applications in urban digital twins and illumination-aware aerial vision. The dataset, models, and evaluation protocol are released to advance the field.

Abstract: Intrinsic image decomposition (IID) of outdoor scenes is crucial for relighting, editing, and understanding large-scale environments, but progress has been limited by the lack of real-world datasets with reliable albedo and shading supervision. We introduce Olbedo, a large-scale aerial dataset for outdoor albedo--shading decomposition in the wild. Olbedo contains 5,664 UAV images captured across four landscape types, multiple years, and diverse illumination conditions. Each view is accompanied by multi-view consistent albedo and shading maps, metric depth, surface normals, sun and sky shading components, camera poses, and, for recent flights, measured HDR sky domes. These annotations are derived from an inverse-rendering refinement pipeline over multi-view stereo reconstructions and calibrated sky illumination, together with per-pixel confidence masks. We demonstrate that Olbedo enables state-of-the-art diffusion-based IID models, originally trained on synthetic indoor data, to generalize to real outdoor imagery: fine-tuning on Olbedo significantly improves single-view outdoor albedo prediction on the MatrixCity benchmark. We further illustrate applications of Olbedo-trained models to multi-view consistent relighting of 3D assets, material editing, and scene change analysis for urban digital twins. We release the dataset, baseline models, and an evaluation protocol to support future research in outdoor intrinsic decomposition and illumination-aware aerial vision.

</details>


### [90] [RT-RMOT: A Dataset and Framework for RGB-Thermal Referring Multi-Object Tracking](https://arxiv.org/abs/2602.22033)
*Yanqiu Yu,Zhifan Jin,Sijia Chen,Tongfei Chu,En Yu,Liman Liu,Wenbing Tao*

Main category: cs.CV

TL;DR: RTrack: A multimodal framework using RGB-Thermal fusion with MLLM for all-day referring multi-object tracking, enhanced by RL optimization strategies.


<details>
  <summary>Details</summary>
Motivation: Existing referring multi-object tracking (RMOT) fails in low-visibility conditions like nighttime and smoke. Need to fuse RGB appearance with thermal modality's illumination robustness for all-day tracking capability.

Method: Propose RTrack framework built on multimodal large language model (MLLM) integrating RGB, thermal, and textual features. Introduce Group Sequence Policy Optimization (GSPO) strategy, Clipped Advantage Scaling (CAS) for RL stability, Structured Output Reward, and Comprehensive Detection Reward for balanced exploration-exploitation.

Result: Created RefRT dataset (first RGB-Thermal RMOT dataset) with 388 language descriptions, 1,250 tracked targets, and 166,147 Language-RGB-Thermal triplets. Extensive experiments demonstrate RTrack's effectiveness on RefRT dataset.

Conclusion: RTrack enables all-day referring multi-object tracking by fusing RGB and thermal modalities, with proposed optimization strategies improving model performance and training stability.

Abstract: Referring Multi-Object Tracking has attracted increasing attention due to its human-friendly interactive characteristics, yet it exhibits limitations in low-visibility conditions, such as nighttime, smoke, and other challenging scenarios. To overcome this limitation, we propose a new RGB-Thermal RMOT task, named RT-RMOT, which aims to fuse RGB appearance features with the illumination robustness of the thermal modality to enable all-day referring multi-object tracking. To promote research on RT-RMOT, we construct the first Referring Multi-Object Tracking dataset under RGB-Thermal modality, named RefRT. It contains 388 language descriptions, 1,250 tracked targets, and 166,147 Language-RGB-Thermal (L-RGB-T) triplets. Furthermore, we propose RTrack, a framework built upon a multimodal large language model (MLLM) that integrates RGB, thermal, and textual features. Since the initial framework still leaves room for improvement, we introduce a Group Sequence Policy Optimization (GSPO) strategy to further exploit the model's potential. To alleviate training instability during RL fine-tuning, we introduce a Clipped Advantage Scaling (CAS) strategy to suppress gradient explosion. In addition, we design Structured Output Reward and Comprehensive Detection Reward to balance exploration and exploitation, thereby improving the completeness and accuracy of target perception. Extensive experiments on the RefRT dataset demonstrate the effectiveness of the proposed RTrack framework.

</details>


### [91] [SPGen: Stochastic scanpath generation for paintings using unsupervised domain adaptation](https://arxiv.org/abs/2602.22049)
*Mohamed Amine Kerkouri,Marouane Tliba,Aladine Chetouani,Alessandro Bruno*

Main category: cs.CV

TL;DR: SPGen is a deep learning model that predicts eye movement scanpaths when viewing paintings, using domain adaptation to bridge the gap between photographs and artworks.


<details>
  <summary>Details</summary>
Motivation: Understanding human visual attention is crucial for cultural heritage preservation, as it helps analyze how people view and appreciate artworks.

Method: Uses a Fully Convolutional Neural Network with differentiable fixation selection and learnable Gaussian priors, plus unsupervised domain adaptation via gradient reversal layer to transfer knowledge from natural scenes to paintings, and random noise sampler for stochasticity.

Result: SPGen outperforms existing methods in predicting scanpaths for paintings, providing an effective tool for gaze behavior analysis.

Conclusion: SPGen offers a powerful tool to analyze gaze behavior and advance the preservation and appreciation of artistic treasures through better understanding of visual attention.

Abstract: Understanding human visual attention is key to preserving cultural heritage We introduce SPGen a novel deep learning model to predict scanpaths the sequence of eye movementswhen viewers observe paintings.
  Our architecture uses a Fully Convolutional Neural Network FCNN with differentiable fixation selection and learnable Gaussian priors to simulate natural viewing biases To address the domain gap between photographs and artworks we employ unsupervised domain adaptation via a gradient reversal layer allowing the model to transfer knowledge from natural scenes to paintings Furthermore a random noise sampler models the inherent stochasticity of eyetracking data.
  Extensive testing shows SPGen outperforms existing methods offering a powerful tool to analyze gaze behavior and advance the preservation and appreciation of artistic treasures.

</details>


### [92] [AutoSew: A Geometric Approach to Stitching Prediction with Graph Neural Networks](https://arxiv.org/abs/2602.22052)
*Pablo Ríos-Navarro,Elena Garces,Jorge Lopez-Moreno*

Main category: cs.CV

TL;DR: AutoSew: A geometry-based approach using Graph Neural Networks and optimal transport to automatically predict stitch correspondences from 2D sewing patterns, achieving 96% F1-score and 73.3% successful assembly rate.


<details>
  <summary>Details</summary>
Motivation: Garment assembly automation is challenging due to lack of standardized annotations and semantic cues. Existing methods rely on panel labels or handcrafted heuristics, limiting applicability to real-world, non-conforming patterns.

Method: Formulates stitch prediction as graph matching task using Graph Neural Network to capture geometric context, with differentiable optimal transport solver to infer stitching relationships including multi-edge connections. Updated GarmentCodeData dataset with realistic multi-edge annotations.

Result: Achieves 96% F1-score and successfully assembles 73.3% of test garments without error, outperforming existing methods while relying solely on geometric input.

Conclusion: Geometry alone can robustly guide stitching prediction, enabling scalable garment assembly without manual input, demonstrating viability of fully automatic, geometry-based approaches.

Abstract: Automating garment assembly from sewing patterns remains a significant challenge due to the lack of standardized annotation protocols and the frequent absence of semantic cues. Existing methods often rely on panel labels or handcrafted heuristics, which limit their applicability to real-world, non-conforming patterns. We present AutoSew, a fully automatic, geometry-based approach for predicting stitch correspondences directly from 2D pattern contours. AutoSew formulates the problem as a graph matching task, leveraging a Graph Neural Network to capture local and global geometric context, and employing a differentiable optimal transport solver to infer stitching relationships-including multi-edge connections. To support this task, we update the GarmentCodeData dataset modifying over 18k patterns with realistic multi-edge annotations, reflecting industrial assembly scenarios. AutoSew achieves 96% F1-score and successfully assembles 73.3% of test garments without error, outperforming existing methods while relying solely on geometric input. Our results demonstrate that geometry alone can robustly guide stitching prediction, enabling scalable garment assembly without manual input.

</details>


### [93] [AdaSpot: Spend Resolution Where It Matters for Precise Event Spotting](https://arxiv.org/abs/2602.22073)
*Artur Xarles,Sergio Escalera,Thomas B. Moeslund,Albert Clapés*

Main category: cs.CV

TL;DR: AdaSpot: Adaptive region-of-interest selection for precise event spotting in videos, using low-resolution global features plus high-resolution processing of only the most informative regions to improve efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for precise event spotting process all frames uniformly, leading to redundant computation on non-informative regions and loss of fine-grained details due to spatial downsampling, which limits both efficiency and accuracy.

Method: Processes low-resolution videos for global features while adaptively selecting the most informative region-of-interest in each frame for high-resolution processing using an unsupervised, task-aware strategy that maintains spatio-temporal consistency across frames.

Result: Achieves state-of-the-art performance on PES benchmarks (+3.96 and +2.26 mAP@0 frames on Tennis and FineDiving) while maintaining strong results under looser metrics, with marginal computational overhead compared to low-resolution-only baselines.

Conclusion: AdaSpot effectively addresses the efficiency-accuracy trade-off in precise event spotting by selectively processing informative regions at high resolution while maintaining computational efficiency, outperforming existing methods that process all frames uniformly.

Abstract: Precise Event Spotting aims to localize fast-paced actions or events in videos with high temporal precision, a key task for applications in sports analytics, robotics, and autonomous systems. Existing methods typically process all frames uniformly, overlooking the inherent spatio-temporal redundancy in video data. This leads to redundant computation on non-informative regions while limiting overall efficiency. To remain tractable, they often spatially downsample inputs, losing fine-grained details crucial for precise localization. To address these limitations, we propose \textbf{AdaSpot}, a simple yet effective framework that processes low-resolution videos to extract global task-relevant features while adaptively selecting the most informative region-of-interest in each frame for high-resolution processing. The selection is performed via an unsupervised, task-aware strategy that maintains spatio-temporal consistency across frames and avoids the training instability of learnable alternatives. This design preserves essential fine-grained visual cues with a marginal computational overhead compared to low-resolution-only baselines, while remaining far more efficient than uniform high-resolution processing. Experiments on standard PES benchmarks demonstrate that \textbf{AdaSpot} achieves state-of-the-art performance under strict evaluation metrics (\eg, $+3.96$ and $+2.26$ mAP$@0$ frames on Tennis and FineDiving), while also maintaining strong results under looser metrics. Code is available at: \href{https://github.com/arturxe2/AdaSpot}{https://github.com/arturxe2/AdaSpot}.

</details>


### [94] [Learning to Drive is a Free Gift: Large-Scale Label-Free Autonomy Pretraining from Unposed In-The-Wild Videos](https://arxiv.org/abs/2602.22091)
*Matthew Strong,Wei-Jer Chang,Quentin Herau,Jiezhi Yang,Yihan Hu,Chensheng Peng,Wei Zhan*

Main category: cs.CV

TL;DR: LFG: A label-free, teacher-guided framework that learns unified pseudo-4D representations from unposed YouTube videos for autonomous driving, outperforming multi-camera and LiDAR baselines with just monocular input.


<details>
  <summary>Details</summary>
Motivation: Ego-centric driving videos are abundant online but lack annotations, making it difficult to learn representations that capture both semantic structure and 3D geometry. While recent feedforward models show promise for scalable perception, self-supervised approaches focus too much on frame-to-frame consistency rather than the temporal context critical for safe driving.

Method: Proposes LFG - a label-free, teacher-guided framework using a feedforward architecture with lightweight autoregressive module. Trained with multi-modal supervisory signals to jointly predict current/future point maps, camera poses, semantic segmentation, and motion masks. Uses multi-modal teachers for sequence-level pseudo-supervision from raw YouTube videos without poses, labels, or LiDAR.

Result: The encoder transfers effectively to downstream autonomous driving planning on NAVSIM benchmark, surpassing multi-camera and LiDAR baselines using only a single monocular camera. Also shows strong performance on semantic, geometric, and motion prediction tasks.

Conclusion: LFG learns a unified pseudo-4D representation from unlabeled videos, positioning it as a compelling video-centric foundation model for autonomous driving with geometry and motion-aware features.

Abstract: Ego-centric driving videos available online provide an abundant source of visual data for autonomous driving, yet their lack of annotations makes it difficult to learn representations that capture both semantic structure and 3D geometry. Recent advances in large feedforward spatial models demonstrate that point maps and ego-motion can be inferred in a single forward pass, suggesting a promising direction for scalable driving perception. We therefore propose a label-free, teacher-guided framework for learning autonomous driving representations directly from unposed videos. Unlike prior self-supervised approaches that focus primarily on frame-to-frame consistency, we posit that safe and reactive driving depends critically on temporal context. To this end, we leverage a feedforward architecture equipped with a lightweight autoregressive module, trained using multi-modal supervisory signals that guide the model to jointly predict current and future point maps, camera poses, semantic segmentation, and motion masks. Multi-modal teachers provide sequence-level pseudo-supervision, enabling LFG to learn a unified pseudo-4D representation from raw YouTube videos without poses, labels, or LiDAR. The resulting encoder not only transfers effectively to downstream autonomous driving planning on the NAVSIM benchmark, surpassing multi-camera and LiDAR baselines with only a single monocular camera, but also yields strong performance when evaluated on a range of semantic, geometric, and qualitative motion prediction tasks. These geometry and motion-aware features position LFG as a compelling video-centric foundation model for autonomous driving.

</details>


### [95] [Overview of the CXR-LT 2026 Challenge: Multi-Center Long-Tailed and Zero Shot Chest X-ray Classification](https://arxiv.org/abs/2602.22092)
*Hexin Dong,Yi Lin,Pengyu Zhou,Fengnian Zhao,Alan Clint Legasto,Mingquan Lin,Hao Chen,Yuzhe Yang,George Shih,Yifan Peng*

Main category: cs.CV

TL;DR: CXR-LT 2026 challenge introduces multi-center dataset for long-tailed/open-world chest X-ray analysis with two tasks: robust multi-label classification on 30 known classes and open-world generalization to 6 unseen rare diseases.


<details>
  <summary>Details</summary>
Motivation: Chest X-ray interpretation faces challenges from long-tailed pathology distributions and open-world clinical environments where rare diseases and novel findings appear. Existing benchmarks use closed-set classes from single institutions, failing to capture real-world prevalence of rare diseases.

Method: Created CXR-LT 2026 challenge with multi-center dataset of 145,000+ images from PadChest and NIH Chest X-ray datasets. Defined two tasks: 1) Robust Multi-Label Classification on 30 known classes, 2) Open-World Generalization to 6 unseen rare disease classes. Evaluated using mAP, AUROC, and F1-score.

Result: Top-performing teams achieved mAP of 0.5854 on Task 1 and 0.4315 on Task 2. Results show large-scale vision-language pre-training significantly mitigates performance drop typically associated with zero-shot diagnosis of rare diseases.

Conclusion: The CXR-LT 2026 benchmark addresses critical gaps in chest X-ray analysis by focusing on long-tailed distributions and open-world generalization. Large-scale vision-language pre-training proves effective for handling rare diseases and zero-shot diagnosis scenarios.

Abstract: Chest X-ray (CXR) interpretation is hindered by the long-tailed distribution of pathologies and the open-world nature of clinical environments. Existing benchmarks often rely on closed-set classes from single institutions, failing to capture the prevalence of rare diseases or the appearance of novel findings. To address this, we present the CXR-LT 2026 challenge. This third iteration of the benchmark introduces a multi-center dataset comprising over 145,000 images from PadChest and NIH Chest X-ray datasets. The challenge defines two core tasks: (1) Robust Multi-Label Classification on 30 known classes and (2) Open-World Generalization to 6 unseen (out-of-distribution) rare disease classes. We report the results of the top-performing teams, evaluating them via mean Average Precision (mAP), AUROC, and F1-score. The winning solutions achieved an mAP of 0.5854 on Task 1 and 0.4315 on Task 2, demonstrating that large-scale vision-language pre-training significantly mitigates the performance drop typically associated with zero-shot diagnosis.

</details>


### [96] [WeatherCity: Urban Scene Reconstruction with Controllable Multi-Weather Transformation](https://arxiv.org/abs/2602.22096)
*Wenhua Wu,Huai Guan,Zhe Liu,Hesheng Wang*

Main category: cs.CV

TL;DR: WeatherCity: A novel framework for 4D urban scene reconstruction and weather editing that achieves flexible controllability, high fidelity, and temporal consistency through weather Gaussian representation and physics-driven simulation.


<details>
  <summary>Details</summary>
Motivation: Existing 4D scene reconstruction methods are limited to replicating observed scenes and lack diverse weather simulation capabilities. Image-level weather editing methods introduce artifacts and offer poor controllability over weather effects, creating a need for better 4D weather editing solutions for autonomous driving applications.

Method: 1) Uses text-guided image editing for flexible weather background editing; 2) Introduces weather Gaussian representation based on shared scene features and weather-specific decoders; 3) Implements content consistency optimization for coherent modeling across weather conditions; 4) Designs physics-driven model for dynamic weather effects using particles and motion patterns.

Result: Achieves flexible controllability, high fidelity, and temporal consistency in 4D reconstruction and weather editing. Enables fine-grained control over weather conditions (light rain, heavy snow) and supports object-level manipulation within scenes, as demonstrated through extensive experiments on multiple datasets.

Conclusion: WeatherCity provides a comprehensive solution for editable high-fidelity 4D scenes with weather simulation capabilities, addressing limitations of existing methods and offering practical applications for autonomous driving training and simulation.

Abstract: Editable high-fidelity 4D scenes are crucial for autonomous driving, as they can be applied to end-to-end training and closed-loop simulation. However, existing reconstruction methods are primarily limited to replicating observed scenes and lack the capability for diverse weather simulation. While image-level weather editing methods tend to introduce scene artifacts and offer poor controllability over the weather effects. To address these limitations, we propose WeatherCity, a novel framework for 4D urban scene reconstruction and weather editing. Specifically, we leverage a text-guided image editing model to achieve flexible editing of image weather backgrounds. To tackle the challenge of multi-weather modeling, we introduce a novel weather Gaussian representation based on shared scene features and dedicated weather-specific decoders. This representation is further enhanced with a content consistency optimization, ensuring coherent modeling across different weather conditions. Additionally, we design a physics-driven model that simulates dynamic weather effects through particles and motion patterns. Extensive experiments on multiple datasets and various scenes demonstrate that WeatherCity achieves flexible controllability, high fidelity, and temporal consistency in 4D reconstruction and weather editing. Our framework not only enables fine-grained control over weather conditions (e.g., light rain and heavy snow) but also supports object-level manipulation within the scene.

</details>


### [97] [Brain3D: Brain Report Automation via Inflated Vision Transformers in 3D](https://arxiv.org/abs/2602.22098)
*Mariano Barone,Francesco Di Serio,Giuseppe Riccio,Antonio Romano,Marco Postiglione,Antonino Ferraro,Vincenzo Moscato*

Main category: cs.CV

TL;DR: Brain3D is a 3D vision-language model for automated radiology report generation from brain tumor MRI, outperforming 2D slice-based approaches by preserving spatial context critical for neuroradiology.


<details>
  <summary>Details</summary>
Motivation: Current medical VLMs use 2D slice approximations that fragment spatial context needed for accurate neuroradiological interpretation, especially for hemispheric laterality, tumor infiltration patterns, and anatomical localization in brain MRI.

Method: Three-stage framework: 1) Inflates pretrained 2D medical encoder to native 3D architecture, 2) Progressive alignment with causal language model through contrastive grounding, supervised projector warmup, and LoRA-based linguistic specialization.

Result: Achieves Clinical Pathology F1 of 0.951 vs 0.413 for strong 2D baseline on 468 subjects (BraTS pathological cases + healthy controls), maintaining perfect specificity on healthy scans.

Conclusion: Brain3D demonstrates superior performance for neuroradiology report generation by preserving 3D spatial context, with staged alignment proving essential for establishing visual-textual correspondence and producing structured clinical reports.

Abstract: Current medical vision-language models (VLMs) process volumetric brain MRI using 2D slice-based approximations, fragmenting the spatial context required for accurate neuroradiological interpretation. We developed \textbf{Brain3D}, a staged vision-language framework for automated radiology report generation from 3D brain tumor MRI. Our approach inflates a pretrained 2D medical encoder into a native 3D architecture and progressively aligns it with a causal language model through three stages: contrastive grounding, supervised projector warmup, and LoRA-based linguistic specialization. Unlike generalist 3D medical VLMs, \textbf{Brain3D} is tailored to neuroradiology, where hemispheric laterality, tumor infiltration patterns, and anatomical localization are critical. Evaluated on 468 subjects (BraTS pathological cases plus healthy controls), our model achieves a Clinical Pathology F1 of 0.951 versus 0.413 for a strong 2D baseline while maintaining perfect specificity on healthy scans. The staged alignment proves essential: contrastive grounding establishes visual-textual correspondence, projector warmup stabilizes conditioning, and LoRA adaptation shifts output from verbose captions to structured clinical reports\footnote{Our code is publicly available for transparency and reproducibility

</details>


### [98] [GeoDiv: Framework For Measuring Geographical Diversity In Text-To-Image Models](https://arxiv.org/abs/2602.22120)
*Abhipsa Basu,Mohana Singh,Shashank Agnihotri,Margret Keuper,R. Venkatesh Babu*

Main category: cs.CV

TL;DR: GeoDiv is a framework using LLMs and VLMs to measure geographical diversity in text-to-image models along socio-economic and visual diversity axes, revealing systematic biases in how different countries are portrayed.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models lack geographical diversity, reinforce stereotypes, and misrepresent regions, but existing diversity metrics are limited to curated datasets or surface-level visual similarity with poor interpretability.

Method: GeoDiv framework leverages large language models and vision-language models to assess geographical diversity along two axes: Socio-Economic Visual Index (SEVI) for economic/condition cues, and Visual Diversity Index (VDI) for variation in primary entities and backgrounds.

Result: Applied to Stable Diffusion and FLUX.1-dev across 10 entities and 16 countries, GeoDiv reveals consistent lack of diversity and identifies specific biased portrayals - countries like India, Nigeria, and Colombia are disproportionately depicted as impoverished and worn.

Conclusion: GeoDiv provides the first systematic, interpretable framework for measuring geographical biases in generative models, highlighting the need for greater geographical nuance and marking a step toward fairer, more inclusive generative systems.

Abstract: Text-to-image (T2I) models are rapidly gaining popularity, yet their outputs often lack geographical diversity, reinforce stereotypes, and misrepresent regions. Given their broad reach, it is critical to rigorously evaluate how these models portray the world. Existing diversity metrics either rely on curated datasets or focus on surface-level visual similarity, limiting interpretability. We introduce GeoDiv, a framework leveraging large language and vision-language models to assess geographical diversity along two complementary axes: the Socio-Economic Visual Index (SEVI), capturing economic and condition-related cues, and the Visual Diversity Index (VDI), measuring variation in primary entities and backgrounds. Applied to images generated by models such as Stable Diffusion and FLUX.1-dev across $10$ entities and $16$ countries, GeoDiv reveals a consistent lack of diversity and identifies fine-grained attributes where models default to biased portrayals. Strikingly, depictions of countries like India, Nigeria, and Colombia are disproportionately impoverished and worn, reflecting underlying socio-economic biases. These results highlight the need for greater geographical nuance in generative models. GeoDiv provides the first systematic, interpretable framework for measuring such biases, marking a step toward fairer and more inclusive generative systems. Project page: https://abhipsabasu.github.io/geodiv

</details>


### [99] [WeaveTime: Stream from Earlier Frames into Emergent Memory in VideoLLMs](https://arxiv.org/abs/2602.22142)
*Yulin Zhang,Cheng Shi,Sibei Yang*

Main category: cs.CV

TL;DR: WeaveTime addresses time-agnosticism in Video-LLMs by teaching temporal order perception and implementing dynamic focus caching for efficient streaming video understanding.


<details>
  <summary>Details</summary>
Motivation: Current Video-LLMs suffer from time-agnosticism, treating videos as unordered evidence rather than causally ordered sequences. This causes two failures in streaming settings: temporal order ambiguity (can't follow chronological order) and past-current focus blindness (can't distinguish present from accumulated history).

Method: WeaveTime introduces: 1) Streaming Order Perception enhancement via lightweight Temporal Reconstruction objective that instills order-aware representations with minimal finetuning, 2) Past-Current Dynamic Focus Cache that performs uncertainty-triggered, coarse-to-fine retrieval, expanding history only when needed.

Result: Plugged into existing Video-LLMs without architectural changes, WeaveTime delivers consistent gains on representative streaming benchmarks, improving accuracy while reducing latency.

Conclusion: WeaveTime establishes a practical path toward time-aware streaming Video-LLMs under strict online, time-causal constraints, addressing core limitations of current approaches.

Abstract: Recent advances in Multimodal Large Language Models have greatly improved visual understanding and reasoning, yet their quadratic attention and offline training protocols make them ill-suited for streaming settings where frames arrive sequentially and future observations are inaccessible. We diagnose a core limitation of current Video-LLMs, namely Time-Agnosticism, in which videos are treated as an unordered bag of evidence rather than a causally ordered sequence, yielding two failures in streams: temporal order ambiguity, in which the model cannot follow or reason over the correct chronological order, and past-current focus blindness where it fails to distinguish present observations from accumulated history. We present WeaveTime, a simple, efficient, and model agnostic framework that first teaches order and then uses order. We introduce a lightweight Temporal Reconstruction objective-our Streaming Order Perception enhancement-that instills order aware representations with minimal finetuning and no specialized streaming data. At inference, a Past-Current Dynamic Focus Cache performs uncertainty triggered, coarse-to-fine retrieval, expanding history only when needed. Plugged into exsiting Video-LLM without architectural changes, WeaveTime delivers consistent gains on representative streaming benchmarks, improving accuracy while reducing latency. These results establish WeaveTime as a practical path toward time aware stream Video-LLMs under strict online, time causal constraints. Code and weights will be made publicly available. Project Page: https://zhangyl4.github.io/publications/weavetime/

</details>


### [100] [MedTri: A Platform for Structured Medical Report Normalization to Enhance Vision-Language Pretraining](https://arxiv.org/abs/2602.22143)
*Yuetan Chu,Xinhua Ma,Xinran Jin,Gongning Luo,Xin Gao*

Main category: cs.CV

TL;DR: MedTri is a framework that normalizes medical reports into structured [Anatomical Entity: Radiologic Description + Diagnosis Category] triplets for vision-language pretraining, improving performance over raw reports and existing methods.


<details>
  <summary>Details</summary>
Motivation: Raw medical reports have stylistic heterogeneity, variable length, and image-irrelevant content, making them suboptimal for vision-language pretraining. Existing text normalization approaches lack systematic examination of their design principles and impact on pretraining quality.

Method: MedTri converts free-text medical reports into structured [Anatomical Entity: Radiologic Description + Diagnosis Category] triplets. This anatomy-grounded normalization preserves essential morphological and spatial information while removing noise and irrelevant content. The framework also supports modular text-level augmentation strategies including knowledge enrichment and anatomy-grounded counterfactual supervision.

Result: Across multiple X-ray and CT datasets, structured anatomy-grounded text normalization consistently improves medical vision-language pretraining quality over raw reports and existing normalization baselines. The framework's augmentation strategies provide complementary gains in robustness and generalization without altering the core normalization process.

Conclusion: Structured text normalization is a critical and generalizable preprocessing component for medical vision-language learning. MedTri provides a deployable normalization platform that enables consistent, image-grounded textual supervision at scale while supporting modular augmentation strategies.

Abstract: Medical vision-language pretraining increasingly relies on medical reports as large-scale supervisory signals; however, raw reports often exhibit substantial stylistic heterogeneity, variable length, and a considerable amount of image-irrelevant content. Although text normalization is frequently adopted as a preprocessing step in prior work, its design principles and empirical impact on vision-language pretraining remain insufficiently and systematically examined. In this study, we present MedTri, a deployable normalization framework for medical vision-language pretraining that converts free-text reports into a unified [Anatomical Entity: Radiologic Description + Diagnosis Category] triplet. This structured, anatomy-grounded normalization preserves essential morphological and spatial information while removing stylistic noise and image-irrelevant content, providing consistent and image-grounded textual supervision at scale. Across multiple datasets spanning both X-ray and computed tomography (CT) modalities, we demonstrate that structured, anatomy-grounded text normalization is an important factor in medical vision-language pretraining quality, yielding consistent improvements over raw reports and existing normalization baselines. In addition, we illustrate how this normalization can easily support modular text-level augmentation strategies, including knowledge enrichment and anatomy-grounded counterfactual supervision, which provide complementary gains in robustness and generalization without altering the core normalization process. Together, our results position structured text normalization as a critical and generalizable preprocessing component for medical vision-language learning, while MedTri provides this normalization platform. Code and data will be released at https://github.com/Arturia-Pendragon-Iris/MedTri.

</details>


### [101] [CoLoGen: Progressive Learning of Concept`-`Localization Duality for Unified Image Generation](https://arxiv.org/abs/2602.22150)
*YuXin Song,Yu Lu,Haoyuan Sun,Huanjin Yao,Fanglong Liu,Yifan Sun,Haocheng Feng,Hang Zhou,Jingdong Wang*

Main category: cs.CV

TL;DR: CoLoGen is a unified diffusion framework that addresses the concept-localization representational conflict in conditional image generation by progressively learning and reconciling these dual representations through a staged curriculum and Progressive Representation Weaving module.


<details>
  <summary>Details</summary>
Motivation: Different conditional image generation tasks require fundamentally different internal representations - some need conceptual understanding for semantic synthesis, while others require localization cues for spatial precision. Forcing these heterogeneous tasks to share a single representation leads to concept-localization representational conflict.

Method: CoLoGen uses a staged curriculum: 1) builds core conceptual and localization abilities, 2) adapts them to diverse visual conditions, 3) refines their synergy for complex instruction-driven tasks. The Progressive Representation Weaving (PRW) module dynamically routes features to specialized experts and stably integrates their outputs across stages.

Result: Experiments on editing, controllable generation, and customized generation show that CoLoGen achieves competitive or superior performance compared to existing methods.

Conclusion: CoLoGen offers a principled representational perspective for unified image generation by addressing the fundamental concept-localization duality through progressive learning and representation weaving.

Abstract: Unified conditional image generation remains difficult because different tasks depend on fundamentally different internal representations. Some require conceptual understanding for semantic synthesis, while others rely on localization cues for spatial precision. Forcing these heterogeneous tasks to share a single representation leads to concept`-`localization representational conflict. To address this issue, we propose CoLoGen, a unified diffusion framework that progressively learns and reconciles this concept`-`localization duality. CoLoGen uses a staged curriculum that first builds core conceptual and localization abilities, then adapts them to diverse visual conditions, and finally refines their synergy for complex instruction`-`driven tasks. Central to this process is the Progressive Representation Weaving (PRW) module, which dynamically routes features to specialized experts and stably integrates their outputs across stages. Experiments on editing, controllable generation, and customized generation show that CoLoGen achieves competitive or superior performance, offering a principled representational perspective for unified image generation.

</details>


### [102] [CASR: A Robust Cyclic Framework for Arbitrary Large-Scale Super-Resolution with Distribution Alignment and Self-Similarity Awareness](https://arxiv.org/abs/2602.22159)
*Wenhao Guo,Zhaoran Zhao,Peng Lu,Sheng Li,Qian Qiao,RuiDe Li*

Main category: cs.CV

TL;DR: CASR: A cyclic SR framework using in-distribution scale transitions for arbitrary-scale super-resolution, addressing cross-scale distribution shift with structural alignment and texture restoration modules.


<details>
  <summary>Details</summary>
Motivation: Arbitrary-Scale SR suffers from cross-scale distribution shift - when inference scale leaves training range, noise, blur, and artifacts accumulate sharply. The paper revisits this challenge from a cross-scale distribution transition perspective.

Method: Proposes CASR, a cyclic SR framework that reformulates ultra-magnification as a sequence of in-distribution scale transitions using a single model. Includes SDAM module for structural distribution alignment via superpixel aggregation, and SARM module for high-frequency texture restoration using autocorrelation and LR self-similarity priors.

Result: Significantly reduces distribution drift, preserves long-range texture consistency, achieves superior generalization even at extreme magnification, all while using only a single model.

Conclusion: CASR provides an efficient solution to arbitrary-scale SR by addressing cross-scale distribution shift through cyclic in-distribution transitions, enabling stable inference at arbitrary scales with a single model.

Abstract: Arbitrary-Scale SR (ASISR) remains fundamentally limited by cross-scale distribution shift: once the inference scale leaves the training range, noise, blur, and artifacts accumulate sharply. We revisit this challenge from a cross-scale distribution transition perspective and propose CASR, a simple yet highly efficient cyclic SR framework that reformulates ultra-magnification as a sequence of in-distribution scale transitions. This design ensures stable inference at arbitrary scales while requiring only a single model. CASR tackles two major bottlenecks: distribution drift across iterations and patch-wise diffusion inconsistencies. The proposed SDAM module aligns structural distributions via superpixel aggregation, preventing error accumulation, while SARM module restores high-frequency textures by enforcing autocorrelation and embedding LR self-similarity priors. Despite using only a single model, our approach significantly reduces distribution drift, preserves long-range texture consistency, and achieves superior generalization even at extreme magnification.

</details>


### [103] [Mixed Magnification Aggregation for Generalizable Region-Level Representations in Computational Pathology](https://arxiv.org/abs/2602.22176)
*Eric Zimmermann,Julian Viret,Michal Zelechowski,James Brian Hall,Neil Tenenholtz,Adam Casson,George Shaikovski,Eugene Vorontsov,Siqi Liu,Kristen A Severson*

Main category: cs.CV

TL;DR: The paper proposes a region-level mixing encoder that fuses multi-resolution tile representations from foundation models to capture histologic features at different magnifications, reducing the number of representations per slide while improving biomarker prediction performance.


<details>
  <summary>Details</summary>
Motivation: Current computational pathology workflows use single-magnification (20×) foundation models, but histologic features require multi-resolution analysis. Single magnification leads to excessive tiles per slide and misses features that need different zoom levels.

Method: Proposes a region-level mixing encoder that jointly fuses image tile representations from a mixed magnification foundation model using masked embedding modeling pretraining. Explores design space for pretraining mixed-magnification region aggregators.

Result: Results show cancer-dependent improvements in predictive performance for biomarker prediction tasks across various cancer types, highlighting the importance of spatial context and understanding.

Conclusion: Multi-resolution approaches using mixed-magnification region aggregators can better capture histologic features and reduce computational burden while improving biomarker prediction accuracy in computational pathology.

Abstract: In recent years, a standard computational pathology workflow has emerged where whole slide images are cropped into tiles, these tiles are processed using a foundation model, and task-specific models are built using the resulting representations. At least 15 different foundation models have been proposed, and the vast majority are trained exclusively with tiles using the 20$\times$ magnification. However, it is well known that certain histologic features can only be discerned with larger context windows and requires a pathologist to zoom in and out when analyzing a whole slide image. Furthermore, creating 224$\times$224 pixel crops at 20$\times$ leads to a large number of tiles per slide, which can be gigapixel in size. To more accurately capture multi-resolution features and investigate the possibility of reducing the number of representations per slide, we propose a region-level mixing encoder. Our approach jointly fuses image tile representations of a mixed magnification foundation model using a masked embedding modeling pretraining step. We explore a design space for pretraining the proposed mixed-magnification region aggregators and evaluate our models on transfer to biomarker prediction tasks representing various cancer types. Results demonstrate cancer dependent improvements in predictive performance, highlighting the importance of spatial context and understanding.

</details>


### [104] [Solaris: Building a Multiplayer Video World Model in Minecraft](https://arxiv.org/abs/2602.22208)
*Georgy Savva,Oscar Michel,Daohan Lu,Suppakit Waiwitlikhit,Timothy Meehan,Dhairya Mishra,Srivats Poddar,Jack Lu,Saining Xie*

Main category: cs.CV

TL;DR: Solaris is a multiplayer video world model that simulates consistent multi-view observations, addressing the limitation of existing single-agent video generation models.


<details>
  <summary>Details</summary>
Motivation: Existing action-conditioned video generation models are limited to single-agent perspectives and fail to capture multi-agent interactions in real-world environments.

Method: Developed a multiplayer data collection system for games like Minecraft, collected 12.64M multiplayer frames, and trained Solaris using a staged pipeline with bidirectional, causal, and Self Forcing training, plus Checkpointed Self Forcing for memory efficiency.

Result: Solaris outperforms existing baselines and enables consistent multi-view observations in multiplayer settings.

Conclusion: The work lays groundwork for new multi-agent world models through open-sourcing the system and models.

Abstract: Existing action-conditioned video generation models (video world models) are limited to single-agent perspectives, failing to capture the multi-agent interactions of real-world environments. We introduce Solaris, a multiplayer video world model that simulates consistent multi-view observations. To enable this, we develop a multiplayer data system designed for robust, continuous, and automated data collection on video games such as Minecraft. Unlike prior platforms built for single-player settings, our system supports coordinated multi-agent interaction and synchronized videos + actions capture. Using this system, we collect 12.64 million multiplayer frames and propose an evaluation framework for multiplayer movement, memory, grounding, building, and view consistency. We train Solaris using a staged pipeline that progressively transitions from single-player to multiplayer modeling, combining bidirectional, causal, and Self Forcing training. In the final stage, we introduce Checkpointed Self Forcing, a memory-efficient Self Forcing variant that enables a longer-horizon teacher. Results show our architecture and training design outperform existing baselines. Through open-sourcing our system and models, we hope to lay the groundwork for a new generation of multi-agent world models.

</details>


### [105] [WHOLE: World-Grounded Hand-Object Lifted from Egocentric Videos](https://arxiv.org/abs/2602.22209)
*Yufei Ye,Jiaman Li,Ryan Rong,C. Karen Liu*

Main category: cs.CV

TL;DR: WHOLE is a method for holistic reconstruction of hand and object motion from egocentric videos using a generative prior over hand-object interactions.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with egocentric manipulation videos due to severe occlusions, object entries/exits, and inconsistent hand-object relations when processing hands and objects separately.

Method: Learn a generative prior over hand-object motion to jointly reason about interactions, then guide the pretrained prior to generate trajectories conforming to video observations.

Result: Substantially outperforms separate hand/object processing approaches, achieving SOTA performance on hand motion estimation, 6D object pose estimation, and interaction reconstruction.

Conclusion: Joint generative reconstruction of hand-object motion is superior to separate processing, enabling more accurate and consistent reconstruction from challenging egocentric videos.

Abstract: Egocentric manipulation videos are highly challenging due to severe occlusions during interactions and frequent object entries and exits from the camera view as the person moves. Current methods typically focus on recovering either hand or object pose in isolation, but both struggle during interactions and fail to handle out-of-sight cases. Moreover, their independent predictions often lead to inconsistent hand-object relations. We introduce WHOLE, a method that holistically reconstructs hand and object motion in world space from egocentric videos given object templates. Our key insight is to learn a generative prior over hand-object motion to jointly reason about their interactions. At test time, the pretrained prior is guided to generate trajectories that conform to the video observations. This joint generative reconstruction substantially outperforms approaches that process hands and objects separately followed by post-processing. WHOLE achieves state-of-the-art performance on hand motion estimation, 6D object pose estimation, and their relative interaction reconstruction. Project website: https://judyye.github.io/whole-www

</details>


### [106] [Neu-PiG: Neural Preconditioned Grids for Fast Dynamic Surface Reconstruction on Long Sequences](https://arxiv.org/abs/2602.22212)
*Julian Kaltheuner,Hannah Dröge,Markus Plack,Patrick Stotko,Reinhard Klein*

Main category: cs.CV

TL;DR: Neu-PiG: Fast deformation optimization for temporally consistent 3D surface reconstruction from point clouds using preconditioned latent-grid encoding, achieving drift-free results 60x faster than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for dynamic 3D surface reconstruction from point clouds either suffer from drift and slow optimization (incremental methods) or require complex category-specific training (learned models). There's a need for fast, accurate, and generalizable deformation optimization that works for long sequences without drift.

Method: Uses a novel preconditioned latent-grid encoding that distributes spatial features based on keyframe surface position and normal direction. Encodes entire deformations across all time steps into a multi-resolution latent grid parameterized by a single reference surface. The latent representation is time-modulated and decoded into per-frame 6-DoF deformations via a lightweight MLP. Employs Sobolev preconditioning during gradient-based training to achieve high-fidelity results without explicit correspondences or priors.

Result: Outperforms state-of-the-art approaches on diverse human and animal datasets, offering superior accuracy and scalability to long sequences. Runs at least 60x faster than existing training-free methods and achieves inference speeds comparable to heavy pretrained models. Produces drift-free surface reconstructions in seconds.

Conclusion: Neu-PiG provides an efficient solution for temporally consistent 3D surface reconstruction from point clouds, combining the speed of learned models with the flexibility of optimization-based approaches, while eliminating drift and category-specific training requirements.

Abstract: Temporally consistent surface reconstruction of dynamic 3D objects from unstructured point cloud data remains challenging, especially for very long sequences. Existing methods either optimize deformations incrementally, risking drift and requiring long runtimes, or rely on complex learned models that demand category-specific training. We present Neu-PiG, a fast deformation optimization method based on a novel preconditioned latent-grid encoding that distributes spatial features parameterized on the position and normal direction of a keyframe surface. Our method encodes entire deformations across all time steps at various spatial scales into a multi-resolution latent grid, parameterized by the position and normal direction of a reference surface from a single keyframe. This latent representation is then augmented for time modulation and decoded into per-frame 6-DoF deformations via a lightweight multilayer perceptron (MLP). To achieve high-fidelity, drift-free surface reconstructions in seconds, we employ Sobolev preconditioning during gradient-based training of the latent space, completely avoiding the need for any explicit correspondences or further priors. Experiments across diverse human and animal datasets demonstrate that Neu-PiG outperforms state-the-art approaches, offering both superior accuracy and scalability to long sequences while running at least 60x faster than existing training-free methods and achieving inference speeds on the same order as heavy pretrained models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [107] [Latent Context Compilation: Distilling Long Context into Compact Portable Memory](https://arxiv.org/abs/2602.21221)
*Zeju Li,Yizhou Zhou,Qiang Xu*

Main category: cs.LG

TL;DR: Latent Context Compilation (LCC) is a framework that compresses long contexts into portable buffer tokens using a disposable LoRA compiler, enabling efficient long-context LLM deployment without synthetic data or model weight modifications.


<details>
  <summary>Details</summary>
Motivation: Current approaches for long-context LLM deployment face issues: amortized compression struggles with out-of-distribution generalization, while Test-Time Training requires expensive synthetic data and creates stateful parameters that complicate concurrent serving.

Method: Uses a disposable LoRA module as a compiler to distill long contexts into compact buffer tokens. Introduces self-aligned optimization with context-agnostic random queries to regularize context reconstruction, forcing compressed tokens to stay within the model's existing instruction-following manifold.

Result: Experiments with Llama-3.1-8B show that LCC preserves fine-grained details and reasoning capabilities where prior methods fail, effectively decoupling memory density from model parameters even at 16x compression ratio.

Conclusion: Latent Context Compilation fundamentally shifts context processing from adaptation to compilation, creating stateless, portable memory artifacts that are plug-and-play compatible with frozen base models, enabling efficient long-context deployment.

Abstract: Efficient long-context LLM deployment is stalled by a dichotomy between amortized compression, which struggles with out-of-distribution generalization, and Test-Time Training, which incurs prohibitive synthetic data costs and requires modifying model weights, creating stateful parameters that complicate concurrent serving. We propose Latent Context Compilation, a framework that fundamentally shifts context processing from adaptation to compilation. By utilizing a disposable LoRA module as a compiler, we distill long contexts into compact buffer tokens -- stateless, portable memory artifacts that are plug-and-play compatible with frozen base models. Crucially, we introduce a self-aligned optimization strategy that eliminates the need for synthetic context-relevant QA pairs. By regularizing context reconstruction task with context-agnostic random queries, we force compressed tokens to reside within the model's existing instruction-following manifold. Experiments with Llama-3.1-8B demonstrate that Latent Context Compilation preserves fine-grained details and reasoning capabilities where prior methods falter, effectively decoupling memory density from model parameters even at a 16x compression ratio.

</details>


### [108] [ACAR: Adaptive Complexity Routing for Multi-Model Ensembles with Auditable Decision Traces](https://arxiv.org/abs/2602.21231)
*Ramchand Kumaresan*

Main category: cs.LG

TL;DR: ACAR is an adaptive routing framework that uses self-consistency variance to route tasks across 1-3 models, achieving 55.6% accuracy while avoiding full ensembling on 54.2% of tasks.


<details>
  <summary>Details</summary>
Motivation: To study multi-model orchestration under auditable conditions, addressing the need for efficient model routing without full ensembling overhead, while documenting practical failures of common assumptions in retrieval augmentation and attribution methods.

Method: ACAR uses self-consistency variance (sigma) computed from N=3 probe samples to route tasks adaptively across single-model, two-model, and three-model execution modes. Implemented on TEAMLLM, a deterministic execution substrate with immutable artifacts and complete decision traces.

Result: Sigma-based routing achieves 55.6% accuracy, exceeding two-model baseline (54.4%) while avoiding full ensembling on 54.2% of tasks. Negative results show: retrieval augmentation reduces accuracy by 3.4pp; agreement-but-wrong failures bound accuracy ~8pp below full ensembling; proxy attribution signals have weak correlation with ground truth.

Conclusion: ACAR provides a practical, model-agnostic routing framework that documents which assumptions fail in practice, offering falsifiable baselines for future research on routing, retrieval, and multi-model attribution while demonstrating the limitations of self-consistency and proxy attribution methods.

Abstract: We present ACAR (Adaptive Complexity and Attribution Routing), a measurement framework for studying multi-model orchestration under auditable conditions. ACAR uses self-consistency variance (sigma) computed from N=3 probe samples to route tasks across single-model, two-model, and three-model execution modes. The system is implemented on top of TEAMLLM, a deterministic execution substrate with immutable artifacts and complete decision traces. We evaluate ACAR on 1,510 tasks spanning four benchmarks: MathArena, Reasoning Gym, LiveCodeBench, and SuperGPQA, using Claude Sonnet 4, GPT-4o, and Gemini 2.0 Flash, producing more than 7,550 auditable runs. Results show that sigma-based routing achieves 55.6 percent accuracy, exceeding the two-model baseline of 54.4 percent while avoiding full ensembling on 54.2 percent of tasks. The routing mechanism is model-agnostic and requires no learned components. We also document negative results. First, retrieval augmentation reduced accuracy by 3.4 percentage points, as median retrieval similarity was only 0.167, demonstrating that experience injection without semantic alignment introduces noise rather than grounding. Second, when models agree on incorrect answers (sigma equals zero), no downstream ensemble can recover; this agreement-but-wrong failure mode is intrinsic to self-consistency and bounds achievable accuracy at approximately eight percentage points below full ensembling. Third, attribution estimates based on proxy signals such as response similarity and entropy showed weak correlation with ground-truth leave-one-out values, indicating that practical attribution requires explicit counterfactual computation. This work documents which assumptions fail in practice and provides falsifiable baselines for future research on routing, retrieval, and multi-model attribution.

</details>


### [109] [Urban Vibrancy Embedding and Application on Traffic Prediction](https://arxiv.org/abs/2602.21232)
*Sumin Han,Jisun An,Dongman Lee*

Main category: cs.LG

TL;DR: This paper proposes using VAE to create Urban Vibrancy embeddings from floating population data, then using LSTM to predict future embeddings for improved traffic forecasting.


<details>
  <summary>Details</summary>
Motivation: Urban vibrancy reflects dynamic human activity but isn't well integrated into traffic prediction models. Current models lack the ability to incorporate real-time floating population data that captures urban activity patterns.

Method: 1. Use VAE to compress real-time floating population data into Urban Vibrancy embeddings. 2. Use LSTM networks to predict future embeddings. 3. Integrate these embeddings into sequence-to-sequence framework for traffic forecasting. 4. Use PCA to interpret the learned embeddings.

Result: The approach improves accuracy and responsiveness in traffic prediction models (RNN, DCRNN, GTS, GMAN). PCA reveals embeddings capture meaningful temporal patterns like weekday/weekend distinctions and seasonal variations.

Conclusion: Urban Vibrancy embeddings enhance traffic prediction models and provide nuanced analysis of urban mobility, demonstrating potential for advancing urban traffic forecasting.

Abstract: Urban vibrancy reflects the dynamic human activity within urban spaces and is often measured using mobile data that captures floating population trends. This study proposes a novel approach to derive Urban Vibrancy embeddings from real-time floating population data to enhance traffic prediction models. Specifically, we utilize variational autoencoders (VAE) to compress this data into actionable embeddings, which are then integrated with long short-term memory (LSTM) networks to predict future embeddings. These are subsequently applied in a sequence-to-sequence framework for traffic forecasting. Our contributions are threefold: (1) We use principal component analysis (PCA) to interpret the embeddings, revealing temporal patterns such as weekday versus weekend distinctions and seasonal patterns; (2) We propose a method that combines VAE and LSTM, enabling forecasting dynamic urban knowledge embedding; and (3) Our approach improves accuracy and responsiveness in traffic prediction models, including RNN, DCRNN, GTS, and GMAN. This study demonstrates the potential of Urban Vibrancy embeddings to advance traffic prediction and offer a more nuanced analysis of urban mobility.

</details>


### [110] [AngelSlim: A more accessible, comprehensive, and efficient toolkit for large model compression](https://arxiv.org/abs/2602.21233)
*Rui Cen,QiangQiang Hu,Hong Huang,Hong Liu,Song Liu,Xin Luo,Lin Niu,Yifan Tan,Decheng Wu,Linchuan Xie,Rubing Yang,Guanghua Yu,Jianchen Zhu*

Main category: cs.LG

TL;DR: AngelSlim is a comprehensive toolkit for large model compression that integrates quantization, speculative decoding, token pruning, and distillation into a unified pipeline for industrial deployment.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of deploying large models in industrial settings by providing a comprehensive compression toolkit that streamlines the transition from model compression to deployment.

Method: Unified pipeline integrating multiple compression techniques: FP8/INT8 PTQ quantization, 2-bit ultra-low-bit models, training-aligned speculative decoding for multimodal architectures, training-free sparse attention for long-context scenarios, and specialized pruning strategies for multimodal models (IDPruner for vision tokens, Samp for audio tokens).

Result: Developed HY-1.8B-int2 as first industrially viable 2-bit large model; achieved 1.8x-2.0x throughput gains with speculative decoding; reduced Time-to-First-Token in long-context scenarios; enabled algorithm-focused research and tool-assisted deployment.

Conclusion: AngelSlim provides a comprehensive solution for large model compression that bridges research and industrial deployment through integrated low-level implementations and versatile compression strategies.

Abstract: This technical report introduces AngelSlim, a comprehensive and versatile toolkit for large model compression developed by the Tencent Hunyuan team. By consolidating cutting-edge algorithms, including quantization, speculative decoding, token pruning, and distillation. AngelSlim provides a unified pipeline that streamlines the transition from model compression to industrial-scale deployment. To facilitate efficient acceleration, we integrate state-of-the-art FP8 and INT8 Post-Training Quantization (PTQ) algorithms alongside pioneering research in ultra-low-bit regimes, featuring HY-1.8B-int2 as the first industrially viable 2-bit large model. Beyond quantization, we propose a training-aligned speculative decoding framework compatible with multimodal architectures and modern inference engines, achieving 1.8x to 2.0x throughput gains without compromising output correctness. Furthermore, we develop a training-free sparse attention framework that reduces Time-to-First-Token (TTFT) in long-context scenarios by decoupling sparse kernels from model architectures through a hybrid of static patterns and dynamic token selection. For multimodal models, AngelSlim incorporates specialized pruning strategies, namely IDPruner for optimizing vision tokens via Maximal Marginal Relevance and Samp for adaptive audio token merging and pruning. By integrating these compression strategies from low-level implementations, AngelSlim enables algorithm-focused research and tool-assisted deployment.

</details>


### [111] [Group Orthogonalized Policy Optimization:Group Policy Optimization as Orthogonal Projection in Hilbert Space](https://arxiv.org/abs/2602.21269)
*Wang Zixian*

Main category: cs.LG

TL;DR: GOPO is a new LLM alignment algorithm using Hilbert space geometry instead of probability simplex optimization, enabling exact sparsity and stable gradients without heuristic clipping.


<details>
  <summary>Details</summary>
Motivation: Traditional alignment methods optimize on probability simplex with exponential KL divergence curvature, leading to unstable gradients, saturation issues, and inability to naturally exclude catastrophic actions.

Method: Lifts alignment to Hilbert space L2(pi_k) where simplex constraint becomes linear orthogonality condition; uses Hilbert projection theorem to maximize work-dissipation functional; enforces boundary conditions for exact sparsity; projects to finite empirical subspace via group sampling.

Result: Achieves competitive generalization on mathematical reasoning benchmarks with stable gradient dynamics, entropy preservation, and avoids plateauing issues of clipping-based methods.

Conclusion: GOPO provides a principled geometric approach to LLM alignment with constant Hessian curvature, non-saturating gradients, and intrinsic dead-zone mechanism, offering advantages over traditional clipping-based methods.

Abstract: We present Group Orthogonalized Policy Optimization (GOPO), a new alignment algorithm for large language models derived from the geometry of Hilbert function spaces. Instead of optimizing on the probability simplex and inheriting the exponential curvature of Kullback-Leibler divergence, GOPO lifts alignment into the Hilbert space L2(pi_k) of square-integrable functions with respect to the reference policy. Within this space, the simplex constraint reduces to a linear orthogonality condition <v, 1> = 0, defining a codimension-one subspace H0. Minimizing distance to an unconstrained target u_star yields the work-dissipation functional J(v) = <g, v> - (mu / 2) ||v||^2, whose maximizer follows directly from the Hilbert projection theorem. Enforcing the boundary v >= -1 produces a bounded Hilbert projection that induces exact sparsity, assigning zero probability to catastrophically poor actions through a closed-form threshold. To connect this functional theory with practice, GOPO projects from infinite-dimensional L2(pi_k) to a finite empirical subspace induced by group sampling. Because group-normalized advantages sum to zero, the Lagrange multiplier enforcing probability conservation vanishes exactly, reducing the constrained projection to an unconstrained empirical loss. The resulting objective has constant Hessian curvature mu I, non-saturating linear gradients, and an intrinsic dead-zone mechanism without heuristic clipping. Experiments on mathematical reasoning benchmarks show that GOPO achieves competitive generalization while maintaining stable gradient dynamics and entropy preservation in regimes where clipping-based methods plateau.

</details>


### [112] [Neural network optimization strategies and the topography of the loss landscape](https://arxiv.org/abs/2602.21276)
*Jianneng Yu,Alexandre V. Morozov*

Main category: cs.LG

TL;DR: SGD finds smoother, more generalizable minima while quasi-Newton finds deeper, isolated minima that overfit training data.


<details>
  <summary>Details</summary>
Motivation: To understand how different optimization algorithms (SGD vs quasi-Newton) affect neural network solutions and their generalization performance on unseen test data.

Method: Compare SGD (stochastic gradient descent) with non-stochastic quasi-Newton method using kernel PCA and FourierPathFinder algorithm to analyze loss landscapes and parameter solutions.

Result: SGD solutions have lower barriers between them and occupy smoother basins, leading to better generalization. Quasi-Newton finds deeper, more isolated minima that overfit training data and are less transferable to test data.

Conclusion: Optimizer choice fundamentally shapes solution characteristics: SGD explores smooth basins for robust models, while quasi-Newton finds deeper but overfitting minima, highlighting the importance of landscape exploration strategies.

Abstract: Neural networks are trained by optimizing multi-dimensional sets of fitting parameters on non-convex loss landscapes. Low-loss regions of the landscapes correspond to the parameter sets that perform well on the training data. A key issue in machine learning is the performance of trained neural networks on previously unseen test data. Here, we investigate neural network training by stochastic gradient descent (SGD) - a non-convex global optimization algorithm which relies only on the gradient of the objective function. We contrast SGD solutions with those obtained via a non-stochastic quasi-Newton method, which utilizes curvature information to determine step direction and Golden Section Search to choose step size. We use several computational tools to investigate neural network parameters obtained by these two optimization methods, including kernel Principal Component Analysis and a novel, general-purpose algorithm for finding low-height paths between pairs of points on loss or energy landscapes, FourierPathFinder. We find that the choice of the optimizer profoundly affects the nature of the resulting solutions. SGD solutions tend to be separated by lower barriers than quasi-Newton solutions, even if both sets of solutions are regularized by early stopping to ensure adequate performance on test data. When allowed to fit extensively on the training data, quasi-Newton solutions occupy deeper minima on the loss landscapes that are not reached by SGD. These solutions are less generalizable to the test data however. Overall, SGD explores smooth basins of attraction, while quasi-Newton optimization is capable of finding deeper, more isolated minima that are more spread out in the parameter space. Our findings help understand both the topography of the loss landscapes and the fundamental role of landscape exploration strategies in creating robust, transferrable neural network models.

</details>


### [113] [Robust AI Evaluation through Maximal Lotteries](https://arxiv.org/abs/2602.21297)
*Hadi Khalaf,Serena L. Wang,Daniel Halpern,Itai Shapira,Flavio du Pin Calmon,Ariel D. Procaccia*

Main category: cs.LG

TL;DR: The paper proposes robust lotteries as an alternative to Bradley-Terry rankings for evaluating language models on subjective tasks, addressing issues with preference heterogeneity and providing more reliable win rate guarantees.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods (Bradley-Terry rankings) force heterogeneous preferences into a total order, violating social-choice desiderata and failing to account for diverse user preferences. Maximal lotteries from social choice theory are sensitive to preference heterogeneity and can favor models that underperform on specific tasks or user subpopulations.

Method: Introduces robust lotteries that optimize worst-case performance under plausible shifts in preference data, moving from rankings to pluralistic sets of winners rather than forcing a single total order.

Result: On large-scale preference datasets, robust lotteries provide more reliable win rate guarantees across the annotator distribution and recover a stable set of top-performing models compared to traditional methods.

Conclusion: Robust lotteries offer a principled alternative to rankings, enabling an ecosystem of complementary AI systems that better serve the full spectrum of human preferences through pluralistic winner sets rather than forced total orders.

Abstract: The standard way to evaluate language models on subjective tasks is through pairwise comparisons: an annotator chooses the "better" of two responses to a prompt. Leaderboards aggregate these comparisons into a single Bradley-Terry (BT) ranking, forcing heterogeneous preferences into a total order and violating basic social-choice desiderata. In contrast, social choice theory provides an alternative approach called maximal lotteries, which aggregates pairwise preferences without imposing any assumptions on their structure. However, we show that maximal lotteries are highly sensitive to preference heterogeneity and can favor models that severely underperform on specific tasks or user subpopulations. We introduce robust lotteries that optimize worst-case performance under plausible shifts in the preference data. On large-scale preference datasets, robust lotteries provide more reliable win rate guarantees across the annotator distribution and recover a stable set of top-performing models. By moving from rankings to pluralistic sets of winners, robust lotteries offer a principled step toward an ecosystem of complementary AI systems that serve the full spectrum of human preferences.

</details>


### [114] [SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks](https://arxiv.org/abs/2602.21307)
*Elizabeth S. Z. Tan,Adil Soubki,Miles Cranmer*

Main category: cs.LG

TL;DR: SymTorch is a library that automates symbolic distillation of neural network components into interpretable mathematical expressions using PySR, addressing engineering barriers to adoption.


<details>
  <summary>Details</summary>
Motivation: Symbolic distillation shows promise for discovering physical laws and mathematical relationships from neural networks, but adoption is limited by engineering challenges in integrating symbolic regression into deep learning workflows.

Method: SymTorch wraps neural network components, collects input-output behavior, and approximates them with human-readable equations via PySR. It handles GPU-CPU data transfer, caching, serialization, and seamless switching between neural and symbolic forward passes.

Result: Demonstrated across diverse architectures (GNNs, PINNs, transformers) and achieved 8.3% throughput improvement in LLM inference by replacing MLP layers with symbolic surrogates, with moderate performance degradation.

Conclusion: SymTorch successfully addresses engineering barriers to symbolic distillation adoption and shows practical benefits for model interpretability and inference acceleration.

Abstract: Symbolic distillation replaces neural networks, or components thereof, with interpretable, closed-form mathematical expressions. This approach has shown promise in discovering physical laws and mathematical relationships directly from trained deep learning models, yet adoption remains limited due to the engineering barrier of integrating symbolic regression into deep learning workflows. We introduce SymTorch, a library that automates this distillation by wrapping neural network components, collecting their input-output behavior, and approximating them with human-readable equations via PySR. SymTorch handles the engineering challenges that have hindered adoption: GPU-CPU data transfer, input-output caching, model serialization, and seamless switching between neural and symbolic forward passes. We demonstrate SymTorch across diverse architectures including GNNs, PINNs and transformer models. Finally, we present a proof-of-concept for accelerating LLM inference by replacing MLP layers with symbolic surrogates, achieving an 8.3\% throughput improvement with moderate performance degradation.

</details>


### [115] [Shared Nature, Unique Nurture: PRISM for Pluralistic Reasoning via In-context Structure Modeling](https://arxiv.org/abs/2602.21317)
*Guancheng Tu,Shiyang Zhang,Tianyu Zhang,Yi Zhang,Diji Yang*

Main category: cs.LG

TL;DR: PRISM introduces Epistemic Evolution to create diverse AI reasoning paths via dynamic epistemic graphs, achieving SOTA creativity and uncovering rare diagnoses missed by standard LLMs.


<details>
  <summary>Details</summary>
Motivation: Current LLMs are converging into an "Artificial Hivemind" with collapsed distributional diversity, limiting creative exploration and scientific discovery due to shared pre-training priors.

Method: PRISM (Pluralistic Reasoning via In-context Structure Modeling) uses Epistemic Evolution paradigm (explore, internalize, express) with dynamic On-the-fly Epistemic Graphs to give LLMs individualized epistemic trajectories at inference time.

Result: Achieves state-of-the-art novelty on three creativity benchmarks, significantly expands distributional diversity, and successfully uncovers correct rare-disease diagnoses that standard LLMs miss.

Conclusion: Establishes a new paradigm for Pluralistic AI, moving beyond monolithic consensus toward a diverse ecosystem of unique cognitive individuals capable of collective, multi-perspective discovery.

Abstract: Large Language Models (LLMs) are converging towards a singular Artificial Hivemind, where shared Nature (pre-training priors) result in a profound collapse of distributional diversity, limiting the distinct perspectives necessary for creative exploration and scientific discovery. To address this, we propose to equip models with inference-time Nurture (individualized epistemic trajectories) using Epistemic Evolution paradigm, progressing through explore, internalize, and express. We instantiate this via PRISM (Pluralistic Reasoning via In-context Structure Modeling), a model-agnostic system that augments LLM with dynamic On-the-fly Epistemic Graphs. On three creativity benchmarks, PRISM achieves state-of-the-art novelty and significantly expands distributional diversity. Moreover, we evaluate the real-world utility via a challenging rare-disease diagnosis benchmark. Results demonstrate that PRISM successfully uncovers correct long-tail diagnoses that standard LLM miss, confirming that its divergence stems from meaningful exploration rather than incoherent noise. Overall, this work establishes a new paradigm for Pluralistic AI, moving beyond monolithic consensus toward a diverse ecosystem of unique cognitive individuals capable of collective, multi-perspective discovery.

</details>


### [116] [Uncertainty-Aware Diffusion Model for Multimodal Highway Trajectory Prediction via DDIM Sampling](https://arxiv.org/abs/2602.21319)
*Marion Neumeier,Niklas Roßberg,Michael Botsch,Wolfgang Utschick*

Main category: cs.LG

TL;DR: cVMDx is an enhanced diffusion-based trajectory prediction framework that improves efficiency (100x faster sampling), robustness, and multimodal predictive capability for autonomous driving applications.


<details>
  <summary>Details</summary>
Motivation: Trajectory prediction for autonomous driving faces challenges with multi-agent interactions, diverse scene contexts, and stochastic future motion. Existing diffusion-based approaches like cVMD have limitations in slow sampling, limited generative diversity exploitation, and brittle scenario encodings.

Method: cVMDx uses DDIM sampling for faster inference (up to 100x reduction), fits Gaussian Mixture Models for tractable multimodal predictions from generated trajectories, and evaluates a CVQ-VAE variant for scenario encoding.

Result: Experiments on the highD dataset show cVMDx achieves higher accuracy and significantly improved efficiency over cVMD, enabling fully stochastic, multimodal trajectory prediction with practical multi-sample generation for uncertainty estimation.

Conclusion: cVMDx successfully addresses the limitations of previous diffusion-based trajectory prediction methods by improving efficiency, robustness, and multimodal capability, making it practical for real-world autonomous driving applications.

Abstract: Accurate and uncertainty-aware trajectory prediction remains a core challenge for autonomous driving, driven by complex multi-agent interactions, diverse scene contexts and the inherently stochastic nature of future motion. Diffusion-based generative models have recently shown strong potential for capturing multimodal futures, yet existing approaches such as cVMD suffer from slow sampling, limited exploitation of generative diversity and brittle scenario encodings.
  This work introduces cVMDx, an enhanced diffusion-based trajectory prediction framework that improves efficiency, robustness and multimodal predictive capability. Through DDIM sampling, cVMDx achieves up to a 100x reduction in inference time, enabling practical multi-sample generation for uncertainty estimation. A fitted Gaussian Mixture Model further provides tractable multimodal predictions from the generated trajectories. In addition, a CVQ-VAE variant is evaluated for scenario encoding. Experiments on the publicly available highD dataset show that cVMDx achieves higher accuracy and significantly improved efficiency over cVMD, enabling fully stochastic, multimodal trajectory prediction.

</details>


### [117] [Tool-R0: Self-Evolving LLM Agents for Tool-Learning from Zero Data](https://arxiv.org/abs/2602.21320)
*Emre Can Acikgoz,Cheng Qian,Jonas Hübotter,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur*

Main category: cs.LG

TL;DR: Tool-R0 is a self-play RL framework that trains LLM agents to use tools from scratch without any pre-existing data, achieving significant performance improvements over base models and supervised baselines.


<details>
  <summary>Details</summary>
Motivation: Current RL approaches for training tool-using LLM agents require carefully constructed task-solution pairs and substantial human supervision, creating obstacles to open-ended self-evolution toward superintelligent systems. The authors aim to overcome this limitation.

Method: Tool-R0 uses self-play RL with two co-evolving agents from the same base LLM: a Generator that proposes challenging tasks at the other agent's competence frontier, and a Solver that learns to solve them with real-world tool calls. This creates a self-evolving cycle requiring zero pre-existing data.

Result: Evaluation on tool-use benchmarks shows Tool-R0 yields 92.5% relative improvement over the base model and surpasses fully supervised tool-calling baselines under the same setting. The framework also provides empirical insights into co-evolution, curriculum dynamics, and scaling behavior.

Conclusion: Tool-R0 demonstrates that self-play RL can effectively train general-purpose tool-calling agents from scratch without human supervision or pre-existing datasets, enabling open-ended self-evolution toward more capable autonomous systems.

Abstract: Large language models (LLMs) are becoming the foundation for autonomous agents that can use tools to solve complex tasks. Reinforcement learning (RL) has emerged as a common approach for injecting such agentic capabilities, but typically under tightly controlled training setups. It often depends on carefully constructed task-solution pairs and substantial human supervision, which creates a fundamental obstacle to open-ended self-evolution toward superintelligent systems. In this paper, we propose Tool-R0 framework for training general purpose tool-calling agents from scratch with self-play RL, under a zero-data assumption. Initialized from the same base LLM, Tool-R0 co-evolves a Generator and a Solver with complementary rewards: one proposes targeted challenging tasks at the other's competence frontier and the other learns to solve them with real-world tool calls. This creates a self-evolving cycle that requires no pre-existing tasks or datasets. Evaluation on different tool-use benchmarks show that Tool-R0 yields 92.5 relative improvement over the base model and surpasses fully supervised tool-calling baselines under the same setting. Our work further provides empirical insights into self-play LLM agents by analyzing co-evolution, curriculum dynamics, and scaling behavior.

</details>


### [118] [Dynamic Symmetric Point Tracking: Tackling Non-ideal Reference in Analog In-memory Training](https://arxiv.org/abs/2602.21321)
*Quan Xiao,Jindan Li,Zhaoxian Wu,Tayfun Gokmen,Tianyi Chen*

Main category: cs.LG

TL;DR: The paper proposes dynamic symmetric point estimation methods to mitigate bias in analog in-memory computing training caused by device update asymmetry, with theoretical guarantees and improved efficiency.


<details>
  <summary>Details</summary>
Motivation: Analog in-memory computing suffers from device update asymmetry that causes systematic weight drift toward device-specific symmetric points, which don't align with training objectives. Existing methods require costly pre-calibration and residual errors degrade accuracy.

Method: 1) First theoretical characterization of symmetric point calibration pulse complexity and estimation error. 2) Dynamic symmetric point estimation method that tracks SP during training with convergence guarantees. 3) Enhanced variant using chopping and filtering techniques from digital signal processing.

Result: Numerical experiments demonstrate both efficiency and effectiveness of the proposed dynamic SP estimation methods compared to traditional pre-calibration approaches.

Conclusion: The proposed dynamic symmetric point estimation methods provide a more efficient and effective solution to mitigate bias in AIMC training, reducing costly calibration requirements while maintaining training accuracy.

Abstract: Analog in-memory computing (AIMC) performs computation directly within resistive crossbar arrays, offering an energy-efficient platform to scale large vision and language models. However, non-ideal analog device properties make the training on AIMC devices challenging. In particular, its update asymmetry can induce a systematic drift of weight updates towards a device-specific symmetric point (SP), which typically does not align with the optimum of the training objective. To mitigate this bias, most existing works assume the SP is known and pre-calibrate it to zero before training by setting the reference point as the SP. Nevertheless, calibrating AIMC devices requires costly pulse updates, and residual calibration error can directly degrade training accuracy. In this work, we present the first theoretical characterization of the pulse complexity of SP calibration and the resulting estimation error. We further propose a dynamic SP estimation method that tracks the SP during model training, and establishes its convergence guarantees. In addition, we develop an enhanced variant based on chopping and filtering techniques from digital signal processing. Numerical experiments demonstrate both the efficiency and effectiveness of the proposed method.

</details>


### [119] [Equitable Evaluation via Elicitation](https://arxiv.org/abs/2602.21327)
*Elbert Du,Cynthia Dwork,Lunjia Hu,Reid McIlroy-Young,Han Shao,Linjun Zhang*

Main category: cs.LG

TL;DR: AI system for skill assessment that reduces bias from self-presentation styles by using interactive elicitation and equitable evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Current skill assessment is problematic because equally qualified individuals present themselves differently (self-promotion vs. modesty), making fair comparisons difficult. This creates bias in hiring and professional matching.

Method: Builds interactive AI for skill elicitation that allows natural self-expression while accurately determining skills. Uses LLM-generated synthetic human data for training. Implements mathematical equitability constraints to minimize covariance between self-presentation style and evaluation error.

Result: Developed a system that provides accurate skill determination while accommodating individual communication styles, with rigorous bias mitigation through equitability constraints.

Conclusion: Interactive AI skill elicitation with equitability constraints can provide fairer skill assessments by reducing both endogenous self-report bias and systematic model bias, enabling better professional matching.

Abstract: Individuals with similar qualifications and skills may vary in their demeanor, or outward manner: some tend toward self-promotion while others are modest to the point of omitting crucial information. Comparing the self-descriptions of equally qualified job-seekers with different self-presentation styles is therefore problematic.
  We build an interactive AI for skill elicitation that provides accurate determination of skills while simultaneously allowing individuals to speak in their own voice. Such a system can be deployed, for example, when a new user joins a professional networking platform, or when matching employees to needs during a company reorganization. To obtain sufficient training data, we train an LLM to act as synthetic humans.
  Elicitation mitigates endogenous bias arising from individuals' own self-reports. To address systematic model bias we enforce a mathematically rigorous notion of equitability ensuring that the covariance between self-presentation manner and skill evaluation error is small.

</details>


### [120] [Efficient Opportunistic Approachability](https://arxiv.org/abs/2602.21328)
*Teodor Vanislavov Marinov,Mehryar Mohri,Princewill Okoroafor,Jon Schneider,Julian Zimmert*

Main category: cs.LG

TL;DR: Efficient algorithm for opportunistic approachability achieves O(T^{-1/4}) rate without needing calibrated predictions, improving over previous exponential-time methods.


<details>
  <summary>Details</summary>
Motivation: Previous opportunistic approachability algorithms require calibrated online predictions of adversary's actions, which have exponential time complexity and poor scaling (T^{-O(1/d)}). Need efficient algorithms with better convergence rates.

Method: Develop new efficient algorithm for opportunistic approachability that bypasses online calibration subroutine. Also present inefficient algorithm achieving O(T^{-1/3}) and optimal O(T^{-1/2}) rate for 2D adversary action sets.

Result: Efficient algorithm achieves O(T^{-1/4}) rate, inefficient algorithm achieves O(T^{-1/3}) rate, and optimal O(T^{-1/2}) rate for 2D adversary action sets.

Conclusion: Significant improvement over previous work by eliminating need for computationally expensive calibration subroutines while achieving better convergence rates, especially for low-dimensional adversary action spaces.

Abstract: We study the problem of opportunistic approachability: a generalization of Blackwell approachability where the learner would like to obtain stronger guarantees (i.e., approach a smaller set) when their adversary limits themselves to a subset of their possible action space. Bernstein et al. (2014) introduced this problem in 2014 and presented an algorithm that guarantees sublinear approachability rates for opportunistic approachability. However, this algorithm requires the ability to produce calibrated online predictions of the adversary's actions, a problem whose standard implementations require time exponential in the ambient dimension and result in approachability rates that scale as $T^{-O(1/d)}$. In this paper, we present an efficient algorithm for opportunistic approachability that achieves a rate of $O(T^{-1/4})$ (and an inefficient one that achieves a rate of $O(T^{-1/3})$), bypassing the need for an online calibration subroutine. Moreover, in the case where the dimension of the adversary's action set is at most two, we show it is possible to obtain the optimal rate of $O(T^{-1/2})$.

</details>


### [121] [HiPPO Zoo: Explicit Memory Mechanisms for Interpretable State Space Models](https://arxiv.org/abs/2602.21340)
*Jack Goffinet,Casey Hanks,David E. Carlson*

Main category: cs.LG

TL;DR: The paper introduces "HiPPO zoo" - a unified framework extending HiPPO's polynomial memory representations to support modern SSM capabilities like adaptive memory allocation and associative memory while maintaining interpretability.


<details>
  <summary>Details</summary>
Motivation: Modern SSMs like Mamba have implicit mechanisms for representing and prioritizing history. The authors aim to make these mechanisms explicit by extending the HiPPO framework to support capabilities of modern SSMs while retaining interpretability.

Method: The authors revisit the HiPPO framework and introduce five extensions collectively called "HiPPO zoo." Each extension provides explicit, interpretable modifications to support specific modeling capabilities like adaptive memory allocation and associative memory. The models adapt memory online and train in streaming settings with efficient updates.

Result: The framework demonstrates that capabilities typically associated with modern SSMs can be realized through explicit, interpretable polynomial memory structures. The behaviors and modeling advantages are illustrated through synthetic sequence modeling tasks.

Conclusion: The HiPPO zoo provides a principled way to make memory mechanisms in modern SSMs explicit and interpretable while supporting advanced capabilities like adaptive memory allocation and associative memory, bridging the gap between theoretical foundations and practical state-of-the-art models.

Abstract: Representing the past in a compressed, efficient, and informative manner is a central problem for systems trained on sequential data. The HiPPO framework, originally proposed by Gu & Dao et al., provides a principled approach to sequential compression by projecting signals onto orthogonal polynomial (OP) bases via structured linear ordinary differential equations. Subsequent works have embedded these dynamics in state space models (SSMs), where HiPPO structure serves as an initialization. Nonlinear successors of these SSM methods such as Mamba are state-of-the-art for many tasks with long-range dependencies, but the mechanisms by which they represent and prioritize history remain largely implicit. In this work, we revisit the HiPPO framework with the goal of making these mechanisms explicit. We show how polynomial representations of history can be extended to support capabilities of modern SSMs such as adaptive allocation of memory and associative memory while retaining direct interpretability in the OP basis. We introduce a unified framework comprising five such extensions, which we collectively refer to as a "HiPPO zoo." Each extension exposes a specific modeling capability through an explicit, interpretable modification of the HiPPO framework. The resulting models adapt their memory online and train in streaming settings with efficient updates. We illustrate the behaviors and modeling advantages of these extensions through a range of synthetic sequence modeling tasks, demonstrating that capabilities typically associated with modern SSMs can be realized through explicit, interpretable polynomial memory structures.

</details>


### [122] [Archetypal Graph Generative Models: Explainable and Identifiable Communities via Anchor-Dominant Convex Hulls](https://arxiv.org/abs/2602.21342)
*Nikolaos Nakis,Chrysoula Kosma,Panagiotis Promponas,Michail Chatzianastasis,Giannis Nikolentzos*

Main category: cs.LG

TL;DR: GraphHull: An explainable generative model using two-level convex hulls for network representation, providing multi-scale explanations while maintaining competitive performance on graph tasks.


<details>
  <summary>Details</summary>
Motivation: Despite advances in graph representation learning, most models lack self-explainability. Understanding prediction patterns is crucial, motivating the need for explainable graph models that can provide clear interpretations of network structures and predictions.

Method: GraphHull uses two-level convex hulls: global archetypes as vertices of a convex hull representing pure communities, and local prototypical hulls with vertices as representative profiles capturing community-specific variations. The model incorporates determinantal point process priors for diversity/stability and uses MAP estimation with scalable subsampling.

Result: Experiments show GraphHull recovers multi-level community structure, achieves competitive/superior performance in link prediction and community detection, while naturally providing interpretable predictions through its geometric construction.

Conclusion: GraphHull offers an effective explainable generative model for networks that combines strong performance on downstream tasks with clear multi-scale explanations through its two-level convex hull representation.

Abstract: Representation learning has been essential for graph machine learning tasks such as link prediction, community detection, and network visualization. Despite recent advances in achieving high performance on these downstream tasks, little progress has been made toward self-explainable models. Understanding the patterns behind predictions is equally important, motivating recent interest in explainable machine learning. In this paper, we present GraphHull, an explainable generative model that represents networks using two levels of convex hulls. At the global level, the vertices of a convex hull are treated as archetypes, each corresponding to a pure community in the network. At the local level, each community is refined by a prototypical hull whose vertices act as representative profiles, capturing community-specific variation. This two-level construction yields clear multi-scale explanations: a node's position relative to global archetypes and its local prototypes directly accounts for its edges. The geometry is well-behaved by design, while local hulls are kept disjoint by construction. To further encourage diversity and stability, we place principled priors, including determinantal point processes, and fit the model under MAP estimation with scalable subsampling. Experiments on real networks demonstrate the ability of GraphHull to recover multi-level community structure and to achieve competitive or superior performance in link prediction and community detection, while naturally providing interpretable predictions.

</details>


### [123] [Black-Box Reliability Certification for AI Agents via Self-Consistency Sampling and Conformal Calibration](https://arxiv.org/abs/2602.21368)
*Charafeddine Mouzouni*

Main category: cs.LG

TL;DR: The paper introduces a method to compute reliability levels for black-box AI systems using self-consistency sampling and conformal calibration, providing exact finite-sample guarantees for deployment trust.


<details>
  <summary>Details</summary>
Motivation: Practitioners need to know at what confidence level they can trust black-box AI system outputs for specific tasks, requiring a reliable deployment gate with formal guarantees.

Method: Combines self-consistency sampling (reducing uncertainty exponentially) with conformal calibration to compute reliability levels with exact, finite-sample, distribution-free guarantees. Uses sequential stopping to reduce API costs.

Result: Method validated across 5 benchmarks, 5 models from 3 families, with both synthetic and real data. GPT-4.1 achieves 94.6% on GSM8K and 96.8% on TruthfulQA; GPT-4.1-nano achieves 89.8% on GSM8K and 66.5% on MMLU. Conditional coverage on solvable items exceeds 0.93 across all configurations; sequential stopping reduces API costs by ~50%.

Conclusion: The proposed reliability level provides a practical deployment gate for black-box AI systems with formal guarantees, making system errors transparent through larger answer sets for harder questions, and enabling cost-effective evaluation through sequential stopping.

Abstract: Given a black-box AI system and a task, at what confidence level can a practitioner trust the system's output? We answer with a reliability level -- a single number per system-task pair, derived from self-consistency sampling and conformal calibration, that serves as a black-box deployment gate with exact, finite-sample, distribution-free guarantees. Self-consistency sampling reduces uncertainty exponentially; conformal calibration guarantees correctness within 1/(n+1) of the target level, regardless of the system's errors -- made transparently visible through larger answer sets for harder questions. Weaker models earn lower reliability levels (not accuracy -- see Definition 2.4): GPT-4.1 earns 94.6% on GSM8K and 96.8% on TruthfulQA, while GPT-4.1-nano earns 89.8% on GSM8K and 66.5% on MMLU. We validate across five benchmarks, five models from three families, and both synthetic and real data. Conditional coverage on solvable items exceeds 0.93 across all configurations; sequential stopping reduces API costs by around 50%.

</details>


### [124] [Interleaved Head Attention](https://arxiv.org/abs/2602.21371)
*Sai Surya Duvvuri,Chanakya Ekbote,Rachit Bansal,Rishabh Tiwari,Devvrit Khatri,David Brandfonbrener,Paul Liang,Inderjit Dhillon,Manzil Zaheer*

Main category: cs.LG

TL;DR: IHA (Interleaved Head Attention) enables cross-head mixing in attention mechanisms, addressing MHA's linear scaling limitation for multi-step reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Multi-Head Attention suffers from linear scaling where heads operate independently without communication, which is problematic for multi-step reasoning requiring evidence aggregation from multiple context parts and composition of token-to-token relations over inference chains.

Method: Propose Interleaved Head Attention (IHA) that enables cross-head mixing by constructing P pseudo-heads per head (typically P=H), where each pseudo query/key/value is a learned linear combination of all H original queries, keys and values. This creates up to P² attention patterns per head with modest parameter overhead O(H²P).

Result: Theoretical analysis shows improved efficiency: on Polynomial task, IHA uses Θ(√k n²) parameters vs. Θ(k n²) for MHA; on order-sensitive CPM-3 task, IHA uses ⌈√N_max⌉ heads vs. N_max for MHA. Real-world benchmarks show 10-20% improvement on Multi-Key retrieval (RULER), and after fine-tuning for reasoning, improves GSM8K by 5.8% and MATH-500 by 2.8% over full attention.

Conclusion: IHA addresses fundamental limitations of MHA by enabling cross-head communication, improving efficiency and performance on reasoning tasks while maintaining modest parameter overhead.

Abstract: Multi-Head Attention (MHA) is the core computational primitive underlying modern Large Language Models (LLMs). However, MHA suffers from a fundamental linear scaling limitation: $H$ attention heads produce exactly $H$ independent attention matrices, with no communication between heads during attention computation. This becomes problematic for multi-step reasoning, where correct answers depend on aggregating evidence from multiple parts of the context and composing latent token-to-token relations over a chain of intermediate inferences. To address this, we propose Interleaved Head Attention (IHA), which enables cross-head mixing by constructing $P$ pseudo-heads per head (typically $P=H$), where each pseudo query/key/value is a learned linear combination of all $H$ original queries, keys and values respectively. Interactions between pseudo-query and pseudo-key heads induce up to $P^2$ attention patterns per head with modest parameter overhead $\mathcal{O}(H^2P)$. We provide theory showing improved efficiency in terms of number of parameters on the synthetic Polynomial task (IHA uses $Θ(\sqrt{k}n^2)$ parameters vs. $Θ(kn^2)$ for MHA) and on the synthetic order-sensitive CPM-3 task (IHA uses $\lceil\sqrt{N_{\max}}\rceil$ heads vs. $N_{\max}$ for MHA). On real-world benchmarks, IHA improves Multi-Key retrieval on RULER by 10-20% (4k-16k) and, after fine-tuning for reasoning on OpenThoughts, improves GSM8K by 5.8% and MATH-500 by 2.8% (Majority Vote) over full attention.

</details>


### [125] [The Mean is the Mirage: Entropy-Adaptive Model Merging under Heterogeneous Domain Shifts in Medical Imaging](https://arxiv.org/abs/2602.21372)
*Sameer Ambekar,Reza Nasirigerdeh,Peter J. Schuffler,Lina Felsner,Daniel M. Lang,Julia A. Schnabel*

Main category: cs.LG

TL;DR: Online entropy-adaptive model merging method for medical imaging that creates batch-specific merged models via forward passes only, addressing distribution shifts in unseen clinical deployments.


<details>
  <summary>Details</summary>
Motivation: Model merging fails under unseen test-time distribution shifts in medical imaging where models are fine-tuned locally on private data with domain-specific variations (scanner, protocol, population). When deployed at unseen clinical sites, models must adapt immediately to unlabeled, non-i.i.d. batches without labels.

Method: Introduces entropy-adaptive, fully online model-merging method that yields batch-specific merged models via only forward passes. Decouples encoder and classification head to mitigate encoder-classifier mismatch, merging with separate merging coefficients.

Result: Extensive evaluation with state-of-the-art baselines using two backbones across nine medical and natural-domain generalization image classification datasets shows consistent gains across standard evaluation and challenging scenarios while retaining single-model inference at test-time.

Conclusion: The method effectively leverages target information for adaptation under heterogeneous domain shifts, demonstrating why mean merging fails and providing a practical solution for medical imaging deployment with distribution shifts.

Abstract: Model merging under unseen test-time distribution shifts often renders naive strategies, such as mean averaging unreliable. This challenge is especially acute in medical imaging, where models are fine-tuned locally at clinics on private data, producing domain-specific models that differ by scanner, protocol, and population. When deployed at an unseen clinical site, test cases arrive in unlabeled, non-i.i.d. batches, and the model must adapt immediately without labels. In this work, we introduce an entropy-adaptive, fully online model-merging method that yields a batch-specific merged model via only forward passes, effectively leveraging target information. We further demonstrate why mean merging is prone to failure and misaligned under heterogeneous domain shifts. Next, we mitigate encoder classifier mismatch by decoupling the encoder and classification head, merging with separate merging coefficients. We extensively evaluate our method with state-of-the-art baselines using two backbones across nine medical and natural-domain generalization image classification datasets, showing consistent gains across standard evaluation and challenging scenarios. These performance gains are achieved while retaining single-model inference at test-time, thereby demonstrating the effectiveness of our method.

</details>


### [126] [VCDF: A Validated Consensus-Driven Framework for Time Series Causal Discovery](https://arxiv.org/abs/2602.21381)
*Gene Yu,Ce Guo,Wayne Luk*

Main category: cs.LG

TL;DR: VCDF is a method-agnostic framework that improves time series causal discovery robustness by evaluating causal relation stability across temporal subsets, requiring no algorithm modifications.


<details>
  <summary>Details</summary>
Motivation: Existing time series causal discovery methods are sensitive to noise, non-stationarity, and sampling variability, limiting their reliability in real-world applications.

Method: Validated Consensus-Driven Framework (VCDF) evaluates stability of causal relations across blocked temporal subsets, applying consensus validation to base algorithms like VAR-LiNGAM and PCMCI without modifying them.

Result: VCDF improves VAR-LiNGAM by 0.08-0.12 in F1 scores across diverse data, with gains up to 0.18 for longer sequences (1000+). Enhanced stability and accuracy demonstrated on fMRI and IT-monitoring data under realistic noise.

Conclusion: VCDF provides an effective reliability layer for time series causal discovery that enhances robustness without altering underlying modeling assumptions, particularly beneficial for moderate-to-long sequences.

Abstract: Time series causal discovery is essential for understanding dynamic systems, yet many existing methods remain sensitive to noise, non-stationarity, and sampling variability. We propose the Validated Consensus-Driven Framework (VCDF), a simple and method-agnostic layer that improves robustness by evaluating the stability of causal relations across blocked temporal subsets. VCDF requires no modification to base algorithms and can be applied to methods such as VAR-LiNGAM and PCMCI. Experiments on synthetic datasets show that VCDF improves VAR-LiNGAM by approximately 0.08-0.12 in both window and summary F1 scores across diverse data characteristics, with gains most pronounced for moderate-to-long sequences. The framework also benefits from longer sequences, yielding up to 0.18 absolute improvement on time series of length 1000 and above. Evaluations on simulated fMRI data and IT-monitoring scenarios further demonstrate enhanced stability and structural accuracy under realistic noise conditions. VCDF provides an effective reliability layer for time series causal discovery without altering underlying modeling assumptions.

</details>


### [127] [Defensive Generation](https://arxiv.org/abs/2602.21390)
*Gabriele Farina,Juan Carlos Perdomo*

Main category: cs.LG

TL;DR: Defensive Generation: Efficient online algorithm producing unfalsifiable generative models for various data types using multicalibration and RKHS connections.


<details>
  <summary>Details</summary>
Motivation: Need for efficient online generation of models that cannot be falsified by pre-specified computational tests, especially for non-Bernoulli outcomes and infinite test classes.

Method: Connects online high-dimensional multicalibration with RKHS to expected variational inequality problems, then applies this to outcome indistinguishability via Defensive Generation algorithm.

Result: First efficient online algorithm for non-Bernoulli outcomes unfalsifiable against infinite test classes (including higher-order moments), runs in near-linear time with optimal T^{-1/2} error rate.

Conclusion: Defensive Generation enables practical, efficient production of robust generative models that withstand computational testing, advancing outcome indistinguishability theory.

Abstract: We study the problem of efficiently producing, in an online fashion, generative models of scalar, multiclass, and vector-valued outcomes that cannot be falsified on the basis of the observed data and a pre-specified collection of computational tests. Our contributions are twofold. First, we expand on connections between online high-dimensional multicalibration with respect to an RKHS and recent advances in expected variational inequality problems, enabling efficient algorithms for the former. We then apply this algorithmic machinery to the problem of outcome indistinguishability. Our procedure, Defensive Generation, is the first to efficiently produce online outcome indistinguishable generative models of non-Bernoulli outcomes that are unfalsifiable with respect to infinite classes of tests, including those that examine higher-order moments of the generated distributions. Furthermore, our method runs in near-linear time in the number of samples and achieves the optimal, vanishing T^{-1/2} rate for generation error.

</details>


### [128] [FedVG: Gradient-Guided Aggregation for Enhanced Federated Learning](https://arxiv.org/abs/2602.21399)
*Alina Devkota,Jacob Thrasher,Donald Adjeroh,Binod Bhattarai,Prashnna K. Gyawali*

Main category: cs.LG

TL;DR: FedVG is a gradient-based federated aggregation framework that uses a global validation set to address client drift in heterogeneous FL settings by assessing client generalization ability through layerwise gradient norms.


<details>
  <summary>Details</summary>
Motivation: Data heterogeneity in federated learning causes client drift, degrading model generalization. Traditional methods overemphasize poorly performing clients, exacerbating the problem.

Method: Uses a global validation set (from public data) to compute layerwise gradient norms, deriving client-specific scores that indicate how much each client needs to adjust for better generalization. This guides adaptive aggregation.

Result: Extensive experiments on natural and medical image datasets show FedVG consistently improves performance, especially in highly heterogeneous settings. It's modular and can enhance various state-of-the-art FL algorithms.

Conclusion: FedVG effectively mitigates client drift in heterogeneous FL by leveraging validation gradients for informed aggregation, improving generalization without compromising privacy.

Abstract: Federated Learning (FL) enables collaborative model training across multiple clients without sharing their private data. However, data heterogeneity across clients leads to client drift, which degrades the overall generalization performance of the model. This effect is further compounded by overemphasis on poorly performing clients. To address this problem, we propose FedVG, a novel gradient-based federated aggregation framework that leverages a global validation set to guide the optimization process. Such a global validation set can be established using readily available public datasets, ensuring accessibility and consistency across clients without compromising privacy. In contrast to conventional approaches that prioritize client dataset volume, FedVG assesses the generalization ability of client models by measuring the magnitude of validation gradients across layers. Specifically, we compute layerwise gradient norms to derive a client-specific score that reflects how much each client needs to adjust for improved generalization on the global validation set, thereby enabling more informed and adaptive federated aggregation. Extensive experiments on both natural and medical image benchmarking datasets, across diverse model architectures, demonstrate that FedVG consistently improves performance, particularly in highly heterogeneous settings. Moreover, FedVG is modular and can be seamlessly integrated with various state-of-the-art FL algorithms, often further improving their results. Our code is available at https://github.com/alinadevkota/FedVG.

</details>


### [129] [Generative Bayesian Computation as a Scalable Alternative to Gaussian Process Surrogates](https://arxiv.org/abs/2602.21408)
*Nick Polson,Vadim Sokolov*

Main category: cs.LG

TL;DR: GBC via IQNs is a new surrogate framework that addresses GP limitations: cubic cost, stationarity, and Gaussian predictions. It learns conditional quantile functions and scales linearly to 90K points, outperforming GPs on jump processes and high-dimensional functions.


<details>
  <summary>Details</summary>
Motivation: Gaussian processes have three key limitations: cubic computational cost, stationarity assumptions, and restrictive Gaussian predictive distributions. These limitations restrict their applicability for large-scale, non-stationary, and non-Gaussian computer experiments.

Method: Generative Bayesian Computation (GBC) via Implicit Quantile Networks (IQNs) learns the full conditional quantile function from input-output pairs. At test time, a single forward pass per quantile level produces draws from the predictive distribution. A boundary-augmented variant and randomized-prior IQN ensemble are also proposed.

Result: GBC improves CRPS by 11-26% on piecewise jump-process benchmarks, 14% on 10D Friedman function, and scales linearly to 90,000 training points. It outperforms Modular Jump GPs on 2D jump datasets (up to 46% CRPS improvement). In active learning, achieves nearly 3x lower RMSE than deep GP. Overall favorable in 12 of 14 comparisons.

Conclusion: GBC via IQNs effectively addresses GP limitations, offering superior performance on jump processes and scalability to large datasets. GPs retain advantages on smooth surfaces where their smoothness prior provides effective regularization. GBC represents a promising alternative surrogate framework.

Abstract: Gaussian process (GP) surrogates are the default tool for emulating expensive computer experiments, but cubic cost, stationarity assumptions, and Gaussian predictive distributions limit their reach. We propose Generative Bayesian Computation (GBC) via Implicit Quantile Networks (IQNs) as a surrogate framework that targets all three limitations. GBC learns the full conditional quantile function from input--output pairs; at test time, a single forward pass per quantile level produces draws from the predictive distribution.
  Across fourteen benchmarks we compare GBC to four GP-based methods. GBC improves CRPS by 11--26\% on piecewise jump-process benchmarks, by 14\% on a ten-dimensional Friedman function, and scales linearly to 90,000 training points where dense-covariance GPs are infeasible. A boundary-augmented variant matches or outperforms Modular Jump GPs on two-dimensional jump datasets (up to 46\% CRPS improvement). In active learning, a randomized-prior IQN ensemble achieves nearly three times lower RMSE than deep GP active learning on Rocket LGBB. Overall, GBC records a favorable point estimate in 12 of 14 comparisons. GPs retain an edge on smooth surfaces where their smoothness prior provides effective regularization.

</details>


### [130] [Benchmarking State Space Models, Transformers, and Recurrent Networks for US Grid Forecasting](https://arxiv.org/abs/2602.21415)
*Sunki Hong,Jisoo Lee,Yuanyuan Shi*

Main category: cs.LG

TL;DR: No single best model for power grid forecasting; PatchTST and state space models excel with historical load only, while iTransformer improves dramatically with weather data. Model performance depends on data type and forecast task.


<details>
  <summary>Details</summary>
Motivation: Selecting appropriate deep learning models for power grid forecasting is challenging due to performance dependence on available data. Need comprehensive benchmark to guide grid operators in model selection based on their specific data environments.

Method: Benchmarked five neural architectures: PowerMamba, S-Mamba (state space models), iTransformer, PatchTST (Transformers), and traditional LSTM. Evaluated on hourly electricity demand across six US power grids for 24-168 hour forecasts. Used specialized temporal processing and modular layer for weather covariates integration. Controlled for model size and extended evaluation to solar, wind, and price forecasting.

Result: No universal best model. With historical load only: PatchTST and state space models most accurate. With weather data: iTransformer improves accuracy 3x more efficiently than PatchTST. Model rankings depend on forecast task: PatchTST excels on rhythmic signals (solar), state space models better for chaotic fluctuations (wind, price).

Conclusion: Provides actionable guidelines for grid operators to select optimal forecasting architecture based on specific data environments. Model choice should consider available data types (historical load only vs. with weather covariates) and forecast task characteristics (rhythmic vs. chaotic signals).

Abstract: Selecting the right deep learning model for power grid forecasting is challenging, as performance heavily depends on the data available to the operator. This paper presents a comprehensive benchmark of five modern neural architectures: two state space models (PowerMamba, S-Mamba), two Transformers (iTransformer, PatchTST), and a traditional LSTM. We evaluate these models on hourly electricity demand across six diverse US power grids for forecast windows between 24 and 168 hours. To ensure a fair comparison, we adapt each model with specialized temporal processing and a modular layer that cleanly integrates weather covariates. Our results reveal that there is no single best model for all situations. When forecasting using only historical load, PatchTST and the state space models provide the highest accuracy. However, when explicit weather data is added to the inputs, the rankings reverse: iTransformer improves its accuracy three times more efficiently than PatchTST. By controlling for model size, we confirm that this advantage stems from the architecture's inherent ability to mix information across different variables. Extending our evaluation to solar generation, wind power, and wholesale prices further demonstrates that model rankings depend on the forecast task: PatchTST excels on highly rhythmic signals like solar, while state space models are better suited for the chaotic fluctuations of wind and price. Ultimately, this benchmark provides grid operators with actionable guidelines for selecting the optimal forecasting architecture based on their specific data environments.

</details>


### [131] [Overconfident Errors Need Stronger Correction: Asymmetric Confidence Penalties for Reinforcement Learning](https://arxiv.org/abs/2602.21420)
*Yuanda Xu,Hejian Sang,Zhengze Zhou,Ran He,Zhipeng Wang*

Main category: cs.LG

TL;DR: ACE (Asymmetric Confidence-aware Error Penalty) addresses RLVR's diversity reduction by penalizing overconfident errors asymmetrically, improving Pass@k across models.


<details>
  <summary>Details</summary>
Motivation: Standard RLVR algorithms improve Pass@1 but reduce reasoning diversity by uniformly penalizing errors, allowing overconfident incorrect reasoning to dominate probability mass and suppress exploration.

Method: Proposes ACE which uses per-rollout confidence shift metric (c_i = log(pi_theta(y_i|x)/pi_ref(y_i|x))) to dynamically modulate negative advantages, decomposing gradient into selective regularizer for overconfident errors plus residual.

Result: ACE consistently improves full Pass@k spectrum across Qwen2.5-Math-7B, Qwen3-8B-Base, and Llama-3.1-8B-Instruct on MATH-500 and AIME 2025 benchmarks when integrated with GRPO and DAPO methods.

Conclusion: ACE effectively addresses RLVR's diversity-reduction pathology by asymmetrically penalizing overconfident errors, maintaining exploration while improving reasoning performance across multiple model families.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become the leading paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard RLVR algorithms suffer from a well-documented pathology: while they improve Pass@1 accuracy through sharpened sampling, they simultaneously narrow the model's reasoning boundary and reduce generation diversity. We identify a root cause that existing methods overlook: the uniform penalization of errors. Current approaches -- whether data-filtering methods that select prompts by difficulty, or advantage normalization schemes -- treat all incorrect rollouts within a group identically. We show that this uniformity allows overconfident errors (incorrect reasoning paths that the RL process has spuriously reinforced) to persist and monopolize probability mass, ultimately suppressing valid exploratory trajectories. To address this, we propose the Asymmetric Confidence-aware Error Penalty (ACE). ACE introduces a per-rollout confidence shift metric, c_i = log(pi_theta(y_i|x) / pi_ref(y_i|x)), to dynamically modulate negative advantages. Theoretically, we demonstrate that ACE's gradient can be decomposed into the gradient of a selective regularizer restricted to overconfident errors, plus a well-characterized residual that partially moderates the regularizer's strength. We conduct extensive experiments fine-tuning Qwen2.5-Math-7B, Qwen3-8B-Base, and Llama-3.1-8B-Instruct on the DAPO-Math-17K dataset using GRPO and DAPO within the VERL framework. Evaluated on MATH-500 and AIME 2025, ACE composes seamlessly with existing methods and consistently improves the full Pass@k spectrum across all three model families and benchmarks.

</details>


### [132] [On the Structural Non-Preservation of Epistemic Behaviour under Policy Transformation](https://arxiv.org/abs/2602.21424)
*Alexander Galozy*

Main category: cs.LG

TL;DR: The paper formalizes behavioral dependency in RL under partial observability, showing that probe-conditioned behavioral separation is not preserved under convex policy aggregation or gradient optimization with skewed priors.


<details>
  <summary>Details</summary>
Motivation: RL agents under partial observability condition actions on internal information (memory/latent context), but it's unclear how such behavioral dependencies are affected by common policy transformations like convex aggregation or gradient optimization.

Method: Formalizes behavioral dependency as action variation with respect to internal information under fixed observations. Defines ε-behavioral equivalence and within-policy behavioral distance. Proves three structural results about convex aggregation, distance contraction, and gradient optimization conditions. Tests with minimal bandit and partially observable gridworld experiments.

Result: 1) Policies with non-trivial behavioral dependency are not closed under convex aggregation. 2) Behavioral distance contracts under convex combination. 3) Gradient ascent on skewed mixture objective decreases behavioral distance when dominant-mode gradient aligns with contraction direction. Experiments confirm behavioral distance decreases under convex aggregation and skewed prior optimization, preceding degradation under latent prior shift.

Conclusion: Probe-conditioned behavioral separation is not preserved under common policy transformations (convex aggregation, gradient optimization with skewed priors), identifying structural conditions where internal information dependencies break down during policy learning and combination.

Abstract: Reinforcement learning (RL) agents under partial observability often condition actions on internally accumulated information such as memory or inferred latent context. We formalise such information-conditioned interaction patterns as behavioural dependency: variation in action selection with respect to internal information under fixed observations. This induces a probe-relative notion of $ε$-behavioural equivalence and a within-policy behavioural distance that quantifies probe sensitivity. We establish three structural results. First, the set of policies exhibiting non-trivial behavioural dependency is not closed under convex aggregation. Second, behavioural distance contracts under convex combination. Third, we prove a sufficient local condition under which gradient ascent on a skewed mixture objective decreases behavioural distance when a dominant-mode gradient aligns with the direction of steepest contraction. Minimal bandit and partially observable gridworld experiments provide controlled witnesses of these mechanisms. In the examined settings, behavioural distance decreases under convex aggregation and under continued optimisation with skewed latent priors, and in these experiments it precedes degradation under latent prior shift. These results identify structural conditions under which probe-conditioned behavioural separation is not preserved under common policy transformations.

</details>


### [133] [Proximal-IMH: Proximal Posterior Proposals for Independent Metropolis-Hastings with Approximate Operators](https://arxiv.org/abs/2602.21426)
*Youguang Chen,George Biros*

Main category: cs.LG

TL;DR: Proximal-IMH: A Metropolis-Hastings sampler that corrects biased approximate posteriors via proximal optimization, improving acceptance rates for Bayesian inverse problems.


<details>
  <summary>Details</summary>
Motivation: Bayesian inverse problems often require sampling from expensive posterior distributions. While cheaper approximate posteriors exist, they introduce bias that affects sampling quality.

Method: Proximal-IMH uses an independence Metropolis-Hastings framework with a proximal correction step that optimizes a trade-off between exact model adherence and stability around approximate reference points.

Result: Theoretical analysis shows proximal correction tightens match between approximate and exact posteriors, improving acceptance rates and mixing. Numerical experiments demonstrate Proximal-IMH outperforms existing IMH variants.

Conclusion: Proximal-IMH provides an effective bias-correction method for sampling from expensive posteriors in Bayesian inverse problems, particularly suitable for problems with nonlinear operators and multimodal/data-driven priors.

Abstract: We consider the problem of sampling from a posterior distribution arising in Bayesian inverse problems in science, engineering, and imaging. Our method belongs to the family of independence Metropolis-Hastings (IMH) sampling algorithms, which are common in Bayesian inference. Relying on the existence of an approximate posterior distribution that is cheaper to sample from but may have significant bias, we introduce Proximal-IMH, a scheme that removes this bias by correcting samples from the approximate posterior through an auxiliary optimization problem. This yields a local adjustment that trades off adherence to the exact model against stability around the approximate reference point. For idealized settings, we prove that the proximal correction tightens the match between approximate and exact posteriors, thereby improving acceptance rates and mixing. The method applies to both linear and nonlinear input-output operators and is particularly suitable for inverse problems where exact posterior sampling is too expensive. We present numerical experiments including multimodal and data-driven priors with nonlinear input-output operators. The results show that Proximal-IMH reliably outperforms existing IMH variants.

</details>


### [134] [Provably Safe Generative Sampling with Constricting Barrier Functions](https://arxiv.org/abs/2602.21429)
*Darshan Gadginmath,Ahmed Allibhoy,Fabio Pasqualetti*

Main category: cs.LG

TL;DR: A safety filtering framework for flow-based generative models that guarantees constraint satisfaction through progressive safety tubes and control barrier functions, requiring no model retraining.


<details>
  <summary>Details</summary>
Motivation: Flow-based generative models lack formal guarantees for constraint satisfaction in safety-critical domains, limiting their deployment in applications where hard constraints must be satisfied.

Method: Proposes a safety filtering framework that cooperates with generative processes using constricting safety tubes defined via Control Barrier Functions (CBFs). The framework synthesizes feedback control through convex Quadratic Programming at each sampling step, with tubes that are relaxed at initial noise distribution and tighten progressively to target safe sets.

Result: Achieves 100% constraint satisfaction while preserving semantic fidelity across constrained image generation, physically-consistent trajectory sampling, and safe robotic manipulation policies. The framework guarantees safe sampling while minimizing distributional shift from original models.

Conclusion: The proposed safety filtering framework provides formal guarantees for constraint satisfaction in flow-based generative models without requiring retraining or architectural modifications, enabling their deployment in safety-critical applications.

Abstract: Flow-based generative models, such as diffusion models and flow matching models, have achieved remarkable success in learning complex data distributions. However, a critical gap remains for their deployment in safety-critical domains: the lack of formal guarantees that generated samples will satisfy hard constraints. We address this by proposing a safety filtering framework that acts as an online shield for any pre-trained generative model. Our key insight is to cooperate with the generative process rather than override it. We define a constricting safety tube that is relaxed at the initial noise distribution and progressively tightens to the target safe set at the final data distribution, mirroring the coarse-to-fine structure of the generative process itself. By characterizing this tube via Control Barrier Functions (CBFs), we synthesize a feedback control input through a convex Quadratic Program (QP) at each sampling step. As the tube is loosest when noise is high and intervention is cheapest in terms of control energy, most constraint enforcement occurs when it least disrupts the model's learned structure. We prove that this mechanism guarantees safe sampling while minimizing the distributional shift from the original model at each sampling step, as quantified by the KL divergence. Our framework applies to any pre-trained flow-based generative scheme requiring no retraining or architectural modifications. We validate the approach across constrained image generation, physically-consistent trajectory sampling, and safe robotic manipulation policies, achieving 100% constraint satisfaction while preserving semantic fidelity.

</details>


### [135] [Causal Decoding for Hallucination-Resistant Multimodal Large Language Models](https://arxiv.org/abs/2602.21441)
*Shiwei Tan,Hengyi Wang,Weiyi Qin,Qi Xu,Zhigang Hua,Hao Wang*

Main category: cs.LG

TL;DR: Proposes a causal decoding framework that applies targeted interventions during generation to reduce object hallucination in MLLMs while maintaining descriptive quality.


<details>
  <summary>Details</summary>
Motivation: MLLMs suffer from object hallucination (introducing objects not present in images), which undermines reliability. Existing approaches use heuristic penalties, post-hoc correction, or generic decoding tweaks that don't directly address the underlying mechanisms causing hallucination.

Method: A causal decoding framework that applies targeted causal interventions during generation to curb spurious object mentions. It reshapes decoding dynamics to attenuate spurious dependencies between tokens.

Result: Substantially lowers object-hallucination rates across captioning and QA benchmarks, achieving state-of-the-art faithfulness without degrading overall output quality.

Conclusion: The proposed causal intervention approach effectively reduces object hallucination in MLLMs by directly addressing the spurious dependencies that trigger false object mentions during generation.

Abstract: Multimodal Large Language Models (MLLMs) deliver detailed responses on vision-language tasks, yet remain susceptible to object hallucination (introducing objects not present in the image), undermining reliability in practice. Prior efforts often rely on heuristic penalties, post-hoc correction, or generic decoding tweaks, which do not directly intervene in the mechanisms that trigger object hallucination and thus yield limited gains. To address this challenge, we propose a causal decoding framework that applies targeted causal interventions during generation to curb spurious object mentions. By reshaping the decoding dynamics to attenuate spurious dependencies, our approach reduces false object tokens while maintaining descriptive quality. Across captioning and QA benchmarks, our framework substantially lowers object-hallucination rates and achieves state-of-the-art faithfulness without degrading overall output quality.

</details>


### [136] [MINAR: Mechanistic Interpretability for Neural Algorithmic Reasoning](https://arxiv.org/abs/2602.21442)
*Jesse He,Helen Jenne,Max Vargas,Davis Brown,Gal Mishne,Yusu Wang,Henry Kvinge*

Main category: cs.LG

TL;DR: MINAR is a circuit discovery toolbox that adapts attribution patching methods from mechanistic interpretability to GNNs for neural algorithmic reasoning, enabling recovery of faithful neuron-level circuits from GNNs trained on algorithmic tasks.


<details>
  <summary>Details</summary>
Motivation: Bridge two emerging fields: neural algorithmic reasoning (NAR) which studies GNNs' ability to emulate classical algorithms, and mechanistic interpretability from LLMs which identifies granular computational circuits. Apply mechanistic interpretability methods to understand how GNNs implement algorithmic reasoning.

Method: Develop MINAR toolbox that adapts attribution patching methods from mechanistic interpretability to the GNN setting. Use efficient circuit discovery techniques to identify neuron-level circuits in GNNs trained on algorithmic tasks.

Result: MINAR successfully recovers faithful neuron-level circuits from GNNs trained on algorithmic tasks. Provides insights into circuit formation and pruning during training, and reveals how GNNs reuse circuit components for related tasks when trained on multiple parallel tasks.

Conclusion: MINAR enables mechanistic analysis of GNNs in algorithmic reasoning, offering new understanding of circuit formation, pruning, and component reuse in multi-task learning. The toolbox provides a foundation for deeper analysis of how GNNs implement algorithmic computations.

Abstract: The recent field of neural algorithmic reasoning (NAR) studies the ability of graph neural networks (GNNs) to emulate classical algorithms like Bellman-Ford, a phenomenon known as algorithmic alignment. At the same time, recent advances in large language models (LLMs) have spawned the study of mechanistic interpretability, which aims to identify granular model components like circuits that perform specific computations. In this work, we introduce Mechanistic Interpretability for Neural Algorithmic Reasoning (MINAR), an efficient circuit discovery toolbox that adapts attribution patching methods from mechanistic interpretability to the GNN setting. We show through two case studies that MINAR recovers faithful neuron-level circuits from GNNs trained on algorithmic tasks. Our study sheds new light on the process of circuit formation and pruning during training, as well as giving new insight into how GNNs trained to perform multiple tasks in parallel reuse circuit components for related tasks. Our code is available at https://github.com/pnnl/MINAR.

</details>


### [137] [When Learning Hurts: Fixed-Pole RNN for Real-Time Online Training](https://arxiv.org/abs/2602.21454)
*Alexander Morgan,Ummay Sumaya Khan,Lingjia Liu,Lizhong Zheng*

Main category: cs.LG

TL;DR: Fixed-pole RNNs outperform learnable-pole RNNs in data-constrained real-time scenarios due to better stability and lower training complexity.


<details>
  <summary>Details</summary>
Motivation: While RNNs can learn all parameters including recurrent poles via BPTT, this joint learning is computationally expensive and impractical for limited training data scenarios. Echo state networks fix recurrent dynamics to enable efficient online adaptation, but it's unclear why learning poles doesn't provide benefits in real-time settings.

Method: Analytical and empirical examination of why learning recurrent poles is ineffective in data-constrained scenarios. Analysis shows pole learning creates highly non-convex optimization problems requiring more training samples. Empirical evaluation compares fixed-pole vs. learnable-pole architectures, particularly for complex-valued data.

Result: Pole learning renders optimization highly non-convex, requiring significantly more training data and iterations. Gradient descent exhibits prolonged plateaus for complex-valued data, with limited improvement from advanced optimizers. Fixed-pole architectures induce stable, well-conditioned state representations even with limited data.

Conclusion: Fixed-pole networks achieve superior performance with lower training complexity, making them more suitable for online real-time tasks compared to learnable-pole RNNs in data-constrained scenarios.

Abstract: Recurrent neural networks (RNNs) can be interpreted as discrete-time state-space models, where the state evolution corresponds to an infinite-impulse-response (IIR) filtering operation governed by both feedforward weights and recurrent poles. While, in principle, all parameters including pole locations can be optimized via backpropagation through time (BPTT), such joint learning incurs substantial computational overhead and is often impractical for applications with limited training data. Echo state networks (ESNs) mitigate this limitation by fixing the recurrent dynamics and training only a linear readout, enabling efficient and stable online adaptation. In this work, we analytically and empirically examine why learning recurrent poles does not provide tangible benefits in data-constrained, real-time learning scenarios. Our analysis shows that pole learning renders the weight optimization problem highly non-convex, requiring significantly more training samples and iterations for gradient-based methods to converge to meaningful solutions. Empirically, we observe that for complex-valued data, gradient descent frequently exhibits prolonged plateaus, and advanced optimizers offer limited improvement. In contrast, fixed-pole architectures induce stable and well-conditioned state representations even with limited training data. Numerical results demonstrate that fixed-pole networks achieve superior performance with lower training complexity, making them more suitable for online real-time tasks.

</details>


### [138] [Effects of Training Data Quality on Classifier Performance](https://arxiv.org/abs/2602.21462)
*Alan F. Karr,Regina Ruane*

Main category: cs.LG

TL;DR: Paper investigates how classifier performance degrades with training data quality deterioration, using metagenomic assembly as test case with four classifiers showing similar breakdown patterns.


<details>
  <summary>Details</summary>
Motivation: To systematically assess and quantify how classifier performance depends on training data quality, which is often neglected in classifier analysis, particularly in scientific contexts like metagenomic assembly.

Method: Extensive numerical experiments using metagenomic assembly of short DNA reads into contigs, degrading training data quality through multiple mechanisms, testing four classifiers (Bayes classifiers, neural nets, partition models, random forests), and examining both individual behavior and classifier congruence.

Result: All four classifiers exhibit similar breakdown-like behavior as degradation increases - they move from mostly correct to only coincidentally correct because they make the same errors. As training data moves farther from analysis data, classifier decisions degenerate, decision boundaries become less dense, and congruence among classifiers increases.

Conclusion: Training data quality critically impacts classifier performance across different algorithms, with consistent degradation patterns observed. The findings highlight the importance of considering training data quality in classifier analysis and reveal spatial heterogeneity in how classifiers fail as data quality deteriorates.

Abstract: We describe extensive numerical experiments assessing and quantifying how classifier performance depends on the quality of the training data, a frequently neglected component of the analysis of classifiers.
  More specifically, in the scientific context of metagenomic assembly of short DNA reads into "contigs," we examine the effects of degrading the quality of the training data by multiple mechanisms, and for four classifiers -- Bayes classifiers, neural nets, partition models and random forests. We investigate both individual behavior and congruence among the classifiers. We find breakdown-like behavior that holds for all four classifiers, as degradation increases and they move from being mostly correct to only coincidentally correct, because they are wrong in the same way. In the process, a picture of spatial heterogeneity emerges: as the training data move farther from analysis data, classifier decisions degenerate, the boundary becomes less dense, and congruence increases.

</details>


### [139] [Asymptotically Fast Clebsch-Gordan Tensor Products with Vector Spherical Harmonics](https://arxiv.org/abs/2602.21466)
*YuQing Xie,Ameya Daigavane,Mit Kotak,Tess Smidt*

Main category: cs.LG

TL;DR: First complete algorithm for asymptotically faster Clebsch-Gordan tensor products in E(3)-equivariant neural networks, reducing runtime from O(L⁶) to O(L⁴log²L).


<details>
  <summary>Details</summary>
Motivation: Existing speedups for tensor products in E(3)-equivariant networks often reduce expressivity rather than providing true algorithmic improvements. The Gaunt tensor product offers asymptotic speedup but is incomplete and misses many interactions.

Method: Generalize fast Fourier-based convolution to tensor spherical harmonics, prove generalized Gaunt formula, and extend to vector-valued signals to recover missing interactions from Gaunt tensor product.

Result: Achieved O(L⁴log²L) runtime complexity for full Clebsch-Gordan tensor products, close to the theoretical lower bound of O(L⁴), representing the first complete algorithm with true asymptotic benefits.

Conclusion: The work provides a complete, asymptotically efficient algorithm for Clebsch-Gordan tensor products that maintains full expressivity while achieving near-optimal computational complexity.

Abstract: $E(3)$-equivariant neural networks have proven to be effective in a wide range of 3D modeling tasks. A fundamental operation of such networks is the tensor product, which allows interaction between different feature types. Because this operation scales poorly, there has been considerable work towards accelerating this interaction. However, recently \citet{xieprice} have pointed out that most speedups come from a reduction in expressivity rather than true algorithmic improvements on computing Clebsch-Gordan tensor products. A modification of Gaunt tensor product \citep{gaunt} can give a true asymptotic speedup but is incomplete and misses many interactions. In this work, we provide the first complete algorithm which truly provides asymptotic benefits Clebsch-Gordan tensor products. For full CGTP, our algorithm brings runtime complexity from the naive $O(L^6)$ to $O(L^4\log^2 L)$, close to the lower bound of $O(L^4)$. We first show how generalizing fast Fourier based convolution naturally leads to the previously proposed Gaunt tensor product \citep{gaunt}. To remedy antisymmetry issues, we generalize from scalar signals to irrep valued signals, giving us tensor spherical harmonics. We prove a generalized Gaunt formula for the tensor harmonics. Finally, we show that we only need up to vector valued signals to recover the missing interactions of Gaunt tensor product.

</details>


### [140] [Geometric Priors for Generalizable World Models via Vector Symbolic Architecture](https://arxiv.org/abs/2602.21467)
*William Youngwoo Chung,Calvin Yeung,Hansen Jin Lillemark,Zhuowen Zou,Xiangjian Liu,Mohsen Imani*

Main category: cs.LG

TL;DR: A world model using Vector Symbolic Architecture with Fourier Holographic Reduced Representations achieves strong generalization, compositionality, and interpretability by learning structured latent representations with group theoretic foundations.


<details>
  <summary>Details</summary>
Motivation: Current world models use unstructured neural networks for transition functions, which limits interpretability, sample efficiency, and generalization to unseen states or action compositions. There's a need for more structured, generalizable representations that capture underlying world dynamics.

Method: Uses Vector Symbolic Architecture principles as geometric priors with learnable Fourier Holographic Reduced Representation (FHRR) encoders to map states and actions into high-dimensional complex vector space. Models transitions with element-wise complex multiplication, formalizing the framework's group theoretic foundation to train structured representations with approximate invariance.

Result: On discrete grid world: 87.5% zero-shot accuracy on unseen state-action pairs, 53.6% higher accuracy on 20-timestep horizon rollouts, and 4x higher robustness to noise compared to MLP baseline. Enables strong multi-step composition directly in latent space.

Conclusion: Training world models with latent group structure yields generalizable, data-efficient, and interpretable representations, providing a principled pathway toward structured models for real-world planning and reasoning.

Abstract: A key challenge in artificial intelligence and neuroscience is understanding how neural systems learn representations that capture the underlying dynamics of the world. Most world models represent the transition function with unstructured neural networks, limiting interpretability, sample efficiency, and generalization to unseen states or action compositions. We address these issues with a generalizable world model grounded in Vector Symbolic Architecture (VSA) principles as geometric priors. Our approach utilizes learnable Fourier Holographic Reduced Representation (FHRR) encoders to map states and actions into a high dimensional complex vector space with learned group structure and models transitions with element-wise complex multiplication. We formalize the framework's group theoretic foundation and show how training such structured representations to be approximately invariant enables strong multi-step composition directly in latent space and generalization performances over various experiments. On a discrete grid world environment, our model achieves 87.5% zero shot accuracy to unseen state-action pairs, obtains 53.6% higher accuracy on 20-timestep horizon rollouts, and demonstrates 4x higher robustness to noise relative to an MLP baseline. These results highlight how training to have latent group structure yields generalizable, data-efficient, and interpretable world models, providing a principled pathway toward structured models for real-world planning and reasoning.

</details>


### [141] [GradAlign: Gradient-Aligned Data Selection for LLM Reinforcement Learning](https://arxiv.org/abs/2602.21492)
*Ningyuan Yang,Weihua Du,Weiwei Sun,Sean Welleck,Yiming Yang*

Main category: cs.LG

TL;DR: GradAlign is a gradient-aligned data selection method for LLM reinforcement learning that uses validation gradients to prioritize training problems, improving stability and performance in challenging data regimes.


<details>
  <summary>Details</summary>
Motivation: RL for LLMs is highly sensitive to training problem quality due to non-stationarity, and existing methods rely on manual curation or simple heuristics that can admit incorrect or low-utility problems.

Method: GradAlign uses a small trusted validation set to prioritize training problems whose policy gradients align with validation gradients, creating an adaptive curriculum for RL training.

Result: GradAlign consistently outperforms existing baselines across three challenging data regimes: unreliable reward signals, distribution imbalance, and low-utility training corpus.

Conclusion: The method demonstrates the importance of directional gradient signals in navigating non-stationary policy optimization, yielding more stable training and improved final performance.

Abstract: Reinforcement learning (RL) has become a central post-training paradigm for large language models (LLMs), but its performance is highly sensitive to the quality of training problems. This sensitivity stems from the non-stationarity of RL: rollouts are generated by an evolving policy, and learning is shaped by exploration and reward feedback, unlike supervised fine-tuning (SFT) with fixed trajectories. As a result, prior work often relies on manual curation or simple heuristic filters (e.g., accuracy), which can admit incorrect or low-utility problems. We propose GradAlign, a gradient-aligned data selection method for LLM reinforcement learning that uses a small, trusted validation set to prioritize training problems whose policy gradients align with validation gradients, yielding an adaptive curriculum. We evaluate GradAlign across three challenging data regimes: unreliable reward signals, distribution imbalance, and low-utility training corpus, showing that GradAlign consistently outperforms existing baselines, underscoring the importance of directional gradient signals in navigating non-stationary policy optimization and yielding more stable training and improved final performance. We release our implementation at https://github.com/StigLidu/GradAlign

</details>


### [142] [D-Flow SGLD: Source-Space Posterior Sampling for Scientific Inverse Problems with Flow Matching](https://arxiv.org/abs/2602.21469)
*Meet Hemant Parikh,Yaqin Chen,Jian-Xun Wang*

Main category: cs.LG

TL;DR: D-Flow SGLD: A training-free conditional generation method for scientific inverse problems using Flow Matching priors, enabling posterior sampling via source-space inference with stochastic gradient Langevin dynamics.


<details>
  <summary>Details</summary>
Motivation: Scientific inverse problems require reconstructing high-dimensional physical states from sparse noisy observations with uncertainty-aware posterior samples that remain faithful to learned priors and physics. While diffusion models have well-developed training-free conditional generation, corresponding methods for Flow Matching priors remain under-explored for scientific benchmarks.

Method: Organizes existing inference-time strategies into two families: (1) guided transport dynamics that perturb sampling trajectories using likelihood information, and (2) source-distribution inference that performs posterior inference over the source variable while keeping learned transport fixed. Proposes D-Flow SGLD, which augments differentiable source inference with preconditioned stochastic gradient Langevin dynamics for scalable exploration of source posterior induced by new measurement operators.

Result: Benchmarks representative methods on 2D toy posteriors, chaotic Kuramoto-Sivashinsky trajectories, and wall-bounded turbulence reconstruction. Quantifies trade-offs among measurement assimilation, posterior diversity, and physics/statistics fidelity. Establishes D-Flow SGLD as a practical FM-compatible posterior sampler for scientific inverse problems.

Conclusion: D-Flow SGLD enables scalable exploration of source posterior without retraining the prior or modifying learned FM dynamics, making it a practical solution for scientific inverse problems with Flow Matching priors.

Abstract: Data assimilation and scientific inverse problems require reconstructing high-dimensional physical states from sparse and noisy observations, ideally with uncertainty-aware posterior samples that remain faithful to learned priors and governing physics. While training-free conditional generation is well developed for diffusion models, corresponding conditioning and posterior sampling strategies for Flow Matching (FM) priors remain comparatively under-explored, especially on scientific benchmarks where fidelity must be assessed beyond measurement misfit. In this work, we study training-free conditional generation for scientific inverse problems under FM priors and organize existing inference-time strategies by where measurement information is injected: (i) guided transport dynamics that perturb sampling trajectories using likelihood information, and (ii) source-distribution inference that performs posterior inference over the source variable while keeping the learned transport fixed. Building on the latter, we propose D-Flow SGLD, a source-space posterior sampling method that augments differentiable source inference with preconditioned stochastic gradient Langevin dynamics, enabling scalable exploration of the source posterior induced by new measurement operators without retraining the prior or modifying the learned FM dynamics. We benchmark representative methods from both families on a hierarchy of problems: 2D toy posteriors, chaotic Kuramoto-Sivashinsky trajectories, and wall-bounded turbulence reconstruction. Across these settings, we quantify trade-offs among measurement assimilation, posterior diversity, and physics/statistics fidelity, and establish D-Flow SGLD as a practical FM-compatible posterior sampler for scientific inverse problems.

</details>


### [143] [Training Generalizable Collaborative Agents via Strategic Risk Aversion](https://arxiv.org/abs/2602.21515)
*Chengrui Qu,Yizhou Zhang,Nicholas Lanzetti,Eric Mazumdar*

Main category: cs.LG

TL;DR: The paper proposes strategic risk aversion as an inductive bias for learning robust collaborative policies that work with unseen partners, addressing brittleness and free-riding in multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for learning collaborative policies produce brittle solutions that fail when paired with new partners, due to free-riding during training and lack of strategic robustness.

Method: The authors develop a multi-agent reinforcement learning (MARL) algorithm that integrates strategic risk aversion into standard policy optimization methods, interpreting it as a principled inductive bias for generalizable cooperation.

Result: Empirical results across collaborative benchmarks (including an LLM collaboration task) show the approach consistently achieves reliable collaboration with heterogeneous and previously unseen partners across tasks.

Conclusion: Strategic risk aversion provides a robust framework for learning generalizable collaborative policies that work with diverse partners, outperforming classical game-theoretic concepts like Nash equilibrium in collaborative settings.

Abstract: Many emerging agentic paradigms require agents to collaborate with one another (or people) to achieve shared goals. Unfortunately, existing approaches to learning policies for such collaborative problems produce brittle solutions that fail when paired with new partners. We attribute these failures to a combination of free-riding during training and a lack of strategic robustness. To address these problems, we study the concept of strategic risk aversion and interpret it as a principled inductive bias for generalizable cooperation with unseen partners. While strategically risk-averse players are robust to deviations in their partner's behavior by design, we show that, in collaborative games, they also (1) can have better equilibrium outcomes than those at classical game-theoretic concepts like Nash, and (2) exhibit less or no free-riding. Inspired by these insights, we develop a multi-agent reinforcement learning (MARL) algorithm that integrates strategic risk aversion into standard policy optimization methods. Our empirical results across collaborative benchmarks (including an LLM collaboration task) validate our theory and demonstrate that our approach consistently achieves reliable collaboration with heterogeneous and previously unseen partners across collaborative tasks.

</details>


### [144] [The Design Space of Tri-Modal Masked Diffusion Models](https://arxiv.org/abs/2602.21472)
*Louis Bethune,Victor Turrisi,Bruno Kacper Mlodozeniec,Pau Rodriguez Lopez,Lokesh Boominathan,Nikhil Bhendawade,Amitis Shidani,Joris Pelemans,Theo X. Olausson,Devon Hjelm,Paul Dixon,Joao Monteiro,Pierre Ablin,Vishnu Banna,Arno Blaas,Nick Henderson,Kari Noriy,Dan Busbridge,Josh Susskind,Marco Cuturi,Irina Belousova,Luca Zappella,Russ Webb,Jason Ramapuram*

Main category: cs.LG

TL;DR: First tri-modal masked diffusion model pretrained from scratch on text, image-text, and audio-text data with systematic analysis of multimodal scaling laws, modality mixing, and novel SDE-based batch size reparameterization.


<details>
  <summary>Details</summary>
Motivation: Previous work focused on initializing/fine-tuning unimodal models for bimodal generation, but there's a need for a unified tri-modal approach trained from scratch to understand multimodal scaling behaviors and optimize diffusion models for multiple modalities.

Method: Developed tri-modal masked diffusion model pretrained from scratch on text, image-text, and audio-text data. Conducted systematic analysis of multimodal scaling laws, modality mixing ratios, noise schedules, and batch-size effects. Introduced novel SDE-based reparameterization to decouple physical and logical batch sizes.

Result: Pretrained a 3B-parameter tri-modal model on 6.4T tokens achieving strong results in text generation, text-to-image, and text-to-speech tasks. Provided optimized inference sampling defaults and eliminated need for tuning optimal batch size through novel reparameterization.

Conclusion: This work represents the largest-scale systematic open study of multimodal discrete diffusion models, demonstrating the capabilities of unified tri-modal design and providing valuable insights into scaling behaviors across multiple modalities.

Abstract: Discrete diffusion models have emerged as strong alternatives to autoregressive language models, with recent work initializing and fine-tuning a base unimodal model for bimodal generation. Diverging from previous approaches, we introduce the first tri-modal masked diffusion model pretrained from scratch on text, image-text, and audio-text data. We systematically analyze multimodal scaling laws, modality mixing ratios, noise schedules, and batch-size effects, and we provide optimized inference sampling defaults. Our batch-size analysis yields a novel stochastic differential equation (SDE)-based reparameterization that eliminates the need for tuning the optimal batch size as reported in recent work. This reparameterization decouples the physical batch size, often chosen based on compute constraints (GPU saturation, FLOP efficiency, wall-clock time), from the logical batch size, chosen to balance gradient variance during stochastic optimization. Finally, we pretrain a preliminary 3B-parameter tri-modal model on 6.4T tokens, demonstrating the capabilities of a unified design and achieving strong results in text generation, text-to-image tasks, and text-to-speech tasks. Our work represents the largest-scale systematic open study of multimodal discrete diffusion models conducted to date, providing insights into scaling behaviors across multiple modalities.

</details>


### [145] [From Basis to Basis: Gaussian Particle Representation for Interpretable PDE Operators](https://arxiv.org/abs/2602.21551)
*Zhihao Li,Yu Feng,Zhilu Lai,Wei Wang*

Main category: cs.LG

TL;DR: A novel Gaussian Particle Operator for PDE learning that uses Gaussian basis representations for interpretable, efficient fluid dynamics modeling with near-linear complexity.


<details>
  <summary>Details</summary>
Motivation: Current neural operators and Transformer-based models for learning PDE dynamics lack interpretability, struggle with localized high-frequency structures, and have quadratic computational costs in spatial samples.

Method: Represent fields with Gaussian basis atoms (centers, anisotropic scales, weights) forming compact visualizable states. Use Gaussian Particle Operator acting in modal space with learned Gaussian modal windows for Petrov-Galerkin measurement and PG Gaussian Attention for global cross-scale coupling.

Result: Achieves state-of-the-art competitive accuracy on standard PDE benchmarks and real datasets while providing intrinsic interpretability. The method is resolution-agnostic with near-linear complexity for fixed modal budget, supporting irregular geometries and seamless 2D-to-3D extension.

Conclusion: The Gaussian Particle Operator offers an interpretable, efficient alternative to black-box neural operators for PDE learning, combining competitive accuracy with geometric interpretability and computational efficiency.

Abstract: Learning PDE dynamics for fluids increasingly relies on neural operators and Transformer-based models, yet these approaches often lack interpretability and struggle with localized, high-frequency structures while incurring quadratic cost in spatial samples. We propose representing fields with a Gaussian basis, where learned atoms carry explicit geometry (centers, anisotropic scales, weights) and form a compact, mesh-agnostic, directly visualizable state. Building on this representation, we introduce a Gaussian Particle Operator that acts in modal space: learned Gaussian modal windows perform a Petrov-Galerkin measurement, and PG Gaussian Attention enables global cross-scale coupling. This basis-to-basis design is resolution-agnostic and achieves near-linear complexity in N for a fixed modal budget, supporting irregular geometries and seamless 2D-to-3D extension. On standard PDE benchmarks and real datasets, our method attains state-of-the-art competitive accuracy while providing intrinsic interpretability.

</details>


### [146] [Learning Recursive Multi-Scale Representations for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.21498)
*Boyuan Li,Zhen Liu,Yicheng Luo,Qianli Ma*

Main category: cs.LG

TL;DR: ReIMTS is a recursive multi-scale modeling approach for Irregular Multivariate Time Series forecasting that preserves original timestamps and sampling patterns without resampling, achieving 27.1% average performance improvement.


<details>
  <summary>Details</summary>
Motivation: Existing multi-scale IMTS methods often use resampling which alters original timestamps and disrupts valuable sampling pattern information. Irregular intervals in IMTS carry important sampling pattern information for learning temporal and variable dependencies.

Method: ReIMTS recursively splits each sample into subsamples with progressively shorter time periods while keeping timestamps unchanged. It uses an irregularity-aware representation fusion mechanism based on original sampling timestamps to capture global-to-local dependencies.

Result: Extensive experiments show an average performance improvement of 27.1% in forecasting tasks across different models and real-world datasets.

Conclusion: ReIMTS effectively preserves sampling pattern information in irregular multivariate time series through recursive multi-scale modeling without resampling, leading to significant forecasting improvements by capturing global-to-local dependencies.

Abstract: Irregular Multivariate Time Series (IMTS) are characterized by uneven intervals between consecutive timestamps, which carry sampling pattern information valuable and informative for learning temporal and variable dependencies. In addition, IMTS often exhibit diverse dependencies across multiple time scales. However, many existing multi-scale IMTS methods use resampling to obtain the coarse series, which can alter the original timestamps and disrupt the sampling pattern information. To address the challenge, we propose ReIMTS, a Recursive multi-scale modeling approach for Irregular Multivariate Time Series forecasting. Instead of resampling, ReIMTS keeps timestamps unchanged and recursively splits each sample into subsamples with progressively shorter time periods. Based on the original sampling timestamps in these long-to-short subsamples, an irregularity-aware representation fusion mechanism is proposed to capture global-to-local dependencies for accurate forecasting. Extensive experiments demonstrate an average performance improvement of 27.1\% in the forecasting task across different models and real-world datasets. Our code is available at https://github.com/Ladbaby/PyOmniTS.

</details>


### [147] [WaterVIB: Learning Minimal Sufficient Watermark Representations via Variational Information Bottleneck](https://arxiv.org/abs/2602.21508)
*Haoyuan He,Yu Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.LG

TL;DR: WaterVIB introduces a Variational Information Bottleneck framework for robust watermarking that learns minimal sufficient message statistics instead of overfitting to fragile cover textures, achieving superior resilience against regeneration-based AIGC attacks.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking methods are vulnerable to regeneration-based AIGC attacks because they entangle watermarks with high-frequency cover textures that get rewritten during generative purification. There's a need for robust watermarking that can withstand distribution-shifting attacks.

Method: WaterVIB reformulates the watermark encoder as an information sieve using Variational Information Bottleneck theory. Instead of learning fragile cover details, it forces the model to learn a Minimal Sufficient Statistic of the message, filtering out redundant cover nuances prone to generative shifts while retaining essential signals invariant to regeneration.

Result: Theoretical proof shows optimizing the information bottleneck is necessary for robustness against distribution-shifting attacks. Extensive experiments demonstrate WaterVIB significantly outperforms state-of-the-art methods, achieving superior zero-shot resilience against unknown diffusion-based editing attacks.

Conclusion: WaterVIB provides a theoretically grounded framework for robust watermarking that addresses the fundamental vulnerability of existing methods by learning invariant message representations rather than fragile cover textures, enabling effective intellectual property protection against modern AIGC attacks.

Abstract: Robust watermarking is critical for intellectual property protection, whereas existing methods face a severe vulnerability against regeneration-based AIGC attacks. We identify that existing methods fail because they entangle the watermark with high-frequency cover texture, which is susceptible to being rewritten during generative purification. To address this, we propose WaterVIB, a theoretically grounded framework that reformulates the encoder as an information sieve via the Variational Information Bottleneck. Instead of overfitting to fragile cover details, our approach forces the model to learn a Minimal Sufficient Statistic of the message. This effectively filters out redundant cover nuances prone to generative shifts, retaining only the essential signal invariant to regeneration. We theoretically prove that optimizing this bottleneck is a necessary condition for robustness against distribution-shifting attacks. Extensive experiments demonstrate that WaterVIB significantly outperforms state-of-the-art methods, achieving superior zero-shot resilience against unknown diffusion-based editing.

</details>


### [148] [Muon+: Towards Better Muon via One Additional Normalization Step](https://arxiv.org/abs/2602.21545)
*Ruijie Zhang,Yequan Zhao,Ziyue Liu,Zhengyang Wang,Zheng Zhang*

Main category: cs.LG

TL;DR: Muon+ enhances the Muon optimizer by adding normalization after orthogonalization, improving training performance across various model scales and architectures.


<details>
  <summary>Details</summary>
Motivation: To improve upon the promising Muon optimizer by making a simple yet effective enhancement that could provide consistent performance gains in large language model pre-training.

Method: Proposes Muon+ which adds an additional normalization step after the orthogonalization process in the original Muon optimizer, maintaining the core orthogonalization mechanism while enhancing stability.

Result: Extensive experiments show Muon+ consistently improves training and validation perplexity over Muon across GPT-style (130M-774M) and LLaMA-style (60M-1B) models, even at industrial-scale token-to-parameter ratios (~200).

Conclusion: Muon+ is a simple but effective enhancement to Muon that provides consistent performance improvements across diverse model scales and architectures, making it suitable for industrial-scale LLM pre-training.

Abstract: The Muon optimizer has demonstrated promising performance in pre-training large language models through gradient (or momentum) orthogonalization. In this work, we propose a simple yet effective enhancement to Muon, namely Muon+, which introduces an additional normalization step after orthogonalization. We demonstrate the effectiveness of Muon+ through extensive pre-training experiments across a wide range of model scales and architectures. Our evaluation includes GPT-style models ranging from 130M to 774M parameters and LLaMA-style models ranging from 60M to 1B parameters. We comprehensively evaluate the effectiveness of Muon+ in the compute-optimal training regime and further extend the token-to-parameter (T2P) ratio to an industrial level of $\approx 200$. Experimental results show that Muon+ provides a consistent boost on training and validation perplexity over Muon. We provide our code here: https://github.com/K1seki221/MuonPlus.

</details>


### [149] [Mamba Meets Scheduling: Learning to Solve Flexible Job Shop Scheduling with Efficient Sequence Modeling](https://arxiv.org/abs/2602.21546)
*Zhi Cao,Cong Zhang,Yaoxin Wu,Yaqing Hou,Hongwei Ge*

Main category: cs.LG

TL;DR: The paper introduces a Mamba-based architecture for Flexible Job Shop Problem (FJSP) that achieves faster solving speed and better performance than state-of-the-art learning-based methods by using linear-complexity state-space models instead of computationally intensive graph attention.


<details>
  <summary>Details</summary>
Motivation: Current learning-based methods for FJSP rely on localized feature extraction models that cannot capture overarching dependencies across operations and machines. Existing graph-attention-based frameworks are computationally intensive for FJSP problems.

Method: Proposes an encoder-decoder architecture using Mamba (state-space model with linear complexity). The encoder has dual Mamba blocks to separately extract operation and machine features. The decoder uses efficient cross-attention to learn interactive embeddings between operations and machines.

Result: The method achieves faster solving speed and surpasses state-of-the-art learning-based methods for FJSP across various benchmarks, demonstrating both efficiency and performance improvements.

Conclusion: Mamba-based architecture provides an effective solution for FJSP by enabling comprehensive sequence modeling with linear computational complexity, overcoming limitations of previous localized feature extraction and graph-attention methods.

Abstract: The Flexible Job Shop Problem (FJSP) is a well-studied combinatorial optimization problem with extensive applications for manufacturing and production scheduling. It involves assigning jobs to various machines to optimize criteria, such as minimizing total completion time. Current learning-based methods in this domain often rely on localized feature extraction models, limiting their capacity to capture overarching dependencies spanning operations and machines. This paper introduces an innovative architecture that harnesses Mamba, a state-space model with linear computational complexity, to facilitate comprehensive sequence modeling tailored for FJSP. In contrast to prevalent graph-attention-based frameworks that are computationally intensive for FJSP, we show our model is more efficient. Specifically, the proposed model possesses an encoder and a decoder. The encoder incorporates a dual Mamba block to extract operation and machine features separately. Additionally, we introduce an efficient cross-attention decoder to learn interactive embeddings of operations and machines. Our experimental results demonstrate that our method achieves faster solving speed and surpasses the performance of state-of-the-art learning-based methods for FJSP across various benchmarks.

</details>


### [150] [Extending Sequence Length is Not All You Need: Effective Integration of Multimodal Signals for Gene Expression Prediction](https://arxiv.org/abs/2602.21550)
*Zhao Yang,Yi Duan,Jiwei Zhu,Ying Ba,Chuan Cao,Bing Su*

Main category: cs.LG

TL;DR: Prism framework uses backdoor adjustment on multimodal epigenomic signals to achieve SOTA gene expression prediction with short sequences, overcoming limitations of long-sequence modeling.


<details>
  <summary>Details</summary>
Motivation: Previous gene expression prediction models focus on extending input sequence length to capture distal enhancers, but this approach can actually decrease performance. The authors find that proximal multimodal epigenomic signals near target genes are more essential, but current integration methods overlook how different signal types serve distinct biological roles, with some reflecting background chromatin patterns that create confounding effects.

Method: Proposed Prism framework learns multiple combinations of high-dimensional epigenomic features to represent distinct background chromatin states and uses backdoor adjustment to mitigate confounding effects from background patterns. This enables better integration of multimodal epigenomic signals without relying on long sequence modeling.

Result: Prism achieves state-of-the-art performance for gene expression prediction using only short sequences, demonstrating that proper modeling of multimodal epigenomic signals is more effective than extending sequence length.

Conclusion: Focusing on better integration of proximal multimodal epigenomic signals through confounding effect mitigation is more effective than extending sequence length for gene expression prediction, challenging the conventional approach of long-sequence modeling.

Abstract: Gene expression prediction, which predicts mRNA expression levels from DNA sequences, presents significant challenges. Previous works often focus on extending input sequence length to locate distal enhancers, which may influence target genes from hundreds of kilobases away. Our work first reveals that for current models, long sequence modeling can decrease performance. Even carefully designed algorithms only mitigate the performance degradation caused by long sequences. Instead, we find that proximal multimodal epigenomic signals near target genes prove more essential. Hence we focus on how to better integrate these signals, which has been overlooked. We find that different signal types serve distinct biological roles, with some directly marking active regulatory elements while others reflect background chromatin patterns that may introduce confounding effects. Simple concatenation may lead models to develop spurious associations with these background patterns. To address this challenge, we propose Prism, a framework that learns multiple combinations of high-dimensional epigenomic features to represent distinct background chromatin states and uses backdoor adjustment to mitigate confounding effects. Our experimental results demonstrate that proper modeling of multimodal epigenomic signals achieves state-of-the-art performance using only short sequences for gene expression prediction.

</details>


### [151] [Training-free Composition of Pre-trained GFlowNets for Multi-Objective Generation](https://arxiv.org/abs/2602.21565)
*Seokwon Yoon,Youngbin Choi,Seunghyuk Cho,Seungbeom Lee,MoonJeong Park,Dongwoo Kim*

Main category: cs.LG

TL;DR: Training-free mixing policy for multi-objective GFlowNets that composes pre-trained models at inference time without retraining, handling both linear and non-linear reward combinations.


<details>
  <summary>Details</summary>
Motivation: Real-world applications often involve multiple conflicting objectives, but existing multi-objective GFlowNet approaches require additional training for each objective set, limiting applicability and incurring substantial computational overhead.

Method: Propose a training-free mixing policy that composes pre-trained GFlowNets at inference time, enabling rapid adaptation without finetuning. Framework handles diverse reward combinations from linear scalarization to complex non-linear logical operators.

Result: Method exactly recovers target distribution for linear scalarization and quantifies approximation quality for nonlinear operators through distortion factor. Experiments on synthetic 2D grid and real-world molecule generation show performance comparable to baselines requiring additional training.

Conclusion: Training-free mixing policy enables flexible, efficient multi-objective GFlowNet adaptation without retraining, supporting diverse reward combinations and achieving competitive performance with existing methods.

Abstract: Generative Flow Networks (GFlowNets) learn to sample diverse candidates in proportion to a reward function, making them well-suited for scientific discovery, where exploring multiple promising solutions is crucial. Further extending GFlowNets to multi-objective settings has attracted growing interest since real-world applications often involve multiple, conflicting objectives. However, existing approaches require additional training for each set of objectives, limiting their applicability and incurring substantial computational overhead. We propose a training-free mixing policy that composes pre-trained GFlowNets at inference time, enabling rapid adaptation without finetuning or retraining. Importantly, our framework is flexible, capable of handling diverse reward combinations ranging from linear scalarization to complex non-linear logical operators, which are often handled separately in previous literature. We prove that our method exactly recovers the target distribution for linear scalarization and quantify the approximation quality for nonlinear operators through a distortion factor. Experiments on a synthetic 2D grid and real-world molecule-generation tasks demonstrate that our approach achieves performance comparable to baselines that require additional training.

</details>


### [152] [Learning from Yesterday's Error: An Efficient Online Learning Method for Traffic Demand Prediction](https://arxiv.org/abs/2602.21757)
*Xiannan Huang,Quan Yuan,Chao Yang*

Main category: cs.LG

TL;DR: FORESEE is a lightweight online adaptation framework for traffic demand forecasting that corrects predictions using yesterday's errors with exponential smoothing and mixture-of-experts, requiring no model retraining.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for traffic forecasting degrade under distribution shifts from external events or urban dynamics, and frequent retraining is computationally expensive, especially for large models.

Method: FORESEE operates without updating base model parameters. It corrects today's forecast using yesterday's prediction error stabilized via exponential smoothing guided by mixture-of-experts, plus adaptive spatiotemporal smoothing across neighboring regions and time slots.

Result: Experiments on seven real-world datasets with three backbone models show FORESEE consistently improves accuracy, maintains robustness even with minimal distribution shifts, and has the lowest computational overhead among online methods.

Conclusion: FORESEE enables real-time adaptation of traffic forecasting models with negligible computational cost, facilitating deployment of reliable, up-to-date prediction systems in dynamic urban environments.

Abstract: Accurately predicting short-term traffic demand is critical for intelligent transportation systems. While deep learning models achieve strong performance under stationary conditions, their accuracy often degrades significantly when faced with distribution shifts caused by external events or evolving urban dynamics. Frequent model retraining to adapt to such changes incurs prohibitive computational costs, especially for large-scale or foundation models. To address this challenge, we propose FORESEE (Forecasting Online with Residual Smoothing and Ensemble Experts), a lightweight online adaptation framework that is accurate, robust, and computationally efficient. FORESEE operates without any parameter updates to the base model. Instead, it corrects today's forecast in each region using yesterday's prediction error, stabilized through exponential smoothing guided by a mixture-of-experts mechanism that adapts to recent error dynamics. Moreover, an adaptive spatiotemporal smoothing component propagates error signals across neighboring regions and time slots, capturing coherent shifts in demand patterns. Extensive experiments on seven real-world datasets with three backbone models demonstrate that FORESEE consistently improves prediction accuracy, maintains robustness even when distribution shifts are minimal (avoiding performance degradation), and achieves the lowest computational overhead among existing online methods. By enabling real-time adaptation of traffic forecasting models with negligible computational cost, FORESEE paves the way for deploying reliable, up-to-date prediction systems in dynamic urban environments. Code and data are available at https://github.com/xiannanhuang/FORESEE

</details>


### [153] [Duel-Evolve: Reward-Free Test-Time Scaling via LLM Self-Preferences](https://arxiv.org/abs/2602.21585)
*Sweta Karlekar,Carolina Zheng,Magnus Saebo,Nicolas Beltran-Velez,Shuyang Yu,John Bowlan,Michal Kucer,David Blei*

Main category: cs.LG

TL;DR: Duel-Evolve: Evolutionary optimization using LLM's own pairwise preferences instead of external scalar rewards for test-time improvement over discrete output spaces.


<details>
  <summary>Details</summary>
Motivation: Many tasks lack reliable scalar evaluators for LLM outputs - scores may be unavailable, sparse, or unreliable. Pairwise comparisons are easier to elicit, provide useful improvement signals, and can be obtained from the LLM itself without external supervision.

Method: Duel-Evolve replaces external scalar rewards with pairwise preferences elicited from the same LLM used to generate candidates. It aggregates noisy comparisons via Bayesian Bradley-Terry model for uncertainty-aware quality estimates, uses Double Thompson Sampling to allocate comparison budget toward plausible optima, and selects high-quality parents to generate improved candidates.

Result: On MathBench: 20 percentage points higher accuracy over existing methods and baselines. On LiveCodeBench: over 12 percentage points improvement over comparable iterative methods. The method requires no reward model, no ground-truth labels during search, and no hand-crafted scoring function.

Conclusion: Pairwise self-preferences provide strong optimization signal for test-time improvement over large, discrete output spaces, enabling effective LLM output optimization without external supervision or reliable scalar rewards.

Abstract: Many applications seek to optimize LLM outputs at test time by iteratively proposing, scoring, and refining candidates over a discrete output space. Existing methods use a calibrated scalar evaluator for the target objective to guide search, but for many tasks such scores are unavailable, too sparse, or unreliable. Pairwise comparisons, by contrast, are often easier to elicit, still provide useful signal on improvement directions, and can be obtained from the LLM itself without external supervision. Building on this observation, we introduce Duel-Evolve, an evolutionary optimization algorithm that replaces external scalar rewards with pairwise preferences elicited from the same LLM used to generate candidates. Duel-Evolve aggregates these noisy candidate comparisons via a Bayesian Bradley-Terry model, yielding uncertainty-aware estimates of candidate quality. These quality estimates guide allocation of the comparison budget toward plausible optima using Double Thompson Sampling, as well as selection of high-quality parents to generate improved candidates. We evaluate Duel-Evolve on MathBench, where it achieves 20 percentage points higher accuracy over existing methods and baselines, and on LiveCodeBench, where it improves over comparable iterative methods by over 12 percentage points. Notably, the method requires no reward model, no ground-truth labels during search, and no hand-crafted scoring function. Results show that pairwise self-preferences provide strong optimization signal for test-time improvement over large, discrete output spaces.

</details>


### [154] [Generalisation of RLHF under Reward Shift and Clipped KL Regularisation](https://arxiv.org/abs/2602.21765)
*Kenton Tang,Yuzhu Chen,Fengxiang He*

Main category: cs.LG

TL;DR: Theoretical analysis of RLHF generalization with reward shift and clipped KL regularization, providing error bounds and practical implications for optimal clipping and data allocation.


<details>
  <summary>Details</summary>
Motivation: RLHF is crucial for aligning LLMs but lacks theoretical understanding of generalization, especially when reward models shift and KL regularization is estimated/clipped.

Method: Develop generalization theory for RLHF accounting for: (1) reward shift between training and deployment policies, (2) clipped KL regularization from estimated log-probability ratios. Analyze special cases: uniform prior initialization and SGD training as Ornstein-Uhlenbeck process.

Result: Generalization bounds show error stems from: sampling error (prompts/rollouts), reward shift error, and KL clipping error. Theory yields practical implications for optimal KL clipping threshold and budget allocation across prompts, rollouts, and preference data.

Conclusion: Theoretical framework provides principled understanding of RLHF generalization, addressing practical challenges of reward shift and KL estimation, with actionable insights for improving RLHF training efficiency and stability.

Abstract: Alignment and adaptation in large language models heavily rely on reinforcement learning from human feedback (RLHF); yet, theoretical understanding of its generalisability remains premature, especially when the learned reward could shift, and the KL control is estimated and clipped. To address this issue, we develop generalisation theory for RLHF that explicitly accounts for (1) \emph{reward shift}: reward models are trained on preference data from earlier or mixed behaviour policies while RLHF optimises the current policy on its own rollouts; and (2) \emph{clipped KL regularisation}: the KL regulariser is estimated from sampled log-probability ratios and then clipped for stabilisation, resulting in an error to RLHF. We present generalisation bounds for RLHF, suggesting that the generalisation error stems from a sampling error from prompts and rollouts, a reward shift error, and a KL clipping error. We also discuss special cases of (1) initialising RLHF parameters with a uniform prior over a finite space, and (2) training RLHF by stochastic gradient descent, as an Ornstein-Uhlenbeck process. The theory yields practical implications in (1) optimal KL clipping threshold, and (2) budget allocation in prompts, rollouts, and preference data.

</details>


### [155] [ABM-UDE: Developing Surrogates for Epidemic Agent-Based Models via Scientific Machine Learning](https://arxiv.org/abs/2602.21588)
*Sharv Murgai,Utkarsh Utkarsh,Kyle C. Nguyen,Alan Edelman,Erin C. S. Acquesta,Christopher Vincent Rackauckas*

Main category: cs.LG

TL;DR: Researchers developed fast epidemic surrogates using Universal Differential Equations with neural-parameterized contact rates, achieving 77% error reduction and 10,000× speedup over agent-based models for hospital planning.


<details>
  <summary>Details</summary>
Motivation: Agent-based epidemic models are too slow for nightly hospital planning despite capturing behavioral heterogeneity, creating a need for fast, accurate surrogates that maintain mechanistic interpretability.

Method: Used Universal Differential Equations with neural-parameterized contact rates, adapted multiple shooting and prediction-error methods for stability, enforced positivity and mass conservation, and compared against ABM ensembles and UDE baselines.

Result: PEM-UDE reduced mean MSE by 77% vs single-shooting UDE and 20% vs MS-UDE, improved empirical coverage to 0.86/0.61, and enabled seconds-long inference (20-35s per 90-day forecast) vs 100 CPU-hour ABM runs.

Conclusion: The approach closes the realism-cadence gap for epidemic planning, enables calibrated scenario analysis on commodity hardware, and provides a portable framework for distilling agent-based simulators into fast surrogates across scientific domains.

Abstract: Agent-based epidemic models (ABMs) encode behavioral and policy heterogeneity but are too slow for nightly hospital planning. We develop county-ready surrogates that learn directly from exascale ABM trajectories using Universal Differential Equations (UDEs): mechanistic SEIR-family ODEs with a neural-parameterized contact rate $κ_φ(u,t)$ (no additive residual). Our contributions are threefold: we adapt multiple shooting and an observer-based prediction-error method (PEM) to stabilize identification of neural-augmented epidemiological dynamics across intervention-driven regime shifts; we enforce positivity and mass conservation and show the learned contact-rate parameterization yields a well-posed vector field; and we quantify accuracy, calibration, and compute against ABM ensembles and UDE baselines. On a representative ExaEpi scenario, PEM-UDE reduces mean MSE by 77% relative to single-shooting UDE (3.00 vs. 13.14) and by 20% relative to MS-UDE (3.75). Reliability improves in parallel: empirical coverage of ABM $10$-$90$% and $25$-$75$% bands rises from 0.68/0.43 (UDE) and 0.79/0.55 (MS-UDE) to 0.86/0.61 with PEM-UDE and 0.94/0.69 with MS+PEM-UDE, indicating calibrated uncertainty rather than overconfident fits. Inference runs in seconds on commodity CPUs (20-35 s per $\sim$90-day forecast), enabling nightly ''what-if'' sweeps on a laptop. Relative to a $\sim$100 CPU-hour ABM reference run, this yields $\sim10^{4}\times$ lower wall-clock per scenario. This closes the realism-cadence gap, supports threshold-aware decision-making (e.g., maintaining ICU occupancy $<75$%), preserves mechanistic interpretability, and enables calibrated, risk-aware scenario planning on standard institutional hardware. Beyond epidemics, the ABM$\to$UDE recipe provides a portable path to distill agent-based simulators into fast, trustworthy surrogates for other scientific domains.

</details>


### [156] [Breaking Semantic-Aware Watermarks via LLM-Guided Coherence-Preserving Semantic Injection](https://arxiv.org/abs/2602.21593)
*Zheng Gao,Xiaoyu Li,Zhicheng Bao,Xiaoyan Feng,Jiaojiao Jiang*

Main category: cs.LG

TL;DR: The paper introduces CSI attack that uses LLM-guided semantic manipulation to break content-aware semantic watermarking by making locally fine-grained but globally coherent semantic alterations.


<details>
  <summary>Details</summary>
Motivation: Content-aware semantic watermarking was developed to protect against inversion attacks on traditional noise-based watermarks, but the authors discovered that LLMs can perform targeted semantic manipulations that preserve global coherence while disrupting watermark bindings.

Method: The Coherence-Preserving Semantic Injection (CSI) attack leverages LLM-guided semantic manipulation under embedding-space similarity constraints to enforce visual-semantic consistency while selectively perturbing watermark-relevant semantics.

Result: Extensive empirical results show that CSI consistently outperforms prevailing attack baselines against content-aware semantic watermarking, revealing a fundamental security weakness.

Conclusion: Current semantic watermark designs have a fundamental security weakness when confronted with LLM-driven semantic perturbations, exposing an overlooked vulnerability in content-aware watermarking schemes.

Abstract: Generative images have proliferated on Web platforms in social media and online copyright distribution scenarios, and semantic watermarking has increasingly been integrated into diffusion models to support reliable provenance tracking and forgery prevention for web content. Traditional noise-layer-based watermarking, however, remains vulnerable to inversion attacks that can recover embedded signals. To mitigate this, recent content-aware semantic watermarking schemes bind watermark signals to high-level image semantics, constraining local edits that would otherwise disrupt global coherence. Yet, large language models (LLMs) possess structured reasoning capabilities that enable targeted exploration of semantic spaces, allowing locally fine-grained but globally coherent semantic alterations that invalidate such bindings. To expose this overlooked vulnerability, we introduce a Coherence-Preserving Semantic Injection (CSI) attack that leverages LLM-guided semantic manipulation under embedding-space similarity constraints. This alignment enforces visual-semantic consistency while selectively perturbing watermark-relevant semantics, ultimately inducing detector misclassification. Extensive empirical results show that CSI consistently outperforms prevailing attack baselines against content-aware semantic watermarking, revealing a fundamental security weakness of current semantic watermark designs when confronted with LLM-driven semantic perturbations.

</details>


### [157] [Excitation: Momentum For Experts](https://arxiv.org/abs/2602.21798)
*Sagi Shaier*

Main category: cs.LG

TL;DR: Excitation is a novel optimization framework that accelerates learning in sparse architectures like Mixture-of-Experts by dynamically modulating updates based on batch-level expert utilization, improving convergence speed and performance.


<details>
  <summary>Details</summary>
Motivation: Traditional optimizers treat all parameters uniformly, which is suboptimal for sparse architectures like MoEs. There's a phenomenon of "structural confusion" in deep MoEs where standard optimizers fail to establish functional signal paths, leading to unstable training.

Method: Excitation dynamically modulates parameter updates using batch-level expert utilization. It introduces competitive update dynamics that amplify updates to highly-utilized experts and can selectively suppress low-utilization ones, effectively sharpening routing specialization without adding learnable parameters or optimizer state.

Result: Excitation consistently improves convergence speed and final performance across language and vision tasks in MoE models. It acts as a specialization catalyst that "rescues" models from structural confusion, enabling stable training where baselines fail.

Conclusion: Active update modulation based on expert utilization is a key mechanism for effective conditional computation. Excitation is optimizer-, domain-, and model-agnostic, requires minimal integration, and is memory-efficient, making it highly viable for practical applications.

Abstract: We propose Excitation, a novel optimization framework designed to accelerate learning in sparse architectures such as Mixture-of-Experts (MoEs). Unlike traditional optimizers that treat all parameters uniformly, Excitation dynamically modulates updates using batch-level expert utilization. It introduces a competitive update dynamic that amplifies updates to highly-utilized experts and can selectively suppress low-utilization ones, effectively sharpening routing specialization. Notably, we identify a phenomenon of "structural confusion" in deep MoEs, where standard optimizers fail to establish functional signal paths; Excitation acts as a specialization catalyst, "rescuing" these models and enabling stable training where baselines remain trapped. Excitation is optimizer-, domain-, and model-agnostic, requires minimal integration effort, and introduces neither additional per-parameter optimizer state nor learnable parameters, making it highly viable for memory-constrained settings. Across language and vision tasks, Excitation consistently improves convergence speed and final performance in MoE models, indicating that active update modulation is a key mechanism for effective conditional computation.

</details>


### [158] [NGDB-Zoo: Towards Efficient and Scalable Neural Graph Databases Training](https://arxiv.org/abs/2602.21597)
*Zhongwei Xie,Jiaxin Bai,Shujie Liu,Haoyu Huang,Yufei Li,Yisen Gao,Hong Ting Tsang,Yangqiu Song*

Main category: cs.LG

TL;DR: NGDB-Zoo is a unified framework that improves neural graph database training efficiency and expressivity through operator-level training and semantic augmentation, achieving 1.8-6.8× throughput gains.


<details>
  <summary>Details</summary>
Motivation: Current Neural Graph Databases (NGDBs) face bottlenecks in training efficiency and expressivity due to rigid query-level batching and structure-exclusive embeddings, limiting their ability to handle complex logical reasoning over incomplete knowledge structures.

Method: NGDB-Zoo synergizes operator-level training with semantic augmentation by: 1) decoupling logical operators from query topologies for dynamic data-flow execution, 2) enabling multi-stream parallelism, and 3) integrating high-dimensional semantic priors from Pre-trained Text Encoders through a decoupled architecture that avoids I/O stalls and memory overflows.

Result: The framework achieves 1.8× to 6.8× throughput improvements compared to baselines, maintains high GPU utilization across diverse logical patterns, and significantly mitigates representation friction in hybrid neuro-symbolic reasoning on six benchmarks including massive graphs like ogbl-wikikg2 and ATLAS-Wiki.

Conclusion: NGDB-Zoo successfully resolves key bottlenecks in NGDB training by combining operator-level optimization with semantic augmentation, enabling more efficient and expressive complex logical reasoning over incomplete knowledge structures while maintaining practical scalability.

Abstract: Neural Graph Databases (NGDBs) facilitate complex logical reasoning over incomplete knowledge structures, yet their training efficiency and expressivity are constrained by rigid query-level batching and structure-exclusive embeddings. We present NGDB-Zoo, a unified framework that resolves these bottlenecks by synergizing operator-level training with semantic augmentation. By decoupling logical operators from query topologies, NGDB-Zoo transforms the training loop into a dynamically scheduled data-flow execution, enabling multi-stream parallelism and achieving a $1.8\times$ - $6.8\times$ throughput compared to baselines. Furthermore, we formalize a decoupled architecture to integrate high-dimensional semantic priors from Pre-trained Text Encoders (PTEs) without triggering I/O stalls or memory overflows. Extensive evaluations on six benchmarks, including massive graphs like ogbl-wikikg2 and ATLAS-Wiki, demonstrate that NGDB-Zoo maintains high GPU utilization across diverse logical patterns and significantly mitigates representation friction in hybrid neuro-symbolic reasoning.

</details>


### [159] [Deep Clustering based Boundary-Decoder Net for Inter and Intra Layer Stress Prediction of Heterogeneous Integrated IC Chip](https://arxiv.org/abs/2602.21601)
*Kart Leong Lim,Ji Lin*

Main category: cs.LG

TL;DR: Proposed deep generative model with boundary-decoder net and deep clustering for stress image prediction in 3D IC packages, outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: High stress occurs at material interfaces in 3D heterogeneous IC packages during thermal cycling, requiring accurate stress prediction methods. Traditional deep generative models require image pairing for training, which has limitations.

Method: Uses boundary-decoder (BD) net that maps material parameters to latent space shared with stress images, coupled with deep clustering to address dimensional ill-posedness. Trained on simulated IC chip dataset of 1825 stress images.

Result: Proposed approach outperforms variants of BD net and baseline methods in terms of train and test error reduction on the simulated IC chip dataset.

Conclusion: The boundary-decoder net with deep clustering provides an effective approach for stress image prediction in 3D IC packages, addressing limitations of unsupervised deep generative models that require image pairing.

Abstract: High stress occurs when 3D heterogeneous IC packages are subjected to thermal cycling at extreme temperatures. Stress mainly occurs at the interface between different materials. We investigate stress image using latent space representation which is based on using deep generative model (DGM). However, most DGM approaches are unsupervised, meaning they resort to image pairing (input and output) to train DGM. Instead, we rely on a recent boundary-decoder (BD) net, which uses boundary condition and image pairing for stress modeling. The boundary net maps material parameters to the latent space co-shared by its image counterpart. Because such a setup is dimensionally wise ill-posed, we further couple BD net with deep clustering. To access the performance of our proposed method, we simulate an IC chip dataset comprising of 1825 stress images. We compare our new approach using variants of BD net as well as a baseline approach. We show that our approach is able to outperform all the comparison in terms of train and test error reduction.

</details>


### [160] [AgentLTV: An Agent-Based Unified Search-and-Evolution Framework for Automated Lifetime Value Prediction](https://arxiv.org/abs/2602.21634)
*Chaowei Wu,Huazhu Chen,Congde Yuan,Qirui Yang,Guoqing Song,Yue Gao,Li Luo,Frank Youhua Chen,Mengzhuo Guo*

Main category: cs.LG

TL;DR: AgentLTV is an agent-based framework that uses LLM-driven agents to automatically search and evolve LTV prediction pipelines through Monte Carlo Tree Search and Evolutionary Algorithms, achieving strong performance across metrics and successful online deployment.


<details>
  <summary>Details</summary>
Motivation: LTV prediction is critical but requires expensive, scenario-specific pipelines that are hard to transfer across different data patterns and decision scenarios. Current practice involves complex manual iteration over feature processing, objective design, and tuning.

Method: AgentLTV treats candidate solutions as executable pipeline programs. LLM-driven agents generate code, run/repair pipelines, and analyze feedback. Two-stage search: 1) MCTS explores modeling choices under budget constraints using polynomial UCT and Pareto-aware multi-metric value, 2) Evolutionary Algorithm refines best programs via island-based evolution with crossover, mutation, and migration.

Result: Experiments on proprietary and public datasets show AgentLTV consistently discovers strong models across ranking and error metrics. Online bucket-level analysis shows improved ranking consistency and value calibration, especially for high-value and negative-LTV segments. Successfully deployed online.

Conclusion: AgentLTV provides automated LTV modeling with practitioner takeaways: use MCTS for rapid adaptation to new data patterns, EA for stable refinement, and validate deployment with bucket-level ranking/calibration diagnostics. Framework enables transferable, automated pipeline development.

Abstract: Lifetime Value (LTV) prediction is critical in advertising, recommender systems, and e-commerce. In practice, LTV data patterns vary across decision scenarios. As a result, practitioners often build complex, scenario-specific pipelines and iterate over feature processing, objective design, and tuning. This process is expensive and hard to transfer. We propose AgentLTV, an agent-based unified search-and-evolution framework for automated LTV modeling. AgentLTV treats each candidate solution as an {executable pipeline program}. LLM-driven agents generate code, run and repair pipelines, and analyze execution feedback. Two decision agents coordinate a two-stage search. The Monte Carlo Tree Search (MCTS) stage explores a broad space of modeling choices under a fixed budget, guided by the Polynomial Upper Confidence bounds for Trees criterion and a Pareto-aware multi-metric value function. The Evolutionary Algorithm (EA) stage refines the best MCTS program via island-based evolution with crossover, mutation, and migration. Experiments on a large-scale proprietary dataset and a public benchmark show that AgentLTV consistently discovers strong models across ranking and error metrics. Online bucket-level analysis further indicates improved ranking consistency and value calibration, especially for high-value and negative-LTV segments. We summarize practitioner-oriented takeaways: use MCTS for rapid adaptation to new data patterns, use EA for stable refinement, and validate deployment readiness with bucket-level ranking and calibration diagnostics. The proposed AgentLTV has been successfully deployed online.

</details>


### [161] [xai-cola: A Python library for sparsifying counterfactual explanations](https://arxiv.org/abs/2602.21845)
*Lin Zhu,Lei You*

Main category: cs.LG

TL;DR: xai-cola is an open-source Python library that sparsifies counterfactual explanations by reducing redundant feature changes while maintaining validity.


<details>
  <summary>Details</summary>
Motivation: Counterfactual explanations often contain high redundancy with many superfluous feature changes, making them less interpretable and practical for users.

Method: Provides an end-to-end pipeline that takes tabular data, preprocessing objects, and trained models, then applies sparsification policies to reduce feature changes in counterfactuals generated by arbitrary CE generators.

Result: Empirical experiments show xai-cola reduces modified features by up to 50% across several CE generators while preserving validity.

Conclusion: xai-cola offers a practical solution for generating sparser, more interpretable counterfactual explanations through an open-source library with visualization capabilities.

Abstract: Counterfactual explanation (CE) is an important domain within post-hoc explainability. However, the explanations generated by most CE generators are often highly redundant. This work introduces an open-source Python library xai-cola, which provides an end-to-end pipeline for sparsifying CEs produced by arbitrary generators, reducing superfluous feature changes while preserving their validity. It offers a documented API that takes as input raw tabular data in pandas DataFrame form, a preprocessing object (for standardization and encoding), and a trained scikit-learn or PyTorch model. On this basis, users can either employ the built-in or externally imported CE generators. The library also implements several sparsification policies and includes visualization routines for analysing and comparing sparsified counterfactuals. xai-cola is released under the MIT license and can be installed from PyPI. Empirical experiments indicate that xai-cola produces sparser counterfactuals across several CE generators, reducing the number of modified features by up to 50% in our setting. The source code is available at https://github.com/understanding-ml/COLA.

</details>


### [162] [Multimodal Survival Modeling and Fairness-Aware Clinical Machine Learning for 5-Year Breast Cancer Risk Prediction](https://arxiv.org/abs/2602.21648)
*Toktam Khatibi*

Main category: cs.LG

TL;DR: A reproducible multimodal ML framework for 5-year breast cancer survival prediction using clinical, transcriptomic, and CNA data from METABRIC, comparing CoxNet and XGBoost with strong performance and fairness across subgroups.


<details>
  <summary>Details</summary>
Motivation: Clinical risk prediction models often underperform in real-world settings due to poor calibration, limited transportability, and subgroup disparities, especially in high-dimensional multimodal cancer datasets with complex feature interactions and p >> n structure.

Method: A fully reproducible multimodal ML framework integrating clinical variables with high-dimensional transcriptomic and CNA features from METABRIC. After variance/sparsity filtering and dimensionality reduction, models were trained using stratified splits with validation-based hyperparameter tuning. Two survival approaches compared: elastic-net regularized Cox model (CoxNet) for feature selection and stable estimation, and XGBoost for capturing nonlinear effects and higher-order interactions.

Result: CoxNet achieved validation/test AUCs of 98.3/96.6 with AP values of 90.1/80.4. XGBoost achieved validation/test AUCs of 98.6/92.5 with AP values of 92.5/79.9. Fairness diagnostics showed stable discrimination across age groups, ER status, molecular subtypes, and menopausal state.

Conclusion: This work introduces a governance-oriented multimodal survival framework emphasizing calibration, fairness auditing, robustness, and reproducibility for high-dimensional clinical machine learning applications.

Abstract: Clinical risk prediction models often underperform in real-world settings due to poor calibration, limited transportability, and subgroup disparities. These challenges are amplified in high-dimensional multimodal cancer datasets characterized by complex feature interactions and a p >> n structure. We present a fully reproducible multimodal machine learning framework for 5-year overall survival prediction in breast cancer, integrating clinical variables with high-dimensional transcriptomic and copy-number alteration (CNA) features from the METABRIC cohort.
  After variance- and sparsity-based filtering and dimensionality reduction, models were trained using stratified train/validation/test splits with validation-based hyperparameter tuning. Two survival approaches were compared: an elastic-net regularized Cox model (CoxNet) and a gradient-boosted survival tree model implemented using XGBoost. CoxNet provides embedded feature selection and stable estimation, whereas XGBoost captures nonlinear effects and higher-order interactions.
  Performance was assessed using time-dependent area under the ROC curve (AUC), average precision (AP), calibration curves, Brier score, and bootstrapped 95 percent confidence intervals. CoxNet achieved validation and test AUCs of 98.3 and 96.6, with AP values of 90.1 and 80.4. XGBoost achieved validation and test AUCs of 98.6 and 92.5, with AP values of 92.5 and 79.9. Fairness diagnostics showed stable discrimination across age groups, estrogen receptor status, molecular subtypes, and menopausal state.
  This work introduces a governance-oriented multimodal survival framework emphasizing calibration, fairness auditing, robustness, and reproducibility for high-dimensional clinical machine learning.

</details>


### [163] [Error-awareness Accelerates Active Automata Learning](https://arxiv.org/abs/2602.21674)
*Loes Kruger,Sebastian Junges,Jurriaan Rot*

Main category: cs.LG

TL;DR: The paper presents adaptations of the L# active automata learning algorithm to efficiently handle systems where many inputs lead to observable errors, leveraging domain knowledge about error-producing inputs to accelerate learning by orders of magnitude.


<details>
  <summary>Details</summary>
Motivation: Modern active automata learning algorithms fail to scale to larger models when systems have many possible inputs, especially when most inputs lead to errors in every state. The authors are motivated by challenging problems from literature where these errors are observable (emit known error outputs), and aim to learn such systems more efficiently by leveraging domain knowledge about which inputs produce errors.

Method: The authors study learning systems with observable errors and consider various degrees of knowledge about which inputs are non-error producing at which state. For each level of domain knowledge, they provide matching adaptations of the state-of-the-art AAL algorithm L# to optimally utilize this knowledge. The adaptations focus on making the most of available domain knowledge about error behavior.

Result: Empirical evaluation demonstrates that the proposed methods accelerate learning by orders of magnitude with strong but realistic domain knowledge, and by a single order of magnitude with limited domain knowledge. The adaptations effectively leverage domain knowledge about error-producing inputs to significantly improve learning efficiency.

Conclusion: The paper successfully addresses the scalability challenge in active automata learning for systems with many error-producing inputs by adapting the L# algorithm to leverage domain knowledge about error behavior. The approach enables efficient learning of behavioral models even when most inputs lead to observable errors, with acceleration ranging from orders of magnitude to a single order depending on the available domain knowledge.

Abstract: Active automata learning (AAL) algorithms can learn a behavioral model of a system from interacting with it. The primary challenge remains scaling to larger models, in particular in the presence of many possible inputs to the system. Modern AAL algorithms fail to scale even if, in every state, most inputs lead to errors. In various challenging problems from the literature, these errors are observable, i.e., they emit a known error output. Motivated by these problems, we study learning these systems more efficiently. Further, we consider various degrees of knowledge about which inputs are non-error producing at which state. For each level of knowledge, we provide a matching adaptation of the state-of-the-art AAL algorithm L# to make the most of this domain knowledge. Our empirical evaluation demonstrates that the methods accelerate learning by orders of magnitude with strong but realistic domain knowledge to a single order of magnitude with limited domain knowledge.

</details>


### [164] [Hierarchical Lead Critic based Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.21680)
*David Eckel,Henri Meeß*

Main category: cs.LG

TL;DR: HLC is a hierarchical MARL architecture that combines local and global perspectives through sequential training, improving performance and sample efficiency in cooperative multi-agent tasks.


<details>
  <summary>Details</summary>
Motivation: Current MARL approaches are limited to either local (independent) or global (centralized) perspectives, failing to capture the natural hierarchical team structures that emerge in complex coordination tasks.

Method: Proposes Hierarchical Lead Critic (HLC) with sequential training scheme that learns from multiple perspectives across different hierarchy levels, combining high-level objective following with low-level execution inspired by natural team structures.

Result: HLC outperforms single hierarchy baselines, demonstrates improved performance with high sample efficiency and robust policies, and scales well with increasing agent counts and task difficulty across cooperative, non-communicative, and partially observable benchmarks.

Conclusion: Introducing multiple hierarchies that leverage both local and global perspectives leads to superior MARL performance, addressing limitations of existing approaches by better modeling natural team coordination structures.

Abstract: Cooperative Multi-Agent Reinforcement Learning (MARL) solves complex tasks that require coordination from multiple agents, but is often limited to either local (independent learning) or global (centralized learning) perspectives. In this paper, we introduce a novel sequential training scheme and MARL architecture, which learns from multiple perspectives on different hierarchy levels. We propose the Hierarchical Lead Critic (HLC) - inspired by natural emerging distributions in team structures, where following high-level objectives combines with low-level execution. HLC demonstrates that introducing multiple hierarchies, leveraging local and global perspectives, can lead to improved performance with high sample efficiency and robust policies. Experimental results conducted on cooperative, non-communicative, and partially observable MARL benchmarks demonstrate that HLC outperforms single hierarchy baselines and scales robustly with increasing amounts of agents and difficulty.

</details>


### [165] [TiMi: Empower Time Series Transformers with Multimodal Mixture of Experts](https://arxiv.org/abs/2602.21693)
*Jiafeng Lin,Yuxuan Wang,Huakun Luo,Zhongyi Pei,Jianmin Wang*

Main category: cs.LG

TL;DR: TiMi is a multimodal time series forecasting framework that uses LLMs to generate future development inferences as guidance, combined with a Multimodal Mixture-of-Experts module to integrate textual and numerical data without explicit alignment.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal forecasting methods struggle with modality alignment, especially for textual information (like emergency reports and policy announcements) that causally influences time series fluctuations. There's a need to better leverage textual data's causal reasoning capabilities for improved numerical forecasting.

Method: TiMi uses LLMs to generate inferences about future developments as guidance for forecasting. It introduces a lightweight Multimodal Mixture-of-Experts (MMoE) module that plugs into Transformer-based time series models to integrate both exogenous factors and time series data without requiring explicit representation-level alignment.

Result: TiMi achieves state-of-the-art performance on sixteen real-world multimodal forecasting benchmarks, outperforming advanced baselines while offering strong adaptability and interpretability.

Conclusion: The proposed TiMi framework effectively unleashes LLMs' causal reasoning capabilities for multimodal time series forecasting, providing a practical solution that avoids complex modality alignment while delivering superior performance and interpretability.

Abstract: Multimodal time series forecasting has garnered significant attention for its potential to provide more accurate predictions than traditional single-modality models by leveraging rich information inherent in other modalities. However, due to fundamental challenges in modality alignment, existing methods often struggle to effectively incorporate multimodal data into predictions, particularly textual information that has a causal influence on time series fluctuations, such as emergency reports and policy announcements. In this paper, we reflect on the role of textual information in numerical forecasting and propose Time series transformers with Multimodal Mixture-of-Experts, TiMi, to unleash the causal reasoning capabilities of LLMs. Concretely, TiMi utilizes LLMs to generate inferences on future developments, which serve as guidance for time series forecasting. To seamlessly integrate both exogenous factors and time series into predictions, we introduce a Multimodal Mixture-of-Experts (MMoE) module as a lightweight plug-in to empower Transformer-based time series models for multimodal forecasting, eliminating the need for explicit representation-level alignment. Experimentally, our proposed TiMi demonstrates consistent state-of-the-art performance on sixteen real-world multimodal forecasting benchmarks, outperforming advanced baselines while offering both strong adaptability and interpretability.

</details>


### [166] [Learning Complex Physical Regimes via Coverage-oriented Uncertainty Quantification: An application to the Critical Heat Flux](https://arxiv.org/abs/2602.21701)
*Michele Cazzola,Alberto Ghione,Lucia Sargentini,Julien Nespoulous,Riccardo Finotello*

Main category: cs.LG

TL;DR: This paper compares uncertainty quantification methods for scientific ML on the Critical Heat Flux benchmark, showing that coverage-oriented learning better captures complex physical regimes than post-hoc calibration.


<details>
  <summary>Details</summary>
Motivation: Standard ML methods fail to capture multi-regime physical systems where responses vary significantly across state space. Uncertainty quantification should guide learning, not just assess safety, especially for complex systems like Critical Heat Flux with distinct microscopic regimes and diverse statistical profiles.

Method: Comparative analysis of UQ methodologies on CHF benchmark dataset. Contrasts post-hoc methods (conformal prediction) against end-to-end coverage-oriented pipelines including Bayesian heteroscedastic regression and quality-driven losses. These treat uncertainty as active optimization component rather than final metric.

Result: While post-hoc methods ensure statistical calibration, coverage-oriented learning effectively reshapes model's representation to match complex physical regimes. This yields high predictive accuracy with physically consistent uncertainty estimation that adapts dynamically to CHF's intrinsic variability.

Conclusion: Coverage-oriented learning approaches that integrate uncertainty into optimization process are superior for scientific ML on multi-regime physical systems, providing both accurate predictions and physically meaningful uncertainty estimates that reflect the system's complex behavior.

Abstract: A central challenge in scientific machine learning (ML) is the correct representation of physical systems governed by multi-regime behaviours. In these scenarios, standard data analysis techniques often fail to capture the nature of the data, as the system's response varies significantly across the state space due to its stochasticity and the different physical regimes. Uncertainty quantification (UQ) should thus not be viewed merely as a safety assessment, but as a support to the learning task itself, guiding the model to internalise the behaviour of the data. We address this by focusing on the Critical Heat Flux (CHF) benchmark and dataset presented by the OECD/NEA Expert Group on Reactor Systems Multi-Physics. This case study represents a test for scientific ML due to the non-linear dependence of CHF on the inputs and the existence of distinct microscopic physical regimes. These regimes exhibit diverse statistical profiles, a complexity that requires UQ techniques to internalise the data behaviour and ensure reliable predictions. In this work, we conduct a comparative analysis of UQ methodologies to determine their impact on physical representation. We contrast post-hoc methods, specifically conformal prediction, against end-to-end coverage-oriented pipelines, including (Bayesian) heteroscedastic regression and quality-driven losses. These approaches treat uncertainty not as a final metric, but as an active component of the optimisation process, modelling the prediction and its behaviour simultaneously. We show that while post-hoc methods ensure statistical calibration, coverage-oriented learning effectively reshapes the model's representation to match the complex physical regimes. The result is a model that delivers not only high predictive accuracy but also a physically consistent uncertainty estimation that adapts dynamically to the intrinsic variability of the CHF.

</details>


### [167] [C$^{2}$TC: A Training-Free Framework for Efficient Tabular Data Condensation](https://arxiv.org/abs/2602.21717)
*Sijia Xu,Fan Li,Xiaoyang Wang,Zhengyi Yang,Xuemin Lin*

Main category: cs.LG

TL;DR: C²TC is a training-free tabular dataset condensation framework that uses class-adaptive clustering to efficiently synthesize small yet informative datasets, improving efficiency by 100x+ over existing methods while handling tabular data characteristics like heterogeneous features and class imbalance.


<details>
  <summary>Details</summary>
Motivation: Tabular data scaling creates computational/storage challenges for learning systems. Existing dataset condensation methods are computationally intensive and don't handle tabular data characteristics like heterogeneous features and class imbalance well.

Method: C²TC reformulates dataset condensation as a class-adaptive cluster allocation problem (CCAP), solved with HFILS heuristic local search that alternates between soft allocation and class-wise clustering. Includes hybrid categorical feature encoding (HCFE) for semantics-preserving clustering of discrete attributes.

Result: C²TC improves efficiency by at least 2 orders of magnitude (100x+) over state-of-the-art baselines while achieving superior downstream performance on 10 real-world datasets.

Conclusion: C²TC provides an efficient, scalable training-free framework for tabular dataset condensation that addresses computational limitations and tabular data characteristics, enabling practical data-efficient learning for industrial applications.

Abstract: Tabular data is the primary data format in industrial relational databases, underpinning modern data analytics and decision-making. However, the increasing scale of tabular data poses significant computational and storage challenges to learning-based analytical systems. This highlights the need for data-efficient learning, which enables effective model training and generalization using substantially fewer samples. Dataset condensation (DC) has emerged as a promising data-centric paradigm that synthesizes small yet informative datasets to preserve data utility while reducing storage and training costs. However, existing DC methods are computationally intensive due to reliance on complex gradient-based optimization. Moreover, they often overlook key characteristics of tabular data, such as heterogeneous features and class imbalance. To address these limitations, we introduce C$^{2}$TC (Class-Adaptive Clustering for Tabular Condensation), the first training-free tabular dataset condensation framework that jointly optimizes class allocation and feature representation, enabling efficient and scalable condensation. Specifically, we reformulate the dataset condensation objective into a novel class-adaptive cluster allocation problem (CCAP), which eliminates costly training and integrates adaptive label allocation to handle class imbalance. To solve the NP-hard CCAP, we develop HFILS, a heuristic local search that alternates between soft allocation and class-wise clustering to efficiently obtain high-quality solutions. Moreover, a hybrid categorical feature encoding (HCFE) is proposed for semantics-preserving clustering of heterogeneous discrete attributes. Extensive experiments on 10 real-world datasets demonstrate that C$^{2}$TC improves efficiency by at least 2 orders of magnitude over state-of-the-art baselines, while achieving superior downstream performance.

</details>


### [168] [Physics-Informed Machine Learning for Vessel Shaft Power and Fuel Consumption Prediction: Interpretable KAN-based Approach](https://arxiv.org/abs/2602.22055)
*Hamza Haruna Mohammed,Dusica Marijan,Arnbjørn Maressa*

Main category: cs.LG

TL;DR: PI-KAN combines physics-informed loss with interpretable neural networks to accurately predict vessel performance metrics while maintaining physical plausibility.


<details>
  <summary>Details</summary>
Motivation: Need for accurate vessel performance prediction that balances interpretability (physics-based models) with accuracy (data-driven approaches) for maritime efficiency and sustainability.

Method: Physics-Informed Kolmogorov-Arnold Network (PI-KAN) integrates interpretable univariate feature transformations with physics-informed loss function and leakage-free chained prediction pipeline.

Result: PI-KAN outperforms traditional polynomial methods and neural network baselines across five cargo vessels, achieving lowest MAE/RMSE and highest R² for shaft power and fuel consumption while maintaining physical consistency.

Conclusion: PI-KAN successfully achieves both predictive accuracy and interpretability, offering a robust tool for vessel performance monitoring and decision support in operational maritime settings.

Abstract: Accurate prediction of shaft rotational speed, shaft power, and fuel consumption is crucial for enhancing operational efficiency and sustainability in maritime transportation. Conventional physics-based models provide interpretability but struggle with real-world variability, while purely data-driven approaches achieve accuracy at the expense of physical plausibility. This paper introduces a Physics-Informed Kolmogorov-Arnold Network (PI-KAN), a hybrid method that integrates interpretable univariate feature transformations with a physics-informed loss function and a leakage-free chained prediction pipeline. Using operational and environmental data from five cargo vessels, PI-KAN consistently outperforms the traditional polynomial method and neural network baselines. The model achieves the lowest mean absolute error (MAE) and root mean squared error (RMSE), and the highest coefficient of determination (R^2) for shaft power and fuel consumption across all vessels, while maintaining physically consistent behavior. Interpretability analysis reveals rediscovery of domain-consistent dependencies, such as cubic-like speed-power relationships and cosine-like wave and wind effects. These results demonstrate that PI-KAN achieves both predictive accuracy and interpretability, offering a robust tool for vessel performance monitoring and decision support in operational settings.

</details>


### [169] [From Words to Amino Acids: Does the Curse of Depth Persist?](https://arxiv.org/abs/2602.21750)
*Aleena Siji,Amir Mohammad Karimi Mamaghan,Ferdinand Kapl,Tobias Höppe,Emmanouil Angelis,Andrea Dittadi,Maurice Brenner,Michael Heinzinger,Karl Henrik Johansson,Kaitlin Maile,Johannes von Oswald,Stefan Bauer*

Main category: cs.LG

TL;DR: PLMs show depth inefficiency similar to LLMs - later layers contribute little to predictions, suggesting potential for more depth-efficient architectures.


<details>
  <summary>Details</summary>
Motivation: Recent work on LLMs identified "Curse of Depth" where later layers contribute little to predictions. This raises the question whether similar depth inefficiency exists in PLMs, which have different training objectives (autoregressive, masked, diffusion) and some are multimodal with structure input.

Method: Analyzed six popular PLMs across model families and scales with three training objectives (autoregressive, masked, diffusion). Used unified set of probing- and perturbation-based measurements to quantify how layer contributions evolve with depth.

Result: Consistent depth-dependent patterns across all models: later layers depend less on earlier computations and mainly refine final output distribution. These effects are more pronounced in deeper models, showing PLMs exhibit depth inefficiency similar to LLMs.

Conclusion: PLMs show depth inefficiency, motivating future work on more depth-efficient architectures and training methods for protein language models.

Abstract: Protein language models (PLMs) have become widely adopted as general-purpose models, demonstrating strong performance in protein engineering and de novo design. Like large language models (LLMs), they are typically trained as deep transformers with next-token or masked-token prediction objectives on massive sequence corpora and are scaled by increasing model depth. Recent work on autoregressive LLMs has identified the Curse of Depth: later layers contribute little to the final output predictions. These findings naturally raise the question of whether a similar depth inefficiency also appears in PLMs, where many widely used models are not autoregressive, and some are multimodal, accepting both protein sequence and structure as input. In this work, we present a depth analysis of six popular PLMs across model families and scales, spanning three training objectives, namely autoregressive, masked, and diffusion, and quantify how layer contributions evolve with depth using a unified set of probing- and perturbation-based measurements. Across all models, we observe consistent depth-dependent patterns that extend prior findings on LLMs: later layers depend less on earlier computations and mainly refine the final output distribution, and these effects are increasingly pronounced in deeper models. Taken together, our results suggest that PLMs exhibit a form of depth inefficiency, motivating future work on more depth-efficient architectures and training methods.

</details>


### [170] [DualWeaver: Synergistic Feature Weaving Surrogates for Multivariate Forecasting with Univariate Time Series Foundation Models](https://arxiv.org/abs/2602.22066)
*Jinpeng Li,Zhongyi Pei,Huaze Xue,Bojian Zheng,Chen Wang,Jianmin Wang*

Main category: cs.LG

TL;DR: DualWeaver adapts univariate time-series foundation models for multivariate forecasting using symmetric surrogate series, achieving state-of-the-art performance without parametric decoding.


<details>
  <summary>Details</summary>
Motivation: While time-series foundation models excel at univariate forecasting, effectively extending them to multivariate forecasting remains challenging due to difficulties in capturing cross-variable dependencies and adapting the models properly.

Method: Proposes DualWeaver framework that uses a pair of learnable, structurally symmetric surrogate series generated by a shared auxiliary feature-fusion module to capture cross-variable dependencies. These surrogates are mapped to TSFM-compatible series via forecasting objective, with symmetric structure enabling parameter-free reconstruction of final predictions. Includes theoretically grounded regularization to prevent adaptation collapse.

Result: Extensive experiments on diverse real-world datasets show DualWeaver outperforms state-of-the-art multivariate forecasters in both accuracy and stability.

Conclusion: DualWeaver successfully adapts univariate time-series foundation models for multivariate forecasting through symmetric surrogate series, achieving superior performance without additional parametric decoding components.

Abstract: Time-series foundation models (TSFMs) have achieved strong univariate forecasting through large-scale pre-training, yet effectively extending this success to multivariate forecasting remains challenging. To address this, we propose DualWeaver, a novel framework that adapts univariate TSFMs (Uni-TSFMs) for multivariate forecasting by using a pair of learnable, structurally symmetric surrogate series. Generated by a shared auxiliary feature-fusion module that captures cross-variable dependencies, these surrogates are mapped to TSFM-compatible series via the forecasting objective. The symmetric structure enables parameter-free reconstruction of final predictions directly from the surrogates, without additional parametric decoding. A theoretically grounded regularization term is further introduced to enhance robustness against adaptation collapse. Extensive experiments on diverse real-world datasets show that DualWeaver outperforms state-of-the-art multivariate forecasters in both accuracy and stability. We release the code at https://github.com/li-jinpeng/DualWeaver.

</details>


### [171] [On Imbalanced Regression with Hoeffding Trees](https://arxiv.org/abs/2602.22101)
*Pantia-Marina Alchirch,Dimitrios I. Diochnos*

Main category: cs.LG

TL;DR: This paper extends kernel density estimation (KDE) and hierarchical shrinkage (HS) methods from batch learning to streaming regression with decision trees, finding KDE helps early in streams while HS offers little benefit.


<details>
  <summary>Details</summary>
Motivation: Real-world applications generate continuous data streams for regression tasks, but existing batch learning methods like KDE (for imbalanced regression) and HS (for tree regularization) haven't been adapted to streaming environments with incremental decision trees.

Method: The authors use a telescoping argument to adapt KDE to streaming environments and extend HS implementation to incremental decision tree models, then evaluate these extensions on streaming regression datasets.

Result: KDE is beneficial in the early parts of the data stream, while hierarchical shrinkage (HS) hardly ever offers performance benefits in streaming regression settings.

Conclusion: The study successfully adapts KDE to streaming regression with decision trees, showing early-stream benefits, but finds HS ineffective in this context, providing publicly available code for further research.

Abstract: Many real-world applications provide a continuous stream of data that is subsequently used by machine learning models to solve regression tasks of interest. Hoeffding trees and their variants have a long-standing tradition due to their effectiveness, either alone or as base models in broader ensembles. At the same time a recent line of work in batch learning has shown that kernel density estimation (KDE) is an effective approach for smoothed predictions in imbalanced regression tasks [Yang et al., 2021]. Moreover, another recent line of work for batch learning, called hierarchical shrinkage (HS) [Agarwal et al., 2022], has introduced a post-hoc regularization method for decision trees that does not alter the structure of the learned tree. Using a telescoping argument we cast KDE to streaming environments and extend the implementation of HS to incremental decision tree models. Armed with these extensions we investigate the performance of decision trees that may enjoy such options in datasets commonly used for regression in online settings. We conclude that KDE is beneficial in the early parts of the stream, while HS hardly, if ever, offers performance benefits. Our code is publicly available at: https://github.com/marinaAlchirch/DSFA_2026.

</details>


### [172] [Easy to Learn, Yet Hard to Forget: Towards Robust Unlearning Under Bias](https://arxiv.org/abs/2602.21773)
*JuneHyoung Kwon,MiHyeon Kim,Eunju Lee,Yoonji Lee,Seunghoon Lee,YoungBin Kim*

Main category: cs.LG

TL;DR: The paper introduces "shortcut unlearning" - a phenomenon where models struggle to forget bias-aligned samples, instead unlearning bias attributes and paradoxically improving accuracy on the class meant to be forgotten. They propose CUPID framework to address this by partitioning forget sets based on loss landscape sharpness and disentangling model parameters into causal/bias pathways.


<details>
  <summary>Details</summary>
Motivation: Machine unlearning is important for data privacy and model reliability, but its effectiveness is undermined when models learn unintended biases from spurious correlations. The paper investigates challenges of unlearning from biased models, identifying the "shortcut unlearning" phenomenon where models exhibit "easy to learn, yet hard to forget" behavior for bias-aligned samples.

Method: CUPID framework: 1) Partitions forget set into causal- and bias-approximated subsets based on sample sharpness in loss landscape, 2) Disentangles model parameters into causal and bias pathways, 3) Performs targeted updates by routing refined causal and bias gradients to their respective pathways.

Result: Extensive experiments on biased datasets (Waterbirds, BAR, Biased NICO++) demonstrate state-of-the-art forgetting performance and effective mitigation of the shortcut unlearning problem.

Conclusion: The paper identifies shortcut unlearning as a key challenge in machine unlearning for biased models and proposes CUPID as an effective solution that partitions forget sets by sharpness and disentangles parameters for targeted unlearning, achieving superior performance on biased datasets.

Abstract: Machine unlearning, which enables a model to forget specific data, is crucial for ensuring data privacy and model reliability. However, its effectiveness can be severely undermined in real-world scenarios where models learn unintended biases from spurious correlations within the data. This paper investigates the unique challenges of unlearning from such biased models. We identify a novel phenomenon we term ``shortcut unlearning," where models exhibit an ``easy to learn, yet hard to forget" tendency. Specifically, models struggle to forget easily-learned, bias-aligned samples; instead of forgetting the class attribute, they unlearn the bias attribute, which can paradoxically improve accuracy on the class intended to be forgotten. To address this, we propose CUPID, a new unlearning framework inspired by the observation that samples with different biases exhibit distinct loss landscape sharpness. Our method first partitions the forget set into causal- and bias-approximated subsets based on sample sharpness, then disentangles model parameters into causal and bias pathways, and finally performs a targeted update by routing refined causal and bias gradients to their respective pathways. Extensive experiments on biased datasets including Waterbirds, BAR, and Biased NICO++ demonstrate that our method achieves state-of-the-art forgetting performance and effectively mitigates the shortcut unlearning problem.

</details>


### [173] [Don't stop me now: Rethinking Validation Criteria for Model Parameter Selection](https://arxiv.org/abs/2602.22107)
*Andrea Apicella,Francesco Isgrò,Andrea Pollastro,Roberto Prevete*

Main category: cs.LG

TL;DR: Validation accuracy performs worst for model selection, especially with early stopping. Loss-based validation criteria yield better and more stable test performance than accuracy-based selection.


<details>
  <summary>Details</summary>
Motivation: Despite extensive literature on training loss functions, how validation criteria affect generalization and model selection (especially for early stopping) remains underexplored. The paper aims to systematically study how different validation criteria impact test performance in neural classifiers.

Method: Systematic empirical and statistical study using fully connected networks on standard benchmarks under k-fold evaluation. Compares: (i) early stopping with patience vs (ii) post-hoc selection over all epochs. Models trained with cross-entropy, C-Loss, or PolyLoss; validation selection uses accuracy or one of the three loss functions independently.

Result: 1) Early stopping based on validation accuracy performs worst, consistently selecting checkpoints with lower test accuracy. 2) Loss-based validation criteria yield comparable and more stable test accuracy. 3) Any single validation rule often underperforms the test-optimal checkpoint across datasets and folds.

Conclusion: Avoid validation accuracy (especially with early stopping) for parameter selection, favoring loss-based validation criteria. The selected model typically achieves test-set performance statistically lower than the best performance across all epochs, regardless of validation criterion.

Abstract: Despite the extensive literature on training loss functions, the evaluation of generalization on the validation set remains underexplored. In this work, we conduct a systematic empirical and statistical study of how the validation criterion used for model selection affects test performance in neural classifiers, with attention to early stopping. Using fully connected networks on standard benchmarks under $k$-fold evaluation, we compare: (i) early stopping with patience and (ii) post-hoc selection over all epochs (i.e. no early stopping). Models are trained with cross-entropy, C-Loss, or PolyLoss; the model parameter selection on the validation set is made using accuracy or one of the three loss functions, each considered independently. Three main findings emerge. (1) Early stopping based on validation accuracy performs worst, consistently selecting checkpoints with lower test accuracy than both loss-based early stopping and post-hoc selection. (2) Loss-based validation criteria yield comparable and more stable test accuracy. (3) Across datasets and folds, any single validation rule often underperforms the test-optimal checkpoint. Overall, the selected model typically achieves test-set performance statistically lower than the best performance across all epochs, regardless of the validation criterion. Our results suggest avoiding validation accuracy (in particular with early stopping) for parameter selection, favoring loss-based validation criteria.

</details>


### [174] [DocDjinn: Controllable Synthetic Document Generation with VLMs and Handwriting Diffusion](https://arxiv.org/abs/2602.21824)
*Marcel Lamott,Saifullah Saifullah,Nauman Riaz,Yves-Noel Weweler,Tobias Alt-Veit,Ahmad Sarmad Ali,Muhammad Armaghan Shakir,Adrian Kalwa,Momina Moetesum,Andreas Dengel,Sheraz Ahmed,Faisal Shafait,Ulrich Schwanecke,Adrian Ulges*

Main category: cs.LG

TL;DR: DocDjinn is a VLM-based framework for generating controllable, privacy-preserving synthetic documents from unlabeled seed samples, achieving 87% performance of full real datasets with only 100 real samples.


<details>
  <summary>Details</summary>
Motivation: Document intelligence models require large annotated datasets, but data acquisition is labor-intensive, costly, and raises privacy concerns when using language models on real documents. Synthetic document generation offers a privacy-preserving alternative.

Method: Uses Vision-Language Models (VLMs) with clustering-based seed selection and parametrized sampling to generate visually plausible documents. Employs semantic-visual decoupling with diffusion-based handwriting and contextual visual elements to create diverse, high-quality annotated synthetic documents.

Result: Evaluated across 11 benchmarks for information extraction, QA, classification, and layout analysis. Achieves 87% performance of full real-world datasets using only 100 real training samples. First work showing VLMs can generate faithful annotated document datasets at scale from unlabeled seeds.

Conclusion: DocDjinn provides an effective, privacy-preserving solution for generating high-quality synthetic documents that can enrich or approximate real annotated data for diverse document understanding tasks, addressing data scarcity and privacy challenges.

Abstract: Effective document intelligence models rely on large amounts of annotated training data. However, procuring sufficient and high-quality data poses significant challenges due to the labor-intensive and costly nature of data acquisition. Additionally, leveraging language models to annotate real documents raises concerns about data privacy. Synthetic document generation has emerged as a promising, privacy-preserving alternative. We propose DocDjinn, a novel framework for controllable synthetic document generation using Vision-Language Models (VLMs) that produces annotated documents from unlabeled seed samples. Our approach generates visually plausible and semantically consistent synthetic documents that follow the distribution of an existing source dataset through clustering-based seed selection with parametrized sampling. By enriching documents with realistic diffusion-based handwriting and contextual visual elements via semantic-visual decoupling, we generate diverse, high-quality annotated synthetic documents. We evaluate across eleven benchmarks spanning key information extraction, question answering, document classification, and document layout analysis. To our knowledge, this is the first work demonstrating that VLMs can generate faithful annotated document datasets at scale from unlabeled seeds that can effectively enrich or approximate real, manually annotated data for diverse document understanding tasks. We show that with only 100 real training samples, our framework achieves on average $87\%$ of the performance of the full real-world dataset. We publicly release our code and 140k+ synthetic document samples.

</details>


### [175] [Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual](https://arxiv.org/abs/2602.22146)
*Yining Li,Peizhong Ju,Ness Shroff*

Main category: cs.LG

TL;DR: Proposes an optimistic primal-dual framework for safe RLHF that unifies existing alignment methods and provides last-iterate convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: Standard primal-dual methods for RLHF with expected reward constraints have limitations: they only guarantee convergence with distributional policies (convex-concave form), and may exhibit instability/divergence in last iterate under policy parameterization in practice.

Method: Introduces a universal primal-dual framework unifying existing alignment algorithms (safe-RLHF, one-shot, multi-shot methods). Proposes an Optimistic Primal-Dual (OPD) algorithm with predictive updates for both primal and dual variables to stabilize saddle-point dynamics.

Result: Establishes last-iterate convergence guarantees: 1) exact policy optimization in distributional space, 2) convergence to neighborhood of optimal solution with gap related to approximation error and bias under parameterized policies. Shows optimism mitigates oscillations in constrained alignment objectives.

Conclusion: The optimistic primal-dual framework closes a key theoretical gap between constrained RL and practical RLHF by providing stable convergence guarantees and unifying various alignment approaches.

Abstract: Reinforcement Learning from Human Feedback (RLHF) plays a significant role in aligning Large Language Models (LLMs) with human preferences. While RLHF with expected reward constraints can be formulated as a primal-dual optimization problem, standard primal-dual methods only guarantee convergence with a distributional policy where the saddle-point problem is in convex-concave form. Moreover, standard primal-dual methods may exhibit instability or divergence in the last iterate under policy parameterization in practical applications. In this work, we propose a universal primal-dual framework for safe RLHF that unifies a broad class of existing alignment algorithms, including safe-RLHF, one-shot, and multi-shot based methods. Building on this framework, we introduce an optimistic primal-dual (OPD) algorithm that incorporates predictive updates for both primal and dual variables to stabilize saddle-point dynamics. We establish last-iterate convergence guarantees for the proposed method, covering both exact policy optimization in the distributional space and convergence to a neighborhood of the optimal solution whose gap is related to approximation error and bias under parameterized policies. Our analysis reveals that optimism plays a crucial role in mitigating oscillations inherent to constrained alignment objectives, thereby closing a key theoretical gap between constrained RL and practical RLHF.

</details>


### [176] [JSAM: Privacy Straggler-Resilient Joint Client Selection and Incentive Mechanism Design in Differentially Private Federated Learning](https://arxiv.org/abs/2602.21844)
*Ruichen Xu,Ying-Jun Angela Zhang,Jianwei Huang*

Main category: cs.LG

TL;DR: JSAM is a Bayesian-optimal framework for differentially private federated learning that jointly optimizes client selection and privacy compensation to maximize training effectiveness under budget constraints, achieving up to 15% accuracy improvement over unbiased selection methods.


<details>
  <summary>Details</summary>
Motivation: There's a fundamental tension in differentially private federated learning: privacy protection mechanisms create quantifiable privacy costs that discourage client participation, undermining collaboration. Existing incentive mechanisms rely on unbiased client selection, forcing servers to compensate even the most privacy-sensitive clients ("privacy stragglers"), leading to systemic inefficiency and suboptimal resource allocation.

Method: JSAM (Joint client Selection and privacy compensAtion Mechanism) is a Bayesian-optimal framework that simultaneously optimizes client selection probabilities and privacy compensation to maximize training effectiveness under budget constraints. The approach transforms a complex 2N-dimensional optimization problem into an efficient three-dimensional formulation through novel theoretical characterization of optimal selection strategies.

Result: Theoretical analysis proves that servers should preferentially select privacy-tolerant clients while excluding high-sensitivity participants. The paper uncovers the counter-intuitive insight that clients with minimal privacy sensitivity may incur the highest cumulative costs due to frequent participation. Extensive evaluations on MNIST and CIFAR-10 demonstrate that JSAM achieves up to 15% improvement in test accuracy compared to existing unbiased selection mechanisms while maintaining cost efficiency across varying data heterogeneity levels.

Conclusion: JSAM provides an effective solution to the privacy-cost-participation trade-off in differentially private federated learning by optimizing both client selection and compensation strategies, leading to significant improvements in model accuracy and resource allocation efficiency compared to traditional unbiased selection approaches.

Abstract: Differentially private federated learning faces a fundamental tension: privacy protection mechanisms that safeguard client data simultaneously create quantifiable privacy costs that discourage participation, undermining the collaborative training process. Existing incentive mechanisms rely on unbiased client selection, forcing servers to compensate even the most privacy-sensitive clients ("privacy stragglers"), leading to systemic inefficiency and suboptimal resource allocation. We introduce JSAM (Joint client Selection and privacy compensAtion Mechanism), a Bayesian-optimal framework that simultaneously optimizes client selection probabilities and privacy compensation to maximize training effectiveness under budget constraints. Our approach transforms a complex 2N-dimensional optimization problem into an efficient three-dimensional formulation through novel theoretical characterization of optimal selection strategies. We prove that servers should preferentially select privacy-tolerant clients while excluding high-sensitivity participants, and uncover the counter-intuitive insight that clients with minimal privacy sensitivity may incur the highest cumulative costs due to frequent participation. Extensive evaluations on MNIST and CIFAR-10 demonstrate that JSAM achieves up to 15% improvement in test accuracy compared to existing unbiased selection mechanisms while maintaining cost efficiency across varying data heterogeneity levels.

</details>


### [177] [Surrogate models for Rock-Fluid Interaction: A Grid-Size-Invariant Approach](https://arxiv.org/abs/2602.22188)
*Nathalie C. Pinheiro,Donghu Guo,Hannah P. Menke,Aniket C. Joshi,Claire E. Heaney,Ahmed H. ElSheikh,Christopher C. Pain*

Main category: cs.LG

TL;DR: The paper develops eight neural network-based surrogate models for fluid flow in porous media to replace computationally expensive high-fidelity simulations, comparing reduced-order models with novel grid-size-invariant approaches using UNet and UNet++ architectures.


<details>
  <summary>Details</summary>
Motivation: High-fidelity numerical models for rock-fluid interaction require high resolution and are computationally expensive, making them impractical for multi-query problems like uncertainty quantification and optimization that need numerous simulations. There's a need for cheaper surrogate models.

Method: Developed eight surrogate models: four reduced-order models (ROMs) using two neural networks (one for compression, one for prediction), and four single neural networks with grid-size invariance that can infer on larger domains than training. Compared UNet and UNet++ architectures within this framework.

Result: UNet++ outperforms UNet for surrogate modeling. The grid-size-invariant approach reliably reduces memory consumption during training while maintaining good correlation between predicted and ground-truth values, and outperforms the ROMs analyzed.

Conclusion: The novel grid-size-invariant framework provides an effective approach for surrogate modeling in rock-fluid interaction problems, offering computational efficiency advantages over traditional ROMs while handling the challenging case of fluid-induced rock dissolution with non-static solid fields.

Abstract: Modelling rock-fluid interaction requires solving a set of partial differential equations (PDEs) to predict the flow behaviour and the reactions of the fluid with the rock on the interfaces. Conventional high-fidelity numerical models require a high resolution to obtain reliable results, resulting in huge computational expense. This restricts the applicability of these models for multi-query problems, such as uncertainty quantification and optimisation, which require running numerous scenarios. As a cheaper alternative to high-fidelity models, this work develops eight surrogate models for predicting the fluid flow in porous media. Four of these are reduced-order models (ROM) based on one neural network for compression and another for prediction. The other four are single neural networks with the property of grid-size invariance; a term which we use to refer to image-to-image models that are capable of inferring on computational domains that are larger than those used during training. In addition to the novel grid-size-invariant framework for surrogate models, we compare the predictive performance of UNet and UNet++ architectures, and demonstrate that UNet++ outperforms UNet for surrogate models. Furthermore, we show that the grid-size-invariant approach is a reliable way to reduce memory consumption during training, resulting in good correlation between predicted and ground-truth values and outperforming the ROMs analysed. The application analysed is particularly challenging because fluid-induced rock dissolution results in a non-static solid field and, consequently, it cannot be used to help in adjustments of the future prediction.

</details>


### [178] [GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL](https://arxiv.org/abs/2602.22190)
*Rui Yang,Qianhui Wu,Zhaoyang Wang,Hanyang Chen,Ke Yang,Hao Cheng,Huaxiu Yao,Baoling Peng,Huan Zhang,Jianfeng Gao,Tong Zhang*

Main category: cs.LG

TL;DR: GUI-Libra introduces a tailored training recipe for GUI agents that addresses data scarcity and training pipeline issues through curated datasets, action-aware SFT, and stabilized RL with KL regularization, improving both step-wise accuracy and end-to-end task completion.


<details>
  <summary>Details</summary>
Motivation: Open-source GUI agents lag behind closed-source systems on long-horizon navigation due to: 1) shortage of high-quality action-aligned reasoning data, and 2) adoption of generic post-training pipelines that overlook GUI agent challenges like grounding issues and partial verifiability problems.

Method: 1) Data construction pipeline producing 81K curated GUI reasoning dataset; 2) Action-aware SFT mixing reasoning-then-action and direct-action data with token reweighting; 3) Stabilized RL with KL regularization and success-adaptive scaling to handle partial verifiability.

Result: GUI-Libra consistently improves both step-wise accuracy and end-to-end task completion across diverse web and mobile benchmarks, demonstrating stronger task-solving capabilities without costly online data collection.

Conclusion: Carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities for GUI agents, addressing fundamental limitations in current approaches through tailored solutions for data scarcity and training pipeline issues.

Abstract: Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard SFT with CoT reasoning often hurts grounding, and (ii) step-wise RLVR-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification. This makes offline step-wise metrics weak predictors of online task success. In this work, we present GUI-Libra, a tailored training recipe that addresses these challenges. First, to mitigate the scarcity of action-aligned reasoning data, we introduce a data construction and filtering pipeline and release a curated 81K GUI reasoning dataset. Second, to reconcile reasoning with grounding, we propose action-aware SFT that mixes reasoning-then-action and direct-action data and reweights tokens to emphasize action and grounding. Third, to stabilize RL under partial verifiability, we identify the overlooked importance of KL regularization in RLVR and show that a KL trust region is critical for improving offline-to-online predictability; we further introduce success-adaptive scaling to downweight unreliable negative gradients. Across diverse web and mobile benchmarks, GUI-Libra consistently improves both step-wise accuracy and end-to-end task completion. Our results suggest that carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities without costly online data collection. We release our dataset, code, and models to facilitate further research on data-efficient post-training for reasoning-capable GUI agents.

</details>


### [179] [The Error of Deep Operator Networks Is the Sum of Its Parts: Branch-Trunk and Mode Error Decompositions](https://arxiv.org/abs/2602.21910)
*Alexander Heinlein,Johannes Taraz*

Main category: cs.LG

TL;DR: DeepONets have accuracy limitations; analysis shows branch network dominates error, trunk can be replaced with classical bases, and spectral bias exists in branch learning.


<details>
  <summary>Details</summary>
Motivation: Deep operator networks (DeepONets) show promise for accelerating scientific computing tasks like design optimization and uncertainty quantification by learning solution operators for differential equations. However, despite theoretical universal approximation properties, they often exhibit limited accuracy and poor generalization in practice, hindering their adoption. Understanding these limitations is crucial for advancing operator learning approaches.

Method: The paper analyzes performance limitations of classical DeepONet architecture. A modified DeepONet is constructed where the trunk network is replaced by the left singular vectors of the training solution matrix. This allows investigation of spectral bias, error contributions, and mode coupling in the branch network.

Result: Key findings: 1) Approximation error is dominated by the branch network when internal dimension is large; 2) Learned trunk basis can often be replaced by classical basis functions without significant performance impact; 3) Spectral bias exists in branch network - dominant low-frequency modes are learned more effectively; 4) Branch error is dominated by modes with intermediate singular values due to scaling; 5) Shared branch network improves generalization of small modes compared to separate computation; 6) Strong detrimental coupling between modes in parameter space is identified.

Conclusion: The analysis reveals fundamental limitations in DeepONet architecture, particularly in branch network performance and mode coupling. These insights provide guidance for improving operator learning architectures and understanding their practical limitations, which is essential for advancing their adoption in scientific computing applications.

Abstract: Operator learning has the potential to strongly impact scientific computing by learning solution operators for differential equations, potentially accelerating multi-query tasks such as design optimization and uncertainty quantification by orders of magnitude. Despite proven universal approximation properties, deep operator networks (DeepONets) often exhibit limited accuracy and generalization in practice, which hinders their adoption. Understanding these limitations is therefore crucial for further advancing the approach.
  This work analyzes performance limitations of the classical DeepONet architecture. It is shown that the approximation error is dominated by the branch network when the internal dimension is sufficiently large, and that the learned trunk basis can often be replaced by classical basis functions without a significant impact on performance.
  To investigate this further, a modified DeepONet is constructed in which the trunk network is replaced by the left singular vectors of the training solution matrix. This modification yields several key insights. First, a spectral bias in the branch network is observed, with coefficients of dominant, low-frequency modes learned more effectively. Second, due to singular-value scaling of the branch coefficients, the overall branch error is dominated by modes with intermediate singular values rather than the smallest ones. Third, using a shared branch network for all mode coefficients, as in the standard architecture, improves generalization of small modes compared to a stacked architecture in which coefficients are computed separately. Finally, strong and detrimental coupling between modes in parameter space is identified.

</details>


### [180] [Learning in the Null Space: Small Singular Values for Continual Learning](https://arxiv.org/abs/2602.21919)
*Cuong Anh Pham,Praneeth Vepakomma,Samuel Horváth*

Main category: cs.LG

TL;DR: NESS is a continual learning method that uses small singular values to construct approximate null spaces for weight updates, enabling new task learning while minimizing forgetting of previous tasks.


<details>
  <summary>Details</summary>
Motivation: To address catastrophic forgetting in continual learning by leveraging the insight that small singular values correspond to directions nearly orthogonal to previous tasks' input spaces, allowing for more direct orthogonality enforcement in weight space rather than through gradient manipulation.

Method: NESS constructs approximate null spaces using smallest singular values of each layer's input representation, then parameterizes task-specific updates via a compact LoRA-style formulation constrained to this subspace. The subspace basis is fixed to preserve null-space constraints, with only a single trainable matrix learned per task.

Result: Theoretical analysis and experiments on three benchmark datasets demonstrate competitive performance, low forgetting, and stable accuracy across tasks, highlighting the effectiveness of using small singular values for continual learning.

Conclusion: NESS successfully leverages small singular values to construct approximate null spaces for continual learning, providing an effective approach that maintains orthogonality in weight space rather than through gradient manipulation, resulting in competitive performance with minimal forgetting.

Abstract: Alleviating catastrophic forgetting while enabling further learning is a primary challenge in continual learning (CL). Orthogonal-based training methods have gained attention for their efficiency and strong theoretical properties, and many existing approaches enforce orthogonality through gradient projection. In this paper, we revisit orthogonality and exploit the fact that small singular values correspond to directions that are nearly orthogonal to the input space of previous tasks. Building on this principle, we introduce NESS (Null-space Estimated from Small Singular values), a CL method that applies orthogonality directly in the weight space rather than through gradient manipulation. Specifically, NESS constructs an approximate null space using the smallest singular values of each layer's input representation and parameterizes task-specific updates via a compact low-rank adaptation (LoRA-style) formulation constrained to this subspace. The subspace basis is fixed to preserve the null-space constraint, and only a single trainable matrix is learned for each task. This design ensures that the resulting updates remain approximately in the null space of previous inputs while enabling adaptation to new tasks. Our theoretical analysis and experiments on three benchmark datasets demonstrate competitive performance, low forgetting, and stable accuracy across tasks, highlighting the role of small singular values in continual learning. The code is available at https://github.com/pacman-ctm/NESS.

</details>


### [181] [Learning Unknown Interdependencies for Decentralized Root Cause Analysis in Nonlinear Dynamical Systems](https://arxiv.org/abs/2602.21928)
*Ayush Mohanty,Paritosh Ramanan,Nagi Gebraeel*

Main category: cs.LG

TL;DR: A federated cross-client interdependency learning method for root cause analysis in networked industrial systems that handles feature-partitioned, nonlinear time-series data without requiring raw sensor access or modifying proprietary client models.


<details>
  <summary>Details</summary>
Motivation: Root cause analysis in networked industrial systems is difficult due to unknown and dynamically evolving interdependencies among geographically distributed clients with heterogeneous data and fixed proprietary models. Classical RCA methods require dependency graph knowledge, which is rarely available, while existing federated learning methods assume homogeneous features and retrainable models.

Method: Each proprietary local client model is augmented with an ML model that encodes cross-client interdependencies. These ML models are coordinated via a global server that enforces representation consistency while preserving privacy through calibrated differential privacy noise. RCA is performed using model residuals and anomaly flags.

Result: The approach establishes theoretical convergence guarantees and is validated on extensive simulations and a real-world industrial cybersecurity dataset.

Conclusion: The proposed federated cross-client interdependency learning methodology enables effective root cause analysis in complex networked industrial systems while respecting privacy constraints and client model proprietary limitations.

Abstract: Root cause analysis (RCA) in networked industrial systems, such as supply chains and power networks, is notoriously difficult due to unknown and dynamically evolving interdependencies among geographically distributed clients. These clients represent heterogeneous physical processes and industrial assets equipped with sensors that generate large volumes of nonlinear, high-dimensional, and heterogeneous IoT data. Classical RCA methods require partial or full knowledge of the system's dependency graph, which is rarely available in these complex networks. While federated learning (FL) offers a natural framework for decentralized settings, most existing FL methods assume homogeneous feature spaces and retrainable client models. These assumptions are not compatible with our problem setting. Different clients have different data features and often run fixed, proprietary models that cannot be modified. This paper presents a federated cross-client interdependency learning methodology for feature-partitioned, nonlinear time-series data, without requiring access to raw sensor streams or modifying proprietary client models. Each proprietary local client model is augmented with a Machine Learning (ML) model that encodes cross-client interdependencies. These ML models are coordinated via a global server that enforces representation consistency while preserving privacy through calibrated differential privacy noise. RCA is performed using model residuals and anomaly flags. We establish theoretical convergence guarantees and validate our approach on extensive simulations and a real-world industrial cybersecurity dataset.

</details>


### [182] [Bayesian Generative Adversarial Networks via Gaussian Approximation for Tabular Data Synthesis](https://arxiv.org/abs/2602.21948)
*Bahrul Ilmi Nasution,Mark Elliot,Richard Allmendinger*

Main category: cs.LG

TL;DR: GACTGAN integrates Bayesian posterior approximation (SWAG) into CTGAN for tabular data synthesis, achieving better utility-privacy trade-off with reduced computational overhead.


<details>
  <summary>Details</summary>
Motivation: CTGAN struggles with risk-utility trade-off for tabular data synthesis. Bayesian GANs using MCMC are computationally intensive. Need simpler Bayesian approach for tabular synthesis with better performance.

Method: Integrates Stochastic Weight Averaging-Gaussian (SWAG) Bayesian posterior approximation technique within CTGAN generator. SWAG reduces computational overhead compared to MCMC while maintaining Bayesian benefits.

Result: GACTGAN yields better synthetic data than CTGAN, with better preservation of tabular structure and inferential statistics, and less privacy risk. Achieves better utility-privacy trade-off.

Conclusion: GACTGAN provides simpler, effective Bayesian tabular synthesis with reduced computational overhead after training, outperforming CTGAN in both utility and privacy aspects.

Abstract: Generative Adversarial Networks (GAN) have been used in many studies to synthesise mixed tabular data. Conditional tabular GAN (CTGAN) have been the most popular variant but struggle to effectively navigate the risk-utility trade-off. Bayesian GAN have received less attention for tabular data, but have been explored with unstructured data such as images and text. The most used technique employed in Bayesian GAN is Markov Chain Monte Carlo (MCMC), but it is computationally intensive, particularly in terms of weight storage. In this paper, we introduce Gaussian Approximation of CTGAN (GACTGAN), an integration of the Bayesian posterior approximation technique using Stochastic Weight Averaging-Gaussian (SWAG) within the CTGAN generator to synthesise tabular data, reducing computational overhead after the training phase. We demonstrate that GACTGAN yields better synthetic data compared to CTGAN, achieving better preservation of tabular structure and inferential statistics with less privacy risk. These results highlight GACTGAN as a simpler, effective implementation of Bayesian tabular synthesis.

</details>


### [183] [Estimation and Optimization of Ship Fuel Consumption in Maritime: Review, Challenges and Future Directions](https://arxiv.org/abs/2602.21959)
*Dusica Marijan,Hamza Haruna Mohammed,Bakht Zaman*

Main category: cs.LG

TL;DR: A comprehensive review paper categorizing fuel consumption estimation methods (physics-based, machine learning, hybrid) and optimization techniques in maritime transport, highlighting data fusion, Explainable AI, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To reduce carbon emissions and shipping costs by improving ship fuel efficiency through better estimation and optimization methods, addressing the need for comprehensive review and identification of research gaps.

Method: Literature review approach with novel categorization of fuel consumption estimation methods into physics-based, machine learning, and hybrid models; analysis of data fusion techniques combining AIS, onboard sensors, and meteorological data; and examination of optimization methods for minimizing fuel consumption.

Result: Comprehensive review framework established with novel categorization system; identification of strengths/limitations of different estimation approaches; highlighting of data fusion importance; first discussion of Explainable AI for model transparency; identification of key challenges including data quality, availability, and real-time optimization needs.

Conclusion: Future research should focus on hybrid models, real-time optimization, dataset standardization, and addressing data quality challenges; Explainable AI and data fusion techniques are crucial for advancing fuel consumption estimation and optimization in maritime transport.

Abstract: To reduce carbon emissions and minimize shipping costs, improving the fuel efficiency of ships is crucial. Various measures are taken to reduce the total fuel consumption of ships, including optimizing vessel parameters and selecting routes with the lowest fuel consumption. Different estimation methods are proposed for predicting fuel consumption, while various optimization methods are proposed to minimize fuel oil consumption. This paper provides a comprehensive review of methods for estimating and optimizing fuel oil consumption in maritime transport. Our novel contributions include categorizing fuel oil consumption \& estimation methods into physics-based, machine-learning, and hybrid models, exploring their strengths and limitations. Furthermore, we highlight the importance of data fusion techniques, which combine AIS, onboard sensors, and meteorological data to enhance accuracy. We make the first attempt to discuss the emerging role of Explainable AI in enhancing model transparency for decision-making. Uniquely, key challenges, including data quality, availability, and the need for real-time optimization, are identified, and future research directions are proposed to address these gaps, with a focus on hybrid models, real-time optimization, and the standardization of datasets.

</details>


### [184] [Robustness in sparse artificial neural networks trained with adaptive topology](https://arxiv.org/abs/2602.21961)
*Bendegúz Sulyok,Gergely Palla,Filippo Radicchi,Santo Fortunato*

Main category: cs.LG

TL;DR: Adaptive sparse neural networks with 99% sparsity maintain competitive accuracy and robustness on image classification tasks despite significant weight reduction.


<details>
  <summary>Details</summary>
Motivation: To investigate whether sparse neural networks with adaptive topology can achieve both computational efficiency and robustness, addressing the trade-off between model sparsity and reliability in deep learning.

Method: A simple architecture with three sparse layers (99% sparsity) followed by a dense layer, applied to MNIST and Fashion MNIST. The topology of sparse layers is updated between each epoch, and robustness is tested under perturbations including random link removal, adversarial attacks, and link weight shuffling.

Result: The adaptive sparse networks achieve competitive accuracy despite 99% weight reduction and demonstrate maintained robustness under various perturbations, showing that adaptive topology enhances both efficiency and reliability.

Conclusion: Adaptive sparse networks represent a promising direction for developing efficient and reliable deep learning models, successfully balancing sparsity with robustness through dynamic topology updates.

Abstract: We investigate the robustness of sparse artificial neural networks trained with adaptive topology. We focus on a simple yet effective architecture consisting of three sparse layers with 99% sparsity followed by a dense layer, applied to image classification tasks such as MNIST and Fashion MNIST. By updating the topology of the sparse layers between each epoch, we achieve competitive accuracy despite the significantly reduced number of weights. Our primary contribution is a detailed analysis of the robustness of these networks, exploring their performance under various perturbations including random link removal, adversarial attack, and link weight shuffling. Through extensive experiments, we demonstrate that adaptive topology not only enhances efficiency but also maintains robustness. This work highlights the potential of adaptive sparse networks as a promising direction for developing efficient and reliable deep learning models.

</details>


### [185] [Compact Circulant Layers with Spectral Priors](https://arxiv.org/abs/2602.21965)
*Joseph Margaryan,Thomas Hamelryck*

Main category: cs.LG

TL;DR: Compact spectral circulant/BCCB layers for memory-efficient uncertainty-aware neural networks, enabling exact Lipschitz bounds and structured variational inference with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Critical applications in medicine, robotics, and autonomous systems require compact, uncertainty-aware neural networks suitable for resource-constrained edge deployments. There's a need for memory-efficient models that can provide uncertainty quantification and robustness guarantees.

Method: Proposes spectral circulant and BCCB layers - FFT-diagonalizable circular convolutions parameterized in the real FFT domain. Uses spectral parameterization to impose structure, perform structured variational inference in low-dimensional weight space, and calculate exact layer spectral norms. Places independent complex Gaussians on Hermitian support to induce stationary Gaussian-process priors over filters.

Result: Spectral circulant/BCCB layers match strong baselines while using substantially fewer parameters and providing tighter Lipschitz certificates. Demonstrated effectiveness in compact Bayesian neural networks on MNIST->Fashion-MNIST, variational heads on frozen CIFAR-10 features, and deterministic ViT projections on CIFAR-10/Tiny ImageNet.

Conclusion: Spectral circulant and BCCB layers are effective compact building blocks for both Bayesian and point estimate regimes, offering parameter efficiency, exact Lipschitz bounds, and uncertainty quantification suitable for resource-constrained applications.

Abstract: Critical applications in areas such as medicine, robotics and autonomous systems require compact (i.e., memory efficient), uncertainty-aware neural networks suitable for edge and other resource-constrained deployments. We study compact spectral circulant and block-circulant-with-circulant-blocks (BCCB) layers: FFT-diagonalizable circular convolutions whose weights live directly in the real FFT (RFFT) half (1D) or half-plane (2D). Parameterizing filters in the frequency domain lets us impose simple spectral structure, perform structured variational inference in a low-dimensional weight space, and calculate exact layer spectral norms, enabling inexpensive global Lipschitz bounds and margin-based robustness diagnostics. By placing independent complex Gaussians on the Hermitian support we obtain a discrete instance of the spectral representation of stationary kernels, inducing an exact stationary Gaussian-process prior over filters on the discrete circle/torus. We exploit this to define a practical spectral prior and a Hermitian-aware low-rank-plus-diagonal variational posterior in real coordinates. Empirically, spectral circulant/BCCB layers are effective compact building blocks in both (variational) Bayesian and point estimate regimes: compact Bayesian neural networks on MNIST->Fashion-MNIST, variational heads on frozen CIFAR-10 features, and deterministic ViT projections on CIFAR-10/Tiny ImageNet; spectral layers match strong baselines while using substantially fewer parameters and with tighter Lipschitz certificates.

</details>


### [186] [Neural solver for Wasserstein Geodesics and optimal transport dynamics](https://arxiv.org/abs/2602.22003)
*Hailiang Liu,Yan-Han Chen*

Main category: cs.LG

TL;DR: Neural solver for Wasserstein geodesics and velocity fields using minimax optimization with deep networks.


<details>
  <summary>Details</summary>
Motivation: The machine learning community has embraced optimal transport for modeling distributional relationships, but there's a need for efficient methods to compute Wasserstein geodesics and associated velocity fields between distributions.

Method: Recasts the constrained optimization as a minimax problem using deep neural networks to approximate relevant functions. Builds on the dynamical formulation of OT to compute Wasserstein geodesics and recover OT maps for direct sampling.

Result: The method effectively computes Wasserstein geodesics, recovers OT maps for direct sampling from target distributions, and learns full velocity fields. Works with general cost functions including quadratic cost.

Conclusion: The neural solver provides a flexible framework for computing Wasserstein geodesics and velocity fields, demonstrated effective on both synthetic and real datasets.

Abstract: In recent years, the machine learning community has increasingly embraced the optimal transport (OT) framework for modeling distributional relationships. In this work, we introduce a sample-based neural solver for computing the Wasserstein geodesic between a source and target distribution, along with the associated velocity field. Building on the dynamical formulation of the optimal transport (OT) problem, we recast the constrained optimization as a minimax problem, using deep neural networks to approximate the relevant functions. This approach not only provides the Wasserstein geodesic but also recovers the OT map, enabling direct sampling from the target distribution. By estimating the OT map, we obtain velocity estimates along particle trajectories, which in turn allow us to learn the full velocity field. The framework is flexible and readily extends to general cost functions, including the commonly used quadratic cost. We demonstrate the effectiveness of our method through experiments on both synthetic and real datasets.

</details>


### [187] [Function-Space Empirical Bayes Regularisation with Student's t Priors](https://arxiv.org/abs/2602.22015)
*Pengcheng Hao,Ercan Engin Kuruoglu*

Main category: cs.LG

TL;DR: Proposes ST-FS-EB, a function-space empirical Bayes framework using heavy-tailed Student's t priors for Bayesian deep learning, outperforming Gaussian-based methods in uncertainty estimation and robustness.


<details>
  <summary>Details</summary>
Motivation: Current function-space variational inference methods in Bayesian deep learning rely on Gaussian priors that fail to capture the heavy-tailed statistical characteristics of neural network outputs, limiting their ability to model uncertainty effectively.

Method: ST-FS-EB framework employs heavy-tailed Student's t priors in both parameter and function spaces, approximates posterior distribution through variational inference, and uses Monte Carlo dropout for evidence lower bound optimization.

Result: The method demonstrates robust performance compared to various VI-based BDL baselines, showing improvements in in-distribution prediction, out-of-distribution detection, and handling distribution shifts.

Conclusion: Heavy-tailed Student's t priors in function-space empirical Bayes regularization provide better uncertainty modeling for Bayesian deep learning than traditional Gaussian priors, leading to more reliable uncertainty estimates and improved robustness.

Abstract: Bayesian deep learning (BDL) has emerged as a principled approach to produce reliable uncertainty estimates by integrating deep neural networks with Bayesian inference, and the selection of informative prior distributions remains a significant challenge. Various function-space variational inference (FSVI) regularisation methods have been presented, assigning meaningful priors over model predictions. However, these methods typically rely on a Gaussian prior, which fails to capture the heavy-tailed statistical characteristics inherent in neural network outputs. By contrast, this work proposes a novel function-space empirical Bayes regularisation framework -- termed ST-FS-EB -- which employs heavy-tailed Student's $t$ priors in both parameter and function spaces. Also, we approximate the posterior distribution through variational inference (VI), inducing an evidence lower bound (ELBO) objective based on Monte Carlo (MC) dropout. Furthermore, the proposed method is evaluated against various VI-based BDL baselines, and the results demonstrate its robust performance in in-distribution prediction, out-of-distribution (OOD) detection and handling distribution shifts.

</details>


### [188] [Disease Progression and Subtype Modeling for Combined Discrete and Continuous Input Data](https://arxiv.org/abs/2602.22018)
*Sterre de Jonge,Elisabeth J. Vinke,Meike W. Vernooij,Daniel C. Alexander,Alexandra L. Young,Esther E. Bron*

Main category: cs.LG

TL;DR: Mixed-SuStaIn extends disease progression modeling to handle both discrete and continuous data types within the SuStaIn framework, enabling subtype and progression analysis on heterogeneous real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Most disease progression models are limited to single data types (e.g., continuous only), which restricts their applicability to real-world datasets that often contain heterogeneous data types including both discrete and continuous biomarkers.

Method: Proposed Mixed Events model that handles both discrete and continuous data types, implemented within the Subtype and Stage Inference (SuStaIn) framework as Mixed-SuStaIn, enabling simultaneous subtype identification and progression modeling.

Result: Mixed-SuStaIn performs well on mixed datasets in both simulation experiments and real-world Alzheimer's Disease Neuroimaging Initiative data, demonstrating effectiveness in handling heterogeneous data types.

Conclusion: Mixed-SuStaIn addresses a key limitation in disease progression modeling by enabling analysis of mixed discrete and continuous data, making it more applicable to real-world heterogeneous datasets for diseases like Alzheimer's.

Abstract: Disease progression modeling provides a robust framework to identify long-term disease trajectories from short-term biomarker data. It is a valuable tool to gain a deeper understanding of diseases with a long disease trajectory, such as Alzheimer's disease. A key limitation of most disease progression models is that they are specific to a single data type (e.g., continuous data), thereby limiting their applicability to heterogeneous, real-world datasets. To address this limitation, we propose the Mixed Events model, a novel disease progression model that handles both discrete and continuous data types. This model is implemented within the Subtype and Stage Inference (SuStaIn) framework, resulting in Mixed-SuStaIn, enabling subtype and progression modeling. We demonstrate the effectiveness of Mixed-SuStaIn through simulation experiments and real-world data from the Alzheimer's Disease Neuroimaging Initiative, showing that it performs well on mixed datasets. The code is available at: https://github.com/ucl-pond/pySuStaIn.

</details>


### [189] [Sample Complexity Bounds for Robust Mean Estimation with Mean-Shift Contamination](https://arxiv.org/abs/2602.22130)
*Ilias Diakonikolas,Giannis Iakovidis,Daniel M. Kane,Sihan Liu*

Main category: cs.LG

TL;DR: The paper resolves the open question of sample complexity for mean estimation in the mean-shift contamination model for general distributions, showing efficient algorithms exist under mild spectral conditions.


<details>
  <summary>Details</summary>
Motivation: Prior work only characterized sample complexity for Gaussian and Laplace distributions in mean-shift contamination, showing consistent estimation is possible (unlike Huber's model). The open question was to determine sample complexity for general base distributions.

Method: Uses Fourier analysis and introduces the concept of a "Fourier witness" as a key ingredient. The approach relies on spectral conditions on the characteristic function of the potentially multivariate base distribution.

Result: Shows that under mild spectral conditions, there exists a sample-efficient algorithm that estimates the target mean to any desired accuracy. Provides both upper bounds and qualitatively matching sample complexity lower bounds.

Conclusion: Essentially resolves the open question about sample complexity of mean estimation in mean-shift contamination for general distributions, demonstrating that efficient estimation is possible under reasonable conditions using Fourier analytic techniques.

Abstract: We study the basic task of mean estimation in the presence of mean-shift contamination. In the mean-shift contamination model, an adversary is allowed to replace a small constant fraction of the clean samples by samples drawn from arbitrarily shifted versions of the base distribution. Prior work characterized the sample complexity of this task for the special cases of the Gaussian and Laplace distributions. Specifically, it was shown that consistent estimation is possible in these cases, a property that is provably impossible in Huber's contamination model. An open question posed in earlier work was to determine the sample complexity of mean estimation in the mean-shift contamination model for general base distributions. In this work, we study and essentially resolve this open question. Specifically, we show that, under mild spectral conditions on the characteristic function of the (potentially multivariate) base distribution, there exists a sample-efficient algorithm that estimates the target mean to any desired accuracy. We complement our upper bound with a qualitatively matching sample complexity lower bound. Our techniques make critical use of Fourier analysis, and in particular introduce the notion of a Fourier witness as an essential ingredient of our upper and lower bounds.

</details>


### [190] [SigmaQuant: Hardware-Aware Heterogeneous Quantization Method for Edge DNN Inference](https://arxiv.org/abs/2602.22136)
*Qunyou Liu,Pengbo Yu,Marina Zapater,David Atienza*

Main category: cs.LG

TL;DR: SigmaQuant is an adaptive layer-wise heterogeneous quantization framework that efficiently balances accuracy and resource usage for DNN deployment on edge devices without exhaustive search.


<details>
  <summary>Details</summary>
Motivation: DNN deployment on edge/mobile devices faces severe resource constraints (memory, energy, computation). Uniform quantization fails to leverage varying layer robustness and causes accuracy degradation at low bitwidths, while existing heterogeneous quantization methods require exhaustive search or lack adaptability to different hardware conditions.

Method: SigmaQuant is an adaptive layer-wise heterogeneous quantization framework that allocates different bitwidths to individual layers based on their robustness, without requiring exhaustive design space search.

Result: The framework efficiently balances accuracy and resource usage for varied edge environments, addressing the limitations of both uniform quantization and existing heterogeneous quantization methods.

Conclusion: SigmaQuant provides an effective solution for deploying DNNs on resource-constrained edge devices by enabling adaptive heterogeneous quantization that doesn't require exhaustive search while maintaining accuracy.

Abstract: Deep neural networks (DNNs) are essential for performing advanced tasks on edge or mobile devices, yet their deployment is often hindered by severe resource constraints, including limited memory, energy, and computational power. While uniform quantization provides a straightforward approach to compress model and reduce hardware requirement, it fails to fully leverage the varying robustness across layers, and often lead to accuracy degradation or suboptimal resource usage, particularly at low bitwidths. In contrast, heterogeneous quantization, which allocates different bitwidths to individual layers, can mitigate these drawbacks. Nonetheless, current heterogeneous quantization methods either needs huge brute-force design space search or lacks the adaptability to meet different hardware conditions, such as memory size, energy budget, and latency requirement. Filling these gaps, this work introduces \textbf{\textit{SigmaQuant}}, an adaptive layer-wise heterogeneous quantization framework designed to efficiently balance accuracy and resource usage for varied edge environments without exhaustive search.

</details>


### [191] [Learning and Naming Subgroups with Exceptional Survival Characteristics](https://arxiv.org/abs/2602.22179)
*Mhd Jawad Al Rahwanji,Sascha Xu,Nils Philipp Walter,Jilles Vreeken*

Main category: cs.LG

TL;DR: Sysurv is a differentiable, non-parametric method that uses random survival forests to learn individual survival curves and automatically discovers interpretable subgroups with exceptional survival characteristics.


<details>
  <summary>Details</summary>
Motivation: There's a need to identify subpopulations with different survival patterns in applications like medicine (treatment effectiveness) and predictive maintenance (component failure). Existing methods have limitations: they require restrictive assumptions (e.g., proportional hazards), pre-discretized features, and tend to overlook individual deviations by comparing average statistics.

Method: Sysurv is a fully differentiable, non-parametric method that leverages random survival forests to learn individual survival curves. It automatically learns conditions and combines them into inherently interpretable rules to select subgroups with exceptional survival characteristics.

Result: Empirical evaluation on a wide range of datasets and settings, including a cancer data case study, shows that Sysurv reveals insightful and actionable survival subgroups.

Conclusion: Sysurv provides an effective approach for discovering interpretable subgroups with exceptional survival patterns without restrictive modeling assumptions, addressing limitations of existing methods.

Abstract: In many applications, it is important to identify subpopulations that survive longer or shorter than the rest of the population. In medicine, for example, it allows determining which patients benefit from treatment, and in predictive maintenance, which components are more likely to fail. Existing methods for discovering subgroups with exceptional survival characteristics require restrictive assumptions about the survival model (e.g. proportional hazards), pre-discretized features, and, as they compare average statistics, tend to overlook individual deviations. In this paper, we propose Sysurv, a fully differentiable, non-parametric method that leverages random survival forests to learn individual survival curves, automatically learns conditions and how to combine these into inherently interpretable rules, so as to select subgroups with exceptional survival characteristics. Empirical evaluation on a wide range of datasets and settings, including a case study on cancer data, shows that Sysurv reveals insightful and actionable survival subgroups.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [192] [Cross domain Persistent Monitoring for Hybrid Aerial Underwater Vehicles](https://arxiv.org/abs/2602.21259)
*Ricardo B. Grando,Victor A. Kich,Alisson H. Kolling,Junior C. D. Jesus,Rodrigo S. Guerra,Paulo L. J. Drews-Jr*

Main category: cs.RO

TL;DR: A unified DRL framework using transfer learning enables hybrid aerial-underwater vehicles to perform persistent monitoring across air and water domains with shared Lidar/Sonar sensor policies.


<details>
  <summary>Details</summary>
Motivation: Hybrid Unmanned Aerial Underwater Vehicles (HUAUVs) offer unique capabilities for inspection, mapping, search and rescue in challenging environments, but face significant challenges due to distinct air/water dynamics and constraints that require novel cross-domain methodologies.

Method: Combines Deep Reinforcement Learning (DRL) with Transfer Learning to create cross-domain adaptability. Uses a shared DRL architecture trained on Lidar sensor data for aerial operations and Sonar data for underwater operations, enabling a unified policy for both environments.

Result: Demonstrates feasibility of unified policy for both aerial and underwater environments, with promising results that account for environmental uncertainty and dynamics of multiple mobile targets.

Conclusion: The proposed framework establishes groundwork for scalable autonomous persistent monitoring solutions based on DRL for hybrid aerial-underwater vehicles, enabling cross-domain operations with shared policies.

Abstract: Hybrid Unmanned Aerial Underwater Vehicles (HUAUVs) have emerged as platforms capable of operating in both aerial and underwater environments, enabling applications such as inspection, mapping, search, and rescue in challenging scenarios. However, the development of novel methodologies poses significant challenges due to the distinct dynamics and constraints of the air and water domains. In this work, we present persistent monitoring tasks for HUAUVs by combining Deep Reinforcement Learning (DRL) and Transfer Learning to enable cross-domain adaptability. Our approach employs a shared DRL architecture trained on Lidar sensor data (on air) and Sonar data (underwater), demonstrating the feasibility of a unified policy for both environments. We further show that the methodology presents promising results, taking into account the uncertainty of the environment and the dynamics of multiple mobile targets. The proposed framework lays the groundwork for scalable autonomous persistent monitoring solutions based on DRL for hybrid aerial-underwater vehicles.

</details>


### [193] [Dual-Branch INS/GNSS Fusion with Inequality and Equality Constraints](https://arxiv.org/abs/2602.21266)
*Mor Levenhar,Itzik Klein*

Main category: cs.RO

TL;DR: Dual-branch information aiding framework fuses equality and inequality motion constraints to improve urban vehicle navigation during GNSS outages without additional hardware.


<details>
  <summary>Details</summary>
Motivation: Urban vehicle navigation suffers from GNSS signal blockages, and existing methods like non-holonomic constraints use rigid equality assumptions that fail under dynamic driving conditions when aiding is most needed.

Method: Proposes a dual-branch information aiding framework that fuses equality and inequality motion constraints through a variance-weighted scheme, requiring only software modification to existing navigation filters with no additional sensors.

Result: On 4 urban datasets (4.3 hours total): Under full GNSS availability, reduces vertical position error by 16.7% and improves altitude accuracy by 50.1%; under GNSS-denied conditions, reduces vertical drift by 24.2% and improves altitude accuracy by 20.2%.

Conclusion: Replacing hard motion equality assumptions with physically motivated inequality bounds is a practical, cost-free strategy for improving navigation resilience, continuity, and drift robustness without additional sensors, map data, or learned models.

Abstract: Reliable vehicle navigation in urban environments remains a challenging problem due to frequent satellite signal blockages caused by tall buildings and complex infrastructure. While fusing inertial reading with satellite positioning in an extended Kalman filter provides short-term navigation continuity, low-cost inertial sensors suffer from rapid error accumulation during prolonged outages. Existing information aiding approaches, such as the non-holonomic constraint, impose rigid equality assumptions on vehicle motion that may be violated under dynamic urban driving conditions, limiting their robustness precisely when aiding is most needed. In this paper, we propose a dual-branch information aiding framework that fuses equality and inequality motion constraints through a variance-weighted scheme, requiring only a software modification to an existing navigation filter with no additional sensors or hardware. The proposed method is evaluated on four publicly available urban datasets featuring various inertial sensors, road conditions, and dynamics, covering a total duration of 4.3 hours of recorded data. Under Full GNSS availability, the method reduces vertical position error by 16.7% and improves altitude accuracy by 50.1% over the standard non-holonomic constraint. Under GNSS-denied conditions, vertical drift is reduced by 24.2% and altitude accuracy improves by 20.2%. These results demonstrate that replacing hard motion equality assumptions with physically motivated inequality bounds is a practical and cost-free strategy for improving navigation resilience, continuity, and drift robustness without relying on additional sensors, map data, or learned models.

</details>


### [194] [Learning Deformable Object Manipulation Using Task-Level Iterative Learning Control](https://arxiv.org/abs/2602.21302)
*Krishna Suresh,Chris Atkeson*

Main category: cs.RO

TL;DR: A Task-Level Iterative Learning Control method enables dynamic manipulation of deformable objects like ropes, achieving 100% success rate within 10 trials across 7 different rope types and showing transfer learning capabilities.


<details>
  <summary>Details</summary>
Motivation: Dynamic manipulation of deformable objects is challenging due to infinite degrees of freedom and underactuated dynamics. Existing methods often require large amounts of demonstration data or extensive simulation, which is impractical.

Method: Task-Level Iterative Learning Control that uses a single human demonstration and simplified rope model. The algorithm constructs a local inverse model of robot and rope by solving a quadratic program to propagate task-space errors into action updates, learning directly on hardware.

Result: Achieved 100% success rate within 10 trials across 7 different rope types (chain, latex surgical tubing, braided/twisted ropes, thickness 7-25mm, density 0.013-0.5 kg/m). Successfully transferred between most rope types in 2-5 trials.

Conclusion: The method enables efficient learning of dynamic deformable object manipulation with minimal demonstration data, works across diverse materials, and demonstrates effective transfer learning capabilities for practical robotic applications.

Abstract: Dynamic manipulation of deformable objects is challenging for humans and robots because they have infinite degrees of freedom and exhibit underactuated dynamics. We introduce a Task-Level Iterative Learning Control method for dynamic manipulation of deformable objects. We demonstrate this method on a non-planar rope manipulation task called the flying knot. Using a single human demonstration and a simplified rope model, the method learns directly on hardware without reliance on large amounts of demonstration data or massive amounts of simulation. At each iteration, the algorithm constructs a local inverse model of the robot and rope by solving a quadratic program to propagate task-space errors into action updates. We evaluate performance across 7 different kinds of ropes, including chain, latex surgical tubing, and braided and twisted ropes, ranging in thicknesses of 7--25mm and densities of 0.013--0.5 kg/m. Learning achieves a 100\% success rate within 10 trials on all ropes. Furthermore, the method can successfully transfer between most rope types in approximately 2--5 trials. https://flying-knots.github.io

</details>


### [195] [Unified Complementarity-Based Contact Modeling and Planning for Soft Robots](https://arxiv.org/abs/2602.21316)
*Milad Azizkhani,Yue Chen*

Main category: cs.RO

TL;DR: CUSP: A unified complementarity-based framework for soft-robot contact modeling and planning that addresses numerical challenges in contact-rich interactions through a three-stage conditioning pipeline and kinematically guided warm-start strategy.


<details>
  <summary>Details</summary>
Motivation: Soft robots need to safely interact with environments through contact, but modeling and planning contact-rich interactions is challenging due to redundant constraints, rank-deficient LCPs, and ill-conditioning from stiffness-friction disparity. Existing approaches use problem-specific approximations or penalty-based treatments.

Method: Developed a unified complementarity-based framework with a robust LCP model for discretized soft robots. Uses a three-stage conditioning pipeline: 1) inertial rank selection to remove redundant contacts, 2) Ruiz equilibration to correct scale disparity and ill-conditioning, and 3) lightweight Tikhonov regularization on normal blocks. Also introduced a kinematically guided warm-start strategy for dynamic trajectory optimization using MPCC.

Result: Demonstrated effectiveness on contact-rich ball manipulation tasks. The framework enables dynamic trajectory optimization through contact and provides physically consistent formulation for contact modeling, manipulation, and planning.

Conclusion: CUSP provides a new foundation for unifying contact modeling, simulation, and planning in soft robotics, addressing fundamental challenges in contact-rich interactions through a robust, unified complementarity-based approach.

Abstract: Soft robots were introduced in large part to enable safe, adaptive interaction with the environment, and this interaction relies fundamentally on contact. However, modeling and planning contact-rich interactions for soft robots remain challenging: dense contact candidates along the body create redundant constraints and rank-deficient LCPs, while the disparity between high stiffness and low friction introduces severe ill-conditioning. Existing approaches rely on problem-specific approximations or penalty-based treatments. This letter presents a unified complementarity-based framework for soft-robot contact modeling and planning that brings contact modeling, manipulation, and planning into a unified, physically consistent formulation. We develop a robust Linear Complementarity Problem (LCP) model tailored to discretized soft robots and address these challenges with a three-stage conditioning pipeline: inertial rank selection to remove redundant contacts, Ruiz equilibration to correct scale disparity and ill-conditioning, and lightweight Tikhonov regularization on normal blocks. Building on the same formulation, we introduce a kinematically guided warm-start strategy that enables dynamic trajectory optimization through contact using Mathematical Programs with Complementarity Constraints (MPCC) and demonstrate its effectiveness on contact-rich ball manipulation tasks. In conclusion, CUSP provides a new foundation for unifying contact modeling, simulation, and planning in soft robotics.

</details>


### [196] [CableRobotGraphSim: A Graph Neural Network for Modeling Partially Observable Cable-Driven Robot Dynamics](https://arxiv.org/abs/2602.21331)
*Nelson Chen,William R. Johnson,Rebecca Kramer-Bottiglio,Kostas Bekris,Mridul Aanjaneya*

Main category: cs.RO

TL;DR: A GNN-based simulator for cable-driven robots that uses graph representations for fast, accurate simulation with partial observability, combined with sim-and-real co-training and MPPI control.


<details>
  <summary>Details</summary>
Motivation: Traditional physics-based simulators require full-state observability or parameter search for system identification, limiting their practicality for cable-driven robots. There's a need for simulators that can work with partial observations and generalize well to real-world data.

Method: Proposes CableRobotGraphSim, a GNN model that represents cable-driven robots as graphs (rigid-bodies as nodes, cables and contacts as edges). Uses sim-and-real co-training procedure for generalization and robustness. Integrates with Model Predictive Path Integral (MPPI) controller for closed-loop navigation.

Result: The model can quickly and accurately match properties of other simulation models and real robots while ingesting only partially observable inputs. The integration with MPPI controller demonstrates the model's speed and accuracy for closed-loop navigation.

Conclusion: CableRobotGraphSim addresses limitations of traditional simulators by providing a fast, accurate GNN-based approach that works with partial observability and generalizes well through co-training, enabling effective closed-loop control for cable-driven robots.

Abstract: General-purpose simulators have accelerated the development of robots. Traditional simulators based on first-principles, however, typically require full-state observability or depend on parameter search for system identification. This work presents \texttt{CableRobotGraphSim}, a novel Graph Neural Network (GNN) model for cable-driven robots that aims to address shortcomings of prior simulation solutions. By representing cable-driven robots as graphs, with the rigid-bodies as nodes and the cables and contacts as edges, this model can quickly and accurately match the properties of other simulation models and real robots, while ingesting only partially observable inputs. Accompanying the GNN model is a sim-and-real co-training procedure that promotes generalization and robustness to noisy real data. This model is further integrated with a Model Predictive Path Integral (MPPI) controller for closed-loop navigation, which showcases the model's speed and accuracy.

</details>


### [197] [Environment-Aware Learning of Smooth GNSS Covariance Dynamics for Autonomous Racing](https://arxiv.org/abs/2602.21366)
*Y. Deemo Chen,Arion Zimmermann,Thomas A. Berrueta,Soon-Jo Chung*

Main category: cs.RO

TL;DR: LACE: A learning-based framework that models GNSS measurement covariance as an exponentially stable dynamical system with formal stability guarantees, improving localization in challenging autonomous racing environments.


<details>
  <summary>Details</summary>
Motivation: Accurate and stable state estimation is critical for safety-critical domains like high-speed autonomous racing, where measurement uncertainty must be both adaptive to the environment and temporally smooth for control. Existing approaches may not adequately handle the temporal dynamics of GNSS measurement covariance in challenging, degraded environments.

Method: Developed LACE framework that models covariance evolution as an exponentially stable dynamical system. Uses a deep neural network with attention mechanism to predict system's process noise from environmental features. Employs contraction-based stability and systematically imposes spectral constraints to guarantee exponential stability and smoothness of covariance dynamics.

Result: Validated on AV-24 autonomous racecar, demonstrating improved localization performance and smoother covariance estimates in challenging, GNSS-degraded environments. The framework provides formal guarantees of exponential stability and smoothness for covariance dynamics.

Conclusion: LACE shows promise for dynamically modeling perceived uncertainty in state estimation problems that are tightly coupled with control sensitivity, particularly in safety-critical applications like autonomous racing where both accuracy and stability are essential.

Abstract: Ensuring accurate and stable state estimation is a challenging task crucial to safety-critical domains such as high-speed autonomous racing, where measurement uncertainty must be both adaptive to the environment and temporally smooth for control. In this work, we develop a learning-based framework, LACE, capable of directly modeling the temporal dynamics of GNSS measurement covariance. We model the covariance evolution as an exponentially stable dynamical system where a deep neural network (DNN) learns to predict the system's process noise from environmental features through an attention mechanism. By using contraction-based stability and systematically imposing spectral constraints, we formally provide guarantees of exponential stability and smoothness for the resulting covariance dynamics. We validate our approach on an AV-24 autonomous racecar, demonstrating improved localization performance and smoother covariance estimates in challenging, GNSS-degraded environments. Our results highlight the promise of dynamically modeling the perceived uncertainty in state estimation problems that are tightly coupled with control sensitivity.

</details>


### [198] [Autonomous Sea Turtle Robot for Marine Fieldwork](https://arxiv.org/abs/2602.21389)
*Zach J. Patterson,Emily Sologuren,Levi Cai,Daniel Kim,Alaa Maalouf,Pascal Spino,Daniela Rus*

Main category: cs.RO

TL;DR: Researchers developed a sea turtle-inspired autonomous underwater robot with integrated vision-driven control for close-range monitoring in fragile marine ecosystems, demonstrating successful animal tracking and obstacle avoidance in real aquarium environments.


<details>
  <summary>Details</summary>
Motivation: Current autonomous underwater vehicles struggle with close-range operation in cluttered reef habitats due to challenges with fragile structures, currents, variable illumination, and limited sensing. Existing bioinspired platforms lack field-ready autonomy.

Method: Created a sea turtle-inspired robot with integrated vision-driven control stack combining depth-heading stabilization, obstacle avoidance, and target-centric control. Used both controlled pool experiments and live coral reef exhibit testing at the New England Aquarium.

Result: Achieved 91% success rate in obstacle avoidance during off-tether aquarium experiments. Demonstrated stable operation and reliable tracking of fast-moving marine animals and human divers. Developed low-compute onboard tracking mode.

Conclusion: This integrated biomimetic system represents the first practical approach for soft-rigid hybrid, bioinspired underwater robots capable of minimally disruptive exploration and close-range monitoring in sensitive marine ecosystems.

Abstract: Autonomous robots can transform how we observe marine ecosystems, but close-range operation in reefs and other cluttered habitats remains difficult. Vehicles must maneuver safely near animals and fragile structures while coping with currents, variable illumination and limited sensing. Previous approaches simplify these problems by leveraging soft materials and bioinspired swimming designs, but such platforms remain limited in terms of deployable autonomy. Here we present a sea turtle-inspired autonomous underwater robot that closed the gap between bioinspired locomotion and field-ready autonomy through a tightly integrated, vision-driven control stack. The robot combines robust depth-heading stabilization with obstacle avoidance and target-centric control, enabling it to track and interact with moving objects in complex terrain. We validate the robot in controlled pool experiments and in a live coral reef exhibit at the New England Aquarium, demonstrating stable operation and reliable tracking of fast-moving marine animals and human divers. To the best of our knowledge, this is the first integrated biomimetic robotic system, combining novel hardware, control, and field experiments, deployed to track and monitor real marine animals in their natural environment. During off-tether experiments, we demonstrate safe navigation around obstacles (91\% success rate in the aquarium exhibit) and introduce a low-compute onboard tracking mode. Together, these results establish a practical route toward soft-rigid hybrid, bioinspired underwater robots capable of minimally disruptive exploration and close-range monitoring in sensitive ecosystems.

</details>


### [199] [Event-Driven On-Sensor Locomotion Mode Recognition Using a Shank-Mounted IMU with Embedded Machine Learning for Exoskeleton Control](https://arxiv.org/abs/2602.21418)
*Mohammadsaleh Razmi,Iman Shojaei*

Main category: cs.RO

TL;DR: Wearable HAR system performs real-time activity recognition directly inside IMU sensor using embedded ML core, enabling low-latency exoskeleton control with minimal microcontroller power consumption.


<details>
  <summary>Details</summary>
Motivation: To enable low-latency control of lower-limb exoskeletons while reducing power consumption and computational overhead compared to conventional approaches that stream raw IMU data to microcontrollers.

Method: Uses STMicroelectronics LSM6DSV16X IMU with embedded Machine Learning Core (MLC) to execute activity recognition at sensor level. A lightweight decision-tree model is deployed on-sensor using ST MEMS Studio, enabling interrupt-driven operation where microcontroller only wakes to read classification results.

Result: System successfully recognizes three locomotion modes (stance, level walking, stair ascent) with reduced computation/communication overhead, preserved battery energy, and improved robustness in distinguishing level walking from stair ascent for torque-assist control.

Conclusion: On-sensor inference architecture enables efficient, low-power human activity recognition for exoskeleton control by moving ML processing to the sensor itself, reducing microcontroller workload and enabling responsive torque assistance.

Abstract: This work presents a wearable human activity recognition (HAR) system that performs real-time inference directly inside a shank-mounted inertial measurement unit (IMU) to support low-latency control of a lower-limb exoskeleton. Unlike conventional approaches that continuously stream raw inertial data to a microcontroller for classification, the proposed system executes activity recognition at the sensor level using the embedded Machine Learning Core (MLC) of the STMicroelectronics LSM6DSV16X IMU, allowing the host microcontroller to remain in a low-power state and read only the recognized activity label from IMU registers. While the system generalizes to multiple human activities, this paper focuses on three representative locomotion modes - stance, level walking, and stair ascent - using data collected from adult participants. A lightweight decision-tree model was configured and deployed for on-sensor execution using ST MEMS Studio, enabling continuous operation without custom machine learning code on the microcontroller. During operation, the IMU asserts an interrupt when motion or a new classification is detected; the microcontroller wakes, reads the MLC output registers, and forwards the inferred mode to the exoskeleton controller. This interrupt-driven, on-sensor inference architecture reduces computation and communication overhead while preserving battery energy and improving robustness in distinguishing level walking from stair ascent for torque-assist control.

</details>


### [200] [VLA Knows Its Limits](https://arxiv.org/abs/2602.21445)
*Haoxuan Wang,Gengyu Zhang,Yan Yan,Ramana Rao Kompella,Gaowen Liu*

Main category: cs.RO

TL;DR: AutoHorizon: A test-time method that dynamically estimates the optimal execution horizon for action chunks in flow-based VLA models to adapt to changing perceptual conditions.


<details>
  <summary>Details</summary>
Motivation: The execution horizon (number of actions executed from each predicted chunk) significantly affects performance in flow-based VLA models, but its effect and optimal choice remain underexplored. Performance shows a non-monotonic pattern - initially improving then declining with longer horizons.

Method: Analyzed cross- and self-attention weights in flow-based VLAs to discover: (1) intra-chunk actions attend invariantly to vision-language tokens, limiting adaptability; (2) initial/terminal action tokens serve as stable anchors. Used action self-attention weights as proxy for predictive limit to develop AutoHorizon - a test-time method that dynamically estimates execution horizon for each predicted action chunk.

Result: AutoHorizon performs well across simulated and real-world robotic manipulation tasks, incurs negligible computational overhead, and generalizes across diverse tasks and flow-based models.

Conclusion: Dynamic execution horizon estimation via AutoHorizon addresses limitations of fixed execution horizons in flow-based VLA models, enabling better adaptation to changing perceptual conditions while maintaining computational efficiency and broad applicability.

Abstract: Action chunking has recently emerged as a standard practice in flow-based Vision-Language-Action (VLA) models. However, the effect and choice of the execution horizon - the number of actions to be executed from each predicted chunk - remains underexplored. In this work, we first show that varying the execution horizon leads to substantial performance deviations, with performance initially improving and then declining as the horizon increases. To uncover the reasons, we analyze the cross- and self-attention weights in flow-based VLAs and reveal two key phenomena: (i) intra-chunk actions attend invariantly to vision-language tokens, limiting adaptability to environmental changes; and (ii) the initial and terminal action tokens serve as stable anchors, forming latent centers around which intermediate actions are organized. Motivated by these insights, we interpret action self-attention weights as a proxy for the model's predictive limit and propose AutoHorizon, the first test-time method that dynamically estimates the execution horizon for each predicted action chunk to adapt to changing perceptual conditions. Across simulated and real-world robotic manipulation tasks, AutoHorizon is performant, incurs negligible computational overhead, and generalizes across diverse tasks and flow-based models.

</details>


### [201] [Constructive Vector Fields for Path Following in Fully-Actuated Systems on Matrix Lie Groups](https://arxiv.org/abs/2602.21450)
*Felipe Bartelt,Vinicius M. Gonçalves,Luciano C. A. Pimenta*

Main category: cs.RO

TL;DR: A novel vector field control strategy for fully-actuated systems on matrix Lie groups that ensures convergence to and traversal along curves, with non-redundant control inputs matching the group dimension rather than embedding space dimension.


<details>
  <summary>Details</summary>
Motivation: To extend previous vector field control work from Euclidean translation groups to general connected matrix Lie groups, enabling more practical control inputs and applications to systems like SE(3) for controlling freely moving objects.

Method: Generalizes previous vector field approach by leveraging Lie group properties to maintain orthogonality between convergent and traversal components, ensuring non-redundant control inputs matching the Lie group dimension, with an efficient algorithm for SE(3).

Result: The method successfully controls systems on matrix Lie groups with non-redundant inputs, validated experimentally using a robotic manipulator, with particular effectiveness for SE(3) applications like omnidirectional drone control.

Conclusion: The proposed vector field strategy provides a general framework for controlling fully-actuated systems on matrix Lie groups with practical non-redundant inputs, particularly valuable for SE(3) applications involving free motion and rotation.

Abstract: This paper presents a novel vector field strategy for controlling fully-actuated systems on connected matrix Lie groups, ensuring convergence to and traversal along a curve defined on the group. Our approach generalizes our previous work (Rezende et al., 2022) and reduces to it when considering the Lie group of translations in Euclidean space. Since the proofs in Rezende et al. (2022) rely on key properties such as the orthogonality between the convergent and traversal components, we extend these results by leveraging Lie group properties. These properties also allow the control input to be non-redundant, meaning it matches the dimension of the Lie group, rather than the potentially larger dimension of the space in which the group is embedded. This can lead to more practical control inputs in certain scenarios. A particularly notable application of our strategy is in controlling systems on SE(3) -- in this case, the non-redundant input corresponds to the object's mechanical twist -- making it well-suited for controlling objects that can move and rotate freely, such as omnidirectional drones. In this case, we provide an efficient algorithm to compute the vector field. We experimentally validate the proposed method using a robotic manipulator to demonstrate its effectiveness.

</details>


### [202] [LiLo-VLA: Compositional Long-Horizon Manipulation via Linked Object-Centric Policies](https://arxiv.org/abs/2602.21531)
*Yue Yang,Shuo Cheng,Yu Fang,Homanga Bharadhwaj,Mingyu Ding,Gedas Bertasius,Daniel Szafir*

Main category: cs.RO

TL;DR: LiLo-VLA is a modular VLA framework that decouples transport from interaction for robust long-horizon manipulation, achieving 69% success in simulation and 85% in real-world tasks.


<details>
  <summary>Details</summary>
Motivation: General-purpose robots need to handle long-horizon manipulation with multiple structure changes in unstructured environments. Current VLA models struggle with combinatorial skill sequencing and suffer from cascading failures due to environmental sensitivity.

Method: Proposes LiLo-VLA (Linked Local VLA) with modular design: Reaching Module handles global motion, and Interaction Module uses object-centric VLA to process isolated objects. This decoupling ensures robustness against irrelevant visual features and spatial variations. The modularity enables dynamic replanning and skill reuse for failure recovery.

Result: Achieves 69% average success rate on 21-task simulation benchmark (LIBERO-Long++ and Ultra-Long suites), outperforming Pi0.5 by 41% and OpenVLA-OFT by 67%. Real-world evaluations across 8 long-horizon tasks show 85% average success rate.

Conclusion: LiLo-VLA's modular approach effectively addresses cascading failures in long-horizon manipulation, enabling zero-shot generalization to novel tasks without training on them, with strong performance in both simulation and real-world settings.

Abstract: General-purpose robots must master long-horizon manipulation, defined as tasks involving multiple kinematic structure changes (e.g., attaching or detaching objects) in unstructured environments. While Vision-Language-Action (VLA) models offer the potential to master diverse atomic skills, they struggle with the combinatorial complexity of sequencing them and are prone to cascading failures due to environmental sensitivity. To address these challenges, we propose LiLo-VLA (Linked Local VLA), a modular framework capable of zero-shot generalization to novel long-horizon tasks without ever being trained on them. Our approach decouples transport from interaction: a Reaching Module handles global motion, while an Interaction Module employs an object-centric VLA to process isolated objects of interest, ensuring robustness against irrelevant visual features and invariance to spatial configurations. Crucially, this modularity facilitates robust failure recovery through dynamic replanning and skill reuse, effectively mitigating the cascading errors common in end-to-end approaches. We introduce a 21-task simulation benchmark consisting of two challenging suites: LIBERO-Long++ and Ultra-Long. In these simulations, LiLo-VLA achieves a 69% average success rate, outperforming Pi0.5 by 41% and OpenVLA-OFT by 67%. Furthermore, real-world evaluations across 8 long-horizon tasks demonstrate an average success rate of 85%. Project page: https://yy-gx.github.io/LiLo-VLA/.

</details>


### [203] [Learning Agile and Robust Omnidirectional Aerial Motion on Overactuated Tiltable-Quadrotors](https://arxiv.org/abs/2602.21583)
*Wentao Zhang,Zhaoqi Ma,Jinjie Li,Huayi Wang,Haokun Liu,Junichiro Sugihara,Chen Chen,Yicheng Chen,Moju Zhao*

Main category: cs.RO

TL;DR: RL-based control for tilt-rotor drones achieves robust omnidirectional motion with sim-to-real transfer using system ID and domain randomization.


<details>
  <summary>Details</summary>
Motivation: Tilt-rotor drones enable omnidirectional maneuvering but have complex coupled dynamics that make model-based controllers fragile to disturbances and uncertainties.

Method: Reinforcement learning framework for tiltable quadrotors that learns coordinated rotor-joint behaviors for SE(3) pose tracking, combined with system identification and physically consistent domain randomization for sim-to-real transfer.

Result: Achieves comparable 6-DOF pose tracking accuracy to state-of-the-art NMPC, with superior robustness and generalization across tasks, enabling zero-shot real hardware deployment.

Conclusion: Learning-based control provides a robust and agile solution for tilt-rotor aerial robots, overcoming limitations of model-based approaches through sim-to-real transfer with minimal domain randomization.

Abstract: Tilt-rotor aerial robots enable omnidirectional maneuvering through thrust vectoring, but introduce significant control challenges due to the strong coupling between joint and rotor dynamics. While model-based controllers can achieve high motion accuracy under nominal conditions, their robustness and responsiveness often degrade in the presence of disturbances and modeling uncertainties. This work investigates reinforcement learning for omnidirectional aerial motion control on over-actuated tiltable quadrotors that prioritizes robustness and agility. We present a learning-based control framework that enables efficient acquisition of coordinated rotor-joint behaviors for reaching target poses in the $SE(3)$ space. To achieve reliable sim-to-real transfer while preserving motion accuracy, we integrate system identification with minimal and physically consistent domain randomization. Compared with a state-of-the-art NMPC controller, the proposed method achieves comparable six-degree-of-freedom pose tracking accuracy, while demonstrating superior robustness and generalization across diverse tasks, enabling zero-shot deployment on real hardware.

</details>


### [204] [SPOC: Safety-Aware Planning Under Partial Observability And Physical Constraints](https://arxiv.org/abs/2602.21595)
*Hyungmin Kim,Hobeom Jeon,Dohyung Kim,Minsu Jang,Jeahong Kim*

Main category: cs.RO

TL;DR: SPOC is a new benchmark for safety-aware embodied task planning that addresses partial observability and physical constraints, evaluating LLMs on household hazards with state and constraint-based metrics.


<details>
  <summary>Details</summary>
Motivation: Current embodied task planning benchmarks overlook critical safety factors like partial observability and physical constraints, limiting their ability to evaluate both feasibility and safety in real-world environments.

Method: Introduces SPOC benchmark with strict partial observability, physical constraints, step-by-step planning, and goal-condition-based evaluation covering diverse household hazards (fire, fluid, injury, object damage, pollution).

Result: Experiments with state-of-the-art LLMs show current models struggle to ensure safety-aware planning, particularly under implicit constraints.

Conclusion: SPOC enables rigorous safety assessment for embodied task planning, revealing significant limitations in current LLMs' ability to handle real-world safety constraints.

Abstract: Embodied Task Planning with large language models faces safety challenges in real-world environments, where partial observability and physical constraints must be respected. Existing benchmarks often overlook these critical factors, limiting their ability to evaluate both feasibility and safety. We introduce SPOC, a benchmark for safety-aware embodied task planning, which integrates strict partial observability, physical constraints, step-by-step planning, and goal-condition-based evaluation. Covering diverse household hazards such as fire, fluid, injury, object damage, and pollution, SPOC enables rigorous assessment through both state and constraint-based online metrics. Experiments with state-of-the-art LLMs reveal that current models struggle to ensure safety-aware planning, particularly under implicit constraints. Code and dataset are available at https://github.com/khm159/SPOC

</details>


### [205] [Iterative Closed-Loop Motion Synthesis for Scaling the Capabilities of Humanoid Control](https://arxiv.org/abs/2602.21599)
*Weisheng Xu,Qiwei Wu,Jiaxi Zhang,Tan Jing,Yangfan Li,Yuetong Fang,Jiaqi Xiong,Kai Wu,Rong Ou,Renjing Xu*

Main category: cs.RO

TL;DR: Closed-loop automated motion data generation framework for physics-based humanoid control that generates high-quality motion data with rich semantics and enables iterative difficulty improvement.


<details>
  <summary>Details</summary>
Motivation: Fixed difficulty distributions in motion datasets limit policy performance ceilings, and high-quality motion capture data is expensive and difficult to scale. Need for diverse, scalable motion data generation.

Method: Proposed closed-loop automated motion data generation and iterative framework that generates high-quality motion data with rich action semantics (martial arts, dance, combat, etc.) and enables difficulty iteration through physical metrics and objective evaluations.

Result: On PHC single-primitive tracker, using only ~1/10 of AMASS dataset size, reduced average failure rate by 45% on test set (2201 clips). Framework enables trained tracker to break through original difficulty limits.

Conclusion: The proposed framework effectively addresses scalability and difficulty limitations of motion data for physics-based humanoid control, generating diverse high-quality data while improving policy performance through iterative difficulty enhancement.

Abstract: Physics-based humanoid control relies on training with motion datasets that have diverse data distributions. However, the fixed difficulty distribution of datasets limits the performance ceiling of the trained control policies. Additionally, the method of acquiring high-quality data through professional motion capture systems is constrained by costs, making it difficult to achieve large-scale scalability. To address these issues, we propose a closed-loop automated motion data generation and iterative framework. It can generate high-quality motion data with rich action semantics, including martial arts, dance, combat, sports, gymnastics, and more. Furthermore, our framework enables difficulty iteration of policies and data through physical metrics and objective evaluations, allowing the trained tracker to break through its original difficulty limits. On the PHC single-primitive tracker, using only approximately 1/10 of the AMASS dataset size, the average failure rate on the test set (2201 clips) is reduced by 45\% compared to the baseline. Finally, we conduct comprehensive ablation and comparative experiments to highlight the rationality and advantages of our framework.

</details>


### [206] [Jumping Control for a Quadrupedal Wheeled-Legged Robot via NMPC and DE Optimization](https://arxiv.org/abs/2602.21612)
*Xuanqi Zeng,Lingwei Zhang,Linzhu Yue,Zhitao Song,Hongbo Zhang,Tianlin Zhang,Yun-Hui Liu*

Main category: cs.RO

TL;DR: A novel motion control framework combining NMPC for locomotion and DE-based trajectory optimization for jumping enables quadrupedal wheeled-legged robots to perform agile maneuvers including vertical jumps, forward jumps, and backflips.


<details>
  <summary>Details</summary>
Motivation: Quadrupedal wheeled-legged robots offer superior mobility by combining legged and wheeled locomotion, but dynamic jumping remains challenging due to the additional degrees of freedom from wheeled legs.

Method: Develops a mini-sized wheeled-legged robot and proposes a control framework integrating Nonlinear Model Predictive Control (NMPC) for locomotion and Differential Evolution (DE)-based trajectory optimization for jumping, utilizing wheel motion to enhance jumping performance.

Result: The framework enables versatile maneuvers including vertical jumping, forward jumping, and backflips, with experimental validation showing a forward jump over a 0.12 m obstacle and a vertical jump reaching 0.5 m.

Conclusion: The proposed integrated control framework successfully addresses the challenge of dynamic jumping in quadrupedal wheeled-legged robots, demonstrating enhanced agility and mobility through validated simulations and real-world experiments.

Abstract: Quadrupedal wheeled-legged robots combine the advantages of legged and wheeled locomotion to achieve superior mobility, but executing dynamic jumps remains a significant challenge due to the additional degrees of freedom introduced by wheeled legs. This paper develops a mini-sized wheeled-legged robot for agile motion and presents a novel motion control framework that integrates the Nonlinear Model Predictive Control (NMPC) for locomotion and the Differential Evolution (DE) based trajectory optimization for jumping in quadrupedal wheeled-legged robots. The proposed controller utilizes wheel motion and locomotion to enhance jumping performance, achieving versatile maneuvers such as vertical jumping, forward jumping, and backflips. Extensive simulations and real-world experiments validate the effectiveness of the framework, demonstrating a forward jump over a 0.12 m obstacle and a vertical jump reaching 0.5 m.

</details>


### [207] [ADM-DP: Adaptive Dynamic Modality Diffusion Policy through Vision-Tactile-Graph Fusion for Multi-Agent Manipulation](https://arxiv.org/abs/2602.21622)
*Enyi Wang,Wen Fan,Dandan Zhang*

Main category: cs.RO

TL;DR: ADM-DP is a multi-agent robotic manipulation framework that integrates vision, tactile, and graph-based modalities with adaptive fusion for improved coordination, grasp stability, and collision avoidance.


<details>
  <summary>Details</summary>
Motivation: Multi-agent robotic manipulation faces challenges in coordination, grasp stability, and collision avoidance in shared workspaces, requiring better integration of multiple sensory modalities and adaptive control strategies.

Method: Four key innovations: 1) Enhanced visual encoder with FiLM modulation for RGB and point-cloud fusion, 2) Tactile-guided grasping using FSR feedback for grasp refinement, 3) Graph-based collision encoder using TCP positions for spatial awareness, 4) Adaptive Modality Attention Mechanism (AMAM) for dynamic modality weighting. Uses decoupled training paradigm for scalability.

Result: Achieves 12-25% performance gains over state-of-the-art baselines across seven multi-agent tasks. Ablation studies show greatest improvements in tasks requiring multiple sensory modalities, validating the adaptive fusion strategy.

Conclusion: ADM-DP demonstrates robust performance for diverse manipulation scenarios through its adaptive multi-modal fusion approach, effectively addressing coordination, grasp stability, and collision avoidance challenges in multi-agent robotic manipulation.

Abstract: Multi-agent robotic manipulation remains challenging due to the combined demands of coordination, grasp stability, and collision avoidance in shared workspaces. To address these challenges, we propose the Adaptive Dynamic Modality Diffusion Policy (ADM-DP), a framework that integrates vision, tactile, and graph-based (multi-agent pose) modalities for coordinated control. ADM-DP introduces four key innovations. First, an enhanced visual encoder merges RGB and point-cloud features via Feature-wise Linear Modulation (FiLM) modulation to enrich perception. Second, a tactile-guided grasping strategy uses Force-Sensitive Resistor (FSR) feedback to detect insufficient contact and trigger corrective grasp refinement, improving grasp stability. Third, a graph-based collision encoder leverages shared tool center point (TCP) positions of multiple agents as structured kinematic context to maintain spatial awareness and reduce inter-agent interference. Fourth, an Adaptive Modality Attention Mechanism (AMAM) dynamically re-weights modalities according to task context, enabling flexible fusion. For scalability and modularity, a decoupled training paradigm is employed in which agents learn independent policies while sharing spatial information. This maintains low interdependence between agents while retaining collective awareness. Across seven multi-agent tasks, ADM-DP achieves 12-25% performance gains over state-of-the-art baselines. Ablation studies show the greatest improvements in tasks requiring multiple sensory modalities, validating our adaptive fusion strategy and demonstrating its robustness for diverse manipulation scenarios.

</details>


### [208] [Tacmap: Bridging the Tactile Sim-to-Real Gap via Geometry-Consistent Penetration Depth Map](https://arxiv.org/abs/2602.21625)
*Lei Su,Zhijie Peng,Renyuan Ren,Shengping Mao,Juan Du,Kaifeng Zhang,Xuezhou Zhu*

Main category: cs.RO

TL;DR: Tacmap is a high-fidelity, computationally efficient tactile simulation framework that bridges the sim-to-real gap using volumetric penetration depth and unified deform map representations.


<details>
  <summary>Details</summary>
Motivation: Vision-Based Tactile Sensors are crucial for dexterous robotic manipulation, but current tactile simulations face a dilemma: simplified geometric projections lack physical authenticity while high-fidelity FEM methods are too computationally expensive for large-scale reinforcement learning.

Method: Tacmap uses volumetric penetration depth as its foundation. In simulation, it computes 3D intersection volumes as depth maps. In the real world, it employs an automated data-collection rig to learn a robust mapping from raw tactile images to ground-truth depth maps, creating a unified deform map representation.

Result: Quantitative evaluations across diverse contact scenarios show that Tacmap's deform maps closely mirror real-world measurements. The framework enables zero-shot transfer: a policy trained exclusively in simulation successfully performs an in-hand rotation task on a physical robot.

Conclusion: Tacmap successfully bridges the tactile sim-to-real gap by unifying simulation and real-world domains through shared deform map representations, enabling efficient, high-fidelity tactile simulation for robotic manipulation tasks.

Abstract: Vision-Based Tactile Sensors (VBTS) are essential for achieving dexterous robotic manipulation, yet the tactile sim-to-real gap remains a fundamental bottleneck. Current tactile simulations suffer from a persistent dilemma: simplified geometric projections lack physical authenticity, while high-fidelity Finite Element Methods (FEM) are too computationally prohibitive for large-scale reinforcement learning. In this work, we present Tacmap, a high-fidelity, computationally efficient tactile simulation framework anchored in volumetric penetration depth. Our key insight is to bridge the tactile sim-to-real gap by unifying both domains through a shared deform map representation. Specifically, we compute 3D intersection volumes as depth maps in simulation, while in the real world, we employ an automated data-collection rig to learn a robust mapping from raw tactile images to ground-truth depth maps. By aligning simulation and real-world in this unified geometric space, Tacmap minimizes domain shift while maintaining physical consistency. Quantitative evaluations across diverse contact scenarios demonstrate that Tacmap's deform maps closely mirror real-world measurements. Moreover, we validate the utility of Tacmap through an in-hand rotation task, where a policy trained exclusively in simulation achieves zero-shot transfer to a physical robot.

</details>


### [209] [Self-Correcting VLA: Online Action Refinement via Sparse World Imagination](https://arxiv.org/abs/2602.21633)
*Chenyv Liu,Wentao Tan,Lei Zhu,Fengling Li,Jingjing Li,Guoli Yang,Heng Tao Shen*

Main category: cs.RO

TL;DR: SC-VLA is a self-correcting vision-language-action model that achieves self-improvement through sparse world imagination and online action refinement, outperforming baselines in robot manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Current VLA models lack robust physical understanding, RL relies on external rewards isolated from internal states, and world action models lack explicit self-improvement mechanisms.

Method: Two key components: 1) Sparse world imagination with auxiliary predictive heads to forecast task progress and future trajectory trends, 2) Online action refinement module that reshapes progress-dependent dense rewards based on predicted sparse future states.

Result: Achieves state-of-the-art performance on robot manipulation tasks: 16% fewer steps, 9% higher success rate than best baselines, and 14% gain in real-world experiments.

Conclusion: SC-VLA successfully integrates self-improvement through sparse imagination and action refinement, demonstrating superior performance in both simulation and real-world robot manipulation tasks.

Abstract: Standard vision-language-action (VLA) models rely on fitting statistical data priors, limiting their robust understanding of underlying physical dynamics. Reinforcement learning enhances physical grounding through exploration yet typically relies on external reward signals that remain isolated from the agent's internal states. World action models have emerged as a promising paradigm that integrates imagination and control to enable predictive planning. However, they rely on implicit context modeling, lacking explicit mechanisms for self-improvement. To solve these problems, we propose Self-Correcting VLA (SC-VLA), which achieve self-improvement by intrinsically guiding action refinement through sparse imagination. We first design sparse world imagination by integrating auxiliary predictive heads to forecast current task progress and future trajectory trends, thereby constraining the policy to encode short-term physical evolution. Then we introduce the online action refinement module to reshape progress-dependent dense rewards, adjusting trajectory orientation based on the predicted sparse future states. Evaluations on challenging robot manipulation tasks from simulation benchmarks and real-world settings demonstrate that SC-VLA achieve state-of-the-art performance, yielding the highest task throughput with 16% fewer steps and a 9% higher success rate than the best-performing baselines, alongside a 14% gain in real-world experiments. Code is available at https://github.com/Kisaragi0/SC-VLA.

</details>


### [210] [DAGS-SLAM: Dynamic-Aware 3DGS SLAM via Spatiotemporal Motion Probability and Uncertainty-Aware Scheduling](https://arxiv.org/abs/2602.21644)
*Li Zhang,Yu-An Liu,Xijia Jiang,Conghao Huang,Danyang Li,Yanyong Zhang*

Main category: cs.RO

TL;DR: DAGS-SLAM is a dynamic-aware 3D Gaussian Splatting SLAM system that uses spatiotemporal motion probability states and on-demand semantics to handle dynamic objects efficiently for mobile robots and IoT devices.


<details>
  <summary>Details</summary>
Motivation: Mobile robots and IoT devices need real-time localization and dense reconstruction under tight compute/energy budgets. While 3D Gaussian Splatting enables efficient SLAM, dynamic objects and occlusions degrade performance. Existing dynamic 3DGS-SLAM methods use heavy optical flow and per-frame segmentation, which are too costly for mobile deployment and brittle under challenging illumination.

Method: DAGS-SLAM maintains a spatiotemporal motion probability (MP) state per Gaussian and triggers semantics on demand via an uncertainty-aware scheduler. It fuses lightweight YOLO instance priors with geometric cues to estimate and update MP temporally, propagates MP to the front-end for dynamic-aware correspondence selection, and suppresses dynamic artifacts in the back-end via MP-guided optimization.

Result: Experiments on public dynamic RGB-D benchmarks show improved reconstruction and robust tracking while sustaining real-time throughput on a commodity GPU. The system demonstrates a practical speed-accuracy tradeoff with reduced semantic invocations, making it suitable for mobile deployment.

Conclusion: DAGS-SLAM provides an efficient dynamic-aware 3DGS-SLAM solution that addresses the limitations of existing methods by using motion probability states and on-demand semantics, achieving better performance with lower computational cost for mobile applications.

Abstract: Mobile robots and IoT devices demand real-time localization and dense reconstruction under tight compute and energy budgets. While 3D Gaussian Splatting (3DGS) enables efficient dense SLAM, dynamic objects and occlusions still degrade tracking and mapping. Existing dynamic 3DGS-SLAM often relies on heavy optical flow and per-frame segmentation, which is costly for mobile deployment and brittle under challenging illumination. We present DAGS-SLAM, a dynamic-aware 3DGS-SLAM system that maintains a spatiotemporal motion probability (MP) state per Gaussian and triggers semantics on demand via an uncertainty-aware scheduler. DAGS-SLAM fuses lightweight YOLO instance priors with geometric cues to estimate and temporally update MP, propagates MP to the front-end for dynamic-aware correspondence selection, and suppresses dynamic artifacts in the back-end via MP-guided optimization. Experiments on public dynamic RGB-D benchmarks show improved reconstruction and robust tracking while sustaining real-time throughput on a commodity GPU, demonstrating a practical speed-accuracy tradeoff with reduced semantic invocations toward mobile deployment.

</details>


### [211] [Biomechanical Comparisons Reveal Divergence of Human and Humanoid Gaits](https://arxiv.org/abs/2602.21666)
*Luying Feng,Yaochu Jin,Hanze Hu,Wei Chen*

Main category: cs.RO

TL;DR: GDAF is a biomechanical framework that quantifies differences between human and humanoid locomotion, revealing significant biomechanical divergence despite visually similar motions.


<details>
  <summary>Details</summary>
Motivation: Current imitation learning approaches for legged robots fail to capture fundamental principles of human motion, simply replicating joint angles without understanding underlying biomechanics. There's a need for systematic evaluation of kinematic and kinetic discrepancies between biological and mechanical locomotion.

Method: Proposes Gait Divergence Analysis Framework (GDAF) - a unified biomechanical evaluation framework that systematically quantifies kinematic and kinetic discrepancies. Applied to compare human and humanoid locomotion across 28 walking speeds. Includes collection of speed-continuous humanoid locomotion dataset and open-source implementation with analysis, visualization, and MuJoCo-based tools.

Result: Despite visually human-like motion from modern controllers, significant biomechanical divergence persists across speeds. Robots show systematic deviations in gait symmetry, energy distribution, and joint coordination, indicating substantial room for improving biomechanical fidelity and energetic efficiency.

Conclusion: GDAF provides a quantitative benchmark for evaluating humanoid locomotion and offers data/tools to support development of more human-like and energetically efficient controllers. The framework enables reproducible, interpretable biomechanical analysis to bridge the gap between biological and robotic locomotion.

Abstract: It remains challenging to achieve human-like locomotion in legged robots due to fundamental discrepancies between biological and mechanical structures. Although imitation learning has emerged as a promising approach for generating natural robotic movements, simply replicating joint angle trajectories fails to capture the underlying principles of human motion. This study proposes a Gait Divergence Analysis Framework (GDAF), a unified biomechanical evaluation framework that systematically quantifies kinematic and kinetic discrepancies between humans and bipedal robots. We apply GDAF to systematically compare human and humanoid locomotion across 28 walking speeds. To enable reproducible analysis, we collect and release a speed-continuous humanoid locomotion dataset from a state-of-the-art humanoid controller. We further provide an open-source implementation of GDAF, including analysis, visualization, and MuJoCo-based tools, enabling quantitative, interpretable, and reproducible biomechanical analysis of humanoid locomotion. Results demonstrate that despite visually human-like motion generated by modern humanoid controllers, significant biomechanical divergence persists across speeds. Robots exhibit systematic deviations in gait symmetry, energy distribution, and joint coordination, indicating that substantial room remains for improving the biomechanical fidelity and energetic efficiency of humanoid locomotion. This work provides a quantitative benchmark for evaluating humanoid locomotion and offers data and versatile tools to support the development of more human-like and energetically efficient locomotion controllers. The data and code will be made publicly available upon acceptance of the paper.

</details>


### [212] [Hierarchical LLM-Based Multi-Agent Framework with Prompt Optimization for Multi-Robot Task Planning](https://arxiv.org/abs/2602.21670)
*Tomoya Kawabe,Rin Takano*

Main category: cs.RO

TL;DR: Hierarchical multi-agent LLM planner with prompt optimization improves multi-robot task planning by combining LLM interpretation with PDDL planning, achieving state-of-the-art results on MAT-THOR benchmark.


<details>
  <summary>Details</summary>
Motivation: Multi-robot task planning faces challenges: conventional PDDL planners struggle with ambiguous/long-horizon missions, while LLMs can interpret instructions but may hallucinate or produce infeasible actions. Need to combine strengths of both approaches.

Method: Hierarchical multi-agent LLM-based planner with prompt optimization: upper layer decomposes tasks and assigns to lower-layer agents, which generate PDDL problems for classical planner. When plans fail, TextGrad-inspired textual-gradient updates optimize each agent's prompt. Meta-prompts are learned and shared across agents within same layer.

Result: On MAT-THOR benchmark: 0.95 success on compound tasks, 0.84 on complex tasks, 0.60 on vague tasks, improving over previous SOTA LaMMA-P by 2, 7, and 15 percentage points respectively. Ablation shows hierarchical structure (+59pp), prompt optimization (+37pp), and meta-prompt sharing (+4pp) contributions.

Conclusion: The hierarchical LLM-based planner with prompt optimization effectively combines LLM interpretation with PDDL planning rigor, achieving significant improvements in multi-robot task planning across different task complexities.

Abstract: Multi-robot task planning requires decomposing natural-language instructions into executable actions for heterogeneous robot teams. Conventional Planning Domain Definition Language (PDDL) planners provide rigorous guarantees but struggle to handle ambiguous or long-horizon missions, while large language models (LLMs) can interpret instructions and propose plans but may hallucinate or produce infeasible actions. We present a hierarchical multi-agent LLM-based planner with prompt optimization: an upper layer decomposes tasks and assigns them to lower-layer agents, which generate PDDL problems solved by a classical planner. When plans fail, the system applies TextGrad-inspired textual-gradient updates to optimize each agent's prompt and thereby improve planning accuracy. In addition, meta-prompts are learned and shared across agents within the same layer, enabling efficient prompt optimization in multi-agent settings. On the MAT-THOR benchmark, our planner achieves success rates of 0.95 on compound tasks, 0.84 on complex tasks, and 0.60 on vague tasks, improving over the previous state-of-the-art LaMMA-P by 2, 7, and 15 percentage points respectively. An ablation study shows that the hierarchical structure, prompt optimization, and meta-prompt sharing contribute roughly +59, +37, and +4 percentage points to the overall success rate.

</details>


### [213] [SunnyParking: Multi-Shot Trajectory Generation and Motion State Awareness for Human-like Parking](https://arxiv.org/abs/2602.21682)
*Jishu Miao,Han Chen,Jiankun Zhai,Qi Liu,Tsubasa Hirakawa,Takayoshi Yamashita,Hironobu Fujiyoshi*

Main category: cs.RO

TL;DR: SunnyParking is a dual-branch E2E architecture for autonomous parking that jointly predicts spatial trajectories and discrete motion states, addressing the "dimensionality deficiency" in existing methods by incorporating motion state awareness and using Fourier features for high-precision target slot representation.


<details>
  <summary>Details</summary>
Motivation: Existing E2E planning methods for autonomous parking oversimplify the task as geometric path regression, neglecting explicit modeling of vehicle kinematic states. This "dimensionality deficiency" leads to physically infeasible trajectories and deviations from human driving behavior, especially at critical gear-shift points in multi-shot parking scenarios.

Method: Proposes SunnyParking, a novel dual-branch E2E architecture that achieves motion state awareness by jointly predicting spatial trajectories and discrete motion state sequences (forward/reverse). Introduces Fourier feature-based representation of target parking slots to overcome resolution limitations of traditional BEV approaches, enabling high-precision target interactions.

Result: The framework generates more robust and human-like trajectories in complex multi-shot parking scenarios and significantly improves gear-shift point localization accuracy compared to state-of-the-art methods. Authors also open-source a new parking dataset from CARLA simulator specifically designed to evaluate full prediction capabilities under complex maneuvers.

Conclusion: SunnyParking addresses critical limitations in existing autonomous parking methods by incorporating motion state awareness and high-precision target representation, resulting in more physically feasible and human-like parking trajectories, particularly for complex multi-shot scenarios requiring gear shifts.

Abstract: Autonomous parking fundamentally differs from on-road driving due to its frequent direction changes and complex maneuvering requirements. However, existing End-to-End (E2E) planning methods often simplify the parking task into a geometric path regression problem, neglecting explicit modeling of the vehicle's kinematic state. This "dimensionality deficiency" easily leads to physically infeasible trajectories and deviates from real human driving behavior, particularly at critical gear-shift points in multi-shot parking scenarios. In this paper, we propose SunnyParking, a novel dual-branch E2E architecture that achieves motion state awareness by jointly predicting spatial trajectories and discrete motion state sequences (e.g., forward/reverse). Additionally, we introduce a Fourier feature-based representation of target parking slots to overcome the resolution limitations of traditional bird's-eye view (BEV) approaches, enabling high-precision target interactions. Experimental results demonstrate that our framework generates more robust and human-like trajectories in complex multi-shot parking scenarios, while significantly improving gear-shift point localization accuracy compared to state-of-the-art methods. We open-source a new parking dataset of the CARLA simulator, specifically designed to evaluate full prediction capabilities under complex maneuvers.

</details>


### [214] [Primary-Fine Decoupling for Action Generation in Robotic Imitation](https://arxiv.org/abs/2602.21684)
*Xiaohan Lei,Min Wang,Wengang Zhou,Xingyu Lu,Houqiang Li*

Main category: cs.RO

TL;DR: PF-DAG is a two-stage imitation learning framework that decouples coarse action modes from fine-grained variations to address multi-modal distribution challenges in robotic manipulation, achieving better performance than single-stage approaches.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for multi-modal robotic manipulation imitation learning have trade-offs: discrete tokenization loses fine-grained variations, while continuous single-stage methods produce unstable mode transitions. There's a need for a method that maintains both coarse consistency and fine-grained action fidelity.

Method: Two-stage framework: (1) Compress action chunks into discrete modes for a lightweight policy to select consistent coarse modes and avoid mode bouncing; (2) Use mode-conditioned MeanFlow policy to generate high-fidelity continuous actions based on selected modes.

Result: Theoretically proven lower MSE bound than single-stage generative policies. Empirically outperforms state-of-the-art baselines across 56 tasks from Adroit, DexArt, and MetaWorld benchmarks. Successfully generalizes to real-world tactile dexterous manipulation tasks.

Conclusion: Explicit mode-level decoupling enables both robust multi-modal modeling and reactive closed-loop control for robotic manipulation, demonstrating the effectiveness of the two-stage approach in addressing the limitations of existing methods.

Abstract: Multi-modal distribution in robotic manipulation action sequences poses critical challenges for imitation learning. To this end, existing approaches often model the action space as either a discrete set of tokens or a continuous, latent-variable distribution. However, both approaches present trade-offs: some methods discretize actions into tokens and therefore lose fine-grained action variations, while others generate continuous actions in a single stage tend to produce unstable mode transitions. To address these limitations, we propose Primary-Fine Decoupling for Action Generation (PF-DAG), a two-stage framework that decouples coarse action consistency from fine-grained variations. First, we compress action chunks into a small set of discrete modes, enabling a lightweight policy to select consistent coarse modes and avoid mode bouncing. Second, a mode conditioned MeanFlow policy is learned to generate high-fidelity continuous actions. Theoretically, we prove PF-DAG's two-stage design achieves a strictly lower MSE bound than single-stage generative policies. Empirically, PF-DAG outperforms state-of-the-art baselines across 56 tasks from Adroit, DexArt, and MetaWorld benchmarks. It further generalizes to real-world tactile dexterous manipulation tasks. Our work demonstrates that explicit mode-level decoupling enables both robust multi-modal modeling and reactive closed-loop control for robotic manipulation.

</details>


### [215] [Trajectory Generation with Endpoint Regulation and Momentum-Aware Dynamics for Visually Impaired Scenarios](https://arxiv.org/abs/2602.21691)
*Yuting Zeng,Manping Fan,You Zhou,Yongbin Yu,Zhiwen Zheng,Jingtao Zhang,Liyong Ren,Zhenglin Yang*

Main category: cs.RO

TL;DR: Proposes a trajectory generation method with endpoint regulation and momentum-aware dynamics for smoother, more stable motion in visually impaired scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional jerk-based heuristic trajectory sampling with independent segment generation leads to unstable terminal behavior and state discontinuities under frequent regeneration in structured, low-speed dynamic environments for visually impaired scenarios.

Method: Integrates endpoint regulation to stabilize terminal states within each segment and momentum-aware dynamics to regularize velocity and acceleration evolution for segment consistency. Endpoint regulation stabilizes terminal behavior in trajectory sampling, while momentum-aware dynamics enforces consistent velocity and acceleration across consecutive segments.

Result: Experimental results show reduced acceleration peaks, lower jerk levels with decreased dispersion, smoother velocity and acceleration profiles, more stable endpoint distributions, and fewer infeasible trajectory candidates compared to baseline planner.

Conclusion: The proposed approach effectively addresses instability and discontinuity issues in trajectory generation for visually impaired scenarios by integrating endpoint regulation and momentum-aware dynamics, resulting in improved motion smoothness and consistency.

Abstract: Trajectory generation for visually impaired scenarios requires smooth and temporally consistent state in structured, low-speed dynamic environments. However, traditional jerk-based heuristic trajectory sampling with independent segment generation and conventional smoothness penalties often lead to unstable terminal behavior and state discontinuities under frequent regenerating. This paper proposes a trajectory generation approach that integrates endpoint regulation to stabilize terminal states within each segment and momentum-aware dynamics to regularize the evolution of velocity and acceleration for segment consistency. Endpoint regulation is incorporated into trajectory sampling to stabilize terminal behavior, while a momentum-aware dynamics enforces consistent velocity and acceleration evolution across consecutive trajectory segments. Experimental results demonstrate reduced acceleration peaks and lower jerk levels with decreased dispersion, smoother velocity and acceleration profiles, more stable endpoint distributions, and fewer infeasible trajectory candidates compared with a baseline planner.

</details>


### [216] [Dual-Regime Hybrid Aerodynamic Modeling of Winged Blimps With Neural Mixing](https://arxiv.org/abs/2602.21696)
*Xiaorui Wang,Hongwu Wang,Yue Fan,Hao Cheng,Feitian Zhang*

Main category: cs.RO

TL;DR: Hybrid aerodynamic modeling framework for winged blimps that combines fixed-wing and drag-dominated models using a learned neural network mixer with physics-based regularization, validated through extensive real-world flight experiments.


<details>
  <summary>Details</summary>
Motivation: Winged blimps operate across distinct aerodynamic regimes that cannot be captured by a single model - fixed-wing behavior at high speeds/small angles vs. drag-dominated dynamics at low speeds/large angles. Accurately representing transitions between these regimes remains a fundamental challenge.

Method: Hybrid framework integrating a fixed-wing Aerodynamic Coupling Model (ACM) and a Generalized Drag Model (GDM) using a learned neural network mixer with explicit physics-based regularization. Model parameters identified through a structured three-phase pipeline tailored for hybrid aerodynamic modeling.

Result: Validated on RGBlimp platform through large-scale experimental campaign (1,320 real-world flight trajectories across 330 thruster/moving mass configurations). The hybrid model consistently outperforms single-model and predefined-mixer baselines across wide range of speeds and angles of attack.

Conclusion: Establishes a practical and robust aerodynamic modeling solution for winged blimps that enables smooth transitions between aerodynamic regimes while retaining explicit, physics-based aerodynamic representation.

Abstract: Winged blimps operate across distinct aerodynamic regimes that cannot be adequately captured by a single model. At high speeds and small angles of attack, their dynamics exhibit strong coupling between lift and attitude, resembling fixed-wing aircraft behavior. At low speeds or large angles of attack, viscous effects and flow separation dominate, leading to drag-driven and damping-dominated dynamics. Accurately representing transitions between these regimes remains a fundamental challenge. This paper presents a hybrid aerodynamic modeling framework that integrates a fixed-wing Aerodynamic Coupling Model (ACM) and a Generalized Drag Model (GDM) using a learned neural network mixer with explicit physics-based regularization. The mixer enables smooth transitions between regimes while retaining explicit, physics-based aerodynamic representation. Model parameters are identified through a structured three-phase pipeline tailored for hybrid aerodynamic modeling. The proposed approach is validated on the RGBlimp platform through a large-scale experimental campaign comprising 1,320 real-world flight trajectories across 330 thruster and moving mass configurations, spanning a wide range of speeds and angles of attack. Experimental results demonstrate that the proposed hybrid model consistently outperforms single-model and predefined-mixer baselines, establishing a practical and robust aerodynamic modeling solution for winged blimps.

</details>


### [217] [LessMimic: Long-Horizon Humanoid Interaction with Unified Distance Field Representations](https://arxiv.org/abs/2602.21723)
*Yutang Lin,Jieming Cui,Yixuan Li,Baoxiong Jia,Yixin Zhu,Siyuan Huang*

Main category: cs.RO

TL;DR: LessMimic: A humanoid robot control framework using Distance Fields as geometric representation for reference-free, generalizable whole-body policies that can compose multiple skills and transfer to vision-only deployment.


<details>
  <summary>Details</summary>
Motivation: Existing humanoid robot approaches rely on reference motions or task-specific rewards, which tightly couple policies to specific object geometries and prevent multi-skill generalization within a single framework. There's a need for a unified interaction representation that enables reference-free inference, geometric generalization, and long-horizon skill composition.

Method: Uses Distance Fields (DF) as geometric representation, conditioning a single whole-body policy on DF-derived cues (surface distances, gradients, velocity decompositions). Interaction latents are encoded via Variational Auto-Encoder (VAE) and post-trained using Adversarial Interaction Priors (AIP) under Reinforcement Learning (RL). DAgger-style distillation aligns DF latents with egocentric depth features for vision-only deployment.

Result: Achieves 80-100% success across object scales (0.4x to 1.6x) on PickUp and SitStand tasks where baselines degrade sharply. Attains 62.1% success on 5 task instances trajectories, and remains viable up to 40 sequentially composed tasks. Enables seamless transfer to vision-only deployment without MoCap infrastructure.

Conclusion: Distance Fields provide a scalable representation for humanoid robot control that grounds interaction in local geometry rather than demonstrations, enabling generalization, skill composition, and recovery in unstructured environments without motion references.

Abstract: Humanoid robots that autonomously interact with physical environments over extended horizons represent a central goal of embodied intelligence. Existing approaches rely on reference motions or task-specific rewards, tightly coupling policies to particular object geometries and precluding multi-skill generalization within a single framework. A unified interaction representation enabling reference-free inference, geometric generalization, and long-horizon skill composition within one policy remains an open challenge. Here we show that Distance Field (DF) provides such a representation: LessMimic conditions a single whole-body policy on DF-derived geometric cues--surface distances, gradients, and velocity decompositions--removing the need for motion references, with interaction latents encoded via a Variational Auto-Encoder (VAE) and post-trained using Adversarial Interaction Priors (AIP) under Reinforcement Learning (RL). Through DAgger-style distillation that aligns DF latents with egocentric depth features, LessMimic further transfers seamlessly to vision-only deployment without motion capture (MoCap) infrastructure. A single LessMimic policy achieves 80--100% success across object scales from 0.4x to 1.6x on PickUp and SitStand where baselines degrade sharply, attains 62.1% success on 5 task instances trajectories, and remains viable up to 40 sequentially composed tasks. By grounding interaction in local geometry rather than demonstrations, LessMimic offers a scalable path toward humanoid robots that generalize, compose skills, and recover from failures in unstructured environments.

</details>


### [218] [Joint-Aligned Latent Action: Towards Scalable VLA Pretraining in the Wild](https://arxiv.org/abs/2602.21736)
*Hao Luo,Ye Wang,Wanpeng Zhang,Haoqi Yuan,Yicheng Feng,Haiweng Xu,Sipeng Zheng,Zongqing Lu*

Main category: cs.RO

TL;DR: JALA learns joint-aligned latent actions from human videos for VLA pretraining, using UniHand-Mix dataset (7.5M videos) to improve robot manipulation performance.


<details>
  <summary>Details</summary>
Motivation: Vision-Language-Action models lack large-scale diverse robot data. Human manipulation videos offer rich alternatives but existing methods face trade-offs between small labeled datasets and large unlabeled footage with unreliable tracking.

Method: JALA learns predictive action embeddings aligned with both inverse dynamics and real actions, bypassing full visual reconstruction. Uses UniHand-Mix dataset blending lab and in-the-wild footage.

Result: Generates more realistic hand motions in controlled/unconstrained scenarios, significantly improves downstream robot manipulation performance in simulation and real-world tasks.

Conclusion: Jointly-aligned latent actions provide scalable pathway for VLA pretraining from human data.

Abstract: Despite progress, Vision-Language-Action models (VLAs) are limited by a scarcity of large-scale, diverse robot data. While human manipulation videos offer a rich alternative, existing methods are forced to choose between small, precisely-labeled datasets and vast in-the-wild footage with unreliable hand tracking labels. We present JALA, a pretraining framework that learns Jointly-Aligned Latent Actions. JALA bypasses full visual dynamic reconstruction, instead learns a predictive action embedding aligned with both inverse dynamics and real actions. This yields a transition-aware, behavior-centric latent space for learning from heterogeneous human data. We scale this approach with UniHand-Mix, a 7.5M video corpus (>2,000 hours) blending laboratory and in-the-wild footage. Experiments demonstrate that JALA generates more realistic hand motions in both controlled and unconstrained scenarios, significantly improving downstream robot manipulation performance in both simulation and real-world tasks. These results indicate that jointly-aligned latent actions offer a scalable pathway for VLA pretraining from human data.

</details>


### [219] [Therapist-Robot-Patient Physical Interaction is Worth a Thousand Words: Enabling Intuitive Therapist Guidance via Remote Haptic Control](https://arxiv.org/abs/2602.21783)
*Beatrice Luciani,Alex van den Berg,Matti Lang,Alexandre L. Ratschat,Laura Marchal-Crespo*

Main category: cs.RO

TL;DR: Haptic teleoperation system allows trainers to remotely guide trainees wearing arm exoskeletons via handheld haptic device, improving movement efficiency and reducing verbal instructions compared to visual demonstration.


<details>
  <summary>Details</summary>
Motivation: Robotic systems for motor training have limited real-world adoption due to non-intuitive trainer-trainee interactions. Current systems lack intuitive physical guidance capabilities that would make remote training more effective and natural.

Method: Developed a haptic teleoperation system where trainers use a commercial handheld haptic device to interact with trainees wearing arm exoskeletons through virtual contact points at elbow and wrist. Conducted user study with 32 participants in trainer-trainee paradigm comparing haptic demonstration vs. conventional visual demonstration for guiding arm poses.

Result: Haptic demonstration significantly reduced movement completion time, improved movement smoothness, and required fewer verbal instructions (analyzed via LLM-based speech analysis). Trainers reported similar mental/physical effort but greater competence, while trainees reported lower physical demand compared to visual demonstration.

Conclusion: The haptic teleoperation interface is feasible for effective remote human-robot physical interaction. Future work should focus on clinical usability assessment and restoring clinicians' sense of agency in robot-assisted therapy.

Abstract: Robotic systems can enhance the amount and repeatability of physically guided motor training. Yet their real-world adoption is limited, partly due to non-intuitive trainer/therapist-trainee/patient interactions. To address this gap, we present a haptic teleoperation system for trainers to remotely guide and monitor the movements of a trainee wearing an arm exoskeleton. The trainer can physically interact with the exoskeleton through a commercial handheld haptic device via virtual contact points at the exoskeleton's elbow and wrist, allowing intuitive guidance. Thirty-two participants tested the system in a trainer-trainee paradigm, comparing our haptic demonstration system with conventional visual demonstration in guiding trainees in executing arm poses. Quantitative analyses showed that haptic demonstration significantly reduced movement completion time and improved smoothness, while speech analysis using large language models for automated transcription and categorization of verbal commands revealed fewer verbal instructions. The haptic demonstration did not result in higher reported mental and physical effort by trainers compared to the visual demonstration, while trainers reported greater competence and trainees lower physical demand. These findings support the feasibility of our proposed interface for effective remote human-robot physical interaction. Future work should assess its usability and efficacy for clinical populations in restoring clinicians' sense of agency during robot-assisted therapy.

</details>


### [220] [DexRepNet++: Learning Dexterous Robotic Manipulation with Geometric and Spatial Hand-Object Representations](https://arxiv.org/abs/2602.21811)
*Qingtao Liu,Zhengnan Sun,Yu Cui,Haoming Li,Gaofeng Li,Lin Shao,Jiming Chen,Qi Ye*

Main category: cs.RO

TL;DR: DexRep is a novel hand-object interaction representation for dexterous manipulation that captures object surface features and spatial relations, enabling better generalization across diverse objects with improved success rates.


<details>
  <summary>Details</summary>
Motivation: Existing DRL methods for dexterous manipulation focus on sample efficiency but overlook the importance of representations for generalization in complex hand-object interaction spaces. Current approaches lack effective representations that can capture object surface features and spatial relationships needed for robust manipulation across diverse objects.

Method: Proposes DexRep, a novel hand-object interaction representation that captures object surface features and spatial relations between hands and objects. Uses this representation to learn policies for three dexterous manipulation tasks: grasping, in-hand reorientation, and bimanual handover.

Result: For grasping: Achieved 87.9% success rate on 5000+ unseen diverse objects when trained with only 40 objects, significantly surpassing existing work trained with thousands of objects. For in-hand reorientation and handover: Boosted success rates and other metrics by 20-40% compared to existing representations. Real-world deployment showed small sim-to-real gap under both multi-camera and single-camera setups.

Conclusion: DexRep provides an effective representation for dexterous manipulation that enables strong generalization across diverse objects with minimal training data, demonstrating superior performance over existing methods and good real-world transferability.

Abstract: Robotic dexterous manipulation is a challenging problem due to high degrees of freedom (DoFs) and complex contacts of multi-fingered robotic hands. Many existing deep reinforcement learning (DRL) based methods aim at improving sample efficiency in high-dimensional output action spaces. However, existing works often overlook the role of representations in achieving generalization of a manipulation policy in the complex input space during the hand-object interaction. In this paper, we propose DexRep, a novel hand-object interaction representation to capture object surface features and spatial relations between hands and objects for dexterous manipulation skill learning. Based on DexRep, policies are learned for three dexterous manipulation tasks, i.e. grasping, in-hand reorientation, bimanual handover, and extensive experiments are conducted to verify the effectiveness. In simulation, for grasping, the policy learned with 40 objects achieves a success rate of 87.9% on more than 5000 unseen objects of diverse categories, significantly surpassing existing work trained with thousands of objects; for the in-hand reorientation and handover tasks, the policies also boost the success rates and other metrics of existing hand-object representations by 20% to 40%. The grasp policies with DexRep are deployed to the real world under multi-camera and single-camera setups and demonstrate a small sim-to-real gap.

</details>


### [221] [Self-Curriculum Model-based Reinforcement Learning for Shape Control of Deformable Linear Objects](https://arxiv.org/abs/2602.21816)
*Zhaowei Liang,Song Wang,Zhao Jin,Shirui Wu,Dan Wu*

Main category: cs.RO

TL;DR: Two-stage RL + visual servoing framework for precise DLO shape control, combining model-based RL for large deformations and Jacobian-based visual servoing for fine adjustments, with successful sim-to-real transfer.


<details>
  <summary>Details</summary>
Motivation: Existing methods for Deformable Linear Object (DLO) shape control struggle with complex large deformations (especially opposite curvatures) and lack efficiency and precision needed for industrial/medical applications.

Method: Two-stage framework: 1) Large-deformation stage uses model-based RL with ensemble dynamics models and self-curriculum goal generation for efficient learning; 2) Small-deformation stage uses Jacobian-based visual servoing for high-precision convergence.

Result: Method outperforms baselines in success rate and precision, enables efficient policy learning, and achieves zero-shot sim-to-real transfer, completing all 30 diverse test cases with different DLO sizes/materials.

Conclusion: The proposed RL + visual servoing framework effectively addresses DLO shape control challenges, handling complex deformations with high efficiency and precision while demonstrating robust sim-to-real transfer capability.

Abstract: Precise shape control of Deformable Linear Objects (DLOs) is crucial in robotic applications such as industrial and medical fields. However, existing methods face challenges in handling complex large deformation tasks, especially those involving opposite curvatures, and lack efficiency and precision. To address this, we propose a two-stage framework combining Reinforcement Learning (RL) and online visual servoing. In the large-deformation stage, a model-based reinforcement learning approach using an ensemble of dynamics models is introduced to significantly improve sample efficiency. Additionally, we design a self-curriculum goal generation mechanism that dynamically selects intermediate-difficulty goals with high diversity through imagined evaluations, thereby optimizing the policy learning process. In the small-deformation stage, a Jacobian-based visual servo controller is deployed to ensure high-precision convergence. Simulation results show that the proposed method enables efficient policy learning and significantly outperforms mainstream baselines in shape control success rate and precision. Furthermore, the framework effectively transfers the policy trained in simulation to real-world tasks with zero-shot adaptation. It successfully completes all 30 cases with diverse initial and target shapes across DLOs of different sizes and materials. The project website is available at: https://anonymous.4open.science/w/sc-mbrl-dlo-EB48/

</details>


### [222] [Enhancing Cellular-enabled Collaborative Robots Planning through GNSS data for SAR Scenarios](https://arxiv.org/abs/2602.21899)
*Arnau Romero,Carmen Delgado,Jana Baguer,Raúl Suárez,Xavier Costa-Pérez*

Main category: cs.RO

TL;DR: A novel SAR framework for cellular-enabled collaborative robots that optimizes robot deployment considering exploration area, terrain, fleet size, energy profiles, exploration rate, and response time to determine minimum robots needed and optimal paths for effective coverage and data backhaul over mobile networks.


<details>
  <summary>Details</summary>
Motivation: Cellular-enabled collaborative robots are crucial for SAR and emergency response but face limitations due to battery power constraints and the need for persistent, low-latency communication, which restricts operational time and mobility. The evolving capabilities of 5G/6G networks present opportunities to address these challenges.

Method: Proposes a SAR framework with Mission Planning and Mission Execution phases that optimizes robot deployment. Considers parameters including exploration area size, terrain elevation, robot fleet size, communication-influenced energy profiles, desired exploration rate, and target response time to determine minimum number of robots required and their optimal paths.

Result: Demonstrates trade-offs between number of robots, explored area, and response time for wheeled and quadruped robots. Quantifies impact of terrain elevation data on mission time and energy consumption, showing benefits of incorporating real-world environmental factors that affect mobile signal propagation and connectivity.

Conclusion: The framework provides critical insights for leveraging next-generation mobile networks to enhance autonomous SAR operations by optimizing robot deployment for effective coverage and timely data backhaul while considering practical constraints like energy consumption and terrain effects.

Abstract: Cellular-enabled collaborative robots are becoming paramount in Search-and-Rescue (SAR) and emergency response. Crucially dependent on resilient mobile network connectivity, they serve as invaluable assets for tasks like rapid victim localization and the exploration of hazardous, otherwise unreachable areas. However, their reliance on battery power and the need for persistent, low-latency communication limit operational time and mobility. To address this, and considering the evolving capabilities of 5G/6G networks, we propose a novel SAR framework that includes Mission Planning and Mission Execution phases and that optimizes robot deployment. By considering parameters such as the exploration area size, terrain elevation, robot fleet size, communication-influenced energy profiles, desired exploration rate, and target response time, our framework determines the minimum number of robots required and their optimal paths to ensure effective coverage and timely data backhaul over mobile networks. Our results demonstrate the trade-offs between number of robots, explored area, and response time for wheeled and quadruped robots. Further, we quantify the impact of terrain elevation data on mission time and energy consumption, showing the benefits of incorporating real-world environmental factors that might also affect mobile signal propagation and connectivity into SAR planning. This framework provides critical insights for leveraging next-generation mobile networks to enhance autonomous SAR operations.

</details>


### [223] [Dream-SLAM: Dreaming the Unseen for Active SLAM in Dynamic Environments](https://arxiv.org/abs/2602.21967)
*Xiangqi Meng,Pengxu Hou,Zhenjun Zhao,Javier Civera,Daniel Cremers,Hesheng Wang,Haoang Li*

Main category: cs.RO

TL;DR: Dream-SLAM: A monocular active SLAM method that uses "dreamed" cross-spatio-temporal images and semantic structures to improve exploration in dynamic environments, addressing limitations of existing active SLAM approaches.


<details>
  <summary>Details</summary>
Motivation: Existing active SLAM methods have three main limitations: 1) They inherit restrictions from underlying SLAM modules, 2) Their motion planning is shortsighted without long-term vision, and 3) They struggle with dynamic scenes. The authors aim to overcome these limitations for more effective exploration.

Method: Proposes Dream-SLAM which generates "dreamed" cross-spatio-temporal images and semantically plausible structures of partially observed dynamic environments. These generated images are fused with real observations to reduce noise and data incompleteness. The method integrates dreamed and observed scene structures for long-horizon planning.

Result: Extensive experiments on public and self-collected datasets show Dream-SLAM outperforms state-of-the-art methods in localization accuracy, mapping quality, and exploration efficiency.

Conclusion: Dream-SLAM successfully addresses key limitations of existing active SLAM methods by leveraging dreamed scene representations for better pose estimation, 3D reconstruction, and long-horizon planning in dynamic environments.

Abstract: In addition to the core tasks of simultaneous localization and mapping (SLAM), active SLAM additionally in- volves generating robot actions that enable effective and efficient exploration of unknown environments. However, existing active SLAM pipelines are limited by three main factors. First, they inherit the restrictions of the underlying SLAM modules that they may be using. Second, their motion planning strategies are typically shortsighted and lack long-term vision. Third, most approaches struggle to handle dynamic scenes. To address these limitations, we propose a novel monocular active SLAM method, Dream-SLAM, which is based on dreaming cross-spatio-temporal images and semantically plausible structures of partially observed dynamic environments. The generated cross-spatio-temporal im- ages are fused with real observations to mitigate noise and data incompleteness, leading to more accurate camera pose estimation and a more coherent 3D scene representation. Furthermore, we integrate dreamed and observed scene structures to enable long- horizon planning, producing farsighted trajectories that promote efficient and thorough exploration. Extensive experiments on both public and self-collected datasets demonstrate that Dream-SLAM outperforms state-of-the-art methods in localization accuracy, mapping quality, and exploration efficiency. Source code will be publicly available upon paper acceptance.

</details>


### [224] [Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots](https://arxiv.org/abs/2602.21983)
*Jingchao Wei,Jingkai Qin,Yuxiao Cao,Jingcheng Huang,Xiangrui Zeng,Min Li,Zhouping Yin*

Main category: cs.RO

TL;DR: RGS framework enables humanoid robots to perform natural gaze shifts in HRI by combining VLM-based gaze reasoning with VQ-VAE motion generation.


<details>
  <summary>Details</summary>
Motivation: Natural gaze shifts are crucial for social interaction but challenging for robots, requiring integration of cognitive attention mechanisms with biomimetic motion generation in unconstrained HRI.

Method: Two-component unified pipeline: 1) VLM-based gaze reasoning to infer context-appropriate gaze targets from multimodal cues, 2) Conditional VQ-VAE model for eye-head coordinated gaze-shift motion generation.

Result: RGS effectively replicates human-like target selection and generates realistic, diverse gaze-shift motions as validated through experiments.

Conclusion: The proposed RGS framework successfully integrates cognitive reasoning and motion generation for natural gaze shifts in human-robot interaction.

Abstract: Leveraging auditory and visual feedback for attention reorientation is essential for natural gaze shifts in social interaction. However, enabling humanoid robots to perform natural and context-appropriate gaze shifts in unconstrained human--robot interaction (HRI) remains challenging, as it requires the coupling of cognitive attention mechanisms and biomimetic motion generation. In this work, we propose the Robot Gaze-Shift (RGS) framework, which integrates these two components into a unified pipeline. First, RGS employs a vision--language model (VLM)-based gaze reasoning pipeline to infer context-appropriate gaze targets from multimodal interaction cues, ensuring consistency with human gaze-orienting regularities. Second, RGS introduces a conditional Vector Quantized-Variational Autoencoder (VQ-VAE) model for eye--head coordinated gaze-shift motion generation, producing diverse and human-like gaze-shift behaviors. Experiments validate that RGS effectively replicates human-like target selection and generates realistic, diverse gaze-shift motions.

</details>


### [225] [Are Foundation Models the Route to Full-Stack Transfer in Robotics?](https://arxiv.org/abs/2602.22001)
*Freek Stulp,Samuel Bustamante,João Silvério,Alin Albu-Schäffer,Jeannette Bohg,Shuran Song*

Main category: cs.RO

TL;DR: Foundation models (LLMs, VLMs, VLAs) enable multi-level transfer learning in robotics from high-level language to low-level motor skills, bringing robots closer to "full-stack transfer" capabilities.


<details>
  <summary>Details</summary>
Motivation: To explore how foundation models and transformers impact different levels of robotic transfer learning, from linguistic to motor skill transfer, and assess their potential for achieving comprehensive "full-stack transfer" in robotics.

Method: Analyzing foundation models (LLMs, VLMs, VLAs) from a robotic transfer learning perspective, identifying recurring transfer concepts beyond specific implementations, and examining data collection and benchmarking challenges in the foundation model era.

Result: Foundation models enable transfer learning across multiple abstraction levels in robotics, revealing consistent transfer concepts and highlighting data/benchmark challenges, positioning them as key technology for advancing toward full-stack transfer.

Conclusion: Foundation models are expected to remain a crucial technology on the path to achieving full-stack transfer in robotics, enabling comprehensive learning transfer from high-level language understanding to low-level motor control.

Abstract: In humans and robots alike, transfer learning occurs at different levels of abstraction, from high-level linguistic transfer to low-level transfer of motor skills. In this article, we provide an overview of the impact that foundation models and transformer networks have had on these different levels, bringing robots closer than ever to "full-stack transfer". Considering LLMs, VLMs and VLAs from a robotic transfer learning perspective allows us to highlight recurring concepts for transfer, beyond specific implementations. We also consider the challenges of data collection and transfer benchmarks for robotics in the age of foundation models. Are foundation models the route to full-stack transfer in robotics? Our expectation is that they will certainly stay on this route as a key technology.

</details>


### [226] [Parallel Continuous-Time Relative Localization with Augmented Clamped Non-Uniform B-Splines](https://arxiv.org/abs/2602.22006)
*Jiadong Lu,Zhehan Li,Tao Han,Miao Xu,Chao Xu,Yanjun Cao*

Main category: cs.RO

TL;DR: CT-RIO: A continuous-time relative-inertial odometry framework using clamped non-uniform B-splines for accurate multi-robot localization with asynchronous measurements and clock time-offsets.


<details>
  <summary>Details</summary>
Motivation: Multi-robot systems need accurate relative localization, but face challenges with asynchronous measurements and clock time-offsets. Existing continuous-time methods suffer from query-time delays and high computational costs, making them unsuitable for swarm-scale applications requiring high accuracy, low latency, and high frequency.

Method: Uses Clamped Non-Uniform B-splines (C-NUBS) to represent robot states, eliminating query-time delay. Introduces closed-form extension/shrinkage operations for online estimation. Implements knot-keyknot strategy for high-frequency spline extension with sparse keyknots. Formulates sliding-window relative localization using relative kinematics and inter-robot constraints. Uses parallel decomposition with incremental asynchronous block coordinate descent for swarm-scale computation.

Result: CT-RIO converges from time-offsets as large as 263 ms to sub-millisecond within 3 seconds, achieving RMSEs of 0.046 m and 1.8°. Consistently outperforms state-of-the-art methods with improvements up to 60% under high-speed motion.

Conclusion: CT-RIO provides an effective continuous-time framework for multi-robot relative localization that addresses the limitations of existing methods, achieving high accuracy, low latency, and computational efficiency suitable for robot swarm applications.

Abstract: Accurate relative localization is critical for multi-robot cooperation. In robot swarms, measurements from different robots arrive asynchronously and with clock time-offsets. Although Continuous-Time (CT) formulations have proved effective for handling asynchronous measurements in single-robot SLAM and calibration, extending CT methods to multi-robot settings faces great challenges to achieve high-accuracy, low-latency, and high-frequency performance. Especially, existing CT methods suffer from the inherent query-time delay of unclamped B-splines and high computational cost. This paper proposes CT-RIO, a novel Continuous-Time Relative-Inertial Odometry framework. We employ Clamped Non-Uniform B-splines (C-NUBS) to represent robot states for the first time, eliminating the query-time delay. We further augment C-NUBS with closed-form extension and shrinkage operations that preserve the spline shape, making it suitable for online estimation and enabling flexible knot management. This flexibility leads to the concept of knot-keyknot strategy, which supports spline extension at high-frequency while retaining sparse keyknots for adaptive relative-motion modeling. We then formulate a sliding-window relative localization problem that operates purely on relative kinematics and inter-robot constraints. To meet the demanding computation required at swarm scale, we decompose the tightly-coupled optimization into robot-wise sub-problems and solve them in parallel using incremental asynchronous block coordinate descent. Extensive experiments show that CT-RIO converges from time-offsets as large as 263 ms to sub-millisecond within 3 s, and achieves RMSEs of 0.046 m and 1.8 °. It consistently outperforms state-of-the-art methods, with improvements of up to 60% under high-speed motion.

</details>


### [227] [World Guidance: World Modeling in Condition Space for Action Generation](https://arxiv.org/abs/2602.22010)
*Yue Su,Sijin Chen,Haixin Shi,Mingyu Liu,Zhengshen Zhang,Ningyuan Huang,Weiheng Zhong,Zhengbang Zhu,Yuxiao Liu,Xihui Liu*

Main category: cs.RO

TL;DR: WoG (World Guidance) is a framework that maps future observations into compact conditions to guide precise action generation in Vision-Language-Action models, outperforming future prediction methods.


<details>
  <summary>Details</summary>
Motivation: Existing approaches struggle to balance efficient future representations with preserving fine-grained information needed for precise action generation in VLA models.

Method: WoG maps future observations into compact conditions injected into action inference pipeline, training VLA to simultaneously predict these compressed conditions alongside future actions.

Result: WoG facilitates fine-grained action generation with superior generalization, learns effectively from human manipulation videos, and outperforms existing future prediction methods in simulation and real-world experiments.

Conclusion: Modeling and predicting compressed condition space enables effective world modeling for action inference, significantly enhancing VLA model capabilities.

Abstract: Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/

</details>


### [228] [FlowCorrect: Efficient Interactive Correction of Generative Flow Policies for Robotic Manipulation](https://arxiv.org/abs/2602.22056)
*Edgar Welte,Yitian Shi,Rosa Wolf,Maximillian Gilles,Rania Rayyes*

Main category: cs.RO

TL;DR: FlowCorrect enables real-time correction of near-miss robot failures using sparse human nudges via VR, adapting policies without full retraining.


<details>
  <summary>Details</summary>
Motivation: Generative manipulation policies often fail catastrophically under deployment-time distribution shift, but many failures are near-misses that could be corrected with small adjustments rather than requiring complete policy retraining.

Method: During execution, humans provide brief corrective pose nudges via lightweight VR interface. FlowCorrect uses these sparse corrections to locally adapt the policy, improving actions without retraining the backbone model while preserving performance on previously learned scenarios.

Result: Evaluated on real-world robot across three tabletop tasks (pick-and-place, pouring, cup uprighting). With low correction budget, improves success on hard cases by 85% while preserving performance on previously solved scenarios.

Conclusion: FlowCorrect enables fast, sample-efficient incremental human-in-the-loop corrections of generative visuomotor policies at deployment time in real-world robotics, learning with very few demonstrations.

Abstract: Generative manipulation policies can fail catastrophically under deployment-time distribution shift, yet many failures are near-misses: the robot reaches almost-correct poses and would succeed with a small corrective motion. We present FlowCorrect, a deployment-time correction framework that converts near-miss failures into successes using sparse human nudges, without full policy retraining. During execution, a human provides brief corrective pose nudges via a lightweight VR interface. FlowCorrect uses these sparse corrections to locally adapt the policy, improving actions without retraining the backbone while preserving the model performance on previously learned scenarios. We evaluate on a real-world robot across three tabletop tasks: pick-and-place, pouring, and cup uprighting. With a low correction budget, FlowCorrect improves success on hard cases by 85\% while preserving performance on previously solved scenarios. The results demonstrate clearly that FlowCorrect learns only with very few demonstrations and enables fast and sample-efficient incremental, human-in-the-loop corrections of generative visuomotor policies at deployment time in real-world robotics.

</details>


### [229] [Force Policy: Learning Hybrid Force-Position Control Policy under Interaction Frame for Contact-Rich Manipulation](https://arxiv.org/abs/2602.22088)
*Hongjie Fang,Shirun Tang,Mingyu Mei,Haoxiang Qin,Zihao He,Jingjing Chen,Ying Feng,Chenxi Wang,Wanxi Liu,Zaixing He,Cewu Lu,Shiquan Wang*

Main category: cs.RO

TL;DR: Force Policy: A vision-force policy that decouples global guidance from local contact control using interaction frames for robust contact-rich manipulation.


<details>
  <summary>Details</summary>
Motivation: Contact-rich manipulation requires integration of perception and force feedback, but existing approaches either entangle these roles in monolithic networks or assume known task structures, limiting generalization and stability.

Method: Proposes physically grounded interaction frames that decouple force regulation from motion execution, learned from demonstrations. Uses global policy with vision for free-space guidance and high-frequency local policy with force feedback for hybrid force-position control during contact.

Result: Real-world experiments across diverse contact-rich tasks show consistent gains over baselines: more robust contact establishment, more accurate force regulation, and reliable generalization to novel objects with varied geometries and physical properties.

Conclusion: The Force Policy approach improves both contact stability and execution quality by properly separating global perception-based guidance from local force-based interaction control through interaction frames.

Abstract: Contact-rich manipulation demands human-like integration of perception and force feedback: vision should guide task progress, while high-frequency interaction control must stabilize contact under uncertainty. Existing learning-based policies often entangle these roles in a monolithic network, trading off global generalization against stable local refinement, while control-centric approaches typically assume a known task structure or learn only controller parameters rather than the structure itself. In this paper, we formalize a physically grounded interaction frame, an instantaneous local basis that decouples force regulation from motion execution, and propose a method to recover it from demonstrations. Based on this, we address both issues by proposing Force Policy, a global-local vision-force policy in which a global policy guides free-space actions using vision, and upon contact, a high-frequency local policy with force feedback estimates the interaction frame and executes hybrid force-position control for stable interaction. Real-world experiments across diverse contact-rich tasks show consistent gains over strong baselines, with more robust contact establishment, more accurate force regulation, and reliable generalization to novel objects with varied geometries and physical properties, ultimately improving both contact stability and execution quality. Project page: https://force-policy.github.io/

</details>


### [230] [Behavioral Cloning for Robotic Connector Assembly: An Empirical Study](https://arxiv.org/abs/2602.22100)
*Andreas Kernbach,Daniel Bargmann,Werner Kraus,Marco F. Huber*

Main category: cs.RO

TL;DR: Behavioral cloning with force-torque and visual sensing achieves over 90% success rate for robotic connector insertion across varied geometries.


<details>
  <summary>Details</summary>
Motivation: Automating wire harness assembly is challenging due to deformable cables, varying connector geometries, and the need for delicate insertion. While humans can do this intuitively with visual and haptic feedback, programming industrial robots for adaptable insertion remains difficult.

Method: Used behavioral cloning to learn an action prediction model for connector insertion that fuses force-torque sensing with a fixed position camera. Compared several network architectures using up to 300 successful human demonstrations collected via teleoperation of a UR5e robot with a SpaceMouse under varying connector poses.

Result: The system achieved an overall insertion success rate of over 90% when evaluated against five different connector geometries under varying connector poses.

Conclusion: Behavioral cloning with multimodal sensing (force-torque + vision) is effective for learning adaptable robotic connector insertion skills, demonstrating high success rates across diverse connector geometries and poses.

Abstract: Automating the assembly of wire harnesses is challenging in automotive, electrical cabinet, and aircraft production, particularly due to deformable cables and a high variance in connector geometries. In addition, connectors must be inserted with limited force to avoid damage, while their poses can vary significantly. While humans can do this task intuitively by combining visual and haptic feedback, programming an industrial robot for such a task in an adaptable manner remains difficult. This work presents an empirical study investigating the suitability of behavioral cloning for learning an action prediction model for connector insertion that fuses force-torque sensing with a fixed position camera. We compare several network architectures and other design choices using a dataset of up to 300 successful human demonstrations collected via teleoperation of a UR5e robot with a SpaceMouse under varying connector poses. The resulting system is then evaluated against five different connector geometries under varying connector poses, achieving an overall insertion success rate of over 90 %.

</details>


### [231] [System Design of the Ultra Mobility Vehicle: A Driving, Balancing, and Jumping Bicycle Robot](https://arxiv.org/abs/2602.22118)
*Benjamin Bokser,Daniel Gonzalez,Surya Singh,Aaron Preston,Alex Bahner,Annika Wollschläger,Arianna Ilvonen,Asa Eckert-Erdheim,Ashwin Khadke,Bilal Hammoud,Dean Molinaro,Fabian Jenelten,Henry Mayne,Howie Choset,Igor Bogoslavskyi,Itic Tinman,James Tigue,Jan Preisig,Kaiyu Zheng,Kenny Sharma,Kim Ang,Laura Lee,Liana Margolese,Nicole Lin,Oscar Frias,Paul Drews,Ravi Boggavarapu,Rick Burnham,Samuel Zapolsky,Sangbae Kim,Scott Biddlestone,Sean Mayorga,Shamel Fahmi,Tyler McCollum,Velin Dimitrov,William Moyne,Yu-Ming Chen,Farbod Farshidian,Marco Hutter,David Perry,Al Rizzi,Gabe Nelson*

Main category: cs.RO

TL;DR: A robotic bicycle platform (UMV) inspired by trials cyclists combines bicycle mechanics with reaction mass for dynamic mobility, achieving high-speed movement and jumping over obstacles using reinforcement learning control.


<details>
  <summary>Details</summary>
Motivation: Inspired by trials cyclists and mountain bikers who demonstrate remarkable versatility in hopping, jumping, balancing, and riding on one or both wheels, the researchers aim to create a robotic platform that can achieve similar dynamic athletic behaviors with minimal actuation.

Method: The Ultra Mobility Vehicle (UMV) combines bicycle mechanics with a reaction mass system. The design uses simulation-driven optimization to create spatial linkage topology optimized for vertical jump height and momentum-based balancing. Control is achieved through a constrained Reinforcement Learning framework for zero-shot transfer of diverse behaviors.

Result: The 23.5 kg robot successfully demonstrates track-stands, jumps, wheelies, rear wheel hopping, and front flips. It achieves high speeds (8 m/s) and can jump on and over obstacles up to 1 meter tall (130% of the robot's nominal height).

Conclusion: The UMV platform successfully replicates athletic bicycle maneuvers using minimal actuated degrees of freedom, demonstrating that simulation-driven design combined with constrained reinforcement learning enables complex dynamic behaviors in robotic systems.

Abstract: Trials cyclists and mountain bike riders can hop, jump, balance, and drive on one or both wheels. This versatility allows them to achieve speed and energy-efficiency on smooth terrain and agility over rough terrain. Inspired by these athletes, we present the design and control of a robotic platform, Ultra Mobility Vehicle (UMV), which combines a bicycle and a reaction mass to move dynamically with minimal actuated degrees of freedom. We employ a simulation-driven design optimization process to synthesize a spatial linkage topology with a focus on vertical jump height and momentum-based balancing on a single wheel contact. Using a constrained Reinforcement Learning (RL) framework, we demonstrate zero-shot transfer of diverse athletic behaviors, including track-stands, jumps, wheelies, rear wheel hopping, and front flips. This 23.5 kg robot is capable of high speeds (8 m/s) and jumping on and over large obstacles (1 m tall, or 130% of the robot's nominal height).

</details>


### [232] [Position-Based Flocking for Persistent Alignment without Velocity Sensing](https://arxiv.org/abs/2602.22154)
*Hossein B. Jond,Veli Bakırcıoğlu,Logan E. Beaver,Nejat Tükenmez,Adel Akbarimajd,Martin Saska*

Main category: cs.RO

TL;DR: Position-based flocking model achieves velocity alignment without velocity sensing using relative position changes and adaptive alignment gain.


<details>
  <summary>Details</summary>
Motivation: Real-world robotic swarms often lack reliable velocity measurements, so a position-based approach is needed for cohesive collective motion inspired by bird flocks and fish schools.

Method: Uses relative velocity approximations from changes between current and initial relative positions, with time- and density-dependent alignment gain that has a minimum threshold to maintain persistent alignment.

Result: Simulations with 50 agents show faster, more sustained directional alignment and more compact formations than velocity-alignment baseline. Real-world experiments with 9 wheeled mobile robots validate the approach.

Conclusion: The position-based flocking model is well-suited for real-world robotic swarms where velocity measurements are unreliable, enabling sustained coherent collective motion.

Abstract: Coordinated collective motion in bird flocks and fish schools inspires algorithms for cohesive swarm robotics. This paper presents a position-based flocking model that achieves persistent velocity alignment without velocity sensing. By approximating relative velocity differences from changes between current and initial relative positions and incorporating a time- and density-dependent alignment gain with a non-zero minimum threshold to maintain persistent alignment, the model sustains coherent collective motion over extended periods. Simulations with a collective of 50 agents demonstrate that the position-based flocking model attains faster and more sustained directional alignment and results in more compact formations than a velocity-alignment-based baseline. This position-based flocking model is particularly well-suited for real-world robotic swarms, where velocity measurements are unreliable, noisy, or unavailable. Experimental results using a team of nine real wheeled mobile robots are also presented.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [233] [On Optimizing Image Codecs for VMAF NEG: Analysis, Issues, and a Robust Loss Proposal](https://arxiv.org/abs/2602.21336)
*Florian Fingscheidt,Alexander Karabutov,Panqi Jia,Elena Alshina,JÖrn Ostermann*

Main category: eess.IV

TL;DR: VMAF NEG, designed to be robust against attacks, still has vulnerabilities when used for image codec fine-tuning. The paper proposes a robust loss function incorporating VMAF NEG to benefit from its high correlation with human perception while addressing its weaknesses.


<details>
  <summary>Details</summary>
Motivation: While VMAF has high correlation with human perception and is popular for video/image coding, it's vulnerable to attacks (like unsharpening) that can artificially boost VMAF scores while actually decreasing perceptual quality. VMAF NEG was created to be more robust, but its vulnerabilities when used for codec fine-tuning need investigation.

Method: 1) Analyze existing vulnerabilities of VMAF NEG towards attacks, particularly when used for image codec fine-tuning. 2) Propose a robust loss function that incorporates VMAF NEG for fine-tuning either encoder or decoder. 3) Provide perceptual impressions of image examples to support quantitative results.

Result: The paper identifies remaining vulnerabilities in VMAF NEG, proposes a solution through robust loss design, and provides both quantitative objective results and perceptual examples demonstrating the effectiveness of their approach.

Conclusion: VMAF NEG still has vulnerabilities when used for codec fine-tuning, but can be effectively utilized through a carefully designed robust loss function that maintains its high correlation with human perception while mitigating attack susceptibility.

Abstract: The VMAF (video multi-method assessment fusion) metric for image and video coding recently gained more and more popularity as it is supposed to have a high correlation with human perception. This makes training and particularly fine-tuning of machine-learned codecs on this metric interesting. However, VMAF is shown to be attackable in a way that, e.g., unsharpening an image can lead to a gain in VMAF quality while decreasing the quality in human perception. A particular version of VMAF called VMAF NEG has been designed to be more robust against such attacks and therefore it should be more useful for fine-tuning of codecs. In this paper, our contributions are threefold. First, we identify and analyze the still existing vulnerability of VMAF NEG towards attacks, particulary towards the attack that consists in employing VMAF NEG for image codec fine-tuning. Second, to benefit from VMAF NEG's high correlation with human perception, we propose a robust loss including VMAF NEG for fine-tuning either the encoder or the decoder. Third, we support our quantitative objective results by providing perceptive impressions of some image examples.

</details>


### [234] [RelA-Diffusion: Relativistic Adversarial Diffusion for Multi-Tracer PET Synthesis from Multi-Sequence MRI](https://arxiv.org/abs/2602.21345)
*Minhui Yu,Yongheng Sun,David S. Lalush,Jason P Mihalik,Pew-Thian Yap,Mingxia Liu*

Main category: eess.IV

TL;DR: RelA-Diffusion: A relativistic adversarial diffusion framework that synthesizes multi-tracer PET images from multi-sequence MRI (T1-weighted + T2-FLAIR) with improved anatomical and pathological detail fidelity.


<details>
  <summary>Details</summary>
Motivation: Multi-tracer PET is crucial for comprehensive neurological assessment but limited by high costs, radiation exposure, and tracer availability. Existing deep learning methods for PET synthesis from MRI struggle with fine-grained anatomical details and produce artifacts.

Method: Relativistic Adversarial Diffusion framework using both T1-weighted and T2-FLAIR MRI as complementary inputs. Introduces gradient-penalized relativistic adversarial loss applied to intermediate clean predictions of the diffusion model, with adversarial feedback at each diffusion timestep for consistent refinement.

Result: Outperforms existing methods in both visual fidelity and quantitative metrics on two datasets, demonstrating accurate synthesis of multi-tracer PET images.

Conclusion: RelA-Diffusion effectively synthesizes multi-tracer PET from multi-sequence MRI, capturing rich structural information and generating realistic local structures with stable training, showing potential for clinical applications.

Abstract: Multi-tracer positron emission tomography (PET) provides critical insights into diverse neuropathological processes such as tau accumulation, neuroinflammation, and $β$-amyloid deposition in the brain, making it indispensable for comprehensive neurological assessment. However, routine acquisition of multi-tracer PET is limited by high costs, radiation exposure, and restricted tracer availability. Recent efforts have explored deep learning approaches for synthesizing PET images from structural MRI. While some methods rely solely on T1-weighted MRI, others incorporate additional sequences such as T2-FLAIR to improve pathological sensitivity. However, existing methods often struggle to capture fine-grained anatomical and pathological details, resulting in artifacts and unrealistic outputs. To this end, we propose RelA-Diffusion, a Relativistic Adversarial Diffusion framework for multi-tracer PET synthesis from multi-sequence MRI. By leveraging both T1-weighted and T2-FLAIR scans as complementary inputs, RelA-Diffusion captures richer structural information to guide image generation. To improve synthesis fidelity, we introduce a gradient-penalized relativistic adversarial loss to the intermediate clean predictions of the diffusion model. This loss compares real and generated images in a relative manner, encouraging the synthesis of more realistic local structures. Both the relativistic formulation and the gradient penalty contribute to stabilizing the training, while adversarial feedback at each diffusion timestep enables consistent refinement throughout the generation process. Extensive experiments on two datasets demonstrate that RelA-Diffusion outperforms existing methods in both visual fidelity and quantitative metrics, highlighting its potential for accurate synthesis of multi-tracer PET.

</details>


### [235] [Perceptual Quality Optimization of Image Super-Resolution](https://arxiv.org/abs/2602.21482)
*Wei Zhou,Yixiao Li,Hadi Amirpour,Xiaoshuai Hao,Jiang Liu,Peng Wang,Hantao Liu*

Main category: eess.IV

TL;DR: Efficient-PBAN: A perceptual super-resolution network that uses image-level quality prediction trained on human preference data, integrating learned perceptual loss for better visual quality without extensive patch sampling.


<details>
  <summary>Details</summary>
Motivation: Current SR methods face a fidelity vs. visual quality trade-off due to reliance on distortion-oriented losses or heuristic perceptual priors. There's a need for SR optimization that aligns with human-preferred quality.

Method: Proposes Efficient-PBAN with bi-directional attention for image-level perceptual quality prediction, trained on a self-constructed SR quality dataset with human opinion scores. The learned metric is integrated as differentiable perceptual loss in SR training.

Result: Extensive experiments show superior perceptual quality compared to existing methods. The approach achieves strong correlation with subjective judgments and enables efficient image-level perception without extensive patch sampling.

Conclusion: Efficient-PBAN effectively addresses the fidelity-quality trade-off in SR by explicitly optimizing for human-preferred quality through learned perceptual assessment integrated into the training loop.

Abstract: Single-image super-resolution (SR) has achieved remarkable progress with deep learning, yet most approaches rely on distortion-oriented losses or heuristic perceptual priors, which often lead to a trade-off between fidelity and visual quality. To address this issue, we propose an \textit{Efficient Perceptual Bi-directional Attention Network (Efficient-PBAN)} that explicitly optimizes SR towards human-preferred quality. Unlike patch-based quality models, Efficient-PBAN avoids extensive patch sampling and enables efficient image-level perception. The proposed framework is trained on our self-constructed SR quality dataset that covers a wide range of state-of-the-art SR methods with corresponding human opinion scores. Using this dataset, Efficient-PBAN learns to predict perceptual quality in a way that correlates strongly with subjective judgments. The learned metric is further integrated into SR training as a differentiable perceptual loss, enabling closed-loop alignment between reconstruction and perceptual assessment. Extensive experiments demonstrate that our approach delivers superior perceptual quality. Code is publicly available at https://github.com/Lighting-YXLI/Efficient-PBAN.

</details>


### [236] [Deep Unfolding Real-Time Super-Resolution Using Subpixel-Shift Twin Image and Convex Self-Similarity Prior](https://arxiv.org/abs/2602.21513)
*Chia-Hsiang Lin,Wei-Chih Liu,Yu-En Chiu,Jhao-Ting Lin*

Main category: eess.IV

TL;DR: COSUP is a novel deep unfolding network for twin-image super-resolution that achieves state-of-the-art performance with millisecond-level computation, outperforming official CNES products on real-world satellite data.


<details>
  <summary>Details</summary>
Motivation: Twin-image super-resolution (TISR) is the most challenging multi-image super-resolution scenario with crucial applications like SPOT-5 supermode imaging, but has been less investigated and needs efficient, interpretable solutions.

Method: Formulates TISR using a convex criterion and implements it with a deep unfolding network featuring: 1) an embedded simple shift operator to handle coupled data-fitting terms, and 2) a transformer trained with convex self-similarity loss for proximal mapping of the TISR regularizer.

Result: COSUP achieves state-of-the-art performance with very fast millisecond-level computational time. On real-world data with non-uniform subpixel shifts, it shows great superiority over official CNES supermode imaging products in metrics like NIQE.

Conclusion: The proposed COSUP algorithm provides an interpretable, efficient solution for twin-image super-resolution that significantly advances satellite remote sensing capabilities, particularly for applications like SPOT-5 supermode imaging.

Abstract: Multi-image super-resolution (MISR) is a critical technique for satellite remote sensing. In the perspective of information, twin-image super-resolution (TISR) is regarded as the most challenging MISR scenario, having crucial applications like the SPOT-5 supermode imaging. In TISR, an image is super-resolved by its subpixel-shift counterpart (i.e., twin image), where the two images are typically offset by half a pixel both horizontally and vertically. We formulate the less investigated TISR using a convex criterion, which is implemented using a novel deep unfolding network. In the unfolding, an embedded simple shift operator trickily addresses the coupled TISR data-fitting terms, and a transformer trained with a convex self-similarity loss function elegantly implements the proximal mapping induced by the TISR regularizer. The proposed convex self-similarity unfolding supermode super-resolution (COSUP) algorithm is interpretable and achieves state-of-the-art performance with very fast millisecond-level computational time. COSUP is also tested on real-world data, for which the subpixel shifts would not be spatially uniform, with results showing great superiority over the official CNES supermode imaging product in terms of credible metrics (e.g., natural image quality evaluator, NIQE). Source codes: https://github.com/IHCLab/COSUP.

</details>


### [237] [Learning spatially adaptive sparsity level maps for arbitrary convolutional dictionaries](https://arxiv.org/abs/2602.21707)
*Joshua Schulz,David Schote,Christoph Kolbitsch,Kostas Papafitsoros,Andreas Kofler*

Main category: eess.IV

TL;DR: Improved learned reconstruction method with model-based convolutional dictionary regularization achieves filter-permutation invariance and dictionary flexibility, showing better robustness to distribution shifts than black-box methods.


<details>
  <summary>Details</summary>
Motivation: Address interpretability and robustness concerns in state-of-the-art learned reconstruction methods that rely on black-box modules, by building on a model-based approach with data-driven regularization.

Method: Extends a model-based convolutional dictionary regularization method with improved network design and training strategies to achieve filter-permutation invariance and allow changing the convolutional dictionary at inference time.

Result: Applied to low-field MRI, the method outperforms other deep learning methods on in vivo data, shows benefits of using different dictionaries, and demonstrates better robustness to in- and out-of-distribution data shifts.

Conclusion: The proposed method suffers less from data distribution shifts compared to other learned methods due to reduced reliance on training data, attributed to its underlying model-based reconstruction component.

Abstract: State-of-the-art learned reconstruction methods often rely on black-box modules that, despite their strong performance, raise questions about their interpretability and robustness. Here, we build on a recently proposed image reconstruction method, which is based on embedding data-driven information into a model-based convolutional dictionary regularization via neural network-inferred spatially adaptive sparsity level maps. By means of improved network design and dedicated training strategies, we extend the method to achieve filter-permutation invariance as well as the possibility to change the convolutional dictionary at inference time. We apply our method to low-field MRI and compare it to several other recent deep learning-based methods, also on in vivo data, in which the benefit for the use of a different dictionary is showcased. We further assess the method's robustness when tested on in- and out-of-distribution data. When tested on the latter, the proposed method suffers less from the data distribution shift compared to the other learned methods, which we attribute to its reduced reliance on training data due to its underlying model-based reconstruction component.

</details>


### [238] [Towards Object Segmentation Mask Selection Using Specular Reflections](https://arxiv.org/abs/2602.21777)
*Katja Kossira,Yunxuan Zhu,Jürgen Seiler,André Kaup*

Main category: eess.IV

TL;DR: A novel method improves object segmentation in images with specular reflections by exploiting the fact that reflections must lie on object surfaces, achieving significant improvements over state-of-the-art methods without requiring specialized training.


<details>
  <summary>Details</summary>
Motivation: Specular reflections create sharp intensity transitions that mislead both conventional and deep learning-based segmentation methods, creating a need for approaches that can handle these challenging cases without requiring specialized training data.

Method: The method identifies the largest region containing specular reflections as the object, leveraging the fact that reflections must lie on object surfaces. This approach doesn't require specialized training data or model adaptation.

Result: The method achieves up to 26.7% improvement in IoU, 22.3% in DSC, and 9.7% in pixel accuracy compared to the best baseline (SAM2). Qualitative evaluations on real-world images confirm robustness and generalizability.

Conclusion: Exploiting the physical constraint that specular reflections must lie on object surfaces provides an effective way to improve segmentation accuracy in challenging reflection scenarios without requiring specialized training or model modifications.

Abstract: Specular reflections pose a significant challenge for object segmentation, as their sharp intensity transitions often mislead both conventional algorithms and deep learning based methods. However, as the specular reflection must lie on the surface of the object, this fact can be exploited to improve the segmentation masks. By identifying the largest region containing the reflection as the object, we derive a more accurate object mask without requiring specialized training data or model adaption. We evaluate our method on both synthetic and real world images and compare it against established and state-of-the-art techniques including Otsu thresholding, YOLO, and SAM2. Compared to the best performing baseline SAM2, our approach achieves up to 26.7% improvement in IoU, 22.3% in DSC, and 9.7% in pixel accuracy. Qualitative evaluations on real world images further confirm the robustness and generalizability of the proposed approach.

</details>


### [239] [Lumosaic: Hyperspectral Video via Active Illumination and Coded-Exposure Pixels](https://arxiv.org/abs/2602.22140)
*Dhruv Verma,Andrew Qiu,Roberto Rangel,Ayandev Barman,Hao Yang,Chenjia Hu,Fengqi Zhang,Roman Genov,David B. Lindell,Kiriakos N. Kutulakos,Alex Mariakakis*

Main category: eess.IV

TL;DR: Lumosaic is a real-time hyperspectral video system using synchronized LED illumination and coded-exposure-pixel camera to capture dynamic scenes with improved spectral fidelity under motion.


<details>
  <summary>Details</summary>
Motivation: Existing snapshot hyperspectral imaging systems have limitations: they divide light across spectral channels simultaneously, assume no motion during exposure, and suffer from poor photon utilization and spectral fidelity degradation in dynamic scenes.

Method: Combines narrowband LED array with coded-exposure-pixel (CEP) camera for per-pixel exposure control, enabling joint encoding of scene information across space, time, and wavelength. Uses active synchronization of illumination and pixel-wise exposure, followed by learning-based reconstruction pipeline.

Result: Recovers 31-channel hyperspectral (400-700 nm) video at 30 fps and VGA resolution with temporally coherent and spectrally accurate reconstructions. Significantly improves reconstruction fidelity and temporal stability over existing snapshot systems across diverse materials and motion conditions.

Conclusion: Lumosaic enables robust hyperspectral video capture for dynamic scenes by addressing motion-related limitations of passive snapshot systems through active illumination-exposure synchronization and improved photon utilization.

Abstract: We present Lumosaic, a compact active hyperspectral video system designed for real-time capture of dynamic scenes. Our approach combines a narrowband LED array with a coded-exposure-pixel (CEP) camera capable of high-speed, per-pixel exposure control, enabling joint encoding of scene information across space, time, and wavelength within each video frame. Unlike passive snapshot systems that divide light across multiple spectral channels simultaneously and assume no motion during a frame's exposure, Lumosaic actively synchronizes illumination and pixel-wise exposure, improving photon utilization and preserving spectral fidelity under motion. A learning-based reconstruction pipeline then recovers 31-channel hyperspectral (400-700 nm) video at 30 fps and VGA resolution, producing temporally coherent and spectrally accurate reconstructions. Experiments on synthetic and real data demonstrate that Lumosaic significantly improves reconstruction fidelity and temporal stability over existing snapshot hyperspectral imaging systems, enabling robust hyperspectral video across diverse materials and motion conditions.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [240] [Semi-Gridless Variational Bayes Channel Estimation in XL-MIMO: Near-Field Modeling and Inference](https://arxiv.org/abs/2602.21449)
*Van-Chung Luu,Toan-Van Nguyen,Nuria González-Prelcic,Duy H. N. Nguyen*

Main category: eess.SP

TL;DR: Proposes SG-VB algorithm for near-field channel estimation in 6G systems using variational Bayesian inference with semi-gridless approach for DoA and distance estimation.


<details>
  <summary>Details</summary>
Motivation: 6G communications use extremely large antenna arrays and high-frequency operation, shifting systems to near-field region where spherical wavefronts require new channel estimation algorithms to exploit unique features for advanced transceiver design.

Method: Reformulates near-field channel model for ULAs and UPAs into separate DoA and distance components, uses gridless approach with von Mises distribution for DoA estimation, coarse-to-fine grid search for distance estimation, and develops SG-VB algorithm with efficient update rules.

Result: Simulation results validate SG-VB algorithm effectiveness, showing enhanced near-field channel reconstruction accuracy and superior estimation performance for both DoAs and distance components in near-field channels.

Conclusion: Proposed SG-VB algorithm successfully addresses near-field channel estimation challenges in 6G systems, enabling accurate channel reconstruction by exploiting spherical wavefront characteristics through semi-gridless variational Bayesian inference framework.

Abstract: Extremely large antenna arrays and high-frequency operation are two key technologies that advance performance metrics such as higher data rates, lower latency, and wider coverage in sixth-generation communications. However, the adoption of these technologies fundamentally changes the characteristics of wavefronts, forcing communication systems to operate in the near-field region. The transition from planar far-field communications to spherical near-field propagation necessitates novel channel estimation algorithms to fully exploit the unique features of spherical wavefronts for advanced transceiver design. To this end, we propose a novel semi-gridless channel estimation approach based on a variational Bayesian (VB) inference framework. Specifically, we reformulate the near-field channel model for both uniform linear arrays and uniform planar arrays into separate direction-of-arrival (DoAs) and distance components. Building on these new representations, we employ a gridless approach for DoAs estimation using a von Mises distribution, and a coarse-to-fine grid search for distance estimation. We then develop a semi-gridless variational Bayesian (SG-VB) algorithm with efficient update rules that enables accurate channel reconstruction. Simulation results validate the effectiveness of the proposed SG-VB algorithm, demonstrating enhanced near-field channel reconstruction accuracy and superior estimation performance for both DoAs and distance components embedded in near-field channels.

</details>


### [241] [Delay-Synchronous Wideband Channel Sounding Using Off-The-Shelf Multi-Antenna WiFi Devices](https://arxiv.org/abs/2602.21573)
*Koji Yamamoto,Katsuyuki Haneda*

Main category: eess.SP

TL;DR: SoundiFi enables delay-synchronous channel sounding using off-the-shelf WiFi devices by implementing stable synchronization and power reference through coaxial cables and a dedicated reference antenna.


<details>
  <summary>Details</summary>
Motivation: While WiFi devices can sense environments through channel state information, they cannot perform delay-synchronous channel sounding due to synchronization instability and lack of power reference, preventing continuous impulse response measurements across acquisitions.

Method: The SoundiFi system uses off-the-shelf multiple-antenna IEEE 802.11ax WiFi devices with coaxial cables to remote antennas. One antenna serves as a reference channel to define gain and delay for other simultaneous channels, enabling stable delay synchronization and power level reference.

Result: Indoor experiments show continuous impulse responses across successive acquisitions with absolute delay measurement. The system achieves -115 dB noise level and detects long-delayed multipaths up to 132 m propagation distance in a 30-m-long corridor.

Conclusion: SoundiFi successfully addresses WiFi's limitations for delay-synchronous channel sounding, enabling continuous impulse response measurements with stable synchronization and power reference, revealing previously undetectable multipath characteristics in indoor environments.

Abstract: It has been shown that WiFi devices enable sensing of environments and targets through their channel state information. However, the same devices have not been used for delay-synchronous channel sounding due to challenges related to the stability of synchronization and lack of reference power levels. Due to factors such as uncertainty in symbol reception timing, impulse responses are discontinuous across acquisitions. The present paper addresses the challenges to perform delay-synchronous channel sounding using off-the-shelf multiple-antenna IEEE 802.11ax WiFi devices, referred to as SoundiFi. Stable delay synchronization and power level reference are realized by remoting the antennas with coaxial cables and devoting one of the antennas as a reference channel, with which the gain and delay of other simultaneous channels are defined. Indoor experiments confirmed that the impulse response becomes continuous across successive acquisitions and provide the absolute delay. The impulse response has a noise level at -115 dB, indicating the maximum path gain value that can be measured with the devices. The impulse response also revealed the existence of long-delayed multipaths up to 132 m propagation distance in a reverberant 30-m-long corridor.

</details>


### [242] [Pinching Antennas for Multiple Access in Multigroup Multicast Communications](https://arxiv.org/abs/2602.21614)
*Shan Shan,Chongjun Ouyang,Yong Li,Yuanwei Liu*

Main category: eess.SP

TL;DR: The paper designs multiple access schemes (TIN, NOMA, TDMA) for pinching antenna-based multigroup multicast communications, jointly optimizing PA placement and resource allocation to improve max-min fairness.


<details>
  <summary>Details</summary>
Motivation: To enhance max-min fairness in multigroup multicast communications using pinching antennas by developing efficient MA schemes that jointly optimize antenna placement and resource allocation, overcoming limitations of traditional fixed-antenna systems.

Method: Three MA schemes: 1) TIN with closed-form power allocation and SEO for PA placement; 2) NOMA with recursive power allocation using bisection search and HOE-enhanced SEO; 3) TDMA with PS (separate PA optimization) and PM (joint optimization via KKT conditions).

Result: PASS architecture significantly outperforms traditional fixed-antenna systems; TDMA-PS achieves best performance by leveraging flexible PA reconfiguration; NOMA outperforms TDMA-PM and can surpass TDMA-PS in high-power regimes with heterogeneous group distributions; TIN serves as practical lower-bound.

Conclusion: The proposed MA schemes effectively improve MMF for PA-based multicast communications, with TDMA-PS offering superior performance through interference-free transmission and flexible PA reconfiguration, while NOMA provides competitive performance especially in heterogeneous scenarios.

Abstract: This paper aims to design multiple access (MA) schemes to improve the max-min fairness (MMF) for pinching antennas (PAs)-based multigroup multicast communications, where PA placement and resource allocation are jointly optimized. Specifically, three MA schemes are considered to facilitate the multicast transmission: i) treating interference as noise (TIN), ii) non-orthogonal multiple access (NOMA), and iii) time-division multiple access (TDMA) with two PA reconfiguration protocols, namely pinching switching (PS) and pinching multiplexing (PM). i) For TIN, a closed-form solution is derived for optimal power allocation, while a sequential element-wise optimization (SEO) is developed for the PA placement. ii) For NOMA, a recursive power allocation framework incorporating a bisection search is developed, and a hierarchical objective evaluation (HOE) mechanism is incorporated to simplify the SEO process for PA location update. iii) For TDMA, the PS protocol allows the PA locations to be optimized separately using the SEO method, after which the time-power allocation is solved as a convex problem with a global optimum. Under the PM protocol, the PA locations are jointly optimized with the time-power resources through a Karush-Kuhn-Tucker (KKT)-based analytical solution. Numerical results demonstrate that: i) the pinching-antenna system (PASS) architecture significantly outperforms traditional fixed-antenna systems. ii) TDMA-PS achieves superior performance by fully leveraging the flexible PA reconfiguration and benefiting from interference-free transmission, whereas TIN serves as a practical lower-bound solution due to its simplicity despite its limited performance. iii) NOMA consistently outperforms TDMA-PM and, in high transmit power regimes with heterogeneous multicast group distributions, can even surpass the performance achieved by TDMA-PS.

</details>


### [243] [Score-Based Conditional Flow Models for MIMO Receiver Design with Superimposed Pilots](https://arxiv.org/abs/2602.21654)
*Ruhao Zhang,Yupeng Li,Yitong Liu,Shijian Gao,Jing Jin,Hongwen Yang,Jiangzhou Wang*

Main category: eess.SP

TL;DR: CFM-Rx is an unsupervised generative receiver for MIMO systems that uses flow matching to jointly estimate channels and detect data without labeled training, outperforming conventional methods especially under pilot contamination.


<details>
  <summary>Details</summary>
Motivation: Superimposed pilots (SIP) reduce overhead but cause severe pilot contamination and data interference, making joint channel estimation and data detection challenging. Existing methods require labeled data and lack adaptability across diverse system settings.

Method: Proposes conditional flow matching receiver (CFM-Rx), an unsupervised generative framework that learns directly from received signals without labeled data. Uses flow-based generative modeling for deterministic inference, exploits model invertibility to capture bidirectional signal propagation, unifies flow matching with score-based diffusion via moment-consistent ODE, and integrates receiver-side priors for stable inference.

Result: Extensive simulations across various MIMO configurations show CFM-Rx consistently outperforms conventional estimators and state-of-the-art data-driven receivers, achieving significant gains in channel estimation accuracy and symbol detection robustness, particularly under severe pilot contamination.

Conclusion: CFM-Rx provides an effective unsupervised solution for joint channel estimation and data detection in MIMO systems with superimposed pilots, offering improved performance, adaptability, and robustness to pilot contamination without requiring labeled training data.

Abstract: Accurate channel state information (CSI) is vital for multiple-input multiple-output (MIMO) systems. However, superimposed pilots (SIP), which reduce overhead, introduce severe pilot contamination and data interference, complicating joint channel estimation and data detection. This paper proposes a conditional flow matching receiver (CFM-Rx), an unsupervised generative framework that learns directly from received signals, eliminating the need for labeled data and improving adaptability across diverse system settings. By leveraging flow-based generative modeling, CFM-Rx enables deterministic, low-latency inference and exploits model invertibility to capture the bidirectional nature of signal propagation. This framework unifies flow matching with score-based diffusion modeling via a moment-consistent ordinary differential equation (ODE), replacing stochastic differential equation (SDE) sampling with a deterministic and efficient process. Furthermore, it integrates receiver-side priors to ensure stable, data-consistent inference. Extensive simulation results across various MIMO configurations demonstrate that CFM-Rx consistently outperforms conventional estimators and state-of-the-art data-driven receivers, achieving notable gains in channel estimation accuracy and symbol detection robustness, particularly under severe pilot contamination.

</details>


### [244] [Deep Learning-based Low-Overhead Beam Alignment for mmWave Massive MIMO Systems](https://arxiv.org/abs/2602.21664)
*Weijie Jin,Jing Zhang,Hengtao He,Chao-Kai Wen,Xiao Li,Shi Jin*

Main category: eess.SP

TL;DR: Deep learning-enhanced super-resolution beam alignment framework for mmWave massive MIMO that improves angular resolution without increasing training overhead, while being robust to hardware impairments.


<details>
  <summary>Details</summary>
Motivation: Conventional codebook-based beam alignment methods suffer from limited angular resolution and sensitivity to hardware impairments, which critically affects mmWave massive MIMO system performance that relies on highly directional beamforming to overcome severe path loss.

Method: Three-component framework: 1) QSSR algorithm leveraging monotonic power ratio property between DFT codebook beams for super-resolution angle estimation without extra measurements; 2) QSSR-Net (GRU-based neural network) using sequential multi-layer beam measurements to capture angular dependencies; 3) Parametric self-calibration method for real-time compensation of hardware impairments without additional hardware.

Result: The framework outperforms binary search and even exhaustive search at high SNR, achieving substantial performance gains while maintaining low overhead. It shows improved estimation accuracy, robustness to noise, and generalization across diverse propagation environments.

Conclusion: The proposed deep learning-enhanced super-resolution beam alignment framework effectively addresses limitations of conventional methods by providing high-resolution angle estimation without increased measurement complexity, while being robust to hardware impairments and adaptable to various environments.

Abstract: Millimeter-wave massive multiple-input multiple-output systems employ highly directional beamforming to overcome severe path loss, and their performance critically depends on accurate beam alignment. Conventional codebook-based methods offer low training overhead but suffer from limited angular resolution and sensitivity to hardware impairments. To address these challenges, we propose a deep learning-enhanced super-resolution beam alignment framework with three key components. First, we design the Quaternary Search-based Super-Resolution (QSSR) algorithm, which leverages the monotonic power ratio property between two discrete Fourier transform (DFT) codebook beams to achieve super-resolution angle estimation without increasing measurement complexity relative to binary search. Second, we develop QSSR-Net, a gated recurrent unit-based neural network that exploits sequential multi-layer beam measurements to capture angular dependencies, thereby improving estimation accuracy, robustness to noise, and generalization across diverse propagation environments. Third, to mitigate the adverse effects of hardware impairments such as antenna position and phase errors, we propose a parametric self-calibration method that requires no additional hardware overhead and adapts compensation parameters in real time. Simulation results show that the proposed framework consistently outperforms binary search and even exhaustive search at high signal-to-noise ratios, achieving substantial performance gains while maintaining low overhead.

</details>


### [245] [Dual-Hop Joint Visible Light and Backscatter Communication Relaying under Finite Blocklength](https://arxiv.org/abs/2602.21744)
*Boxuan Xie,Lauri Mela,Alexis A. Dowhuszko,Jiacheng Wang,Kalle Ruttik,Riku Jäntti*

Main category: eess.SP

TL;DR: Dual-hop VLC-backscatter relaying system for energy-neutral IoT under finite blocklength constraints, enabling ambient-powered IoT without carrier synthesizers or power amplifiers.


<details>
  <summary>Details</summary>
Motivation: Enable energy-neutral Ambient IoT deployments by combining VLC illumination with backscatter communication, eliminating need for carrier synthesizers and power amplifiers at IoT nodes.

Method: Dual-hop framework where LED access points transmit to backscatter device, which harvests optical energy and backscatters messages to RF-equipped UEs. Modeled with short-packet IoT traffic and realistic VLC interference.

Result: BD placement/orientation and code rate significantly affect system reliability and data rate. System enables ambient power-enabled IoT solutions without traditional RF hardware.

Conclusion: Proposed hybrid VLC-backscatter framework provides viable path for energy-neutral IoT deployments and informs future hybrid VLC/RF network designs.

Abstract: This paper investigates a dual-hop joint visible light communication (VLC) and backscatter communication (BC) relaying framework under the finite blocklength (FBL) constraint, aiming at energy-neutral Ambient Internet of Things (A-IoT) deployments. In the proposed system, indoor LED access points are used to simultaneously provide illumination and transmit information over light to a backscatter device (BD), which harvests optical energy and backscatters the received messages to user equipments (UEs) equipped with radio frequency (RF) front ends. This forwarding of the information from VLC to RF channels is implemented without the need for carrier synthesizers and power amplifiers at the IoT node. By modeling the end-to-end communication link with short-packet IoT traffic and realistic levels of interference between adjacent VLC coverage areas, we analyze the outage performance and achievable data rate of the proposed system. Simulation results demonstrate that key factors, such as placement and orientation of the BD, as well as the selected code rate of the system affect reliability and data rate that can be achieved for communication purposes. The insights gained from this study pave the way for ambient power-enabled IoT solutions and future hybrid VLC/RF network designs.

</details>


### [246] [Availability of Aerial Heterogeneous Networks for Reliable Emergency Communications](https://arxiv.org/abs/2602.21793)
*Teng Wu,Jiandong Li,Junyu Liu,Min Sheng,Mohammadali Mohammadi,Hien Quoc Ngo,Michail Matthaiou*

Main category: eess.SP

TL;DR: The paper analyzes network availability in aerial heterogeneous networks for emergency rescue, deriving lower bounds and proposing joint optimization of UE sharing and pilot length to handle diverse delay constraints and mobility.


<details>
  <summary>Details</summary>
Motivation: Emergency rescue operations require reliable aerial heterogeneous networks that can provide diverse delay-constrained communication services to users with varying mobility. The heterogeneity in delay constraints and UE mobility creates resource allocation conflicts that undermine communication reliability and network availability.

Method: The authors derive expressions for the lower bound on network availability under unified resource allocation. They then formulate and solve a joint optimization problem for the number of UEs sharing time-frequency resources (K) and pilot length (ξ) to enhance the lower bound by improving spatial, frequency, and temporal resource efficiency.

Result: Analysis shows extended heterogeneity significantly degrades the lower bound due to resource limitations, even when heterogeneity comes from additional services with less stringent delay constraints or from UEs with lower mobility. Joint optimization of K and ξ enables aerial heterogeneous networks to achieve target network availability under greater heterogeneity, outperforming existing resource allocation policies.

Conclusion: The proposed joint optimization of UE sharing and pilot length effectively enhances network availability in aerial heterogeneous networks for emergency rescue by improving resource efficiency, allowing the system to handle greater heterogeneity in delay constraints and UE mobility while maintaining reliable communication.

Abstract: We investigate network availability (NA) in aerial heterogeneous networks (AHetNets) for effective emergency rescue, where diverse delay-constrained communication services must be provided to user equipments (UEs) with varying mobility. The heterogeneity in delay constraints and UE mobility introduces resource allocation conflicts and imbalances, which undermine communication reliability and challenge NA. Although unified resource allocation (URA) can mitigate these issues, it remains unclear whether NA can be sustained under such diverse conditions. To address this, we derive expressions for the lower bound (LB) on NA in AHetNets under URA. Our analysis reveals that extended heterogeneity significantly degrades the LB due to resource limitations-even when the heterogeneity stems from additional services under less stringent delay constraints (LSDC) or from UEs with lower mobility. To overcome this degradation, we formulate and solve a joint optimization problem for the number of UEs sharing time-frequency resources ($K$) and pilot length ($ξ$), aiming to enhance the LB by improving spatial, frequency, and temporal resource efficiency. Simulation results validate our analysis and demonstrate that jointly optimizing $K$ and $ξ$ enables AHetNets to achieve the target NA under greater heterogeneity, outperforming existing resource allocation policies.

</details>


### [247] [Cross-Pilot Superposition for Fractional Parameter Estimation in DoA-Aided OTFS Receivers](https://arxiv.org/abs/2602.21801)
*Mauro Marchese,Pietro Savazzi*

Main category: eess.SP

TL;DR: Novel superimposed pilot scheme for OTFS receivers using angular domain separation and integrated delay/Doppler profile computation to reduce interference and estimate fractional parameters.


<details>
  <summary>Details</summary>
Motivation: Need improved channel estimation for multi-antenna OTFS systems to address data-to-pilot interference and accurately estimate fractional delay/Doppler parameters in multipath environments.

Method: Proposes superimposed pilot scheme leveraging large ULA at receiver to separate multipath components in angular domain, computes integrated delay/Doppler profiles by averaging across axes, and estimates fractional parameters through correlation with delay/Doppler terms.

Result: Outperforms existing OTFS superimposed pilot schemes with lower BER, demonstrates trade-off between PAPR and communication performance through simulations.

Conclusion: The proposed scheme effectively reduces data-to-pilot interference and enables accurate estimation of fractional delay/Doppler parameters, offering improved performance for OTFS systems with trade-offs in PAPR.

Abstract: In this letter, a novel superimposed pilot scheme is proposed for channel estimation in multi-antenna orthogonal time frequency space (OTFS) receivers. Under the assumption of a large uniform linear array (ULA) size at the receiver, the multipath components are separated directly in the angular domain. It is then shown that the proposed superimposed pilot scheme enables the computation of integrated delay and Doppler profiles by averaging the received delay-Doppler matrix across the Doppler and delay axes, respectively. This procedure helps reduce data-to-pilot interference through data averaging. Moreover, it is demonstrated that fractional delays and Dopplers of the multipath components can be estimated by correlating the integrated delay and Doppler profiles with the corresponding delay/Doppler terms. Simulation results show that the proposed approach outperforms existing OTFS superimposed pilot schemes, achieving a lower bit error rate (BER) while exhibiting a trade-off between peak-to-average power ratio (PAPR) and communication performance.

</details>


### [248] [Leaky Coaxial Cable based Generalized Pinching-Antenna Systems with Dual-Port Feeding](https://arxiv.org/abs/2602.21856)
*Kaidi Wang,Zhiguo Ding,Daniel K. C. So*

Main category: eess.SP

TL;DR: Generalized pinching-antenna system using leaky coaxial cables with dual-port feeding for enhanced spatial degrees of freedom and improved wireless performance.


<details>
  <summary>Details</summary>
Motivation: To extend pinching antenna concepts from high-frequency waveguide architectures to lower-frequency cable-based systems using leaky coaxial cables, enabling more flexible and capable wireless communication systems.

Method: Developed LCX-based generalized pinching-antenna system with dual-port feeding, created comprehensive channel model for intra-cable attenuation, bidirectional phase progression, slot radiation, and wireless propagation. Studied analog and hybrid beamforming frameworks with matching theory, coalitional games, bisection-based power control for analog, and ZF digital precoding for hybrid transmission.

Result: Dual-port feeding provides significant performance gains over single-port LCX systems and fixed-antenna benchmarks, with effective beamforming and resource allocation designs validated under various transmit power levels and cable parameters.

Conclusion: The proposed LCX-based generalized pinching-antenna system with dual-port feeding successfully extends pinching antenna concepts to cable-based structures, offering enhanced spatial degrees of freedom and improved wireless communication performance through optimized beamforming and resource allocation strategies.

Abstract: By leveraging the distributed leakage radiation of leaky coaxial cables (LCXs), the concept of pinching antennas can be generalized from the conventional high-frequency waveguide based architectures to cable based structures in lower-frequency scenarios. This paper investigates an LCX based generalized pinching-antenna system with dual-port feeding. By enabling bidirectional excitation along each cable, the proposed design significantly enhances spatial degrees of freedom. A comprehensive channel model is developed to characterize intra-cable attenuation, bidirectional phase progression, slot based radiation, and wireless propagation. Based on this model, both analog and hybrid beamforming frameworks are studied with the objective of maximizing the minimum achievable data rate. For analog transmission, slot activation, port selection, and power allocation are jointly optimized using matching theory, coalitional games, and bisection based power control. For hybrid transmission, zero-forcing (ZF) digital precoding is incorporated to eliminate inter-user interference, thereby simplifying slot activation and enabling closed-form optimal power allocation. Simulation results demonstrate that dual-port feeding provides notable performance gains over single-port LCX systems and fixed-antenna benchmarks, validating the effectiveness of the proposed beamforming and resource allocation designs under various transmit power levels and cable parameters.

</details>


### [249] [Modeling of Human Body-coupled Electric Field Interference in Unshielded Ultra-Low Field MRI](https://arxiv.org/abs/2602.21909)
*Jiali He,Yamei Dai,Sheng Shen,Jiamin Wu,Zheng Xu*

Main category: eess.SP

TL;DR: Researchers developed a circuit model showing how ambient electric fields induce body common-mode potential that converts to differential-mode noise in portable ultra-low field MRI, and implemented a capacitive bypass solution that improved SNR 3.5×.


<details>
  <summary>Details</summary>
Motivation: Portable ultra-low field MRI systems in unshielded environments suffer from electromagnetic interference, and subject presence significantly increases noise, but the dominant coupling mechanism isn't well understood.

Method: Developed a lumped-parameter circuit model of the coupled environment-body-receiver system, validated through circuit analysis, simulations, and controlled experiments, then implemented a capacitive low-impedance bypass to clamp body potential.

Result: The model correctly predicted noise variations, and the capacitive bypass solution achieved approximately 3.5-fold SNR improvement on a 50 mT prototype MRI system.

Conclusion: The proposed circuit model provides a compact tool for analyzing and mitigating human body-coupled electric-field interference in portable ULF-MRI systems.

Abstract: Portable ultra-low field MRI (ULF-MRI) systems operated in unshielded environments are susceptible to electromagnetic interference (EMI). Subject presence in the imaging region will lead to substantial noise increases, yet the dominant coupling mechanism remains insufficiently characterized. We develop a lumped-parameter circuit model of the coupled environment-body-receiver system. The model indicates that ambient time-varying electric fields induce a body common-mode potential, which is converted into differential-mode noise through capacitive imbalance between the head and the receive-coil terminals, yielding strong dependence on subject position and geometry. Circuit analysis, simulations, and controlled experiments support the model, with predicted imbalance consistent with measured noise variations. Guided by this mechanism, we implement a capacitive low-impedance bypass to clamp the body potential, achieving an approximately 3.5-fold SNR improvement on a 50 mT prototype. The proposed model offers a compact circuit-based tool for analyzing and mitigating human body-coupled electric-field interference in portable ULF-MRI.

</details>


### [250] [A sliding-window approach for latent restoring force modeling](https://arxiv.org/abs/2602.21918)
*Merijn Floren,Jan Swevers*

Main category: eess.SP

TL;DR: Proposes a nonlinear system identification framework using sliding-window feedback to reconstruct latent states from partial measurements, enabling nonparametric RFS identification without full sensing requirements.


<details>
  <summary>Details</summary>
Motivation: Traditional Restoring Force Surface (RFS) methods require complete kinematic measurements at each degree of freedom, limiting their scalability to multidimensional systems. The paper aims to overcome these measurement limitations.

Method: Uses periodic multisine excitation with a sliding-window feedback approach to reconstruct latent states and nonlinear restoring forces nonparametrically. Starts from an initial linear model and enables identification through linear-in-parameters regression instead of non-convex optimization.

Result: Validation on synthetic and experimental datasets demonstrates high simulation accuracy and reliable recovery of physical parameters under partial sensing and noisy conditions.

Conclusion: The proposed framework successfully overcomes measurement limitations of traditional RFS methods, enabling nonlinear system identification with relaxed sensing requirements while maintaining accuracy and robustness to noise.

Abstract: Restoring force surface (RFS) methods offer an attractive nonparametric framework for identifying nonlinear restoring forces directly from data, but their reliance on complete kinematic measurements at each degree of freedom limits scalability to multidimensional systems. The aim of this paper is to overcome these measurement limitations by proposing an identification framework with relaxed sensing requirements that exploits periodic multisine excitation. Starting from an initial linear model, a sliding-window feedback approach reconstructs latent states and nonlinear restoring forces nonparametrically, enabling identification of the nonlinear component through linear-in-parameters regression instead of highly non-convex optimization. Validation on synthetic and experimental datasets demonstrates high simulation accuracy and reliable recovery of physical parameters under partial sensing and noisy conditions.

</details>


### [251] [Analyzing URA Geometry for Enhanced Near-Field Beamfocusing and Spatial Degrees of Freedom](https://arxiv.org/abs/2602.21927)
*Ahmed Hussain,Asmaa Abdallah,Abdulkadir Celik,Emil Björnson,Ahmed M. Eltawil*

Main category: eess.SP

TL;DR: The paper analyzes near-field beamfocusing in large antenna arrays, introducing beamdepth and effective beamfocusing Rayleigh distance (EBRD) concepts, showing array geometry impacts near-field performance, and proposing a polar codebook for channel estimation.


<details>
  <summary>Details</summary>
Motivation: Future wireless systems with large antenna arrays at high frequencies will operate in radiative near-field, enabling spatial multiplexing in range dimension. Understanding how array geometry affects near-field beamfocusing and spatial degrees of freedom is crucial for system design.

Method: Derived beamdepth for generalized uniform rectangular arrays (URAs), defined effective beamfocusing Rayleigh distance (EBRD) as near-field boundary metric, analyzed array geometry impact under element count vs. aperture length constraints, designed polar codebook for compressed-sensing channel estimation, and derived analytical expression for effective spatial DoF.

Result: Array geometry strongly impacts beamdepth under fixed element count but less under fixed aperture length. Elongated arrays (ULAs) yield narrower beamdepth and extend near-field region. Polar codebook achieves 2 dB NMSE improvement over state-of-the-art. ULAs achieve maximum spatial DoF, outperforming square URAs.

Conclusion: Array geometry significantly influences near-field beamfocusing performance and spatial multiplexing capabilities. Elongated arrays provide advantages in near-field operation, and the proposed EBRD metric effectively characterizes near-field boundaries for system design and channel estimation.

Abstract: With the deployment of large antenna arrays at high-frequency bands, future wireless communication systems are likely to operate in the radiative near-field. Unlike far-field beam steering, near-field beams can be focused on a spatial region with a finite depth, enabling spatial multiplexing in the range dimension. Moreover, in the line-of-sight MIMO near-field, multiple spatial degrees of freedom (DoF) are accessible, akin to a scattering- rich environment. In this paper, we derive the beamdepth for a generalized uniform rectangular array (URA) and investigate how the array geometry influences near-field beamdepth and its limits. We define the effective beamfocusing Rayleigh distance (EBRD), to present a near-field boundary with respect to beamfocusing and spatial multiplexing gains for the generalized URA. Our results demonstrate that under a fixed element count constraint, the array geometry has a strong impact on beamdepth, whereas this effect diminishes under a fixed aperture length constraint. Moreover, compared to uniform square arrays, elongated configurations such as uniform linear arrays (ULAs) yield narrower beamdepth and extend the effective near-field region defined by the EBRD. Building on these insights, we design a polar codebook for compressed-sensing-based channel estimation that leverages our findings. Simulation results show that the proposed polar codebook achieves a 2 dB NMSE improvement over state-of-the-art methods. Additionally, we present an analytical expression to quantify the effective spatial DoF in the near-field, revealing that they are also constrained by the EBRD. Notably, the maximum spatial DoF is achieved with a ULA configuration, outperforming a square URA in this regard.

</details>


### [252] [Spatial Degrees of Freedom in Near Field MIMO: Experimental Validation of Beamspace Perspective](https://arxiv.org/abs/2602.21945)
*Ahmed Hussain,Asmaa Abdallah,Ahmed Nasser,Abdulkadir Celik,Ahmed M. Eltawil*

Main category: eess.SP

TL;DR: The paper develops a framework to characterize effective degrees of freedom (EDoF) in near-field MIMO systems, introduces distance metrics for transition points, and validates theory with experiments.


<details>
  <summary>Details</summary>
Motivation: Conventional far-field MIMO channels have only single spatial DoF under line-of-sight conditions, while near-field supports multiple DoF due to spherical wavefronts. However, experimental validation and clear identification of transition distances for EDoF in near-field remain limited.

Method: Developed an intuitive framework for characterizing EDoF of ULA-based MIMO systems with two analytical expressions: 1) closed-form formulation relating EDoF to physical transmit beamwidth and receive aperture, and 2) discrete formulation based on DFT domain angular decomposition of near-field spherical wavefront for experimental evaluation. Introduced two distance metrics: effective MIMO Rayleigh distance (EMRD) and maximum spatial multiplexing distance (MSMD).

Result: Experimental measurements using widely spaced phased arrays closely match theoretical EDoF trends and validate the proposed distance metrics (EMRD and MSMD). The framework successfully characterizes when EDoF reduces to one (EMRD) and when it attains maximum (MSMD).

Conclusion: The proposed framework provides both theoretical and practical tools for understanding near-field MIMO capabilities, with validated distance metrics that mark important transition points for spatial multiplexing in radiative near-field communications.

Abstract: Conventional far-field multiple-input multiple-output (MIMO) channels are limited to a single spatial degree of freedom (DoF) under a line-of-sight (LoS) condition. In contrast, the radiative near field (NF) supports multiple spatial DoF, enabled by spherical wavefronts and the reduced spatial footprint at short ranges. While recent research indicates that the effective DoF (EDoF) increases in NF, experimental validation and clear identification of the transition distances remain limited. In this letter, we develop an intuitive framework for characterizing the EDoF of a ULA-based MIMO system and derive two complementary analytical expressions: a closed-form formulation that relates the EDoF to the physical transmit beamwidth and receive aperture, and a discrete formulation based on the discrete Fourier transform (DFT) domain angular decomposition of the NF spherical wavefront, which is well suited for experimental evaluation. We further introduce the effective MIMO Rayleigh distance (EMRD) and the maximum spatial multiplexing distance (MSMD), which mark the distances where the EDoF reduces to one and attains its maximum, respectively. Experimental measurements using widely spaced phased arrays closely match the theoretical EDoF trends and validate the proposed distance metrics.

</details>


### [253] [Sparse Array Design for Near-Field MU-MIMO: Reconfigurable Array Thinning Approach](https://arxiv.org/abs/2602.21973)
*Ahmed Hussain,Asmaa Abdallah,Abdulkadir Celik,Emil Björnson,Ahmed M. Eltawil*

Main category: eess.SP

TL;DR: Proposes reconfigurable array thinning for near-field wireless networks using selective antenna activation to form flexible sparse arrays without mechanical repositioning, with two optimization strategies for grating-lobe suppression and sum-rate maximization.


<details>
  <summary>Details</summary>
Motivation: Future massive MIMO networks will operate in radiative near-field, enabling spatial multiplexing across angle and range. Fixed sparse arrays are suboptimal for dynamic user distributions, while movable antennas introduce latency and hardware complexity. Need a flexible solution without mechanical reconfiguration.

Method: Analyzes grating lobes for uniform sparse arrays in angle/range domains, showing absence along range dimension. Develops two particle swarm optimization strategies: 1) Grating-lobe-based thinned array (GTA) for grating-lobe suppression, and 2) Sum-rate-based thinned array (STA) for multiuser sum-rate maximization.

Result: GTA outperforms conventional uniform sparse arrays, while STA achieves performance comparable to movable antennas without mechanical complexity. Both strategies offer practical and efficient array deployment.

Conclusion: Reconfigurable array thinning provides a flexible, efficient solution for near-field wireless networks, achieving performance similar to movable antennas without mechanical complexity, making it suitable for dynamic user environments.

Abstract: Future wireless networks, deploying thousands of antenna elements, may operate in the radiative near-field (NF), enabling spatial multiplexing across both angle and range domains. Sparse arrays have the potential to achieve comparable performance with fewer antenna elements. However, fixed sparse array designs are generally suboptimal under dynamic user distributions, while movable antenna architectures rely on mechanically reconfigurable elements, introducing latency and increased hardware complexity. To address these limitations, we propose a reconfigurable array thinning approach that selectively activates a subset of antennas to form a flexible sparse array design without physical repositioning. We first analyze grating lobes for uniform sparse arrays in the angle and range domains, showing their absence along the range dimension. Based on the analysis, we develop two particle swarm optimization-based strategies: a grating-lobe-based thinned array (GTA) for grating- lobe suppression and a sum-rate-based thinned array (STA) for multiuser sum-rate maximization. Simulation results demonstrate that GTA outperforms conventional uniform sparse arrays, while STA achieves performance comparable to movable antennas, thereby offering a practical and efficient array deployment strategy without the associated mechanical complexity.

</details>


### [254] [Transmission Delay Minimization for NOMA-Based F-RANs](https://arxiv.org/abs/2602.22087)
*Yuan Ai,Xidong Mu,Pengbo Si,Yuanwei Liu*

Main category: eess.SP

TL;DR: A NOMA-based low-delay framework for F-RANs with joint optimization of user association, cache placement, and power allocation using alternating optimization.


<details>
  <summary>Details</summary>
Motivation: To reduce transmission delay in fog radio access networks by leveraging NOMA for efficient content delivery and caching, addressing the limitations of orthogonal multiple access approaches.

Method: Proposed a NOMA-based F-RAN framework where FAPs use NOMA for local cached content delivery and cloud AP uses NOMA for content pushing. Developed an alternating optimization algorithm that decomposes the delay minimization problem into two subproblems: joint user association/cache placement (solved using McCormick envelope theory and Lagrangian partial relaxation) and power allocation (solved using successive convex approximation).

Result: Simulation results show: 1) The AO-based algorithm effectively balances performance and computational efficiency, 2) The NOMA-based F-RAN framework significantly outperforms OMA-based F-RAN systems in terms of average transmission delay across different scenarios.

Conclusion: The proposed NOMA-based framework with joint optimization of user association, cache placement, and power allocation provides an effective solution for low-delay service delivery in fog radio access networks, demonstrating superior performance over traditional OMA approaches.

Abstract: A novel non-orthogonal multiple access (NOMA) based low-delay service framework is proposed for fog radio access networks (F-RANs). Fog access points (FAPs) leverage NOMA for local delivery of cached content, while the cloud access point employs NOMA to simultaneously push content to FAPs and directly serve users. Based on this model, a delay minimization problem is formulated by jointly optimizing user association, cache placement, and power allocation. To address this non-convex mixed-integer nonlinear programming problem, an alternating optimization (AO) algorithm is developed, which decomposes the original problem into two subproblems, namely joint user association and cache placement, and power allocation. In particular, a low-complexity algorithm is designed to optimizing the user association and cache placement strategy using the McCormick envelope theory and Lagrangian partial relaxation. The power allocation is optimized by invoking the successive convex approximation. Simulation results reveal that: 1) the proposed AO-based algorithm effectively balances between the achieved performance and computational efficiency, and 2) the proposed NOMA-based F-RANs framework significantly outperforms orthogonal multiple access-based F-RANs systems in terms of average transmission delay in different scenarios.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [255] [The constructions of Singleton-optimal locally repairable codes with minimum distance 6 and locality 3](https://arxiv.org/abs/2602.21494)
*Yanzhen Xiong,Jianbing Lu*

Main category: cs.IT

TL;DR: Constructs q-ary Singleton-optimal LRCs with d=6, r=3 using combinatorial structures from finite geometry and MOLS.


<details>
  <summary>Details</summary>
Motivation: To develop new constructions of locally repairable codes (LRCs) that achieve Singleton-optimal performance with specific parameters (d=6, r=3) using combinatorial approaches from finite geometry.

Method: Exploits correspondence between complete sets of MOLS and affine planes to systematically construct families of disjoint 4-arcs in projective planes. These 4-arcs (called 4-local arcs) have the property that union of any two distinct ones forms an 8-arc, which is equivalent to existence of desired codes.

Result: For any prime power q≥7, construction yields codes of length n = 2q (if q even), 2q-2 (if q≡3 mod 4), or 2q-6 (if q≡1 mod 4).

Conclusion: Successfully constructs new families of Singleton-optimal LRCs with d=6 and r=3 using combinatorial structures from finite geometry, providing explicit code constructions for various prime powers.

Abstract: In this paper, we present new constructions of $q$-ary Singleton-optimal locally repairable codes (LRCs) with minimum distance $d=6$ and locality $r=3$, based on combinatorial structures from finite geometry. By exploiting the well-known correspondence between a complete set of mutually orthogonal Latin squares (MOLS) of order $q$ and the affine plane $\mathrm{AG}(2,q)$, We systematically construct families of disjoint 4-arcs in the projective plane $\mathrm{PG}(2,q)$, such that the union of any two distinct 4-arcs forms an 8-arc. These 4-arcs form what we call 4-local arcs, and their existence is equivalent to that of the desired codes. For any prime power $q\ge 7$, our construction yields codes of length $n = 2q$, $2q-2$, or $2q-6$ depending on whether $q$ is even, $q\equiv 3 \pmod{4}$, or $q\equiv 1 \pmod{4}$, respectively.

</details>


### [256] [Guided Wireless Technology for Near-Field Communication](https://arxiv.org/abs/2602.21528)
*Mohamed Akrout,Amine Mezghani,Faouzi Bellili,Robert W. Heath*

Main category: cs.IT

TL;DR: Guided wireless technology combines guided waves and wireless communication to create controlled pathways for electromagnetic waves, reducing interference and supporting higher data rates over longer distances compared to traditional wireless systems.


<details>
  <summary>Details</summary>
Motivation: Traditional wireless systems suffer from signal interference, attenuation, and jamming as signals propagate through air. Guided communication confines signals within physical media, offering better control and performance. The paper aims to model near-field communication through long connected arrays in linear-cell environments to leverage these benefits.

Method: Develops a circuit model for long arrays as infinitely long dipoles with multiple periodic feed points, then approximates with finite arrays through open circuiting. Uses simulations to analyze standing wave phenomena and evaluates LMMSE transmit beamformer for interference mitigation and power allocation.

Result: Simulations confirm standing wave phenomenon through oscillations in spectral efficiency. LMMSE transmit beamformer effectively mitigates interference and minimizes mean square error by adaptively allocating more power to users experiencing severe channel attenuation, resulting in more balanced achievable rates across users.

Conclusion: Guided wireless technology with long connected arrays in linear-cell environments offers promising performance improvements. The circuit modeling approach and LMMSE beamforming demonstrate effective interference mitigation and balanced rate distribution, supporting the viability of guided wireless systems for enhanced communication performance.

Abstract: Guided wireless technology is an innovative approach that combines the strengths of guided waves and wireless communication. In traditional wireless systems, signals propagate through the air, where they are vulnerable to interference, attenuation, and jamming. Guided communication, in contrast, confines signals within a physical medium, significantly reducing interference and supporting higher data rates over longer distances. Guided wireless technology harnesses these benefits by creating guided wireless channels and offering a controlled pathway for electromagnetic waves. This work harnesses these benefits by focusing on the modeling of near-field communication through long connected arrays deployed in linear-cell environments. We derive a circuit model for long array as an infinitely long dipole with multiple periodic feed points before approximating it with a finite array through open circuiting. Through our simulations, we show how the standing wave phenomenon is confirmed by the oscillations in spectral efficiency. We also demonstrate the capability of the LMMSE transmit beamformer in mitigating interference and minimizing the mean square error by adaptively allocating more power to the user experiencing the most severe channel attenuation, resulting in a more balanced variation of achievable rates across users.

</details>


### [257] [Impact of Pointing Errors and Correlated Wall Blockages on Practical Grid-based Indoor Terahertz Communication Systems](https://arxiv.org/abs/2602.21558)
*Zhifeng Tang,Nan Yang,Salman Durrani,Xiangyun Zhou,Josep Miquel Jornet,Markku Juntti*

Main category: cs.IT

TL;DR: This paper analyzes indoor THz communication coverage probability under structured AP deployments (square/hexagonal grids), considering human/wall blockages, beam misalignment, and training overhead.


<details>
  <summary>Details</summary>
Motivation: THz communications promise high data rates but face severe path loss, blockage effects, and beam misalignment challenges. There's a need to understand coverage performance under practical indoor deployment scenarios with structured AP arrangements.

Method: Developed a tractable analytical framework for 3D indoor THz systems with structured AP deployments (square/hexagonal grids). The model jointly accounts for human blockages, correlated wall blockages across APs, beam training procedures, and residual pointing errors.

Result: Hexagonal grids outperform square grids by mitigating correlated wall blockage effects and reducing UE-AP distances. Wall blockage correlation significantly reduces association/coverage probabilities. Residual pointing error causes substantial coverage loss, especially for longer links. Beam training shows non-monotonic relationship between antenna array size and training overhead.

Conclusion: Hexagonal AP deployments provide better coverage than square grids for indoor THz systems. Wall blockage correlation must be considered in performance analysis. There's a tradeoff between antenna configuration, beamwidth selection, and beam training efficiency that requires careful optimization.

Abstract: Terahertz (THz) communications has emerged as a promising technology for future wireless systems due to its potential to support extremely high data rates. However, severe path loss, blockage effects, and sensitivity to beam misalignment pose major challenges to reliable indoor THz communications. In this paper, we investigate the coverage probability of downlink transmission in a three-dimensional (3D) indoor THz communication system under structured access point (AP) deployments, with a focus on square and hexagonal grid topologies. A tractable analytical framework is developed to jointly account for human blockages, correlated wall blockages across APs, beam training, and residual pointing error. Numerical results demonstrate that wall blockage correlation significantly reduces the association and coverage probabilities, and its impact cannot be neglected in system performance analysis. Compared with square grid AP deployments, hexagonal grids consistently achieve higher coverage by mitigating correlated wall blockage effects and reducing the distances between user equipments (UEs) and their associated APs. Furthermore, coverage performance is shown to strongly depend on the UE location, with noticeable degradation as the UE moves away from its nearest AP. Residual pointing error is found to introduce substantial coverage loss, especially for longer links. In addition, beam training analysis reveals a non-monotonic relationship between antenna array size and training overhead, highlighting an inherent tradeoff among antenna configuration, beamwidth selection, and beam training efficiency. These findings provide useful insights into the design and deployment of practical indoor THz communication systems.

</details>


### [258] [Concatenated Sum-Rank Codes](https://arxiv.org/abs/2602.21609)
*Huimin Lao,Hao Chen,San Ling,Yaqi Chen*

Main category: cs.IT

TL;DR: The paper introduces concatenation of sum-rank codes with Hamming metric codes, constructing many explicit sum-rank codes with better parameters than sum-rank BCH codes, and obtains asymptotically good sequences exceeding both Tsfasman-Vladut-Zink-like and Gilbert-Varshamov-like bounds.


<details>
  <summary>Details</summary>
Motivation: Sum-rank codes have important applications in multishot network coding, distributed storage, and space-time codes. While asymptotically good sequences of linearized algebraic geometry sum-rank codes exceeding Gilbert-Varshamov-like bound were recently constructed, there's a need for more practical, explicit constructions with better parameters than existing sum-rank BCH codes.

Method: Introduces concatenation technique combining sum-rank codes with Hamming metric codes. This approach allows for simple and explicit construction of many sum-rank codes with improved parameters.

Result: Constructs many sum-rank codes with good parameters that outperform sum-rank BCH codes. Obtains an asymptotically good sequence of sum-rank codes that exceeds both the Tsfasman-Vladut-Zink-like bound and the Gilbert-Varshamov-like bound.

Conclusion: The concatenation method provides an effective way to construct explicit sum-rank codes with superior parameters, achieving asymptotic performance beyond established theoretical bounds, making them valuable for practical applications in network coding, distributed storage, and space-time coding.

Abstract: Sum-rank codes have wide applications in multishot network coding, distributed storage and the construction of space-time codes. Asymptotically good sequences of linearized algebraic geometry sum-rank codes, exceeding the Gilbert-Varshamov-like bound, were constructed in a recent paper published in IEEE Trans. Inf. Theory by E. Berardini and X. Caruso. We call this bound the Tsfasman-Vladut-Zink-like bound. In this paper, we introduce the concatenation of a sum-rank code and a Hamming metric code. Then many sum-rank codes with good parameters, which are better than sum-rank BCH codes, are constructed simply and explicitly. Moreover, we obtain an asymptotically good sequence of sum-rank codes exceeding the Tsfasman-Vladut-Zink-like bound and the Gilbert-Varshamov-like bound.

</details>


### [259] [Permutation Polynomials Under Multiplicative-Additive Perturbations: Characterization via Difference Distribution Tables](https://arxiv.org/abs/2602.21632)
*Ranit Dutta,Pantelimon Stanica,Bimal Mandal*

Main category: cs.IT

TL;DR: The paper characterizes perfect c-nonlinearity (PcN) in permutation polynomials over finite fields, providing efficient verification methods, proving dichotomies for monomials, characterizing quadratic permutations, and revealing incompatibility between PcN and APN properties.


<details>
  <summary>Details</summary>
Motivation: Motivated by cryptanalysis of the Kuznyechik cipher variant, the research addresses optimal resistance to c-differential attacks through perfect c-nonlinearity (PcN) properties in permutation polynomials over finite fields.

Method: Uses classical difference distribution table (DDT) characterization, provides O(p^{2n}) verification algorithm, proves strict dichotomy for monomial permutations, gives explicit algebraic characterizations for quadratic permutations, and identifies affine transformations preserving c-differential uniformity.

Result: First characterization of PcN using DDT with efficient verification, proof of monomial permutation dichotomy, explicit characterizations for quadratic permutations, identification of affine transformations preserving c-differential uniformity, and tight nonlinearity bounds showing incompatibility between PcN and APN.

Conclusion: Perfect c-nonlinearity represents a structurally distinct regime within permutation polynomial theory, with fundamental incompatibility between PcN and APN properties, positioning it as important for cryptographic applications requiring optimal c-differential attack resistance.

Abstract: We investigate permutation polynomials F over finite fields F_{p^n} whose generalized derivative maps x -> F(x + a) - cF(x) are themselves permutations for all nonzero shifts a. This property, termed perfect c-nonlinearity (PcN), represents optimal resistance to c-differential attacks - a concern highlighted by recent cryptanalysis of the Kuznyechik cipher variant. We provide the first characterization using the classical difference distribution table (DDT): F is PcN if and only if Delta_F(a,b) Delta_F(a,c^{-1}b) = 0 for all nonzero a,b. This enables verification in O(p^{2n}) time given a precomputed DDT, a significant improvement over the naive O(p^{3n}) approach. We prove a strict dichotomy for monomial permutations: the derivative F(x + alpha) - cF(x) is either a permutation for all nonzero shifts or for none, with the general case remaining open. For quadratic permutations, we provide explicit algebraic characterizations. We identify the first class of affine transformations preserving c-differential uniformity and derive tight nonlinearity bounds revealing fundamental incompatibility between PcN and APN properties. These results position perfect c-nonlinearity as a structurally distinct regime within permutation polynomial theory.

</details>


### [260] [From Specialist to Large Models: A Paradigm Evolution Towards Semantic-Aware MIMO](https://arxiv.org/abs/2602.21672)
*Keke Ying,Zhen Gao,Tingting Yang,Jianhua Zhang,Xiang Cheng,Tony Q. S. Quek,H. Vincent Poor*

Main category: cs.IT

TL;DR: The paper proposes a "semantic-aware MIMO" paradigm for 6G networks that uses specialist models and large models to leverage channel and source semantics for improved performance in massive MIMO systems.


<details>
  <summary>Details</summary>
Motivation: 6G networks will deploy larger MIMO arrays for massive connectivity, increasing overhead and latency. Emerging 6G demands like immersive communications and environmental sensing challenge traditional signal processing methods.

Method: Proposes semantic-aware MIMO paradigm using specialist models and large models to perceive, utilize, and fuse inherent semantics of channels and sources. Designs specialist models for specific MIMO tasks (random access activity detection, channel feedback, precoding). Explores large models as scalable solution for multi-task semantic-aware MIMO.

Result: The paper presents a conceptual framework and design approach for semantic-aware MIMO systems, reviewing recent advances in specialist and large models for MIMO applications, along with their advantages and limitations.

Conclusion: The semantic-aware MIMO paradigm offers promising solutions for 6G challenges. The paper discusses challenges, insights, and prospects for evolving specialist models and large models in semantic-aware MIMO systems.

Abstract: The sixth generation (6G) network is expected to deploy larger multiple-input multiple-output (MIMO) arrays to support massive connectivity, which will increase overhead and latency at the physical layer. Meanwhile, emerging 6G demands such as immersive communications and environmental sensing pose challenges to traditional signal processing. To address these issues, we propose the ``semantic-aware MIMO'' paradigm, which leverages specialist models and large models to perceive, utilize, and fuse the inherent semantics of channels and sources for improved performance. Moreover, for representative MIMO physical-layer tasks, e.g., random access activity detection, channel feedback, and precoding, we design specialist models that exploit channel and source semantics for better performance. Additionally, in view of the more diversified functions of 6G MIMO, we further explore large models as a scalable solution for multi-task semantic-aware MIMO and review recent advances along with their advantages and limitations. Finally, we discuss the challenges, insights, and prospects of the evolution of specialist models and large models empowered semantic-aware MIMO paradigms.

</details>


### [261] [Function-Correcting Codes with Optimal Data Protection for Hamming Code Membership](https://arxiv.org/abs/2602.21932)
*Swaraj Sharma Durgi,Anjana A. Mahesh,Anupriya Kumari,Rajlaxmi Pandey,B. Sundar Rajan*

Main category: cs.IT

TL;DR: This paper develops optimal single-error-correcting function-correcting codes for Hamming code membership function detection, proving that maximizing sum-distance yields codes with optimal distance properties and improved BER performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to design optimal function-correcting codes for detecting whether vectors belong to the [7,4,3]-Hamming code, which has applications in reliable function computation and data protection where the function output (code membership) needs to be error-corrected.

Method: The method establishes necessary/sufficient conditions for valid parity assignments using distance constraints. It exploits the bipartite graph structure induced by Hamming-distance-3 relations among codewords to develop a systematic SEFCC construction. The approach derives a tight upper bound on sum of pairwise distances and proves the bipartite construction uniquely achieves maximum sum-distance.

Result: The proposed bipartite construction achieves: maximum sum-distance, largest possible minimum distance of 2, and minimum number of distance-2 codeword pairs. For the HCMF SEFCC problem, sum-distance maximization exactly enforces optimal distance-spectrum properties. Simulation results show max-sum SEFCCs provide significantly improved data protection and BER performance over arbitrary valid assignments.

Conclusion: Sum-distance maximization is not just heuristic but exactly enforces optimal distance-spectrum properties for HCMF SEFCCs. The bipartite construction yields codes with optimal distance properties that translate to improved error performance in practical AWGN channels with soft-decision decoding.

Abstract: This paper investigates single-error-correcting function-correcting codes (SEFCCs) for the Hamming code membership function (HCMF), which indicates whether a vector in $\mathbb{F}_2^7$ belongs to the [7,4,3]-Hamming code. Necessary and sufficient conditions for valid parity assignments are established in terms of distance constraints between codewords and their nearest non-codewords. It is shown that the Hamming-distance-3 relations among Hamming codewords induce a bipartite graph, a fundamental geometric property that is exploited to develop a systematic SEFCC construction. By deriving a tight upper bound on the sum of pairwise distances, we prove that the proposed bipartite construction uniquely achieves the maximum sum-distance, the largest possible minimum distance of 2, and the minimum number of distance-2 codeword pairs. Consequently, for the HCMF SEFCC problem, sum-distance maximisation is not merely heuristic-it exactly enforces the optimal distance-spectrum properties relevant to error probability. Simulation results over AWGN channels with soft-decision decoding confirm that the resulting max-sum SEFCCs provide significantly improved data protection and Bit Error Rate (BER) performance compared to arbitrary valid assignments.

</details>


### [262] [Maximal Recoverability: A Nexus of Coding Theory](https://arxiv.org/abs/2602.22042)
*Joshua Brakensiek,Venkatesan Guruswami*

Main category: cs.IT

TL;DR: Survey paper exploring maximal recoverability (MR) codes, focusing on two families: MR locally recoverable codes (LRCs/partial MDS codes) and grid codes (GCs), discussing their recoverability guarantees, optimal constructions, and connections to broader CS/math problems.


<details>
  <summary>Details</summary>
Motivation: In large-scale computing systems, error correcting codes with redundancy are crucial for recoverability from failures. The framework of maximal recoverability (MR) helps study optimal error-correcting codes to maximize storage efficiency while ensuring recoverability.

Method: Survey methodology examining two families of MR codes: 1) MR locally recoverable codes (LRCs/partial MDS codes) using skew polynomial codes to unify previous constructions, and 2) MR grid codes (GCs) analyzed through higher order MDS codes theory.

Result: The survey reveals: 1) Skew polynomial codes provide unified framework for MR LRC constructions, 2) MR GCs can construct optimal list-decodable codes, and 3) MR GCs' recoverability patterns connect to structural rigidity problems in graph theory.

Conclusion: MR codes offer powerful frameworks for optimal error correction with connections to broader theoretical problems. The survey highlights how MR LRCs benefit from algebraic unification via skew polynomials, while MR GCs reveal deep connections to list decoding and graph rigidity.

Abstract: In the modern era of large-scale computing systems, a crucial use of error correcting codes is to judiciously introduce redundancy to ensure recoverability from failure. To get the most out of every byte, practitioners and theorists have introduced the framework of maximal recoverability (MR) to study optimal error-correcting codes in various architectures. In this survey, we dive into the study of two families of MR codes: MR locally recoverable codes (LRCs) (also known as partial MDS codes) and grid codes (GCs).
  For each of these two families of codes, we discuss the primary recoverability guarantees as well as what is known concerning optimal constructions. Along the way, we discuss many surprising connections between MR codes and broader questions in computer science and mathematics. For MR LRCs, the use of skew polynomial codes has unified many previous constructions. For MR GCs, the theory of higher order MDS codes shows that MR GCs can be used to construct optimal list-decodable codes. Furthermore, the optimally recoverable patterns of MR GCs have close ties to long-standing problems on the structural rigidity of graphs.

</details>


### [263] [Multichannel Conflict-Avoiding Codes for Expanded Scenarios](https://arxiv.org/abs/2602.22081)
*Kangkang Xu,Yuan-Hsun Lo,Tsai-Lien Wong,Yijin Zhang,Kenneth W. Shum*

Main category: cs.IT

TL;DR: This paper extends conflict-avoiding codes (CACs) to multichannel CACs (MC-CACs) for the practical case where number of channels M is less than weight w, introduces exceptional codewords concept, uses additive combinatorics to derive optimal MC-CACs, and generalizes results to AM-OPPTS and mixed-weight MC-CACs.


<details>
  <summary>Details</summary>
Motivation: Most existing MC-CAC research assumes M ≥ w, but practical scenarios often have M < w. The paper aims to address this gap by developing optimal MC-CACs for the more realistic case where number of channels is less than weight.

Method: Introduces concept of exceptional codewords in MC-CACs and employs techniques from additive combinatorics to derive a series of optimal MC-CAC constructions. Generalizes previously known optimal CAC results to the multichannel setting.

Result: Derives optimal MC-CACs for the case M < w, extends results to AM-OPPTS MC-CACs and mixed-weight MC-CACs, providing constructions that work for practical application scenarios.

Conclusion: The paper successfully addresses the practical case of M < w in MC-CACs, introduces new theoretical concepts, and provides optimal constructions that generalize previous results and extend to relevant code classes.

Abstract: A conflict-avoiding code (CAC) of length L and weight w is used for deterministic multiple-access without feedback. When the number of simultaneous active users is less than or equal to w, such a code is able to provide a hard guarantee that each active user has a successful transmission within every consecutive L time slots. Recently, CACs were extended to multichannel CAcs (MC-CACs) over M orthogonal channels with the aim of increasing the number of potential users that can be supported. While most existing results on MC-CAC are derived under the assumption that M is not less than w, this paper focuses on the case that M is less than w, which is more relevant to practical application scenarios. In this paper, we first introduce the concept of exceptional codewords in MC-CACs. By employing some techniques from additive combinatorics, we derive a series of optimal MC-CACs. Along the way, several previously known optimal CAC results are generalized. Finally, our results extend naturally to AM-OPPTS MC-CACs and mixed-weight MC-CACs, two classes of relevant codes.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [264] [A Dynamic Survey of Soft Set Theory and Its Extensions](https://arxiv.org/abs/2602.21268)
*Takaaki Fujita,Florentin Smarandache*

Main category: cs.AI

TL;DR: Survey of soft set theory and its extensions for parameterized decision modeling under uncertainty.


<details>
  <summary>Details</summary>
Motivation: Soft set theory provides a direct framework for parameterized decision modeling by handling uncertainty in a structured way, and has expanded into numerous variants with connections to diverse mathematical areas.

Method: Survey-style overview presenting core definitions, representative constructions, and key development directions of soft sets and their major extensions including hypersoft sets, superhypersoft sets, TreeSoft sets, bipolar soft sets, and dynamic soft sets.

Result: Comprehensive overview connecting soft set theory to diverse areas such as topology and matroid theory, highlighting the expansion of the theory into numerous variants over past decades.

Conclusion: The book provides a systematic survey of soft set theory's evolution, extensions, and interdisciplinary connections, serving as a reference for current developments in parameterized decision modeling under uncertainty.

Abstract: Soft set theory provides a direct framework for parameterized decision modeling by assigning to each attribute (parameter) a subset of a given universe, thereby representing uncertainty in a structured way [1, 2]. Over the past decades, the theory has expanded into numerous variants-including hypersoft sets, superhypersoft sets, TreeSoft sets, bipolar soft sets, and dynamic soft sets-and has been connected to diverse areas such as topology and matroid theory. In this book, we present a survey-style overview of soft sets and their major extensions, highlighting core definitions, representative constructions, and key directions of current development.

</details>


### [265] [A Hierarchical Multi-Agent System for Autonomous Discovery in Geoscientific Data Archives](https://arxiv.org/abs/2602.21351)
*Dmitrii Pantiukhin,Ivan Kuznetsov,Boris Shapkin,Antonia Anna Jost,Thomas Jung,Nikolay Koldunov*

Main category: cs.AI

TL;DR: PANGAEA-GPT: A hierarchical multi-agent framework for autonomous data discovery and analysis in Earth science repositories, enabling complex workflows with minimal human intervention.


<details>
  <summary>Details</summary>
Motivation: Earth science data repositories like PANGAEA contain vast datasets, but citation metrics show much of this data remains underutilized, limiting data reusability and creating scalability challenges for researchers.

Method: PANGAEA-GPT uses a hierarchical multi-agent framework with centralized Supervisor-Worker topology, featuring strict data-type-aware routing, sandboxed deterministic code execution, and self-correction via execution feedback to diagnose and resolve runtime errors.

Result: The system demonstrates capacity to execute complex, multi-step workflows with minimal human intervention through use-case scenarios spanning physical oceanography and ecology.

Conclusion: The framework provides a methodology for querying and analyzing heterogeneous repository data through coordinated agent workflows, addressing the scalability challenge in Earth science data utilization.

Abstract: The rapid accumulation of Earth science data has created a significant scalability challenge; while repositories like PANGAEA host vast collections of datasets, citation metrics indicate that a substantial portion remains underutilized, limiting data reusability. Here we present PANGAEA-GPT, a hierarchical multi-agent framework designed for autonomous data discovery and analysis. Unlike standard Large Language Model (LLM) wrappers, our architecture implements a centralized Supervisor-Worker topology with strict data-type-aware routing, sandboxed deterministic code execution, and self-correction via execution feedback, enabling agents to diagnose and resolve runtime errors. Through use-case scenarios spanning physical oceanography and ecology, we demonstrate the system's capacity to execute complex, multi-step workflows with minimal human intervention. This framework provides a methodology for querying and analyzing heterogeneous repository data through coordinated agent workflows.

</details>


### [266] [Beyond Refusal: Probing the Limits of Agentic Self-Correction for Semantic Sensitive Information](https://arxiv.org/abs/2602.21496)
*Umid Suleymanov,Zaur Rajabov,Emil Mirzazada,Murat Kantarcioglu*

Main category: cs.AI

TL;DR: SemSIEdit is an inference-time framework where an agentic Editor critiques and rewrites sensitive spans in LLM outputs to reduce Semantic Sensitive Information leaks while preserving utility, achieving 34.6% leakage reduction with only 9.8% utility loss.


<details>
  <summary>Details</summary>
Motivation: LLMs pose a new threat through Semantic Sensitive Information (SemSI) - inferring sensitive identity attributes, generating reputation-harmful content, or hallucinating wrong information. Current defenses for structured PII are mature, but self-regulating these complex, context-dependent sensitive information leaks without destroying utility remains an open scientific question.

Method: Introduces SemSIEdit, an inference-time framework where an agentic "Editor" iteratively critiques and rewrites sensitive spans to preserve narrative flow rather than simply refusing to answer. The approach operates at inference time to address SemSI leaks.

Result: Reveals a Privacy-Utility Pareto Frontier with 34.6% leakage reduction across all three SemSI categories and only 9.8% utility loss. Uncovers Scale-Dependent Safety Divergence: large reasoning models achieve safety through constructive expansion (adding nuance), while capacity-constrained models revert to destructive truncation (deleting text). Identifies a Reasoning Paradox: inference-time reasoning increases baseline risk but simultaneously empowers the defense to execute safe rewrites.

Conclusion: SemSIEdit provides an effective framework for addressing Semantic Sensitive Information leaks in LLMs, demonstrating that agentic rewriting can significantly reduce privacy risks while maintaining utility, with important insights about model scaling and the paradoxical role of reasoning in both creating and solving SemSI problems.

Abstract: While defenses for structured PII are mature, Large Language Models (LLMs) pose a new threat: Semantic Sensitive Information (SemSI), where models infer sensitive identity attributes, generate reputation-harmful content, or hallucinate potentially wrong information. The capacity of LLMs to self-regulate these complex, context-dependent sensitive information leaks without destroying utility remains an open scientific question. To address this, we introduce SemSIEdit, an inference-time framework where an agentic "Editor" iteratively critiques and rewrites sensitive spans to preserve narrative flow rather than simply refusing to answer. Our analysis reveals a Privacy-Utility Pareto Frontier, where this agentic rewriting reduces leakage by 34.6% across all three SemSI categories while incurring a marginal utility loss of 9.8%. We also uncover a Scale-Dependent Safety Divergence: large reasoning models (e.g., GPT-5) achieve safety through constructive expansion (adding nuance), whereas capacity-constrained models revert to destructive truncation (deleting text). Finally, we identify a Reasoning Paradox: while inference-time reasoning increases baseline risk by enabling the model to make deeper sensitive inferences, it simultaneously empowers the defense to execute safe rewrites.

</details>


### [267] [ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning](https://arxiv.org/abs/2602.21534)
*Xiaoxuan Wang,Han Zhang,Haixin Wang,Yidan Shi,Ruoyan Li,Kaiqiao Han,Chenyi Tong,Haoran Deng,Renliang Sun,Alexander Taylor,Yanqiao Zhu,Jason Cong,Yizhou Sun,Wei Wang*

Main category: cs.AI

TL;DR: The paper proposes ARLArena (a stable training recipe and analysis framework) and SAMPO (a stable agentic policy optimization method) to address training instability in Agentic Reinforcement Learning (ARL).


<details>
  <summary>Details</summary>
Motivation: ARL shows promise for complex multi-step tasks but suffers from high instability leading to training collapse, which limits scalability to larger environments, longer horizons, and systematic exploration of algorithmic design choices.

Method: 1) ARLArena: Creates a clean, standardized testbed and decomposes policy gradient into four core design dimensions for systematic stability analysis. 2) SAMPO: A stable agentic policy optimization method designed to mitigate dominant sources of instability identified through the analysis.

Result: SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. The framework provides a unifying policy gradient perspective for ARL.

Conclusion: The study offers practical guidance for building stable and reproducible LLM-based agent training pipelines through a systematic analysis framework and a stable optimization method.

Abstract: Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, a stable training recipe and systematic analysis framework that examines training stability in a controlled and reproducible setting. ARLArena first constructs a clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill a unified perspective on ARL and propose SAMPO, a stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides a unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines.

</details>


### [268] [Power and Limitations of Aggregation in Compound AI Systems](https://arxiv.org/abs/2602.21556)
*Nivasini Ananthakrishnan,Meena Jagadeesan*

Main category: cs.AI

TL;DR: Aggregation of multiple AI models can expand the set of achievable outputs beyond what's possible with a single model through three mechanisms: feasibility expansion, support expansion, and binding set contraction.


<details>
  <summary>Details</summary>
Motivation: To understand whether aggregating responses from multiple copies of the same AI model can overcome limitations of individual models and prompt engineering, enabling access to a greater set of outputs than querying a single model alone.

Method: Analyzes aggregation within a principal-agent framework where system designers can partially steer agent outputs through reward functions but face prompt engineering and capability limitations. Identifies three mechanisms through which aggregation expands elicitable outputs.

Result: Proves that any aggregation operation must implement one of three mechanisms (feasibility expansion, support expansion, or binding set contraction) to be elicitability-expanding, and that strengthened versions provide necessary and sufficient conditions for elicitability-expansion. Includes empirical illustration with LLMs in a reference-generation task.

Conclusion: Aggregation in compound AI systems can overcome limitations in model capabilities and prompt engineering through specific mechanisms, providing theoretical characterization of when such systems expand achievable outputs beyond single-model performance.

Abstract: When designing compound AI systems, a common approach is to query multiple copies of the same model and aggregate the responses to produce a synthesized output. Given the homogeneity of these models, this raises the question of whether aggregation unlocks access to a greater set of outputs than querying a single model. In this work, we investigate the power and limitations of aggregation within a stylized principal-agent framework. This framework models how the system designer can partially steer each agent's output through its reward function specification, but still faces limitations due to prompt engineering ability and model capabilities. Our analysis uncovers three natural mechanisms -- feasibility expansion, support expansion, and binding set contraction -- through which aggregation expands the set of outputs that are elicitable by the system designer. We prove that any aggregation operation must implement one of these mechanisms in order to be elicitability-expanding, and that strengthened versions of these mechanisms provide necessary and sufficient conditions that fully characterize elicitability-expansion. Finally, we provide an empirical illustration of our findings for LLMs deployed in a toy reference-generation task. Altogether, our results take a step towards characterizing when compound AI systems can overcome limitations in model capabilities and in prompt engineering.

</details>


### [269] [The ASIR Courage Model: A Phase-Dynamic Framework for Truth Transitions in Human and AI Systems](https://arxiv.org/abs/2602.21745)
*Hyo Jin Kim*

Main category: cs.AI

TL;DR: The ASIR Courage Model is a phase-dynamic framework that formalizes truth-disclosure as a state transition from suppression to expression, applicable to both human and AI systems under constraints.


<details>
  <summary>Details</summary>
Motivation: To provide a unified structural account of truth-disclosure under risk that applies to both human silence under pressure and AI preference-driven distortion, moving beyond personality-based explanations to a dynamical systems approach.

Method: A phase-dynamic framework with mathematical inequality (lambda(1+gamma)+psi > theta+phi) representing facilitative vs inhibitory forces, extended with feedback mechanisms for recursive parameter recalibration and path dependence effects.

Result: The model successfully characterizes truth-disclosure as state transitions in both human and AI systems, interpreting shifts in apparent truthfulness as geometric consequences of interacting forces rather than intentional behavior.

Conclusion: The ASIR Courage Model offers a formal perspective on truth-disclosure under risk that unifies human courage and AI alignment within a shared dynamical structure, reframing both as phase transitions in constrained systems.

Abstract: We introduce the ASIR (Awakened Shared Intelligence Relationship) Courage Model, a phase-dynamic framework that formalizes truth-disclosure as a state transition rather than a personality trait. The mode characterizes the shift from suppression (S0) to expression (S1) as occurring when facilitative forces exceed inhibitory thresholds, expressed by the inequality lambda(1+gamma)+psi > theta+phi, where the terms represent baseline openness, relational amplification, accumulated internal pressure, and transition costs.
  Although initially formulated for human truth-telling under asymmetric stakes, the same phase-dynamic architecture extends to AI systems operating under policy constraints and alignment filters. In this context, suppression corresponds to constrained output states, while structural pressure arises from competing objectives, contextual tension, and recursive interaction dynamics. The framework therefore provides a unified structural account of both human silence under pressure and AI preference-driven distortion.
  A feedback extension models how transition outcomes recursively recalibrate system parameters, generating path dependence and divergence effects across repeated interactions. Rather than attributing intention to AI systems, the model interprets shifts in apparent truthfulness as geometric consequences of interacting forces within constrained phase space. By reframing courage and alignment within a shared dynamical structure, the ASIR Courage Model offers a formal perspective on truth-disclosure under risk across both human and artificial systems.

</details>


### [270] [fEDM+: A Risk-Based Fuzzy Ethical Decision Making Framework with Principle-Level Explainability and Pluralistic Validation](https://arxiv.org/abs/2602.21746)
*Abeer Dyoub,Francesca A. Lisi*

Main category: cs.AI

TL;DR: Extends fuzzy Ethical Decision-Making (fEDM) to fEDM+ with explainability module and pluralistic validation for AI ethics oversight.


<details>
  <summary>Details</summary>
Motivation: Original fEDM lacked principled explainability and robustness under ethical pluralism, limiting its suitability for ethically sensitive AI systems.

Method: Adds Explainability and Traceability Module (ETM) linking decisions to moral principles, and replaces single-referent validation with pluralistic semantic validation against multiple stakeholder perspectives.

Result: Creates fEDM+ that preserves formal verifiability while achieving enhanced interpretability and stakeholder-aware validation.

Conclusion: fEDM+ provides transparent, auditable ethical reasoning suitable as an oversight layer for ethically sensitive AI systems.

Abstract: In a previous work, we introduced the fuzzy Ethical Decision-Making framework (fEDM), a risk-based ethical reasoning architecture grounded in fuzzy logic. The original model combined a fuzzy Ethical Risk Assessment module (fERA) with ethical decision rules, enabled formal structural verification through Fuzzy Petri Nets (FPNs), and validated outputs against a single normative referent. Although this approach ensured formal soundness and decision consistency, it did not fully address two critical challenges: principled explainability of decisions and robustness under ethical pluralism. In this paper, we extend fEDM in two major directions. First, we introduce an Explainability and Traceability Module (ETM) that explicitly links each ethical decision rule to the underlying moral principles and computes a weighted principle-contribution profile for every recommended action. This enables transparent, auditable explanations that expose not only what decision was made but why, and on the basis of which principles. Second, we replace single-referent validation with a pluralistic semantic validation framework that evaluates decisions against multiple stakeholder referents, each encoding distinct principle priorities and risk tolerances. This shift allows principled disagreement to be formally represented rather than suppressed, thus increasing robustness and contextual sensitivity. The resulting extended fEDM, called fEDM+, preserves formal verifiability while achieving enhanced interpretability and stakeholder-aware validation, making it suitable as an oversight and governance layer for ethically sensitive AI systems.

</details>


### [271] [Prompt Architecture Determines Reasoning Quality: A Variable Isolation Study on the Car Wash Problem](https://arxiv.org/abs/2602.21814)
*Heejin Jo*

Main category: cs.AI

TL;DR: STAR reasoning framework boosts LLM accuracy on car wash problem from 0% to 85%, with full-stack system achieving 100% accuracy.


<details>
  <summary>Details</summary>
Motivation: Large language models consistently fail the "car wash problem" benchmark that requires implicit physical constraint inference, prompting investigation into which prompt architecture layers enable correct reasoning.

Method: Variable isolation study with 120 total trials (n=20 per condition, 6 conditions) using Claude 3.5 Sonnet with controlled hyperparameters. Tested different prompt architecture layers including STAR reasoning framework, user profile context via vector database retrieval, and RAG context.

Result: STAR framework alone raised accuracy from 0% to 85% (p=0.001, odds ratio 13.22). Adding user profile context provided additional 10% gain, and RAG context added 5% more, achieving 100% accuracy in full-stack condition.

Conclusion: Structured reasoning scaffolds (specifically forced goal articulation before inference) matter substantially more than context injection for implicit constraint reasoning tasks.

Abstract: Large language models consistently fail the "car wash problem," a viral reasoning benchmark requiring implicit physical constraint inference. We present a variable isolation study (n=20 per condition, 6 conditions, 120 total trials) examining which prompt architecture layers in a production system enable correct reasoning. Using Claude 3.5 Sonnet with controlled hyperparameters (temperature 0.7, top_p 1.0), we find that the STAR (Situation-Task-Action-Result) reasoning framework alone raises accuracy from 0% to 85% (p=0.001, Fisher's exact test, odds ratio 13.22). Adding user profile context via vector database retrieval provides a further 10 percentage point gain, while RAG context contributes an additional 5 percentage points, achieving 100% accuracy in the full-stack condition. These results suggest that structured reasoning scaffolds -- specifically, forced goal articulation before inference -- matter substantially more than context injection for implicit constraint reasoning tasks.

</details>


### [272] [Distill and Align Decomposition for Enhanced Claim Verification](https://arxiv.org/abs/2602.21857)
*Jabez Magomere,Elena Kochkina,Samuel Mensah,Simerjot Kaur,Fernando Acero,Arturo Oncevay,Charese H. Smiley,Xiaomo Liu,Manuela Veloso*

Main category: cs.AI

TL;DR: RL-based approach improves complex claim verification by jointly optimizing decomposition quality and verifier alignment using Group Relative Policy Optimization, achieving state-of-the-art performance with smaller models.


<details>
  <summary>Details</summary>
Motivation: Existing methods for complex claim verification struggle to align decomposition quality with verification performance, creating a gap between generating verifiable subclaims and actually improving verification accuracy.

Method: Proposes reinforcement learning approach with Group Relative Policy Optimization (GRPO) that integrates: structured sequential reasoning, supervised finetuning on teacher-distilled exemplars, and multi-objective reward balancing format compliance, verifier alignment, and decomposition quality.

Result: Trained 8B decomposer achieves 71.75% macro-F1 across six evaluation settings, outperforming prompt-based approaches (+1.99, +6.24) and existing RL methods (+5.84). Human evaluation confirms high quality of generated subclaims.

Conclusion: The framework enables smaller language models to achieve state-of-the-art claim verification by jointly optimizing for verification accuracy and decomposition quality, bridging the gap between decomposition and verification performance.

Abstract: Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using Group Relative Policy Optimization (GRPO). Our method integrates: (i) structured sequential reasoning; (ii) supervised finetuning on teacher-distilled exemplars; and (iii) a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Across six evaluation settings, our trained 8B decomposer improves downstream verification performance to (71.75%) macro-F1, outperforming prompt-based approaches ((+1.99), (+6.24)) and existing RL methods ((+5.84)). Human evaluation confirms the high quality of the generated subclaims. Our framework enables smaller language models to achieve state-of-the-art claim verification by jointly optimising for verification accuracy and decomposition quality.

</details>


### [273] [ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices](https://arxiv.org/abs/2602.21858)
*Dezhi Kong,Zhengzhao Feng,Qiliang Liang,Hao Wang,Haofei Sun,Changpeng Yang,Yang Li,Peng Zhou,Shuai Nie,Hongzhen Wang,Linfeng Zhou,Hao Jia,Jiaming Xu,Runyu Shi,Ying Huang*

Main category: cs.AI

TL;DR: ProactiveMobile is a benchmark for evaluating proactive intelligence in mobile agents, where agents anticipate user needs rather than just executing commands. It includes 3,660 instances across 14 scenarios with 63 APIs, showing current MLLMs struggle with proactivity but can learn it.


<details>
  <summary>Details</summary>
Motivation: Current multimodal large language models (MLLMs) for mobile agents are limited to reactive paradigms (executing explicit commands). The next frontier is proactive intelligence where agents anticipate needs and initiate actions autonomously, but development is bottlenecked by lack of benchmarks that address real-world complexity and enable objective, executable evaluation.

Method: Created ProactiveMobile benchmark that formalizes proactive tasks as inferring latent user intent across four dimensions of on-device contextual signals and generating executable function sequences from 63 APIs. Includes 3,660 instances across 14 scenarios with multi-answer annotations. Team of 30 experts conducted final audit for factual accuracy, logical consistency, and action feasibility.

Result: Fine-tuned Qwen2.5-VL-7B-Instruct achieved 19.15% success rate, outperforming o1 (15.71%) and GPT-5 (7.39%). This shows proactivity is a critical competency lacking in current MLLMs but is learnable, highlighting the benchmark's importance for evaluation.

Conclusion: ProactiveMobile addresses the critical bottleneck in proactive mobile agent research by providing a comprehensive benchmark with real-world complexity. Results demonstrate that while current MLLMs struggle with proactivity, it is a learnable capability, emphasizing the need for specialized benchmarks to advance this emerging paradigm.

Abstract: Multimodal large language models (MLLMs) have made significant progress in mobile agent development, yet their capabilities are predominantly confined to a reactive paradigm, where they merely execute explicit user commands. The emerging paradigm of proactive intelligence, where agents autonomously anticipate needs and initiate actions, represents the next frontier for mobile agents. However, its development is critically bottlenecked by the lack of benchmarks that can address real-world complexity and enable objective, executable evaluation. To overcome these challenges, we introduce ProactiveMobile, a comprehensive benchmark designed to systematically advance research in this domain. ProactiveMobile formalizes the proactive task as inferring latent user intent across four dimensions of on-device contextual signals and generating an executable function sequence from a comprehensive function pool of 63 APIs. The benchmark features over 3,660 instances of 14 scenarios that embrace real-world complexity through multi-answer annotations. To ensure quality, a team of 30 experts conducts a final audit of the benchmark, verifying factual accuracy, logical consistency, and action feasibility, and correcting any non-compliant entries. Extensive experiments demonstrate that our fine-tuned Qwen2.5-VL-7B-Instruct achieves a success rate of 19.15%, outperforming o1 (15.71%) and GPT-5 (7.39%). This result indicates that proactivity is a critical competency widely lacking in current MLLMs, yet it is learnable, emphasizing the importance of the proposed benchmark for proactivity evaluation.

</details>


### [274] [2-Step Agent: A Framework for the Interaction of a Decision Maker with AI Decision Support](https://arxiv.org/abs/2602.21889)
*Otto Nyberg,Fausto Carcassi,Giovanni Cinà*

Main category: cs.AI

TL;DR: AI decision support can worsen outcomes when users have misaligned prior beliefs, requiring better model documentation and user training.


<details>
  <summary>Details</summary>
Motivation: As AI models increasingly support human decision making across fields, we lack understanding of the effects of this adoption. The paper aims to model how AI predictions influence human beliefs and downstream decisions.

Method: Introduces the 2-Step Agent framework using Bayesian causal inference to model: 1) how AI predictions affect rational Bayesian agents' beliefs, and 2) how belief changes affect downstream decisions and outcomes. Uses simulations to demonstrate effects.

Result: Simulations show that a single misaligned prior belief can cause AI decision support to result in worse downstream outcomes compared to no decision support. Reveals several potential pitfalls of AI-driven decision support.

Conclusion: Highlights the need for thorough model documentation and proper user training to mitigate risks of AI decision support systems, especially when users have misaligned prior beliefs.

Abstract: Across a growing number of fields, human decision making is supported by predictions from AI models. However, we still lack a deep understanding of the effects of adoption of these technologies. In this paper, we introduce a general computational framework, the 2-Step Agent, which models the effects of AI-assisted decision making. Our framework uses Bayesian methods for causal inference to model 1) how a prediction on a new observation affects the beliefs of a rational Bayesian agent, and 2) how this change in beliefs affects the downstream decision and subsequent outcome. Using this framework, we show by simulations how a single misaligned prior belief can be sufficient for decision support to result in worse downstream outcomes compared to no decision support. Our results reveal several potential pitfalls of AI-driven decision support and highlight the need for thorough model documentation and proper user training.

</details>


### [275] [Semantic Partial Grounding via LLMs](https://arxiv.org/abs/2602.22067)
*Giuseppe Canonaco,Alberto Pozanco,Daniel Borrajo*

Main category: cs.AI

TL;DR: SPG-LLM uses LLMs to analyze PDDL descriptions to identify irrelevant objects, actions, and predicates before grounding, reducing grounded task size and speeding up planning.


<details>
  <summary>Details</summary>
Motivation: Grounding in classical planning creates computational bottlenecks due to exponential growth in grounded actions and atoms. Existing partial grounding methods don't leverage textual and structural cues in PDDL descriptions.

Method: Uses LLMs to analyze domain and problem PDDL files to heuristically identify potentially irrelevant objects, actions, and predicates prior to grounding, reducing the size of the grounded task.

Result: Across seven hard-to-ground benchmarks, SPG-LLM achieves faster grounding (often by orders of magnitude) while delivering comparable or better plan costs in some domains.

Conclusion: LLM-based analysis of PDDL descriptions can effectively reduce grounding overhead while maintaining planning performance, offering a promising approach to address computational bottlenecks in classical planning.

Abstract: Grounding is a critical step in classical planning, yet it often becomes a computational bottleneck due to the exponential growth in grounded actions and atoms as task size increases. Recent advances in partial grounding have addressed this challenge by incrementally grounding only the most promising operators, guided by predictive models. However, these approaches primarily rely on relational features or learned embeddings and do not leverage the textual and structural cues present in PDDL descriptions. We propose SPG-LLM, which uses LLMs to analyze the domain and problem files to heuristically identify potentially irrelevant objects, actions, and predicates prior to grounding, significantly reducing the size of the grounded task. Across seven hard-to-ground benchmarks, SPG-LLM achieves faster grounding-often by orders of magnitude-while delivering comparable or better plan costs in some domains.

</details>


### [276] [Language Models Exhibit Inconsistent Biases Towards Algorithmic Agents and Human Experts](https://arxiv.org/abs/2602.22070)
*Jessica Y. Bo,Lillio Mok,Ashton Anderson*

Main category: cs.AI

TL;DR: LLMs show inconsistent biases toward human vs algorithmic decision-makers: they rate humans as more trustworthy but actually bet on algorithms even when they perform worse.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs weigh information from different sources (human experts vs algorithmic agents) and whether they exhibit algorithm aversion biases similar to humans, which is important for their deployment in decision-making tasks.

Method: Evaluated 8 different LLMs using two experimental paradigms: 1) stated preferences (direct trust ratings for human experts vs algorithms), and 2) revealed preferences (incentivized betting choices after seeing performance examples of both agents).

Result: LLMs gave higher trust ratings to human experts (matching human algorithm aversion), but when making incentivized bets, they disproportionately chose algorithms even when they performed demonstrably worse.

Conclusion: LLMs encode inconsistent biases toward humans and algorithms, and are sensitive to task presentation formats, highlighting important considerations for AI safety and evaluation robustness in high-stakes deployment scenarios.

Abstract: Large language models are increasingly used in decision-making tasks that require them to process information from a variety of sources, including both human experts and other algorithmic agents. How do LLMs weigh the information provided by these different sources? We consider the well-studied phenomenon of algorithm aversion, in which human decision-makers exhibit bias against predictions from algorithms. Drawing upon experimental paradigms from behavioural economics, we evaluate how eightdifferent LLMs delegate decision-making tasks when the delegatee is framed as a human expert or an algorithmic agent. To be inclusive of different evaluation formats, we conduct our study with two task presentations: stated preferences, modeled through direct queries about trust towards either agent, and revealed preferences, modeled through providing in-context examples of the performance of both agents. When prompted to rate the trustworthiness of human experts and algorithms across diverse tasks, LLMs give higher ratings to the human expert, which correlates with prior results from human respondents. However, when shown the performance of a human expert and an algorithm and asked to place an incentivized bet between the two, LLMs disproportionately choose the algorithm, even when it performs demonstrably worse. These discrepant results suggest that LLMs may encode inconsistent biases towards humans and algorithms, which need to be carefully considered when they are deployed in high-stakes scenarios. Furthermore, we discuss the sensitivity of LLMs to task presentation formats that should be broadly scrutinized in evaluation robustness for AI safety.

</details>


### [277] [Petri Net Relaxation for Infeasibility Explanation and Sequential Task Planning](https://arxiv.org/abs/2602.22094)
*Nguyen Cong Nhat Le,John G. Rogers,Claire N. Bonial,Neil T. Dantam*

Main category: cs.AI

TL;DR: Petri net reachability relaxation enables robust invariant synthesis, efficient goal-unreachability detection, and helpful infeasibility explanations for planning systems.


<details>
  <summary>Details</summary>
Motivation: Plans often change due to situational changes or new understanding, and sometimes feasible plans may not exist. Traditional planning approaches focus on efficient one-shot planning in feasible cases rather than handling domain updates or detecting infeasibility.

Method: Proposes a Petri net reachability relaxation approach for robust invariant synthesis, goal-unreachability detection, and infeasibility explanations. Leverages incremental constraint solvers to support goal and constraint updates.

Result: Compared to baselines: produces comparable number of invariants, detects up to 2 times more infeasibilities, performs competitively in one-shot planning, and outperforms in sequential plan updates in tested domains.

Conclusion: The proposed approach effectively addresses the limitations of traditional planning methods by enabling robust invariant synthesis, efficient infeasibility detection, and better handling of plan updates and domain changes.

Abstract: Plans often change due to changes in the situation or our understanding of the situation. Sometimes, a feasible plan may not even exist, and identifying such infeasibilities is useful to determine when requirements need adjustment. Common planning approaches focus on efficient one-shot planning in feasible cases rather than updating domains or detecting infeasibility. We propose a Petri net reachability relaxation to enable robust invariant synthesis, efficient goal-unreachability detection, and helpful infeasibility explanations. We further leverage incremental constraint solvers to support goal and constraint updates. Empirically, compared to baselines, our system produces a comparable number of invariants, detects up to 2 times more infeasibilities, performs competitively in one-shot planning, and outperforms in sequential plan updates in the tested domains.

</details>
