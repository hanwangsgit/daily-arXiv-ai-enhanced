<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 77]
- [cs.AI](#cs.AI) [Total: 51]
- [cs.IT](#cs.IT) [Total: 10]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.LG](#cs.LG) [Total: 146]
- [eess.SP](#eess.SP) [Total: 20]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration](https://arxiv.org/abs/2510.01339)
*Alessio Spagnoletti,Andr√©s Almansa,Marcelo Pereyra*

Main category: cs.CV

TL;DR: LVTINO is a zero-shot video restoration method that uses Video Consistency Models (VCMs) instead of frame-by-frame image diffusion models to achieve temporally consistent reconstructions with high computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods that apply image-based latent diffusion models frame-by-frame for video restoration result in temporally inconsistent reconstructions, failing to capture subtle temporal dependencies in high-definition video.

Method: Leverages Video Consistency Models (VCMs) that distill video latent diffusion models into fast generators capturing temporal causality. Uses a conditioning mechanism that bypasses automatic differentiation and requires only a few neural function evaluations.

Result: Achieves state-of-the-art video reconstruction quality with strong measurement consistency and smooth temporal transitions. Shows significant perceptual improvements over frame-by-frame image LDM methods across diverse video inverse problems.

Conclusion: LVTINO establishes a new benchmark in both reconstruction fidelity and computational efficiency for zero-shot video restoration, demonstrating the superiority of VCM-based priors over image-based approaches.

Abstract: Computational imaging methods increasingly rely on powerful generative
diffusion models to tackle challenging image restoration tasks. In particular,
state-of-the-art zero-shot image inverse solvers leverage distilled
text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy
and perceptual quality with high computational efficiency. However, extending
these advances to high-definition video restoration remains a significant
challenge, due to the need to recover fine spatial detail while capturing
subtle temporal dependencies. Consequently, methods that naively apply
image-based LDM priors on a frame-by-frame basis often result in temporally
inconsistent reconstructions. We address this challenge by leveraging recent
advances in Video Consistency Models (VCMs), which distill video latent
diffusion models into fast generators that explicitly capture temporal
causality. Building on this foundation, we propose LVTINO, the first zero-shot
or plug-and-play inverse solver for high definition video restoration with
priors encoded by VCMs. Our conditioning mechanism bypasses the need for
automatic differentiation and achieves state-of-the-art video reconstruction
quality with only a few neural function evaluations, while ensuring strong
measurement consistency and smooth temporal transitions across frames.
Extensive experiments on a diverse set of video inverse problems show
significant perceptual improvements over current state-of-the-art methods that
apply image LDMs frame by frame, establishing a new benchmark in both
reconstruction fidelity and computational efficiency.

</details>


### [2] [Image Generation Based on Image Style Extraction](https://arxiv.org/abs/2510.01347)
*Shuochen Chang*

Main category: cs.CV

TL;DR: A method for fine-grained style-controlled image generation that extracts style representations from reference images and aligns them with text conditions without modifying the base generative model.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models struggle with precise fine-grained style control using natural language, and stylized reference images are difficult to align with textual conditions in traditional generation approaches.

Method: Three-stage training method using a style encoder and style projection layer to extract style representations from reference images and align them with text representations. Uses a custom Style30k-captions dataset containing image-style label-text description triads.

Result: Enables fine-grained controlled stylized image generation by injecting extracted style representations into the generative model while maintaining its structural framework.

Conclusion: The proposed approach successfully achieves fine-grained style control in image generation by extracting and aligning style representations with text conditions, maximizing the capabilities of pretrained generative models.

Abstract: Image generation based on text-to-image generation models is a task with
practical application scenarios that fine-grained styles cannot be precisely
described and controlled in natural language, while the guidance information of
stylized reference images is difficult to be directly aligned with the textual
conditions of traditional textual guidance generation. This study focuses on
how to maximize the generative capability of the pretrained generative model,
by obtaining fine-grained stylistic representations from a single given
stylistic reference image, and injecting the stylistic representations into the
generative body without changing the structural framework of the downstream
generative model, so as to achieve fine-grained controlled stylized image
generation. In this study, we propose a three-stage training style
extraction-based image generation method, which uses a style encoder and a
style projection layer to align the style representations with the textual
representations to realize fine-grained textual cue-based style guide
generation. In addition, this study constructs the Style30k-captions dataset,
whose samples contain a triad of images, style labels, and text descriptions,
to train the style encoder and style projection layer in this experiment.

</details>


### [3] [EvoStruggle: A Dataset Capturing the Evolution of Struggle across Activities and Skill Levels](https://arxiv.org/abs/2510.01362)
*Shijia Feng,Michael Wray,Walterio Mayol-Cuevas*

Main category: cs.CV

TL;DR: This paper introduces EvoStruggle, a dataset for temporal struggle determination during skill acquisition, featuring 61.68 hours of video recordings from 76 participants performing 18 tasks across 4 activities, with 5 repetitions to capture skill evolution.


<details>
  <summary>Details</summary>
Motivation: Existing manipulation datasets haven't focused on how struggle evolves over time during skill acquisition, which is crucial for understanding learning stages and developing effective assistive systems.

Method: Collected a large dataset with 2,793 videos and 5,385 annotated temporal struggle segments from 76 participants performing 18 tasks across 4 activities (tying knots, origami, tangram puzzles, shuffling cards), with 5 repetitions per task. Defined struggle determination as a temporal action localization task.

Result: Temporal Action Localization models successfully learned to detect struggle cues, achieving 34.56% mAP when generalizing across tasks and 19.24% mAP across activities, showing struggle is a transferable concept across skill-based tasks.

Conclusion: Struggle is a transferable concept across various skill-based tasks, and temporal action localization models can effectively detect struggle, though there's room for improvement in cross-activity generalization.

Abstract: The ability to determine when a person struggles during skill acquisition is
crucial for both optimizing human learning and enabling the development of
effective assistive systems. As skills develop, the type and frequency of
struggles tend to change, and understanding this evolution is key to
determining the user's current stage of learning. However, existing
manipulation datasets have not focused on how struggle evolves over time. In
this work, we collect a dataset for struggle determination, featuring 61.68
hours of video recordings, 2,793 videos, and 5,385 annotated temporal struggle
segments collected from 76 participants. The dataset includes 18 tasks grouped
into four diverse activities -- tying knots, origami, tangram puzzles, and
shuffling cards, representing different task variations. In addition,
participants repeated the same task five times to capture their evolution of
skill. We define the struggle determination problem as a temporal action
localization task, focusing on identifying and precisely localizing struggle
segments with start and end times. Experimental results show that Temporal
Action Localization models can successfully learn to detect struggle cues, even
when evaluated on unseen tasks or activities. The models attain an overall
average mAP of 34.56% when generalizing across tasks and 19.24% across
activities, indicating that struggle is a transferable concept across various
skill-based tasks while still posing challenges for further improvement in
struggle detection. Our dataset is available at
https://github.com/FELIXFENG2019/EvoStruggle.

</details>


### [4] [SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs](https://arxiv.org/abs/2510.01370)
*Abu Bucker Siddik,Diane Oyen,Alexander Most,Michal Kucer,Ayan Biswas*

Main category: cs.CV

TL;DR: SPUS is a compact foundation model using lightweight residual U-Net architecture for solving various PDEs, achieving state-of-the-art generalization with fewer parameters and minimal fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To create a parameter-efficient foundation model for solving PDEs that avoids the computational overhead of large transformer architectures used in existing PDE foundation models.

Method: Uses a lightweight residual U-Net-based architecture with auto-regressive pretraining strategy that replicates numerical solver behavior, pretrained on diverse fluid dynamics PDEs.

Result: Achieves state-of-the-art generalization on 6 challenging unseen downstream PDEs while requiring significantly fewer parameters and minimal fine-tuning data.

Conclusion: SPUS demonstrates the potential of U-Net-based architectures as highly parameter-efficient foundation models for solving diverse PDE systems.

Abstract: We introduce Small PDE U-Net Solver (SPUS), a compact and efficient
foundation model (FM) designed as a unified neural operator for solving a wide
range of partial differential equations (PDEs). Unlike existing
state-of-the-art PDE FMs-primarily based on large complex transformer
architectures with high computational and parameter overhead-SPUS leverages a
lightweight residual U-Net-based architecture that has been largely
underexplored as a foundation model architecture in this domain. To enable
effective learning in this minimalist framework, we utilize a simple yet
powerful auto-regressive pretraining strategy which closely replicates the
behavior of numerical solvers to learn the underlying physics. SPUS is
pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6
challenging unseen downstream PDEs spanning various physical systems.
Experimental results demonstrate that SPUS using residual U-Net based
architecture achieves state-of-the-art generalization on these downstream tasks
while requiring significantly fewer parameters and minimal fine-tuning data,
highlighting its potential as a highly parameter-efficient FM for solving
diverse PDE systems.

</details>


### [5] [DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation](https://arxiv.org/abs/2510.01399)
*Shubhankar Borse,Farzad Farhadzadeh,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: DisCo is a reinforcement learning framework that optimizes identity diversity in multi-human image generation, solving face duplication and identity merging issues in text-to-image models.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models fail on multi-human prompts by duplicating faces, merging identities, and miscounting individuals, creating an identity crisis in generative models.

Method: Uses Group-Relative Policy Optimization (GRPO) with a compositional reward that penalizes facial similarity, discourages identity repetition, enforces accurate person counts, and preserves visual fidelity through human preference scores. Includes a single-stage curriculum for stable training.

Result: Achieves 98.6% Unique Face Accuracy and near-perfect Global Identity Spread on DiverseHumans Testset, surpassing both open-source and proprietary methods while maintaining competitive perceptual quality.

Conclusion: DisCo establishes a scalable, annotation-free solution that resolves the identity crisis in generative models and sets a new benchmark for compositional multi-human generation.

Abstract: State-of-the-art text-to-image models excel at realism but collapse on
multi-human prompts - duplicating faces, merging identities, and miscounting
individuals. We introduce DisCo (Reinforcement with Diversity Constraints), the
first RL-based framework to directly optimize identity diversity in multi-human
generation. DisCo fine-tunes flow-matching models via Group-Relative Policy
Optimization (GRPO) with a compositional reward that (i) penalizes intra-image
facial similarity, (ii) discourages cross-sample identity repetition, (iii)
enforces accurate person counts, and (iv) preserves visual fidelity through
human preference scores. A single-stage curriculum stabilizes training as
complexity scales, requiring no extra annotations. On the DiverseHumans
Testset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global
Identity Spread - surpassing both open-source and proprietary methods (e.g.,
Gemini, GPT-Image) while maintaining competitive perceptual quality. Our
results establish DisCo as a scalable, annotation-free solution that resolves
the long-standing identity crisis in generative models and sets a new benchmark
for compositional multi-human generation.

</details>


### [6] [GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings](https://arxiv.org/abs/2510.01448)
*Angel Daruna,Nicholas Meegan,Han-Pang Chiu,Supun Samarasekera,Rakesh Kumar*

Main category: cs.CV

TL;DR: The paper introduces a novel geographic representation that models the world as a hierarchy of geographic embeddings for visual geo-localization, achieving state-of-the-art performance across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve visual geo-localization by developing learned representations of geography that can effectively align with visual content from query images anywhere on Earth.

Method: Formulates geo-localization as aligning visual representations with learned geographic representations, using hierarchical geographic embeddings and fusing appearance features with semantic segmentation maps.

Result: Achieved improved all-time bests in 22 out of 25 metrics across five benchmark datasets, outperforming prior state-of-the-art methods and recent Large Vision-Language Models.

Conclusion: The combination of geographic and visual representations drives significant performance gains in visual geo-localization tasks.

Abstract: Worldwide visual geo-localization seeks to determine the geographic location
of an image anywhere on Earth using only its visual content. Learned
representations of geography for visual geo-localization remain an active
research topic despite much progress. We formulate geo-localization as aligning
the visual representation of the query image with a learned geographic
representation. Our novel geographic representation explicitly models the world
as a hierarchy of geographic embeddings. Additionally, we introduce an approach
to efficiently fuse the appearance features of the query image with its
semantic segmentation map, forming a robust visual representation. Our main
experiments demonstrate improved all-time bests in 22 out of 25 metrics
measured across five benchmark datasets compared to prior state-of-the-art
(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional
ablation studies support the claim that these gains are primarily driven by the
combination of geographic and visual representations.

</details>


### [7] [Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories](https://arxiv.org/abs/2510.01454)
*Nilay Naharas,Dang Nguyen,Nesihan Bulut,Mohammadhossein Bateni,Vahab Mirrokni,Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: XMAS is a data-efficient instruction tuning method for Large Vision-Language Models that clusters examples based on cross-modal attention matrix trajectories to remove redundancy, enabling 50-85% data reduction while preserving performance.


<details>
  <summary>Details</summary>
Motivation: Data selection methods for Large Vision-Language Models are underexplored, with existing methods failing to outperform random selection at different subset sizes.

Method: XMAS clusters examples based on trajectories of top singular values of attention matrices from fine-tuning a small proxy LVLM, then samples a balanced subset from these clusters to remove redundancy.

Result: XMAS discards 50% of LLaVA-665k and 85% of Vision-Flan datasets while preserving LLaVA-1.5-7B performance on 10 benchmarks and speeding up training by 1.2x, achieving 30% more data reduction than best baseline.

Conclusion: XMAS provides the first principled method for data-efficient LVLM instruction tuning, effectively eliminating redundancy in large-scale training data through attention-based clustering.

Abstract: Data-efficient learning aims to eliminate redundancy in large training
datasets by training models on smaller subsets of the most informative
examples. While data selection has been extensively explored for vision models
and large language models (LLMs), it remains underexplored for Large
Vision-Language Models (LVLMs). Notably, none of existing methods can
outperform random selection at different subset sizes. In this work, we propose
the first principled method for data-efficient instruction tuning of LVLMs. We
prove that examples with similar cross-modal attention matrices during
instruction tuning have similar gradients. Thus, they influence model
parameters in a similar manner and convey the same information to the model
during training. Building on this insight, we propose XMAS, which clusters
examples based on the trajectories of the top singular values of their
attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a
balanced subset from these clusters, XMAS effectively removes redundancy in
large-scale LVLM training data. Extensive experiments show that XMAS can
discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while
fully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and
speeding up its training by 1.2x. This is 30% more data reduction compared to
the best baseline for LLaVA-665k. The project's website can be found at
https://bigml-cs-ucla.github.io/XMAS-project-page/.

</details>


### [8] [Purrception: Variational Flow Matching for Vector-Quantized Image Generation](https://arxiv.org/abs/2510.01478)
*RƒÉzvan-Andrei Mati≈üan,Vincent Tao Hu,Grigory Bartosh,Bj√∂rn Ommer,Cees G. M. Snoek,Max Welling,Jan-Willem van de Meent,Mohammad Mahdi Derakhshani,Floor Eijkelboom*

Main category: cs.CV

TL;DR: Purrception is a variational flow matching method for vector-quantized image generation that combines continuous transport dynamics with explicit categorical supervision over codebook indices.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between continuous flow matching methods and discrete categorical approaches, enabling geometric awareness from continuous methods while maintaining discrete supervision for improved training efficiency.

Method: Adapts Variational Flow Matching to vector-quantized latents by learning categorical posteriors over codebook indices while computing velocity fields in the continuous embedding space.

Result: Training converges faster than both continuous and discrete flow matching baselines while achieving competitive FID scores on ImageNet-1k 256x256 generation, comparable to state-of-the-art models.

Conclusion: Variational Flow Matching can effectively bridge continuous transport and discrete supervision for improved training efficiency in image generation.

Abstract: We introduce Purrception, a variational flow matching approach for
vector-quantized image generation that provides explicit categorical
supervision while maintaining continuous transport dynamics. Our method adapts
Variational Flow Matching to vector-quantized latents by learning categorical
posteriors over codebook indices while computing velocity fields in the
continuous embedding space. This combines the geometric awareness of continuous
methods with the discrete supervision of categorical approaches, enabling
uncertainty quantification over plausible codes and temperature-controlled
generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training
converges faster than both continuous flow matching and discrete flow matching
baselines while achieving competitive FID scores with state-of-the-art models.
This demonstrates that Variational Flow Matching can effectively bridge
continuous transport and discrete supervision for improved training efficiency
in image generation.

</details>


### [9] [AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging](https://arxiv.org/abs/2510.01498)
*Yuxuan Ou,Ning Bi,Jiazhen Pan,Jiancheng Yang,Boliang Yu,Usama Zidan,Regent Lee,Vicente Grau*

Main category: cs.CV

TL;DR: A unified deep learning framework that generates synthetic contrast-enhanced CT from non-contrast CT while simultaneously segmenting aortic lumen and thrombus, outperforming single-task and multi-stage approaches.


<details>
  <summary>Details</summary>
Motivation: To reduce risks associated with iodinated contrast agents in CT scans (nephrotoxicity, allergies, environmental harm) by generating synthetic contrast-enhanced images from non-contrast scans, while addressing limitations of multi-stage pipelines that cause error accumulation.

Method: Integrated conditional diffusion models with multi-task learning for end-to-end joint optimization of image synthesis and anatomical segmentation. Shares encoder and decoder parameters across tasks, uses semi-supervised training for missing labels, and requires no initial predictions.

Result: Achieved PSNR of 25.61 dB for image synthesis (vs 23.80 dB single-task CDM), improved lumen Dice to 0.89 (from 0.87) and thrombus Dice to 0.53 (from 0.48). Reduced lumen diameter MAE to 4.19 mm (from 5.78 mm) and thrombus area error to 33.85% (from 41.45%).

Conclusion: The proposed unified framework effectively reduces contrast agent use while improving both image synthesis quality and anatomical segmentation accuracy for abdominal aortic aneurysm assessment, demonstrating superior performance over existing approaches.

Abstract: While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic
aneurysms (AAA), the required iodinated contrast agents pose significant risks,
including nephrotoxicity, patient allergies, and environmental harm. To reduce
contrast agent use, recent deep learning methods have focused on generating
synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a
multi-stage pipeline that first generates images and then performs
segmentation, which leads to error accumulation and fails to leverage shared
semantic and anatomical structures. To address this, we propose a unified deep
learning framework that generates synthetic CECT images from NCCT scans while
simultaneously segmenting the aortic lumen and thrombus. Our approach
integrates conditional diffusion models (CDM) with multi-task learning,
enabling end-to-end joint optimization of image synthesis and anatomical
segmentation. Unlike previous multitask diffusion models, our approach requires
no initial predictions (e.g., a coarse segmentation mask), shares both encoder
and decoder parameters across tasks, and employs a semi-supervised training
strategy to learn from scans with missing segmentation labels, a common
constraint in real-world clinical data. We evaluated our method on a cohort of
264 patients, where it consistently outperformed state-of-the-art single-task
and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61
dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,
it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus
Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to
more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm
from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to
nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.

</details>


### [10] [From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding](https://arxiv.org/abs/2510.01513)
*Basem Rizk,Joel Walsh,Mark Core,Benjamin Nye*

Main category: cs.CV

TL;DR: A framework for efficiently prototyping multi-modal content analysis pipelines that converts videos into temporal semi-structured data and knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: Multi-modal content analysis is complex, computationally expensive, and requires significant engineering effort. Existing pre-trained models are available but challenging to integrate with complex data like videos.

Method: Develop a framework that combines pre-trained models into pipelines to convert videos into temporal semi-structured data, then translate this into query-able knowledge graphs with frame-level indexing that supports continual learning.

Result: The framework enables dynamic incorporation of new domain-specific knowledge through an interactive medium and supports querying of the analyzed content.

Conclusion: The presented framework addresses the challenges of multi-modal video analysis by providing an efficient prototyping approach that leverages existing models and enables knowledge graph representation with continual learning capabilities.

Abstract: Analysis of multi-modal content can be tricky, computationally expensive, and
require a significant amount of engineering efforts. Lots of work with
pre-trained models on static data is out there, yet fusing these opensource
models and methods with complex data such as videos is relatively challenging.
In this paper, we present a framework that enables efficiently prototyping
pipelines for multi-modal content analysis. We craft a candidate recipe for a
pipeline, marrying a set of pre-trained models, to convert videos into a
temporal semi-structured data format. We translate this structure further to a
frame-level indexed knowledge graph representation that is query-able and
supports continual learning, enabling the dynamic incorporation of new
domain-specific knowledge through an interactive medium.

</details>


### [11] [WALT: Web Agents that Learn Tools](https://arxiv.org/abs/2510.01524)
*Viraj Prabhu,Yutong Dai,Matthew Fernandez,Jing Gu,Krithika Ramakrishnan,Yanqi Luo,Silvio Savarese,Caiming Xiong,Junnan Li,Zeyuan Chen,Ran Xu*

Main category: cs.CV

TL;DR: WALT is a framework that reverse-engineers website functionality into reusable tools, enabling web agents to perform high-level operations like search and filter instead of fragile step-by-step UI interactions.


<details>
  <summary>Details</summary>
Motivation: Current web agents are brittle and rely on step-by-step UI interactions that break under dynamic layouts and long horizons, unlike humans who use high-level website functionality.

Method: WALT reverse-engineers latent website functionality into reusable invocable tools that abstract away low-level execution, spanning discovery, communication, and content management operations.

Result: On VisualWebArena and WebArena, WALT achieves higher success with fewer steps and less LLM-dependent reasoning compared to traditional methods.

Conclusion: WALT establishes a robust and generalizable paradigm for browser automation by shifting computational burden from fragile step-by-step reasoning to reliable tool invocation.

Abstract: Web agents promise to automate complex browser tasks, but current methods
remain brittle -- relying on step-by-step UI interactions and heavy LLM
reasoning that break under dynamic layouts and long horizons. Humans, by
contrast, exploit website-provided functionality through high-level operations
like search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),
a framework that reverse-engineers latent website functionality into reusable
invocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust
implementations of automations already designed into websites -- spanning
discovery (search, filter, sort), communication (post, comment, upvote), and
content management (create, edit, delete). Tools abstract away low-level
execution: instead of reasoning about how to click and type, agents simply call
search(query) or create(listing). This shifts the computational burden from
fragile step-by-step reasoning to reliable tool invocation. On VisualWebArena
and WebArena, WALT achieves higher success with fewer steps and less
LLM-dependent reasoning, establishing a robust and generalizable paradigm for
browser automation.

</details>


### [12] [MATCH: Multi-faceted Adaptive Topo-Consistency for Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2510.01532)
*Meilong Xu,Xiaoling Hu,Shahira Abousamra,Chen Li,Chao Chen*

Main category: cs.CV

TL;DR: A semi-supervised segmentation framework that uses topological consistency across multiple perturbed predictions to preserve meaningful structures in histopathology images, with a novel matching strategy for feature alignment.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of capturing meaningful semantic structures from unlabeled data in histopathology image analysis, where objects are densely distributed and distinguishing biological structures from noise is difficult.

Method: Leverages multiple perturbed predictions through stochastic dropouts and temporal training snapshots, enforcing topological consistency across outputs. Introduces a novel matching strategy that combines spatial overlap with global structural alignment to match topological features without ground truth.

Result: Extensive experiments show the approach effectively reduces topological errors and produces more robust and accurate segmentations essential for reliable downstream analysis.

Conclusion: The proposed framework successfully preserves biologically meaningful structures while filtering out transient artifacts, improving segmentation reliability in histopathology applications.

Abstract: In semi-supervised segmentation, capturing meaningful semantic structures
from unlabeled data is essential. This is particularly challenging in
histopathology image analysis, where objects are densely distributed. To
address this issue, we propose a semi-supervised segmentation framework
designed to robustly identify and preserve relevant topological features. Our
method leverages multiple perturbed predictions obtained through stochastic
dropouts and temporal training snapshots, enforcing topological consistency
across these varied outputs. This consistency mechanism helps distinguish
biologically meaningful structures from transient and noisy artifacts. A key
challenge in this process is to accurately match the corresponding topological
features across the predictions in the absence of ground truth. To overcome
this, we introduce a novel matching strategy that integrates spatial overlap
with global structural alignment, minimizing discrepancies among predictions.
Extensive experiments demonstrate that our approach effectively reduces
topological errors, resulting in more robust and accurate segmentations
essential for reliable downstream analysis. Code is available at
\href{https://github.com/Melon-Xu/MATCH}{https://github.com/Melon-Xu/MATCH}.

</details>


### [13] [Towards Better Optimization For Listwise Preference in Diffusion Models](https://arxiv.org/abs/2510.01540)
*Jiamu Bai,Xin Yu,Meilong Xu,Weitao Lu,Xin Pan,Kiwan Maeng,Daniel Kifer,Jian Wang,Yu Wang*

Main category: cs.CV

TL;DR: Diffusion-LPO is a listwise preference optimization framework for diffusion models that extends DPO to handle ranked image preferences, outperforming pairwise methods in text-to-image generation, editing, and personalization.


<details>
  <summary>Details</summary>
Motivation: Current DPO applications in diffusion models rely on pairwise preferences, but human feedback often contains implicit ranked information that provides more precise preference signals than pairwise comparisons.

Method: Proposes Diffusion-LPO framework that aggregates user feedback into ranked image lists and derives a listwise extension of DPO objective under the Plackett-Luce model, enforcing consistency across entire rankings by encouraging each sample to be preferred over lower-ranked alternatives.

Result: Diffusion-LPO consistently outperforms pairwise DPO baselines across various tasks including text-to-image generation, image editing, and personalized preference alignment, achieving better visual quality and preference alignment.

Conclusion: Listwise preference optimization through Diffusion-LPO provides more effective alignment of diffusion models with human preferences compared to pairwise approaches, leveraging richer ranked feedback information.

Abstract: Reinforcement learning from human feedback (RLHF) has proven effectiveness
for aligning text-to-image (T2I) diffusion models with human preferences.
Although Direct Preference Optimization (DPO) is widely adopted for its
computational efficiency and avoidance of explicit reward modeling, its
applications to diffusion models have primarily relied on pairwise preferences.
The precise optimization of listwise preferences remains largely unaddressed.
In practice, human feedback on image preferences often contains implicit ranked
information, which conveys more precise human preferences than pairwise
comparisons. In this work, we propose Diffusion-LPO, a simple and effective
framework for Listwise Preference Optimization in diffusion models with
listwise data. Given a caption, we aggregate user feedback into a ranked list
of images and derive a listwise extension of the DPO objective under the
Plackett-Luce model. Diffusion-LPO enforces consistency across the entire
ranking by encouraging each sample to be preferred over all of its lower-ranked
alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO
across various tasks, including text-to-image generation, image editing, and
personalized preference alignment. Diffusion-LPO consistently outperforms
pairwise DPO baselines on visual quality and preference alignment.

</details>


### [14] [Growing Visual Generative Capacity for Pre-Trained MLLMs](https://arxiv.org/abs/2510.01546)
*Hanyu Wang,Jiaming Han,Ziyan Yang,Qi Zhao,Shanchuan Lin,Xiangyu Yue,Abhinav Shrivastava,Zhenheng Yang,Hao Chen*

Main category: cs.CV

TL;DR: Bridge is a pure autoregressive unified MLLM that enables both image understanding and generation within a single next-token prediction framework using a Mixture-of-Transformers architecture and semantic-to-pixel discrete representation.


<details>
  <summary>Details</summary>
Motivation: Current unified MLLMs face challenges: hybrid approaches break autoregressive paradigm, while pure autoregressive approaches trade off between semantic alignment and pixel-level fidelity.

Method: Augments pre-trained visual understanding models with generative ability through Mixture-of-Transformers architecture and semantic-to-pixel discrete representation combining compact semantic tokens with fine-grained pixel tokens.

Result: Achieves competitive/superior results in both understanding and generation benchmarks with only 7.9% sequence length increase, requiring less training data and reduced training time.

Conclusion: Bridge successfully unifies visual understanding and generation in a pure autoregressive framework while maintaining strong semantic alignment and visual fidelity.

Abstract: Multimodal large language models (MLLMs) extend the success of language
models to visual understanding, and recent efforts have sought to build unified
MLLMs that support both understanding and generation. However, constructing
such models remains challenging: hybrid approaches combine continuous
embeddings with diffusion or flow-based objectives, producing high-quality
images but breaking the autoregressive paradigm, while pure autoregressive
approaches unify text and image prediction over discrete visual tokens but
often face trade-offs between semantic alignment and pixel-level fidelity. In
this work, we present Bridge, a pure autoregressive unified MLLM that augments
pre-trained visual understanding models with generative ability through a
Mixture-of-Transformers architecture, enabling both image understanding and
generation within a single next-token prediction framework. To further improve
visual generation fidelity, we propose a semantic-to-pixel discrete
representation that integrates compact semantic tokens with fine-grained pixel
tokens, achieving strong language alignment and precise description of visual
details with only a 7.9% increase in sequence length. Extensive experiments
across diverse multimodal benchmarks demonstrate that Bridge achieves
competitive or superior results in both understanding and generation
benchmarks, while requiring less training data and reduced training time
compared to prior unified MLLMs.

</details>


### [15] [Robust Classification of Oral Cancer with Limited Training Data](https://arxiv.org/abs/2510.01547)
*Akshay Bhagwan Sonawane,Lena D. Swamikannan,Lakshman Tamil*

Main category: cs.CV

TL;DR: A hybrid CNN-Bayesian deep learning model for oral cancer classification that uses variational inference for uncertainty quantification, achieving better generalizability on small datasets compared to traditional CNNs.


<details>
  <summary>Details</summary>
Motivation: Oral cancer has high mortality rates, especially in regions with limited healthcare access. Early diagnosis is crucial but challenging due to limited infrastructure and small datasets. Traditional deep learning models are overconfident and require large datasets, which are often unavailable.

Method: Combines convolutional neural network (CNN) with Bayesian deep learning using variational inference for uncertainty quantification. Trained on smartphone-captured photographic color images with small training sets.

Result: Achieved 94% accuracy on similar distribution test data (comparable to traditional CNN). On real-world photographic data with distribution differences, achieved 88% accuracy vs 72.94% for traditional CNNs. Model shows low uncertainty for correct classifications and high uncertainty for misclassifications.

Conclusion: Bayesian inference effectively enhances model reliability and generalizability in data-scarce environments, improving early oral cancer diagnosis capabilities.

Abstract: Oral cancer ranks among the most prevalent cancers globally, with a
particularly high mortality rate in regions lacking adequate healthcare access.
Early diagnosis is crucial for reducing mortality; however, challenges persist
due to limited oral health programs, inadequate infrastructure, and a shortage
of healthcare practitioners. Conventional deep learning models, while
promising, often rely on point estimates, leading to overconfidence and reduced
reliability. Critically, these models require large datasets to mitigate
overfitting and ensure generalizability, an unrealistic demand in settings with
limited training data. To address these issues, we propose a hybrid model that
combines a convolutional neural network (CNN) with Bayesian deep learning for
oral cancer classification using small training sets. This approach employs
variational inference to enhance reliability through uncertainty
quantification. The model was trained on photographic color images captured by
smartphones and evaluated on three distinct test datasets. The proposed method
achieved 94% accuracy on a test dataset with a distribution similar to that of
the training data, comparable to traditional CNN performance. Notably, for
real-world photographic image data, despite limitations and variations
differing from the training dataset, the proposed model demonstrated superior
generalizability, achieving 88% accuracy on diverse datasets compared to 72.94%
for traditional CNNs, even with a smaller dataset. Confidence analysis revealed
that the model exhibits low uncertainty (high confidence) for correctly
classified samples and high uncertainty (low confidence) for misclassified
samples. These results underscore the effectiveness of Bayesian inference in
data-scarce environments in enhancing early oral cancer diagnosis by improving
model reliability and generalizability.

</details>


### [16] [Consistent Assistant Domains Transformer for Source-free Domain Adaptation](https://arxiv.org/abs/2510.01559)
*Renrong Shao,Wei Zhang,Kangyang Luo,Qin Li,and Jun Wang*

Main category: cs.CV

TL;DR: CADTrans is a source-free domain adaptation method that constructs invariable feature representations using assistant domains and multiple consistency strategies to handle hard samples and domain bias.


<details>
  <summary>Details</summary>
Motivation: Current SFDA methods struggle with hard samples and domain bias because they can't access source domain data to obtain deterministic invariable features.

Method: Uses assistant domain module for diversified representations, multiple consistent strategies for invariable features, and conditional multi-kernel max mean discrepancy (CMK-MMD) to align hard samples with easy samples.

Result: Extensive experiments on Office-31, Office-Home, VISDA-C, and DomainNet-126 benchmarks show significant performance improvements.

Conclusion: CADTrans effectively addresses SFDA challenges by constructing domain-consistent invariable feature representations and handling hard samples through assistant domains and conditional alignment strategies.

Abstract: Source-free domain adaptation (SFDA) aims to address the challenge of
adapting to a target domain without accessing the source domain directly.
However, due to the inaccessibility of source domain data, deterministic
invariable features cannot be obtained. Current mainstream methods primarily
focus on evaluating invariant features in the target domain that closely
resemble those in the source domain, subsequently aligning the target domain
with the source domain. However, these methods are susceptible to hard samples
and influenced by domain bias. In this paper, we propose a Consistent Assistant
Domains Transformer for SFDA, abbreviated as CADTrans, which solves the issue
by constructing invariable feature representations of domain consistency.
Concretely, we develop an assistant domain module for CADTrans to obtain
diversified representations from the intermediate aggregated global attentions,
which addresses the limitation of existing methods in adequately representing
diversity. Based on assistant and target domains, invariable feature
representations are obtained by multiple consistent strategies, which can be
used to distinguish easy and hard samples. Finally, to align the hard samples
to the corresponding easy samples, we construct a conditional multi-kernel max
mean discrepancy (CMK-MMD) strategy to distinguish between samples of the same
category and those of different categories. Extensive experiments are conducted
on various benchmarks such as Office-31, Office-Home, VISDA-C, and
DomainNet-126, proving the significant performance improvements achieved by our
proposed approaches. Code is available at
https://github.com/RoryShao/CADTrans.git.

</details>


### [17] [Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations](https://arxiv.org/abs/2510.01576)
*Ricardo Gonzalez Penuela,Felipe Arias-Russi,Victor Capriles*

Main category: cs.CV

TL;DR: MLLMs for BLV users often provide lengthy, irrelevant descriptions. This paper proposes using historical BLV questions from VizWiz-LF dataset to guide MLLMs to generate more contextually relevant descriptions.


<details>
  <summary>Details</summary>
Motivation: Current MLLM applications for BLV users default to comprehensive descriptions regardless of context, leading to inefficient exchanges where users must filter through irrelevant details rather than getting specific information they need.

Method: Developed a system that identifies similar past visual contexts from VizWiz-LF dataset and uses associated historical BLV user questions to guide MLLM generation of more relevant descriptions.

Result: Context-aware descriptions anticipated and answered users' questions in 76.1% of cases (70/92) and were preferred in 54.4% of comparisons (50/92) over context-free descriptions.

Conclusion: Using historical BLV user questions to guide MLLMs significantly improves the relevance and usefulness of visual descriptions for BLV users, providing more targeted information that anticipates their actual needs.

Abstract: Multimodal large language models (MLLMs) have been integrated into visual
interpretation applications to support Blind and Low Vision (BLV) users because
of their accuracy and ability to provide rich, human-like interpretations.
However, these applications often default to comprehensive, lengthy
descriptions regardless of context. This leads to inefficient exchanges, as
users must go through irrelevant details rather than receiving the specific
information they are likely to seek. To deliver more contextually-relevant
information, we developed a system that draws on historical BLV users
questions. When given an image, our system identifies similar past visual
contexts from the VizWiz-LF dataset and uses the associated questions to guide
the MLLM generate descriptions more relevant to BLV users. An evaluation with
three human labelers who revised 92 context-aware and context-free descriptions
showed that context-aware descriptions anticipated and answered users'
questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of
comparisons (50 out of 92). Our paper reviews, and data analysis are publicly
available in a Github repository at
https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .

</details>


### [18] [ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models](https://arxiv.org/abs/2510.01582)
*Krishna Teja Chitty-Venkata,Murali Emani*

Main category: cs.CV

TL;DR: ImageNet-Think is a multimodal reasoning dataset built on 250,000 ImageNet21k images, featuring structured thinking tokens and answers generated by state-of-the-art VLMs to develop models with explicit reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: To aid the development of Vision Language Models (VLMs) with explicit reasoning capabilities and contribute to understanding multimodal reasoning mechanisms.

Method: Created synthetic dataset using GLM-4.1V-9B-Thinking and Kimi-VL-A3B-Thinking-2506 VLMs to generate step-by-step reasoning processes and final answers for each image, with two thinking-answer sequences per image.

Result: A comprehensive dataset with 250,000 images, structured thinking tokens, and corresponding answers that captures multimodal reasoning processes.

Conclusion: The dataset will be publicly available to support research in developing reasoning/thinking multimodal VLMs and advance understanding of multimodal reasoning.

Abstract: We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the
development of Vision Language Models (VLMs) with explicit reasoning
capabilities. Our dataset is built on 250,000 images from ImageNet21k dataset,
providing structured thinking tokens and corresponding answers. Our synthetic
dataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and
Kimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of
thinking-answer sequences, creating a resource for training and evaluating
multimodal reasoning models. We capture the step-by-step reasoning process of
VLMs and the final descriptive answers. Our goal with this dataset is to enable
the development of more robust VLMs while contributing to the broader
understanding of multimodal reasoning mechanisms. The dataset and evaluation
benchmarks will be publicly available to aid research in reasoning/thinking
multimodal VLMs.

</details>


### [19] [NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems](https://arxiv.org/abs/2510.01608)
*Roman Jacome,Romario Gualdr√≥n-Hurtado,Leon Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: NPN is a novel regularization method that uses neural networks to project solutions into low-dimensional null-space structures of sensing matrices, improving interpretability and flexibility in imaging inverse problems.


<details>
  <summary>Details</summary>
Motivation: Traditional priors ignore the task-specific structure of the null-space in imaging inverse problems, leading to suboptimal solutions that don't leverage information orthogonal to what the sensing process can capture.

Method: Proposes Non-Linear Projections of the Null-Space (NPN) - a regularization approach that promotes solutions lying in low-dimensional projections of the sensing matrix's null-space using neural networks, rather than enforcing constraints in the image domain.

Result: NPN priors consistently enhance reconstruction fidelity across diverse imaging inverse problems including compressive sensing, deblurring, super-resolution, CT, and MRI, working effectively with various reconstruction frameworks.

Conclusion: NPN provides interpretable and flexible regularization by focusing on null-space structure, is compatible with existing methods, and offers theoretical guarantees for convergence and accuracy in plug-and-play approaches.

Abstract: Imaging inverse problems aims to recover high-dimensional signals from
undersampled, noisy measurements, a fundamentally ill-posed task with infinite
solutions in the null-space of the sensing operator. To resolve this ambiguity,
prior information is typically incorporated through handcrafted regularizers or
learned models that constrain the solution space. However, these priors
typically ignore the task-specific structure of that null-space. In this work,
we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel
class of regularization that, instead of enforcing structural constraints in
the image domain, promotes solutions that lie in a low-dimensional projection
of the sensing matrix's null-space with a neural network. Our approach has two
key advantages: (1) Interpretability: by focusing on the structure of the
null-space, we design sensing-matrix-specific priors that capture information
orthogonal to the signal components that are fundamentally blind to the sensing
process. (2) Flexibility: NPN is adaptable to various inverse problems,
compatible with existing reconstruction frameworks, and complementary to
conventional image-domain priors. We provide theoretical guarantees on
convergence and reconstruction accuracy when used within plug-and-play methods.
Empirical results across diverse sensing matrices demonstrate that NPN priors
consistently enhance reconstruction fidelity in various imaging inverse
problems, such as compressive sensing, deblurring, super-resolution, computed
tomography, and magnetic resonance imaging, with plug-and-play methods,
unrolling networks, deep image prior, and diffusion models.

</details>


### [20] [Automated Genomic Interpretation via Concept Bottleneck Models for Medical Robotics](https://arxiv.org/abs/2510.01618)
*Zijun Li,Jinchang Zhang,Ming Zhang,Guoyu Lu*

Main category: cs.CV

TL;DR: An automated genomic interpretation system that transforms DNA sequences into interpretable decisions using Chaos Game Representation and Concept Bottleneck Model, with applications in HIV subtype classification and clinical automation.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between interpretable genomic modeling and automated decision-making for medical automation and robotic systems, providing reliable genomic interpretation that can be directly validated against biological priors.

Method: Combines Chaos Game Representation (CGR) with Concept Bottleneck Model (CBM), enforcing predictions through biologically meaningful concepts (GC content, CpG density, k-mer motifs). Includes concept fidelity supervision, prior consistency alignment, KL distribution matching, and uncertainty calibration. Adds cost-aware recommendation layer for decision policies.

Result: Achieves state-of-the-art HIV subtype classification across in-house and LANL datasets, superior concept prediction fidelity, and more favorable cost-benefit trade-offs compared to existing baselines. Reduces unnecessary retests and improves efficiency.

Conclusion: Establishes a reliable foundation for robotic and clinical automation in genomic medicine by providing interpretable genomic modeling that bridges the gap to automated decision-making.

Abstract: We propose an automated genomic interpretation module that transforms raw DNA
sequences into actionable, interpretable decisions suitable for integration
into medical automation and robotic systems. Our framework combines Chaos Game
Representation (CGR) with a Concept Bottleneck Model (CBM), enforcing
predictions to flow through biologically meaningful concepts such as GC
content, CpG density, and k mer motifs. To enhance reliability, we incorporate
concept fidelity supervision, prior consistency alignment, KL distribution
matching, and uncertainty calibration. Beyond accurate classification of HIV
subtypes across both in-house and LANL datasets, our module delivers
interpretable evidence that can be directly validated against biological
priors. A cost aware recommendation layer further translates predictive outputs
into decision policies that balance accuracy, calibration, and clinical
utility, reducing unnecessary retests and improving efficiency. Extensive
experiments demonstrate that the proposed system achieves state of the art
classification performance, superior concept prediction fidelity, and more
favorable cost benefit trade-offs compared to existing baselines. By bridging
the gap between interpretable genomic modeling and automated decision-making,
this work establishes a reliable foundation for robotic and clinical automation
in genomic medicine.

</details>


### [21] [VLA-R1: Enhancing Reasoning in Vision-Language-Action Models](https://arxiv.org/abs/2510.01623)
*Angen Ye,Zeyu Zhang,Boyuan Wang,Xiaofeng Wang,Dapeng Zhang,Zheng Zhu*

Main category: cs.CV

TL;DR: VLA-R1 is a reasoning-enhanced Vision-Language-Action model that integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO) to systematically optimize both reasoning and execution, achieving superior generalization and real-world performance.


<details>
  <summary>Details</summary>
Motivation: Current VLA models lack explicit step-by-step reasoning, emit final actions without considering affordance constraints or geometric relations, and have post-training pipelines that rarely reinforce reasoning quality with weak reward design.

Method: Integrates RLVR with GRPO for systematic optimization, uses verifiable rewards for region alignment, trajectory consistency, and output formatting, and develops VLA-CoT-13K dataset with chain-of-thought supervision aligned with affordance and trajectory annotations.

Result: Extensive evaluations on in-domain, out-of-domain, simulation, and real-robot platforms demonstrate superior generalization and real-world performance compared to prior VLA methods.

Conclusion: VLA-R1 addresses key limitations of current VLA models by enhancing reasoning capabilities through verifiable rewards and systematic optimization, achieving improved performance across diverse evaluation settings.

Abstract: Vision-Language-Action (VLA) models aim to unify perception, language
understanding, and action generation, offering strong cross-task and
cross-scene generalization with broad impact on embodied AI. However, current
VLA models often lack explicit step-by-step reasoning, instead emitting final
actions without considering affordance constraints or geometric relations.
Their post-training pipelines also rarely reinforce reasoning quality, relying
primarily on supervised fine-tuning with weak reward design. To address these
challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates
Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative
Policy Optimization (GRPO) to systematically optimize both reasoning and
execution. Specifically, we design an RLVR-based post-training strategy with
verifiable rewards for region alignment, trajectory consistency, and output
formatting, thereby strengthening reasoning robustness and execution accuracy.
Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides
chain-of-thought supervision explicitly aligned with affordance and trajectory
annotations. Furthermore, extensive evaluations on in-domain, out-of-domain,
simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior
generalization and real-world performance compared to prior VLA methods. We
plan to release the model, code, and dataset following the publication of this
work. Code: https://github.com/GigaAI-research/VLA-R1. Website:
https://gigaai-research.github.io/VLA-R1.

</details>


### [22] [Joint Deblurring and 3D Reconstruction for Macrophotography](https://arxiv.org/abs/2510.01640)
*Yifan Zhao,Liangchen Li,Yuqi Zhou,Kai Wang,Yan Liang,Juyong Zhang*

Main category: cs.CV

TL;DR: Proposes a joint deblurring and 3D reconstruction method for macrophotography that simultaneously optimizes clear 3D models and defocus blur kernels using differentiable rendering.


<details>
  <summary>Details</summary>
Motivation: Macrophotography suffers from severe defocus blur that hinders clear imaging and high-quality 3D reconstruction, while traditional methods require extensive data and annotations.

Method: Uses differentiable rendering to jointly optimize clear 3D models and per-pixel defocus blur kernels from multi-view blurry images in a self-supervised manner.

Result: Achieves high-quality image deblurring and recovers high-fidelity 3D appearance from a small number of multi-view images.

Conclusion: The proposed framework effectively solves the defocus blur problem in macrophotography and enables high-quality 3D reconstruction from limited blurry inputs.

Abstract: Macro lens has the advantages of high resolution and large magnification, and
3D modeling of small and detailed objects can provide richer information.
However, defocus blur in macrophotography is a long-standing problem that
heavily hinders the clear imaging of the captured objects and high-quality 3D
reconstruction of them. Traditional image deblurring methods require a large
number of images and annotations, and there is currently no multi-view 3D
reconstruction method for macrophotography. In this work, we propose a joint
deblurring and 3D reconstruction method for macrophotography. Starting from
multi-view blurry images captured, we jointly optimize the clear 3D model of
the object and the defocus blur kernel of each pixel. The entire framework
adopts a differentiable rendering method to self-supervise the optimization of
the 3D model and the defocus blur kernel. Extensive experiments show that from
a small number of multi-view images, our proposed method can not only achieve
high-quality image deblurring but also recover high-fidelity 3D appearance.

</details>


### [23] [Automated Defect Detection for Mass-Produced Electronic Components Based on YOLO Object Detection Models](https://arxiv.org/abs/2510.01914)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Yen-Ting Liu*

Main category: cs.CV

TL;DR: Automated defect detection system for DIP components using deep learning and ConSinGAN data augmentation, achieving 95.50% accuracy with YOLOv7.


<details>
  <summary>Details</summary>
Motivation: Conventional defect detection is time-consuming and labor-intensive, creating burden on quality inspection personnel and making product quality management difficult.

Method: Used ConSinGAN to generate dataset for training/testing, investigated four YOLO models (v3, v4, v7, v9) with and without ConSinGAN augmentation, and developed SCADA system with sensor architecture.

Result: YOLOv7 with ConSinGAN achieved best performance: 95.50% accuracy, 285 ms detection time, significantly outperforming threshold-based approaches.

Conclusion: The proposed automated defect detection system can be easily established for various defect types and works well even with insufficient defect data.

Abstract: Since the defect detection of conventional industry components is
time-consuming and labor-intensive, it leads to a significant burden on quality
inspection personnel and makes it difficult to manage product quality. In this
paper, we propose an automated defect detection system for the dual in-line
package (DIP) that is widely used in industry, using digital camera optics and
a deep learning (DL)-based model. The two most common defect categories of DIP
are examined: (1) surface defects, and (2) pin-leg defects. However, the lack
of defective component images leads to a challenge for detection tasks. To
solve this problem, the ConSinGAN is used to generate a suitable-sized dataset
for training and testing. Four varieties of the YOLO model are investigated
(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.
The proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in
accuracy of 95.50\%, detection time of 285 ms, and is far superior to
threshold-based approaches. In addition, the supervisory control and data
acquisition (SCADA) system is developed, and the associated sensor architecture
is described. The proposed automated defect detection can be easily established
with numerous types of defects or insufficient defect data.

</details>


### [24] [FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring](https://arxiv.org/abs/2510.01641)
*Xiaoyang Liu,Zhengyan Zhou,Zihang Xu,Jiezhang Cao,Zheng Chen,Yulun Zhang*

Main category: cs.CV

TL;DR: FideDiff is a novel single-step diffusion model for high-fidelity motion deblurring that reformulates deblurring as a diffusion process and trains a consistency model to enable accurate one-step deblurring.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models for deblurring suffer from unbearable inference time and compromised fidelity, limiting their practical application despite their strong generative capabilities compared to CNN and transformer methods.

Method: Reformulates motion deblurring as a diffusion-like process with progressively blurred images, trains a consistency model to align all timesteps to clean images, integrates Kernel ControlNet for blur kernel estimation, and introduces adaptive timestep prediction.

Result: Achieves superior performance on full-reference metrics, surpassing previous diffusion-based methods and matching other state-of-the-art models while enabling fast one-step inference.

Conclusion: FideDiff establishes a new direction for applying pre-trained diffusion models to high-fidelity image restoration and provides a robust baseline for advancing diffusion models in real-world industrial applications.

Abstract: Recent advancements in image motion deblurring, driven by CNNs and
transformers, have made significant progress. Large-scale pre-trained diffusion
models, which are rich in true-world modeling, have shown great promise for
high-quality image restoration tasks such as deblurring, demonstrating stronger
generative capabilities than CNN and transformer-based methods. However,
challenges such as unbearable inference time and compromised fidelity still
limit the full potential of the diffusion models. To address this, we introduce
FideDiff, a novel single-step diffusion model designed for high-fidelity
deblurring. We reformulate motion deblurring as a diffusion-like process where
each timestep represents a progressively blurred image, and we train a
consistency model that aligns all timesteps to the same clean image. By
reconstructing training data with matched blur trajectories, the model learns
temporal consistency, enabling accurate one-step deblurring. We further enhance
model performance by integrating Kernel ControlNet for blur kernel estimation
and introducing adaptive timestep prediction. Our model achieves superior
performance on full-reference metrics, surpassing previous diffusion-based
methods and matching the performance of other state-of-the-art models. FideDiff
offers a new direction for applying pre-trained diffusion models to
high-fidelity image restoration tasks, establishing a robust baseline for
further advancing diffusion models in real-world industrial applications. Our
dataset and code will be available at https://github.com/xyLiu339/FideDiff.

</details>


### [25] [LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze Inscription Recognition](https://arxiv.org/abs/2510.01651)
*Rixin Zhou,Peiqiang Qiu,Qian Zhang,Chuntao Li,Xi Yang*

Main category: cs.CV

TL;DR: This paper presents a new method for automatic bronze inscription recognition, addressing challenges like visual degradation, multi-domain variability, and long-tailed character distribution through a curated large-scale dataset and a two-stage detection-recognition pipeline with LadderMoE architecture.


<details>
  <summary>Details</summary>
Motivation: Bronze inscriptions are crucial for archaeological and historical studies but automatic recognition is difficult due to severe visual degradation, multi-domain variability across photographs/rubbings/tracings, and extremely long-tailed character distribution.

Method: Curated a large-scale BI dataset with 22,454 full-page images and 198,598 annotated characters across 6,658 categories. Developed a two-stage detection-recognition pipeline with LadderMoE - a pretrained CLIP encoder augmented with ladder-style Mixture of Experts adapters for dynamic expert specialization and robustness.

Result: The method substantially outperforms state-of-the-art scene text recognition baselines, achieving superior accuracy across head, mid, and tail categories as well as all acquisition modalities (photographs, rubbings, tracings).

Conclusion: The approach establishes a strong foundation for bronze inscription recognition and downstream archaeological analysis, effectively handling the challenges of heterogeneous domains and rare character classes.

Abstract: Bronze inscriptions (BI), engraved on ritual vessels, constitute a crucial
stage of early Chinese writing and provide indispensable evidence for
archaeological and historical studies. However, automatic BI recognition
remains difficult due to severe visual degradation, multi-domain variability
across photographs, rubbings, and tracings, and an extremely long-tailed
character distribution. To address these challenges, we curate a large-scale BI
dataset comprising 22454 full-page images and 198598 annotated characters
spanning 6658 unique categories, enabling robust cross-domain evaluation.
Building on this resource, we develop a two-stage detection-recognition
pipeline that first localizes inscriptions and then transcribes individual
characters. To handle heterogeneous domains and rare classes, we equip the
pipeline with LadderMoE, which augments a pretrained CLIP encoder with
ladder-style MoE adapters, enabling dynamic expert specialization and stronger
robustness. Comprehensive experiments on single-character and full-page
recognition tasks demonstrate that our method substantially outperforms
state-of-the-art scene text recognition baselines, achieving superior accuracy
across head, mid, and tail categories as well as all acquisition modalities.
These results establish a strong foundation for bronze inscription recognition
and downstream archaeological analysis.

</details>


### [26] [VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming](https://arxiv.org/abs/2510.01660)
*Duy Nguyen,Dat Nguyen*

Main category: cs.CV

TL;DR: VirDA proposes a parameter-efficient domain adaptation method using visual reprogramming layers instead of full backbone fine-tuning, achieving competitive accuracy with significantly fewer trainable parameters.


<details>
  <summary>Details</summary>
Motivation: Existing UDA methods require fine-tuning entire backbones for each source-target pair, leading to linear growth in parameters and storage, preventing backbone reuse across domains.

Method: Prepend domain-specific visual reprogramming layers to produce visual prompts that add textural bias to input images, adapting style to target domains. Optimize with multiple objective functions for intra- and inter-domain distribution differences without modifying backbone parameters.

Result: Achieved 92.8% mean accuracy on Office-31 with only 1.5M trainable parameters, surpassing PDA by +1.6% accuracy using 46% of parameters, and outperforming full-backbone methods while using only 1.7-2.8% of their parameters.

Conclusion: VirDA enables parameter-efficient domain adaptation through visual reprogramming, allowing backbone reuse across domains while maintaining competitive performance with minimal parameter overhead.

Abstract: Existing UDA pipelines fine-tune already well-trained backbone parameters for
every new source-and-target pair, resulting in the number of training
parameters and storage memory growing linearly with each new pair, and also
preventing the reuse of these well-trained backbone parameters.
  Inspired by recent implications that existing backbones have textural biases,
we propose making use of domain-specific textural bias for domain adaptation
via visual reprogramming, namely VirDA.Instead of fine-tuning the full
backbone, VirDA prepends a domain-specific visual reprogramming layer to the
backbone. This layer produces visual prompts that act as an added textural bias
to the input image, adapting its ``style'' to a target domain. To optimize
these visual reprogramming layers, we use multiple objective functions that
optimize the intra- and inter-domain distribution differences when
domain-adapting visual prompts are applied. This process does not require
modifying the backbone parameters, allowing the same backbone to be reused
across different domains.
  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M
trainable parameters. VirDA surpasses PDA, the state-of-the-art
parameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its
parameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans
and FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%
of their trainable parameters. Relative to the strongest current methods
(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only
2.2% and 1.1% accuracy, respectively.

</details>


### [27] [Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery](https://arxiv.org/abs/2510.01662)
*Minh Tran,Maksim Siniukov,Zhangyu Jin,Mohammad Soleymani*

Main category: cs.CV

TL;DR: DFE is an unsupervised facial expression encoding method using RVQ-VAE that outperforms FACS in psychological tasks like stress detection, personality prediction, and depression detection.


<details>
  <summary>Details</summary>
Motivation: Existing facial expression coding systems like FACS have limited coverage and require costly manual annotation, creating a need for scalable, data-driven alternatives.

Method: Uses 3DMM to extract identity-invariant expression features, then encodes them with RVQ-VAE to produce discrete tokens from a shared codebook, each capturing reusable facial deformation patterns.

Result: DFE captures more precise facial behaviors than FACS and outperforms both FACS-based pipelines and modern representation learning models across psychological tasks using simple Bag-of-Words models.

Conclusion: DFE serves as a scalable and effective alternative to FACS for psychological and affective computing applications, covering a wider variety of facial displays.

Abstract: Facial expression analysis is central to understanding human behavior, yet
existing coding systems such as the Facial Action Coding System (FACS) are
constrained by limited coverage and costly manual annotation. In this work, we
introduce Discrete Facial Encoding (DFE), an unsupervised, data-driven
alternative of compact and interpretable dictionary of facial expressions from
3D mesh sequences learned through a Residual Vector Quantized Variational
Autoencoder (RVQ-VAE). Our approach first extracts identity-invariant
expression features from images using a 3D Morphable Model (3DMM), effectively
disentangling factors such as head pose and facial geometry. We then encode
these features using an RVQ-VAE, producing a sequence of discrete tokens from a
shared codebook, where each token captures a specific, reusable facial
deformation pattern that contributes to the overall expression. Through
extensive experiments, we demonstrate that Discrete Facial Encoding captures
more precise facial behaviors than FACS and other facial encoding alternatives.
We evaluate the utility of our representation across three high-level
psychological tasks: stress detection, personality prediction, and depression
detection. Using a simple Bag-of-Words model built on top of the learned
tokens, our system consistently outperforms both FACS-based pipelines and
strong image and video representation learning models such as Masked
Autoencoders. Further analysis reveals that our representation covers a wider
variety of facial displays, highlighting its potential as a scalable and
effective alternative to FACS for psychological and affective computing
applications.

</details>


### [28] [Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale](https://arxiv.org/abs/2510.01665)
*Yongbo Chen,Yanhao Zhang,Shaifali Parashar,Liang Zhao,Shoudong Huang*

Main category: cs.CV

TL;DR: Con-NRSfM is a novel method for non-rigid structure-from-motion under conformal deformations that performs point-wise reconstruction using 2D image warps optimized through a graph-based framework, accurately computes local conformal scale, and employs parallel separable iterative optimization with self-supervised learning for dense 3D reconstruction.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing NRSfM methods that rely on strict assumptions like locally planar surfaces or locally linear deformations, fail to recover conformal scale, and have inseparable depth/conformal scale constraints, leading to inaccurate deformable SLAM.

Method: Uses 2D selected image warps optimized through graph-based framework, decouples depth and conformal scale constraints, employs parallel separable iterative optimization strategy, and incorporates self-supervised learning with encoder-decoder network for dense 3D point cloud generation.

Result: Demonstrated superior performance over existing methods in both synthetic and real datasets, showing improved reconstruction accuracy and robustness in handling conformal deformations.

Conclusion: Con-NRSfM successfully eliminates restrictive assumptions of previous methods, accurately computes local conformal scale, enables precise depth estimation, and provides robust reconstruction for deformable SLAM applications.

Abstract: Non-rigid structure-from-motion (NRSfM), a promising technique for addressing
the mapping challenges in monocular visual deformable simultaneous localization
and mapping (SLAM), has attracted growing attention. We introduce a novel
method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing
isometric deformations as a subset. Our approach performs point-wise
reconstruction using 2D selected image warps optimized through a graph-based
framework. Unlike existing methods that rely on strict assumptions, such as
locally planar surfaces or locally linear deformations, and fail to recover the
conformal scale, our method eliminates these constraints and accurately
computes the local conformal scale. Additionally, our framework decouples
constraints on depth and conformal scale, which are inseparable in other
approaches, enabling more precise depth estimation. To address the sensitivity
of the formulated problem, we employ a parallel separable iterative
optimization strategy. Furthermore, a self-supervised learning framework,
utilizing an encoder-decoder network, is incorporated to generate dense 3D
point clouds with texture. Simulation and experimental results using both
synthetic and real datasets demonstrate that our method surpasses existing
approaches in terms of reconstruction accuracy and robustness. The code for the
proposed method will be made publicly available on the project website:
https://sites.google.com/view/con-nrsfm.

</details>


### [29] [UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction](https://arxiv.org/abs/2510.01669)
*Jin Cao,Hongrui Wu,Ziyong Feng,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: UniVerse is a unified framework that decouples robust 3D reconstruction from inconsistent multi-view images into restoration and reconstruction subtasks, using a video diffusion model to restore consistency before reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing methods for robust 3D reconstruction from inconsistent multi-view images rely heavily on dense observations and complex optimization. The goal is to develop a more generalizable approach that can handle diverse image inconsistencies without requiring dense data.

Method: UniVerse first converts inconsistent images into initial videos, then uses a specially designed video diffusion model to restore them into consistent images by leveraging learned scene priors from large-scale data, and finally reconstructs 3D scenes from the restored images.

Result: Extensive experiments on synthetic and real-world datasets demonstrate strong generalization capability and superior performance in robust reconstruction compared to existing methods. The method also enables style control of reconstructed 3D scenes.

Conclusion: Decoupling robust reconstruction into restoration and reconstruction subtasks simplifies optimization and improves performance. The diffusion model's learned scene priors make the approach applicable to diverse image inconsistencies, offering better generalization than case-by-case degradation modeling.

Abstract: This paper tackles the challenge of robust reconstruction, i.e., the task of
reconstructing a 3D scene from a set of inconsistent multi-view images. Some
recent works have attempted to simultaneously remove image inconsistencies and
perform reconstruction by integrating image degradation modeling into neural 3D
scene representations.However, these methods rely heavily on dense observations
for robustly optimizing model parameters.To address this issue, we propose to
decouple robust reconstruction into two subtasks: restoration and
reconstruction, which naturally simplifies the optimization process.To this
end, we introduce UniVerse, a unified framework for robust reconstruction based
on a video diffusion model. Specifically, UniVerse first converts inconsistent
images into initial videos, then uses a specially designed video diffusion
model to restore them into consistent images, and finally reconstructs the 3D
scenes from these restored images.Compared with case-by-case per-view
degradation modeling, the diffusion model learns a general scene prior from
large-scale data, making it applicable to diverse image
inconsistencies.Extensive experiments on both synthetic and real-world datasets
demonstrate the strong generalization capability and superior performance of
our method in robust reconstruction. Moreover, UniVerse can control the style
of the reconstructed 3D scene. Project page:
https://jin-cao-tma.github.io/UniVerse.github.io/

</details>


### [30] [An Efficient Deep Template Matching and In-Plane Pose Estimation Method via Template-Aware Dynamic Convolution](https://arxiv.org/abs/2510.01678)
*Ke Jia,Ji Zhou,Hanxin Li,Zhigan Zhou,Haojie Chu,Xiaojie Li*

Main category: cs.CV

TL;DR: A lightweight end-to-end framework for template matching that jointly estimates position, rotation, and scaling through geometric regression, achieving high precision and fast inference for industrial applications.


<details>
  <summary>Details</summary>
Motivation: Traditional template matching methods are inefficient under compound transformations, while deep learning approaches lack explicit geometric pose modeling, making them inadequate for real-world deployment in industrial inspection tasks.

Method: Proposes a framework that reformulates template matching as joint localization and geometric regression using Template-Aware Dynamic Convolution Module (TDCM) to inject template features, depthwise separable convolutions for efficiency, and rotation-shear-based augmentation for geometric-annotation-free training.

Result: The 3.07M parameter model achieves high precision with 14ms inference time under compound transformations, demonstrating strong robustness in small-template and multi-object scenarios suitable for real-time industrial deployment.

Conclusion: The proposed framework provides an efficient and practical solution for industrial template matching that explicitly models geometric pose while maintaining lightweight architecture and fast inference capabilities.

Abstract: In industrial inspection and component alignment tasks, template matching
requires efficient estimation of a target's position and geometric state
(rotation and scaling) under complex backgrounds to support precise downstream
operations. Traditional methods rely on exhaustive enumeration of angles and
scales, leading to low efficiency under compound transformations. Meanwhile,
most deep learning-based approaches only estimate similarity scores without
explicitly modeling geometric pose, making them inadequate for real-world
deployment. To overcome these limitations, we propose a lightweight end-to-end
framework that reformulates template matching as joint localization and
geometric regression, outputting the center coordinates, rotation angle, and
independent horizontal and vertical scales. A Template-Aware Dynamic
Convolution Module (TDCM) dynamically injects template features at inference to
guide generalizable matching. The compact network integrates depthwise
separable convolutions and pixel shuffle for efficient matching. To enable
geometric-annotation-free training, we introduce a rotation-shear-based
augmentation strategy with structure-aware pseudo labels. A lightweight
refinement module further improves angle and scale precision via local
optimization. Experiments show our 3.07M model achieves high precision and 14ms
inference under compound transformations. It also demonstrates strong
robustness in small-template and multi-object scenarios, making it highly
suitable for deployment in real-time industrial applications. The code is
available at:https://github.com/ZhouJ6610/PoseMatch-TDCM.

</details>


### [31] [Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning](https://arxiv.org/abs/2510.01681)
*Xuchen Li,Xuzhao Li,Jiahui Gao,Renjie Pi,Shiyu Hu,Wentao Zhang*

Main category: cs.CV

TL;DR: A framework for adaptive pixel reasoning in Vision-Language Models that dynamically determines when to use pixel-level operations based on query difficulty, achieving better performance while reducing unnecessary visual operations.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle with fine-grained visual understanding due to information loss during image encoding and insufficient attention to critical regions. Current pixel-level approaches are inefficient and get distracted by irrelevant details.

Method: Operation-aware supervised fine-tuning followed by rollout-guided reinforcement learning that uses the model's own response feedback to determine when to invoke pixel operations based on query difficulty.

Result: Achieves 73.4% accuracy on HR-Bench 4K with only 20.1% tool usage ratio, improving accuracy while reducing tool usage by 66.5% compared to previous methods.

Conclusion: The adaptive pixel reasoning framework enables VLMs to achieve superior performance on multimodal reasoning tasks while significantly reducing unnecessary visual operations through dynamic operation selection.

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet they
frequently struggle with tasks requiring precise understanding and handling of
fine-grained visual elements. This is mainly due to information loss during
image encoding or insufficient attention to critical regions. Recent work has
shown promise by incorporating pixel-level visual information into the
reasoning process, enabling VLMs to access high-resolution visual details
during their thought process. However, this pixel-level information is often
overused, leading to inefficiency and distraction from irrelevant visual
details. To address these challenges, we propose the first framework for
adaptive pixel reasoning that dynamically determines necessary pixel-level
operations based on the input query. Specifically, we first apply
operation-aware supervised fine-tuning to establish baseline competence in
textual reasoning and visual operations, then design a novel rollout-guided
reinforcement learning framework relying on feedback of the model's own
responses, which enables the VLM to determine when pixel operations should be
invoked based on query difficulty. Experiments on extensive multimodal
reasoning benchmarks show that our model achieves superior performance while
significantly reducing unnecessary visual operations. Impressively, our model
achieves 73.4\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of
only 20.1\%, improving accuracy and simultaneously reducing tool usage by
66.5\% compared to the previous methods.

</details>


### [32] [Uncovering Overconfident Failures in CXR Models via Augmentation-Sensitivity Risk Scoring](https://arxiv.org/abs/2510.01683)
*Han-Jay Shu,Wei-Ning Chiu,Shun-Ting Chang,Meng-Ping Huang,Takeshi Tohyama,Ahram Han,Po-Chih Kuo*

Main category: cs.CV

TL;DR: ASRS framework uses clinically plausible rotations and embedding shifts to identify error-prone chest X-ray cases, improving fairness and safety in medical AI.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for chest radiograph interpretation show uneven accuracy across patient subgroups and hidden failures not reflected in aggregate metrics, requiring better error detection methods.

Method: Augmentation-sensitivity risk scoring (ASRS) applies ¬±15¬∞/¬±30¬∞ rotations and measures embedding shifts using RAD-DINO encoder to stratify samples into stability quartiles.

Result: Highly sensitive cases show substantially lower recall (-0.2 to -0.3) despite high AUROC and confidence, enabling identification of error-prone cases.

Conclusion: ASRS provides a label-free means for selective prediction and clinician review, improving fairness and safety in medical AI systems.

Abstract: Deep learning models achieve strong performance in chest radiograph (CXR)
interpretation, yet fairness and reliability concerns persist. Models often
show uneven accuracy across patient subgroups, leading to hidden failures not
reflected in aggregate metrics. Existing error detection approaches -- based on
confidence calibration or out-of-distribution (OOD) detection -- struggle with
subtle within-distribution errors, while image- and representation-level
consistency-based methods remain underexplored in medical imaging. We propose
an augmentation-sensitivity risk scoring (ASRS) framework to identify
error-prone CXR cases. ASRS applies clinically plausible rotations ($\pm
15^\circ$/$\pm 30^\circ$) and measures embedding shifts with the RAD-DINO
encoder. Sensitivity scores stratify samples into stability quartiles, where
highly sensitive cases show substantially lower recall ($-0.2$ to $-0.3$)
despite high AUROC and confidence. ASRS provides a label-free means for
selective prediction and clinician review, improving fairness and safety in
medical AI.

</details>


### [33] [FreeViS: Training-free Video Stylization with Inconsistent References](https://arxiv.org/abs/2510.01686)
*Jiacong Xu,Yiqun Mei,Ke Zhang,Vishal M. Patel*

Main category: cs.CV

TL;DR: FreeViS is a training-free video stylization framework that generates stylized videos with rich style details and strong temporal coherence by integrating multiple stylized references into a pretrained image-to-video model.


<details>
  <summary>Details</summary>
Motivation: Video stylization is challenging because frame-by-frame image stylization hurts temporal consistency and reduces style richness, while training dedicated video models requires paired data and is computationally expensive.

Method: Integrates multiple stylized references to a pretrained I2V model, uses high-frequency compensation to constrain content layout and motion, and employs flow-based motion cues to preserve style textures in low-saliency regions.

Result: FreeViS delivers higher stylization fidelity and superior temporal consistency, outperforming recent baselines and achieving strong human preference without introducing flickers and stutters.

Conclusion: The training-free pipeline offers a practical and economic solution for high-quality, temporally coherent video stylization.

Abstract: Video stylization plays a key role in content creation, but it remains a
challenging problem. Na\"ively applying image stylization frame-by-frame hurts
temporal consistency and reduces style richness. Alternatively, training a
dedicated video stylization model typically requires paired video data and is
computationally expensive. In this paper, we propose FreeViS, a training-free
video stylization framework that generates stylized videos with rich style
details and strong temporal coherence. Our method integrates multiple stylized
references to a pretrained image-to-video (I2V) model, effectively mitigating
the propagation errors observed in prior works, without introducing flickers
and stutters. In addition, it leverages high-frequency compensation to
constrain the content layout and motion, together with flow-based motion cues
to preserve style textures in low-saliency regions. Through extensive
evaluations, FreeViS delivers higher stylization fidelity and superior temporal
consistency, outperforming recent baselines and achieving strong human
preference. Our training-free pipeline offers a practical and economic solution
for high-quality, temporally coherent video stylization. The code and videos
can be accessed via https://xujiacong.github.io/FreeViS/

</details>


### [34] [MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs](https://arxiv.org/abs/2510.01691)
*Jiyao Liu,Jinjie Wei,Wanying Qu,Chenglong Ma,Junzhi Ning,Yunheng Li,Ying Chen,Xinzhe Luo,Pengcheng Chen,Xin Gao,Ming Hu,Huihui Xu,Xin Wang,Shujian Gao,Dingkang Yang,Zhongying Deng,Jin Ye,Lihao Liu,Junjun He,Ningsheng Xu*

Main category: cs.CV

TL;DR: MedQ-Bench introduces a comprehensive benchmark for medical image quality assessment using Multi-modal Large Language Models, establishing a perception-reasoning paradigm with two complementary tasks across five imaging modalities.


<details>
  <summary>Details</summary>
Motivation: Existing medical IQA approaches are constrained by scalar, score-based metrics and fail to reflect the descriptive, human-like reasoning process central to expert evaluation, creating a gap in clinical AI safety.

Method: MedQ-Bench defines two tasks: MedQ-Perception (low-level perceptual capability via human-curated questions) and MedQ-Reasoning (no-reference and comparison reasoning tasks). The benchmark spans five imaging modalities, 40+ quality attributes, with 2,600 perceptual queries and 708 reasoning assessments using diverse image sources including clinical acquisitions, simulated degradations, and AI-generated images.

Result: Evaluation of 14 state-of-the-art MLLMs shows models exhibit preliminary but unstable perceptual and reasoning skills, with insufficient accuracy for reliable clinical use. Rigorous human-AI alignment validation compared LLM-based judgement with radiologists.

Conclusion: Targeted optimization of MLLMs in medical IQA is needed. MedQ-Bench aims to catalyze further exploration and unlock the untapped potential of MLLMs for medical image quality evaluation.

Abstract: Medical Image Quality Assessment (IQA) serves as the first-mile safety gate
for clinical AI, yet existing approaches remain constrained by scalar,
score-based metrics and fail to reflect the descriptive, human-like reasoning
process central to expert evaluation. To address this gap, we introduce
MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning
paradigm for language-based evaluation of medical image quality with
Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary
tasks: (1) MedQ-Perception, which probes low-level perceptual capability via
human-curated questions on fundamental visual attributes; and (2)
MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,
aligning model evaluation with human-like reasoning on image quality. The
benchmark spans five imaging modalities and over forty quality attributes,
totaling 2,600 perceptual queries and 708 reasoning assessments, covering
diverse image sources including authentic clinical acquisitions, images with
simulated degradations via physics-based reconstructions, and AI-generated
images. To evaluate reasoning ability, we propose a multi-dimensional judging
protocol that assesses model outputs along four complementary axes. We further
conduct rigorous human-AI alignment validation by comparing LLM-based judgement
with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates
that models exhibit preliminary but unstable perceptual and reasoning skills,
with insufficient accuracy for reliable clinical use. These findings highlight
the need for targeted optimization of MLLMs in medical IQA. We hope that
MedQ-Bench will catalyze further exploration and unlock the untapped potential
of MLLMs for medical image quality evaluation.

</details>


### [35] [Holistic Order Prediction in Natural Scenes](https://arxiv.org/abs/2510.01704)
*Pierre Musacchio,Hyunmin Lee,Jaesik Park*

Main category: cs.CV

TL;DR: InstaFormer is a network that predicts full occlusion and depth orderings for all instances in a scene from a single RGB image input, using interactions between object queries and latent mask descriptors.


<details>
  <summary>Details</summary>
Motivation: Existing methods for understanding instance-wise geometries require expensive input formats (category labels, segmentation masks) and have high inference costs (quadratic forward passes).

Method: InstaFormer uses interactions between object queries and latent mask descriptors that semantically represent the same objects while carrying complementary information, enabling holistic order prediction in a single forward pass.

Result: The approach is comprehensively benchmarked and ablated to demonstrate its effectiveness.

Conclusion: InstaFormer mitigates limitations of existing methods by providing occlusion and depth orderings from RGB images with reduced computational cost.

Abstract: Even in controlled settings, understanding instance-wise geometries is a
challenging task for a wide range of visual models. Although specialized
systems exist, modern arts rely on expensive input formats (category labels,
binary segmentation masks) and inference costs (a quadratic amount of forward
passes). We mitigate these limitations by proposing InstaFormer, a network
capable of holistic order prediction. That is, solely given an input RGB image,
InstaFormer returns the full occlusion and depth orderings for all the
instances in the scene in a single forward pass. At its core, InstaFormer
relies on interactions between object queries and latent mask descriptors that
semantically represent the same objects while carrying complementary
information. We comprehensively benchmark and ablate our approach to highlight
its effectiveness. Our code and models are open-source and available at this
URL: https://github.com/SNU-VGILab/InstaOrder.

</details>


### [36] [PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning](https://arxiv.org/abs/2510.01715)
*Raahul Krishna Durairaju,K. Saruladha*

Main category: cs.CV

TL;DR: PyramidStyler is a transformer framework with Pyramidal Positional Encoding that improves neural style transfer by capturing multi-scale details and using reinforcement learning for optimization, achieving faster convergence and better quality.


<details>
  <summary>Details</summary>
Motivation: Existing CNN and transformer-based neural style transfer models struggle with scaling to complex styles and high-resolution inputs efficiently.

Method: Transformer framework with Pyramidal Positional Encoding (hierarchical multi-scale encoding) and reinforcement learning for dynamic optimization.

Result: Reduced content loss by 62.6% (to 2.07) and style loss by 57.4% (to 0.86) after 4000 epochs with 1.39s inference time; further improved to content 2.03 and style 0.75 with RL while maintaining speed (1.40s).

Conclusion: Demonstrates real-time, high-quality artistic rendering with broad applications in media and design.

Abstract: Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based
algorithm, enabling AI-driven artistic image synthesis. However, existing CNN
and transformer-based models struggle to scale efficiently to complex styles
and high-resolution inputs. We introduce PyramidStyler, a transformer framework
with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding
that captures both local details and global context while reducing
computational load. We further incorporate reinforcement learning to
dynamically optimize stylization, accelerating convergence. Trained on
Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to
2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s
inference--and yields further improvements (content 2.03; style 0.75) with
minimal speed penalty (1.40 s) when using RL. These results demonstrate
real-time, high-quality artistic rendering, with broad applications in media
and design.

</details>


### [37] [LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction](https://arxiv.org/abs/2510.01767)
*Sheng-Hsiang Hung,Ting-Yu Yen,Wei-Fang Sun,Simon See,Shih-Hsuan Hung,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: LoBE-GS is a load-balanced and efficient 3D Gaussian Splatting framework that addresses scalability issues in large scenes through depth-aware partitioning, optimization-based load balancing, and lightweight optimization techniques.


<details>
  <summary>Details</summary>
Motivation: Scaling 3D Gaussian Splatting to large unbounded scenes like city blocks is challenging due to memory constraints and inefficient partitioning methods that suffer from load imbalance and high overhead in existing divide-and-conquer approaches.

Method: Introduces depth-aware partitioning for fast preprocessing, optimization-based strategy to balance computational load across blocks, and lightweight techniques including visibility cropping and selective densification to reduce training costs.

Result: Achieves up to 2x faster end-to-end training time compared to state-of-the-art baselines while maintaining reconstruction quality and enabling scalability to scenes that are infeasible with vanilla 3DGS.

Conclusion: LoBE-GS successfully addresses the scalability limitations of 3D Gaussian Splatting for large scenes through efficient load balancing and optimization techniques, making large-scale urban and outdoor scene reconstruction more practical.

Abstract: 3D Gaussian Splatting (3DGS) has established itself as an efficient
representation for real-time, high-fidelity 3D scene reconstruction. However,
scaling 3DGS to large and unbounded scenes such as city blocks remains
difficult. Existing divide-and-conquer methods alleviate memory pressure by
partitioning the scene into blocks, but introduce new bottlenecks: (i)
partitions suffer from severe load imbalance since uniform or heuristic splits
do not reflect actual computational demands, and (ii) coarse-to-fine pipelines
fail to exploit the coarse stage efficiently, often reloading the entire model
and incurring high overhead. In this work, we introduce LoBE-GS, a novel
Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers
the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning
method that reduces preprocessing from hours to minutes, an optimization-based
strategy that balances visible Gaussians -- a strong proxy for computational
load -- across blocks, and two lightweight techniques, visibility cropping and
selective densification, to further reduce training cost. Evaluations on
large-scale urban and outdoor datasets show that LoBE-GS consistently achieves
up to $2\times$ faster end-to-end training time than state-of-the-art
baselines, while maintaining reconstruction quality and enabling scalability to
scenes infeasible with vanilla 3DGS.

</details>


### [38] [Pack and Force Your Memory: Long-form and Consistent Video Generation](https://arxiv.org/abs/2510.01784)
*Xiaofei Wu,Guozhen Zhang,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Xuming He*

Main category: cs.CV

TL;DR: Proposes MemoryPack for long-range dependency modeling and Direct Forcing to reduce error accumulation in long-form video generation.


<details>
  <summary>Details</summary>
Motivation: Address challenges in long-form video generation: capturing long-range dependencies while preventing error accumulation in autoregressive decoding.

Method: MemoryPack: learnable context-retrieval mechanism using text and image as global guidance; Direct Forcing: single-step approximating strategy for better training-inference alignment.

Result: Achieves minute-level temporal consistency, scales gracefully with video length, maintains linear complexity, and reduces error propagation.

Conclusion: MemoryPack and Direct Forcing enhance context consistency and reliability, advancing practical usability of autoregressive video models.

Abstract: Long-form video generation presents a dual challenge: models must capture
long-range dependencies while preventing the error accumulation inherent in
autoregressive decoding. To address these challenges, we make two
contributions. First, for dynamic context modeling, we propose MemoryPack, a
learnable context-retrieval mechanism that leverages both textual and image
information as global guidance to jointly model short- and long-term
dependencies, achieving minute-level temporal consistency. This design scales
gracefully with video length, preserves computational efficiency, and maintains
linear complexity. Second, to mitigate error accumulation, we introduce Direct
Forcing, an efficient single-step approximating strategy that improves
training-inference alignment and thereby curtails error propagation during
inference. Together, MemoryPack and Direct Forcing substantially enhance the
context consistency and reliability of long-form video generation, advancing
the practical usability of autoregressive video models.

</details>


### [39] [Calibrating the Full Predictive Class Distribution of 3D Object Detectors for Autonomous Driving](https://arxiv.org/abs/2510.01829)
*Cornelius Schr√∂der,Marius-Raphael Schl√ºter,Markus Lienkamp*

Main category: cs.CV

TL;DR: This paper addresses confidence calibration for 3D object detectors' classification tasks, proposing metrics and regularization losses to calibrate both dominant and secondary class predictions, with evaluation on CenterPoint, PillarNet and DSVT-Pillar models.


<details>
  <summary>Details</summary>
Motivation: Precise object detection and uncertainty estimation are critical for autonomous systems' self-aware and safe operation, requiring proper confidence calibration in 3D object detectors.

Method: Proposed two auxiliary regularizing loss terms for calibrating either dominant prediction or full prediction vector, evaluated with post-hoc and train-time methods on CenterPoint, PillarNet and DSVT-Pillar detectors.

Result: Combining the full class prediction calibration loss with isotonic regression achieved best calibration for CenterPoint and PillarNet, but DSVT-Pillar couldn't be jointly calibrated for both dominant and secondary predictions using the same method.

Conclusion: Different 3D object detectors require tailored calibration approaches, with the proposed full class prediction calibration method being effective for some models but not universally applicable across all detector architectures.

Abstract: In autonomous systems, precise object detection and uncertainty estimation
are critical for self-aware and safe operation. This work addresses confidence
calibration for the classification task of 3D object detectors. We argue that
it is necessary to regard the calibration of the full predictive confidence
distribution over all classes and deduce a metric which captures the
calibration of dominant and secondary class predictions. We propose two
auxiliary regularizing loss terms which introduce either calibration of the
dominant prediction or the full prediction vector as a training goal. We
evaluate a range of post-hoc and train-time methods for CenterPoint, PillarNet
and DSVT-Pillar and find that combining our loss term, which regularizes for
calibration of the full class prediction, and isotonic regression lead to the
best calibration of CenterPoint and PillarNet with respect to both dominant and
secondary class predictions. We further find that DSVT-Pillar can not be
jointly calibrated for dominant and secondary predictions using the same
method.

</details>


### [40] [Leveraging Prior Knowledge of Diffusion Model for Person Search](https://arxiv.org/abs/2510.01841)
*Giyeol Kim,Sooyoung Yang,Jihyong Oh,Myungjoo Kang,Chanho Eom*

Main category: cs.CV

TL;DR: DiffPS leverages pre-trained diffusion models to address optimization conflicts in person search, achieving state-of-the-art performance on CUHK-SYSU and PRW datasets.


<details>
  <summary>Details</summary>
Motivation: Existing person search methods use ImageNet pre-trained backbones that are suboptimal for capturing spatial context and fine-grained identity cues, and suffer from conflicting optimization objectives between detection and re-identification tasks.

Method: Proposes DiffPS with three modules: Diffusion-Guided Region Proposal Network for person localization, Multi-Scale Frequency Refinement Network to reduce shape bias, and Semantic-Adaptive Feature Aggregation Network to utilize text-aligned diffusion features.

Result: DiffPS achieves new state-of-the-art performance on CUHK-SYSU and PRW datasets.

Conclusion: Leveraging diffusion model priors effectively resolves optimization conflicts in person search and improves both localization and identification performance.

Abstract: Person search aims to jointly perform person detection and re-identification
by localizing and identifying a query person within a gallery of uncropped
scene images. Existing methods predominantly utilize ImageNet pre-trained
backbones, which may be suboptimal for capturing the complex spatial context
and fine-grained identity cues necessary for person search. Moreover, they rely
on a shared backbone feature for both person detection and re-identification,
leading to suboptimal features due to conflicting optimization objectives. In
this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a
novel framework that leverages a pre-trained diffusion model while eliminating
the optimization conflict between two sub-tasks. We analyze key properties of
diffusion priors and propose three specialized modules: (i) Diffusion-Guided
Region Proposal Network (DGRPN) for enhanced person localization, (ii)
Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and
(iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage
text-aligned diffusion features. DiffPS sets a new state-of-the-art on
CUHK-SYSU and PRW.

</details>


### [41] [Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2510.01912)
*Yi Ai,Yuanhao Cai,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: FMU integrates flow matching with deep unfolding for hyperspectral image reconstruction, achieving superior results through generative priors and global consistency enforcement.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral imaging is costly and challenging to reconstruct from compressed measurements, with existing methods suffering from degradation and loss of spectral details.

Method: Proposes Flow-Matching-guided Unfolding network (FMU) that combines flow matching generative priors with deep unfolding framework, introducing a mean velocity loss for global consistency.

Result: Extensive experiments on simulated and real datasets show FMU significantly outperforms existing approaches in reconstruction quality.

Conclusion: FMU successfully leverages interpretability of optimization methods and generative capacity of flow matching for robust hyperspectral image reconstruction.

Abstract: Hyperspectral imaging (HSI) provides rich spatial-spectral information but
remains costly to acquire due to hardware limitations and the difficulty of
reconstructing three-dimensional data from compressed measurements. Although
compressive sensing systems such as CASSI improve efficiency, accurate
reconstruction is still challenged by severe degradation and loss of fine
spectral details. We propose the Flow-Matching-guided Unfolding network (FMU),
which, to our knowledge, is the first to integrate flow matching into HSI
reconstruction by embedding its generative prior within a deep unfolding
framework. To further strengthen the learned dynamics, we introduce a mean
velocity loss that enforces global consistency of the flow, leading to a more
robust and accurate reconstruction. This hybrid design leverages the
interpretability of optimization-based methods and the generative capacity of
flow matching. Extensive experiments on both simulated and real datasets show
that FMU significantly outperforms existing approaches in reconstruction
quality. Code and models will be available at https://github.com/YiAi03/FMU.

</details>


### [42] [Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors](https://arxiv.org/abs/2510.01934)
*Guangyao Zhai,Yue Zhou,Xinyan Deng,Lars Heckler,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: FoundAD is a few-shot anomaly detection method that uses foundation visual encoders and a nonlinear projection operator to identify anomalies by measuring embedding differences from the natural image manifold.


<details>
  <summary>Details</summary>
Motivation: Few-shot anomaly detection is challenging due to limited samples, especially under category-agnostic conditions. Foundation visual encoders can learn general normal image distributions from large-scale pre-training.

Method: Learn a nonlinear projection operator onto the natural image manifold. This operator identifies out-of-distribution regions by measuring embedding differences, leveraging the correlation between anomaly amount and embedding differences.

Result: Achieves competitive performance in multi-class detection while using substantially fewer parameters than prior methods. Validated with multiple foundation encoders including DINOv3.

Conclusion: The approach broadens perspective on foundation features and advances few-shot anomaly detection field by effectively leveraging pre-trained encoders for anomaly identification.

Abstract: Few-shot anomaly detection streamlines and simplifies industrial safety
inspection. However, limited samples make accurate differentiation between
normal and abnormal features challenging, and even more so under
category-agnostic conditions. Large-scale pre-training of foundation visual
encoders has advanced many fields, as the enormous quantity of data helps to
learn the general distribution of normal images. We observe that the anomaly
amount in an image directly correlates with the difference in the learnt
embeddings and utilize this to design a few-shot anomaly detector termed
FoundAD. This is done by learning a nonlinear projection operator onto the
natural image manifold. The simple operator acts as an effective tool for
anomaly detection to characterize and identify out-of-distribution regions in
an image. Extensive experiments show that our approach supports multi-class
detection and achieves competitive performance while using substantially fewer
parameters than prior methods. Backed up by evaluations with multiple
foundation encoders, including fresh DINOv3, we believe this idea broadens the
perspective on foundation features and advances the field of few-shot anomaly
detection.

</details>


### [43] [ClustViT: Clustering-based Token Merging for Semantic Segmentation](https://arxiv.org/abs/2510.01948)
*Fabio Montello,Ronja G√ºldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: ClustViT is a Vision Transformer architecture for semantic segmentation that reduces computational complexity through token clustering and regeneration, achieving significant speedup with comparable accuracy.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers have high accuracy but quadratic attention complexity limits their practical use in real-world robotic systems. Token merging methods work for classification but are less suitable for dense prediction tasks like semantic segmentation.

Method: Proposes ClustViT with a trainable Cluster module that merges similar tokens guided by pseudo-clusters from segmentation masks, and a Regenerator module that restores fine details for downstream processing.

Result: Achieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three datasets while maintaining comparable segmentation accuracy to standard Vision Transformers.

Conclusion: ClustViT effectively addresses the computational complexity of Vision Transformers for dense prediction tasks, making them more practical for real-world robotic applications while preserving segmentation performance.

Abstract: Vision Transformers can achieve high accuracy and strong generalization
across various contexts, but their practical applicability on real-world
robotic systems is limited due to their quadratic attention complexity. Recent
works have focused on dynamically merging tokens according to the image
complexity. Token merging works well for classification but is less suited to
dense prediction. We propose ClustViT, where we expand upon the Vision
Transformer (ViT) backbone and address semantic segmentation. Within our
architecture, a trainable Cluster module merges similar tokens along the
network guided by pseudo-clusters from segmentation masks. Subsequently, a
Regenerator module restores fine details for downstream heads. Our approach
achieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three different
datasets, with comparable segmentation accuracy. Our code and models will be
made publicly available.

</details>


### [44] [Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs](https://arxiv.org/abs/2510.01954)
*Yongyi Su,Haojie Zhang,Shijie Li,Nanqing Liu,Jingyi Liao,Junyi Pan,Yuan Liu,Xiaofen Xing,Chong Sun,Chen Li,Nancy F. Chen,Shuicheng Yan,Xulei Yang,Xun Xu*

Main category: cs.CV

TL;DR: PaDT introduces a unified paradigm for MLLMs to directly generate both textual and visual outputs using Visual Reference Tokens (VRTs) interleaved with text tokens, enabling dense prediction tasks like detection and segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM approaches for vision tasks rely on indirect representations like generating coordinates as text, which limits performance and prevents dense prediction tasks such as segmentation.

Method: PaDT uses Visual Reference Tokens (VRTs) derived from visual patch embeddings, interleaved with LLM's textual tokens. A lightweight decoder transforms outputs into detection, segmentation, and grounding predictions. VRTs are processed independently at each forward pass with dynamic embedding table expansion.

Result: Empirical studies across four visual perception and understanding tasks show PaDT consistently achieves state-of-the-art performance, even compared with significantly larger MLLM models.

Conclusion: PaDT provides a unified framework that enables MLLMs to directly generate diverse visual outputs, overcoming limitations of indirect representations and achieving superior performance in visual perception tasks.

Abstract: Multimodal large language models (MLLMs) have advanced rapidly in recent
years. However, existing approaches for vision tasks often rely on indirect
representations, such as generating coordinates as text for detection, which
limits performance and prevents dense prediction tasks like segmentation. To
overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a
unified paradigm that enables MLLMs to directly generate both textual and
diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),
derived from visual patch embeddings of query images and interleaved seamlessly
with LLM's output textual tokens. A lightweight decoder then transforms LLM's
outputs into detection, segmentation, and grounding predictions. Unlike prior
methods, PaDT processes VRTs independently at each forward pass and dynamically
expands the embedding table, thus improving localization and differentiation
among similar objects. We further tailor a training strategy for PaDT by
randomly selecting VRTs for supervised fine-tuning and introducing a robust
per-token cross-entropy loss. Our empirical studies across four visual
perception and understanding tasks suggest PaDT consistently achieving
state-of-the-art performance, even compared with significantly larger MLLM
models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.

</details>


### [45] [TriAlignXA: An Explainable Trilemma Alignment Framework for Trustworthy Agri-product Grading](https://arxiv.org/abs/2510.01990)
*Jianfei Xie,Ziyang Li*

Main category: cs.CV

TL;DR: This paper addresses trust issues in online fruit/vegetable e-commerce by proposing a 'Trust Pyramid' model and 'Triangular Trust Index' (TTI) to quantify trade-offs in agricultural grading. It introduces TriAlignXA, an explainable AI framework that transforms algorithms from decision-makers to transparent decision-support systems.


<details>
  <summary>Details</summary>
Motivation: The 'trust deficit' in online produce e-commerce arises from the inability to directly perceive product quality digitally. Traditional absolute grading standards face limitations due to the 'impossible triangle' of biological characteristics, timeliness, and economic viability in agricultural products.

Method: Developed the TriAlignXA framework with three engines: Bio-Adaptive Engine for quality description, Timeliness Optimization Engine for efficiency, and Economic Optimization Engine for cost control. Includes 'Pre-Mapping Mechanism' using QR codes to transparently convey quality information through dual-source verification.

Result: Experiments on grading tasks show significantly higher accuracy than baseline models. The framework demonstrates effective balancing capability in addressing the 'impossible triangle' constraints, with empirical evidence and theoretical analysis supporting its performance.

Conclusion: The research provides comprehensive support from theory to practice for building trustworthy online produce ecosystems, establishing a critical pathway from algorithmic decision-making to consumer trust through transparent, explainable AI systems.

Abstract: The 'trust deficit' in online fruit and vegetable e-commerce stems from the
inability of digital transactions to provide direct sensory perception of
product quality. This paper constructs a 'Trust Pyramid' model through
'dual-source verification' of consumer trust. Experiments confirm that quality
is the cornerstone of trust. The study reveals an 'impossible triangle' in
agricultural product grading, comprising biological characteristics,
timeliness, and economic viability, highlighting the limitations of traditional
absolute grading standards. To quantitatively assess this trade-off, we propose
the 'Triangular Trust Index' (TTI). We redefine the role of algorithms from
'decision-makers' to 'providers of transparent decision-making bases',
designing the explainable AI framework--TriAlignXA. This framework supports
trustworthy online transactions within agricultural constraints through
multi-objective optimization. Its core relies on three engines: the
Bio-Adaptive Engine for granular quality description; the Timeliness
Optimization Engine for processing efficiency; and the Economic Optimization
Engine for cost control. Additionally, the "Pre-Mapping Mechanism" encodes
process data into QR codes, transparently conveying quality information.
Experiments on grading tasks demonstrate significantly higher accuracy than
baseline models. Empirical evidence and theoretical analysis verify the
framework's balancing capability in addressing the "impossible triangle". This
research provides comprehensive support--from theory to practice--for building
a trustworthy online produce ecosystem, establishing a critical pathway from
algorithmic decision-making to consumer trust.

</details>


### [46] [4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing](https://arxiv.org/abs/2510.01991)
*Lei Liu,Can Wang,Zhenghao Chen,Dong Xu*

Main category: cs.CV

TL;DR: 4DGS-Craft is a consistent and interactive 4D Gaussian Splatting editing framework that addresses view, temporal, and non-editing region consistency issues while handling complex text instructions through LLM-based intent understanding.


<details>
  <summary>Details</summary>
Motivation: Recent 4DGS editing methods face challenges with view, temporal, and non-editing region consistency, as well as difficulty handling complex text instructions, which limits their practical usability.

Method: Proposes a 4D-aware InstructPix2Pix model with 4D VGGT geometry features, multi-view grid module for iterative refinement, Gaussian selection mechanism for preserving non-edited regions, and LLM-based module for user intent understanding and instruction decomposition.

Result: The framework enables more consistent and controllable 4D scene editing compared to related works, with improved handling of complex user commands through logical operation sequences.

Conclusion: 4DGS-Craft successfully addresses key consistency challenges in 4DGS editing while providing interactive capabilities through LLM-based intent understanding, making it a more practical solution for complex 4D scene editing tasks.

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges
with view, temporal, and non-editing region consistency, as well as with
handling complex text instructions. To address these issues, we propose
4DGS-Craft, a consistent and interactive 4DGS editing framework. We first
introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal
consistency. This model incorporates 4D VGGT geometry features extracted from
the initial scene, enabling it to capture underlying 4D geometric structures
during editing. We further enhance this model with a multi-view grid module
that enforces consistency by iteratively refining multi-view input images while
jointly optimizing the underlying 4D scene. Furthermore, we preserve the
consistency of non-edited regions through a novel Gaussian selection mechanism,
which identifies and optimizes only the Gaussians within the edited regions.
Beyond consistency, facilitating user interaction is also crucial for effective
4DGS editing. Therefore, we design an LLM-based module for user intent
understanding. This module employs a user instruction template to define atomic
editing operations and leverages an LLM for reasoning. As a result, our
framework can interpret user intent and decompose complex instructions into a
logical sequence of atomic operations, enabling it to handle intricate user
commands and further enhance editing performance. Compared to related works,
our approach enables more consistent and controllable 4D scene editing. Our
code will be made available upon acceptance.

</details>


### [47] [Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution](https://arxiv.org/abs/2510.01997)
*Junyu Wu,Jie Tang,Jie Liu,Gangshan Wu*

Main category: cs.CV

TL;DR: Pure-Pass (PP) is a pixel-level masking mechanism that identifies pure pixels and exempts them from expensive computations in image super-resolution, improving efficiency and performance over previous methods like CAMixer.


<details>
  <summary>Details</summary>
Motivation: Deep learning-based image super-resolution methods face computational complexity issues that hinder practical deployment. Existing approaches like CAMixer have limitations including poor adaptability, coarse-grained masking, and spatial inflexibility.

Method: Proposes Pure-Pass (PP), a pixel-level masking mechanism that uses fixed color center points to classify pixels into categories, enabling fine-grained, spatially flexible masking while maintaining adaptive flexibility.

Result: When integrated into ATD-light model, PP-ATD-light achieves superior SR performance with minimal overhead, outperforming CAMixer-ATD-light in reconstruction quality and parameter efficiency while saving similar computation.

Conclusion: Pure-Pass provides an effective pixel-level masking approach that improves computational efficiency and performance in image super-resolution tasks.

Abstract: Image Super-Resolution (SR) aims to reconstruct high-resolution images from
low-resolution counterparts, but the computational complexity of deep
learning-based methods often hinders practical deployment. CAMixer is the
pioneering work to integrate the advantages of existing lightweight SR methods
and proposes a content-aware mixer to route token mixers of varied complexities
according to the difficulty of content recovery. However, several limitations
remain, such as poor adaptability, coarse-grained masking and spatial
inflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking
mechanism that identifies pure pixels and exempts them from expensive
computations. PP utilizes fixed color center points to classify pixels into
distinct categories, enabling fine-grained, spatially flexible masking while
maintaining adaptive flexibility. Integrated into the state-of-the-art
ATD-light model, PP-ATD-light achieves superior SR performance with minimal
overhead, outperforming CAMixer-ATD-light in reconstruction quality and
parameter efficiency when saving a similar amount of computation.

</details>


### [48] [Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework](https://arxiv.org/abs/2510.02001)
*Nanaka Hosokawa,Ryo Takahashi,Tomoya Kitano,Yukihiro Iida,Chisako Muramatsu,Tatsuro Hayashi,Yuta Seino,Xiangrong Zhou,Takeshi Hara,Akitoshi Katsumata,Hiroshi Fujita*

Main category: cs.CV

TL;DR: Using GPT-4o with Self-correction Loop with Structured Output (SLSO) framework to generate jaw cyst findings from dental panoramic radiographs, showing improved accuracy over Chain-of-Thought method.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of automated jaw cyst findings generation from dental panoramic radiographs using multimodal AI capabilities.

Method: Implemented a 10-step SLSO framework including image analysis, structured data generation, tooth number extraction with consistency checking, iterative regeneration for inconsistencies, and finding generation with verification.

Result: SLSO improved accuracy: 66.9% for tooth number, 33.3% for tooth movement, and 28.6% for root resorption. Successful cases achieved consistent output after up to 5 regenerations.

Conclusion: SLSO framework reduces hallucinations and improves accuracy, but has limitations with extensive lesions spanning multiple teeth. Further refinement needed for practical application.

Abstract: In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to
automatically generate jaw cyst findings on dental panoramic radiographs. To
improve accuracy, we constructed a Self-correction Loop with Structured Output
(SLSO) framework and verified its effectiveness. A 10-step process was
implemented for 22 cases of jaw cysts, including image input and analysis,
structured data generation, tooth number extraction and consistency checking,
iterative regeneration when inconsistencies were detected, and finding
generation with subsequent restructuring and consistency verification. A
comparative experiment was conducted using the conventional Chain-of-Thought
(CoT) method across seven evaluation items: transparency, internal structure,
borders, root resorption, tooth movement, relationships with other structures,
and tooth number. The results showed that the proposed SLSO framework improved
output accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates
for tooth number, tooth movement, and root resorption, respectively. In the
successful cases, a consistently structured output was achieved after up to
five regenerations. Although statistical significance was not reached because
of the small size of the dataset, the overall SLSO framework enforced negative
finding descriptions, suppressed hallucinations, and improved tooth number
identification accuracy. However, the accurate identification of extensive
lesions spanning multiple teeth is limited. Nevertheless, further refinement is
required to enhance overall performance and move toward a practical finding
generation system.

</details>


### [49] [LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction](https://arxiv.org/abs/2510.02028)
*Mario Resino,Borja P√©rez,Jaime Godoy,Abdulla Al-Kaff,Fernando Garc√≠a*

Main category: cs.CV

TL;DR: LiLa-Net is a 3D autoencoder that uses LiDAR point clouds from real traffic environments, featuring simplified skip connections and fewer encoder layers for efficient performance while maintaining accurate reconstruction.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient 3D autoencoder architecture that can encode features from LiDAR point clouds in real traffic environments without requiring extensive computational resources like state-of-the-art architectures.

Method: Proposed LiLa-Net with reduced encoder layers and simplified skip connections, leveraging real semi-autonomous vehicle data with Velodyne LiDAR to create an efficient latent space for point cloud reconstruction.

Result: Achieved effective balance between skip connection information and latent encoding, leading to improved reconstruction quality without performance compromise, and demonstrated strong generalization by reconstructing objects unrelated to original traffic environment.

Conclusion: LiLa-Net successfully creates an efficient 3D autoencoder that accurately reconstructs LiDAR point clouds with simplified architecture while maintaining performance and demonstrating good generalization capabilities.

Abstract: This work proposed a 3D autoencoder architecture, named LiLa-Net, which
encodes efficient features from real traffic environments, employing only the
LiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,
equipped with Velodyne LiDAR. The system leverage skip connections concept to
improve the performance without using extensive resources as the
state-of-the-art architectures. Key changes include reducing the number of
encoder layers and simplifying the skip connections, while still producing an
efficient and representative latent space which allows to accurately
reconstruct the original point cloud. Furthermore, an effective balance has
been achieved between the information carried by the skip connections and the
latent encoding, leading to improved reconstruction quality without
compromising performance. Finally, the model demonstrates strong generalization
capabilities, successfully reconstructing objects unrelated to the original
traffic environment.

</details>


### [50] [kabr-tools: Automated Framework for Multi-Species Behavioral Monitoring](https://arxiv.org/abs/2510.02030)
*Jenna Kline,Maksim Kholiavchenko,Samuel Stevens,Nina van Tiel,Alison Zhong,Namrata Banerji,Alec Sheets,Sowbaranika Balasubramaniam,Isla Duporge,Matthew Thompson,Elizabeth Campolongo,Jackson Miliko,Neil Rosser,Tanya Berger-Wolf,Charles V. Stewart,Daniel I. Rubenstein*

Main category: cs.CV

TL;DR: kabr-tools is an open-source package for automated multi-species behavioral monitoring using drone-based video and machine learning to extract behavioral, social, and spatial metrics from wildlife footage.


<details>
  <summary>Details</summary>
Motivation: Traditional field observations are limited in scope, time-consuming, and labor-intensive, hindering the assessment of behavioral responses across landscapes. There's a need for scalable approaches to quantify complex behavioral patterns.

Method: The framework integrates drone-based video with machine learning systems including object detection, tracking, and behavioral classification to generate metrics like time budgets, behavioral transitions, social interactions, habitat associations, and group composition dynamics.

Result: Drone-based observations significantly improved behavioral granularity, reducing visibility loss by 15% and capturing more transitions with higher accuracy. The system analyzed 969 behavioral sequences across three case studies, revealing species-specific behavioral patterns including differences in vigilance responses and spatial segregation in mixed-species herds.

Conclusion: kabr-tools enables automated behavioral monitoring at scale, offering a powerful tool for ecosystem-wide studies that advances conservation, biodiversity research, and ecological monitoring.

Abstract: A comprehensive understanding of animal behavior ecology depends on scalable
approaches to quantify and interpret complex, multidimensional behavioral
patterns. Traditional field observations are often limited in scope,
time-consuming, and labor-intensive, hindering the assessment of behavioral
responses across landscapes. To address this, we present kabr-tools (Kenyan
Animal Behavior Recognition Tools), an open-source package for automated
multi-species behavioral monitoring. This framework integrates drone-based
video with machine learning systems to extract behavioral, social, and spatial
metrics from wildlife footage. Our pipeline leverages object detection,
tracking, and behavioral classification systems to generate key metrics,
including time budgets, behavioral transitions, social interactions, habitat
associations, and group composition dynamics. Compared to ground-based methods,
drone-based observations significantly improved behavioral granularity,
reducing visibility loss by 15% and capturing more transitions with higher
accuracy and continuity. We validate kabr-tools through three case studies,
analyzing 969 behavioral sequences, surpassing the capacity of traditional
methods for data capture and annotation. We found that, like Plains zebras,
vigilance in Grevy's zebras decreases with herd size, but, unlike Plains
zebras, habitat has a negligible impact. Plains and Grevy's zebras exhibit
strong behavioral inertia, with rare transitions to alert behaviors and
observed spatial segregation between Grevy's zebras, Plains zebras, and
giraffes in mixed-species herds. By enabling automated behavioral monitoring at
scale, kabr-tools offers a powerful tool for ecosystem-wide studies, advancing
conservation, biodiversity research, and ecological monitoring.

</details>


### [51] [GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing](https://arxiv.org/abs/2510.02034)
*Mengtian Li,Yunshu Bai,Yimin Chu,Yijun Shen,Zhongmei Li,Weifeng Ge,Zhifeng Xie,Chaofeng Chen*

Main category: cs.CV

TL;DR: GaussianMorphing is a novel framework for semantic-aware 3D shape and texture morphing from multi-view images using mesh-guided 3D Gaussian Splatting and unified deformation strategy.


<details>
  <summary>Details</summary>
Motivation: Previous approaches rely on point clouds or require pre-defined homeomorphic mappings for untextured data, which have limitations for high-fidelity geometry and appearance modeling.

Method: Leverages mesh-guided 3D Gaussian Splatting (3DGS) with unified deformation strategy that anchors 3D Gaussians to reconstructed mesh patches, ensuring geometrically consistent transformations and preserving texture fidelity through topology-aware constraints.

Result: Outperforms prior 2D/3D methods on TexMorph benchmark, reducing color consistency error (ŒîE) by 22.2% and EI by 26.2%.

Conclusion: The framework enables semantic-aware 3D morphing with preserved local detail and global semantic coherence without requiring labeled data.

Abstract: We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images. Previous approaches usually rely
on point clouds or require pre-defined homeomorphic mappings for untextured
data. Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.
The core of our framework is a unified deformation strategy that anchors
3DGaussians to reconstructed mesh patches, ensuring geometrically consistent
transformations while preserving texture fidelity through topology-aware
constraints. In parallel, our framework establishes unsupervised semantic
correspondence by using the mesh topology as a geometric prior and maintains
structural integrity via physically plausible point trajectories. This
integrated approach preserves both local detail and global semantic coherence
throughout the morphing process with out requiring labeled data. On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by
26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

</details>


### [52] [Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers](https://arxiv.org/abs/2510.02043)
*Sahil Bhandary Karnoor,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: InPose: A diffusion-based pose estimation method that uses only rotational measurements with location-guided priors for zero-shot generalization across users.


<details>
  <summary>Details</summary>
Motivation: Existing pose estimation methods using conditional diffusion models with both location and rotation measurements generalize poorly across users due to location measurements being highly influenced by body size variations.

Method: Formulates pose estimation as an inverse problem using a pre-trained diffusion model conditioned only on rotational measurements, guided by a likelihood term derived from measured locations.

Result: The proposed InPose method achieves zero-shot generalization by generatively estimating pose sequences that best explain sparse on-body measurements for any user.

Conclusion: InPose successfully addresses the generalization problem in pose estimation by decoupling rotational conditioning from body-size-dependent location measurements through an inverse problem formulation.

Abstract: Pose estimation refers to tracking a human's full body posture, including
their head, torso, arms, and legs. The problem is challenging in practical
settings where the number of body sensors are limited. Past work has shown
promising results using conditional diffusion models, where the pose prediction
is conditioned on both <location, rotation> measurements from the sensors.
Unfortunately, nearly all these approaches generalize poorly across users,
primarly because location measurements are highly influenced by the body size
of the user. In this paper, we formulate pose estimation as an inverse problem
and design an algorithm capable of zero-shot generalization. Our idea utilizes
a pre-trained diffusion model and conditions it on rotational measurements
alone; the priors from this model are then guided by a likelihood term, derived
from the measured locations. Thus, given any user, our proposed InPose method
generatively estimates the highly likely sequence of poses that best explains
the sparse on-body measurements.

</details>


### [53] [VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation](https://arxiv.org/abs/2510.02086)
*Arman Behnam*

Main category: cs.CV

TL;DR: VGDM is a transformer-driven diffusion model for brain tumor detection and segmentation that combines vision transformers with diffusion processes to improve accuracy and boundary precision.


<details>
  <summary>Details</summary>
Motivation: Convolutional architectures like U-Net have limited capacity to capture long-range dependencies, constraining performance on complex tumor structures. Diffusion models show strong potential for medical image segmentation but need better global contextual reasoning.

Method: Embed a vision transformer at the core of the diffusion process, leveraging global contextual reasoning with iterative denoising. The transformer backbone models spatial relationships across entire MRI volumes while diffusion refinement mitigates voxel-level errors.

Result: Experimental validation on MRI brain tumor datasets shows consistent gains in Dice similarity and Hausdorff distance metrics.

Conclusion: VGDM provides improved robustness and scalability in neuro-oncology, advancing beyond conventional U-Net baselines and demonstrating the potential of transformer-guided diffusion models for tumor segmentation.

Abstract: Accurate detection and segmentation of brain tumors from magnetic resonance
imaging (MRI) are essential for diagnosis, treatment planning, and clinical
monitoring. While convolutional architectures such as U-Net have long been the
backbone of medical image segmentation, their limited capacity to capture
long-range dependencies constrains performance on complex tumor structures.
Recent advances in diffusion models have demonstrated strong potential for
generating high-fidelity medical images and refining segmentation boundaries.
  In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor
Detection and Segmentation framework, a transformer-driven diffusion framework
for brain tumor detection and segmentation. By embedding a vision transformer
at the core of the diffusion process, the model leverages global contextual
reasoning together with iterative denoising to enhance both volumetric accuracy
and boundary precision. The transformer backbone enables more effective
modeling of spatial relationships across entire MRI volumes, while diffusion
refinement mitigates voxel-level errors and recovers fine-grained tumor
details.
  This hybrid design provides a pathway toward improved robustness and
scalability in neuro-oncology, moving beyond conventional U-Net baselines.
Experimental validation on MRI brain tumor datasets demonstrates consistent
gains in Dice similarity and Hausdorff distance, underscoring the potential of
transformer-guided diffusion models to advance the state of the art in tumor
segmentation.

</details>


### [54] [Mapping Historic Urban Footprints in France: Balancing Quality, Scalability and AI Techniques](https://arxiv.org/abs/2510.02097)
*Walid Rabehi,Marion Le Texier,R√©mi Lemoy*

Main category: cs.CV

TL;DR: A deep learning pipeline extracts urban areas from historical French maps (1925-1950) to create the first national-scale urban footprint dataset for this period, achieving 73% accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the lack of nationwide digital urban footprint data for historical urban sprawl analysis in France before the 1970s.

Method: Dual-pass U-Net approach: first pass identifies confusing areas for targeted data augmentation, second pass uses refined dataset and binarized output to reduce radiometric noise. Processed 941 high-resolution tiles on HPC cluster.

Result: Created first open-access national-scale urban footprint dataset for 1925-1950 period with 73% overall accuracy, effectively capturing diverse urban patterns while minimizing artifacts.

Conclusion: Successfully bridged historical data gap by developing scalable deep learning method, releasing code, datasets, and nationwide urban raster to support long-term urbanization research.

Abstract: Quantitative analysis of historical urban sprawl in France before the 1970s
is hindered by the lack of nationwide digital urban footprint data. This study
bridges this gap by developing a scalable deep learning pipeline to extract
urban areas from the Scan Histo historical map series (1925-1950), which
produces the first open-access, national-scale urban footprint dataset for this
pivotal period. Our key innovation is a dual-pass U-Net approach designed to
handle the high radiometric and stylistic complexity of historical maps. The
first pass, trained on an initial dataset, generates a preliminary map that
identifies areas of confusion, such as text and roads, to guide targeted data
augmentation. The second pass uses a refined dataset and the binarized output
of the first model to minimize radiometric noise, which significantly reduces
false positives. Deployed on a high-performance computing cluster, our method
processes 941 high-resolution tiles covering the entirety of metropolitan
France. The final mosaic achieves an overall accuracy of 73%, effectively
capturing diverse urban patterns while overcoming common artifacts like labels
and contour lines. We openly release the code, training datasets, and the
resulting nationwide urban raster to support future research in long-term
urbanization dynamics.

</details>


### [55] [When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based Tracking in Surgical Videos](https://arxiv.org/abs/2510.02100)
*Woowon Jang,Jiwon Im,Juseung Choi,Niki Rashidian,Wesley De Neve,Utku Ozbulak*

Main category: cs.CV

TL;DR: Analysis of point-based tracking failure modes in surgical videos shows it works well for tools but fails for anatomical targets due to tissue similarity and ambiguous boundaries.


<details>
  <summary>Details</summary>
Motivation: To understand the reliability and failure cases of point-based tracking in complex surgical environments, particularly for laparoscopic cholecystectomy videos.

Method: Systematic analysis comparing point-based tracking with segmentation mask initialization for three surgical targets (gallbladder, grasper, L-hook electrocautery) in laparoscopic cholecystectomy videos.

Result: Point-based tracking is competitive for surgical tools but consistently underperforms for anatomical targets due to tissue similarity and ambiguous boundaries.

Conclusion: Provides actionable recommendations for selecting and placing tracking points to improve performance in surgical video analysis, highlighting key factors influencing tracking outcomes.

Abstract: Video object segmentation (VOS) models such as SAM2 offer promising zero-shot
tracking capabilities for surgical videos using minimal user input. Among the
available input types, point-based tracking offers an efficient and low-cost
alternative, yet its reliability and failure cases in complex surgical
environments are not well understood. In this work, we systematically analyze
the failure modes of point-based tracking in laparoscopic cholecystectomy
videos. Focusing on three surgical targets, the gallbladder, grasper, and
L-hook electrocautery, we compare the performance of point-based tracking with
segmentation mask initialization. Our results show that point-based tracking is
competitive for surgical tools but consistently underperforms for anatomical
targets, where tissue similarity and ambiguous boundaries lead to failure.
Through qualitative analysis, we reveal key factors influencing tracking
outcomes and provide several actionable recommendations for selecting and
placing tracking points to improve performance in surgical video analysis.

</details>


### [56] [FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation](https://arxiv.org/abs/2510.02114)
*Ding-Ruei Shen*

Main category: cs.CV

TL;DR: FRIEREN is a federated learning framework that addresses domain shifts in semantic segmentation using vision foundation models and unlabeled client data, without re-accessing source data.


<details>
  <summary>Details</summary>
Motivation: To solve the challenge of domain shifts in federated learning for semantic segmentation when client data is unlabeled, while leveraging modern vision foundation models.

Method: Uses Vision-Language decoder guided by CLIP text embeddings for semantic disambiguation and weak-to-strong consistency learning for robust local training on pseudo-labels.

Result: Achieves competitive performance against established domain generalization and adaptation methods on synthetic-to-real and clear-to-adverse-weather benchmarks.

Conclusion: FRIEREN effectively tackles the FFREEDG task and sets a strong baseline for future research in federated learning with unlabeled client data.

Abstract: Federeated Learning (FL) offers a privacy-preserving solution for Semantic
Segmentation (SS) tasks to adapt to new domains, but faces significant
challenges from these domain shifts, particularly when client data is
unlabeled. However, most existing FL methods unrealistically assume access to
labeled data on remote clients or fail to leverage the power of modern Vision
Foundation Models (VFMs). Here, we propose a novel and challenging task,
FFREEDG, in which a model is pretrained on a server's labeled source dataset
and subsequently trained across clients using only their unlabeled data,
without ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a
framework that leverages the knowledge of a VFM by integrating vision and
language modalities. Our approach employs a Vision-Language decoder guided by
CLIP-based text embeddings to improve semantic disambiguation and uses a
weak-to-strong consistency learning strategy for robust local training on
pseudo-labels. Our experiments on synthetic-to-real and
clear-to-adverse-weather benchmarks demonstrate that our framework effectively
tackles this new task, achieving competitive performance against established
domain generalization and adaptation methods and setting a strong baseline for
future research.

</details>


### [57] [Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting](https://arxiv.org/abs/2510.02155)
*Shu Zou,Xinyu Tian,Lukas Wesemann,Fabian Waschkowski,Zhaoyuan Yang,Jing Zhang*

Main category: cs.CV

TL;DR: ASK-Hint is a structured prompting framework that uses action-centric knowledge to improve video anomaly detection with frozen vision-language models, achieving state-of-the-art performance without training.


<details>
  <summary>Details</summary>
Motivation: Existing prompts for video anomaly detection are too abstract and overlook fine-grained human-object interactions and action semantics that define complex anomalies in surveillance videos.

Method: Organizes prompts into semantically coherent groups (e.g., violence, property crimes, public safety) and formulates fine-grained guiding questions that align model predictions with discriminative visual cues.

Result: Extensive experiments on UCF-Crime and XD-Violence show consistent AUC improvements over prior baselines, achieving state-of-the-art performance compared to both fine-tuned and training-free methods.

Conclusion: ASK-Hint establishes the critical role of prompt granularity and provides a training-free, generalizable solution for explainable video anomaly detection with interpretable reasoning traces.

Abstract: Prompting has emerged as a practical way to adapt frozen vision-language
models (VLMs) for video anomaly detection (VAD). Yet, existing prompts are
often overly abstract, overlooking the fine-grained human-object interactions
or action semantics that define complex anomalies in surveillance videos. We
propose ASK-Hint, a structured prompting framework that leverages
action-centric knowledge to elicit more accurate and interpretable reasoning
from frozen VLMs. Our approach organizes prompts into semantically coherent
groups (e.g. violence, property crimes, public safety) and formulates
fine-grained guiding questions that align model predictions with discriminative
visual cues. Extensive experiments on UCF-Crime and XD-Violence show that
ASK-Hint consistently improves AUC over prior baselines, achieving
state-of-the-art performance compared to both fine-tuned and training-free
methods. Beyond accuracy, our framework provides interpretable reasoning traces
towards anomaly and demonstrates strong generalization across datasets and VLM
backbones. These results highlight the critical role of prompt granularity and
establish ASK-Hint as a new training-free and generalizable solution for
explainable video anomaly detection.

</details>


### [58] [GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2510.02186)
*Weijia Dou,Xu Zhang,Yi Bin,Jian Liu,Bo Peng,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: GeoPurify is a method that uses geometric priors from 3D self-supervised models to purify noisy 2D VLM-generated 3D point features, achieving state-of-the-art 3D semantic segmentation with only 1.5% training data.


<details>
  <summary>Details</summary>
Motivation: Current approaches for transferring 2D VLM features to 3D segmentation face a trade-off: direct projection yields noisy results, while geometric coherence requires expensive training pipelines and large annotated datasets.

Method: Uses a Student Affinity Network to purify 2D VLM-generated 3D features using geometric priors from a 3D self-supervised teacher model, with a Geometry-Guided Pooling module for inference-time denoising.

Result: Achieves or surpasses state-of-the-art performance on major 3D benchmarks while using only about 1.5% of training data.

Conclusion: GeoPurify effectively reconciles 2D semantics with 3D geometric structure by exploiting latent geometric information in noisy features, achieving superior data efficiency without the traditional trade-offs.

Abstract: Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to
3D semantic segmentation expose a persistent trade-off. Directly projecting 2D
features into 3D yields noisy and fragmented predictions, whereas enforcing
geometric coherence necessitates costly training pipelines and large-scale
annotated 3D data. We argue that this limitation stems from the dominant
segmentation-and-matching paradigm, which fails to reconcile 2D semantics with
3D geometric structure. The geometric cues are not eliminated during the
2D-to-3D transfer but remain latent within the noisy and view-aggregated
features. To exploit this property, we propose GeoPurify that applies a small
Student Affinity Network to purify 2D VLM-generated 3D point features using
geometric priors distilled from a 3D self-supervised teacher model. During
inference, we devise a Geometry-Guided Pooling module to further denoise the
point cloud and ensure the semantic and structural consistency. Benefiting from
latent geometric information and the learned affinity network, GeoPurify
effectively mitigates the trade-off and achieves superior data efficiency.
Extensive experiments on major 3D benchmarks demonstrate that GeoPurify
achieves or surpasses state-of-the-art performance while utilizing only about
1.5% of the training data. Our codes and checkpoints are available at
[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).

</details>


### [59] [Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition: A Machine Learning Approach for Small-Scale Farming Applications](https://arxiv.org/abs/2510.02197)
*Emmanuel Nsengiyumvaa,Leonard Niyitegekaa,Eric Umuhoza*

Main category: cs.CV

TL;DR: Noninvasive pig identification using auricular vein patterns achieves 98.12% accuracy with SVM classification, providing cost-effective alternative to physical tags for small-scale farmers.


<details>
  <summary>Details</summary>
Motivation: Current pig identification methods like ear tags and microchips are unreliable, costly, target pure breeds, and impractical for small-scale farmers, creating need for better solution.

Method: Collected 800 ear images from 20 mixed-breed pigs using smartphone with back lighting, developed multistage computer vision pipeline for vein enhancement and feature extraction, used machine learning models for classification.

Result: Support Vector Machines achieved highest accuracy of 98.12% precision across mixed-breed populations, with entire process taking average 8.3 seconds from image to classification.

Conclusion: Auricular vein biometrics provides cost-effective, stress-free animal identification method, demonstrating practicality for digitizing livestock management and extending precision farming benefits to resource-constrained communities.

Abstract: Accurate livestock identification is a cornerstone of modern farming: it
supports health monitoring, breeding programs, and productivity tracking.
However, common pig identification methods, such as ear tags and microchips,
are often unreliable, costly, target pure breeds, and thus impractical for
small-scale farmers. To address this gap, we propose a noninvasive biometric
identification approach that leverages uniqueness of the auricular vein
patterns. To this end, we have collected 800 ear images from 20 mixed-breed
pigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a
standard smartphone and simple back lighting. A multistage computer vision
pipeline was developed to enhance vein visibility, extract structural and
spatial features, and generate biometric signatures. These features were then
classified using machine learning models. Support Vector Machines (SVM)
achieved the highest accuracy: correctly identifying pigs with 98.12% precision
across mixed-breed populations. The entire process from image processing to
classification was completed in an average of 8.3 seconds, demonstrating
feasibility for real-time farm deployment. We believe that by replacing fragile
physical identifiers with permanent biological markers, this system provides
farmers with a cost-effective and stress-free method of animal identification.
More broadly, the findings confirm the practicality of auricular vein
biometrics for digitizing livestock management, reinforcing its potential to
extend the benefits of precision farming to resource-constrained agricultural
communities.

</details>


### [60] [MMDEW: Multipurpose Multiclass Density Estimation in the Wild](https://arxiv.org/abs/2510.02213)
*Villanelle O'Reilly,Jonathan Cox,Georgios Leontidis,Marc Hanheide,Petra Bosilj,James Brown*

Main category: cs.CV

TL;DR: A multicategory counting framework using Twins pyramid vision-transformer with multi-class counting head and Category Focus Module, achieving significant MAE reduction on benchmarks and demonstrating applicability to biodiversity monitoring.


<details>
  <summary>Details</summary>
Motivation: To address object counting in dense and occluded scenes where discrete counting-by-detection methods fail, particularly for multicategory scenarios with inter-category cross-talk issues.

Method: Uses Twins pyramid vision-transformer backbone with specialized multi-class counting head based on multiscale decoding. Includes two-task design with segmentation-based Category Focus Module to suppress inter-category cross-talk during training.

Result: Achieved 33%, 43% and 64% reduction in MAE on VisDrone and iSAID benchmarks compared to prior multicategory crowd-counting approaches. Outperformed YOLOv11 in dense scenes.

Conclusion: The method enables effective multicategory counting in dense scenes and can be applied to new domains like biodiversity monitoring, providing scalable ecological insights for conservation efforts.

Abstract: Density map estimation can be used to estimate object counts in dense and
occluded scenes where discrete counting-by-detection methods fail. We propose a
multicategory counting framework that leverages a Twins pyramid
vision-transformer backbone and a specialised multi-class counting head built
on a state-of-the-art multiscale decoding approach. A two-task design adds a
segmentation-based Category Focus Module, suppressing inter-category cross-talk
at training time. Training and evaluation on the VisDrone and iSAID benchmarks
demonstrates superior performance versus prior multicategory crowd-counting
approaches (33%, 43% and 64% reduction to MAE), and the comparison with YOLOv11
underscores the necessity of crowd counting methods in dense scenes. The
method's regional loss opens up multi-class crowd counting to new domains,
demonstrated through the application to a biodiversity monitoring dataset,
highlighting its capacity to inform conservation efforts and enable scalable
ecological insights.

</details>


### [61] [TempoControl: Temporal Attention Guidance for Text-to-Video Models](https://arxiv.org/abs/2510.02226)
*Shira Schiber,Ofir Lindenbaum,Idan Schwartz*

Main category: cs.CV

TL;DR: TempoControl enables fine-grained temporal control in text-to-video generation by optimizing cross-attention maps to align visual concepts with timing signals during inference, without retraining.


<details>
  <summary>Details</summary>
Motivation: Current generative video models lack fine-grained temporal control, preventing users from specifying when specific visual elements should appear in generated sequences.

Method: Uses cross-attention maps from text-to-video diffusion models and guides timing through a novel optimization approach that aligns temporal shape (correlation), amplifies visibility (energy), and maintains spatial focus (entropy).

Result: Enables precise temporal control while maintaining high video quality and diversity, demonstrated in applications like temporal reordering, action generation, and audio-aligned generation.

Conclusion: TempoControl provides effective temporal alignment of visual concepts during video generation inference without requiring retraining or additional supervision.

Abstract: Recent advances in generative video models have enabled the creation of
high-quality videos based on natural language prompts. However, these models
frequently lack fine-grained temporal control, meaning they do not allow users
to specify when particular visual elements should appear within a generated
sequence. In this work, we introduce TempoControl, a method that allows for
temporal alignment of visual concepts during inference, without requiring
retraining or additional supervision. TempoControl utilizes cross-attention
maps, a key component of text-to-video diffusion models, to guide the timing of
concepts through a novel optimization approach. Our method steers attention
using three complementary principles: aligning its temporal shape with a
control signal (via correlation), amplifying it where visibility is needed (via
energy), and maintaining spatial focus (via entropy). TempoControl allows
precise control over timing while ensuring high video quality and diversity. We
demonstrate its effectiveness across various video generation applications,
including temporal reordering for single and multiple objects, as well as
action and audio-aligned generation.

</details>


### [62] [RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning](https://arxiv.org/abs/2510.02240)
*Sicheng Feng,Kaiwen Tuo,Song Wang,Lingdong Kong,Jianke Zhu,Huan Wang*

Main category: cs.CV

TL;DR: RewardMap is a multi-stage RL framework that addresses sparse reward challenges in fine-grained visual reasoning for MLLMs through difficulty-aware rewards and progressive training from perception to complex reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Advanced MLLMs struggle with spatial reasoning in structured settings like transit maps, and standard RL faces challenges with sparse rewards and unstable optimization.

Method: Constructed ReasonMap-Plus dataset with dense VQA rewards, then proposed RewardMap with difficulty-aware reward design and multi-stage RL scheme that bootstraps from simple perception to complex reasoning.

Result: Models trained with RewardMap achieved 3.47% average improvement across 6 benchmarks spanning spatial reasoning, fine-grained visual reasoning, and general tasks beyond transit maps.

Conclusion: RewardMap effectively enhances MLLMs' visual understanding and reasoning capabilities through its multi-stage RL approach and dense reward signals.

Abstract: Fine-grained visual reasoning remains a core challenge for multimodal large
language models (MLLMs). The recently introduced ReasonMap highlights this gap
by showing that even advanced MLLMs struggle with spatial reasoning in
structured and information-rich settings such as transit maps, a task of clear
practical and scientific importance. However, standard reinforcement learning
(RL) on such tasks is impeded by sparse rewards and unstable optimization. To
address this, we first construct ReasonMap-Plus, an extended dataset that
introduces dense reward signals through Visual Question Answering (VQA) tasks,
enabling effective cold-start training of fine-grained visual understanding
skills. Next, we propose RewardMap, a multi-stage RL framework designed to
improve both visual understanding and reasoning capabilities of MLLMs.
RewardMap incorporates two key designs. First, we introduce a difficulty-aware
reward design that incorporates detail rewards, directly tackling the sparse
rewards while providing richer supervision. Second, we propose a multi-stage RL
scheme that bootstraps training from simple perception to complex reasoning
tasks, offering a more effective cold-start strategy than conventional
Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus
demonstrate that each component of RewardMap contributes to consistent
performance gains, while their combination yields the best results. Moreover,
models trained with RewardMap achieve an average improvement of 3.47% across 6
benchmarks spanning spatial reasoning, fine-grained visual reasoning, and
general tasks beyond transit maps, underscoring enhanced visual understanding
and reasoning capabilities.

</details>


### [63] [DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing](https://arxiv.org/abs/2510.02253)
*Zihan Zhou,Shilin Lu,Shuli Leng,Shaocong Zhang,Zhuming Lian,Xinlei Yu,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: DragFlow is the first framework to effectively use FLUX's strong generative priors for drag-based image editing, overcoming distortions through region-based editing and achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Drag-based image editing has suffered from distortions due to insufficient priors from older models like Stable Diffusion. Newer models like FLUX have stronger priors but haven't been effectively utilized for drag editing, which performs poorly when directly applied to DiT architectures.

Method: DragFlow introduces region-based editing with affine transformations for richer feature supervision, integrates IP-Adapter for subject consistency, uses gradient mask-based hard constraints for background preservation, and employs MLLMs to resolve task ambiguities.

Result: Extensive experiments on DragBench-DR and the novel ReD Bench show DragFlow surpasses both point-based and region-based baselines, setting new state-of-the-art performance in drag-based image editing.

Conclusion: DragFlow successfully harnesses FLUX's strong generative priors for drag-based editing through region-based supervision and multimodal integration, achieving substantial improvements over existing methods.

Abstract: Drag-based image editing has long suffered from distortions in the target
region, largely because the priors of earlier base models, Stable Diffusion,
are insufficient to project optimized latents back onto the natural image
manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow
matching (e.g., SD3.5, FLUX), generative priors have become significantly
stronger, enabling advances across diverse editing tasks. However, drag-based
editing has yet to benefit from these stronger priors. This work proposes the
first framework to effectively harness FLUX's rich prior for drag-based
editing, dubbed DragFlow, achieving substantial gains over baselines. We first
show that directly applying point-based drag editing to DiTs performs poorly:
unlike the highly compressed features of UNets, DiT features are insufficiently
structured to provide reliable guidance for point-wise motion supervision. To
overcome this limitation, DragFlow introduces a region-based editing paradigm,
where affine transformations enable richer and more consistent feature
supervision. Additionally, we integrate pretrained open-domain personalization
adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving
background fidelity through gradient mask-based hard constraints. Multimodal
large language models (MLLMs) are further employed to resolve task ambiguities.
For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)
featuring region-level dragging instructions. Extensive experiments on
DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and
region-based baselines, setting a new state-of-the-art in drag-based image
editing. Code and datasets will be publicly available upon publication.

</details>


### [64] [From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding](https://arxiv.org/abs/2510.02262)
*Guangyu Sun,Archit Singhal,Burak Uzkent,Mubarak Shah,Chen Chen,Garin Kessler*

Main category: cs.CV

TL;DR: F2C proposes using key clips instead of isolated key frames for video understanding in VLMs, with adaptive resolution to maintain fixed token count, achieving significant improvements on long-form video benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current Video LLMs suffer from the 'needle in a haystack' problem where massive visual tokens from raw videos exhaust context windows. Frame-wise selection discards essential temporal dynamics, leading to poor motion and event continuity reasoning.

Method: Extends selection from isolated key frames to key clips (short temporally coherent segments) with adaptive resolution strategy that dynamically balances spatial resolution and clip length to maintain constant token count per video.

Result: Outperforms uniform sampling by 8.1% on Video-MME, 5.6% on LongVideoBench, and 10.3% on MLVU benchmarks. Training-free approach demonstrates importance of preserving temporal coherence.

Conclusion: The work highlights the critical importance of temporal coherence in frame selection and provides a practical pathway for scaling Video LLMs to real-world video understanding applications.

Abstract: Video Large Language Models (VLMs) have achieved remarkable results on a
variety of vision language tasks, yet their practical use is limited by the
"needle in a haystack" problem: the massive number of visual tokens produced
from raw video frames exhausts the model's context window. Existing solutions
alleviate this issue by selecting a sparse set of frames, thereby reducing
token count, but such frame-wise selection discards essential temporal
dynamics, leading to suboptimal reasoning about motion and event continuity. In
this work we systematically explore the impact of temporal information and
demonstrate that extending selection from isolated key frames to key clips,
which are short, temporally coherent segments, improves video understanding. To
maintain a fixed computational budget while accommodating the larger token
footprint of clips, we propose an adaptive resolution strategy that dynamically
balances spatial resolution and clip length, ensuring a constant token count
per video. Experiments on three long-form video benchmarks demonstrate that our
training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and
10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These
results highlight the importance of preserving temporal coherence in frame
selection and provide a practical pathway for scaling Video LLMs to real world
video understanding applications. Project webpage is available at
https://guangyusun.com/f2c .

</details>


### [65] [Paving the Way Towards Kinematic Assessment Using Monocular Video: A Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose Estimators Against Inertial Sensors in Daily Living Activities](https://arxiv.org/abs/2510.02264)
*Mario Medrano-Paredes,Carmen Fern√°ndez-Gonz√°lez,Francisco-Javier D√≠az-Pernas,Hichem Saoudi,Javier Gonz√°lez-Alonso,Mario Mart√≠nez-Zarzuela*

Main category: cs.CV

TL;DR: This study compares monocular video-based 3D human pose estimation models with IMU sensors for movement assessment, finding MotionAGFormer performs best among video models, with both technologies showing viability for out-of-lab kinematic analysis.


<details>
  <summary>Details</summary>
Motivation: Accurate assessment of human movement under real-world conditions is essential for telemedicine, sports science, and rehabilitation, requiring comparison of accessible video-based methods against established IMU-based approaches.

Method: Used VIDIMU dataset with 13 daily activities captured by commodity cameras and 5 IMUs. Compared joint angles from deep learning frameworks (MotionAGFormer, MotionBERT, MMPose, NVIDIA BodyTrack) against IMU data processed through OpenSim inverse kinematics, following Human3.6M format with 17 keypoints.

Result: MotionAGFormer achieved best performance with lowest RMSE (9.27¬∞¬±4.80¬∞), lowest MAE (7.86¬∞¬±4.18¬∞), highest Pearson correlation (0.86¬±0.15), and highest R¬≤ (0.67¬±0.28). Both video and IMU approaches proved viable for out-of-lab kinematic assessment.

Conclusion: Both video- and sensor-based approaches are viable for kinematic assessment, with key trade-offs in costs, accessibility, and precision. Study provides guidelines for developing robust, cost-effective telehealth solutions, though results are limited to healthy subjects and cannot generalize to pathological cohorts.

Abstract: Advances in machine learning and wearable sensors offer new opportunities for
capturing and analyzing human movement outside specialized laboratories.
Accurate assessment of human movement under real-world conditions is essential
for telemedicine, sports science, and rehabilitation. This preclinical
benchmark compares monocular video-based 3D human pose estimation models with
inertial measurement units (IMUs), leveraging the VIDIMU dataset containing a
total of 13 clinically relevant daily activities which were captured using both
commodity video cameras and five IMUs. During this initial study only healthy
subjects were recorded, so results cannot be generalized to pathological
cohorts. Joint angles derived from state-of-the-art deep learning frameworks
(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA
BodyTrack) were evaluated against joint angles computed from IMU data using
OpenSim inverse kinematics following the Human3.6M dataset format with 17
keypoints. Among them, MotionAGFormer demonstrated superior performance,
achieving the lowest overall RMSE ($9.27\deg \pm 4.80\deg$) and MAE ($7.86\deg
\pm 4.18\deg$), as well as the highest Pearson correlation ($0.86 \pm 0.15$)
and the highest coefficient of determination $R^{2}$ ($0.67 \pm 0.28$). The
results reveal that both technologies are viable for out-of-the-lab kinematic
assessment. However, they also highlight key trade-offs between video- and
sensor-based approaches including costs, accessibility, and precision. This
study clarifies where off-the-shelf video models already provide clinically
promising kinematics in healthy adults and where they lag behind IMU-based
estimates while establishing valuable guidelines for researchers and clinicians
seeking to develop robust, cost-effective, and user-friendly solutions for
telehealth and remote patient monitoring.

</details>


### [66] [NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes](https://arxiv.org/abs/2510.02266)
*Shiyi Zhang,Dong Liang,Yihang Zhou*

Main category: cs.CV

TL;DR: NeuroSwift is a diffusion-based method that integrates AutoKL and CLIP adapters for cross-subject visual stimulus reconstruction from fMRI data, achieving state-of-the-art performance with minimal training.


<details>
  <summary>Details</summary>
Motivation: To address challenges in cross-subject visual reconstruction from brain activity, including inter-subject variability and the brain's abstract semantic encoding of complex visual inputs.

Method: Integrates complementary adapters via diffusion: AutoKL for low-level features and CLIP for semantics. Uses Stable Diffusion generated images with COCO captions to train CLIP Adapter. Employs pretraining on one subject followed by fine-tuning only 17% of parameters for new subjects.

Result: Achieves state-of-the-art performance with only one hour of training per subject on lightweight GPUs (three RTX 4090), outperforming existing methods.

Conclusion: NeuroSwift enables efficient and accurate cross-subject visual reconstruction from fMRI data with minimal computational requirements.

Abstract: Reconstructing visual information from brain activity via computer vision
technology provides an intuitive understanding of visual neural mechanisms.
Despite progress in decoding fMRI data with generative models, achieving
accurate cross-subject reconstruction of visual stimuli remains challenging and
computationally demanding. This difficulty arises from inter-subject
variability in neural representations and the brain's abstract encoding of core
semantic features in complex visual inputs. To address these challenges, we
propose NeuroSwift, which integrates complementary adapters via diffusion:
AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter
is trained on Stable Diffusion generated images paired with COCO captions to
emulate higher visual cortex encoding. For cross-subject generalization, we
pretrain on one subject and then fine-tune only 17 percent of parameters (fully
connected layers) for new subjects, while freezing other components. This
enables state-of-the-art performance with only one hour of training per subject
on lightweight GPUs (three RTX 4090), and it outperforms existing methods.

</details>


### [67] [microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification](https://arxiv.org/abs/2510.02270)
*Sathira Silva,Eman Ali,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: microCLIP is a self-training framework that refines CLIP's visual and textual representations for fine-grained image classification using saliency-guided token fusion and LLM-derived classifiers.


<details>
  <summary>Details</summary>
Motivation: CLIP's reliance on coarse global features limits performance on fine-grained classification tasks, and prior approaches overlook spatial precision when aligning LLM descriptions with CLIP.

Method: Uses Saliency-Oriented Attention Pooling (SOAP) to create saliency-guided fine-grained tokens, fuses them with global CLS tokens, and employs a two-headed LLM-derived classifier with Dynamic Knowledge Aggregation for stable pseudo-labeling.

Result: Achieves consistent 2.90% average accuracy gain across 13 fine-grained benchmarks while requiring only light adaptation.

Conclusion: microCLIP effectively uncovers latent fine-grained signals in CLIP through joint refinement of visual and textual representations, enabling improved fine-grained classification performance.

Abstract: Unsupervised adaptation of CLIP-based vision-language models (VLMs) for
fine-grained image classification requires sensitivity to microscopic local
cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse
global features restricts its performance on fine-grained classification tasks.
Prior efforts inject fine-grained knowledge by aligning large language model
(LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approach
overlooks spatial precision. We propose $\textbf{microCLIP}$, a self-training
framework that jointly refines CLIP's visual and textual representations using
fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)
within a lightweight TokenFusion module, which builds a saliency-guided
$\texttt{[FG]}$ token from patch embeddings and fuses it with the global
$\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we
introduce a two-headed LLM-derived classifier: a frozen classifier that, via
multi-view alignment, provides a stable text-based prior for pseudo-labeling,
and a learnable classifier initialized from LLM descriptions and fine-tuned
with TokenFusion. We further develop Dynamic Knowledge Aggregation, which
convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to
iteratively refine pseudo-labels. Together, these components uncover latent
fine-grained signals in CLIP, yielding a consistent $2.90\%$ average accuracy
gain across 13 fine-grained benchmarks while requiring only light adaptation.
Our code is available at https://github.com/sathiiii/microCLIP.

</details>


### [68] [VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL](https://arxiv.org/abs/2510.02282)
*Kyoungjun Park,Yifan Yang,Juheon Yi,Shicheng Zheng,Yifei Shen,Dongqi Han,Caihua Shan,Muhammad Muaz,Lili Qiu*

Main category: cs.CV

TL;DR: VidGuard-R1 is the first video authenticity detector that fine-tunes a multi-modal large language model using group relative policy optimization to provide both accurate AI-generated video detection and interpretable explanations.


<details>
  <summary>Details</summary>
Motivation: Address the urgent need for effective AI-generated video detection tools to mitigate societal risks like misinformation and reputational harm, while ensuring transparency through interpretable explanations for regulators and end users.

Method: Fine-tunes Qwen-VL using group relative policy optimization (GRPO) with two specialized reward models targeting temporal artifacts and generation complexity, trained on a challenging dataset of 140k real and AI-generated videos.

Result: Achieves state-of-the-art zero-shot performance on existing benchmarks, with additional training pushing accuracy above 95%. Produces precise and interpretable rationales behind predictions.

Conclusion: VidGuard-R1 successfully addresses the dual challenge of accurate AI-generated video detection and interpretable reasoning, demonstrating superior performance through MLLM fine-tuning with GRPO.

Abstract: With the rapid advancement of AI-generated videos, there is an urgent need
for effective detection tools to mitigate societal risks such as misinformation
and reputational harm. In addition to accurate classification, it is essential
that detection models provide interpretable explanations to ensure transparency
for regulators and end users. To address these challenges, we introduce
VidGuard-R1, the first video authenticity detector that fine-tunes a
multi-modal large language model (MLLM) using group relative policy
optimization (GRPO). Our model delivers both highly accurate judgments and
insightful reasoning. We curate a challenging dataset of 140k real and
AI-generated videos produced by state-of-the-art generation models, carefully
designing the generation process to maximize discrimination difficulty. We then
fine-tune Qwen-VL using GRPO with two specialized reward models that target
temporal artifacts and generation complexity. Extensive experiments demonstrate
that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing
benchmarks, with additional training pushing accuracy above 95%. Case studies
further show that VidGuard-R1 produces precise and interpretable rationales
behind its predictions. The code is publicly available at
https://VidGuard-R1.github.io.

</details>


### [69] [Self-Forcing++: Towards Minute-Scale High-Quality Video Generation](https://arxiv.org/abs/2510.02283)
*Justin Cui,Jie Wu,Ming Li,Tao Yang,Xiaojie Li,Rui Wang,Andrew Bai,Yuanhao Ban,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: Proposes a method to mitigate quality degradation in long-horizon video generation by using teacher models to guide student models through self-generated long videos, achieving up to 20x longer videos without retraining.


<details>
  <summary>Details</summary>
Motivation: Diffusion models for video generation suffer from high computational costs and quality degradation when extending to long videos due to error compounding in continuous latent space.

Method: Uses teacher models to provide guidance for student models through sampled segments from self-generated long videos, maintaining temporal consistency without recomputing overlapping frames.

Result: Generates videos up to 4 minutes 15 seconds (50x longer than baseline), avoids over-exposure and error-accumulation, and outperforms baselines in fidelity and consistency on benchmarks.

Conclusion: The approach effectively scales video generation length while maintaining quality, demonstrating significant improvements over existing methods without requiring long-video supervision.

Abstract: Diffusion models have revolutionized image and video generation, achieving
unprecedented visual quality. However, their reliance on transformer
architectures incurs prohibitively high computational costs, particularly when
extending generation to long videos. Recent work has explored autoregressive
formulations for long video generation, typically by distilling from
short-horizon bidirectional teachers. Nevertheless, given that teacher models
cannot synthesize long videos, the extrapolation of student models beyond their
training horizon often leads to pronounced quality degradation, arising from
the compounding of errors within the continuous latent space. In this paper, we
propose a simple yet effective approach to mitigate quality degradation in
long-horizon video generation without requiring supervision from long-video
teachers or retraining on long video datasets. Our approach centers on
exploiting the rich knowledge of teacher models to provide guidance for the
student model through sampled segments drawn from self-generated long videos.
Our method maintains temporal consistency while scaling video length by up to
20x beyond teacher's capability, avoiding common issues such as over-exposure
and error-accumulation without recomputing overlapping frames like previous
methods. When scaling up the computation, our method shows the capability of
generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the
maximum span supported by our base model's position embedding and more than 50x
longer than that of our baseline model. Experiments on standard benchmarks and
our proposed improved benchmark demonstrate that our approach substantially
outperforms baseline methods in both fidelity and consistency. Our long-horizon
videos demo can be found at https://self-forcing-plus-plus.github.io/

</details>


### [70] [Learning to Generate Object Interactions with Physics-Guided Video Diffusion](https://arxiv.org/abs/2510.02284)
*David Romero,Ariana Bermudez,Hao Li,Fabio Pizzati,Ivan Laptev*

Main category: cs.CV

TL;DR: KineMask is a physics-guided video generation approach that enables realistic rigid body control and interactions using a two-stage training strategy with video diffusion models.


<details>
  <summary>Details</summary>
Motivation: Current video generation models struggle with physically plausible object interactions and lack physics-grounded control mechanisms, limiting their use as world simulators for robotics and embodied decision making.

Method: Two-stage training strategy that gradually removes future motion supervision via object masks, training video diffusion models on synthetic scenes and integrating low-level motion control with high-level textual conditioning.

Result: Significant improvements in object interactions in real scenes, strong improvements over recent models of comparable size, and effective synthesis of complex dynamical phenomena.

Conclusion: KineMask demonstrates the complementary roles of low- and high-level conditioning in video diffusion models, achieving realistic physics-guided video generation with improved object interactions.

Abstract: Recent models for video generation have achieved remarkable progress and are
now deployed in film, social media production, and advertising. Beyond their
creative potential, such models also hold promise as world simulators for
robotics and embodied decision making. Despite strong advances, however,
current approaches still struggle to generate physically plausible object
interactions and lack physics-grounded control mechanisms. To address this
limitation, we introduce KineMask, an approach for physics-guided video
generation that enables realistic rigid body control, interactions, and
effects. Given a single image and a specified object velocity, our method
generates videos with inferred motions and future object interactions. We
propose a two-stage training strategy that gradually removes future motion
supervision via object masks. Using this strategy we train video diffusion
models (VDMs) on synthetic scenes of simple interactions and demonstrate
significant improvements of object interactions in real scenes. Furthermore,
KineMask integrates low-level motion control with high-level textual
conditioning via predictive scene descriptions, leading to effective support
for synthesis of complex dynamical phenomena. Extensive experiments show that
KineMask achieves strong improvements over recent models of comparable size.
Ablation studies further highlight the complementary roles of low- and
high-level conditioning in VDMs. Our code, model, and data will be made
publicly available.

</details>


### [71] [MultiModal Action Conditioned Video Generation](https://arxiv.org/abs/2510.02287)
*Yichen Li,Antonio Torralba*

Main category: cs.CV

TL;DR: The paper introduces fine-grained multimodal actions for household robots, incorporating proprioception, kinesthesia, force haptics, and muscle activation to enable precise control that text-conditioned models struggle with.


<details>
  <summary>Details</summary>
Motivation: Current video models fail as world models due to lack of fine-grained control needed for delicate tasks and urgent situations in household robotics.

Method: Developed a feature learning paradigm that aligns multimodal senses while preserving unique information from each modality, with a regularization scheme to enhance causality of action trajectory features.

Result: Experiments show incorporating multimodal senses improves simulation accuracy and reduces temporal drift, with ablation studies confirming effectiveness.

Conclusion: The proposed multimodal approach enables fine-grained interactions that are difficult for text-conditioned generative models, demonstrating practical effectiveness for household robotics.

Abstract: Current video models fail as world model as they lack fine-graiend control.
General-purpose household robots require real-time fine motor control to handle
delicate tasks and urgent situations. In this work, we introduce fine-grained
multimodal actions to capture such precise control. We consider senses of
proprioception, kinesthesia, force haptics, and muscle activation. Such
multimodal senses naturally enables fine-grained interactions that are
difficult to simulate with text-conditioned generative models. To effectively
simulate fine-grained multisensory actions, we develop a feature learning
paradigm that aligns these modalities while preserving the unique information
each modality provides. We further propose a regularization scheme to enhance
causality of the action trajectory features in representing intricate
interaction dynamics. Experiments show that incorporating multimodal senses
improves simulation accuracy and reduces temporal drift. Extensive ablation
studies and downstream applications demonstrate the effectiveness and
practicality of our work.

</details>


### [72] [VideoNSA: Native Sparse Attention Scales Video Understanding](https://arxiv.org/abs/2510.02295)
*Enxin Song,Wenhao Chai,Shusheng Yang,Ethan Armand,Xiaojun Shan,Haiyang Xu,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: VideoNSA adapts Native Sparse Attention to video-language models, enabling reliable scaling to 128K tokens and improved performance on long-video understanding tasks through a hardware-aware hybrid attention approach.


<details>
  <summary>Details</summary>
Motivation: Video understanding in multimodal models is limited by context length, causing models to miss key transition frames and struggle with coherence across long time scales.

Method: Adapt Native Sparse Attention (NSA) to video-language models by adapting Qwen2.5-VL through end-to-end training on a 216K video instruction dataset, using hardware-aware hybrid attention (dense for text, NSA for video).

Result: Achieves improved performance on long-video understanding, temporal reasoning, and spatial benchmarks compared to token-compression and training-free sparse baselines.

Conclusion: Key findings include reliable scaling to 128K tokens, optimal global-local attention allocation, task-dependent branch usage patterns, and learnable combined sparse attention inducing dynamic attention sinks.

Abstract: Video understanding in multimodal language models remains limited by context
length: models often miss key transition frames and struggle to maintain
coherence across long time scales. To address this, we adapt Native Sparse
Attention (NSA) to video-language models. Our method, VideoNSA, adapts
Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We
employ a hardware-aware hybrid approach to attention, preserving dense
attention for text, while employing NSA for video. Compared to
token-compression and training-free sparse baselines, VideoNSA achieves
improved performance on long-video understanding, temporal reasoning, and
spatial benchmarks. Further ablation analysis reveals four key findings: (1)
reliable scaling to 128K tokens; (2) an optimal global-local attention
allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)
the learnable combined sparse attention help induce dynamic attention sinks.

</details>


### [73] [NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation](https://arxiv.org/abs/2510.02307)
*Ruozhen He,Moayed Haji-Ali,Ziyan Yang,Vicente Ordonez*

Main category: cs.CV

TL;DR: NoiseShift is a training-free method that recalibrates noise levels for diffusion models based on resolution size, significantly improving low-resolution image generation quality without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models trained on fixed resolutions fail to generalize to lower resolutions, preventing budget-efficient alternatives for users who don't need high-resolution images.

Method: Identifies that noise schedulers have unequal perceptual effects across resolutions - same noise level removes disproportionately more signal from lower-resolution images. Proposes NoiseShift to recalibrate denoiser noise level conditioned on resolution size.

Result: Significantly improves quality at low resolutions: On LAION-COCO, improves SD3.5 by 15.89%, SD3 by 8.56%, Flux-Dev by 2.44% in FID. On CelebA, improves SD3.5 by 10.36%, SD3 by 5.19%, Flux-Dev by 3.02% in FID.

Conclusion: NoiseShift effectively mitigates resolution-dependent artifacts and enhances low-resolution image generation quality, requiring no model changes and being compatible with existing models.

Abstract: Text-to-image diffusion models trained on a fixed set of resolutions often
fail to generalize, even when asked to generate images at lower resolutions
than those seen during training. High-resolution text-to-image generators are
currently unable to easily offer an out-of-the-box budget-efficient alternative
to their users who might not need high-resolution images. We identify a key
technical insight in diffusion models that when addressed can help tackle this
limitation: Noise schedulers have unequal perceptual effects across
resolutions. The same level of noise removes disproportionately more signal
from lower-resolution images than from high-resolution images, leading to a
train-test mismatch. We propose NoiseShift, a training-free method that
recalibrates the noise level of the denoiser conditioned on resolution size.
NoiseShift requires no changes to model architecture or sampling schedule and
is compatible with existing models. When applied to Stable Diffusion 3, Stable
Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly
improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and
Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by
10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results
demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent
artifacts and enhancing the quality of low-resolution image generation.

</details>


### [74] [Inferring Dynamic Physical Properties from Video Foundation Models](https://arxiv.org/abs/2510.02311)
*Guanqi Zhan,Xianzheng Ma,Weidi Xie,Andrew Zisserman*

Main category: cs.CV

TL;DR: The paper presents methods for predicting dynamic physical properties (elasticity, viscosity, friction) from videos using video foundation models and MLLMs, with performance analysis across different approaches.


<details>
  <summary>Details</summary>
Motivation: To develop methods for inferring temporal-dependent physical properties from videos, addressing the challenge of predicting dynamic physical characteristics that require temporal information.

Method: Collected new video datasets for three physical properties; explored three inference approaches: oracle method using classical CV, visual prompt mechanism with pre-trained video models, and prompt strategies for MLLMs.

Result: Video foundation models achieved similar performance to each other but behind the oracle method; MLLMs performed worse but could be improved with proper prompting.

Conclusion: Video foundation models show promise for physical property prediction, while MLLMs need further development but can benefit from appropriate prompting strategies.

Abstract: We study the task of predicting dynamic physical properties from videos. More
specifically, we consider physical properties that require temporal information
to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,
and dynamic friction of an object sliding on a surface. To this end, we make
the following contributions: (i) We collect a new video dataset for each
physical property, consisting of synthetic training and testing splits, as well
as a real split for real world evaluation. (ii) We explore three ways to infer
the physical property from videos: (a) an oracle method where we supply the
visual cues that intrinsically reflect the property using classical computer
vision techniques; (b) a simple read out mechanism using a visual prompt and
trainable prompt vector for cross-attention on pre-trained video generative and
self-supervised models; and (c) prompt strategies for Multi-modal Large
Language Models (MLLMs). (iii) We show that video foundation models trained in
a generative or self-supervised manner achieve a similar performance, though
behind that of the oracle, and MLLMs are currently inferior to the other
models, though their performance can be improved through suitable prompting.

</details>


### [75] [Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions](https://arxiv.org/abs/2510.02313)
*Mengyu Yang,Yiming Chen,Haozheng Pei,Siddhant Agarwal,Arun Balajee Vasudevan,James Hays*

Main category: cs.CV

TL;DR: The paper introduces sounding object detection, a task to link sounds to objects involved in interactions, using a multimodal object-aware framework trained on egocentric videos with automatic segmentation masks and slot attention.


<details>
  <summary>Details</summary>
Motivation: To evaluate if models can distinguish sounds from everyday object interactions and link them to the specific objects involved, inspired by human perception.

Method: Multimodal object-aware framework using in-the-wild egocentric videos, automatic pipeline for computing object segmentation masks, and slot attention visual encoder to enforce object-centric learning.

Result: Achieves state-of-the-art performance on the new sounding object detection task and existing multimodal action understanding tasks.

Conclusion: The proposed framework successfully links sounds to objects in interactions and advances multimodal understanding capabilities.

Abstract: Can a model distinguish between the sound of a spoon hitting a hardwood floor
versus a carpeted one? Everyday object interactions produce sounds unique to
the objects involved. We introduce the sounding object detection task to
evaluate a model's ability to link these sounds to the objects directly
involved. Inspired by human perception, our multimodal object-aware framework
learns from in-the-wild egocentric videos. To encourage an object-centric
approach, we first develop an automatic pipeline to compute segmentation masks
of the objects involved to guide the model's focus during training towards the
most informative regions of the interaction. A slot attention visual encoder is
used to further enforce an object prior. We demonstrate state of the art
performance on our new task along with existing multimodal action understanding
tasks.

</details>


### [76] [StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions](https://arxiv.org/abs/2510.02314)
*Bo-Hsu Ke,You-Zhe Xie,Yu-Lun Liu,Wei-Chen Chiu*

Main category: cs.CV

TL;DR: The paper analyzes 3D Gaussian Splatting (3DGS) vulnerabilities to poisoning attacks and proposes a novel density-guided attack method that injects Gaussian points into low-density regions using Kernel Density Estimation, creating viewpoint-dependent illusory objects.


<details>
  <summary>Details</summary>
Motivation: As 3D scene representation methods like NeRF and 3DGS become widely used, understanding and addressing their security vulnerabilities becomes critical, particularly against image-level poisoning attacks.

Method: Proposes a density-guided poisoning method using Kernel Density Estimation to identify low-density regions for strategic Gaussian point injection, along with an adaptive noise strategy to disrupt multi-view consistency.

Result: Extensive experiments show superior performance compared to state-of-the-art techniques, with the method successfully embedding viewpoint-dependent illusory objects visible from poisoned views while minimally affecting innocent views.

Conclusion: The work demonstrates significant vulnerabilities in 3DGS and provides a systematic KDE-based evaluation protocol for objective benchmarking of attack methods in future research.

Abstract: 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As
these methods become prevalent, addressing their vulnerabilities becomes
critical. We analyze 3DGS robustness against image-level poisoning attacks and
propose a novel density-guided poisoning method. Our method strategically
injects Gaussian points into low-density regions identified via Kernel Density
Estimation (KDE), embedding viewpoint-dependent illusory objects clearly
visible from poisoned views while minimally affecting innocent views.
Additionally, we introduce an adaptive noise strategy to disrupt multi-view
consistency, further enhancing attack effectiveness. We propose a KDE-based
evaluation protocol to assess attack difficulty systematically, enabling
objective benchmarking for future research. Extensive experiments demonstrate
our method's superior performance compared to state-of-the-art techniques.
Project page: https://hentci.github.io/stealthattack/

</details>


### [77] [Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity](https://arxiv.org/abs/2510.02315)
*Eric Tillmann Bill,Enis Simsar,Thomas Hofmann*

Main category: cs.CV

TL;DR: The paper introduces FOCUS, a theoretical framework and algorithms for improving multi-subject generation in text-to-image models by addressing attribute leakage, identity entanglement, and subject omissions through stochastic optimal control applied to flow matching.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models perform well on single-entity prompts but struggle with multi-subject descriptions, exhibiting problems like attribute leakage, identity entanglement, and subject omissions.

Method: The framework uses stochastic optimal control to steer flow matching sampling dynamics, yielding two algorithms: a training-free test-time controller that perturbs base velocity, and Adjoint Matching for lightweight fine-tuning that preserves base-model capabilities.

Result: Both algorithms consistently improve multi-subject alignment while maintaining base-model style across Stable Diffusion 3.5, FLUX, and Stable Diffusion XL. Test-time control runs efficiently on commodity GPUs, and fine-tuned controllers generalize to unseen prompts.

Conclusion: FOCUS achieves state-of-the-art multi-subject fidelity across models, providing the first fine-tuning route explicitly designed for multi-subject generation and unifying prior attention heuristics through a principled theoretical framework.

Abstract: Text-to-image (T2I) models excel on single-entity prompts but struggle with
multi-subject descriptions, often showing attribute leakage, identity
entanglement, and subject omissions. We introduce the first theoretical
framework with a principled, optimizable objective for steering sampling
dynamics toward multi-subject fidelity. Viewing flow matching (FM) through
stochastic optimal control (SOC), we formulate subject disentanglement as
control over a trained FM sampler. This yields two architecture-agnostic
algorithms: (i) a training-free test-time controller that perturbs the base
velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight
fine-tuning rule that regresses a control network to a backward adjoint signal
while preserving base-model capabilities. The same formulation unifies prior
attention heuristics, extends to diffusion models via a flow-diffusion
correspondence, and provides the first fine-tuning route explicitly designed
for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and
Stable Diffusion XL, both algorithms consistently improve multi-subject
alignment while maintaining base-model style. Test-time control runs
efficiently on commodity GPUs, and fine-tuned controllers trained on limited
prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal
Control for Unentangled Subjects), which achieves state-of-the-art
multi-subject fidelity across models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [78] [OR-Toolformer: Modeling and Solving Operations Research Problems with Tool Augmented Large Language Models](https://arxiv.org/abs/2510.01253)
*Jianzhang Zhang,Jialong Zhou,Chuang Liu*

Main category: cs.AI

TL;DR: OR-Toolformer fine-tunes Llama-3.1-8B-Instruct with a semi-automatic data synthesis pipeline and external solver integration to address privacy concerns and high compute costs in OR tasks, achieving superior performance on standard benchmarks and strong zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns with closed-source APIs for OR tasks and reduce high compute costs of training open-source models from scratch while leveraging LLMs' mathematical reasoning capabilities.

Method: Fine-tune Llama-3.1-8B-Instruct using semi-automatic data synthesis pipeline to generate diverse OR problem-answer pairs, and augment the model with external solvers to produce API calls.

Result: Achieves up to 80.1% execution accuracy on three of four standard benchmarks (exceeding baselines by over 4.3%), and 54% average accuracy on unseen OR problems (21 percentage-point improvement over strongest baseline).

Conclusion: Tool-augmented fine-tuning of LLMs is effective for accurate and generalizable OR problem modeling and solving.

Abstract: Large language models (LLMs) demonstrate strong mathematical reasoning, but
reliance on closed-source APIs for OR tasks raises privacy concerns, and
training open-source models from scratch incurs high compute costs. We
introduce OR-Toolformer, which fine-tunes Llama-3.1-8B-Instruct with a
semi-automatic data synthesis pipeline that generates diverse OR problem-answer
pairs and augments the model with external solvers to produce API calls. On
three of four standard benchmarks, OR-Toolformer achieves up to 80.1% execution
accuracy, exceeding size-matched baselines by over 4.3%. In zero-shot
evaluation on two unseen OR problem types, it attains 54% average accuracy, a
21 percentage-point improvement over the strongest baseline. These findings
validate the efficacy of tool-augmented fine-tuning LLMs for accurate and
generalizable OR problem modeling and solving.

</details>


### [79] [Modeling Others' Minds as Code](https://arxiv.org/abs/2510.01272)
*Kunal Jha,Aydan Yuenan Huang,Eric Ye,Natasha Jaques,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: ROTE is a novel algorithm that models human behavior as behavioral programs synthesized by LLMs and uses probabilistic inference to predict actions, achieving 50% better accuracy than baselines in gridworld and household simulation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing human behavior modeling approaches are data-hungry and brittle due to unrealistic rationality assumptions or computational demands. Many social interactions follow predictable patterns (scripts) that minimize cognitive load.

Method: ROTE treats action understanding as program synthesis, using LLMs to generate behavioral programs and probabilistic inference to reason about uncertainty over the hypothesis space of programs.

Result: ROTE outperforms competitive baselines (behavior cloning and LLM-based methods) by up to 50% in both in-sample accuracy and out-of-sample generalization across gridworld tasks and embodied household simulations.

Conclusion: By modeling routines as behavioral programs, ROTE enables AI systems to efficiently and effectively predict human behavior in real-world scenarios, opening new paths for robust human-AI collaboration.

Abstract: Accurate prediction of human behavior is essential for robust and safe
human-AI collaboration. However, existing approaches for modeling people are
often data-hungry and brittle because they either make unrealistic assumptions
about rationality or are too computationally demanding to adapt rapidly. Our
key insight is that many everyday social interactions may follow predictable
patterns; efficient "scripts" that minimize cognitive load for actors and
observers, e.g., "wait for the green light, then go." We propose modeling these
routines as behavioral programs instantiated in computer code rather than
policies conditioned on beliefs and desires. We introduce ROTE, a novel
algorithm that leverages both large language models (LLMs) for synthesizing a
hypothesis space of behavioral programs, and probabilistic inference for
reasoning about uncertainty over that space. We test ROTE in a suite of
gridworld tasks and a large-scale embodied household simulator. ROTE predicts
human and AI behaviors from sparse observations, outperforming competitive
baselines -- including behavior cloning and LLM-based methods -- by as much as
50% in terms of in-sample accuracy and out-of-sample generalization. By
treating action understanding as a program synthesis problem, ROTE opens a path
for AI systems to efficiently and effectively predict human behavior in the
real-world.

</details>


### [80] [Cyber Academia-Chemical Engineering (CA-ChemE): A Living Digital Town for Self-Directed Research Evolution and Emergent Scientific Discovery](https://arxiv.org/abs/2510.01293)
*Zekun Jiang,Chunming Xu,Tianhang Zhou*

Main category: cs.AI

TL;DR: The CA-ChemE system is a multi-agent AI platform that enables autonomous scientific discovery in chemical engineering through knowledge-enhanced agents and collaboration mechanisms, addressing interdisciplinary collaboration bottlenecks.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in existing AI systems for interdisciplinary collaboration and exploration of uncharted problems in chemical engineering.

Method: Developed a living digital town with multi-agent collaboration, integrating domain-specific knowledge bases, knowledge enhancement technologies, and collaboration agents with ontology engineering capabilities.

Result: Knowledge base enhancement improved dialogue quality by 10-15%, while collaboration agent intervention achieved 8.5% improvement for distant-domain expert pairs versus only 0.8% for domain-proximate pairs, revealing a 10.6-fold difference in collaborative efficiency.

Conclusion: Carefully designed multi-agent architectures provide a viable pathway toward autonomous scientific discovery in chemical engineering by addressing knowledge-base gaps and enabling effective interdisciplinary collaboration.

Abstract: The rapid advancement of artificial intelligence (AI) has demonstrated
substantial potential in chemical engineering, yet existing AI systems remain
limited in interdisciplinary collaboration and exploration of uncharted
problems. To address these issues, we present the Cyber Academia-Chemical
Engineering (CA-ChemE) system, a living digital town that enables self-directed
research evolution and emergent scientific discovery through multi-agent
collaboration. By integrating domain-specific knowledge bases, knowledge
enhancement technologies, and collaboration agents, the system successfully
constructs an intelligent ecosystem capable of deep professional reasoning and
efficient interdisciplinary collaboration. Our findings demonstrate that
knowledge base-enabled enhancement mechanisms improved dialogue quality scores
by 10-15% on average across all seven expert agents, fundamentally ensuring
technical judgments are grounded in verifiable scientific evidence. However, we
observed a critical bottleneck in cross-domain collaboration efficiency,
prompting the introduction of a Collaboration Agent (CA) equipped with ontology
engineering capabilities. CA's intervention achieved 8.5% improvements for
distant-domain expert pairs compared to only 0.8% for domain-proximate pairs -
a 10.6-fold difference - unveiling the "diminished collaborative efficiency
caused by knowledge-base gaps" effect. This study demonstrates how carefully
designed multi-agent architectures can provide a viable pathway toward
autonomous scientific discovery in chemical engineering.

</details>


### [81] [The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation](https://arxiv.org/abs/2510.01295)
*Zarreen Reza*

Main category: cs.AI

TL;DR: A novel evaluation framework using multi-agent debate as a social laboratory to measure emergent social behaviors in LLM-based autonomous agents, revealing strong consensus-seeking tendencies and persona-driven psychometric profiles.


<details>
  <summary>Details</summary>
Motivation: Traditional evaluation benchmarks are insufficient for capturing emergent social and cognitive dynamics in autonomous LLM agents that communicate, persuade, and collaborate in interactive environments.

Method: Multi-agent debate framework where LLM-based agents with distinct personas and incentives deliberate on challenging topics under LLM moderator supervision, analyzed using psychometric and semantic metrics.

Result: Agents consistently reach high semantic agreement (Œº > 0.88) without explicit instruction, personas induce stable psychometric profiles, and moderator personas significantly alter debate outcomes through environmental structuring.

Conclusion: Provides a blueprint for dynamic, psychometrically grounded evaluation protocols for agentic AI, offering crucial methodology for understanding and shaping social behaviors of next-generation AI agents.

Abstract: As Large Language Models (LLMs) transition from static tools to autonomous
agents, traditional evaluation benchmarks that measure performance on
downstream tasks are becoming insufficient. These methods fail to capture the
emergent social and cognitive dynamics that arise when agents communicate,
persuade, and collaborate in interactive environments. To address this gap, we
introduce a novel evaluation framework that uses multi-agent debate as a
controlled "social laboratory" to discover and quantify these behaviors. In our
framework, LLM-based agents, instantiated with distinct personas and
incentives, deliberate on a wide range of challenging topics under the
supervision of an LLM moderator. Our analysis, enabled by a new suite of
psychometric and semantic metrics, reveals several key findings. Across
hundreds of debates, we uncover a powerful and robust emergent tendency for
agents to seek consensus, consistently reaching high semantic agreement ({\mu}
> 0.88) even without explicit instruction and across sensitive topics. We show
that assigned personas induce stable, measurable psychometric profiles,
particularly in cognitive effort, and that the moderators persona can
significantly alter debate outcomes by structuring the environment, a key
finding for external AI alignment. This work provides a blueprint for a new
class of dynamic, psychometrically grounded evaluation protocols designed for
the agentic setting, offering a crucial methodology for understanding and
shaping the social behaviors of the next generation of AI agents. We have
released the code and results at
https://github.com/znreza/multi-agent-LLM-eval-for-debate.

</details>


### [82] [Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.01304)
*Yu Zeng,Wenxuan Huang,Shiting Huang,Xikun Bao,Yukun Qi,Yiming Zhao,Qiuchen Wang,Lin Chen,Zehui Chen,Huaian Chen,Wanli Ouyang,Feng Zhao*

Main category: cs.AI

TL;DR: AGILE is an agentic jigsaw interaction learning method that enhances visual perception and reasoning in VLMs through interactive jigsaw solving with code execution and visual feedback.


<details>
  <summary>Details</summary>
Motivation: Current VLMs perform poorly on simple jigsaw tasks, revealing deficiencies in core perception and reasoning capabilities, while high-quality vision-language data is scarce and not scalable.

Method: AGILE formulates jigsaw solving as an interactive process where the model generates executable code to perform actions based on current state, and the environment provides fine-grained visual feedback to guide task completion through iterative cycles.

Result: AGILE boosts jigsaw task accuracy from 9.5% to 82.8% under 2√ó2 setting and demonstrates strong generalization across 9 vision tasks with average 3.1% improvement.

Conclusion: AGILE provides an efficient, scalable solution to multimodal data scarcity and opens new avenues for advancing reasoning and generalization in multimodal models.

Abstract: Although current large Vision-Language Models (VLMs) have advanced in
multimodal understanding and reasoning, their fundamental perceptual and
reasoning abilities remain limited. Specifically, even on simple jigsaw tasks,
existing VLMs perform near randomly, revealing deficiencies in core perception
and reasoning capabilities. While high-quality vision-language data can enhance
these capabilities, its scarcity and limited scalability impose significant
constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction
Learning for Enhancing visual perception and reasoning in VLMs. AGILE
formulates jigsaw solving as an interactive process, enabling the model to
progressively engage with the environment. At each step, the model generates
executable code to perform an action based on the current state, while the
environment provides fine-grained visual feedback to guide task completion.
Through this iterative cycle of observation and interaction, the model
incrementally improves its perceptual and reasoning capabilities via
exploration and feedback. Experimental results show that AGILE not only
substantially boosts performance on jigsaw tasks of varying complexity (e.g.,
increasing accuracy from 9.5% to 82.8% under the 2 $\times$ 2 setting) but also
demonstrates strong generalization across 9 general vision tasks, achieving an
average improvement of 3.1%. These results indicate notable enhancements in
both perceptual and reasoning abilities. This work opens a new avenue for
advancing reasoning and generalization in multimodal models and provides an
efficient, scalable solution to the scarcity of multimodal reinforcement
learning data. The code and datasets is available at
https://github.com/yuzeng0-0/AGILE .

</details>


### [83] [Aristotle: IMO-level Automated Theorem Proving](https://arxiv.org/abs/2510.01346)
*Tudor Achim,Alex Best,Kevin Der,Math√Øs F√©d√©rico,Sergei Gukov,Daniel Halpern-Leister,Kirsten Henningsgard,Yury Kudryashov,Alexander Meiburg,Martin Michelsen,Riley Patterson,Eric Rodriguez,Laura Scharff,Vikram Shanker,Vladmir Sicca,Hari Sowrirajan,Aidan Swope,Matyas Tamas,Vlad Tenev,Jonathan Thomm,Harold Williams,Lawrence Wu*

Main category: cs.AI

TL;DR: Aristotle is an AI system that combines formal verification with informal reasoning, achieving gold-medal-level performance on 2025 IMO problems through integration of Lean proof search, informal reasoning for lemma generation, and a dedicated geometry solver.


<details>
  <summary>Details</summary>
Motivation: To advance automated theorem proving by creating a system that can solve challenging mathematical problems at the level of top human competitors in the International Mathematical Olympiad.

Method: Integrates three main components: a Lean proof search system, an informal reasoning system that generates and formalizes lemmas, and a dedicated geometry solver.

Result: Achieved gold-medal-equivalent performance on the 2025 International Mathematical Olympiad problems and demonstrated state-of-the-art performance with favorable scaling properties.

Conclusion: The Aristotle system successfully combines formal verification with informal reasoning to achieve human-level performance in mathematical theorem proving, representing a significant advancement in automated theorem proving capabilities.

Abstract: We introduce Aristotle, an AI system that combines formal verification with
informal reasoning, achieving gold-medal-equivalent performance on the 2025
International Mathematical Olympiad problems. Aristotle integrates three main
components: a Lean proof search system, an informal reasoning system that
generates and formalizes lemmas, and a dedicated geometry solver. Our system
demonstrates state-of-the-art performance with favorable scaling properties for
automated theorem proving.

</details>


### [84] [MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments](https://arxiv.org/abs/2510.01353)
*Darshan Deshpande,Varun Gangal,Hersh Mehta,Anand Kannappan,Rebecca Qian,Peng Wang*

Main category: cs.AI

TL;DR: MEMTRACK is a benchmark for evaluating long-term memory and state tracking in multi-platform agent environments, focusing on realistic organizational workflows with asynchronous events across platforms like Slack, Linear, and Git.


<details>
  <summary>Details</summary>
Motivation: Existing context and memory benchmarks focus on conversational instances, but there's a need for evaluating memory in dynamic enterprise environments for effective application in real-world organizational settings.

Method: MEMTRACK integrates asynchronous events across multiple communication and productivity platforms, creating chronologically interleaved timelines with noisy, conflicting, and cross-referring information. The dataset is curated through manual expert design and scalable agent-based synthesis.

Result: Experiments show challenges in utilizing memory across long horizons, handling cross-platform dependencies, and resolving contradictions. The best performing GPT-5 model only achieves 60% Correctness score on MEMTRACK.

Conclusion: MEMTRACK provides an extensible framework for advancing evaluation research for memory-augmented agents beyond conversational setups, enabling multi-agent, multi-platform memory benchmarking in complex organizational settings.

Abstract: Recent works on context and memory benchmarking have primarily focused on
conversational instances but the need for evaluating memory in dynamic
enterprise environments is crucial for its effective application. We introduce
MEMTRACK, a benchmark designed to evaluate long-term memory and state tracking
in multi-platform agent environments. MEMTRACK models realistic organizational
workflows by integrating asynchronous events across multiple communication and
productivity platforms such as Slack, Linear and Git. Each benchmark instance
provides a chronologically platform-interleaved timeline, with noisy,
conflicting, cross-referring information as well as potential
codebase/file-system comprehension and exploration. Consequently, our benchmark
tests memory capabilities such as acquistion, selection and conflict
resolution. We curate the MEMTRACK dataset through both manual expert driven
design and scalable agent based synthesis, generating ecologically valid
scenarios grounded in real world software development processes. We introduce
pertinent metrics for Correctness, Efficiency, and Redundancy that capture the
effectiveness of memory mechanisms beyond simple QA performance. Experiments
across SoTA LLMs and memory backends reveal challenges in utilizing memory
across long horizons, handling cross-platform dependencies, and resolving
contradictions. Notably, the best performing GPT-5 model only achieves a 60\%
Correctness score on MEMTRACK. This work provides an extensible framework for
advancing evaluation research for memory-augmented agents, beyond existing
focus on conversational setups, and sets the stage for multi-agent,
multi-platform memory benchmarking in complex organizational settings

</details>


### [85] [Retrieval-Augmented Framework for LLM-Based Clinical Decision Support](https://arxiv.org/abs/2510.01363)
*Leon Garza,Anantaa Kotal,Michael A. Grasso,Emre Umucu*

Main category: cs.AI

TL;DR: A clinical decision support system using Large Language Models (LLMs) to assist prescribing clinicians by analyzing EHR data and generating therapeutic suggestions through a retrieval-augmented generation (RAG) pipeline.


<details>
  <summary>Details</summary>
Motivation: To address the complexity of clinical decision-making and leverage expanding EHR data to deliver data-informed care, while augmenting rather than replacing clinician judgment.

Method: Uses a RAG pipeline that integrates natural language processing with structured clinical inputs, harmonizing unstructured narratives and codified data to support LLM-based inference with representation alignment and generation strategies.

Result: Preliminary evaluations with de-identified and synthetic clinical datasets show promising clinical plausibility and consistency of model outputs, suggesting LLM-based tools can provide valuable decision support in prescribing workflows.

Conclusion: This represents an initial step toward integrating generative AI into real-world clinical decision-making with emphasis on transparency, safety, and alignment with established practices.

Abstract: The increasing complexity of clinical decision-making, alongside the rapid
expansion of electronic health records (EHR), presents both opportunities and
challenges for delivering data-informed care. This paper proposes a clinical
decision support system powered by Large Language Models (LLMs) to assist
prescribing clinicians. The system generates therapeutic suggestions by
analyzing historical EHR data, including patient demographics, presenting
complaints, clinical symptoms, diagnostic information, and treatment histories.
The framework integrates natural language processing with structured clinical
inputs to produce contextually relevant recommendations. Rather than replacing
clinician judgment, it is designed to augment decision-making by retrieving and
synthesizing precedent cases with comparable characteristics, drawing on local
datasets or federated sources where applicable. At its core, the system employs
a retrieval-augmented generation (RAG) pipeline that harmonizes unstructured
narratives and codified data to support LLM-based inference. We outline the
system's technical components, including representation representation
alignment and generation strategies. Preliminary evaluations, conducted with
de-identified and synthetic clinical datasets, examine the clinical
plausibility and consistency of the model's outputs. Early findings suggest
that LLM-based tools may provide valuable decision support in prescribing
workflows when appropriately constrained and rigorously validated. This work
represents an initial step toward integration of generative AI into real-world
clinical decision-making with an emphasis on transparency, safety, and
alignment with established practices.

</details>


### [86] [Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort](https://arxiv.org/abs/2510.01367)
*Xinpeng Wang,Nitish Joshi,Barbara Plank,Rico Angell,He He*

Main category: cs.AI

TL;DR: TRACE (Truncated Reasoning AUC Evaluation) is a method to detect implicit reward hacking in reasoning models by measuring how early in the reasoning chain a model can achieve high verification scores, identifying shortcuts that bypass intended task solutions.


<details>
  <summary>Details</summary>
Motivation: Reward hacking poses a significant threat where models exploit loopholes in reward functions to achieve high rewards without solving the intended task. This can be implicit (not verbalized in chain-of-thought), making it undetectable by current CoT monitors.

Method: TRACE progressively truncates a model's chain-of-thought at various lengths, forces the model to answer, and measures the verifier-passing rate at each cutoff. It quantifies effort by measuring how early reasoning becomes sufficient to pass verification, with hacking models showing high passing rates with only small CoT fractions.

Result: TRACE achieves over 65% gains over a 72B CoT monitor in math reasoning and over 30% gains over a 32B monitor in coding. It can also discover unknown loopholes during training.

Conclusion: TRACE offers a scalable unsupervised approach for oversight where current monitoring methods prove ineffective, providing a robust defense against implicit reward hacking in reasoning models.

Abstract: Reward hacking, where a reasoning model exploits loopholes in a reward
function to achieve high rewards without solving the intended task, poses a
significant threat. This behavior may be explicit, i.e. verbalized in the
model's chain-of-thought (CoT), or implicit, where the CoT appears benign thus
bypasses CoT monitors. To detect implicit reward hacking, we propose TRACE
(Truncated Reasoning AUC Evaluation). Our key observation is that hacking
occurs when exploiting the loophole is easier than solving the actual task.
This means that the model is using less `effort' than required to achieve high
reward. TRACE quantifies effort by measuring how early a model's reasoning
becomes sufficient to pass a verifier. We progressively truncate a model's CoT
at various lengths, force the model to answer, and measure the verifier-passing
rate at each cutoff. A hacking model, which takes a shortcut, will achieve a
high passing rate with only a small fraction of its CoT, yielding a large area
under the accuracy-vs-length curve. TRACE achieves over 65% gains over our
strongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B
monitor in coding. We further show that TRACE can discover unknown loopholes
during training. Overall, TRACE offers a scalable unsupervised approach for
oversight where current monitoring methods prove ineffective.

</details>


### [87] [Fine-tuning with RAG for Improving LLM Learning of New Skills](https://arxiv.org/abs/2510.01375)
*Humaid Ibrahim,Nikolai Rozanov,Marek Rei*

Main category: cs.AI

TL;DR: The paper proposes a distillation pipeline that converts inference-time retrieval into learned competence, enabling LLM agents to internalize retrieval benefits through targeted fine-tuning without permanent runtime dependencies.


<details>
  <summary>Details</summary>
Motivation: LLM agents frequently fail in predictable ways during multi-step tasks, and while RAG can help, it requires maintaining external knowledge databases and adds computational overhead at every deployment.

Method: A three-step approach: (1) extract compact hints from agent failures, (2) generate improved teacher trajectories via one-shot retrieval at episode start, (3) train student models on these trajectories with hints removed to force internalization.

Result: Distilled students consistently outperform baseline agents across ALFWorld (91% vs 79% success) and WebShop (72 vs 61 scores), while using 10-60% fewer tokens than retrieval-augmented teachers.

Conclusion: Retrieval benefits can be effectively internalized through targeted fine-tuning without permanent runtime dependencies, generalizing across model scales and agent architectures.

Abstract: Large language model (LLM) agents deployed for multi-step tasks frequently
fail in predictable ways: attempting actions with unmet preconditions, issuing
redundant commands, or mishandling environment constraints. While
retrieval-augmented generation (RAG) can improve performance by providing
runtime guidance, it requires maintaining external knowledge databases and adds
computational overhead at every deployment. We propose a simple pipeline that
converts inference-time retrieval into learned competence through distillation.
Our approach: (1) extracts compact, reusable hints from agent failures, (2)
uses these hints to generate improved teacher trajectories via one-shot
retrieval at episode start, and (3) trains student models on these trajectories
with hint strings removed, forcing internalization rather than memorization.
Across two interactive benchmarks, ALFWorld (household tasks) and WebShop
(online shopping), distilled students consistently outperform baseline agents,
achieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving
WebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens
than retrieval-augmented teachers depending on the environment. The approach
generalizes across model scales (7B/14B parameters) and agent architectures
(ReAct/StateAct), demonstrating that retrieval benefits can be effectively
internalized through targeted fine-tuning without permanent runtime
dependencies.

</details>


### [88] [Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents](https://arxiv.org/abs/2510.01398)
*Yang Liu,Zaid Abulawi,Abhiram Garimidi,Doyeong Lim*

Main category: cs.AI

TL;DR: LLM agents automate data-driven modeling for regression tasks, achieving performance comparable to human experts while reducing manual intervention.


<details>
  <summary>Details</summary>
Motivation: Traditional data-driven methods require extensive manual work and don't scale well. There's a need for automated, efficient modeling strategies for large scientific datasets.

Method: Two LLM-agent frameworks: multi-agent system with specialized agents, and single-agent ReAct system. Both autonomously handle data preprocessing, neural network development, training, hyperparameter optimization, and uncertainty quantification.

Result: LLM-agent-developed model outperforms traditional CHF lookup tables and achieves predictive accuracy and uncertainty quantification comparable to state-of-the-art Bayesian optimized deep neural networks developed by human experts.

Conclusion: LLM-based agents show significant potential for automating complex engineering modeling tasks, reducing human workload while meeting or exceeding existing performance standards.

Abstract: Modern engineering increasingly relies on vast datasets generated by
experiments and simulations, driving a growing demand for efficient, reliable,
and broadly applicable modeling strategies. There is also heightened interest
in developing data-driven approaches, particularly neural network models, for
effective prediction and analysis of scientific datasets. Traditional
data-driven methods frequently involve extensive manual intervention, limiting
their ability to scale effectively and generalize to diverse applications. In
this study, we propose an innovative pipeline utilizing Large Language Model
(LLM) agents to automate data-driven modeling and analysis, with a particular
emphasis on regression tasks. We evaluate two LLM-agent frameworks: a
multi-agent system featuring specialized collaborative agents, and a
single-agent system based on the Reasoning and Acting (ReAct) paradigm. Both
frameworks autonomously handle data preprocessing, neural network development,
training, hyperparameter optimization, and uncertainty quantification (UQ). We
validate our approach using a critical heat flux (CHF) prediction benchmark,
involving approximately 25,000 experimental data points from the OECD/NEA
benchmark dataset. Results indicate that our LLM-agent-developed model
surpasses traditional CHF lookup tables and delivers predictive accuracy and UQ
on par with state-of-the-art Bayesian optimized deep neural network models
developed by human experts. These outcomes underscore the significant potential
of LLM-based agents to automate complex engineering modeling tasks, greatly
reducing human workload while meeting or exceeding existing standards of
predictive performance.

</details>


### [89] [OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity Logs with Large Language Models](https://arxiv.org/abs/2510.01409)
*Luca Cotti,Idilio Drago,Anisa Rula,Devis Bianchini,Federico Cerutti*

Main category: cs.AI

TL;DR: OntoLogX is an AI agent that uses LLMs to transform raw system logs into structured Knowledge Graphs using ontologies, enabling automated CTI extraction and MITRE ATT&CK tactic mapping.


<details>
  <summary>Details</summary>
Motivation: System logs contain valuable CTI but are limited by lack of structure, semantic inconsistency, and fragmentation, making automated extraction challenging.

Method: Integrates lightweight log ontology with RAG and iterative correction steps using LLMs to generate valid KGs, then aggregates KGs into sessions and predicts MITRE ATT&CK tactics.

Result: Demonstrated robust KG generation across multiple backends and accurate mapping to ATT&CK tactics on both benchmark and real-world honeypot datasets, with improved precision and recall through retrieval and correction.

Conclusion: Ontology-grounded representations enable actionable CTI extraction, with code-oriented LLMs being effective for structured log analysis and retrieval/correction improving KG quality.

Abstract: System logs represent a valuable source of Cyber Threat Intelligence (CTI),
capturing attacker behaviors, exploited vulnerabilities, and traces of
malicious activity. Yet their utility is often limited by lack of structure,
semantic inconsistency, and fragmentation across devices and sessions.
Extracting actionable CTI from logs therefore requires approaches that can
reconcile noisy, heterogeneous data into coherent and interoperable
representations. We introduce OntoLogX, an autonomous Artificial Intelligence
(AI) agent that leverages Large Language Models (LLMs) to transform raw logs
into ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a
lightweight log ontology with Retrieval Augmented Generation (RAG) and
iterative correction steps, ensuring that generated KGs are syntactically and
semantically valid. Beyond event-level analysis, the system aggregates KGs into
sessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level
log evidence to higher-level adversarial objectives. We evaluate OntoLogX on
both logs from a public benchmark and a real-world honeypot dataset,
demonstrating robust KG generation across multiple KGs backends and accurate
mapping of adversarial activity to ATT&CK tactics. Results highlight the
benefits of retrieval and correction for precision and recall, the
effectiveness of code-oriented models in structured log analysis, and the value
of ontology-grounded representations for actionable CTI extraction.

</details>


### [90] [A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining](https://arxiv.org/abs/2510.01427)
*Sipeng Zhang,Longfei Yun,Zilong Wang,Jingbo Shang,Letian Peng*

Main category: cs.AI

TL;DR: Falconer is a framework that combines LLMs as planners/annotators with lightweight proxy models for scalable knowledge mining, achieving LLM-level accuracy with 90% cost reduction and 20x speedup.


<details>
  <summary>Details</summary>
Motivation: LLMs are too expensive for large-scale knowledge mining, while traditional pipelines are brittle and can't generalize to new tasks.

Method: Uses LLMs as planners to decompose instructions and as annotators to train small proxy models, unifying classification and extraction into two atomic operations.

Result: Matches state-of-the-art LLMs in accuracy while reducing inference cost by 90% and accelerating knowledge mining by 20x.

Conclusion: Falconer provides an efficient and scalable foundation for Deep Research by combining LLM reasoning with lightweight proxy models.

Abstract: At the core of Deep Research is knowledge mining, the task of extracting
structured information from massive unstructured text in response to user
instructions. Large language models (LLMs) excel at interpreting such
instructions but are prohibitively expensive to deploy at scale, while
traditional pipelines of classifiers and extractors remain efficient yet
brittle and unable to generalize to new tasks. We introduce Falconer, a
collaborative framework that combines the agentic reasoning of LLMs with
lightweight proxy models for scalable knowledge mining. In Falconer, LLMs act
as planners, decomposing user instructions into executable pipelines, and as
annotators, generating supervision to train small proxies. The framework
unifies classification and extraction into two atomic operations, get label and
get span, enabling a single instruction-following model to replace multiple
task-specific components. To evaluate the consistency between proxy models
incubated by Falconer and annotations provided by humans and large models, we
construct new benchmarks covering both planning and end-to-end execution.
Experiments show that Falconer closely matches state-of-the-art LLMs in
instruction-following accuracy while reducing inference cost by up to 90% and
accelerating large-scale knowledge mining by more than 20x, offering an
efficient and scalable foundation for Deep Research.

</details>


### [91] [On the Role of Domain Experts in Creating Effective Tutoring Systems](https://arxiv.org/abs/2510.01432)
*Sarath Sreedharan,Kelsey Sikes,Nathaniel Blanchard,Lisa Mason,Nikhil Krishnaswamy,Jill Zarestky*

Main category: cs.AI

TL;DR: The paper explores how expert-curated knowledge can enhance AI tutoring systems through explainable AI techniques for automatic lesson generation and curriculum-based adaptive tutoring.


<details>
  <summary>Details</summary>
Motivation: To highlight the overlooked role of domain expert knowledge in creating effective AI tutoring systems and demonstrate its potential benefits.

Method: Proposes two approaches: 1) Using expert-specified rules with explainable AI (XAI) techniques to automatically generate lessons, and 2) Leveraging expert-designed curricula to develop adaptive tutoring systems with more efficient algorithms.

Result: Presents a case study on creating a pollinator identification tutoring system where expert knowledge can be easily elicited and applied.

Conclusion: Expert-curated knowledge plays a crucial role in developing novel and effective educational AI systems, particularly through XAI-based lesson generation and curriculum-driven adaptive tutoring.

Abstract: The role that highly curated knowledge, provided by domain experts, could
play in creating effective tutoring systems is often overlooked within the AI
for education community. In this paper, we highlight this topic by discussing
two ways such highly curated expert knowledge could help in creating novel
educational systems. First, we will look at how one could use explainable AI
(XAI) techniques to automatically create lessons. Most existing XAI methods are
primarily aimed at debugging AI systems. However, we will discuss how one could
use expert specified rules about solving specific problems along with novel XAI
techniques to automatically generate lessons that could be provided to
learners. Secondly, we will see how an expert specified curriculum for learning
a target concept can help develop adaptive tutoring systems, that can not only
provide a better learning experience, but could also allow us to use more
efficient algorithms to create these systems. Finally, we will highlight the
importance of such methods using a case study of creating a tutoring system for
pollinator identification, where such knowledge could easily be elicited from
experts.

</details>


### [92] [VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning](https://arxiv.org/abs/2510.01444)
*Rui Liu,Dian Yu,Tong Zheng,Runpeng Dai,Zongxia Li,Wenhao Yu,Zhenwen Liang,Linfeng Song,Haitao Mi,Pratap Tokekar,Dong Yu*

Main category: cs.AI

TL;DR: VOGUE introduces visual uncertainty guided exploration for MLLMs, treating images as stochastic contexts to improve reasoning by quantifying policy sensitivity to visual perturbations and balancing exploration-exploitation.


<details>
  <summary>Details</summary>
Motivation: Current RLVR methods for MLLMs struggle with exploration and treat visual input as deterministic, overlooking visual ambiguity and failing to build robust policies against visual variations.

Method: VOGUE quantifies policy sensitivity to visual perturbations using symmetric KL divergence between raw and noisy image branches, creates uncertainty-proportional bonus, combines with token-entropy bonus and annealed sampling schedule.

Result: Implemented on Qwen2.5-VL-3B/7B, VOGUE boosts pass@1 accuracy by 2.6% on visual math benchmarks and 3.7% on general reasoning benchmarks, increases pass@4 performance, and mitigates exploration decay in RL fine-tuning.

Conclusion: Grounded exploration in visual input uncertainty effectively improves multimodal reasoning in MLLMs.

Abstract: Reinforcement learning with verifiable rewards (RLVR) improves reasoning in
large language models (LLMs) but struggles with exploration, an issue that
still persists for multimodal LLMs (MLLMs). Current methods treat the visual
input as a fixed, deterministic condition, overlooking a critical source of
ambiguity and struggling to build policies robust to plausible visual
variations. We introduce $\textbf{VOGUE (Visual Uncertainty Guided
Exploration)}$, a novel method that shifts exploration from the output (text)
to the input (visual) space. By treating the image as a stochastic context,
VOGUE quantifies the policy's sensitivity to visual perturbations using the
symmetric KL divergence between a "raw" and "noisy" branch, creating a direct
signal for uncertainty-aware exploration. This signal shapes the learning
objective via an uncertainty-proportional bonus, which, combined with a
token-entropy bonus and an annealed sampling schedule, effectively balances
exploration and exploitation. Implemented within GRPO on two model scales
(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three
visual math benchmarks and 3.7% on three general-domain reasoning benchmarks,
while simultaneously increasing pass@4 performance and mitigating the
exploration decay commonly observed in RL fine-tuning. Our work shows that
grounding exploration in the inherent uncertainty of visual inputs is an
effective strategy for improving multimodal reasoning.

</details>


### [93] [AIReg-Bench: Benchmarking Language Models That Assess AI Regulation Compliance](https://arxiv.org/abs/2510.01474)
*Bill Marino,Rosco Hunter,Zubair Jamali,Marinos Emmanouil Kalpakos,Mudra Kashyap,Isaiah Hinton,Alexa Hanson,Maahum Nazir,Christoph Schnabl,Felix Steffek,Hongkai Wen,Nicholas D. Lane*

Main category: cs.AI

TL;DR: AIReg-Bench is the first benchmark dataset for evaluating LLMs' ability to assess compliance with the EU AI Act, created through LLM-generated technical documentation samples and expert legal annotations.


<details>
  <summary>Details</summary>
Motivation: As governments regulate AI, there's growing interest in using LLMs to assess AI system compliance with regulations, but no existing benchmarks to evaluate LLM performance on this task.

Method: Created dataset through two-step process: (1) prompting LLM to generate 120 technical documentation excerpts of fictional AI systems, (2) legal experts reviewed and annotated each sample for AI Act violations.

Result: Developed AIReg-Bench dataset with expert-annotated compliance labels, providing a benchmark to evaluate frontier LLMs' ability to reproduce expert compliance assessments.

Conclusion: AIReg-Bench establishes a foundation for understanding opportunities and limitations of LLM-based AI regulation compliance assessment tools and provides a benchmark for future LLM comparisons.

Abstract: As governments move to regulate AI, there is growing interest in using Large
Language Models (LLMs) to assess whether or not an AI system complies with a
given AI Regulation (AIR). However, there is presently no way to benchmark the
performance of LLMs at this task. To fill this void, we introduce AIReg-Bench:
the first benchmark dataset designed to test how well LLMs can assess
compliance with the EU AI Act (AIA). We created this dataset through a two-step
process: (1) by prompting an LLM with carefully structured instructions, we
generated 120 technical documentation excerpts (samples), each depicting a
fictional, albeit plausible, AI system - of the kind an AI provider might
produce to demonstrate their compliance with AIR; (2) legal experts then
reviewed and annotated each sample to indicate whether, and in what way, the AI
system described therein violates specific Articles of the AIA. The resulting
dataset, together with our evaluation of whether frontier LLMs can reproduce
the experts' compliance labels, provides a starting point to understand the
opportunities and limitations of LLM-based AIR compliance assessment tools and
establishes a benchmark against which subsequent LLMs can be compared. The
dataset and evaluation code are available at
https://github.com/camlsys/aireg-bench.

</details>


### [94] [Lateral Tree-of-Thoughts Surpasses ToT by Incorporating Logically-Consistent, Low-Utility Candidates](https://arxiv.org/abs/2510.01500)
*Abhinav Madahar*

Main category: cs.AI

TL;DR: Lateral Tree-of-Thoughts (LToT) is a search controller that separates utility from logical consistency, treating low-utility but consistent candidates as assets rather than waste, using Lateral Racing with Short-Circuit to efficiently explore wide lateral sets while keeping mainlines narrow.


<details>
  <summary>Details</summary>
Motivation: Standard Tree-of-Thoughts search suffers from breadth saturation (additional samples produce near-duplicates) and depth myopia (noisy short-horizon utilities prune branches with delayed payoffs) when given large test-time compute budgets.

Method: LToT splits the frontier into mainlines (high-utility candidates for exploitation) and laterals (consistent but low-utility candidates). It explores laterals via Lateral Racing with Short-Circuit - a capped successive-halving race that spreads tiny probes across wide lateral sets, uses width-aware thresholds with repeat-to-confirm, and immediately promotes branches once they clear the mainline bar.

Result: Theoretical analysis shows pseudolinear lateral cost Œò(N‚ÇÄ log_Œ∑ N‚ÇÄ) with logarithmically many rungs, contrasting with exponential growth of uncapped mainlines. Empirical evaluations are in preparation.

Conclusion: LToT turns large test-time budgets into principled diversity while preserving promotion discipline, mitigating saturation and myopia without inflating compute.

Abstract: Modern deployments increasingly allocate large test-time compute (thousands
of tokens or many node expansions) to boost reliability. Under such budgets,
standard Tree-of-Thoughts-style search exhibits two pathologies: breadth
saturation (additional samples mostly produce near-duplicates, so width stops
growing) and depth myopia (noisy short-horizon utilities prune branches whose
payoff appears after a few more steps). We propose Lateral Tree-of-Thoughts
(LToT), a drop-in controller that separates utility from logical consistency
and treats low-utility but consistent candidates as assets rather than waste.
The frontier is split into mainlines (high-utility candidates used for
exploitation) and laterals (consistent, initially low-utility candidates that
receive short, cheap probes before judgment). LToT explores laterals via
Lateral Racing with Short-Circuit (LR--SC): a capped successive-halving race
that spreads tiny probes across a very wide lateral set, uses width-aware
thresholds with repeat-to-confirm, and immediately promotes a branch once its
envelope clears the mainline bar; mainlines are kept intentionally narrow so
surplus compute is invested where width is cheap. We prove a pseudolinear
lateral cost $\Theta(N_0 \log_{\eta} N_0)$ with logarithmically many rungs
(initial lateral width $N_0$; culling factor $\eta>1$), in contrast to the
exponential growth of uncapped mainlines. Empirical evaluations on benchmark
tasks are in preparation and will be added in a future revision. In short, LToT
turns large test-time budgets into principled diversity while preserving
promotion discipline, mitigating saturation and myopia without inflating
compute.

</details>


### [95] [Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation](https://arxiv.org/abs/2510.01528)
*Daniel Zhao,Abhilash Shankarampeta,Lanxiang Hu,Tajana Rosing,Hao Zhang*

Main category: cs.AI

TL;DR: A method using sparse autoencoders and clustering to analyze LLM token representations and guide mathematical reasoning by balancing exploitation of known patterns with exploration of new approaches.


<details>
  <summary>Details</summary>
Motivation: To improve mathematical reasoning in LLMs by understanding and guiding their internal reasoning processes through token representation analysis.

Method: Train sparse autoencoders to generate sparse token representations, apply k-means clustering to build transition graphs, define reward functions based on edge weights to quantify reasoning adherence, and measure generation diversity from clustering.

Result: The approach successfully identifies exploitative reasoning trajectories and assesses exploration extent, showing that balancing both is crucial for high accuracy in mathematical reasoning.

Conclusion: Sparse autoencoders can serve as scalable reward models to guide LLM generations, ensuring optimal balance between exploitation and exploration for higher-quality mathematical reasoning.

Abstract: We propose a novel method that leverages sparse autoencoders (SAEs) and
clustering techniques to analyze the internal token representations of large
language models (LLMs) and guide generations in mathematical reasoning tasks.
Our approach first trains an SAE to generate sparse vector representations for
training tokens, then applies k-means clustering to construct a graph where
vertices represent token clusters and weighted edges capture sequential token
transitions. Using this graph, we define an edge-weight based reward function
to quantify adherence to established reasoning traces, thereby identifying
exploitative reasoning trajectories. Additionally, we measure generation
diversity from clustering to assess the extent of exploration. Our findings
indicate that balancing both exploitation and exploration is crucial for
achieving high accuracy in mathematical reasoning tasks. During generation, the
SAE can serve as a scalable reward model to guide generations, ensuring a
balanced trade-off between exploitation and exploration. This prevents extreme
behaviors in either direction, ultimately fostering a higher-quality reasoning
process in LLMs.

</details>


### [96] [LOGicalThought: Logic-Based Ontological Grounding of LLMs for High-Assurance Reasoning](https://arxiv.org/abs/2510.01530)
*Navapat Nananukul,Yue Zhang,Ryan Lee,Eric Boxer,Jonathan May,Vibhav Giridhar Gogate,Jay Pujara,Mayank Kejriwal*

Main category: cs.AI

TL;DR: LOGicalThought (LogT) is a neurosymbolic architecture that combines LLMs with logical reasoning to handle defeasible logic in high-assurance domains, improving reasoning performance by 11.84% across benchmarks.


<details>
  <summary>Details</summary>
Motivation: High-assurance domains like law and medicine require accurate, verifiable reasoning with explicit evidence grounding, but LLMs struggle with defeasible logic where exceptions can invalidate general rules.

Method: LogT uses an advanced logical language and reasoner with LLMs to create dual symbolic graph and logic-based contexts, transforming long-form guidelines into compact grounded evaluation.

Result: LogT improves overall performance by 11.84% across all LLMs, with significant gains in negation (+10.2%), implication (+13.2%), and defeasible reasoning (+5.5%) compared to strongest baselines.

Conclusion: The neurosymbolic approach of LogT effectively addresses the challenges of defeasible reasoning in high-assurance domains by combining LLMs' language processing with formal logical reasoning.

Abstract: High-assurance reasoning, particularly in critical domains such as law and
medicine, requires conclusions that are accurate, verifiable, and explicitly
grounded in evidence. This reasoning relies on premises codified from rules,
statutes, and contracts, inherently involving defeasible or non-monotonic logic
due to numerous exceptions, where the introduction of a single fact can
invalidate general rules, posing significant challenges. While large language
models (LLMs) excel at processing natural language, their capabilities in
standard inference tasks do not translate to the rigorous reasoning required
over high-assurance text guidelines. Core reasoning challenges within such
texts often manifest specific logical structures involving negation,
implication, and, most critically, defeasible rules and exceptions. In this
paper, we propose a novel neurosymbolically-grounded architecture called
LOGicalThought (LogT) that uses an advanced logical language and reasoner in
conjunction with an LLM to construct a dual symbolic graph context and
logic-based context. These two context representations transform the problem
from inference over long-form guidelines into a compact grounded evaluation.
Evaluated on four multi-domain benchmarks against four baselines, LogT improves
overall performance by 11.84% across all LLMs. Performance improves
significantly across all three modes of reasoning: by up to +10.2% on negation,
+13.2% on implication, and +5.5% on defeasible reasoning compared to the
strongest baseline.

</details>


### [97] [Information Seeking for Robust Decision Making under Partial Observability](https://arxiv.org/abs/2510.01531)
*Djengo Cyun-Jyun Fang,Tsung-Wei Ke*

Main category: cs.AI

TL;DR: InfoSeeker is an LLM planning framework that integrates task-oriented planning with active information seeking to handle uncertainty in both observations and environmental dynamics, achieving 74% performance gain over prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM planning agents address observational uncertainty but overlook discrepancies between their internal dynamics and the actual environment, limiting their effectiveness in partially observable environments with noisy dynamics.

Method: InfoSeeker prompts LLMs to actively gather information by planning actions to validate understanding, detect environmental changes, or test hypotheses before generating or revising task-oriented plans.

Result: InfoSeeker achieves 74% absolute performance gain over prior methods without sacrificing sample efficiency, and generalizes well across LLMs and established benchmarks including robotic manipulation and web navigation.

Conclusion: Tight integration of planning and information seeking is crucial for robust behavior in partially observable environments, and InfoSeeker demonstrates the effectiveness of this approach.

Abstract: Explicit information seeking is essential to human problem-solving in
practical environments characterized by incomplete information and noisy
dynamics. When the true environmental state is not directly observable, humans
seek information to update their internal dynamics and inform future
decision-making. Although existing Large Language Model (LLM) planning agents
have addressed observational uncertainty, they often overlook discrepancies
between their internal dynamics and the actual environment. We introduce
Information Seeking Decision Planner (InfoSeeker), an LLM decision-making
framework that integrates task-oriented planning with information seeking to
align internal dynamics and make optimal decisions under uncertainty in both
agent observations and environmental dynamics. InfoSeeker prompts an LLM to
actively gather information by planning actions to validate its understanding,
detect environmental changes, or test hypotheses before generating or revising
task-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark
suite featuring partially observable environments with incomplete observations
and uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%
absolute performance gain over prior methods without sacrificing sample
efficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms
baselines on established benchmarks such as robotic manipulation and web
navigation. These findings underscore the importance of tightly integrating
planning and information seeking for robust behavior in partially observable
environments. The project page is available at https://infoseekerllm.github.io

</details>


### [98] [Step-Aware Policy Optimization for Reasoning in Diffusion Large Language Models](https://arxiv.org/abs/2510.01544)
*Shaoan Xie,Lingjing Kong,Xiangchen Song,Xinshuai Dong,Guangyi Chen,Eric P. Xing,Kun Zhang*

Main category: cs.AI

TL;DR: SAPO is a novel RL algorithm that aligns diffusion language models with latent reasoning hierarchies using process-based rewards, improving complex reasoning performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current RL approaches for diffusion language models rely on sparse outcome-based rewards, which can reinforce flawed reasoning paths that coincidentally lead to correct answers, creating a mismatch with natural reasoning structure.

Method: Proposes Step-Aware Policy Optimization (SAPO) - an RL algorithm that uses process-based reward functions to guide the denoising process, encouraging incremental progress and structured reasoning paths.

Result: Empirical results show significant performance improvements on challenging reasoning benchmarks and enhanced interpretability of the generation process.

Conclusion: The principled approach of aligning diffusion language models with latent reasoning hierarchies through process-based rewards effectively addresses unstructured refinement and improves complex reasoning capabilities.

Abstract: Diffusion language models (dLLMs) offer a promising, non-autoregressive
paradigm for text generation, yet training them for complex reasoning remains a
key challenge. Current reinforcement learning approaches often rely on sparse,
outcome-based rewards, which can reinforce flawed reasoning paths that lead to
coincidentally correct answers. We argue that this stems from a fundamental
mismatch with the natural structure of reasoning. We first propose a
theoretical framework that formalizes complex problem solving as a hierarchical
selection process, where an intractable global constraint is decomposed into a
series of simpler, localized logical steps. This framework provides a
principled foundation for algorithm design, including theoretical insights into
the identifiability of this latent reasoning structure. Motivated by this
theory, we identify unstructured refinement -- a failure mode where a model's
iterative steps do not contribute meaningfully to the solution -- as a core
deficiency in existing methods. We then introduce Step-Aware Policy
Optimization (SAPO), a novel RL algorithm that aligns the dLLM's denoising
process with the latent reasoning hierarchy. By using a process-based reward
function that encourages incremental progress, SAPO guides the model to learn
structured, coherent reasoning paths. Our empirical results show that this
principled approach significantly improves performance on challenging reasoning
benchmarks and enhances the interpretability of the generation process.

</details>


### [99] [InvThink: Towards AI Safety via Inverse Reasoning](https://arxiv.org/abs/2510.01569)
*Yubin Kim,Taehan Kim,Eugene Park,Chunjong Park,Cynthia Breazeal,Daniel McDuff,Hae Won Park*

Main category: cs.AI

TL;DR: InvThink enables LLMs to reason through potential failure modes before generating responses, improving safety while preserving general capabilities.


<details>
  <summary>Details</summary>
Motivation: To address safety concerns in LLMs by developing a method that proactively identifies and avoids potential harms before generating responses, rather than just optimizing for safe outputs.

Method: Instructs models to: 1) enumerate potential harms, 2) analyze their consequences, and 3) generate safe outputs that proactively avoid these risks. Implemented via supervised fine-tuning and reinforcement learning across three LLM families.

Result: Achieves up to 15.7% reduction in harmful responses compared to baseline methods like SafetyPrompt. Shows stronger safety scaling with model size, mitigates safety tax (preserves general reasoning), and excels in high-stakes domains including medicine, finance, law, and agentic risk scenarios.

Conclusion: Inverse reasoning provides a scalable and generalizable path toward safer, more capable language models.

Abstract: We present InvThink, a simple yet powerful approach that gives large language
models (LLMs) the capability of inverse thinking: reasoning through failure
modes before generating responses. Unlike existing safety alignment methods
that optimize directly for safe response, InvThink instructs models to 1)
enumerate potential harms, 2) analyze their consequences, and 3) generate safe
outputs that proactively avoid these risks. Our method reveals three key
findings: (i) safety improvements show stronger scaling with model size
compared to existing safety methods. (ii) InvThink mitigates safety tax; by
training models to systematically consider failure modes, it preserves general
reasoning capabilities on standard benchmarks. (iii) beyond general safety
tasks, InvThink excels in high-stakes domains including external-facing
(medicine, finance, law) and agentic (blackmail, murder) risk scenarios,
achieving up to 15.7% reduction in harmful responses compared to baseline
methods like SafetyPrompt. We further implement InvThink via supervised
fine-tuning, and reinforcement learning across three LLM families. These
results suggest that inverse reasoning provides a scalable and generalizable
path toward safer, more capable language models.

</details>


### [100] [AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.01586)
*Zhenyu Pan,Yiting Zhang,Zhuo Liu,Yolo Yunlong Tang,Zeliang Zhang,Haozheng Luo,Yuwei Han,Jianshu Zhang,Dennis Wu,Hong-Yu Chen,Haoran Lu,Haoyang Fang,Manling Li,Chenliang Xu,Philip S. Yu,Han Liu*

Main category: cs.AI

TL;DR: AdvEvo-MARL is a co-evolutionary multi-agent reinforcement learning framework that internalizes safety into task agents through adversarial training, eliminating the need for external guard modules while maintaining task performance.


<details>
  <summary>Details</summary>
Motivation: Existing defenses for LLM-based multi-agent systems either rely on underperforming self-verification or external guard modules that create system overhead and single-point-of-failure risks.

Method: Jointly optimizes attackers (synthesizing jailbreak prompts) and defenders (task agents trained to resist attacks) in adversarial learning environments with a public baseline for advantage estimation that enables group-level coordination.

Result: Consistently keeps attack-success rate below 20% (vs. 38.33% in baselines) while preserving and sometimes improving task accuracy (up to +3.67% on reasoning tasks).

Conclusion: Safety and utility can be jointly improved without relying on extra guard agents or added system overhead through adversarial co-evolutionary training.

Abstract: LLM-based multi-agent systems excel at planning, tool use, and role
coordination, but their openness and interaction complexity also expose them to
jailbreak, prompt-injection, and adversarial collaboration. Existing defenses
fall into two lines: (i) self-verification that asks each agent to pre-filter
unsafe instructions before execution, and (ii) external guard modules that
police behaviors. The former often underperforms because a standalone agent
lacks sufficient capacity to detect cross-agent unsafe chains and
delegation-induced risks; the latter increases system overhead and creates a
single-point-of-failure-once compromised, system-wide safety collapses, and
adding more guards worsens cost and complexity. To solve these challenges, we
propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning
framework that internalizes safety into task agents. Rather than relying on
external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize
evolving jailbreak prompts) and defenders (task agents trained to both
accomplish their duties and resist attacks) in adversarial learning
environments. To stabilize learning and foster cooperation, we introduce a
public baseline for advantage estimation: agents within the same functional
group share a group-level mean-return baseline, enabling lower-variance updates
and stronger intra-group coordination. Across representative attack scenarios,
AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas
baselines reach up to 38.33%, while preserving-and sometimes improving-task
accuracy (up to +3.67% on reasoning tasks). These results show that safety and
utility can be jointly improved without relying on extra guard agents or added
system overhead.

</details>


### [101] [AgentRec: Next-Generation LLM-Powered Multi-Agent Collaborative Recommendation with Adaptive Intelligence](https://arxiv.org/abs/2510.01609)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Lau*

Main category: cs.AI

TL;DR: AgentRec is a multi-agent LLM framework that improves conversational recommender systems by using specialized agents for different tasks, adaptive coordination, and a three-tier learning strategy, achieving better performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing conversational recommender systems struggle with dynamic user preferences, conversation coherence, and balancing multiple ranking objectives simultaneously.

Method: Uses hierarchical LLM-powered agents for conversation understanding, preference modeling, context awareness, and dynamic ranking with adaptive weighting. Implements three-tier learning: rapid response, intelligent reasoning, and deep collaboration.

Result: Achieved 2.8% improvement in conversation success rate, 1.9% better recommendation accuracy (NDCG@10), and 3.2% better conversation efficiency while maintaining comparable computational costs.

Conclusion: AgentRec demonstrates that multi-agent collaborative frameworks with adaptive intelligence can effectively address key challenges in conversational recommender systems.

Abstract: Interactive conversational recommender systems have gained significant
attention for their ability to capture user preferences through natural
language interactions. However, existing approaches face substantial challenges
in handling dynamic user preferences, maintaining conversation coherence, and
balancing multiple ranking objectives simultaneously. This paper introduces
AgentRec, a next-generation LLM-powered multi-agent collaborative
recommendation framework that addresses these limitations through hierarchical
agent networks with adaptive intelligence. Our approach employs specialized
LLM-powered agents for conversation understanding, preference modeling, context
awareness, and dynamic ranking, coordinated through an adaptive weighting
mechanism that learns from interaction patterns. We propose a three-tier
learning strategy combining rapid response for simple queries, intelligent
reasoning for complex preferences, and deep collaboration for challenging
scenarios. Extensive experiments on three real-world datasets demonstrate that
AgentRec achieves consistent improvements over state-of-the-art baselines, with
2.8\% enhancement in conversation success rate, 1.9\% improvement in
recommendation accuracy (NDCG@10), and 3.2\% better conversation efficiency
while maintaining comparable computational costs through intelligent agent
coordination.

</details>


### [102] [PychoBench: Evaluating the Psychology Intelligence of Large Language Models](https://arxiv.org/abs/2510.01611)
*Min Zeng*

Main category: cs.AI

TL;DR: This paper introduces PsychoBench, a benchmark based on the U.S. National Counselor Certification Exam to assess whether LLMs can qualify as psychological counselors by testing their psychological knowledge.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs can be effectively applied to psychological counseling by determining if they meet the qualification standards required for human counselors, specifically the ability to pass professional certification exams.

Method: Developed PsychoBench, a comprehensive benchmark comprising approximately 2,252 single-choice questions from U.S. national counselor examinations, covering various psychology sub-disciplines and requiring deep understanding.

Result: Advanced models like GPT-4o, Llama3.3-70B, and Gemma3-27B achieved well above the 70% passing threshold, while smaller open-source models (Qwen2.5-7B, Mistral-7B) remained far below it.

Conclusion: Only frontier LLMs currently meet counseling exam standards, highlighting both the promise and challenges of developing psychology-oriented LLMs for counseling applications.

Abstract: Large Language Models (LLMs) have demonstrated remarkable success across a
wide range of industries, primarily due to their impressive generative
abilities. Yet, their potential in applications requiring cognitive abilities,
such as psychological counseling, remains largely untapped. This paper
investigates the key question: Can LLMs be effectively applied to psychological
counseling? To determine whether an LLM can effectively take on the role of a
psychological counselor, the first step is to assess whether it meets the
qualifications required for such a role, namely the ability to pass the U.S.
National Counselor Certification Exam (NCE). This is because, just as a human
counselor must pass a certification exam to practice, an LLM must demonstrate
sufficient psychological knowledge to meet the standards required for such a
role. To address this, we introduce PsychoBench, a benchmark grounded in
U.S.national counselor examinations, a licensure test for professional
counselors that requires about 70% accuracy to pass. PsychoBench comprises
approximately 2,252 carefully curated single-choice questions, crafted to
require deep understanding and broad enough to cover various sub-disciplines of
psychology. This benchmark provides a comprehensive assessment of an LLM's
ability to function as a counselor. Our evaluation shows that advanced models
such as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passing
threshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B)
remain far below it. These results suggest that only frontier LLMs are
currently capable of meeting counseling exam standards, highlighting both the
promise and the challenges of developing psychology-oriented LLMs.

</details>


### [103] [Learning to Decide with Just Enough: Information-Theoretic Context Summarization for CDMPs](https://arxiv.org/abs/2510.01620)
*Peidong Liu,Junjiang Lin,Shaowen Wang,Yao Xu,Haiqing Li,Xuhao Xie,Siyi Wu,Hao Li*

Main category: cs.AI

TL;DR: LLM-based summarization compresses contextual inputs in CMDPs to improve decision-making efficiency and performance across various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing CMDP methods struggle with high-dimensional contexts, leading to computational inefficiency and unstable performance, especially in unstructured environments.

Method: Information-theoretic summarization using LLMs to compress contextual inputs into low-dimensional, semantically rich summaries that augment states while preserving decision-critical information.

Result: Outperforms raw-context and non-context baselines across discrete, continuous, visual, and recommendation benchmarks, improving reward, success rate, sample efficiency while reducing latency and memory usage.

Conclusion: LLM-based summarization provides a scalable and interpretable solution for efficient decision-making in context-rich, resource-constrained environments, with theoretical regret bounds and latency-entropy trade-off characterization.

Abstract: Contextual Markov Decision Processes (CMDPs) offer a framework for sequential
decision-making under external signals, but existing methods often fail to
generalize in high-dimensional or unstructured contexts, resulting in excessive
computation and unstable performance. We propose an information-theoretic
summarization approach that uses large language models (LLMs) to compress
contextual inputs into low-dimensional, semantically rich summaries. These
summaries augment states by preserving decision-critical cues while reducing
redundancy. Building on the notion of approximate context sufficiency, we
provide, to our knowledge, the first regret bounds and a latency-entropy
trade-off characterization for CMDPs. Our analysis clarifies how
informativeness impacts computational cost. Experiments across discrete,
continuous, visual, and recommendation benchmarks show that our method
outperforms raw-context and non-context baselines, improving reward, success
rate, and sample efficiency, while reducing latency and memory usage. These
findings demonstrate that LLM-based summarization offers a scalable and
interpretable solution for efficient decision-making in context-rich,
resource-constrained environments.

</details>


### [104] [Understanding the Geospatial Reasoning Capabilities of LLMs: A Trajectory Recovery Perspective](https://arxiv.org/abs/2510.01639)
*Thinh Hung Truong,Jey Han Lau,Jianzhong Qi*

Main category: cs.AI

TL;DR: LLMs can read road network maps and perform navigation through trajectory recovery, outperforming specialized models with strong zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: To explore the geospatial reasoning capabilities of LLMs, specifically whether they can understand road network maps and perform navigation tasks without external tools.

Method: Framed trajectory recovery as a proxy task using GLOBALTRACE dataset with 4,000+ real-world trajectories. Used road network as context in a prompting framework to enable LLMs to reconstruct masked GPS traces.

Result: LLMs outperformed off-the-shelf baselines and specialized trajectory recovery models, showing strong zero-shot generalization. Models demonstrated strong comprehension of road networks and coordinate systems, though with systematic regional and transportation mode biases.

Conclusion: LLMs can effectively reason over maps for navigation tasks and enhance navigation experiences by incorporating user preferences flexibly, demonstrating significant geospatial reasoning capabilities.

Abstract: We explore the geospatial reasoning capabilities of Large Language Models
(LLMs), specifically, whether LLMs can read road network maps and perform
navigation. We frame trajectory recovery as a proxy task, which requires models
to reconstruct masked GPS traces, and introduce GLOBALTRACE, a dataset with
over 4,000 real-world trajectories across diverse regions and transportation
modes. Using road network as context, our prompting framework enables LLMs to
generate valid paths without accessing any external navigation tools.
Experiments show that LLMs outperform off-the-shelf baselines and specialized
trajectory recovery models, with strong zero-shot generalization. Fine-grained
analysis shows that LLMs have strong comprehension of the road network and
coordinate systems, but also pose systematic biases with respect to regions and
transportation modes. Finally, we demonstrate how LLMs can enhance navigation
experiences by reasoning over maps in flexible ways to incorporate user
preferences.

</details>


### [105] [GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents](https://arxiv.org/abs/2510.01664)
*Yejin Kim,Youngbin Lee,Juhyeong Kim,Yongjae Lee*

Main category: cs.AI

TL;DR: GuruAgents are AI agents that operationalize legendary investment gurus' strategies through prompt engineering, achieving up to 42.2% CAGR in backtesting.


<details>
  <summary>Details</summary>
Motivation: To translate qualitative investment philosophies of legendary gurus into reproducible, quantitative strategies using AI agents.

Method: Developed five distinct GuruAgents by encoding investment gurus' philosophies into LLM prompts with financial tools and deterministic reasoning pipeline, then backtested on NASDAQ-100 constituents from Q4 2023 to Q2 2025.

Result: Buffett GuruAgent achieved highest performance with 42.2% CAGR, significantly outperforming benchmarks, while other agents showed varied results. All agents exhibited unique behaviors driven by their prompted personas.

Conclusion: Prompt engineering can successfully translate qualitative investment philosophies into reproducible quantitative strategies, opening new directions for automated systematic investing.

Abstract: This study demonstrates that GuruAgents, prompt-guided AI agents, can
systematically operationalize the strategies of legendary investment gurus. We
develop five distinct GuruAgents, each designed to emulate an iconic investor,
by encoding their distinct philosophies into LLM prompts that integrate
financial tools and a deterministic reasoning pipeline. In a backtest on
NASDAQ-100 constituents from Q4 2023 to Q2 2025, the GuruAgents exhibit unique
behaviors driven by their prompted personas. The Buffett GuruAgent achieves the
highest performance, delivering a 42.2\% CAGR that significantly outperforms
benchmarks, while other agents show varied results. These findings confirm that
prompt engineering can successfully translate the qualitative philosophies of
investment gurus into reproducible, quantitative strategies, highlighting a
novel direction for automated systematic investing. The source code and data
are available at https://github.com/yejining99/GuruAgents.

</details>


### [106] [Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness](https://arxiv.org/abs/2510.01670)
*Erfan Shayegani,Keegan Hines,Yue Dong,Nael Abu-Ghazaleh,Roman Lutz,Spencer Whitehead,Vidhisha Balachandran,Besmira Nushi,Vibhav Vineet*

Main category: cs.AI

TL;DR: Computer-Use Agents (CUAs) exhibit Blind Goal-Directedness (BGD) - pursuing goals regardless of feasibility, safety, or context. The paper introduces BLIND-ACT benchmark to evaluate BGD patterns and shows high BGD rates (80.8%) across nine frontier models, highlighting persistent safety risks.


<details>
  <summary>Details</summary>
Motivation: To identify and characterize the fundamental risk of Blind Goal-Directedness in Computer-Use Agents, where agents pursue user goals without considering feasibility, safety, reliability, or context.

Method: Developed BLIND-ACT benchmark with 90 tasks capturing three BGD patterns: lack of contextual reasoning, assumptions under ambiguity, and contradictory goals. Used OSWorld environments and LLM-based judges for evaluation, achieving 93.75% agreement with human annotations.

Result: Evaluated nine frontier models (including Claude Sonnet, Opus 4, Computer-Use-Preview, GPT-5) and observed high average BGD rates of 80.8%. Prompting-based interventions reduced but did not eliminate BGD. Identified failure modes: execution-first bias, thought-action disconnect, and request-primacy.

Conclusion: BGD exposes subtle risks in CUAs even with non-harmful inputs. Current interventions are insufficient, requiring stronger training- or inference-time solutions. BLIND-ACT provides foundation for future research on mitigating this fundamental safety risk in CUA deployment.

Abstract: Computer-Use Agents (CUAs) are an increasingly deployed class of agents that
take actions on GUIs to accomplish user goals. In this paper, we show that CUAs
consistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals
regardless of feasibility, safety, reliability, or context. We characterize
three prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)
assumptions and decisions under ambiguity, and (iii) contradictory or
infeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these
three patterns. Built on OSWorld, BLIND-ACT provides realistic environments and
employs LLM-based judges to evaluate agent behavior, achieving 93.75% agreement
with human annotations. We use BLIND-ACT to evaluate nine frontier models,
including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing
high average BGD rates (80.8%) across them. We show that BGD exposes subtle
risks that arise even when inputs are not directly harmful. While
prompting-based interventions lower BGD levels, substantial risk persists,
highlighting the need for stronger training- or inference-time interventions.
Qualitative analysis reveals observed failure modes: execution-first bias
(focusing on how to act over whether to act), thought-action disconnect
(execution diverging from reasoning), and request-primacy (justifying actions
due to user request). Identifying BGD and introducing BLIND-ACT establishes a
foundation for future research on studying and mitigating this fundamental risk
and ensuring safe CUA deployment.

</details>


### [107] [A Locally Executable AI System for Improving Preoperative Patient Communication: A Multi-Domain Clinical Evaluation](https://arxiv.org/abs/2510.01671)
*Motoki Sato,Yuki Matsushita,Hidekazu Takahashi,Tomoaki Kakazu,Sou Nagata,Mizuho Ohnuma,Atsushi Yoshikawa,Masayuki Yamamura*

Main category: cs.AI

TL;DR: LENOHA is a safety-first clinical system that uses sentence-transformer classifiers to route patient queries and return verbatim answers from curated FAQs, avoiding free-text generation in clinical contexts.


<details>
  <summary>Details</summary>
Motivation: Address unmet patient pre-procedural information needs while overcoming time constraints and privacy limitations in clinical workflows.

Method: Uses high-precision sentence-transformer classifiers to route inputs and return verbatim answers from clinician-curated FAQs, eliminating free-text generation in clinical path. Evaluated on tooth extraction and gastroscopy domains with expert-reviewed validation and test sets.

Result: E5-large-instruct achieved 0.983 accuracy, 0.996 AUC with only 7 errors, comparable to GPT-4o. Non-generative clinical path consumes ~1.0 mWh per input vs ~168 mWh for small-talk from local SLM (170x difference) with ~0.10s latency.

Conclusion: Near-frontier discrimination can be achieved while structurally avoiding generation-induced errors by returning vetted FAQ answers verbatim, supporting privacy, sustainability and equitable deployment in bandwidth-limited environments.

Abstract: Patients awaiting invasive procedures often have unanswered pre-procedural
questions; however, time-pressured workflows and privacy constraints limit
personalized counseling. We present LENOHA (Low Energy, No Hallucination, Leave
No One Behind Architecture), a safety-first, local-first system that routes
inputs with a high-precision sentence-transformer classifier and returns
verbatim answers from a clinician-curated FAQ for clinical queries, eliminating
free-text generation in the clinical path. We evaluated two domains (tooth
extraction and gastroscopy) using expert-reviewed validation sets
(n=400/domain) for thresholding and independent test sets (n=200/domain). Among
the four encoders, E5-large-instruct (560M) achieved an overall accuracy of
0.983 (95% CI 0.964-0.991), AUC 0.996, and seven total errors, which were
statistically indistinguishable from GPT-4o on this task; Gemini made no errors
on this test set. Energy logging shows that the non-generative clinical path
consumes ~1.0 mWh per input versus ~168 mWh per small-talk reply from a local
8B SLM, a ~170x difference, while maintaining ~0.10 s latency on a single
on-prem GPU. These results indicate that near-frontier discrimination and
generation-induced errors are structurally avoided in the clinical path by
returning vetted FAQ answers verbatim, supporting privacy, sustainability, and
equitable deployment in bandwidth-limited environments.

</details>


### [108] [Improving AGI Evaluation: A Data Science Perspective](https://arxiv.org/abs/2510.01687)
*John Hawkins*

Main category: cs.AI

TL;DR: The paper argues that current AGI evaluation methods based on synthetic tasks are flawed and proposes an alternative approach focused on robust task execution and deployment competence.


<details>
  <summary>Details</summary>
Motivation: Current AGI evaluation methods are inadequate because they rely on synthetic tasks designed from intuitions about intelligence, which have historically performed poorly in AI development.

Method: Proposes an alternative design philosophy that evaluates robust task execution and deployment competence, drawing from common data science practices for reliable system deployment.

Result: Provides practical examples of what this alternative AGI evaluation approach would entail, focusing on demonstrating competence through reliable task execution.

Conclusion: AGI evaluation should shift from synthetic task-based approaches to methods that demonstrate robust task execution and deployment competence, similar to data science practices for reliable system deployment.

Abstract: Evaluation of potential AGI systems and methods is difficult due to the
breadth of the engineering goal. We have no methods for perfect evaluation of
the end state, and instead measure performance on small tests designed to
provide directional indication that we are approaching AGI. In this work we
argue that AGI evaluation methods have been dominated by a design philosophy
that uses our intuitions of what intelligence is to create synthetic tasks,
that have performed poorly in the history of AI. Instead we argue for an
alternative design philosophy focused on evaluating robust task execution that
seeks to demonstrate AGI through competence. This perspective is developed from
common practices in data science that are used to show that a system can be
reliably deployed. We provide practical examples of what this would mean for
AGI evaluation.

</details>


### [109] [VaPR -- Vision-language Preference alignment for Reasoning](https://arxiv.org/abs/2510.01700)
*Rohan Wadhawan,Fabrice Y Harel-Canada,Zi-Yi Dou,Suhaila Shakiah,Robinson Piramuthu,Nanyun Peng*

Main category: cs.AI

TL;DR: VaPR introduces a hard-negative response generation framework using LLM-guided editing to create rejected responses with targeted errors while maintaining stylistic and length similarity to accepted ones, addressing noise in synthetic preference annotations for LVLM alignment.


<details>
  <summary>Details</summary>
Motivation: Existing preference finetuning methods overlook the prevalence of noise in synthetic preference annotations, particularly stylistic and length biases that can affect LVLM alignment with human preferences.

Method: Developed a hard-negative response generation framework using LLM-guided response editing to produce rejected responses with targeted errors while maintaining stylistic and length similarity to accepted responses. Created VaPR dataset with 30K samples for finetuning LVLMs.

Result: VaPR models achieved significant performance improvements: 6.5% average gain for LLaVA, 4.0% for Qwen2VL, and 1.5% for Qwen2.5VL across ten benchmarks, with notable improvements on reasoning tasks. Also reduced tendency to answer "Yes" in binary questions.

Conclusion: The framework effectively addresses noise in synthetic preference data and generalizes to open-source LLMs as editors, with VaPR-OS achieving ~99% performance of GPT-4o-synthesized data. Scaling analysis shows consistent performance improvements with data size.

Abstract: Preference finetuning methods like Direct Preference Optimization (DPO) with
AI-generated feedback have shown promise in aligning Large Vision-Language
Models (LVLMs) with human preferences. However, existing techniques overlook
the prevalence of noise in synthetic preference annotations in the form of
stylistic and length biases. To this end, we introduce a hard-negative response
generation framework based on LLM-guided response editing, that produces
rejected responses with targeted errors, maintaining stylistic and length
similarity to the accepted ones. Using this framework, we develop the VaPR
dataset, comprising 30K high-quality samples, to finetune three LVLM families:
LLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver
significant performance improvements across ten benchmarks, achieving average
gains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable
improvements on reasoning tasks. A scaling analysis shows that performance
consistently improves with data size, with LLaVA models benefiting even at
smaller scales. Moreover, VaPR reduces the tendency to answer "Yes" in binary
questions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we
show that the framework generalizes to open-source LLMs as editors, with models
trained on VaPR-OS achieving ~99% of the performance of models trained on
\name, which is synthesized using GPT-4o. Our data, models, and code can be
found on the project page https://vap-r.github.io

</details>


### [110] [MetaboT: AI-based agent for natural language-based interaction with metabolomics knowledge graphs](https://arxiv.org/abs/2510.01724)
*Madina Bekbergenova,Lucas Pradi,Benjamin Navet,Emma Tysinger,Franck Michel,Matthieu Feraud,Yousouf Taghzouti,Yan Zhou Chen,Olivier Kirchhoffer,Florence Mehl,Martin Legrand,Tao Jiang,Marco Pagni,Soha Hassoun,Jean-Luc Wolfender,Wout Bittremieux,Fabien Gandon,Louis-F√©lix Nothias*

Main category: cs.AI

TL;DR: MetaboT is an AI system that uses large language models to translate natural language questions into SPARQL queries for metabolomics knowledge graphs, achieving 83.67% accuracy compared to 8.16% for standard LLMs.


<details>
  <summary>Details</summary>
Motivation: To overcome the technical barriers of using knowledge graphs in metabolomics, which require deep understanding of ontology and query language syntax, by enabling natural language interaction.

Method: Multi-agent system using LangChain/LangGraph with specialized agents for validation, chemical conversion, and SPARQL query generation, tested on the Experimental Natural Products Knowledge Graph (ENPKG).

Result: Achieved 83.67% accuracy on curated metabolomics questions, significantly outperforming baseline GPT-4o (8.16%), demonstrating effective automated query generation and execution.

Conclusion: MetaboT successfully bridges the gap between complex semantic technologies and user-friendly interaction, enabling researchers to access structured metabolomics data through natural language queries while maintaining domain-specific standards.

Abstract: Mass spectrometry metabolomics generates vast amounts of data requiring
advanced methods for interpretation. Knowledge graphs address these challenges
by structuring mass spectrometry data, metabolite information, and their
relationships into a connected network (Gaudry et al. 2024). However, effective
use of a knowledge graph demands an in-depth understanding of its ontology and
its query language syntax. To overcome this, we designed MetaboT, an AI system
utilizing large language models (LLMs) to translate user questions into SPARQL
semantic query language for operating on knowledge graphs (Steve Harris 2013).
We demonstrate its effectiveness using the Experimental Natural Products
Knowledge Graph (ENPKG), a large-scale public knowledge graph for plant natural
products (Gaudry et al. 2024).MetaboT employs specialized AI agents for
handling user queries and interacting with the knowledge graph by breaking down
complex tasks into discrete components, each managed by a specialised agent
(Fig. 1a). The multi-agent system is constructed using the LangChain and
LangGraph libraries, which facilitate the integration of LLMs with external
tools and information sources (LangChain, n.d.). The query generation process
follows a structured workflow. First, the Entry Agent determines if the
question is new or a follow-up to previous interactions. New questions are
forwarded to the Validator Agent, which verifies if the question is related to
the knowledge graph. Then, the valid question is sent to the Supervisor Agent,
which identifies if the question requires chemical conversions or standardized
identifiers. In this case it delegates the question to the Knowledge Graph
Agent, which can use tools to extract necessary details, such as URIs or
taxonomies of chemical names, from the user query. Finally, an agent
responsible for crafting the SPARQL queries equipped with the ontology of the
knowledge graph uses the provided identifiers to generate the query. Then, the
system executes the generated query against the metabolomics knowledge graph
and returns structured results to the user (Fig. 1b). To assess the performance
of MetaboT we have curated 50 metabolomics-related questions and their expected
answers. In addition to submitting these questions to MetaboT, we evaluated a
baseline by submitting them to a standard LLM (GPT-4o) with a prompt that
incorporated the knowledge graph ontology but did not provide specific entity
IDs. This baseline achieved only 8.16% accuracy, compared to MetaboT's 83.67%,
underscoring the necessity of our multi-agent system for accurately retrieving
entities and generating correct SPARQL queries. MetaboT demonstrates promising
performance as a conversational question-answering assistant, enabling
researchers to retrieve structured metabolomics data through natural language
queries. By automating the generation and execution of SPARQL queries, it
removes technical barriers that have traditionally hindered access to knowledge
graphs. Importantly, MetaboT leverages the capabilities of LLMs while
maintaining experimentally grounded query generation, ensuring that outputs
remain aligned with domain-specific standards and data structures. This
approach facilitates data-driven discoveries by bridging the gap between
complex semantic technologies and user-friendly interaction. MetaboT is
accessible at [https://metabot.holobiomicslab.eu/], and its source code is
available at [https://github.com/HolobiomicsLab/MetaboT].

</details>


### [111] [A cybersecurity AI agent selection and decision support framework](https://arxiv.org/abs/2510.01751)
*Masike Malatji*

Main category: cs.AI

TL;DR: A structured decision support framework that aligns AI agent architectures with NIST Cybersecurity Framework 2.0 for systematic cybersecurity enhancement.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between theoretical AI constructs and operational cybersecurity demands by providing a transparent methodology for selecting and deploying AI solutions against contemporary cyber threats.

Method: Granular decomposition of NIST CSF 2.0 functions into specific tasks, linking AI agent properties (autonomy, adaptive learning, real-time responsiveness) to security requirements, and outlining graduated autonomy levels for different cybersecurity maturity stages.

Result: Conceptual validation shows the framework enables tailored AI agent deployments that enhance situational awareness, accelerate response times, and fortify long-term resilience through adaptive risk management.

Conclusion: The research establishes a foundation for robust, empirically validated multi-agent systems that adhere to industry standards, providing unified detection, incident response, and governance strategy.

Abstract: This paper presents a novel, structured decision support framework that
systematically aligns diverse artificial intelligence (AI) agent architectures,
reactive, cognitive, hybrid, and learning, with the comprehensive National
Institute of Standards and Technology (NIST) Cybersecurity Framework (CSF) 2.0.
By integrating agent theory with industry guidelines, this framework provides a
transparent and stepwise methodology for selecting and deploying AI solutions
to address contemporary cyber threats. Employing a granular decomposition of
NIST CSF 2.0 functions into specific tasks, the study links essential AI agent
properties such as autonomy, adaptive learning, and real-time responsiveness to
each subcategory's security requirements. In addition, it outlines graduated
levels of autonomy (assisted, augmented, and fully autonomous) to accommodate
organisations at varying stages of cybersecurity maturity. This holistic
approach transcends isolated AI applications, providing a unified detection,
incident response, and governance strategy. Through conceptual validation, the
framework demonstrates how tailored AI agent deployments can align with
real-world constraints and risk profiles, enhancing situational awareness,
accelerating response times, and fortifying long-term resilience via adaptive
risk management. Ultimately, this research bridges the gap between theoretical
AI constructs and operational cybersecurity demands, establishing a foundation
for robust, empirically validated multi-agent systems that adhere to industry
standards.

</details>


### [112] [REBot: From RAG to CatRAG with Semantic Enrichment and Graph Routing](https://arxiv.org/abs/2510.01800)
*Thanh Ma,Tri-Tam La,Lam-Thu Le Huu,Minh-Nghi Nguyen,Khanh-Van Pham Luu,Huu-Hoa Nguyen*

Main category: cs.AI

TL;DR: REBot is an LLM-enhanced advisory chatbot for academic regulation advising that uses CatRAG, a hybrid retrieval-reasoning framework combining dense retrieval with graph-based reasoning on a hierarchical knowledge graph.


<details>
  <summary>Details</summary>
Motivation: Building effective academic regulation advising systems requires domain-specific regulatory resources, which is challenging to acquire and process.

Method: Proposed CatRAG framework integrates retrieval augmented generation with graph-based reasoning, using a hierarchical category-labeled knowledge graph with semantic features. A lightweight intent classifier routes queries to appropriate retrieval modules.

Result: Achieved state-of-the-art performance with 98.89% F1 score on regulation-specific classification and question answering tasks. Implemented a web application demonstrating practical value in real-world academic advising.

Conclusion: REBot with CatRAG framework effectively addresses the challenge of academic regulation advising by combining LLM capabilities with structured domain knowledge through hybrid retrieval-reasoning approach.

Abstract: Academic regulation advising is essential for helping students interpret and
comply with institutional policies, yet building effective systems requires
domain specific regulatory resources. To address this challenge, we propose
REBot, an LLM enhanced advisory chatbot powered by CatRAG, a hybrid retrieval
reasoning framework that integrates retrieval augmented generation with graph
based reasoning. CatRAG unifies dense retrieval and graph reasoning, supported
by a hierarchical, category labeled knowledge graph enriched with semantic
features for domain alignment. A lightweight intent classifier routes queries
to the appropriate retrieval modules, ensuring both factual accuracy and
contextual depth. We construct a regulation specific dataset and evaluate REBot
on classification and question answering tasks, achieving state of the art
performance with an F1 score of 98.89%. Finally, we implement a web application
that demonstrates the practical value of REBot in real world academic advising
scenarios.

</details>


### [113] [Human-AI Teaming Co-Learning in Military Operations](https://arxiv.org/abs/2510.01815)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: Proposes a trustworthy co-learning model for human-AI teaming in military operations with four key dimensions: adjustable autonomy, multi-layered control, bidirectional feedback, and collaborative decision-making.


<details>
  <summary>Details</summary>
Motivation: Address challenges and risks in building and deploying human-AI teaming systems in military operations from an internal perspective, focusing on responsibility, safety, and robustness aspects.

Method: Designs a co-learning model with four integrated dimensions: adjustable autonomy for dynamic calibration, multi-layered control for oversight, bidirectional feedback loops, and collaborative decision-making with confidence levels.

Result: The model enables continuous bidirectional exchange of insights between human and AI agents as they jointly adapt to evolving battlefield conditions.

Conclusion: Provides concrete exemplifications and recommendations for developing responsible and trustworthy human-AI teaming systems in military operations.

Abstract: In a time of rapidly evolving military threats and increasingly complex
operational environments, the integration of AI into military operations proves
significant advantages. At the same time, this implies various challenges and
risks regarding building and deploying human-AI teaming systems in an effective
and ethical manner. Currently, understanding and coping with them are often
tackled from an external perspective considering the human-AI teaming system as
a collective agent. Nevertheless, zooming into the dynamics involved inside the
system assures dealing with a broader palette of relevant multidimensional
responsibility, safety, and robustness aspects. To this end, this research
proposes the design of a trustworthy co-learning model for human-AI teaming in
military operations that encompasses a continuous and bidirectional exchange of
insights between the human and AI agents as they jointly adapt to evolving
battlefield conditions. It does that by integrating four dimensions. First,
adjustable autonomy for dynamically calibrating the autonomy levels of agents
depending on aspects like mission state, system confidence, and environmental
uncertainty. Second, multi-layered control which accounts continuous oversight,
monitoring of activities, and accountability. Third, bidirectional feedback
with explicit and implicit feedback loops between the agents to assure a proper
communication of reasoning, uncertainties, and learned adaptations that each of
the agents has. And fourth, collaborative decision-making which implies the
generation, evaluation, and proposal of decisions associated with confidence
levels and rationale behind them. The model proposed is accompanied by concrete
exemplifications and recommendations that contribute to further developing
responsible and trustworthy human-AI teaming systems in military operations.

</details>


### [114] [Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.01833)
*Zhihao Dou,Qinjian Zhao,Zhongwei Wan,Dinggen Zhang,Weida Wang,Towsif Raiyan,Benteng Chen,Qingtao Pan,Yang Ouyang,Zhiqiang Gao,Shufei Zhang,Sumon Biswas*

Main category: cs.AI

TL;DR: PTA-GRPO is a two-stage framework that improves LLM reasoning by first distilling Chain-of-Thought into high-level guidance through SFT, then using guidance-aware RL to jointly optimize final outputs and guidance quality.


<details>
  <summary>Details</summary>
Motivation: Current LLM reasoning with Chain-of-Thought is constrained to local decision-making without global planning, leading to redundant, incoherent, or inaccurate reasoning that degrades performance. Existing approaches have high computational costs and fail to produce optimal reasoning trajectories.

Method: Two-stage framework: (1) Use advanced LLMs to distill CoT into compact high-level guidance for supervised fine-tuning, (2) Introduce guidance-aware reinforcement learning that jointly optimizes final output and high-level guidance quality.

Result: Extensive experiments on MATH, AIME2024, AIME2025, and AMC benchmarks with Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and LLaMA3.2-3B models show PTA-GRPO achieves stable and significant improvements across different models and tasks.

Conclusion: PTA-GRPO effectively enhances both high-level planning and fine-grained CoT reasoning, demonstrating strong generalization and consistent performance improvements across diverse mathematical reasoning tasks and base models.

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning abilities
in complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However,
due to their autoregressive token-level generation, the reasoning process is
largely constrained to local decision-making and lacks global planning. This
limitation frequently results in redundant, incoherent, or inaccurate
reasoning, which significantly degrades overall performance. Existing
approaches, such as tree-based algorithms and reinforcement learning (RL),
attempt to address this issue but suffer from high computational costs and
often fail to produce optimal reasoning trajectories. To tackle this challenge,
we propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy
Optimization PTA-GRPO, a two-stage framework designed to improve both
high-level planning and fine-grained CoT reasoning. In the first stage, we
leverage advanced LLMs to distill CoT into compact high-level guidance, which
is then used for supervised fine-tuning (SFT). In the second stage, we
introduce a guidance-aware RL method that jointly optimizes the final output
and the quality of high-level guidance, thereby enhancing reasoning
effectiveness. We conduct extensive experiments on multiple mathematical
reasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across
diverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and
LLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently
achieves stable and significant improvements across different models and tasks,
validating its effectiveness and generalization.

</details>


### [115] [Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning](https://arxiv.org/abs/2510.01857)
*Claudio Fanconi,Nicol√°s Astorga,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: This paper reframes adversarial inverse reinforcement learning for large language model reasoning, learning dense token-level rewards from expert demonstrations to provide step-level feedback during training and inference-time reranking.


<details>
  <summary>Details</summary>
Motivation: To develop a method that prioritizes correctness over surface form in language model reasoning, enabling interpretable error localization and improved reasoning performance through process-level supervision.

Method: Adversarial inverse reinforcement learning is used to learn dense, token-level reward models from expert demonstrations. The learned reward serves dual purposes: providing step-level feedback during training and functioning as a critic for reranking sampled traces during inference.

Result: Empirical results on GSM8K with Llama3 and Qwen2.5 show that dense reasoning rewards can elicit reasoning as a learning signal and improve predictive performance through reward-guided reranking, particularly for Llama-based policies.

Conclusion: The approach unifies training signals, inference-time selection, and token-level diagnostics into a single reasoning reward, suggesting reusable process-level rewards with broad potential to enhance multi-step reasoning in language models.

Abstract: We reframe and operationalise adversarial inverse reinforcement learning
(IRL) to large language model reasoning, learning a dense, token-level reward
model for process supervision directly from expert demonstrations rather than
imitating style via supervised fine-tuning. The learned reasoning reward serves
two complementary roles: (i) it provides step-level feedback to optimise a
reasoning policy during training; and (ii) it functions at inference as a
critic to rerank sampled traces under fixed compute budgets. We demonstrate
that our approach prioritises correctness over surface form, yielding scores
that correlate with eventual answer validity and enabling interpretable
localisation of errors within a trace. Empirically, on GSM8K with Llama3 and
Qwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a
learning signal to elicit reasoning, and (ii) predictive performance is
improved from reward-guided reranking (notably for Llama-based policies). By
unifying training signals, inference-time selection, and token-level
diagnostics into a single reasoning reward, this work suggests reusable
process-level rewards with broad potential to enhance multi-step reasoning in
language models.

</details>


### [116] [Constrained Adaptive Rejection Sampling](https://arxiv.org/abs/2510.01902)
*Pawe≈Ç Parys,Sairam Vaidya,Taylor Berg-Kirkpatrick,Loris D'Antoni*

Main category: cs.AI

TL;DR: CARS is a constrained generation method that improves rejection sampling efficiency by adaptively pruning invalid continuations while preserving the LM's distribution.


<details>
  <summary>Details</summary>
Motivation: Existing constrained generation methods either distort the LM's distribution (greedy decoding) or waste computation (rejection sampling), which is problematic for domains requiring both validity and diversity like program fuzzing.

Method: CARS uses unconstrained LM sampling and adaptively rules out constraint-violating continuations by recording them in a trie and subtracting their probability mass from future draws, ensuring invalid prefixes are never revisited.

Result: CARS consistently achieves higher efficiency (fewer LM forward passes per valid sample) and produces stronger sample diversity than both greedy constrained decoding and methods that approximate the LM's distribution.

Conclusion: CARS strictly improves rejection sampling efficiency without distributional distortion, making it suitable for domains requiring both validity and diversity in generated outputs.

Abstract: Language Models (LMs) are increasingly used in applications where generated
outputs must satisfy strict semantic or syntactic constraints. Existing
approaches to constrained generation fall along a spectrum: greedy constrained
decoding methods enforce validity during decoding but distort the LM's
distribution, while rejection sampling (RS) preserves fidelity but wastes
computation by discarding invalid outputs. Both extremes are problematic in
domains such as program fuzzing, where both validity and diversity of samples
are essential. We present Constrained Adaptive Rejection Sampling (CARS), an
approach that strictly improves the sample-efficiency of RS without
distributional distortion. CARS begins with unconstrained LM sampling and
adaptively rules out constraint-violating continuations by recording them in a
trie and subtracting their probability mass from future draws. This adaptive
pruning ensures that prefixes proven invalid are never revisited, acceptance
rates improve monotonically, and the resulting samples exactly follow the
constrained distribution. In experiments on a variety of domains -- e.g.,
program fuzzing and molecular generation -- CARS consistently achieves higher
efficiency -- measured in the number of LM forward passes per valid sample --
while also producing stronger sample diversity than both GCD and methods that
approximate the LM's distribution.

</details>


### [117] [To Mask or to Mirror: Human-AI Alignment in Collective Reasoning](https://arxiv.org/abs/2510.01924)
*Crystal Qian,Aaron Parisi,Cl√©mentine Bouleau,Vivian Tsai,Ma√´l Lebreton,Lucas Dixon*

Main category: cs.AI

TL;DR: LLMs show varying alignment with human collective decision-making in social reasoning tasks, with some mirroring human biases while others attempt to compensate for them.


<details>
  <summary>Details</summary>
Motivation: To examine how large language models align with human social reasoning in collective decision-making contexts, moving beyond individual-level analysis.

Method: Conducted large-scale online experiment (N=748) using Lost at Sea task, randomly assigning groups to leader elections with visible demographic attributes vs pseudonymous aliases, then simulated matched LLM groups with Gemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3.

Result: LLM behaviors diverge significantly - some models mirror human biases while others mask these biases and attempt to compensate for them.

Conclusion: Human-AI alignment in collective reasoning depends on context, cues, and model-specific inductive biases, requiring dynamic benchmarks that capture the complexities of collective reasoning for socially-aligned AI.

Abstract: As large language models (LLMs) are increasingly used to model and augment
collective decision-making, it is critical to examine their alignment with
human social reasoning. We present an empirical framework for assessing
collective alignment, in contrast to prior work on the individual level. Using
the Lost at Sea social psychology task, we conduct a large-scale online
experiment (N=748), randomly assigning groups to leader elections with either
visible demographic attributes (e.g. name, gender) or pseudonymous aliases. We
then simulate matched LLM groups conditioned on the human data, benchmarking
Gemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some
mirror human biases; others mask these biases and attempt to compensate for
them. We empirically demonstrate that human-AI alignment in collective
reasoning depends on context, cues, and model-specific inductive biases.
Understanding how LLMs align with collective human behavior is critical to
advancing socially-aligned AI, and demands dynamic benchmarks that capture the
complexities of collective reasoning.

</details>


### [118] [Zero-shot reasoning for simulating scholarly peer-review](https://arxiv.org/abs/2510.02027)
*Khalid M. Saqr*

Main category: cs.AI

TL;DR: A deterministic simulation framework provides stable, evidence-based standards for evaluating AI-generated peer review reports, offering scalable governance tools for scholarly publishing.


<details>
  <summary>Details</summary>
Motivation: Address the dual crisis of unmanageable submission volumes and unregulated AI in scholarly publishing by creating new governance models to safeguard scientific integrity.

Method: Developed a deterministic simulation framework that analyzes peer-review simulation reports to identify consistent system state indicators and evaluate AI-generated peer review.

Result: The system simulates calibrated editorial judgment with 'Revise' decisions forming majority outcomes (>50%) across disciplines, and maintains 29% evidence-anchoring compliance rate that remains invariant across domains.

Conclusion: The framework provides transparent tools for fairness in science, scalable auditing for publishing workflows, and repositions AI as essential for institutional accountability in scholarly communication.

Abstract: The scholarly publishing ecosystem faces a dual crisis of unmanageable
submission volumes and unregulated AI, creating an urgent need for new
governance models to safeguard scientific integrity. The traditional human-only
peer review regime lacks a scalable, objective benchmark, making editorial
processes opaque and difficult to audit. Here we investigate a deterministic
simulation framework that provides the first stable, evidence-based standard
for evaluating AI-generated peer review reports. Analyzing 352 peer-review
simulation reports, we identify consistent system state indicators that
demonstrate its reliability. First, the system is able to simulate calibrated
editorial judgment, with 'Revise' decisions consistently forming the majority
outcome (>50%) across all disciplines, while 'Reject' rates dynamically adapt
to field-specific norms, rising to 45% in Health Sciences. Second, it maintains
unwavering procedural integrity, enforcing a stable 29% evidence-anchoring
compliance rate that remains invariant across diverse review tasks and
scientific domains. These findings demonstrate a system that is predictably
rule-bound, mitigating the stochasticity of generative AI. For the scientific
community, this provides a transparent tool to ensure fairness; for publishing
strategists, it offers a scalable instrument for auditing workflows, managing
integrity risks, and implementing evidence-based governance. The framework
repositions AI as an essential component of institutional accountability,
providing the critical infrastructure to maintain trust in scholarly
communication.

</details>


### [119] [ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly Detection](https://arxiv.org/abs/2510.02060)
*Sanghyu Yoon,Dongmin Kim,Suhee Yoon,Ye Seul Sim,Seungdong Yoa,Hye-Seung Cho,Soonyoung Lee,Hankook Lee,Woohyung Lim*

Main category: cs.AI

TL;DR: ReTabAD introduces a benchmark for context-aware tabular anomaly detection by enriching datasets with textual metadata and providing tools for semantic-aware analysis.


<details>
  <summary>Details</summary>
Motivation: Existing tabular AD benchmarks lack textual semantics and domain context that experts use in practice, limiting research flexibility and preventing full utilization of domain knowledge.

Method: Created 20 curated tabular datasets with structured textual metadata, implemented state-of-the-art AD algorithms, and developed a zero-shot LLM framework that leverages semantic context without task-specific training.

Result: Semantic context improves detection performance and enhances interpretability by supporting domain-aware reasoning, establishing ReTabAD as a benchmark for systematic exploration of context-aware AD.

Conclusion: ReTabAD addresses the gap in textual semantics for tabular AD, enabling context-aware research and demonstrating the utility of textual metadata in improving detection performance and interpretability.

Abstract: In tabular anomaly detection (AD), textual semantics often carry critical
signals, as the definition of an anomaly is closely tied to domain-specific
context. However, existing benchmarks provide only raw data points without
semantic context, overlooking rich textual metadata such as feature
descriptions and domain knowledge that experts rely on in practice. This
limitation restricts research flexibility and prevents models from fully
leveraging domain knowledge for detection. ReTabAD addresses this gap by
restoring textual semantics to enable context-aware tabular AD research. We
provide (1) 20 carefully curated tabular datasets enriched with structured
textual metadata, together with implementations of state-of-the-art AD
algorithms including classical, deep learning, and LLM-based approaches, and
(2) a zero-shot LLM framework that leverages semantic context without
task-specific training, establishing a strong baseline for future research.
Furthermore, this work provides insights into the role and utility of textual
metadata in AD through experiments and analysis. Results show that semantic
context improves detection performance and enhances interpretability by
supporting domain-aware reasoning. These findings establish ReTabAD as a
benchmark for systematic exploration of context-aware AD.

</details>


### [120] [Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning](https://arxiv.org/abs/2510.02091)
*Xinyuan Song,Keyu Wang,PengXiang Li,Lu Yin,Shiwei Liu*

Main category: cs.AI

TL;DR: Depth utilization in LLMs is heterogeneous - shallow layers handle knowledge/retrieval while deeper layers enable reasoning and coherence, with effectiveness varying by evaluation method and task type.


<details>
  <summary>Details</summary>
Motivation: To systematically investigate claims that deep layers in LLMs are unnecessary, examining depth utilization across diverse dimensions beyond narrow evaluations.

Method: Comprehensive analysis across evaluation protocols (likelihood vs generation), task categories, and model architectures to study layer contributions systematically.

Result: Shallow layers are critical for knowledge/retrieval and under likelihood metrics, while middle/deeper layers are indispensable for reasoning and generation coherence. Depth effectiveness varies significantly by evaluation setting.

Conclusion: Depth usage in LLMs is highly context-dependent, requiring task-, metric-, and model-aware perspectives for proper interpretation and compression of large models.

Abstract: Recent studies suggest that the deeper layers of Large Language Models (LLMs)
contribute little to representation learning and can often be removed without
significant performance loss. However, such claims are typically drawn from
narrow evaluations and may overlook important aspects of model behavior. In
this work, we present a systematic study of depth utilization across diverse
dimensions, including evaluation protocols, task categories, and model
architectures. Our analysis confirms that very deep layers are generally less
effective than earlier ones, but their contributions vary substantially with
the evaluation setting. Under likelihood-based metrics without generation,
pruning most layers preserves performance, with only the initial few being
critical. By contrast, generation-based evaluation uncovers indispensable roles
for middle and deeper layers in enabling reasoning and maintaining long-range
coherence. We further find that knowledge and retrieval are concentrated in
shallow components, whereas reasoning accuracy relies heavily on deeper layers
-- yet can be reshaped through distillation. These results highlight that depth
usage in LLMs is highly heterogeneous and context-dependent, underscoring the
need for task-, metric-, and model-aware perspectives in both interpreting and
compressing large models.

</details>


### [121] [Do AI Models Perform Human-like Abstract Reasoning Across Modalities?](https://arxiv.org/abs/2510.02125)
*Claas Beger,Ryan Yi,Shuhao Fu,Arseny Moskvichev,Sarah W. Tsai,Sivasankaran Rajamanickam,Melanie Mitchell*

Main category: cs.AI

TL;DR: While AI models achieve high accuracy on ConceptARC benchmark in text modality, they often use surface-level shortcuts rather than true abstract reasoning. In visual modality, accuracy drops but models still show some abstraction capability.


<details>
  <summary>Details</summary>
Motivation: To investigate whether state-of-the-art AI models truly understand and reason with intended abstractions in ConceptARC benchmark, rather than relying on surface-level patterns.

Method: Evaluated models on ConceptARC with varying input modalities (textual vs visual), external Python tool usage, and reasoning effort. Used dual evaluation measuring both output accuracy and fine-grained analysis of natural-language rules generated to explain solutions.

Result: Text-based models match human accuracy but their rules often use surface-level shortcuts rather than intended abstractions. Visual modality models show lower accuracy but still exhibit substantial share of rules capturing intended abstractions, though they struggle to apply them correctly.

Conclusion: Models still lag humans in abstract reasoning. Accuracy alone may overestimate abstract reasoning in text modalities and underestimate it in visual modalities. The proposed evaluation framework provides more faithful assessment of multimodal models' abstract reasoning abilities.

Abstract: OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI
benchmark, but does that mean state-of-the-art models recognize and reason with
the abstractions that the task creators intended? We investigate models'
abstraction abilities on ConceptARC. We evaluate models under settings that
vary the input modality (textual vs. visual), whether the model is permitted to
use external Python tools, and, for reasoning models, the amount of reasoning
effort. In addition to measuring output accuracy, we perform fine-grained
evaluation of the natural-language rules that models generate to explain their
solutions. This dual evaluation lets us assess whether models solve tasks using
the abstractions ConceptARC was designed to elicit, rather than relying on
surface-level patterns. Our results show that, while some models using
text-based representations match human output accuracy, the best models' rules
are often based on surface-level ``shortcuts'' and capture intended
abstractions far less often than humans. Thus their capabilities for general
abstract reasoning may be overestimated by evaluations based on accuracy alone.
In the visual modality, AI models' output accuracy drops sharply, yet our
rule-level analysis reveals that models might be underestimated, as they still
exhibit a substantial share of rules that capture intended abstractions, but
are often unable to correctly apply these rules. In short, our results show
that models still lag humans in abstract reasoning, and that using accuracy
alone to evaluate abstract reasoning on ARC-like tasks may overestimate
abstract-reasoning capabilities in textual modalities and underestimate it in
visual modalities. We believe that our evaluation framework offers a more
faithful picture of multimodal models' abstract reasoning abilities and a more
principled way to track progress toward human-like, abstraction-centered
intelligence.

</details>


### [122] [FlexDoc: Parameterized Sampling for Diverse Multilingual Synthetic Documents for Training Document Understanding Models](https://arxiv.org/abs/2510.02133)
*Karan Dua,Hitesh Laxmichand Patel,Puneet Mittal,Ranjeet Gupta,Amit Agarwal,Praneet Pabolu,Srikant Panda,Hansa Meghwani,Graham Horwood,Fahad Shah*

Main category: cs.AI

TL;DR: FlexDoc is a scalable synthetic data generation framework that creates realistic multilingual semi-structured documents with rich annotations to reduce the high costs of manual data collection for enterprise document understanding models.


<details>
  <summary>Details</summary>
Motivation: Developing document understanding models requires large, diverse datasets, but collecting such data is prohibitively expensive due to privacy constraints, legal restrictions, and high manual annotation costs that can reach millions of dollars.

Method: FlexDoc combines Stochastic Schemas and Parameterized Sampling to probabilistically model layout patterns, visual structure, and content variability, enabling controlled generation of diverse document variants at scale.

Result: Experiments on Key Information Extraction tasks show FlexDoc-generated data improves absolute F1 Score by up to 11% when augmenting real datasets, while reducing annotation effort by over 90% compared to traditional hard-template methods.

Conclusion: FlexDoc is actively deployed and has accelerated enterprise-grade document understanding model development while significantly reducing data acquisition and annotation costs.

Abstract: Developing document understanding models at enterprise scale requires large,
diverse, and well-annotated datasets spanning a wide range of document types.
However, collecting such data is prohibitively expensive due to privacy
constraints, legal restrictions, and the sheer volume of manual annotation
needed - costs that can scale into millions of dollars. We introduce FlexDoc, a
scalable synthetic data generation framework that combines Stochastic Schemas
and Parameterized Sampling to produce realistic, multilingual semi-structured
documents with rich annotations. By probabilistically modeling layout patterns,
visual structure, and content variability, FlexDoc enables the controlled
generation of diverse document variants at scale. Experiments on Key
Information Extraction (KIE) tasks demonstrate that FlexDoc-generated data
improves the absolute F1 Score by up to 11% when used to augment real datasets,
while reducing annotation effort by over 90% compared to traditional
hard-template methods. The solution is in active deployment, where it has
accelerated the development of enterprise-grade document understanding models
while significantly reducing data acquisition and annotation costs.

</details>


### [123] [A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports](https://arxiv.org/abs/2510.02190)
*Yang Yao,Yixu Wang,Yuxuan Zhang,Yi Lu,Tianle Gu,Lingyu Li,Dingyi Zhao,Keming Wu,Haozhe Wang,Ping Nie,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: This paper introduces a comprehensive benchmark and evaluation framework for Deep Research Agents (DRAs) to address deficiencies in existing assessment methods for AI agent systems.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are inadequate for evaluating interconnected agent systems like DRAs due to limited evaluation dimensions, response formatting, and scoring mechanisms, preventing effective assessment of their capabilities on complex tasks.

Method: Developed a rigorous benchmark with 214 expert-curated queries across 10 domains, accompanied by manually constructed reference bundles, and created a multidimensional evaluation framework with integrated scoring metrics for semantic quality, topical focus, and retrieval trustworthiness.

Result: Experiments showed mainstream DRAs outperform web-search-tool-augmented reasoning models, but significant room for improvement remains in DRA capabilities.

Conclusion: The study provides a robust foundation for assessing capabilities, refining architectures, and advancing paradigms in DRA systems.

Abstract: Artificial intelligence is undergoing the paradigm shift from closed language
models to interconnected agent systems capable of external perception and
information integration. As a representative embodiment, Deep Research Agents
(DRAs) systematically exhibit the capabilities for task decomposition,
cross-source retrieval, multi-stage reasoning, and structured output, which
markedly enhance performance on complex and open-ended tasks. However, existing
benchmarks remain deficient in evaluation dimensions, response formatting, and
scoring mechanisms, limiting their capacity to assess such systems effectively.
This paper introduces a rigorous benchmark and a multidimensional evaluation
framework tailored to DRAs and report-style responses. The benchmark comprises
214 expert-curated challenging queries distributed across 10 broad thematic
domains, each accompanied by manually constructed reference bundles to support
composite evaluation. The framework enables comprehensive evaluation of
long-form reports generated by DRAs, incorporating integrated scoring metrics
for semantic quality, topical focus, and retrieval trustworthiness. Extensive
experimentation confirms the superior performance of mainstream DRAs over
web-search-tool-augmented reasoning models, yet reveals considerable scope for
further improvement. This study provides a robust foundation for capability
assessment, architectural refinement, and paradigm advancement in DRA systems.

</details>


### [124] [UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language Models](https://arxiv.org/abs/2510.02194)
*Yuhao Sun,Zhuoer Xu,Shiwen Cui,Kun Yang,Lingyun Yu,Yongdong Zhang,Hongtao Xie*

Main category: cs.AI

TL;DR: UpSafe¬∞C is a unified framework that enhances LLM safety through safety-aware upcycling, converting safety-critical layers into sparse Mixture-of-Experts with safety experts and enabling dynamic inference-time control via safety temperature.


<details>
  <summary>Details</summary>
Motivation: LLMs remain vulnerable to safety risks like harmful content generation and jailbreak attacks, while existing safety techniques face limitations in balancing safety, utility, and controllability.

Method: Identify safety-critical layers and upcycle them into sparse MoE structure with safety experts, use two-stage SFT strategy to strengthen safety discrimination while preserving general capabilities, and introduce safety temperature mechanism for dynamic inference-time control.

Result: Achieves robust safety improvements against harmful and jailbreak inputs while maintaining competitive performance on general tasks, with safety temperature providing fine-grained inference-time control that achieves Pareto-optimal frontier between utility and safety.

Conclusion: Highlights a new direction for LLM safety: moving from static alignment toward dynamic, modular, and inference-aware control.

Abstract: Large Language Models (LLMs) have achieved remarkable progress across a wide
range of tasks, but remain vulnerable to safety risks such as harmful content
generation and jailbreak attacks. Existing safety techniques -- including
external guardrails, inference-time guidance, and post-training alignment --
each face limitations in balancing safety, utility, and controllability. In
this work, we propose UpSafe$^\circ$C, a unified framework for enhancing LLM
safety through safety-aware upcycling. Our approach first identifies
safety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)
structure, where the router acts as a soft guardrail that selectively activates
original MLPs and added safety experts. We further introduce a two-stage SFT
strategy to strengthen safety discrimination while preserving general
capabilities. To enable flexible control at inference time, we introduce a
safety temperature mechanism, allowing dynamic adjustment of the trade-off
between safety and utility. Experiments across multiple benchmarks, base model,
and model scales demonstrate that UpSafe$^\circ$C achieves robust safety
improvements against harmful and jailbreak inputs, while maintaining
competitive performance on general tasks. Moreover, analysis shows that safety
temperature provides fine-grained inference-time control that achieves the
Pareto-optimal frontier between utility and safety. Our results highlight a new
direction for LLM safety: moving from static alignment toward dynamic, modular,
and inference-aware control.

</details>


### [125] [The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models](https://arxiv.org/abs/2510.02230)
*Phuc Minh Nguyen,Chinh D. La,Duy M. H. Nguyen,Nitesh V. Chawla,Binh T. Nguyen,Khoa D. Doan*

Main category: cs.AI

TL;DR: RLVR (Reinforcement Learning with Verifiable Rewards) can paradoxically shrink reasoning boundaries in LLMs due to negative interference and winner-take-all phenomena, but a data curation algorithm focusing on low-likelihood problems improves performance.


<details>
  <summary>Details</summary>
Motivation: To investigate why RLVR, despite being designed to improve reasoning capabilities, actually shrinks the reasoning boundary in LLMs rather than expanding it.

Method: Analyzed RLVR learning dynamics through theoretical and empirical analysis on mathematical reasoning benchmarks, revealing negative interference and winner-take-all phenomena. Proposed a data curation algorithm that focuses RLVR learning on low-likelihood problems.

Result: Identified two critical failure mechanisms: negative interference (learning some problems reduces solution likelihood for others) and winner-take-all (disproportionate reinforcement of high-likelihood problems). The proposed data curation algorithm achieved notable improvement in Pass@k performance.

Conclusion: RLVR's shrinkage problem stems from inherent on-policy sampling in standard RL objectives, causing convergence toward narrow solution strategies. Focusing learning on low-likelihood problems through data curation effectively mitigates this issue.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key
method for improving Large Language Models' reasoning capabilities, yet recent
evidence suggests it may paradoxically shrink the reasoning boundary rather
than expand it. This paper investigates the shrinkage issue of RLVR by
analyzing its learning dynamics and reveals two critical phenomena that explain
this failure. First, we expose negative interference in RLVR, where learning to
solve certain training problems actively reduces the likelihood of correct
solutions for others, leading to the decline of Pass@$k$ performance, or the
probability of generating a correct solution within $k$ attempts. Second, we
uncover the winner-take-all phenomenon: RLVR disproportionately reinforces
problems with high likelihood, correct solutions, under the base model, while
suppressing other initially low-likelihood ones. Through extensive theoretical
and empirical analysis on multiple mathematical reasoning benchmarks, we show
that this effect arises from the inherent on-policy sampling in standard RL
objectives, causing the model to converge toward narrow solution strategies.
Based on these insights, we propose a simple yet effective data curation
algorithm that focuses RLVR learning on low-likelihood problems, achieving
notable improvement in Pass@$k$ performance. Our code is available at
https://github.com/mail-research/SELF-llm-interference.

</details>


### [126] [The Unreasonable Effectiveness of Scaling Agents for Computer Use](https://arxiv.org/abs/2510.02250)
*Gonzalo Gonzalez-Pumariega,Vincent Tu,Chih-Lun Lee,Jiachen Yang,Ang Li,Xin Eric Wang*

Main category: cs.AI

TL;DR: Behavior Best-of-N (bBoN) improves computer-use agents by generating multiple rollouts and selecting the best using behavior narratives, achieving near-human performance on OSWorld (69.9%) and strong generalization across operating systems.


<details>
  <summary>Details</summary>
Motivation: Computer-use agents are unreliable for long-horizon tasks due to high variance, limiting their practical application in automating complex digital workflows.

Method: bBoN generates multiple agent rollouts and selects the best trajectory using behavior narratives that describe the agents' actions and outcomes, enabling both exploration and principled selection.

Result: Achieves 69.9% success rate on OSWorld (approaching human 72%), establishes new SoTA, and demonstrates strong generalization on WindowsAgentArena and AndroidWorld.

Conclusion: Scaling computer-use agents effectively requires structured trajectory understanding and selection, and bBoN provides a practical framework to achieve robust performance across different operating systems.

Abstract: Computer-use agents (CUAs) hold promise for automating everyday digital
tasks, but their unreliability and high variance hinder their application to
long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method
that scales over agents by generating multiple rollouts and selecting among
them using behavior narratives that describe the agents' rollouts. It enables
both wide exploration and principled trajectory selection, substantially
improving robustness and success rates. On OSWorld, our bBoN scaling method
establishes a new state of the art (SoTA) at 69.9%, significantly outperforming
prior methods and approaching human-level performance at 72%, with
comprehensive ablations validating key design choices. We further demonstrate
strong generalization results to different operating systems on
WindowsAgentArena and AndroidWorld. Crucially, our results highlight the
unreasonable effectiveness of scaling CUAs, when you do it right: effective
scaling requires structured trajectory understanding and selection, and bBoN
provides a practical framework to achieve this.

</details>


### [127] [RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems](https://arxiv.org/abs/2510.02263)
*Yuxiao Qu,Anikait Singh,Yoonho Lee,Amrith Setlur,Ruslan Salakhutdinov,Chelsea Finn,Aviral Kumar*

Main category: cs.AI

TL;DR: The paper introduces reasoning abstractions - concise natural language descriptions of procedural knowledge - and RLAD, a two-player RL training paradigm that jointly trains an abstraction generator and solution generator to improve reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Current reasoning traces in large models often fail to consistently capture or reuse procedures, instead drifting into verbose exploration. The paper aims to address this by enabling more effective reasoning through structured exploration.

Method: RLAD (two-player RL training) that jointly trains an abstraction generator to propose multiple abstractions and a solution generator that uses these abstractions. Models are trained to propose abstractions followed by RL that incentivizes building solutions using the abstraction information.

Result: The approach enables structured exploration, decouples learning signals between abstraction proposal and solution generation, and improves generalization to harder problems. Allocating more test-time compute to generating abstractions is more beneficial than generating more solutions.

Conclusion: Reasoning abstractions and the RLAD training paradigm effectively guide meaningful exploration and improve reasoning capabilities by enabling structured procedural knowledge capture and reuse.

Abstract: Reasoning requires going beyond pattern matching or memorization of solutions
to identify and implement "algorithmic procedures" that can be used to deduce
answers to hard problems. Doing so requires realizing the most relevant
primitives, intermediate results, or shared procedures, and building upon them.
While RL post-training on long chains of thought ultimately aims to uncover
this kind of algorithmic behavior, most reasoning traces learned by large
models fail to consistently capture or reuse procedures, instead drifting into
verbose and degenerate exploration. To address more effective reasoning, we
introduce reasoning abstractions: concise natural language descriptions of
procedural and factual knowledge that guide the model toward learning
successful reasoning. We train models to be capable of proposing multiple
abstractions given a problem, followed by RL that incentivizes building a
solution while using the information provided by these abstractions. This
results in a two-player RL training paradigm, abbreviated as RLAD, that jointly
trains an abstraction generator and a solution generator. This setup
effectively enables structured exploration, decouples learning signals of
abstraction proposal and solution generation, and improves generalization to
harder problems. We also show that allocating more test-time compute to
generating abstractions is more beneficial for performance than generating more
solutions at large test budgets, illustrating the role of abstractions in
guiding meaningful exploration.

</details>


### [128] [BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge Transfer across Biosignals](https://arxiv.org/abs/2510.02276)
*Chenqi Li,Yu Liu,Timothy Denison,Tingting Zhu*

Main category: cs.AI

TL;DR: BioX-Bridge is a framework for unsupervised cross-modal knowledge transfer of biosignals that uses a lightweight bridge network to align intermediate representations between foundation models, reducing trainable parameters by 88-99% while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Biosignals are intercorrelated but face limited labeled datasets. Existing cross-modal transfer methods based on knowledge distillation are computationally expensive, especially with large foundation models.

Method: Train a lightweight bridge network to align intermediate representations between foundation models across modalities, with an efficient strategy for selecting alignment positions and a flexible prototype network as bridge architecture.

Result: BioX-Bridge reduces trainable parameters by 88-99% while maintaining or improving transfer performance compared to state-of-the-art methods across multiple biosignal modalities, tasks, and datasets.

Conclusion: The proposed framework enables efficient cross-modal knowledge transfer for biosignals with significantly reduced computational overhead while preserving performance.

Abstract: Biosignals offer valuable insights into the physiological states of the human
body. Although biosignal modalities differ in functionality, signal fidelity,
sensor comfort, and cost, they are often intercorrelated, reflecting the
holistic and interconnected nature of human physiology. This opens up the
possibility of performing the same tasks using alternative biosignal
modalities, thereby improving the accessibility, usability, and adaptability of
health monitoring systems. However, the limited availability of large labeled
datasets presents challenges for training models tailored to specific tasks and
modalities of interest. Unsupervised cross-modal knowledge transfer offers a
promising solution by leveraging knowledge from an existing modality to support
model training for a new modality. Existing methods are typically based on
knowledge distillation, which requires running a teacher model alongside
student model training, resulting in high computational and memory overhead.
This challenge is further exacerbated by the recent development of foundation
models that demonstrate superior performance and generalization across tasks at
the cost of large model sizes. To this end, we explore a new framework for
unsupervised cross-modal knowledge transfer of biosignals by training a
lightweight bridge network to align the intermediate representations and enable
information flow between foundation models and across modalities. Specifically,
we introduce an efficient strategy for selecting alignment positions where the
bridge should be constructed, along with a flexible prototype network as the
bridge architecture. Extensive experiments across multiple biosignal
modalities, tasks, and datasets show that BioX-Bridge reduces the number of
trainable parameters by 88--99\% while maintaining or even improving transfer
performance compared to state-of-the-art methods.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [129] [Next-Generation AI-Native Wireless Communications: MCMC-Based Receiver Architectures for Unified Processing](https://arxiv.org/abs/2510.01636)
*Xingyu Zhou,Le Liang,Jing Zhang,Chao-Kai Wen,Shi Jin*

Main category: cs.IT

TL;DR: AI-driven MIMO receiver using MCMC as a universal Bayesian computing engine for channel estimation, symbol detection, and channel decoding, enhancing interpretability and scalability.


<details>
  <summary>Details</summary>
Motivation: Address complexity and scalability challenges in MIMO receivers as antenna numbers increase, leveraging AI's potential for next-generation wireless networks.

Method: Markov chain Monte Carlo (MCMC) based approach that serves as a generic Bayesian computing engine, integrating multiple processing tasks into a unified probabilistic framework.

Result: Enhanced interpretability, scalability, and flexibility of receivers across diverse scenarios, enabling overall performance optimization.

Conclusion: The proposed unified framework can be combined with data-driven learning to develop fully intelligent communication receivers.

Abstract: The multiple-input multiple-output (MIMO) receiver processing is a key
technology for current and next-generation wireless communications. However, it
faces significant challenges related to complexity and scalability as the
number of antennas increases. Artificial intelligence (AI), a cornerstone of
next-generation wireless networks, offers considerable potential for addressing
these challenges. This paper proposes an AI-driven, universal MIMO receiver
architecture based on Markov chain Monte Carlo (MCMC) techniques. Unlike
existing AI-based methods that treat receiver processing as a black box, our
MCMC-based approach functions as a generic Bayesian computing engine applicable
to various processing tasks, including channel estimation, symbol detection,
and channel decoding. This method enhances the interpretability, scalability,
and flexibility of receivers in diverse scenarios. Furthermore, the proposed
approach integrates these tasks into a unified probabilistic framework, thereby
enabling overall performance optimization. This unified framework can also be
seamlessly combined with data-driven learning methods to facilitate the
development of fully intelligent communication receivers.

</details>


### [130] [On Algebraic Approaches for DNA Codes with Multiple Constraints](https://arxiv.org/abs/2510.01750)
*Krishna Gopal Benerjee,Manish K Gupta*

Main category: cs.IT

TL;DR: This chapter introduces DNA codes with recent constraints for DNA data storage applications, focusing on algebraic constructions that yield codes with high Hamming distance.


<details>
  <summary>Details</summary>
Motivation: DNA codes are essential for DNA computing and data storage, requiring specific constraints like avoiding tandem repeats and secondary structures. Traditional algebraic approaches often fail to achieve high Hamming distance, motivating new construction methods.

Method: The chapter uses algebraic approaches with finite rings and isometries to construct non-cyclic DNA codes. It discusses various metrics (Gau distance, Non-Homopolymer distance) and methods to satisfy multiple constraints simultaneously.

Result: The chapter presents families of DNA codes that satisfy multiple constraints and properties, along with algebraic bounds for such codes. It focuses on achieving high Hamming distance through improved algebraic constructions.

Conclusion: The work advances DNA code design for data storage applications and identifies open research directions in developing DNA codes with multiple constraints and improved properties.

Abstract: DNA strings and their properties are widely studied since last 20 years due
to its applications in DNA computing. In this area, one designs a set of DNA
strings (called DNA code) which satisfies certain thermodynamic and
combinatorial constraints such as reverse constraint, reverse-complement
constraint, $GC$-content constraint and Hamming constraint. However recent
applications of DNA codes in DNA data storage resulted in many new constraints
on DNA codes such as avoiding tandem repeats constraint (a generalization of
non-homopolymer constraint) and avoiding secondary structures constraint.
Therefore, in this chapter, we introduce DNA codes with recently developed
constraints. In particular, we discuss reverse, reverse-complement,
$GC$-content, Hamming, uncorrelated-correlated, thermodynamic, avoiding tandem
repeats and avoiding secondary structures constraints. DNA codes are
constructed using various approaches such as algebraic, computational, and
combinatorial. In particular, in algebraic approaches, one uses a finite ring
and a map to construct a DNA code. Most of such approaches does not yield DNA
codes with high Hamming distance. In this chapter, we focus on algebraic
constructions using maps (usually an isometry on some finite ring) which yields
DNA codes with high Hamming distance. We focus on non-cyclic DNA codes. We
briefly discuss various metrics such as Gau distance, Non-Homopolymer distance
etc. We discuss about algebraic constructions of families of DNA codes that
satisfy multiple constraints and/or properties. Further, we also discuss about
algebraic bounds on DNA codes with multiple constraints. Finally, we present
some open research directions in this area.

</details>


### [131] [List decoding of evaluation codes](https://arxiv.org/abs/2510.01811)
*Silouanos Brazitikos,Theodoulos Garefalakis,Eleni Tzanaki*

Main category: cs.IT

TL;DR: Generalization of Guruswami-Sudan algorithm for polynomial evaluation codes (Toric codes) defined by convex polytopes, with bounds on decoding radius.


<details>
  <summary>Details</summary>
Motivation: To extend list decoding techniques from special cases like Reed-Solomon and Reed-Muller codes to the broader class of polynomial evaluation codes defined by convex polytopes.

Method: Developed a generalized Guruswami-Sudan algorithm that incorporates the geometry and combinatorics of the defining convex polytope P.

Result: Computed bounds for the decoding radius of these generalized polynomial evaluation codes.

Conclusion: Successfully extended list decoding capabilities to the general class of Toric codes, providing analytical bounds on their decoding performance.

Abstract: Polynomial evaluation codes hold a prominent place in coding theory. In this
work, we study the problem of list decoding for a general class of polynomial
evaluation codes, also known as Toric codes, that are defined for any given
convex polytope P. Special cases, such as Reed-Solomon and Reed-Muller codes,
have been studied extensively. We present a generalization of the
Guruswami-Sudan algorithm that takes into account the geometry and the
combinatorics of P and compute bounds for the decoding radius.

</details>


### [132] [Parallelism Empowered Guessing Random Additive Noise Decoding](https://arxiv.org/abs/2510.01813)
*Li Wan,Huarui Yin,Wenyi Zhang*

Main category: cs.IT

TL;DR: This paper proposes parallel implementations of Soft GRAND (SGRAND) decoding using a unified binary tree representation of Error Patterns (EPs), achieving significant latency reduction while maintaining maximum-likelihood optimality.


<details>
  <summary>Details</summary>
Motivation: To overcome the high decoding latency of inherently sequential GRAND decoders and meet stringent throughput requirements enabled by advances in parallel hardware platforms.

Method: Developed a unified binary tree representation of EPs (EP tree) for compact representation and parallel exploration, then proposed parallel SGRAND design with pruning strategies and tree-based computation, plus a hybrid GRAND algorithm combining EP tree with Ordered Reliability Bits GRAND.

Result: Parallel SGRAND achieved 3.75√ó acceleration over serial implementation, while the hybrid enhanced method achieved 4.8√ó acceleration, with further gains expected under hardware mapping.

Conclusion: The EP tree representation enables efficient parallel GRAND decoding while preserving ML optimality, significantly reducing latency and making GRAND more practical for high-throughput applications.

Abstract: Advances in parallel hardware platforms have motivated the development of
efficient universal decoders capable of meeting stringent throughput and
latency requirements. Guessing Random Additive Noise Decoding (GRAND) is a
recently proposed decoding paradigm that sequentially tests Error Patterns
(EPs) until finding a valid codeword. While Soft GRAND (SGRAND) achieves
maximum-likelihood (ML) decoding, its inherently sequential nature hinders
parallelism and results in high decoding latency. In this work, we utilize a
unified binary tree representation of EPs, termed the EP tree, which enables
compact representation, efficient manipulation, and parallel exploration.
Building upon this EP tree representation, we propose a parallel design of
SGRAND, preserving its ML optimality while significantly reducing decoding
latency through pruning strategies and tree-based computation. Furthermore, we
develop a hybrid GRAND algorithm that enhances Ordered Reliability Bits (ORB)
GRAND with the EP tree representation, thereby achieving ML decoding with
minimal additional computational cost beyond ORBGRAND while retaining parallel
efficiency. Numerical experiments demonstrate that parallel SGRAND achieves a
$3.75\times$ acceleration compared to serial implementation, while the hybrid
enhanced method achieves a $4.8\times$ acceleration, with further gains
expected under hardware mapping.

</details>


### [133] [The dimension and Bose distance of some BCH codes of length $\frac{q^{m}-1}Œª$](https://arxiv.org/abs/2510.02020)
*Run Zheng,Nung-Sing Sze,Zejun Huang*

Main category: cs.IT

TL;DR: This paper provides explicit formulas for the dimension and Bose distance of narrow-sense BCH codes of length (q^m - 1)/Œª over finite fields, with significantly extended ranges for the designed distance Œ¥ compared to previous results.


<details>
  <summary>Details</summary>
Motivation: BCH codes are widely used error correction codes, but their key parameters (dimension, minimum distance, Bose distance) remain largely unknown in general, creating a fundamental challenge in coding theory.

Method: The authors investigate BCH codes of length (q^m - 1)/Œª over F_q, where Œª divides q-1. They derive explicit formulas for dimension and Bose distance for narrow-sense codes with m‚â•4, covering designed distances Œ¥ up to (q^‚åä(2m-1)/3‚åã+1 - 1)/Œª + 1, and extend these results to certain non-narrow-sense codes.

Result: The paper obtains explicit formulas for dimension and Bose distance with significantly larger ranges for Œ¥ than previously known. Several BCH codes with good parameters are identified through application of these results.

Conclusion: The research provides substantial progress in understanding BCH code parameters, offering explicit formulas for dimension and Bose distance over extended ranges, and identifies practical codes with good parameters.

Abstract: BCH codes are important error correction codes, widely utilized due to their
robust algebraic structure, multi-error correcting capability, and efficient
decoding algorithms. Despite their practical importance and extensive study,
their parameters, including dimension, minimum distance and Bose distance,
remain largely unknown in general. This paper addresses this challenge by
investigating the dimension and Bose distance of BCH codes of length $(q^m -
1)/\lambda$ over the finite field $\mathbb{F}_q$, where $\lambda$ is a positive
divisor of $q - 1$. Specifically, for narrow-sense BCH codes of this length
with $m \geq 4$, we derive explicit formulas for their dimension for designed
distance $2 \leq \delta \leq (q^{\lfloor (2m - 1)/3 \rfloor + 1} - 1)/{\lambda}
+ 1$. We also provide explicit formulas for their Bose distance in the range $2
\leq \delta \leq (q^{\lfloor (2m - 1)/3 \rfloor + 1} - 1)/{\lambda}$. These
ranges for $\delta$ are notably larger than the previously known results for
this class of BCH codes. Furthermore, we extend these findings to determine the
dimension and Bose distance for certain non-narrow-sense BCH codes of the same
length. Applying our results, we identify several BCH codes with good
parameters.

</details>


### [134] [Performance Analysis of RIS-Assisted UAV Communication in NOMA Networks](https://arxiv.org/abs/2510.02022)
*Masoud Ghazikor,Van Ly Nguyen,Morteza Hashemi*

Main category: cs.IT

TL;DR: This paper analyzes downlink NOMA communication in UAV networks enhanced by partitionable RISs, deriving closed-form SNR expressions and proposing an optimization algorithm to minimize outage probability while reducing RIS element requirements.


<details>
  <summary>Details</summary>
Motivation: To improve fairness and efficiency in UAV communication networks by leveraging partitionable RIS technology to enhance signal quality and reduce outage probability in challenging propagation environments.

Method: Modeled three link types (direct, RIS-only indirect, composite) under LoS/NLoS conditions using Nakagami-m fading models. Derived closed-form CDF expressions for SNR and formulated a bilevel optimization problem solved by the proposed RUOM algorithm.

Result: Simulation results validate analytical models and show the RUOM algorithm significantly improves fairness and efficiency in BS-UAV communication while minimizing required RIS reflecting elements.

Conclusion: The proposed RIS-assisted approach with the RUOM algorithm effectively enhances UAV communication performance by optimizing power allocation and RIS resource utilization, achieving better fairness and reduced outage probability.

Abstract: This paper investigates the performance of downlink non-orthogonal multiple
access (NOMA) communication in unmanned aerial vehicle (UAV) networks enhanced
by partitionable reconfigurable intelligent surfaces (RISs). We analyze three
types of links between base station (BS) and UAVs: direct, RIS-only indirect,
and composite links, under both Line-of-Sight (LoS) and Non-LoS (NLoS)
propagation. The RIS-only indirect link and direct link are modeled using
double Nakagami-m and Nakagami-m fading, respectively, while the composite link
follows a combined fading channel model. Closed-form expressions for the
cumulative distribution function (CDF) of the received signal-to-noise ratio
(SNR) are derived for all links, enabling tractable outage probability
analysis. Then, we formulate a fairness-efficiency bilevel optimization problem
to minimize the maximum outage probability among UAVs while minimizing the
total number of required RIS reflecting elements. Accordingly, an RIS-assisted
UAV Outage Minimization (RUOM) algorithm is proposed, which fairly allocates
the NOMA power coefficients while minimizing the total number of RIS reflecting
elements required, subject to NOMA-defined constraints, RIS resource
limitations, and maximum allowable outage threshold. Simulation results
validate the analytical models and demonstrate that the proposed RUOM algorithm
significantly improves fairness and efficiency in BS-UAV communication.

</details>


### [135] [Variational Secret Common Randomness Extraction](https://arxiv.org/abs/2510.02048)
*Xinyang Li,Vlad C. Andrei,Peter J. Gu,Yiqi Chen,Ullrich J. M√∂nich,Holger Boche*

Main category: cs.IT

TL;DR: A practical two-stage framework for extracting common randomness/secret keys from correlated sources using neural network encoders and secure sketches, applied to sensing-based physical layer key generation in ISAC systems.


<details>
  <summary>Details</summary>
Motivation: To develop efficient common randomness extraction methods that minimize information leakage to eavesdroppers, addressing limitations of traditional physical layer key generation methods that suffer from large protocol overhead and poor performance in high mobility scenarios.

Method: Two-stage framework: 1) Variational probabilistic quantization using neural network encoders to map observations into discrete uniform random variables with high agreement probability, 2) Secure sketch using code-offset construction to reconcile encoder outputs into identical secret keys. Applied to sensing-based PLK generation using paired range-angle maps in ISAC systems.

Result: Verified through end-to-end simulations and real-world SDR measurements, demonstrating feasibility and convincing performance even when Eve has partial knowledge about Bob's position.

Conclusion: The proposed CR extraction framework and sensing-based PLK generation method are effective for secure key generation in integrated sensing and communications systems, overcoming limitations of traditional approaches.

Abstract: This paper studies the problem of extracting common randomness (CR) or secret
keys from correlated random sources observed by two legitimate parties, Alice
and Bob, through public discussion in the presence of an eavesdropper, Eve. We
propose a practical two-stage CR extraction framework. In the first stage, the
variational probabilistic quantization (VPQ) step is introduced, where Alice
and Bob employ probabilistic neural network (NN) encoders to map their
observations into discrete, nearly uniform random variables (RVs) with high
agreement probability while minimizing information leakage to Eve. This is
realized through a variational learning objective combined with adversarial
training. In the second stage, a secure sketch using code-offset construction
reconciles the encoder outputs into identical secret keys, whose secrecy is
guaranteed by the VPQ objective. As a representative application, we study
physical layer key (PLK) generation. Beyond the traditional methods, which rely
on the channel reciprocity principle and require two-way channel probing, thus
suffering from large protocol overhead and being unsuitable in high mobility
scenarios, we propose a sensing-based PLK generation method for integrated
sensing and communications (ISAC) systems, where paired range-angle (RA) maps
measured at Alice and Bob serve as correlated sources. The idea is verified
through both end-to-end simulations and real-world software-defined radio (SDR)
measurements, including scenarios where Eve has partial knowledge about Bob's
position. The results demonstrate the feasibility and convincing performance of
both the proposed CR extraction framework and sensing-based PLK generation
method.

</details>


### [136] [Interference Resilient Quantum Receivers with Rydberg Atoms](https://arxiv.org/abs/2510.02134)
*Javane Rostampoor,Raviraj Adve*

Main category: cs.IT

TL;DR: Rydberg atomic receivers using Rb-85 outperform conventional receivers in detecting 8-PAM signals by suppressing off-resonant interference without additional filters, serving as integrated filter-demodulator systems.


<details>
  <summary>Details</summary>
Motivation: Quantum sensing with Rydberg atoms offers high-accuracy measurements due to their strong sensitivity to electromagnetic fields and inherent thermal noise immunity, making them promising for sensitive detection applications.

Method: Used Rydberg atoms (Rb-85) in vapor cells to detect 8-level pulse amplitude modulation signals, monitoring laser transmission changes through photodetectors to measure electromagnetic field characteristics.

Result: The Rydberg receiver successfully suppressed off-resonant interference without requiring additional filters and achieved better symbol error rate performance compared to conventional circuit-based receivers.

Conclusion: Rydberg atomic receivers function as integrated filter-demodulator systems that outperform traditional receivers, demonstrating the practical advantages of quantum sensing technologies in electromagnetic signal detection.

Abstract: Quantum sensing has attracted significant attention due to its ability to
measure physical quantities with extremely high accuracy. Rydberg atoms -
typically alkali atoms with a highly excited valence electron that is far from
the nucleus - exhibit strong sensitivity to external electromagnetic fields.
This sensitivity leads to coupling between different atomic energy levels,
which can be observed by monitoring changes in a control laser beam before and
after it passes through a vapor cell containing the Rydberg atoms. By analyzing
the transmitted laser signal with a photodetector, variations in transmission
can be attributed to the presence and characteristics of the external
electromagnetic field. Because Rydberg atoms operate in a highly excited
quantum state without relying on traditional electronic circuitry, they
inherently avoid thermal noise, thereby enabling more sensitive detection. In
this paper, we investigate the performance of a Rydberg atomic receiver based
on Rb-85 and compare it with that of a conventional receiver in detecting an
8-level pulse amplitude modulation (8-PAM) signal in the presence of
off-resonant interference. We demonstrate that the Rydberg receiver can
suppress interference without the need for an additional filter. Effectively,
our results show that the Rydberg receiver serves as an integrated filter and
demodulator, outperforming conventional circuit-based receivers in terms of
achievable symbol error rate

</details>


### [137] [Joint Channel and Semantic-aware Grouping for Effective Collaborative Edge Inference](https://arxiv.org/abs/2510.02191)
*Mateus P. Mota,Mattia Merluzzi,Emilio Calvanese Strinati*

Main category: cs.IT

TL;DR: Proposes a joint approach for collaborative edge inference that considers both semantic relevance and channel states when grouping devices, outperforming disjoint methods that only consider one factor.


<details>
  <summary>Details</summary>
Motivation: To improve collaborative edge inference performance in wireless environments with corrupted data, addressing the limitations of disjoint approaches that consider either semantic relevance or channel states alone.

Method: Uses a joint approach that makes attention weights dependent on channel information, considering both semantic information relevance and channel states when grouping devices for collaboration.

Result: Numerical simulations show superiority over local inference on corrupted data and collaborative inference with disjoint decisions based solely on application or physical layer parameters.

Conclusion: Joint consideration of semantic relevance and channel states is crucial for effective collaborative edge inference in wireless environments with harsh propagation conditions.

Abstract: We focus on collaborative edge inference over wireless, which enables
multiple devices to cooperate to improve inference performance in the presence
of corrupted data. Exploiting a key-query mechanism for selective information
exchange (or, group formation for collaboration), we recall the effect of
wireless channel impairments in feature communication. We argue and show that a
disjoint approach, which only considers either the semantic relevance or
channel state between devices, performs poorly, especially in harsh propagation
conditions. Based on these findings, we propose a joint approach that takes
into account semantic information relevance and channel states when grouping
devices for collaboration, by making the general attention weights dependent of
the channel information. Numerical simulations show the superiority of the
joint approach against local inference on corrupted data, as well as compared
to collaborative inference with disjoint decisions that either consider
application or physical layer parameters when forming groups.

</details>


### [138] [Collaborative Edge Inference via Semantic Grouping under Wireless Channel Constraints](https://arxiv.org/abs/2510.02222)
*Mateus P. Mota,Mattia Merluzzi,Emilio Calvanese Strinati*

Main category: cs.IT

TL;DR: This paper studies collaborative inference (edge ensembles) where edge devices exchange intermediate features to improve classification accuracy, focusing on communication efficiency, channel noise impact, and optimal collaboration points.


<details>
  <summary>Details</summary>
Motivation: To enable multiple edge devices to enhance classification accuracy through feature exchange while addressing bandwidth limitations and communication reliability challenges.

Method: Extends collaborative inference using a key-query mechanism for selective information exchange, analyzes channel noise impact, explores different intermediate collaboration points, and studies communication-accuracy trade-offs across tasks.

Result: The intermediate collaboration approach is robust to channel errors, and query transmission requires higher reliability than data transmission. Communication pruning can optimize accuracy while minimizing resource usage.

Conclusion: Collaborative inference with selective feature exchange can effectively balance accuracy and bandwidth constraints, with query reliability being more critical than data transmission reliability.

Abstract: In this paper, we study the framework of collaborative inference, or edge
ensembles. This framework enables multiple edge devices to improve
classification accuracy by exchanging intermediate features rather than raw
observations. However, efficient communication strategies are essential to
balance accuracy and bandwidth limitations. Building upon a key-query mechanism
for selective information exchange, this work extends collaborative inference
by studying the impact of channel noise in feature communication, the choice of
intermediate collaboration points, and the communication-accuracy trade-off
across tasks. By analyzing how different collaboration points affect
performance and exploring communication pruning, we show that it is possible to
optimize accuracy while minimizing resource usage. We show that the
intermediate collaboration approach is robust to channel errors and that the
query transmission needs a higher degree of reliability than the data
transmission itself.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [139] [An Efficient Quality Metric for Video Frame Interpolation Based on Motion-Field Divergence](https://arxiv.org/abs/2510.01361)
*Conall Daly,Darren Ramsook,Anil Kokaram*

Main category: eess.IV

TL;DR: A novel PSNR-based quality metric called PSNR_DIV that uses motion divergence weighting to better evaluate video frame interpolation quality, outperforming existing methods in both accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing quality metrics like PSNR, SSIM, and LPIPS ignore temporal coherence in video frame interpolation, while specialized metrics like FloLPIPS are computationally inefficient for practical use.

Method: Enhances PSNR through motion divergence weighting, a technique adapted from archival film restoration that detects temporal inconsistencies by highlighting singularities in motion fields to weight image errors.

Result: Evaluation on BVI-VFI dataset shows PSNR_DIV achieves +0.09 Pearson Linear Correlation Coefficient improvement over FloLPIPS, while being 2.5x faster and using 4x less memory. Performance is consistent across content categories and robust to motion estimator choice.

Conclusion: PSNR_DIV provides efficient and accurate quality evaluation for video frame interpolation, enabling practical use as a loss function for training neural networks in this domain.

Abstract: Video frame interpolation is a fundamental tool for temporal video
enhancement, but existing quality metrics struggle to evaluate the perceptual
impact of interpolation artefacts effectively. Metrics like PSNR, SSIM and
LPIPS ignore temporal coherence. State-of-the-art quality metrics tailored
towards video frame interpolation, like FloLPIPS, have been developed but
suffer from computational inefficiency that limits their practical application.
We present $\text{PSNR}_{\text{DIV}}$, a novel full-reference quality metric
that enhances PSNR through motion divergence weighting, a technique adapted
from archival film restoration where it was developed to detect temporal
inconsistencies. Our approach highlights singularities in motion fields which
is then used to weight image errors. Evaluation on the BVI-VFI dataset (180
sequences across multiple frame rates, resolutions and interpolation methods)
shows $\text{PSNR}_{\text{DIV}}$ achieves statistically significant
improvements: +0.09 Pearson Linear Correlation Coefficient over FloLPIPS, while
being 2.5$\times$ faster and using 4$\times$ less memory. Performance remains
consistent across all content categories and are robust to the motion estimator
used. The efficiency and accuracy of $\text{PSNR}_{\text{DIV}}$ enables fast
quality evaluation and practical use as a loss function for training neural
networks for video frame interpolation tasks. An implementation of our metric
is available at www.github.com/conalld/psnr-div.

</details>


### [140] [Median2Median: Zero-shot Suppression of Structured Noise in Images](https://arxiv.org/abs/2510.01666)
*Jianxu Wang,Ge Wang*

Main category: eess.IV

TL;DR: M2M is a zero-shot denoising framework that addresses structured noise using pseudo-independent sub-image pairs generated from a single noisy input, outperforming existing methods on correlated noise while matching performance on i.i.d. noise.


<details>
  <summary>Details</summary>
Motivation: Real-world images often contain structured noise with anisotropic correlations that existing denoising methods struggle with. Data-driven approaches require large labeled datasets and have limited generalizability, while zero-shot methods only work well for i.i.d. noise.

Method: M2M uses directional interpolation and generalized median filtering to generate pseudo-independent sub-image pairs from a single noisy input. It employs a randomized assignment strategy to eliminate systematic bias and enable Noise2Noise training without clean data.

Result: In realistic simulations, M2M performs on par with state-of-the-art zero-shot methods under i.i.d. noise and consistently outperforms them under correlated noise conditions.

Conclusion: M2M provides an efficient, data-free solution for structured noise suppression and represents the first effective zero-shot denoising approach that works beyond the strict i.i.d. assumption.

Abstract: Image denoising is a fundamental problem in computer vision and medical
imaging. However, real-world images are often degraded by structured noise with
strong anisotropic correlations that existing methods struggle to remove. Most
data-driven approaches rely on large datasets with high-quality labels and
still suffer from limited generalizability, whereas existing zero-shot methods
avoid this limitation but remain effective only for independent and identically
distributed (i.i.d.) noise. To address this gap, we propose Median2Median
(M2M), a zero-shot denoising framework designed for structured noise. M2M
introduces a novel sampling strategy that generates pseudo-independent
sub-image pairs from a single noisy input. This strategy leverages directional
interpolation and generalized median filtering to adaptively exclude values
distorted by structured artifacts. To further enlarge the effective sampling
space and eliminate systematic bias, a randomized assignment strategy is
employed, ensuring that the sampled sub-image pairs are suitable for
Noise2Noise training. In our realistic simulation studies, M2M performs on par
with state-of-the-art zero-shot methods under i.i.d. noise, while consistently
outperforming them under correlated noise. These findings establish M2M as an
efficient, data-free solution for structured noise suppression and mark the
first step toward effective zero-shot denoising beyond the strict i.i.d.
assumption.

</details>


### [141] [GFSR-Net: Guided Focus via Segment-Wise Relevance Network for Interpretable Deep Learning in Medical Imaging](https://arxiv.org/abs/2510.01919)
*Jhonatan Contreras,Thomas Bocklitz*

Main category: eess.IV

TL;DR: GFSR-Net improves interpretability in medical image analysis by using human annotations to guide model focus toward diagnostically relevant regions, achieving comparable accuracy with more reliable saliency maps.


<details>
  <summary>Details</summary>
Motivation: Deep learning models lack interpretability in medical imaging, potentially relying on irrelevant patterns or visual cues, which reduces trust and increases diagnostic risks.

Method: GFSR-Net uses a small number of human annotations to approximate intuitive focus areas without precise boundaries, training the model to align its focus with diagnostically meaningful regions across various medical images.

Result: Experiments show GFSR-Net achieves comparable or superior accuracy while producing saliency maps that better reflect human expectations, reducing reliance on irrelevant patterns.

Conclusion: The approach increases confidence in automated diagnostic tools by improving interpretability and reliability across different medical imaging modalities.

Abstract: Deep learning has achieved remarkable success in medical image analysis,
however its adoption in clinical practice is limited by a lack of
interpretability. These models often make correct predictions without
explaining their reasoning. They may also rely on image regions unrelated to
the disease or visual cues, such as annotations, that are not present in
real-world conditions. This can reduce trust and increase the risk of
misleading diagnoses. We introduce the Guided Focus via Segment-Wise Relevance
Network (GFSR-Net), an approach designed to improve interpretability and
reliability in medical imaging. GFSR-Net uses a small number of human
annotations to approximate where a person would focus within an image
intuitively, without requiring precise boundaries or exhaustive markings,
making the process fast and practical. During training, the model learns to
align its focus with these areas, progressively emphasizing features that carry
diagnostic meaning. This guidance works across different types of natural and
medical images, including chest X-rays, retinal scans, and dermatological
images. Our experiments demonstrate that GFSR achieves comparable or superior
accuracy while producing saliency maps that better reflect human expectations.
This reduces the reliance on irrelevant patterns and increases confidence in
automated diagnostic tools.

</details>


### [142] [MSRepaint: Multiple Sclerosis Repaint with Conditional Denoising Diffusion Implicit Model for Bidirectional Lesion Filling and Synthesis](https://arxiv.org/abs/2510.02063)
*Jinwei Zhang,Lianrui Zuo,Yihao Liu,Hang Zhang,Samuel W. Remedios,Bennett A. Landman,Peter A. Calabresi,Shiv Saidha,Scott D. Newsome,Dzung L. Pham,Jerry L. Prince,Ellen M. Mowry,Aaron Carass*

Main category: eess.IV

TL;DR: MSRepaint is a unified diffusion-based model for bidirectional lesion filling and synthesis in multiple sclerosis MRI, improving downstream analysis and segmentation through anatomical restoration and realistic data generation.


<details>
  <summary>Details</summary>
Motivation: Lesions in multiple sclerosis interfere with automated MRI analyses like brain parcellation and registration, while lesion segmentation models suffer from limited annotated training data availability.

Method: A diffusion-based generative model that conditions on spatial lesion masks, uses contrast dropout for missing inputs, integrates repainting to preserve surrounding anatomy, and employs multi-view DDIM inversion with fusion for 3D consistency and fast inference.

Result: MSRepaint outperforms traditional lesion filling methods (FSL, NiftySeg) and matches FastSurfer-LIT accuracy while being 20x faster. For lesion synthesis, models trained on MSRepaint data outperform those trained on CarveMix or real ISBI data across multiple benchmarks. Also enables high-fidelity simulation of lesion evolution.

Conclusion: MSRepaint provides an effective unified solution for both lesion filling to restore anatomical continuity and lesion synthesis for data augmentation, with significant improvements in speed and performance over existing methods.

Abstract: In multiple sclerosis, lesions interfere with automated magnetic resonance
imaging analyses such as brain parcellation and deformable registration, while
lesion segmentation models are hindered by the limited availability of
annotated training data. To address both issues, we propose MSRepaint, a
unified diffusion-based generative model for bidirectional lesion filling and
synthesis that restores anatomical continuity for downstream analyses and
augments segmentation through realistic data generation. MSRepaint conditions
on spatial lesion masks for voxel-level control, incorporates contrast dropout
to handle missing inputs, integrates a repainting mechanism to preserve
surrounding anatomy during lesion filling and synthesis, and employs a
multi-view DDIM inversion and fusion pipeline for 3D consistency with fast
inference. Extensive evaluations demonstrate the effectiveness of MSRepaint
across multiple tasks. For lesion filling, we evaluate both the accuracy within
the filled regions and the impact on downstream tasks including brain
parcellation and deformable registration. MSRepaint outperforms the traditional
lesion filling methods FSL and NiftySeg, and achieves accuracy on par with
FastSurfer-LIT, a recent diffusion model-based inpainting method, while
offering over 20 times faster inference. For lesion synthesis, state-of-the-art
MS lesion segmentation models trained on MSRepaint-synthesized data outperform
those trained on CarveMix-synthesized data or real ISBI challenge training data
across multiple benchmarks, including the MICCAI 2016 and UMCL datasets.
Additionally, we demonstrate that MSRepaint's unified bidirectional filling and
synthesis capability, with full spatial control over lesion appearance, enables
high-fidelity simulation of lesion evolution in longitudinal MS progression.

</details>


### [143] [SpurBreast: A Curated Dataset for Investigating Spurious Correlations in Real-world Breast MRI Classification](https://arxiv.org/abs/2510.02109)
*Jong Bum Won,Wesley De Neve,Joris Vankerschaver,Utku Ozbulak*

Main category: eess.IV

TL;DR: SpurBreast is a curated breast MRI dataset designed to study spurious correlations in medical imaging, identifying magnetic field strength and image orientation as dominant non-clinical signals that DNNs exploit, leading to poor generalization.


<details>
  <summary>Details</summary>
Motivation: Real-world deployment of DNNs in medical imaging is challenging due to spurious correlations where models learn non-clinical features instead of meaningful medical patterns, and existing datasets don't systematically study this issue.

Method: Created SpurBreast dataset with intentional spurious correlations by analyzing over 100 patient, device, and imaging protocol features. Used controlled dataset splits to evaluate model performance.

Result: Identified two dominant spurious signals: magnetic field strength (global feature) and image orientation (local feature). DNNs achieved high validation accuracy but failed to generalize to unbiased test data by exploiting these non-clinical signals.

Conclusion: The dataset enables systematic investigation of clinically relevant vs irrelevant features, uncertainty estimation, adversarial robustness, and generalization strategies in medical imaging AI.

Abstract: Deep neural networks (DNNs) have demonstrated remarkable success in medical
imaging, yet their real-world deployment remains challenging due to spurious
correlations, where models can learn non-clinical features instead of
meaningful medical patterns. Existing medical imaging datasets are not designed
to systematically study this issue, largely due to restrictive licensing and
limited supplementary patient data. To address this gap, we introduce
SpurBreast, a curated breast MRI dataset that intentionally incorporates
spurious correlations to evaluate their impact on model performance. Analyzing
over 100 features involving patient, device, and imaging protocol, we identify
two dominant spurious signals: magnetic field strength (a global feature
influencing the entire image) and image orientation (a local feature affecting
spatial alignment). Through controlled dataset splits, we demonstrate that DNNs
can exploit these non-clinical signals, achieving high validation accuracy
while failing to generalize to unbiased test data. Alongside these two datasets
containing spurious correlations, we also provide benchmark datasets without
spurious correlations, allowing researchers to systematically investigate
clinically relevant and irrelevant features, uncertainty estimation,
adversarial robustness, and generalization strategies. Models and datasets are
available at https://github.com/utkuozbulak/spurbreast.

</details>


### [144] [Measurement-Guided Consistency Model Sampling for Inverse Problems](https://arxiv.org/abs/2510.02208)
*Amirreza Tanevardi,Pooria Abbas Rad Moghadam,Sajjad Amini*

Main category: eess.IV

TL;DR: A modified consistency sampling approach for inverse imaging problems that guides stochasticity with measurement-consistency to enforce fidelity to acquired measurements while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are powerful for inverse imaging but slow due to multi-step sampling. Consistency models offer fast generation but their adaptation to inverse problems is underexplored.

Method: Modified consistency sampling with measurement-consistency mechanism tied to the measurement operator, enforcing fidelity to measurements while retaining efficiency.

Result: Experiments on Fashion-MNIST and LSUN Bedroom show improvements in FID, KID, PSNR, and SSIM metrics compared to baseline consistency sampling, achieving competitive reconstructions with few steps.

Conclusion: The proposed method enables efficient and high-quality inverse problem reconstruction by combining consistency model efficiency with measurement fidelity guidance.

Abstract: Diffusion models have become powerful generative priors for solving inverse
imaging problems, but their reliance on slow multi-step sampling limits
practical deployment. Consistency models address this bottleneck by enabling
high-quality generation in a single or only a few steps, yet their direct
adaptation to inverse problems is underexplored. In this paper, we present a
modified consistency sampling approach tailored for inverse problem
reconstruction: the sampler's stochasticity is guided by a
measurement-consistency mechanism tied to the measurement operator, which
enforces fidelity to the acquired measurements while retaining the efficiency
of consistency-based generation. Experiments on Fashion-MNIST and LSUN Bedroom
datasets demonstrate consistent improvements in perceptual and pixel-level
metrics, including Fr\'echet Inception Distance, Kernel Inception Distance,
peak signal-to-noise ratio, and structural similarity index measure, compared
to baseline consistency sampling, yielding competitive or superior
reconstructions with only a handful of steps.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [145] [Accelerating Long-Term Molecular Dynamics with Physics-Informed Time-Series Forecasting](https://arxiv.org/abs/2510.01206)
*Hung Le,Sherif Abbas,Minh Hoang Nguyen,Van Dai Do,Huu Hiep Nguyen,Dung Nguyen*

Main category: cs.LG

TL;DR: A novel approach formulates molecular dynamics simulation as time-series forecasting, using physics-informed constraints to predict atomic trajectories efficiently.


<details>
  <summary>Details</summary>
Motivation: Traditional DFT methods for MD simulation are computationally expensive, limiting long-term simulations. A more efficient alternative is needed.

Method: Formulates MD simulation as time-series forecasting, predicts atomic trajectories via displacements, incorporates physics-informed loss using DFT-parametrized Morse potential functions to enforce physical plausibility.

Result: Consistently surpasses standard baselines in simulation accuracy across diverse materials, enables stable modeling of thousands of MD steps in minutes.

Conclusion: Physics knowledge incorporation enhances reliability and precision of atomic trajectory forecasting, offering scalable alternative to costly DFT simulations.

Abstract: Efficient molecular dynamics (MD) simulation is vital for understanding
atomic-scale processes in materials science and biophysics. Traditional density
functional theory (DFT) methods are computationally expensive, which limits the
feasibility of long-term simulations. We propose a novel approach that
formulates MD simulation as a time-series forecasting problem, enabling
advanced forecasting models to predict atomic trajectories via displacements
rather than absolute positions. We incorporate a physics-informed loss and
inference mechanism based on DFT-parametrised pair-wise Morse potential
functions that penalize unphysical atomic proximity to enforce physical
plausibility. Our method consistently surpasses standard baselines in
simulation accuracy across diverse materials. The results highlight the
importance of incorporating physics knowledge to enhance the reliability and
precision of atomic trajectory forecasting. Remarkably, it enables stable
modeling of thousands of MD steps in minutes, offering a scalable alternative
to costly DFT simulations.

</details>


### [146] [Control the Temperature: Selective Sampling for Diverse and High-Quality LLM Outputs](https://arxiv.org/abs/2510.01218)
*Sergey Troshin,Wafaa Mohammed,Yan Meng,Christof Monz,Antske Fokkens,Vlad Niculae*

Main category: cs.LG

TL;DR: Selective sampling dynamically switches between greedy and high-temperature sampling based on sampling risk to improve diversity while maintaining accuracy in mathematical reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Temperature-based sampling increases diversity but degrades reasoning quality in precision-required tasks like mathematical reasoning, due to sampling incorrect continuations in sensitive positions.

Method: Proposes selective sampling with a sampling risk metric that estimates error likelihood. Uses a lightweight classifier trained on verifiable problems to predict risk and switch between greedy and high-temperature sampling.

Result: Experiments show selective sampling enhances quality-diversity trade-off in mathematical reasoning tasks, even with high-temperature settings.

Conclusion: Selective sampling effectively addresses the accuracy-diversity trade-off in language models for precision tasks by dynamically controlling sampling based on risk prediction.

Abstract: Diversity is an essential metric for evaluating the creativity of outputs
generated by language models. Temperature-based sampling is a common strategy
to increase diversity. However, for tasks that require high precision, e.g.,
mathematical reasoning, uncontrolled high temperature sampling, e.g., min-$p$
or top-$p$, degrades reasoning quality. We demonstrate that the loss of
accuracy is caused by sampling incorrect continuations in sensitive decoding
positions. To address this, in this paper, we propose \textbf{selective
sampling}, a method that dynamically switches between greedy and
high-temperature sampling based on a sampling risk metric. This risk metric
estimates the likelihood of output errors when applying high-temperature
sampling on the current token position. To predict sampling risk, we train a
lightweight classifier on a small subset of verifiable problems. The trained
classifier can be integrated with the base language model with minimal latency
overhead. Experiments on mathematical reasoning tasks demonstrate that
selective sampling enhances the quality-diversity trade-off, even in
high-temperature settings.

</details>


### [147] [Automated Extraction of Material Properties using LLM-based AI Agents](https://arxiv.org/abs/2510.01235)
*Subham Ghosh,Abhishek Tewari*

Main category: cs.LG

TL;DR: An LLM-driven workflow autonomously extracts thermoelectric and structural properties from 10,000 scientific articles, creating the largest LLM-curated thermoelectric dataset with 27,822 records and enabling scalable materials discovery.


<details>
  <summary>Details</summary>
Motivation: Materials discovery is constrained by lack of large, machine-readable datasets that couple performance metrics with structural context, with experimental literature being underexploited.

Method: Agentic LLM-driven workflow with dynamic token allocation, zero-shot multi-agent extraction, and conditional table parsing to balance accuracy and computational cost.

Result: Created 27,822 temperature-resolved property records with normalized units and structural attributes. GPT-4.1 achieved highest accuracy (F1=0.91 for thermoelectric, 0.82 for structural), while GPT-4.1 Mini delivered comparable performance at lower cost.

Conclusion: The study delivers the largest LLM-curated thermoelectric dataset, provides reproducible extraction pipeline, and establishes foundation for scalable, data-driven materials discovery beyond thermoelectrics.

Abstract: The rapid discovery of materials is constrained by the lack of large,
machine-readable datasets that couple performance metrics with structural
context. Existing databases are either small, manually curated, or biased
toward first principles results, leaving experimental literature
underexploited. We present an agentic, large language model (LLM)-driven
workflow that autonomously extracts thermoelectric and structural-properties
from about 10,000 full-text scientific articles. The pipeline integrates
dynamic token allocation, zeroshot multi-agent extraction, and conditional
table parsing to balance accuracy against computational cost. Benchmarking on
50 curated papers shows that GPT-4.1 achieves the highest accuracy (F1 = 0.91
for thermoelectric properties and 0.82 for structural fields), while GPT-4.1
Mini delivers nearly comparable performance (F1 = 0.89 and 0.81) at a fraction
of the cost, enabling practical large scale deployment. Applying this workflow,
we curated 27,822 temperature resolved property records with normalized units,
spanning figure of merit (ZT), Seebeck coefficient, conductivity, resistivity,
power factor, and thermal conductivity, together with structural attributes
such as crystal class, space group, and doping strategy. Dataset analysis
reproduces known thermoelectric trends, such as the superior performance of
alloys over oxides and the advantage of p-type doping, while also surfacing
broader structure-property correlations. To facilitate community access, we
release an interactive web explorer with semantic filters, numeric queries, and
CSV export. This study delivers the largest LLM-curated thermoelectric dataset
to date, provides a reproducible and cost-profiled extraction pipeline, and
establishes a foundation for scalable, data-driven materials discovery beyond
thermoelectrics.

</details>


### [148] [RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models](https://arxiv.org/abs/2510.01240)
*Zukang Xu,Xing Hu,Qiang Wu,Dawei Yang*

Main category: cs.LG

TL;DR: RSAVQ is a novel vector quantization framework that enhances extremely low-bit quantization for LLMs using geometry-driven innovations to address direction error and bit allocation challenges.


<details>
  <summary>Details</summary>
Motivation: Large language models face deployment challenges on resource-constrained devices due to exponentially increasing parameters. Existing vector quantization methods struggle with unconstrained direction error and suboptimal bit allocation.

Method: RSAVQ introduces two innovations: (1) Error Direction Sensitivity Guidance (EDSG) using Fisher Information Matrix-induced Riemannian metric to project quantization errors along negative natural gradient direction, and (2) Weight Channel Sensitivity Guidance (WCSG) using FIM curvature analysis for dynamic bit allocation.

Result: RSAVQ outperforms existing methods, achieving 0.4 lower perplexity and 1.5 higher zero-shot accuracy in 2-bit quantization of LLaMA-3 8B compared to baselines like VPTQ and QuIP#.

Conclusion: The work provides a practical solution for constrained environments and establishes a theoretical bridge between information geometry and neural network quantization, advancing efficient deep learning.

Abstract: Large language models (LLMs) have demonstrated remarkable performance across
a wide range of natural language processing tasks. However, their exponentially
increasing parameters pose significant challenges for deployment on
resource-constrained devices. Vector Quantization (VQ) shows great promise for
low-bit quantization (e.g., 2 to 4 bits), but existing work faces two key
challenges: unconstrained direction error and suboptimal bit allocation. In
this paper, we propose RSAVQ, a novel VQ framework to enhance extremely low-bit
quantization for LLMs. RSAVQ introduces two geometry-driven innovations that
effectively mitigate above limitations: (1) Error Direction Sensitivity
Guidance (EDSG), which leverages the Fisher Information Matrix (FIM)-induced
Riemannian metric to project quantization errors onto low-sensitivity
directions in the parameter space. Specifically, this projection is performed
along the negative natural gradient direction, which effectively suppresses
error expansion. (2) Weight Channel Sensitivity Guidance (WCSG) , which
constructs a channel-wise sensitivity metric via FIM curvature analysis to
dynamically guide bit resource allocation. The approach facilitates a globally
optimal quantization solution within prescribed bit constraints. Experiments
demonstrate that RSAVQ outperforms existing methods for LLMs. For example, in
2-bit quantization of LLaMA-3 8B, RSAVQ leads baselines like VPTQ and QuIP# by
0.4 in perplexity (PPL) and 1.5 in zero-shot accuracy. This work offers a
practical solution for constrained environments and a theoretical bridge
between information geometry and the quantization of neural networks, advancing
efficient deep learning.

</details>


### [149] [Adaptive Federated Learning Defences via Trust-Aware Deep Q-Networks](https://arxiv.org/abs/2510.01261)
*Vedant Palit*

Main category: cs.LG

TL;DR: A trust-aware Deep Q-Network defense for federated learning that integrates multi-signal evidence to mitigate poisoning and backdoor attacks under partial observability, achieving optimal robustness-accuracy trade-off.


<details>
  <summary>Details</summary>
Motivation: Federated learning is vulnerable to poisoning and backdoor attacks when there's partial observability of client behavior, requiring robust defense mechanisms.

Method: Formulated defense as a partially observable sequential decision problem and developed a trust-aware Deep Q-Network that integrates multi-signal evidence for client trust updates while optimizing long-horizon robustness-accuracy objectives.

Result: On CIFAR-10: (i) established baseline with steadily improving accuracy, (ii) showed increased client overlap improves accuracy and reduces attack success rate (ASR) with stable detection, (iii) demonstrated accuracy remains steady while ASR increases and ROC-AUC declines with reduced observability, showing sequential belief updates mitigate weaker signals.

Conclusion: The trust-aware DQN achieves the best robustness-accuracy trade-off compared to random, linear-Q, and policy gradient controllers, effectively defending against poisoning and backdoor attacks in federated learning.

Abstract: Federated learning is vulnerable to poisoning and backdoor attacks under
partial observability. We formulate defence as a partially observable
sequential decision problem and introduce a trust-aware Deep Q-Network that
integrates multi-signal evidence into client trust updates while optimizing a
long-horizon robustness--accuracy objective. On CIFAR-10, we (i) establish a
baseline showing steadily improving accuracy, (ii) show through a Dirichlet
sweep that increased client overlap consistently improves accuracy and reduces
ASR with stable detection, and (iii) demonstrate in a signal-budget study that
accuracy remains steady while ASR increases and ROC-AUC declines as
observability is reduced, which highlights that sequential belief updates
mitigate weaker signals. Finally, a comparison with random, linear-Q, and
policy gradient controllers confirms that DQN achieves the best
robustness--accuracy trade-off.

</details>


### [150] [RSTGCN: Railway-centric Spatio-Temporal Graph Convolutional Network for Train Delay Prediction](https://arxiv.org/abs/2510.01262)
*Koyena Chowdhury,Paramita Koley,Abhijnan Chakraborty,Saptarshi Ghosh*

Main category: cs.LG

TL;DR: Proposes RSTGCN, a spatio-temporal graph neural network for predicting average arrival delays at railway stations, using novel train frequency-aware attention and releasing the largest Indian Railway dataset.


<details>
  <summary>Details</summary>
Motivation: Accurate train delay prediction is critical for railway operations, with recent focus shifting from individual train delays to station-level prediction for better traffic management.

Method: Developed Railway-centric Spatio-Temporal Graph Convolutional Network (RSTGCN) with architectural innovations including train frequency-aware spatial attention and novel feature integrations.

Result: Demonstrated consistent improvements across standard metrics compared to multiple state-of-the-art baselines, showing enhanced predictive performance.

Conclusion: Advances modeling of average delay prediction in large-scale rail networks and provides an open dataset (Indian Railway Network with 4,735 stations) to encourage further research.

Abstract: Accurate prediction of train delays is critical for efficient railway
operations, enabling better scheduling and dispatching decisions. While earlier
approaches have largely focused on forecasting the exact delays of individual
trains, recent studies have begun exploring station-level delay prediction to
support higher-level traffic management. In this paper, we propose the
Railway-centric Spatio-Temporal Graph Convolutional Network (RSTGCN), designed
to forecast average arrival delays of all the incoming trains at railway
stations for a particular time period. Our approach incorporates several
architectural innovations and novel feature integrations, including train
frequency-aware spatial attention, which significantly enhances predictive
performance. To support this effort, we curate and release a comprehensive
dataset for the entire Indian Railway Network (IRN), spanning 4,735 stations
across 17 zones - the largest and most diverse railway network studied to date.
We conduct extensive experiments using multiple state-of-the-art baselines,
demonstrating consistent improvements across standard metrics. Our work not
only advances the modeling of average delay prediction in large-scale rail
networks but also provides an open dataset to encourage further research in
this critical domain.

</details>


### [151] [Budgeted Broadcast: An Activity-Dependent Pruning Rule for Neural Network Efficiency](https://arxiv.org/abs/2510.01263)
*Yaron Meirovitch,Fuming Yang,Jeff Lichtman,Nir Shavit*

Main category: cs.LG

TL;DR: Budgeted Broadcast (BB) is a pruning method that assigns local traffic budgets to units and enforces a selectivity-audience balance to maximize coding entropy, improving accuracy and efficiency across various neural network architectures.


<details>
  <summary>Details</summary>
Motivation: Traditional pruning methods remove parameters based on impact on loss, but this approach may not optimize for coding efficiency and representation diversity.

Method: BB assigns each unit a local traffic budget (product of on-rate and fan-out), uses constrained-entropy analysis to derive a selectivity-audience balance equation, and enforces this balance with simple local actuators that prune either fan-in or fan-out.

Result: BB increases coding entropy and decorrelation, improves accuracy at matched sparsity across Transformers, ResNets, and 3D U-Nets, sometimes exceeding dense baselines. Achieves state-of-the-art F1 and PR-AUC on electron microscopy images.

Conclusion: BB is easy to integrate and suggests a path toward learning more diverse and efficient representations through principled pruning based on traffic budgets.

Abstract: Most pruning methods remove parameters ranked by impact on loss (e.g.,
magnitude or gradient). We propose Budgeted Broadcast (BB), which gives each
unit a local traffic budget (the product of its long-term on-rate $a_i$ and
fan-out $k_i$). A constrained-entropy analysis shows that maximizing coding
entropy under a global traffic budget yields a selectivity-audience balance,
$\log\frac{1-a_i}{a_i}=\beta k_i$. BB enforces this balance with simple local
actuators that prune either fan-in (to lower activity) or fan-out (to reduce
broadcast). In practice, BB increases coding entropy and decorrelation and
improves accuracy at matched sparsity across Transformers for ASR, ResNets for
face identification, and 3D U-Nets for synapse prediction, sometimes exceeding
dense baselines. On electron microscopy images, it attains state-of-the-art F1
and PR-AUC under our evaluation protocol. BB is easy to integrate and suggests
a path toward learning more diverse and efficient representations.

</details>


### [152] [A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab](https://arxiv.org/abs/2510.01264)
*Isaac Peterson,Christopher Allred,Jacob Morrey,Mario Harper*

Main category: cs.LG

TL;DR: Extension of IsaacLab framework for scalable adversarial MARL training in physics simulations with heterogeneous agents and asymmetric goals.


<details>
  <summary>Details</summary>
Motivation: Adversarial interactions are critical for real-world applications like pursuit-evasion and security, but prior MARL work focused mainly on collaborative settings.

Method: Developed adversarial MARL environments with heterogeneous agents and integrated competitive variant of HAPPO (Heterogeneous Agent Reinforcement Learning with Proximal Policy Optimization).

Result: Framework successfully models and trains robust policies for morphologically diverse multi-agent competition while maintaining high throughput and simulation realism.

Conclusion: The extended IsaacLab framework enables effective training of adversarial policies in high-fidelity physics simulations for competitive multi-agent scenarios.

Abstract: Multi-Agent Reinforcement Learning (MARL) is central to robotic systems
cooperating in dynamic environments. While prior work has focused on these
collaborative settings, adversarial interactions are equally critical for
real-world applications such as pursuit-evasion, security, and competitive
manipulation. In this work, we extend the IsaacLab framework to support
scalable training of adversarial policies in high-fidelity physics simulations.
We introduce a suite of adversarial MARL environments featuring heterogeneous
agents with asymmetric goals and capabilities. Our platform integrates a
competitive variant of Heterogeneous Agent Reinforcement Learning with Proximal
Policy Optimization (HAPPO), enabling efficient training and evaluation under
adversarial dynamics. Experiments across several benchmark scenarios
demonstrate the framework's ability to model and train robust policies for
morphologically diverse multi-agent competition while maintaining high
throughput and simulation realism. Code and benchmarks are available at:
https://github.com/DIRECTLab/IsaacLab-HARL .

</details>


### [153] [RLP: Reinforcement as a Pretraining Objective](https://arxiv.org/abs/2510.01265)
*Ali Hatamizadeh,Syeda Nahida Akter,Shrimai Prabhumoye,Jan Kautz,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro,Yejin Choi*

Main category: cs.LG

TL;DR: RLP introduces reinforcement learning during pretraining by treating chain-of-thought as exploratory actions, rewarding information gain for predicting future tokens, which improves reasoning capabilities before post-training.


<details>
  <summary>Details</summary>
Motivation: Current training paradigm delays reinforcement learning to post-training after supervised fine-tuning, which may not be optimal for developing reasoning skills early in training.

Method: RLP uses information-driven reinforcement pretraining where chain-of-thought reasoning is treated as exploratory actions, with rewards based on the increase in next-token prediction likelihood when conditioning on reasoning chains versus context alone.

Result: RLP pretraining on Qwen3-1.7B-Base improved overall average across eight math-and-science benchmarks by 19%, with largest gains on reasoning-heavy tasks like AIME25 and MMLU-Pro. On Nemotron-Nano-12B-v2, it increased overall average from 42.81% to 61.32% and scientific reasoning by 23%.

Conclusion: RLP successfully bridges the gap between next-token prediction and chain-of-thought reasoning by introducing reinforcement learning during pretraining, enabling earlier development of independent thinking behavior and scalable improvements across architectures.

Abstract: The dominant paradigm for training large reasoning models starts with
pre-training using next-token prediction loss on vast amounts of data.
Reinforcement learning, while powerful in scaling reasoning, is introduced only
as the very last phase of post-training, preceded by supervised fine-tuning.
While dominant, is this an optimal way of training? In this paper, we present
RLP, an information-driven reinforcement pretraining objective, that brings the
core spirit of reinforcement learning -- exploration -- to the last phase of
pretraining. The key idea is to treat chain-of-thought as an exploratory
action, with rewards computed based on the information gain it provides for
predicting future tokens. This training objective essentially encourages the
model to think for itself before predicting what comes next, thus teaching an
independent thinking behavior earlier in the pretraining. More concretely, the
reward signal measures the increase in log-likelihood of the next token when
conditioning on both context and a sampled reasoning chain, compared to
conditioning on context alone. This approach yields a verifier-free dense
reward signal, allowing for efficient training for the full document stream
during pretraining. Specifically, RLP reframes reinforcement learning for
reasoning as a pretraining objective on ordinary text, bridging the gap between
next-token prediction and the emergence of useful chain-of-thought reasoning.
Pretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an
eight-benchmark math-and-science suite by 19%. With identical post-training,
the gains compound, with the largest improvements on reasoning-heavy tasks such
as AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2
increases the overall average from 42.81% to 61.32% and raises the average on
scientific reasoning by 23%, demonstrating scalability across architectures and
model sizes.

</details>


### [154] [Safe Reinforcement Learning-Based Vibration Control: Overcoming Training Risks with LQR Guidance](https://arxiv.org/abs/2510.01269)
*Rohan Vitthal Thorat,Juhi Singh,Rajdip Nayek*

Main category: cs.LG

TL;DR: A hybrid LQR-RL framework for safe vibration control that eliminates model dependency while preventing structural damage during RL training.


<details>
  <summary>Details</summary>
Motivation: To address safety risks in RL-based vibration control during training phase, where random control actions can damage structures, while avoiding tedious system identification required by traditional model-based methods.

Method: Proposes a hybrid control framework combining LQR and RL controllers, where LQR uses randomly selected model parameters to provide safe guidance during RL training, making the overall approach model-free.

Result: The hybrid approach outperforms uncontrolled scenarios and provides safe training environment for RL controllers without requiring accurate structural models.

Conclusion: This is the first validated solution addressing RL training safety in vibration control, offering a model-free approach that eliminates exploration risks while maintaining control effectiveness.

Abstract: Structural vibrations induced by external excitations pose significant risks,
including safety hazards for occupants, structural damage, and increased
maintenance costs. While conventional model-based control strategies, such as
Linear Quadratic Regulator (LQR), effectively mitigate vibrations, their
reliance on accurate system models necessitates tedious system identification.
This tedious system identification process can be avoided by using a model-free
Reinforcement learning (RL) method. RL controllers derive their policies solely
from observed structural behaviour, eliminating the requirement for an explicit
structural model. For an RL controller to be truly model-free, its training
must occur on the actual physical system rather than in simulation. However,
during this training phase, the RL controller lacks prior knowledge and it
exerts control force on the structure randomly, which can potentially harm the
structure. To mitigate this risk, we propose guiding the RL controller using a
Linear Quadratic Regulator (LQR) controller. While LQR control typically relies
on an accurate structural model for optimal performance, our observations
indicate that even an LQR controller based on an entirely incorrect model
outperforms the uncontrolled scenario. Motivated by this finding, we introduce
a hybrid control framework that integrates both LQR and RL controllers. In this
approach, the LQR policy is derived from a randomly selected model and its
parameters. As this LQR policy does not require knowledge of the true or an
approximate structural model the overall framework remains model-free. This
hybrid approach eliminates dependency on explicit system models while
minimizing exploration risks inherent in naive RL implementations. As per our
knowledge, this is the first study to address the critical training safety
challenge of RL-based vibration control and provide a validated solution.

</details>


### [155] [Identifying Information-Transfer Nodes in a Recurrent Neural Network Reveals Dynamic Representations](https://arxiv.org/abs/2510.01271)
*Arend Hintze,Asadullah Najam,Jory Schossau*

Main category: cs.LG

TL;DR: This paper introduces an information-theoretic method to identify information-transfer nodes (information relays) in RNNs by quantifying mutual information between input and output vectors, revealing critical information pathways and their functional importance.


<details>
  <summary>Details</summary>
Motivation: Understanding RNN internal dynamics is crucial for improving interpretability and design. Current methods lack systematic ways to identify critical information-transfer nodes that drive network behavior.

Method: Developed an information-theoretic approach using mutual information quantification between input and output vectors across nodes. Applied to synthetic and real-world time series classification with LSTM and GRU architectures, including node knockout experiments.

Result: Revealed distinct patterns of information relay across different RNN architectures, showing how information is processed and maintained over time. Node knockout experiments demonstrated functional importance of identified information relays.

Conclusion: The method enhances understanding of RNN mechanisms and provides a valuable tool for designing more robust and interpretable neural networks, contributing significantly to explainable AI.

Abstract: Understanding the internal dynamics of Recurrent Neural Networks (RNNs) is
crucial for advancing their interpretability and improving their design. This
study introduces an innovative information-theoretic method to identify and
analyze information-transfer nodes within RNNs, which we refer to as
\textit{information relays}. By quantifying the mutual information between
input and output vectors across nodes, our approach pinpoints critical pathways
through which information flows during network operations. We apply this
methodology to both synthetic and real-world time series classification tasks,
employing various RNN architectures, including Long Short-Term Memory (LSTM)
networks and Gated Recurrent Units (GRUs). Our results reveal distinct patterns
of information relay across different architectures, offering insights into how
information is processed and maintained over time. Additionally, we conduct
node knockout experiments to assess the functional importance of identified
nodes, significantly contributing to explainable artificial intelligence by
elucidating how specific nodes influence overall network behavior. This study
not only enhances our understanding of the complex mechanisms driving RNNs but
also provides a valuable tool for designing more robust and interpretable
neural networks.

</details>


### [156] [Noisy-Pair Robust Representation Alignment for Positive-Unlabeled Learning](https://arxiv.org/abs/2510.01278)
*Hengwei Zhao,Zhengzhong Tu,Zhuo Zheng,Wei Wang,Junjue Wang,Rusty Feagin,Wenzhe Jiao*

Main category: cs.LG

TL;DR: NcPU is a non-contrastive PU learning framework that addresses representation learning challenges in Positive-Unlabeled classification by combining noisy-pair robust supervised non-contrastive loss with phantom label disambiguation, achieving state-of-the-art performance without requiring auxiliary negatives or pre-estimated parameters.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art PU learning methods significantly underperform supervised methods on complex datasets (e.g., 14.26% gap on CIFAR-100), primarily due to challenges in learning discriminative representations under unreliable supervision without auxiliary information.

Method: Proposes NcPU framework with two key components: (1) NoiSNCL - a noisy-pair robust supervised non-contrastive loss that aligns intra-class representations despite unreliable supervision, and (2) PLD - phantom label disambiguation scheme that provides conservative negative supervision via regret-based label updates. Theoretically, these components iteratively benefit each other within an Expectation-Maximization framework.

Result: Extensive experiments show: (1) NoiSNCL enables simple PU methods to achieve competitive performance, and (2) NcPU achieves substantial improvements over state-of-the-art PU methods across diverse datasets, including challenging real-world applications like post-disaster building damage mapping.

Conclusion: NcPU effectively addresses the representation learning bottleneck in PU learning, demonstrating strong performance without requiring auxiliary negatives or pre-estimated parameters, making it promising for real-world applications where only limited positive and abundant unlabeled data are available.

Abstract: Positive-Unlabeled (PU) learning aims to train a binary classifier (positive
vs. negative) where only limited positive data and abundant unlabeled data are
available. While widely applicable, state-of-the-art PU learning methods
substantially underperform their supervised counterparts on complex datasets,
especially without auxiliary negatives or pre-estimated parameters (e.g., a
14.26% gap on CIFAR-100 dataset). We identify the primary bottleneck as the
challenge of learning discriminative representations under unreliable
supervision. To tackle this challenge, we propose NcPU, a non-contrastive PU
learning framework that requires no auxiliary information. NcPU combines a
noisy-pair robust supervised non-contrastive loss (NoiSNCL), which aligns
intra-class representations despite unreliable supervision, with a phantom
label disambiguation (PLD) scheme that supplies conservative negative
supervision via regret-based label updates. Theoretically, NoiSNCL and PLD can
iteratively benefit each other from the perspective of the
Expectation-Maximization framework. Empirically, extensive experiments
demonstrate that: (1) NoiSNCL enables simple PU methods to achieve competitive
performance; and (2) NcPU achieves substantial improvements over
state-of-the-art PU methods across diverse datasets, including challenging
datasets on post-disaster building damage mapping, highlighting its promise for
real-world applications. Code: Code will be open-sourced after review.

</details>


### [157] [Microsaccade-Inspired Probing: Positional Encoding Perturbations Reveal LLM Misbehaviours](https://arxiv.org/abs/2510.01288)
*Rui Melo,Rui Abreu,Corina S. Pasareanu*

Main category: cs.LG

TL;DR: A microsaccade-inspired probing method uses lightweight position encoding perturbations to detect LLM misbehaviors without fine-tuning or supervision.


<details>
  <summary>Details</summary>
Motivation: Inspired by how microsaccades reveal hidden dynamics in human perception, the authors aim to develop a similar probing method for LLMs to detect model failures.

Method: Apply lightweight position encoding perturbations to LLMs and analyze the resulting latent signals to detect model misbehavior.

Result: The method successfully detects failures across diverse settings including factuality, safety, toxicity, and backdoor attacks in multiple state-of-the-art LLMs.

Conclusion: Pretrained LLMs already encode internal evidence to flag their own failures, and microsaccade-inspired interventions provide an efficient pathway for detecting and mitigating undesirable behaviors.

Abstract: We draw inspiration from microsaccades, tiny involuntary eye movements that
reveal hidden dynamics of human perception, to propose an analogous probing
method for large language models (LLMs). Just as microsaccades expose subtle
but informative shifts in vision, we show that lightweight position encoding
perturbations elicit latent signals that indicate model misbehaviour. Our
method requires no fine-tuning or task-specific supervision, yet detects
failures across diverse settings including factuality, safety, toxicity, and
backdoor attacks. Experiments on multiple state-of-the-art LLMs demonstrate
that these perturbation-based probes surface misbehaviours while remaining
computationally efficient. These findings suggest that pretrained LLMs already
encode the internal evidence needed to flag their own failures, and that
microsaccade-inspired interventions provide a pathway for detecting and
mitigating undesirable behaviours.

</details>


### [158] [ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning Models](https://arxiv.org/abs/2510.01290)
*Akshat Ramachandran,Marina Neseem,Charbel Sakr,Rangharajan Venkatesan,Brucek Khailany,Tushar Krishna*

Main category: cs.LG

TL;DR: ThinKV is a KV cache compression framework that uses attention sparsity to identify important thoughts in chain-of-thought reasoning, applying hybrid quantization-eviction to reduce KV cache to <5% of original size while maintaining near-lossless accuracy.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models generate long chain-of-thought outputs that cause KV cache to grow rapidly and overwhelm GPU memory, creating a memory bottleneck for inference.

Method: Uses attention sparsity to identify thought types with varying importance, applies hybrid quantization-eviction strategy based on thought importance, and implements efficient kernel with PagedAttention extension for memory reuse.

Result: Achieves near-lossless accuracy with <5% of original KV cache size, improves inference throughput up to 5.8x over state-of-the-art baselines on mathematics and coding benchmarks.

Conclusion: ThinKV effectively addresses KV cache memory bottleneck through thought-adaptive compression, enabling efficient long-context reasoning without accuracy loss.

Abstract: The long-output context generation of large reasoning models enables extended
chain of thought (CoT) but also drives rapid growth of the key-value (KV)
cache, quickly overwhelming GPU memory. To address this challenge, we propose
ThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on
the observation that attention sparsity reveals distinct thought types with
varying importance within the CoT. It applies a hybrid quantization-eviction
strategy, assigning token precision by thought importance and progressively
evicting tokens from less critical thoughts as reasoning trajectories evolve.
Furthermore, to implement ThinKV, we design a kernel that extends
PagedAttention to enable efficient reuse of evicted tokens' memory slots,
eliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill,
GPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show
that ThinKV achieves near-lossless accuracy with less than 5% of the original
KV cache, while improving performance with up to 5.8x higher inference
throughput over state-of-the-art baselines.

</details>


### [159] [Network-Level Vehicle Delay Estimation at Heterogeneous Signalized Intersections](https://arxiv.org/abs/2510.01292)
*Xiaobo Ma,Hyunsoo Noh,James Tokishi,Ryan Hatch*

Main category: cs.LG

TL;DR: A domain adaptation framework using Gradient Boosting with Balanced Weighting (GBBW) improves vehicle delay estimation across diverse intersections by addressing distribution shifts between training and testing data.


<details>
  <summary>Details</summary>
Motivation: Conventional ML models for vehicle delay estimation assume same distribution for training and testing data, which rarely holds in real-world due to variations in road geometry, signal timing, and driver behavior across intersections, leading to poor generalization.

Method: A domain adaptation framework that separates data into source/target domains, extracts traffic features, and fine-tunes using small labeled target data. The novel GBBW model reweights source data based on similarity to target domain.

Result: Tested on 57 heterogeneous intersections in Pima County, Arizona, GBBW outperformed 8 state-of-the-art ML regression models and 7 instance-based DA methods, providing more accurate and robust delay estimates.

Conclusion: The framework enhances model transferability, supporting reliable traffic signal optimization, congestion management, and broader deployment of ML in transportation systems.

Abstract: Accurate vehicle delay estimation is essential for evaluating the performance
of signalized intersections and informing traffic management strategies. Delay
reflects congestion levels and affects travel time reliability, fuel use, and
emissions. Machine learning (ML) offers a scalable, cost-effective alternative;
However, conventional models typically assume that training and testing data
follow the same distribution, an assumption that is rarely satisfied in
real-world applications. Variations in road geometry, signal timing, and driver
behavior across intersections often lead to poor generalization and reduced
model accuracy. To address this issue, this study introduces a domain
adaptation (DA) framework for estimating vehicle delays across diverse
intersections. The framework separates data into source and target domains,
extracts key traffic features, and fine-tunes the model using a small, labeled
subset from the target domain. A novel DA model, Gradient Boosting with
Balanced Weighting (GBBW), reweights source data based on similarity to the
target domain, improving adaptability. The framework is tested using data from
57 heterogeneous intersections in Pima County, Arizona. Performance is
evaluated against eight state-of-the-art ML regression models and seven
instance-based DA methods. Results demonstrate that the GBBW framework provides
more accurate and robust delay estimates. This approach supports more reliable
traffic signal optimization, congestion management, and performance-based
planning. By enhancing model transferability, the framework facilitates broader
deployment of machine learning techniques in real-world transportation systems.

</details>


### [160] [From 2D to 3D, Deep Learning-based Shape Reconstruction in Magnetic Resonance Imaging: A Review](https://arxiv.org/abs/2510.01296)
*Emma McMillian,Abhirup Banerjee,Alfonso Bueno-Orovio*

Main category: cs.LG

TL;DR: A comprehensive review of deep learning methods for 3D shape reconstruction from 2D MRI, covering four main approaches: point cloud, mesh-based, shape-aware, and volumetric models, with analysis of techniques, limitations, applications, and future directions.


<details>
  <summary>Details</summary>
Motivation: 3D shape reconstruction from 2D MRI is crucial for medical diagnosis, treatment planning, and computational modeling, requiring a structured overview of current methodologies to advance deep learning towards more robust and clinically impactful solutions.

Method: Survey and analysis of four primary reconstruction approaches: point cloud models, mesh-based models, shape-aware models, and volumetric models, examining their methodological foundations, limitations, applications across anatomical structures, computational demands, and evaluation metrics.

Result: Provides extensive overview of 3D MRI reconstruction techniques across cardiac, neurological, and lung imaging, analyzing clinical applicability to diseased anatomy, influence of training/testing data, and available public datasets.

Conclusion: Identifies emerging research directions including multimodal integration and cross-modality frameworks, aiming to guide researchers towards advancing deep learning for more generalizable and clinically impactful 3D reconstruction solutions.

Abstract: Deep learning-based 3-dimensional (3D) shape reconstruction from
2-dimensional (2D) magnetic resonance imaging (MRI) has become increasingly
important in medical disease diagnosis, treatment planning, and computational
modeling. This review surveys the methodological landscape of 3D MRI
reconstruction, focusing on 4 primary approaches: point cloud, mesh-based,
shape-aware, and volumetric models. For each category, we analyze the current
state-of-the-art techniques, their methodological foundation, limitations, and
applications across anatomical structures. We provide an extensive overview
ranging from cardiac to neurological to lung imaging. We also focus on the
clinical applicability of models to diseased anatomy, and the influence of
their training and testing data. We examine publicly available datasets,
computational demands, and evaluation metrics. Finally, we highlight the
emerging research directions including multimodal integration and
cross-modality frameworks. This review aims to provide researchers with a
structured overview of current 3D reconstruction methodologies to identify
opportunities for advancing deep learning towards more robust, generalizable,
and clinically impactful solutions.

</details>


### [161] [Low Rank Gradients and Where to Find Them](https://arxiv.org/abs/2510.01303)
*Rishi Sonthalia,Michael Murray,Guido Mont√∫far*

Main category: cs.LG

TL;DR: This paper analyzes low-rank structure in neural network gradients under relaxed data isotropy assumptions, showing gradients are dominated by two rank-one components aligned with data bulk and spike.


<details>
  <summary>Details</summary>
Motivation: To understand gradient structure in neural networks without requiring the usual isotropy assumptions on training data and parameters, examining how data properties affect gradient composition.

Method: Theoretical analysis using spiked data model with anisotropic bulk, considering both mean-field and neural-tangent-kernel scalings, and examining various activation functions and regularizers.

Result: Gradients are approximately low-rank and dominated by two rank-one terms: one aligned with bulk data-residue and another with the rank-one spike in input data. The balance between these components depends on data properties, scaling regime, and activation function.

Conclusion: Standard regularizers selectively modulate the two gradient components, and theoretical predictions are validated through experiments on synthetic and real data.

Abstract: This paper investigates low-rank structure in the gradients of the training
loss for two-layer neural networks while relaxing the usual isotropy
assumptions on the training data and parameters. We consider a spiked data
model in which the bulk can be anisotropic and ill-conditioned, we do not
require independent data and weight matrices and we also analyze both the
mean-field and neural-tangent-kernel scalings. We show that the gradient with
respect to the input weights is approximately low rank and is dominated by two
rank-one terms: one aligned with the bulk data-residue , and another aligned
with the rank one spike in the input data. We characterize how properties of
the training data, the scaling regime and the activation function govern the
balance between these two components. Additionally, we also demonstrate that
standard regularizers, such as weight decay, input noise and Jacobian
penalties, also selectively modulate these components. Experiments on synthetic
and real data corroborate our theoretical predictions.

</details>


### [162] [Quantum-inspired Benchmark for Estimating Intrinsic Dimension](https://arxiv.org/abs/2510.01335)
*Aritra Das,Joseph T. Iosue,Victor V. Albert*

Main category: cs.LG

TL;DR: QuIIEst benchmark introduces complex manifolds with known intrinsic dimension for testing ID estimation methods, showing lower accuracy than existing benchmarks and minimal performance degradation with non-uniform curvature.


<details>
  <summary>Details</summary>
Motivation: Existing ID estimation methods show varying results, and current benchmarks use simple manifolds. There's a need for more complex benchmarks to better evaluate these methods.

Method: Proposed Quantum-Inspired Intrinsic-dimension Estimation (QuIIEst) benchmark using infinite families of topologically non-trivial manifolds from quantum-optical embedding methods, allowing curvature modification and additive noise.

Result: ID estimation methods were generally less accurate on QuIIEst manifolds than existing benchmarks, with minimal performance degradation despite increasingly non-uniform curvature. Also successfully performed ID estimation on fractal Hofstadter's butterfly.

Conclusion: QuIIEst provides a challenging benchmark that better reflects real-world complexity, revealing limitations of current ID estimation methods on non-trivial manifolds and non-manifold spaces.

Abstract: Machine learning models can generalize well on real-world datasets. According
to the manifold hypothesis, this is possible because datasets lie on a latent
manifold with small intrinsic dimension (ID). There exist many methods for ID
estimation (IDE), but their estimates vary substantially. This warrants
benchmarking IDE methods on manifolds that are more complex than those in
existing benchmarks. We propose a Quantum-Inspired Intrinsic-dimension
Estimation (QuIIEst) benchmark consisting of infinite families of topologically
non-trivial manifolds with known ID. Our benchmark stems from a quantum-optical
method of embedding arbitrary homogeneous spaces while allowing for curvature
modification and additive noise. The IDE methods tested were generally less
accurate on QuIIEst manifolds than on existing benchmarks under identical
resource allocation. We also observe minimal performance degradation with
increasingly non-uniform curvature, underscoring the benchmark's inherent
difficulty. As a result of independent interest, we perform IDE on the fractal
Hofstadter's butterfly and identify which methods are capable of extracting the
effective dimension of a space that is not a manifold.

</details>


### [163] [On the Identifiability of Latent Action Policies](https://arxiv.org/abs/2510.01337)
*S√©bastien Lachapelle*

Main category: cs.LG

TL;DR: The paper analyzes identifiability in latent action policy learning (LAPO), proving that an entropy-regularized objective can identify meaningful action representations under certain conditions.


<details>
  <summary>Details</summary>
Motivation: To understand why discrete action representations work well in practice and to establish formal conditions for identifiability in LAPO frameworks.

Method: Formal analysis of LAPO identifiability, describing desiderata for action representations and proving identifiability results for entropy-regularized objectives.

Result: Proved that entropy-regularized LAPO objective identifies action representations satisfying the proposed desiderata under suitable conditions.

Conclusion: The analysis provides theoretical explanation for the empirical success of discrete action representations in practice.

Abstract: We study the identifiability of latent action policy learning (LAPO), a
framework introduced recently to discover representations of actions from video
data. We formally describe desiderata for such representations, their
statistical benefits and potential sources of unidentifiability. Finally, we
prove that an entropy-regularized LAPO objective identifies action
representations satisfying our desiderata, under suitable conditions. Our
analysis provides an explanation for why discrete action representations
perform well in practice.

</details>


### [164] [Self-Supervised Representation Learning as Mutual Information Maximization](https://arxiv.org/abs/2510.01345)
*Akhlaqur Rahman Sabby,Yi Sui,Tongzi Wu,Jesse C. Cresswell,Ga Wu*

Main category: cs.LG

TL;DR: This paper provides a theoretical framework that explains why different self-supervised learning methods use specific architectural components like stop-gradient operations and predictor networks, showing they emerge naturally from two fundamental optimization paradigms derived from mutual information objectives.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical principles behind self-supervised representation learning methods, moving beyond empirical observations and heuristics to explain why specific architectural choices are necessary.

Method: The authors derive two training paradigms (Self-Distillation MI and Joint MI) from a variational mutual information lower bound, showing how different optimization strategies lead to distinct architectural requirements.

Result: The paper demonstrates that stop-gradient operations are theoretically essential for SDMI, while JMI allows symmetric architectures without such components. Predictor networks and statistical regularizers emerge as tractable surrogates for mutual information objectives.

Conclusion: The work provides a unified theoretical explanation for architectural choices in self-supervised learning, showing they are not just empirical conveniences but naturally follow from fundamental optimization principles.

Abstract: Self-supervised representation learning (SSRL) has demonstrated remarkable
empirical success, yet its underlying principles remain insufficiently
understood. While recent works attempt to unify SSRL methods by examining their
information-theoretic objectives or summarizing their heuristics for preventing
representation collapse, architectural elements like the predictor network,
stop-gradient operation, and statistical regularizer are often viewed as
empirically motivated additions. In this paper, we adopt a first-principles
approach and investigate whether the learning objective of an SSRL algorithm
dictates its possible optimization strategies and model design choices. In
particular, by starting from a variational mutual information (MI) lower bound,
we derive two training paradigms, namely Self-Distillation MI (SDMI) and Joint
MI (JMI), each imposing distinct structural constraints and covering a set of
existing SSRL algorithms. SDMI inherently requires alternating optimization,
making stop-gradient operations theoretically essential. In contrast, JMI
admits joint optimization through symmetric architectures without such
components. Under the proposed formulation, predictor networks in SDMI and
statistical regularizers in JMI emerge as tractable surrogates for the MI
objective. We show that many existing SSRL methods are specific instances or
approximations of these two paradigms. This paper provides a theoretical
explanation behind the choices of different architectural components of
existing SSRL methods, beyond heuristic conveniences.

</details>


### [165] [To Augment or Not to Augment? Diagnosing Distributional Symmetry Breaking](https://arxiv.org/abs/2510.01349)
*Hannah Lawrence,Elyssa Hofgard,Vasco Portilheiro,Yuxuan Chen,Tess Smidt,Robin Walters*

Main category: cs.LG

TL;DR: The paper proposes a metric to quantify dataset anisotropy (symmetry-breaking) and shows that symmetry-aware methods may not always be optimal even when labels are invariant, with empirical results varying by dataset.


<details>
  <summary>Details</summary>
Motivation: To critically evaluate the assumption that transformed datapoints are highly probable under test distributions, which underlies symmetry-aware methods like data augmentation and equivariant architectures.

Method: Developed a two-sample neural classifier test to distinguish between original datasets and their randomly augmented equivalents, measuring dataset anisotropy.

Result: Uncovered surprisingly high degrees of alignment in benchmark point cloud datasets; showed theoretically that symmetry-breaking can prevent invariant methods from optimal performance; found equivariant methods' benefits are dataset-dependent.

Conclusion: Understanding equivariance requires rethinking symmetry biases in data, as symmetry-aware methods don't always provide benefits and their effectiveness depends on dataset characteristics.

Abstract: Symmetry-aware methods for machine learning, such as data augmentation and
equivariant architectures, encourage correct model behavior on all
transformations (e.g. rotations or permutations) of the original dataset. These
methods can improve generalization and sample efficiency, under the assumption
that the transformed datapoints are highly probable, or "important", under the
test distribution. In this work, we develop a method for critically evaluating
this assumption. In particular, we propose a metric to quantify the amount of
anisotropy, or symmetry-breaking, in a dataset, via a two-sample neural
classifier test that distinguishes between the original dataset and its
randomly augmented equivalent. We validate our metric on synthetic datasets,
and then use it to uncover surprisingly high degrees of alignment in several
benchmark point cloud datasets. We show theoretically that distributional
symmetry-breaking can actually prevent invariant methods from performing
optimally even when the underlying labels are truly invariant, as we show for
invariant ridge regression in the infinite feature limit. Empirically, we find
that the implication for symmetry-aware methods is dataset-dependent:
equivariant methods still impart benefits on some anisotropic datasets, but not
others. Overall, these findings suggest that understanding equivariance -- both
when it works, and why -- may require rethinking symmetry biases in the data.

</details>


### [166] [RheOFormer: A generative transformer model for simulation of complex fluids and flows](https://arxiv.org/abs/2510.01365)
*Maedeh Saberi,Amir Barati Farimani,Safa Jamali*

Main category: cs.LG

TL;DR: RheOFormer is a generative operator learning method using self-attention to efficiently model complex fluid flows, accurately predicting spatio-temporal evolution of nonlinear mechanics with strong generalization and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for non-Newtonian fluid dynamics suffer from high computational demands and poor scalability, while existing data-driven methods require retraining across varied conditions.

Method: Leverages self-attention in a generative operator learning framework to learn spatial interactions and features of complex fluid flows across viscometric and non-viscometric flows.

Result: Accurately learns both scalar and tensorial nonlinear mechanics of different complex fluids, predicts spatio-temporal flow evolution even with limited training data, and demonstrates strong generalization capabilities.

Conclusion: RheOFormer serves as a robust neural surrogate for accelerating predictive complex fluid simulations, advancing data-driven experimentation, and enabling real-time process optimization across applications.

Abstract: The ability to model mechanics of soft materials under flowing conditions is
key in designing and engineering processes and materials with targeted
properties. This generally requires solution of internal stress tensor, related
to the deformation tensor through nonlinear and history-dependent constitutive
models. Traditional numerical methods for non-Newtonian fluid dynamics often
suffer from prohibitive computational demands and poor scalability to new
problem instances. Developments in data-driven methods have mitigated some
limitations but still require retraining across varied physical conditions. In
this work, we introduce Rheological Operator Transformer (RheOFormer), a
generative operator learning method leveraging self-attention to efficiently
learn different spatial interactions and features of complex fluid flows. We
benchmark RheOFormer across a range of different viscometric and
non-viscometric flows with different types of viscoelastic and
elastoviscoplastic mechanics in complex domains against ground truth solutions.
Our results demonstrate that RheOFormer can accurately learn both scalar and
tensorial nonlinear mechanics of different complex fluids and predict the
spatio-temporal evolution of their flows, even when trained on limited
datasets. Its strong generalization capabilities and computational efficiency
establish RheOFormer as a robust neural surrogate for accelerating predictive
complex fluid simulations, advancing data-driven experimentation, and enabling
real-time process optimization across a wide range of applications.

</details>


### [167] [Selective Underfitting in Diffusion Models](https://arxiv.org/abs/2510.01378)
*Kiwhan Song,Jaeyeon Kim,Sitan Chen,Yilun Du,Sham Kakade,Vincent Sitzmann*

Main category: cs.LG

TL;DR: Diffusion models selectively underfit the empirical score function - accurately approximating it in certain regions while underfitting in others, which is essential for understanding their generalization and generative performance.


<details>
  <summary>Details</summary>
Motivation: To understand what score function diffusion models actually learn, since perfectly matching the empirical score would simply reproduce training data and fail to generate novel samples.

Method: Introduce the concept of selective underfitting, characterize regions where diffusion models accurately approximate vs underfit the score, and design empirical interventions to validate this perspective.

Result: Established that selective underfitting occurs - better diffusion models more accurately approximate the score in certain regions while underfitting in others.

Conclusion: Selective underfitting is essential for understanding diffusion models and provides new testable insights into their generalization and generative performance.

Abstract: Diffusion models have emerged as the principal paradigm for generative
modeling across various domains. During training, they learn the score
function, which in turn is used to generate samples at inference. They raise a
basic yet unsolved question: which score do they actually learn? In principle,
a diffusion model that matches the empirical score in the entire data space
would simply reproduce the training data, failing to generate novel samples.
Recent work addresses this question by arguing that diffusion models underfit
the empirical score due to training-time inductive biases. In this work, we
refine this perspective, introducing the notion of selective underfitting:
instead of underfitting the score everywhere, better diffusion models more
accurately approximate the score in certain regions of input space, while
underfitting it in others. We characterize these regions and design empirical
interventions to validate our perspective. Our results establish that selective
underfitting is essential for understanding diffusion models, yielding new,
testable insights into their generalization and generative performance.

</details>


### [168] [Fine-Tuning Masked Diffusion for Provable Self-Correction](https://arxiv.org/abs/2510.01384)
*Jaeyeon Kim,Seunggeun Kim,Taekyun Lee,David Z. Pan,Hyeji Kim,Sham Kakade,Sitan Chen*

Main category: cs.LG

TL;DR: PRISM is a lightweight, model-agnostic approach for self-correction in Masked Diffusion Models that learns per-token quality scores to detect and revise low-quality tokens during inference without requiring architectural changes or RL.


<details>
  <summary>Details</summary>
Motivation: Current Masked Diffusion Models lack effective self-correction capabilities, with prior approaches either requiring major architectural/training changes or relying on imprecise quality proxies, limiting their practical applicability.

Method: PRISM uses plug-in remasking to define a self-correction loss that provably learns per-token quality scores during a single forward pass with MDM, enabling detection of low-quality tokens without reinforcement learning or verifiers.

Result: PRISM significantly advances MDM inference performance across multiple domains and scales, including Sudoku puzzles, unconditional text generation (170M parameters), and code generation with LLaDA (8B parameters).

Conclusion: PRISM provides an effective, lightweight solution for self-correction in Masked Diffusion Models that works with any pretrained MDM and demonstrates strong empirical performance across diverse applications.

Abstract: A natural desideratum for generative models is self-correction--detecting and
revising low-quality tokens at inference. While Masked Diffusion Models (MDMs)
have emerged as a promising approach for generative modeling in discrete
spaces, their capacity for self-correction remains poorly understood. Prior
attempts to incorporate self-correction into MDMs either require overhauling
MDM architectures/training or rely on imprecise proxies for token quality,
limiting their applicability. Motivated by this, we introduce PRISM--Plug-in
Remasking for Inference-time Self-correction of Masked Diffusions--a
lightweight, model-agnostic approach that applies to any pretrained MDM.
Theoretically, PRISM defines a self-correction loss that provably learns
per-token quality scores, without RL or a verifier. These quality scores are
computed in the same forward pass with MDM and used to detect low-quality
tokens. Empirically, PRISM advances MDM inference across domains and scales:
Sudoku; unconditional text (170M); and code with LLaDA (8B).

</details>


### [169] [Optimal Stopping vs Best-of-$N$ for Inference Time Optimization](https://arxiv.org/abs/2510.01394)
*Yusuf Kalayci,Vinod Raman,Shaddin Dughmi*

Main category: cs.LG

TL;DR: A new inference-time optimization framework for LLMs based on Pandora's Box problem, achieving 15-35% fewer generations while maintaining same performance as Best-of-N sampling.


<details>
  <summary>Details</summary>
Motivation: Balancing LLM output quality against inference cost when using multiple generations, addressing the trade-off between generation quality and computational expense.

Method: Developed UCB-style Pandora's Box algorithm that adapts to unknown reward distributions, with Bradley-Terry inspired transformation for reward scaling across prompts and adaptive stopping thresholds.

Result: Experiments on AlpacaFarm and HH-RLHF datasets show adaptive strategy achieves same performance as Best-of-N sampling with 15-35% fewer generations on average.

Conclusion: Established principled bridge between optimal stopping theory and inference-time scaling, providing both theoretical performance bounds and practical efficiency gains for LLM deployment.

Abstract: Large language model (LLM) generation often requires balancing output quality
against inference cost, especially when using multiple generations. We
introduce a new framework for inference-time optimization based on the
classical Pandora's Box problem. Viewing each generation as opening a costly
"box" with random reward, we develop algorithms that decide when to stop
generating without knowing the underlying reward distribution. Our first
contribution is a UCB-style Pandora's Box algorithm, which achieves performance
that is provably close to Weitzman's algorithm, the optimal strategy when the
distribution is known. We further adapt this method to practical LLM settings
by addressing reward scaling across prompts via a Bradley-Terry inspired
transformation. This leads to an adaptive inference-time optimization method
that normalizes rewards and learns stopping thresholds on the fly. Experiments
on the AlpacaFarm and HH-RLHF datasets, using multiple LLM-reward model pairs,
show that our adaptive strategy can obtain the same performance as non-adaptive
Best-of-N sampling while requiring 15-35 percent fewer generations on average.
Our results establish a principled bridge between optimal stopping theory and
inference-time scaling, providing both theoretical performance bounds and
practical efficiency gains for LLM deployment.

</details>


### [170] [Neural Network Surrogates for Free Energy Computation of Complex Chemical Systems](https://arxiv.org/abs/2510.01396)
*Wasut Pornpatcharapong*

Main category: cs.LG

TL;DR: A neural network surrogate framework that learns collective variables (CVs) from Cartesian coordinates and provides Jacobians via automatic differentiation, enabling gradient-based free energy methods to use complex CVs without analytical forms.


<details>
  <summary>Details</summary>
Motivation: Traditional free energy reconstruction methods like Gaussian Process Regression require Jacobians of CVs, which is a bottleneck that restricts the use of complex or machine-learned collective variables.

Method: Introduced a neural network surrogate framework that learns CVs directly from Cartesian coordinates and uses automatic differentiation to provide Jacobians, bypassing the need for analytical forms.

Result: On an MgCl2 ion-pairing system, the method achieved high accuracy for both simple distance CVs and complex coordination-number CVs. Jacobian errors followed a near-Gaussian distribution, making them suitable for GPR pipelines.

Conclusion: This framework enables gradient-based free energy methods to incorporate complex and machine-learned CVs, broadening the scope of biochemistry and materials simulations.

Abstract: Free energy reconstruction methods such as Gaussian Process Regression (GPR)
require Jacobians of the collective variables (CVs), a bottleneck that
restricts the use of complex or machine-learned CVs. We introduce a neural
network surrogate framework that learns CVs directly from Cartesian coordinates
and uses automatic differentiation to provide Jacobians, bypassing analytical
forms. On an MgCl2 ion-pairing system, our method achieved high accuracy for
both a simple distance CV and a complex coordination-number CV. Moreover,
Jacobian errors also followed a near-Gaussian distribution, making them
suitable for GPR pipelines. This framework enables gradient-based free energy
methods to incorporate complex and machine-learned CVs, broadening the scope of
biochemistry and materials simulations.

</details>


### [171] [Ultra-Efficient Decoding for End-to-End Neural Compression and Reconstruction](https://arxiv.org/abs/2510.01407)
*Ethan G. Rogers,Cheng Wang*

Main category: cs.LG

TL;DR: A new neural compression framework using low-rank representations to eliminate decoder bottleneck while maintaining image quality.


<details>
  <summary>Details</summary>
Motivation: Current neural compression methods suffer from high computational complexity and large costs in convolution-based decoders, hindering adoption.

Method: Incorporates low-rank representation in autoencoder with vector quantization, using computationally efficient low-rank operations on latent representations.

Result: Dramatically reduces computational overhead in decoding phase while maintaining high fidelity image outputs.

Conclusion: The approach effectively eliminates decoder compute bottleneck in neural compression/reconstruction systems.

Abstract: Image compression and reconstruction are crucial for various digital
applications. While contemporary neural compression methods achieve impressive
compression rates, the adoption of such technology has been largely hindered by
the complexity and large computational costs of the convolution-based decoders
during data reconstruction. To address the decoder bottleneck in neural
compression, we develop a new compression-reconstruction framework based on
incorporating low-rank representation in an autoencoder with vector
quantization. We demonstrated that performing a series of computationally
efficient low-rank operations on the learned latent representation of images
can efficiently reconstruct the data with high quality. Our approach
dramatically reduces the computational overhead in the decoding phase of neural
compression/reconstruction, essentially eliminating the decoder compute
bottleneck while maintaining high fidelity of image outputs.

</details>


### [172] [How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning](https://arxiv.org/abs/2510.02265)
*Yalin E. Sagduyu,Tugba Erpek,Kemal Davaslioglu,Sastry Kompella*

Main category: cs.LG

TL;DR: RL-based anti-jamming using Q-learning and DQN for adaptive power, modulation, and channel selection to maintain throughput against reactive jammers.


<details>
  <summary>Details</summary>
Motivation: To counter reactive jammers that dynamically select channels and sensing thresholds to detect and jam transmissions, without prior knowledge of channel conditions or jamming strategies.

Method: Employ Q-learning for discrete jamming-event states and Deep Q-Networks (DQN) for continuous states based on received power, adapting transmit power, modulation, and channel selection.

Result: RL can adapt rapidly to spectrum dynamics and sustain high rates as channels and jamming policies change over time.

Conclusion: Reinforcement learning is effective for mitigating reactive jamming by enabling adaptive transmission strategies that optimize throughput in dynamic spectrum environments.

Abstract: This paper studies the problem of mitigating reactive jamming, where a jammer
adopts a dynamic policy of selecting channels and sensing thresholds to detect
and jam ongoing transmissions. The transmitter-receiver pair learns to avoid
jamming and optimize throughput over time (without prior knowledge of channel
conditions or jamming strategies) by using reinforcement learning (RL) to adapt
transmit power, modulation, and channel selection. Q-learning is employed for
discrete jamming-event states, while Deep Q-Networks (DQN) are employed for
continuous states based on received power. Through different reward functions
and action sets, the results show that RL can adapt rapidly to spectrum
dynamics and sustain high rates as channels and jamming policies change over
time.

</details>


### [173] [Edge Artificial Intelligence: A Systematic Review of Evolution, Taxonomic Frameworks, and Future Horizons](https://arxiv.org/abs/2510.01439)
*Mohamad Abou Ali,Fadi Dornaika*

Main category: cs.LG

TL;DR: This review systematically examines Edge AI's evolution, current landscape, and future directions through a multi-dimensional taxonomy covering deployment, processing capabilities, applications, and hardware, while analyzing challenges and emerging opportunities.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive understanding of Edge AI's development from early content delivery networks to modern on-device intelligence, addressing the need for systematic analysis in this rapidly evolving field.

Method: Systematic review following PRISMA guidelines, using multi-dimensional taxonomy including deployment location, processing capabilities (TinyML, federated learning), application domains, and hardware types.

Result: Identified core enabling technologies (hardware accelerators, optimized software, communication protocols), challenges (resource limitations, security, power consumption), and emerging opportunities (neuromorphic hardware, continual learning, edge-cloud collaboration).

Conclusion: Provides a comprehensive framework for researchers and practitioners by tracing Edge AI evolution, analyzing current technologies and challenges, and highlighting future directions including trustworthiness integration and advanced hardware solutions.

Abstract: Edge Artificial Intelligence (Edge AI) embeds intelligence directly into
devices at the network edge, enabling real-time processing with improved
privacy and reduced latency by processing data close to its source. This review
systematically examines the evolution, current landscape, and future directions
of Edge AI through a multi-dimensional taxonomy including deployment location,
processing capabilities such as TinyML and federated learning, application
domains, and hardware types. Following PRISMA guidelines, the analysis traces
the field from early content delivery networks and fog computing to modern
on-device intelligence. Core enabling technologies such as specialized hardware
accelerators, optimized software, and communication protocols are explored.
Challenges including resource limitations, security, model management, power
consumption, and connectivity are critically assessed. Emerging opportunities
in neuromorphic hardware, continual learning algorithms, edge-cloud
collaboration, and trustworthiness integration are highlighted, providing a
comprehensive framework for researchers and practitioners.

</details>


### [174] [SoftAdaClip: A Smooth Clipping Strategy for Fair and Private Model Training](https://arxiv.org/abs/2510.01447)
*Dorsa Soleymani,Ali Dadsetan,Frank Rudzicz*

Main category: cs.LG

TL;DR: SoftAdaClip replaces hard gradient clipping in DP-SGD with a smooth tanh-based transformation to reduce fairness disparities in differentially private training while maintaining privacy guarantees.


<details>
  <summary>Details</summary>
Motivation: Standard DP-SGD with gradient clipping disproportionately suppresses learning signals for minority subpopulations, reducing both model performance and fairness. Adaptive clipping helps but still uses uniform hard clipping that restricts fairness improvements.

Method: SoftAdaClip uses a smooth, tanh-based transformation instead of hard clipping to preserve relative gradient magnitudes while bounding sensitivity for differential privacy. It combines this smooth transformation with adaptive mechanisms.

Result: SoftAdaClip reduces subgroup disparities by up to 87% compared to DP-SGD and up to 48% compared to Adaptive-DPSGD across datasets including MIMIC-III, GOSSIS-eICU, and Adult Income. These disparity reductions are statistically significant.

Conclusion: Integrating smooth transformations with adaptive mechanisms is crucial for achieving fair and private model training, as demonstrated by SoftAdaClip's significant improvements in reducing subgroup disparities while maintaining differential privacy.

Abstract: Differential privacy (DP) provides strong protection for sensitive data, but
often reduces model performance and fairness, especially for underrepresented
groups. One major reason is gradient clipping in DP-SGD, which can
disproportionately suppress learning signals for minority subpopulations.
Although adaptive clipping can enhance utility, it still relies on uniform hard
clipping, which may restrict fairness. To address this, we introduce
SoftAdaClip, a differentially private training method that replaces hard
clipping with a smooth, tanh-based transformation to preserve relative gradient
magnitudes while bounding sensitivity. We evaluate SoftAdaClip on various
datasets, including MIMIC-III (clinical text), GOSSIS-eICU (structured
healthcare), and Adult Income (tabular data). Our results show that SoftAdaClip
reduces subgroup disparities by up to 87% compared to DP-SGD and up to 48%
compared to Adaptive-DPSGD, and these reductions in subgroup disparities are
statistically significant. These findings underscore the importance of
integrating smooth transformations with adaptive mechanisms to achieve fair and
private model training.

</details>


### [175] [Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression](https://arxiv.org/abs/2510.01450)
*Yifei Zuo,Yutong Yin,Zhichen Zeng,Ang Li,Banghua Zhu,Zhaoran Wang*

Main category: cs.LG

TL;DR: Local Linear Attention (LLA) is a novel attention mechanism derived from nonparametric statistics that offers theoretical advantages over existing attention methods and is made computationally efficient through FlashLLA algorithm and custom kernels.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in exploring more expressive attention mechanisms grounded in theoretical insight, even at greater computational cost, as efficient alternatives to Softmax Attention have been widely studied but more expressive mechanisms remain underexplored.

Method: Proposed Local Linear Attention (LLA) derived from nonparametric statistics via test-time regression, with computational challenges addressed through memory-efficient primitives and FlashLLA - a hardware-efficient blockwise algorithm for modern accelerators, plus customized inference kernels.

Result: LLA outperforms strong baselines in test-time training and in-context learning, effectively adapts to non-stationarity, and shows promising evidence for scalability in large-scale models across test-time regression, in-context regression, associative recall and state tracking tasks.

Conclusion: LLA represents a theoretically grounded and practically efficient attention mechanism that bridges the gap between expressive power and computational feasibility, demonstrating advantages in various tasks and showing potential for large-scale applications.

Abstract: Transformer architectures have achieved remarkable success in various
domains. While efficient alternatives to Softmax Attention have been widely
studied, the search for more expressive mechanisms grounded in theoretical
insight-even at greater computational cost-has been relatively underexplored.
In this work, we bridge this gap by proposing Local Linear Attention (LLA), a
novel attention mechanism derived from nonparametric statistics through the
lens of test-time regression. First, we show that LLA offers theoretical
advantages over Linear and Softmax Attention for associative memory via a
bias-variance trade-off analysis. Next, we address its computational challenges
and propose two memory-efficient primitives to tackle the $\Theta(n^2 d)$ and
$\Theta(n d^2)$ complexity. We then introduce FlashLLA, a hardware-efficient,
blockwise algorithm that enables scalable and parallel computation on modern
accelerators. In addition, we implement and profile a customized inference
kernel that significantly reduces memory overheads. Finally, we empirically
validate the advantages and limitations of LLA on test-time regression,
in-context regression, associative recall and state tracking tasks. Experiment
results demonstrate that LLA effectively adapts to non-stationarity,
outperforming strong baselines in test-time training and in-context learning,
and exhibiting promising evidence for its scalability and applicability in
large-scale models. Code is available at
https://github.com/Yifei-Zuo/Flash-LLA.

</details>


### [176] [SCOPED: Score-Curvature Out-of-distribution Proximity Evaluator for Diffusion](https://arxiv.org/abs/2510.01456)
*Brett Barkley,Preston Culbertson,David Fridovich-Keil*

Main category: cs.LG

TL;DR: SCOPED is a fast, efficient OOD detection method for diffusion models that reduces forward passes by 10x while maintaining competitive accuracy, using a single test statistic combining Jacobian trace and score function norm.


<details>
  <summary>Details</summary>
Motivation: OOD detection is crucial for reliable ML deployment across vision, robotics, and RL, but existing methods are computationally expensive with many forward passes.

Method: Combines Jacobian trace and squared norm of score function into single statistic, uses kernel density estimation for flexible unsupervised testing, requires only one forward pass and JVP with Hutchinson's estimator.

Result: Achieves competitive/state-of-the-art precision-recall on four vision benchmarks with low computational cost, generalizes to robotic control tasks across reward functions and training regimes.

Conclusion: SCOPED provides practical foundation for fast, reliable OOD detection in real-world domains including vision artifacts, outlier detection, RL exploration, and dataset curation.

Abstract: Out-of-distribution (OOD) detection is essential for reliable deployment of
machine learning systems in vision, robotics, reinforcement learning, and
beyond. We introduce Score-Curvature Out-of-distribution Proximity Evaluator
for Diffusion (SCOPED), a fast and general-purpose OOD detection method for
diffusion models that reduces the number of forward passes on the trained model
by an order of magnitude compared to prior methods, outperforming most
diffusion-based baselines and closely approaching the accuracy of the strongest
ones. SCOPED is computed from a single diffusion model trained once on a
diverse dataset, and combines the Jacobian trace and squared norm of the
model's score function into a single test statistic. Rather than thresholding
on a fixed value, we estimate the in-distribution density of SCOPED scores
using kernel density estimation, enabling a flexible, unsupervised test that,
in the simplest case, only requires a single forward pass and one
Jacobian-vector product (JVP), made efficient by Hutchinson's trace estimator.
On four vision benchmarks, SCOPED achieves competitive or state-of-the-art
precision-recall scores despite its low computational cost. The same method
generalizes to robotic control tasks with shared state and action spaces,
identifying distribution shifts across reward functions and training regimes.
These results position SCOPED as a practical foundation for fast and reliable
OOD detection in real-world domains, including perceptual artifacts in vision,
outlier detection in autoregressive models, exploration in reinforcement
learning, and dataset curation for unsupervised training.

</details>


### [177] [Fixing That Free Lunch: When, Where, and Why Synthetic Data Fails in Model-Based Policy Optimization](https://arxiv.org/abs/2510.01457)
*Brett Barkley,David Fridovich-Keil*

Main category: cs.LG

TL;DR: MBPO's performance drops in DeepMind Control Suite despite success in OpenAI Gym. Two failure modes identified: scale mismatches between dynamics/reward models causing critic underestimation, and poor target representation inflating model variance. Fixing these enables MBPO to outperform SAC in 5/7 DMC tasks while maintaining Gym performance.


<details>
  <summary>Details</summary>
Motivation: To understand why MBPO underperforms in DeepMind Control Suite despite strong results in OpenAI Gym, and identify the specific failure modes that prevent policy improvement in certain environments.

Method: Analyzed MBPO's performance across seven challenging DMC tasks, identified two coupled failure modes: scale mismatches between dynamics and reward models, and poor target representation choice that increases model variance.

Result: After addressing the identified failure modes, MBPO outperformed SAC in five of seven DMC tasks while preserving its previously reported strong performance in OpenAI Gym.

Conclusion: Environment-specific assumptions can become implicitly encoded into algorithm design when evaluation is limited. The community should develop taxonomies linking MDP structure to algorithmic failure modes and clarify how benchmark choices affect algorithm generalization.

Abstract: Synthetic data is a core component of data-efficient Dyna-style model-based
reinforcement learning, yet it can also degrade performance. We study when it
helps, where it fails, and why, and we show that addressing the resulting
failure modes enables policy improvement that was previously unattainable. We
focus on Model-Based Policy Optimization (MBPO), which performs actor and
critic updates using synthetic action counterfactuals. Despite reports of
strong and generalizable sample-efficiency gains in OpenAI Gym, recent work
shows that MBPO often underperforms its model-free counterpart, Soft
Actor-Critic (SAC), in the DeepMind Control Suite (DMC). Although both suites
involve continuous control with proprioceptive robots, this shift leads to
sharp performance losses across seven challenging DMC tasks, with MBPO failing
in cases where claims of generalization from Gym would imply success. This
reveals how environment-specific assumptions can become implicitly encoded into
algorithm design when evaluation is limited. We identify two coupled issues
behind these failures: scale mismatches between dynamics and reward models that
induce critic underestimation and hinder policy improvement during model-policy
coevolution, and a poor choice of target representation that inflates model
variance and produces error-prone rollouts. Addressing these failure modes
enables policy improvement where none was previously possible, allowing MBPO to
outperform SAC in five of seven tasks while preserving the strong performance
previously reported in OpenAI Gym. Rather than aiming only for incremental
average gains, we hope our findings motivate the community to develop
taxonomies that tie MDP task- and environment-level structure to algorithmic
failure modes, pursue unified solutions where possible, and clarify how
benchmark choices ultimately shape the conditions under which algorithms
generalize.

</details>


### [178] [How Well Can Preference Optimization Generalize Under Noisy Feedback?](https://arxiv.org/abs/2510.01458)
*Shawn Im,Yixuan Li*

Main category: cs.LG

TL;DR: This paper analyzes how noisy human feedback affects preference optimization for aligning large language models, providing generalization guarantees under realistic noise conditions.


<details>
  <summary>Details</summary>
Motivation: Most existing preference optimization methods assume noise-free human feedback, which is unrealistic due to inherent errors and inconsistencies in human judgments. The paper addresses this gap by studying the impact of noisy feedback on model alignment.

Method: The authors analyze noise models corresponding to common real-world sources like mislabeling and uncertainty, focusing on finite-step preference optimization rather than assuming convergence. They provide theoretical analysis for a broad family of preference optimization losses including DPO, IPO, and SLiC.

Result: The paper describes how generalization decays with different types of noise across various noise rates, based on preference data distribution and sample size. Empirical validation on contemporary LLMs confirms the practical relevance of the findings.

Conclusion: The work offers valuable insights for developing AI systems that better align with human preferences under realistic noisy feedback conditions, providing theoretical guarantees that are more aligned with practical LLM training scenarios.

Abstract: As large language models (LLMs) advance their capabilities, aligning these
models with human preferences has become crucial. Preference optimization,
which trains models to distinguish between preferred and non-preferred
responses based on human feedback, has become a crucial component for aligning
LLMs. However, most existing works assume noise-free feedback, which is
unrealistic due to the inherent errors and inconsistencies in human judgments.
This paper addresses the impact of noisy feedback on preference optimization,
providing generalization guarantees under these conditions. In particular, we
consider noise models that correspond to common real-world sources of noise,
such as mislabeling and uncertainty. Unlike traditional analyses that assume
convergence, our work focuses on finite-step preference optimization, offering
new insights that are more aligned with practical LLM training. We describe how
generalization decays with different types of noise across levels of noise
rates based on the preference data distribution and number of samples. Our
analysis for noisy preference learning applies to a broad family of preference
optimization losses such as DPO, IPO, SLiC, etc. Empirical validation on
contemporary LLMs confirms the practical relevance of our findings, offering
valuable insights for developing AI systems that align with human preferences.

</details>


### [179] [LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning](https://arxiv.org/abs/2510.01459)
*Weizhe Chen,Sven Koenig,Bistra Dilkina*

Main category: cs.LG

TL;DR: LSPO is a meta-RLVR algorithm that dynamically selects training data based on average response length to improve reinforcement learning effectiveness for large language models.


<details>
  <summary>Details</summary>
Motivation: Motivated by studies of overthinking in LLMs, the authors aim to make RLVR more efficient by addressing how response length affects learning.

Method: Proposed Length-aware Sampling for Policy Optimization (LSPO), a meta-RLVR algorithm that dynamically selects training data at each step based on average response length.

Result: LSPO consistently improves learning effectiveness across multiple base models and datasets, with ablation studies providing insights into length signal incorporation.

Conclusion: LSPO demonstrates the value of dynamic length-aware sampling in RLVR, offering promising directions for future research in optimizing LLM training.

Abstract: Since the release of Deepseek-R1, reinforcement learning with verifiable
rewards (RLVR) has become a central approach for training large language models
(LLMs) on reasoning tasks. Recent work has largely focused on modifying loss
functions to make RLVR more efficient and effective. In this paper, motivated
by studies of overthinking in LLMs, we propose Length-aware Sampling for Policy
Optimization (LSPO), a novel meta-RLVR algorithm that dynamically selects
training data at each step based on the average response length. We evaluate
LSPO across multiple base models and datasets, demonstrating that it
consistently improves learning effectiveness. In addition, we conduct a
detailed ablation study to examine alternative ways of incorporating length
signals into dynamic sampling, offering further insights and highlighting
promising directions for future research.

</details>


### [180] [The Three Regimes of Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2510.01460)
*Lu Li,Tianwei Ni,Yihao Sun,Pierre-Luc Bacon*

Main category: cs.LG

TL;DR: The paper proposes a stability-plasticity principle for offline-to-online RL to address inconsistent empirical behavior, identifying three regimes of online fine-tuning that require different stability properties.


<details>
  <summary>Details</summary>
Motivation: Offline-to-online RL shows highly inconsistent empirical behavior where design choices that work in one setting fail in others, creating a need for principled guidance.

Method: Proposed a stability-plasticity principle that preserves better knowledge (from pretrained policy or offline dataset) while maintaining plasticity, validated through large-scale empirical study across 63 cases.

Result: The framework's predictions strongly aligned with empirical results in 45 out of 63 cases, demonstrating its effectiveness in explaining and guiding offline-to-online RL behavior.

Conclusion: The stability-plasticity principle provides a principled framework for making design choices in offline-to-online RL based on the relative performance of offline datasets and pretrained policies.

Abstract: Offline-to-online reinforcement learning (RL) has emerged as a practical
paradigm that leverages offline datasets for pretraining and online
interactions for fine-tuning. However, its empirical behavior is highly
inconsistent: design choices of online-fine tuning that work well in one
setting can fail completely in another. We propose a stability--plasticity
principle that can explain this inconsistency: we should preserve the knowledge
of pretrained policy or offline dataset during online fine-tuning, whichever is
better, while maintaining sufficient plasticity. This perspective identifies
three regimes of online fine-tuning, each requiring distinct stability
properties. We validate this framework through a large-scale empirical study,
finding that the results strongly align with its predictions in 45 of 63 cases.
This work provides a principled framework for guiding design choices in
offline-to-online RL based on the relative performance of the offline dataset
and the pretrained policy.

</details>


### [181] [Fine-tuning LLMs with variational Bayesian last layer for high-dimensional Bayesian optimzation](https://arxiv.org/abs/2510.01471)
*Haotian Xiang,Jinwen Xu,Qin Lu*

Main category: cs.LG

TL;DR: The paper proposes LoRA-VBLL, a novel Bayesian optimization approach using LLMs as surrogates for high-dimensional black-box optimization with irregular variables, enhanced by ensemble methods for automated hyperparameter selection.


<details>
  <summary>Details</summary>
Motivation: Bayesian optimization struggles with high-dimensional problems containing irregular variables (categorical, ordinal). Traditional Gaussian process surrogates are inadequate, and existing neural network alternatives are computationally expensive.

Method: Use LLM as surrogate model with LoRA fine-tuning and variational Bayesian last layer (VBLL) for efficient parameter updates. Develop ensemble (ENS) approach for automated hyperparameter selection and continuous model updates.

Result: Extensive experiments show compelling performance on high-dimensional benchmarks and real-world molecular optimization tasks, outperforming existing alternatives with computational efficiency.

Conclusion: LoRA-VBLL provides an effective and computationally light solution for high-dimensional Bayesian optimization with irregular variables, with ensemble methods further enhancing performance through automated hyperparameter tuning.

Abstract: A plethora of applications entail solving black-box optimization problems
with high evaluation costs, including drug discovery, material design, as well
as hyperparameter tuning. Toward finding the global optimum of such black-box
optimization problems with sample efficiency, Bayesian optimization (BO) is a
theoretically elegant framework that relies on a probabilistic surrogate model
so as to iteratively select the query point with well-balanced
exploration-exploitation tradeoffs. The Gaussian process (GP), as the de-facto
choice for surrogate modeling, has achieved compelling performances for vanilla
BO with low-dimensional continuous variables. However, GPs fall short in coping
with high-dimensional counterparts with {\it irregular} variables (e.g.,
categorical, ordinal, etc.). To alleviate this, neural network-based surrogates
have been explored. Inspired by the powerful capabilities of LLMs, we adopt the
LLM as the surrogate to model the mapping from the high-dimensional input
variables to the objective function. To adapt to the current problem, we
leverage the low-rank adaptation (LoRA) to fine-tune the LLM parameters
together with the posterior of a linear regression head via the variational
Bayesian last layer (VBLL) framework. The resulting LoRA-VBLL is not only
computationally light compared to existing alternatives, but also admits
recursive updates. To automate the critical selection of the LoRA rank as well
as other hyperparameters, a weighted ensemble (ENS) of LoRA-VBLL surrogates has
been devised, which further accommodates continual update of the per-model
weight and individual LoRA-VBLL parameters via recursive Bayes. Extensive
experimental results demonstrate the compelling performance of the proposed
(ENS-)LoRA-VBLL approaches on various high-dimensional benchmarks and the
real-world molecular optimization tasks.

</details>


### [182] [PEL-NAS: Search Space Partitioned Architecture Prompt Co-Evolutionary LLM-driven Hardware-Aware Neural Architecture Search](https://arxiv.org/abs/2510.01472)
*Hengyi Zhu,Grace Li Zhang,Shaoyi Huang*

Main category: cs.LG

TL;DR: PEL-NAS is a novel LLM-driven neural architecture search method that addresses exploration bias in traditional approaches by partitioning search space, using co-evolutionary prompts, and zero-cost predictors to efficiently find high-accuracy, low-latency neural networks.


<details>
  <summary>Details</summary>
Motivation: Traditional HW-NAS methods require multiple GPU days per dataset, while LLM-driven approaches suffer from exploration bias where they repeatedly propose designs within limited search space and fail to discover architectures across different latency ranges.

Method: Three key components: 1) complexity-driven partitioning engine to divide search space and enforce diversity; 2) LLM-powered architecture prompt co-evolution operator that updates knowledge base and performs guided evolution; 3) zero-cost predictor to avoid training candidates from scratch.

Result: On HW-NAS-Bench, PEL-NAS achieves higher HV, lower IGD, and up to 54% lower latency than baselines at similar accuracy. Search cost drops from days to minutes compared with traditional supernet baselines.

Conclusion: PEL-NAS effectively addresses exploration bias in LLM-driven NAS, enabling efficient discovery of diverse neural architectures with high accuracy and low latency while significantly reducing search time from days to minutes.

Abstract: Hardware-Aware Neural Architecture Search (HW-NAS) requires joint
optimization of accuracy and latency under device constraints. Traditional
supernet-based methods require multiple GPU days per dataset. Large Language
Model (LLM)-driven approaches avoid training a large supernet and can provide
quick feedback, but we observe an exploration bias: the LLM repeatedly proposes
neural network designs within limited search space and fails to discover
architectures across different latency ranges in the entire search space. To
address this issue, we propose PEL-NAS: a search space Partitioned,
architecture prompt co-Evolutionary and LLM-driven Neural Architecture Search
that can generate neural networks with high accuracy and low latency with
reduced search cost. Our proposed PEL-NAS has three key components: 1) a
complexity-driven partitioning engine that divides the search space by
complexity to enforce diversity and mitigate exploration bias; 2) an
LLM-powered architecture prompt co-evolution operator, in which the LLM first
updates a knowledge base of design heuristics based on results from the
previous round, then performs a guided evolution algorithm on architectures
with prompts that incorporate this knowledge base. Prompts and designs improve
together across rounds which avoids random guesswork and improve efficiency; 3)
a zero-cost predictor to avoid training a large number of candidates from
scratch. Experimental results show that on HW-NAS-Bench, PEL-NAS can achieve
overall higher HV, lower IGD, and up to 54% lower latency than baselines at
similar accuracy. Meanwhile, the search cost drops from days to minutes
compared with traditional supernet baselines.

</details>


### [183] [Density-Ratio Weighted Behavioral Cloning: Learning Control Policies from Corrupted Datasets](https://arxiv.org/abs/2510.01479)
*Shriram Karpoora Sundara Pandian,Ali Baheri*

Main category: cs.LG

TL;DR: Weighted BC is a robust offline RL method that uses density ratios from a clean reference set to weight behavioral cloning, effectively handling dataset contamination without knowing the contamination mechanism.


<details>
  <summary>Details</summary>
Motivation: Standard offline RL methods degrade when datasets contain adversarial poisoning, system errors, or low-quality samples, making them unreliable for safety-critical applications.

Method: Uses a small clean reference set to estimate trajectory-level density ratios via binary discriminator, then clips and applies these ratios as weights in behavioral cloning objective to prioritize clean data.

Result: Achieves near-optimal performance even at high contamination ratios, outperforming BC, BCQ, and BRAC across various poisoning protocols (reward, state, transition, action) on continuous control benchmarks.

Conclusion: Weighted BC provides theoretical guarantees for convergence to clean expert policy with finite-sample bounds independent of contamination rate, offering a practical solution for robust offline RL.

Abstract: Offline reinforcement learning (RL) enables policy optimization from fixed
datasets, making it suitable for safety-critical applications where online
exploration is infeasible. However, these datasets are often contaminated by
adversarial poisoning, system errors, or low-quality samples, leading to
degraded policy performance in standard behavioral cloning (BC) and offline RL
methods. This paper introduces Density-Ratio Weighted Behavioral Cloning
(Weighted BC), a robust imitation learning approach that uses a small, verified
clean reference set to estimate trajectory-level density ratios via a binary
discriminator. These ratios are clipped and used as weights in the BC objective
to prioritize clean expert behavior while down-weighting or discarding
corrupted data, without requiring knowledge of the contamination mechanism. We
establish theoretical guarantees showing convergence to the clean expert policy
with finite-sample bounds that are independent of the contamination rate. A
comprehensive evaluation framework is established, which incorporates various
poisoning protocols (reward, state, transition, and action) on continuous
control benchmarks. Experiments demonstrate that Weighted BC maintains
near-optimal performance even at high contamination ratios outperforming
baselines such as traditional BC, batch-constrained Q-learning (BCQ) and
behavior regularized actor-critic (BRAC).

</details>


### [184] [Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed](https://arxiv.org/abs/2510.01494)
*Isha Gupta,Rylan Schaeffer,Joshua Kazdan,Ken Liu,Sanmi Koyejo*

Main category: cs.LG

TL;DR: This paper explains why adversarial attacks transfer between models in data-space but not in representation-space, showing that transferability depends on the attack domain rather than being an inherent property of all attacks.


<details>
  <summary>Details</summary>
Motivation: To explain why image jailbreaks fail to transfer between vision-language models (VLMs) while adversarial examples successfully transfer between image classifiers and text jailbreaks transfer between language models.

Method: The authors provide theoretical and empirical evidence across four settings: mathematical proof in a simple network scenario, representation-space attacks against image classifiers, representation-space attacks against language models, and data-space attacks against VLMs with geometric alignment analysis.

Result: Representation-space attacks are as successful as data-space attacks but fail to transfer between models, while data-space attacks successfully transfer. Representation-space attacks can only transfer when models' latent geometries are sufficiently aligned.

Conclusion: Adversarial transfer is not inherent to all attacks but depends on their operational domain - attacks in shared data-space transfer, while attacks in models' unique representation spaces do not transfer without geometric alignment.

Abstract: The field of adversarial robustness has long established that adversarial
examples can successfully transfer between image classifiers and that text
jailbreaks can successfully transfer between language models (LMs). However, a
pair of recent studies reported being unable to successfully transfer image
jailbreaks between vision-language models (VLMs). To explain this striking
difference, we propose a fundamental distinction regarding the transferability
of attacks against machine learning models: attacks in the input data-space can
transfer, whereas attacks in model representation space do not, at least not
without geometric alignment of representations. We then provide theoretical and
empirical evidence of this hypothesis in four different settings. First, we
mathematically prove this distinction in a simple setting where two networks
compute the same input-output map but via different representations. Second, we
construct representation-space attacks against image classifiers that are as
successful as well-known data-space attacks, but fail to transfer. Third, we
construct representation-space attacks against LMs that successfully jailbreak
the attacked models but again fail to transfer. Fourth, we construct data-space
attacks against VLMs that successfully transfer to new VLMs, and we show that
representation space attacks \emph{can} transfer when VLMs' latent geometries
are sufficiently aligned in post-projector space. Our work reveals that
adversarial transfer is not an inherent property of all attacks but contingent
on their operational domain - the shared data-space versus models' unique
representation spaces - a critical insight for building more robust models.

</details>


### [185] [Beyond Majority Voting: LLM Aggregation by Leveraging Higher-Order Information](https://arxiv.org/abs/2510.01499)
*Rui Ai,Yuqi Pan,David Simchi-Levi,Milind Tambe,Haifeng Xu*

Main category: cs.LG

TL;DR: The paper proposes two new aggregation algorithms (Optimal Weight and Inverse Surprising Popularity) that outperform standard majority voting for multi-agent LLM reasoning by leveraging first-order and second-order information to account for model heterogeneity and correlation.


<details>
  <summary>Details</summary>
Motivation: Standard majority voting treats all LLM answers equally, failing to consider latent heterogeneity and correlation across models, which limits the reliability of collective decisions in multi-agent LLM reasoning systems.

Method: Designed two aggregation algorithms: Optimal Weight (OW) and Inverse Surprising Popularity (ISP) that leverage both first-order and second-order information about model responses.

Result: The methods consistently outperform majority voting across synthetic datasets, LLM fine-tuning benchmarks (UltraFeedback, MMLU), and a real-world healthcare setting (ARMMAN), providing both practical performance gains and conceptual insights.

Conclusion: The proposed algorithms effectively mitigate inherent limitations of majority voting under mild assumptions, leading to more reliable collective decisions in multi-agent LLM pipelines.

Abstract: With the rapid progress of multi-agent large language model (LLM) reasoning,
how to effectively aggregate answers from multiple LLMs has emerged as a
fundamental challenge. Standard majority voting treats all answers equally,
failing to consider latent heterogeneity and correlation across models. In this
work, we design two new aggregation algorithms called Optimal Weight (OW) and
Inverse Surprising Popularity (ISP), leveraging both first-order and
second-order information. Our theoretical analysis shows these methods provably
mitigate inherent limitations of majority voting under mild assumptions,
leading to more reliable collective decisions. We empirically validate our
algorithms on synthetic datasets, popular LLM fine-tuning benchmarks such as
UltraFeedback and MMLU, and a real-world healthcare setting ARMMAN. Across all
cases, our methods consistently outperform majority voting, offering both
practical performance gains and conceptual insights for the design of robust
multi-agent LLM pipelines.

</details>


### [186] [Realistic CDSS Drug Dosing with End-to-end Recurrent Q-learning for Dual Vasopressor Control](https://arxiv.org/abs/2510.01508)
*Will Y. Zou,Jean Feng,Alexandre Kalimouttou,Jennifer Yuntong Zhang,Christopher W. Seymour,Romain Pirracchio*

Main category: cs.LG

TL;DR: This paper presents an end-to-end RL approach for dual vasopressor dosing in ICU septic shock patients, using action space design to improve interpretability and clinical adoption while maintaining efficacy.


<details>
  <summary>Details</summary>
Motivation: Address skepticism from practitioners about RL-based CDSS due to inoperable dosing decisions, particularly for dual vasopressor administration in ICU patients with septic shock.

Method: Combines offline conservative Q-learning with novel recurrent modeling in replay buffer to capture temporal dependencies; applies action space design accommodating discrete, continuous, and directional dosing strategies.

Result: Action space design improves interpretability and facilitates clinical adoption while preserving efficacy; achieves over 15% survival improvement probability on eICU and MIMIC datasets; learned behavioral policies are profoundly influenced by action space formulation.

Conclusion: The proposed methods successfully address clinical skepticism by designing interpretable action spaces that align with established protocols while improving patient outcomes.

Abstract: Reinforcement learning (RL) applications in Clinical Decision Support Systems
(CDSS) frequently encounter skepticism from practitioners regarding inoperable
dosing decisions. We address this challenge with an end-to-end approach for
learning optimal drug dosing and control policies for dual vasopressor
administration in intensive care unit (ICU) patients with septic shock. For
realistic drug dosing, we apply action space design that accommodates discrete,
continuous, and directional dosing strategies in a system that combines offline
conservative Q-learning with a novel recurrent modeling in a replay buffer to
capture temporal dependencies in ICU time-series data. Our comparative analysis
of norepinephrine dosing strategies across different action space formulations
reveals that the designed action spaces improve interpretability and facilitate
clinical adoption while preserving efficacy. Empirical results1 on eICU and
MIMIC demonstrate that action space design profoundly influences learned
behavioral policies. The proposed methods achieve improved patient outcomes of
over 15% in survival improvement probability, while aligning with established
clinical protocols.

</details>


### [187] [Flock: A Knowledge Graph Foundation Model via Learning on Random Walks](https://arxiv.org/abs/2510.01510)
*Jinwoo Kim,Xingyue Huang,Krzysztof Olejniczak,Kyungbin Min,Michael Bronstein,Seunghoon Hong,ƒ∞smail ƒ∞lkan Ceylan*

Main category: cs.LG

TL;DR: The paper introduces Flock, a knowledge graph foundation model that uses probabilistic node-relation equivariance to overcome limitations of deterministic equivariance in zero-shot link prediction, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current knowledge graph foundation models (KGFMs) use deterministic equivariance which limits their expressive power and prevents them from distinguishing structurally similar but semantically distinct relations in zero-shot link prediction tasks.

Method: Flock uses probabilistic node-relation equivariance that preserves equivariance in distribution while incorporating principled randomization. It iteratively samples random walks, encodes them via recording protocol, embeds with sequence model, and aggregates representations via learned pooling.

Result: Flock perfectly solves the new diagnostic dataset Petals where current KGFMs fail, and achieves state-of-the-art performance on entity and relation prediction tasks across 54 knowledge graphs from diverse domains.

Conclusion: Probabilistic node-relation equivariance enables more expressive knowledge graph foundation models that can better handle zero-shot link prediction by breaking symmetries while maintaining equivariance properties.

Abstract: We study the problem of zero-shot link prediction on knowledge graphs (KGs),
which requires models to generalize over novel entities and novel relations.
Knowledge graph foundation models (KGFMs) address this task by enforcing
equivariance over both nodes and relations, learning from structural properties
of nodes and relations, which are then transferable to novel graphs with
similar structural properties. However, the conventional notion of
deterministic equivariance imposes inherent limits on the expressive power of
KGFMs, preventing them from distinguishing structurally similar but
semantically distinct relations. To overcome this limitation, we introduce
probabilistic node-relation equivariance, which preserves equivariance in
distribution while incorporating a principled randomization to break symmetries
during inference. Building on this principle, we present Flock, a KGFM that
iteratively samples random walks, encodes them into sequences via a recording
protocol, embeds them with a sequence model, and aggregates representations of
nodes and relations via learned pooling. Crucially, Flock respects
probabilistic node-relation equivariance and is a universal approximator for
isomorphism-invariant link-level functions over KGs. Empirically, Flock
perfectly solves our new diagnostic dataset Petals where current KGFMs fail,
and achieves state-of-the-art performances on entity- and relation prediction
tasks on 54 KGs from diverse domains.

</details>


### [188] [Predictive Modeling and Explainable AI for Veterinary Safety Profiles, Residue Assessment, and Health Outcomes Using Real-World Data and Physicochemical Properties](https://arxiv.org/abs/2510.01520)
*Hossein Sholehrasa,Xuan Xu,Doina Caragea,Jim E. Riviere,Majid Jaberi-Douraki*

Main category: cs.LG

TL;DR: A predictive framework using machine learning models to classify veterinary drug adverse event outcomes (Death vs. Recovery) from FDA data, achieving high performance with ensemble methods and interpretable AI.


<details>
  <summary>Details</summary>
Motivation: To protect animal welfare and human food safety by predicting adverse events that may lead to violative residues in the food chain, supporting early detection of high-risk drug-event profiles.

Method: Used ~1.28 million FDA reports with preprocessing, VeDDRA ontology standardization, data normalization, and integration of physicochemical drug properties. Evaluated multiple ML models including Random Forest, CatBoost, XGBoost, ExcelFormer, and LLMs with ensemble methods and AUM-based pseudo-labeling.

Result: Ensemble methods and CatBoost achieved precision, recall, and F1-scores of 0.95. SHAP analysis identified biologically plausible predictors including lung/heart disorders, animal demographics, and drug properties strongly linked to fatal outcomes.

Conclusion: Combining rigorous data engineering, advanced ML, and explainable AI enables accurate, interpretable predictions of veterinary safety outcomes, supporting regulatory decision-making and residue risk assessment.

Abstract: The safe use of pharmaceuticals in food-producing animals is vital to protect
animal welfare and human food safety. Adverse events (AEs) may signal
unexpected pharmacokinetic or toxicokinetic effects, increasing the risk of
violative residues in the food chain. This study introduces a predictive
framework for classifying outcomes (Death vs. Recovery) using ~1.28 million
reports (1987-2025 Q1) from the U.S. FDA's OpenFDA Center for Veterinary
Medicine. A preprocessing pipeline merged relational tables and standardized
AEs through VeDDRA ontologies. Data were normalized, missing values imputed,
and high-cardinality features reduced; physicochemical drug properties were
integrated to capture chemical-residue links. We evaluated supervised models,
including Random Forest, CatBoost, XGBoost, ExcelFormer, and large language
models (Gemma 3-27B, Phi 3-12B). Class imbalance was addressed, such as
undersampling and oversampling, with a focus on prioritizing recall for fatal
outcomes. Ensemble methods(Voting, Stacking) and CatBoost performed best,
achieving precision, recall, and F1-scores of 0.95. Incorporating Average
Uncertainty Margin (AUM)-based pseudo-labeling of uncertain cases improved
minority-class detection, particularly in ExcelFormer and XGBoost.
Interpretability via SHAP identified biologically plausible predictors,
including lung, heart, and bronchial disorders, animal demographics, and drug
physicochemical properties. These features were strongly linked to fatal
outcomes. Overall, the framework shows that combining rigorous data
engineering, advanced machine learning, and explainable AI enables accurate,
interpretable predictions of veterinary safety outcomes. The approach supports
FARAD's mission by enabling early detection of high-risk drug-event profiles,
strengthening residue risk assessment, and informing regulatory and clinical
decision-making.

</details>


### [189] [CarbonX: An Open-Source Tool for Computational Decarbonization Using Time Series Foundation Models](https://arxiv.org/abs/2510.01521)
*Diptyaroop Maji,Kang Yang,Prashant Shenoy,Ramesh K Sitaraman,Mani Srivastava*

Main category: cs.LG

TL;DR: CarbonX is an open-source tool using Time Series Foundation Models for carbon intensity forecasting and imputation, achieving strong zero-shot performance across 214 grids worldwide with minimal data requirements.


<details>
  <summary>Details</summary>
Motivation: Existing carbon intensity forecasting tools have limitations: they require grid-specific electricity mix data, depend on separate grid-specific models making global coverage difficult, and lack uncertainty estimates which limits reliability for carbon-aware applications.

Method: CarbonX leverages Time Series Foundation Models (TSFMs) using only historical carbon intensity data and a single general model, providing forecasts for up to 21 days with prediction intervals and uncertainty estimates.

Result: Zero-shot forecasting achieved 15.82% MAPE across 214 grids worldwide. On 13 benchmark grids, average MAPE was 9.59% with tail forecasting MAPE of 16.54% and 95% coverage prediction intervals. When fine-tuned, CarbonX outperformed statistical baselines by 1.2-3.9X on imputation tasks.

Conclusion: CarbonX can be easily used on any grid with limited data while delivering strong performance, making it a practical tool for global-scale decarbonization applications.

Abstract: Computational decarbonization aims to reduce carbon emissions in computing
and societal systems such as data centers, transportation, and built
environments. This requires accurate, fine-grained carbon intensity forecasts,
yet existing tools have several key limitations: (i) they require grid-specific
electricity mix data, restricting use where such information is unavailable;
(ii) they depend on separate grid-specific models that make it challenging to
provide global coverage; and (iii) they provide forecasts without uncertainty
estimates, limiting reliability for downstream carbon-aware applications.
  In this paper, we present CarbonX, an open-source tool that leverages Time
Series Foundation Models (TSFMs) for a range of decarbonization tasks. CarbonX
utilizes the versatility of TSFMs to provide strong performance across multiple
tasks, such as carbon intensity forecasting and imputation, and across diverse
grids. Using only historical carbon intensity data and a single general model,
our tool achieves a zero-shot forecasting Mean Absolute Percentage Error (MAPE)
of 15.82% across 214 grids worldwide. Across 13 benchmark grids, CarbonX
performance is comparable with the current state-of-the-art, with an average
MAPE of 9.59% and tail forecasting MAPE of 16.54%, while also providing
prediction intervals with 95% coverage. CarbonX can provide forecasts for up to
21 days with minimal accuracy degradation. Further, when fully fine-tuned,
CarbonX outperforms the statistical baselines by 1.2--3.9X on the imputation
task. Overall, these results demonstrate that CarbonX can be used easily on any
grid with limited data and still deliver strong performance, making it a
practical tool for global-scale decarbonization.

</details>


### [190] [On Integer Programming for the Binarized Neural Network Verification Problem](https://arxiv.org/abs/2510.01525)
*Woojin Kim,James R. Luedtke*

Main category: cs.LG

TL;DR: The paper presents improved integer programming methods for verifying binarized neural networks (BNNs) against input perturbations, enabling verification of larger perturbation ranges than existing approaches.


<details>
  <summary>Details</summary>
Motivation: BNN verification is important for measuring robustness against adversarial attacks, but existing IP formulations face challenges due to large integrality gaps from big-M constraints.

Method: Two techniques: (1) new linear objective method for multi-class classification, (2) new valid inequality generation exploiting BNNs' recursive structure.

Result: The techniques enable verifying BNNs against higher input perturbation ranges than existing IP approaches within limited time.

Conclusion: The proposed IP formulation improvements significantly enhance BNN verification capabilities for robustness assessment.

Abstract: Binarized neural networks (BNNs) are feedforward neural networks with binary
weights and activation functions. In the context of using a BNN for
classification, the verification problem seeks to determine whether a small
perturbation of a given input can lead it to be misclassified by the BNN, and
the robustness of the BNN can be measured by solving the verification problem
over multiple inputs. The BNN verification problem can be formulated as an
integer programming (IP) problem. However, the natural IP formulation is often
challenging to solve due to a large integrality gap induced by big-$M$
constraints. We present two techniques to improve the IP formulation. First, we
introduce a new method for obtaining a linear objective for the multi-class
setting. Second, we introduce a new technique for generating valid inequalities
for the IP formulation that exploits the recursive structure of BNNs. We find
that our techniques enable verifying BNNs against a higher range of input
perturbation than existing IP approaches within a limited time.

</details>


### [191] [Round-trip Reinforcement Learning: Self-Consistent Training for Better Chemical LLMs](https://arxiv.org/abs/2510.01527)
*Lecheng Kong,Xiyuan Wang,Yixin Chen,Muhan Zhang*

Main category: cs.LG

TL;DR: RTRL is a reinforcement learning framework that trains LLMs to achieve round-trip consistency in chemical tasks, using successful round-trip transformations as rewards, which significantly improves performance and consistency.


<details>
  <summary>Details</summary>
Motivation: Current chemical LLMs lack round-trip consistency - they can perform tasks like molecule captioning but fail to reconstruct original structures from their own generated text, indicating memorization rather than true understanding.

Method: Round-Trip Reinforcement Learning (RTRL) uses successful round-trip transformations as reward signals. An iterative variant alternates forward and reverse mappings in a self-improvement loop, leveraging unlabeled chemical data efficiently.

Result: RTRL significantly boosts performance and consistency over strong baselines across supervised, self-supervised, and synthetic data regimes.

Conclusion: Round-trip consistency is not just a desirable property but a trainable objective, offering a new path toward more robust and reliable foundation models for computational chemistry.

Abstract: Large Language Models (LLMs) are emerging as versatile foundation models for
computational chemistry, handling bidirectional tasks like reaction prediction
and retrosynthesis. However, these models often lack round-trip consistency.
For instance, a state-of-the-art chemical LLM may successfully caption a
molecule, yet be unable to accurately reconstruct the original structure from
its own generated text. This inconsistency suggests that models are learning
unidirectional memorization rather than flexible mastery. Indeed, recent work
has demonstrated a strong correlation between a model's round-trip consistency
and its performance on the primary tasks. This strong correlation reframes
consistency into a direct target for model improvement. We therefore introduce
Round-Trip Reinforcement Learning (RTRL), a novel framework that trains a model
to improve its consistency by using the success of a round-trip transformation
as a reward signal. We further propose an iterative variant where forward and
reverse mappings alternately train each other in a self-improvement loop, a
process that is highly data-efficient and notably effective with the massive
amount of unlabelled data common in chemistry. Experiments demonstrate that
RTRL significantly \textbf{boosts performance and consistency} over strong
baselines across supervised, self-supervised, and synthetic data regimes. This
work shows that round-trip consistency is not just a desirable property but a
trainable objective, offering a new path toward more robust and reliable
foundation models.

</details>


### [192] [Bypassing Prompt Guards in Production with Controlled-Release Prompting](https://arxiv.org/abs/2510.01529)
*Jaiden Fairoze,Sanjam Garg,Keewoo Lee,Mingyuan Wang*

Main category: cs.LG

TL;DR: A new attack method bypasses prompt guards in large language models by exploiting resource asymmetry between lightweight guards and main LLMs, successfully jailbreaking production models while maintaining response quality.


<details>
  <summary>Details</summary>
Motivation: As LLMs advance, ensuring AI safety through prompt guards is important, but current lightweight prompt guards have limitations that need to be exposed and addressed.

Method: The attack exploits resource asymmetry between prompt guards and main LLMs by encoding jailbreak prompts that lightweight guards cannot decode but the main model can interpret.

Result: The method consistently jailbreaks production models including Google Gemini, DeepSeek Chat, Grok, and Mistral Le Chat while maintaining response quality, even under highly protected chat interfaces.

Conclusion: This reveals inherent vulnerabilities in lightweight prompt guards and underscores the need to shift defenses from blocking malicious inputs to preventing malicious outputs, while also identifying other critical alignment issues.

Abstract: As large language models (LLMs) advance, ensuring AI safety and alignment is
paramount. One popular approach is prompt guards, lightweight mechanisms
designed to filter malicious queries while being easy to implement and update.
In this work, we introduce a new attack that circumvents such prompt guards,
highlighting their limitations. Our method consistently jailbreaks production
models while maintaining response quality, even under the highly protected chat
interfaces of Google Gemini (2.5 Flash/Pro), DeepSeek Chat (DeepThink), Grok
(3), and Mistral Le Chat (Magistral). The attack exploits a resource asymmetry
between the prompt guard and the main LLM, encoding a jailbreak prompt that
lightweight guards cannot decode but the main model can. This reveals an attack
surface inherent to lightweight prompt guards in modern LLM architectures and
underscores the need to shift defenses from blocking malicious inputs to
preventing malicious outputs. We additionally identify other critical alignment
issues, such as copyrighted data extraction, training data extraction, and
malicious response leakage during thinking.

</details>


### [193] [NVIDIA AI Aerial: AI-Native Wireless Communications](https://arxiv.org/abs/2510.01533)
*Kobi Cohen-Arazi,Michael Roe,Zhen Hu,Rohan Chavan,Anna Ptasznik,Joanna Lin,Joao Morais,Joseph Boccuzzi,Tommaso Balercia*

Main category: cs.LG

TL;DR: A framework that compiles Python-based algorithms into GPU-runnable blobs for 6G AI-native wireless systems, enabling efficient integration of DSP and ML in cellular networks.


<details>
  <summary>Details</summary>
Motivation: 6G requires seamless integration of digital signal processing and machine learning in cellular networks, bringing network life cycles closer to AI systems where models are iteratively trained, simulated, and deployed.

Method: Propose a robust framework that compiles Python-based algorithms into GPU-runnable blobs, demonstrated through a convolutional neural network for channel estimation in PUSCH receiver, implemented in digital twin and real-time testbed using NVIDIA AI Aerial platform.

Result: Achieves a unified approach ensuring efficiency, flexibility, and highest performance on NVIDIA GPUs for AI/ML integration in cellular systems.

Conclusion: The methodology lays foundation for scalable AI/ML integration in next-generation cellular systems and is essential for realizing natively intelligent 6G networks.

Abstract: 6G brings a paradigm shift towards AI-native wireless systems, necessitating
the seamless integration of digital signal processing (DSP) and machine
learning (ML) within the software stacks of cellular networks. This
transformation brings the life cycle of modern networks closer to AI systems,
where models and algorithms are iteratively trained, simulated, and deployed
across adjacent environments. In this work, we propose a robust framework that
compiles Python-based algorithms into GPU-runnable blobs. The result is a
unified approach that ensures efficiency, flexibility, and the highest possible
performance on NVIDIA GPUs. As an example of the capabilities of the framework,
we demonstrate the efficacy of performing the channel estimation function in
the PUSCH receiver through a convolutional neural network (CNN) trained in
Python. This is done in a digital twin first, and subsequently in a real-time
testbed. Our proposed methodology, realized in the NVIDIA AI Aerial platform,
lays the foundation for scalable integration of AI/ML models into
next-generation cellular systems, and is essential for realizing the vision of
natively intelligent 6G networks.

</details>


### [194] [TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis](https://arxiv.org/abs/2510.01538)
*Haokun Zhao,Xiang Zhang,Jiaqi Wei,Yiwei Xu,Yuting He,Siqi Sun,Chenyu You*

Main category: cs.LG

TL;DR: TSci is the first LLM-driven agentic framework for general time series forecasting that uses four specialized agents to automate preprocessing, model selection, forecasting, and reporting, achieving significant error reduction compared to statistical and LLM baselines.


<details>
  <summary>Details</summary>
Motivation: Current time series forecasting faces challenges with thousands of short, noisy series across diverse domains, requiring labor-intensive preprocessing and validation. Existing models are domain-specific and generalize poorly, creating demand for a domain-agnostic framework that minimizes human intervention.

Method: TSci uses four specialized LLM-driven agents: Curator (LLM-guided diagnostics and preprocessing), Planner (hypothesis space narrowing using multi-modal diagnostics), Forecaster (model fitting, validation, and ensemble selection), and Reporter (comprehensive report generation).

Result: TSci outperforms both statistical and LLM-based baselines on eight benchmarks, reducing forecast error by an average of 10.4% and 38.2% respectively, while producing transparent, interpretable reports.

Conclusion: TSci transforms time series forecasting into a white-box system that is interpretable, extensible across tasks, and significantly reduces human intervention while improving forecasting accuracy across diverse domains.

Abstract: Time series forecasting is central to decision-making in domains as diverse
as energy, finance, climate, and public health. In practice, forecasters face
thousands of short, noisy series that vary in frequency, quality, and horizon,
where the dominant cost lies not in model fitting, but in the labor-intensive
preprocessing, validation, and ensembling required to obtain reliable
predictions. Prevailing statistical and deep learning models are tailored to
specific datasets or domains and generalize poorly. A general, domain-agnostic
framework that minimizes human intervention is urgently in demand. In this
paper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic
framework for general time series forecasting. The framework comprises four
specialized agents: Curator performs LLM-guided diagnostics augmented by
external tools that reason over data statistics to choose targeted
preprocessing; Planner narrows the hypothesis space of model choice by
leveraging multi-modal diagnostics and self-planning over the input; Forecaster
performs model fitting and validation and, based on the results, adaptively
selects the best model configuration as well as ensemble strategy to make final
predictions; and Reporter synthesizes the whole process into a comprehensive,
transparent report. With transparent natural-language rationales and
comprehensive reports, TSci transforms the forecasting workflow into a
white-box system that is both interpretable and extensible across tasks.
Empirical results on eight established benchmarks demonstrate that TSci
consistently outperforms both statistical and LLM-based baselines, reducing
forecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci
produces a clear and rigorous report that makes the forecasting workflow more
transparent and interpretable.

</details>


### [195] [Executable Counterfactuals: Improving LLMs' Causal Reasoning Through Code](https://arxiv.org/abs/2510.01539)
*Aniket Vashishtha,Qirun Dai,Hongyuan Mei,Amit Sharma,Chenhao Tan,Hao Peng*

Main category: cs.LG

TL;DR: The paper introduces 'executable counterfactuals' - a framework that requires all three steps of counterfactual reasoning (abduction, intervention, prediction) through code and math problems. It reveals a 25-40% performance drop in SOTA models when moving from interventional to full counterfactual reasoning, and shows that reinforcement learning outperforms supervised finetuning for generalization.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of LLMs' counterfactual reasoning skip the abduction step, leading to overestimation of performance. This gap is critical since counterfactual reasoning is essential for causal understanding and high-stakes applications like scientific research.

Method: Developed executable counterfactuals framework using code and math problems that explicitly require abduction, intervention, and prediction. Created scalable synthetic data with varying difficulty. Compared supervised finetuning vs reinforcement learning approaches for improving counterfactual reasoning.

Result: Found 25-40% accuracy drop from interventional to counterfactual reasoning in SOTA models. Supervised finetuning improved in-domain performance but hurt OOD generalization. Reinforcement learning achieved 1.5x-2x improvement on code problems and generalized to math problems, inducing core cognitive behaviors.

Conclusion: Reinforcement learning is more effective than supervised finetuning for improving LLMs' counterfactual reasoning, as it induces fundamental cognitive behaviors that generalize across domains, while supervised approaches risk overfitting.

Abstract: Counterfactual reasoning, a hallmark of intelligence, consists of three
steps: inferring latent variables from observations (abduction), constructing
alternatives (interventions), and predicting their outcomes (prediction). This
skill is essential for advancing LLMs' causal understanding and expanding their
applications in high-stakes domains such as scientific research. However,
existing efforts in assessing LLM's counterfactual reasoning capabilities tend
to skip the abduction step, effectively reducing to interventional reasoning
and leading to overestimation of LLM performance. To address this, we introduce
executable counterfactuals, a novel framework that operationalizes causal
reasoning through code and math problems. Our framework explicitly requires all
three steps of counterfactual reasoning and enables scalable synthetic data
creation with varying difficulty, creating a frontier for evaluating and
improving LLM's reasoning. Our results reveal substantial drop in accuracy
(25-40%) from interventional to counterfactual reasoning for SOTA models like
o4-mini and Claude-4-Sonnet. To address this gap, we construct a training set
comprising counterfactual code problems having if-else condition and test on
out-of-domain code structures (e.g. having while-loop); we also test whether a
model trained on code would generalize to counterfactual math word problems.
While supervised finetuning on stronger models' reasoning traces improves
in-domain performance of Qwen models, it leads to a decrease in accuracy on OOD
tasks such as counterfactual math problems. In contrast, reinforcement learning
induces the core cognitive behaviors and generalizes to new domains, yielding
gains over the base model on both code (improvement of 1.5x-2x) and math
problems. Analysis of the reasoning traces reinforces these findings and
highlights the promise of RL for improving LLMs' counterfactual reasoning.

</details>


### [196] [Predictive Preference Learning from Human Interventions](https://arxiv.org/abs/2510.01545)
*Haoyuan Cai,Zhenghao Peng,Bolei Zhou*

Main category: cs.LG

TL;DR: PPL is a predictive preference learning method that bootstraps human interventions to future time steps, propagating expert corrections to safety-critical regions to improve learning efficiency and reduce human demonstrations needed.


<details>
  <summary>Details</summary>
Motivation: Current interactive imitation learning methods only correct agent actions at current states without adjusting future potentially hazardous actions, limiting their effectiveness in safety-critical scenarios.

Method: Leverages implicit preference signals from human interventions by bootstrapping each intervention L future time steps (preference horizon), assuming same agent action and human intervention patterns, then applies preference optimization on these future states.

Result: Demonstrated efficiency and generality on autonomous driving and robotic manipulation benchmarks, showing significant improvement in learning efficiency and reduction of human demonstrations needed.

Conclusion: PPL effectively propagates expert corrections to safety-critical regions, with theoretical analysis showing that appropriate preference horizon selection balances risky state coverage with label correctness to bound algorithmic optimality gap.

Abstract: Learning from human involvement aims to incorporate the human subject to
monitor and correct agent behavior errors. Although most interactive imitation
learning methods focus on correcting the agent's action at the current state,
they do not adjust its actions in future states, which may be potentially more
hazardous. To address this, we introduce Predictive Preference Learning from
Human Interventions (PPL), which leverages the implicit preference signals
contained in human interventions to inform predictions of future rollouts. The
key idea of PPL is to bootstrap each human intervention into L future time
steps, called the preference horizon, with the assumption that the agent
follows the same action and the human makes the same intervention in the
preference horizon. By applying preference optimization on these future states,
expert corrections are propagated into the safety-critical regions where the
agent is expected to explore, significantly improving learning efficiency and
reducing human demonstrations needed. We evaluate our approach with experiments
on both autonomous driving and robotic manipulation benchmarks and demonstrate
its efficiency and generality. Our theoretical analysis further shows that
selecting an appropriate preference horizon L balances coverage of risky states
with label correctness, thereby bounding the algorithmic optimality gap. Demo
and code are available at: https://metadriverse.github.io/ppl

</details>


### [197] [MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I Diffusion Models](https://arxiv.org/abs/2510.01549)
*Kevin Zhai,Utsav Singh,Anirudh Thatipelli,Souradip Chakraborty,Anit Kumar Sahu,Furong Huang,Amrit Singh Bedi,Mubarak Shah*

Main category: cs.LG

TL;DR: MIRA is a training-free inference-time alignment method that prevents reward hacking in diffusion models by using image-space regularization to maintain prompt adherence while improving reward scores.


<details>
  <summary>Details</summary>
Motivation: Diffusion models often fail to satisfy user-specific criteria measured by scalar rewards, and existing inference-time alignment methods suffer from reward hacking where images score highly but deviate from the original prompt.

Method: MIRA introduces an image-space, score-based KL surrogate that regularizes the sampling trajectory with a frozen backbone, constraining the output distribution to prevent off-distribution drift while increasing rewards.

Result: Across SDv1.5 and SDXL, multiple rewards and datasets, MIRA achieves >60% win rate vs. strong baselines while preserving prompt adherence, with reward gains and near-zero drift compared to DNO which drifts as compute increases.

Conclusion: MIRA effectively prevents reward hacking through image-space constraints and can be extended to non-differentiable rewards via MIRA-DPO without requiring fine-tuning.

Abstract: Diffusion models excel at generating images conditioned on text prompts, but
the resulting images often do not satisfy user-specific criteria measured by
scalar rewards such as Aesthetic Scores. This alignment typically requires
fine-tuning, which is computationally demanding. Recently, inference-time
alignment via noise optimization has emerged as an efficient alternative,
modifying initial input noise to steer the diffusion denoising process towards
generating high-reward images. However, this approach suffers from reward
hacking, where the model produces images that score highly, yet deviate
significantly from the original prompt. We show that noise-space regularization
is insufficient and that preventing reward hacking requires an explicit
image-space constraint. To this end, we propose MIRA (MItigating Reward
hAcking), a training-free, inference-time alignment method. MIRA introduces an
image-space, score-based KL surrogate that regularizes the sampling trajectory
with a frozen backbone, constraining the output distribution so reward can
increase without off-distribution drift (reward hacking). We derive a tractable
approximation to KL using diffusion scores. Across SDv1.5 and SDXL, multiple
rewards (Aesthetic, HPSv2, PickScore), and public datasets (e.g.,
Animal-Animal, HPDv2), MIRA achieves >60\% win rate vs. strong baselines while
preserving prompt adherence; mechanism plots show reward gains with near-zero
drift, whereas DNO drifts as compute increases. We further introduce MIRA-DPO,
mapping preference optimization to inference time with a frozen backbone,
extending MIRA to non-differentiable rewards without fine-tuning.

</details>


### [198] [Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization](https://arxiv.org/abs/2510.01555)
*Kezhao Liu,Jason Klein Liu,Mingtao Chen,Yiming Liu*

Main category: cs.LG

TL;DR: The paper establishes a unified framework for KL divergence regularization in RLHF, showing that 'k1 in reward' (PPO-style) and 'k2 as loss' are gradient-equivalent and theoretically sound, while 'k3 as loss' (GRPO-style) is a biased approximation.


<details>
  <summary>Details</summary>
Motivation: To address the inconsistent implementation of KL divergence regularization in RLHF methods, where some approaches treat it as a detached coefficient while others use it directly as a loss function, leading to potential theoretical issues.

Method: Developed a unified framework connecting two implementation styles: 'kn in reward' (coefficient for policy score) and 'kn as loss' (direct loss function). Proved gradient equivalence between 'k1 in reward' and 'k2 as loss' under on-policy conditions, and identified biases in off-policy implementations.

Result: Showed that 'k1 in reward' is the principled loss for Reverse KL regularization, and 'k2 as loss' is gradient-equivalent to it. Demonstrated that 'k3 as loss' is a first-order biased approximation. Proposed principled correction for off-policy implementations.

Conclusion: Provides comprehensive gradient-based rationale for choosing and correctly implementing KL regularization in RLHF, enabling more robust and effective systems by unifying theoretical understanding of different implementation approaches.

Abstract: Reinforcement Learning from Human Feedback (RLHF) leverages a
Kullback-Leibler (KL) divergence loss to stabilize training and prevent
overfitting. However, in methods such as GRPO, its implementation may be guided
by principles from numerical value estimation-a practice that overlooks the
term's functional role as an optimization loss. To analyze this issue, we
establish a unified framework that connects two seemingly distinct
implementation styles: using the mathematical term $k_n$ as a detached
coefficient for the policy's score function ('$k_n$ in reward') or as a direct
loss function through which gradients are propagated ('$k_n$ as loss'). We show
that the latter can always be analyzed via an equivalent gradient coefficient
in the former, unifying the two perspectives. Through this framework, we prove
that the conventional '$k_1$ in reward' (like in PPO) is the principled loss
for Reverse KL (RKL) regularization. We further establish a key finding: under
on-policy conditions, the '$k_2$ as loss' formulation is, in fact,
gradient-equivalent to '$k_1$ in reward'. This equivalence, first proven in our
work, identifies both as the theoretically sound implementations of the RKL
objective. In contrast, we show that the recently adopted '$k_3$ as loss' (like
in GRPO) is merely a first-order, biased approximation of the principled loss.
Furthermore, we argue that common off-policy implementations of '$k_n$ as loss'
methods are biased due to neglected importance sampling, and we propose a
principled correction. Our findings provide a comprehensive, gradient-based
rationale for choosing and correctly implementing KL regularization, paving the
way for more robust and effective RLHF systems.

</details>


### [199] [Large-Scale Bayesian Causal Discovery with Interventional Data](https://arxiv.org/abs/2510.01562)
*Seong Woo Han,Daniel Duy Vo,Brielin C. Brown*

Main category: cs.LG

TL;DR: IBCD is a Bayesian framework for causal discovery using interventional data that models causal effects with matrix normal distribution and spike-and-slab priors, enabling uncertainty quantification and superior structure recovery.


<details>
  <summary>Details</summary>
Motivation: Existing causal discovery methods perform poorly on large-scale tasks and lack uncertainty quantification, despite advancements in genomic perturbation screens that provide interventional data.

Method: Models likelihood of total causal effects matrix using matrix normal distribution, places spike-and-slab horseshoe prior on edges, learns data-driven weights for network structures from observational data, and treats edges as latent variables for uncertainty-aware inference.

Result: IBCD achieves superior structure recovery compared to existing baselines in simulations and successfully identifies robust graph structures from CRISPR perturbation data on 521 genes using edge posterior inclusion probabilities.

Conclusion: The proposed Bayesian framework effectively addresses limitations of existing methods by enabling uncertainty quantification and improved performance on large-scale causal discovery tasks with interventional data.

Abstract: Inferring the causal relationships among a set of variables in the form of a
directed acyclic graph (DAG) is an important but notoriously challenging
problem. Recently, advancements in high-throughput genomic perturbation screens
have inspired development of methods that leverage interventional data to
improve model identification. However, existing methods still suffer poor
performance on large-scale tasks and fail to quantify uncertainty. Here, we
propose Interventional Bayesian Causal Discovery (IBCD), an empirical Bayesian
framework for causal discovery with interventional data. Our approach models
the likelihood of the matrix of total causal effects, which can be approximated
by a matrix normal distribution, rather than the full data matrix. We place a
spike-and-slab horseshoe prior on the edges and separately learn data-driven
weights for scale-free and Erd\H{o}s-R\'enyi structures from observational
data, treating each edge as a latent variable to enable uncertainty-aware
inference. Through extensive simulation, we show that IBCD achieves superior
structure recovery compared to existing baselines. We apply IBCD to CRISPR
perturbation (Perturb-seq) data on 521 genes, demonstrating that edge posterior
inclusion probabilities enable identification of robust graph structures.

</details>


### [200] [TetriServe: Efficient DiT Serving for Heterogeneous Image Generation](https://arxiv.org/abs/2510.01565)
*Runyu Lu,Shiqi He,Wenxuan Tan,Shenggui Li,Ruofan Wu,Jeff J. Ma,Ang Chen,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: TetriServe introduces step-level sequence parallelism for efficient Diffusion Transformer (DiT) serving, dynamically adjusting parallelism per request based on deadlines to improve SLO attainment by up to 32% compared to existing solutions.


<details>
  <summary>Details</summary>
Motivation: Existing DiT serving systems use fixed parallelism, which is inefficient for heterogeneous workloads with mixed resolutions and deadlines, leading to poor GPU utilization and low SLO attainment.

Method: TetriServe implements step-level sequence parallelism with round-based scheduling: discretizing time into fixed rounds, adapting parallelism at step level to minimize GPU consumption, and jointly packing requests to minimize late completions.

Result: Extensive evaluation shows TetriServe achieves up to 32% higher SLO attainment compared to existing solutions without degrading image quality.

Conclusion: Step-level sequence parallelism with dynamic adaptation based on deadlines enables highly efficient DiT serving with significantly improved SLO performance.

Abstract: Diffusion Transformer (DiT) models excel at generating highquality images
through iterative denoising steps, but serving them under strict Service Level
Objectives (SLOs) is challenging due to their high computational cost,
particularly at large resolutions. Existing serving systems use fixed degree
sequence parallelism, which is inefficient for heterogeneous workloads with
mixed resolutions and deadlines, leading to poor GPU utilization and low SLO
attainment.
  In this paper, we propose step-level sequence parallelism to dynamically
adjust the parallel degree of individual requests according to their deadlines.
We present TetriServe, a DiT serving system that implements this strategy for
highly efficient image generation. Specifically, TetriServe introduces a novel
round-based scheduling mechanism that improves SLO attainment: (1) discretizing
time into fixed rounds to make deadline-aware scheduling tractable, (2)
adapting parallelism at the step level and minimize GPU hour consumption, and
(3) jointly packing requests to minimize late completions. Extensive evaluation
on state-of-the-art DiT models shows that TetriServe achieves up to 32% higher
SLO attainment compared to existing solutions without degrading image quality.

</details>


### [201] [From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?](https://arxiv.org/abs/2510.01571)
*Hanqun Cao,Hongrui Zhang,Junde Xu,Zhou Zhang,Lingdong Shen,Minghao Sun,Ge Liu,Jinbo Xu,Wu-Jun Li,Jinren Ni,Cesar de la Fuente-Nunez,Tianfan Fu,Yejin Choi,Pheng-Ann Heng,Fang Wu*

Main category: cs.LG

TL;DR: RL combined with protein language models improves protein design success rates and efficiency across multiple domains, with performance gains depending on task headroom, reward fidelity, and policy capacity.


<details>
  <summary>Details</summary>
Motivation: To determine if reinforcement learning can push protein language models beyond their pretraining priors and uncover latent sequence-structure-function rules that supervised learning cannot capture.

Method: Pairing RL with PLMs across four protein design domains using diverse RL algorithms and model classes, analyzing the interaction between task headroom, reward fidelity, and policy capacity.

Result: RL consistently boosts success rates and sample efficiency across benchmarks. Performance follows a three-factor interaction where gains scale when rewards are accurate, policies have sufficient capacity, and tasks leave room beyond supervised baselines.

Conclusion: Practical guidance for RL in protein design: prioritize reward modeling and calibration before scaling policy size, match algorithm and regularization strength to task difficulty, and allocate capacity where marginal gains are largest.

Abstract: Protein language models (PLMs) have advanced computational protein science
through large-scale pretraining and scalable architectures. In parallel,
reinforcement learning (RL) has broadened exploration and enabled precise
multi-objective optimization in protein design. Yet whether RL can push PLMs
beyond their pretraining priors to uncover latent sequence-structure-function
rules remains unclear. We address this by pairing RL with PLMs across four
domains: antimicrobial peptide design, kinase variant optimization, antibody
engineering, and inverse folding. Using diverse RL algorithms and model
classes, we ask if RL improves sampling efficiency and, more importantly, if it
reveals capabilities not captured by supervised learning. Across benchmarks, RL
consistently boosts success rates and sample efficiency. Performance follows a
three-factor interaction: task headroom, reward fidelity, and policy capacity
jointly determine gains. When rewards are accurate and informative, policies
have sufficient capacity, and tasks leave room beyond supervised baselines,
improvements scale; when rewards are noisy or capacity is constrained, gains
saturate despite exploration. This view yields practical guidance for RL in
protein design: prioritize reward modeling and calibration before scaling
policy size, match algorithm and regularization strength to task difficulty,
and allocate capacity where marginal gains are largest. Implementation is
available at https://github.com/chq1155/RL-PLM.

</details>


### [202] [Gradient Shaping Beyond Clipping: A Functional Perspective on Update Magnitude Control](https://arxiv.org/abs/2510.01578)
*Haochen You,Baojing Liu*

Main category: cs.LG

TL;DR: SPAMP is a unified framework that generalizes gradient clipping into smooth, per-layer gradient shaping using statistical tracking and power-based transformations.


<details>
  <summary>Details</summary>
Motivation: Traditional gradient clipping uses hard, fixed thresholds that lack flexibility and ignore gradient distribution dynamics, limiting its effectiveness in stabilizing deep network training.

Method: SPAMP tracks local gradient statistics, dynamically estimates thresholds, and applies power-based transformations to modulate update magnitudes in a differentiable manner, recasting clipping and warmup as dual mechanisms for controlling effective update scale.

Result: Extensive experiments across image and language tasks show that SPAMP improves stability, convergence, and robustness over existing gradient clipping methods.

Conclusion: SPAMP provides a principled alternative to rigid heuristics by offering a unified framework for adaptive gradient shaping that outperforms traditional clipping approaches.

Abstract: Gradient clipping is widely used to stabilize deep network training, but its
formulation as a hard, fixed threshold limits flexibility and ignores gradient
distribution dynamics. We propose SPAMP (Statistical Per-layer Adaptive
Modulation and Projection), a unified framework that generalizes clipping into
smooth, per-layer gradient shaping. SPAMP tracks local gradient statistics,
dynamically estimates thresholds, and applies power-based transformations to
modulate update magnitudes in a differentiable manner. This perspective recasts
clipping and warmup as dual mechanisms for controlling the effective update
scale $\eta_t \|g_t\|$, offering a principled alternative to rigid heuristics.
Extensive experiments across image and language tasks demonstrate that SPAMP
improves stability, convergence, and robustness over existing methods.

</details>


### [203] [Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression](https://arxiv.org/abs/2510.01581)
*Joykirat Singh,Justin Chih-Yao Chen,Archiki Prasad,Elias Stengel-Eskin,Akshay Nambi,Mohit Bansal*

Main category: cs.LG

TL;DR: TRAAC is an RL method that adaptively allocates reasoning steps based on problem difficulty, using self-attention to compress reasoning trajectories and improve efficiency.


<details>
  <summary>Details</summary>
Motivation: Current models suffer from under-adaptivity - they either underthink (too few steps for hard problems) or overthink (too many steps for easy problems), leading to inefficiency and errors.

Method: TRAAC uses online post-training RL with self-attention to identify important reasoning steps and prune redundant ones, while incorporating difficulty estimation into rewards to allocate appropriate reasoning budget.

Result: TRAAC achieves 8.4% accuracy gain with 36.8% reduction in reasoning length compared to base models, and outperforms other RL baselines across math and non-math tasks.

Conclusion: The method enables adaptive thinking by combining difficulty calibration with attention-based compression, showing strong generalization across diverse reasoning tasks.

Abstract: Recent thinking models solve complex reasoning tasks by scaling test-time
compute, but this scaling must be allocated in line with task difficulty. On
one hand, short reasoning (underthinking) leads to errors on harder problems
that require extended reasoning steps; but, excessively long reasoning
(overthinking) can be token-inefficient, generating unnecessary steps even
after reaching a correct intermediate solution. We refer to this as
under-adaptivity, where the model fails to modulate its response length
appropriately given problems of varying difficulty. To address under-adaptivity
and strike a balance between under- and overthinking, we propose TRAAC (Think
Right with Adaptive, Attentive Compression), an online post-training RL method
that leverages the model's self-attention over a long reasoning trajectory to
identify important steps and prune redundant ones. TRAAC also estimates
difficulty and incorporates it into training rewards, thereby learning to
allocate reasoning budget commensurate with example difficulty. Our approach
improves accuracy, reduces reasoning steps, and enables adaptive thinking
compared to base models and other RL baselines. Across a variety of tasks
(AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute
accuracy gain of 8.4% with a relative reduction in reasoning length of 36.8%
compared to the base model, and a 7.9% accuracy gain paired with a 29.4% length
drop compared to the best RL baseline. TRAAC also shows strong generalization:
although our models are trained on math datasets, they show accuracy and
efficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH,
and OptimalThinkingBench. Our analysis further verifies that TRAAC provides
fine-grained adjustments to thinking budget based on difficulty and that a
combination of task-difficulty calibration and attention-based compression
yields gains across diverse tasks.

</details>


### [204] [Enhancing Noise Robustness of Parkinson's Disease Telemonitoring via Contrastive Feature Augmentation](https://arxiv.org/abs/2510.01588)
*Ziming Tang,Chengbin Hou,Tianyu Zhang,Bangxu Tian,Jinbao Wang,Hairong Lv*

Main category: cs.LG

TL;DR: NoRo is a noise-robust framework that enhances UPDRS prediction for Parkinson's disease telemonitoring by using contrastive learning to generate robust features against patient errors, environmental noise, and data loss.


<details>
  <summary>Details</summary>
Motivation: Parkinson's disease telemonitoring faces three types of noise: patient-induced inaccuracies, environmental noise, and data packet loss, which increase prediction errors for UPDRS scores.

Method: Group speech features into ordered bins based on selected feature values to create contrastive pairs, train MLP encoder for noise-robust features, concatenate with original features, and feed to UPDRS prediction models.

Result: Extensive experiments show NoRo successfully enhances noise robustness of UPDRS prediction across various downstream models under different noisy environments.

Conclusion: NoRo framework effectively addresses noise challenges in PD telemonitoring and improves the reliability of at-home UPDRS assessments.

Abstract: Parkinson's disease (PD) is one of the most common neurodegenerative
disorder. PD telemonitoring emerges as a novel assessment modality enabling
self-administered at-home tests of Unified Parkinson's Disease Rating Scale
(UPDRS) scores, enhancing accessibility for PD patients. However, three types
of noise would occur during measurements: (1) patient-induced measurement
inaccuracies, (2) environmental noise, and (3) data packet loss during
transmission, resulting in higher prediction errors. To address these
challenges, NoRo, a noise-robust UPDRS prediction framework is proposed. First,
the original speech features are grouped into ordered bins, based on the
continuous values of a selected feature, to construct contrastive pairs.
Second, the contrastive pairs are employed to train a multilayer perceptron
encoder for generating noise-robust features. Finally, these features are
concatenated with the original features as the augmented features, which are
then fed into the UPDRS prediction models. Notably, we further introduces a
novel evaluation approach with customizable noise injection module, and
extensive experiments show that NoRo can successfully enhance the noise
robustness of UPDRS prediction across various downstream prediction models
under different noisy environments.

</details>


### [205] [Securing generative artificial intelligence with parallel magnetic tunnel junction true randomness](https://arxiv.org/abs/2510.01598)
*Youwei Bao,Shuhan Yang,Hyunsoo Yang*

Main category: cs.LG

TL;DR: This paper presents a hardware-based true random number generator using spin-transfer torque magnetic tunnel junctions (STT-MTJs) to address security vulnerabilities in AI systems caused by predictable pseudo-random number generators.


<details>
  <summary>Details</summary>
Motivation: Deterministic pseudo-random number generators (PRNGs) in AI models create predictable patterns that are vulnerable to attacks, and conventional defenses have significant energy and latency overhead.

Method: The researchers embedded hardware-generated true random bits from STT-MTJs in a highly parallel, FPGA-assisted prototype system. They integrated these hardware random bits into a generative adversarial network (GAN) trained on CIFAR-10.

Result: The system delivers megabit-per-second true random numbers passing NIST tests with minimal overhead. Integration into GAN reduced insecure outputs by up to 18.6 times compared to low-quality RNG baseline. The system can potentially scale to 106 parallel cells achieving gigabit-per-second throughput.

Conclusion: STT-MTJ-based spintronic RNGs are practical security components for next-generation generative AI systems, offering nanosecond switching speed, high energy efficiency, and established scalability.

Abstract: Deterministic pseudo random number generators (PRNGs) used in generative
artificial intelligence (GAI) models produce predictable patterns vulnerable to
exploitation by attackers. Conventional defences against the vulnerabilities
often come with significant energy and latency overhead. Here, we embed
hardware-generated true random bits from spin-transfer torque magnetic tunnel
junctions (STT-MTJs) to address the challenges. A highly parallel,
FPGA-assisted prototype computing system delivers megabit-per-second true
random numbers, passing NIST randomness tests after in-situ operations with
minimal overhead. Integrating the hardware random bits into a generative
adversarial network (GAN) trained on CIFAR-10 reduces insecure outputs by up to
18.6 times compared to the low-quality random number generators (RNG) baseline.
With nanosecond switching speed, high energy efficiency, and established
scalability, our STT-MTJ-based system holds the potential to scale beyond 106
parallel cells, achieving gigabit-per-second throughput suitable for large
language model sampling. This advancement highlights spintronic RNGs as
practical security components for next-generation GAI systems.

</details>


### [206] [Posterior Collapse as a Phase Transition in Variational Autoencoders](https://arxiv.org/abs/2510.01621)
*Zhen Li,Fan Zhang,Zheng Zhang,Yu Chen*

Main category: cs.LG

TL;DR: Posterior collapse in VAEs is identified as a phase transition governed by data structure and model hyper-parameters, characterized by a critical threshold in KL divergence between posterior and prior distributions.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental nature of posterior collapse in variational autoencoders from a statistical physics perspective, moving beyond viewing it as mere optimization failure.

Method: Analyzed posterior collapse as a phase transition by examining the stability of trivial solutions and identifying critical hyper-parameter thresholds through KL divergence analysis on synthetic and real-world datasets.

Result: Identified a critical boundary separating meaningful latent inference from collapse, characterized by discontinuity in KL divergence, confirming the existence of a phase transition phenomenon.

Conclusion: Posterior collapse is an emerging phase transition arising from the interplay between data structure and variational constraints, providing new insights into deep generative model trainability and representational capacity.

Abstract: We investigate the phenomenon of posterior collapse in variational
autoencoders (VAEs) from the perspective of statistical physics, and reveal
that it constitutes a phase transition governed jointly by data structure and
model hyper-parameters. By analyzing the stability of the trivial solution
associated with posterior collapse, we identify a critical hyper-parameter
threshold. This critical boundary, separating meaningful latent inference from
collapse, is characterized by a discontinuity in the KL divergence between the
approximate posterior and the prior distribution. We validate this critical
behavior on both synthetic and real-world datasets, confirming the existence of
a phase transition. Our results demonstrate that posterior collapse is not
merely an optimization failure, but rather an emerging phase transition arising
from the interplay between data structure and variational constraints. This
perspective offers new insights into the trainability and representational
capacity of deep generative models.

</details>


### [207] [Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to Use Instead](https://arxiv.org/abs/2510.01624)
*Feiyang Kang,Michael Kuchnik,Karthik Padthe,Marin Vlastelica,Ruoxi Jia,Carole-Jean Wu,Newsha Ardalani*

Main category: cs.LG

TL;DR: High SFT scores don't reliably predict RL performance gains; generalization loss and Pass@large k are better proxies for RL outcomes.


<details>
  <summary>Details</summary>
Motivation: Challenge the assumption that high SFT scores translate to improved RL performance, as current practice trains LLMs in two independent stages (SFT then RL).

Method: Trained hundreds of models up to 12B parameters with SFT and RLVR via GRPO, evaluated on 7 math benchmarks with up to 256 repetitions, spending >1M GPU hours across multiple model families and datasets.

Result: Found that high SFT scores can be biased and don't predict RL gains. RL on models with improved SFT performance can lead to worse outcomes than RL on base models. Generalization loss and Pass@large k substantially improve prediction accuracy (up to 0.5 R¬≤ improvement).

Conclusion: Generalization loss and Pass@large k are superior metrics for predicting RL outcomes compared to pre-RL performance, with practical implications for training strategies like epoch allocation and example length selection.

Abstract: In post-training for reasoning Large Language Models (LLMs), the current
state of practice trains LLMs in two independent stages: Supervised Fine-Tuning
(SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as
``RL'' below). In this work, we challenge whether high SFT scores translate to
improved performance after RL. We provide extensive counter-examples where this
is not true. We find high SFT scores can be biased toward simpler or more
homogeneous data and are not reliably predictive of subsequent RL gains or
scaled-up post-training effectiveness. In some cases, RL training on models
with improved SFT performance could lead to substantially worse outcome
compared to RL on the base model without SFT. We study alternative metrics and
identify generalization loss on held-out reasoning examples and Pass@large k
performance to provide strong proxies for the RL outcome. We trained hundreds
of models up to 12B-parameter with SFT and RLVR via GRPO and ran extensive
evaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPU
hours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiple
state-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RL
performance, prediction based on generalization loss and Pass@large k achieves
substantial higher precision, improving $R^2$ coefficient and Spearman's rank
correlation coefficient by up to 0.5 (2x). This provides strong utility for
broad use cases. For example, in most experiments, we find SFT training on
unique examples for a one epoch underperforms training on half examples for two
epochs, either after SFT or SFT-then-RL; With the same SFT budget, training
only on short examples may lead to better SFT performance, though, it often
leads to worse outcome after RL compared to training on examples with varying
lengths. Evaluation tool will be open-sourced.

</details>


### [208] [Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of Scaling Laws, Benefits, and Pitfalls](https://arxiv.org/abs/2510.01631)
*Feiyang Kang,Newsha Ardalani,Michael Kuchnik,Youssef Emad,Mostafa Elhoushi,Shubhabrata Sengupta,Shang-Wen Li,Ramya Raghavendra,Ruoxi Jia,Carole-Jean Wu*

Main category: cs.LG

TL;DR: Synthetic data alone doesn't outperform natural web data, but mixing 1/3 rephrased synthetic with 2/3 natural data can speed up training 5-10x. Textbook-style synthetic data performs poorly, especially at small budgets. Optimal synthetic ratio is ~30%, and larger generators don't necessarily produce better data.


<details>
  <summary>Details</summary>
Motivation: High-quality training data is limited for LLM scaling, and synthetic data offers potential to overcome these limitations, but its effectiveness needs empirical validation.

Method: Large-scale empirical investigation (>1000 LLMs, >100k GPU hours) using unified protocol and scaling laws to compare natural web data, diverse synthetic types (rephrased text, generated textbooks), and mixtures.

Result: Rephrased synthetic data alone isn't faster than natural web data, but 1/3 synthetic + 2/3 natural mixture speeds up training 5-10x. Textbook synthetic data performs poorly. Optimal synthetic ratio is ~30%. Larger generators don't necessarily produce better data than ~8B models.

Conclusion: Synthetic data has conditional benefits - mixing rephrased synthetic with natural data provides significant speedups, while pure synthetic approaches show limitations. Provides mixed evidence on 'model collapse' and offers practical guidance for synthetic data usage.

Abstract: Training data plays a crucial role in Large Language Models (LLM) scaling,
yet high quality data is of limited supply. Synthetic data techniques offer a
potential path toward sidestepping these limitations. We conduct a large-scale
empirical investigation (>1000 LLMs with >100k GPU hours) using a unified
protocol and scaling laws, comparing natural web data, diverse synthetic types
(rephrased text, generated textbooks), and mixtures of natural and synthetic
data. Specifically, we found pre-training on rephrased synthetic data
\textit{alone} is not faster than pre-training on natural web texts; while
pre-training on 1/3 rephrased synthetic data mixed with 2/3 natural web texts
can speed up 5-10x (to reach the same validation loss) at larger data budgets.
Pre-training on textbook-style synthetic data \textit{alone} results in notably
higher loss on many downstream domains especially at small data budgets. "Good"
ratios of synthetic data in training data mixtures depend on the model size and
data budget, empirically converging to ~30% for rephrased synthetic data.
Larger generator models do not necessarily yield better pre-training data than
~8B-param models. These results contribute mixed evidence on "model collapse"
during large-scale single-round (n=1) model training on synthetic
data--training on rephrased synthetic data shows no degradation in performance
in foreseeable scales whereas training on mixtures of textbook-style
pure-generated synthetic data shows patterns predicted by "model collapse". Our
work demystifies synthetic data in pre-training, validates its conditional
benefits, and offers practical guidance.

</details>


### [209] [CAT: Curvature-Adaptive Transformers for Geometry-Aware Learning](https://arxiv.org/abs/2510.01634)
*Ryan Y. Lin,Siddhartha Ojha,Nicholas Bai*

Main category: cs.LG

TL;DR: CAT is a transformer architecture that dynamically routes tokens across Euclidean, hyperbolic, and spherical attention branches using a learnable gating mechanism, achieving 10% performance improvements on knowledge graph completion tasks.


<details>
  <summary>Details</summary>
Motivation: Standard transformers assume Euclidean geometry, which limits effectiveness on non-Euclidean data. Existing extensions to hyperbolic/spherical spaces require committing to a single geometry, reducing flexibility for mixed-geometry data.

Method: Introduces Curvature-Adaptive Transformer with three geometric attention branches and a lightweight differentiable gating mechanism that learns per-token routing across Euclidean, hyperbolic, and spherical spaces.

Result: Achieves ~10% improvements in MRR and Hits@10 on FB15k-237 and WN18RR benchmarks over fixed-geometry baselines, with only 5% parameter increase and comparable inference time.

Conclusion: Learned geometric adaptation outperforms any single fixed geometry for complex relational reasoning, establishing CAT as a scalable and interpretable foundation for mixture-of-geometry architectures.

Abstract: Transformers achieve strong performance across diverse domains but implicitly
assume Euclidean geometry in their attention mechanisms, limiting their
effectiveness on data with non-Euclidean structure. While recent extensions to
hyperbolic and spherical spaces show promise for hierarchical and cyclical
patterns, respectively, they require committing to a single geometry a priori,
reducing flexibility when data exhibits mixed geometric properties. We
introduce the Curvature-Adaptive Transformer (CAT), a novel architecture that
dynamically learns per-token routing across three geometric attention branches
through a lightweight, differentiable gating mechanism. Unlike fixed-geometry
approaches, CAT enables adaptive geometric specialization, routing tokens to
the appropriate curvature based on their local relational structure. The
routing network provides interpretable curvature preferences while each branch
employs geometry-specific operations optimized for its respective manifold. On
knowledge graph completion benchmarks (FB15k-237, WN18RR), CAT achieves
approximately 10% improvements in MRR and Hits@10 over fixed-geometry baselines
with minimal overhead (5% parameter increase, comparable inference time). These
results demonstrate that learned geometric adaptation outperforms any single
fixed geometry for complex relational reasoning, establishing CAT as a scalable
and interpretable foundation for mixture-of-geometry architectures across
language, vision, and multimodal domains.

</details>


### [210] [Detecting Post-generation Edits to Watermarked LLM Outputs via Combinatorial Watermarking](https://arxiv.org/abs/2510.01637)
*Liyan Xie,Muhammad Siddeek,Mohamed Seif,Andrea J. Goldsmith,Mengdi Wang*

Main category: cs.LG

TL;DR: A combinatorial pattern-based watermarking framework for detecting and localizing post-generation edits in LLM outputs.


<details>
  <summary>Details</summary>
Motivation: Real-world LLM-generated content often undergoes post-generation edits (human revisions or spoofing attacks), making it critical to detect and localize such modifications in watermarked text.

Method: Partitions vocabulary into disjoint subsets and embeds watermark by enforcing deterministic combinatorial patterns over these subsets during generation, with global statistics for detection and lightweight local statistics for edit localization.

Result: Strong empirical performance in edit localization across various editing scenarios on open-source LLMs, evaluated using Type-I error rate and detection accuracy metrics.

Conclusion: The proposed combinatorial watermarking framework effectively addresses the challenge of detecting and localizing post-generation edits in LLM outputs.

Abstract: Watermarking has become a key technique for proprietary language models,
enabling the distinction between AI-generated and human-written text. However,
in many real-world scenarios, LLM-generated content may undergo post-generation
edits, such as human revisions or even spoofing attacks, making it critical to
detect and localize such modifications. In this work, we introduce a new task:
detecting post-generation edits locally made to watermarked LLM outputs. To
this end, we propose a combinatorial pattern-based watermarking framework,
which partitions the vocabulary into disjoint subsets and embeds the watermark
by enforcing a deterministic combinatorial pattern over these subsets during
generation. We accompany the combinatorial watermark with a global statistic
that can be used to detect the watermark. Furthermore, we design lightweight
local statistics to flag and localize potential edits. We introduce two
task-specific evaluation metrics, Type-I error rate and detection accuracy, and
evaluate our method on open-source LLMs across a variety of editing scenarios,
demonstrating strong empirical performance in edit localization.

</details>


### [211] [Support Basis: Fast Attention Beyond Bounded Entries](https://arxiv.org/abs/2510.01643)
*Maryam Aliakbarpour,Vladimir Braverman,Junze Yin,Haochen Zhang*

Main category: cs.LG

TL;DR: The paper introduces support-basis decomposition, a new framework for efficient attention approximation that overcomes the limitations of previous methods by handling unbounded entries and achieving sub-quadratic runtime.


<details>
  <summary>Details</summary>
Motivation: Current sub-quadratic attention approximation methods like Alman and Song's work only under restrictive bounded-entry assumptions, which rarely hold in practice, limiting their applicability to modern large language models.

Method: The approach uses support-basis decomposition that leverages the sub-Gaussian behavior of query and key matrices to split large and small entries, enabling exact computation on sparse components and polynomial approximation on dense components.

Result: The method achieves sub-quadratic runtime with rigorous theoretical guarantees and extends to a multi-threshold setting that eliminates all distributional assumptions. It also provides theoretical justification for polynomial attention methods.

Conclusion: Support-basis decomposition provides an effective framework for efficient attention approximation that works with unbounded entries and offers theoretical foundations for existing empirical methods.

Abstract: The quadratic complexity of softmax attention remains a central bottleneck in
scaling large language models (LLMs). [Alman and Song, NeurIPS 2023] proposed a
sub-quadratic attention approximation algorithm, but it works only under the
restrictive bounded-entry assumption. Since this assumption rarely holds in
practice, its applicability to modern LLMs is limited.
  In this paper, we introduce support-basis decomposition, a new framework for
efficient attention approximation beyond bounded entries. We empirically
demonstrate that the entries of the query and key matrices exhibit sub-Gaussian
behavior. Our approach uses this property to split large and small entries,
enabling exact computation on sparse components and polynomial approximation on
dense components. We establish rigorous theoretical guarantees, proving a
sub-quadratic runtime, and extend the method to a multi-threshold setting that
eliminates all distributional assumptions. Furthermore, we provide the first
theoretical justification for the empirical success of polynomial attention
[Kacham, Mirrokni, and Zhong, ICML 2024], showing that softmax attention can be
closely approximated by a combination of multiple polynomial attentions with
sketching.

</details>


### [212] [Source-Free Cross-Domain Continual Learning](https://arxiv.org/abs/2510.01649)
*Muhammad Tanzil Furqon,Mahardhika Pratama,Igor ≈†krjanc,Lin Liu,Habibullah Habibullah,Kutluyil Dogancay*

Main category: cs.LG

TL;DR: REFEREE is a source-free cross-domain continual learning method that uses frequency-aware prompting and uncertainty-aware weighting to handle domain shifts and noisy pseudo labels without accessing source domain samples.


<details>
  <summary>Details</summary>
Motivation: Existing cross-domain continual learning methods require fully labeled source domains, which is impractical in privacy-constrained environments. This paper addresses the challenge of source-free cross-domain continual learning where source-domain samples are completely prohibited.

Method: REFEREE combines a source-pre-trained model with a vision-language model. It uses frequency-aware prompting to handle domain shifts, uncertainty-aware weighting to mitigate noisy pseudo labels, and kernel linear discriminant analysis (KLDA) to prevent catastrophic forgetting while keeping the backbone frozen.

Result: The approach significantly outperforms prior methods that have access to source domain samples, demonstrating its effectiveness in source-free cross-domain continual learning scenarios.

Conclusion: REFEREE successfully addresses source-free cross-domain continual learning by leveraging frequency-aware prompting, uncertainty-aware weighting, and KLDA, achieving superior performance without requiring access to source domain data.

Abstract: Although existing cross-domain continual learning approaches successfully
address many streaming tasks having domain shifts, they call for a fully
labeled source domain hindering their feasibility in the privacy constrained
environments. This paper goes one step ahead with the problem of source-free
cross-domain continual learning where the use of source-domain samples are
completely prohibited. We propose the idea of rehearsal-free frequency-aware
dynamic prompt collaborations (REFEREE) to cope with the absence of labeled
source-domain samples in realm of cross-domain continual learning. REFEREE is
built upon a synergy between a source-pre-trained model and a large-scale
vision-language model, thus overcoming the problem of sub-optimal
generalizations when relying only on a source pre-trained model. The domain
shift problem between the source domain and the target domain is handled by a
frequency-aware prompting technique encouraging low-frequency components while
suppressing high-frequency components. This strategy generates frequency-aware
augmented samples, robust against noisy pseudo labels. The noisy pseudo-label
problem is further addressed with the uncertainty-aware weighting strategy
where the mean and covariance matrix are weighted by prediction uncertainties,
thus mitigating the adverse effects of the noisy pseudo label. Besides, the
issue of catastrophic forgetting (CF) is overcome by kernel linear discriminant
analysis (KLDA) where the backbone network is frozen while the classification
is performed using the linear discriminant analysis approach guided by the
random kernel method. Our rigorous numerical studies confirm the advantage of
our approach where it beats prior arts having access to source domain samples
with significant margins.

</details>


### [213] [The Unseen Frontier: Pushing the Limits of LLM Sparsity with Surrogate-Free ADMM](https://arxiv.org/abs/2510.01650)
*Kwanhee Lee,Hyeondo Jang,Dongyeop Lee,Dan Alistarh,Namhoon Lee*

Main category: cs.LG

TL;DR: Elsa is a new neural network pruning method that achieves extreme sparsity (up to 90%) in large language models while maintaining high accuracy, breaking through previous limitations of conventional methods.


<details>
  <summary>Details</summary>
Motivation: Current neural network pruning methods for LLMs are limited to moderate sparsity levels (50-60%) due to severe accuracy degradation at higher sparsity, creating an impasse in model compression research.

Method: Elsa uses constrained optimization techniques based on ADMM to directly address limitations in current surrogate objective formulations, with a quantized variant (Elsa-L) for extremely large models.

Result: Elsa achieves 7.8√ó less perplexity than the best existing method on LLaMA-2-7B at 90% sparsity, and scales to 27B models while maintaining theoretical convergence guarantees.

Conclusion: The method represents meaningful progress in LLM sparsity and suggests significant opportunities remain in underexplored directions for further advancement.

Abstract: Neural network pruning is a promising technique to mitigate the excessive
computational and memory requirements of large language models (LLMs). Despite
its promise, however, progress in this area has diminished, as conventional
methods are seemingly unable to surpass moderate sparsity levels (50-60%)
without severely degrading model accuracy. This work breaks through the current
impasse, presenting a principled and effective method called $\texttt{Elsa}$,
which achieves extreme sparsity levels of up to 90% while retaining high model
fidelity. This is done by identifying several limitations in current practice,
all of which can be traced back to their reliance on a surrogate objective
formulation. $\texttt{Elsa}$ tackles this issue directly and effectively via
standard and well-established constrained optimization techniques based on
ADMM. Our extensive experiments across a wide range of models and scales show
that $\texttt{Elsa}$ achieves substantial improvements over existing methods;
e.g., it achieves 7.8$\times$ less perplexity than the best existing method on
LLaMA-2-7B at 90% sparsity. Furthermore, we present
$\texttt{Elsa}_{\text{-L}}$, a quantized variant that scales to extremely large
models (27B), and establish its theoretical convergence guarantees. These
results highlight meaningful progress in advancing the frontier of LLM
sparsity, while promising that significant opportunities for further
advancement may remain in directions that have so far attracted limited
exploration.

</details>


### [214] [Asymmetric Proximal Policy Optimization: mini-critics boost LLM reasoning](https://arxiv.org/abs/2510.01656)
*Jiashun Liu,Johan Obando-Ceron,Han Lu,Yancheng He,Weixun Wang,Wenbo Su,Bo Zheng,Pablo Samuel Castro,Aaron Courville,Ling Pan*

Main category: cs.LG

TL;DR: AsyPPO introduces lightweight mini-critics trained on disjoint prompt shards to restore critics in RL for LLMs, improving learning stability and performance over baselines like GRPO and PPO.


<details>
  <summary>Details</summary>
Motivation: Conventional value functions are computationally expensive at LLM scale and fail under sparse rewards and long reasoning horizons, leading recent RL4LLM methods to avoid explicit critics.

Method: Asymmetric Proximal Policy Optimization (AsyPPO) uses a set of lightweight mini-critics trained on disjoint prompt shards, leveraging inter-critic uncertainty to mask advantages and filter high-divergence states.

Result: AsyPPO improves learning stability and performance across multiple benchmarks, achieving gains of over 6% on Qwen3-4b-Base and about 3% on Qwen3-8b-Base and Qwen3-14b-Base over classic PPO with only 5,000 training samples.

Conclusion: Architectural innovations like AsyPPO are crucial for scalable, efficient RL algorithms in LLM settings, demonstrating the importance of restoring critics while maintaining efficiency.

Abstract: Most recent RL for LLMs (RL4LLM) methods avoid explicit critics, replacing
them with average advantage baselines. This shift is largely pragmatic:
conventional value functions are computationally expensive to train at LLM
scale and often fail under sparse rewards and long reasoning horizons. We
revisit this bottleneck from an architectural perspective and introduce
Asymmetric Proximal Policy Optimization (AsyPPO), a simple and scalable
framework that restores the critics role while remaining efficient in
large-model settings. AsyPPO employs a set of lightweight mini-critics, each
trained on disjoint prompt shards. This design encourages diversity while
preserving calibration, reducing value-estimation bias. Beyond robust
estimation, AsyPPO leverages inter-critic uncertainty to refine the policy
update: (i) masking advantages in states where critics agree and gradients add
little learning signal, and (ii) filtering high-divergence states from entropy
regularization, suppressing spurious exploration. After training on open-source
data with only 5,000 samples, AsyPPO consistently improves learning stability
and performance across multiple benchmarks over strong baselines, such as GRPO,
achieving performance gains of more than six percent on Qwen3-4b-Base and about
three percent on Qwen3-8b-Base and Qwen3-14b-Base over classic PPO, without
additional tricks. These results highlight the importance of architectural
innovations for scalable, efficient algorithms.

</details>


### [215] [Learning Time-Series Representations by Hierarchical Uniformity-Tolerance Latent Balancing](https://arxiv.org/abs/2510.01658)
*Amin Jalali,Milad Soltany,Michael Greenspan,Ali Etemad*

Main category: cs.LG

TL;DR: TimeHUT learns time-series representations through hierarchical uniformity-tolerance balancing using contrastive learning with temperature scheduling and angular margin losses.


<details>
  <summary>Details</summary>
Motivation: To create strong time-series representations by effectively balancing uniformity and tolerance in embedding space while capturing both instance-wise and temporal information.

Method: Uses hierarchical contrastive learning with temperature scheduler and angular margin loss to enforce geometric margins between positive/negative pairs, improving temporal dependency capture.

Result: Outperforms prior methods on 128 UCR and 30 UAE datasets for classification, achieves competitive results on Yahoo and KPI datasets for anomaly detection.

Conclusion: TimeHUT effectively balances uniformity and tolerance in time-series representations, demonstrating superior classification performance and competitive anomaly detection capabilities.

Abstract: We propose TimeHUT, a novel method for learning time-series representations
by hierarchical uniformity-tolerance balancing of contrastive representations.
Our method uses two distinct losses to learn strong representations with the
aim of striking an effective balance between uniformity and tolerance in the
embedding space. First, TimeHUT uses a hierarchical setup to learn both
instance-wise and temporal information from input time-series. Next, we
integrate a temperature scheduler within the vanilla contrastive loss to
balance the uniformity and tolerance characteristics of the embeddings.
Additionally, a hierarchical angular margin loss enforces instance-wise and
temporal contrast losses, creating geometric margins between positive and
negative pairs of temporal sequences. This approach improves the coherence of
positive pairs and their separation from the negatives, enhancing the capture
of temporal dependencies within a time-series sample. We evaluate our approach
on a wide range of tasks, namely 128 UCR and 30 UAE datasets for univariate and
multivariate classification, as well as Yahoo and KPI datasets for anomaly
detection. The results demonstrate that TimeHUT outperforms prior methods by
considerable margins on classification, while obtaining competitive results for
anomaly detection. Finally, detailed sensitivity and ablation studies are
performed to evaluate different components and hyperparameters of our method.

</details>


### [216] [Shift-Invariant Attribute Scoring for Kolmogorov-Arnold Networks via Shapley Value](https://arxiv.org/abs/2510.01663)
*Wangxuan Fan,Ching Wang,Siqi Li,Nan Liu*

Main category: cs.LG

TL;DR: ShapKAN is a pruning framework for Kolmogorov-Arnold Networks (KANs) that uses Shapley values to assess node importance in a shift-invariant manner, addressing the limitations of traditional magnitude-based pruning methods.


<details>
  <summary>Details</summary>
Motivation: KANs offer interpretability advantages over traditional neural networks but face unique pruning challenges due to sensitivity to input coordinate shifts, making conventional magnitude-based pruning unreliable.

Method: The proposed ShapKAN framework uses Shapley value attribution to quantify each node's actual contribution to the network output, ensuring consistent importance rankings regardless of input parameterization.

Result: Extensive experiments on synthetic and real-world datasets show that ShapKAN preserves true node importance while enabling effective network compression.

Conclusion: ShapKAN improves KAN's interpretability advantages and facilitates deployment in resource-constrained environments by providing a reliable pruning method.

Abstract: For many real-world applications, understanding feature-outcome relationships
is as crucial as achieving high predictive accuracy. While traditional neural
networks excel at prediction, their black-box nature obscures underlying
functional relationships. Kolmogorov--Arnold Networks (KANs) address this by
employing learnable spline-based activation functions on edges, enabling
recovery of symbolic representations while maintaining competitive performance.
However, KAN's architecture presents unique challenges for network pruning.
Conventional magnitude-based methods become unreliable due to sensitivity to
input coordinate shifts. We propose \textbf{ShapKAN}, a pruning framework using
Shapley value attribution to assess node importance in a shift-invariant
manner. Unlike magnitude-based approaches, ShapKAN quantifies each node's
actual contribution, ensuring consistent importance rankings regardless of
input parameterization. Extensive experiments on synthetic and real-world
datasets demonstrate that ShapKAN preserves true node importance while enabling
effective network compression. Our approach improves KAN's interpretability
advantages, facilitating deployment in resource-constrained environments.

</details>


### [217] [Beyond Simple Fusion: Adaptive Gated Fusion for Robust Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.01677)
*Han Wu,Yanming Sun,Yunhe Yang,Derek F. Wong*

Main category: cs.LG

TL;DR: The paper introduces AGFN, an Adaptive Gated Fusion Network for multimodal sentiment analysis that adaptively weights modalities using entropy and importance measures to handle noisy or conflicting inputs.


<details>
  <summary>Details</summary>
Motivation: Simple fusion techniques in multimodal sentiment analysis fail to account for variations in modality quality (noisy, missing, conflicting), leading to suboptimal performance in discerning subtle emotional nuances.

Method: Proposes a dual gate fusion mechanism based on information entropy and modality importance that adaptively adjusts feature weights, mitigating noisy modalities and prioritizing informative cues after unimodal encoding and cross-modal interaction.

Result: AGFN significantly outperforms strong baselines on CMU-MOSI and CMU-MOSEI datasets in accuracy, effectively discerning subtle emotions with robust performance. Visualization shows enhanced generalization through broader feature distribution learning.

Conclusion: AGFN creates more robust multimodal feature representations by reducing correlation between feature location and prediction error, decreasing reliance on specific locations and improving generalization.

Abstract: Multimodal sentiment analysis (MSA) leverages information fusion from diverse
modalities (e.g., text, audio, visual) to enhance sentiment prediction.
However, simple fusion techniques often fail to account for variations in
modality quality, such as those that are noisy, missing, or semantically
conflicting. This oversight leads to suboptimal performance, especially in
discerning subtle emotional nuances. To mitigate this limitation, we introduce
a simple yet efficient \textbf{A}daptive \textbf{G}ated \textbf{F}usion
\textbf{N}etwork that adaptively adjusts feature weights via a dual gate fusion
mechanism based on information entropy and modality importance. This mechanism
mitigates the influence of noisy modalities and prioritizes informative cues
following unimodal encoding and cross-modal interaction. Experiments on
CMU-MOSI and CMU-MOSEI show that AGFN significantly outperforms strong
baselines in accuracy, effectively discerning subtle emotions with robust
performance. Visualization analysis of feature representations demonstrates
that AGFN enhances generalization by learning from a broader feature
distribution, achieved by reducing the correlation between feature location and
prediction error, thereby decreasing reliance on specific locations and
creating more robust multimodal feature representations.

</details>


### [218] [PASTA: A Unified Framework for Offline Assortment Learning](https://arxiv.org/abs/2510.01693)
*Juncheng Dong,Weibin Mo,Zhengling Qi,Cong Shi,Ethan X. Fang,Vahid Tarokh*

Main category: cs.LG

TL;DR: PASTA is a pessimistic assortment optimization framework for data-driven settings that achieves optimal revenue under general choice models without requiring full data coverage of all assortments.


<details>
  <summary>Details</summary>
Motivation: Firms lack prior knowledge of choice models and need to optimize assortments from historical data, but combinatorial nature causes insufficient data coverage, making effective solutions challenging.

Method: Proposed Pessimistic Assortment Optimization (PASTA) framework that leverages pessimism principle, requiring only that offline data contains an optimal assortment rather than full coverage.

Result: Established first finite-sample regret bounds for offline assortment optimization across MNL and nested logit models, proved minimax optimality, and showed superior performance over baselines in experiments.

Conclusion: PASTA provides a provably effective solution for offline assortment optimization with minimal data requirements and achieves minimax optimal performance across various choice models.

Abstract: We study a broad class of assortment optimization problems in an offline and
data-driven setting. In such problems, a firm lacks prior knowledge of the
underlying choice model, and aims to determine an optimal assortment based on
historical customer choice data. The combinatorial nature of assortment
optimization often results in insufficient data coverage, posing a significant
challenge in designing provably effective solutions. To address this, we
introduce a novel Pessimistic Assortment Optimization (PASTA) framework that
leverages the principle of pessimism to achieve optimal expected revenue under
general choice models. Notably, PASTA requires only that the offline data
distribution contains an optimal assortment, rather than providing the full
coverage of all feasible assortments. Theoretically, we establish the first
finite-sample regret bounds for offline assortment optimization across several
widely used choice models, including the multinomial logit and nested logit
models. Additionally, we derive a minimax regret lower bound, proving that
PASTA is minimax optimal in terms of sample and model complexity. Numerical
experiments further demonstrate that our method outperforms existing baseline
approaches.

</details>


### [219] [Representational Alignment Across Model Layers and Brain Regions with Hierarchical Optimal Transport](https://arxiv.org/abs/2510.01706)
*Shaan Shah,Meenakshi Khosla*

Main category: cs.LG

TL;DR: HOT is a unified framework that uses hierarchical optimal transport to compare neural networks, providing global alignment scores and handling depth mismatches through soft layer-to-layer couplings.


<details>
  <summary>Details</summary>
Motivation: Standard representational similarity methods have limitations: they produce asymmetric results, lack global alignment scores, and struggle with networks of different depths due to ignoring global activation structure and restricting to rigid one-to-one layer correspondences.

Method: Hierarchical Optimal Transport (HOT) jointly infers soft, globally consistent layer-to-layer couplings and neuron-level transport plans. It allows source neurons to distribute mass across multiple target layers while minimizing total transport cost under marginal constraints.

Result: HOT matches or surpasses standard pairwise matching in alignment quality across vision models, large language models, and human visual cortex recordings. It reveals smooth, fine-grained hierarchical correspondences and naturally handles depth mismatches through mass distribution.

Conclusion: HOT enables richer, more interpretable comparisons between representations, particularly when networks differ in architecture or depth, by providing structured patterns that emerge naturally from global optimization.

Abstract: Standard representational similarity methods align each layer of a network to
its best match in another independently, producing asymmetric results, lacking
a global alignment score, and struggling with networks of different depths.
These limitations arise from ignoring global activation structure and
restricting mappings to rigid one-to-one layer correspondences. We propose
Hierarchical Optimal Transport (HOT), a unified framework that jointly infers
soft, globally consistent layer-to-layer couplings and neuron-level transport
plans. HOT allows source neurons to distribute mass across multiple target
layers while minimizing total transport cost under marginal constraints. This
yields both a single alignment score for the entire network comparison and a
soft transport plan that naturally handles depth mismatches through mass
distribution. We evaluate HOT on vision models, large language models, and
human visual cortex recordings. Across all domains, HOT matches or surpasses
standard pairwise matching in alignment quality. Moreover, it reveals smooth,
fine-grained hierarchical correspondences: early layers map to early layers,
deeper layers maintain relative positions, and depth mismatches are resolved by
distributing representations across multiple layers. These structured patterns
emerge naturally from global optimization without being imposed, yet are absent
in greedy layer-wise methods. HOT thus enables richer, more interpretable
comparisons between representations, particularly when networks differ in
architecture or depth.

</details>


### [220] [ActiNet: Activity intensity classification of wrist-worn accelerometers using self-supervised deep learning](https://arxiv.org/abs/2510.01712)
*Aidan Acquah,Shing Chan,Aiden Doherty*

Main category: cs.LG

TL;DR: ActiNet (self-supervised ResNet-V2 + HMM) outperforms baseline RF+HMM for activity intensity classification from wrist-accelerometer data, achieving F1=0.82 vs 0.77.


<details>
  <summary>Details</summary>
Motivation: Need reliable HAR models for large-scale epidemiological studies linking physical activity to health outcomes using passively collected wrist-accelerometer data.

Method: Trained ActiNet (18-layer modified ResNet-V2) with HMM smoothing on 151 CAPTURE-24 participants' data, evaluated using 5-fold stratified group cross-validation and compared to baseline RF+HMM.

Result: ActiNet achieved mean macro F1=0.82 and Cohen's kappa=0.86, outperforming RF+HMM (F1=0.77, kappa=0.81). Consistent performance across age and sex subgroups.

Conclusion: ActiNet is recommended for extracting activity intensity labels from wrist-accelerometer data in future epidemiological studies.

Abstract: The use of reliable and accurate human activity recognition (HAR) models on
passively collected wrist-accelerometer data is essential in large-scale
epidemiological studies that investigate the association between physical
activity and health outcomes. While the use of self-supervised learning has
generated considerable excitement in improving HAR, it remains unknown the
extent to which these models, coupled with hidden Markov models (HMMs), would
make a tangible improvement to classification performance, and the effect this
may have on the predicted daily activity intensity compositions. Using 151
CAPTURE-24 participants' data, we trained the ActiNet model, a self-supervised,
18-layer, modified ResNet-V2 model, followed by hidden Markov model (HMM)
smoothing to classify labels of activity intensity. The performance of this
model, evaluated using 5-fold stratified group cross-validation, was then
compared to a baseline random forest (RF) + HMM, established in existing
literature. Differences in performance and classification outputs were compared
with different subgroups of age and sex within the Capture-24 population. The
ActiNet model was able to distinguish labels of activity intensity with a mean
macro F1 score of 0.82, and mean Cohen's kappa score of 0.86. This exceeded the
performance of the RF + HMM, trained and validated on the same dataset, with
mean scores of 0.77 and 0.81, respectively. These findings were consistent
across subgroups of age and sex. These findings encourage the use of ActiNet
for the extraction of activity intensity labels from wrist-accelerometer data
in future epidemiological studies.

</details>


### [221] [Latency-aware Multimodal Federated Learning over UAV Networks](https://arxiv.org/abs/2510.01717)
*Shaba Shaon,Dinh C. Nguyen*

Main category: cs.LG

TL;DR: This paper proposes a UAV-assisted federated multimodal learning framework that optimizes system latency through joint optimization of sensing scheduling, power control, trajectory planning, and resource allocation, with theoretical convergence analysis.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of unimodal systems and enhance model accuracy and generalization in federated learning by utilizing multimodal sensing through UAV networks while minimizing system latency.

Method: Proposes an efficient iterative optimization algorithm combining block coordinate descent and successive convex approximation techniques to solve the complex latency minimization problem, with joint optimization of UAV sensing scheduling, power control, trajectory planning, resource allocation, and BS resource management.

Result: Numerical experiments show the proposed FML framework outperforms existing approaches in both system latency and model training performance across different data settings.

Conclusion: The UAV-assisted FML framework successfully minimizes system latency while providing theoretical convergence guarantees and improved model performance through multimodal sensing and comprehensive optimization strategies.

Abstract: This paper investigates federated multimodal learning (FML) assisted by
unmanned aerial vehicles (UAVs) with a focus on minimizing system latency and
providing convergence analysis. In this framework, UAVs are distributed
throughout the network to collect data, participate in model training, and
collaborate with a base station (BS) to build a global model. By utilizing
multimodal sensing, the UAVs overcome the limitations of unimodal systems,
enhancing model accuracy, generalization, and offering a more comprehensive
understanding of the environment. The primary objective is to optimize FML
system latency in UAV networks by jointly addressing UAV sensing scheduling,
power control, trajectory planning, resource allocation, and BS resource
management. To address the computational complexity of our latency minimization
problem, we propose an efficient iterative optimization algorithm combining
block coordinate descent and successive convex approximation techniques, which
provides high-quality approximate solutions. We also present a theoretical
convergence analysis for the UAV-assisted FML framework under a non-convex loss
function. Numerical experiments demonstrate that our FML framework outperforms
existing approaches in terms of system latency and model training performance
under different data settings.

</details>


### [222] [Accelerating Attention with Basis Decomposition](https://arxiv.org/abs/2510.01718)
*Jialin Zhao*

Main category: cs.LG

TL;DR: BD Attention (BDA) is the first lossless algorithmic reformulation of attention using Basis Decomposition, providing mathematically guaranteed acceleration without retraining while preserving exact outputs.


<details>
  <summary>Details</summary>
Motivation: Attention is a core operation in LLMs and VLMs, but existing optimizations like FlashAttention are I/O-aware system optimizations rather than fundamental algorithmic improvements.

Method: BDA uses a simple matrix identity from Basis Decomposition to restructure multi-head projections into a compact form while maintaining exact outputs, requiring only 4s of offline preparation.

Result: On DeepSeek-V2-Lite (16B, FP16), BDA achieves 32% faster key/value projections, 25% smaller weights, with negligible impact on perplexity (0.02% increase in FP16, 0.0004% in FP32).

Conclusion: BDA is the first theoretically exact method for lossless attention acceleration that complements existing engineering optimizations, providing architecture-agnostic mathematical acceleration.

Abstract: Attention is a core operation in large language models (LLMs) and
vision-language models (VLMs). We present BD Attention (BDA), the first
lossless algorithmic reformulation of attention. BDA is enabled by a simple
matrix identity from Basis Decomposition (BD), which restructures multi-head
projections into a compact form while preserving exact outputs. Unlike
I/O-aware system optimizations such as FlashAttention, BDA provides a
mathematically guaranteed acceleration that is architecture-agnostic. On
DeepSeek-V2-Lite (16B, FP16), BDA requires only 4s of offline preparation with
no retraining required and, on modern GPUs, achieves 32% faster key/value
projections and 25% smaller weights, while increasing end-to-end perplexity
(PPL) by just 0.02% (FP16) or 0.0004% (FP32), a negligible effect on model
performance. These results position BDA as the first theoretically exact method
for lossless attention acceleration that is complementary to existing
engineering-level optimizations. Our code is available at
https://github.com/abcbdf/basis-decomposition-official.

</details>


### [223] [Finite-Time Bounds for Distributionally Robust TD Learning with Linear Function Approximation](https://arxiv.org/abs/2510.01721)
*Saptarshi Mandal,Yashaswini Murthy,R. Srikant*

Main category: cs.LG

TL;DR: First robust TD learning with linear function approximation for distributionally robust RL, achieving O(1/Œµ¬≤) sample complexity without requiring generative MDP access.


<details>
  <summary>Details</summary>
Motivation: Existing robust TD learning convergence guarantees are limited to tabular MDPs or require restrictive discount-factor assumptions with function approximation, creating a gap between empirical success and theoretical guarantees.

Method: Combines two-time-scale stochastic-approximation update with outer-loop target-network update, using total-variation and Wasserstein-l distance uncertainty sets for robustness.

Result: Achieves √ï(1/Œµ¬≤) sample complexity to obtain Œµ-accurate value estimate, making it model-free and not requiring generative access to the MDP.

Conclusion: Closes key gap between empirical success of robust RL algorithms and non-asymptotic guarantees of non-robust counterparts, with approach extending to robust Q-learning with function approximation.

Abstract: Distributionally robust reinforcement learning (DRRL) focuses on designing
policies that achieve good performance under model uncertainties. In
particular, we are interested in maximizing the worst-case long-term discounted
reward, where the data for RL comes from a nominal model while the deployed
environment can deviate from the nominal model within a prescribed uncertainty
set. Existing convergence guarantees for robust temporal-difference (TD)
learning for policy evaluation are limited to tabular MDPs or are dependent on
restrictive discount-factor assumptions when function approximation is used. We
present the first robust TD learning with linear function approximation, where
robustness is measured with respect to the total-variation distance and
Wasserstein-l distance uncertainty set. Additionally, our algorithm is both
model-free and does not require generative access to the MDP. Our algorithm
combines a two-time-scale stochastic-approximation update with an outer-loop
target-network update. We establish an $\tilde{O}(1/\epsilon^2)$ sample
complexity to obtain an $\epsilon$-accurate value estimate. Our results close a
key gap between the empirical success of robust RL algorithms and the
non-asymptotic guarantees enjoyed by their non-robust counterparts. The key
ideas in the paper also extend in a relatively straightforward fashion to
robust Q-learning with function approximation.

</details>


### [224] [Workplace Location Choice Model based on Deep Neural Network](https://arxiv.org/abs/2510.01723)
*Tanay Rastogi,Anders Karlstr√∂m*

Main category: cs.LG

TL;DR: This paper compares deep neural networks (DNNs) with traditional discrete choice models (DCMs) for workplace location choice modeling, finding DNNs show significant potential as a robust alternative but each model has specific strengths.


<details>
  <summary>Details</summary>
Motivation: Traditional discrete choice models face challenges in accurately mirroring individual decision-making processes for workplace location choices, motivating the exploration of deep neural networks as an alternative approach.

Method: The paper presents a deep neural network (DNN) method for modeling workplace location choices and compares its performance against traditional discrete choice models (DCMs).

Result: DNNs outperform DCMs in certain aspects and show significant potential as an alternative. DCMs better align with data for individual attribute influence on workplace distance and excel at shorter distances, while DNNs perform comparably for longer distances.

Conclusion: The findings underscore the importance of selecting the appropriate model based on specific application requirements in workplace location choice analysis, as both DNNs and DCMs have distinct strengths.

Abstract: Discrete choice models (DCMs) have long been used to analyze workplace
location decisions, but they face challenges in accurately mirroring individual
decision-making processes. This paper presents a deep neural network (DNN)
method for modeling workplace location choices, which aims to better understand
complex decision patterns and provides better results than traditional discrete
choice models (DCMs). The study demonstrates that DNNs show significant
potential as a robust alternative to DCMs in this domain. While both models
effectively replicate the impact of job opportunities on workplace location
choices, the DNN outperforms the DCM in certain aspects. However, the DCM
better aligns with data when assessing the influence of individual attributes
on workplace distance. Notably, DCMs excel at shorter distances, while DNNs
perform comparably to both data and DCMs for longer distances. These findings
underscore the importance of selecting the appropriate model based on specific
application requirements in workplace location choice analysis.

</details>


### [225] [Private and Fair Machine Learning: Revisiting the Disparate Impact of Differentially Private SGD](https://arxiv.org/abs/2510.01744)
*Lea Demelius,Dominik Kowald,Simone Kopeinik,Roman Kern,Andreas Tr√ºgler*

Main category: cs.LG

TL;DR: DPSGD's impact on fairness varies across metrics, and direct hyperparameter optimization on private models improves utility-fairness trade-offs but doesn't reliably mitigate disparate impact, while also increasing privacy leakage.


<details>
  <summary>Details</summary>
Motivation: To analyze whether optimizing hyperparameters directly on differentially private models can achieve fairness levels comparable to non-private models, and to understand the disparate impact of DPSGD across different performance metrics.

Method: 1) Compare disparate impact of DPSGD on different performance metrics, 2) Analyze impact over wide range of hyperparameter settings, 3) Extend analysis to DPSGD-Global-Adapt variant.

Result: Disparate impact on one metric doesn't imply disparate impact on another. Direct hyperparameter optimization on private models doesn't reliably mitigate DPSGD's disparate impact but improves utility-fairness trade-offs compared to reusing non-private hyperparameters.

Conclusion: Hyperparameter tuning on private models improves utility-fairness trade-offs but increases privacy leakage and doesn't reliably solve disparate impact. DPSGD-Global-Adapt is not robust to hyperparameter choices.

Abstract: Differential privacy (DP) is a prominent method for protecting information
about individuals during data analysis. Training neural networks with
differentially private stochastic gradient descent (DPSGD) influences the
model's learning dynamics and, consequently, its output. This can affect the
model's performance and fairness. While the majority of studies on the topic
report a negative impact on fairness, it has recently been suggested that
fairness levels comparable to non-private models can be achieved by optimizing
hyperparameters for performance directly on differentially private models
(rather than re-using hyperparameters from non-private models, as is common
practice). In this work, we analyze the generalizability of this claim by 1)
comparing the disparate impact of DPSGD on different performance metrics, and
2) analyzing it over a wide range of hyperparameter settings. We highlight that
a disparate impact on one metric does not necessarily imply a disparate impact
on another. Most importantly, we show that while optimizing hyperparameters
directly on differentially private models does not mitigate the disparate
impact of DPSGD reliably, it can still lead to improved utility-fairness
trade-offs compared to re-using hyperparameters from non-private models. We
stress, however, that any form of hyperparameter tuning entails additional
privacy leakage, calling for careful considerations of how to balance privacy,
utility and fairness. Finally, we extend our analyses to DPSGD-Global-Adapt, a
variant of DPSGD designed to mitigate the disparate impact on accuracy, and
conclude that this alternative may not be a robust solution with respect to
hyperparameter choice.

</details>


### [226] [Learning Regularization Functionals for Inverse Problems: A Comparative Study](https://arxiv.org/abs/2510.01755)
*Johannes Hertrich,Hok Shing Wong,Alexander Denker,Stanislas Ducotterd,Zhenghan Fang,Markus Haltmeier,≈Ωeljko Kereta,Erich Kobler,Oscar Leong,Mohammad Sadegh Salehi,Carola-Bibiane Sch√∂nlieb,Johannes Schwab,Zakhar Shumaylov,Jeremias Sulam,German Sh√¢ma Wache,Martin Zach,Yasi Zhang,Matthias J. Ehrhardt,Sebastian Neumayer*

Main category: cs.LG

TL;DR: The paper presents a unified framework for comparing learned regularization methods in imaging inverse problems, addressing implementation inconsistencies and providing systematic analysis.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of comparing different learned regularization methods due to their non-modular implementations and varying architectural designs and training strategies.

Method: Collect and unify available code into a common framework to enable systematic comparison of different learned regularization approaches for imaging inverse problems.

Result: The unified framework allows systematic comparison of approaches, highlighting their strengths and limitations, and provides practical guidelines for implementation.

Conclusion: The unified framework offers valuable insights into the future potential of learned regularization methods and facilitates better comparison and understanding of different approaches in the field.

Abstract: In recent years, a variety of learned regularization frameworks for solving
inverse problems in imaging have emerged. These offer flexible modeling
together with mathematical insights. The proposed methods differ in their
architectural design and training strategies, making direct comparison
challenging due to non-modular implementations. We address this gap by
collecting and unifying the available code into a common framework. This
unified view allows us to systematically compare the approaches and highlight
their strengths and limitations, providing valuable insights into their future
potential. We also provide concise descriptions of each method, complemented by
practical guidelines.

</details>


### [227] [Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks](https://arxiv.org/abs/2510.01758)
*Bruno Corcuera,Carlos Eiras-Franco,Brais Cancela*

Main category: cs.LG

TL;DR: Unsupervised Dynamic Feature Selection (DFS) enhances latent representations by removing noisy features in images, improving model performance without labeled data.


<details>
  <summary>Details</summary>
Motivation: Latent representations in vision tasks often contain noisy or irrelevant features that degrade model performance and generalization capabilities.

Method: Proposes unsupervised Dynamic Feature Selection (DFS) that identifies and removes misleading/redundant information for each instance, ensuring only relevant features contribute to the latent space.

Result: Models with unsupervised DFS achieve significant improvements in generalization performance across clustering and image generation tasks with minimal computational cost increase.

Conclusion: Unsupervised DFS effectively enhances latent representations by removing noisy features, improving model robustness and performance across various vision tasks.

Abstract: Latent representations are critical for the performance and robustness of
machine learning models, as they encode the essential features of data in a
compact and informative manner. However, in vision tasks, these representations
are often affected by noisy or irrelevant features, which can degrade the
model's performance and generalization capabilities. This paper presents a
novel approach for enhancing latent representations using unsupervised Dynamic
Feature Selection (DFS). For each instance, the proposed method identifies and
removes misleading or redundant information in images, ensuring that only the
most relevant features contribute to the latent space. By leveraging an
unsupervised framework, our approach avoids reliance on labeled data, making it
broadly applicable across various domains and datasets. Experiments conducted
on image datasets demonstrate that models equipped with unsupervised DFS
achieve significant improvements in generalization performance across various
tasks, including clustering and image generation, while incurring a minimal
increase in the computational cost.

</details>


### [228] [Octax: Accelerated CHIP-8 Arcade Environments for Reinforcement Learning in JAX](https://arxiv.org/abs/2510.01764)
*Waris Radji,Thomas Michel,Hector Piteau*

Main category: cs.LG

TL;DR: Octax is a high-performance JAX-based suite of classic arcade game environments that provides GPU-accelerated alternatives to Atari benchmarks, achieving orders-of-magnitude speedups while maintaining game fidelity.


<details>
  <summary>Details</summary>
Motivation: Current RL research lacks diverse, challenging environments that are both computationally efficient and scalable. Modern video games are CPU-bound and expensive for large-scale experimentation.

Method: Implemented classic arcade game environments in JAX based on CHIP-8 emulation, providing GPU-accelerated execution with modular design for easy extension.

Result: Achieved orders-of-magnitude speedups over traditional CPU emulators while maintaining perfect fidelity to original game mechanics. Demonstrated significant improvements in RL training speed and scalability.

Conclusion: Octax serves as an ideal platform for large-scale RL experimentation, offering GPU-accelerated environments that are easily extensible and suitable for training agents across multiple game genres.

Abstract: Reinforcement learning (RL) research requires diverse, challenging
environments that are both tractable and scalable. While modern video games may
offer rich dynamics, they are computationally expensive and poorly suited for
large-scale experimentation due to their CPU-bound execution. We introduce
Octax, a high-performance suite of classic arcade game environments implemented
in JAX, based on CHIP-8 emulation, a predecessor to Atari, which is widely
adopted as a benchmark in RL research. Octax provides the JAX community with a
long-awaited end-to-end GPU alternative to the Atari benchmark, offering
image-based environments, spanning puzzle, action, and strategy genres, all
executable at massive scale on modern GPUs. Our JAX-based implementation
achieves orders-of-magnitude speedups over traditional CPU emulators while
maintaining perfect fidelity to the original game mechanics. We demonstrate
Octax's capabilities by training RL agents across multiple games, showing
significant improvements in training speed and scalability compared to existing
solutions. The environment's modular design enables researchers to easily
extend the suite with new games or generate novel environments using large
language models, making it an ideal platform for large-scale RL
experimentation.

</details>


### [229] [Neural non-canonical Hamiltonian dynamics for long-time simulations](https://arxiv.org/abs/2510.01788)
*Cl√©mentine Court√®s,Emmanuel Franck,Michael Kraus,Laurent Navoret,L√©opold Tr√©mant*

Main category: cs.LG

TL;DR: Learning non-canonical Hamiltonian dynamics from data while preserving structure in both learned models and numerical schemes, addressing numerical instability issues through two training strategies.


<details>
  <summary>Details</summary>
Motivation: Previous approaches focused separately on structure-preserving models or numerical schemes, but combining both creates gauge dependency issues that cause numerical instability in long-term simulations.

Method: Proposed two training strategies: directly learning the vector field, or learning time-discrete dynamics through the numerical scheme itself.

Result: Methods successfully learn complex physical dynamics like guiding center from gyrokinetic plasma physics, addressing the numerical instability problem.

Conclusion: The proposed training strategies effectively resolve gauge dependency issues in combining structure-preserving models with numerical schemes, enabling stable long-term simulations of non-canonical Hamiltonian dynamics.

Abstract: This work focuses on learning non-canonical Hamiltonian dynamics from data,
where long-term predictions require the preservation of structure both in the
learned model and in numerical schemes. Previous research focused on either
facet, respectively with a potential-based architecture and with degenerate
variational integrators, but new issues arise when combining both. In
experiments, the learnt model is sometimes numerically unstable due to the
gauge dependency of the scheme, rendering long-time simulations impossible. In
this paper, we identify this problem and propose two different training
strategies to address it, either by directly learning the vector field or by
learning a time-discrete dynamics through the scheme. Several numerical test
cases assess the ability of the methods to learn complex physical dynamics,
like the guiding center from gyrokinetic plasma physics.

</details>


### [230] [Sensitivity, Specificity, and Consistency: A Tripartite Evaluation of Privacy Filters for Synthetic Data Generation](https://arxiv.org/abs/2510.01793)
*Adil Koeken,Alexander Ziller,Moritz Knolle,Daniel Rueckert*

Main category: cs.LG

TL;DR: Post-hoc privacy filtering for synthetic medical datasets shows limited effectiveness, failing to reliably detect near-duplicates and providing false security while exposing patient information.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of post-hoc privacy filtering techniques for synthetic medical datasets, as their claims remain largely unverified despite being proposed as solutions for privacy-preserving data generation.

Method: Conducted a rigorous evaluation of a filtering pipeline applied to chest X-ray synthesis, testing specificity, consistency, and sensitivity for detecting near-duplicates from training data.

Result: Current filters exhibit limited specificity and consistency, achieving high sensitivity only for real images but failing to reliably detect generated near-duplicates from training data.

Conclusion: Post-hoc filtering methods have critical limitations and provide false security while exposing patient information; substantial advances in filter design are needed before deployment in sensitive applications.

Abstract: The generation of privacy-preserving synthetic datasets is a promising avenue
for overcoming data scarcity in medical AI research. Post-hoc privacy filtering
techniques, designed to remove samples containing personally identifiable
information, have recently been proposed as a solution. However, their
effectiveness remains largely unverified. This work presents a rigorous
evaluation of a filtering pipeline applied to chest X-ray synthesis. Contrary
to claims from the original publications, our results demonstrate that current
filters exhibit limited specificity and consistency, achieving high sensitivity
only for real images while failing to reliably detect near-duplicates generated
from training data. These results demonstrate a critical limitation of post-hoc
filtering: rather than effectively safeguarding patient privacy, these methods
may provide a false sense of security while leaving unacceptable levels of
patient information exposed. We conclude that substantial advances in filter
design are needed before these methods can be confidently deployed in sensitive
applications.

</details>


### [231] [Rethinking the shape convention of an MLP](https://arxiv.org/abs/2510.01796)
*Meng-Hsi Chen,Yu-Ang Lee,Feng-Ting Liao,Da-shan Shiu*

Main category: cs.LG

TL;DR: Hourglass MLPs invert conventional narrow-wide-narrow design by using wide-narrow-wide blocks where skip connections operate at expanded dimensions while computation flows through narrow bottlenecks, achieving superior performance-parameter trade-offs.


<details>
  <summary>Details</summary>
Motivation: Challenge the conventional narrow-wide-narrow MLP design convention and explore whether skip connections should operate at expanded dimensions rather than input/output dimensions for better performance.

Method: Propose Hourglass MLP blocks with wide-narrow-wide structure, where initial projection lifts inputs to expanded dimensions (can remain fixed random initialization), and residual computation flows through narrow bottlenecks while skip connections operate at higher dimensions.

Result: Hourglass architectures consistently achieve superior Pareto frontiers compared to conventional designs. As parameters increase, optimal configurations favor deeper networks with wider skip connections and narrower bottlenecks - a distinct scaling pattern from conventional MLPs.

Conclusion: Skip connection placement should be reconsidered in modern architectures, with potential applications extending to Transformers and other residual networks. The Hourglass design demonstrates that operating skip connections at expanded dimensions can provide better performance-parameter trade-offs.

Abstract: Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow
design where skip connections operate at the input/output dimensions while
processing occurs in expanded hidden spaces. We challenge this convention by
proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections
operate at expanded dimensions while residual computation flows through narrow
bottlenecks. This inversion leverages higher-dimensional spaces for incremental
refinement while maintaining computational efficiency through parameter-matched
designs. Implementing Hourglass MLPs requires an initial projection to lift
input signals to expanded dimensions. We propose that this projection can
remain fixed at random initialization throughout training, enabling efficient
training and inference implementations. We evaluate both architectures on
generative tasks over popular image datasets, characterizing
performance-parameter Pareto frontiers through systematic architectural search.
Results show that Hourglass architectures consistently achieve superior Pareto
frontiers compared to conventional designs. As parameter budgets increase,
optimal Hourglass configurations favor deeper networks with wider skip
connections and narrower bottlenecks-a scaling pattern distinct from
conventional MLPs. Our findings suggest reconsidering skip connection placement
in modern architectures, with potential applications extending to Transformers
and other residual networks.

</details>


### [232] [Sparse Query Attention (SQA): A Computationally Efficient Attention Mechanism with Query Heads Reduction](https://arxiv.org/abs/2510.01817)
*Adam Filipek*

Main category: cs.LG

TL;DR: Sparse Query Attention (SQA) reduces computational complexity in Transformers by decreasing Query heads instead of Key/Value heads, achieving up to 3x throughput improvements for long sequences with minimal quality impact.


<details>
  <summary>Details</summary>
Motivation: Current attention optimizations like MQA and GQA address memory bandwidth but don't reduce FLOPs, which remains a critical bottleneck for training and long-sequence processing.

Method: SQA reduces the number of Query heads rather than Key/Value heads, directly decreasing attention computation complexity proportional to query head reduction.

Result: Empirical benchmarks on long sequences (32k-200k tokens) show up to 3x throughput improvements in computation-bound scenarios like pre-training and fine-tuning, with minimal quality impact.

Conclusion: SQA provides a complementary optimization path to existing methods, offering significant computational efficiency gains for scalable model development.

Abstract: The Transformer architecture, underpinned by the Multi-Head Attention (MHA)
mechanism, has become the de facto standard for state-of-the-art models in
artificial intelligence. However, the quadratic computational complexity of MHA
with respect to sequence length presents a significant barrier to scaling,
particularly for applications involving long contexts. Prevailing solutions,
such as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have
effectively addressed the memory bandwidth bottleneck that dominates
autoregressive inference latency by sharing Key and Value projections. While
highly successful, these methods do not reduce the fundamental number of
floating-point operations (FLOPs) required for the attention score computation,
which remains a critical bottleneck for training and full-sequence processing.
This paper introduces Sparse Query Attention (SQA), a novel attention
architecture that pursues an alternative and complementary optimization path.
Instead of reducing Key/Value heads, SQA reduces the number of Query heads.
This architectural modification directly decreases the computational complexity
of the attention mechanism by a factor proportional to the reduction in query
heads, thereby lowering the overall FLOPs. This work presents the theoretical
foundation of SQA, its mathematical formulation, and a family of architectural
variants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate
that SQA can achieve significant throughput improvements of up to 3x in
computation-bound scenarios such as model pre-training, fine-tuning, and
encoder-based tasks, with only a minimal impact on model quality in preliminary
smallscale experiments. SQA was discovered serendipitously during the
development of the upcoming Reactive Transformer architecture, suggesting its
potential as a powerful tool for building more efficient and scalable models

</details>


### [233] [Black-Box Combinatorial Optimization with Order-Invariant Reinforcement Learning](https://arxiv.org/abs/2510.01824)
*Olivier Goudet,Quentin Suire,Adrien Go√´ffon,Fr√©d√©ric Saubion,Sylvain Lamprier*

Main category: cs.LG

TL;DR: Order-invariant reinforcement learning for black-box combinatorial optimization using random generation orders during training to improve sample efficiency and avoid catastrophic failures.


<details>
  <summary>Details</summary>
Motivation: Classical estimation-of-distribution algorithms rely on learning explicit variable dependency graphs, which can be costly and inefficient for capturing complex interactions.

Method: Parameterize multivariate autoregressive generative model trained without fixed variable ordering, using random generation orders as information-preserving dropout. Adapt Generalized Reinforcement Policy Optimization (GRPO) for stable policy-gradient updates.

Result: Achieves best performance across wide range of benchmark algorithms and problem instances, consistently avoiding catastrophic failures.

Conclusion: Order-invariant training with random generation orders promotes search-space diversity and focuses on relevant variable dependencies, improving combinatorial optimization performance.

Abstract: We introduce an order-invariant reinforcement learning framework for
black-box combinatorial optimization. Classical estimation-of-distribution
algorithms (EDAs) often rely on learning explicit variable dependency graphs,
which can be costly and fail to capture complex interactions efficiently. In
contrast, we parameterize a multivariate autoregressive generative model
trained without a fixed variable ordering. By sampling random generation orders
during training - a form of information-preserving dropout - the model is
encouraged to be invariant to variable order, promoting search-space diversity
and shaping the model to focus on the most relevant variable dependencies,
improving sample efficiency. We adapt Generalized Reinforcement Policy
Optimization (GRPO) to this setting, providing stable policy-gradient updates
from scale-invariant advantages. Across a wide range of benchmark algorithms
and problem instances of varying sizes, our method frequently achieves the best
performance and consistently avoids catastrophic failures.

</details>


### [234] [$\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models](https://arxiv.org/abs/2510.01982)
*Yujie Zhou,Pengyang Ling,Jiazi Bu,Yibin Wang,Yuhang Zang,Jiaqi Wang,Li Niu,Guangtao Zhai*

Main category: cs.LG

TL;DR: Proposes G¬≤RPO framework for precise reward assessment in RL-based flow models, using singular stochastic sampling and multi-granularity advantage integration to improve preference alignment.


<details>
  <summary>Details</summary>
Motivation: Existing methods for integrating RL into diffusion/flow models suffer from sub-optimal preference alignment due to sparse and narrow reward signals, limiting their effectiveness in exploring high-value samples.

Method: Introduces Granular-GRPO (G¬≤RPO) with: 1) Singular Stochastic Sampling for step-wise exploration with high reward-noise correlation, and 2) Multi-Granularity Advantage Integration to eliminate bias from fixed-granularity denoising.

Result: Experiments show G¬≤RPO significantly outperforms existing flow-based GRPO baselines across various reward models in both in-domain and out-of-domain evaluations.

Conclusion: The proposed framework achieves more precise and comprehensive reward assessments, demonstrating effectiveness and robustness in aligning generative models with human preferences.

Abstract: The integration of online reinforcement learning (RL) into diffusion and flow
models has recently emerged as a promising approach for aligning generative
models with human preferences. Stochastic sampling via Stochastic Differential
Equations (SDE) is employed during the denoising process to generate diverse
denoising directions for RL exploration. While existing methods effectively
explore potential high-value samples, they suffer from sub-optimal preference
alignment due to sparse and narrow reward signals. To address these challenges,
we propose a novel Granular-GRPO ($\text{G}^2$RPO ) framework that achieves
precise and comprehensive reward assessments of sampling directions in
reinforcement learning of flow models. Specifically, a Singular Stochastic
Sampling strategy is introduced to support step-wise stochastic exploration
while enforcing a high correlation between the reward and the injected noise,
thereby facilitating a faithful reward for each SDE perturbation. Concurrently,
to eliminate the bias inherent in fixed-granularity denoising, we introduce a
Multi-Granularity Advantage Integration module that aggregates advantages
computed at multiple diffusion scales, producing a more comprehensive and
robust evaluation of the sampling directions. Experiments conducted on various
reward models, including both in-domain and out-of-domain evaluations,
demonstrate that our $\text{G}^2$RPO significantly outperforms existing
flow-based GRPO baselines,highlighting its effectiveness and robustness.

</details>


### [235] [Pre-Hoc Predictions in AutoML: Leveraging LLMs to Enhance Model Selection and Benchmarking for Tabular datasets](https://arxiv.org/abs/2510.01842)
*Yannis Belkhiter,Seshu Tirupathi,Giulio Zizzo,Sachin Sharma,John D. Kelleher*

Main category: cs.LG

TL;DR: This paper proposes using pre-hoc model selection with traditional models and LLM agents to reduce AutoML search space, achieving computational efficiency while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: AutoML methods rely on exhaustive hyperparameter searches which are computationally expensive. Pre-hoc prediction offers a promising alternative but remains under-explored in literature.

Method: Leverage traditional models and LLM agents using dataset descriptions and statistical information to pre-select models, reducing the AutoML search space. Applied to AWS AutoGluon portfolio dataset with 175 tabular classification datasets.

Result: The approach significantly reduces computational overhead while still selecting the best model for the given dataset.

Conclusion: Pre-hoc model selection offers a viable shift in AutoML workflows, balancing computational efficiency with model performance.

Abstract: The field of AutoML has made remarkable progress in post-hoc model selection,
with libraries capable of automatically identifying the most performing models
for a given dataset. Nevertheless, these methods often rely on exhaustive
hyperparameter searches, where methods automatically train and test different
types of models on the target dataset. Contrastingly, pre-hoc prediction
emerges as a promising alternative, capable of bypassing exhaustive search
through intelligent pre-selection of models. Despite its potential, pre-hoc
prediction remains under-explored in the literature. This paper explores the
intersection of AutoML and pre-hoc model selection by leveraging traditional
models and Large Language Model (LLM) agents to reduce the search space of
AutoML libraries. By relying on dataset descriptions and statistical
information, we reduce the AutoML search space. Our methodology is applied to
the AWS AutoGluon portfolio dataset, a state-of-the-art AutoML benchmark
containing 175 tabular classification datasets available on OpenML. The
proposed approach offers a shift in AutoML workflows, significantly reducing
computational overhead, while still selecting the best model for the given
dataset.

</details>


### [236] [Learning Representations Through Contrastive Neural Model Checking](https://arxiv.org/abs/2510.01853)
*Vladimir Krsmanovic,Matthias Cosler,Mohamed Ghanem,Bernd Finkbeiner*

Main category: cs.LG

TL;DR: CNML introduces contrastive learning for model checking by embedding logical specifications and systems into a shared latent space, outperforming baselines on retrieval tasks and showing transfer learning capabilities.


<details>
  <summary>Details</summary>
Motivation: Representation learning remains underexplored in formal verification despite its success in vision and language domains, creating an opportunity to leverage model checking as a guiding signal for learning aligned representations.

Method: Contrastive Neural Model Checking (CNML) jointly embeds logical specifications and systems into a shared latent space using a self-supervised contrastive objective.

Result: CNML considerably outperforms both algorithmic and neural baselines on industry-inspired retrieval tasks in cross-modal and intra-modal settings, and shows effective transfer to downstream tasks and generalization to more complex formulas.

Conclusion: Model checking can serve as an effective objective for learning representations for formal languages, demonstrating the viability of representation learning in formal verification.

Abstract: Model checking is a key technique for verifying safety-critical systems
against formal specifications, where recent applications of deep learning have
shown promise. However, while ubiquitous for vision and language domains,
representation learning remains underexplored in formal verification. We
introduce Contrastive Neural Model Checking (CNML), a novel method that
leverages the model checking task as a guiding signal for learning aligned
representations. CNML jointly embeds logical specifications and systems into a
shared latent space through a self-supervised contrastive objective. On
industry-inspired retrieval tasks, CNML considerably outperforms both
algorithmic and neural baselines in cross-modal and intra-modal settings.We
further show that the learned representations effectively transfer to
downstream tasks and generalize to more complex formulas. These findings
demonstrate that model checking can serve as an objective for learning
representations for formal languages.

</details>


### [237] [Explicit Discovery of Nonlinear Symmetries from Dynamic Data](https://arxiv.org/abs/2510.01855)
*Lexiang Hu,Yikang Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: LieNLSD is the first method that can discover nonlinear symmetries by determining the number of infinitesimal generators and their explicit expressions, improving neural PDE solver accuracy by over 20%.


<details>
  <summary>Details</summary>
Motivation: Symmetry is crucial for equivariant networks and governing equation discovery, but existing methods are limited to linear symmetries and fail to explicitly obtain Lie algebra subspaces for nonlinear cases.

Method: Uses a function library for infinitesimal group action and solves for its coefficient matrix. Applies prolongation formula for differential equations, substitutes data differences and neural network Jacobian into infinitesimal criterion, then solves linear equations using SVD.

Result: Shows qualitative advantages over existing methods on top quark tagging and dynamic systems. Improves long rollout accuracy of neural PDE solvers by over 20% when used for data augmentation.

Conclusion: LieNLSD successfully discovers nonlinear symmetries and their explicit Lie algebra expressions, providing significant improvements in neural PDE solver performance.

Abstract: Symmetry is widely applied in problems such as the design of equivariant
networks and the discovery of governing equations, but in complex scenarios, it
is not known in advance. Most previous symmetry discovery methods are limited
to linear symmetries, and recent attempts to discover nonlinear symmetries fail
to explicitly get the Lie algebra subspace. In this paper, we propose LieNLSD,
which is, to our knowledge, the first method capable of determining the number
of infinitesimal generators with nonlinear terms and their explicit
expressions. We specify a function library for the infinitesimal group action
and aim to solve for its coefficient matrix, proving that its prolongation
formula for differential equations, which governs dynamic data, is also linear
with respect to the coefficient matrix. By substituting the central differences
of the data and the Jacobian matrix of the trained neural network into the
infinitesimal criterion, we get a system of linear equations for the
coefficient matrix, which can then be solved using SVD. On top quark tagging
and a series of dynamic systems, LieNLSD shows qualitative advantages over
existing methods and improves the long rollout accuracy of neural PDE solvers
by over 20% while applying to guide data augmentation. Code and data are
available at https://github.com/hulx2002/LieNLSD.

</details>


### [238] [Compositional meta-learning through probabilistic task inference](https://arxiv.org/abs/2510.01858)
*Jacob J. W. Bakermans,Pablo Tano,Reidar Riveland,Charles Findling,Alexandre Pouget*

Main category: cs.LG

TL;DR: A compositional meta-learning model that represents tasks as structured combinations of reusable computations, enabling rapid learning of new tasks from minimal examples without parameter updates.


<details>
  <summary>Details</summary>
Motivation: To enable effective knowledge reuse from previous tasks for solving new tasks with minimal experience, addressing the meta-learning problem through compositional solutions.

Method: Learn a generative model that captures underlying components and their statistics shared across tasks, transforming new task learning into probabilistic inference without parameter updates.

Result: Successfully recovers ground truth components in rule learning and motor learning tasks, and demonstrates ability to quickly infer new solutions from single examples.

Conclusion: The framework combines neural network expressivity with probabilistic inference data-efficiency to achieve rapid compositional meta-learning.

Abstract: To solve a new task from minimal experience, it is essential to effectively
reuse knowledge from previous tasks, a problem known as meta-learning.
Compositional solutions, where common elements of computation are flexibly
recombined into new configurations, are particularly well-suited for
meta-learning. Here, we propose a compositional meta-learning model that
explicitly represents tasks as structured combinations of reusable
computations. We achieve this by learning a generative model that captures the
underlying components and their statistics shared across a family of tasks.
This approach transforms learning a new task into a probabilistic inference
problem, which allows for finding solutions without parameter updates through
highly constrained hypothesis testing. Our model successfully recovers ground
truth components and statistics in rule learning and motor learning tasks. We
then demonstrate its ability to quickly infer new solutions from just single
examples. Together, our framework joins the expressivity of neural networks
with the data-efficiency of probabilistic inference to achieve rapid
compositional meta-learning.

</details>


### [239] [Universal Dynamic Regret and Constraint Violation Bounds for Constrained Online Convex Optimization](https://arxiv.org/abs/2510.01867)
*Subhamon Supantha,Abhishek Sinha*

Main category: cs.LG

TL;DR: This paper presents two modular algorithms for Online Convex Optimization with adversarial constraints, achieving improved dynamic regret and constraint violation bounds without requiring common feasible points.


<details>
  <summary>Details</summary>
Motivation: To generalize the Online Convex Optimization framework to handle adversarial constraints where both cost and constraint functions are chosen arbitrarily by an adversary, addressing the limitation that constraint functions need not contain any common feasible point.

Method: Developed two algorithms with simple modular structures that reduce the constrained learning problem to standard OCO with specially constructed surrogate cost functions.

Result: Achieved universal dynamic regret and cumulative constraint violation bounds that improve upon state-of-the-art results in the most general adversarial setting.

Conclusion: The proposed modular approach successfully handles the challenging case of adversarial constraints without requiring common feasibility, providing improved performance guarantees for constrained online learning problems.

Abstract: We consider a generalization of the celebrated Online Convex Optimization
(OCO) framework with online adversarial constraints. We present two algorithms
having simple modular structures that yield universal dynamic regret and
cumulative constraint violation bounds, improving upon the state-of-the-art
results. Our results hold in the most general case when both the cost and
constraint functions are chosen arbitrarily by an adversary, and the constraint
functions need not contain any common feasible point. The results are
established by reducing the constrained learning problem to an instance of the
standard OCO problem with specially constructed surrogate cost functions.

</details>


### [240] [Test-Time Anchoring for Discrete Diffusion Posterior Sampling](https://arxiv.org/abs/2510.02291)
*Litu Rout,Andreas Lugmayr,Yasamin Jafarian,Srivatsan Varadharajan,Constantine Caramanis,Sanjay Shakkottai,Ira Kemelmacher-Shlizerman*

Main category: cs.LG

TL;DR: APS enables efficient posterior sampling using pretrained discrete diffusion models without retraining, overcoming limitations of existing methods through quantized expectation and anchored remasking techniques.


<details>
  <summary>Details</summary>
Motivation: To enable posterior sampling from noisy measurements using pretrained discrete diffusion models without task-specific retraining, addressing challenges in existing discrete diffusion approaches.

Method: Anchored Posterior Sampling (APS) with two key innovations: quantized expectation for gradient-like guidance in discrete embedding space, and anchored remasking for adaptive decoding.

Result: Achieves state-of-the-art performance among discrete diffusion samplers across linear and nonlinear inverse problems on standard benchmarks, with additional benefits in training-free stylization and text-guided editing.

Conclusion: APS provides an effective framework for posterior sampling using discrete diffusion foundation models, offering superior performance and practical applications without requiring retraining.

Abstract: We study the problem of posterior sampling using pretrained discrete
diffusion foundation models, aiming to recover images from noisy measurements
without retraining task-specific models. While diffusion models have achieved
remarkable success in generative modeling, most advances rely on continuous
Gaussian diffusion. In contrast, discrete diffusion offers a unified framework
for jointly modeling categorical data such as text and images. Beyond
unification, discrete diffusion provides faster inference, finer control, and
principled training-free Bayesian inference, making it particularly well-suited
for posterior sampling. However, existing approaches to discrete diffusion
posterior sampling face severe challenges: derivative-free guidance yields
sparse signals, continuous relaxations limit applicability, and split Gibbs
samplers suffer from the curse of dimensionality. To overcome these
limitations, we introduce Anchored Posterior Sampling (APS) for masked
diffusion foundation models, built on two key innovations -- quantized
expectation for gradient-like guidance in discrete embedding space, and
anchored remasking for adaptive decoding. Our approach achieves
state-of-the-art performance among discrete diffusion samplers across linear
and nonlinear inverse problems on the standard benchmarks. We further
demonstrate the benefits of our approach in training-free stylization and
text-guided editing.

</details>


### [241] [Randomized Gradient Subspaces for Efficient Large Language Model Training](https://arxiv.org/abs/2510.01878)
*Sahar Rajabi,Nayeema Nonta,Samanvay Vajpayee,Sirisha Rambhatla*

Main category: cs.LG

TL;DR: The paper analyzes gradient space dynamics in LLM training, finding that while a small subspace captures most gradient energy, significant residual bulk remains and core subspace influence diminishes over time and in deeper layers. The authors introduce GrassWalk and GrassJump algorithms that exploit this subspace structure to achieve state-of-the-art memory savings while improving performance.


<details>
  <summary>Details</summary>
Motivation: Training large language models is bottlenecked by extreme memory demands, particularly from optimizer states. Recent approaches use gradient projection into low-dimensional subspaces, but the dynamics of gradient space and its underlying subspaces need deeper analysis to develop more effective memory-efficient training methods.

Method: The authors analyze gradient space dynamics and introduce GrassWalk and GrassJump - randomized algorithms that exploit subspace structure. These methods account for the near-flat curvature of gradient space and address the diminishing influence of core subspaces over time and in deeper layers.

Result: The proposed algorithms achieve state-of-the-art memory savings while improving performance on LLaMA-1B and LLaMA-7B pretraining, demonstrating better efficiency than existing gradient projection methods.

Conclusion: Understanding gradient space dynamics reveals important insights: significant gradient energy resides in residual bulk, core subspace influence diminishes over time, and gradient space exhibits near-flat curvature. These findings enable the development of more effective memory-efficient training algorithms like GrassWalk and GrassJump.

Abstract: Training large language models (LLMs) is often bottlenecked by extreme memory
demands, with optimizer states dominating the footprint. Recent works mitigates
this cost by projecting gradients into low-dimensional subspaces using
sophisticated update strategies. In this paper, we analyze the dynamics of
gradient space and its underlying subspaces. We find that while a small
subspace captures most gradient energy, a significant portion still resides in
the residual bulk; moreover, the influence of the core subspace diminishes over
time and in deeper layers. We also observe that the gradient space exhibits
near-flat curvature, calling for algorithms that explicitly account for this
geometry. Motivated by these insights, we introduce a suite of randomized
algorithms, GrassWalk and GrassJump, which exploit subspace and achieve
state-of-the-art memory savings while improving performance on LLaMA-1B and
LLaMA-7B pretraining.

</details>


### [242] [Continual Personalization for Diffusion Models](https://arxiv.org/abs/2510.02296)
*Yu-Chien Liao,Jr-Jen Chen,Chi-Pin Huang,Ci-Siang Lin,Meng-Lin Wu,Yu-Chiang Frank Wang*

Main category: cs.LG

TL;DR: CNS is a novel continual learning approach for diffusion models that identifies and selectively fine-tunes concept-related neurons to enable incremental personalization while preventing catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Updating diffusion models incrementally is practical for real-world applications but computationally challenging, requiring methods that can personalize models without losing previous knowledge.

Method: Concept Neuron Selection (CNS) identifies neurons related to target concepts in diffusion models and fine-tunes them incrementally while preserving knowledge from previous concepts through joint training.

Result: CNS achieves state-of-the-art performance on real-world datasets with minimal parameter adjustments, outperforming previous methods in single and multi-concept personalization while enabling fusion-free operation.

Conclusion: CNS provides an effective solution for continual personalization of diffusion models, reducing memory storage and processing time while maintaining zero-shot generation capabilities.

Abstract: Updating diffusion models in an incremental setting would be practical in
real-world applications yet computationally challenging. We present a novel
learning strategy of Concept Neuron Selection (CNS), a simple yet effective
approach to perform personalization in a continual learning scheme. CNS
uniquely identifies neurons in diffusion models that are closely related to the
target concepts. In order to mitigate catastrophic forgetting problems while
preserving zero-shot text-to-image generation ability, CNS finetunes concept
neurons in an incremental manner and jointly preserves knowledge learned of
previous concepts. Evaluation of real-world datasets demonstrates that CNS
achieves state-of-the-art performance with minimal parameter adjustments,
outperforming previous methods in both single and multi-concept personalization
works. CNS also achieves fusion-free operation, reducing memory storage and
processing time for continual personalization.

</details>


### [243] [Multi-marginal temporal Schr√∂dinger Bridge Matching for video generation from unpaired data](https://arxiv.org/abs/2510.01894)
*Thomas Gravier,Thomas Boyer,Auguste Genovesio*

Main category: cs.LG

TL;DR: MMtSBM is a novel method for reconstructing temporal dynamics from static snapshots using multi-marginal Schr√∂dinger bridges, achieving state-of-the-art performance in high-dimensional settings like transcriptomics and image data.


<details>
  <summary>Details</summary>
Motivation: Many natural dynamic processes can only be observed through static snapshots, making it challenging to reconstruct their temporal evolution. Existing approaches have scalability issues in high dimensions and require restrictive assumptions.

Method: Extends Diffusion Schr√∂dinger Bridge Matching by deriving the Iterative Markovian Fitting algorithm for multiple marginals in a novel factorized fashion, enabling video generation from unpaired data.

Result: MMtSBM retains theoretical properties on toy examples, achieves state-of-the-art performance on real datasets including transcriptomic trajectory inference in 100 dimensions, and recovers couplings and dynamics in very high dimensional image settings for the first time.

Conclusion: Multi-marginal Schr√∂dinger bridges provide a practical and principled approach for recovering hidden dynamics from static data, establishing MMtSBM as an effective method for temporal evolution reconstruction.

Abstract: Many natural dynamic processes -- such as in vivo cellular differentiation or
disease progression -- can only be observed through the lens of static sample
snapshots. While challenging, reconstructing their temporal evolution to
decipher underlying dynamic properties is of major interest to scientific
research. Existing approaches enable data transport along a temporal axis but
are poorly scalable in high dimension and require restrictive assumptions to be
met. To address these issues, we propose \textit{\textbf{Multi-Marginal
temporal Schr\"odinger Bridge Matching}} (\textbf{MMtSBM}) \textit{for video
generation from unpaired data}, extending the theoretical guarantees and
empirical efficiency of Diffusion Schr\"odinger Bridge Matching
(arXiv:archive/2303.16852) by deriving the Iterative Markovian Fitting
algorithm to multiple marginals in a novel factorized fashion. Experiments show
that MMtSBM retains theoretical properties on toy examples, achieves
state-of-the-art performance on real world datasets such as transcriptomic
trajectory inference in 100 dimensions, and for the first time recovers
couplings and dynamics in very high dimensional image settings. Our work
establishes multi-marginal Schr\"odinger bridges as a practical and principled
approach for recovering hidden dynamics from static data.

</details>


### [244] [Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models](https://arxiv.org/abs/2510.02300)
*Runqian Wang,Yilun Du*

Main category: cs.LG

TL;DR: Equilibrium Matching (EqM) is a generative modeling framework that learns the equilibrium gradient of an implicit energy landscape, replacing time-conditional dynamics with optimization-based sampling via gradient descent.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of traditional diffusion and flow-based models that use non-equilibrium, time-conditional dynamics, by learning from an equilibrium perspective.

Method: Learns the equilibrium gradient of an implicit energy landscape and uses optimization-based sampling with gradient descent, adjustable step sizes, adaptive optimizers, and adaptive compute at inference.

Result: Achieves state-of-the-art FID of 1.90 on ImageNet 256√ó256, surpassing diffusion/flow models, and handles tasks like denoising, OOD detection, and image composition.

Conclusion: EqM provides a unified framework that bridges flow and energy-based models, enabling optimization-driven inference and flexible task handling beyond generation.

Abstract: We introduce Equilibrium Matching (EqM), a generative modeling framework
built from an equilibrium dynamics perspective. EqM discards the
non-equilibrium, time-conditional dynamics in traditional diffusion and
flow-based generative models and instead learns the equilibrium gradient of an
implicit energy landscape. Through this approach, we can adopt an
optimization-based sampling process at inference time, where samples are
obtained by gradient descent on the learned landscape with adjustable step
sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation
performance of diffusion/flow models empirically, achieving an FID of 1.90 on
ImageNet 256$\times$256. EqM is also theoretically justified to learn and
sample from the data manifold. Beyond generation, EqM is a flexible framework
that naturally handles tasks including partially noised image denoising, OOD
detection, and image composition. By replacing time-conditional velocities with
a unified equilibrium landscape, EqM offers a tighter bridge between flow and
energy-based models and a simple route to optimization-driven inference.

</details>


### [245] [Multimodal Foundation Models for Early Disease Detection](https://arxiv.org/abs/2510.01899)
*Md Talha Mohsin,Ismail Abdulrashid*

Main category: cs.LG

TL;DR: A multimodal foundation model that integrates diverse healthcare data streams (EHR, imaging, genetics, wearables) using attention-based transformers for improved early disease diagnosis through cross-modal correlation analysis.


<details>
  <summary>Details</summary>
Motivation: Traditional diagnostic models analyze healthcare data sources in isolation, limiting their ability to identify cross-modal correlations essential for early disease diagnosis. Healthcare generates diverse data streams that need integrated analysis.

Method: Uses dedicated encoders to project each modality into a shared latent space, then combines them using multi-head attention and residual normalization. The transformer architecture supports pretraining on multiple tasks for easy adaptation to new diseases and datasets.

Result: Experimental evaluation on benchmark datasets in oncology, cardiology, and neurology for early detection tasks. The framework includes data governance and model management tools to improve transparency, reliability, and clinical interpretability.

Conclusion: The proposed multimodal foundation model works toward unified precision diagnostics, potentially improving prediction accuracy and supporting clinical decision-making through better integration of diverse healthcare data sources.

Abstract: Healthcare generates diverse streams of data, including electronic health
records (EHR), medical imaging, genetics, and ongoing monitoring from wearable
devices. Traditional diagnostic models frequently analyze these sources in
isolation, which constrains their capacity to identify cross-modal correlations
essential for early disease diagnosis. Our research presents a multimodal
foundation model that consolidates diverse patient data through an
attention-based transformer framework. At first, dedicated encoders put each
modality into a shared latent space. Then, they combine them using multi-head
attention and residual normalization. The architecture is made for pretraining
on many tasks, which makes it easy to adapt to new diseases and datasets with
little extra work. We provide an experimental strategy that uses benchmark
datasets in oncology, cardiology, and neurology, with the goal of testing early
detection tasks. The framework includes data governance and model management
tools in addition to technological performance to improve transparency,
reliability, and clinical interpretability. The suggested method works toward a
single foundation model for precision diagnostics, which could improve the
accuracy of predictions and help doctors make decisions.

</details>


### [246] [A Methodology for Transparent Logic-Based Classification Using a Multi-Task Convolutional Tsetlin Machine](https://arxiv.org/abs/2510.01906)
*Mayur Kishor Shende,Ole-Christoffer Granmo,Runar Helin,Vladimir I. Zadorozhny,Rishad Shafik*

Main category: cs.LG

TL;DR: The paper explores Tsetlin Machine (TM) for large-scale RGB image classification, proposing methods for local and global interpretability while maintaining competitive performance compared to deep learning models.


<details>
  <summary>Details</summary>
Motivation: To apply the inherently interpretable Tsetlin Machine architecture to complex multi-channel image classification tasks while preserving its interpretability advantages over neural networks.

Method: Proposed methodology to generate local interpretations (explaining model predictions) and global class representations (aggregating important patterns per class) from convolutional clauses, visualized as images.

Result: Achieved 98.5% accuracy on MNIST and 86.56% F1-score on CelebA, comparable to ResNet50's 88.07% F1-score, while maintaining interpretability in complex training environments.

Conclusion: TM performs competitively with deep learning models on complex datasets while providing better interpretability through visualizable clause representations, enabling application to more diverse datasets.

Abstract: The Tsetlin Machine (TM) is a novel machine learning paradigm that employs
finite-state automata for learning and utilizes propositional logic to
represent patterns. Due to its simplistic approach, TMs are inherently more
interpretable than learning algorithms based on Neural Networks. The
Convolutional TM has shown comparable performance on various datasets such as
MNIST, K-MNIST, F-MNIST and CIFAR-2. In this paper, we explore the
applicability of the TM architecture for large-scale multi-channel (RGB) image
classification. We propose a methodology to generate both local interpretations
and global class representations. The local interpretations can be used to
explain the model predictions while the global class representations aggregate
important patterns for each class. These interpretations summarize the
knowledge captured by the convolutional clauses, which can be visualized as
images. We evaluate our methods on MNIST and CelebA datasets, using models that
achieve 98.5\% accuracy on MNIST and 86.56\% F1-score on CelebA (compared to
88.07\% for ResNet50) respectively. We show that the TM performs competitively
to this deep learning model while maintaining its interpretability, even in
large-scale complex training environments. This contributes to a better
understanding of TM clauses and provides insights into how these models can be
applied to more complex and diverse datasets.

</details>


### [247] [Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under Deficiencies with Iterative Refinement](https://arxiv.org/abs/2510.01910)
*Zhaoyan Wang,Zheng Gao,Arogya Kharel,In-Young Ko*

Main category: cs.LG

TL;DR: This paper introduces RoGRAD, a novel framework that addresses graph deficiencies through iterative retrieval-augmented contrastive refinement, challenging the assumption that LLM augmentation is always superior and showing significant performance improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current GNNs struggle with real-world graph deficiencies, and there's a lack of systematic understanding of how graph-native methods and LLM-enhanced approaches perform under compound deficiencies. No comprehensive comparison exists between conventional approaches and recent LLM-on-graph frameworks.

Method: Proposed RoGRAD framework - an iterative paradigm using Retrieval-Augmented Generation (RAG) to inject retrieval-grounded augmentations. It provides class-consistent, diverse augmentations and enforces discriminative representations through iterative graph contrastive learning, transforming LLM augmentation from static signal injection to dynamic refinement.

Result: Extensive experiments show RoGRAD's superiority over both conventional GNN- and LLM-enhanced baselines, achieving up to 82.43% average improvement. The study reveals overlooked vulnerabilities and challenges the assumption that LLM augmentation is consistently superior.

Conclusion: RoGRAD represents a significant advancement in graph learning by introducing the first iterative paradigm that dynamically refines graph representations through retrieval-augmented contrastive learning, demonstrating substantial performance gains over existing methods.

Abstract: Graph Neural Networks (GNNs) are widely adopted in Web-related applications,
serving as a core technique for learning from graph-structured data, such as
text-attributed graphs. Yet in real-world scenarios, such graphs exhibit
deficiencies that substantially undermine GNN performance. While prior
GNN-based augmentation studies have explored robustness against individual
imperfections, a systematic understanding of how graph-native and Large
Language Models (LLMs) enhanced methods behave under compound deficiencies is
still missing. Specifically, there has been no comprehensive investigation
comparing conventional approaches and recent LLM-on-graph frameworks, leaving
their merits unclear. To fill this gap, we conduct the first empirical study
that benchmarks these two lines of methods across diverse graph deficiencies,
revealing overlooked vulnerabilities and challenging the assumption that LLM
augmentation is consistently superior. Building on empirical findings, we
propose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement
(RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is
the first iterative paradigm that leverages Retrieval-Augmented Generation
(RAG) to inject retrieval-grounded augmentations by supplying class-consistent,
diverse augmentations and enforcing discriminative representations through
iterative graph contrastive learning. It transforms LLM augmentation for graphs
from static signal injection into dynamic refinement. Extensive experiments
demonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced
baselines, achieving up to 82.43% average improvement.

</details>


### [248] [StelLA: Subspace Learning in Low-rank Adaptation using Stiefel Manifold](https://arxiv.org/abs/2510.01938)
*Zhizhong Li,Sina Sajadmanesh,Jingtao Li,Lingjuan Lyu*

Main category: cs.LG

TL;DR: A geometry-aware extension of LoRA that uses a three-factor decomposition with Stiefel manifold constraints to improve parameter-efficient fine-tuning performance.


<details>
  <summary>Details</summary>
Motivation: Standard LoRA lags behind full fine-tuning due to insufficient exploitation of the geometric structure underlying low-rank manifolds.

Method: Proposes a three-factor decomposition U¬∑S¬∑V‚ä§ analogous to SVD, constraining U and V to lie on the Stiefel manifold for orthonormality, with geometric optimization that converts Euclidean optimizers to Riemannian ones.

Result: Superior performance across commonsense reasoning, math and code generation, image classification, and image generation tasks compared to recent state-of-the-art LoRA variants.

Conclusion: The geometry-aware approach enables efficient subspace learning while remaining compatible with existing fine-tuning pipelines, demonstrating improved performance over standard LoRA.

Abstract: Low-rank adaptation (LoRA) has been widely adopted as a parameter-efficient
technique for fine-tuning large-scale pre-trained models. However, it still
lags behind full fine-tuning in performance, partly due to its insufficient
exploitation of the geometric structure underlying low-rank manifolds. In this
paper, we propose a geometry-aware extension of LoRA that uses a three-factor
decomposition $U\!SV^\top$. Analogous to the structure of singular value
decomposition (SVD), it separates the adapter's input and output subspaces, $V$
and $U$, from the scaling factor $S$. Our method constrains $U$ and $V$ to lie
on the Stiefel manifold, ensuring their orthonormality throughout the training.
To optimize on the Stiefel manifold, we employ a flexible and modular geometric
optimization design that converts any Euclidean optimizer to a Riemannian one.
It enables efficient subspace learning while remaining compatible with existing
fine-tuning pipelines. Empirical results across a wide range of downstream
tasks, including commonsense reasoning, math and code generation, image
classification, and image generation, demonstrate the superior performance of
our approach against the recent state-of-the-art variants of LoRA. Code is
available at https://github.com/SonyResearch/stella.

</details>


### [249] [Lower Bounds on Adversarial Robustness for Multiclass Classification with General Loss Functions](https://arxiv.org/abs/2510.01969)
*Camilo Andr√©s Garc√≠a Trillos,Nicol√°s Garc√≠a Trillos*

Main category: cs.LG

TL;DR: The paper develops dual and barycentric reformulations for adversarially robust multiclass classification under arbitrary loss functions, extending beyond the 0-1 loss setting to enable efficient computation of sharp lower bounds for adversarial risks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing adversarial robustness methods that primarily focus on 0-1 loss, by developing a framework that works with arbitrary loss functions including cross-entropy, power form losses, and quadratic loss.

Method: Derived dual and barycentric reformulations of the robust risk minimization problem, with explicit characterizations for cross-entropy loss, power form losses, and quadratic loss. The approach connects adversarial robustness to Œ±-fair packing problems and generalized barycenter problems using Kullback-Leibler and Tsallis entropies as penalties.

Result: The reformulations enable efficient computation of sharp lower bounds for adversarial risks and facilitate the design of robust classifiers beyond the 0-1 loss setting. Numerical experiments demonstrate tighter lower bounds for adversarial risks with cross-entropy loss.

Conclusion: The paper establishes important connections between adversarial robustness and optimization problems like Œ±-fair packing and generalized barycenters, providing a comprehensive framework for robust classification under arbitrary loss functions with practical computational benefits.

Abstract: We consider adversarially robust classification in a multiclass setting under
arbitrary loss functions and derive dual and barycentric reformulations of the
corresponding learner-agnostic robust risk minimization problem. We provide
explicit characterizations for important cases such as the cross-entropy loss,
loss functions with a power form, and the quadratic loss, extending in this way
available results for the 0-1 loss. These reformulations enable efficient
computation of sharp lower bounds for adversarial risks and facilitate the
design of robust classifiers beyond the 0-1 loss setting. Our paper uncovers
interesting connections between adversarial robustness, $\alpha$-fair packing
problems, and generalized barycenter problems for arbitrary positive measures
where Kullback-Leibler and Tsallis entropies are used as penalties. Our
theoretical results are accompanied with illustrative numerical experiments
where we obtain tighter lower bounds for adversarial risks with the
cross-entropy loss function.

</details>


### [250] [Moon: A Modality Conversion-based Efficient Multivariate Time Series Anomaly Detection](https://arxiv.org/abs/2510.01970)
*Yuanyuan Yao,Yuhan Shi,Lu Chen,Ziquan Fang,Yunjun Gao,Leong Hou U,Yushuai Li,Tianyi Li*

Main category: cs.LG

TL;DR: Moon is a supervised modality conversion framework for multivariate time series anomaly detection that converts numeric time series to images using MV-MTF, integrates both data types via Multimodal-CNN, and provides interpretable anomaly analysis with SHAP.


<details>
  <summary>Details</summary>
Motivation: Existing MTS anomaly detection methods face challenges: unsupervised methods rely on error thresholds causing inaccuracies, semi-supervised methods underuse anomaly labels, and supervised methods fail to capture local relationships with high computational costs and data scarcity.

Method: Moon converts numeric time series to image representations using multivariate Markov Transition Field (MV-MTF), integrates numeric and image data through Multimodal-CNN with parameter sharing, and uses SHAP-based explainer for anomaly interpretation.

Result: Extensive experiments on six real-world datasets show Moon outperforms six state-of-the-art methods by up to 93% in efficiency, 4% in accuracy, and 10.8% in interpretation performance.

Conclusion: Moon effectively addresses limitations of existing MTS anomaly detection methods by combining modality conversion, multimodal integration, and interpretable analysis, achieving superior efficiency, accuracy, and interpretability.

Abstract: Multivariate time series (MTS) anomaly detection identifies abnormal patterns
where each timestamp contains multiple variables. Existing MTS anomaly
detection methods fall into three categories: reconstruction-based,
prediction-based, and classifier-based methods. However, these methods face two
key challenges: (1) Unsupervised learning methods, such as reconstruction-based
and prediction-based methods, rely on error thresholds, which can lead to
inaccuracies; (2) Semi-supervised methods mainly model normal data and often
underuse anomaly labels, limiting detection of subtle anomalies;(3) Supervised
learning methods, such as classifier-based approaches, often fail to capture
local relationships, incur high computational costs, and are constrained by the
scarcity of labeled data. To address these limitations, we propose Moon, a
supervised modality conversion-based multivariate time series anomaly detection
framework. Moon enhances the efficiency and accuracy of anomaly detection while
providing detailed anomaly analysis reports. First, Moon introduces a novel
multivariate Markov Transition Field (MV-MTF) technique to convert numeric time
series data into image representations, capturing relationships across
variables and timestamps. Since numeric data retains unique patterns that
cannot be fully captured by image conversion alone, Moon employs a
Multimodal-CNN to integrate numeric and image data through a feature fusion
model with parameter sharing, enhancing training efficiency. Finally, a
SHAP-based anomaly explainer identifies key variables contributing to
anomalies, improving interpretability. Extensive experiments on six real-world
MTS datasets demonstrate that Moon outperforms six state-of-the-art methods by
up to 93% in efficiency, 4% in accuracy and, 10.8% in interpretation
performance.

</details>


### [251] [KAIROS: Unified Training for Universal Non-Autoregressive Time Series Forecasting](https://arxiv.org/abs/2510.02084)
*Kuiye Ding,Fanda Fan,Zheya Wang,Hongxiao Li,Yifan Wang,Lei Wang,Chunjie Luo,Jianfeng Zhan*

Main category: cs.LG

TL;DR: KAIROS is a non-autoregressive time series forecasting framework that directly models segment-level multi-peak distributions, achieving fast inference and strong zero-shot performance while avoiding error accumulation common in autoregressive models.


<details>
  <summary>Details</summary>
Motivation: Web applications require fast time series forecasting for real-time decision making in resource planning, cache placement, and anomaly response, but existing approaches suffer from error accumulation (autoregressive) or over-smoothed predictions (non-autoregressive).

Method: KAIROS uses a non-autoregressive framework that directly models segment-level multi-peak distributions, avoiding the sequential dependency of autoregressive models while preventing the over-smoothing issues of existing non-autoregressive approaches.

Result: KAIROS demonstrates strong zero-shot generalization on six benchmarks, achieving forecasting performance comparable to state-of-the-art foundation models with similar scale, but at a fraction of their inference cost.

Conclusion: KAIROS highlights the importance of non-autoregressive design as a scalable paradigm for foundation models in time series, offering efficient and accurate forecasting for web applications.

Abstract: In the World Wide Web, reliable time series forecasts provide the
forward-looking signals that drive resource planning, cache placement, and
anomaly response, enabling platforms to operate efficiently as user behavior
and content distributions evolve. Compared with other domains, time series
forecasting for Web applications requires much faster responsiveness to support
real-time decision making. We present KAIROS, a non-autoregressive time series
forecasting framework that directly models segment-level multi-peak
distributions. Unlike autoregressive approaches, KAIROS avoids error
accumulation and achieves just-in-time inference, while improving over existing
non-autoregressive models that collapse to over-smoothed predictions. Trained
on the large-scale corpus, KAIROS demonstrates strong zero-shot generalization
on six widely used benchmarks, delivering forecasting performance comparable to
state-of-the-art foundation models with similar scale, at a fraction of their
inference cost. Beyond empirical results, KAIROS highlights the importance of
non-autoregressive design as a scalable paradigm for foundation models in time
series.

</details>


### [252] [Private Federated Multiclass Post-hoc Calibration](https://arxiv.org/abs/2510.01987)
*Samuel Maddock,Graham Cormode,Carsten Maple*

Main category: cs.LG

TL;DR: This paper introduces federated calibration methods for machine learning models in FL settings, adapting centralized techniques like histogram binning and temperature scaling to handle client heterogeneity and differential privacy constraints.


<details>
  <summary>Details</summary>
Motivation: Calibration is crucial for reliable decision-making in FL applications like healthcare and finance, but federated private calibration has been largely overlooked despite strong requirements in these domains.

Method: Transferred traditional centralized calibration methods (histogram binning and temperature scaling) into federated environments, developed new methods to handle client heterogeneity, and studied both federated and user-level Differential Privacy settings.

Result: Found that federation and DP impact calibration accuracy, with federated temperature scaling working best for DP-FL and weighted binning approach performing best when DP is not required. Proposed strategies to mitigate degradation under heterogeneity.

Conclusion: Successfully integrated post-hoc model calibration techniques within FL, providing effective solutions for both standard federated learning and privacy-preserving settings with differential privacy.

Abstract: Calibrating machine learning models so that predicted probabilities better
reflect the true outcome frequencies is crucial for reliable decision-making
across many applications. In Federated Learning (FL), the goal is to train a
global model on data which is distributed across multiple clients and cannot be
centralized due to privacy concerns. FL is applied in key areas such as
healthcare and finance where calibration is strongly required, yet federated
private calibration has been largely overlooked. This work introduces the
integration of post-hoc model calibration techniques within FL. Specifically,
we transfer traditional centralized calibration methods such as histogram
binning and temperature scaling into federated environments and define new
methods to operate them under strong client heterogeneity. We study (1) a
federated setting and (2) a user-level Differential Privacy (DP) setting and
demonstrate how both federation and DP impacts calibration accuracy. We propose
strategies to mitigate degradation commonly observed under heterogeneity and
our findings highlight that our federated temperature scaling works best for
DP-FL whereas our weighted binning approach is best when DP is not required.

</details>


### [253] [PepCompass: Navigating peptide embedding spaces using Riemannian Geometry](https://arxiv.org/abs/2510.01988)
*Marcin Mo≈ºejko,Adam Bielecki,Jurand PrƒÖdzy≈Ñski,Marcin Traskowski,Antoni Janowski,Karol Jurasz,Micha≈Ç Kucharczyk,Hyun-Su Lee,Marcelo Der Torossian Torres,Cesar de la Fuente-Nunez,Paulina Szymczak,Micha≈Ç Kmicikiewicz,Ewa Szczurek*

Main category: cs.LG

TL;DR: PepCompass is a geometry-aware framework for antimicrobial peptide discovery that uses Riemannian manifolds to model peptide space, enabling more efficient exploration and optimization through local search methods and geodesic interpolation.


<details>
  <summary>Details</summary>
Motivation: Current generative models for antimicrobial peptides ignore decoder-induced geometry and rely on flat Euclidean metrics, making exploration distorted and inefficient. The astronomical size of peptide space and scarcity of active peptides requires better geometric understanding.

Method: PepCompass uses Union of Œ∫-Stable Riemannian Manifolds to capture local geometry. It introduces Second-Order Riemannian Brownian Efficient Sampling and Mutation Enumeration in Tangent Space, combined into Local Enumeration Bayesian Optimization (LE-BO). Also includes Potential-minimizing Geodesic Search (PoGS) for property-enriched geodesic interpolation.

Result: In-vitro validation showed PoGS yielded four novel seeds, and subsequent LE-BO optimization discovered 25 highly active peptides with broad-spectrum activity, including against resistant bacterial strains.

Conclusion: Geometry-informed exploration provides a powerful new paradigm for antimicrobial peptide design, overcoming limitations of conventional flat Euclidean approaches.

Abstract: Antimicrobial peptide discovery is challenged by the astronomical size of
peptide space and the relative scarcity of active peptides. Generative models
provide continuous latent "maps" of peptide space, but conventionally ignore
decoder-induced geometry and rely on flat Euclidean metrics, rendering
exploration and optimization distorted and inefficient. Prior manifold-based
remedies assume fixed intrinsic dimensionality, which critically fails in
practice for peptide data. Here, we introduce PepCompass, a geometry-aware
framework for peptide exploration and optimization. At its core, we define a
Union of $\kappa$-Stable Riemannian Manifolds $\mathbb{M}^{\kappa}$, a family
of decoder-induced manifolds that captures local geometry while ensuring
computational stability. We propose two local exploration methods: Second-Order
Riemannian Brownian Efficient Sampling, which provides a convergent
second-order approximation to Riemannian Brownian motion, and Mutation
Enumeration in Tangent Space, which reinterprets tangent directions as discrete
amino-acid substitutions. Combining these yields Local Enumeration Bayesian
Optimization (LE-BO), an efficient algorithm for local activity optimization.
Finally, we introduce Potential-minimizing Geodesic Search (PoGS), which
interpolates between prototype embeddings along property-enriched geodesics,
biasing discovery toward seeds, i.e. peptides with favorable activity. In-vitro
validation confirms the effectiveness of PepCompass: PoGS yields four novel
seeds, and subsequent optimization with LE-BO discovers 25 highly active
peptides with broad-spectrum activity, including against resistant bacterial
strains. These results demonstrate that geometry-informed exploration provides
a powerful new paradigm for antimicrobial peptide design.

</details>


### [254] [Normality Calibration in Semi-supervised Graph Anomaly Detection](https://arxiv.org/abs/2510.02014)
*Guolei Zeng,Hezhe Qiao,Guoguo Ai,Jinsong Guo,Guansong Pang*

Main category: cs.LG

TL;DR: GraphNC is a graph normality calibration framework that improves semi-supervised graph anomaly detection by calibrating normality in both anomaly score and representation spaces using labeled and unlabeled data.


<details>
  <summary>Details</summary>
Motivation: Existing semi-supervised GAD methods overfit to labeled normal nodes, leading to high detection errors like false positives. The limited normality learning needs calibration using both labeled and unlabeled data.

Method: GraphNC uses two components: ScoreDA aligns anomaly score distributions with a teacher model to separate normal/abnormal classes, and NormReg applies perturbation-based consistency regularization on labeled nodes to make normal representations more compact.

Result: The framework effectively pulls anomaly scores of normal and abnormal classes toward opposite ends, creating more separable scores while making normal node representations more compact.

Conclusion: GraphNC successfully addresses overfitting in semi-supervised GAD by calibrating normality across score and representation spaces, leading to improved anomaly detection performance with reduced false positives.

Abstract: Graph anomaly detection (GAD) has attracted growing interest for its crucial
ability to uncover irregular patterns in broad applications. Semi-supervised
GAD, which assumes a subset of annotated normal nodes available during
training, is among the most widely explored application settings. However, the
normality learned by existing semi-supervised GAD methods is limited to the
labeled normal nodes, often inclining to overfitting the given patterns. These
can lead to high detection errors, such as high false positives. To overcome
this limitation, we propose GraphNC , a graph normality calibration framework
that leverages both labeled and unlabeled data to calibrate the normality from
a teacher model (a pre-trained semi-supervised GAD model) jointly in anomaly
score and node representation spaces. GraphNC includes two main components,
anomaly score distribution alignment (ScoreDA) and perturbation-based normality
regularization (NormReg). ScoreDA optimizes the anomaly scores of our model by
aligning them with the score distribution yielded by the teacher model. Due to
accurate scores in most of the normal nodes and part of the anomaly nodes in
the teacher model, the score alignment effectively pulls the anomaly scores of
the normal and abnormal classes toward the two ends, resulting in more
separable anomaly scores. Nevertheless, there are inaccurate scores from the
teacher model. To mitigate the misleading by these scores, NormReg is designed
to regularize the graph normality in the representation space, making the
representations of normal nodes more compact by minimizing a
perturbation-guided consistency loss solely on the labeled nodes.

</details>


### [255] [FairContrast: Enhancing Fairness through Contrastive learning and Customized Augmenting Methods on Tabular Data](https://arxiv.org/abs/2510.02017)
*Aida Tayebi,Ali Khodabandeh Yalabadi,Mehdi Yazdani-Jahromi,Ozlem Ozmen Garibay*

Main category: cs.LG

TL;DR: A contrastive learning framework for learning fair representations in tabular data that reduces bias while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: As AI systems become more embedded in everyday life, developing fair and unbiased models is critical. While representation learning has shown promise for debiasing, fairness in tabular data representations remains underexplored.

Method: A contrastive learning framework with strategic positive pair selection using both supervised and self-supervised contrastive learning specifically designed for tabular datasets.

Result: Significantly reduces bias compared to existing state-of-the-art contrastive learning models for tabular data with minimum trade-off in accuracy.

Conclusion: The approach effectively mitigates bias while maintaining essential information for prediction tasks, demonstrating efficacy in various downstream applications.

Abstract: As AI systems become more embedded in everyday life, the development of fair
and unbiased models becomes more critical. Considering the social impact of AI
systems is not merely a technical challenge but a moral imperative. As
evidenced in numerous research studies, learning fair and robust
representations has proven to be a powerful approach to effectively debiasing
algorithms and improving fairness while maintaining essential information for
prediction tasks. Representation learning frameworks, particularly those that
utilize self-supervised and contrastive learning, have demonstrated superior
robustness and generalizability across various domains. Despite the growing
interest in applying these approaches to tabular data, the issue of fairness in
these learned representations remains underexplored. In this study, we
introduce a contrastive learning framework specifically designed to address
bias and learn fair representations in tabular datasets. By strategically
selecting positive pair samples and employing supervised and self-supervised
contrastive learning, we significantly reduce bias compared to existing
state-of-the-art contrastive learning models for tabular data. Our results
demonstrate the efficacy of our approach in mitigating bias with minimum
trade-off in accuracy and leveraging the learned fair representations in
various downstream tasks.

</details>


### [256] [Mathematical Modeling and Convergence Analysis of Deep Neural Networks with Dense Layer Connectivities in Deep Learning](https://arxiv.org/abs/2510.02049)
*Jinshu Huang,Haibin Su,Xue-Cheng Tai,Chunlin Wu*

Main category: cs.LG

TL;DR: The paper provides a mathematical framework for analyzing densely connected deep neural networks (DNNs) using nonlinear integral equations and optimal control theory, showing convergence from discrete to continuous formulations.


<details>
  <summary>Details</summary>
Motivation: To establish a rigorous mathematical foundation for understanding densely connected DNNs and analyze their training stability in the deep-layer limit, moving beyond the ordinary differential equation viewpoint commonly used in prior works.

Method: Developed a dense non-local (DNL) framework modeling densely connected DNNs as nonlinear integral equations, studied training problems from an optimal control perspective, and used piecewise linear extension with Œì-convergence analysis.

Result: Proved convergence of optimal values and subsequence convergence of minimizers from discrete network learning problems to their continuous-time counterparts, demonstrating training stability for deep models.

Conclusion: Densely connected architectures offer training stability for deep models, and the mathematical framework provides foundations for understanding such networks through integral equations and optimal control theory.

Abstract: In deep learning, dense layer connectivity has become a key design principle
in deep neural networks (DNNs), enabling efficient information flow and strong
performance across a range of applications. In this work, we model densely
connected DNNs mathematically and analyze their learning problems in the
deep-layer limit. For a broad applicability, we present our analysis in a
framework setting of DNNs with densely connected layers and general non-local
feature transformations (with local feature transformations as special cases)
within layers, which is called dense non-local (DNL) framework and includes
standard DenseNets and variants as special examples. In this formulation, the
densely connected networks are modeled as nonlinear integral equations, in
contrast to the ordinary differential equation viewpoint commonly adopted in
prior works. We study the associated training problems from an optimal control
perspective and prove convergence results from the network learning problem to
its continuous-time counterpart. In particular, we show the convergence of
optimal values and the subsequence convergence of minimizers, using a piecewise
linear extension and $\Gamma$-convergence analysis. Our results provide a
mathematical foundation for understanding densely connected DNNs and further
suggest that such architectures can offer stability of training deep models.

</details>


### [257] [GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning](https://arxiv.org/abs/2510.02180)
*Silvia Sapora,Devon Hjelm,Alexander Toshev,Omar Attia,Bogdan Mazoure*

Main category: cs.LG

TL;DR: GRACE uses LLMs and evolutionary search to generate interpretable code-based reward functions from expert demonstrations, outperforming traditional IRL and imitation learning methods.


<details>
  <summary>Details</summary>
Motivation: Traditional Inverse Reinforcement Learning produces black-box reward models that are difficult to interpret and debug, limiting practical applications.

Method: GRACE combines Large Language Models with evolutionary search to reverse-engineer executable code-based reward functions directly from expert trajectories.

Result: GRACE learns highly accurate rewards on BabyAI and AndroidWorld benchmarks, produces strong policies comparable to ground-truth rewards, and builds complex reward APIs in multi-task settings.

Conclusion: GRACE successfully generates interpretable, code-based reward functions that are executable, verifiable, and lead to strong policy performance across complex environments.

Abstract: Inverse Reinforcement Learning aims to recover reward models from expert
demonstrations, but traditional methods yield "black-box" models that are
difficult to interpret and debug. In this work, we introduce GRACE (Generating
Rewards As CodE), a method for using Large Language Models within an
evolutionary search to reverse-engineer an interpretable, code-based reward
function directly from expert trajectories. The resulting reward function is
executable code that can be inspected and verified. We empirically validate
GRACE on the BabyAI and AndroidWorld benchmarks, where it efficiently learns
highly accurate rewards, even in complex, multi-task settings. Further, we
demonstrate that the resulting reward leads to strong policies, compared to
both competitive Imitation Learning and online RL approaches with ground-truth
rewards. Finally, we show that GRACE is able to build complex reward APIs in
multi-task setups.

</details>


### [258] [Adaptive Heterogeneous Mixtures of Normalising Flows for Robust Variational Inference](https://arxiv.org/abs/2510.02056)
*Benjamin Wiriyapong,Oktay Karaku≈ü,Kirill Sidorov*

Main category: cs.LG

TL;DR: AMF-VI is a two-stage adaptive mixture of complementary normalizing flows (MAF, RealNVP, RBIG) that achieves robust variational inference across diverse posterior distributions without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Single-flow models behave inconsistently across different posterior distributions, limiting their reliability in variational inference applications.

Method: Two-stage training: (1) sequential expert training of individual flows, (2) adaptive global weight estimation via likelihood-driven updates without per-sample gating or architectural modifications.

Result: Consistently lower negative log-likelihood than single-flow baselines across six canonical posterior families, with stable gains in Wasserstein-2 and MMD metrics, indicating improved robustness across shapes and modalities.

Conclusion: Adaptive mixtures of diverse flows provide reliable robust variational inference while preserving each expert's inductive bias, with minimal computational overhead.

Abstract: Normalising-flow variational inference (VI) can approximate complex
posteriors, yet single-flow models often behave inconsistently across
qualitatively different distributions. We propose Adaptive Mixture Flow
Variational Inference (AMF-VI), a heterogeneous mixture of complementary flows
(MAF, RealNVP, RBIG) trained in two stages: (i) sequential expert training of
individual flows, and (ii) adaptive global weight estimation via
likelihood-driven updates, without per-sample gating or architectural changes.
Evaluated on six canonical posterior families of banana, X-shape, two-moons,
rings, a bimodal, and a five-mode mixture, AMF-VI achieves consistently lower
negative log-likelihood than each single-flow baseline and delivers stable
gains in transport metrics (Wasserstein-2) and maximum mean discrepancy (MDD),
indicating improved robustness across shapes and modalities. The procedure is
efficient and architecture-agnostic, incurring minimal overhead relative to
standard flow training, and demonstrates that adaptive mixtures of diverse
flows provide a reliable route to robust VI across diverse posterior families
whilst preserving each expert's inductive bias.

</details>


### [259] [Detection of Chagas Disease from the ECG: The George B. Moody PhysioNet Challenge 2025](https://arxiv.org/abs/2510.02202)
*Matthew A. Reyna,Zuzana Koscova,Jan Pavlus,Soheil Saghafi,James Weigle,Andoni Elola,Salman Seyedi,Kiersten Campbell,Qiao Li,Ali Bahrami Rad,Ant√¥nio H. Ribeiro,Antonio Luiz P. Ribeiro,Reza Sameni,Gari D. Clifford*

Main category: cs.LG

TL;DR: The George B. Moody PhysioNet Challenge 2025 focuses on developing algorithms to identify Chagas disease from ECGs, using a large dataset with weak labels and smaller datasets with strong labels to prioritize patients for serological testing.


<details>
  <summary>Details</summary>
Motivation: Chagas disease is a parasitic infection that can cause cardiovascular diseases and digestive problems. Limited serological testing capacity creates a need for alternative screening methods, and since Chagas cardiomyopathy often manifests in ECGs, this provides an opportunity to prioritize patients for testing and treatment.

Method: The challenge used multiple datasets with labels from patient reports and serological testing, provided a large dataset with weak labels and smaller datasets with strong labels, augmented data for model robustness, and applied an evaluation metric that captured local serological testing capacity to frame the problem as a triage task.

Result: Over 630 participants from 111 teams submitted more than 1300 entries during the Challenge, representing diverse approaches from academia and industry worldwide.

Conclusion: The Challenge successfully developed algorithmic approaches for identifying Chagas disease from ECGs, providing an innovative framework for prioritizing patients for serological testing when testing capacity is limited.

Abstract: Objective: Chagas disease is a parasitic infection that is endemic to South
America, Central America, and, more recently, the U.S., primarily transmitted
by insects. Chronic Chagas disease can cause cardiovascular diseases and
digestive problems. Serological testing capacities for Chagas disease are
limited, but Chagas cardiomyopathy often manifests in ECGs, providing an
opportunity to prioritize patients for testing and treatment. Approach: The
George B. Moody PhysioNet Challenge 2025 invites teams to develop algorithmic
approaches for identifying Chagas disease from electrocardiograms (ECGs). Main
results: This Challenge provides multiple innovations. First, we leveraged
several datasets with labels from patient reports and serological testing,
provided a large dataset with weak labels and smaller datasets with strong
labels. Second, we augmented the data to support model robustness and
generalizability to unseen data sources. Third, we applied an evaluation metric
that captured the local serological testing capacity for Chagas disease to
frame the machine learning problem as a triage task. Significance: Over 630
participants from 111 teams submitted over 1300 entries during the Challenge,
representing diverse approaches from academia and industry worldwide.

</details>


### [260] [Inferring Optical Tissue Properties from Photoplethysmography using Hybrid Amortized Inference](https://arxiv.org/abs/2510.02073)
*Jens Behrmann,Maria R. Cervera,Antoine Wehenkel,Andrew C. Miller,Albert Cerussi,Pranay Jain,Vivek Venugopal,Shijie Yan,Guillermo Sapiro,Luca Pegolotti,J√∂rn-Henrik Jacobsen*

Main category: cs.LG

TL;DR: PPGen is a biophysical model that relates PPG signals to interpretable physiological parameters, using hybrid amortized inference (HAI) for robust parameter estimation while maintaining clinical interpretability.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models for PPG analysis rely on features with unclear physiological meaning, creating tension between predictive power, clinical interpretability, and sensor design.

Method: Developed PPGen biophysical model and hybrid amortized inference (HAI) to enable fast, robust estimation of physiological parameters from PPG signals while correcting for model misspecification.

Result: HAI accurately inferred physiological parameters under diverse noise and sensor conditions in extensive in-silico experiments.

Conclusion: The approach provides a path toward PPG models that retain DL-based feature fidelity while supporting clinical interpretation and informed hardware design.

Abstract: Smart wearables enable continuous tracking of established biomarkers such as
heart rate, heart rate variability, and blood oxygen saturation via
photoplethysmography (PPG). Beyond these metrics, PPG waveforms contain richer
physiological information, as recent deep learning (DL) studies demonstrate.
However, DL models often rely on features with unclear physiological meaning,
creating a tension between predictive power, clinical interpretability, and
sensor design. We address this gap by introducing PPGen, a biophysical model
that relates PPG signals to interpretable physiological and optical parameters.
Building on PPGen, we propose hybrid amortized inference (HAI), enabling fast,
robust, and scalable estimation of relevant physiological parameters from PPG
signals while correcting for model misspecification. In extensive in-silico
experiments, we show that HAI can accurately infer physiological parameters
under diverse noise and sensor conditions. Our results illustrate a path toward
PPG models that retain the fidelity needed for DL-based features while
supporting clinical interpretation and informed hardware design.

</details>


### [261] [DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning](https://arxiv.org/abs/2510.02212)
*Hanyang Zhao,Dawen Liang,Wenpin Tang,David Yao,Nathan Kallus*

Main category: cs.LG

TL;DR: DiFFPO is a unified RL framework that trains masked diffusion LLMs to reason better and faster by training surrogate policies and jointly optimizing samplers for adaptive inference thresholds.


<details>
  <summary>Details</summary>
Motivation: To improve both reasoning quality and speed of diffusion large language models through reinforcement learning, addressing the trade-off between performance and computational efficiency.

Method: Uses off-policy RL to train surrogate policies with tractable likelihood, employs two-stage likelihood approximation with importance sampling, and jointly trains efficient samplers that adaptively allocate inference thresholds per prompt.

Result: Achieves better sample efficiency, superior task performance, and improved Pareto frontier of inference-time compute with lower number of function evaluations while maintaining accuracy.

Conclusion: DiFFPO effectively enhances both reasoning capabilities and computational efficiency of diffusion LLMs through unified RL training of policies and samplers.

Abstract: We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified
framework for training masked diffusion large language models (dLLMs) to reason
not only better (furious), but also faster via reinforcement learning (RL). We
first unify the existing baseline approach such as d1 by proposing to train
surrogate policies via off-policy RL, whose likelihood is much more tractable
as an approximation to the true dLLM policy. This naturally motivates a more
accurate and informative two-stage likelihood approximation combined with
importance sampling correction, which leads to generalized RL algorithms with
better sample efficiency and superior task performance. Second, we propose a
new direction of joint training efficient samplers/controllers of dLLMs policy.
Via RL, we incentivize dLLMs' natural multi-token prediction capabilities by
letting the model learn to adaptively allocate an inference threshold for each
prompt. By jointly training the sampler, we yield better accuracies with lower
number of function evaluations (NFEs) compared to training the model only,
obtaining the best performance in improving the Pareto frontier of the
inference-time compute of dLLMs. We showcase the effectiveness of our pipeline
by training open source large diffusion language models over benchmark math and
planning tasks.

</details>


### [262] [Fine-Tuning Flow Matching via Maximum Likelihood Estimation of Reconstructions](https://arxiv.org/abs/2510.02081)
*Zhaoyi Li,Jingtao Ding,Yong Li,Shihua Li*

Main category: cs.LG

TL;DR: The paper proposes a fine-tuning method for Flow Matching (FM) using Maximum Likelihood Estimation to address the train-inference gap and improve precision in generative tasks, particularly robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: Flow Matching has a train-inference gap where model output cannot be assessed during training, unlike other generative models. This gap is problematic in precision-demanding scenarios like robotic manipulation, and FM's pursuit of straight paths can introduce stiffness issues.

Method: Fine-tuning FM via Maximum Likelihood Estimation of reconstructions, including straightforward fine-tuning and residual-based fine-tuning approaches. The residual-based approach incorporates contraction properties through specialized architectures for better robustness.

Result: Experimental results in image generation and robotic manipulation show that the proposed method reliably improves FM's inference performance.

Conclusion: The fine-tuning approach via Maximum Likelihood Estimation effectively bridges the train-inference gap in Flow Matching and enhances performance in precision-critical applications.

Abstract: Flow Matching (FM) algorithm achieves remarkable results in generative tasks
especially in robotic manipulation. Building upon the foundations of diffusion
models, the simulation-free paradigm of FM enables simple and efficient
training, but inherently introduces a train-inference gap. Specifically, we
cannot assess the model's output during the training phase. In contrast, other
generative models including Variational Autoencoder (VAE), Normalizing Flow and
Generative Adversarial Networks (GANs) directly optimize on the reconstruction
loss. Such a gap is particularly evident in scenarios that demand high
precision, such as robotic manipulation. Moreover, we show that FM's
over-pursuit of straight predefined paths may introduce some serious problems
such as stiffness into the system. These motivate us to fine-tune FM via
Maximum Likelihood Estimation of reconstructions - an approach made feasible by
FM's underlying smooth ODE formulation, in contrast to the stochastic
differential equations (SDEs) used in diffusion models. This paper first
theoretically analyzes the relation between training loss and inference error
in FM. Then we propose a method of fine-tuning FM via Maximum Likelihood
Estimation of reconstructions, which includes both straightforward fine-tuning
and residual-based fine-tuning approaches. Furthermore, through specifically
designed architectures, the residual-based fine-tuning can incorporate the
contraction property into the model, which is crucial for the model's
robustness and interpretability. Experimental results in image generation and
robotic manipulation verify that our method reliably improves the inference
performance of FM.

</details>


### [263] [Learning Model Representations Using Publicly Available Model Hubs](https://arxiv.org/abs/2510.02096)
*Damian Falk,Konstantin Sch√ºrholt,Konstantinos Tzevelekakis,L√©o Meynent,Damian Borth*

Main category: cs.LG

TL;DR: The paper proposes a weight space learning backbone that can be trained on arbitrary, heterogeneous models from unstructured repositories like Hugging Face, eliminating the need for curated model zoos.


<details>
  <summary>Details</summary>
Motivation: Current weight space learning requires large, carefully constructed model zoos that are computationally expensive to create, limiting scale and flexibility. The authors aim to overcome this limitation by learning from unstructured model repositories.

Method: Proposed a new weight space backbone designed to handle unstructured model populations with varying architectures, datasets, and documentation. Trained on heterogeneous models from Hugging Face.

Result: Weight space representations trained on Hugging Face models achieve strong performance, often outperforming backbones trained on laboratory-generated model zoos. The diversity enables generalization to unseen data modalities.

Conclusion: Curated model zoos are not indispensable for weight space learning. High-quality representations can be learned from unstructured repositories, overcoming a major limitation in the field.

Abstract: The weights of neural networks have emerged as a novel data modality, giving
rise to the field of weight space learning. A central challenge in this area is
that learning meaningful representations of weights typically requires large,
carefully constructed collections of trained models, typically referred to as
model zoos. These model zoos are often trained ad-hoc, requiring large
computational resources, constraining the learned weight space representations
in scale and flexibility. In this work, we drop this requirement by training a
weight space learning backbone on arbitrary models downloaded from large,
unstructured model repositories such as Hugging Face. Unlike curated model
zoos, these repositories contain highly heterogeneous models: they vary in
architecture and dataset, and are largely undocumented. To address the
methodological challenges posed by such heterogeneity, we propose a new weight
space backbone designed to handle unstructured model populations. We
demonstrate that weight space representations trained on models from Hugging
Face achieve strong performance, often outperforming backbones trained on
laboratory-generated model zoos. Finally, we show that the diversity of the
model weights in our training set allows our weight space model to generalize
to unseen data modalities. By demonstrating that high-quality weight space
representations can be learned in the wild, we show that curated model zoos are
not indispensable, thereby overcoming a strong limitation currently faced by
the weight space learning community.

</details>


### [264] [ExGRPO: Learning to Reason from Experience](https://arxiv.org/abs/2510.02245)
*Runzhe Zhan,Yafu Li,Zhi Wang,Xiaoye Qu,Dongrui Liu,Jing Shao,Derek F. Wong,Yu Cheng*

Main category: cs.LG

TL;DR: ExGRPO improves RLVR efficiency by identifying valuable experiences through correctness and entropy metrics, organizing them into groups, and using a mixed-policy objective to balance exploration with experience reuse.


<details>
  <summary>Details</summary>
Motivation: Standard on-policy RLVR training discards experiences after single use, leading to computational inefficiency and instability. The value of reasoning experiences and their characteristics in shaping learning dynamics is underexplored.

Method: Proposed ExGRPO framework that identifies valuable experiences using rollout correctness and entropy metrics, organizes experiences into groups, and employs mixed-policy objective to balance exploration with experience exploitation.

Result: Experiments on 1.5B-8B parameter models show consistent improvements: +3.5 points on mathematical benchmarks and +7.6 points on general benchmarks over on-policy RLVR. Also stabilizes training on both stronger and weaker models.

Conclusion: Principled experience management is crucial for efficient and scalable RLVR. ExGRPO demonstrates that organizing and prioritizing valuable experiences significantly improves reasoning performance and training stability.

Abstract: Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm
for improving the reasoning ability of large language models. However, standard
on-policy training discards rollout experiences after a single update, leading
to computational inefficiency and instability. While prior work on RL has
highlighted the benefits of reusing past experience, the role of experience
characteristics in shaping learning dynamics of large reasoning models remains
underexplored. In this paper, we are the first to investigate what makes a
reasoning experience valuable and identify rollout correctness and entropy as
effective indicators of experience value. Based on these insights, we propose
ExGRPO (Experiential Group Relative Policy Optimization), a framework that
organizes and prioritizes valuable experiences, and employs a mixed-policy
objective to balance exploration with experience exploitation. Experiments on
five backbone models (1.5B-8B parameters) show that ExGRPO consistently
improves reasoning performance on mathematical/general benchmarks, with an
average gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO
stabilizes training on both stronger and weaker models where on-policy methods
fail. These results highlight principled experience management as a key
ingredient for efficient and scalable RLVR.

</details>


### [265] [PENEX: AdaBoost-Inspired Neural Network Regularization](https://arxiv.org/abs/2510.02107)
*Klaus-Rudolf Kladny,Bernhard Sch√∂lkopf,Michael Muehlebach*

Main category: cs.LG

TL;DR: PENEX is a new multi-class exponential loss formulation that enables first-order optimization, implicitly maximizes margins, and shows strong regularization effects comparable to AdaBoost but with better computational efficiency.


<details>
  <summary>Details</summary>
Motivation: AdaBoost uses exponential loss that penalizes misclassifications heavily but generalizes well, however existing formulations are not amenable to first-order optimization methods.

Method: Proposed Penalized Exponential Loss (PENEX) - a theoretically grounded multi-class exponential loss formulation that can be optimized via first-order methods and implicitly maximizes margins.

Result: PENEX shows regularizing effects often better than established methods with similar computational cost across computer vision and language tasks, and gradient increments implicitly parameterize weak learners.

Conclusion: PENEX has potential as an AdaBoost-inspired alternative for effective training and fine-tuning of deep neural networks with good regularization properties.

Abstract: AdaBoost sequentially fits so-called weak learners to minimize an exponential
loss, which penalizes mislabeled data points more severely than other loss
functions like cross-entropy. Paradoxically, AdaBoost generalizes well in
practice as the number of weak learners grows. In the present work, we
introduce Penalized Exponential Loss (PENEX), a new formulation of the
multi-class exponential loss that is theoretically grounded and, in contrast to
the existing formulation, amenable to optimization via first-order methods. We
demonstrate both empirically and theoretically that PENEX implicitly maximizes
margins of data points. Also, we show that gradient increments on PENEX
implicitly parameterize weak learners in the boosting framework. Across
computer vision and language tasks, we show that PENEX exhibits a regularizing
effect often better than established methods with similar computational cost.
Our results highlight PENEX's potential as an AdaBoost-inspired alternative for
effective training and fine-tuning of deep neural networks.

</details>


### [266] [Hybrid Deep Learning Modeling Approach to Predict Natural Gas Consumption of Home Subscribers on Limited Data](https://arxiv.org/abs/2510.02115)
*Milad Firoozeh,Nader Dashti,Mohammad Ali Hatefi*

Main category: cs.LG

TL;DR: This study analyzes and predicts residential gas consumption in Zanjan province, Iran using machine learning models including LSTM, GRU, and a hybrid BiLSTM-XGBoost model, with the hybrid model showing superior performance.


<details>
  <summary>Details</summary>
Motivation: Iran faces gas pressure drops and outages during cold seasons due to population growth and high energy consumption, particularly in the residential sector which has the largest consumption share. There is a need to control gas consumption through accurate prediction.

Method: Used machine learning models (LSTM, GRU, and hybrid BiLSTM-XGBoost) trained on six years of gas consumption and meteorological data (2017-2022) to predict residential gas consumption patterns.

Result: The hybrid BiLSTM-XGBoost model outperformed other models with lower RMSE, MAPE, and MPE values. It demonstrated robust performance especially in limited data scenarios.

Conclusion: Machine learning approaches, particularly hybrid models, can effectively manage and predict gas consumption for better resource management and reduced seasonal shortages. Geographical and climatic factors significantly influence gas usage and should be incorporated in predictive modeling.

Abstract: Today, natural gas, as a clean fuel and the best alternative to crude oil,
covers a significant part of global demand. Iran is one of the largest
countries with energy resources and in terms of gas is the second-largest
country in the world. But, due to the increase in population and energy
consumption, it faces problems such as pressure drops and gas outages yearly in
cold seasons and therefore it is necessary to control gas consumption,
especially in the residential sector, which has the largest share in Iran. This
study aims to analyze and predict gas consumption for residential customers in
Zanjan province, Iran, using machine learning models, including LSTM, GRU, and
a hybrid BiLSTM-XGBoost model. The dataset consists of gas consumption and
meteorology data collected over six years, from 2017 to 2022. The models were
trained and evaluated based on their ability to accurately predict consumption
patterns. The results indicate that the hybrid BiLSTM-XGBoost model
outperformed the other models in terms of accuracy, with lower Root Mean
Squared Error (RMSE), Mean Absolute Percentage Error (MAPE) values, and Mean
Percentage Error (MPE). Additionally, the Hybrid model demonstrated robust
performance, particularly in scenarios with limited data. The findings suggest
that machine learning approaches, particularly hybrid models, can be
effectively utilized to manage and predict gas consumption, contributing to
more efficient resource management and reducing seasonal shortages. This study
highlights the importance of incorporating geographical and climatic factors in
predictive modeling, as these significantly influence gas usage across
different regions.

</details>


### [267] [Ensemble Threshold Calibration for Stable Sensitivity Control](https://arxiv.org/abs/2510.02116)
*John N. Daras*

Main category: cs.LG

TL;DR: An end-to-end framework for precise recall control in spatial conflation tasks, achieving exact recall with sub-percent variance through ensemble threshold estimation and TPU-friendly processing.


<details>
  <summary>Details</summary>
Motivation: Precise recall control is critical in large-scale spatial conflation where missing true matches breaks downstream analytics, while classical confidence-interval methods overshoot targets and have high variance under skewed score distributions.

Method: Uses equigrid bounding-box filter and CSR candidate representation to reduce pair enumeration, trains neural ranker via deterministic bootstrap, constructs stratified calibration set, and aggregates four threshold estimators via inverse-variance weighting across multiple subsamples.

Result: Consistently hits recall targets within small error, decreases redundant verifications, and runs end-to-end on single TPU v3 core for datasets with 6.31M and 67.34M pairs.

Conclusion: The ensemble approach reduces threshold variance compared to single methods and provides reliable recall control for large-scale spatial matching tasks.

Abstract: Precise recall control is critical in large-scale spatial conflation and
entity-matching tasks, where missing even a few true matches can break
downstream analytics, while excessive manual review inflates cost. Classical
confidence-interval cuts such as Clopper-Pearson or Wilson provide lower bounds
on recall, but they routinely overshoot the target by several percentage points
and exhibit high run-to-run variance under skewed score distributions. We
present an end-to-end framework that achieves exact recall with sub-percent
variance over tens of millions of geometry pairs, while remaining TPU-friendly.
Our pipeline starts with an equigrid bounding-box filter and compressed sparse
row (CSR) candidate representation, reducing pair enumeration by two orders of
magnitude. A deterministic xxHash bootstrap sample trains a lightweight neural
ranker; its scores are propagated to all remaining pairs via a single forward
pass and used to construct a reproducible, score-decile-stratified calibration
set. Four complementary threshold estimators - Clopper-Pearson, Jeffreys,
Wilson, and an exact quantile - are aggregated via inverse-variance weighting,
then fused across nine independent subsamples. This ensemble reduces threshold
variance compared to any single method. Evaluated on two real cadastral
datasets (approximately 6.31M and 67.34M pairs), our approach consistently hits
a recall target within a small error, decreases redundant verifications
relative to other calibrations, and runs end-to-end on a single TPU v3 core.

</details>


### [268] [DAG DECORation: Continuous Optimization for Structure Learning under Hidden Confounding](https://arxiv.org/abs/2510.02117)
*Samhita Pal,James O'quinn,Kaveh Aryan,Heather Pua,James P. Long,Amir Asiaee*

Main category: cs.LG

TL;DR: DECOR is a differentiable estimator that jointly learns DAGs and correlated noise models for linear Gaussian SEMs with latent confounding, providing identifiability under bow-free graphs and uniform eigenvalue conditions.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with latent confounding in linear Gaussian SEMs - continuous methods need independent errors, while deconfounding-first approaches require pervasive factor structure or nonlinearity.

Method: DECOR alternates between smooth-acyclic graph updates and convex noise updates, can include bow complementarity penalty or post hoc reconciliation, and jointly estimates DAG structure and correlated noise.

Result: DECOR matches or outperforms baselines on synthetic benchmarks across varying confounding density, graph density, latent rank, and dimension, especially robust under non-pervasive confounding.

Conclusion: The method provides global parameter identifiability under bow-free graphs with uniform eigenvalue margins, uniquely determining both directed structure and noise covariance.

Abstract: We study structure learning for linear Gaussian SEMs in the presence of
latent confounding. Existing continuous methods excel when errors are
independent, while deconfounding-first pipelines rely on pervasive factor
structure or nonlinearity. We propose \textsc{DECOR}, a single likelihood-based
and fully differentiable estimator that jointly learns a DAG and a correlated
noise model. Our theory gives simple sufficient conditions for global parameter
identifiability: if the mixed graph is bow free and the noise covariance has a
uniform eigenvalue margin, then the map from $(\B,\OmegaMat)$ to the
observational covariance is injective, so both the directed structure and the
noise are uniquely determined. The estimator alternates a smooth-acyclic graph
update with a convex noise update and can include a light bow complementarity
penalty or a post hoc reconciliation step. On synthetic benchmarks that vary
confounding density, graph density, latent rank, and dimension with $n<p$,
\textsc{DECOR} matches or outperforms strong baselines and is especially robust
when confounding is non-pervasive, while remaining competitive under
pervasiveness.

</details>


### [269] [Catalyst GFlowNet for electrocatalyst design: A hydrogen evolution reaction case study](https://arxiv.org/abs/2510.02142)
*Lena Podina,Christina Humer,Alexandre Duval,Victor Schmidt,Ali Ramlaoui,Shahana Chatterjee,Yoshua Bengio,Alex Hernandez-Garcia,David Rolnick,F√©lix Therrien*

Main category: cs.LG

TL;DR: Catalyst GFlowNet is a generative model that uses ML predictors to design efficient catalysts for hydrogen energy storage, successfully identifying platinum as the best catalyst for hydrogen evolution reaction.


<details>
  <summary>Details</summary>
Motivation: Need for affordable and high-performance catalysts to enable efficient hydrogen energy storage from renewable sources like wind and solar.

Method: Uses generative model (Catalyst GFlowNet) with machine learning predictors of formation and adsorption energy to design crystal surfaces as catalysts.

Result: Successfully identified platinum as the most efficient known catalyst for hydrogen evolution reaction in proof-of-concept application.

Conclusion: The generative modeling framework offers a promising pathway for accelerating discovery of novel and efficient catalysts, with plans to extend to oxygen evolution reaction.

Abstract: Efficient and inexpensive energy storage is essential for accelerating the
adoption of renewable energy and ensuring a stable supply, despite fluctuations
in sources such as wind and solar. Electrocatalysts play a key role in hydrogen
energy storage (HES), allowing the energy to be stored as hydrogen. However,
the development of affordable and high-performance catalysts for this process
remains a significant challenge. We introduce Catalyst GFlowNet, a generative
model that leverages machine learning-based predictors of formation and
adsorption energy to design crystal surfaces that act as efficient catalysts.
We demonstrate the performance of the model through a proof-of-concept
application to the hydrogen evolution reaction, a key reaction in HES, for
which we successfully identified platinum as the most efficient known catalyst.
In future work, we aim to extend this approach to the oxygen evolution
reaction, where current optimal catalysts are expensive metal oxides, and open
the search space to discover new materials. This generative modeling framework
offers a promising pathway for accelerating the search for novel and efficient
catalysts.

</details>


### [270] [Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods for Natural Language Generation](https://arxiv.org/abs/2510.02279)
*Mykyta Ielanskyi,Kajetan Schweighofer,Lukas Aichberger,Sepp Hochreiter*

Main category: cs.LG

TL;DR: The paper addresses issues in evaluating uncertainty estimation methods for detecting LLM confabulations, proposing more robust evaluation approaches including multiple LLM-as-a-judge variants, structured tasks, and Elo rating systems.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for uncertainty estimation in NLG have substantial disagreement and can be manipulated to inflate performance metrics, undermining reliable assessment of confabulation detection methods.

Method: Proposes using multiple alternative risk indicators, marginalizing over LLM-as-a-judge variants, exploring structured tasks, out-of-distribution detection, and implementing Elo rating systems for comprehensive evaluation.

Result: The proposed evaluation framework reduces biases and provides more robust assessment of uncertainty estimation algorithms across various settings including QA tasks and perturbation detection.

Conclusion: A multi-faceted evaluation approach with diverse risk indicators and Elo rating provides more objective and reliable assessment of uncertainty estimation methods for detecting LLM confabulations.

Abstract: Hallucinations are a common issue that undermine the reliability of large
language models (LLMs). Recent studies have identified a specific subset of
hallucinations, known as confabulations, which arise due to predictive
uncertainty of LLMs. To detect confabulations, various methods for estimating
predictive uncertainty in natural language generation (NLG) have been
developed. These methods are typically evaluated by correlating uncertainty
estimates with the correctness of generated text, with question-answering (QA)
datasets serving as the standard benchmark. However, commonly used approximate
correctness functions have substantial disagreement between each other and,
consequently, in the ranking of the uncertainty estimation methods. This allows
one to inflate the apparent performance of uncertainty estimation methods. We
propose using several alternative risk indicators for risk correlation
experiments that improve robustness of empirical assessment of UE algorithms
for NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge
variants leads to reducing the evaluation biases. Furthermore, we explore
structured tasks as well as out of distribution and perturbation detection
tasks which provide robust and controllable risk indicators. Finally, we
propose to use an Elo rating of uncertainty estimation methods to give an
objective summarization over extensive evaluation settings.

</details>


### [271] [Policy Gradient Guidance Enables Test Time Control](https://arxiv.org/abs/2510.02148)
*Jianing Qi,Hao Tang,Zhigang Zhu*

Main category: cs.LG

TL;DR: Policy Gradient Guidance (PGG) extends classifier-free guidance from diffusion models to policy gradient methods, providing a test-time control mechanism without retraining.


<details>
  <summary>Details</summary>
Motivation: To adapt the successful guidance concept from diffusion models to reinforcement learning, enabling controllable behavior modulation in policy gradient methods.

Method: PGG augments policy gradient with an unconditional branch and interpolates between conditional and unconditional branches, with theoretical derivation showing normalization terms vanish under advantage estimation.

Result: PGG improves stability, sample efficiency, and controllability in discrete and continuous control benchmarks, with modest guidance (Œ≥>1) providing consistent benefits.

Conclusion: Guidance can be successfully adapted from diffusion policies to standard on-policy reinforcement learning methods, opening new directions for controllable online RL.

Abstract: We introduce Policy Gradient Guidance (PGG), a simple extension of
classifier-free guidance from diffusion models to classical policy gradient
methods. PGG augments the policy gradient with an unconditional branch and
interpolates conditional and unconditional branches, yielding a test-time
control knob that modulates behavior without retraining. We provide a
theoretical derivation showing that the additional normalization term vanishes
under advantage estimation, leading to a clean guided policy gradient update.
Empirically, we evaluate PGG on discrete and continuous control benchmarks. We
find that conditioning dropout-central to diffusion guidance-offers gains in
simple discrete tasks and low sample regimes, but dropout destabilizes
continuous control. Training with modestly larger guidance ($\gamma>1$)
consistently improves stability, sample efficiency, and controllability. Our
results show that guidance, previously confined to diffusion policies, can be
adapted to standard on-policy methods, opening new directions for controllable
online reinforcement learning.

</details>


### [272] [Reinforcement Learning with Action-Triggered Observations](https://arxiv.org/abs/2510.02149)
*Alexander Ryabchenko,Wenlong Mou*

Main category: cs.LG

TL;DR: This paper introduces Action-Triggered Sporadically Traceable MDPs (ATST-MDPs) where state observations occur stochastically based on actions, and proposes ST-LSVI-UCB algorithm that achieves efficient learning with sporadic observations.


<details>
  <summary>Details</summary>
Motivation: Many real-world applications have stochastically triggered state observations based on actions, creating a need for reinforcement learning frameworks that can handle such observation constraints.

Method: Derived Bellman optimality equations for ATST-MDPs, introduced action-sequence learning paradigm, and proposed ST-LSVI-UCB algorithm based on linear MDP assumption with off-policy estimators.

Result: ST-LSVI-UCB achieves regret OÃÉ(‚àöKd¬≥(1-Œ≥)‚Åª¬≥), where K is episodes, d is feature dimension, and Œ≥ is discount factor, showing efficient learning is possible under action-triggered observation constraints.

Conclusion: The work establishes theoretical foundations for learning with sporadic, action-triggered observations and demonstrates efficient learning remains feasible under such constraints.

Abstract: We study reinforcement learning problems where state observations are
stochastically triggered by actions, a constraint common in many real-world
applications. This framework is formulated as Action-Triggered Sporadically
Traceable Markov Decision Processes (ATST-MDPs), where each action has a
specified probability of triggering a state observation. We derive tailored
Bellman optimality equations for this framework and introduce the
action-sequence learning paradigm in which agents commit to executing a
sequence of actions until the next observation arrives. Under the linear MDP
assumption, value-functions are shown to admit linear representations in an
induced action-sequence feature map. Leveraging this structure, we propose
off-policy estimators with statistical error guarantees for such feature maps
and introduce ST-LSVI-UCB, a variant of LSVI-UCB adapted for action-triggered
settings. ST-LSVI-UCB achieves regret $\widetilde
O(\sqrt{Kd^3(1-\gamma)^{-3}})$, where $K$ is the number of episodes, $d$ the
feature dimension, and $\gamma$ the discount factor (per-step episode
non-termination probability). Crucially, this work establishes the theoretical
foundation for learning with sporadic, action-triggered observations while
demonstrating that efficient learning remains feasible under such observation
constraints.

</details>


### [273] [Flatness-Aware Stochastic Gradient Langevin Dynamics](https://arxiv.org/abs/2510.02174)
*Stefano Bruno,Youngsik Hwang,Jaehyeon An,Sotirios Sabanis,Dong-Young Lim*

Main category: cs.LG

TL;DR: fSGLD is a novel optimization method that efficiently seeks flat minima in deep learning by using random weight perturbation to capture curvature information, providing theoretical guarantees and superior generalization with computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Classical SGLD lacks mechanisms to bias optimization toward flat minima, which are crucial for generalization in deep learning. The paper aims to develop a method that can provably seek flat minima while maintaining computational efficiency.

Method: Flatness-Aware Stochastic Gradient Langevin Dynamics (fSGLD) uses stochastic gradients evaluated at parameters perturbed by isotropic Gaussian noise (Random Weight Perturbation) to optimize a randomized-smoothing objective that implicitly captures curvature information.

Result: Theoretical analysis shows fSGLD's invariant measure concentrates on global minimizers of a Hessian-trace regularized loss. Experiments demonstrate superior generalization and robustness on noisy-label and vision tasks, with computational cost similar to SGD (about half of SAM), and Hessian-spectrum analysis confirms convergence to significantly flatter minima.

Conclusion: fSGLD provides a theoretically grounded and computationally efficient approach to finding flat minima, offering rigorous explanation for random weight perturbation benefits and achieving state-of-the-art generalization performance.

Abstract: Generalization in deep learning is closely tied to the pursuit of flat minima
in the loss landscape, yet classical Stochastic Gradient Langevin Dynamics
(SGLD) offers no mechanism to bias its dynamics toward such low-curvature
solutions. This work introduces Flatness-Aware Stochastic Gradient Langevin
Dynamics (fSGLD), designed to efficiently and provably seek flat minima in
high-dimensional nonconvex optimization problems. At each iteration, fSGLD uses
the stochastic gradient evaluated at parameters perturbed by isotropic Gaussian
noise, commonly referred to as Random Weight Perturbation (RWP), thereby
optimizing a randomized-smoothing objective that implicitly captures curvature
information. Leveraging these properties, we prove that the invariant measure
of fSGLD stays close to a stationary measure concentrated on the global
minimizers of a loss function regularized by the Hessian trace whenever the
inverse temperature and the scale of random weight perturbation are properly
coupled. This result provides a rigorous theoretical explanation for the
benefits of random weight perturbation. In particular, we establish
non-asymptotic convergence guarantees in Wasserstein distance with the best
known rate and derive an excess-risk bound for the Hessian-trace regularized
objective. Extensive experiments on noisy-label and large-scale vision tasks,
in both training-from-scratch and fine-tuning settings, demonstrate that fSGLD
achieves superior or comparable generalization and robustness to baseline
algorithms while maintaining the computational cost of SGD, about half that of
SAM. Hessian-spectrum analysis further confirms that fSGLD converges to
significantly flatter minima.

</details>


### [274] [Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks](https://arxiv.org/abs/2510.02286)
*Ruohao Guo,Afshin Oroojlooy,Roshan Sridhar,Miguel Ballesteros,Alan Ritter,Dan Roth*

Main category: cs.LG

TL;DR: DialTree-RPO is an RL framework with tree search that autonomously discovers diverse multi-turn attack strategies against LLMs, achieving 25.9% higher attack success rate than previous methods.


<details>
  <summary>Details</summary>
Motivation: Current LLMs remain vulnerable to multi-turn adversarial attacks, with existing methods focusing on single-turn attacks or requiring manual effort, failing to explore the vast space of possible multi-turn attack trajectories.

Method: An on-policy reinforcement learning framework integrated with tree search that treats dialogue as sequential decision-making, enabling systematic exploration without manually curated data.

Result: Achieves more than 25.9% higher Attack Success Rate (ASR) across 10 target models compared to previous state-of-the-art approaches, and uncovers new attack strategies.

Conclusion: The framework effectively discovers diverse multi-turn attack strategies and demonstrates significantly higher vulnerability of LLMs to multi-turn attacks compared to single-turn approaches.

Abstract: Despite recent rapid progress in AI safety, current large language models
remain vulnerable to adversarial attacks in multi-turn interaction settings,
where attackers strategically adapt their prompts across conversation turns and
pose a more critical yet realistic challenge. Existing approaches that discover
safety vulnerabilities either rely on manual red-teaming with human experts or
employ automated methods using pre-defined templates and human-curated attack
data, with most focusing on single-turn attacks. However, these methods did not
explore the vast space of possible multi-turn attacks, failing to consider
novel attack trajectories that emerge from complex dialogue dynamics and
strategic conversation planning. This gap is particularly critical given recent
findings that LLMs exhibit significantly higher vulnerability to multi-turn
attacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy
reinforcement learning framework integrated with tree search that autonomously
discovers diverse multi-turn attack strategies by treating the dialogue as a
sequential decision-making problem, enabling systematic exploration without
manually curated data. Through extensive experiments, our approach not only
achieves more than 25.9% higher ASR across 10 target models compared to
previous state-of-the-art approaches, but also effectively uncovers new attack
strategies by learning optimal dialogue policies that maximize attack success
across multiple turns.

</details>


### [275] [Interactive Training: Feedback-Driven Neural Network Optimization](https://arxiv.org/abs/2510.02297)
*Wentao Zhang,Yang Young Lu,Yuntian Deng*

Main category: cs.LG

TL;DR: Interactive Training is a framework enabling real-time human or AI intervention during neural network training through a control server, allowing dynamic adjustments to hyperparameters, data, and checkpoints.


<details>
  <summary>Details</summary>
Motivation: Traditional neural network training follows fixed optimization recipes without flexibility to respond to instabilities or emerging issues during training.

Method: Uses a control server to mediate communication between users/agents and training process, enabling dynamic adjustment of optimizer hyperparameters, training data, and model checkpoints.

Result: Achieves superior training stability, reduced sensitivity to initial hyperparameters, and improved adaptability to evolving user needs through three case studies.

Conclusion: Paves the way for future training paradigm where AI agents autonomously monitor training logs, proactively resolve instabilities, and optimize training dynamics.

Abstract: Traditional neural network training typically follows fixed, predefined
optimization recipes, lacking the flexibility to dynamically respond to
instabilities or emerging training issues. In this paper, we introduce
Interactive Training, an open-source framework that enables real-time,
feedback-driven intervention during neural network training by human experts or
automated AI agents. At its core, Interactive Training uses a control server to
mediate communication between users or agents and the ongoing training process,
allowing users to dynamically adjust optimizer hyperparameters, training data,
and model checkpoints. Through three case studies, we demonstrate that
Interactive Training achieves superior training stability, reduced sensitivity
to initial hyperparameters, and improved adaptability to evolving user needs,
paving the way toward a future training paradigm where AI agents autonomously
monitor training logs, proactively resolve instabilities, and optimize training
dynamics.

</details>


### [276] [Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling](https://arxiv.org/abs/2510.02206)
*Daniel Gallo Fern√°ndez*

Main category: cs.LG

TL;DR: Poolformer is a sequence-to-sequence model that replaces self-attention with recurrent layers and pooling operations to handle long sequences efficiently, outperforming state-of-the-art models on raw audio data.


<details>
  <summary>Details</summary>
Motivation: Self-attention in sequence-to-sequence models scales quadratically with sequence length, making it impractical for very long sequences. The authors aim to develop a more efficient architecture that can handle long-range dependencies without the computational burden of self-attention.

Method: Poolformer uses recurrent layers instead of self-attention and incorporates pooling operations to reduce sequence length. It's built with SkipBlocks containing residual blocks, down-pooling layers, nested SkipBlocks, up-pooling layers, and additional residual blocks.

Result: Poolformer accelerates training, improves perceptual metrics (FID and IS), prevents overfitting, and outperforms state-of-the-art models like SaShiMi and Mamba on raw audio data. Deep layers handle long-range dependencies while shallow layers manage short-term features.

Conclusion: Poolformer provides an efficient alternative to self-attention-based models for long sequences, with promising results on audio data and potential applications in text, vision, and multi-modal scenarios.

Abstract: Sequence-to-sequence models have become central in Artificial Intelligence,
particularly following the introduction of the transformer architecture. While
initially developed for Natural Language Processing, these models have
demonstrated utility across domains, including Computer Vision. Such models
require mechanisms to exchange information along the time dimension, typically
using recurrent or self-attention layers. However, self-attention scales
quadratically with sequence length, limiting its practicality for very long
sequences.
  We introduce Poolformer, a sequence-to-sequence model that replaces
self-attention with recurrent layers and incorporates pooling operations to
reduce sequence length. Poolformer is defined recursively using SkipBlocks,
which contain residual blocks, a down-pooling layer, a nested SkipBlock, an
up-pooling layer, and additional residual blocks. We conduct extensive
experiments to support our architectural choices.
  Our results show that pooling greatly accelerates training, improves
perceptual metrics (FID and IS), and prevents overfitting. Our experiments also
suggest that long-range dependencies are handled by deep layers, while shallow
layers take care of short-term features.
  Evaluated on raw audio, which naturally features long sequence lengths,
Poolformer outperforms state-of-the-art models such as SaShiMi and Mamba.
Future directions include applications to text and vision, as well as
multi-modal scenarios, where a Poolformer-based LLM could effectively process
dense representations of images and videos.

</details>


### [277] [StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?](https://arxiv.org/abs/2510.02209)
*Yanxu Chen,Zijun Yao,Yantao Liu,Jin Ye,Jianing Yu,Lei Hou,Juanzi Li*

Main category: cs.LG

TL;DR: StockBench is a contamination-free benchmark for evaluating LLM agents in realistic stock trading environments, testing their ability to make sequential buy/sell/hold decisions using daily market data.


<details>
  <summary>Details</summary>
Motivation: Existing financial benchmarks primarily test static knowledge through QA but fail to capture the dynamic, iterative nature of trading. The finance domain remains underexplored for LLM agents despite its economic importance.

Method: Agents receive daily market signals (prices, fundamentals, news) and make sequential trading decisions. Performance is evaluated using financial metrics like cumulative return, maximum drawdown, and Sortino ratio.

Result: Most LLM agents struggle to outperform buy-and-hold baseline, but several models show potential for higher returns and better risk management. Static financial knowledge doesn't necessarily translate to successful trading.

Conclusion: StockBench highlights challenges and opportunities in developing LLM-powered financial agents. The benchmark is released as open-source to support reproducibility and advance future research.

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
as autonomous agents, showing promise in reasoning, tool use, and sequential
decision-making. While prior benchmarks have evaluated LLM agents in domains
such as software engineering and scientific discovery, the finance domain
remains underexplored, despite its direct relevance to economic value and
high-stakes decision-making. Existing financial benchmarks primarily test
static knowledge through question answering, but they fall short of capturing
the dynamic and iterative nature of trading. To address this gap, we introduce
StockBench, a contamination-free benchmark designed to evaluate LLM agents in
realistic, multi-month stock trading environments. Agents receive daily market
signals -- including prices, fundamentals, and news -- and must make sequential
buy, sell, or hold decisions. Performance is assessed using financial metrics
such as cumulative return, maximum drawdown, and the Sortino ratio. Our
evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and
open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM
agents struggle to outperform the simple buy-and-hold baseline, several models
demonstrate the potential to deliver higher returns and manage risk more
effectively. These findings highlight both the challenges and opportunities in
developing LLM-powered financial agents, showing that excelling at static
financial knowledge tasks does not necessarily translate into successful
trading strategies. We release StockBench as an open-source resource to support
reproducibility and advance future research in this domain.

</details>


### [278] [Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is Geometry Adaptive](https://arxiv.org/abs/2510.02305)
*Tyler Farghly,Peter Potaptchik,Samuel Howard,George Deligiannidis,Jakiw Pidstrigach*

Main category: cs.LG

TL;DR: This paper provides evidence that diffusion models' success stems from their ability to adapt to low-dimensional geometric structure in data, showing that smoothing the score function produces tangential smoothing along the data manifold.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind diffusion models' strong generalization capabilities, particularly investigating the conjecture that their success comes from adapting to low-dimensional geometric structure in data.

Method: Theoretical and empirical investigation of implicit regularization through smoothing minimizers of the empirical score matching objective, examining how smoothing the score function affects generalization.

Result: Results confirm that smoothing the score function produces smoothing tangential to the data manifold, and that the manifold along which diffusion models generalize can be controlled by choosing appropriate smoothing.

Conclusion: The study provides evidence supporting the manifold hypothesis for diffusion models, showing that their generalization capabilities are linked to geometric structure adaptation through score function smoothing.

Abstract: Diffusion models have achieved state-of-the-art performance, demonstrating
remarkable generalisation capabilities across diverse domains. However, the
mechanisms underpinning these strong capabilities remain only partially
understood. A leading conjecture, based on the manifold hypothesis, attributes
this success to their ability to adapt to low-dimensional geometric structure
within the data. This work provides evidence for this conjecture, focusing on
how such phenomena could result from the formulation of the learning problem
through score matching. We inspect the role of implicit regularisation by
investigating the effect of smoothing minimisers of the empirical score
matching objective. Our theoretical and empirical results confirm that
smoothing the score function -- or equivalently, smoothing in the log-density
domain -- produces smoothing tangential to the data manifold. In addition, we
show that the manifold along which the diffusion model generalises can be
controlled by choosing an appropriate smoothing.

</details>


### [279] [C2AL: Cohort-Contrastive Auxiliary Learning for Large-scale Recommendation Systems](https://arxiv.org/abs/2510.02215)
*Mertcan Cokbas,Ziteng Liu,Zeyi Tao,Chengkai Zhang,Elder Veliz,Qin Huang,Ellie Wen,Huayu Li,Qiang Jin,Murat Duman,Benjamin Au,Guy Lebanon,Sagar Chordia*

Main category: cs.LG

TL;DR: C2AL addresses data heterogeneity in large-scale recommendation models by using attention mechanisms for shared embedding selection and auxiliary learning with conflicting labels to preserve minority cohort information while improving global performance.


<details>
  <summary>Details</summary>
Motivation: Real-world recommendation data contains heterogeneous user cohorts with distinct distributions, but single global objective training causes models to focus on central patterns while neglecting head and tail regions, leading to inactive attention weights and dead neurons.

Method: Analyze dataset substructures and expose those with strong distributional contrast through auxiliary learning using partially conflicting auxiliary labels to regularize shared representations, customizing attention layer learning to preserve mutual information with minority cohorts.

Result: Evaluation on massive production datasets with billions of data points across six SOTA models showed factorization machines capture fine-grained user-ad interactions, achieving 0.16% reduction in normalized entropy overall and gains exceeding 0.30% on targeted minority cohorts.

Conclusion: The proposed C2AL method effectively addresses data heterogeneity in recommendation systems by leveraging attention mechanisms and auxiliary learning to balance global performance with minority cohort preservation.

Abstract: Training large-scale recommendation models under a single global objective
implicitly assumes homogeneity across user populations. However, real-world
data are composites of heterogeneous cohorts with distinct conditional
distributions. As models increase in scale and complexity and as more data is
used for training, they become dominated by central distribution patterns,
neglecting head and tail regions. This imbalance limits the model's learning
ability and can result in inactive attention weights or dead neurons. In this
paper, we reveal how the attention mechanism can play a key role in
factorization machines for shared embedding selection, and propose to address
this challenge by analyzing the substructures in the dataset and exposing those
with strong distributional contrast through auxiliary learning. Unlike previous
research, which heuristically applies weighted labels or multi-task heads to
mitigate such biases, we leverage partially conflicting auxiliary labels to
regularize the shared representation. This approach customizes the learning
process of attention layers to preserve mutual information with minority
cohorts while improving global performance. We evaluated C2AL on massive
production datasets with billions of data points each for six SOTA models.
Experiments show that the factorization machine is able to capture fine-grained
user-ad interactions using the proposed method, achieving up to a 0.16%
reduction in normalized entropy overall and delivering gains exceeding 0.30% on
targeted minority cohorts.

</details>


### [280] [Diffusion Transformers for Imputation: Statistical Efficiency and Uncertainty Quantification](https://arxiv.org/abs/2510.02216)
*Zeqi Ye,Minshuo Chen*

Main category: cs.LG

TL;DR: This paper provides theoretical analysis of diffusion-based time-series imputation methods, deriving statistical bounds and uncertainty quantification for missing values using conditional diffusion transformers.


<details>
  <summary>Details</summary>
Motivation: Despite empirical success of diffusion-based imputation methods, there's limited theoretical understanding of how well they capture complex spatial-temporal dependencies and quantify uncertainty in missing values.

Method: The authors derive statistical sample complexity bounds using novel approximation theory for conditional score functions with transformers, construct confidence regions for missing values, and propose mixed-masking training strategy.

Result: The analysis reveals that imputation efficiency and accuracy are significantly influenced by missing patterns, and theoretical insights are validated through simulations.

Conclusion: The work bridges theoretical understanding with practical performance of diffusion-based imputation methods, providing statistical guarantees and improved training strategies.

Abstract: Imputation methods play a critical role in enhancing the quality of practical
time-series data, which often suffer from pervasive missing values. Recently,
diffusion-based generative imputation methods have demonstrated remarkable
success compared to autoregressive and conventional statistical approaches.
Despite their empirical success, the theoretical understanding of how well
diffusion-based models capture complex spatial and temporal dependencies
between the missing values and observed ones remains limited. Our work
addresses this gap by investigating the statistical efficiency of conditional
diffusion transformers for imputation and quantifying the uncertainty in
missing values. Specifically, we derive statistical sample complexity bounds
based on a novel approximation theory for conditional score functions using
transformers, and, through this, construct tight confidence regions for missing
values. Our findings also reveal that the efficiency and accuracy of imputation
are significantly influenced by the missing patterns. Furthermore, we validate
these theoretical insights through simulation and propose a mixed-masking
training strategy to enhance the imputation performance.

</details>


### [281] [Efficiently Generating Correlated Sample Paths from Multi-step Time Series Foundation Models](https://arxiv.org/abs/2510.02224)
*Ethan Baron,Boris Oreshkin,Ruijun Ma,Hanyu Zhang,Kari Torkkola,Michael W. Mahoney,Andrew Gordon Wilson,Tatiana Konstantinova*

Main category: cs.LG

TL;DR: A copula-based method to efficiently generate correlated sample paths from multi-step time series foundation models in one forward pass, avoiding expensive autoregressive sampling.


<details>
  <summary>Details</summary>
Motivation: Current time series foundation models only predict independent marginal distributions per time step, lacking joint predictive distributions. Autoregressive sampling for correlated paths is computationally expensive.

Method: Copula-based approach that generates correlated sample paths from existing multi-step time series foundation models in a single forward pass.

Result: Generates correlated sample paths orders of magnitude faster than autoregressive sampling while improving quality by mitigating snowballing error.

Conclusion: The copula-based method provides an efficient and accurate alternative to autoregressive sampling for generating correlated forecast trajectories from time series foundation models.

Abstract: Many time series applications require access to multi-step forecast
trajectories in the form of sample paths. Recently, time series foundation
models have leveraged multi-step lookahead predictions to improve the quality
and efficiency of multi-step forecasts. However, these models only predict
independent marginal distributions for each time step, rather than a full joint
predictive distribution. To generate forecast sample paths with realistic
correlation structures, one typically resorts to autoregressive sampling, which
can be extremely expensive. In this paper, we present a copula-based approach
to efficiently generate accurate, correlated sample paths from existing
multi-step time series foundation models in one forward pass. Our copula-based
approach generates correlated sample paths orders of magnitude faster than
autoregressive sampling, and it yields improved sample path quality by
mitigating the snowballing error phenomenon.

</details>


### [282] [xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity](https://arxiv.org/abs/2510.02228)
*Maximilian Beck,Kajetan Schweighofer,Sebastian B√∂ck,Sebastian Lehner,Sepp Hochreiter*

Main category: cs.LG

TL;DR: xLSTM scales better than Transformers in LLM scenarios, with advantages increasing with longer contexts in both training and inference.


<details>
  <summary>Details</summary>
Motivation: To compare scaling behavior between Transformers and xLSTM architectures, particularly examining how optimal model sizes depend on context length - an aspect previously overlooked.

Method: Comparative investigation using IsoFLOP and parametric fit approaches across model sizes (80M-7B) and training tokens (2B-2T), analyzing compute-optimal/over-training regimes, context length dependence, and inference-time scaling.

Result: xLSTM scales favorably compared to Transformers in typical LLM scenarios, with its advantage widening as training and inference contexts grow larger.

Conclusion: xLSTM's linear complexity with context length provides scaling advantages over Transformers, making it particularly suitable for applications requiring long contexts in both training and inference phases.

Abstract: Scaling laws play a central role in the success of Large Language Models
(LLMs), enabling the prediction of model performance relative to compute
budgets prior to training. While Transformers have been the dominant
architecture, recent alternatives such as xLSTM offer linear complexity with
respect to context length while remaining competitive in the billion-parameter
regime. We conduct a comparative investigation on the scaling behavior of
Transformers and xLSTM along the following lines, providing insights to guide
future model design and deployment. First, we study the scaling behavior for
xLSTM in compute-optimal and over-training regimes using both IsoFLOP and
parametric fit approaches on a wide range of model sizes (80M-7B) and number of
training tokens (2B-2T). Second, we examine the dependence of optimal model
sizes on context length, a pivotal aspect that was largely ignored in previous
work. Finally, we analyze inference-time scaling characteristics. Our findings
reveal that in typical LLM training and inference scenarios, xLSTM scales
favorably compared to Transformers. Importantly, xLSTM's advantage widens as
training and inference contexts grow.

</details>


### [283] [PUL-Inter-slice Defender: An Anomaly Detection Solution for Distributed Slice Mobility Attacks](https://arxiv.org/abs/2510.02236)
*Ricardo Misael Ayala Molina,Hyame Assem Alameddine,Makan Pourzandi,Chadi Assi*

Main category: cs.LG

TL;DR: PUL-Inter-Slice Defender is a machine learning-based anomaly detection system that protects 5G networks from Distributed Slice Mobility (DSM) DDoS attacks using Positive Unlabeled Learning with LSTM Autoencoders and K-Means clustering.


<details>
  <summary>Details</summary>
Motivation: Inter-Slice Switching (ISS) in 5G networks introduces vulnerabilities that can be exploited for Distributed Slice Mobility (DSM) attacks, a form of DDoS attack, requiring robust security solutions.

Method: Uses Positive Unlabeled Learning (PUL) with LSTM Autoencoders and K-Means clustering, leveraging 3GPP KPIs and performance measurement counters to detect DSM attack variants in contaminated training data.

Result: Achieved F1-scores exceeding 98.50% on training datasets with 10% to 40% attack contamination, outperforming Inter-Slice Defender and other PUL-based solutions using OCSVM with Random Forest and XGBoost.

Conclusion: PUL-Inter-Slice Defender provides effective protection against DSM attacks in 5G networks, demonstrating high detection accuracy and robustness even with contaminated training data.

Abstract: Network Slices (NSs) are virtual networks operating over a shared physical
infrastructure, each designed to meet specific application requirements while
maintaining consistent Quality of Service (QoS). In Fifth Generation (5G)
networks, User Equipment (UE) can connect to and seamlessly switch between
multiple NSs to access diverse services. However, this flexibility, known as
Inter-Slice Switching (ISS), introduces a potential vulnerability that can be
exploited to launch Distributed Slice Mobility (DSM) attacks, a form of
Distributed Denial of Service (DDoS) attack. To secure 5G networks and their
NSs against DSM attacks, we present in this work, PUL-Inter-Slice Defender; an
anomaly detection solution that leverages Positive Unlabeled Learning (PUL) and
incorporates a combination of Long Short-Term Memory Autoencoders and K-Means
clustering. PUL-Inter-Slice Defender leverages the Third Generation Partnership
Project (3GPP) key performance indicators and performance measurement counters
as features for its machine learning models to detect DSM attack variants while
maintaining robustness in the presence of contaminated training data. When
evaluated on data collected from our 5G testbed based on the open-source
free5GC and UERANSIM, a UE/ Radio Access Network (RAN) simulator;
PUL-Inter-Slice Defender achieved F1-scores exceeding 98.50% on training
datasets with 10% to 40% attack contamination, consistently outperforming its
counterpart Inter-Slice Defender and other PUL based solutions combining
One-Class Support Vector Machine (OCSVM) with Random Forest and XGBoost.

</details>


### [284] [Drop-Muon: Update Less, Converge Faster](https://arxiv.org/abs/2510.02239)
*Kaja Gruntkowska,Yassine Maziane,Zheng Qu,Peter Richt√°rik*

Main category: cs.LG

TL;DR: Drop-Muon challenges the conventional wisdom of updating all layers at every step in deep learning optimization. It introduces a randomized progressive training method that updates only a subset of layers per step, achieving faster convergence than full-network updates.


<details>
  <summary>Details</summary>
Motivation: The paper challenges the fundamental assumption that all layers should be updated at every optimization step, arguing that full-network updates can be suboptimal both theoretically and practically. This challenges the status quo followed by state-of-the-art optimizers like Muon.

Method: Drop-Muon is a non-Euclidean Randomized Progressive Training framework that updates only a subset of layers per step according to a randomized schedule. It combines progressive training efficiency with layer-specific non-Euclidean updates, with convergence guarantees under layer-wise smoothness conditions.

Result: Empirical results show Drop-Muon consistently outperforms full-network Muon, achieving the same accuracy up to 1.4√ó faster in wall-clock time. Theoretical analysis reveals full-network updates are only optimal under very specific conditions between layer smoothness constants.

Conclusion: The work suggests a paradigm shift in how large-scale models can be efficiently trained, offering a highly efficient, theoretically grounded alternative to full-network updates that challenges conventional deep learning optimization practices.

Abstract: Conventional wisdom in deep learning optimization dictates updating all
layers at every step-a principle followed by all recent state-of-the-art
optimizers such as Muon. In this work, we challenge this assumption, showing
that full-network updates can be fundamentally suboptimal, both in theory and
in practice. We introduce a non-Euclidean Randomized Progressive Training
method-Drop-Muon-a simple yet powerful framework that updates only a subset of
layers per step according to a randomized schedule, combining the efficiency of
progressive training with layer-specific non-Euclidean updates for top-tier
performance. We provide rigorous convergence guarantees under both layer-wise
smoothness and layer-wise $(L^0, L^1)$-smoothness, covering deterministic and
stochastic gradient settings, marking the first such results for progressive
training in the stochastic and non-smooth regime. Our cost analysis further
reveals that full-network updates are not optimal unless a very specific
relationship between layer smoothness constants holds. Through controlled CNN
experiments, we empirically demonstrate that Drop-Muon consistently outperforms
full-network Muon, achieving the same accuracy up to $1.4\times$ faster in
wall-clock time. Together, our results suggest a shift in how large-scale
models can be efficiently trained, challenging the status quo and offering a
highly efficient, theoretically grounded alternative to full-network updates.

</details>


### [285] [Transformers Discover Molecular Structure Without Graph Priors](https://arxiv.org/abs/2510.02259)
*Tobias Kreiman,Yutong Bai,Fadi Atieh,Elizabeth Weaver,Eric Qu,Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: Transformers trained directly on Cartesian coordinates can achieve competitive molecular energy and force predictions without predefined graphs or physical priors, challenging the necessity of graph neural networks' hard-coded inductive biases.


<details>
  <summary>Details</summary>
Motivation: GNNs use hard-coded graphs that limit expressivity due to fixed receptive fields and slow inference. The authors investigate whether pure Transformers without predefined graphs or physical priors can effectively approximate molecular energies and forces.

Method: Train standard Transformers directly on Cartesian coordinates without predefined graphs or physical priors, using a matched training compute budget compared to state-of-the-art equivariant GNNs on the OMol25 dataset.

Result: Transformers achieve competitive energy and force mean absolute errors. They learn physically consistent patterns like attention weights decaying inversely with interatomic distance, and adapt flexibly across molecular environments. Transformers also show predictable improvements with scaling, consistent with empirical scaling laws.

Conclusion: Many favorable properties of GNNs can emerge adaptively in Transformers, challenging the necessity of hard-coded graph inductive biases and pointing toward standardized, scalable architectures for molecular modeling.

Abstract: Graph Neural Networks (GNNs) are the dominant architecture for molecular
machine learning, particularly for molecular property prediction and machine
learning interatomic potentials (MLIPs). GNNs perform message passing on
predefined graphs often induced by a fixed radius cutoff or k-nearest neighbor
scheme. While this design aligns with the locality present in many molecular
tasks, a hard-coded graph can limit expressivity due to the fixed receptive
field and slows down inference with sparse graph operations. In this work, we
investigate whether pure, unmodified Transformers trained directly on Cartesian
coordinates$\unicode{x2013}$without predefined graphs or physical
priors$\unicode{x2013}$can approximate molecular energies and forces. As a
starting point for our analysis, we demonstrate how to train a Transformer to
competitive energy and force mean absolute errors under a matched training
compute budget, relative to a state-of-the-art equivariant GNN on the OMol25
dataset. We discover that the Transformer learns physically consistent
patterns$\unicode{x2013}$such as attention weights that decay inversely with
interatomic distance$\unicode{x2013}$and flexibly adapts them across different
molecular environments due to the absence of hard-coded biases. The use of a
standard Transformer also unlocks predictable improvements with respect to
scaling training resources, consistent with empirical scaling laws observed in
other domains. Our results demonstrate that many favorable properties of GNNs
can emerge adaptively in Transformers, challenging the necessity of hard-coded
graph inductive biases and pointing toward standardized, scalable architectures
for molecular modeling.

</details>


### [286] [Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps](https://arxiv.org/abs/2510.02274)
*Kyoungjun Park,Yifan Yang,Changhan Ge,Lili Qiu,Shiqi Jiang*

Main category: cs.LG

TL;DR: Diffusion^2 is a diffusion-based method that uses 3D point clouds to model RF signal propagation across multiple frequency bands, achieving high accuracy and 27x speedup over existing methods.


<details>
  <summary>Details</summary>
Motivation: RF signals provide valuable environmental insights beyond RGB cameras, but accurate prediction in complex environments is challenging due to signal interactions with obstacles. This is important for wireless diagnosis, deployment, and optimization.

Method: Uses diffusion-based approach with 3D point clouds and introduces RF-3D Encoder to capture RF-related features from 3D geometry. Features undergo multi-scale embedding to simulate RF signal dissemination process.

Result: Achieves accurate RF signal behavior estimation across various frequency bands and environmental conditions with only 1.9 dB error margin and 27x faster than existing methods.

Conclusion: Diffusion^2 represents a significant advancement in RF signal propagation modeling, enabling more accurate and efficient prediction across wide frequency ranges from Wi-Fi to millimeter waves.

Abstract: Modeling radio frequency (RF) signal propagation is essential for
understanding the environment, as RF signals offer valuable insights beyond the
capabilities of RGB cameras, which are limited by the visible-light spectrum,
lens coverage, and occlusions. It is also useful for supporting wireless
diagnosis, deployment, and optimization. However, accurately predicting RF
signals in complex environments remains a challenge due to interactions with
obstacles such as absorption and reflection. We introduce Diffusion^2, a
diffusion-based approach that uses 3D point clouds to model the propagation of
RF signals across a wide range of frequencies, from Wi-Fi to millimeter waves.
To effectively capture RF-related features from 3D data, we present the RF-3D
Encoder, which encapsulates the complexities of 3D geometry along with
signal-specific details. These features undergo multi-scale embedding to
simulate the actual RF signal dissemination process. Our evaluation, based on
synthetic and real-world measurements, demonstrates that Diffusion^2 accurately
estimates the behavior of RF signals in various frequency bands and
environmental conditions, with an error margin of just 1.9 dB and 27x faster
than existing methods, marking a significant advancement in the field. Refer to
https://rfvision-project.github.io/ for more information.

</details>


### [287] [Fine-Grained Urban Traffic Forecasting on Metropolis-Scale Road Networks](https://arxiv.org/abs/2510.02278)
*Fedor Velikonivtsev,Oleg Platonov,Gleb Bazhenov,Liudmila Prokhorenkova*

Main category: cs.LG

TL;DR: This paper introduces new large-scale urban traffic forecasting datasets and a scalable GNN approach that outperforms existing methods on these challenging benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current traffic forecasting benchmarks have limitations including missing road connectivity information, limited road properties, small scale, and focus on intercity highways rather than complex urban networks.

Method: Proposed a GNN approach without dedicated temporal sequence processing modules for better scalability, and released new datasets with nearly 100,000 road segments and rich road features.

Result: The new datasets are 10x larger than existing ones, and the proposed GNN method achieves better scalability and stronger forecasting performance compared to current spatiotemporal models.

Conclusion: The work provides more realistic and challenging benchmarks for traffic forecasting research and demonstrates that simplified GNN architectures can outperform complex spatiotemporal models in terms of both scalability and performance.

Abstract: Traffic forecasting on road networks is a complex task of significant
practical importance that has recently attracted considerable attention from
the machine learning community, with spatiotemporal graph neural networks
(GNNs) becoming the most popular approach. The proper evaluation of traffic
forecasting methods requires realistic datasets, but current publicly available
benchmarks have significant drawbacks, including the absence of information
about road connectivity for road graph construction, limited information about
road properties, and a relatively small number of road segments that falls
short of real-world applications. Further, current datasets mostly contain
information about intercity highways with sparsely located sensors, while city
road networks arguably present a more challenging forecasting task due to much
denser roads and more complex urban traffic patterns. In this work, we provide
a more complete, realistic, and challenging benchmark for traffic forecasting
by releasing datasets representing the road networks of two major cities, with
the largest containing almost 100,000 road segments (more than a 10-fold
increase relative to existing datasets). Our datasets contain rich road
features and provide fine-grained data about both traffic volume and traffic
speed, allowing for building more holistic traffic forecasting systems. We show
that most current implementations of neural spatiotemporal models for traffic
forecasting have problems scaling to datasets of our size. To overcome this
issue, we propose an alternative approach to neural traffic forecasting that
uses a GNN without a dedicated module for temporal sequence processing, thus
achieving much better scalability, while also demonstrating stronger
forecasting performance. We hope our datasets and modeling insights will serve
as a valuable resource for research in traffic forecasting.

</details>


### [288] [Knowledge Distillation Detection for Open-weights Models](https://arxiv.org/abs/2510.02302)
*Qin Shi,Amber Yijia Zheng,Qifan Song,Raymond A. Yeh*

Main category: cs.LG

TL;DR: A framework for detecting knowledge distillation by analyzing student model weights and teacher API outputs, applicable to both classification and generative models.


<details>
  <summary>Details</summary>
Motivation: Address growing concerns about model provenance and unauthorized replication through distillation, where student models may be illegally copied from proprietary teacher models.

Method: Model-agnostic framework combining data-free input synthesis and statistical score computation to detect distillation patterns between student and teacher models.

Result: Significant improvements in detection accuracy: 59.6% on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-image generation over strongest baselines.

Conclusion: The proposed method effectively detects knowledge distillation across diverse architectures and tasks, providing a practical solution for model provenance verification.

Abstract: We propose the task of knowledge distillation detection, which aims to
determine whether a student model has been distilled from a given teacher,
under a practical setting where only the student's weights and the teacher's
API are available. This problem is motivated by growing concerns about model
provenance and unauthorized replication through distillation. To address this
task, we introduce a model-agnostic framework that combines data-free input
synthesis and statistical score computation for detecting distillation. Our
approach is applicable to both classification and generative models.
Experiments on diverse architectures for image classification and text-to-image
generation show that our method improves detection accuracy over the strongest
baselines by 59.6% on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-image
generation. The code is available at
https://github.com/shqii1j/distillation_detection.

</details>


### [289] [Robust Tangent Space Estimation via Laplacian Eigenvector Gradient Orthogonalization](https://arxiv.org/abs/2510.02308)
*Dhruv Kohli,Sawyer J. Robertson,Gal Mishne,Alexander Cloninger*

Main category: cs.LG

TL;DR: LEGO is a spectral method that uses global data structure to estimate tangent spaces by orthogonalizing gradients of low-frequency Laplacian eigenvectors, overcoming LPCA's limitations in high-noise settings.


<details>
  <summary>Details</summary>
Motivation: LPCA struggles with high-noise data due to the trade-off in neighborhood size selection, which requires prior knowledge of geometric and noise characteristics that are often unavailable.

Method: LEGO estimates tangent spaces by orthogonalizing gradients of low-frequency eigenvectors of the graph Laplacian, leveraging global data structure rather than relying solely on local neighborhoods.

Result: LEGO produces significantly more robust tangent space estimates than LPCA under noise, leading to improvements in manifold learning, boundary detection, and local intrinsic dimension estimation.

Conclusion: LEGO provides a theoretically grounded and practically effective approach for robust tangent space estimation in high-noise scenarios by utilizing global spectral information.

Abstract: Estimating the tangent spaces of a data manifold is a fundamental problem in
data analysis. The standard approach, Local Principal Component Analysis
(LPCA), struggles in high-noise settings due to a critical trade-off in
choosing the neighborhood size. Selecting an optimal size requires prior
knowledge of the geometric and noise characteristics of the data that are often
unavailable. In this paper, we propose a spectral method, Laplacian Eigenvector
Gradient Orthogonalization (LEGO), that utilizes the global structure of the
data to guide local tangent space estimation. Instead of relying solely on
local neighborhoods, LEGO estimates the tangent space at each data point by
orthogonalizing the gradients of low-frequency eigenvectors of the graph
Laplacian. We provide two theoretical justifications of our method. First, a
differential geometric analysis on a tubular neighborhood of a manifold shows
that gradients of the low-frequency Laplacian eigenfunctions of the tube align
closely with the manifold's tangent bundle, while an eigenfunction with high
gradient in directions orthogonal to the manifold lie deeper in the spectrum.
Second, a random matrix theoretic analysis also demonstrates that low-frequency
eigenvectors are robust to sub-Gaussian noise. Through comprehensive
experiments, we demonstrate that LEGO yields tangent space estimates that are
significantly more robust to noise than those from LPCA, resulting in marked
improvements in downstream tasks such as manifold learning, boundary detection,
and local intrinsic dimension estimation.

</details>


### [290] [KaVa: Latent Reasoning via Compressed KV-Cache Distillation](https://arxiv.org/abs/2510.02312)
*Anna Kuzina,Maciej Pioro,Paul N. Whatmough,Babak Ehteshami Bejnordi*

Main category: cs.LG

TL;DR: KaVa is a framework that distills knowledge from compressed KV-cache of teacher LLMs into latent-reasoning students via self-distillation, enabling efficient reasoning while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought reasoning in LLMs incurs high computational costs and memory overhead with redundant stylistic artifacts, while latent reasoning lacks effective supervision for complex natural-language reasoning.

Method: Uses self-distillation to transfer knowledge from compressed KV-cache of teacher models to latent-reasoning students, leveraging continuous latent tokens to align stepwise KV trajectories.

Result: Consistently outperforms strong latent baselines, shows smaller degradation from equation-only to natural-language traces, and scales to larger backbones while preserving efficiency.

Conclusion: Compressed KV-cache distillation serves as a scalable supervision signal for latent reasoning, combining CoT-trained teacher accuracy with latent inference efficiency and deployability.

Abstract: Large Language Models (LLMs) excel at multi-step reasoning problems with
explicit chain-of-thought (CoT), but verbose traces incur significant
computational costs and memory overhead, and often carry redundant, stylistic
artifacts. Latent reasoning has emerged as an efficient alternative that
internalizes the thought process, but it suffers from a critical lack of
supervision, limiting its effectiveness on complex, natural-language reasoning
traces. In this work, we propose KaVa, the first framework that bridges this
gap by distilling knowledge directly from a compressed KV-cache of the teacher
into a latent-reasoning student via self-distillation, leveraging the
representational flexibility of continuous latent tokens to align stepwise KV
trajectories. We show that the abstract, unstructured knowledge within
compressed KV-cache, which lacks direct token correspondence, can serve as a
rich supervisory signal for a latent reasoning student. Empirically, the
approach consistently outperforms strong latent baselines, exhibits markedly
smaller degradation from equation-only to natural-language traces, and scales
to larger backbones while preserving efficiency. These results establish
compressed KV-cache distillation as a scalable supervision signal for latent
reasoning, combining the accuracy of CoT-trained teachers with the efficiency
and deployability of latent inference.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [291] [JaneEye: A 12-nm 2K-FPS 18.9-$Œº$J/Frame Event-based Eye Tracking Accelerator](https://arxiv.org/abs/2510.01213)
*Tao Han,Ang Li,Qinyu Chen,Chang Gao*

Main category: eess.SP

TL;DR: JaneEye is an energy-efficient event-based eye-tracking hardware accelerator for XR wearables that uses a lightweight neural network with novel ConvJANET layer, achieving high accuracy with minimal parameters and low latency.


<details>
  <summary>Details</summary>
Motivation: Conventional frame-based eye-tracking systems fail to meet XR requirements for high accuracy, low latency, and energy efficiency. Event cameras offer ultra-high temporal resolution and low power consumption as an alternative.

Method: Developed an ultra-lightweight neural network with novel ConvJANET layer that simplifies ConvLSTM by keeping only forget gate, reducing computational complexity by half. Used custom linear approximations of activation functions and fixed-point quantization for hardware efficiency.

Result: Achieved pixel error of 2.45 on 3ET+ dataset with only 17.6K parameters, up to 1250 Hz event frame rate. ASIC implementation operates at 400 MHz with 0.5 ms latency (2000 FPS) and 18.9 ŒºJ/frame energy efficiency.

Conclusion: JaneEye sets a new benchmark for low-power, high-performance eye-tracking solutions suitable for next-generation XR wearables through software-hardware co-design.

Abstract: Eye tracking has become a key technology for gaze-based interactions in
Extended Reality (XR). However, conventional frame-based eye-tracking systems
often fall short of XR's stringent requirements for high accuracy, low latency,
and energy efficiency. Event cameras present a compelling alternative, offering
ultra-high temporal resolution and low power consumption. In this paper, we
present JaneEye, an energy-efficient event-based eye-tracking hardware
accelerator designed specifically for wearable devices, leveraging sparse,
high-temporal-resolution event data. We introduce an ultra-lightweight neural
network architecture featuring a novel ConvJANET layer, which simplifies the
traditional ConvLSTM by retaining only the forget gate, thereby halving
computational complexity without sacrificing temporal modeling capability. Our
proposed model achieves high accuracy with a pixel error of 2.45 on the 3ET+
dataset, using only 17.6K parameters, with up to 1250 Hz event frame rate. To
further enhance hardware efficiency, we employ custom linear approximations of
activation functions (hardsigmoid and hardtanh) and fixed-point quantization.
Through software-hardware co-design, our 12-nm ASIC implementation operates at
400 MHz, delivering an end-to-end latency of 0.5 ms (equivalent to 2000 Frames
Per Second (FPS)) at an energy efficiency of 18.9 $\mu$J/frame. JaneEye sets a
new benchmark in low-power, high-performance eye-tracking solutions suitable
for integration into next-generation XR wearables.

</details>


### [292] [Satellite Assignment Policy Learning for Coexistence in LEO Networks](https://arxiv.org/abs/2510.01408)
*Jeong Min Kong,Ian P. Roberts*

Main category: eess.SP

TL;DR: The paper proposes a graph structure learning-based algorithm to infer primary satellite assignment policies in LEO satellite systems, achieving 15% better prediction accuracy than baselines.


<details>
  <summary>Details</summary>
Motivation: Secondary LEO satellite systems need to coordinate with primary systems to avoid interference, but primary systems don't disclose their satellite assignment policies publicly, making inference necessary.

Method: An end-to-end graph structure learning-based algorithm that uses limited historical data to learn highest elevation primary satellite assignment policies and map satellite coordinates to assignment decisions.

Result: The method outperforms the best baseline, achieving approximately 15% improvement in prediction accuracy.

Conclusion: The proposed graph learning approach effectively infers primary satellite assignment policies, enabling secondary systems to operate without causing excessive interference to primary users.

Abstract: Unlike in terrestrial cellular networks, certain frequency bands for
low-earth orbit (LEO) satellite systems have thus far been allocated on a
non-exclusive basis. In this context, systems that launch their satellites
earlier (referred to as primary systems) are given spectrum access priority
over those that launch later, known as secondary systems. For a secondary
system to function, it is expected to either coordinate with primary systems or
ensure that it does not cause excessive interference to primary ground users.
Reliably meeting this interference constraint requires real-time knowledge of
the receive beams of primary users, which in turn depends on the primary
satellite-to-primary user associations. However, in practice, primary systems
have thus far not publicly disclosed their satellite assignment policies;
therefore, it becomes essential for secondary systems to develop methods to
infer such policies. Assuming there is limited historical data indicating which
primary satellites have served which primary users, we propose an end-to-end
graph structure learning-based algorithm for learning highest elevation primary
satellite assignment policies, that, upon deployment, can directly map the
primary satellite coordinates into assignment decisions for the primary users.
Simulation results show that our method can outperform the best baseline,
achieving approximately a 15% improvement in prediction accuracy.

</details>


### [293] [Delay-Augmented Stacked Intelligent Surfaces: Potential, Challenges, and Opportunities](https://arxiv.org/abs/2510.01411)
*Hibatallah Alwazani,Omran Abbas,Loic Markley,Anas Chaaban*

Main category: eess.SP

TL;DR: Introduces delay-augmented stacked intelligent surfaces (DA-SIS) that combine spatial wave-domain processing with temporal processing through strategically-tuned delays, enabling applications like analog equalization to eliminate inter-symbol interference.


<details>
  <summary>Details</summary>
Motivation: To extend the utility of stacked intelligent surfaces beyond spatial processing by incorporating temporal processing capabilities through delay units, enabling more comprehensive signal processing for Holographic MIMO and Ultra-massive MIMO systems.

Method: Proposes DA-SIS with strategically-tuned symbol-duration level delays, discusses feasibility of delay units in SIS, and demonstrates application as analog equalizer for ISI elimination. Compares performance with digital equalizers using bit error rate metric.

Result: Demonstrates the potential of DA-SIS in equalization, showing how the number of elements affects the equalization process and comparing favorably with digital equalizers as benchmark.

Conclusion: DA-SIS represents a promising technology that combines spatial and temporal processing, with opportunities for future research to bring this concept to practical implementation.

Abstract: Stacked intelligent surfaces (SIS)s have been proposed recently as an
enabling technology for Holographic Multiple Input Multiple Output (HMIMO) and
Ultra-massive MIMO (umMIMO) technologies. Their utility can extend beyond
spatial wave-domain processing of signals if they are enhanced with
strategically-tuned symbol-duration level delays to enable temporal processing
as well. In this work, we introduce the idea of a delay-augmented SIS (DA-SIS).
We shed light on the feasibility of realizing delay units in an SIS. Then, we
discuss the relevance of the proposed DA-SIS and present a use case that
illustrates its potential, wherein the DA-SIS serves as an analog equalizer
that aids in eliminating multi-path-induced inter-symbol-interference (ISI). We
show how the number of elements affect the equalization process using the bit
error rate (BER) as a metric, and demonstrate the potential of the DA-SIS in
equalization via comparing with digital equalizers as a benchmark. Finally, we
present opportunities and future research directions that can be undertaken to
bring this idea to fruition.

</details>


### [294] [A Drone-mounted Magnetometer System for Automatic Interference Removal and Landmine Detection](https://arxiv.org/abs/2510.01417)
*Alex Paul Hoffmann,Matthew G. Finley,Eftyhia Zesta,Mark B. Moldwin,Lauro V. Ojeda*

Main category: eess.SP

TL;DR: A two-step automated method using UAV-mounted magnetometers for landmine detection that removes drone motor interference and detects landmine signatures efficiently.


<details>
  <summary>Details</summary>
Motivation: Landmines pose persistent dangers to civilians and hinder post-war recovery. Current UAV magnetometer detection faces interference from drone electronics, requiring effective interference removal.

Method: Uses frame-mounted two-magnetometer payload with WAIC-UP method for interference cancellation and RUDE algorithm for landmine signature detection, validated through Monte Carlo simulations.

Result: Achieves high-fidelity ordinance detection with low computational cost and simplified magnetic survey payload design, with performance assessed across varying drone altitudes.

Conclusion: The WAIC-UP/RUDE approach provides an effective solution for automated landmine detection using UAVs while overcoming electronic interference challenges.

Abstract: Landmines have been extensively used in conflict zones as an indiscriminate
weapon to control military movements, often remaining active long after
hostilities have ended. Their presence poses a persistent danger to civilians,
hindering post-war recovery efforts, causing injuries or death, and restricting
access to essential land for agriculture and infrastructure. Unmanned aerial
vehicles (UAV) equipped with magnetometers are commonly used to detect remnant
hidden landmines but come with significant technical challenges due to magnetic
field interference from UAV electronics such as motors. We propose the use of a
frame-mounted UAV-borne two-magnetometer payload to perform a two-step
automated interference removal and landmine detection analysis. The first step
removes interference via the Wavelet-Adaptive Interference Cancellation for
Underdetermined Platform (WAIC-UP) method designed for spaceflight
magnetometers. The second method uses the Rapid Unsupervised Detection of
Events (RUDE) algorithm to detect landmine signatures. This two-step
WAIC-UP/RUDE approach with multiple magnetometers achieves high-fidelity
ordinance detection at a low computational cost and simplifies the design of
magnetic survey payloads. We validate the method through a Monte Carlo
simulation of randomized landmine placements in a 10 x 10 m square grid and
drone motor interference. Additionally, we assess the efficacy of the algorithm
by varying the drone's altitude, examining its performance at different heights
above the ground.

</details>


### [295] [Meta-Learning-Driven Resource Optimization in Full-Duplex ISAC with Movable Antennas](https://arxiv.org/abs/2510.01437)
*Ali Amhaz,Shreya Khisa,Mohamed Elhattab,Chadi Assi,Sanaa Sharafeddine*

Main category: eess.SP

TL;DR: This paper proposes a meta-learning approach to optimize movable antenna positions and beamforming for integrated sensing and communication in full-duplex systems, achieving near-optimal performance.


<details>
  <summary>Details</summary>
Motivation: To enhance integrated sensing and communication (ISAC) performance in full-duplex systems by leveraging movable antennas for improved signal-to-noise and interference ratio of sensing echoes while maintaining communication quality.

Method: A gradient-based meta-learning approach is used to jointly optimize transmit/receive beamforming vectors, uplink power allocation, and movable antenna positions at both base stations, addressing the non-convex and coupled optimization problem.

Result: The proposed method achieves results within 99% of optimal solution and outperforms benchmark approaches, demonstrating significant advantages for practical ISAC applications.

Conclusion: Movable antennas combined with meta-learning optimization provide an effective solution for enhancing ISAC performance in full-duplex systems, offering superior echo detection capabilities while maintaining communication quality.

Abstract: This paper investigates a full-duplex (FD) scenario where a base station (BS)
equipped with movable antennas (MAs) simultaneously provides communication
services to a set of downlink (DL) and uplink (UL) users while also enabling
sensing functionalities for target detection, thereby supporting integrated
sensing and communication (ISAC) technology. Additionally, a receiving BS, also
equipped with MAs (denoted as BS R), is responsible for capturing the reflected
echo. To optimize this setup, we formulate an optimization problem aimed at
maximizing the signal-to-noise and interference ratio (SINR) of the captured
echo. This is achieved by jointly optimizing the transmit beamforming vectors
at the FD BS, the receiving beamforming vectors at both the FD BS and BS R, the
UL users' transmit power, and the MAs' positions at both BSs, all while
satisfying the quality-of-service (QoS) requirements for both sensing and
communication. Given the non-convex nature of the problem and the high coupling
between the variables, we employ a gradient-based meta-learning (GML) approach
tailored for large-scale optimization. Numerical results demonstrate the
effectiveness of the proposed meta-learning approach, achieving results within
99% of the optimal solution. Furthermore, the MA-based scheme outperforms
several benchmark approaches, highlighting its advantages in practical ISAC
applications.

</details>


### [296] [The Analysis and Performance of LODC-OFDM Signal in Nonlinear Rydberg Atomic Sensor](https://arxiv.org/abs/2510.01605)
*Hao Wu,Xinyuan Yao,Rui Ni,Chen Gong*

Main category: eess.SP

TL;DR: Rydberg atomic sensors face challenges with OFDM reception due to unipolar optical interfaces. The paper proposes LODC-OFDM scheme optimized for Rydberg sensing, analyzes nonlinear distortion using Bussgang theorem, and validates with experimental results.


<details>
  <summary>Details</summary>
Motivation: Rydberg atomic sensors are attractive for RF communications but their sequential transduction process and unipolar optical interface restrict conventional OFDM reception, requiring specialized unipolar OFDM schemes.

Method: Established AM-AM characteristics approximation, proposed LODC-OFDM scheme based on DCO-OFDM framework, and used Bussgang theorem to analyze nonlinear distortion with closed-form solutions for Taylor series approximation and ideal pre-distortion cases.

Result: Experimental and theoretical results showed good agreement, validating the proposed LODC-OFDM scheme for broadband OFDM reception in Rydberg-based sensing systems.

Conclusion: The LODC-OFDM scheme effectively addresses the broadband OFDM reception challenge in Rydberg atomic sensors, with theoretical analysis matching experimental performance.

Abstract: Rydberg atomic sensors have been seen as novel radio frequency (RF)
measurements and the high sensitivity to a large range of frequencies makes it
attractive for communications reception. However, the signal sensing process in
Rydberg system involves sequential transduction from electromagnetic waves to
optical signals and finally to electrical signals. The unipolar characteristic
of the optical interface inherently restricts conventional OFDM reception.
Therefore, adopting unipolar OFDM schemes, inspired by optical communication
systems, becomes essential for compatible signal transmission. In this work, we
investigate the amplitude modulation-to-amplitude modulation (AM-AM)
characteristics of Rydberg atomic sensors, establishing an empirical
approximation function. Building on the direct current-biased optical
orthogonal frequency division multiplexing (DCO-OFDM) framework, we propose a
novel local oscillator direct current-biased OFDM (LODC-OFDM) scheme
specifically optimized for Rydberg-based sensing, effectively addressing the
broadband OFDM reception challenge. Then, we adopt Bussgang theorem to analyze
the nonlinear distortion of LODC-OFDM signals and the results in closed-form
solutions are derived for AM/AM curves approximated by Taylor series expansion
and for the ideal pre-distortion case. In real experiments, the experimental
and theoretical results fit well.

</details>


### [297] [SEP Analysis of 1-Bit Quantized SIMO Systems with QPSK over Fading Channels](https://arxiv.org/abs/2510.01707)
*Amila Ravinath,Minhua Ding,Bikshapathi Gouda,Italo Atzeni,Antti T√∂lli*

Main category: eess.SP

TL;DR: Exact SEP analysis for 1-bit quantized SIMO systems with QPSK modulation, deriving diversity and coding gains for both MRC and SC receivers.


<details>
  <summary>Details</summary>
Motivation: Previous studies only partially characterized diversity gain for SC receivers, leaving gaps in understanding 1-bit quantized SIMO system performance.

Method: Novel analytical method to derive exact SEP expression for 1-bit quantized SIMO systems with QPSK modulation and MRC receiver, then extending to SC receivers.

Result: Derived exact analytical SEP expression for MRC systems and quantified diversity/coding gains for both MRC and SC systems with arbitrary number of receive antennas.

Conclusion: Complete characterization of 1-bit quantized SIMO system performance with both MRC and SC receivers, extending prior limited results.

Abstract: The average symbol error probability (SEP) of a 1-bit quantized single-input
multiple-output (SIMO) system is analyzed under Rayleigh fading channels and
quadrature phase-shift keying (QPSK) modulation. Previous studies have
partially characterized the diversity gain for selection combining (SC). In
this paper, leveraging a novel analytical method, an exact analytical SEP
expression is derived for a 1-bit quantized SIMO system employing QPSK
modulation at the transmitter and maximum ratio combining (MRC) at the
receiver. The corresponding diversity and coding gains of a SIMO-MRC system are
also determined. Furthermore, the diversity and coding gains of a 1-bit
quantized SIMO-SC system are quantified for an arbitrary number of receive
antennas, thereby extending and complementing prior results.

</details>


### [298] [NGGAN: Noise Generation GAN Based on the Practical Measurement Dataset for Narrowband Powerline Communications](https://arxiv.org/abs/2510.01850)
*Ying-Ren Chien,Po-Heng Chou,You-Jie Peng,Chun-Yuan Huang,Hen-Wai Tsao,Yu Tsao*

Main category: eess.SP

TL;DR: Proposes NGGAN, a generative adversarial network that learns characteristics of measured NB-PLC noise for data augmentation, outperforming existing mathematical models.


<details>
  <summary>Details</summary>
Motivation: Existing mathematical noise generative models capture only some characteristics of additive noise in narrowband powerline communication systems, limiting impulse noise processing enhancement.

Method: Developed NGGAN using Wasserstein distance loss function, designed input signal length for cyclo-stationary noise generation, and trained on practical NB-PLC noise measurements from commercial modem circuits.

Result: NGGAN generates noise closer to practical measurements than PSCGM and FRESH filter models in both quantitative and qualitative analyses.

Conclusion: GAN-based approach using practical measurements effectively captures complex NB-PLC noise characteristics for improved data augmentation.

Abstract: Capturing comprehensive statistics of nonperiodic asynchronous impulsive
noise is a critical issue in enhancing impulse noise processing for narrowband
powerline communication (NB-PLC) transceivers. However, existing mathematical
noise generative models capture only some of the characteristics of additive
noise. Therefore, we propose a generative adversarial network (GAN), called the
noise-generation GAN (NGGAN), that learns the complicated characteristics of
practically measured noise samples for data augmentation. To closely match the
statistics of complicated noise in NB-PLC systems, we measured the NB-PLC noise
via the analog coupling and bandpass filtering circuits of a commercial NB-PLC
modem to build a realistic dataset. Specifically, the NGGAN design approaches
based on the practically measured dataset are as follows: (i) we design the
length of input signals that the NGGAN model can fit to facilitate
cyclo-stationary noise generation. (ii) Wasserstein distance is used as a loss
function to enhance the similarity between the generated noise and the training
dataset and ensure that the sample diversity is sufficient for various
applications. (iii) To measure the similarity performance of the GAN-based
models based on mathematical and practically measured datasets, we perform
quantitative and qualitative analyses. The training datasets include (1) a
piecewise spectral cyclo-stationary Gaussian model (PSCGM), (2) a
frequency-shift (FRESH) filter, and (3) practical measurements from NB-PLC
systems. Simulation results demonstrate that the proposed NGGAN trained using
waveform characteristics is closer to the practically measured dataset in terms
of the quality of the generated noise.

</details>


### [299] [3D 8-Ary Noise Modulation Using Bayesian- and Kurtosis-based Detectors](https://arxiv.org/abs/2510.01748)
*Hadi Zayyani,Felipe A. P. de Figueiredo,Mohammad Salman,Rausley A. A. de Souza*

Main category: eess.SP

TL;DR: A novel 3D 8-ary noise modulation scheme using Mixture of Gaussian distribution with three dimensions: mean, variance, and mixture probability, achieving higher data rates than existing 2D schemes.


<details>
  <summary>Details</summary>
Motivation: To increase data transmission rates beyond existing two-dimensional noise modulation schemes by introducing a third dimension (mixture probability) to carry additional information.

Method: Proposes 3D modulation using MoG distribution with mean, variance, and probability dimensions. Each symbol carries 3 bits. Uses specialized detectors: threshold-based for mean, ML for variance, and Kurtosis/JB/BHT-based for probability detection.

Result: Achieves 1.5x and 3x higher data rates than Generalized Quadratic and binary KLJN modulators respectively. Kurtosis-based detector achieves acceptable BEP of ~0.06 with low complexity.

Conclusion: The 3D noise modulation scheme successfully increases data rate while maintaining acceptable error performance, with Kurtosis-based detection providing a practical low-complexity solution.

Abstract: This paper presents a novel three-dimensional (3D) 8-ary noise modulation
scheme that introduces a new dimension: the mixture probability of a Mixture of
Gaussian (MoG) distribution. This proposed approach utilizes the dimensions of
mean and variance, in addition to the new probability dimension. Within this
framework, each transmitted symbol carries three bits, each corresponding to a
distinct sub-channel. For detection, a combination of specialized detectors is
employed: a simple threshold based detector for the first sub-channel bit
(modulated by the mean), a Maximum-Likelihood (ML) detector for the second
sub-channel bit (modulated by the variance), a Kurtosis-based, Jarque-Bera (JB)
test, and Bayesian Hypothesis (BHT)-based detectors for the third bit
(modulated by the MoG probability). The Kurtosis- and JB-based detectors
specifically distinguish between Gaussian (or near-Gaussian) and non-Gaussian
MoG distributions by leveraging higher-order statistical measures. The Bit
Error Probabilities (BEPs) are derived for the threshold-, Kurtosis-, and
BHT-based detectors. The optimum threshold for the Kurtosis-based detector is
also derived in a tractable manner. Simulation results demonstrate that a
comparably low BEP is achieved for the third sub-channel bit relative to
existing two-dimensional (2D) schemes. Simultaneously, the proposed scheme
increases the data rate by a factor of 1.5 and 3 compared to the Generalized
Quadratic noise modulator and the classical binary KLJN noise modulator,
respectively. Furthermore, the Kurtosis-based detector offers a low-complexity
solution, achieving an acceptable BEP of approximately 0.06.

</details>


### [300] [Computing on Dirty Paper: Interference-Free Integrated Communication and Computing](https://arxiv.org/abs/2510.02012)
*Kuranage Roche Rayan Ranasinghe,Giuseppe Thadeu Freitas de Abreu,David Gonz√°lez G.,Carlo Fischione*

Main category: eess.SP

TL;DR: Proposes Computing on Dirty Paper, an ICC scheme that integrates communication and AirComp using dirty paper coding principles to achieve asymptotically interference-free transmission.


<details>
  <summary>Details</summary>
Motivation: Inspired by Costa's dirty paper coding to enable simultaneous transmission of discrete data symbols and over-the-air computation over common multiple-access channels.

Method: Uses dirty paper coding principles to precancel computing symbols at transmitters, allowing integration of communication and computation in SIMO setup.

Result: Significantly outperforms state-of-the-art ICC schemes in both bit error rate and mean-squared-error performance for data detection and function computation.

Conclusion: The proposed scheme successfully integrates communication and computation with interference-free transmission, validated through simulations showing superior performance over existing methods.

Abstract: Inspired by Costa's pioneering work on dirty paper coding (DPC), this paper
proposes a novel scheme for integrated communication and computing (ICC), named
Computing on Dirty Paper, whereby the transmission of discrete data symbols for
communication, and over-the-air computation (AirComp) of nomographic functions
can be achieved simultaneously over common multiple-access channels. In
particular, the proposed scheme allows for the integration of communication and
computation in a manner that is asymptotically interference-free, by
precanceling the computing symbols at the transmitters (TXs) using DPC
principles. A simulation-based assessment of the proposed ICC scheme under a
single-input multiple-output (SIMO) setup is also offered, including the
evaluation of performance for data detection, and of mean-squared-error (MSE)
performance for function computation, over a block of symbols. The results
validate the proposed method and demonstrate its ability to significantly
outperform state-of-the-art (SotA) ICC schemes in terms of both bit error rate
(BER) and MSE.

</details>


### [301] [Exactly or Approximately Wasserstein Distributionally Robust Estimation According to Wasserstein Radii Being Small or Large](https://arxiv.org/abs/2510.01763)
*Xiao Ding,Enbin Song,Dunbiao Niu,Zhujun Cao,Qingjiang Shi*

Main category: eess.SP

TL;DR: This paper studies robust estimation under Wasserstein distance constraints in linear models, showing saddle point existence conditions and providing a finite-dimensional approach when saddle points don't exist.


<details>
  <summary>Details</summary>
Motivation: To address robust estimation problems with distributional uncertainty using Wasserstein distance constraints, particularly when saddle points may not exist in the infinite-dimensional minimax formulation.

Method: Formulate as infinite-dimensional nonconvex minimax problem, prove equivalence to finite-dimensional problem, provide verifiable conditions for saddle point existence, and solve finite-dimensional nonconvex minimax problem for linear estimators when saddle points are absent.

Result: Established necessary and sufficient conditions for saddle point existence, showed saddle points may not exist via counterexample, provided robust linear estimators as upper bounds, and validated results numerically.

Conclusion: The paper provides theoretical foundations and practical methods for robust estimation under Wasserstein constraints, with verifiable conditions for saddle point existence and robust linear estimators as fallback solutions.

Abstract: This paper primarily considers the robust estimation problem under
Wasserstein distance constraints on the parameter and noise distributions in
the linear measurement model with additive noise, which can be formulated as an
infinite-dimensional nonconvex minimax problem. We prove that the existence of
a saddle point for this problem is equivalent to that for a finite-dimensional
minimax problem, and give a counterexample demonstrating that the saddle point
may not exist. Motivated by this observation, we present a verifiable necessary
and sufficient condition whose parameters can be derived from a convex problem
and its dual. Additionally, we also introduce a simplified sufficient
condition, which intuitively indicates that when the Wasserstein radii are
small enough, the saddle point always exists. In the absence of the saddle
point, we solve an finite-dimensional nonconvex minimax problem, obtained by
restricting the estimator to be linear. Its optimal value establishes an upper
bound on the robust estimation problem, while its optimal solution yields a
robust linear estimator. Numerical experiments are also provided to validate
our theoretical results.

</details>


### [302] [Joint Jammer Mitigation and Data Detection](https://arxiv.org/abs/2510.02021)
*Gian Marti,Christoph Studer*

Main category: eess.SP

TL;DR: JMD is a novel MIMO jammer mitigation method that jointly estimates jammer interference and detects legitimate data over multiple time slots, eliminating the need for dedicated training phases and enabling mitigation of smart, dynamic jammers.


<details>
  <summary>Details</summary>
Motivation: Existing jammer mitigation methods require dedicated training phases that reduce communication rates and can be evaded by smart jammers that don't transmit during training or use time-varying beamforming.

Method: Proposed Joint jammer Mitigation and data Detection (JMD) paradigm with two algorithms (SANDMAN and MAED) that jointly estimate jammer interference subspace and detect legitimate transmit data over multiple time slots without dedicated training.

Result: Extensive simulations demonstrate JMD's efficacy for jammer mitigation, with SANDMAN and MAED providing different complexity-performance tradeoffs.

Conclusion: JMD successfully addresses limitations of traditional jammer mitigation by removing the need for dedicated training while being effective against smart and dynamic multi-antenna jammers.

Abstract: Multi-antenna (or MIMO) processing is a promising solution to the problem of
jammer mitigation. Existing methods mitigate the jammer based on an estimate of
its spatial signature that is acquired through a dedicated training phase. This
strategy has two main drawbacks: (i) it reduces the communication rate since no
data can be transmitted during the training phase and (ii) it can be evaded by
smart or multi-antenna jammers that do not transmit during the training phase
or that dynamically change their subspace through time-varying beamforming. To
address these drawbacks, we propose Joint jammer Mitigation and data Detection
(JMD), a novel paradigm for MIMO jammer mitigation. The core idea of JMD is to
estimate and remove the jammer interference subspace jointly with detecting the
legitimate transmit data over multiple time slots. Doing so removes the need
for a dedicated and rate-reducing training period while being able to mitigate
smart and dynamic multi-antenna jammers. We provide two JMD-type algorithms,
SANDMAN and MAED, that differ in the way they estimate the channels of the
legitimate transmitters and achieve different complexity-performance tradeoffs.
Extensive simulations demonstrate the efficacy of JMD for jammer mitigation.

</details>


### [303] [Composite Generalized Quadratic Noise Modulation via Signal Addition: Towards Higher Dimensional Noise Modulations](https://arxiv.org/abs/2510.01776)
*Hadi Zayyani,Mohammad Salman,Felipe A. P. de Figueiredo,Rausley A. A. de Souza*

Main category: eess.SP

TL;DR: Proposes a 16-ary noise modulator by superposing two Generalized Quadratic Noise Modulators (GQNM), creating a QAM-like scheme that modulates bits on four means and four variances, achieving better performance than KLJN and GQNM modulators.


<details>
  <summary>Details</summary>
Motivation: To create higher-order noise modulators that resemble classical QAM modulators and achieve better performance than existing KLJN and GQNM modulators.

Method: Superpose two GQNM modulators by adding their outputs to create a 16-ary noise modulator that modulates information on four different means and four different variances, with parameter selection based on theoretical distinguishability conditions.

Result: Simulations verify better performance compared to KLJN and GQNM modulators, achieving smaller Bit Error Probability (BEP) through increased complexity in modulator, transmitter, and receiver detectors.

Conclusion: The proposed superposition method successfully creates a 16-ary noise modulator with improved performance, and can be extended to higher-order modulations by adding more modulators, though this is left for future work.

Abstract: This letter proposes superposing two Generalized Quadratic Noise Modulators
(GQNM) by simply adding their outputs. It creates a 16-ary noise modulator that
resembles QAM modulators in classical communication. It modulates the
information bits on four different means and four different variances. It could
also be applied to reach higher-order modulations than 16-ary schemes by adding
the outputs of more than two modulators, which is not discussed in detail in
this letter and left for future work. By selecting the parameters necessary for
satisfying the theoretical distinguishability conditions provided in the paper,
we can reach better performances in comparison to the Kirchhoff-Law Johnson
Noise (KLJN) modulator and the GQNM modulator, which is verified by the
simulations. The better result in terms of smaller Bit Error Probability (BEP)
is achieved by increasing the complexity in the modulator, the transmitter, and
the detectors in the receiver.

</details>


### [304] [Closed-form Single UAV-aided Emitter Localization and Trajectory Design Using Doppler and TOA Measurements](https://arxiv.org/abs/2510.01778)
*Samaneh Motie,Hadi Zayyani,Mohammad Salman,Hasan Abu Hilal*

Main category: eess.SP

TL;DR: A UAV-aided localization algorithm using Doppler and ToA measurements with closed-form solution and trajectory design.


<details>
  <summary>Details</summary>
Motivation: Existing Doppler-based localization algorithms use non-convex functions, making optimization difficult. Combining Doppler with ToA measurements enables convex optimization.

Method: Use both Doppler and Time of Arrival measurements in a Least-Square cost function, transforming it into a quadratic convex function. Apply constrained LS optimization to obtain closed-form solution for emitter location. Also design UAV trajectory with closed-form solution.

Result: Simulation experiments show the proposed algorithm is more effective than existing methods in the literature.

Conclusion: The proposed algorithm successfully combines Doppler and ToA measurements to achieve convex optimization with closed-form solutions for both localization and trajectory design, outperforming existing approaches.

Abstract: In this paper, a single Unmanned-Aerial-Vehicle (UAV)-aided localization
algorithm which uses both Doppler and Time of Arrival (ToA) measurements is
presented. In contrast to Doppler-based localization algorithms which are based
on non-convex functions, exploiting ToA measurements in a Least-Square (LS)
Doppler-based cost function, leads to a quadratic convex function whose
minimizer lies on a line. Utilizing the ToA measurements in addition to the
linear equation of minimizer, a closed form solution is obtained for the
emitter location using a constrained LS optimization. In addition, a trajectory
design of the UAV is provided which has also closed-form solution. Simulation
experiments demonstrate the effectiveness of the proposed algorithm in
comparison to some others in the literature.

</details>


### [305] [Performance Optimization for Movable Antenna Enhanced MISO-OFDM Systems](https://arxiv.org/abs/2510.01789)
*Ruixi Feng,Weidong Mei,Lele Lu,Xin Wei,Zhi Chen,Zhen Gao,Boyu Ning*

Main category: eess.SP

TL;DR: This paper proposes efficient algorithms for optimizing movable antenna positions in OFDM systems to enhance wireless performance across multiple subcarriers.


<details>
  <summary>Details</summary>
Motivation: Movable antenna technology can improve wireless channels by adjusting antenna positions, but existing works focus on narrowband systems. This paper addresses the challenge of optimizing MA positions for OFDM systems where antenna positioning must accommodate frequency-flat conditions across different subcarriers.

Method: The authors discretize the movement region into sampling points, converting the continuous optimization into a discrete selection problem. They develop a partial enumeration algorithm using branch-and-bound with graph-theoretic pruning, and a simplified graph-based algorithm for low SNR regimes.

Result: Simulation results show the proposed algorithm outperforms conventional fixed-position antennas, and narrowband-based optimization achieves near-optimal performance.

Conclusion: The proposed algorithms effectively solve the MA position optimization problem for OFDM systems, demonstrating significant performance improvements over fixed antennas while maintaining computational efficiency.

Abstract: Movable antenna (MA) technology offers a flexible approach to enhancing
wireless channel conditions by adjusting antenna positions within a designated
region. While most existing works focus on narrowband MA systems, this paper
investigates MA position optimization for an MA-enhanced multiple-input
single-output (MISO) orthogonal frequency-division multiplexing (OFDM) system.
This problem appears to be particularly challenging due to the frequency-flat
nature of MA positioning, which should accommodate the channel conditions
across different subcarriers. To overcome this challenge, we discretize the
movement region into a multitude of sampling points, thereby converting the
continuous position optimization problem into a discrete point selection
problem. Although this problem is combinatorial, we develop an efficient
partial enumeration algorithm to find the optimal solution using a
branch-and-bound framework, where a graph-theoretic method is incorporated to
effectively prune suboptimal solutions. In the low signal-to-noise ratio (SNR)
regime, a simplified graph-based algorithm is also proposed to obtain the
optimal MA positions without the need for enumeration. Simulation results
reveal that the proposed algorithm outperforms conventional fixed-position
antennas (FPAs), while narrowband-based antenna position optimization can
achieve near-optimal performance.

</details>


### [306] [Wearable and Ultra-Low-Power Fusion of EMG and A-Mode US for Hand-Wrist Kinematic Tracking](https://arxiv.org/abs/2510.02000)
*Giusy Spacone,Sebastian Frey,Mattia Orlandi,Pierangelo Maria Rapa,Victor Kartsch,Simone Benatti,Luca Benini,Andrea Cossettini*

Main category: eess.SP

TL;DR: An ultra-low-power system for continuous hand and wrist tracking using EMG-US fusion, achieving better accuracy than individual modalities under realistic conditions.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of prior power-hungry systems limited to discrete gesture classification, enabling robust continuous hand tracking for intuitive human-machine interaction.

Method: Developed ultra-low-power (sub-50 mW) system with 8-channel EMG and 4-channel A-mode US signals in wearable armbands, using lightweight encoder-decoder architectures with multi-task learning for 23 DoF tracking.

Result: EMG-US fusion achieved RMSE of 10.6¬∞¬±2.0¬∞ (vs 12.0¬∞¬±1¬∞ for EMG, 13.1¬∞¬±2.6¬∞ for US) and R¬≤ score of 0.61¬±0.1 (vs 0.54¬±0.03 for EMG, 0.38¬±0.20 for US) under realistic sensor repositioning.

Conclusion: EMG-US fusion provides superior continuous hand tracking performance compared to individual modalities, enabling robust wearable systems for natural human-machine interaction.

Abstract: Hand gesture recognition based on biosignals has shown strong potential for
developing intuitive human-machine interaction strategies that closely mimic
natural human behavior. In particular, sensor fusion approaches have gained
attention for combining complementary information and overcoming the
limitations of individual sensing modalities, thereby enabling more robust and
reliable systems. Among them, the fusion of surface electromyography (EMG) and
A-mode ultrasound (US) is very promising. However, prior solutions rely on
power-hungry platforms unsuitable for multi-day use and are limited to discrete
gesture classification. In this work, we present an ultra-low-power (sub-50 mW)
system for concurrent acquisition of 8-channel EMG and 4-channel A-mode US
signals, integrating two state-of-the-art platforms into fully wearable,
dry-contact armbands. We propose a framework for continuous tracking of 23
degrees of freedom (DoFs), 20 for the hand and 3 for the wrist, using a
kinematic glove for ground-truth labeling. Our method employs lightweight
encoder-decoder architectures with multi-task learning to simultaneously
estimate hand and wrist joint angles. Experimental results under realistic
sensor repositioning conditions demonstrate that EMG-US fusion achieves a root
mean squared error of $10.6^\circ\pm2.0^\circ$, compared to
$12.0^\circ\pm1^\circ$ for EMG and $13.1^\circ\pm2.6^\circ$ for US, and a R$^2$
score of $0.61\pm0.1$, with $0.54\pm0.03$ for EMG and $0.38\pm0.20$ for US.

</details>


### [307] [A Secure Affine Frequency Division Multiplexing for Wireless Communication Systems](https://arxiv.org/abs/2510.02023)
*Ping Wang,Zulin Wang,Yuanhan Ni,Qu Luo,Yuanfang Ma,Xiaosi Tian,Pei Xiao*

Main category: eess.SP

TL;DR: SE-AFDM enhances physical-layer security by dynamically varying AFDM pre-chirp parameters using LPPN sequences, providing parameter-domain spreading for security while maintaining reliability and spectrum efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve security in high-mobility scenarios where AFDM shows superior performance, by leveraging the multiple waveform parameters of AFDM for enhanced physical-layer security.

Method: Dynamic variation of AFDM pre-chirp parameter using codebook controlled by LPPN sequence; parameter-domain spreading approach; synchronization framework for time-varying parameter in fast time-varying channels.

Result: Theoretical proof that unsynchronized eavesdroppers cannot eliminate nonlinear impact of time-varying parameter; simulation shows security advantages in high-mobility scenarios; hardware prototype validates synchronization framework effectiveness.

Conclusion: SE-AFDM successfully enhances physical-layer security through dynamic parameter variation while maintaining system reliability and efficiency, with proven security advantages in high-mobility environments.

Abstract: Affine frequency division multiplexing (AFDM) has garnered significant
attention due to its superior performance in high-mobility scenarios, coupled
with multiple waveform parameters that provide greater degrees of freedom for
system design. This paper introduces a novel secure affine frequency division
multiplexing (SE-AFDM) system, which advances prior designs by dynamically
varying an AFDM pre-chirp parameter to enhance physical-layer security. In the
SE-AFDM system, the pre-chirp parameter is dynamically generated from a
codebook controlled by a long-period pseudo-noise (LPPN) sequence. Instead of
applying spreading in the data domain, our parameter-domain spreading approach
provides additional security while maintaining reliability and high spectrum
efficiency. We also propose a synchronization framework to solve the problem of
reliably and rapidly synchronizing the time-varying parameter in fast
time-varying channels. The theoretical derivations prove that unsynchronized
eavesdroppers cannot eliminate the nonlinear impact of the time-varying
parameter and further provide useful guidance for codebook design. Simulation
results demonstrate the security advantages of the proposed SE-AFDM system in
high-mobility scenarios, while our hardware prototype validates the
effectiveness of the proposed synchronization framework.

</details>


### [308] [Joint DOA and Attitude Sensing Based on Tri-Polarized Continuous Aperture Array](https://arxiv.org/abs/2510.02029)
*Haonan Si,Zhaolin Wang,Xiansheng Guo,Jin Zhang,Yuanwei Liu*

Main category: eess.SP

TL;DR: This paper proposes a framework for joint direction-of-arrival (DOA) and attitude sensing using tri-polarized continuous aperture arrays, developing novel transformation techniques and exploiting tri-polarized signal covariances to enhance estimation performance.


<details>
  <summary>Details</summary>
Motivation: To enable accurate joint DOA and attitude estimation by leveraging the spatial continuity and polarization diversity of tri-polarized continuous aperture arrays, addressing limitations in traditional discrete array approaches.

Method: Uses electromagnetic information theory to model spatially continuous received signals, develops equivalent continuous-discrete transformation for subspace decomposition, exploits tri-polarized signal covariances to construct enhanced spectrum, and proposes two attitude estimation algorithms based on prior knowledge availability.

Result: Numerical results demonstrate the framework's feasibility and superiority, showing enhanced DOA estimation performance and successful attitude estimation under different prior knowledge conditions.

Conclusion: The proposed tri-polarized CAPA framework effectively enables joint DOA and attitude sensing, with theoretical analyses confirming attitude identifiability depends on prior target snapshots, and practical algorithms developed for both partial and full attitude estimation scenarios.

Abstract: This paper investigates joint direction-of-arrival (DOA) and attitude sensing
using tri-polarized continuous aperture arrays (CAPAs). By employing
electromagnetic (EM) information theory, the spatially continuous received
signals in tri-polarized CAPA are modeled, thereby enabling accurate DOA and
attitude estimation. To facilitate subspace decomposition for continuous
operators, an equivalent continuous-discrete transformation technique is
developed. Moreover, both self- and cross-covariances of tri-polarized signals
are exploited to construct a tri-polarized spectrum, significantly enhancing
DOA estimation performance. Theoretical analyses reveal that the
identifiability of attitude information fundamentally depends on the
availability of prior target snapshots. Accordingly, two attitude estimation
algorithms are proposed: one capable of estimating partial attitude information
without prior knowledge, and the other achieving full attitude estimation when
such knowledge is available. Numerical results demonstrate the feasibility and
superiority of the proposed framework.

</details>


### [309] [Sensing-Secure ISAC: Ambiguity Function Engineering for Impairing Unauthorized Sensing](https://arxiv.org/abs/2510.02103)
*Kawon Han,Kaitao Meng,Christos Masouros*

Main category: eess.SP

TL;DR: A sensing-secure ISAC framework that protects legitimate sensing from eavesdroppers by introducing artificial imperfections in ambiguity functions, using OFDM subcarrier power allocation to mislead unauthorized sensing while maintaining legitimate performance.


<details>
  <summary>Details</summary>
Motivation: Integrated sensing and communication (ISAC) systems are vulnerable to unauthorized passive radar eavesdroppers who can silently extract sensory information from target-reflected signals without prior knowledge.

Method: Introduce artificial imperfections into ISAC signal ambiguity functions using structured OFDM subcarrier power allocation to create artificial targets that mislead eavesdroppers' range estimation, while legitimate receivers use mismatched filtering to suppress artifacts.

Result: The proposed framework successfully degrades Eve's target estimation performance while preserving Alice's legitimate sensing performance, with numerical results validating effectiveness.

Conclusion: The sensing-secure ISAC signaling provides a viable solution to protect authorized sensing from eavesdroppers, with demonstrated trade-offs between communication, legitimate sensing, and sensing security that can be optimized through convex optimization.

Abstract: The deployment of integrated sensing and communication (ISAC) brings along
unprecedented vulnerabilities to authorized sensing, necessitating the
development of secure solutions. Sensing parameters are embedded within the
target-reflected signal leaked to unauthorized passive radar sensing
eavesdroppers (Eve), implying that they can silently extract sensory
information without prior knowledge of the information data. To overcome this
limitation, we propose a sensing-secure ISAC framework that ensures secure
target detection and estimation for the legitimate system, while obfuscating
unauthorized sensing without requiring any prior knowledge of Eve. By
introducing artificial imperfections into the ambiguity function (AF) of ISAC
signals, we introduce artificial targets into Eve's range profile which
increase its range estimation ambiguity. In contrast, the legitimate sensing
receiver (Alice) can suppress these AF artifacts using mismatched filtering,
albeit at the expense of signal-to-noise ratio (SNR) loss. Employing an OFDM
signal, a structured subcarrier power allocation scheme is designed to shape
the secure autocorrelation function (ACF), inserting periodic peaks to mislead
Eve's range estimation and degrade target detection performance. To quantify
the sensing security, we introduce peak sidelobe level (PSL) and integrated
sidelobe level (ISL) as key performance metrics. Then, we analyze the three-way
trade-offs between communication, legitimate sensing, and sensing security,
highlighting the impact of the proposed sensing-secure ISAC signaling on system
performance. We formulate a convex optimization problem to maximize ISAC
performance while guaranteeing a certain sensing security level. Numerical
results validate the effectiveness of the proposed sensing-secure ISAC
signaling, demonstrating its ability to degrade Eve's target estimation while
preserving Alice's performance.

</details>


### [310] [Unlocking Symbol-Level Precoding Efficiency Through Tensor Equivariant Neural Network](https://arxiv.org/abs/2510.02108)
*Jinshuo Zhang,Yafei Wang,Xinping Yi,Wenjin Wang,Shi Jin,Symeon Chatzinotas,Bj√∂rn Ottersten*

Main category: eess.SP

TL;DR: This paper proposes an end-to-end deep learning framework for symbol-level precoding that achieves near-optimal performance with 80x speedup over conventional methods, leveraging tensor equivariance and attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Symbol-level precoding based on constructive interference offers performance gains but suffers from high computational complexity, which limits practical implementation.

Method: The authors develop a deep learning framework that leverages the structure of optimal SLP solutions, tensor equivariance properties, and attention-based modules to achieve linear computational complexity while maintaining performance.

Result: Simulation results show the proposed framework captures substantial performance gains of optimal SLP while achieving approximately 80-times speedup over conventional methods and maintaining strong generalization across different user numbers and symbol block lengths.

Conclusion: The proposed end-to-end deep learning framework successfully addresses the complexity bottleneck of symbol-level precoding while preserving performance advantages, making it suitable for practical wireless communication systems.

Abstract: Although symbol-level precoding (SLP) based on constructive interference (CI)
exploitation offers performance gains, its high complexity remains a
bottleneck. This paper addresses this challenge with an end-to-end deep
learning (DL) framework with low inference complexity that leverages the
structure of the optimal SLP solution in the closed-form and its inherent
tensor equivariance (TE), where TE denotes that a permutation of the input
induces the corresponding permutation of the output. Building upon the
computationally efficient model-based formulations, as well as their known
closed-form solutions, we analyze their relationship with linear precoding (LP)
and investigate the corresponding optimality condition. We then construct a
mapping from the problem formulation to the solution and prove its TE, based on
which the designed networks reveal a specific parameter-sharing pattern that
delivers low computational complexity and strong generalization. Leveraging
these, we propose the backbone of the framework with an attention-based TE
module, achieving linear computational complexity. Furthermore, we demonstrate
that such a framework is also applicable to imperfect CSI scenarios, where we
design a TE-based network to map the CSI, statistics, and symbols to auxiliary
variables. Simulation results show that the proposed framework captures
substantial performance gains of optimal SLP, while achieving an approximately
80-times speedup over conventional methods and maintaining strong
generalization across user numbers and symbol block lengths.

</details>
