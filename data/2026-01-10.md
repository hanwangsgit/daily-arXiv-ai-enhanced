<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 19]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Autonomous Reasoning for Spacecraft Control: A Large Language Model Framework with Group Relative Policy Optimization](https://arxiv.org/abs/2601.04334)
*Amit Jain,Richard Linares*

Main category: cs.RO

TL;DR: LLM+GRPO framework for autonomous control: SFT learns formatting/control primitives, then GRPO optimizes policies through interaction, enabling reasoning-based control with human-readable explanations across linear/nonlinear systems.


<details>
  <summary>Details</summary>
Motivation: To develop autonomous control systems that combine reasoning capabilities of LLMs with reinforcement learning optimization, creating controllers that can handle complex dynamics while providing explainable decision-making.

Method: Two-stage approach: 1) Supervised Fine-Tuning (SFT) to learn formatting and control primitives, 2) Group Relative Policy Optimization (GRPO) for interaction-driven policy improvement. Applied to four control problems with varying dynamical complexity.

Result: LLM with explicit reasoning optimized via GRPO can synthesize feasible stabilizing policies across linear and nonlinear systems. Framework enables control sequence generation with human-readable explanations of decision-making.

Conclusion: Establishes foundation for applying GRPO-based reasoning to autonomous control systems, with potential applications in aerospace and safety-critical domains where explainable AI is crucial.

Abstract: This paper presents a learning-based guidance-and-control approach that couples a reasoning-enabled Large Language Model (LLM) with Group Relative Policy Optimization (GRPO). A two-stage procedure consisting of Supervised Fine-Tuning (SFT) to learn formatting and control primitives, followed by GRPO for interaction-driven policy improvement, trains controllers for each environment. The framework is demonstrated on four control problems spanning a gradient of dynamical complexity, from canonical linear systems through nonlinear oscillatory dynamics to three-dimensional spacecraft attitude control with gyroscopic coupling and thrust constraints. Results demonstrate that an LLM with explicit reasoning, optimized via GRPO, can synthesize feasible stabilizing policies under consistent training settings across both linear and nonlinear systems. The two-stage training methodology enables models to generate control sequences while providing human-readable explanations of their decision-making process. This work establishes a foundation for applying GRPO-based reasoning to autonomous control systems, with potential applications in aerospace and other safety-critical domains.

</details>


### [2] [UNIC: Learning Unified Multimodal Extrinsic Contact Estimation](https://arxiv.org/abs/2601.04356)
*Zhengtong Xu,Yuki Shirai*

Main category: cs.RO

TL;DR: UNIC is a unified multimodal framework for extrinsic contact estimation that works without prior knowledge or camera calibration, using visual, proprioceptive, and tactile data to estimate object-environment interactions for contact-rich manipulation.


<details>
  <summary>Details</summary>
Motivation: Existing contact estimation approaches rely on restrictive assumptions like predefined contact types, fixed grasp configurations, or camera calibration, which limit generalization to novel objects and deployment in unstructured environments.

Method: UNIC directly encodes visual observations in camera frame and integrates them with proprioceptive and tactile modalities in a fully data-driven manner. It uses a unified contact representation based on scene affordance maps and employs multimodal fusion with random masking for robust representation learning.

Result: UNIC achieves 9.6 mm average Chamfer distance error on unseen contact locations, performs well on unseen objects, remains robust under missing modalities, and adapts to dynamic camera viewpoints.

Conclusion: The results establish extrinsic contact estimation as a practical and versatile capability for contact-rich manipulation, enabling reliable estimation of object-environment interactions without restrictive assumptions.

Abstract: Contact-rich manipulation requires reliable estimation of extrinsic contacts-the interactions between a grasped object and its environment which provide essential contextual information for planning, control, and policy learning. However, existing approaches often rely on restrictive assumptions, such as predefined contact types, fixed grasp configurations, or camera calibration, that hinder generalization to novel objects and deployment in unstructured environments. In this paper, we present UNIC, a unified multimodal framework for extrinsic contact estimation that operates without any prior knowledge or camera calibration. UNIC directly encodes visual observations in the camera frame and integrates them with proprioceptive and tactile modalities in a fully data-driven manner. It introduces a unified contact representation based on scene affordance maps that captures diverse contact formations and employs a multimodal fusion mechanism with random masking, enabling robust multimodal representation learning. Extensive experiments demonstrate that UNIC performs reliably. It achieves a 9.6 mm average Chamfer distance error on unseen contact locations, performs well on unseen objects, remains robust under missing modalities, and adapts to dynamic camera viewpoints. These results establish extrinsic contact estimation as a practical and versatile capability for contact-rich manipulation.

</details>


### [3] [Transformer-based Multi-agent Reinforcement Learning for Separation Assurance in Structured and Unstructured Airspaces](https://arxiv.org/abs/2601.04401)
*Arsyi Aziz,Peng Wei*

Main category: cs.RO

TL;DR: MARL with relative polar state space and transformer encoder outperforms conventional optimization for AAM separation assurance, achieving near-zero collision rates with better generalization across airspace configurations.


<details>
  <summary>Details</summary>
Motivation: Conventional optimization-based metering lacks flexibility for stochastic AAM operations, while current MARL approaches overfit to specific airspace structures and lack adaptability to new configurations.

Method: Recast MARL problem in relative polar state space, train transformer encoder model across diverse traffic patterns and intersection angles to provide speed advisories for conflict resolution while maintaining cruising speeds.

Result: Single-layer encoder configuration outperformed deeper variants (2-3 layers), achieving near-zero near mid-air collision rates and shorter loss-of-separation infringements, and outperformed pure attention baseline model.

Conclusion: The novel state representation, neural network architecture design, and training strategy provide an adaptable, scalable decentralized solution for aircraft separation assurance in both structured and unstructured airspaces.

Abstract: Conventional optimization-based metering depends on strict adherence to precomputed schedules, which limits the flexibility required for the stochastic operations of Advanced Air Mobility (AAM). In contrast, multi-agent reinforcement learning (MARL) offers a decentralized, adaptive framework that can better handle uncertainty, required for safe aircraft separation assurance. Despite this advantage, current MARL approaches often overfit to specific airspace structures, limiting their adaptability to new configurations. To improve generalization, we recast the MARL problem in a relative polar state space and train a transformer encoder model across diverse traffic patterns and intersection angles. The learned model provides speed advisories to resolve conflicts while maintaining aircraft near their desired cruising speeds. In our experiments, we evaluated encoder depths of 1, 2, and 3 layers in both structured and unstructured airspaces, and found that a single encoder configuration outperformed deeper variants, yielding near-zero near mid-air collision rates and shorter loss-of-separation infringements than the deeper configurations. Additionally, we showed that the same configuration outperforms a baseline model designed purely with attention. Together, our results suggest that the newly formulated state representation, novel design of neural network architecture, and proposed training strategy provide an adaptable and scalable decentralized solution for aircraft separation assurance in both structured and unstructured airspaces.

</details>


### [4] [Fast Continuum Robot Shape and External Load State Estimation on SE(3)](https://arxiv.org/abs/2601.04493)
*James M. Ferguson,Alan Kuntz,Tucker Hermans*

Main category: cs.RO

TL;DR: A general spacetime estimation framework for continuum robots that incorporates actuation, loads, and uncertainty models using factor graph optimization.


<details>
  <summary>Details</summary>
Motivation: Previous on-manifold approaches use simplified Cosserat rod models that cannot account for actuation inputs or external loads, limiting practical applicability.

Method: Introduces a general framework with uncertainty models for actuation, forces, process noise, boundary conditions, and measurements. Adds temporal priors for joint spatial-temporal estimation. Discretizes arclength domain into factor graph representation for batch sparse nonlinear optimization.

Result: Demonstrated on tendon-driven robots in simulation (real-time kinematics with uncertainty, tip force sensing, distributed load estimation) and a surgical concentric tube robot in experiment (accurate kinematics and tip force estimation for surgical palpation).

Conclusion: The framework enables full spacetime state estimation for continuum robots, is general enough for broad applications, and shows promise for surgical applications like palpation through accurate force estimation.

Abstract: Previous on-manifold approaches to continuum robot state estimation have typically adopted simplified Cosserat rod models, which cannot directly account for actuation inputs or external loads. We introduce a general framework that incorporates uncertainty models for actuation (e.g., tendon tensions), applied forces and moments, process noise, boundary conditions, and arbitrary backbone measurements. By adding temporal priors across time steps, our method additionally performs joint estimation in both the spatial (arclength) and temporal domains, enabling full \textit{spacetime} state estimation. Discretizing the arclength domain yields a factor graph representation of the continuum robot model, which can be exploited for fast batch sparse nonlinear optimization in the style of SLAM. The framework is general and applies to a broad class of continuum robots; as illustrative cases, we show (i) tendon-driven robots in simulation, where we demonstrate real-time kinematics with uncertainty, tip force sensing from position feedback, and distributed load estimation from backbone strain, and (ii) a surgical concentric tube robot in experiment, where we validate accurate kinematics and tip force estimation, highlighting potential for surgical palpation.

</details>


### [5] [Multiagent Reinforcement Learning with Neighbor Action Estimation](https://arxiv.org/abs/2601.04511)
*Zhenglong Luo,Zhiyong Chen,Aoxiang Liu*

Main category: cs.RO

TL;DR: Enhanced MARL framework using action estimation networks to infer neighboring agents' behaviors without explicit action exchange, validated in dual-arm robotic manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing MARL approaches require explicit action exchange between agents, which is impractical in real-world engineering due to communication constraints, latency, energy consumption, and reliability requirements.

Method: Proposes a MARL framework with action estimation neural networks that infer agent behaviors using only locally observable information. Integrates lightweight action estimation module compatible with standard TD3 algorithms, enabling collaborative policy learning without explicit action sharing.

Result: Framework implemented and validated in dual-arm robotic manipulation tasks (collaborative object lifting). Experimental results show significant enhancement in robustness and deployment feasibility while reducing dependence on information infrastructure.

Conclusion: Advances development of decentralized multiagent AI systems and enables effective AI operation in dynamic, information-constrained real-world environments.

Abstract: Multiagent reinforcement learning, as a prominent intelligent paradigm, enables collaborative decision-making within complex systems. However, existing approaches often rely on explicit action exchange between agents to evaluate action value functions, which is frequently impractical in real-world engineering environments due to communication constraints, latency, energy consumption, and reliability requirements. From an artificial intelligence perspective, this paper proposes an enhanced multiagent reinforcement learning framework that employs action estimation neural networks to infer agent behaviors. By integrating a lightweight action estimation module, each agent infers neighboring agents' behaviors using only locally observable information, enabling collaborative policy learning without explicit action sharing. This approach is fully compatible with standard TD3 algorithms and scalable to larger multiagent systems. At the engineering application level, this framework has been implemented and validated in dual-arm robotic manipulation tasks: two robotic arms collaboratively lift objects. Experimental results demonstrate that this approach significantly enhances the robustness and deployment feasibility of real-world robotic systems while reducing dependence on information infrastructure. Overall, this research advances the development of decentralized multiagent artificial intelligence systems while enabling AI to operate effectively in dynamic, information-constrained real-world environments.

</details>


### [6] [Design and Development of Modular Limbs for Reconfigurable Robots on the Moon](https://arxiv.org/abs/2601.04541)
*Gustavo H. Diaz,A. Sejal Jain,Matteo Brugnera,Elian Neppel,Shreya Santra,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: Researchers developed modular 4-DOF robot limbs (Moonbots) that connect with wheel modules for space exploration, featuring a unified high-torque actuator and demonstrating nine functional configurations for different tasks.


<details>
  <summary>Details</summary>
Motivation: To create modular robotic systems for space exploration and construction on the Moon, addressing resource constraints through flexible, versatile modular components that can adapt to different environments and tasks.

Method: Developed 4-DOF robot limbs with a common high torque-to-speed ratio actuator, designed to connect with each other and wheel modules. Implemented hardware, mechanical design, and software architecture for control and coordination. Evaluated actuator performance under various loads.

Result: Successfully created modular Moonbot system with nine functional configurations: 4DOF-limb, 8DOF-limb, vehicle, dragon, minimal, quadruped, cargo, cargo-minimal, and bike. Demonstrated adaptability for different locomotion strategies and task-specific behaviors.

Conclusion: The modular Moonbot system with unified actuator design provides a practical foundation for reconfigurable robotic systems in space exploration, offering flexibility, simplified development/maintenance, and adaptability to various mission requirements.

Abstract: In this paper, we present the development of 4-DOF robot limbs, which we call Moonbots, designed to connect in various configurations with each other and wheel modules, enabling adaptation to different environments and tasks. These modular components are intended primarily for robotic systems in space exploration and construction on the Moon in our Moonshot project. Such modular robots add flexibility and versatility for space missions where resources are constrained. Each module is driven by a common actuator characterized by a high torque-to-speed ratio, supporting both precise control and dynamic motion when required. This unified actuator design simplifies development and maintenance across the different module types. The paper describes the hardware implementation, the mechanical design of the modules, and the overall software architecture used to control and coordinate them. Additionally, we evaluate the control performance of the actuator under various load conditions to characterize its suitability for modular robot applications. To demonstrate the adaptability of the system, we introduce nine functional configurations assembled from the same set of modules: 4DOF-limb, 8DOF-limb, vehicle, dragon, minimal, quadruped, cargo, cargo-minimal, and bike. These configurations reflect different locomotion strategies and task-specific behaviors, offering a practical foundation for further research in reconfigurable robotic systems.

</details>


### [7] [Data-Driven Terramechanics Approach Towards a Realistic Real-Time Simulator for Lunar Rovers](https://arxiv.org/abs/2601.04547)
*Jakob M. Kern,James M. Hurrell,Shreya Santra,Keisuke Takehana,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: Developed a lunar surface simulator combining high visual fidelity with realistic terrain interaction using data-driven regression models for wheel-soil dynamics, enabling real-time physically plausible rover simulation.


<details>
  <summary>Details</summary>
Motivation: Current lunar surface simulators focus on either visual realism or physical accuracy, but not both, limiting comprehensive replication of lunar conditions for rover testing and mission planning.

Method: Used data-driven approach with regression models for slip and sinkage from full-rover and single-wheel experiments/simulations, avoiding computationally expensive direct simulation of wheel-soil interactions.

Result: Regression-based terramechanics model accurately reproduced steady-state/dynamic slip and sinkage on flat terrain and slopes up to 20°, validated against field tests, with improved terrain deformation and wheel trace visualization.

Conclusion: Combined high visual fidelity with realistic terrain interaction enables real-time applications requiring physically plausible terrain response for comprehensive lunar rover simulation and testing.

Abstract: High-fidelity simulators for the lunar surface provide a digital environment for extensive testing of rover operations and mission planning. However, current simulators focus on either visual realism or physical accuracy, which limits their capability to replicate lunar conditions comprehensively. This work addresses that gap by combining high visual fidelity with realistic terrain interaction for a realistic representation of rovers on the lunar surface. Because direct simulation of wheel-soil interactions is computationally expensive, a data-driven approach was adopted, using regression models for slip and sinkage from data collected in both full-rover and single-wheel experiments and simulations. The resulting regression-based terramechanics model accurately reproduced steady-state and dynamic slip, as well as sinkage behavior, on flat terrain and slopes up to 20 degrees, with validation against field test results. Additionally, improvements were made to enhance the realism of terrain deformation and wheel trace visualization. This method supports real-time applications that require physically plausible terrain response alongside high visual fidelity.

</details>


### [8] [Discrete Fourier Transform-based Point Cloud Compression for Efficient SLAM in Featureless Terrain](https://arxiv.org/abs/2601.04551)
*Riku Suzuki,Ayumi Umemura,Shreya Santra,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: A DFT-based compression method for point cloud maps that converts DEM to frequency domain and removes high-frequency components, effective for gradual terrains like planets and deserts.


<details>
  <summary>Details</summary>
Motivation: SLAM systems for robotic exploration face computational and bandwidth limitations, requiring efficient compression of large point cloud data, especially for gradual terrain exploration.

Method: Convert Digital Elevation Model (DEM) to frequency-domain 2D image using Discrete Fourier Transform (DFT), then omit high-frequency components that contribute little to gradual terrain representation.

Result: Evaluated using camera sequences of two terrains with different elevation profiles, showing effective compression without significant degradation of point cloud accuracy.

Conclusion: The DFT-based compression method effectively reduces data size for gradual terrains while maintaining accuracy, making it suitable for resource-constrained robotic exploration missions.

Abstract: Simultaneous Localization and Mapping (SLAM) is an essential technology for the efficiency and reliability of unmanned robotic exploration missions. While the onboard computational capability and communication bandwidth are critically limited, the point cloud data handled by SLAM is large in size, attracting attention to data compression methods. To address such a problem, in this paper, we propose a new method for compressing point cloud maps by exploiting the Discrete Fourier Transform (DFT). The proposed technique converts the Digital Elevation Model (DEM) to the frequency-domain 2D image and omits its high-frequency components, focusing on the exploration of gradual terrains such as planets and deserts. Unlike terrains with detailed structures such as artificial environments, high-frequency components contribute little to the representation of gradual terrains. Thus, this method is effective in compressing data size without significant degradation of the point cloud. We evaluated the method in terms of compression rate and accuracy using camera sequences of two terrains with different elevation profiles.

</details>


### [9] [UniBiDex: A Unified Teleoperation Framework for Robotic Bimanual Dexterous Manipulation](https://arxiv.org/abs/2601.04629)
*Zhongxuan Li,Zeliang Guo,Jun Hu,David Navarro-Alarcon,Jia Pan,Hongmin Wu,Peng Zhou*

Main category: cs.RO

TL;DR: UniBiDex is a unified teleoperation framework for bimanual dexterous manipulation that supports both VR and leader-follower inputs, enabling real-time contact-rich dual-arm teleoperation with safety guarantees and optimized motion.


<details>
  <summary>Details</summary>
Motivation: To create a unified teleoperation framework that supports multiple input modalities for bimanual dexterous manipulation, enabling collection of large-scale, high-quality human demonstration datasets for robot learning.

Method: Integrates heterogeneous input devices into a shared control stack with consistent kinematic treatment and safety guarantees. Uses nullspace control to optimize bimanual configurations for smooth, collision-free, singularity-aware motion.

Result: Validated on a long-horizon kitchen-tidying task with five sequential manipulation subtasks, demonstrating higher task success rates, smoother trajectories, and improved robustness compared to strong baselines.

Conclusion: UniBiDex lowers the barrier to collecting large-scale, high-quality human demonstration datasets and accelerates progress in robot learning through open-source release of hardware and software components.

Abstract: We present UniBiDex a unified teleoperation framework for robotic bimanual dexterous manipulation that supports both VRbased and leaderfollower input modalities UniBiDex enables realtime contactrich dualarm teleoperation by integrating heterogeneous input devices into a shared control stack with consistent kinematic treatment and safety guarantees The framework employs nullspace control to optimize bimanual configurations ensuring smooth collisionfree and singularityaware motion across tasks We validate UniBiDex on a longhorizon kitchentidying task involving five sequential manipulation subtasks demonstrating higher task success rates smoother trajectories and improved robustness compared to strong baselines By releasing all hardware and software components as opensource we aim to lower the barrier to collecting largescale highquality human demonstration datasets and accelerate progress in robot learning.

</details>


### [10] [Model of Spatial Human-Agent Interaction with Consideration for Others](https://arxiv.org/abs/2601.04657)
*Takafumi Sakamoto,Yugo Takeuchi*

Main category: cs.RO

TL;DR: A computational spatial interaction model with consideration parameter for robots to initiate conversations without disturbing pedestrians, validated in VR experiments showing consideration values affect movement inhibition.


<details>
  <summary>Details</summary>
Motivation: Communication robots in public spaces need to balance initiating conversations with not disturbing pedestrians, requiring estimation of others' communication desires and adjustment of robot behavior accordingly.

Method: Constructed a computational spatial interaction model with consideration parameter (adjustment of internal state to estimated internal state of others), validated through VR experiments with human participants interacting with virtual robots.

Result: Low consideration robots inhibited participant movement when moving toward target, while high consideration robots did not. When participants approached robots, robots exhibited approaching behavior regardless of consideration value, decreasing participant movement.

Conclusion: The proposed model successfully clarifies interactions with consideration for others, demonstrating how consideration values affect spatial interaction dynamics between robots and humans in public spaces.

Abstract: Communication robots often need to initiate conversations with people in public spaces. At the same time, such robots must not disturb pedestrians. To handle these two requirements, an agent needs to estimate the communication desires of others based on their behavior and then adjust its own communication activities accordingly. In this study, we construct a computational spatial interaction model that considers others. Consideration is expressed as a quantitative parameter: the amount of adjustment of one's internal state to the estimated internal state of the other. To validate the model, we experimented with a human and a virtual robot interacting in a VR environment. The results show that when the participant moves to the target, a virtual robot with a low consideration value inhibits the participant's movement, while a robot with a higher consideration value did not inhibit the participant's movement. When the participant approached the robot, the robot also exhibited approaching behavior, regardless of the consideration value, thus decreasing the participant's movement. These results appear to verify the proposed model's ability to clarify interactions with consideration for others.

</details>


### [11] [Optimizing Path Planning using Deep Reinforcement Learning for UGVs in Precision Agriculture](https://arxiv.org/abs/2601.04668)
*Laukik Patade,Rohan Rane,Sandeep Pillai*

Main category: cs.RO

TL;DR: This paper optimizes UGV path planning in precision agriculture using DRL in continuous action spaces, showing TD3 achieves 95% success rate in dynamic 3D environments.


<details>
  <summary>Details</summary>
Motivation: Traditional grid-based path planning methods (A*, Dijkstra) have limitations in dynamic agricultural environments, creating need for adaptive learning strategies that can handle moving obstacles and ensure safety for both crops and robots.

Method: The study progresses from reviewing traditional methods to exploring DRL approaches: starts with DQN in 2D simulations, then enhances with Double Q-Networks and Dueling Networks, and finally implements continuous action space models (DDPG and TD3) tested in increasingly complex 3D environments using ROS and Gazebo.

Result: Continuous DRL algorithms demonstrate effectiveness in dynamic agricultural scenarios, with pretrained TD3 agent achieving 95% success rate in dynamic environments, showing robustness in handling moving obstacles while ensuring safety.

Conclusion: Continuous DRL approaches, particularly TD3, provide superior adaptive path planning for UGVs in precision agriculture compared to traditional methods, offering high success rates and safety in dynamic environments with moving obstacles.

Abstract: This study focuses on optimizing path planning for unmanned ground vehicles (UGVs) in precision agriculture using deep reinforcement learning (DRL) techniques in continuous action spaces. The research begins with a review of traditional grid-based methods, such as A* and Dijkstra's algorithms, and discusses their limitations in dynamic agricultural environments, highlighting the need for adaptive learning strategies. The study then explores DRL approaches, including Deep Q-Networks (DQN), which demonstrate improved adaptability and performance in two-dimensional simulations. Enhancements such as Double Q-Networks and Dueling Networks are evaluated to further improve decision-making. Building on these results, the focus shifts to continuous action space models, specifically Deep Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic Policy Gradient (TD3), which are tested in increasingly complex environments. Experiments conducted in a three-dimensional environment using ROS and Gazebo demonstrate the effectiveness of continuous DRL algorithms in navigating dynamic agricultural scenarios. Notably, the pretrained TD3 agent achieves a 95 percent success rate in dynamic environments, demonstrating the robustness of the proposed approach in handling moving obstacles while ensuring safety for both crops and the robot.

</details>


### [12] [SeqWalker: Sequential-Horizon Vision-and-Language Navigation with Hierarchical Planning](https://arxiv.org/abs/2601.04699)
*Zebin Han,Xudong Wang,Baichen Liu,Qi Lyu,Zhenduo Shang,Jiahua Dong,Lianqing Liu,Zhi Han*

Main category: cs.RO

TL;DR: SeqWalker is a hierarchical planning model for Sequential-Horizon Vision-and-Language Navigation that uses a High-Level Planner to break down complex instructions and a Low-Level Planner with Exploration-Verification for error correction, outperforming existing methods on a new SH-VLN benchmark.


<details>
  <summary>Details</summary>
Motivation: Current VLN models struggle with multi-task, long-horizon instructions due to information overload, which impairs their ability to focus on observationally relevant details in Sequential-Horizon VLN scenarios.

Method: SeqWalker uses a hierarchical planning framework with: 1) High-Level Planner that dynamically selects global instructions into contextually relevant sub-instructions based on current visual observations, reducing cognitive load; 2) Low-Level Planner with Exploration-Verification strategy that leverages the logical structure of instructions for trajectory error correction.

Result: The authors extend the IVLN dataset to create a new SH-VLN benchmark and demonstrate through extensive experiments that SeqWalker achieves superior performance compared to existing methods.

Conclusion: SeqWalker effectively addresses the information overload problem in SH-VLN through hierarchical planning and instruction decomposition, establishing a strong baseline for future research in sequential-horizon navigation tasks.

Abstract: Sequential-Horizon Vision-and-Language Navigation (SH-VLN) presents a challenging scenario where agents should sequentially execute multi-task navigation guided by complex, long-horizon language instructions. Current vision-and-language navigation models exhibit significant performance degradation with such multi-task instructions, as information overload impairs the agent's ability to attend to observationally relevant details. To address this problem, we propose SeqWalker, a navigation model built on a hierarchical planning framework. Our SeqWalker features: i) A High-Level Planner that dynamically selects global instructions into contextually relevant sub-instructions based on the agent's current visual observations, thus reducing cognitive load; ii) A Low-Level Planner incorporating an Exploration-Verification strategy that leverages the inherent logical structure of instructions for trajectory error correction. To evaluate SH-VLN performance, we also extend the IVLN dataset and establish a new benchmark. Extensive experiments are performed to demonstrate the superiority of the proposed SeqWalker.

</details>


### [13] [Zero Wrench Control via Wrench Disturbance Observer for Learning-free Peg-in-hole Assembly](https://arxiv.org/abs/2601.04881)
*Kiyoung Choi,Juwon Jeong,Sehoon Oh*

Main category: cs.RO

TL;DR: DW-DOB enables sensitive zero-wrench control for contact-rich manipulation by separating intrinsic dynamics from external forces, achieving stable, compliant insertions at industrial tolerances.


<details>
  <summary>Details</summary>
Motivation: Conventional wrench disturbance observers fail to compensate for inertial effects, limiting sensitivity to small forces and moments in contact-rich manipulation tasks that require precise zero-wrench control.

Method: Proposes Dynamic Wrench Disturbance Observer (DW-DOB) that embeds task-space inertia into the observer nominal model to cleanly separate intrinsic dynamic reactions from true external wrenches, enabling passivity-based stable interactions.

Result: Peg-in-hole experiments at industrial tolerances (H7/h6) show deeper, more compliant insertions with minimal residual wrenches, outperforming conventional wrench disturbance observer and PD baseline.

Conclusion: DW-DOB provides a practical learning-free solution for high-precision zero-wrench control in contact-rich tasks by preserving sensitivity to small forces while ensuring robust contact wrench regulation.

Abstract: This paper proposes a Dynamic Wrench Disturbance Observer (DW-DOB) designed to achieve highly sensitive zero-wrench control in contact-rich manipulation. By embedding task-space inertia into the observer nominal model, DW-DOB cleanly separates intrinsic dynamic reactions from true external wrenches. This preserves sensitivity to small forces and moments while ensuring robust regulation of contact wrenches. A passivity-based analysis further demonstrates that DW-DOB guarantees stable interactions under dynamic conditions, addressing the shortcomings of conventional observers that fail to compensate for inertial effects. Peg-in-hole experiments at industrial tolerances (H7/h6) validate the approach, yielding deeper and more compliant insertions with minimal residual wrenches and outperforming a conventional wrench disturbance observer and a PD baseline. These results highlight DW-DOB as a practical learning-free solution for high-precision zero-wrench control in contact-rich tasks.

</details>


### [14] [SKATER: Synthesized Kinematics for Advanced Traversing Efficiency on a Humanoid Robot via Roller Skate Swizzles](https://arxiv.org/abs/2601.04948)
*Junchi Gu,Feiyang Yuan,Weize Shi,Tianchen Huang,Haopeng Zhang,Xiaohu Zhang,Yu Wang,Wei Gao,Shiwu Zhang*

Main category: cs.RO

TL;DR: Humanoid robot with passive wheels learns roller skating via deep reinforcement learning, achieving 75.86% lower impact and 63.34% lower energy cost compared to walking.


<details>
  <summary>Details</summary>
Motivation: Traditional humanoid walking/running generates high impact forces causing joint wear and poor energy efficiency. Roller skating offers continuous sliding with minimal kinetic energy loss through body inertia utilization.

Method: Developed humanoid robot with four passive wheels on each foot. Used deep reinforcement learning control framework with reward function designed based on roller skating's intrinsic characteristics for swizzle gait.

Result: Simulation analysis followed by physical deployment showed smooth, efficient swizzle gait. Achieved 75.86% reduction in Impact Intensity and 63.34% reduction in Cost of Transport compared to traditional bipedal walking.

Conclusion: Roller skating is a superior locomotion mode for humanoid robots, offering enhanced energy efficiency and joint longevity through reduced impact forces and energy consumption.

Abstract: Although recent years have seen significant progress of humanoid robots in walking and running, the frequent foot strikes with ground during these locomotion gaits inevitably generate high instantaneous impact forces, which leads to exacerbated joint wear and poor energy utilization. Roller skating, as a sport with substantial biomechanical value, can achieve fast and continuous sliding through rational utilization of body inertia, featuring minimal kinetic energy loss. Therefore, this study proposes a novel humanoid robot with each foot equipped with a row of four passive wheels for roller skating. A deep reinforcement learning control framework is also developed for the swizzle gait with the reward function design based on the intrinsic characteristics of roller skating. The learned policy is first analyzed in simulation and then deployed on the physical robot to demonstrate the smoothness and efficiency of the swizzle gait over traditional bipedal walking gait in terms of Impact Intensity and Cost of Transport during locomotion. A reduction of $75.86\%$ and $63.34\%$ of these two metrics indicate roller skating as a superior locomotion mode for enhanced energy efficiency and joint longevity.

</details>


### [15] [When to Act: Calibrated Confidence for Reliable Human Intention Prediction in Assistive Robotics](https://arxiv.org/abs/2601.04982)
*Johannes A. Gaus,Winfried Ilg,Daniel Haeufle*

Main category: cs.RO

TL;DR: A safety-critical triggering framework for assistive devices that uses calibrated probabilities for next-action prediction, ensuring reliable assistance only when confidence is high.


<details>
  <summary>Details</summary>
Motivation: Assistive devices need to determine both what users intend to do and how reliable those predictions are before providing support. Raw model confidence often fails to reflect true correctness, posing safety risks in assistive applications.

Method: Introduces a safety-critical triggering framework based on calibrated probabilities for multimodal next-action prediction. Uses post-hoc calibration to align predicted confidence with empirical reliability, reducing miscalibration by about an order of magnitude without affecting accuracy. The calibrated confidence drives a simple ACT/HOLD rule that acts only when reliability is high and withholds assistance otherwise.

Result: Post-hoc calibration reduces miscalibration by about an order of magnitude without affecting accuracy. The calibrated confidence enables a reliable ACT/HOLD decision rule for assistive actions.

Conclusion: The framework turns the confidence threshold into a quantitative safety parameter for assisted actions and enables verifiable behavior in an assistive control loop, making assistive devices safer by ensuring they only act when predictions are sufficiently reliable.

Abstract: Assistive devices must determine both what a user intends to do and how reliable that prediction is before providing support. We introduce a safety-critical triggering framework based on calibrated probabilities for multimodal next-action prediction in Activities of Daily Living. Raw model confidence often fails to reflect true correctness, posing a safety risk. Post-hoc calibration aligns predicted confidence with empirical reliability and reduces miscalibration by about an order of magnitude without affecting accuracy. The calibrated confidence drives a simple ACT/HOLD rule that acts only when reliability is high and withholds assistance otherwise. This turns the confidence threshold into a quantitative safety parameter for assisted actions and enables verifiable behavior in an assistive control loop.

</details>


### [16] [The RoboSense Challenge: Sense Anything, Navigate Anywhere, Adapt Across Platforms](https://arxiv.org/abs/2601.05014)
*Lingdong Kong,Shaoyuan Xie,Zeying Gong,Ye Li,Meng Chu,Ao Liang,Yuhao Dong,Tianshuai Hu,Ronghe Qiu,Rong Li,Hanjiang Hu,Dongyue Lu,Wei Yin,Wenhao Ding,Linfeng Li,Hang Song,Wenwei Zhang,Yuexin Ma,Junwei Liang,Zhedong Zheng,Lai Xing Ng,Benoit R. Cottereau,Wei Tsang Ooi,Ziwei Liu,Zhanpeng Zhang,Weichao Qiu,Wei Zhang,Ji Ao,Jiangpeng Zheng,Siyu Wang,Guang Yang,Zihao Zhang,Yu Zhong,Enzhu Gao,Xinhan Zheng,Xueting Wang,Shouming Li,Yunkai Gao,Siming Lan,Mingfei Han,Xing Hu,Dusan Malic,Christian Fruhwirth-Reisinger,Alexander Prutsch,Wei Lin,Samuel Schulter,Horst Possegger,Linfeng Li,Jian Zhao,Zepeng Yang,Yuhang Song,Bojun Lin,Tianle Zhang,Yuchen Yuan,Chi Zhang,Xuelong Li,Youngseok Kim,Sihwan Hwang,Hyeonjun Jeong,Aodi Wu,Xubo Luo,Erjia Xiao,Lingfeng Zhang,Yingbo Tang,Hao Cheng,Renjing Xu,Wenbo Ding,Lei Zhou,Long Chen,Hangjun Ye,Xiaoshuai Hao,Shuangzhi Li,Junlong Shen,Xingyu Li,Hao Ruan,Jinliang Lin,Zhiming Luo,Yu Zang,Cheng Wang,Hanshi Wang,Xijie Gong,Yixiang Yang,Qianli Ma,Zhipeng Zhang,Wenxiang Shi,Jingmeng Zhou,Weijun Zeng,Kexin Xu,Yuchen Zhang,Haoxiang Fu,Ruibin Hu,Yanbiao Ma,Xiyan Feng,Wenbo Zhang,Lu Zhang,Yunzhi Zhuge,Huchuan Lu,You He,Seungjun Yu,Junsung Park,Youngsun Lim,Hyunjung Shim,Faduo Liang,Zihang Wang,Yiming Peng,Guanyu Zong,Xu Li,Binghao Wang,Hao Wei,Yongxin Ma,Yunke Shi,Shuaipeng Liu,Dong Kong,Yongchun Lin,Huitong Yang,Liang Lei,Haoang Li,Xinliang Zhang,Zhiyong Wang,Xiaofeng Wang,Yuxia Fu,Yadan Luo,Djamahl Etchegaray,Yang Li,Congfei Li,Yuxiang Sun,Wenkai Zhu,Wang Xu,Linru Li,Longjie Liao,Jun Yan,Benwu Wang,Xueliang Ren,Xiaoyu Yue,Jixian Zheng,Jinfeng Wu,Shurui Qin,Wei Cong,Yao He*

Main category: cs.RO

TL;DR: RoboSense 2025 Challenge advances robot perception robustness across 5 tracks with 143 teams, providing benchmarks for real-world reliability under domain shifts and sensor failures.


<details>
  <summary>Details</summary>
Motivation: Autonomous systems need reliable perception in dynamic environments, but current methods degrade under unseen conditions like sensor noise, environmental variation, and platform shifts.

Method: Unified challenge with 5 complementary tracks: language-grounded decision making, socially compliant navigation, sensor configuration generalization, cross-view/cross-modal correspondence, and cross-platform 3D perception.

Result: 143 teams from 85 institutions across 16 countries participated; 23 winning solutions analyzed to identify methodological trends and design principles.

Conclusion: RoboSense 2025 provides comprehensive benchmarks and insights toward building robots that can sense reliably and adapt across platforms in real-world environments.

Abstract: Autonomous systems are increasingly deployed in open and dynamic environments -- from city streets to aerial and indoor spaces -- where perception models must remain reliable under sensor noise, environmental variation, and platform shifts. However, even state-of-the-art methods often degrade under unseen conditions, highlighting the need for robust and generalizable robot sensing. The RoboSense 2025 Challenge is designed to advance robustness and adaptability in robot perception across diverse sensing scenarios. It unifies five complementary research tracks spanning language-grounded decision making, socially compliant navigation, sensor configuration generalization, cross-view and cross-modal correspondence, and cross-platform 3D perception. Together, these tasks form a comprehensive benchmark for evaluating real-world sensing reliability under domain shifts, sensor failures, and platform discrepancies. RoboSense 2025 provides standardized datasets, baseline models, and unified evaluation protocols, enabling large-scale and reproducible comparison of robust perception methods. The challenge attracted 143 teams from 85 institutions across 16 countries, reflecting broad community engagement. By consolidating insights from 23 winning solutions, this report highlights emerging methodological trends, shared design principles, and open challenges across all tracks, marking a step toward building robots that can sense reliably, act robustly, and adapt across platforms in real-world environments.

</details>


### [17] [Compensation Effect Amplification Control (CEAC): A movement-based approach for coordinated position and velocity control of the elbow of upper-limb prostheses](https://arxiv.org/abs/2601.05074)
*Julian Kulozik,Nathanaël Jarrassé*

Main category: cs.RO

TL;DR: CEAC is a novel control method for prosthetic elbows that uses trunk flexion/extension as input, amplifying natural trunk-prosthesis coupling with controlled delay for intuitive velocity and position control.


<details>
  <summary>Details</summary>
Motivation: Current upper-limb prostheses struggle with intuitive control of intermediate joints (wrist, elbow) for continuous, velocity-modulated movements. The trunk is both functional and compensatory in upper-limb actions, but this natural coupling isn't effectively leveraged in prosthetic control.

Method: Compensation Effect Amplification Control (CEAC) uses trunk flexion and extension as input for prosthetic elbow velocity control. It amplifies natural trunk-prosthesis coupling while introducing controlled delay to allow users to modulate both position and velocity of the prosthetic joint.

Result: Evaluation with 12 able-bodied participants using supernumerary prosthesis showed task performance comparable to natural arm movements, even with varied gesture velocity or drawing size. Maintained ergonomic trunk postures and effectively restored joint coordinated action while distributing movement effort between trunk and elbow.

Conclusion: CEAC offers a promising control strategy for intermediate joints of upper-limb prostheses, particularly for tasks requiring continuous and precise coordination, enabling intuitive trajectory control without extreme compensatory movements.

Abstract: Despite advances in upper-limb (UL) prosthetic design, achieving intuitive control of intermediate joints - such as the wrist and elbow - remains challenging, particularly for continuous and velocity-modulated movements. We introduce a novel movement-based control paradigm entitled Compensation Effect Amplification Control (CEAC) that leverages users' trunk flexion and extension as input for controlling prosthetic elbow velocity. Considering that the trunk can be both a functional and compensatory joint when performing upper-limb actions, CEAC amplifies the natural coupling between trunk and prosthesis while introducing a controlled delay that allows users to modulate both the position and velocity of the prosthetic joint. We evaluated CEAC in a generic drawing task performed by twelve able-bodied participants using a supernumerary prosthesis with an active elbow. Additionally a multiple-target-reaching task was performed by a subset of ten participants. Results demonstrate task performances comparable to those obtained with natural arm movements, even when gesture velocity or drawing size were varied, while maintaining ergonomic trunk postures. Analysis revealed that CEAC effectively restores joint coordinated action, distributes movement effort between trunk and elbow, enabling intuitive trajectory control without requiring extreme compensatory movements. Overall, CEAC offers a promising control strategy for intermediate joints of UL prostheses, particularly in tasks requiring continuous and precise coordination.

</details>


### [18] [Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration](https://arxiv.org/abs/2601.05243)
*Xingyi He,Adhitya Polavaram,Yunhao Cao,Om Deshmukh,Tianrui Wang,Xiaowei Zhou,Kuan Fang*

Main category: cs.RO

TL;DR: CorDex learns dexterous functional grasps for novel objects using synthetic data from single human demonstrations, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Two main bottlenecks constrain functional grasping with dexterous robotic hands: scarcity of large-scale datasets and lack of integrated semantic/geometric reasoning in learned models.

Method: Uses correspondence-based data engine to generate diverse training data from single human demo, then multimodal prediction network with local-global fusion and importance-aware sampling for efficient grasp prediction.

Result: CorDex generalizes well to unseen object instances and significantly outperforms state-of-the-art baselines across various object categories.

Conclusion: The framework enables robust learning of dexterous functional grasps from minimal human demonstrations, addressing key bottlenecks in robotic manipulation.

Abstract: Functional grasping with dexterous robotic hands is a key capability for enabling tool use and complex manipulation, yet progress has been constrained by two persistent bottlenecks: the scarcity of large-scale datasets and the absence of integrated semantic and geometric reasoning in learned models. In this work, we present CorDex, a framework that robustly learns dexterous functional grasps of novel objects from synthetic data generated from just a single human demonstration. At the core of our approach is a correspondence-based data engine that generates diverse, high-quality training data in simulation. Based on the human demonstration, our data engine generates diverse object instances of the same category, transfers the expert grasp to the generated objects through correspondence estimation, and adapts the grasp through optimization. Building on the generated data, we introduce a multimodal prediction network that integrates visual and geometric information. By devising a local-global fusion module and an importance-aware sampling mechanism, we enable robust and computationally efficient prediction of functional dexterous grasps. Through extensive experiments across various object categories, we demonstrate that CorDex generalizes well to unseen object instances and significantly outperforms state-of-the-art baselines.

</details>


### [19] [LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model](https://arxiv.org/abs/2601.05248)
*Zhuoyang Liu,Jiaming Liu,Hao Chen,Ziyu Guo,Chengkai Hou,Chenyang Gu,Jiale Yu,Xiangju Mi,Renrui Zhang,Zhengping Che,Jian Tang,Pheng-Ann Heng,Shanghang Zhang*

Main category: cs.RO

TL;DR: LaST₀ is a Vision-Language-Action framework that uses latent spatio-temporal chain-of-thought reasoning to improve robotic manipulation efficiency and accuracy without explicit linguistic reasoning bottlenecks.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models suffer from inference latency due to explicit linguistic reasoning and struggle to capture ineffable physical attributes that are difficult to verbalize, creating representational bottlenecks for robotic manipulation.

Method: Introduces a latent CoT space modeling future visual dynamics, 3D structure, and robot proprioceptive states across time. Uses a dual-system Mixture-of-Transformers architecture with reasoning (low-frequency) and acting (high-frequency) experts trained with heterogeneous operation frequencies.

Result: Achieves 8% and 13% higher mean success rates over prior VLA methods in simulated and real-world tasks respectively, with substantially faster inference speeds.

Conclusion: LaST₀ demonstrates that latent spatio-temporal reasoning enables more efficient and accurate robotic manipulation by capturing fine-grained physical dynamics that are difficult to verbalize, overcoming limitations of explicit linguistic reasoning in VLA models.

Abstract: Vision-Language-Action (VLA) models have recently demonstrated strong generalization capabilities in robotic manipulation. Some existing VLA approaches attempt to improve action accuracy by explicitly generating linguistic reasoning traces or future visual observations before action execution. However, explicit reasoning typically incurs non-negligible inference latency, which constrains the temporal resolution required for robotic manipulation. Moreover, such reasoning is confined to the linguistic space, imposing a representational bottleneck that struggles to faithfully capture ineffable physical attributes. To mitigate these limitations, we propose LaST$_0$, a framework that enables efficient reasoning before acting through a Latent Spatio-Temporal Chain-of-Thought (CoT), capturing fine-grained physical and robotic dynamics that are often difficult to verbalize. Specifically, we introduce a token-efficient latent CoT space that models future visual dynamics, 3D structural information, and robot proprioceptive states, and further extends these representations across time to enable temporally consistent implicit reasoning trajectories. Furthermore, LaST$_0$ adopts a dual-system architecture implemented via a Mixture-of-Transformers design, where a reasoning expert conducts low-frequency latent inference and an acting expert generates high-frequency actions conditioned on robotics-oriented latent representations. To facilitate coordination, LaST$_0$ is trained with heterogeneous operation frequencies, enabling adaptive switching between reasoning and action inference rates during deployment. Across ten simulated and six real-world manipulation tasks, LaST$_0$ improves mean success rates by 8% and 13% over prior VLA methods, respectively, while achieving substantially faster inference. Project website: https://sites.google.com/view/last0

</details>
