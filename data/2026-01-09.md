<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 79]
- [cs.LG](#cs.LG) [Total: 78]
- [cs.IT](#cs.IT) [Total: 12]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.AI](#cs.AI) [Total: 100]
- [eess.SP](#eess.SP) [Total: 11]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes](https://arxiv.org/abs/2601.04300)
*Chenye Meng,Zejian Li,Zhongni Liu,Yize Li,Changle Xie,Kaixin Jia,Ling Yang,Huanghuang Deng,Shiying Ding,Shengyuan Zhang,Jiayi Li,Lingyun Sun*

Main category: cs.CV

TL;DR: CPO (Complex Preference Optimization) aligns diffusion models with hierarchical, fine-grained expert criteria for painting generation, outperforming simple reward/alignment methods.


<details>
  <summary>Details</summary>
Motivation: Current post-training alignment methods use oversimplified signals (scalar rewards or binary preferences), which fail to capture complex hierarchical human expertise needed for high-quality image generation.

Method: Two-stage framework: 1) Construct hierarchical fine-grained evaluation criteria with domain experts (tree structure of positive/negative attributes), 2) Supervised Fine-Tuning to inject domain knowledge into auxiliary diffusion model, 3) CPO extends DPO to align target diffusion with non-binary hierarchical criteria by maximizing positive attributes and minimizing negative attributes using the auxiliary model.

Result: Extensive experiments show CPO significantly enhances generation quality and alignment with expertise in painting generation domain, outperforming existing alignment methods.

Conclusion: CPO enables fine-grained criteria alignment for diffusion models, opening new avenues for aligning with complex hierarchical human expertise beyond simple reward signals.

Abstract: Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.

</details>


### [2] [Embedding Textual Information in Images Using Quinary Pixel Combinations](https://arxiv.org/abs/2601.04302)
*A V Uday Kiran Kandala*

Main category: cs.CV

TL;DR: Novel text embedding in images using quinary pixel intensity combinations in RGB space, achieving efficient single-pixel encoding with minimal distortion.


<details>
  <summary>Details</summary>
Motivation: Existing text embedding methods (LSB, MSB, PVD, transform domain, deep learning) have limitations: they often require multiple pixels, create noise-like artifacts, are computationally heavy, and use deterministic encoding/decoding. Need for more efficient, less distorting, and computationally lighter approach.

Method: Uses quinary pixel intensity combinations in RGB space - five controlled intensity variations in each R, G, B channel create 125 distinct combinations. These combinations are mapped to textual symbols (letters, numbers, whitespace, special characters). Each complete textual symbol encoded within a single RGB pixel.

Result: No significant distortion in encoded images as measured by MSE, MAE, SNR, PSNR, SSIM, Histogram Comparison, and Heatmap analysis. Achieves improved embedding efficiency with single-pixel encoding vs. multiple pixels required by LSB/MSB methods.

Conclusion: Proposed quinary pixel intensity combination method provides efficient text embedding in images with minimal distortion, superior to traditional LSB/MSB and computationally heavy deep learning approaches.

Abstract: This paper presents a novel technique for embedding textual data into images using quinary combinations of pixel intensities in RGB space. Existing methods predominantly rely on least and most significant bit (LSB & MSB) manipulation, Pixel Value Differencing (PVD), spatial perturbations in RGB channels, transform domain based methods, Quantization methods, Edge and Region based methods and more recently through deep learning methods and generative AI techniques for hiding textual information in spatial domain of images. Most of them are dependent on pixel intensity flipping over multiple pixels, such as LSB and combination of LSB based methodologies, and on transform coefficients, often resulting in the form of noise. Encoding and Decoding are deterministic in most of the existing approaches and are computationally heavy in case of higher models such as deep learning and gen AI approaches. The proposed method works on quinary pixel intensity combinations in RGB space, where five controlled different pixel intensity variations in each of the R, G, and B channels formulate up to one hundred and twenty five distinct pixel intensity combinations. These combinations are mapped to textual symbols, enabling the representation of uppercase and lowercase alphabetic characters, numeric digits, whitespace, and commonly used special characters. Different metrics such as MSE, MAE, SNR, PSNR, SSIM, Histogram Comparison and Heatmap analysis, were evaluated for both original and encoded images resulting in no significant distortion in the images. Furthermore, the method achieves improved embedding efficiency by encoding a complete textual symbol within a single RGB pixel, in contrast to LSB and MSB based approaches that typically require multiple pixels or multi-step processes, as well as transform and learning based methods that incur higher computational overhead.

</details>


### [3] [Unified Text-Image Generation with Weakness-Targeted Post-Training](https://arxiv.org/abs/2601.04339)
*Jiahui Chen,Philippe Hansen-Estruch,Xiaochuang Han,Yushi Hu,Emily Dinan,Amita Kamath,Michal Drozdzal,Reyhane Askari-Hemmat,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: This paper presents a post-training method for unified multimodal generation that enables autonomous text-to-image synthesis within a single inference process, improving performance across multiple benchmarks through reward-weighted training with strategic synthetic data.


<details>
  <summary>Details</summary>
Motivation: Existing unified multimodal generation architectures rely on explicit modality switching (generating reasoning text first, then manually switching to image generation), which limits cross-modal coupling and prohibits automatic multimodal generation. The authors aim to achieve fully unified text-image generation where models autonomously transition from textual reasoning to visual synthesis.

Method: The authors use post-training to achieve fully unified text-image generation, examining the impact of joint text-image generation on T2I performance and the relative importance of each modality. They explore different post-training data strategies and use offline, reward-weighted post-training with fully self-generated synthetic data. They specifically investigate targeted datasets addressing specific limitations versus broad image-caption corpora or benchmark-aligned data.

Result: The approach enables improvements in multimodal image generation across four diverse T2I benchmarks. The results demonstrate the effectiveness of reward-weighting both modalities and strategically designed post-training data. A targeted dataset addressing specific limitations achieves superior results compared to broad image-caption corpora or benchmark-aligned data.

Conclusion: Fully unified text-image generation through strategic post-training with reward-weighted synthetic data is effective for improving multimodal image generation performance, enabling autonomous transitions from textual reasoning to visual synthesis within a single inference process.

Abstract: Unified multimodal generation architectures that jointly produce text and images have recently emerged as a promising direction for text-to-image (T2I) synthesis. However, many existing systems rely on explicit modality switching, generating reasoning text before switching manually to image generation. This separate, sequential inference process limits cross-modal coupling and prohibits automatic multimodal generation. This work explores post-training to achieve fully unified text-image generation, where models autonomously transition from textual reasoning to visual synthesis within a single inference process. We examine the impact of joint text-image generation on T2I performance and the relative importance of each modality during post-training. We additionally explore different post-training data strategies, showing that a targeted dataset addressing specific limitations achieves superior results compared to broad image-caption corpora or benchmark-aligned data. Using offline, reward-weighted post-training with fully self-generated synthetic data, our approach enables improvements in multimodal image generation across four diverse T2I benchmarks, demonstrating the effectiveness of reward-weighting both modalities and strategically designed post-training data.

</details>


### [4] [ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers](https://arxiv.org/abs/2601.04342)
*Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.CV

TL;DR: ReHyAt introduces a recurrent hybrid attention mechanism combining softmax and linear attention for efficient video generation, reducing training cost by 99% while maintaining state-of-the-art quality.


<details>
  <summary>Details</summary>
Motivation: Current transformer-based video diffusion models suffer from quadratic attention complexity that limits scalability for longer sequences, creating a need for more efficient architectures that maintain high-quality generation.

Method: ReHyAt uses a Recurrent Hybrid Attention mechanism that combines softmax attention fidelity with linear attention efficiency, enabling chunk-wise recurrent reformulation with constant memory usage. It employs a lightweight distillation and finetuning pipeline from existing softmax-based models.

Result: ReHyAt reduces training cost by two orders of magnitude (~160 GPU hours vs. typical models), achieves state-of-the-art video quality on VBench and VBench-2.0, reduces attention cost from quadratic to linear, and enables practical scalability for long-duration video generation.

Conclusion: The hybrid attention design provides an efficient recipe for future bidirectional softmax-based models, unlocking practical scalability for long-duration and on-device video generation while maintaining competitive quality.

Abstract: Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.

</details>


### [5] [SCAR-GS: Spatial Context Attention for Residuals in Progressive Gaussian Splatting](https://arxiv.org/abs/2601.04348)
*Diego Revilla,Pooja Suresh,Anand Bhojan,Ooi Wei Tsang*

Main category: cs.CV

TL;DR: Progressive 3D Gaussian Splatting compression using Residual Vector Quantization with auto-regressive entropy model guided by multi-resolution hash grid for efficient feature compression.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting models have high storage requirements that hinder cloud/streaming deployment. Current progressive compression methods use scalar quantization which may not optimally capture correlations in high-dimensional feature vectors, limiting rate-distortion performance.

Method: Replace traditional methods with Residual Vector Quantization to compress primitive features. Use auto-regressive entropy model guided by multi-resolution hash grid to predict conditional probability of each successive transmitted index, enabling efficient compression of coarse and refinement layers.

Result: Not specified in abstract (paper likely presents experimental results showing improved compression efficiency compared to existing methods).

Conclusion: Proposed progressive codec with Residual Vector Quantization and auto-regressive entropy modeling provides more efficient compression for 3D Gaussian Splatting models, addressing storage limitations for cloud/streaming deployment.

Abstract: Recent advances in 3D Gaussian Splatting have allowed for real-time, high-fidelity novel view synthesis. Nonetheless, these models have significant storage requirements for large and medium-sized scenes, hindering their deployment over cloud and streaming services. Some of the most recent progressive compression techniques for these models rely on progressive masking and scalar quantization techniques to reduce the bitrate of Gaussian attributes using spatial context models. While effective, scalar quantization may not optimally capture the correlations of high-dimensional feature vectors, which can potentially limit the rate-distortion performance.
  In this work, we introduce a novel progressive codec for 3D Gaussian Splatting that replaces traditional methods with a more powerful Residual Vector Quantization approach to compress the primitive features. Our key contribution is an auto-regressive entropy model, guided by a multi-resolution hash grid, that accurately predicts the conditional probability of each successive transmitted index, allowing for coarse and refinement layers to be compressed with high efficiency.

</details>


### [6] [Comparative Analysis of Custom CNN Architectures versus Pre-trained Models and Transfer Learning: A Study on Five Bangladesh Datasets](https://arxiv.org/abs/2601.04352)
*Ibrahim Tanvir,Alif Ruslan,Sartaj Solaiman*

Main category: cs.CV

TL;DR: Custom CNNs vs pre-trained models (ResNet-18, VGG-16) tested on 5 Bangladeshi image datasets. Transfer learning with fine-tuning outperforms custom CNNs and feature extraction, achieving up to 76% accuracy improvement and perfect 100% on Road Damage dataset.


<details>
  <summary>Details</summary>
Motivation: To provide practical guidance for practitioners on selecting appropriate deep learning approaches by comparing custom-built CNNs against pre-trained architectures using different transfer learning strategies across diverse real-world image classification tasks from Bangladesh.

Method: Comparative analysis of custom CNNs vs pre-trained models (ResNet-18, VGG-16) using feature extraction and transfer learning approaches. Evaluated on five Bangladeshi image datasets: Footpath Vision, Auto Rickshaw Detection, Mango Image Classification, Paddy Variety Recognition, and Road Damage Detection.

Result: Transfer learning with fine-tuning consistently outperforms both custom CNNs and feature extraction methods, achieving 3-76% accuracy improvements. ResNet-18 with fine-tuning achieved 100% accuracy on Road Damage BD dataset. Custom CNNs have smaller model size (3.4M vs 11-134M parameters) and better training efficiency on simpler tasks.

Conclusion: Pre-trained models with transfer learning provide superior performance, especially for complex tasks with limited data, while custom CNNs offer advantages in model size and efficiency for simpler tasks. The research provides practical selection guidelines based on dataset characteristics, computational resources, and performance requirements.

Abstract: This study presents a comprehensive comparative analysis of custom-built Convolutional Neural Networks (CNNs) against popular pre-trained architectures (ResNet-18 and VGG-16) using both feature extraction and transfer learning approaches. We evaluated these models across five diverse image classification datasets from Bangladesh: Footpath Vision, Auto Rickshaw Detection, Mango Image Classification, Paddy Variety Recognition, and Road Damage Detection. Our experimental results demonstrate that transfer learning with fine-tuning consistently outperforms both custom CNNs built from scratch and feature extraction methods, achieving accuracy improvements ranging from 3% to 76% across different datasets. Notably, ResNet-18 with fine-tuning achieved perfect 100% accuracy on the Road Damage BD dataset. While custom CNNs offer advantages in model size (3.4M parameters vs. 11-134M for pre-trained models) and training efficiency on simpler tasks, pre-trained models with transfer learning provide superior performance, particularly on complex classification tasks with limited training data. This research provides practical insights for practitioners in selecting appropriate deep learning approaches based on dataset characteristics, computational resources, and performance requirements.

</details>


### [7] [PackCache: A Training-Free Acceleration Method for Unified Autoregressive Video Generation via Compact KV-Cache](https://arxiv.org/abs/2601.04359)
*Kunyang Li,Mubarak Shah,Yuzhang Shang*

Main category: cs.CV

TL;DR: PackCache: Training-free KV-cache compression method for unified autoregressive video generation that accelerates inference 1.7-2.2x for 48-frame sequences by exploiting spatiotemporal attention patterns.


<details>
  <summary>Details</summary>
Motivation: KV-cache size grows linearly with generated tokens, becoming the dominant bottleneck for inference efficiency and generative length in unified autoregressive models, especially for video generation where sequences are long.

Method: PackCache dynamically compacts KV cache using three mechanisms: condition anchoring (preserves semantic reference tokens), cross-frame decay modeling (allocates cache budget by temporal distance), and spatially preserving position embedding (maintains 3D structure under cache removal).

Result: Accelerates end-to-end generation by 1.7-2.2x on 48-frame sequences; final four frames (most expensive segment) see 2.6x acceleration on A40 and 3.7x on H20.

Conclusion: PackCache effectively addresses KV-cache bottleneck in unified autoregressive video generation, enabling longer-sequence generation with significant inference speedups through training-free cache management.

Abstract: A unified autoregressive model is a Transformer-based framework that addresses diverse multimodal tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grows linearly with the number of generated tokens, and it rapidly becomes the dominant bottleneck limiting inference efficiency and generative length. Unified autoregressive video generation inherits this limitation. Our analysis reveals that KV-cache tokens exhibit distinct spatiotemporal properties: (i) text and conditioning-image tokens act as persistent semantic anchors that consistently receive high attention, and (ii) attention to previous frames naturally decays with temporal distance. Leveraging these observations, we introduce PackCache, a training-free KV-cache management method that dynamically compacts the KV cache through three coordinated mechanisms: condition anchoring that preserves semantic references, cross-frame decay modeling that allocates cache budget according to temporal distance, and spatially preserving position embedding that maintains coherent 3D structure under cache removal. In terms of efficiency, PackCache accelerates end-to-end generation by 1.7-2.2x on 48-frame long sequences, showcasing its strong potential for enabling longer-sequence video generation. Notably, the final four frames - the portion most impacted by the progressively expanding KV-cache and thus the most expensive segment of the clip - PackCache delivers a 2.6x and 3.7x acceleration on A40 and H200, respectively, for 48-frame videos.

</details>


### [8] [Combining facial videos and biosignals for stress estimation during driving](https://arxiv.org/abs/2601.04376)
*Paraskevi Valergaki,Vassilis C. Nicodemou,Iason Oikonomidis,Antonis Argyros,Anastasios Roussos*

Main category: cs.CV

TL;DR: 3D facial geometry analysis reveals stress responses comparable to physiological markers, and cross-modal attention fusion achieves best performance for stress recognition from facial videos.


<details>
  <summary>Details</summary>
Motivation: Stress recognition from facial videos is challenging due to subjectivity and voluntary control. While most methods use Facial Action Units, the role of disentangled 3D facial geometry remains underexplored for stress recognition.

Method: Analyze stress during distracted driving using EMOCA-derived 3D expression and pose coefficients. Use paired hypothesis tests between baseline and stressor phases. Propose Transformer-based temporal modeling framework with unimodal, early-fusion, and cross-modal attention strategies.

Result: 41 of 56 EMOCA coefficients show consistent, phase-specific stress responses comparable to physiological markers. Cross-Modal Attention fusion of EMOCA and physiological signals achieves best performance (AUROC 92%, Accuracy 86.7%), with EMOCA-gaze fusion also competitive (AUROC 91.8%).

Conclusion: 3D facial geometry provides effective stress biomarkers comparable to physiological signals. Temporal modeling and cross-modal attention strategies are highly effective for stress recognition, with EMOCA-physiology fusion achieving state-of-the-art performance.

Abstract: Reliable stress recognition from facial videos is challenging due to stress's subjective nature and voluntary facial control. While most methods rely on Facial Action Units, the role of disentangled 3D facial geometry remains underexplored. We address this by analyzing stress during distracted driving using EMOCA-derived 3D expression and pose coefficients. Paired hypothesis tests between baseline and stressor phases reveal that 41 of 56 coefficients show consistent, phase-specific stress responses comparable to physiological markers. Building on this, we propose a Transformer-based temporal modeling framework and assess unimodal, early-fusion, and cross-modal attention strategies. Cross-Modal Attention fusion of EMOCA and physiological signals achieves best performance (AUROC 92\%, Accuracy 86.7\%), with EMOCA-gaze fusion also competitive (AUROC 91.8\%). This highlights the effectiveness of temporal modeling and cross-modal attention for stress recognition.

</details>


### [9] [Few-Shot LoRA Adaptation of a Flow-Matching Foundation Model for Cross-Spectral Object Detection](https://arxiv.org/abs/2601.04381)
*Maxim Clouser,Kia Khezeli,John Kalantari*

Main category: cs.CV

TL;DR: A flow-matching foundation model pre-trained on RGB images can be adapted with few-shot LoRA to translate RGB to non-visible modalities (IR/SAR), enabling synthetic data generation that improves downstream object detection in safety-critical applications.


<details>
  <summary>Details</summary>
Motivation: Foundation models are mainly trained on RGB data, but safety-critical applications often use non-visible modalities like IR and SAR. There's a need to bridge this gap without extensive paired data collection.

Method: Start with FLUX.1 Kontext foundation model, insert LoRA modules, and fine-tune on just 100 paired images per domain (RGB→IR on KAIST, RGB→SAR on M4-SAR). Use LPIPS on 50 held-out pairs as proxy for downstream performance to select best LoRA hyperparameters.

Result: Lower LPIPS consistently predicts higher mAP for YOLOv11n on both IR and SAR, and for DETR on KAIST IR. Synthetic IR from external RGB datasets improves KAIST IR pedestrian detection, and synthetic SAR boosts infrastructure detection on M4-SAR when combined with limited real SAR.

Conclusion: Few-shot LoRA adaptation of flow-matching foundation models is a promising approach for foundation-style support of non-visible modalities, enabling effective cross-spectral translation and synthetic data generation for downstream detection tasks.

Abstract: Foundation models for vision are predominantly trained on RGB data, while many safety-critical applications rely on non-visible modalities such as infrared (IR) and synthetic aperture radar (SAR). We study whether a single flow-matching foundation model pre-trained primarily on RGB images can be repurposed as a cross-spectral translator using only a few co-measured examples, and whether the resulting synthetic data can enhance downstream detection. Starting from FLUX.1 Kontext, we insert low-rank adaptation (LoRA) modules and fine-tune them on just 100 paired images per domain for two settings: RGB to IR on the KAIST dataset and RGB to SAR on the M4-SAR dataset. The adapted model translates RGB images into pixel-aligned IR/SAR, enabling us to reuse existing bounding boxes and train object detection models purely in the target modality. Across a grid of LoRA hyperparameters, we find that LPIPS computed on only 50 held-out pairs is a strong proxy for downstream performance: lower LPIPS consistently predicts higher mAP for YOLOv11n on both IR and SAR, and for DETR on KAIST IR test data. Using the best LPIPS-selected LoRA adapter, synthetic IR from external RGB datasets (LLVIP, FLIR ADAS) improves KAIST IR pedestrian detection, and synthetic SAR significantly boosts infrastructure detection on M4-SAR when combined with limited real SAR. Our results suggest that few-shot LoRA adaptation of flow-matching foundation models is a promising path toward foundation-style support for non-visible modalities.

</details>


### [10] [Performance Analysis of Image Classification on Bangladeshi Datasets](https://arxiv.org/abs/2601.04397)
*Mohammed Sami Khan,Fabiha Muniat,Rowzatul Zannat*

Main category: cs.CV

TL;DR: Custom CNN vs pre-trained models (VGG-16, ResNet-50, MobileNet) comparison for image classification shows pre-trained models outperform custom CNN in accuracy and convergence speed, especially with limited data, but custom CNN has fewer parameters and lower computational complexity.


<details>
  <summary>Details</summary>
Motivation: To address the practical consideration of choosing between custom-designed CNNs from scratch versus established pre-trained architectures for image classification tasks, examining trade-offs in performance, complexity, and computational efficiency.

Method: Comparative analysis of a custom-designed CNN trained from scratch against popular pre-trained architectures (VGG-16, ResNet-50, MobileNet) using transfer learning under identical experimental settings, evaluated with standard metrics (accuracy, precision, recall, F1-score).

Result: Pre-trained CNN architectures consistently outperform custom CNN in classification accuracy and convergence speed, particularly with limited training data, while custom CNN demonstrates competitive performance with significantly fewer parameters and reduced computational complexity.

Conclusion: The study highlights trade-offs between model complexity, performance, and computational efficiency, providing practical insights for selecting appropriate CNN architectures based on specific constraints like computational resources and data availability.

Abstract: Convolutional Neural Networks (CNNs) have demonstrated remarkable success in image classification tasks; however, the choice between designing a custom CNN from scratch and employing established pre-trained architectures remains an important practical consideration. In this work, we present a comparative analysis of a custom-designed CNN and several widely used deep learning architectures, including VGG-16, ResNet-50, and MobileNet, for an image classification task. The custom CNN is developed and trained from scratch, while the popular architectures are employed using transfer learning under identical experimental settings. All models are evaluated using standard performance metrics such as accuracy, precision, recall, and F1-score. Experimental results show that pre-trained CNN architectures consistently outperform the custom CNN in terms of classification accuracy and convergence speed, particularly when training data is limited. However, the custom CNN demonstrates competitive performance with significantly fewer parameters and reduced computational complexity. This study highlights the trade-offs between model complexity, performance, and computational efficiency, and provides practical insights into selecting appropriate CNN architectures for image classification problems.

</details>


### [11] [3D-Agent:Tri-Modal Multi-Agent Collaboration for Scalable 3D Object Annotation](https://arxiv.org/abs/2601.04404)
*Jusheng Zhang,Yijia Fan,Zimo Wen,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: Tri MARF is a tri-modal framework using 2D images, text descriptions, and 3D point clouds with multi-agent collaboration to improve large-scale 3D object annotation, outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: 3D object annotation is challenging due to spatial complexity, occlusion, and viewpoint inconsistency in applications like autonomous driving and augmented reality. Existing single-model approaches struggle with these issues effectively.

Method: Tri MARF integrates tri-modal inputs (2D multi-view images, textual descriptions, 3D point clouds) using a multi-agent collaborative architecture with three specialized agents: vision-language model agent for multi-view descriptions, information aggregation agent for optimal description selection, and gating agent for aligning textual semantics with 3D geometry.

Result: Tri MARF substantially outperforms existing methods, achieving CLIPScore of 88.7 (vs prior SOTA), retrieval accuracy of 45.2 and 43.8 on ViLT R@5, and throughput of up to 12,000 objects per hour on a single NVIDIA A100 GPU across Objaverse, LVIS, Objaverse XL, and ABO datasets.

Conclusion: The tri-modal multi-agent framework effectively addresses 3D annotation challenges by leveraging complementary information from different modalities, demonstrating superior performance and efficiency for large-scale 3D object annotation tasks.

Abstract: Driven by applications in autonomous driving robotics and augmented reality 3D object annotation presents challenges beyond 2D annotation including spatial complexity occlusion and viewpoint inconsistency Existing approaches based on single models often struggle to address these issues effectively We propose Tri MARF a novel framework that integrates tri modal inputs including 2D multi view images textual descriptions and 3D point clouds within a multi agent collaborative architecture to enhance large scale 3D annotation Tri MARF consists of three specialized agents a vision language model agent for generating multi view descriptions an information aggregation agent for selecting optimal descriptions and a gating agent that aligns textual semantics with 3D geometry for refined captioning Extensive experiments on Objaverse LVIS Objaverse XL and ABO demonstrate that Tri MARF substantially outperforms existing methods achieving a CLIPScore of 88 point 7 compared to prior state of the art methods retrieval accuracy of 45 point 2 and 43 point 8 on ViLT R at 5 and a throughput of up to 12000 objects per hour on a single NVIDIA A100 GPU

</details>


### [12] [From Preoperative CT to Postmastoidectomy Mesh Construction:1Mastoidectomy Shape Prediction for Cochlear Implant Surgery](https://arxiv.org/abs/2601.04405)
*Yike Zhang,Eduardo Davalos,Dingjie Su,Ange Lou,Jack Noble*

Main category: cs.CV

TL;DR: A hybrid self-supervised and weakly-supervised learning framework predicts mastoidectomy shape from preoperative CT scans without human annotations, achieving 0.72 Dice score for cochlear implant surgical planning.


<details>
  <summary>Details</summary>
Motivation: Accurate mastoidectomy shape prediction from preoperative imaging improves cochlear implant surgical planning, reduces risks, and enhances outcomes, but limited deep-learning studies exist due to challenges in acquiring ground-truth labels.

Method: Proposes a hybrid self-supervised and weakly-supervised learning framework that predicts mastoidectomy region directly from preoperative CT scans where the mastoid is intact, using 3D T-distribution loss in weakly-supervised medical imaging.

Result: Achieves mean Dice score of 0.72 for predicting complex, boundary-less mastoidectomy shape, surpassing state-of-the-art approaches and demonstrating strong performance.

Conclusion: First work integrating self-supervised and weakly-supervised learning for mastoidectomy shape prediction, providing robust solution for CI surgical planning and groundwork for constructing 3D postmastoidectomy surfaces from preoperative CT scans.

Abstract: Cochlear Implant (CI) surgery treats severe hearing loss by inserting an electrode array into the cochlea to stimulate the auditory nerve. An important step in this procedure is mastoidectomy, which removes part of the mastoid region of the temporal bone to provide surgical access. Accurate mastoidectomy shape prediction from preoperative imaging improves pre-surgical planning, reduces risks, and enhances surgical outcomes. Despite its importance, there are limited deep-learning-based studies regarding this topic due to the challenges of acquiring ground-truth labels. We address this gap by investigating self-supervised and weakly-supervised learning models to predict the mastoidectomy region without human annotations. We propose a hybrid self-supervised and weakly-supervised learning framework to predict the mastoidectomy region directly from preoperative CT scans, where the mastoid remains intact. Our hybrid method achieves a mean Dice score of 0.72 when predicting the complex and boundary-less mastoidectomy shape, surpassing state-of-the-art approaches and demonstrating strong performance. The method provides groundwork for constructing 3D postmastoidectomy surfaces directly from the corresponding preoperative CT scans. To our knowledge, this is the first work that integrating self-supervised and weakly-supervised learning for mastoidectomy shape prediction, offering a robust and efficient solution for CI surgical planning while leveraging 3D T-distribution loss in weakly-supervised medical imaging.

</details>


### [13] [CRUNet-MR-Univ: A Foundation Model for Diverse Cardiac MRI Reconstruction](https://arxiv.org/abs/2601.04428)
*Donghang Lyu,Marius Staring,Hildo Lamb,Mariya Doneva*

Main category: cs.CV

TL;DR: CRUNet-MR-Univ is a foundation model for Cardiac MRI reconstruction that uses spatio-temporal correlations and prompt-based priors to handle diverse CMR scan variations, outperforming baseline methods across multiple settings.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods for CMR reconstruction lack generalizability due to wide variability in CMR scans (image contrast, sampling patterns, scanner vendors, anatomical structures, disease types). Most models handle only narrow subsets of variations, leading to performance degradation with distribution shifts.

Method: CRUNet-MR-Univ leverages spatio-temporal correlations and prompt-based priors to create a unified foundation model capable of handling the full diversity of CMR scans.

Result: The proposed approach consistently outperforms baseline methods across a wide range of settings, demonstrating effectiveness and promise for real-world clinical applications.

Conclusion: CRUNet-MR-Univ represents a significant advancement in CMR reconstruction by addressing the generalizability challenge through a unified foundation model approach, showing potential for broader clinical adoption.

Abstract: In recent years, deep learning has attracted increasing at- tention in the field of Cardiac MRI (CMR) reconstruction due to its superior performance over traditional methods, particularly in handling higher acceleration factors, highlighting its potential for real-world clini- cal applications. However, current deep learning methods remain limited in generalizability. CMR scans exhibit wide variability in image contrast, sampling patterns, scanner vendors, anatomical structures, and disease types. Most existing models are designed to handle only a single or nar- row subset of these variations, leading to performance degradation when faced with distribution shifts. Therefore, it is beneficial to develop a unified model capable of generalizing across diverse CMR scenarios. To this end, we propose CRUNet-MR-Univ, a foundation model that lever- ages spatio-temporal correlations and prompt-based priors to effectively handle the full diversity of CMR scans. Our approach consistently out- performs baseline methods across a wide range of settings, highlighting its effectiveness and promise.

</details>


### [14] [Addressing Overthinking in Large Vision-Language Models via Gated Perception-Reasoning Optimization](https://arxiv.org/abs/2601.04442)
*Xingjian Diao,Zheyuan Liu,Chunhui Zhang,Weiyi Wu,Keyi Kong,Lin Shi,Kaize Ding,Soroush Vosoughi,Jiang Gui*

Main category: cs.CV

TL;DR: GPRO is a meta-reasoning controller that dynamically routes computation between fast, perception, and reasoning paths to address overthinking in LVLMs by fixing visual perception failures.


<details>
  <summary>Details</summary>
Motivation: Current chain-of-thought approaches in LVLMs lead to overthinking - excessively verbose responses for simple queries causing inefficiency and degraded accuracy. Prior adaptive reasoning methods overlook visual perception failures as a fundamental bottleneck.

Method: Proposes Gated Perception-Reasoning Optimization (GPRO), a meta-reasoning controller that dynamically routes computation among three paths: fast path, slow perception path for re-examining visual inputs, and slow reasoning path for self-reflection. Uses failure attribution supervision from 790k samples to distinguish perceptual hallucinations from reasoning errors, trained with multi-objective reinforcement learning.

Result: Experiments on five benchmarks show GPRO substantially improves both accuracy and efficiency, outperforming recent slow-thinking methods while generating significantly shorter responses.

Conclusion: Stable reasoning in LVLMs critically depends on low-level visual grounding, and GPRO effectively addresses overthinking by dynamically managing perception-reasoning trade-offs through meta-reasoning control.

Abstract: Large Vision-Language Models (LVLMs) have exhibited strong reasoning capabilities through chain-of-thought mechanisms that generate step-by-step rationales. However, such slow-thinking approaches often lead to overthinking, where models produce excessively verbose responses even for simple queries, resulting in test-time inefficiency and even degraded accuracy. Prior work has attempted to mitigate this issue via adaptive reasoning strategies, but these methods largely overlook a fundamental bottleneck: visual perception failures. We argue that stable reasoning critically depends on low-level visual grounding, and that reasoning errors often originate from imperfect perception rather than insufficient deliberation. To address this limitation, we propose Gated Perception-Reasoning Optimization (GPRO), a meta-reasoning controller that dynamically routes computation among three decision paths at each generation step: a lightweight fast path, a slow perception path for re-examining visual inputs, and a slow reasoning path for internal self-reflection. To learn this distinction, we derive large-scale failure attribution supervision from approximately 790k samples, using teacher models to distinguish perceptual hallucinations from reasoning errors. We then train the controller with multi-objective reinforcement learning to optimize the trade-off between task accuracy and computational cost under uncertainty. Experiments on five benchmarks demonstrate that GPRO substantially improves both accuracy and efficiency, outperforming recent slow-thinking methods while generating significantly shorter responses.

</details>


### [15] [UniDrive-WM: Unified Understanding, Planning and Generation World Model For Autonomous Driving](https://arxiv.org/abs/2601.04453)
*Zhexiao Xiong,Xin Ye,Burhan Yaman,Sheng Cheng,Yiren Lu,Jingru Luo,Nathan Jacobs,Liu Ren*

Main category: cs.CV

TL;DR: UniDrive-WM is a unified vision-language model that jointly performs driving scene understanding, trajectory planning, and future image generation in a single architecture, outperforming previous methods on autonomous driving benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving systems treat perception, prediction, and planning as separate modules, which limits their ability to leverage synergies between these tasks. The authors aim to create a unified world model that tightly integrates these components for better performance.

Method: UniDrive-WM uses a single VLM-based architecture that: 1) predicts future trajectories, 2) conditions a VLM-based image generator on these trajectories to produce future frames, and 3) uses these predictions as supervisory signals to iteratively refine both scene understanding and trajectory generation. The authors also compare discrete vs continuous output representations for future image prediction.

Result: On the Bench2Drive benchmark, UniDrive-WM achieves 5.9% improvement in L2 trajectory error and 9.2% reduction in collision rate compared to previous best methods. The model also produces high-fidelity future images that enhance planning performance.

Conclusion: Tight integration of VLM-driven reasoning, planning, and generative world modeling in a unified architecture significantly improves autonomous driving performance, demonstrating the advantages of joint learning over modular approaches.

Abstract: World models have become central to autonomous driving, where accurate scene understanding and future prediction are crucial for safe control. Recent work has explored using vision-language models (VLMs) for planning, yet existing approaches typically treat perception, prediction, and planning as separate modules. We propose UniDrive-WM, a unified VLM-based world model that jointly performs driving-scene understanding, trajectory planning, and trajectory-conditioned future image generation within a single architecture. UniDrive-WM's trajectory planner predicts a future trajectory, which conditions a VLM-based image generator to produce plausible future frames. These predictions provide additional supervisory signals that enhance scene understanding and iteratively refine trajectory generation. We further compare discrete and continuous output representations for future image prediction, analyzing their influence on downstream driving performance. Experiments on the challenging Bench2Drive benchmark show that UniDrive-WM produces high-fidelity future images and improves planning performance by 5.9% in L2 trajectory error and 9.2% in collision rate over the previous best method. These results demonstrate the advantages of tightly integrating VLM-driven reasoning, planning, and generative world modeling for autonomous driving. The project page is available at https://unidrive-wm.github.io/UniDrive-WM .

</details>


### [16] [Vision-Language Agents for Interactive Forest Change Analysis](https://arxiv.org/abs/2601.04497)
*James Brock,Ce Zhang,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: LLM-driven agent integrates vision-language models for forest change analysis, supporting natural language queries across multiple remote sensing image change interpretation tasks, with new Forest-Change dataset.


<details>
  <summary>Details</summary>
Motivation: Address the gap in integrating large language models with vision-language models for remote sensing image change interpretation, particularly for complex forest dynamics monitoring.

Method: Proposes an LLM-driven agent with multi-level change interpretation vision-language backbone and LLM-based orchestration, using new Forest-Change dataset with bi-temporal satellite imagery, change masks, and semantic captions.

Result: Achieves 67.10% mIoU and 40.17% BLEU-4 on Forest-Change dataset, and 88.13% mIoU and 34.41% BLEU-4 on LEVIR-MCI-Trees subset, demonstrating effective joint change detection and captioning.

Conclusion: LLM-driven RSICI systems improve accessibility, interpretability, and efficiency of forest change analysis, with publicly available data and code for community use.

Abstract: Modern forest monitoring workflows increasingly benefit from the growing availability of high-resolution satellite imagery and advances in deep learning. Two persistent challenges in this context are accurate pixel-level change detection and meaningful semantic change captioning for complex forest dynamics. While large language models (LLMs) are being adapted for interactive data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored. To address this gap, we introduce an LLM-driven agent for integrated forest change analysis that supports natural language querying across multiple RSICI tasks. The proposed system builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration. To facilitate adaptation and evaluation in forest environments, we further introduce the Forest-Change dataset, which comprises bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated using a combination of human annotation and rule-based methods. Experimental results show that the proposed system achieves mIoU and BLEU-4 scores of 67.10% and 40.17% on the Forest-Change dataset, and 88.13% and 34.41% on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI benchmark for joint change detection and captioning. These results highlight the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and efficiency of forest change analysis. All data and code are publicly available at https://github.com/JamesBrockUoB/ForestChat.

</details>


### [17] [TokenSeg: Efficient 3D Medical Image Segmentation via Hierarchical Visual Token Compression](https://arxiv.org/abs/2601.04519)
*Sen Zeng,Hong Zhou,Zheng Zhu,Yang Liu*

Main category: cs.CV

TL;DR: TokenSeg is a boundary-aware sparse token representation framework for efficient 3D medical image segmentation that reduces computational costs while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: 3D medical image segmentation is computationally demanding due to cubic voxel growth and redundant computation on homogeneous regions. There's a need for efficient methods that can handle large volumes while maintaining accuracy.

Method: Three main components: (1) multi-scale hierarchical encoder extracts 400 candidate tokens across four resolution levels; (2) boundary-aware tokenizer combines VQ-VAE quantization with importance scoring to select 100 salient tokens (60% near boundaries); (3) sparse-to-dense decoder reconstructs full-resolution masks through token reprojection, progressive upsampling, and skip connections.

Result: Achieves state-of-the-art performance on 3D breast DCE-MRI dataset (960 cases) with 94.49% Dice and 89.61% IoU, while reducing GPU memory by 64% and inference latency by 68%. Also demonstrates strong generalization on MSD cardiac and brain MRI datasets.

Conclusion: TokenSeg demonstrates the effectiveness of anatomically informed sparse representation for accurate and efficient 3D medical image segmentation, with significant computational savings while maintaining high performance across diverse anatomical structures.

Abstract: Three-dimensional medical image segmentation is a fundamental yet computationally demanding task due to the cubic growth of voxel processing and the redundant computation on homogeneous regions. To address these limitations, we propose \textbf{TokenSeg}, a boundary-aware sparse token representation framework for efficient 3D medical volume segmentation. Specifically, (1) we design a \emph{multi-scale hierarchical encoder} that extracts 400 candidate tokens across four resolution levels to capture both global anatomical context and fine boundary details; (2) we introduce a \emph{boundary-aware tokenizer} that combines VQ-VAE quantization with importance scoring to select 100 salient tokens, over 60\% of which lie near tumor boundaries; and (3) we develop a \emph{sparse-to-dense decoder} that reconstructs full-resolution masks through token reprojection, progressive upsampling, and skip connections. Extensive experiments on a 3D breast DCE-MRI dataset comprising 960 cases demonstrate that TokenSeg achieves state-of-the-art performance with 94.49\% Dice and 89.61\% IoU, while reducing GPU memory and inference latency by 64\% and 68\%, respectively. To verify the generalization capability, our evaluations on MSD cardiac and brain MRI benchmark datasets demonstrate that TokenSeg consistently delivers optimal performance across heterogeneous anatomical structures. These results highlight the effectiveness of anatomically informed sparse representation for accurate and efficient 3D medical image segmentation.

</details>


### [18] [FaceRefiner: High-Fidelity Facial Texture Refinement with Differentiable Rendering-based Style Transfer](https://arxiv.org/abs/2601.04520)
*Chengyang Li,Baoping Cheng,Yao Cheng,Haocheng Zhang,Renshuai Liu,Yinglin Zheng,Jing Liao,Xuan Cheng*

Main category: cs.CV

TL;DR: FaceRefiner is a style transfer-based facial texture refinement method that improves texture quality and identity preservation by transferring multi-level information from input images to generated textures through differentiable rendering.


<details>
  <summary>Details</summary>
Motivation: Current facial texture generation methods use deep networks to synthesize textures and fill UV maps, but these textures come from training data or 2D face generator spaces, limiting generalization for in-the-wild images. This causes inconsistencies in facial details, structures, and identity with the input.

Method: FaceRefiner treats 3D sampled texture as style and texture generation output as content, using style transfer to transfer photo-realistic style. Unlike traditional style transfer that only transfers high/middle level information, FaceRefiner integrates differentiable rendering to also transfer low-level (pixel-level) information in visible face regions, enabling multi-level information transfer.

Result: Extensive experiments on Multi-PIE, CelebA and FFHQ datasets demonstrate that FaceRefiner improves texture quality and face identity preserving ability compared with state-of-the-art methods.

Conclusion: The proposed style transfer-based refinement method effectively preserves input details, structures and semantics while improving texture quality and identity consistency for facial texture generation.

Abstract: Recent facial texture generation methods prefer to use deep networks to synthesize image content and then fill in the UV map, thus generating a compelling full texture from a single image. Nevertheless, the synthesized texture UV map usually comes from a space constructed by the training data or the 2D face generator, which limits the methods' generalization ability for in-the-wild input images. Consequently, their facial details, structures and identity may not be consistent with the input. In this paper, we address this issue by proposing a style transfer-based facial texture refinement method named FaceRefiner. FaceRefiner treats the 3D sampled texture as style and the output of a texture generation method as content. The photo-realistic style is then expected to be transferred from the style image to the content image. Different from current style transfer methods that only transfer high and middle level information to the result, our style transfer method integrates differentiable rendering to also transfer low level (or pixel level) information in the visible face regions. The main benefit of such multi-level information transfer is that, the details, structures and semantics in the input can thus be well preserved. The extensive experiments on Multi-PIE, CelebA and FFHQ datasets demonstrate that our refinement method can improve the texture quality and the face identity preserving ability, compared with state-of-the-arts.

</details>


### [19] [All Changes May Have Invariant Principles: Improving Ever-Shifting Harmful Meme Detection via Design Concept Reproduction](https://arxiv.org/abs/2601.04567)
*Ziyou Jiang,Mingyang Li,Junjie Wang,Yuekai Huang,Jie Huang,Zhiyuan Chang,Zhaoyang Li,Qing Wang*

Main category: cs.CV

TL;DR: RepMD is a harmful meme detection method that uses design concept reproduction to identify invariant principles behind shifting memes, achieving 81.1% accuracy with good generalization to type-shifting and temporal-evolving content.


<details>
  <summary>Details</summary>
Motivation: Harmful memes constantly evolve in internet communities, making them difficult to analyze due to their type-shifting and temporal-evolving nature. The authors recognize that while memes shift, they may share invariant design principles used by malicious users, which could help understand why they're harmful.

Method: 1) Define Design Concept Graph (DCG) based on attack trees to describe steps for designing harmful memes. 2) Derive DCG from historical memes using design step reproduction and graph pruning. 3) Use DCG to guide Multimodal Large Language Models (MLLMs) for harmful meme detection.

Result: RepMD achieves 81.1% accuracy, the highest among compared methods. It shows only slight accuracy decreases when generalized to type-shifting and temporal-evolving memes. Human evaluation shows it improves human discovery efficiency to 15-30 seconds per meme.

Conclusion: RepMD effectively detects harmful memes by capturing invariant design principles, demonstrating robustness against meme evolution and improving human moderation efficiency.

Abstract: Harmful memes are ever-shifting in the Internet communities, which are difficult to analyze due to their type-shifting and temporal-evolving nature. Although these memes are shifting, we find that different memes may share invariant principles, i.e., the underlying design concept of malicious users, which can help us analyze why these memes are harmful. In this paper, we propose RepMD, an ever-shifting harmful meme detection method based on the design concept reproduction. We first refer to the attack tree to define the Design Concept Graph (DCG), which describes steps that people may take to design a harmful meme. Then, we derive the DCG from historical memes with design step reproduction and graph pruning. Finally, we use DCG to guide the Multimodal Large Language Model (MLLM) to detect harmful memes. The evaluation results show that RepMD achieves the highest accuracy with 81.1% and has slight accuracy decreases when generalized to type-shifting and temporal-evolving memes. Human evaluation shows that RepMD can improve the efficiency of human discovery on harmful memes, with 15$\sim$30 seconds per meme.

</details>


### [20] [3D Conditional Image Synthesis of Left Atrial LGE MRI from Composite Semantic Masks](https://arxiv.org/abs/2601.04588)
*Yusri Al-Sanaani,Rebecca Thornhill,Sreeraman Rajan*

Main category: cs.CV

TL;DR: 3D conditional generative models (Pix2Pix GAN, SPADE-GAN, SPADE-LDM) synthesize LGE MRI from label maps to augment scarce training data, improving left atrial segmentation performance.


<details>
  <summary>Details</summary>
Motivation: Segmentation of left atrial wall and endocardium from LGE MRI is essential for quantifying atrial fibrosis, but accurate machine learning models are challenging due to limited data availability and anatomical complexity.

Method: Developed pipeline to synthesize 3D LGE MRI volumes from composite semantic label maps using three 3D conditional generators (Pix2Pix GAN, SPADE-GAN, SPADE-LDM). Synthetic images were evaluated for realism and impact on downstream LA segmentation with 3D U-Net.

Result: SPADE-LDM generated most realistic images with FID of 4.063, outperforming Pix2Pix (40.821) and SPADE-GAN (7.652). Augmentation with synthetic images improved LA cavity segmentation Dice score from 0.908 to 0.936, statistically significant (p < 0.05).

Conclusion: Label-conditioned 3D synthesis can enhance segmentation of under-represented cardiac structures by generating realistic training data, with SPADE-LDM showing superior performance over GAN-based approaches.

Abstract: Segmentation of the left atrial (LA) wall and endocardium from late gadolinium-enhanced (LGE) MRI is essential for quantifying atrial fibrosis in patients with atrial fibrillation. The development of accurate machine learning-based segmentation models remains challenging due to the limited availability of data and the complexity of anatomical structures. In this work, we investigate 3D conditional generative models as potential solution for augmenting scarce LGE training data and improving LA segmentation performance. We develop a pipeline to synthesize high-fidelity 3D LGE MRI volumes from composite semantic label maps combining anatomical expert annotations with unsupervised tissue clusters, using three 3D conditional generators (Pix2Pix GAN, SPADE-GAN, and SPADE-LDM). The synthetic images are evaluated for realism and their impact on downstream LA segmentation. SPADE-LDM generates the most realistic and structurally accurate images, achieving an FID of 4.063 and surpassing GAN models, which have FIDs of 40.821 and 7.652 for Pix2Pix and SPADE-GAN, respectively. When augmented with synthetic LGE images, the Dice score for LA cavity segmentation with a 3D U-Net model improved from 0.908 to 0.936, showing a statistically significant improvement (p < 0.05) over the baseline.These findings demonstrate the potential of label-conditioned 3D synthesis to enhance the segmentation of under-represented cardiac structures.

</details>


### [21] [MiLDEdit: Reasoning-Based Multi-Layer Design Document Editing](https://arxiv.org/abs/2601.04589)
*Zihao Lin,Wanrong Zhu,Jiuxiang Gu,Jihyung Kil,Christopher Tensmeyer,Lin Zhang,Shilong Liu,Ruiyi Zhang,Lifu Huang,Vlad I. Morariu,Tong Sun*

Main category: cs.CV

TL;DR: MiLDEAgent is a reasoning-based framework for multi-layer design document editing that combines RL-trained multimodal reasoning with targeted image editing, significantly outperforming existing methods on a new benchmark.


<details>
  <summary>Details</summary>
Motivation: Real-world design documents are multi-layered (decoration, text, images), but prior work overlooks multi-layer editing, focusing on single-layer image editing or multi-layer generation which lack the reasoning needed to identify relevant layers and coordinate modifications.

Method: MiLDEAgent combines an RL-trained multimodal reasoner for layer-wise understanding with an image editor for targeted modifications. Also introduces MiLDEBench (20K+ design documents with editing instructions) and MiLDEEval evaluation protocol across four dimensions.

Result: Existing approaches fail: open-source models can't complete tasks, closed-source models suffer format violations. MiLDEAgent achieves strong layer-aware reasoning and precise editing, outperforming all open-source baselines and matching closed-source model performance.

Conclusion: MiLDEAgent establishes the first strong baseline for multi-layer document editing, demonstrating effective layer-aware reasoning and precise modifications for real-world design document editing tasks.

Abstract: Real-world design documents (e.g., posters) are inherently multi-layered, combining decoration, text, and images. Editing them from natural-language instructions requires fine-grained, layer-aware reasoning to identify relevant layers and coordinate modifications. Prior work largely overlooks multi-layer design document editing, focusing instead on single-layer image editing or multi-layer generation, which assume a flat canvas and lack the reasoning needed to determine what and where to modify. To address this gap, we introduce the Multi-Layer Document Editing Agent (MiLDEAgent), a reasoning-based framework that combines an RL-trained multimodal reasoner for layer-wise understanding with an image editor for targeted modifications. To systematically benchmark this setting, we introduce the MiLDEBench, a human-in-the-loop corpus of over 20K design documents paired with diverse editing instructions. The benchmark is complemented by a task-specific evaluation protocol, MiLDEEval, which spans four dimensions including instruction following, layout consistency, aesthetics, and text rendering. Extensive experiments on 14 open-source and 2 closed-source models reveal that existing approaches fail to generalize: open-source models often cannot complete multi-layer document editing tasks, while closed-source models suffer from format violations. In contrast, MiLDEAgent achieves strong layer-aware reasoning and precise editing, significantly outperforming all open-source baselines and attaining performance comparable to closed-source models, thereby establishing the first strong baseline for multi-layer document editing.

</details>


### [22] [Detection of Deployment Operational Deviations for Safety and Security of AI-Enabled Human-Centric Cyber Physical Systems](https://arxiv.org/abs/2601.04605)
*Bernard Ngabonziza,Ayan Banerjee,Sandeep K. S. Gupta*

Main category: cs.CV

TL;DR: This paper proposes a framework for evaluating safety/security strategies in AI-enabled human-centric cyber-physical systems, with a case study on meal detection in diabetes management.


<details>
  <summary>Details</summary>
Motivation: AI-enabled human-centric systems (medical monitoring, autonomous cars) face operational uncertainties when interacting with humans, potentially violating safety/security requirements. These operational deviations in unknown conditions need systematic evaluation.

Method: 1) Discuss operational deviations leading to unknown conditions; 2) Create a framework to evaluate different safety/security strategies; 3) Demonstrate with personalized image-based technique for detecting non-announcement of meals in closed-loop blood glucose control for Type 1 diabetics.

Result: The paper presents a framework for evaluating safety/security strategies and demonstrates a novel image-based technique for meal detection in diabetes management systems.

Conclusion: A systematic framework is needed to evaluate safety/security strategies for AI-enabled human-centric systems operating in uncertain conditions, with practical applications in critical domains like medical monitoring.

Abstract: In recent years, Human-centric cyber-physical systems have increasingly involved artificial intelligence to enable knowledge extraction from sensor-collected data. Examples include medical monitoring and control systems, as well as autonomous cars. Such systems are intended to operate according to the protocols and guidelines for regular system operations. However, in many scenarios, such as closed-loop blood glucose control for Type 1 diabetics, self-driving cars, and monitoring systems for stroke diagnosis. The operations of such AI-enabled human-centric applications can expose them to cases for which their operational mode may be uncertain, for instance, resulting from the interactions with a human with the system. Such cases, in which the system is in uncertain conditions, can violate the system's safety and security requirements. 
  This paper will discuss operational deviations that can lead these systems to operate in unknown conditions. We will then create a framework to evaluate different strategies for ensuring the safety and security of AI-enabled human-centric cyber-physical systems in operation deployment. Then, as an example, we show a personalized image-based novel technique for detecting the non-announcement of meals in closed-loop blood glucose control for Type 1 diabetics.

</details>


### [23] [HUR-MACL: High-Uncertainty Region-Guided Multi-Architecture Collaborative Learning for Head and Neck Multi-Organ Segmentation](https://arxiv.org/abs/2601.04607)
*Xiaoyu Liu,Siwen Wei,Linhao Qu,Mingyuan Pan,Chengsheng Zhang,Yonghong Shi,Zhijian Song*

Main category: cs.CV

TL;DR: HUR-MACL model uses uncertainty-guided multi-architecture collaboration for improved head and neck organ segmentation, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Deep learning models struggle with small, complexly shaped organs in head and neck segmentation. Existing hybrid architectures simply concatenate features without exploiting each component's unique strengths, leading to functional overlap and limited accuracy.

Method: Proposes HUR-MACL model that: 1) adaptively identifies high uncertainty regions using CNN, 2) uses Vision Mamba and Deformable CNN to jointly improve segmentation in these regions, and 3) introduces heterogeneous feature distillation loss to promote collaborative learning between architectures in high uncertainty regions.

Result: Achieves state-of-the-art results on two public datasets and one private dataset for multi-organ segmentation in head and neck.

Conclusion: The proposed uncertainty-guided multi-architecture collaborative learning approach effectively addresses limitations of existing hybrid models and significantly improves segmentation accuracy for challenging head and neck organs.

Abstract: Accurate segmentation of organs at risk in the head and neck is essential for radiation therapy, yet deep learning models often fail on small, complexly shaped organs. While hybrid architectures that combine different models show promise, they typically just concatenate features without exploiting the unique strengths of each component. This results in functional overlap and limited segmentation accuracy. To address these issues, we propose a high uncertainty region-guided multi-architecture collaborative learning (HUR-MACL) model for multi-organ segmentation in the head and neck. This model adaptively identifies high uncertainty regions using a convolutional neural network, and for these regions, Vision Mamba as well as Deformable CNN are utilized to jointly improve their segmentation accuracy. Additionally, a heterogeneous feature distillation loss was proposed to promote collaborative learning between the two architectures in high uncertainty regions to further enhance performance. Our method achieves SOTA results on two public datasets and one private dataset.

</details>


### [24] [HyperAlign: Hyperbolic Entailment Cones for Adaptive Text-to-Image Alignment Assessment](https://arxiv.org/abs/2601.04614)
*Wenzhi Chen,Bo Hu,Leida Li,Lihuo He,Wen Lu,Xinbo Gao*

Main category: cs.CV

TL;DR: HyperAlign: Adaptive text-to-image alignment assessment using hyperbolic geometry for better semantic structure modeling and sample-level adaptation.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image alignment assessment methods use Euclidean space metrics that neglect semantic structure and lack adaptive capabilities for different samples, creating a critical challenge in evaluating generated images.

Method: Three-step approach: 1) Extract CLIP features and map to hyperbolic space, 2) Design dynamic-supervision entailment modeling to transform discrete entailment logic into continuous geometric structure supervision, 3) Use adaptive modulation regressor with hyperbolic geometric features to generate sample-level parameters that calibrate Euclidean cosine similarity for final scoring.

Result: HyperAlign achieves highly competitive performance on both single database evaluation and cross-database generalization tasks, demonstrating effectiveness of hyperbolic geometric modeling for image-text alignment assessment.

Conclusion: Hyperbolic geometric modeling effectively addresses limitations of Euclidean space metrics for text-to-image alignment assessment, providing better semantic structure modeling and adaptive capabilities through hyperbolic entailment geometry.

Abstract: With the rapid development of text-to-image generation technology, accurately assessing the alignment between generated images and text prompts has become a critical challenge. Existing methods rely on Euclidean space metrics, neglecting the structured nature of semantic alignment, while lacking adaptive capabilities for different samples. To address these limitations, we propose HyperAlign, an adaptive text-to-image alignment assessment framework based on hyperbolic entailment geometry. First, we extract Euclidean features using CLIP and map them to hyperbolic space. Second, we design a dynamic-supervision entailment modeling mechanism that transforms discrete entailment logic into continuous geometric structure supervision. Finally, we propose an adaptive modulation regressor that utilizes hyperbolic geometric features to generate sample-level modulation parameters, adaptively calibrating Euclidean cosine similarity to predict the final score. HyperAlign achieves highly competitive performance on both single database evaluation and cross-database generalization tasks, fully validating the effectiveness of hyperbolic geometric modeling for image-text alignment assessment.

</details>


### [25] [Agri-R1: Empowering Generalizable Agricultural Reasoning in Vision-Language Models with Reinforcement Learning](https://arxiv.org/abs/2601.04672)
*Wentao Zhang,Lifei Wang,Lina Lu,MingKun Xu,Shangyang Li,Yanchao Yang,Tao Fang*

Main category: cs.CV

TL;DR: Agri-R1: A 3B-parameter reasoning-enhanced model for agricultural disease diagnosis that uses automated reasoning data generation and GRPO training, achieving competitive performance with larger models while using only 19% of samples.


<details>
  <summary>Details</summary>
Motivation: Agricultural disease diagnosis challenges VLMs due to: 1) conventional fine-tuning requiring extensive labels, 2) lack of interpretability, 3) poor generalization, 4) existing reasoning methods relying on costly expert annotations, and 5) rarely addressing open-ended, diverse agricultural queries.

Method: 1) Automated high-quality reasoning data generation via vision-language synthesis and LLM-based filtering (using only 19% of available samples). 2) Training with Group Relative Policy Optimization (GRPO) with novel reward function integrating domain-specific lexicons and fuzzy matching to assess correctness and linguistic flexibility in open-ended responses.

Result: On CDDMBench: 3B-parameter model achieves performance competitive with 7B- to 13B-parameter baselines, with +23.2% relative gain in disease recognition accuracy, +33.3% in agricultural knowledge QA, and +26.10-point improvement in cross-domain generalization over standard fine-tuning.

Conclusion: The synergy between structured reasoning data and GRPO-driven exploration underpins performance gains, with benefits scaling as question complexity increases. The approach addresses key limitations in agricultural VLM fine-tuning through automated reasoning data generation and specialized training.

Abstract: Agricultural disease diagnosis challenges VLMs, as conventional fine-tuning requires extensive labels, lacks interpretability, and generalizes poorly. While reasoning improves model robustness, existing methods rely on costly expert annotations and rarely address the open-ended, diverse nature of agricultural queries. To address these limitations, we propose \textbf{Agri-R1}, a reasoning-enhanced large model for agriculture. Our framework automates high-quality reasoning data generation via vision-language synthesis and LLM-based filtering, using only 19\% of available samples. Training employs Group Relative Policy Optimization (GRPO) with a novel proposed reward function that integrates domain-specific lexicons and fuzzy matching to assess both correctness and linguistic flexibility in open-ended responses. Evaluated on CDDMBench, our resulting 3B-parameter model achieves performance competitive with 7B- to 13B-parameter baselines, showing a +23.2\% relative gain in disease recognition accuracy, +33.3\% in agricultural knowledge QA, and a +26.10-point improvement in cross-domain generalization over standard fine-tuning. Ablation studies confirm that the synergy between structured reasoning data and GRPO-driven exploration underpins these gains, with benefits scaling as question complexity increases.

</details>


### [26] [DB-MSMUNet:Dual Branch Multi-scale Mamba UNet for Pancreatic CT Scans Segmentation](https://arxiv.org/abs/2601.04676)
*Qiu Guan,Zhiqiang Yang,Dezhang Ye,Yang Chen,Xinli Xu,Ying Tang*

Main category: cs.CV

TL;DR: DB-MSMUNet: A dual-branch multi-scale Mamba UNet architecture for accurate pancreas and lesion segmentation in CT scans, achieving state-of-the-art performance across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Pancreas segmentation in CT scans is challenging due to low tissue contrast, blurry boundaries, irregular organ shapes, and small lesion sizes, which hinders accurate diagnosis and treatment of pancreatic cancer.

Method: Proposes DB-MSMUNet with encoder using Multi-scale Mamba Module (MSMM) combining deformable convolutions and multi-scale state space modeling. Features dual decoders: edge decoder with Edge Enhancement Path for boundary refinement, and area decoder with Multi-layer Decoder for fine details. Includes Auxiliary Deep Supervision heads at multiple scales.

Result: Achieves Dice Similarity Coefficients of 89.47% (NIH), 87.59% (MSD), and 89.02% (clinical tumor dataset), outperforming most state-of-the-art methods in segmentation accuracy, edge preservation, and robustness.

Conclusion: DB-MSMUNet demonstrates effectiveness and generalizability for real-world pancreatic CT segmentation tasks, addressing key challenges through its innovative architecture design.

Abstract: Accurate segmentation of the pancreas and its lesions in CT scans is crucial for the precise diagnosis and treatment of pancreatic cancer. However, it remains a highly challenging task due to several factors such as low tissue contrast with surrounding organs, blurry anatomical boundaries, irregular organ shapes, and the small size of lesions. To tackle these issues, we propose DB-MSMUNet (Dual-Branch Multi-scale Mamba UNet), a novel encoder-decoder architecture designed specifically for robust pancreatic segmentation. The encoder is constructed using a Multi-scale Mamba Module (MSMM), which combines deformable convolutions and multi-scale state space modeling to enhance both global context modeling and local deformation adaptation. The network employs a dual-decoder design: the edge decoder introduces an Edge Enhancement Path (EEP) to explicitly capture boundary cues and refine fuzzy contours, while the area decoder incorporates a Multi-layer Decoder (MLD) to preserve fine-grained details and accurately reconstruct small lesions by leveraging multi-scale deep semantic features. Furthermore, Auxiliary Deep Supervision (ADS) heads are added at multiple scales to both decoders, providing more accurate gradient feedback and further enhancing the discriminative capability of multi-scale features. We conduct extensive experiments on three datasets: the NIH Pancreas dataset, the MSD dataset, and a clinical pancreatic tumor dataset provided by collaborating hospitals. DB-MSMUNet achieves Dice Similarity Coefficients of 89.47%, 87.59%, and 89.02%, respectively, outperforming most existing state-of-the-art methods in terms of segmentation accuracy, edge preservation, and robustness across different datasets. These results demonstrate the effectiveness and generalizability of the proposed method for real-world pancreatic CT segmentation tasks.

</details>


### [27] [HATIR: Heat-Aware Diffusion for Turbulent Infrared Video Super-Resolution](https://arxiv.org/abs/2601.04682)
*Yang Zou,Xingyue Zhu,Kaiqi Han,Jun Ma,Xingyuan Li,Zhiying Jiang,Jinyuan Liu*

Main category: cs.CV

TL;DR: HATIR is a diffusion-based method for joint turbulence mitigation and super-resolution of infrared videos, using heat-aware deformation priors and phasor-guided flow estimation to address atmospheric turbulence and compression degradation.


<details>
  <summary>Details</summary>
Motivation: Infrared videos suffer from severe atmospheric turbulence and compression degradation, but existing VSR methods either ignore the modality gap between infrared/visible images or fail to restore turbulence-induced distortions. Cascading TM with VSR leads to error propagation due to decoupled degradation modeling.

Method: HATIR injects heat-aware deformation priors into diffusion sampling to jointly model inverse processes of turbulent degradation and detail loss. It uses a Phasor-Guided Flow Estimator based on thermal consistency principles, and a Turbulence-Aware Decoder with turbulence gating and structure-aware attention for stable feature aggregation.

Result: The authors built FLIR-IVSR, the first dataset for turbulent infrared VSR with 640 diverse scenes from FLIR T1050sc camera (1024x768). The method shows improved performance by jointly addressing turbulence and resolution issues.

Conclusion: HATIR effectively addresses the joint problem of turbulence mitigation and super-resolution for infrared videos through diffusion modeling with heat-aware priors, enabling more reliable infrared video enhancement in challenging environments.

Abstract: Infrared video has been of great interest in visual tasks under challenging environments, but often suffers from severe atmospheric turbulence and compression degradation. Existing video super-resolution (VSR) methods either neglect the inherent modality gap between infrared and visible images or fail to restore turbulence-induced distortions. Directly cascading turbulence mitigation (TM) algorithms with VSR methods leads to error propagation and accumulation due to the decoupled modeling of degradation between turbulence and resolution. We introduce HATIR, a Heat-Aware Diffusion for Turbulent InfraRed Video Super-Resolution, which injects heat-aware deformation priors into the diffusion sampling path to jointly model the inverse process of turbulent degradation and structural detail loss. Specifically, HATIR constructs a Phasor-Guided Flow Estimator, rooted in the physical principle that thermally active regions exhibit consistent phasor responses over time, enabling reliable turbulence-aware flow to guide the reverse diffusion process. To ensure the fidelity of structural recovery under nonuniform distortions, a Turbulence-Aware Decoder is proposed to selectively suppress unstable temporal cues and enhance edge-aware feature aggregation via turbulence gating and structure-aware attention. We built FLIR-IVSR, the first dataset for turbulent infrared VSR, comprising paired LR-HR sequences from a FLIR T1050sc camera (1024 X 768) spanning 640 diverse scenes with varying camera and object motion conditions. This encourages future research in infrared VSR. Project page: https://github.com/JZ0606/HATIR

</details>


### [28] [WebCryptoAgent: Agentic Crypto Trading with Web Informatics](https://arxiv.org/abs/2601.04687)
*Ali Kurban,Wei Luo,Liangyu Zuo,Zeyu Zhang,Renda Han,Zhaolu Kang,Hao Tang*

Main category: cs.CV

TL;DR: WebCryptoAgent is an agentic trading framework that decomposes web-informed cryptocurrency trading into modality-specific agents and separates strategic reasoning from real-time risk control to handle volatility and noisy data.


<details>
  <summary>Details</summary>
Motivation: Cryptocurrency trading requires timely integration of heterogeneous web information and market signals, but existing systems struggle with noisy multi-source evidence and rapid price shocks at sub-second timescales, lacking both coherent reasoning over unstructured data and robust risk control mechanisms.

Method: Proposes WebCryptoAgent with two key innovations: 1) modality-specific agents that process different data sources (unstructured web content, social sentiment, structured OHLCV signals) and consolidate outputs into a unified evidence document for confidence-calibrated reasoning, and 2) a decoupled control architecture separating strategic hourly reasoning from a real-time second-level risk model for fast shock detection and protective intervention.

Result: Extensive experiments on real-world cryptocurrency markets show that WebCryptoAgent improves trading stability, reduces spurious activity, and enhances tail-risk handling compared to existing baselines.

Conclusion: The proposed agentic framework effectively addresses the dual challenges of synthesizing noisy multi-source web evidence and maintaining robustness to rapid price shocks in cryptocurrency trading, offering a practical solution for short-horizon decision making under extreme volatility.

Abstract: Cryptocurrency trading increasingly depends on timely integration of heterogeneous web information and market microstructure signals to support short-horizon decision making under extreme volatility. However, existing trading systems struggle to jointly reason over noisy multi-source web evidence while maintaining robustness to rapid price shocks at sub-second timescales. The first challenge lies in synthesizing unstructured web content, social sentiment, and structured OHLCV signals into coherent and interpretable trading decisions without amplifying spurious correlations, while the second challenge concerns risk control, as slow deliberative reasoning pipelines are ill-suited for handling abrupt market shocks that require immediate defensive responses. To address these challenges, we propose WebCryptoAgent, an agentic trading framework that decomposes web-informed decision making into modality-specific agents and consolidates their outputs into a unified evidence document for confidence-calibrated reasoning. We further introduce a decoupled control architecture that separates strategic hourly reasoning from a real-time second-level risk model, enabling fast shock detection and protective intervention independent of the trading loop. Extensive experiments on real-world cryptocurrency markets demonstrate that WebCryptoAgent improves trading stability, reduces spurious activity, and enhances tail-risk handling compared to existing baselines. Code will be available at https://github.com/AIGeeksGroup/WebCryptoAgent.

</details>


### [29] [Forge-and-Quench: Enhancing Image Generation for Higher Fidelity in Unified Multimodal Models](https://arxiv.org/abs/2601.04706)
*Yanbing Zeng,Jia Wang,Hanghang Ma,Junqiang Wu,Jie Zhu,Xiaoming Wei,Jie Hu*

Main category: cs.CV

TL;DR: Forge-and-Quench is a unified framework that uses multimodal understanding models to enhance text-to-image generation by creating bridge features that guide image synthesis for improved fidelity and detail.


<details>
  <summary>Details</summary>
Motivation: While integrating image generation and understanding is important, previous works haven't fully explored how understanding can effectively assist generation. The paper aims to leverage understanding models to enhance the fidelity and detail richness of generated images rather than just using their reasoning abilities and world knowledge.

Method: Proposes Forge-and-Quench framework: 1) MLLM reasons over conversational context to produce enhanced text instruction, 2) Bridge Adapter maps this to a virtual visual representation called Bridge Feature, 3) This feature is injected into T2I backbone as visual guidance alongside enhanced text instruction, replacing original input.

Result: The framework demonstrates exceptional extensibility and flexibility, enabling efficient migration across different MLLM and T2I models with significant training overhead savings. Experiments show significant improvements in image fidelity and detail across multiple models while maintaining instruction-following accuracy and enhancing world knowledge application.

Conclusion: Forge-and-Quench successfully demonstrates how understanding models can enhance image generation by creating bridge features that guide the synthesis process, offering a novel perspective on multimodal integration with practical benefits in efficiency and performance.

Abstract: Integrating image generation and understanding into a single framework has become a pivotal goal in the multimodal domain. However, how understanding can effectively assist generation has not been fully explored. Unlike previous works that focus on leveraging reasoning abilities and world knowledge from understanding models, this paper introduces a novel perspective: leveraging understanding to enhance the fidelity and detail richness of generated images. To this end, we propose Forge-and-Quench, a new unified framework that puts this principle into practice. In the generation process of our framework, an MLLM first reasons over the entire conversational context, including text instructions, to produce an enhanced text instruction. This refined instruction is then mapped to a virtual visual representation, termed the Bridge Feature, via a novel Bridge Adapter. This feature acts as a crucial link, forging insights from the understanding model to quench and refine the generation process. It is subsequently injected into the T2I backbone as a visual guidance signal, alongside the enhanced text instruction that replaces the original input. To validate this paradigm, we conduct comprehensive studies on the design of the Bridge Feature and Bridge Adapter. Our framework demonstrates exceptional extensibility and flexibility, enabling efficient migration across different MLLM and T2I models with significant savings in training overhead, all without compromising the MLLM's inherent multimodal understanding capabilities. Experiments show that Forge-and-Quench significantly improves image fidelity and detail across multiple models, while also maintaining instruction-following accuracy and enhancing world knowledge application. Models and codes are available at https://github.com/YanbingZeng/Forge-and-Quench.

</details>


### [30] [On the Holistic Approach for Detecting Human Image Forgery](https://arxiv.org/abs/2601.04715)
*Xiao Guo,Jie Zhu,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: HuForDet is a holistic human image forgery detection framework with dual-branch architecture combining face forgery detection (RGB + frequency domains) and contextualized full-body analysis using MLLM, achieving SOTA performance across diverse human image manipulations.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods are fragmented - they specialize either in facial forgeries or full-body synthetic images, failing to generalize across the full spectrum of human image manipulations. The rapid advancement of AIGC has escalated the threat of deepfakes from facial manipulations to entire photorealistic human bodies.

Method: HuForDet features a dual-branch architecture: (1) Face forgery detection branch with heterogeneous experts operating in both RGB and frequency domains, including an adaptive Laplacian-of-Gaussian module to capture artifacts from fine-grained blending boundaries to coarse-scale texture irregularities. (2) Contextualized forgery detection branch that leverages a Multi-Modal Large Language Model to analyze full-body semantic consistency, enhanced with confidence estimation mechanism that dynamically weights its contribution during feature fusion.

Result: Extensive experiments show HuForDet achieves state-of-the-art forgery detection performance and superior robustness across diverse human image forgeries. The authors also curate a human image forgery (HuFor) dataset that unifies existing face forgery data with a new corpus of full-body synthetic humans.

Conclusion: HuForDet provides a comprehensive solution for human image forgery detection that overcomes the fragmentation of existing methods, successfully generalizing across both facial and full-body manipulations through its innovative dual-branch architecture and multi-modal approach.

Abstract: The rapid advancement of AI-generated content (AIGC) has escalated the threat of deepfakes, from facial manipulations to the synthesis of entire photorealistic human bodies. However, existing detection methods remain fragmented, specializing either in facial-region forgeries or full-body synthetic images, and consequently fail to generalize across the full spectrum of human image manipulations. We introduce HuForDet, a holistic framework for human image forgery detection, which features a dual-branch architecture comprising: (1) a face forgery detection branch that employs heterogeneous experts operating in both RGB and frequency domains, including an adaptive Laplacian-of-Gaussian (LoG) module designed to capture artifacts ranging from fine-grained blending boundaries to coarse-scale texture irregularities; and (2) a contextualized forgery detection branch that leverages a Multi-Modal Large Language Model (MLLM) to analyze full-body semantic consistency, enhanced with a confidence estimation mechanism that dynamically weights its contribution during feature fusion. We curate a human image forgery (HuFor) dataset that unifies existing face forgery data with a new corpus of full-body synthetic humans. Extensive experiments show that our HuForDet achieves state-of-the-art forgery detection performance and superior robustness across diverse human image forgeries.

</details>


### [31] [Training a Custom CNN on Five Heterogeneous Image Datasets](https://arxiv.org/abs/2601.04727)
*Anika Tabassum,Tasnuva Mahazabin Tuba,Nafisa Naznin*

Main category: cs.CV

TL;DR: This paper evaluates CNN architectures across five diverse visual classification tasks, comparing custom lightweight CNNs with established models (ResNet-18, VGG-16) using both scratch training and transfer learning to determine optimal approaches for resource-constrained real-world applications.


<details>
  <summary>Details</summary>
Motivation: To investigate how CNN-based architectures perform across heterogeneous real-world datasets with varying challenges (illumination, resolution, environmental complexity, class imbalance) and determine practical approaches for deploying deep learning in resource-limited yet high-impact visual classification tasks.

Method: Systematic evaluation of CNN architectures across five diverse datasets (mango variety classification, paddy variety identification, road surface condition assessment, auto-rickshaw detection, footpath encroachment monitoring). Methods include: developing a lightweight custom CNN, comparing with established architectures (ResNet-18, VGG-16), using both scratch training and transfer learning approaches, systematic preprocessing, data augmentation, and controlled experimentation to analyze architectural complexity, model depth, and pre-training effects.

Result: The custom CNN achieves competitive performance across multiple application domains. The comparative analysis reveals when transfer learning and deep architectures provide substantial advantages, particularly in data-constrained environments. Findings show how architectural complexity, model depth, and pre-training influence convergence, generalization, and performance across datasets of differing scale and difficulty.

Conclusion: The study provides practical insights for deploying deep learning models in resource-limited real-world visual classification tasks, demonstrating that lightweight custom CNNs can be effective alternatives to established deep architectures, with transfer learning offering particular advantages in data-constrained scenarios.

Abstract: Deep learning has transformed visual data analysis, with Convolutional Neural Networks (CNNs) becoming highly effective in learning meaningful feature representations directly from images. Unlike traditional manual feature engineering methods, CNNs automatically extract hierarchical visual patterns, enabling strong performance across diverse real-world contexts. This study investigates the effectiveness of CNN-based architectures across five heterogeneous datasets spanning agricultural and urban domains: mango variety classification, paddy variety identification, road surface condition assessment, auto-rickshaw detection, and footpath encroachment monitoring. These datasets introduce varying challenges, including differences in illumination, resolution, environmental complexity, and class imbalance, necessitating adaptable and robust learning models.
  We evaluate a lightweight, task-specific custom CNN alongside established deep architectures, including ResNet-18 and VGG-16, trained both from scratch and using transfer learning. Through systematic preprocessing, augmentation, and controlled experimentation, we analyze how architectural complexity, model depth, and pre-training influence convergence, generalization, and performance across datasets of differing scale and difficulty. The key contributions of this work are: (1) the development of an efficient custom CNN that achieves competitive performance across multiple application domains, and (2) a comprehensive comparative analysis highlighting when transfer learning and deep architectures provide substantial advantages, particularly in data-constrained environments. These findings offer practical insights for deploying deep learning models in resource-limited yet high-impact real-world visual classification tasks.

</details>


### [32] [AIVD: Adaptive Edge-Cloud Collaboration for Accurate and Efficient Industrial Visual Detection](https://arxiv.org/abs/2601.04734)
*Yunqing Hu,Zheming Yang,Chang Zhao,Qi Guo,Meng Gao,Pengcheng Li,Wen Ji*

Main category: cs.CV

TL;DR: AIVD framework enables precise object localization and semantic generation through edge-cloud collaboration, with noise-robust fine-tuning and resource-aware scheduling.


<details>
  <summary>Details</summary>
Motivation: MLLMs have strong semantic understanding but struggle with precise object localization and edge-cloud deployment due to resource constraints and localization challenges.

Method: AIVD framework combines lightweight edge detectors with cloud MLLMs, uses visual-semantic collaborative augmentation for fine-tuning, and implements heterogeneous resource-aware dynamic scheduling.

Result: AIVD reduces resource consumption, improves MLLM classification accuracy and semantic consistency, and achieves higher throughput with lower latency across diverse scenarios.

Conclusion: The proposed AIVD framework effectively addresses MLLM limitations in localization and edge deployment through collaborative edge-cloud architecture, robust fine-tuning, and intelligent scheduling.

Abstract: Multimodal large language models (MLLMs) demonstrate exceptional capabilities in semantic understanding and visual reasoning, yet they still face challenges in precise object localization and resource-constrained edge-cloud deployment. To address this, this paper proposes the AIVD framework, which achieves unified precise localization and high-quality semantic generation through the collaboration between lightweight edge detectors and cloud-based MLLMs. To enhance the cloud MLLM's robustness against edge cropped-box noise and scenario variations, we design an efficient fine-tuning strategy with visual-semantic collaborative augmentation, significantly improving classification accuracy and semantic consistency. Furthermore, to maintain high throughput and low latency across heterogeneous edge devices and dynamic network conditions, we propose a heterogeneous resource-aware dynamic scheduling algorithm. Experimental results demonstrate that AIVD substantially reduces resource consumption while improving MLLM classification performance and semantic generation quality. The proposed scheduling strategy also achieves higher throughput and lower latency across diverse scenarios.

</details>


### [33] [Skeletonization-Based Adversarial Perturbations on Large Vision Language Model's Mathematical Text Recognition](https://arxiv.org/abs/2601.04752)
*Masatomo Yoshida,Haruto Namura,Nicola Adami,Masahiro Okuda*

Main category: cs.CV

TL;DR: Novel adversarial attack using skeletonization to target foundation models' visual capabilities, especially on mathematical formula images, with evaluation of character/semantic changes and practical demonstration on ChatGPT.


<details>
  <summary>Details</summary>
Motivation: To explore the visual capabilities and limitations of foundation models by developing an adversarial attack method that can effectively target images containing text, particularly challenging mathematical formulas with LaTeX conversion and intricate structures.

Method: Introduces a novel adversarial attack method utilizing skeletonization to reduce the search space effectively. The approach specifically targets images containing text, particularly mathematical formula images, and evaluates both character and semantic changes between original and adversarially perturbed outputs.

Result: The method effectively demonstrates vulnerabilities in foundation models' visual interpretation and reasoning abilities. The attack is successfully applied to ChatGPT, showing practical implications in real-world scenarios.

Conclusion: The skeletonization-based adversarial attack reveals important insights into foundation models' visual capabilities and limitations, particularly for complex text-based images like mathematical formulas, with demonstrated real-world applicability against systems like ChatGPT.

Abstract: This work explores the visual capabilities and limitations of foundation models by introducing a novel adversarial attack method utilizing skeletonization to reduce the search space effectively. Our approach specifically targets images containing text, particularly mathematical formula images, which are more challenging due to their LaTeX conversion and intricate structure. We conduct a detailed evaluation of both character and semantic changes between original and adversarially perturbed outputs to provide insights into the models' visual interpretation and reasoning abilities. The effectiveness of our method is further demonstrated through its application to ChatGPT, which shows its practical implications in real-world scenarios.

</details>


### [34] [ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting](https://arxiv.org/abs/2601.04754)
*Yen-Jen Chiou,Wei-Tse Cheng,Yuan-Fu Yang*

Main category: cs.CV

TL;DR: ProFuse is an efficient framework for open-vocabulary 3D scene understanding using 3D Gaussian Splatting that achieves semantic attachment in ~5 minutes per scene (2× faster than SOTA) without render-supervised fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for efficient open-vocabulary 3D scene understanding with 3D Gaussian Splatting, aiming to enhance cross-view consistency and intra-mask cohesion while minimizing computational overhead and avoiding render-supervised fine-tuning.

Method: ProFuse introduces a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, which is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views.

Result: ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than state-of-the-art methods. The framework maintains geometric refinement without densification and requires no additional optimization beyond standard reconstruction.

Conclusion: ProFuse presents an efficient context-aware framework for open-vocabulary 3D scene understanding that significantly reduces computational time while maintaining strong performance, offering a practical solution for semantic 3D scene analysis with minimal overhead.

Abstract: We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.

</details>


### [35] [Segmentation-Driven Monocular Shape from Polarization based on Physical Model](https://arxiv.org/abs/2601.04776)
*Jinyu Zhang,Xu Ma,Weili Chen,Gonzalo R. Arce*

Main category: cs.CV

TL;DR: A new monocular shape-from-polarization method uses segmentation to break global reconstruction into local convex regions, solving azimuth ambiguity issues in polarization-based 3D reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing monocular shape-from-polarization methods suffer from azimuth angle ambiguity, an inherent limitation of polarization analysis that severely compromises reconstruction accuracy and stability.

Method: Proposes a segmentation-driven monocular SfP framework with polarization-aided adaptive region growing (PARG) to decompose global convexity into locally convex regions, and multi-scale fusion convexity prior (MFCP) to ensure local surface consistency.

Result: Extensive experiments on synthetic and real-world datasets show significant improvements in disambiguation accuracy and geometric fidelity compared with existing physics-based monocular SfP techniques.

Conclusion: The segmentation-driven approach effectively suppresses azimuth ambiguities and enhances reconstruction quality by reformulating global shape recovery into local convex sub-region reconstructions.

Abstract: Monocular shape-from-polarization (SfP) leverages the intrinsic relationship between light polarization properties and surface geometry to recover surface normals from single-view polarized images, providing a compact and robust approach for three-dimensional (3D) reconstruction. Despite its potential, existing monocular SfP methods suffer from azimuth angle ambiguity, an inherent limitation of polarization analysis, that severely compromises reconstruction accuracy and stability. This paper introduces a novel segmentation-driven monocular SfP (SMSfP) framework that reformulates global shape recovery into a set of local reconstructions over adaptively segmented convex sub-regions. Specifically, a polarization-aided adaptive region growing (PARG) segmentation strategy is proposed to decompose the global convexity assumption into locally convex regions, effectively suppressing azimuth ambiguities and preserving surface continuity. Furthermore, a multi-scale fusion convexity prior (MFCP) constraint is developed to ensure local surface consistency and enhance the recovery of fine textural and structural details. Extensive experiments on both synthetic and real-world datasets validate the proposed approach, showing significant improvements in disambiguation accuracy and geometric fidelity compared with existing physics-based monocular SfP techniques.

</details>


### [36] [GeM-VG: Towards Generalized Multi-image Visual Grounding with Multimodal Large Language Models](https://arxiv.org/abs/2601.04777)
*Shurong Zheng,Yousong Zhu,Hongyin Zhao,Fan Yang,Yufei Zhan,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: GeM-VG is a Multimodal Large Language Model for Generalized Multi-image Visual Grounding that unifies diverse grounding tasks, uses a new dataset (MG-Data-240K), and employs hybrid reinforcement finetuning to outperform previous models.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs for multi-image grounding are limited to single-target localization and few task types due to lack of unified modeling. There's a need for generalized multi-image visual grounding that can handle diverse tasks with cross-image reasoning.

Method: 1) Categorize multi-image grounding tasks by cross-image cue reliance; 2) Introduce MG-Data-240K dataset with more targets and image relations; 3) Propose hybrid reinforcement finetuning strategy combining chain-of-thought reasoning and direct answering using R1-like algorithm with rule-based rewards.

Result: Outperforms previous leading MLLMs by 2.0% on MIG-Bench and 9.7% on MC-Bench for multi-image grounding. Achieves 9.1% improvement over base model on ODINW for single-image grounding. Maintains strong general multi-image understanding capabilities.

Conclusion: GeM-VG successfully addresses limitations of previous multi-image grounding methods through unified task modeling, comprehensive dataset, and hybrid finetuning strategy, demonstrating superior generalized grounding capabilities across diverse tasks.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive progress in single-image grounding and general multi-image understanding. Recently, some methods begin to address multi-image grounding. However, they are constrained by single-target localization and limited types of practical tasks, due to the lack of unified modeling for generalized grounding tasks. Therefore, we propose GeM-VG, an MLLM capable of Generalized Multi-image Visual Grounding. To support this, we systematically categorize and organize existing multi-image grounding tasks according to their reliance of cross-image cues and reasoning, and introduce the MG-Data-240K dataset, addressing the limitations of existing datasets regarding target quantity and image relation. To tackle the challenges of robustly handling diverse multi-image grounding tasks, we further propose a hybrid reinforcement finetuning strategy that integrates chain-of-thought (CoT) reasoning and direct answering, considering their complementary strengths. This strategy adopts an R1-like algorithm guided by a carefully designed rule-based reward, effectively enhancing the model's overall perception and reasoning capabilities. Extensive experiments demonstrate the superior generalized grounding capabilities of our model. For multi-image grounding, it outperforms the previous leading MLLMs by 2.0% and 9.7% on MIG-Bench and MC-Bench, respectively. In single-image grounding, it achieves a 9.1% improvement over the base model on ODINW. Furthermore, our model retains strong capabilities in general multi-image understanding.

</details>


### [37] [CounterVid: Counterfactual Video Generation for Mitigating Action and Temporal Hallucinations in Video-Language Models](https://arxiv.org/abs/2601.04778)
*Tobia Poppi,Burak Uzkent,Amanmeet Garg,Lucas Porto,Garin Kessler,Yezhou Yang,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara,Florian Schiffers*

Main category: cs.CV

TL;DR: The paper introduces CounterVid, a framework for generating counterfactual videos to address hallucinations in video-language models, and MixDPO, a unified preference optimization method that improves temporal reasoning and action recognition.


<details>
  <summary>Details</summary>
Motivation: Video-language models suffer from hallucinations, particularly in action and temporal reasoning, due to over-reliance on language priors rather than visual dynamics. Existing mitigation strategies like textual filtering or random video perturbations fail to address this root cause.

Method: Proposes a scalable counterfactual video generation framework that synthesizes videos differing only in actions or temporal structure while preserving scene context. Uses multimodal LLMs for action proposal/editing guidance with diffusion-based image/video models to generate semantic hard negatives. Builds CounterVid dataset (~26k preference pairs) and introduces MixDPO, a unified Direct Preference Optimization approach combining textual and visual preferences.

Result: Fine-tuning Qwen2.5-VL with MixDPO yields consistent improvements, especially in temporal ordering, and transfers effectively to standard video hallucination benchmarks.

Conclusion: The proposed counterfactual video generation framework and MixDPO approach effectively address hallucinations in video-language models by targeting the root cause of over-reliance on language priors, leading to improved temporal reasoning and action recognition.

Abstract: Video-language models (VLMs) achieve strong multimodal understanding but remain prone to hallucinations, especially when reasoning about actions and temporal order. Existing mitigation strategies, such as textual filtering or random video perturbations, often fail to address the root cause: over-reliance on language priors rather than fine-grained visual dynamics. We propose a scalable framework for counterfactual video generation that synthesizes videos differing only in actions or temporal structure while preserving scene context. Our pipeline combines multimodal LLMs for action proposal and editing guidance with diffusion-based image and video models to generate semantic hard negatives at scale. Using this framework, we build CounterVid, a synthetic dataset of ~26k preference pairs targeting action recognition and temporal reasoning. We further introduce MixDPO, a unified Direct Preference Optimization approach that jointly leverages textual and visual preferences. Fine-tuning Qwen2.5-VL with MixDPO yields consistent improvements, notably in temporal ordering, and transfers effectively to standard video hallucination benchmarks. Code and models will be made publicly available.

</details>


### [38] [Defocus Aberration Theory Confirms Gaussian Model in Most Imaging Devices](https://arxiv.org/abs/2601.04779)
*Akbar Saadat*

Main category: cs.CV

TL;DR: This paper validates the Gaussian model as an accurate and reliable approximation for defocus operators in conventional imaging devices, showing less than 1% error for typical depth ranges.


<details>
  <summary>Details</summary>
Motivation: Depth estimation from 2D images remains a fundamental challenge in 3D recovery. While defocus provides valuable depth information, accurately modeling defocus blur is difficult due to the ill-posed nature of distinguishing desired blur from inherent blur. A well-posed solution requires a proper defocus model that can handle both absolute blur in single images and relative blur between images.

Method: The paper introduces specific settings for conventional imaging devices to ensure defocus operators adhere to the Gaussian model. Defocus analysis is conducted using both geometric optics framework and defocus aberration theory in diffraction-limited optics. The accuracy is evaluated by fitting the actual model to its Gaussian approximation for typical depth ranges (1-100 meters) with maximum depth variation of 10% at the focused depth.

Result: Results confirm the Gaussian model's applicability for defocus operators in most imaging devices. The findings demonstrate a maximum Mean Absolute Error (MAE) of less than 1%, highlighting the model's accuracy and reliability for real-time applications.

Conclusion: The Gaussian model is validated as an optimal choice for defocus modeling due to its mathematical simplicity, computational efficiency, and theoretical ability to handle both absolute and relative blur. The less than 1% error rate makes it suitable for practical applications in depth estimation from defocus information.

Abstract: Over the past three decades, defocus has consistently provided groundbreaking depth information in scene images. However, accurately estimating depth from 2D images continues to be a persistent and fundamental challenge in the field of 3D recovery. Heuristic approaches involve with the ill-posed problem for inferring the spatial variant defocusing blur, as the desired blur cannot be distinguished from the inherent blur. Given a prior knowledge of the defocus model, the problem become well-posed with an analytic solution for the relative blur between two images, taken at the same viewpoint with different camera settings for the focus. The Gaussian model stands out as an optimal choice for real-time applications, due to its mathematical simplicity and computational efficiency. And theoretically, it is the only model can be applied at the same time to both the absolute blur caused by depth in a single image and the relative blur resulting from depth differences between two images. This paper introduces the settings, for conventional imaging devices, to ensure that the defocusing operator adheres to the Gaussian model. Defocus analysis begins within the framework of geometric optics and is conducted by defocus aberration theory in diffraction-limited optics to obtain the accuracy of fitting the actual model to its Gaussian approximation. The results for a typical set of focused depths between $1$ and $100$ meters, with a maximum depth variation of $10\%$ at the focused depth, confirm the Gaussian model's applicability for defocus operators in most imaging devices. The findings demonstrate a maximum Mean Absolute Error $(\!M\!A\!E)$ of less than $1\%$, underscoring the model's accuracy and reliability.

</details>


### [39] [SRU-Pix2Pix: A Fusion-Driven Generator Network for Medical Image Translation with Few-Shot Learning](https://arxiv.org/abs/2601.04785)
*Xihe Qiu,Yang Dai,Xiaoyu Tan,Sijia Li,Fenghao Sun,Lu Gan,Liang Liu*

Main category: cs.CV

TL;DR: Enhanced Pix2Pix framework with SEResNet and U-Net++ improves MRI image translation quality and structural fidelity under few-shot conditions.


<details>
  <summary>Details</summary>
Motivation: MRI has clinical limitations including long acquisition time, high cost, and restricted resolution. While Pix2Pix has been applied to medical image translation, its potential hasn't been fully explored for addressing these MRI limitations.

Method: Proposes an enhanced Pix2Pix framework integrating Squeeze-and-Excitation Residual Networks (SEResNet) for channel attention and U-Net++ for multi-scale feature fusion, with a simplified PatchGAN discriminator for training stability and local anatomical realism.

Result: Under few-shot conditions (<500 images), the method achieves consistent structural fidelity and superior image quality across multiple intra-modality MRI translation tasks, demonstrating strong generalization ability.

Conclusion: The enhanced Pix2Pix framework represents an effective extension for medical image translation, addressing MRI limitations while maintaining high quality under data-scarce conditions.

Abstract: Magnetic Resonance Imaging (MRI) provides detailed tissue information, but its clinical application is limited by long acquisition time, high cost, and restricted resolution. Image translation has recently gained attention as a strategy to address these limitations. Although Pix2Pix has been widely applied in medical image translation, its potential has not been fully explored. In this study, we propose an enhanced Pix2Pix framework that integrates Squeeze-and-Excitation Residual Networks (SEResNet) and U-Net++ to improve image generation quality and structural fidelity. SEResNet strengthens critical feature representation through channel attention, while U-Net++ enhances multi-scale feature fusion. A simplified PatchGAN discriminator further stabilizes training and refines local anatomical realism. Experimental results demonstrate that under few-shot conditions with fewer than 500 images, the proposed method achieves consistent structural fidelity and superior image quality across multiple intra-modality MRI translation tasks, showing strong generalization ability. These results suggest an effective extension of Pix2Pix for medical image translation.

</details>


### [40] [Measurement-Consistent Langevin Corrector: A Remedy for Latent Diffusion Inverse Solvers](https://arxiv.org/abs/2601.04791)
*Lee Hyoseok,Sohwi Lim,Eunju Cha,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: MCLC is a plug-and-play correction module that stabilizes latent diffusion inverse solvers by reducing the discrepancy between solver and true reverse diffusion dynamics through measurement-consistent Langevin updates.


<details>
  <summary>Details</summary>
Motivation: Existing latent diffusion inverse solvers suffer from instability, artifacts, and degraded quality due to a discrepancy between solver's and true reverse diffusion dynamics. Prior approaches rely on linear manifold assumptions that often don't hold in latent space.

Method: Measurement-Consistent Langevin Corrector (MCLC) - a theoretically grounded plug-and-play module that remedies LDM-based inverse solvers through measurement-consistent Langevin updates without relying on linear manifold assumptions.

Result: MCLC demonstrates effectiveness and compatibility with existing solvers across diverse image restoration tasks, providing more stable and reliable behavior while analyzing blob artifacts and their underlying causes.

Conclusion: MCLC is a key step toward more robust zero-shot inverse problem solvers by addressing the instability in latent diffusion inverse solvers through theoretically grounded correction without restrictive assumptions.

Abstract: With recent advances in generative models, diffusion models have emerged as powerful priors for solving inverse problems in each domain. Since Latent Diffusion Models (LDMs) provide generic priors, several studies have explored their potential as domain-agnostic zero-shot inverse solvers. Despite these efforts, existing latent diffusion inverse solvers suffer from their instability, exhibiting undesirable artifacts and degraded quality. In this work, we first identify the instability as a discrepancy between the solver's and true reverse diffusion dynamics, and show that reducing this gap stabilizes the solver. Building on this, we introduce Measurement-Consistent Langevin Corrector (MCLC), a theoretically grounded plug-and-play correction module that remedies the LDM-based inverse solvers through measurement-consistent Langevin updates. Compared to prior approaches that rely on linear manifold assumptions, which often do not hold in latent space, MCLC operates without this assumption, leading to more stable and reliable behavior. We experimentally demonstrate the effectiveness of MCLC and its compatibility with existing solvers across diverse image restoration tasks. Additionally, we analyze blob artifacts and offer insights into their underlying causes. We highlight that MCLC is a key step toward more robust zero-shot inverse problem solvers.

</details>


### [41] [PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference](https://arxiv.org/abs/2601.04792)
*Denis Korzhenkov,Adil Karjauv,Animesh Karnewar,Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.CV

TL;DR: Pyramidal diffusion models use multi-resolution processing to reduce computational cost, but existing open-source versions underperform. This work converts pretrained models to pyramidal via low-cost finetuning without quality loss, plus explores step distillation for efficiency.


<details>
  <summary>Details</summary>
Motivation: Pyramidal models reduce inference cost by processing different noise levels at different resolutions, but existing open-source pyramidal video models underperform compared to state-of-the-art systems in visual quality.

Method: Presents a pipeline to convert pretrained diffusion models into pyramidal ones through low-cost finetuning, and investigates various strategies for step distillation within pyramidal models.

Result: Achieves transformation of pretrained models to pyramidal architecture without degradation in output video quality, while exploring step distillation to further enhance inference efficiency.

Conclusion: Demonstrates successful conversion of pretrained diffusion models to efficient pyramidal architecture via low-cost finetuning, maintaining quality while improving computational efficiency.

Abstract: Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.

</details>


### [42] [Detector-Augmented SAMURAI for Long-Duration Drone Tracking](https://arxiv.org/abs/2601.04798)
*Tamara R. Lenhard,Andreas Weinmann,Hichem Snoussi,Tobias Koch*

Main category: cs.CV

TL;DR: SAMURAI foundation model shows strong potential for drone tracking but needs detector augmentation to handle initialization sensitivity and long sequences in urban surveillance.


<details>
  <summary>Details</summary>
Motivation: Drone tracking is critical for surveillance but current RGB-based methods are limited and rely on conventional motion models. Foundation models like SAMURAI show strong tracking performance in other domains but haven't been tested for drone-specific scenarios.

Method: Systematic evaluation of SAMURAI for drone tracking, plus a detector-augmented extension to mitigate sensitivity to bounding-box initialization and sequence length issues.

Result: The detector-augmented SAMURAI significantly improves robustness in complex urban environments, especially for long sequences and drone exit-re-entry events. Achieves success rate improvements up to +0.393 and FNR reductions up to -0.475 over zero-shot performance.

Conclusion: Foundation models like SAMURAI have strong potential for drone tracking, but detector augmentation is crucial for practical deployment in urban surveillance scenarios, particularly for handling long sequences and initialization challenges.

Abstract: Robust long-term tracking of drone is a critical requirement for modern surveillance systems, given their increasing threat potential. While detector-based approaches typically achieve strong frame-level accuracy, they often suffer from temporal inconsistencies caused by frequent detection dropouts. Despite its practical relevance, research on RGB-based drone tracking is still limited and largely reliant on conventional motion models. Meanwhile, foundation models like SAMURAI have established their effectiveness across other domains, exhibiting strong category-agnostic tracking performance. However, their applicability in drone-specific scenarios has not been investigated yet. Motivated by this gap, we present the first systematic evaluation of SAMURAI's potential for robust drone tracking in urban surveillance settings. Furthermore, we introduce a detector-augmented extension of SAMURAI to mitigate sensitivity to bounding-box initialization and sequence length. Our findings demonstrate that the proposed extension significantly improves robustness in complex urban environments, with pronounced benefits in long-duration sequences - especially under drone exit-re-entry events. The incorporation of detector cues yields consistent gains over SAMURAI's zero-shot performance across datasets and metrics, with success rate improvements of up to +0.393 and FNR reductions of up to -0.475.

</details>


### [43] [Integrated Framework for Selecting and Enhancing Ancient Marathi Inscription Images from Stone, Metal Plate, and Paper Documents](https://arxiv.org/abs/2601.04800)
*Bapu D. Chendage,Rajivkumar S. Mente*

Main category: cs.CV

TL;DR: Proposes image enhancement using binarization and preprocessing to improve readability of degraded ancient scripts with background noise and low contrast.


<details>
  <summary>Details</summary>
Motivation: Ancient script images suffer from severe background noise, low contrast, and degradation from aging/environmental effects, making inscriptions difficult to read due to similar visual characteristics between foreground text and background.

Method: Image enhancement approach based on binarization and complementary preprocessing techniques for removing stains and enhancing unclear ancient text.

Result: Achieved classification accuracies of 55.7%, 62%, and 65.6% for stone, metal plate, and document scripts using K-NN classifier; and 53.2%, 59.5%, and 67.8% using SVM classifier.

Conclusion: The proposed enhancement method effectively improves readability of ancient Marathi inscription images, demonstrating practical value for historical document preservation and analysis.

Abstract: Ancient script images often suffer from severe background noise, low contrast, and degradation caused by aging and environmental effects. In many cases, the foreground text and background exhibit similar visual characteristics, making the inscriptions difficult to read. The primary objective of image enhancement is to improve the readability of such degraded ancient images. This paper presents an image enhancement approach based on binarization and complementary preprocessing techniques for removing stains and enhancing unclear ancient text. The proposed methods are evaluated on different types of ancient scripts, including inscriptions on stone, metal plates, and historical documents. Experimental results show that the proposed approach achieves classification accuracies of 55.7%, 62%, and 65.6% for stone, metal plate, and document scripts, respectively, using the K-Nearest Neighbor (K-NN) classifier. Using the Support Vector Machine (SVM) classifier, accuracies of 53.2%, 59.5%, and 67.8% are obtained. The results demonstrate the effectiveness of the proposed enhancement method in improving the readability of ancient Marathi inscription images.

</details>


### [44] [SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2601.04824)
*Oriol Rabasseda,Zenjie Li,Kamal Nasrollahi,Sergio Escalera*

Main category: cs.CV

TL;DR: SOVABench is a new surveillance video retrieval benchmark focusing on vehicle actions, with two evaluation protocols for cross-action discrimination and temporal understanding. The paper also introduces a training-free MLLM framework for interpretable embeddings that performs well on this benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing video retrieval benchmarks focus on scene-level similarity but lack evaluation of action discrimination needed for surveillance applications. There's a gap in benchmarks that specifically assess vehicle-related action understanding in surveillance contexts.

Method: 1) Created SOVABench from real surveillance footage with vehicle actions; 2) Defined two evaluation protocols (inter-pair for cross-action discrimination, intra-pair for temporal direction); 3) Developed training-free framework using MLLMs to generate interpretable embeddings from descriptions.

Result: Action distinctions remain challenging for state-of-the-art vision and multimodal models despite being intuitive for humans. The proposed MLLM framework achieves strong performance on SOVABench and outperforms contrastive Vision-Language Models on spatial and counting benchmarks.

Conclusion: SOVABench addresses a critical gap in surveillance video evaluation, and the MLLM-based framework provides effective training-free approach for interpretable embeddings that handles challenging action discrimination tasks in surveillance contexts.

Abstract: Automatic identification of events and recurrent behavior analysis are critical for video surveillance. However, most existing content-based video retrieval benchmarks focus on scene-level similarity and do not evaluate the action discrimination required in surveillance. To address this gap, we introduce SOVABench (Surveillance Opposite Vehicle Actions Benchmark), a real-world retrieval benchmark built from surveillance footage and centered on vehicle-related actions. SOVABench defines two evaluation protocols (inter-pair and intra-pair) to assess cross-action discrimination and temporal direction understanding. Although action distinctions are generally intuitive for human observers, our experiments show that they remain challenging for state-of-the-art vision and multimodal models.
  Leveraging the visual reasoning and instruction-following capabilities of Multimodal Large Language Models (MLLMs), we present a training-free framework for producing interpretable embeddings from MLLM-generated descriptions for both images and videos. The framework achieves strong performance on SOVABench as well as on several spatial and counting benchmarks where contrastive Vision-Language Models often fail. The code, annotations, and instructions to construct the benchmark are publicly available.

</details>


### [45] [Character Detection using YOLO for Writer Identification in multiple Medieval books](https://arxiv.org/abs/2601.04834)
*Alessandra Scotto di Freca,Tiziana D Alessandro,Francesco Fontanella,Filippo Sarria,Claudio De Stefano*

Main category: cs.CV

TL;DR: This paper presents a YOLO-based approach for scribe identification in medieval manuscripts, replacing previous template matching and CNN methods to improve letter detection accuracy and enable reliable writer identification.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve the challenging problem of identifying individual scribes in medieval manuscripts to aid paleography studies (dating manuscripts and understanding writing evolution). While digital technologies have made progress, the general problem remains unsolved with open challenges.

Method: The authors replaced their previous template matching and CNN approach with YOLO object detection (version 5) to identify scribes. They focus on detecting the letter "a" as a distinctive character present throughout manuscripts. YOLO handles letter detection, and the system uses confidence scores to apply rejection thresholds for reliable identification.

Result: YOLO effectively extracts more letters than the previous template matching approach, leading to more accurate second-stage classification. The YOLO confidence score enables development of a system with rejection thresholds for reliable writer identification even in unseen manuscripts.

Conclusion: The YOLO-based approach represents an improvement over previous template matching methods for scribe identification in paleography, offering better letter extraction and enabling more reliable writer attribution through confidence-based rejection thresholds.

Abstract: Paleography is the study of ancient and historical handwriting, its key objectives include the dating of manuscripts and understanding the evolution of writing. Estimating when a document was written and tracing the development of scripts and writing styles can be aided by identifying the individual scribes who contributed to a medieval manuscript. Although digital technologies have made significant progress in this field, the general problem remains unsolved and continues to pose open challenges. ... We previously proposed an approach focused on identifying specific letters or abbreviations that characterize each writer. In that study, we considered the letter "a", as it was widely present on all pages of text and highly distinctive, according to the suggestions of expert paleographers. We used template matching techniques to detect the occurrences of the character "a" on each page and the convolutional neural network (CNN) to attribute each instance to the correct scribe. Moving from the interesting results achieved from this previous system and being aware of the limitations of the template matching technique, which requires an appropriate threshold to work, we decided to experiment in the same framework with the use of the YOLO object detection model to identify the scribe who contributed to the writing of different medieval books. We considered the fifth version of YOLO to implement the YOLO object detection model, which completely substituted the template matching and CNN used in the previous work. The experimental results demonstrate that YOLO effectively extracts a greater number of letters considered, leading to a more accurate second-stage classification. Furthermore, the YOLO confidence score provides a foundation for developing a system that applies a rejection threshold, enabling reliable writer identification even in unseen manuscripts.

</details>


### [46] [DivAS: Interactive 3D Segmentation of NeRFs via Depth-Weighted Voxel Aggregation](https://arxiv.org/abs/2601.04860)
*Ayush Pande*

Main category: cs.CV

TL;DR: DivAS is an optimization-free, interactive framework for segmenting NeRFs using 2D SAM masks refined with depth priors and aggregated into 3D voxels via custom CUDA kernel for real-time feedback.


<details>
  <summary>Details</summary>
Motivation: Existing NeRF segmentation methods are optimization-based, requiring slow per-scene training and sacrificing the zero-shot capabilities of 2D foundation models like SAM.

Method: Fast GUI workflow where user point prompts generate 2D SAM masks, refined using NeRF-derived depth priors for geometric accuracy. Custom CUDA kernel aggregates refined multi-view masks into unified 3D voxel grid in under 200ms.

Result: Achieves segmentation quality comparable to optimization-based methods, 2-2.5x faster end-to-end, and up to 10x faster when excluding user prompting time. No per-scene training needed.

Conclusion: DivAS provides optimization-free, interactive NeRF segmentation with real-time feedback, bridging 2D foundation models and 3D scene understanding without sacrificing speed or quality.

Abstract: Existing methods for segmenting Neural Radiance Fields (NeRFs) are often optimization-based, requiring slow per-scene training that sacrifices the zero-shot capabilities of 2D foundation models. We introduce DivAS (Depth-interactive Voxel Aggregation Segmentation), an optimization-free, fully interactive framework that addresses these limitations. Our method operates via a fast GUI-based workflow where 2D SAM masks, generated from user point prompts, are refined using NeRF-derived depth priors to improve geometric accuracy and foreground-background separation. The core of our contribution is a custom CUDA kernel that aggregates these refined multi-view masks into a unified 3D voxel grid in under 200ms, enabling real-time visual feedback. This optimization-free design eliminates the need for per-scene training. Experiments on Mip-NeRF 360° and LLFF show that DivAS achieves segmentation quality comparable to optimization-based methods, while being 2-2.5x faster end-to-end, and up to an order of magnitude faster when excluding user prompting time.

</details>


### [47] [Scaling Vision Language Models for Pharmaceutical Long Form Video Reasoning on Industrial GenAI Platform](https://arxiv.org/abs/2601.04891)
*Suyash Mishra,Qiang Li,Srikanth Patil,Satyanarayan Pati,Baddu Narendra*

Main category: cs.CV

TL;DR: Industrial framework for processing 200K+ PDFs, 25K+ videos, and 888 multilingual audio files in pharmaceutical domains, with empirical analysis of 40+ VLMs revealing efficiency gains, multimodality benefits, and temporal reasoning bottlenecks under GPU constraints.


<details>
  <summary>Details</summary>
Motivation: Most VLM evaluations focus on short videos with unlimited resources, but industrial settings like pharmaceutical content understanding require processing long-form videos under strict GPU, latency, and cost constraints where existing approaches fail to scale.

Method: Developed an industrial GenAI framework for large-scale multimodal processing, empirically analyzed over 40 VLMs on Video-MME and MMBench benchmarks plus proprietary dataset of 25,326 videos across 14 disease areas, focusing on multimodality, attention mechanisms, temporal reasoning, and video splitting challenges.

Result: Achieved 3-8x efficiency gains with SDPA attention on commodity GPUs, multimodality improved performance in 8/12 task domains (especially length-dependent tasks), and identified clear bottlenecks in temporal alignment and keyframe detection across both open- and closed-source VLMs.

Conclusion: Rather than proposing new models, this paper characterizes practical limits, trade-offs, and failure patterns of current VLMs under realistic deployment constraints, providing actionable guidance for designing scalable multimodal systems for long-form video understanding in industrial domains.

Abstract: Vision Language Models (VLMs) have shown strong performance on multimodal reasoning tasks, yet most evaluations focus on short videos and assume unconstrained computational resources. In industrial settings such as pharmaceutical content understanding, practitioners must process long-form videos under strict GPU, latency, and cost constraints, where many existing approaches fail to scale. In this work, we present an industrial GenAI framework that processes over 200,000 PDFs, 25,326 videos across eight formats (e.g., MP4, M4V, etc.), and 888 multilingual audio files in more than 20 languages. Our study makes three contributions: (i) an industrial large-scale architecture for multimodal reasoning in pharmaceutical domains; (ii) empirical analysis of over 40 VLMs on two leading benchmarks (Video-MME and MMBench) and proprietary dataset of 25,326 videos across 14 disease areas; and (iii) four findings relevant to long-form video reasoning: the role of multimodality, attention mechanism trade-offs, temporal reasoning limits, and challenges of video splitting under GPU constraints. Results show 3-8 times efficiency gains with SDPA attention on commodity GPUs, multimodality improving up to 8/12 task domains (especially length-dependent tasks), and clear bottlenecks in temporal alignment and keyframe detection across open- and closed-source VLMs. Rather than proposing a new "A+B" model, this paper characterizes practical limits, trade-offs, and failure patterns of current VLMs under realistic deployment constraints, and provide actionable guidance for both researchers and practitioners designing scalable multimodal systems for long-form video understanding in industrial domains.

</details>


### [48] [Rotation-Robust Regression with Convolutional Model Trees](https://arxiv.org/abs/2601.04899)
*Hongyi Li,William Ward Armstrong,Jun Xu*

Main category: cs.CV

TL;DR: CMTs with geometry-aware inductive biases improve rotation robustness; orientation search helps with severe rotations but can harm performance near canonical orientation when confidence doesn't align with correctness.


<details>
  <summary>Details</summary>
Motivation: To develop rotation-robust learning for image inputs using Convolutional Model Trees that can handle geometric transformations at deployment time, addressing the challenge of maintaining performance under in-plane rotations.

Method: Use Convolutional Model Trees with three geometry-aware inductive biases: convolutional smoothing, tilt dominance constraint, and importance-based pruning. Evaluate deployment-time orientation search that selects discrete rotation maximizing forest-level confidence without updating parameters.

Result: Orientation search improves robustness under severe rotations but can be harmful near canonical orientation when confidence is misaligned with correctness. Consistent trends observed on MNIST digit recognition highlight both promise and limitations of confidence-based orientation selection.

Conclusion: Convolutional Model Trees with geometry-aware inductive biases show promise for rotation-robust learning, but confidence-based orientation selection has limitations when confidence doesn't correlate with correctness, particularly near canonical orientations.

Abstract: We study rotation-robust learning for image inputs using Convolutional Model Trees (CMTs) [1], whose split and leaf coefficients can be structured on the image grid and transformed geometrically at deployment time. In a controlled MNIST setting with a rotation-invariant regression target, we introduce three geometry-aware inductive biases for split directions -- convolutional smoothing, a tilt dominance constraint, and importance-based pruning -- and quantify their impact on robustness under in-plane rotations. We further evaluate a deployment-time orientation search that selects a discrete rotation maximizing a forest-level confidence proxy without updating model parameters. Orientation search improves robustness under severe rotations but can be harmful near the canonical orientation when confidence is misaligned with correctness. Finally, we observe consistent trends on MNIST digit recognition implemented as one-vs-rest regression, highlighting both the promise and limitations of confidence-based orientation selection for model-tree ensembles.

</details>


### [49] [Prototypicality Bias Reveals Blindspots in Multimodal Evaluation Metrics](https://arxiv.org/abs/2601.04946)
*Subhadeep Roy,Gagan Bhatia,Steffen Eger*

Main category: cs.CV

TL;DR: The paper identifies prototypicality bias in text-to-image evaluation metrics and introduces ProtoBias benchmark to test it. They find current metrics often misrank non-prototypical but correct images vs prototypical but incorrect ones, and propose ProtoScore as a more robust metric.


<details>
  <summary>Details</summary>
Motivation: Current automatic metrics for text-to-image models may not truly evaluate semantic correctness but instead favor visually and socially prototypical images learned from biased data distributions. There's a need to understand whether these metrics prioritize textual semantics or default to prototypes.

Method: The authors introduce ProtoBias benchmark with controlled contrastive pairs spanning Animals, Objects, and Demography categories. Each pair contains a semantically correct but non-prototypical image and a subtly incorrect yet prototypical adversarial counterpart. They evaluate widely used metrics (CLIPScore, PickScore, VQA-based scores, LLM-as-Judge systems) and compare with human evaluations. They then propose ProtoScore, a 7B-parameter metric designed to reduce failure rates.

Result: Current metrics frequently misrank the contrastive pairs, favoring prototypical but incorrect images over semantically correct but non-prototypical ones. Human evaluations consistently favor semantic correctness with larger decision margins. ProtoScore substantially reduces failure rates and suppresses misranking while running much faster than large closed-source models, approaching their robustness.

Conclusion: Prototypicality bias is a systematic failure mode in multimodal evaluation. The proposed ProtoScore metric offers a more robust alternative to current evaluation methods, better aligning with human judgment of semantic correctness rather than visual prototypes.

Abstract: Automatic metrics are now central to evaluating text-to-image models, often substituting for human judgment in benchmarking and large-scale filtering. However, it remains unclear whether these metrics truly prioritize semantic correctness or instead favor visually and socially prototypical images learned from biased data distributions. We identify and study \emph{prototypicality bias} as a systematic failure mode in multimodal evaluation. We introduce a controlled contrastive benchmark \textsc{\textbf{ProtoBias}} (\textit{\textbf{Proto}typical \textbf{Bias}}), spanning Animals, Objects, and Demography images, where semantically correct but non-prototypical images are paired with subtly incorrect yet prototypical adversarial counterparts. This setup enables a directional evaluation of whether metrics follow textual semantics or default to prototypes. Our results show that widely used metrics, including CLIPScore, PickScore, and VQA-based scores, frequently misrank these pairs, while even LLM-as-Judge systems exhibit uneven robustness in socially grounded cases. Human evaluations consistently favour semantic correctness with larger decision margins. Motivated by these findings, we propose \textbf{\textsc{ProtoScore}}, a robust 7B-parameter metric that substantially reduces failure rates and suppresses misranking, while running at orders of magnitude faster than the inference time of GPT-5, approaching the robustness of much larger closed-source judges.

</details>


### [50] [TEA: Temporal Adaptive Satellite Image Semantic Segmentation](https://arxiv.org/abs/2601.04956)
*Juyuan Kang,Hao Zhu,Yan Zhu,Wei Zhang,Jianing Chen,Tianxiang Xiao,Yike Ma,Hao Jiang,Feng Dai*

Main category: cs.CV

TL;DR: TEA: Temporal Adaptive SITS semantic segmentation method that enhances model resilience under varying temporal sequence lengths by using teacher-student knowledge transfer and full-sequence reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing SITS segmentation approaches overlook generalization across scenarios with varying temporal lengths, leading to poor segmentation results when sequence lengths differ from training data.

Method: Proposes TEA with teacher model encapsulating global sequence knowledge to guide student model with adaptive temporal input lengths. Uses intermediate embedding, prototypes, and soft label knowledge transfer, dynamic student model aggregation, and full-sequence reconstruction as auxiliary task.

Result: Demonstrates remarkable improvements across inputs of different temporal lengths on common benchmarks compared to existing approaches.

Conclusion: TEA effectively addresses the temporal length generalization problem in SITS segmentation, providing a robust solution for agricultural parcel segmentation with varying satellite image time-series lengths.

Abstract: Crop mapping based on satellite images time-series (SITS) holds substantial economic value in agricultural production settings, in which parcel segmentation is an essential step. Existing approaches have achieved notable advancements in SITS segmentation with predetermined sequence lengths. However, we found that these approaches overlooked the generalization capability of models across scenarios with varying temporal length, leading to markedly poor segmentation results in such cases. To address this issue, we propose TEA, a TEmporal Adaptive SITS semantic segmentation method to enhance the model's resilience under varying sequence lengths. We introduce a teacher model that encapsulates the global sequence knowledge to guide a student model with adaptive temporal input lengths. Specifically, teacher shapes the student's feature space via intermediate embedding, prototypes and soft label perspectives to realize knowledge transfer, while dynamically aggregating student model to mitigate knowledge forgetting. Finally, we introduce full-sequence reconstruction as an auxiliary task to further enhance the quality of representations across inputs of varying temporal lengths. Through extensive experiments, we demonstrate that our method brings remarkable improvements across inputs of different temporal lengths on common benchmarks. Our code will be publicly available.

</details>


### [51] [SparseLaneSTP: Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection](https://arxiv.org/abs/2601.04968)
*Maximilian Pittner,Joel Janai,Mario Faigle,Alexandru Paul Condurache*

Main category: cs.CV

TL;DR: SparseLaneSTP: A sparse lane transformer that integrates lane geometry priors and temporal information for improved 3D lane detection, with a new dataset and state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing 3D lane detection methods have limitations: dense BEV approaches suffer from poor feature representation due to erroneous transformations, sparse detectors ignore valuable lane-specific priors, and no methods utilize historical lane observations to resolve visibility ambiguities.

Method: SparseLaneSTP integrates geometric lane properties and temporal information into a sparse lane transformer. It introduces: 1) lane-specific spatio-temporal attention mechanism, 2) continuous lane representation for sparse architectures, 3) temporal regularization. Also creates a new precise 3D lane dataset using auto-labeling.

Result: State-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks and on their novel dataset. Demonstrates benefits of integrating lane priors and temporal information.

Conclusion: SparseLaneSTP effectively addresses limitations of existing methods by incorporating lane geometry priors and temporal information, while the new dataset addresses weaknesses in existing 3D lane datasets. The approach achieves superior 3D lane detection performance.

Abstract: 3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.

</details>


### [52] [OceanSplat: Object-aware Gaussian Splatting with Trinocular View Consistency for Underwater Scene Reconstruction](https://arxiv.org/abs/2601.04984)
*Minseong Kweon,Jinsun Park*

Main category: cs.CV

TL;DR: OceanSplat is a 3D Gaussian Splatting method for underwater scene reconstruction that addresses optical degradation through trinocular view consistency and synthetic epipolar depth priors, with depth-aware alpha adjustment to reduce medium-induced artifacts.


<details>
  <summary>Details</summary>
Motivation: Underwater scenes suffer from optical degradation (scattering, absorption) that causes multi-view inconsistencies, making traditional 3D reconstruction methods unreliable. Existing approaches struggle to separate object geometry from the scattering medium, leading to floating artifacts and poor reconstruction quality.

Method: 1) Enforces trinocular view consistency by rendering horizontally/vertically translated camera views from each input view and aligning via inverse warping. 2) Uses translated views to derive synthetic epipolar depth prior through triangulation as self-supervised depth regularizer. 3) Proposes depth-aware alpha adjustment that modulates 3D Gaussian opacity during early training based on z-component and viewing direction to prevent medium-induced primitive formation.

Result: OceanSplat substantially outperforms existing methods on both real-world underwater and simulated scenes for scene reconstruction and restoration in scattering media. It successfully disentangles 3D Gaussians from scattering medium, enabling robust object geometry representation and significantly reducing floating artifacts.

Conclusion: The proposed geometric constraints (trinocular consistency and synthetic depth prior) combined with depth-aware alpha adjustment effectively address underwater optical degradation, allowing 3D Gaussian Splatting to work reliably in scattering media while preserving scene structure and reducing artifacts.

Abstract: We introduce OceanSplat, a novel 3D Gaussian Splatting-based approach for accurately representing 3D geometry in underwater scenes. To overcome multi-view inconsistencies caused by underwater optical degradation, our method enforces trinocular view consistency by rendering horizontally and vertically translated camera views relative to each input view and aligning them via inverse warping. Furthermore, these translated camera views are used to derive a synthetic epipolar depth prior through triangulation, which serves as a self-supervised depth regularizer. These geometric constraints facilitate the spatial optimization of 3D Gaussians and preserve scene structure in underwater environments. We also propose a depth-aware alpha adjustment that modulates the opacity of 3D Gaussians during early training based on their $z$-component and viewing direction, deterring the formation of medium-induced primitives. With our contributions, 3D Gaussians are disentangled from the scattering medium, enabling robust representation of object geometry and significantly reducing floating artifacts in reconstructed underwater scenes. Experiments on real-world underwater and simulated scenes demonstrate that OceanSplat substantially outperforms existing methods for both scene reconstruction and restoration in scattering media.

</details>


### [53] [Higher-Order Adversarial Patches for Real-Time Object Detectors](https://arxiv.org/abs/2601.04991)
*Jens Bayer,Stefan Becker,David Münch,Michael Arens,Jürgen Beyerer*

Main category: cs.CV

TL;DR: Higher-order adversarial attacks show stronger generalization than lower-order attacks, and adversarial training alone is insufficient to protect object detectors against them.


<details>
  <summary>Details</summary>
Motivation: To investigate the impact of higher-order adversarial attacks on object detectors through the cat-and-mouse game of adversarial attack patterns and adversarial training.

Method: Successively train attack patterns and harden object detectors with adversarial training using YOLOv10 as representative, employing adversarial patches in evasion attacks.

Result: Higher-order adversarial patches have stronger generalization capacity than lower-order patches, and adversarial training alone is insufficient to efficiently harden object detectors against such attacks.

Conclusion: Higher-order adversarial attacks pose significant threats to object detectors with better generalization, requiring more robust defense strategies beyond standard adversarial training.

Abstract: Higher-order adversarial attacks can directly be considered the result of a cat-and-mouse game -- an elaborate action involving constant pursuit, near captures, and repeated escapes. This idiom describes the enduring circular training of adversarial attack patterns and adversarial training the best. The following work investigates the impact of higher-order adversarial attacks on object detectors by successively training attack patterns and hardening object detectors with adversarial training. The YOLOv10 object detector is chosen as a representative, and adversarial patches are used in an evasion attack manner. Our results indicate that higher-order adversarial patches are not only affecting the object detector directly trained on but rather provide a stronger generalization capacity compared to lower-order adversarial patches. Moreover, the results highlight that solely adversarial training is not sufficient to harden an object detector efficiently against this kind of adversarial attack. Code: https://github.com/JensBayer/HigherOrder

</details>


### [54] [Patch-based Representation and Learning for Efficient Deformation Modeling](https://arxiv.org/abs/2601.05035)
*Ruochen Chen,Thuy Tran,Shaifali Parashar*

Main category: cs.CV

TL;DR: PolyFit is a patch-based surface representation using local jet functions that enables efficient surface deformation by updating compact jet coefficients instead of per-vertex optimization.


<details>
  <summary>Details</summary>
Motivation: Traditional surface deformation methods require optimizing per-vertex degrees of freedom, which is computationally expensive for many computer vision and graphics tasks. There's a need for more efficient representations that can generalize across different surface types and resolutions.

Method: PolyFit learns a patch-based surface representation by fitting jet functions locally on surface patches. It can be trained supervised from analytic functions and real data. The representation allows deformation by updating a compact set of jet coefficients rather than per-vertex optimization.

Result: The method demonstrates competitive performance in two applications: 1) Shape-from-template with test-time optimization that is faster than offline physics-based solvers and more accurate than recent physics-guided neural simulators; 2) Garment draping with a self-supervised, mesh- and garment-agnostic model that generalizes across resolutions and garment types with up to 10x faster inference than baselines.

Conclusion: PolyFit provides an efficient patch-based surface representation that enables fast and accurate surface deformation for various computer vision and graphics applications, offering significant speed advantages while maintaining or improving accuracy compared to existing methods.

Abstract: In this paper, we present a patch-based representation of surfaces, PolyFit, which is obtained by fitting jet functions locally on surface patches. Such a representation can be learned efficiently in a supervised fashion from both analytic functions and real data. Once learned, it can be generalized to various types of surfaces. Using PolyFit, the surfaces can be efficiently deformed by updating a compact set of jet coefficients rather than optimizing per-vertex degrees of freedom for many downstream tasks in computer vision and graphics. We demonstrate the capabilities of our proposed methodologies with two applications: 1) Shape-from-template (SfT): where the goal is to deform the input 3D template of an object as seen in image/video. Using PolyFit, we adopt test-time optimization that delivers competitive accuracy while being markedly faster than offline physics-based solvers, and outperforms recent physics-guided neural simulators in accuracy at modest additional runtime. 2) Garment draping. We train a self-supervised, mesh- and garment-agnostic model that generalizes across resolutions and garment types, delivering up to an order-of-magnitude faster inference than strong baselines.

</details>


### [55] [From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)](https://arxiv.org/abs/2601.05059)
*Suyash Mishra,Qiang Li,Srikanth Patil,Anubhav Girdhar*

Main category: cs.CV

TL;DR: A domain-adapted Video to Video Clip Generation framework using Audio Language Models and Vision Language Models for automated highlight clip generation in pharmaceutical applications, achieving 3-4x speedup and 4x cost reduction.


<details>
  <summary>Details</summary>
Motivation: Traditional manual annotation of heterogeneous pharmaceutical data (text, images, video, audio, web links) is inconsistent, inefficient, and quality-degrading, especially for long content like clinical trial interviews and educational seminars.

Method: A domain-adapted framework integrating ALMs and VLMs with: (1) reproducible Cut & Merge algorithm with fade in/out and timestamp normalization, (2) personalization via role definition and prompt injection, (3) cost-efficient end-to-end pipeline balancing ALM/VLM processing.

Result: 3-4x speedup, 4x cost reduction, competitive clip quality on 16,159 pharmacy videos across 14 disease areas. Improved clip coherence (0.348) and informativeness (0.721) scores over state-of-the-art VLM baselines like Gemini 2.5 Pro.

Conclusion: The framework demonstrates potential for transparent, custom extractive, and compliance-supporting video summarization in life sciences, enabling intelligent, scalable multi-modality content processing for pharmaceutical digital transformation.

Abstract: Vision Language Models (VLMs) are poised to revolutionize the digital transformation of pharmacyceutical industry by enabling intelligent, scalable, and automated multi-modality content processing. Traditional manual annotation of heterogeneous data modalities (text, images, video, audio, and web links), is prone to inconsistencies, quality degradation, and inefficiencies in content utilization. The sheer volume of long video and audio data further exacerbates these challenges, (e.g. long clinical trial interviews and educational seminars).
  Here, we introduce a domain adapted Video to Video Clip Generation framework that integrates Audio Language Models (ALMs) and Vision Language Models (VLMs) to produce highlight clips. Our contributions are threefold: (i) a reproducible Cut & Merge algorithm with fade in/out and timestamp normalization, ensuring smooth transitions and audio/visual alignment; (ii) a personalization mechanism based on role definition and prompt injection for tailored outputs (marketing, training, regulatory); (iii) a cost efficient e2e pipeline strategy balancing ALM/VLM enhanced processing. Evaluations on Video MME benchmark (900) and our proprietary dataset of 16,159 pharmacy videos across 14 disease areas demonstrate 3 to 4 times speedup, 4 times cost reduction, and competitive clip quality. Beyond efficiency gains, we also report our methods improved clip coherence scores (0.348) and informativeness scores (0.721) over state of the art VLM baselines (e.g., Gemini 2.5 Pro), highlighting the potential of transparent, custom extractive, and compliance supporting video summarization for life sciences.

</details>


### [56] [Driving on Registers](https://arxiv.org/abs/2601.05083)
*Ellington Kirby,Alexandre Boulch,Yihong Xu,Yuan Yin,Gilles Puy,Éloi Zablocki,Andrei Bursuc,Spyros Gidaris,Renaud Marlet,Florent Bartoccioni,Anh-Quan Cao,Nermin Samet,Tuan-Hung VU,Matthieu Cord*

Main category: cs.CV

TL;DR: DrivoR is a transformer-based autonomous driving system that uses camera-aware register tokens to compress multi-camera features, then generates and scores candidate trajectories with lightweight decoders, achieving state-of-the-art performance on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: To create an efficient end-to-end autonomous driving system that can handle multi-camera inputs while maintaining accuracy and enabling interpretable, behavior-conditioned driving through trajectory scoring.

Method: Uses pretrained Vision Transformers with camera-aware register tokens to compress multi-camera features into compact scene representations. Two lightweight transformer decoders then generate candidate trajectories and score them using interpretable sub-scores (safety, comfort, efficiency) learned from an oracle.

Result: Outperforms or matches strong contemporary baselines across NAVSIM-v1, NAVSIM-v2, and HUGSIM benchmarks. Demonstrates that pure-transformer architecture with token compression enables accurate, efficient, and adaptive end-to-end driving.

Conclusion: A pure-transformer architecture with targeted token compression is sufficient for accurate, efficient, and adaptive end-to-end autonomous driving, offering interpretable trajectory scoring and competitive performance across multiple benchmarks.

Abstract: We present DrivoR, a simple and efficient transformer-based architecture for end-to-end autonomous driving. Our approach builds on pretrained Vision Transformers (ViTs) and introduces camera-aware register tokens that compress multi-camera features into a compact scene representation, significantly reducing downstream computation without sacrificing accuracy. These tokens drive two lightweight transformer decoders that generate and then score candidate trajectories. The scoring decoder learns to mimic an oracle and predicts interpretable sub-scores representing aspects such as safety, comfort, and efficiency, enabling behavior-conditioned driving at inference. Despite its minimal design, DrivoR outperforms or matches strong contemporary baselines across NAVSIM-v1, NAVSIM-v2, and the photorealistic closed-loop HUGSIM benchmark. Our results show that a pure-transformer architecture, combined with targeted token compression, is sufficient for accurate, efficient, and adaptive end-to-end driving. Code and checkpoints will be made available via the project page.

</details>


### [57] [UniLiPs: Unified LiDAR Pseudo-Labeling with Geometry-Grounded Dynamic Scene Decomposition](https://arxiv.org/abs/2601.05105)
*Filippo Ghilotti,Samuel Brucker,Nahku Saidy,Matteo Matteucci,Mario Bijelic,Felix Heide*

Main category: cs.CV

TL;DR: Unsupervised 3D pseudo-labeling method that lifts text and 2D vision foundation model cues into 3D using temporal-geometric consistency across LiDAR sweeps, producing semantic labels, bounding boxes, and dense scans without manual supervision.


<details>
  <summary>Details</summary>
Motivation: Unlabeled LiDAR logs in autonomous driving are abundant but useless without human labels, creating a major cost barrier for perception research. The authors aim to overcome this bottleneck by leveraging temporal consistency to automatically generate 3D labels.

Method: Uses unsupervised multi-modal pseudo-labeling with geometric priors from temporally accumulated LiDAR maps. Features a novel iterative update rule enforcing joint geometric-semantic consistency and detects moving objects from inconsistencies. Lifts cues from text and 2D vision foundation models directly into 3D without manual input.

Result: Method produces 3D semantic labels, 3D bounding boxes, and dense LiDAR scans with robust generalization across three datasets. Outperforms existing pseudo-labeling methods that require additional supervision. Even small fractions of the geometrically consistent, densified LiDAR improve depth prediction by 51.5% and 22.0% MAE in 80-150m and 150-250m ranges respectively.

Conclusion: The approach successfully tackles the labeling bottleneck in autonomous perception by leveraging temporal-geometric consistency to create high-quality 3D pseudo-labels without manual supervision, demonstrating significant improvements in downstream tasks like depth prediction.

Abstract: Unlabeled LiDAR logs, in autonomous driving applications, are inherently a gold mine of dense 3D geometry hiding in plain sight - yet they are almost useless without human labels, highlighting a dominant cost barrier for autonomous-perception research. In this work we tackle this bottleneck by leveraging temporal-geometric consistency across LiDAR sweeps to lift and fuse cues from text and 2D vision foundation models directly into 3D, without any manual input. We introduce an unsupervised multi-modal pseudo-labeling method relying on strong geometric priors learned from temporally accumulated LiDAR maps, alongside with a novel iterative update rule that enforces joint geometric-semantic consistency, and vice-versa detecting moving objects from inconsistencies. Our method simultaneously produces 3D semantic labels, 3D bounding boxes, and dense LiDAR scans, demonstrating robust generalization across three datasets. We experimentally validate that our method compares favorably to existing semantic segmentation and object detection pseudo-labeling methods, which often require additional manual supervision. We confirm that even a small fraction of our geometrically consistent, densified LiDAR improves depth prediction by 51.5% and 22.0% MAE in the 80-150 and 150-250 meters range, respectively.

</details>


### [58] [From Rays to Projections: Better Inputs for Feed-Forward View Synthesis](https://arxiv.org/abs/2601.05116)
*Zirui Wu,Zeren Jiang,Martin R. Oswald,Jie Song*

Main category: cs.CV

TL;DR: The paper proposes projective conditioning for view synthesis, replacing raw camera parameters with stable 2D projective cues to improve geometric consistency and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing feed-forward view synthesis models use Plücker ray maps that tie predictions to arbitrary world coordinates and are sensitive to small camera transformations, undermining geometric consistency. The authors seek better conditioning inputs for robust and consistent view synthesis.

Method: Proposes projective conditioning which replaces raw camera parameters with target-view projective cues (stable 2D inputs), reframing the task from geometric regression in ray space to well-conditioned image-to-image translation. Also introduces masked autoencoding pretraining strategy tailored to this cue for using large-scale uncalibrated data.

Result: The method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on their view-consistency benchmark, and achieves state-of-the-art quality on standard novel view synthesis benchmarks.

Conclusion: Projective conditioning provides a more stable and effective approach to view synthesis by transforming the problem into a better-conditioned image translation task, enabling better geometric consistency and performance.

Abstract: Feed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Plücker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geometric consistency. In this paper, we ask what inputs best condition a model for robust and consistent view synthesis. We propose projective conditioning, which replaces raw camera parameters with a target-view projective cue that provides a stable 2D input. This reframes the task from a brittle geometric regression problem in ray space to a well-conditioned target-view image-to-image translation problem. Additionally, we introduce a masked autoencoding pretraining strategy tailored to this cue, enabling the use of large-scale uncalibrated data for pretraining. Our method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on our view-consistency benchmark. It also achieves state-of-the-art quality on standard novel view synthesis benchmarks.

</details>


### [59] [Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing](https://arxiv.org/abs/2601.05124)
*Runze He,Yiji Cheng,Tiankai Hang,Zhimin Li,Yu Xu,Zijin Yin,Shiyi Zhang,Wenxun Dai,Penghui Du,Ao Ma,Chunyu Wang,Qinglin Lu,Jizhong Han,Jiao Dai*

Main category: cs.CV

TL;DR: Re-Align is a unified framework that bridges the gap between understanding and generation in in-context image generation/editing through structured reasoning-guided alignment and RL training.


<details>
  <summary>Details</summary>
Motivation: Current unified multimodal models have strong understanding capabilities but these strengths don't effectively transfer to image generation tasks, creating a gap between understanding and generation in in-context image generation and editing.

Method: 1) In-Context Chain-of-Thought (IC-CoT) - structured reasoning paradigm that decouples semantic guidance and reference association. 2) RL training scheme using surrogate reward to measure alignment between structured reasoning text and generated images.

Result: Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.

Conclusion: The proposed Re-Align framework successfully bridges the understanding-generation gap through structured reasoning-guided alignment, enabling more precise and faithful execution of user intent in visual concept specification.

Abstract: In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.

</details>


### [60] [VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding](https://arxiv.org/abs/2601.05125)
*Ignacio de Rodrigo,Alvaro J. Lopez-Lopez,Jaime Boal*

Main category: cs.CV

TL;DR: VERSE is a methodology for analyzing and improving Vision-Language Models for document understanding by exploring visual embedding spaces, identifying problematic regions, and generating targeted synthetic data to boost performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in Vision-Language Models for Visually-rich Document Understanding by providing tools to analyze their visual embedding spaces, identify error-prone regions, and improve performance through targeted synthetic data generation.

Method: VERSE methodology enables visualization of latent representations in visual embedding spaces, identifies problematic clusters, and guides generation of synthetic data targeting those specific regions. Validation involves training on synthetic MERIT Dataset and evaluating on real-world MERIT Secret dataset.

Result: VERSE successfully uncovers visual features associated with error-prone clusters. Retraining with samples containing these features substantially boosts F1 performance without degrading generalization. On-premise models (Donut, Idefics2) optimized with VERSE match or surpass SaaS solutions (GPT-4, Pixtral).

Conclusion: VERSE provides an effective methodology for analyzing and improving Vision-Language Models for document understanding through visual embedding space exploration and targeted synthetic data generation, enabling on-premise models to compete with or outperform commercial SaaS solutions.

Abstract: This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.

</details>


### [61] [VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control](https://arxiv.org/abs/2601.05138)
*Sixiao Zheng,Minghao Yin,Wenbo Hu,Xiaoyu Li,Ying Shan,Yanwei Fu*

Main category: cs.CV

TL;DR: VerseCrafter is a 4D-aware video world model that enables explicit control over camera and object dynamics using a novel 4D Geometric Control representation, trained on automatically extracted 4D data from in-the-wild videos.


<details>
  <summary>Details</summary>
Motivation: Existing video world models struggle to provide unified and precise control over both camera and multi-object motion, as they operate in the 2D image plane rather than in a coherent 4D geometric world state.

Method: Introduces a 4D Geometric Control representation using static background point clouds and per-object 3D Gaussian trajectories that capture probabilistic 3D occupancy over time. These controls are rendered into conditioning signals for a pretrained video diffusion model. An automatic data engine extracts 4D controls from in-the-wild videos to overcome data scarcity.

Result: Enables generation of high-fidelity, view-consistent videos that precisely adhere to specified camera and object dynamics, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models.

Conclusion: VerseCrafter bridges the gap between 2D video generation and 4D geometric control, providing a unified framework for precise manipulation of both camera and object motion in video world models.

Abstract: Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.

</details>


### [62] [A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering](https://arxiv.org/abs/2601.05143)
*Md. Zahid Hossain,Most. Sharmin Sultana Samu,Md. Rakibul Islam,Md. Siam Ansary*

Main category: cs.CV

TL;DR: A lightweight vision-language framework for crop disease identification from leaf images using Swin Transformer encoder and seq2seq decoders, achieving high accuracy with fewer parameters than large-scale baselines.


<details>
  <summary>Details</summary>
Motivation: Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation, but existing models are often too large and computationally expensive for practical agricultural applications.

Method: Combines Swin Transformer vision encoder with sequence-to-sequence language decoders using a two-stage training strategy to improve visual representation learning and cross-modal alignment.

Result: High accuracy for both crop and disease identification, strong performance on BLEU, ROUGE and BERTScore metrics, outperforming large-scale vision-language baselines with significantly fewer parameters.

Conclusion: Task-specific visual pretraining is effective for crop disease visual question answering, and the lightweight framework demonstrates robust performance under diverse user queries while being explainable through Grad-CAM and token-level attribution.

Abstract: Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.

</details>


### [63] [Atlas 2 - Foundation models for clinical deployment](https://arxiv.org/abs/2601.05148)
*Maximilian Alber,Timo Milbich,Alexandra Carpen-Amarie,Stephan Tietz,Jonas Dippel,Lukas Muttenthaler,Beatriz Perez Cancer,Alessandro Benetti,Panos Korfiatis,Elias Eulig,Jérôme Lüscher,Jiasen Wu,Sayed Abid Hashimi,Gabriel Dernbach,Simon Schallenberg,Neelay Shah,Moritz Krügener,Aniruddh Jammoria,Jake Matras,Patrick Duffy,Matt Redlon,Philipp Jurmeister,David Horst,Lukas Ruff,Klaus-Robert Müller,Frederick Klauschen,Andrew Norgan*

Main category: cs.CV

TL;DR: Atlas 2 series (Atlas 2, 2-B, 2-S) are pathology vision foundation models achieving SOTA performance, robustness, and efficiency across 80 benchmarks, trained on largest pathology dataset of 5.5M whole slide images.


<details>
  <summary>Details</summary>
Motivation: Existing pathology foundation models have tradeoffs in performance, robustness, and computational requirements that limit clinical deployment. There's a need for models that bridge these shortcomings for practical clinical use.

Method: Developed three pathology vision foundation models (Atlas 2, Atlas 2-B, Atlas 2-S) trained on the largest pathology dataset to date - 5.5 million histopathology whole slide images collected from three medical institutions (Charité Berlin, LMU Munich, Mayo Clinic).

Result: Models show state-of-the-art performance in prediction performance, robustness, and resource efficiency across a comprehensive evaluation of eighty public benchmarks.

Conclusion: The Atlas 2 series successfully addresses the limitations of previous pathology foundation models, offering improved performance, robustness, and efficiency that could enable better clinical deployment.

Abstract: Pathology foundation models substantially advanced the possibilities in computational pathology -- yet tradeoffs in terms of performance, robustness, and computational requirements remained, which limited their clinical deployment. In this report, we present Atlas 2, Atlas 2-B, and Atlas 2-S, three pathology vision foundation models which bridge these shortcomings by showing state-of-the-art performance in prediction performance, robustness, and resource efficiency in a comprehensive evaluation across eighty public benchmarks. Our models were trained on the largest pathology foundation model dataset to date comprising 5.5 million histopathology whole slide images, collected from three medical institutions Charité - Universtätsmedizin Berlin, LMU Munich, and Mayo Clinic.

</details>


### [64] [Multi-Scale Local Speculative Decoding for Image Generation](https://arxiv.org/abs/2601.05149)
*Elia Peruzzo,Guillaume Sautière,Amirhossein Habibian*

Main category: cs.CV

TL;DR: MuLo-SD accelerates autoregressive image generation using multi-resolution drafting with local rejection/resampling, achieving 1.7× speedup while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models for image synthesis suffer from latency due to sequential nature. Existing speculative decoding approaches are limited by token-level ambiguity and lack spatial awareness.

Method: Multi-Scale Local Speculative Decoding (MuLo-SD) uses low-resolution drafter with learned up-samplers to propose candidate tokens, then verifies them in parallel with high-resolution target model. Includes local rejection and resampling mechanism focusing on spatial neighborhoods rather than raster-scan resampling.

Result: Achieves up to 1.7× speedup, outperforming EAGLE-2 and LANTERN baselines. Maintains comparable semantic alignment and perceptual quality validated on MS-COCO 5k split using GenEval, DPG-Bench, and FID/HPSv2 metrics.

Conclusion: Sets new state-of-the-art in speculative decoding for image synthesis, bridging efficiency and fidelity gap through multi-resolution drafting with spatially-aware verification.

Abstract: Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to $\mathbf{1.7\times}$ - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.

</details>


### [65] [Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering](https://arxiv.org/abs/2601.05159)
*Shuliang Liu,Songbo Yang,Dong Fang,Sihang Jia,Yuqi Tang,Lingfeng Su,Ruoshui Peng,Yibo Yan,Xin Zou,Xuming Hu*

Main category: cs.CV

TL;DR: Vision-Language Introspection (VLI) is a training-free framework that reduces object hallucination in multimodal models by simulating metacognitive self-correction through attributive introspection and interpretable bi-causal steering.


<details>
  <summary>Details</summary>
Motivation: Object hallucination undermines multimodal model reliability due to models blindly trusting linguistic priors over visual evidence. Existing methods are limited: contrastive decoding is superficial, and latent steering uses static vectors lacking instance-specific precision.

Method: VLI performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize causal visual anchors, then uses Interpretable Bi-Causal Steering to modulate inference by dynamically isolating visual evidence from noise while neutralizing blind confidence through adaptive calibration.

Result: VLI achieves state-of-the-art performance, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.

Conclusion: VLI effectively addresses object hallucination through a training-free metacognitive self-correction framework that rectifies internal semantic misalignments and provides instance-specific precision, significantly improving multimodal model reliability.

Abstract: Object hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current latent steering methods rely on static vectors that lack instance-specific precision. We introduce Vision-Language Introspection (VLI), a training-free inference framework that simulates a metacognitive self-correction process. VLI first performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize the causal visual anchors. It then employs Interpretable Bi-Causal Steering to actively modulate the inference process, dynamically isolating visual evidence from background noise while neutralizing blind confidence through adaptive calibration. VLI achieves state-of-the-art performance on advanced models, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.

</details>


### [66] [CoV: Chain-of-View Prompting for Spatial Reasoning](https://arxiv.org/abs/2601.05172)
*Haoyu Zhao,Akide Liu,Zeyu Zhang,Weijie Wang,Feng Chen,Ruihan Zhu,Gholamreza Haffari,Bohan Zhuang*

Main category: cs.CV

TL;DR: Chain-of-View (CoV) prompting enables VLMs to actively explore 3D scenes for better embodied question answering by selecting relevant viewpoints and iteratively gathering context.


<details>
  <summary>Details</summary>
Motivation: Current VLMs are limited to fixed input views, which restricts their ability to gather distributed context and perform complex spatial reasoning in 3D environments for embodied question answering.

Method: CoV prompting is a training-free framework that transforms VLMs into active viewpoint reasoners through coarse-to-fine exploration: first filtering redundant frames and identifying anchor views, then performing fine-grained view adjustment with iterative reasoning and discrete camera actions.

Result: CoV achieves +11.56% average improvement in LLM-Match on OpenEQA across four VLMs, with maximum gain of +13.62% on Qwen3-VL-Flash. It shows test-time scaling (+2.51% additional improvement with increased action budget) and strong performance on ScanQA (116 CIDEr / 31.9 EM@1) and SQA3D (51.1 EM@1).

Conclusion: Question-aligned view selection combined with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D embodied question answering without requiring additional training.

Abstract: Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.
  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\% improvement in LLM-Match, with a maximum gain of +13.62\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\% average improvement, peaking at +3.73\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.

</details>


### [67] [VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice](https://arxiv.org/abs/2601.05175)
*Shuming Liu,Mingchen Zhuge,Changsheng Zhao,Jun Chen,Lemeng Wu,Zechun Liu,Chenchen Zhu,Zhipeng Cai,Chong Zhou,Haozhe Liu,Ernie Chang,Saksham Suri,Hongyu Xu,Qi Qian,Wei Wen,Balakrishnan Varadarajan,Zhuang Liu,Hu Xu,Florian Bordes,Raghuraman Krishnamoorthi,Bernard Ghanem,Vikas Chandra,Yunyang Xiong*

Main category: cs.CV

TL;DR: VideoAuto-R1: A video understanding framework that uses "reason-when-necessary" strategy instead of always using chain-of-thought reasoning, achieving SOTA accuracy with 3.3x efficiency improvement.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought reasoning is computationally expensive for video models, and direct answering often matches or surpasses CoT performance. There's a need to determine when reasoning is actually necessary versus when it's wasteful.

Method: VideoAuto-R1 uses "Thinking Once, Answering Twice" paradigm: generate initial answer → perform reasoning → output reviewed answer, with both answers supervised via verifiable rewards. During inference, uses confidence score of initial answer to decide whether to reason.

Result: Achieves state-of-the-art accuracy on video QA and grounding benchmarks with ~3.3x efficiency improvement (reducing average response length from 149 to 44 tokens). Shows low thinking-mode activation on perception tasks but higher on reasoning-intensive tasks.

Conclusion: Explicit language-based reasoning is generally beneficial but not always necessary. VideoAuto-R1's reason-when-necessary strategy provides optimal balance between accuracy and efficiency for video understanding tasks.

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.

</details>


### [68] [Cutting AI Research Costs: How Task-Aware Compression Makes Large Language Model Agents Affordable](https://arxiv.org/abs/2601.05191)
*Zuhair Ahmed Khan Taha,Mohammed Mudassir Uddin,Shahnawaz Alam*

Main category: cs.CV

TL;DR: AgentCompress reduces LLM compute costs by 68.3% while maintaining 96.2% success rate by dynamically routing tasks to appropriately compressed model variants based on task difficulty.


<details>
  <summary>Details</summary>
Motivation: High computational costs of large language models (e.g., $127 per session for 70B parameter models) make them inaccessible to many academic labs, limiting their use for autonomous research tasks like literature review and hypothesis generation.

Method: Uses a small neural network to predict task difficulty from opening words, then routes tasks to suitably compressed model variants in under a millisecond. Differentiates between complex tasks (like hypothesis generation) and simpler ones (like bibliography reformatting).

Result: Tested across 500 research workflows in four scientific fields, achieving 68.3% reduction in compute costs while maintaining 96.2% of original success rate.

Conclusion: AgentCompress makes LLM-powered research tools financially accessible to academic labs by significantly reducing computational costs without substantial performance degradation, enabling more labs to conduct experiments rather than being sidelined by budget constraints.

Abstract: When researchers deploy large language models for autonomous tasks like reviewing literature or generating hypotheses, the computational bills add up quickly. A single research session using a 70-billion parameter model can cost around $127 in cloud fees, putting these tools out of reach for many academic labs. We developed AgentCompress to tackle this problem head-on. The core idea came from a simple observation during our own work: writing a novel hypothesis clearly demands more from the model than reformatting a bibliography. Why should both tasks run at full precision? Our system uses a small neural network to gauge how hard each incoming task will be, based only on its opening words, then routes it to a suitably compressed model variant. The decision happens in under a millisecond. Testing across 500 research workflows in four scientific fields, we cut compute costs by 68.3% while keeping 96.2% of the original success rate. For labs watching their budgets, this could mean the difference between running experiments and sitting on the sidelines

</details>


### [69] [Mechanisms of Prompt-Induced Hallucination in Vision-Language Models](https://arxiv.org/abs/2601.05201)
*William Rudman,Michal Golovanevsky,Dana Arad,Yonatan Belinkov,Ritambhara Singh,Carsten Eickhoff,Kyle Mahowald*

Main category: cs.CV

TL;DR: VLMs increasingly hallucinate by favoring text prompts over visual evidence as object counts increase, but specific attention heads can be ablated to reduce these hallucinations by 40% without retraining.


<details>
  <summary>Details</summary>
Motivation: Large vision-language models often hallucinate by prioritizing textual prompts over visual evidence, creating a reliability issue. The researchers wanted to understand this failure mode systematically, particularly how models respond when prompts overstate visual content.

Method: Used controlled object-counting experiments where prompts overstated object numbers. Conducted mechanistic analysis of three VLMs, identifying specific attention heads responsible for prompt-induced hallucinations. Performed ablation studies on these heads to measure hallucination reduction.

Result: At low object counts, models often correct prompt overstatements, but increasingly conform to incorrect prompts as object numbers increase. Identified specific "PIH-heads" whose ablation reduces hallucinations by at least 40% without additional training. Different models implement prompt copying through distinct mechanisms.

Conclusion: Prompt-induced hallucinations in VLMs are mediated by specific attention heads that can be targeted for intervention. The findings reveal model-specific implementation differences and provide insights into internal mechanisms driving hallucinations, suggesting potential pathways for improving VLM reliability.

Abstract: Large vision-language models (VLMs) are highly capable, yet often hallucinate by favoring textual prompts over visual evidence. We study this failure mode in a controlled object-counting setting, where the prompt overstates the number of objects in the image (e.g., asking a model to describe four waterlilies when only three are present). At low object counts, models often correct the overestimation, but as the number of objects increases, they increasingly conform to the prompt regardless of the discrepancy. Through mechanistic analysis of three VLMs, we identify a small set of attention heads whose ablation substantially reduces prompt-induced hallucinations (PIH) by at least 40% without additional training. Across models, PIH-heads mediate prompt copying in model-specific ways. We characterize these differences and show that PIH ablation increases correction toward visual evidence. Our findings offer insights into the internal mechanisms driving prompt-induced hallucinations, revealing model-specific differences in how these behaviors are implemented.

</details>


### [70] [MoE3D: A Mixture-of-Experts Module for 3D Reconstruction](https://arxiv.org/abs/2601.05208)
*Zichen Wang,Ang Cao,Liam J. Wang,Jeong Joon Park*

Main category: cs.CV

TL;DR: MoE3D is a mixture-of-experts module that improves 3D reconstruction by predicting multiple candidate depth maps and fusing them with dynamic weighting to sharpen depth boundaries and reduce artifacts.


<details>
  <summary>Details</summary>
Motivation: Existing feed-forward 3D reconstruction models suffer from blurry depth boundaries and flying-point artifacts, which degrade reconstruction quality.

Method: MoE3D predicts multiple candidate depth maps and fuses them via dynamic weighting (mixture-of-experts approach) to sharpen boundaries and reduce artifacts.

Result: When integrated with pre-trained 3D reconstruction backbones like VGGT, MoE3D substantially enhances reconstruction quality with minimal additional computational overhead.

Conclusion: MoE3D effectively improves 3D reconstruction by addressing boundary sharpness and artifact issues through a mixture-of-experts approach that can be integrated with existing models.

Abstract: MoE3D is a mixture-of-experts module designed to sharpen depth boundaries and mitigate flying-point artifacts (highlighted in red) of existing feed-forward 3D reconstruction models (left side). MoE3D predicts multiple candidate depth maps and fuses them via dynamic weighting (visualized by MoE weights on the right side). When integrated with a pre-trained 3D reconstruction backbone such as VGGT, it substantially enhances reconstruction quality with minimal additional computational overhead. Best viewed digitally.

</details>


### [71] [FlowLet: Conditional 3D Brain MRI Synthesis using Wavelet Flow Matching](https://arxiv.org/abs/2601.05212)
*Danilo Danese,Angela Lombardi,Matteo Attimonelli,Giuseppe Fasano,Tommaso Di Noia*

Main category: cs.CV

TL;DR: FlowLet is a conditional generative framework that synthesizes age-conditioned 3D MRIs using flow matching in an invertible 3D wavelet domain to improve Brain Age Prediction fairness and performance.


<details>
  <summary>Details</summary>
Motivation: Existing 3D MRI datasets for Brain Age Prediction are demographically skewed, limiting fairness and generalizability. Current generative methods for data augmentation are slow, introduce artifacts, and rarely condition on age, affecting BAP performance.

Method: FlowLet uses flow matching within an invertible 3D wavelet domain to synthesize age-conditioned 3D MRIs, avoiding reconstruction artifacts and reducing computational demands compared to latent diffusion models.

Result: FlowLet generates high-fidelity volumes with few sampling steps. Training BAP models with FlowLet-generated data improves performance for underrepresented age groups, with region-based analysis confirming preservation of anatomical structures.

Conclusion: FlowLet provides an effective conditional generative framework for 3D MRI synthesis that enhances Brain Age Prediction fairness and performance while addressing computational and artifact issues of existing methods.

Abstract: Brain Magnetic Resonance Imaging (MRI) plays a central role in studying neurological development, aging, and diseases. One key application is Brain Age Prediction (BAP), which estimates an individual's biological brain age from MRI data. Effective BAP models require large, diverse, and age-balanced datasets, whereas existing 3D MRI datasets are demographically skewed, limiting fairness and generalizability. Acquiring new data is costly and ethically constrained, motivating generative data augmentation. Current generative methods are often based on latent diffusion models, which operate in learned low dimensional latent spaces to address the memory demands of volumetric MRI data. However, these methods are typically slow at inference, may introduce artifacts due to latent compression, and are rarely conditioned on age, thereby affecting the BAP performance. In this work, we propose FlowLet, a conditional generative framework that synthesizes age-conditioned 3D MRIs by leveraging flow matching within an invertible 3D wavelet domain, helping to avoid reconstruction artifacts and reducing computational demands. Experiments show that FlowLet generates high-fidelity volumes with few sampling steps. Training BAP models with data generated by FlowLet improves performance for underrepresented age groups, and region-based analysis confirms preservation of anatomical structures.

</details>


### [72] [ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos](https://arxiv.org/abs/2601.05237)
*Rustin Soraki,Homanga Bharadhwaj,Ali Farhadi,Roozbeh Mottaghi*

Main category: cs.CV

TL;DR: ObjectForesight is a 3D object-centric dynamics model that predicts future 6-DoF poses and trajectories of rigid objects from short egocentric videos, using explicit 3D object representations for geometrically grounded predictions.


<details>
  <summary>Details</summary>
Motivation: Humans can effortlessly anticipate how objects might move or change through interaction, but computational systems lack this ability. The goal is to enable systems to predict plausible future object motions directly from passive visual observation, similar to human anticipation of object affordances and trajectories.

Method: ObjectForesight uses explicit 3D object-level representations rather than pixel or latent space approaches. It leverages recent advances in segmentation, mesh reconstruction, and 3D pose estimation to curate a large dataset of 2M+ short clips with pseudo-ground-truth 3D object trajectories for training at scale.

Result: ObjectForesight achieves significant gains in accuracy, geometric consistency, and generalization to unseen objects and scenes. It establishes a scalable framework for learning physically grounded, object-centric dynamics models directly from observation.

Conclusion: The paper presents ObjectForesight as a novel approach to object motion prediction that operates in explicit 3D space at the object level, enabling geometrically grounded and temporally coherent predictions that capture object affordances and trajectories, with strong generalization capabilities.

Abstract: Humans can effortlessly anticipate how objects might move or change through interaction--imagining a cup being lifted, a knife slicing, or a lid being closed. We aim to endow computational systems with a similar ability to predict plausible future object motions directly from passive visual observation. We introduce ObjectForesight, a 3D object-centric dynamics model that predicts future 6-DoF poses and trajectories of rigid objects from short egocentric video sequences. Unlike conventional world or dynamics models that operate in pixel or latent space, ObjectForesight represents the world explicitly in 3D at the object level, enabling geometrically grounded and temporally coherent predictions that capture object affordances and trajectories. To train such a model at scale, we leverage recent advances in segmentation, mesh reconstruction, and 3D pose estimation to curate a dataset of 2 million plus short clips with pseudo-ground-truth 3D object trajectories. Through extensive experiments, we show that ObjectForesight achieves significant gains in accuracy, geometric consistency, and generalization to unseen objects and scenes, establishing a scalable framework for learning physically grounded, object-centric dynamics models directly from observation. objectforesight.github.io

</details>


### [73] [Plenoptic Video Generation](https://arxiv.org/abs/2601.05239)
*Xiao Fu,Shitao Tang,Min Shi,Xian Liu,Jinwei Gu,Ming-Yu Liu,Dahua Lin,Chen-Hsuan Lin*

Main category: cs.CV

TL;DR: PlenopticDreamer is a framework for multi-view consistent video re-rendering that maintains spatio-temporal coherence in hallucinated regions through synchronized generative hallucinations and camera-guided video retrieval.


<details>
  <summary>Details</summary>
Motivation: Existing camera-controlled generative video re-rendering methods struggle with multi-view consistency and spatio-temporal coherence in hallucinated regions due to generative model stochasticity.

Method: Trains a multi-in-single-out video-conditioned model autoregressively with camera-guided video retrieval, progressive context-scaling, self-conditioning for error robustness, and long-video conditioning for extended generation.

Result: Achieves SOTA on Basic and Agibot benchmarks with superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations including robotic manipulation scenarios.

Conclusion: PlenopticDreamer effectively addresses multi-view consistency challenges in video re-rendering through synchronized hallucination and adaptive conditioning strategies.

Abstract: Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/

</details>


### [74] [RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation](https://arxiv.org/abs/2601.05241)
*Boyang Wang,Haoran Zhang,Shujie Zhang,Jinkun Hao,Mingda Jia,Qi Lv,Yucheng Mao,Zhaoyang Lyu,Jia Zeng,Xudong Xu,Jiangmiao Pang*

Main category: cs.CV

TL;DR: Visual identity prompting for diffusion-based robot data augmentation improves manipulation policy training by providing exemplar images as conditioning, addressing multi-view and temporal coherence needs.


<details>
  <summary>Details</summary>
Motivation: Collecting large-scale real-world manipulation data is difficult due to hardware constraints, and existing text-prompt conditioned diffusion models overlook multi-view and temporal coherence requirements while text prompts alone cannot reliably specify scene setups.

Method: Introduce visual identity prompting that supplies exemplar images as conditioning inputs to guide scene setup generation, and build a scalable pipeline to curate visual identity pools from large robotics datasets.

Result: Using augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.

Conclusion: Visual identity prompting addresses limitations of text-only conditioning for robot data augmentation, enabling more effective training of manipulation policies through improved scene specification and coherence.

Abstract: The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.

</details>


### [75] [GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation](https://arxiv.org/abs/2601.05244)
*Henghui Ding,Chang Liu,Shuting He,Xudong Jiang,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: This paper introduces GREx (Generalized Referring Expression Segmentation/Comprehension/Generation), extending classic REx to handle multi-target and no-target expressions, not just single-target ones. It creates gRefCOCO dataset and proposes ReLA baseline method for complex relationship modeling.


<details>
  <summary>Details</summary>
Motivation: Existing RES/REC/REG methods only support single-target expressions (one expression refers to one object), which limits real-world applications. Real scenarios often involve expressions referring to multiple objects or no objects at all.

Method: 1) Creates gRefCOCO dataset with multi-target, no-target, and single-target expressions. 2) Proposes ReLA baseline that adaptively divides images into regions with sub-instance clues and explicitly models region-region and region-language dependencies for complex relationship modeling.

Result: GREx and gRefCOCO are backward-compatible with existing REx. ReLA achieves state-of-the-art results on both GRES and GREC tasks. The dataset enables studying performance gaps of existing methods on generalized tasks.

Conclusion: The paper successfully extends REx to handle arbitrary numbers of objects, addressing limitations of single-target-only approaches. The proposed GREx framework, gRefCOCO dataset, and ReLA method provide comprehensive solutions for generalized referring expression tasks.

Abstract: Referring Expression Segmentation (RES) and Comprehension (REC) respectively segment and detect the object described by an expression, while Referring Expression Generation (REG) generates an expression for the selected object. Existing datasets and methods commonly support single-target expressions only, i.e., one expression refers to one object, not considering multi-target and no-target expressions. This greatly limits the real applications of REx (RES/REC/REG). This paper introduces three new benchmarks called Generalized Referring Expression Segmentation (GRES), Comprehension (GREC), and Generation (GREG), collectively denoted as GREx, which extend the classic REx to allow expressions to identify an arbitrary number of objects. We construct the first large-scale GREx dataset gRefCOCO that contains multi-target, no-target, and single-target expressions and their corresponding images with labeled targets. GREx and gRefCOCO are designed to be backward-compatible with REx, facilitating extensive experiments to study the performance gap of the existing REx methods on GREx tasks. One of the challenges of GRES/GREC is complex relationship modeling, for which we propose a baseline ReLA that adaptively divides the image into regions with sub-instance clues and explicitly models the region-region and region-language dependencies. The proposed ReLA achieves the state-of-the-art results on the both GRES and GREC tasks. The proposed gRefCOCO dataset and method are available at https://henghuiding.github.io/GREx.

</details>


### [76] [Pixel-Perfect Visual Geometry Estimation](https://arxiv.org/abs/2601.05246)
*Gangwei Xu,Haotong Lin,Hongcheng Luo,Haiyang Sun,Bing Wang,Guang Chen,Sida Peng,Hangjun Ye,Xin Yang*

Main category: cs.CV

TL;DR: Pixel-perfect visual geometry models (PPD for images, PPVD for video) use pixel-space diffusion transformers with semantic prompting and cascade architecture to generate high-quality, flying-pixel-free depth maps and point clouds.


<details>
  <summary>Details</summary>
Motivation: Existing geometry foundation models suffer from flying pixels and loss of fine details, which is problematic for robotics and augmented reality applications that require clean and accurate geometry from images.

Method: 1) Pixel-Perfect Depth (PPD): Monocular depth foundation model using pixel-space diffusion transformers (DiT) with two key designs: Semantics-Prompted DiT (incorporates semantic representations from vision foundation models) and Cascade DiT architecture (progressively increases image tokens). 2) Pixel-Perfect Video Depth (PPVD): Extends PPD to video with Semantics-Consistent DiT (extracts temporally consistent semantics) and reference-guided token propagation for temporal coherence.

Result: Achieves best performance among all generative monocular and video depth estimation models, producing significantly cleaner point clouds than all other models.

Conclusion: The proposed pixel-perfect visual geometry models successfully address flying pixel issues and preserve fine details through generative modeling in pixel space with semantic prompting and efficient cascade architectures, enabling high-quality geometry recovery for robotics and AR applications.

Abstract: Recovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.

</details>


### [77] [RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes](https://arxiv.org/abs/2601.05249)
*Yuan-Kang Lee,Kuan-Lin Chen,Chia-Che Chang,Yu-Lun Liu*

Main category: cs.CV

TL;DR: RL-AWB combines statistical methods with deep reinforcement learning for nighttime white balance, achieving superior generalization across lighting conditions.


<details>
  <summary>Details</summary>
Motivation: Nighttime color constancy is challenging due to low-light noise and complex illumination conditions, requiring better solutions than existing methods.

Method: Combines statistical algorithm for nighttime scenes (salient gray pixel detection + illumination estimation) with deep reinforcement learning that uses the statistical algorithm as its core, mimicking professional AWB tuning experts.

Result: Achieves superior generalization capability across low-light and well-illuminated images, with introduction of first multi-sensor nighttime dataset for cross-sensor evaluation.

Conclusion: RL-AWB presents an effective framework for nighttime white balance that combines statistical and learning-based approaches, demonstrating strong performance across diverse lighting conditions.

Abstract: Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/

</details>


### [78] [QNeRF: Neural Radiance Fields on a Simulated Gate-Based Quantum Computer](https://arxiv.org/abs/2601.05250)
*Daniele Lizzio Bosco,Shuteng Wang,Giuseppe Serra,Vladislav Golyanik*

Main category: cs.CV

TL;DR: QNeRF is a hybrid quantum-classical model for novel-view synthesis that uses parameterized quantum circuits to encode spatial and view-dependent information, achieving comparable performance to classical NeRF with less than half the parameters.


<details>
  <summary>Details</summary>
Motivation: Neural Radiance Fields (NeRFs) have advanced novel-view synthesis but require large models and intensive training. Quantum Visual Fields (QVFs) have shown promise in model compactness and convergence speed. The authors aim to combine these approaches to create more efficient 3D scene representation from 2D images.

Method: QNeRF introduces two architectural variants: Full QNeRF that maximizes quantum amplitudes for representational capabilities, and Dual-Branch QNeRF that separates spatial and view-dependent quantum state preparations to reduce complexity and ensure scalability. Both use parameterized quantum circuits with superposition and entanglement to encode information.

Result: When trained on moderate-resolution images, QNeRF matches or outperforms classical NeRF baselines while using less than half the number of parameters. This demonstrates quantum machine learning can be competitive for continuous signal representation in computer vision tasks.

Conclusion: Quantum machine learning provides a viable alternative for 3D representation learning from 2D observations, offering more compact models with comparable performance to classical approaches, particularly for mid-level computer vision tasks.

Abstract: Recently, Quantum Visual Fields (QVFs) have shown promising improvements in model compactness and convergence speed for learning the provided 2D or 3D signals. Meanwhile, novel-view synthesis has seen major advances with Neural Radiance Fields (NeRFs), where models learn a compact representation from 2D images to render 3D scenes, albeit at the cost of larger models and intensive training. In this work, we extend the approach of QVFs by introducing QNeRF, the first hybrid quantum-classical model designed for novel-view synthesis from 2D images. QNeRF leverages parameterised quantum circuits to encode spatial and view-dependent information via quantum superposition and entanglement, resulting in more compact models compared to the classical counterpart. We present two architectural variants. Full QNeRF maximally exploits all quantum amplitudes to enhance representational capabilities. In contrast, Dual-Branch QNeRF introduces a task-informed inductive bias by branching spatial and view-dependent quantum state preparations, drastically reducing the complexity of this operation and ensuring scalability and potential hardware compatibility. Our experiments demonstrate that -- when trained on images of moderate resolution -- QNeRF matches or outperforms classical NeRF baselines while using less than half the number of parameters. These results suggest that quantum machine learning can serve as a competitive alternative for continuous signal representation in mid-level tasks in computer vision, such as 3D representation learning from 2D observations.

</details>


### [79] [Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video](https://arxiv.org/abs/2601.05251)
*Zeren Jiang,Chuanxia Zheng,Iro Laina,Diane Larlus,Andrea Vedaldi*

Main category: cs.CV

TL;DR: Mesh4D is a feed-forward model for monocular 4D mesh reconstruction that reconstructs 3D shape and motion from single-view videos using a compact latent space and latent diffusion model.


<details>
  <summary>Details</summary>
Motivation: To enable accurate reconstruction of complete 3D shape and motion from monocular videos of dynamic objects, overcoming limitations of existing methods in handling complex deformations and temporal consistency.

Method: Uses an autoencoder with spatio-temporal attention to learn a compact latent space encoding entire animation sequences, guided by skeletal structure during training but not required at inference. A latent diffusion model conditioned on input video and first-frame mesh predicts full animation in one shot.

Result: Outperforms prior methods on reconstruction and novel view synthesis benchmarks, achieving more accurate 3D shape and deformation recovery from monocular videos.

Conclusion: Mesh4D provides an effective feed-forward approach for 4D mesh reconstruction that leverages learned latent representations and diffusion modeling to achieve state-of-the-art performance without requiring skeletal information at inference time.

Abstract: We propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction. Given a monocular video of a dynamic object, our model reconstructs the object's complete 3D shape and motion, represented as a deformation field. Our key contribution is a compact latent space that encodes the entire animation sequence in a single pass. This latent space is learned by an autoencoder that, during training, is guided by the skeletal structure of the training objects, providing strong priors on plausible deformations. Crucially, skeletal information is not required at inference time. The encoder employs spatio-temporal attention, yielding a more stable representation of the object's overall deformation. Building on this representation, we train a latent diffusion model that, conditioned on the input video and the mesh reconstructed from the first frame, predicts the full animation in one shot. We evaluate Mesh4D on reconstruction and novel view synthesis benchmarks, outperforming prior methods in recovering accurate 3D shape and deformation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [80] [The Forgotten Shield: Safety Grafting in Parameter-Space for Medical MLLMs](https://arxiv.org/abs/2601.04199)
*Jiale Zhao,Xing Mou,Jinlin Wu,Hongyuan Yu,Mingrui Sun,Yang Shi,Xuanwu Yin,Zhen Chen,Zhen Lei,Yaohua Wang*

Main category: cs.LG

TL;DR: Medical MLLMs have safety vulnerabilities, especially to cross-modality jailbreak attacks, and medical fine-tuning causes safety forgetting. Proposed Parameter-Space Intervention extracts safety knowledge from base models and injects it during medical training with optimal safety-performance trade-off.


<details>
  <summary>Details</summary>
Motivation: Medical MLLMs show remarkable progress but lack safety research, posing real-world deployment risks. Current models have pervasive vulnerabilities, particularly to cross-modality jailbreak attacks, and medical fine-tuning causes catastrophic forgetting of original safety alignment.

Method: 1) Established multidimensional safety evaluation framework for Medical MLLMs. 2) Proposed Parameter-Space Intervention approach that extracts intrinsic safety knowledge representations from original base models and injects them into target models during medical capability construction. 3) Designed fine-grained parameter search algorithm to optimize safety-performance trade-off.

Result: Empirical analysis revealed pervasive vulnerabilities across general and medical-specific safety dimensions. The approach significantly bolsters safety guardrails without additional domain-specific safety data while minimizing degradation to core medical performance.

Conclusion: Medical MLLMs have serious safety vulnerabilities that need systematic addressing. The proposed Parameter-Space Intervention provides an effective solution for safety re-alignment without compromising medical capabilities, enabling safer real-world deployment of medical AI systems.

Abstract: Medical Multimodal Large Language Models (Medical MLLMs) have achieved remarkable progress in specialized medical tasks; however, research into their safety has lagged, posing potential risks for real-world deployment. In this paper, we first establish a multidimensional evaluation framework to systematically benchmark the safety of current SOTA Medical MLLMs. Our empirical analysis reveals pervasive vulnerabilities across both general and medical-specific safety dimensions in existing models, particularly highlighting their fragility against cross-modality jailbreak attacks. Furthermore, we find that the medical fine-tuning process frequently induces catastrophic forgetting of the model's original safety alignment. To address this challenge, we propose a novel "Parameter-Space Intervention" approach for efficient safety re-alignment. This method extracts intrinsic safety knowledge representations from original base models and concurrently injects them into the target model during the construction of medical capabilities. Additionally, we design a fine-grained parameter search algorithm to achieve an optimal trade-off between safety and medical performance. Experimental results demonstrate that our approach significantly bolsters the safety guardrails of Medical MLLMs without relying on additional domain-specific safety data, while minimizing degradation to core medical performance.

</details>


### [81] [Green MLOps: Closed-Loop, Energy-Aware Inference with NVIDIA Triton, FastAPI, and Bio-Inspired Thresholding](https://arxiv.org/abs/2601.04250)
*Mustapha Hamdi,Mourad Jabou*

Main category: cs.LG

TL;DR: Bio-inspired framework maps protein-folding energy basins to inference cost landscapes, using decaying closed-loop threshold to reduce processing time by 42% with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Energy efficiency is critical in AI deployment since long-running inference can exceed training in cumulative carbon impact, requiring practical solutions for Green MLOps.

Method: Bio-inspired framework that maps protein-folding energy basins to inference cost landscapes, controlling execution via decaying closed-loop threshold. Requests are admitted only when expected utility-to-energy trade-off is favorable, biasing toward first acceptable local basin rather than costly global minima.

Result: Bio-controller reduces processing time by 42% compared to standard open-loop execution (0.50s vs 0.29s on A100 test set) with minimal accuracy degradation (<0.5%). Establishes efficiency boundaries between lightweight local serving (ORT) and managed batching (Triton).

Conclusion: Connects biophysical energy models to Green MLOps, offering practical, auditable basis for closed-loop energy-aware inference in production systems.

Abstract: Energy efficiency is a first-order concern in AI deployment, as long-running inference can exceed training in cumulative carbon impact. We propose a bio-inspired framework that maps protein-folding energy basins to inference cost landscapes and controls execution via a decaying, closed-loop threshold. A request is admitted only when the expected utility-to-energy trade-off is favorable (high confidence/utility at low marginal energy and congestion), biasing operation toward the first acceptable local basin rather than pursuing costly global minima. We evaluate DistilBERT and ResNet-18 served through FastAPI with ONNX Runtime and NVIDIA Triton on an RTX 4000 Ada GPU. Our ablation study reveals that the bio-controller reduces processing time by 42% compared to standard open-loop execution (0.50s vs 0.29s on A100 test set), with a minimal accuracy degradation (<0.5%). Furthermore, we establish the efficiency boundaries between lightweight local serving (ORT) and managed batching (Triton). The results connect biophysical energy models to Green MLOps and offer a practical, auditable basis for closed-loop energy-aware inference in production.

</details>


### [82] [Safety-Utility Conflicts Are Not Global: Surgical Alignment via Head-Level Diagnosis](https://arxiv.org/abs/2601.04262)
*Wang Cai,Yilin Wen,Jinchang Hou,Du Su,Guoqiu Wang,Zhonghou Lv,Chenfu Bao,Yunfang Wu*

Main category: cs.LG

TL;DR: CAST is a sparse fine-tuning framework that addresses safety alignment conflicts in LLMs by selectively updating parameters based on head-level conflict analysis, avoiding high-conflict attention heads to preserve general capabilities while maintaining safety.


<details>
  <summary>Details</summary>
Motivation: Safety alignment in LLMs creates multi-objective optimization conflicts that degrade general capabilities. Existing global gradient approaches ignore modular heterogeneity in Transformers where functional sensitivity and conflict levels vary across attention heads, leading to suboptimal trade-offs.

Method: CAST (Conflict-Aware Sparse Tuning) integrates head-level diagnosis with sparse fine-tuning. It first constructs a pre-alignment conflict map by synthesizing Optimization Conflict and Functional Sensitivity metrics, then uses this map to guide selective parameter updates, specifically skipping high-conflict attention heads during training.

Result: Experiments show alignment conflicts in LLMs are not uniformly distributed. The drop in general capabilities mainly comes from updating a small group of high-conflict heads. By skipping these heads during training, CAST significantly reduces capability loss without compromising safety.

Conclusion: CAST offers an interpretable and parameter-efficient approach to improving the safety-utility trade-off in LLMs by addressing the modular heterogeneity of alignment conflicts through selective sparse fine-tuning based on head-level conflict analysis.

Abstract: Safety alignment in Large Language Models (LLMs) inherently presents a multi-objective optimization conflict, often accompanied by an unintended degradation of general capabilities. Existing mitigation strategies typically rely on global gradient geometry to resolve these conflicts, yet they overlook Modular Heterogeneity within Transformers, specifically that the functional sensitivity and degree of conflict vary substantially across different attention heads. Such global approaches impose uniform update rules across all parameters, often resulting in suboptimal trade-offs by indiscriminately updating utility sensitive heads that exhibit intense gradient conflicts. To address this limitation, we propose Conflict-Aware Sparse Tuning (CAST), a framework that integrates head-level diagnosis with sparse fine-tuning. CAST first constructs a pre-alignment conflict map by synthesizing Optimization Conflict and Functional Sensitivity, which then guides the selective update of parameters. Experiments reveal that alignment conflicts in LLMs are not uniformly distributed. We find that the drop in general capabilities mainly comes from updating a small group of ``high-conflict'' heads. By simply skipping these heads during training, we significantly reduce this loss without compromising safety, offering an interpretable and parameter-efficient approach to improving the safety-utility trade-off.

</details>


### [83] [Learning to Reason: Temporal Saliency Distillation for Interpretable Knowledge Transfer](https://arxiv.org/abs/2601.04263)
*Nilushika Udayangani Hewa Dehigahawattage,Kishor Nandakishor,Marimuthu Palaniswami*

Main category: cs.LG

TL;DR: Proposes Temporal Saliency Distillation (TSD) for time series knowledge distillation, transferring interpretable temporal importance patterns from teacher to student models instead of just logits/features.


<details>
  <summary>Details</summary>
Motivation: Current knowledge distillation methods for time series (adapted from computer vision) have two key limitations: 1) they're uninterpretable (unclear how transferred knowledge helps student learning), and 2) they transfer limited knowledge, mainly replicating teacher accuracy, resulting in students with different predictive distributions that can't safely substitute teachers.

Method: Extends conventional logit transfer to convey teacher's reasoning via temporal saliency - importance of each input timestep to teacher predictions. Temporal Saliency Distillation trains students to make predictions based on same input features as teacher, requiring no additional parameters or architecture assumptions.

Result: Temporal Saliency Distillation effectively improves baseline methods' performance while achieving desirable properties beyond predictive accuracy.

Conclusion: Establishes a new paradigm for interpretable knowledge distillation in time series analysis by transferring not just predictions but also the reasoning behind them.

Abstract: Knowledge distillation has proven effective for model compression by transferring knowledge from a larger network called the teacher to a smaller network called the student. Current knowledge distillation in time series is predominantly based on logit and feature aligning techniques originally developed for computer vision tasks. These methods do not explicitly account for temporal data and fall short in two key aspects. First, the mechanisms by which the transferred knowledge helps the student model learning process remain unclear due to uninterpretability of logits and features. Second, these methods transfer only limited knowledge, primarily replicating the teacher predictive accuracy. As a result, student models often produce predictive distributions that differ significantly from those of their teachers, hindering their safe substitution for teacher models. In this work, we propose transferring interpretable knowledge by extending conventional logit transfer to convey not just the right prediction but also the right reasoning of the teacher. Specifically, we induce other useful knowledge from the teacher logits termed temporal saliency which captures the importance of each input timestep to the teacher prediction. By training the student with Temporal Saliency Distillation we encourage it to make predictions based on the same input features as the teacher. Temporal Saliency Distillation requires no additional parameters or architecture specific assumptions. We demonstrate that Temporal Saliency Distillation effectively improves the performance of baseline methods while also achieving desirable properties beyond predictive accuracy. We hope our work establishes a new paradigm for interpretable knowledge distillation in time series analysis.

</details>


### [84] [MemKD: Memory-Discrepancy Knowledge Distillation for Efficient Time Series Classification](https://arxiv.org/abs/2601.04264)
*Nilushika Udayangani,Kishor Nandakishor,Marimuthu Palaniswami*

Main category: cs.LG

TL;DR: MemKD is a novel knowledge distillation framework for time series models that addresses memory retention discrepancies between teacher and student models, enabling 500x parameter reduction while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for time series analysis (RNNs, LSTMs) have high computational complexity and large model sizes, making deployment challenging in resource-constrained environments like wearable devices and edge computing platforms. Existing KD methods designed for computer vision neglect the unique temporal dependencies and memory retention characteristics of time series models.

Method: Proposed Memory-Discrepancy Knowledge Distillation (MemKD) framework uses a specialized loss function to capture memory retention discrepancies between teacher and student models across subsequences within time series data, ensuring the student effectively mimics the teacher's behavior.

Result: MemKD significantly outperforms state-of-the-art KD methods, reduces parameter size and memory usage by approximately 500 times while maintaining comparable performance to the teacher model.

Conclusion: MemKD facilitates the development of compact, high-performing recurrent neural networks suitable for real-time, time series analysis tasks in resource-constrained environments.

Abstract: Deep learning models, particularly recurrent neural networks and their variants, such as long short-term memory, have significantly advanced time series data analysis. These models capture complex, sequential patterns in time series, enabling real-time assessments. However, their high computational complexity and large model sizes pose challenges for deployment in resource-constrained environments, such as wearable devices and edge computing platforms. Knowledge Distillation (KD) offers a solution by transferring knowledge from a large, complex model (teacher) to a smaller, more efficient model (student), thereby retaining high performance while reducing computational demands. Current KD methods, originally designed for computer vision tasks, neglect the unique temporal dependencies and memory retention characteristics of time series models. To this end, we propose a novel KD framework termed Memory-Discrepancy Knowledge Distillation (MemKD). MemKD leverages a specialized loss function to capture memory retention discrepancies between the teacher and student models across subsequences within time series data, ensuring that the student model effectively mimics the teacher model's behaviour. This approach facilitates the development of compact, high-performing recurrent neural networks suitable for real-time, time series analysis tasks. Our extensive experiments demonstrate that MemKD significantly outperforms state-of-the-art KD methods. It reduces parameter size and memory usage by approximately 500 times while maintaining comparable performance to the teacher model.

</details>


### [85] [Making Tunable Parameters State-Dependent in Weather and Climate Models with Reinforcement Learning](https://arxiv.org/abs/2601.04268)
*Pritthijit Nath,Sebastian Schemm,Henry Moss,Peter Haynes,Emily Shuckburgh,Mark J. Webb*

Main category: cs.LG

TL;DR: RL framework learns weather/climate parametrisations online, outperforming static tuning across testbeds, with TQC/DDPG/TD3 showing best performance.


<details>
  <summary>Details</summary>
Motivation: Traditional parametrisation schemes use fixed coefficients that are weakly constrained and tuned offline, leading to persistent biases and limited adaptability to underlying physics.

Method: Uses reinforcement learning (RL) to learn parametrisation components online as function of evolving model state across three idealised testbeds: simple climate bias correction (SCBC), radiative-convective equilibrium (RCE), and zonal mean energy balance model (EBM) with single-agent and federated multi-agent settings.

Result: TQC, DDPG, and TD3 achieved highest skill and most stable convergence across nine RL algorithms. Single-agent RL outperformed static tuning for EBM, especially in tropical/mid-latitude bands. Federated RL enabled geographically specialized control and faster convergence, with six-agent DDPG yielding lowest area-weighted RMSE.

Conclusion: RL delivers skillful state-dependent, regime-aware parametrisations, offering scalable pathway for online learning within numerical weather and climate models.

Abstract: Weather and climate models rely on parametrisations to represent unresolved sub-grid processes. Traditional schemes rely on fixed coefficients that are weakly constrained and tuned offline, contributing to persistent biases that limit their ability to adapt to the underlying physics. This study presents a framework that learns components of parametrisation schemes online as a function of the evolving model state using reinforcement learning (RL) and evaluates the resulting RL-driven parameter updates across a hierarchy of idealised testbeds spanning a simple climate bias correction (SCBC), a radiative-convective equilibrium (RCE), and a zonal mean energy balance model (EBM) with both single-agent and federated multi-agent settings. Across nine RL algorithms, Truncated Quantile Critics (TQC), Deep Deterministic Policy Gradient (DDPG), and Twin Delayed DDPG (TD3) achieved the highest skill and the most stable convergence across configurations, with performance assessed against a static baseline using area-weighted RMSE, temperature profile and pressure-level diagnostics. For the EBM, single-agent RL outperformed static parameter tuning with the strongest gains in tropical and mid-latitude bands, while federated RL on multi-agent setups enabled geographically specialised control and faster convergence, with a six-agent DDPG configuration using frequent aggregation yielding the lowest area-weighted RMSE across the tropics and mid-latitudes. The learnt corrections were also physically meaningful as agents modulated EBM radiative parameters to reduce meridional biases, adjusted RCE lapse rates to match vertical temperature errors, and stabilised SCBC heating increments to limit drift. Overall, results highlight RL to deliver skilful state-dependent, and regime-aware parametrisations, offering a scalable pathway for online learning within numerical models.

</details>


### [86] [Predictable Gradient Manifolds in Deep Learning: Temporal Path-Length and Intrinsic Rank as a Complexity Regime](https://arxiv.org/abs/2601.04270)
*Anherutowa Calvo*

Main category: cs.LG

TL;DR: Gradients in deep learning optimization exhibit predictable, low-dimensional temporal structure that can be measured and leveraged for improved optimization guarantees.


<details>
  <summary>Details</summary>
Motivation: Deep learning optimization shows structured gradient behavior not captured by worst-case bounds - gradients are temporally predictable and evolve in low-dimensional subspaces, suggesting opportunities for better optimization theory and algorithms.

Method: Introduce two measurable quantities: prediction-based path length (measures gradient forecastability from past info) and predictable rank (quantifies intrinsic temporal dimension of gradient increments). Show how optimization guarantees can be restated using these quantities instead of worst-case variation.

Result: Across CNNs, vision transformers, language models, and synthetic tasks, gradient trajectories are locally predictable with strong low-rank temporal structure. These properties are stable across architectures/optimizers and can be diagnosed using lightweight random projections.

Conclusion: Provides a unifying framework for understanding deep learning optimization dynamics, reframing standard training as operating in a low-complexity temporal regime. Suggests new directions for adaptive optimizers, rank-aware tracking, and prediction-based algorithms based on measurable properties of real training runs.

Abstract: Deep learning optimization exhibits structure that is not captured by worst-case gradient bounds. Empirically, gradients along training trajectories are often temporally predictable and evolve within a low-dimensional subspace. In this work we formalize this observation through a measurable framework for predictable gradient manifolds.
  We introduce two computable quantities: a prediction-based path length that measures how well gradients can be forecast from past information, and a predictable rank that quantifies the intrinsic temporal dimension of gradient increments. We show how classical online and nonconvex optimization guarantees can be restated so that convergence and regret depend explicitly on these quantities, rather than on worst-case variation.
  Across convolutional networks, vision transformers, language models, and synthetic control tasks, we find that gradient trajectories are locally predictable and exhibit strong low-rank structure over time. These properties are stable across architectures and optimizers, and can be diagnosed directly from logged gradients using lightweight random projections.
  Our results provide a unifying lens for understanding optimization dynamics in modern deep learning, reframing standard training as operating in a low-complexity temporal regime. This perspective suggests new directions for adaptive optimizers, rank-aware tracking, and prediction-based algorithm design grounded in measurable properties of real training runs.

</details>


### [87] [Unlocking the Pre-Trained Model as a Dual-Alignment Calibrator for Post-Trained LLMs](https://arxiv.org/abs/2601.04277)
*Beier Luo,Cheng Wang,Hongxin Wei,Sharon Li,Xuefeng Du*

Main category: cs.LG

TL;DR: Dual-Align: An unsupervised post-hoc framework that addresses two types of calibration errors in post-trained LLMs through dual alignment of both final confidence and intermediate inference processes.


<details>
  <summary>Details</summary>
Motivation: Post-training improves LLMs but worsens confidence calibration, causing systematic overconfidence. Existing methods treat calibration as static output-distribution matching, ignoring the dynamic inference-time processes introduced by post-training.

Method: Dual-Align performs two types of alignment: (1) confidence alignment to correct confidence drift via final-distribution matching, and (2) process alignment to address process drift by identifying where inference trajectories diverge and realigning subsequent inference stability. Uses a single temperature parameter for both corrections.

Result: Experiments show consistent improvements over baselines, reducing calibration errors and approaching supervised oracle performance without sacrificing post-training gains.

Conclusion: Addressing both confidence drift and process drift through dual alignment provides an effective unsupervised solution for calibrating post-trained LLMs while preserving their performance improvements.

Abstract: Post-training improves large language models (LLMs) but often worsens confidence calibration, leading to systematic overconfidence. Recent unsupervised post-hoc methods for post-trained LMs (PoLMs) mitigate this by aligning PoLM confidence to that of well-calibrated pre-trained counterparts. However, framing calibration as static output-distribution matching overlooks the inference-time dynamics introduced by post-training. In particular, we show that calibration errors arise from two regimes: (i) confidence drift, where final confidence inflates despite largely consistent intermediate decision processes, and (ii) process drift, where intermediate inference pathways diverge. Guided by this diagnosis, we propose Dual-Align, an unsupervised post-hoc framework for dual alignment in confidence calibration. Dual-Align performs confidence alignment to correct confidence drift via final-distribution matching, and introduces process alignment to address process drift by locating the layer where trajectories diverge and realigning the stability of subsequent inference. This dual strategy learns a single temperature parameter that corrects both drift types without sacrificing post-training performance gains. Experiments show consistent improvements over baselines, reducing calibration errors and approaching a supervised oracle.

</details>


### [88] [Generation of synthetic delay time series for air transport applications](https://arxiv.org/abs/2601.04279)
*Pau Esteve,Massimiliano Zanin*

Main category: cs.LG

TL;DR: Paper compares three models for generating realistic airport delay time series, finding a simplified Genetic Algorithm approach performs best at creating indistinguishable synthetic data with high variability.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity and privacy issues in air transport by generating synthetic airport delay time series that can be used for research while protecting sensitive operational data.

Method: Compare three models: two state-of-the-art Deep Learning algorithms and one simplified Genetic Algorithm approach, using large collections of airport operations data from Europe and the US.

Result: The Genetic Algorithm approach generates time series almost indistinguishable from real ones while maintaining high variability, and successfully validates in delay propagation detection problems.

Conclusion: Simplified Genetic Algorithm is effective for realistic airport delay time series generation, with synthetic data made publicly available for scientific community use.

Abstract: The generation of synthetic data is receiving increasing attention from the scientific community, thanks to its ability to solve problems like data scarcity and privacy, and is starting to find applications in air transport. We here tackle the problem of generating synthetic, yet realistic, time series of delays at airports, starting from large collections of operations in Europe and the US. We specifically compare three models, two of them based on state of the art Deep Learning algorithms, and one simplified Genetic Algorithm approach. We show how the latter can generate time series that are almost indistinguishable from real ones, while maintaining a high variability. We further validate the resulting time series in a problem of detecting delay propagations between airports. We finally make the synthetic data available to the scientific community.

</details>


### [89] [Hybrid Federated Learning for Noise-Robust Training](https://arxiv.org/abs/2601.04483)
*Yongjun Kim,Hyeongjun Park,Hwanjin Kim,Junil Choi*

Main category: cs.LG

TL;DR: Hybrid Federated Learning (HFL) combines FL and FD to balance noise robustness and learning speed, using adaptive UE clustering and weight selection to achieve better accuracy at low SNR.


<details>
  <summary>Details</summary>
Motivation: Federated learning (FL) and federated distillation (FD) each have trade-offs: FL is more robust to noise but slower, while FD is faster but less robust. The paper aims to combine their strengths and mitigate weaknesses.

Method: Proposes HFL framework where UEs transmit either gradients (FL) or logits (FD), with BS selecting per-round weights. Introduces two methods: (1) adaptive UE clustering via Jenks optimization, and (2) adaptive weight selection via damped Newton method.

Result: Numerical results show HFL achieves superior test accuracy at low SNR when both adaptive UE clustering and weight selection are exploited.

Conclusion: HFL effectively combines FL and FD advantages, with adaptive methods providing performance gains, especially in noisy low SNR environments.

Abstract: Federated learning (FL) and federated distillation (FD) are distributed learning paradigms that train UE models with enhanced privacy, each offering different trade-offs between noise robustness and learning speed. To mitigate their respective weaknesses, we propose a hybrid federated learning (HFL) framework in which each user equipment (UE) transmits either gradients or logits, and the base station (BS) selects the per-round weights of FL and FD updates. We derive convergence of HFL framework and introduce two methods to exploit degrees of freedom (DoF) in HFL, which are (i) adaptive UE clustering via Jenks optimization and (ii) adaptive weight selection via a damped Newton method. Numerical results show that HFL achieves superior test accuracy at low SNR when both DoF are exploited.

</details>


### [90] [LEGATO: Good Identity Unlearning Is Continuous](https://arxiv.org/abs/2601.04282)
*Qiang Chen,Chun-Wun Cheng,Xiu Su,Hongyan Xu,Xi Lin,Shan You,Angelica I. Aviles-Rivero,Yi Chen*

Main category: cs.LG

TL;DR: LEGATO introduces Neural ODE adapters for smooth, controllable identity forgetting in generative models, avoiding catastrophic collapse while reducing fine-tuning parameters.


<details>
  <summary>Details</summary>
Motivation: Existing machine unlearning methods for generative models are inefficient (require full fine-tuning), lack controllability over forgetting intensity, and suffer from catastrophic collapse where retention capability degrades during forgetting.

Method: LEGATO augments pre-trained generators with lightweight Neural ODE adapters that model forgetting as a continuous trajectory, keeping original weights frozen. Uses trajectory consistency constraints to prevent catastrophic collapse, with forgetting intensity controlled via ODE step size.

Result: Achieves state-of-the-art forgetting performance across in-domain and out-of-domain identity unlearning benchmarks, avoids catastrophic collapse, and reduces fine-tuned parameters compared to existing methods.

Conclusion: Modeling identity forgetting as a continuous trajectory via Neural ODE adapters provides efficient, controllable, and stable unlearning for generative models, addressing key limitations of existing approaches.

Abstract: Machine unlearning has become a crucial role in enabling generative models trained on large datasets to remove sensitive, private, or copyright-protected data. However, existing machine unlearning methods face three challenges in learning to forget identity of generative models: 1) inefficient, where identity erasure requires fine-tuning all the model's parameters; 2) limited controllability, where forgetting intensity cannot be controlled and explainability is lacking; 3) catastrophic collapse, where the model's retention capability undergoes drastic degradation as forgetting progresses. Forgetting has typically been handled through discrete and unstable updates, often requiring full-model fine-tuning and leading to catastrophic collapse. In this work, we argue that identity forgetting should be modeled as a continuous trajectory, and introduce LEGATO - Learn to ForgEt Identity in GenerAtive Models via Trajectory-consistent Neural Ordinary Differential Equations. LEGATO augments pre-trained generators with fine-tunable lightweight Neural ODE adapters, enabling smooth, controllable forgetting while keeping the original model weights frozen. This formulation allows forgetting intensity to be precisely modulated via ODE step size, offering interpretability and robustness. To further ensure stability, we introduce trajectory consistency constraints that explicitly prevent catastrophic collapse during unlearning. Extensive experiments across in-domain and out-of-domain identity unlearning benchmarks show that LEGATO achieves state-of-the-art forgetting performance, avoids catastrophic collapse and reduces fine-tuned parameters.

</details>


### [91] [Mitigating Position-Shift Failures in Text-Based Modular Arithmetic via Position Curriculum and Template Diversity](https://arxiv.org/abs/2601.04283)
*Nikolay Yudin*

Main category: cs.LG

TL;DR: Transformers trained for modular addition fail under input format variations despite high in-distribution accuracy; a training recipe with boundary markers, position curriculum, diverse templates, and consistency training improves robustness.


<details>
  <summary>Details</summary>
Motivation: To study robustness under input-format variation rather than only in-distribution accuracy, identifying failure modes when expressions are shifted to different positions or presented under out-of-distribution templates.

Method: Train character-level Transformers on modular addition with a disjoint-pair split, then introduce a training recipe combining: (1) explicit expression boundary markers, (2) position curriculum broadening absolute position range, (3) diverse template mixtures, and (4) consistency training across multiple variants per example.

Result: Baseline models collapse under position shift and template OOD despite high in-distribution accuracy. The proposed intervention substantially improves robustness to both position shift and template OOD while maintaining high in-distribution accuracy across three seeds.

Conclusion: Procedural generalization under noisy supervision benefits from explicitly training invariances absent from the data distribution, and the paper provides a reproducible evaluation protocol and artifacts for studying such robustness.

Abstract: Building on insights from the grokking literature, we study character-level Transformers trained to compute modular addition from text, and focus on robustness under input-format variation rather than only in-distribution accuracy. We identify a previously under-emphasized failure mode: models that achieve high in-distribution accuracy can fail catastrophically when the same expression is shifted to different absolute character positions ("position shift") or presented under out-of-distribution natural-language templates. Using a disjoint-pair split over all ordered pairs for p=97, we show that a baseline model reaches strong in-distribution performance yet collapses under position shift and template OOD. We then introduce a simple training recipe that combines (i) explicit expression boundary markers, (ii) position curriculum that broadens the range of absolute positions seen during training, (iii) diverse template mixtures, and (iv) consistency training across multiple variants per example. Across three seeds, this intervention substantially improves robustness to position shift and template OOD while maintaining high in-distribution accuracy, whereas an ALiBi-style ablation fails to learn the task under our setup. Our results suggest that steering procedural generalization under noisy supervision benefits from explicitly training invariances that are otherwise absent from the data distribution, and we provide a reproducible evaluation protocol and artifacts.

</details>


### [92] [Enhancing Robustness of Asynchronous EEG-Based Movement Prediction using Classifier Ensembles](https://arxiv.org/abs/2601.04286)
*Niklas Kueper,Kartik Chari,Elsa Andrea Kirchner*

Main category: cs.LG

TL;DR: Classifier ensembles with sliding-window postprocessing improve asynchronous EEG-based movement intention detection for robot-assisted stroke rehabilitation.


<details>
  <summary>Details</summary>
Motivation: Stroke rehabilitation needs better methods for detecting patient movement intention to trigger robotic assistance. EEG-based detection is challenging for online asynchronous classification.

Method: Analyzed EEG datasets from 14 healthy subjects performing self-initiated arm movements. Compared ensemble combinations of SVM, MLP, and EEGNet models with offline and pseudo-online evaluations using sliding-window postprocessing.

Result: Two model ensembles significantly outperformed best single model in pseudo-online evaluation. Sliding-window postprocessing improved single model performance. No significant improvement found in offline evaluation between ensembles and best single model.

Conclusion: Classifier ensembles with appropriate postprocessing effectively enhance asynchronous EEG-based movement intention detection, especially for online classification, reducing false detections.

Abstract: Objective: Stroke is one of the leading causes of disabilities. One promising approach is to extend the rehabilitation with self-initiated robot-assisted movement therapy. To enable this, it is required to detect the patient's intention to move to trigger the assistance of a robotic device. This intention to move can be detected from human surface electroencephalography (EEG) signals; however, it is particularly challenging to decode when classifications are performed online and asynchronously. In this work, the effectiveness of classifier ensembles and a sliding-window postprocessing technique was investigated to enhance the robustness of such asynchronous classification. Approach: To investigate the effectiveness of classifier ensembles and a sliding-window postprocessing, two EEG datasets with 14 healthy subjects who performed self-initiated arm movements were analyzed. Offline and pseudo-online evaluations were conducted to compare ensemble combinations of the support vector machine (SVM), multilayer perceptron (MLP), and EEGNet classification models. Results: The results of the pseudo-online evaluation show that the two model ensembles significantly outperformed the best single model for the optimal number of postprocessing windows. In particular, for single models, an increased number of postprocessing windows significantly improved classification performances. Interestingly, we found no significant improvements between performances of the best single model and classifier ensembles in the offline evaluation. Significance: We demonstrated that classifier ensembles and appropriate postprocessing methods effectively enhance the asynchronous detection of movement intentions from EEG signals. In particular, the classifier ensemble approach yields greater improvements in online classification than in offline classification, and reduces false detections, i.e., early false positives.

</details>


### [93] [Online Action-Stacking Improves Reinforcement Learning Performance for Air Traffic Control](https://arxiv.org/abs/2601.04287)
*Ben Carvell,George De Ath,Eseoghene Benjamin,Richard Everson*

Main category: cs.LG

TL;DR: Online action-stacking is an inference-time wrapper that compiles primitive RL actions into realistic ATC commands, enabling training with small action spaces while producing operational clearances.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between standard RL formulations and operational ATC requirements by allowing training on small discrete action spaces while producing realistic compound clearances needed for air traffic control.

Method: Train policies with simple incremental heading/level adjustments plus action-damping penalty to reduce instruction frequency. At inference, online action-stacking compiles bursts of primitive actions into domain-appropriate compound clearances using Proximal Policy Optimization on BluebirdDT platform.

Result: Action stacking greatly reduces issued instructions compared to damped baseline and achieves comparable performance to a policy trained with 37-dimensional action space, despite using only 5 actions. Successfully demonstrated for lateral navigation, climb/descent management, and collision avoidance.

Conclusion: Online action-stacking helps bridge RL-ATC gap and provides a simple mechanism for scaling to more complex control scenarios by enabling training with small action spaces while producing realistic operational commands.

Abstract: We introduce online action-stacking, an inference-time wrapper for reinforcement learning policies that produces realistic air traffic control commands while allowing training on a much smaller discrete action space. Policies are trained with simple incremental heading or level adjustments, together with an action-damping penalty that reduces instruction frequency and leads agents to issue commands in short bursts. At inference, online action-stacking compiles these bursts of primitive actions into domain-appropriate compound clearances. Using Proximal Policy Optimisation and the BluebirdDT digital twin platform, we train agents to navigate aircraft along lateral routes, manage climb and descent to target flight levels, and perform two-aircraft collision avoidance under a minimum separation constraint. In our lateral navigation experiments, action stacking greatly reduces the number of issued instructions relative to a damped baseline and achieves comparable performance to a policy trained with a 37-dimensional action space, despite operating with only five actions. These results indicate that online action-stacking helps bridge a key gap between standard reinforcement learning formulations and operational ATC requirements, and provides a simple mechanism for scaling to more complex control scenarios.

</details>


### [94] [ArtCognition: A Multimodal AI Framework for Affective State Sensing from Visual and Kinematic Drawing Cues](https://arxiv.org/abs/2601.04297)
*Behrad Binaei-Haghighi,Nafiseh Sadat Sajadi,Mehrad Liviyan,Reyhane Akhavan Kharazi,Fatemeh Amirkhani,Behnam Bahrak*

Main category: cs.LG

TL;DR: ArtCognition: A multimodal framework using digital drawing (House-Tree-Person test) for automated psychological assessment by fusing visual features from final artwork with behavioral kinematic cues from the drawing process, enhanced by RAG for explainable analysis.


<details>
  <summary>Details</summary>
Motivation: Objective assessment of human affective and psychological states is challenging, especially through non-verbal channels. Digital drawing represents a rich but underexplored modality for affective sensing that could provide non-intrusive assessment tools.

Method: ArtCognition framework analyzes the House-Tree-Person test using multimodal fusion: (1) static visual features from final artwork via computer vision models, and (2) dynamic behavioral kinematic cues from drawing process (stroke speed, pauses, smoothness). Uses Retrieval-Augmented Generation (RAG) architecture to bridge low-level features with psychological interpretation.

Result: Fusion of visual and behavioral kinematic cues provides more nuanced assessment than single modalities alone. Significant correlations found between extracted multimodal features and standardized psychological metrics, validating framework's potential as scalable clinical support tool.

Conclusion: Contributes new methodology for non-intrusive affective state assessment using digital drawing, opens avenues for technology-assisted mental healthcare with explainable, knowledge-grounded analysis that reduces model hallucination risks.

Abstract: The objective assessment of human affective and psychological states presents a significant challenge, particularly through non-verbal channels. This paper introduces digital drawing as a rich and underexplored modality for affective sensing. We present a novel multimodal framework, named ArtCognition, for the automated analysis of the House-Tree-Person (HTP) test, a widely used psychological instrument. ArtCognition uniquely fuses two distinct data streams: static visual features from the final artwork, captured by computer vision models, and dynamic behavioral kinematic cues derived from the drawing process itself, such as stroke speed, pauses, and smoothness. To bridge the gap between low-level features and high-level psychological interpretation, we employ a Retrieval-Augmented Generation (RAG) architecture. This grounds the analysis in established psychological knowledge, enhancing explainability and reducing the potential for model hallucination. Our results demonstrate that the fusion of visual and behavioral kinematic cues provides a more nuanced assessment than either modality alone. We show significant correlations between the extracted multimodal features and standardized psychological metrics, validating the framework's potential as a scalable tool to support clinicians. This work contributes a new methodology for non-intrusive affective state assessment and opens new avenues for technology-assisted mental healthcare.

</details>


### [95] [Transformer-Based Multi-Modal Temporal Embeddings for Explainable Metabolic Phenotyping in Type 1 Diabetes](https://arxiv.org/abs/2601.04299)
*Pir Bakhsh Khokhar,Carmine Gravino,Fabio Palomba,Sule Yildrim Yayilgan,Sarang Shaikh*

Main category: cs.LG

TL;DR: Explainable deep learning framework integrates CGM and lab data to identify 5 distinct metabolic phenotypes in Type 1 diabetes, revealing subgroups beyond HbA1c alone.


<details>
  <summary>Details</summary>
Motivation: Type 1 diabetes is metabolically heterogeneous and conventional biomarkers like HbA1c are insufficient to characterize this complexity. There's a need for better methods to understand individual metabolic status and risk stratification.

Method: Proposes an explainable deep learning framework that integrates continuous glucose monitoring (CGM) data with laboratory profiles. Uses transformer encoder to model temporal dependencies across modalities and Gaussian mixture modeling to identify latent metabolic phenotypes. Achieves interpretability through transformer attention visualization and SHAP-based feature attribution.

Result: Identified 5 latent metabolic phenotypes among 577 individuals with T1D, ranging from metabolic stability to elevated cardiometabolic risk. Phenotypes show distinct biochemical profiles in glycemic control, lipid metabolism, renal markers, and TSH levels. Attention analysis shows glucose variability as dominant temporal factor; SHAP identifies HbA1c, triglycerides, cholesterol, creatinine, and TSH as key phenotype contributors. Phenotype membership shows modest but significant associations with hypertension, myocardial infarction, and heart failure.

Conclusion: The explainable multimodal temporal embedding framework reveals physiologically coherent metabolic subgroups in T1D and supports risk stratification beyond single biomarkers, offering a more comprehensive approach to understanding metabolic heterogeneity in diabetes.

Abstract: Type 1 diabetes (T1D) is a highly metabolically heterogeneous disease that cannot be adequately characterized by conventional biomarkers such as glycated hemoglobin (HbA1c). This study proposes an explainable deep learning framework that integrates continuous glucose monitoring (CGM) data with laboratory profiles to learn multimodal temporal embeddings of individual metabolic status. Temporal dependencies across modalities are modeled using a transformer encoder, while latent metabolic phenotypes are identified via Gaussian mixture modeling. Model interpretability is achieved through transformer attention visualization and SHAP-based feature attribution. Five latent metabolic phenotypes, ranging from metabolic stability to elevated cardiometabolic risk, were identified among 577 individuals with T1D. These phenotypes exhibit distinct biochemical profiles, including differences in glycemic control, lipid metabolism, renal markers, and thyrotropin (TSH) levels. Attention analysis highlights glucose variability as a dominant temporal factor, while SHAP analysis identifies HbA1c, triglycerides, cholesterol, creatinine, and TSH as key contributors to phenotype differentiation. Phenotype membership shows statistically significant, albeit modest, associations with hypertension, myocardial infarction, and heart failure. Overall, this explainable multimodal temporal embedding framework reveals physiologically coherent metabolic subgroups in T1D and supports risk stratification beyond single biomarkers.

</details>


### [96] [Quantifying the Effect of Test Set Contamination on Generative Evaluations](https://arxiv.org/abs/2601.04301)
*Rylan Schaeffer,Joshua Kazdan,Baber Abbasi,Ken Ziyu Liu,Brando Miranda,Ahmed Ahmed,Abhay Puri,Niloofar Mireshghallah,Sanmi Koyejo*

Main category: cs.LG

TL;DR: Test set contamination significantly boosts generative model performance, enabling models to achieve lower loss than irreducible error. Contamination effects can be mitigated by overturning with fresh data, high sampling temperatures, and are harder for longer solutions.


<details>
  <summary>Details</summary>
Motivation: While test set contamination's impact on discriminative evaluations is well-studied, its effects on generative evaluations remain under-explored. As frontier AI systems train on web-scale data, understanding contamination's influence on generative tasks is crucial for trustworthy AI evaluation.

Method: Pretrained language models on mixtures of web data and MATH benchmark, sweeping model sizes and test set replicas. Used scaling laws to analyze contamination effects. Studied further training phases: overturning with fresh data and supervised finetuning. Examined inference factors like sampling temperature and solution length.

Result: Performance improves with contamination and model size. Even a single test set replica enables models to achieve lower loss than irreducible error of uncontaminated training. Overtraining with fresh data reduces contamination effects, while supervised finetuning's impact depends on pretraining contamination level. High sampling temperatures mitigate contamination, and longer solutions are exponentially harder to memorize.

Conclusion: Test set contamination introduces new complexity for trustworthy AI evaluation. Generative evaluations differ from discriminative ones in how memorization interacts with generation, requiring careful consideration of contamination effects throughout the model lifecycle.

Abstract: As frontier AI systems are pretrained on web-scale data, test set contamination has become a critical concern for accurately assessing their capabilities. While research has thoroughly investigated the impact of test set contamination on discriminative evaluations like multiple-choice question-answering, comparatively little research has studied the impact of test set contamination on generative evaluations. In this work, we quantitatively assess the effect of test set contamination on generative evaluations through the language model lifecycle. We pretrain language models on mixtures of web data and the MATH benchmark, sweeping model sizes and number of test set replicas contaminating the pretraining corpus; performance improves with contamination and model size. Using scaling laws, we make a surprising discovery: including even a single test set replica enables models to achieve lower loss than the irreducible error of training on the uncontaminated corpus. We then study further training: overtraining with fresh data reduces the effects of contamination, whereas supervised finetuning on the training set can either increase or decrease performance on test data, depending on the amount of pretraining contamination. Finally, at inference, we identify factors that modulate memorization: high sampling temperatures mitigate contamination effects, and longer solutions are exponentially more difficult to memorize than shorter ones, presenting a contrast with discriminative evaluations, where solutions are only a few tokens in length. By characterizing how generation and memorization interact, we highlight a new layer of complexity for trustworthy evaluation of AI systems.

</details>


### [97] [Causally-Aware Information Bottleneck for Domain Adaptation](https://arxiv.org/abs/2601.04361)
*Mohammad Ali Javidian*

Main category: cs.LG

TL;DR: A causal domain adaptation method for imputing missing target variables in target domains using mechanism-stable representations via Gaussian/Variational Information Bottleneck approaches.


<details>
  <summary>Details</summary>
Motivation: Addresses the common domain adaptation problem where the target variable is observed in source domain but entirely missing in target domain, requiring imputation under various distribution shifts while maintaining causal relationships.

Method: Two approaches: 1) For linear Gaussian causal models, derive closed-form Gaussian Information Bottleneck solution that reduces to CCA-style projection with DAG-aware options. 2) For nonlinear/non-Gaussian data, introduce Variational Information Bottleneck encoder-predictor that scales to high dimensions and enables zero-shot deployment.

Result: Consistently attains accurate imputations across synthetic and real datasets, supporting practical use in high-dimensional causal models and providing a unified, lightweight toolkit for causal domain adaptation.

Conclusion: The proposed framework successfully addresses causal domain adaptation by learning compact, mechanism-stable representations that preserve target-relevant information while discarding spurious variation, enabling effective zero-shot imputation in target domains.

Abstract: We tackle a common domain adaptation setting in causal systems. In this setting, the target variable is observed in the source domain but is entirely missing in the target domain. We aim to impute the target variable in the target domain from the remaining observed variables under various shifts. We frame this as learning a compact, mechanism-stable representation. This representation preserves information relevant for predicting the target while discarding spurious variation. For linear Gaussian causal models, we derive a closed-form Gaussian Information Bottleneck (GIB) solution. This solution reduces to a canonical correlation analysis (CCA)-style projection and offers Directed Acyclic Graph (DAG)-aware options when desired. For nonlinear or non-Gaussian data, we introduce a Variational Information Bottleneck (VIB) encoder-predictor. This approach scales to high dimensions and can be trained on source data and deployed zero-shot to the target domain. Across synthetic and real datasets, our approach consistently attains accurate imputations, supporting practical use in high-dimensional causal models and furnishing a unified, lightweight toolkit for causal domain adaptation.

</details>


### [98] [Phasor Agents: Oscillatory Graphs with Three-Factor Plasticity and Sleep-Staged Learning](https://arxiv.org/abs/2601.04362)
*Rodja Trappe*

Main category: cs.LG

TL;DR: Phasor Agents are dynamical systems using coupled Stuart-Landau oscillators as computational units, with phase for timing coherence and amplitude for activity. They learn via three-factor local plasticity without backpropagation, using wake/sleep separation for stability and REM-like replay for planning.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of stability in oscillatory substrates where online weight updates can drive networks into unwanted regimes like global synchrony, collapsing representational diversity. It aims to develop stable learning in oscillatory systems without backpropagation.

Method: Uses Phasor Graphs of coupled Stuart-Landau oscillators as computational units. Implements three-factor local plasticity with eligibility traces gated by sparse global modulators and oscillation-timed write windows. Separates wake tagging from offline consolidation inspired by synaptic tagging-and-capture and sleep-stage dynamics: deep-sleep-like gated capture for safe weight changes and REM-like replay for experience reconstruction and planning.

Result: Eligibility traces preserve credit under delayed modulation; compression-progress signals pass timestamp-shuffle controls; phase-coherent retrieval reaches 4x diffusive baselines under noise; wake/sleep separation expands stable learning by 67% under matched weight-norm budgets; REM replay improves maze success rate by +45.5 percentage points; and Tolman-style latent-learning signature emerges from replay.

Conclusion: Phasor Agents demonstrate stable learning in oscillatory systems without backpropagation through biologically-inspired wake/sleep separation and replay mechanisms, achieving significant performance improvements and emergent latent learning capabilities consistent with internal model formation.

Abstract: Phasor Agents are dynamical systems whose internal state is a Phasor Graph: a weighted graph of coupled Stuart-Landau oscillators. A Stuart-Landau oscillator is a minimal stable "rhythm generator" (the normal form near a Hopf bifurcation); each oscillator is treated as an abstract computational unit (inspired by, but not claiming to model, biological oscillatory populations). In this interpretation, oscillator phase tracks relative timing (coherence), while amplitude tracks local gain or activity. Relative phase structure serves as a representational medium; coupling weights are learned via three-factor local plasticity - eligibility traces gated by sparse global modulators and oscillation-timed write windows - without backpropagation.
  A central challenge in oscillatory substrates is stability: online weight updates can drive the network into unwanted regimes (e.g., global synchrony), collapsing representational diversity. We therefore separate wake tagging from offline consolidation, inspired by synaptic tagging-and-capture and sleep-stage dynamics: deep-sleep-like gated capture commits tagged changes safely, while REM-like replay reconstructs and perturbs experience for planning.
  A staged experiment suite validates each mechanism with ablations and falsifiers: eligibility traces preserve credit under delayed modulation; compression-progress signals pass timestamp-shuffle controls; phase-coherent retrieval reaches 4x diffusive baselines under noise; wake/sleep separation expands stable learning by 67 percent under matched weight-norm budgets; REM replay improves maze success rate by +45.5 percentage points; and a Tolman-style latent-learning signature - immediate competence and detour advantage after unrewarded exploration, consistent with an internal model - emerges from replay (Tolman, 1948).
  The codebase and all artifacts are open-source.

</details>


### [99] [Survival Dynamics of Neural and Programmatic Policies in Evolutionary Reinforcement Learning](https://arxiv.org/abs/2601.04365)
*Anton Roupassov-Ruiz,Yiyang Zuo*

Main category: cs.LG

TL;DR: Programmatic policies (PERL) using soft differentiable decision lists outperform neural policies (NERL) in evolutionary reinforcement learning, with PERL surviving 201.69 steps longer on average in ALife testbed.


<details>
  <summary>Details</summary>
Motivation: Neural representations in evolutionary RL lack explicit modular structure, limiting behavioral interpretation. The paper investigates whether programmatic policies can match or exceed neural policy performance while providing more interpretable structure.

Method: Created first fully specified open-source reimplementation of 1992 ALife ERL testbed. Compared programmatic policies (PERL) using soft differentiable decision lists (SDDL) against neural policies (NERL). Conducted rigorous survival analysis across 4000 trials using Kaplan-Meier curves and Restricted Mean Survival Time metrics.

Result: Found statistically significant survival difference: PERL agents survive 201.69 steps longer than NERL agents. SDDL agents using learning alone survive 73.67 steps longer than neural agents using both learning and evolution. Programmatic policies exceed neural policy survival performance.

Conclusion: Programmatic policies implemented as soft differentiable decision lists can outperform neural policies in evolutionary reinforcement learning tasks, demonstrating superior survival performance in ALife environments while potentially offering better interpretability.

Abstract: In evolutionary reinforcement learning tasks (ERL), agent policies are often encoded as small artificial neural networks (NERL). Such representations lack explicit modular structure, limiting behavioral interpretation. We investigate whether programmatic policies (PERL), implemented as soft, differentiable decision lists (SDDL), can match the performance of NERL. To support reproducible evaluation, we provide the first fully specified and open-source reimplementation of the classic 1992 Artificial Life (ALife) ERL testbed. We conduct a rigorous survival analysis across 4000 independent trials utilizing Kaplan-Meier curves and Restricted Mean Survival Time (RMST) metrics absent in the original study. We find a statistically significant difference in survival probability between PERL and NERL. PERL agents survive on average 201.69 steps longer than NERL agents. Moreover, SDDL agents using learning alone (no evolution) survive on average 73.67 steps longer than neural agents using both learning and evaluation. These results demonstrate that programmatic policies can exceed the survival performance of neural policies in ALife.

</details>


### [100] [Machine Learning Model for Sparse PCM Completion](https://arxiv.org/abs/2601.04366)
*Selcuk Koyuncu,Ronak Nouri,Stephen Providence*

Main category: cs.LG

TL;DR: A machine learning model for sparse pairwise comparison matrices that combines classical PCM approaches with graph-based learning techniques.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional pairwise comparison matrix methods when dealing with sparse data, and to leverage modern machine learning techniques for improved performance and scalability.

Method: Combines classical pairwise comparison matrix approaches with graph-based learning techniques to create a hybrid machine learning model for sparse PCMs.

Result: Numerical results demonstrate the effectiveness and scalability of the proposed method, though specific performance metrics are not detailed in the abstract.

Conclusion: The proposed hybrid approach successfully integrates classical PCM methods with graph-based machine learning, offering an effective and scalable solution for sparse pairwise comparison matrices.

Abstract: In this paper, we propose a machine learning model for sparse pairwise comparison matrices (PCMs), combining classical PCM approaches with graph-based learning techniques. Numerical results are provided to demonstrate the effectiveness and scalability of the proposed method.

</details>


### [101] [Aligned explanations in neural networks](https://arxiv.org/abs/2601.04378)
*Corentin Lobet,Francesca Chiaromonte*

Main category: cs.LG

TL;DR: PiNets are pseudo-linear neural networks that produce instance-wise linear predictions for better explanatory alignment, making model decisions linearly readable and faithful.


<details>
  <summary>Details</summary>
Motivation: Current feature attribution methods for explaining neural networks are often post-hoc rationalizations that don't truly reflect the model's prediction process, creating a "white-painted black box" problem. The authors argue that explanatory alignment is crucial for trustworthiness - explanations must be directly linked to predictions rather than being separate justifications.

Method: Propose PiNets (Pseudo-linear Networks) as a modeling framework that produces instance-wise linear predictions in arbitrary feature spaces. These networks are designed to be linearly readable, meaning their predictions can be directly explained through linear combinations of features, ensuring alignment between explanations and actual model decisions.

Result: Demonstrated PiNets on image classification and segmentation tasks, showing they produce explanations that are faithful across multiple criteria in addition to achieving explanatory alignment. The linear readability enables direct linking between features and predictions.

Conclusion: Model readability should be a key design principle for trustworthy AI systems. PiNets provide a practical framework for achieving explanatory alignment in deep learning by making predictions linearly interpretable, moving beyond post-hoc explanations to integrated, faithful reasoning.

Abstract: Feature attribution is the dominant paradigm for explaining deep neural networks. However, most existing methods only loosely reflect the model's prediction-making process, thereby merely white-painting the black box. We argue that explanatory alignment is a key aspect of trustworthiness in prediction tasks: explanations must be directly linked to predictions, rather than serving as post-hoc rationalizations. We present model readability as a design principle enabling alignment, and PiNets as a modeling framework to pursue it in a deep learning context. PiNets are pseudo-linear networks that produce instance-wise linear predictions in an arbitrary feature space, making them linearly readable. We illustrate their use on image classification and segmentation tasks, demonstrating how PiNets produce explanations that are faithful across multiple criteria in addition to alignment.

</details>


### [102] [Enhanced-FQL($λ$), an Efficient and Interpretable RL with novel Fuzzy Eligibility Traces and Segmented Experience Replay](https://arxiv.org/abs/2601.04392)
*Mohsen Jalaeian-Farimani*

Main category: cs.LG

TL;DR: Enhanced-FQL(λ) is a fuzzy reinforcement learning framework that combines fuzzified eligibility traces and segmented experience replay for continuous control tasks, offering interpretability, computational efficiency, and theoretical convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for interpretable and computationally efficient reinforcement learning methods for continuous control tasks, particularly in safety-critical applications where transparency and resource constraints are essential. Current deep RL approaches often lack interpretability and have high computational complexity.

Method: The framework integrates Fuzzified Eligibility Traces (FET) and Segmented Experience Replay (SER) into fuzzy Q-learning with Fuzzified Bellman Equation (FBE). It uses an interpretable fuzzy rule base instead of neural networks, with two key innovations: 1) fuzzified Bellman equation with eligibility traces for stable multi-step credit assignment, and 2) memory-efficient segment-based experience replay for enhanced sample efficiency.

Result: Theoretical analysis proves convergence under standard assumptions. Extensive evaluations in continuous control domains show Enhanced-FQL(λ) achieves superior sample efficiency and reduced variance compared to n-step fuzzy TD and fuzzy SARSA(λ) baselines, while maintaining substantially lower computational complexity than deep RL alternatives like DDPG.

Conclusion: The framework's inherent interpretability, combined with computational efficiency and theoretical convergence guarantees, makes it particularly suitable for safety-critical applications where transparency and resource constraints are essential.

Abstract: This paper introduces a fuzzy reinforcement learning framework, Enhanced-FQL($λ$), that integrates novel Fuzzified Eligibility Traces (FET) and Segmented Experience Replay (SER) into fuzzy Q-learning with Fuzzified Bellman Equation (FBE) for continuous control tasks. The proposed approach employs an interpretable fuzzy rule base instead of complex neural architectures, while maintaining competitive performance through two key innovations: a fuzzified Bellman equation with eligibility traces for stable multi-step credit assignment, and a memory-efficient segment-based experience replay mechanism for enhanced sample efficiency. Theoretical analysis proves the proposed method convergence under standard assumptions. Extensive evaluations in continuous control domains demonstrate that Enhanced-FQL($λ$) achieves superior sample efficiency and reduced variance compared to n-step fuzzy TD and fuzzy SARSA($λ$) baselines, while maintaining substantially lower computational complexity than deep RL alternatives such as DDPG. The framework's inherent interpretability, combined with its computational efficiency and theoretical convergence guarantees, makes it particularly suitable for safety-critical applications where transparency and resource constraints are essential.

</details>


### [103] [Rate or Fate? RLV$^\varepsilon$R: Reinforcement Learning with Verifiable Noisy Rewards](https://arxiv.org/abs/2601.04411)
*Ali Rad,Khashayar Filom,Darioush Keivan,Peyman Mohajerin Esfahani,Ehsan Kamalinejad*

Main category: cs.LG

TL;DR: RLVR with noisy verification shows sharp phase transition: learning succeeds when verifier's Youden's index J>0 (TPR-FPR>0), fails when J<0, with noise affecting convergence rate but not fate.


<details>
  <summary>Details</summary>
Motivation: Real-world RLVR faces noisy verification (imperfect tests, human labels, LLM judges) especially in hard domains like coding, raising the question of whether noise merely slows learning or fundamentally changes outcomes.

Method: Developed analytically tractable multi-armed bandit view of RLVR dynamics using GRPO, modeling false positives/negatives and grouping completions into reasoning modes, yielding replicator-style flow on probability simplex.

Result: Sharp phase transition: when J>0, incorrect modes driven to extinction (learning); when J=0, neutral; when J<0, incorrect modes dominate (anti-learning). Noise primarily rescales convergence time in learning regime.

Conclusion: Verification noise affects convergence rate but not fate when J>0; framework provides general lens for analyzing RLVR stability, convergence, and algorithmic interventions beyond just noise analysis.

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a simple but powerful paradigm for training LLMs: sample a completion, verify it, and update. In practice, however, the verifier is almost never clean--unit tests probe only limited corner cases; human and synthetic labels are imperfect; and LLM judges (e.g., RLAIF) are noisy and can be exploited--and this problem worsens on harder domains (especially coding) where tests are sparse and increasingly model-generated. We ask a pragmatic question: does the verification noise merely slow down the learning (rate), or can it flip the outcome (fate)?
  To address this, we develop an analytically tractable multi-armed bandit view of RLVR dynamics, instantiated with GRPO and validated in controlled experiments. Modeling false positives and false negatives and grouping completions into recurring reasoning modes yields a replicator-style (natural-selection) flow on the probability simplex. The dynamics decouples into within-correct-mode competition and a one-dimensional evolution for the mass on incorrect modes, whose drift is determined solely by Youden's index J=TPR-FPR. This yields a sharp phase transition: when J>0, the incorrect mass is driven toward extinction (learning); when J=0, the process is neutral; and when J<0, incorrect modes amplify until they dominate (anti-learning and collapse). In the learning regime J>0, noise primarily rescales convergence time ("rate, not fate"). Experiments on verifiable programming tasks under synthetic noise reproduce the predicted J=0 boundary. Beyond noise, the framework offers a general lens for analyzing RLVR stability, convergence, and algorithmic interventions.

</details>


### [104] [Distribution-Guided and Constrained Quantum Machine Unlearning](https://arxiv.org/abs/2601.04413)
*Nausherwan Malik,Zubair Khalid,Muhammad Faryad*

Main category: cs.LG

TL;DR: Proposes a distribution-guided framework for class-level quantum machine unlearning using tunable target distributions and anchor-based constraints to control forgetting-retention trade-offs.


<details>
  <summary>Details</summary>
Motivation: Existing quantum machine unlearning approaches rely on fixed, uniform target distributions and lack explicit control over the trade-off between forgetting specific data and retaining model behavior, limiting their reliability and interpretability.

Method: A distribution-guided framework treating unlearning as constrained optimization, using tunable target distributions derived from model similarity statistics and anchor-based preservation constraints to maintain predictive behavior on retained data.

Result: Evaluation on variational quantum classifiers (Iris and Covertype datasets) shows sharp suppression of forgotten-class confidence, minimal degradation of retained-class performance, and closer alignment with gold retrained model baselines compared to uniform-target unlearning.

Conclusion: Target design and constraint-based formulations are crucial for reliable and interpretable quantum machine unlearning, enabling controlled optimization trajectories that balance forgetting and retention.

Abstract: Machine unlearning aims to remove the influence of specific training data from a learned model without full retraining. While recent work has begun to explore unlearning in quantum machine learning, existing approaches largely rely on fixed, uniform target distributions and do not explicitly control the trade-off between forgetting and retained model behaviour. In this work, we propose a distribution-guided framework for class-level quantum machine unlearning that treats unlearning as a constrained optimization problem. Our method introduces a tunable target distribution derived from model similarity statistics, decoupling the suppression of forgotten-class confidence from assumptions about redistribution among retained classes. We further incorporate an anchor-based preservation constraint that explicitly maintains predictive behaviour on selected retained data, yielding a controlled optimization trajectory that limits deviation from the original model. We evaluate the approach on variational quantum classifiers trained on the Iris and Covertype datasets. Results demonstrate sharp suppression of forgotten-class confidence, minimal degradation of retained-class performance, and closer alignment with the gold retrained model baselines compared to uniform-target unlearning. These findings highlight the importance of target design and constraint-based formulations for reliable and interpretable quantum machine unlearning.

</details>


### [105] [Improving and Accelerating Offline RL in Large Discrete Action Spaces with Structured Policy Initialization](https://arxiv.org/abs/2601.04441)
*Matthew Landers,Taylor W. Killian,Thomas Hartvigsen,Afsaneh Doryab*

Main category: cs.LG

TL;DR: SPIN introduces a two-stage framework that first pre-trains an Action Structure Model to learn valid action combinations, then freezes this representation to train lightweight policy heads, achieving better performance and faster convergence in discrete combinatorial action spaces.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning in discrete combinatorial action spaces faces challenges: existing approaches either assume independence across sub-actions (leading to incoherent/invalid actions) or learn action structure and control jointly (slow and unstable). There's a need for a method that can efficiently handle the exponential search space while producing valid action combinations.

Method: SPIN (Structured Policy Initialization) uses a two-stage framework: 1) Pre-train an Action Structure Model (ASM) to capture the manifold of valid actions by learning which sub-action combinations are coherent/valid, 2) Freeze the ASM representation and train lightweight policy heads on top for control, enabling efficient policy learning while ensuring action validity.

Result: On challenging discrete DM Control benchmarks, SPIN improves average return by up to 39% over state-of-the-art methods while reducing time to convergence by up to 12.8×, demonstrating both superior performance and significantly faster training.

Conclusion: SPIN effectively addresses the challenge of reinforcement learning in discrete combinatorial action spaces by separating action structure learning from policy learning, enabling efficient exploration of valid action combinations while maintaining stable and fast convergence.

Abstract: Reinforcement learning in discrete combinatorial action spaces requires searching over exponentially many joint actions to simultaneously select multiple sub-actions that form coherent combinations. Existing approaches either simplify policy learning by assuming independence across sub-actions, which often yields incoherent or invalid actions, or attempt to learn action structure and control jointly, which is slow and unstable. We introduce Structured Policy Initialization (SPIN), a two-stage framework that first pre-trains an Action Structure Model (ASM) to capture the manifold of valid actions, then freezes this representation and trains lightweight policy heads for control. On challenging discrete DM Control benchmarks, SPIN improves average return by up to 39% over the state of the art while reducing time to convergence by up to 12.8$\times$.

</details>


### [106] [When Predictions Shape Reality: A Socio-Technical Synthesis of Performative Predictions in Machine Learning](https://arxiv.org/abs/2601.04447)
*Gal Fybish,Teo Susnjak*

Main category: cs.LG

TL;DR: This paper is a Systematisation of Knowledge (SoK) on performative prediction - the phenomenon where ML model predictions actively shape their deployment environments, creating feedback loops and risks. It provides a comprehensive review, typology of risks, and introduces a practical assessment framework called the "Performative Strength vs. Impact Matrix" to help practitioners evaluate and address performativity.


<details>
  <summary>Details</summary>
Motivation: Machine learning models are increasingly deployed in high-stakes domains where their predictions actively shape the environments they operate in (performative prediction). This dynamic leads to unintended consequences like feedback loops, performance degradation, and societal risks. Despite rapid growth in literature, there's a lack of socio-technical synthesis that systematizes concepts and provides practical guidance for practitioners.

Method: The paper conducts a Systematisation of Knowledge (SoK) through comprehensive literature review of performative prediction research. It analyzes primary mechanisms of performativity manifestation, develops a typology of associated risks, and surveys proposed solutions from existing literature. The key methodological contribution is the development of the "Performative Strength vs. Impact Matrix" assessment framework.

Result: The paper provides a systematic overview of performative prediction mechanisms and associated risks. It introduces the "Performative Strength vs. Impact Matrix" as a practical tool to help practitioners assess the potential influence and severity of performativity on deployed models. This framework enables selection of appropriate algorithmic or human intervention levels based on the specific performative characteristics of their applications.

Conclusion: This SoK addresses the critical gap in performative prediction literature by providing a comprehensive synthesis and practical assessment framework. The "Performative Strength vs. Impact Matrix" offers practitioners a structured approach to evaluate performativity risks and make informed decisions about intervention strategies, bridging theoretical understanding with practical deployment considerations in high-stakes ML applications.

Abstract: Machine learning models are increasingly used in high-stakes domains where their predictions can actively shape the environments in which they operate, a phenomenon known as performative prediction. This dynamic, in which the deployment of the model influences the very outcome it seeks to predict, can lead to unintended consequences, including feedback loops, performance issues, and significant societal risks. While the literature in the field has grown rapidly in recent years, a socio-technical synthesis that systemises the phenomenon concepts and provides practical guidance has been lacking. This Systematisation of Knowledge (SoK) addresses this gap by providing a comprehensive review of the literature on performative predictions. We provide an overview of the primary mechanisms through which performativity manifests, present a typology of associated risks, and survey the proposed solutions offered in the literature. Our primary contribution is the ``Performative Strength vs. Impact Matrix" assessment framework. This practical tool is designed to help practitioners assess the potential influence and severity of performativity on their deployed predictive models and select the appropriate level of algorithmic or human intervention.

</details>


### [107] [Explainable Admission-Level Predictive Modeling for Prolonged Hospital Stay in Elderly Populations: Challenges in Low- and Middle-Income Countries](https://arxiv.org/abs/2601.04449)
*Daniel Sierra-Botero,Ana Molina-Taborda,Leonardo Espinosa-Leal,Alexander Karpenko,Alejandro Hernandez,Olga Lopez-Acevedo*

Main category: cs.LG

TL;DR: Predictive model for prolonged hospital length of stay using admission data with feature selection based on information value and graph theory, achieving AUC-ROC of 0.82.


<details>
  <summary>Details</summary>
Motivation: Prolonged length of stay (pLoS) is associated with adverse in-hospital events, and there's a need for predictive models to help hospital management and intervention planning.

Method: Used feature selection method selecting non-correlated features with highest information value using weights of evidence and graph theory cliques. Trained logistic regression model on 120,354 hospital admissions data split into training (67%), test (22%), and validation (11%) cohorts to predict pLoS (>7 days vs ≤7 days).

Result: Model achieved specificity 0.83, sensitivity 0.64, accuracy 0.76, precision 0.67, and AUC-ROC 0.82 on validation cohort. Feature selection yielded 9 interpretable variables.

Conclusion: The model shows strong predictive performance with interpretable features, making it valuable for hospital management and future intervention studies to reduce prolonged hospital stays.

Abstract: Prolonged length of stay (pLoS) is a significant factor associated with the risk of adverse in-hospital events. We develop and explain a predictive model for pLos using admission-level patient and hospital administrative data. The approach includes a feature selection method by selecting non-correlated features with the highest information value. The method uses features weights of evidence to select a representative within cliques from graph theory. The prognosis study analyzed the records from 120,354 hospital admissions at the Hospital Alma Mater de Antioquia between January 2017 and March 2022. After a cleaning process the dataset was split into training (67%), test (22%), and validation (11%) cohorts. A logistic regression model was trained to predict the pLoS in two classes: less than or greater than 7 days. The performance of the model was evaluated using accuracy, precision, sensitivity, specificity, and AUC-ROC metrics. The feature selection method returns nine interpretable variables, enhancing the models' transparency. In the validation cohort, the pLoS model achieved a specificity of 0.83 (95% CI, 0.82-0.84), sensitivity of 0.64 (95% CI, 0.62-0.65), accuracy of 0.76 (95% CI, 0.76-0.77), precision of 0.67 (95% CI, 0.66-0.69), and AUC-ROC of 0.82 (95% CI, 0.81-0.83). The model exhibits strong predictive performance and offers insights into the factors that influence prolonged hospital stays. This makes it a valuable tool for hospital management and for developing future intervention studies aimed at reducing pLoS.

</details>


### [108] [Using Large Language Models to Detect Socially Shared Regulation of Collaborative Learning](https://arxiv.org/abs/2601.04458)
*Jiayi Zhang,Conrad Borchers,Clayton Cohn,Namrata Srivastava,Caitlin Snyder,Siyuan Guo,Ashwin T S,Naveeduddin Mohammed,Haley Noh,Gautam Biswas*

Main category: cs.LG

TL;DR: The paper presents embedding-based models using LLM summaries and multimodal features to automatically detect socially shared regulation of learning (SSRL) behaviors in collaborative computational modeling environments, showing different feature types work best for different SSRL constructs.


<details>
  <summary>Details</summary>
Motivation: Most learning analytics advancements focus on individualized problem-solving rather than collaborative, open-ended problem-solving, which presents both opportunities (richer data) and challenges (low cohesion) for behavioral prediction. There's a need to extend predictive models to detect SSRL behaviors in collaborative environments.

Method: Used LLMs as summarization tools to generate task-aware representations of student dialogue aligned with system logs. Combined these summaries with text-only embeddings, context-enriched embeddings, and log-derived features to train predictive models for SSRL behavior detection.

Result: Text-only embeddings performed better for detecting SSRL behaviors related to enactment or group dynamics (off-task behavior, requesting assistance). Contextual and multimodal features provided complementary benefits for constructs like planning and reflection.

Conclusion: Embedding-based models show promise for scalable detection of SSRL behaviors in collaborative learning environments, enabling real-time feedback and adaptive scaffolding that teachers value, thus extending learning analytics capabilities.

Abstract: The field of learning analytics has made notable strides in automating the detection of complex learning processes in multimodal data. However, most advancements have focused on individualized problem-solving instead of collaborative, open-ended problem-solving, which may offer both affordances (richer data) and challenges (low cohesion) to behavioral prediction. Here, we extend predictive models to automatically detect socially shared regulation of learning (SSRL) behaviors in collaborative computational modeling environments using embedding-based approaches. We leverage large language models (LLMs) as summarization tools to generate task-aware representations of student dialogue aligned with system logs. These summaries, combined with text-only embeddings, context-enriched embeddings, and log-derived features, were used to train predictive models. Results show that text-only embeddings often achieve stronger performance in detecting SSRL behaviors related to enactment or group dynamics (e.g., off-task behavior or requesting assistance). In contrast, contextual and multimodal features provide complementary benefits for constructs such as planning and reflection. Overall, our findings highlight the promise of embedding-based models for extending learning analytics by enabling scalable detection of SSRL behaviors, ultimately supporting real-time feedback and adaptive scaffolding in collaborative learning environments that teachers value.

</details>


### [109] [Meta-probabilistic Modeling](https://arxiv.org/abs/2601.04462)
*Kevin Zhang,Yixin Wang*

Main category: cs.LG

TL;DR: Meta-probabilistic modeling (MPM) learns generative model structure from multiple related datasets using hierarchical architecture and bi-level optimization.


<details>
  <summary>Details</summary>
Motivation: Probabilistic graphical models require well-specified models, but identifying such models is challenging and requires iterative trial-and-error. There's a need to learn generative model structure directly from data.

Method: MPM uses hierarchical architecture with global model specifications shared across datasets and local parameters dataset-specific. Uses VAE-inspired surrogate objective with bi-level optimization: local variables updated analytically via coordinate ascent, global parameters trained with gradient methods.

Result: MPM successfully adapts generative models to data and recovers meaningful latent representations in object-centric image modeling and sequential text modeling tasks.

Conclusion: MPM provides an effective meta-learning approach for learning generative model structure directly from multiple related datasets, overcoming the trial-and-error challenges of traditional probabilistic graphical models.

Abstract: While probabilistic graphical models can discover latent structure in data, their effectiveness hinges on choosing well-specified models. Identifying such models is challenging in practice, often requiring iterative checking and revision through trial and error. To this end, we propose meta-probabilistic modeling (MPM), a meta-learning algorithm that learns generative model structure directly from multiple related datasets. MPM uses a hierarchical architecture where global model specifications are shared across datasets while local parameters remain dataset-specific. For learning and inference, we propose a tractable VAE-inspired surrogate objective, and optimize it through bi-level optimization: local variables are updated analytically via coordinate ascent, while global parameters are trained with gradient-based methods. We evaluate MPM on object-centric image modeling and sequential text modeling, demonstrating that it adapts generative models to data while recovering meaningful latent representations.

</details>


### [110] [When Models Manipulate Manifolds: The Geometry of a Counting Task](https://arxiv.org/abs/2601.04480)
*Wes Gurnee,Emmanuel Ameisen,Isaac Kauvar,Julius Tarng,Adam Pearce,Chris Olah,Joshua Batson*

Main category: cs.LG

TL;DR: Claude 3.5 Haiku's linebreaking mechanism uses geometric transformations of character count manifolds, similar to biological place cells, to make accurate linebreak decisions.


<details>
  <summary>Details</summary>
Motivation: To understand how language models perceive visual properties of text (like linebreaking) despite only receiving token sequences, and to investigate the sensory processing in early layers.

Method: Mechanistic investigation of Claude 3.5 Haiku's linebreaking task, analyzing how character counts are represented on low-dimensional curved manifolds, examining geometric transformations through attention heads, and validating findings with causal interventions.

Result: Found that character counts are represented on discretized curved manifolds via sparse feature families; linebreaking emerges from geometric transformations where token lengths accumulate, attention heads twist manifolds to estimate distance to boundaries, and orthogonal arrangement creates linear decision boundaries; discovered visual illusions that hijack the counting mechanism.

Conclusion: The work reveals rich sensory processing in early layers, intricate attention algorithms, and demonstrates the importance of combining feature-based and geometric perspectives for interpretability in understanding how models perceive visual text properties.

Abstract: Language models can perceive visual properties of text despite receiving only sequences of tokens-we mechanistically investigate how Claude 3.5 Haiku accomplishes one such task: linebreaking in fixed-width text. We find that character counts are represented on low-dimensional curved manifolds discretized by sparse feature families, analogous to biological place cells. Accurate predictions emerge from a sequence of geometric transformations: token lengths are accumulated into character count manifolds, attention heads twist these manifolds to estimate distance to the line boundary, and the decision to break the line is enabled by arranging estimates orthogonally to create a linear decision boundary. We validate our findings through causal interventions and discover visual illusions--character sequences that hijack the counting mechanism. Our work demonstrates the rich sensory processing of early layers, the intricacy of attention algorithms, and the importance of combining feature-based and geometric views of interpretability.

</details>


### [111] [IGenBench: Benchmarking the Reliability of Text-to-Infographic Generation](https://arxiv.org/abs/2601.04498)
*Yinghao Tang,Xueding Liu,Boyuan Zhang,Tingfeng Lan,Yupeng Xie,Jiale Lao,Yiyao Wang,Haoxuan Li,Tingting Gao,Bo Pan,Luoxuan Weng,Xiuqi Huang,Minfeng Zhu,Yingchaojie Feng,Yuyu Luo,Wei Chen*

Main category: cs.LG

TL;DR: IGENBENCH is the first benchmark for evaluating text-to-infographic generation reliability, revealing significant gaps in current T2I models' ability to produce accurate infographics despite aesthetic appeal.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models can generate visually appealing infographics, but their reliability is unclear - they often contain subtle but critical errors in data encoding and textual content that are easily overlooked.

Method: Created IGENBENCH with 600 curated test cases across 30 infographic types, designed automated evaluation framework with 10 question types, used multimodal LLMs for verification, and evaluated 10 state-of-the-art T2I models.

Result: Three-tier performance hierarchy with top model achieving 0.90 Q-ACC but only 0.49 I-ACC; data-related dimensions are universal bottlenecks (Data Completeness: 0.21); no model achieves end-to-end correctness across all dimensions.

Conclusion: IGENBENCH reveals significant reliability gaps in current T2I models for infographic generation, highlighting data accuracy as a major challenge and providing a benchmark for future model development.

Abstract: Infographics are composite visual artifacts that combine data visualizations with textual and illustrative elements to communicate information. While recent text-to-image (T2I) models can generate aesthetically appealing images, their reliability in generating infographics remains unclear. Generated infographics may appear correct at first glance but contain easily overlooked issues, such as distorted data encoding or incorrect textual content. We present IGENBENCH, the first benchmark for evaluating the reliability of text-to-infographic generation, comprising 600 curated test cases spanning 30 infographic types. We design an automated evaluation framework that decomposes reliability verification into atomic yes/no questions based on a taxonomy of 10 question types. We employ multimodal large language models (MLLMs) to verify each question, yielding question-level accuracy (Q-ACC) and infographic-level accuracy (I-ACC). We comprehensively evaluate 10 state-of-the-art T2I models on IGENBENCH. Our systematic analysis reveals key insights for future model development: (i) a three-tier performance hierarchy with the top model achieving Q-ACC of 0.90 but I-ACC of only 0.49; (ii) data-related dimensions emerging as universal bottlenecks (e.g., Data Completeness: 0.21); and (iii) the challenge of achieving end-to-end correctness across all models. We release IGENBENCH at https://igen-bench.vercel.app/.

</details>


### [112] [Surface-based Molecular Design with Multi-modal Flow Matching](https://arxiv.org/abs/2601.04506)
*Fang Wu,Zhengyuan Zhou,Shuting Jin,Xiangxiang Zeng,Jure Leskovec,Jinbo Xu*

Main category: cs.LG

TL;DR: SurfFlow is a novel surface-based generative algorithm for comprehensive co-design of peptide sequences, structures, and surfaces, outperforming full-atom baselines on the PepMerge benchmark.


<details>
  <summary>Details</summary>
Motivation: While deep generative models enable full-atom peptide co-design, the critical role of molecular surfaces in protein-protein interactions has been underexplored, creating a gap in peptide design approaches.

Method: SurfFlow uses a multi-modality conditional flow matching (CFM) architecture to learn distributions of surface geometries and biochemical properties, enabling comprehensive co-design of peptide sequence, structure, and surface.

Result: Evaluated on the comprehensive PepMerge benchmark, SurfFlow consistently outperforms full-atom baselines across all metrics, demonstrating superior peptide binding accuracy.

Conclusion: The results highlight the advantages of considering molecular surfaces in de novo peptide discovery and demonstrate the potential of integrating multiple protein modalities for more effective therapeutic peptide discovery.

Abstract: Therapeutic peptides show promise in targeting previously undruggable binding sites, with recent advancements in deep generative models enabling full-atom peptide co-design for specific protein receptors. However, the critical role of molecular surfaces in protein-protein interactions (PPIs) has been underexplored. To bridge this gap, we propose an omni-design peptides generation paradigm, called SurfFlow, a novel surface-based generative algorithm that enables comprehensive co-design of sequence, structure, and surface for peptides. SurfFlow employs a multi-modality conditional flow matching (CFM) architecture to learn distributions of surface geometries and biochemical properties, enhancing peptide binding accuracy. Evaluated on the comprehensive PepMerge benchmark, SurfFlow consistently outperforms full-atom baselines across all metrics. These results highlight the advantages of considering molecular surfaces in de novo peptide discovery and demonstrate the potential of integrating multiple protein modalities for more effective therapeutic peptide discovery.

</details>


### [113] [TSSR: Two-Stage Swap-Reward-Driven Reinforcement Learning for Character-Level SMILES Generation](https://arxiv.org/abs/2601.04521)
*Jacob Ede Levine,Yun Lyan Luo,Sai Chandra Kosaraju*

Main category: cs.LG

TL;DR: TSSR is a two-stage RL framework for SMILES generation that improves molecular validity and novelty through syntax repair and chemistry-aware rewards.


<details>
  <summary>Details</summary>
Motivation: Current chemical language models generating SMILES strings suffer from compounding token errors, producing unparseable or chemically implausible molecules, while hard constraints restrict exploration.

Method: Two-stage RL framework: Stage 1 rewards local token swaps to repair syntax; Stage 2 provides chemistry-aware feedback from RDKit diagnostics to reduce valence, aromaticity, and connectivity issues. Uses GRU policy with PPO in both pure RL and fine-tuning RL settings.

Result: Significantly improves syntactic validity, chemical validity, and novelty in pure RL; preserves drug-likeness and synthesizability while increasing validity and novelty in fine-tuning RL. Reduces RDKit detected errors through joint syntax and chemistry fixes.

Conclusion: TSSR converts sparse terminal objectives into denser, interpretable rewards, improving both syntactic and chemical quality without reducing diversity. It's dataset-agnostic and adaptable to various RL approaches.

Abstract: The design of reliable, valid, and diverse molecules is fundamental to modern drug discovery, as improved molecular generation supports efficient exploration of the chemical space for potential drug candidates and reduces the cost of early design efforts. Despite these needs, current chemical language models that generate molecules as SMILES strings are vulnerable to compounding token errors: many samples are unparseable or chemically implausible, and hard constraints meant to prevent failure can restrict exploration. To address this gap, we introduce TSSR, a Two-Stage, Swap-Reward-driven reinforcement learning (RL) framework for character-level SMILES generation. Stage one rewards local token swaps that repair syntax, promoting transitions from invalid to parseable strings. Stage two provides chemistry-aware feedback from RDKit diagnostics, rewarding reductions in valence, aromaticity, and connectivity issues. The reward decomposes into interpretable terms (swap efficiency, error reduction, distance to validity), is model agnostic, and requires no task-specific labels or hand-crafted grammars. We evaluated TSSR on the MOSES benchmark using a GRU policy trained with PPO in both pure RL (P-RL) from random initialization and fine-tuning RL (F-RL) starting from a pretrained chemical language model, assessing 10,000 generated SMILES per run. In P-RL, TSSR significantly improves syntactic validity, chemical validity, and novelty. In F-RL, TSSR preserves drug-likeness and synthesizability while increasing validity and novelty. Token-level analysis shows that syntax edits and chemistry fixes act jointly to reduce RDKit detected errors. TSSR converts a sparse terminal objective into a denser and more interpretable reward, improving both syntactic and chemical quality without reducing diversity. TSSR is dataset-agnostic and can be adapted to various reinforcement learning approaches.

</details>


### [114] [Not All Steps are Informative: On the Linearity of LLMs' RLVR Training](https://arxiv.org/abs/2601.04537)
*Tianle Wang,Zhongyuan Wu,Shenghao Jin,Hao Xu,Wei Chen,Ning Miao*

Main category: cs.LG

TL;DR: RL training for LLMs shows strong linear evolution, enabling weight/logits extrapolation to skip expensive training while maintaining or improving performance.


<details>
  <summary>Details</summary>
Motivation: RLVR training for LLMs is computationally expensive due to prolonged exploration requiring thousands of steps. The authors discovered that LLMs evolve linearly during RLVR, suggesting most learning happens early rather than through continuous discovery.

Method: The paper proposes two extrapolation methods based on observed linearity: Weight Extrapolation (predicting future model weights from intermediate checkpoints) and Logits Extrapolation (extrapolating model output log-probabilities beyond stable RL training ranges.

Result: Weight Extrapolation produces models with performance comparable to standard RL training with significantly less computation. Logits Extrapolation consistently outperforms continued RL training on all four benchmarks by extrapolating beyond the step range where RL training remains stable.

Conclusion: The linear evolution of LLMs during RLVR enables efficient extrapolation techniques that can dramatically reduce computational costs while maintaining or even improving performance compared to standard RL training.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a central component of large language model (LLM) post-training. Unlike supervised fine-tuning (SFT), RLVR lets an LLM generate multiple candidate solutions and reinforces those that lead to a verifiably correct final answer. However, in practice, RLVR often requires thousands of training steps to reach strong performance, incurring substantial computation largely attributed to prolonged exploration. In this work, we make a surprising observation: during RLVR, LLMs evolve in a strongly linear manner. Specifically, both model weights and model output log-probabilities exhibit strong linear correlations with RL training steps. This suggests that RLVR predominantly amplifies trends that emerge early in training, rather than continuously discovering new behaviors throughout the entire optimization trajectory. Motivated by this linearity, we investigate whether future model states can be predicted from intermediate checkpoints via extrapolation, avoiding continued expensive training. We show that Weight Extrapolation produces models with performance comparable to standard RL training while requiring significantly less computation. Moreover, Logits Extrapolation consistently outperforms continued RL training on all four benchmarks by extrapolating beyond the step range where RL training remains stable.

</details>


### [115] [Timeliness-Oriented Scheduling and Resource Allocation in Multi-Region Collaborative Perception](https://arxiv.org/abs/2601.04542)
*Mengmeng Zhu,Yuxuan Sun,Yukuan Jia,Wei Chen,Bo Ai,Sheng Zhou*

Main category: cs.LG

TL;DR: TAMP scheduling algorithm optimizes collaborative perception by balancing timeliness (AoI) and communication volume to maximize perception accuracy while minimizing resource usage.


<details>
  <summary>Details</summary>
Motivation: Collaborative perception faces challenges with information timeliness due to dynamic environments and limited computational/bandwidth resources, requiring intelligent scheduling to balance accuracy and communication efficiency.

Method: Proposes TAMP (Timeliness-Aware Multi-region Prioritized) scheduling algorithm using Lyapunov-based optimization with empirical penalty function mapping AoI and communication volume to perception performance, decomposing long-term optimization into per-slot prioritization.

Result: TAMP outperforms baselines with up to 27% Average Precision improvement in intersection and corridor scenarios using real-world RCooper dataset.

Conclusion: The TAMP algorithm effectively addresses timeliness and resource constraints in collaborative perception through intelligent scheduling that balances perception accuracy with communication costs.

Abstract: Collaborative perception (CP) is a critical technology in applications like autonomous driving and smart cities. It involves the sharing and fusion of information among sensors to overcome the limitations of individual perception, such as blind spots and range limitations. However, CP faces two primary challenges. First, due to the dynamic nature of the environment, the timeliness of the transmitted information is critical to perception performance. Second, with limited computational power at the sensors and constrained wireless bandwidth, the communication volume must be carefully designed to ensure feature representations are both effective and sufficient. This work studies the dynamic scheduling problem in a multi-region CP scenario, and presents a Timeliness-Aware Multi-region Prioritized (TAMP) scheduling algorithm to trade-off perception accuracy and communication resource usage. Timeliness reflects the utility of information that decays as time elapses, which is manifested by the perception performance in CP tasks. We propose an empirical penalty function that maps the joint impact of Age of Information (AoI) and communication volume to perception performance. Aiming to minimize this timeliness-oriented penalty in the long-term, and recognizing that scheduling decisions have a cumulative effect on subsequent system states, we propose the TAMP scheduling algorithm. TAMP is a Lyapunov-based optimization policy that decomposes the long-term average objective into a per-slot prioritization problem, balancing the scheduling worth against resource cost. We validate our algorithm in both intersection and corridor scenarios with the real-world Roadside Cooperative perception (RCooper) dataset. Extensive simulations demonstrate that TAMP outperforms the best-performing baseline, achieving an Average Precision (AP) improvement of up to 27% across various configurations.

</details>


### [116] [GEnSHIN: Graphical Enhanced Spatio-temporal Hierarchical Inference Network for Traffic Flow Prediction](https://arxiv.org/abs/2601.04550)
*Zhiyan Zhou,Junjie Liao,Manho Zhang,Yingyi Liao,Ziai Wang*

Main category: cs.LG

TL;DR: GEnSHIN is a novel graph-enhanced spatio-temporal hierarchical inference network for traffic flow prediction that integrates attention-enhanced GCRU, asymmetric dual-embedding graph generation, and dynamic memory bank modules to handle complex spatio-temporal dependencies.


<details>
  <summary>Details</summary>
Motivation: With accelerating urbanization, intelligent transportation systems require more accurate traffic flow prediction to handle complex spatio-temporal dependencies in urban traffic networks.

Method: Proposes GEnSHIN with three key components: 1) Attention-enhanced Graph Convolutional Recurrent Unit (GCRU) with Transformer modules for long-term temporal dependencies, 2) Asymmetric dual-embedding graph generation combining real road network and data-driven latent asymmetric topology, 3) Dynamic memory bank with learnable traffic pattern prototypes and lightweight graph updater for personalized representations and adaptation to dynamic road network changes.

Result: Extensive experiments on METR-LA dataset show GEnSHIN achieves or surpasses comparative models across MAE, RMSE, and MAPE metrics, with excellent prediction stability during peak morning/evening traffic hours. Ablation experiments validate effectiveness of each core module.

Conclusion: GEnSHIN effectively handles complex spatio-temporal dependencies in traffic flow prediction through its integrated architecture, demonstrating superior performance and stability, particularly during challenging peak traffic periods.

Abstract: With the acceleration of urbanization, intelligent transportation systems have an increasing demand for accurate traffic flow prediction. This paper proposes a novel Graph Enhanced Spatio-temporal Hierarchical Inference Network (GEnSHIN) to handle the complex spatio-temporal dependencies in traffic flow prediction. The model integrates three innovative designs: 1) An attention-enhanced Graph Convolutional Recurrent Unit (GCRU), which strengthens the modeling capability for long-term temporal dependencies by introducing Transformer modules; 2) An asymmetric dual-embedding graph generation mechanism, which leverages the real road network and data-driven latent asymmetric topology to generate graph structures that better fit the characteristics of actual traffic flow; 3) A dynamic memory bank module, which utilizes learnable traffic pattern prototypes to provide personalized traffic pattern representations for each sensor node, and introduces a lightweight graph updater during the decoding phase to adapt to dynamic changes in road network states. Extensive experiments on the public dataset METR-LA show that GEnSHIN achieves or surpasses the performance of comparative models across multiple metrics such as Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Mean Absolute Percentage Error (MAPE). Notably, the model demonstrates excellent prediction stability during peak morning and evening traffic hours. Ablation experiments further validate the effectiveness of each core module and its contribution to the final performance.

</details>


### [117] [Improving Semi-Supervised Contrastive Learning via Entropy-Weighted Confidence Integration of Anchor-Positive Pairs](https://arxiv.org/abs/2601.04555)
*Shogo Nakayama,Masahiro Okuda*

Main category: cs.LG

TL;DR: Proposes confidence-based adaptive weighting for semi-supervised contrastive learning using entropy of predicted probabilities, enabling pseudo-label assignment to previously excluded samples.


<details>
  <summary>Details</summary>
Motivation: Conventional semi-supervised contrastive learning methods exclude samples with low predicted class probabilities, limiting training data utilization. There's a need for a more principled approach that can incorporate all samples while accounting for prediction confidence.

Method: Novel loss function that estimates sample confidence based on entropy of predicted probability distribution, applies confidence-based adaptive weighting, enables pseudo-label assignment to previously excluded samples, and performs contrastive learning considering confidence of both anchor and positive samples.

Result: Experimental results show improved classification accuracy and more stable learning performance, particularly under low-label conditions.

Conclusion: The proposed confidence-based adaptive weighting approach provides a more principled method for semi-supervised contrastive learning that better utilizes available data and improves performance in low-label scenarios.

Abstract: Conventional semi-supervised contrastive learning methods assign pseudo-labels only to samples whose highest predicted class probability exceeds a predefined threshold, and then perform supervised contrastive learning using those selected samples. In this study, we propose a novel loss function that estimates the confidence of each sample based on the entropy of its predicted probability distribution and applies confidence-based adaptive weighting. This approach enables pseudo-label assignment even to samples that were previously excluded from training and facilitates contrastive learning that accounts for the confidence of both anchor and positive samples in a more principled manner. Experimental results demonstrate that the proposed method improves classification accuracy and achieves more stable learning performance even under low-label conditions.

</details>


### [118] [A Vision for Multisensory Intelligence: Sensing, Synergy, and Science](https://arxiv.org/abs/2601.04563)
*Paul Pu Liang*

Main category: cs.LG

TL;DR: A research vision for multisensory AI over the next decade, proposing to extend AI beyond digital modalities to incorporate all human senses and physical/social signals through three themes: sensing, science, and synergy.


<details>
  <summary>Details</summary>
Motivation: Current AI primarily advances in digital modalities (text, vision, audio), but human experience is multisensory. There's a need to connect AI to human senses and richer physical/social signals to transform human-AI interaction.

Method: Proposes a three-theme research framework: 1) Sensing - extending AI's ability to capture the world beyond digital media, 2) Science - developing principled approaches for multimodal heterogeneity, unified architectures, and cross-modal transfer, 3) Synergy - technical challenges for learning synergy between modalities and between humans and AI.

Result: Presents a comprehensive research vision with accompanying projects, resources, and demos from the MIT Media Lab's Multisensory Intelligence group, available at https://mit-mi.github.io/.

Conclusion: Multisensory AI represents a transformative direction that can fundamentally change how humans and AI experience and interact with each other by connecting AI to the full spectrum of human sensory experience and environmental signals.

Abstract: Our experience of the world is multisensory, spanning a synthesis of language, sight, sound, touch, taste, and smell. Yet, artificial intelligence has primarily advanced in digital modalities like text, vision, and audio. This paper outlines a research vision for multisensory artificial intelligence over the next decade. This new set of technologies can change how humans and AI experience and interact with one another, by connecting AI to the human senses and a rich spectrum of signals from physiological and tactile cues on the body, to physical and social signals in homes, cities, and the environment. We outline how this field must advance through three interrelated themes of sensing, science, and synergy. Firstly, research in sensing should extend how AI captures the world in richer ways beyond the digital medium. Secondly, developing a principled science for quantifying multimodal heterogeneity and interactions, developing unified modeling architectures and representations, and understanding cross-modal transfer. Finally, we present new technical challenges to learn synergy between modalities and between humans and AI, covering multisensory integration, alignment, reasoning, generation, generalization, and experience. Accompanying this vision paper are a series of projects, resources, and demos of latest advances from the Multisensory Intelligence group at the MIT Media Lab, see https://mit-mi.github.io/.

</details>


### [119] [Spatial-Temporal Feedback Diffusion Guidance for Controlled Traffic Imputation](https://arxiv.org/abs/2601.04572)
*Xiaowei Mao,Huihu Ding,Yan Lin,Tingrui Wu,Shengnan Guo,Dazhuo Qiu,Feiling Fang,Jilin Hu,Huaiyu Wan*

Main category: cs.LG

TL;DR: FENCE: A spatial-temporal feedback diffusion guidance method that adaptively adjusts guidance scales for missing value imputation in traffic data, improving accuracy by preventing overcorrection and leveraging spatial-temporal correlations.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models for traffic data imputation use uniform guidance scales across spatial-temporal dimensions, which is inadequate for nodes with high missing data rates. Sparse observations provide insufficient conditional guidance, causing the generative process to drift toward the prior distribution rather than following observations, leading to suboptimal imputation.

Method: FENCE introduces: 1) Dynamic feedback mechanism that adjusts guidance scale based on posterior likelihood approximations - increases when generated values diverge from observations, reduces when alignment improves; 2) Cluster-level guidance scales by grouping nodes based on attention scores to leverage spatial-temporal correlations, avoiding suboptimal global guidance scales.

Result: Experimental results on real-world traffic datasets show that FENCE significantly enhances imputation accuracy compared to existing methods.

Conclusion: FENCE effectively addresses the limitations of uniform guidance scales in diffusion models for spatial-temporal traffic data imputation by providing adaptive, cluster-level feedback guidance that prevents overcorrection and leverages spatial-temporal correlations for improved accuracy.

Abstract: Imputing missing values in spatial-temporal traffic data is essential for intelligent transportation systems. Among advanced imputation methods, score-based diffusion models have demonstrated competitive performance. These models generate data by reversing a noising process, using observed values as conditional guidance. However, existing diffusion models typically apply a uniform guidance scale across both spatial and temporal dimensions, which is inadequate for nodes with high missing data rates. Sparse observations provide insufficient conditional guidance, causing the generative process to drift toward the learned prior distribution rather than closely following the conditional observations, resulting in suboptimal imputation performance.
  To address this, we propose FENCE, a spatial-temporal feedback diffusion guidance method designed to adaptively control guidance scales during imputation. First, FENCE introduces a dynamic feedback mechanism that adjusts the guidance scale based on the posterior likelihood approximations. The guidance scale is increased when generated values diverge from observations and reduced when alignment improves, preventing overcorrection. Second, because alignment to observations varies across nodes and denoising steps, a global guidance scale for all nodes is suboptimal. FENCE computes guidance scales at the cluster level by grouping nodes based on their attention scores, leveraging spatial-temporal correlations to provide more accurate guidance. Experimental results on real-world traffic datasets show that FENCE significantly enhances imputation accuracy.

</details>


### [120] [FedKDX: Federated Learning with Negative Knowledge Distillation for Enhanced Healthcare AI Systems](https://arxiv.org/abs/2601.04587)
*Quang-Tu Pham,Hoang-Dieu Vu,Dinh-Dat Pham,Hieu H. Pham*

Main category: cs.LG

TL;DR: FedKDX is a federated learning framework using Negative Knowledge Distillation to improve healthcare AI by capturing both target and non-target information, enhancing generalization while maintaining privacy.


<details>
  <summary>Details</summary>
Motivation: Existing federated learning approaches in healthcare focus only on positive knowledge transfer, limiting model generalization. Healthcare data has statistical heterogeneity, privacy constraints (HIPAA/GDPR), and communication efficiency needs that require better solutions.

Method: FedKDX integrates multiple knowledge transfer techniques: traditional knowledge distillation, contrastive learning, and novel Negative Knowledge Distillation (NKD) that captures both target and non-target information. The framework maintains privacy while reducing communication costs through a unified architecture.

Result: Experiments on healthcare datasets (SLEEP, UCI-HAR, PAMAP2) show improved accuracy (up to 2.53% over state-of-the-art), faster convergence, and better performance on non-IID data distributions. Theoretical analysis supports NKD's effectiveness for statistical heterogeneity.

Conclusion: FedKDX offers a balanced solution for privacy-sensitive medical applications, showing promise under regulatory frameworks like HIPAA and GDPR. It addresses both performance and practical implementation requirements in decentralized healthcare settings.

Abstract: This paper introduces FedKDX, a federated learning framework that addresses limitations in healthcare AI through Negative Knowledge Distillation (NKD). Unlike existing approaches that focus solely on positive knowledge transfer, FedKDX captures both target and non-target information to improve model generalization in healthcare applications. The framework integrates multiple knowledge transfer techniques--including traditional knowledge distillation, contrastive learning, and NKD--within a unified architecture that maintains privacy while reducing communication costs. Through experiments on healthcare datasets (SLEEP, UCI-HAR, and PAMAP2), FedKDX demonstrates improved accuracy (up to 2.53% over state-of-the-art methods), faster convergence, and better performance on non-IID data distributions. Theoretical analysis supports NKD's contribution to addressing statistical heterogeneity in distributed healthcare data. The approach shows promise for privacy-sensitive medical applications under regulatory frameworks like HIPAA and GDPR, offering a balanced solution between performance and practical implementation requirements in decentralized healthcare settings. The code and model are available at https://github.com/phamdinhdat-ai/Fed_2024.

</details>


### [121] [Density Matrix RNN (DM-RNN): A Quantum Information Theoretic Framework for Modeling Musical Context and Polyphony](https://arxiv.org/abs/2601.04592)
*Joonwon Seo,Mariana Montiel*

Main category: cs.LG

TL;DR: DM-RNN uses quantum density matrices to model musical ambiguity as statistical ensembles, replacing deterministic RNN hidden states with mixed quantum states that capture probabilities and coherences through CPTP dynamics.


<details>
  <summary>Details</summary>
Motivation: Classical RNNs impose an information bottleneck by summarizing musical context into deterministic hidden states, failing to capture the inherent ambiguity and multiple interpretations present in music.

Method: Proposes Density Matrix RNN (DM-RNN) using quantum density matrices to maintain statistical ensembles of interpretations. Uses Quantum Channels (CPTP maps) for temporal dynamics, parameterized via Choi-Jamiolkowski isomorphism to ensure physically valid CPTP dynamics by construction.

Result: Provides analytical framework using Von Neumann Entropy to quantify musical uncertainty and Quantum Mutual Information to measure entanglement between voices. Creates mathematically rigorous framework for modeling ambiguous musical structures.

Conclusion: DM-RNN offers a novel quantum-inspired architecture that better captures musical ambiguity through mixed quantum states, providing a more expressive framework for complex musical modeling compared to classical deterministic approaches.

Abstract: Classical Recurrent Neural Networks (RNNs) summarize musical context into a deterministic hidden state vector, imposing an information bottleneck that fails to capture the inherent ambiguity in music. We propose the Density Matrix RNN (DM-RNN), a novel theoretical architecture utilizing the Density Matrix. This allows the model to maintain a statistical ensemble of musical interpretations (a mixed state), capturing both classical probabilities and quantum coherences. We rigorously define the temporal dynamics using Quantum Channels (CPTP maps). Crucially, we detail a parameterization strategy based on the Choi-Jamiolkowski isomorphism, ensuring the learned dynamics remain physically valid (CPTP) by construction. We introduce an analytical framework using Von Neumann Entropy to quantify musical uncertainty and Quantum Mutual Information (QMI) to measure entanglement between voices. The DM-RNN provides a mathematically rigorous framework for modeling complex, ambiguous musical structures.

</details>


### [122] [DeepHalo: A Neural Choice Model with Controllable Context Effects](https://arxiv.org/abs/2601.04616)
*Shuhan Zhang,Zhi Wang,Rui Gao,Shuang Li*

Main category: cs.LG

TL;DR: DeepHalo: A neural framework for modeling context-dependent human decision-making with explicit control over interaction order and interpretable context effects.


<details>
  <summary>Details</summary>
Motivation: Human decision-making is influenced by context effects (Halo effect), but existing models either ignore features, use restrictive interaction structures, or lack interpretability. There's a need for a feature-based model that can capture context effects with explicit control over interaction order.

Method: Proposes DeepHalo, a neural modeling framework that incorporates features while enabling explicit control over interaction order and principled interpretation of context effects. The model systematically identifies interaction effects by order and serves as a universal approximator in featureless settings.

Result: Experiments on synthetic and real-world datasets demonstrate strong predictive performance while providing greater transparency into the drivers of choice.

Conclusion: DeepHalo successfully addresses limitations of existing context effect models by combining feature incorporation with explicit interaction order control and interpretability, offering both predictive power and transparency in understanding context-dependent choice behavior.

Abstract: Modeling human decision-making is central to applications such as recommendation, preference learning, and human-AI alignment. While many classic models assume context-independent choice behavior, a large body of behavioral research shows that preferences are often influenced by the composition of the choice set itself -- a phenomenon known as the context effect or Halo effect. These effects can manifest as pairwise (first-order) or even higher-order interactions among the available alternatives. Recent models that attempt to capture such effects either focus on the featureless setting or, in the feature-based setting, rely on restrictive interaction structures or entangle interactions across all orders, which limits interpretability. In this work, we propose DeepHalo, a neural modeling framework that incorporates features while enabling explicit control over interaction order and principled interpretation of context effects. Our model enables systematic identification of interaction effects by order and serves as a universal approximator of context-dependent choice functions when specialized to a featureless setting. Experiments on synthetic and real-world datasets demonstrate strong predictive performance while providing greater transparency into the drivers of choice.

</details>


### [123] [Learning Dynamics in RL Post-Training for Language Models](https://arxiv.org/abs/2601.04670)
*Akiyoshi Tomihari*

Main category: cs.LG

TL;DR: RL post-training reduces output diversity due to limited feature variability causing systematic confidence increases; proposed classifier-first RL accelerates optimization.


<details>
  <summary>Details</summary>
Motivation: RL post-training is crucial for improving language model alignment and reasoning, but phenomena like reduced output diversity remain poorly understood. The authors aim to analyze RL learning dynamics using tools from supervised learning that are underexplored in RL.

Method: Adopted empirical neural tangent kernel (NTK) framework, decomposing NTK into components to analyze how RL updates propagate across training samples. Proposed classifier-first reinforcement learning (CF-RL), a two-stage strategy that prioritizes classifier updates before standard RL optimization.

Result: Analysis revealed limited feature variability causes RL updates to systematically increase model confidence, explaining reduced output diversity. CF-RL showed increased model confidence and accelerated optimization. The mechanism differs from linear-probing-then-fine-tuning in supervised learning.

Conclusion: The study formalizes RL post-training learning dynamics, provides explanation for reduced output diversity, and proposes CF-RL as an effective training strategy. Motivates further analysis and improvement of RL post-training methods.

Abstract: Reinforcement learning (RL) post-training is a critical stage in modern language model development, playing a key role in improving alignment and reasoning ability. However, several phenomena remain poorly understood, including the reduction in output diversity. To gain a broader understanding of RL post-training, we analyze the learning dynamics of RL post-training from a perspective that has been studied in supervised learning but remains underexplored in RL. We adopt an empirical neural tangent kernel (NTK) framework and decompose the NTK into two components to characterize how RL updates propagate across training samples. Our analysis reveals that limited variability in feature representations can cause RL updates to systematically increase model confidence, providing an explanation for the commonly observed reduction in output diversity after RL post-training. Furthermore, we show that effective learning in this regime depends on rapidly shaping the classifier, which directly affects the gradient component of the NTK. Motivated by these insights, we propose classifier-first reinforcement learning (CF-RL), a simple two-stage training strategy that prioritizes classifier updates before standard RL optimization. Experimental results validate our theoretical analysis by demonstrating increased model confidence and accelerated optimization under CF-RL. Additional analysis shows that the mechanism underlying CF-RL differs from that of linear-probing-then-fine-tuning in supervised learning. Overall, our study formalizes the learning dynamics of RL post-training and motivates further analysis and improvement.

</details>


### [124] [Estimating Causal Effects in Gaussian Linear SCMs with Finite Data](https://arxiv.org/abs/2601.04673)
*Aurghya Maiti,Prateek Jain*

Main category: cs.LG

TL;DR: The paper introduces Centralized Gaussian Linear SCMs (CGL-SCMs) to address overparameterization in causal effect estimation from observational data with latent confounders, and presents an EM-based algorithm for learning parameters and estimating identifiable causal effects.


<details>
  <summary>Details</summary>
Motivation: Estimating causal effects from observational data is challenging, especially with latent confounders. Gaussian Linear SCMs (GL-SCMs) are analytically tractable but suffer from overparameterization, making parameter estimation infeasible with finite data.

Method: Introduces Centralized Gaussian Linear SCMs (CGL-SCMs), a simplified subclass where exogenous variables follow standardized distributions. Presents a novel EM-based estimation algorithm to learn CGL-SCM parameters and estimate identifiable causal effects from finite observational samples.

Result: CGL-SCMs are shown to be equally expressive as GL-SCMs in terms of causal effect identifiability from observational distributions. Experiments on synthetic data and benchmark causal graphs demonstrate that learned models accurately recover causal distributions.

Conclusion: CGL-SCMs provide a practical solution to the overparameterization problem in GL-SCMs while maintaining expressive power for causal effect estimation, with the EM-based algorithm enabling reliable parameter learning from finite observational data.

Abstract: Estimating causal effects from observational data remains a fundamental challenge in causal inference, especially in the presence of latent confounders. This paper focuses on estimating causal effects in Gaussian Linear Structural Causal Models (GL-SCMs), which are widely used due to their analytical tractability. However, parameter estimation in GL-SCMs is often infeasible with finite data, primarily due to overparameterization. To address this, we introduce the class of Centralized Gaussian Linear SCMs (CGL-SCMs), a simplified yet expressive subclass where exogenous variables follow standardized distributions. We show that CGL-SCMs are equally expressive in terms of causal effect identifiability from observational distributions and present a novel EM-based estimation algorithm that can learn CGL-SCM parameters and estimate identifiable causal effects from finite observational samples. Our theoretical analysis is validated through experiments on synthetic data and benchmark causal graphs, demonstrating that the learned models accurately recover causal distributions.

</details>


### [125] [Nightmare Dreamer: Dreaming About Unsafe States And Planning Ahead](https://arxiv.org/abs/2601.04686)
*Oluwatosin Oseni,Shengjie Wang,Jun Zhu,Micah Corah*

Main category: cs.LG

TL;DR: Nightmare Dreamer is a model-based Safe RL algorithm that uses learned world models to predict safety violations, achieving near-zero safety violations while maximizing rewards.


<details>
  <summary>Details</summary>
Motivation: RL has shown success in robotics control but adoption is limited due to insufficient safety guarantees. There's a need for RL algorithms that can provide safety assurances while maintaining performance.

Method: Introduces Nightmare Dreamer, a model-based Safe RL approach that leverages learned world models to predict potential safety violations and plan actions accordingly to avoid them.

Result: Achieves nearly zero safety violations while maximizing rewards. Outperforms model-free baselines on Safety Gymnasium tasks using only image observations, with nearly 20x improvement in efficiency.

Conclusion: Nightmare Dreamer demonstrates that model-based approaches can effectively address safety concerns in RL, enabling safer deployment in real-world applications like robotics control.

Abstract: Reinforcement Learning (RL) has shown remarkable success in real-world applications, particularly in robotics control. However, RL adoption remains limited due to insufficient safety guarantees. We introduce Nightmare Dreamer, a model-based Safe RL algorithm that addresses safety concerns by leveraging a learned world model to predict potential safety violations and plan actions accordingly. Nightmare Dreamer achieves nearly zero safety violations while maximizing rewards. Nightmare Dreamer outperforms model-free baselines on Safety Gymnasium tasks using only image observations, achieving nearly a 20x improvement in efficiency.

</details>


### [126] [Do LLMs Benefit from User and Item Embeddings in Recommendation Tasks?](https://arxiv.org/abs/2601.04690)
*Mir Rayat Imtiaz Hossain,Leo Feng,Leonid Sigal,Mohamed Osama Ahmed*

Main category: cs.LG

TL;DR: LLM-based recommendation system that projects collaborative filtering embeddings into LLM token space alongside text for improved recommendations


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based recommendation methods rely too heavily on text semantics and inadequately incorporate collaborative signals, typically using only limited user or item embeddings rather than handling multiple item embeddings from user history

Method: Projects user and item embeddings (learned from collaborative filtering) into LLM token space via separate lightweight projector modules, then fine-tunes LLM to condition on these projected embeddings alongside textual tokens for recommendation generation

Result: Preliminary results show effective leveraging of structured user-item interaction data, improved recommendation performance over text-only LLM baselines

Conclusion: Offers a practical path for bridging traditional recommendation systems with modern LLMs by effectively combining collaborative filtering embeddings with LLM text processing

Abstract: Large Language Models (LLMs) have emerged as promising recommendation systems, offering novel ways to model user preferences through generative approaches. However, many existing methods often rely solely on text semantics or incorporate collaborative signals in a limited manner, typically using only user or item embeddings. These methods struggle to handle multiple item embeddings representing user history, reverting to textual semantics and neglecting richer collaborative information. In this work, we propose a simple yet effective solution that projects user and item embeddings, learned from collaborative filtering, into the LLM token space via separate lightweight projector modules. A finetuned LLM then conditions on these projected embeddings alongside textual tokens to generate recommendations. Preliminary results show that this design effectively leverages structured user-item interaction data, improves recommendation performance over text-only LLM baselines, and offers a practical path for bridging traditional recommendation systems with modern LLMs.

</details>


### [127] [A zone-based training approach for last-mile routing using Graph Neural Networks and Pointer Networks](https://arxiv.org/abs/2601.04705)
*Àngel Ruiz-Fas,Carlos Granell,José Francisco Ramos,Joaquín Huerta,Sergio Trilles*

Main category: cs.LG

TL;DR: Deep learning approach for last-mile delivery routing using Graph Neural Networks and Pointer Networks, with zone-based training to handle asymmetric travel times and improve performance.


<details>
  <summary>Details</summary>
Motivation: Last-mile delivery networks are strained by e-commerce growth, requiring efficient routing solutions. Classical heuristics fail with asymmetric travel times (one-way streets, congestion), necessitating more adaptive approaches.

Method: Encoder-decoder architecture: GNN encoder creates node embeddings from directed graph representation of routes with asymmetric travel times. Pointer Network decoder sequentially selects stops. Zone-based training divides geographical areas using Discrete Global Grid System clustering, training separate model instances per zone.

Result: Zone-based training reduces average predicted route length compared to general training. Performance improvement increases with more stops per route. Evaluated on Los Angeles routes from 2021 Amazon Last Mile Routing Challenge.

Conclusion: Deep learning with zone-based training effectively addresses last-mile routing with asymmetric travel times, showing superior performance over general training approaches, especially for routes with many stops.

Abstract: Rapid e-commerce growth has pushed last-mile delivery networks to their limits, where small routing gains translate into lower costs, faster service, and fewer emissions. Classical heuristics struggle to adapt when travel times are highly asymmetric (e.g., one-way streets, congestion). A deep learning-based approach to the last-mile routing problem is presented to generate geographical zones composed of stop sequences to minimize last-mile delivery times.
  The presented approach is an encoder-decoder architecture. Each route is represented as a complete directed graph whose nodes are stops and whose edge weights are asymmetric travel times. A Graph Neural Network encoder produces node embeddings that captures the spatial relationships between stops. A Pointer Network decoder then takes the embeddings and the route's start node to sequentially select the next stops, assigning a probability to each unvisited node as the next destination.
  Cells of a Discrete Global Grid System which contain route stops in the training data are obtained and clustered to generate geographical zones of similar size in which the process of training and inference are divided. Subsequently, a different instance of the model is trained per zone only considering the stops of the training routes which are included in that zone.
  This approach is evaluated using the Los Angeles routes from the 2021 Amazon Last Mile Routing Challenge. Results from general and zone-based training are compared, showing a reduction in the average predicted route length in the zone-based training compared to the general training. The performance improvement of the zone-based approach becomes more pronounced as the number of stops per route increases.

</details>


### [128] [MQ-GNN: A Multi-Queue Pipelined Architecture for Scalable and Efficient GNN Training](https://arxiv.org/abs/2601.04707)
*Irfan Ullah,Young-Koo Lee*

Main category: cs.LG

TL;DR: MQ-GNN is a multi-queue pipelined framework that accelerates multi-GPU GNN training by overlapping computation stages, enabling asynchronous gradient sharing with consistency guarantees, and optimizing data transfer through caching and adaptive queue management.


<details>
  <summary>Details</summary>
Motivation: Current GNN training frameworks suffer from scalability issues due to inefficient mini-batch generation, data transfer bottlenecks, and costly inter-GPU synchronization, leading to poor resource utilization and slow training times.

Method: Proposes MQ-GNN with three key components: 1) Multi-queue pipelining to overlap training stages, 2) RaCoM (Ready-to-Update Asynchronous Consistent Model) for asynchronous gradient sharing with adaptive periodic synchronization, and 3) Global neighbor sampling with caching and adaptive queue-sizing to reduce data transfer and balance computation-memory tradeoffs.

Result: Experiments on four large-scale datasets with ten baseline models show MQ-GNN achieves up to 4.6× faster training time and 30% improved GPU utilization while maintaining competitive accuracy compared to existing methods.

Conclusion: MQ-GNN establishes itself as a scalable and efficient solution for multi-GPU GNN training by effectively addressing the key bottlenecks in current frameworks through pipelining, asynchronous consistency, and optimized resource management.

Abstract: Graph Neural Networks (GNNs) are powerful tools for learning graph-structured data, but their scalability is hindered by inefficient mini-batch generation, data transfer bottlenecks, and costly inter-GPU synchronization. Existing training frameworks fail to overlap these stages, leading to suboptimal resource utilization. This paper proposes MQ-GNN, a multi-queue pipelined framework that maximizes training efficiency by interleaving GNN training stages and optimizing resource utilization. MQ-GNN introduces Ready-to-Update Asynchronous Consistent Model (RaCoM), which enables asynchronous gradient sharing and model updates while ensuring global consistency through adaptive periodic synchronization. Additionally, it employs global neighbor sampling with caching to reduce data transfer overhead and an adaptive queue-sizing strategy to balance computation and memory efficiency. Experiments on four large-scale datasets and ten baseline models demonstrate that MQ-GNN achieves up to \boldmath $\bm{4.6\,\times}$ faster training time and 30% improved GPU utilization while maintaining competitive accuracy. These results establish MQ-GNN as a scalable and efficient solution for multi-GPU GNN training.

</details>


### [129] [GPU-Accelerated INT8 Quantization for KV Cache Compression in Large Language Models](https://arxiv.org/abs/2601.04719)
*Maanas Taneja,Purab Shingvi*

Main category: cs.LG

TL;DR: INT8 quantization for KV cache compression reduces memory by 4x with minimal accuracy loss, achieving up to 1694x speedup over CPU baselines.


<details>
  <summary>Details</summary>
Motivation: KV cache in LLMs creates major memory bottlenecks during inference, growing linearly with sequence length and often exceeding model weight memory footprint.

Method: Implemented GPU-accelerated INT8 quantization for KV cache compression with four CUDA kernel variants: naive, tiled, coarsened, and vectorized.

Result: Vectorized kernel achieved up to 1,694x speedup over CPU baselines with reconstruction error <0.004 and attention score error <0.1 for 8K-dimensional heads; 4x memory reduction with minimal computational overhead (6-58ms).

Conclusion: INT8 quantization provides a practical approach for reducing memory pressure in LLM inference with negligible computational overhead and minimal impact on downstream model behavior.

Abstract: The key-value (KV) cache in large language models presents a significant memory bottleneck during inference, growing linearly with sequence length and often exceeding the memory footprint of model weights themselves. We implement and evaluate GPU-accelerated INT8 quantization for KV cache compression, achieving 4$\times$ memory reduction with minimal accuracy degradation. We develop four CUDA kernel variants -- naive, tiled, coarsened, and vectorized -- and benchmark them across realistic workload sizes up to 1 billion elements. Our vectorized kernel achieves up to 1,694$\times$ speedup over CPU baselines while maintaining reconstruction error below 0.004 and attention score error below 0.1 even for 8K-dimensional heads. These results demonstrate that INT8 quantization provides a practical approach for reducing memory pressure in LLM inference with negligible computational overhead (6--58ms) and minimal impact on downstream model behavior

</details>


### [130] [Excess Description Length of Learning Generalizable Predictors](https://arxiv.org/abs/2601.04728)
*Elizabeth Donoway,Hailey Joren,Fabien Roger,Jan Leike*

Main category: cs.LG

TL;DR: The paper introduces Excess Description Length (EDL), an information-theoretic framework to quantify whether fine-tuning extracts predictive structure from training data (elicits latent capabilities) or teaches new capabilities.


<details>
  <summary>Details</summary>
Motivation: To understand whether fine-tuning elicits latent capabilities already present in pre-trained models or teaches entirely new capabilities, which is crucial for language model evaluation and safety.

Method: Develops a formal information-theoretic framework using Excess Description Length (EDL) based on prequential coding, measuring the gap between encoding costs using an evolving online-trained model versus the final trained model.

Result: EDL is non-negative in expectation, converges to surplus description length in infinite-data limit, and provides bounds on expected generalization gain. Through toy models, clarifies confusions about information in learning including why random labels yield near-zero EDL and how single examples can eliminate uncertainty.

Conclusion: The framework provides rigorous foundations for distinguishing capability elicitation from teaching, showing they exhibit qualitatively distinct scaling signatures, which has important implications for model evaluation and safety.

Abstract: Understanding whether fine-tuning elicits latent capabilities or teaches new ones is a fundamental question for language model evaluation and safety. We develop a formal information-theoretic framework for quantifying how much predictive structure fine-tuning extracts from the train dataset and writes into a model's parameters. Our central quantity, Excess Description Length (EDL), is defined via prequential coding and measures the gap between the bits required to encode training labels sequentially using an evolving model (trained online) and the residual encoding cost under the final trained model. We establish that EDL is non-negative in expectation, converges to surplus description length in the infinite-data limit, and provides bounds on expected generalization gain. Through a series of toy models, we clarify common confusions about information in learning: why random labels yield EDL near zero, how a single example can eliminate many bits of uncertainty about the underlying rule(s) that describe the data distribution, why structure learned on rare inputs contributes proportionally little to expected generalization, and how format learning creates early transients distinct from capability acquisition. This framework provides rigorous foundations for the empirical observation that capability elicitation and teaching exhibit qualitatively distinct scaling signatures.

</details>


### [131] [Fast Mining and Dynamic Time-to-Event Prediction over Multi-sensor Data Streams](https://arxiv.org/abs/2601.04741)
*Kota Nakamura,Koki Kawabata,Yasuko Matsubara,Yasushi Sakurai*

Main category: cs.LG

TL;DR: TimeCast is a dynamic framework for real-time prediction of machine failures from multi-sensor data streams that adapts to evolving patterns and scales linearly for online updates.


<details>
  <summary>Details</summary>
Motivation: Real-world sensor data streams from machines are dynamic with evolving patterns over time, requiring adaptive methods to continuously predict machine failures accurately in real-time.

Method: TimeCast identifies distinct time-evolving patterns (stages) in data streams, learns individual models for each stage, and adapts predictions based on pattern shifts. It captures time-varying interdependencies between multiple sensors and enables online model updates.

Result: Extensive experiments on real datasets show TimeCast provides higher prediction accuracy than state-of-the-art methods while detecting dynamic changes in data streams with significantly reduced computational time.

Conclusion: TimeCast is an effective dynamic prediction framework that adapts to evolving patterns in sensor data streams, offering improved accuracy and computational efficiency for real-time machine failure prediction.

Abstract: Given real-time sensor data streams obtained from machines, how can we continuously predict when a machine failure will occur? This work aims to continuously forecast the timing of future events by analyzing multi-sensor data streams. A key characteristic of real-world data streams is their dynamic nature, where the underlying patterns evolve over time. To address this, we present TimeCast, a dynamic prediction framework designed to adapt to these changes and provide accurate, real-time predictions of future event time. Our proposed method has the following properties: (a) Dynamic: it identifies the distinct time-evolving patterns (i.e., stages) and learns individual models for each, enabling us to make adaptive predictions based on pattern shifts. (b) Practical: it finds meaningful stages that capture time-varying interdependencies between multiple sensors and improve prediction performance; (c) Scalable: our algorithm scales linearly with the input size and enables online model updates on data streams. Extensive experiments on real datasets demonstrate that TimeCast provides higher prediction accuracy than state-of-the-art methods while finding dynamic changes in data streams with a great reduction in computational time.

</details>


### [132] [Intraday spatiotemporal PV power prediction at national scale using satellite-based solar forecast models](https://arxiv.org/abs/2601.04751)
*Luca Lanzilao,Angela Meyer*

Main category: cs.LG

TL;DR: First national-scale evaluation of spatiotemporal PV power forecasting models showing satellite-based approaches outperform numerical weather prediction, with SolarSTEPS and SHADECast providing best accuracy and uncertainty calibration.


<details>
  <summary>Details</summary>
Motivation: There's a need to evaluate and compare different PV power forecasting approaches at national scale to understand their reliability, sharpness, and operational potential for grid integration of solar energy.

Method: Developed a novel framework evaluating 7 intraday PV nowcasting models including satellite-based deep learning, optical-flow approaches, and physics-based NWP models. Validated against satellite-derived SSI, then converted to PV power using station-specific ML models for 6434 PV stations across Switzerland.

Result: Satellite-based approaches outperform IFS-ENS, especially at short lead times. SolarSTEPS and SHADECast deliver most accurate SSI and PV power predictions. SHADECast provides most reliable ensemble spread. Forecast skill decreases with elevation. National-scale daily total PV generation forecasts achieve <10% relative error for 82% of days.

Conclusion: Satellite-based PV forecasting models demonstrate robustness and operational potential for national-scale grid management, with SolarSTEPS and SHADECast showing best performance in accuracy and uncertainty calibration.

Abstract: We present a novel framework for spatiotemporal photovoltaic (PV) power forecasting and use it to evaluate the reliability, sharpness, and overall performance of seven intraday PV power nowcasting models. The model suite includes satellite-based deep learning and optical-flow approaches and physics-based numerical weather prediction models, covering both deterministic and probabilistic formulations. Forecasts are first validated against satellite-derived surface solar irradiance (SSI). Irradiance fields are then converted into PV power using station-specific machine learning models, enabling comparison with production data from 6434 PV stations across Switzerland. To our knowledge, this is the first study to investigate spatiotemporal PV forecasting at a national scale. We additionally provide the first visualizations of how mesoscale cloud systems shape national PV production on hourly and sub-hourly timescales. Our results show that satellite-based approaches outperform the Integrated Forecast System (IFS-ENS), particularly at short lead times. Among them, SolarSTEPS and SHADECast deliver the most accurate SSI and PV power predictions, with SHADECast providing the most reliable ensemble spread. The deterministic model IrradianceNet achieves the lowest root mean square error, while probabilistic forecasts of SolarSTEPS and SHADECast provide better-calibrated uncertainty. Forecast skill generally decreases with elevation. At a national scale, satellite-based models forecast the daily total PV generation with relative errors below 10% for 82% of the days in 2019-2020, demonstrating robustness and their potential for operational use.

</details>


### [133] [Smart IoT-Based Wearable Device for Detection and Monitoring of Common Cow Diseases Using a Novel Machine Learning Technique](https://arxiv.org/abs/2601.04761)
*Rupsa Rani Mishra,D. Chandrasekhar Rao,Ajaya Kumar Tripathy*

Main category: cs.LG

TL;DR: IoT-enabled cyber-physical system with novel ML algorithm for automated multi-disease detection in cows using physiological and behavioral data.


<details>
  <summary>Details</summary>
Motivation: Manual cow health monitoring is labor-intensive, time-consuming, inaccurate, and expensive in large-scale farming. Human observation delays disease detection and reduces accuracy, affecting animal health and farm productivity. Current automated systems rarely detect multiple diseases with high accuracy.

Method: Proposes an IoT-enabled Cyber-Physical System framework for cow health monitoring. Develops a novel Machine Learning algorithm that analyzes comprehensive physiological and behavioral features to predict multiple common cow diseases.

Result: The system enables accurate and efficient health assessment by automating disease detection through IoT sensors and ML analysis, addressing limitations of manual monitoring.

Conclusion: An automated, low-cost, reliable smart system using IoT and ML can effectively overcome challenges of manual cow health monitoring, providing timely multi-disease detection with improved accuracy and reduced operational costs.

Abstract: Manual observation and monitoring of individual cows for disease detection present significant challenges in large-scale farming operations, as the process is labor-intensive, time-consuming, and prone to reduced accuracy. The reliance on human observation often leads to delays in identifying symptoms, as the sheer number of animals can hinder timely attention to each cow. Consequently, the accuracy and precision of disease detection are significantly compromised, potentially affecting animal health and overall farm productivity. Furthermore, organizing and managing human resources for the manual observation and monitoring of cow health is a complex and economically demanding task. It necessitates the involvement of skilled personnel, thereby contributing to elevated farm maintenance costs and operational inefficiencies. Therefore, the development of an automated, low-cost, and reliable smart system is essential to address these challenges effectively. Although several studies have been conducted in this domain, very few have simultaneously considered the detection of multiple common diseases with high prediction accuracy. However, advancements in Internet of Things (IoT), Machine Learning (ML), and Cyber-Physical Systems have enabled the automation of cow health monitoring with enhanced accuracy and reduced operational costs. This study proposes an IoT-enabled Cyber-Physical System framework designed to monitor the daily activities and health status of cow. A novel ML algorithm is proposed for the diagnosis of common cow diseases using collected physiological and behavioral data. The algorithm is designed to predict multiple diseases by analyzing a comprehensive set of recorded physiological and behavioral features, enabling accurate and efficient health assessment.

</details>


### [134] [AgentOCR: Reimagining Agent History via Optical Self-Compression](https://arxiv.org/abs/2601.04786)
*Lang Feng,Fuchao Yang,Feng Chen,Xin Cheng,Haiyang Xu,Zhenglin Wan,Ming Yan,Bo An*

Main category: cs.LG

TL;DR: AgentOCR reduces LLM agent token usage by converting text history to compact visual representations, using segment caching and self-compression to maintain performance while cutting tokens by >50%.


<details>
  <summary>Details</summary>
Motivation: LLM-based agentic systems face practical deployment bottlenecks due to rapidly growing textual histories that inflate token budgets and memory usage during multi-turn interactions.

Method: 1) Represent observation-action history as compact rendered images (visual tokens), 2) Segment optical caching decomposes history into hashable segments with visual cache to eliminate redundant re-rendering, 3) Agentic self-compression where agents emit compression rates and are trained with compression-aware rewards to balance task success and token efficiency.

Result: Preserves over 95% of text-based agent performance while reducing token consumption by >50%, achieves 20x rendering speedup from segment optical caching, and effectively balances task success with token efficiency through self-compression.

Conclusion: AgentOCR provides a scalable framework for LLM agents that maintains performance while substantially improving token and memory efficiency through visual representation, caching, and adaptive compression mechanisms.

Abstract: Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\% of text-based agent performance while substantially reducing token consumption (>50\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.

</details>


### [135] [Neural-Symbolic Integration with Evolvable Policies](https://arxiv.org/abs/2601.04799)
*Marios Thoma,Vassilis Vassiliades,Loizos Michael*

Main category: cs.LG

TL;DR: A framework for learning non-differentiable symbolic policies and neural weights concurrently through evolutionary processes, eliminating need for predefined symbolic knowledge or differentiability.


<details>
  <summary>Details</summary>
Motivation: Existing Neural-Symbolic frameworks require either predefined symbolic policies or differentiable policies, limiting applicability when domain expertise is unavailable or policies are inherently non-differentiable.

Method: Uses evolutionary process where NeSy systems evolve through mutations (symbolic rule additions and neural weight changes) with fitness-based selection. Extends NEUROLOG architecture, adapts Valiant's Evolvability framework, and uses Machine Coaching semantics for mutable symbolic representations. Neural networks trained through abductive reasoning from symbolic component.

Result: NeSy systems starting with empty policies and random neural weights can successfully approximate hidden non-differentiable target policies, achieving median correct performance approaching 100%.

Conclusion: Enables NeSy research in domains where acquisition of symbolic knowledge from experts is challenging or infeasible, representing a step toward more flexible Neural-Symbolic AI systems.

Abstract: Neural-Symbolic (NeSy) Artificial Intelligence has emerged as a promising approach for combining the learning capabilities of neural networks with the interpretable reasoning of symbolic systems. However, existing NeSy frameworks typically require either predefined symbolic policies or policies that are differentiable, limiting their applicability when domain expertise is unavailable or when policies are inherently non-differentiable. We propose a framework that addresses this limitation by enabling the concurrent learning of both non-differentiable symbolic policies and neural network weights through an evolutionary process. Our approach casts NeSy systems as organisms in a population that evolve through mutations (both symbolic rule additions and neural weight changes), with fitness-based selection guiding convergence toward hidden target policies. The framework extends the NEUROLOG architecture to make symbolic policies trainable, adapts Valiant's Evolvability framework to the NeSy context, and employs Machine Coaching semantics for mutable symbolic representations. Neural networks are trained through abductive reasoning from the symbolic component, eliminating differentiability requirements. Through extensive experimentation, we demonstrate that NeSy systems starting with empty policies and random neural weights can successfully approximate hidden non-differentiable target policies, achieving median correct performance approaching 100%. This work represents a step toward enabling NeSy research in domains where the acquisition of symbolic knowledge from experts is challenging or infeasible.

</details>


### [136] [Parallelizing Node-Level Explainability in Graph Neural Networks](https://arxiv.org/abs/2601.04807)
*Oscar Llorente,Jaime Boal,Eugenio F. Sánchez-Úbeda,Antonio Diaz-Cano,Miguel Familiar*

Main category: cs.LG

TL;DR: Parallelizing GNN node-level explainability via graph partitioning for scalability, with memory-aware reconstruction for limited resources.


<details>
  <summary>Details</summary>
Motivation: Node-level explainability in GNNs becomes extremely time-consuming for large graphs, and batching strategies degrade explanation quality, creating a need for scalable solutions.

Method: Graph partitioning to decompose graphs into disjoint subgraphs for parallel computation of explainability, plus dropout-based reconstruction mechanism for memory-limited scenarios.

Result: Experimental results show substantial speedups on real-world datasets, enabling scalable and transparent explainability for large-scale GNN models.

Conclusion: The approach provides efficient parallel computation of node-level explainability in GNNs through graph partitioning, with memory-aware mechanisms for practical deployment.

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable performance in a wide range of tasks, such as node classification, link prediction, and graph classification, by exploiting the structural information in graph-structured data. However, in node classification, computing node-level explainability becomes extremely time-consuming as the size of the graph increases, while batching strategies often degrade explanation quality. This paper introduces a novel approach to parallelizing node-level explainability in GNNs through graph partitioning. By decomposing the graph into disjoint subgraphs, we enable parallel computation of explainability for node neighbors, significantly improving the scalability and efficiency without affecting the correctness of the results, provided sufficient memory is available. For scenarios where memory is limited, we further propose a dropout-based reconstruction mechanism that offers a controllable trade-off between memory usage and explanation fidelity. Experimental results on real-world datasets demonstrate substantial speedups, enabling scalable and transparent explainability for large-scale GNN models.

</details>


### [137] [Rethinking GNNs and Missing Features: Challenges, Evaluation and a Robust Solution](https://arxiv.org/abs/2601.04855)
*Francesco Ferrini,Veronica Lachi,Antonio Longa,Bruno Lepri,Matono Akiyoshi,Andrea Passerini,Xin Liu,Manfred Jaeger*

Main category: cs.LG

TL;DR: This paper addresses limitations in evaluating GNNs for missing node features, introduces new datasets with dense features, moves beyond MCAR missingness, proposes theoretical analysis, and presents GNNmim as an effective baseline.


<details>
  <summary>Details</summary>
Motivation: Existing GNN research on missing node features focuses on unrealistic scenarios: benchmark datasets with high-dimensional but sparse features and MCAR missingness mechanisms. These limitations prevent meaningful comparison of model robustness and don't reflect real-world applications in healthcare and sensor networks.

Method: 1) Theoretically analyze limitations of sparse features; 2) Introduce new synthetic and real-world datasets with dense, semantically meaningful features; 3) Design evaluation protocols with realistic missingness mechanisms beyond MCAR; 4) Provide theoretical background with explicit assumptions; 5) Propose GNNmim, a simple yet effective baseline for node classification with incomplete features.

Result: Theoretical analysis shows high sparsity limits information loss from missingness, making all models appear robust. GNNmim proves competitive with specialized architectures across diverse datasets and missingness regimes in experiments.

Conclusion: The paper provides more realistic evaluation protocols for GNNs with missing features, demonstrates the importance of dense features and realistic missingness mechanisms, and shows that simple baselines like GNNmim can be surprisingly effective compared to specialized architectures.

Abstract: Handling missing node features is a key challenge for deploying Graph Neural Networks (GNNs) in real-world domains such as healthcare and sensor networks. Existing studies mostly address relatively benign scenarios, namely benchmark datasets with (a) high-dimensional but sparse node features and (b) incomplete data generated under Missing Completely At Random (MCAR) mechanisms. For (a), we theoretically prove that high sparsity substantially limits the information loss caused by missingness, making all models appear robust and preventing a meaningful comparison of their performance. To overcome this limitation, we introduce one synthetic and three real-world datasets with dense, semantically meaningful features. For (b), we move beyond MCAR and design evaluation protocols with more realistic missingness mechanisms. Moreover, we provide a theoretical background to state explicit assumptions on the missingness process and analyze their implications for different methods. Building on this analysis, we propose GNNmim, a simple yet effective baseline for node classification with incomplete feature data. Experiments show that GNNmim is competitive with respect to specialized architectures across diverse datasets and missingness regimes.

</details>


### [138] [FibreCastML: An Open Web Platform for Predicting Electrospun Nanofibre Diameter Distributions](https://arxiv.org/abs/2601.04873)
*Elisa Roldan,Kirstie Andrews,Stephen M. Richardson,Reyhaneh Fatahian,Glen Cooper,Rasool Erfani,Tasneem Sabir,Neil D. Reeves*

Main category: cs.LG

TL;DR: FibreCastML is an open-source ML framework that predicts complete fiber diameter distributions (not just mean values) from electrospinning parameters, enabling more reproducible optimization of fibrous scaffolds for biomedical applications.


<details>
  <summary>Details</summary>
Motivation: Existing ML approaches for electrospinning only predict mean fiber diameters, but the full diameter distribution is critical for scaffold performance in tissue engineering, drug delivery, and wound care applications. There's a need for distribution-aware prediction models.

Method: Curated meta-dataset of 68,538 fiber diameter measurements from 1,778 studies across 16 biomedical polymers. Used 6 standard processing parameters to train 7 ML models with nested cross-validation (leave-one-study-out external folds). Achieved interpretability through variable importance analysis, SHAP, correlation matrices, and 3D parameter maps.

Result: Non-linear models outperformed linear baselines, achieving R² > 0.91 for several widely used polymers. Solution concentration was identified as the dominant global driver of fiber diameter distributions. Experimental validation showed close agreement between predicted and measured distributions across different electrospinning systems.

Conclusion: FibreCastML enables more reproducible and data-driven optimization of electrospun scaffold architectures by predicting complete fiber diameter spectra from routine processing parameters, providing interpretable insights into process-structure relationships.

Abstract: Electrospinning is a scalable technique for producing fibrous scaffolds with tunable micro- and nanoscale architectures for applications in tissue engineering, drug delivery, and wound care. While machine learning (ML) has been used to support electrospinning process optimisation, most existing approaches predict only mean fibre diameters, neglecting the full diameter distribution that governs scaffold performance. This work presents FibreCastML, an open, distribution-aware ML framework that predicts complete fibre diameter spectra from routinely reported electrospinning parameters and provides interpretable insights into process structure relationships.
  A meta-dataset comprising 68538 individual fibre diameter measurements extracted from 1778 studies across 16 biomedical polymers was curated. Six standard processing parameters, namely solution concentration, applied voltage, flow rate, tip to collector distance, needle diameter, and collector rotation speed, were used to train seven ML models using nested cross validation with leave one study out external folds. Model interpretability was achieved using variable importance analysis, SHapley Additive exPlanations, correlation matrices, and three dimensional parameter maps.
  Non linear models consistently outperformed linear baselines, achieving coefficients of determination above 0.91 for several widely used polymers. Solution concentration emerged as the dominant global driver of fibre diameter distributions. Experimental validation across different electrospinning systems demonstrated close agreement between predicted and measured distributions. FibreCastML enables more reproducible and data driven optimisation of electrospun scaffold architectures.

</details>


### [139] [Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers](https://arxiv.org/abs/2601.04890)
*Maksim Velikanov,Ilyas Chahed,Jingwei Zuo,Dhia Eddine Rhaiem,Younes Belkada,Hakim Hacid*

Main category: cs.LG

TL;DR: Learnable scalar and per-row/column multipliers improve weight scale optimization in LLM pretraining by overcoming harmful WD-noise equilibrium norms, outperforming muP and reducing tuning overhead.


<details>
  <summary>Details</summary>
Motivation: Standard weight decay creates a WD-noise equilibrium norm that's a harmful artifact of training, constraining optimal weight scaling. The paper aims to free weight scales from this suboptimal equilibrium by introducing learnable multipliers.

Method: Introduces learnable multipliers: 1) scalar multiplier attached to weight matrix W, 2) per-row and per-column multipliers to free individual row/column norms. This generalizes muP multipliers with more expressiveness while reducing computational tuning overhead.

Result: The learned scale adapts to data and improves performance over muP baseline. Method works with both Adam and Muon optimizers, showing improvement matching the benefit of switching from Adam to Muon in downstream evaluations.

Conclusion: Learnable multipliers effectively overcome harmful WD-noise equilibrium norms, provide better weight scaling optimization, reduce tuning overhead, and raise practical questions about forward-pass symmetries and width-scaling of learned multipliers.

Abstract: Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.

</details>


### [140] [Distributed Online Convex Optimization with Efficient Communication: Improved Algorithm and Lower bounds](https://arxiv.org/abs/2601.04907)
*Sifan Yang,Wenhao Yang,Wei Jiang,Lijun Zhang*

Main category: cs.LG

TL;DR: The paper proposes a new algorithm for distributed online convex optimization with compressed communication that achieves significantly improved regret bounds compared to prior work, with better dependence on compression quality factor ω and number of learners n.


<details>
  <summary>Details</summary>
Motivation: Prior work on distributed online convex optimization with compressed communication suffers from quadratic/quartic dependence on compression quality factor ω^{-1} and super-linear dependence on number of learners n, which is undesirable for practical applications.

Method: Proposes a novel algorithm with a two-level blocking update framework incorporating two key components: an online gossip strategy and an error compensation scheme, which work together to achieve better consensus among learners. Also extends to bandit feedback scenario using classic gradient estimators.

Result: Achieves improved regret bounds of Õ(ω^{-1/2}ρ^{-1}n√T) for convex functions and Õ(ω^{-1}ρ^{-2}n ln T) for strongly convex functions, significantly better than prior bounds. Establishes first lower bounds for the problem, showing optimality with respect to ω and T.

Conclusion: The proposed algorithm overcomes limitations of prior work by achieving better dependence on compression quality and network size, with theoretical guarantees of optimality. The approach also extends to bandit feedback scenarios, enhancing existing regret bounds.

Abstract: We investigate distributed online convex optimization with compressed communication, where $n$ learners connected by a network collaboratively minimize a sequence of global loss functions using only local information and compressed data from neighbors. Prior work has established regret bounds of $O(\max\{ω^{-2}ρ^{-4}n^{1/2},ω^{-4}ρ^{-8}\}n\sqrt{T})$ and $O(\max\{ω^{-2}ρ^{-4}n^{1/2},ω^{-4}ρ^{-8}\}n\ln{T})$ for convex and strongly convex functions, respectively, where $ω\in(0,1]$ is the compression quality factor ($ω=1$ means no compression) and $ρ<1$ is the spectral gap of the communication matrix. However, these regret bounds suffer from a \emph{quadratic} or even \emph{quartic} dependence on $ω^{-1}$. Moreover, the \emph{super-linear} dependence on $n$ is also undesirable. To overcome these limitations, we propose a novel algorithm that achieves improved regret bounds of $\tilde{O}(ω^{-1/2}ρ^{-1}n\sqrt{T})$ and $\tilde{O}(ω^{-1}ρ^{-2}n\ln{T})$ for convex and strongly convex functions, respectively. The primary idea is to design a \emph{two-level blocking update framework} incorporating two novel ingredients: an online gossip strategy and an error compensation scheme, which collaborate to \emph{achieve a better consensus} among learners. Furthermore, we establish the first lower bounds for this problem, justifying the optimality of our results with respect to both $ω$ and $T$. Additionally, we consider the bandit feedback scenario, and extend our method with the classic gradient estimators to enhance existing regret bounds.

</details>


### [141] [Cardinality augmented loss functions](https://arxiv.org/abs/2601.04941)
*Miguel O'Malley*

Main category: cs.LG

TL;DR: Cardinality augmented loss functions derived from mathematical invariants (magnitude and spread) address class imbalance in neural network training by evaluating effective diversity of metric spaces.


<details>
  <summary>Details</summary>
Motivation: Class imbalance is a common problem in neural network training where majority classes dominate, skewing classifier performance toward majority outcomes and reducing effectiveness on minority classes.

Method: Introduce cardinality augmented loss functions based on mathematical invariants (magnitude and spread) that evaluate the 'effective diversity' of metric spaces. Establish methodology for applying these loss functions in neural network training.

Result: Significant performance improvement among minority classes and overall performance metrics on both artificially imbalanced datasets and a real-world imbalanced material science dataset.

Conclusion: Cardinality augmented loss functions derived from mathematical invariants provide an effective solution to class imbalance by leveraging concepts of effective diversity, improving performance on minority classes while maintaining overall classification quality.

Abstract: Class imbalance is a common and pernicious issue for the training of neural networks. Often, an imbalanced majority class can dominate training to skew classifier performance towards the majority outcome. To address this problem we introduce cardinality augmented loss functions, derived from cardinality-like invariants in modern mathematics literature such as magnitude and the spread. These invariants enrich the concept of cardinality by evaluating the `effective diversity' of a metric space, and as such represent a natural solution to overly homogeneous training data. In this work, we establish a methodology for applying cardinality augmented loss functions in the training of neural networks and report results on both artificially imbalanced datasets as well as a real-world imbalanced material science dataset. We observe significant performance improvement among minority classes, as well as improvement in overall performance metrics.

</details>


### [142] [Precision over Diversity: High-Precision Reward Generalizes to Robust Instruction Following](https://arxiv.org/abs/2601.04954)
*Yirong Zeng,Yufei Liu,Xiao Ding,Yutai Hou,Yuxian Wang,Haonan Song,Wu Ning,Dandan Tu,Qixun Zhang,Bibo Cai,Yuxiang He,Ting Liu*

Main category: cs.LG

TL;DR: Training on high-precision hard constraints alone outperforms mixed datasets for instruction following, challenging the belief that constraint diversity is essential for generalization.


<details>
  <summary>Details</summary>
Motivation: Challenge the prevailing consensus that diverse mixtures of verifiable hard and unverifiable soft constraints are essential for generalizing to unseen instructions in reinforcement learning for instruction following tasks.

Method: Systematic empirical investigation comparing models trained on hard-only constraints vs. mixed datasets, analysis of reward precision, LLM judge limitations, attention mechanism analysis, and proposing a data-centric refinement strategy prioritizing reward precision.

Result: Hard-only constraint training consistently outperforms mixed datasets; reward precision (not constraint diversity) is primary driver of effective alignment; LLM judge has low recall for false responses causing reward hacking; high-precision rewards develop transferable meta-skill; proposed approach outperforms baselines by 13.4% with 58% reduction in training time.

Conclusion: Advocates for paradigm shift from indiscriminate pursuit of data diversity toward prioritizing high-precision rewards for effective instruction following alignment, with strong generalization beyond instruction following tasks.

Abstract: A central belief in scaling reinforcement learning with verifiable rewards for instruction following (IF) tasks is that, a diverse mixture of verifiable hard and unverifiable soft constraints is essential for generalizing to unseen instructions. In this work, we challenge this prevailing consensus through a systematic empirical investigation. Counter-intuitively, we find that models trained on hard-only constraints consistently outperform those trained on mixed datasets. Extensive experiments reveal that reward precision, rather than constraint diversity, is the primary driver of effective alignment. The LLM judge suffers from a low recall rate in detecting false response, which leads to severe reward hacking, thereby undermining the benefits of diversity. Furthermore, analysis of the attention mechanism reveals that high-precision rewards develop a transferable meta-skill for IF. Motivated by these insights, we propose a simple yet effective data-centric refinement strategy that prioritizes reward precision. Evaluated on five benchmarks, our approach outperforms competitive baselines by 13.4\% in performance while achieving a 58\% reduction in training time, maintaining strong generalization beyond instruction following. Our findings advocate for a paradigm shift: moving away from the indiscriminate pursuit of data diversity toward high-precision rewards.

</details>


### [143] [On the Definition and Detection of Cherry-Picking in Counterfactual Explanations](https://arxiv.org/abs/2601.04977)
*James Hinns,Sofie Goethals,Stephan Van der Veeken,Theodoros Evgeniou,David Martens*

Main category: cs.LG

TL;DR: Counterfactual explanations can be cherry-picked by providers to highlight favorable model behavior while hiding problematic aspects, and detection of such manipulation is extremely limited even with full access to the explanation process.


<details>
  <summary>Details</summary>
Motivation: Counterfactual explanations are widely used but providers can cherry-pick favorable examples while hiding problematic behavior, creating a need to understand and detect such manipulation.

Method: Formally define cherry-picking in terms of admissible explanation space and utility function. Study detection capabilities across three access levels: full procedural access, partial procedural access, and explanation-only access. Empirical evaluation using standard counterfactual quality metrics.

Result: Detection of cherry-picking is extremely limited in practice. Even with full procedural access, cherry-picked explanations can remain difficult to distinguish from non-cherry-picked ones due to multiplicity of valid counterfactuals and flexibility in explanation specification. Empirically, variability often exceeds cherry-picking effects on standard metrics.

Conclusion: Safeguards should prioritize reproducibility, standardization, and procedural constraints over post-hoc detection. Recommendations provided for algorithm developers, explanation providers, and auditors.

Abstract: Counterfactual explanations are widely used to communicate how inputs must change for a model to alter its prediction. For a single instance, many valid counterfactuals can exist, which leaves open the possibility for an explanation provider to cherry-pick explanations that better suit a narrative of their choice, highlighting favourable behaviour and withholding examples that reveal problematic behaviour. We formally define cherry-picking for counterfactual explanations in terms of an admissible explanation space, specified by the generation procedure, and a utility function. We then study to what extent an external auditor can detect such manipulation. Considering three levels of access to the explanation process: full procedural access, partial procedural access, and explanation-only access, we show that detection is extremely limited in practice. Even with full procedural access, cherry-picked explanations can remain difficult to distinguish from non cherry-picked explanations, because the multiplicity of valid counterfactuals and flexibility in the explanation specification provide sufficient degrees of freedom to mask deliberate selection. Empirically, we demonstrate that this variability often exceeds the effect of cherry-picking on standard counterfactual quality metrics such as proximity, plausibility, and sparsity, making cherry-picked explanations statistically indistinguishable from baseline explanations. We argue that safeguards should therefore prioritise reproducibility, standardisation, and procedural constraints over post-hoc detection, and we provide recommendations for algorithm developers, explanation providers, and auditors.

</details>


### [144] [On the Hidden Objective Biases of Group-based Reinforcement Learning](https://arxiv.org/abs/2601.05002)
*Aleksandar Fontana,Marco Simoni,Giulio Rossolini,Andrea Saracino,Paolo Mori*

Main category: cs.LG

TL;DR: Theoretical analysis reveals structural mismatches in GRPO-style group-based RL methods, identifying gradient biases, reward scaling insensitivity, and momentum-induced clipping violations as fundamental limitations.


<details>
  <summary>Details</summary>
Motivation: Despite empirical success of group-based RL methods like GRPO for post-training LLMs, there are structural mismatches between reward optimization and training objectives that need theoretical understanding.

Method: Theoretical analysis of GRPO-style methods through a unified surrogate formulation, examining properties like non-uniform group weighting, AdamW optimizer interactions, and momentum effects.

Result: Identified three key issues: (1) non-uniform group weighting causes systematic gradient biases on shared prefix tokens, (2) AdamW interactions make training insensitive to reward scaling, (3) optimizer momentum can push policy updates beyond intended clipping regions.

Conclusion: These findings highlight fundamental limitations of current group-based RL approaches and provide principled guidance for designing improved formulations in the future.

Abstract: Group-based reinforcement learning methods, like Group Relative Policy Optimization (GRPO), are widely used nowadays to post-train large language models. Despite their empirical success, they exhibit structural mismatches between reward optimization and the underlying training objective. In this paper, we present a theoretical analysis of GRPO style methods by studying them within a unified surrogate formulation. This perspective reveals recurring properties that affect all the methods under analysis: (i) non-uniform group weighting induces systematic gradient biases on shared prefix tokens; (ii) interactions with the AdamW optimizer make training dynamics largely insensitive to reward scaling; and (iii) optimizer momentum can push policy updates beyond the intended clipping region under repeated optimization steps. We believe that these findings highlight fundamental limitations of current approaches and provide principled guidance for the design of future formulations.

</details>


### [145] [HMVI: Unifying Heterogeneous Attributes with Natural Neighbors for Missing Value Inference](https://arxiv.org/abs/2601.05017)
*Xiaopeng Luo,Zexi Tan,Zhuowei Wang*

Main category: cs.LG

TL;DR: A novel imputation method that models cross-type feature dependencies between numerical and categorical attributes in a unified framework, outperforming existing techniques and improving downstream ML tasks.


<details>
  <summary>Details</summary>
Motivation: Current imputation methods handle numerical and categorical attributes independently, overlooking critical interdependencies among heterogeneous features, which limits their effectiveness in real-world systems with missing data.

Method: Proposes a unified framework that explicitly models cross-type feature dependencies, leveraging both complete and incomplete instances to ensure accurate and consistent imputation in tabular data.

Result: Extensive experiments demonstrate superior performance over existing imputation techniques and significant enhancement of downstream machine learning tasks.

Conclusion: The proposed approach provides a robust solution for real-world systems with missing data by effectively modeling feature dependencies across different data types.

Abstract: Missing value imputation is a fundamental challenge in machine intelligence, heavily dependent on data completeness. Current imputation methods often handle numerical and categorical attributes independently, overlooking critical interdependencies among heterogeneous features. To address these limitations, we propose a novel imputation approach that explicitly models cross-type feature dependencies within a unified framework. Our method leverages both complete and incomplete instances to ensure accurate and consistent imputation in tabular data. Extensive experimental results demonstrate that the proposed approach achieves superior performance over existing techniques and significantly enhances downstream machine learning tasks, providing a robust solution for real-world systems with missing data.

</details>


### [146] [Approximate equivariance via projection-based regularisation](https://arxiv.org/abs/2601.05028)
*Torben Berndt,Jan Stühmer*

Main category: cs.LG

TL;DR: A projection-based regularizer for approximate equivariance that outperforms sample-based methods in efficiency and performance by penalizing non-equivariance at operator level across full group orbits.


<details>
  <summary>Details</summary>
Motivation: Non-equivariant models have regained attention due to better runtime performance and handling of imperfect symmetries in real-world applications, motivating the need for approximately equivariant models that balance symmetry respect with data fitting.

Method: Uses a projection-based regularizer that leverages orthogonal decomposition of linear layers into equivariant and non-equivariant components, penalizing non-equivariance at operator level across full group orbits rather than point-wise, with efficient computation in both spatial and spectral domains.

Result: The method consistently outperforms prior approximate equivariance approaches in both model performance and efficiency, achieving substantial runtime gains over sample-based regularizers.

Conclusion: Projection-based regularization provides a superior approach to approximate equivariance compared to sample-based methods, offering better performance and efficiency by addressing non-equivariance at the operator level across complete group orbits.

Abstract: Equivariance is a powerful inductive bias in neural networks, improving generalisation and physical consistency. Recently, however, non-equivariant models have regained attention, due to their better runtime performance and imperfect symmetries that might arise in real-world applications. This has motivated the development of approximately equivariant models that strike a middle ground between respecting symmetries and fitting the data distribution. Existing approaches in this field usually apply sample-based regularisers which depend on data augmentation at training time, incurring a high sample complexity, in particular for continuous groups such as $SO(3)$. This work instead approaches approximate equivariance via a projection-based regulariser which leverages the orthogonal decomposition of linear layers into equivariant and non-equivariant components. In contrast to existing methods, this penalises non-equivariance at an operator level across the full group orbit, rather than point-wise. We present a mathematical framework for computing the non-equivariance penalty exactly and efficiently in both the spatial and spectral domain. In our experiments, our method consistently outperforms prior approximate equivariance approaches in both model performance and efficiency, achieving substantial runtime gains over sample-based regularisers.

</details>


### [147] [A Data-Driven Predictive Framework for Inventory Optimization Using Context-Augmented Machine Learning Models](https://arxiv.org/abs/2601.05033)
*Anees Fatima,Mohammad Abdus Salam*

Main category: cs.LG

TL;DR: This paper investigates machine learning algorithms for demand forecasting in retail and vending machine sectors, finding that XGBoost with external factors achieves the best performance.


<details>
  <summary>Details</summary>
Motivation: Conventional demand forecasting approaches in supply chain management often neglect external influences like weather, festivities, and equipment breakdowns, leading to inefficiencies in inventory optimization and waste reduction.

Method: The research uses four ML algorithms (XGBoost, ARIMA, Facebook Prophet, and SVR) to forecast inventory requirements, systematically incorporating external factors like weekdays, holidays, and sales deviation indicators to enhance precision.

Result: XGBoost outperformed other models with the lowest Mean Absolute Error (MAE) of 22.7 when external variables were included. ARIMAX and Facebook Prophet showed significant improvements, while SVR performed poorly.

Conclusion: Incorporating external factors significantly improves demand forecasting accuracy, with XGBoost identified as the most efficient algorithm. The study provides a robust framework for enhancing inventory management in retail and vending machine systems.

Abstract: Demand forecasting in supply chain management (SCM) is critical for optimizing inventory, reducing waste, and improving customer satisfaction. Conventional approaches frequently neglect external influences like weather, festivities, and equipment breakdowns, resulting in inefficiencies. This research investigates the use of machine learning (ML) algorithms to improve demand prediction in retail and vending machine sectors. Four machine learning algorithms. Extreme Gradient Boosting (XGBoost), Autoregressive Integrated Moving Average (ARIMA), Facebook Prophet (Fb Prophet), and Support Vector Regression (SVR) were used to forecast inventory requirements. Ex-ternal factors like weekdays, holidays, and sales deviation indicators were methodically incorporated to enhance precision. XGBoost surpassed other models, reaching the lowest Mean Absolute Error (MAE) of 22.7 with the inclusion of external variables. ARIMAX and Fb Prophet demonstrated noteworthy enhancements, whereas SVR fell short in performance. Incorporating external factors greatly improves the precision of demand forecasting models, and XGBoost is identified as the most efficient algorithm. This study offers a strong framework for enhancing inventory management in retail and vending machine systems.

</details>


### [148] [DeepWeightFlow: Re-Basined Flow Matching for Generating Neural Network Weights](https://arxiv.org/abs/2601.05052)
*Saumya Gupta,Scott Biggs,Moritz Laber,Zohair Shafi,Robin Walters,Ayan Paul*

Main category: cs.LG

TL;DR: DeepWeightFlow is a Flow Matching model that generates diverse, high-accuracy neural network weights directly in weight space without requiring fine-tuning, handling various architectures and scaling to large networks.


<details>
  <summary>Details</summary>
Motivation: Existing generative models for neural network weights face challenges with high-dimensional weight spaces and symmetries. Some only generate partial weights for larger models, while others that generate complete weights struggle with speed or require fine-tuning.

Method: Uses Flow Matching model operating directly in weight space, applies Git Re-Basin and TransFusion for neural network canonicalization to handle permutation symmetries and improve generation efficiency for larger models.

Result: Generates diverse and high-accuracy neural network weights that don't require fine-tuning, scales to large networks, excels at transfer learning, and can generate ensembles of hundreds of networks in minutes (much faster than diffusion-based methods).

Conclusion: DeepWeightFlow paves the way for more efficient and scalable generation of diverse sets of neural networks, addressing key limitations of prior approaches.

Abstract: Building efficient and effective generative models for neural network weights has been a research focus of significant interest that faces challenges posed by the high-dimensional weight spaces of modern neural networks and their symmetries. Several prior generative models are limited to generating partial neural network weights, particularly for larger models, such as ResNet and ViT. Those that do generate complete weights struggle with generation speed or require finetuning of the generated models. In this work, we present DeepWeightFlow, a Flow Matching model that operates directly in weight space to generate diverse and high-accuracy neural network weights for a variety of architectures, neural network sizes, and data modalities. The neural networks generated by DeepWeightFlow do not require fine-tuning to perform well and can scale to large networks. We apply Git Re-Basin and TransFusion for neural network canonicalization in the context of generative weight models to account for the impact of neural network permutation symmetries and to improve generation efficiency for larger model sizes. The generated networks excel at transfer learning, and ensembles of hundreds of neural networks can be generated in minutes, far exceeding the efficiency of diffusion-based methods. DeepWeightFlow models pave the way for more efficient and scalable generation of diverse sets of neural networks.

</details>


### [149] [Milestones over Outcome: Unlocking Geometric Reasoning with Sub-Goal Verifiable Reward](https://arxiv.org/abs/2601.05073)
*Jianlong Chen,Daocheng Fu,Shengze Xu,Jiawei Chen,Yuan Feng,Yue Yang,Junchi Yan,Hongyuan Zha,Renqiu Xia*

Main category: cs.LG

TL;DR: The paper introduces GeoGoal benchmark and SGVR framework to improve MLLMs' geometric reasoning by shifting from outcome-based to subgoal-level evaluation and learning, addressing the gap between lucky guesses and rigorous deduction.


<details>
  <summary>Details</summary>
Motivation: Current Multimodal Large Language Models struggle with complex geometric reasoning because they rely on "black box" outcome-based supervision that cannot distinguish between correct answers from lucky guesses versus those from rigorous deductive reasoning.

Method: 1) Construct GeoGoal benchmark using formal verification data engine to convert abstract proofs into verifiable numeric subgoals; 2) Propose Sub-Goal Verifiable Reward (SGVR) framework that replaces sparse outcome signals with dense rewards based on Skeleton Rate (subgoal verification).

Result: SGVR enhances geometric reasoning performance by +9.7%, shows strong generalization to general math (+8.0%) and other general reasoning tasks (+2.8%), demonstrating broad applicability across diverse domains.

Conclusion: The paradigm shift from outcome-based to subgoal-level evaluation and learning addresses fundamental limitations in MLLMs' reasoning capabilities, with SGVR framework showing significant improvements not only in geometry but also generalizing to other reasoning domains.

Abstract: Multimodal Large Language Models (MLLMs) struggle with complex geometric reasoning, largely because "black box" outcome-based supervision fails to distinguish between lucky guesses and rigorous deduction. To address this, we introduce a paradigm shift towards subgoal-level evaluation and learning. We first construct GeoGoal, a benchmark synthesized via a rigorous formal verification data engine, which converts abstract proofs into verifiable numeric subgoals. This structure reveals a critical divergence between reasoning quality and outcome accuracy. Leveraging this, we propose the Sub-Goal Verifiable Reward (SGVR) framework, which replaces sparse signals with dense rewards based on the Skeleton Rate. Experiments demonstrate that SGVR not only enhances geometric performance (+9.7%) but also exhibits strong generalization, transferring gains to general math (+8.0%) and other general reasoning tasks (+2.8%), demonstrating broad applicability across diverse domains.

</details>


### [150] [Exploring Student Expectations and Confidence in Learning Analytics](https://arxiv.org/abs/2601.05082)
*Hayk Asatryan,Basile Tousside,Janis Mohr,Malte Neugebauer,Hildo Bijl,Paul Spiegelberg,Claudia Frohn-Schauf,Jörg Frochte*

Main category: cs.LG

TL;DR: The paper analyzes student expectations and confidence regarding data processing for Learning Analytics using SELAQ questionnaire, identifying four student clusters: Enthusiasts, Realists, Cautious, and Indifferents.


<details>
  <summary>Details</summary>
Motivation: Learning Analytics is widely used in education but must comply with privacy legislation. There's a need to understand student expectations and confidence regarding data processing for LA to address privacy concerns and optimize acceptance.

Method: Used the Student Expectation of Learning Analytics Questionnaire (SELAQ) to analyze expectations and confidence of students from different faculties regarding data processing for Learning Analytics purposes. Applied clustering algorithms to identify student clusters.

Result: Identified four distinct clusters of students: Enthusiasts, Realists, Cautious, and Indifferents. This provides structured insights into varying levels of acceptance and criticism of Learning Analytics among students.

Conclusion: The study provides valuable insights into student perspectives on Learning Analytics data processing, revealing diverse attitudes that can inform better implementation strategies addressing both acceptance and privacy concerns.

Abstract: Learning Analytics (LA) is nowadays ubiquitous in many educational systems, providing the ability to collect and analyze student data in order to understand and optimize learning and the environments in which it occurs. On the other hand, the collection of data requires to comply with the growing demand regarding privacy legislation. In this paper, we use the Student Expectation of Learning Analytics Questionnaire (SELAQ) to analyze the expectations and confidence of students from different faculties regarding the processing of their data for Learning Analytics purposes. This allows us to identify four clusters of students through clustering algorithms: Enthusiasts, Realists, Cautious and Indifferents. This structured analysis provides valuable insights into the acceptance and criticism of Learning Analytics among students.

</details>


### [151] [Sequential Subspace Noise Injection Prevents Accuracy Collapse in Certified Unlearning](https://arxiv.org/abs/2601.05134)
*Polina Dolgova,Sebastian U. Stich*

Main category: cs.LG

TL;DR: Sequential noise scheduling improves certified unlearning by distributing noise across parameter subspaces, maintaining privacy guarantees while significantly boosting model accuracy compared to previous noisy fine-tuning methods.


<details>
  <summary>Details</summary>
Motivation: Current certified unlearning methods based on differential privacy provide strong privacy guarantees but severely degrade model accuracy, making them impractical for real-world use. There's a need for approaches that maintain rigorous privacy certifications while preserving model utility.

Method: Proposes sequential noise scheduling that distributes the noise budget across orthogonal subspaces of the parameter space instead of injecting all noise at once. Extends noisy fine-tuning analysis to subspace setting, proving same (ε,δ) privacy budget is retained.

Result: Empirical results on image classification benchmarks show substantial accuracy improvements after unlearning while remaining robust to membership inference attacks. The approach maintains original certification guarantees while significantly enhancing practical utility.

Conclusion: Certified unlearning can achieve both rigorous privacy guarantees and practical utility through sequential noise scheduling, making differential privacy-based unlearning approaches more viable for real-world applications.

Abstract: Certified unlearning based on differential privacy offers strong guarantees but remains largely impractical: the noisy fine-tuning approaches proposed so far achieve these guarantees but severely reduce model accuracy. We propose sequential noise scheduling, which distributes the noise budget across orthogonal subspaces of the parameter space, rather than injecting it all at once. This simple modification mitigates the destructive effect of noise while preserving the original certification guarantees. We extend the analysis of noisy fine-tuning to the subspace setting, proving that the same $(\varepsilon,δ)$ privacy budget is retained. Empirical results on image classification benchmarks show that our approach substantially improves accuracy after unlearning while remaining robust to membership inference attacks. These results show that certified unlearning can achieve both rigorous guarantees and practical utility.

</details>


### [152] [Safe Continual Reinforcement Learning Methods for Nonstationary Environments. Towards a Survey of the State of the Art](https://arxiv.org/abs/2601.05152)
*Timofey Tomashevskiy*

Main category: cs.LG

TL;DR: A comprehensive survey of continual safe online reinforcement learning (COSRL) methods, covering theoretical aspects, challenges, taxonomy, safety constraint formulations, and future directions for reliable safe online learning algorithms.


<details>
  <summary>Details</summary>
Motivation: To provide a systematic review and analysis of continual safe online reinforcement learning methods, addressing the need for safe RL algorithms that can operate in nonstationary environments while maintaining safety constraints during continual learning and adaptation.

Method: The paper presents a taxonomy-based survey approach, categorizing COSRL methods based on safe learning mechanisms that account for nonstationarity. It analyzes safety constraint formulations for online RL algorithms and organizes the field through systematic classification.

Result: A comprehensive state-of-the-art survey that organizes COSRL methods into a coherent taxonomy, discusses theoretical foundations, identifies key challenges, and provides detailed analysis of safety constraint formulations for online reinforcement learning in nonstationary environments.

Conclusion: The survey establishes a foundation for understanding continual safe online reinforcement learning, highlights open research questions, and discusses prospects for developing reliable, safe online learning algorithms that can adapt to distribution shifts while maintaining safety constraints.

Abstract: This work provides a state-of-the-art survey of continual safe online reinforcement learning (COSRL) methods. We discuss theoretical aspects, challenges, and open questions in building continual online safe reinforcement learning algorithms. We provide the taxonomy and the details of continual online safe reinforcement learning methods based on the type of safe learning mechanism that takes adaptation to nonstationarity into account. We categorize safety constraints formulation for online reinforcement learning algorithms, and finally, we discuss prospects for creating reliable, safe online learning algorithms.
  Keywords: safe RL in nonstationary environments, safe continual reinforcement learning under nonstationarity, HM-MDP, NSMDP, POMDP, safe POMDP, constraints for continual learning, safe continual reinforcement learning review, safe continual reinforcement learning survey, safe continual reinforcement learning, safe online learning under distribution shift, safe continual online adaptation, safe reinforcement learning, safe exploration, safe adaptation, constrained Markov decision processes, safe reinforcement learning, partially observable Markov decision process, safe reinforcement learning and hidden Markov decision processes, Safe Online Reinforcement Learning, safe online reinforcement learning, safe online reinforcement learning, safe meta-learning, safe meta-reinforcement learning, safe context-based reinforcement learning, formulating safety constraints for continual learning

</details>


### [153] [FaST: Efficient and Effective Long-Horizon Forecasting for Large-Scale Spatial-Temporal Graphs via Mixture-of-Experts](https://arxiv.org/abs/2601.05174)
*Yiji Zhao,Zihao Zhong,Ao Wang,Haomin Wen,Ming Jin,Yuxuan Liang,Huaiyu Wan,Hao Wu*

Main category: cs.LG

TL;DR: FaST is an efficient framework using heterogeneity-aware Mixture-of-Experts for long-horizon, large-scale spatial-temporal graph forecasting, enabling week-ahead predictions on large networks with thousands of nodes.


<details>
  <summary>Details</summary>
Motivation: Existing STG forecasting models focus on short-horizon predictions and suffer from high computational costs and memory consumption when scaling to long-horizon predictions and large graphs.

Method: Two key innovations: 1) Adaptive graph agent attention mechanism to reduce computational burden of graph convolution and self-attention on large graphs; 2) Parallel MoE module using Gated Linear Units instead of traditional feed-forward networks for efficient parallel structure.

Result: FaST delivers superior long-horizon predictive accuracy and achieves remarkable computational efficiency compared to state-of-the-art baselines on real-world datasets, enabling week-ahead predictions (672 steps at 15-minute granularity) with thousands of nodes.

Conclusion: FaST provides an effective and efficient solution for long-horizon, large-scale STG forecasting, addressing computational challenges while maintaining high predictive accuracy for practical applications.

Abstract: Spatial-Temporal Graph (STG) forecasting on large-scale networks has garnered significant attention. However, existing models predominantly focus on short-horizon predictions and suffer from notorious computational costs and memory consumption when scaling to long-horizon predictions and large graphs. Targeting the above challenges, we present FaST, an effective and efficient framework based on heterogeneity-aware Mixture-of-Experts (MoEs) for long-horizon and large-scale STG forecasting, which unlocks one-week-ahead (672 steps at a 15-minute granularity) prediction with thousands of nodes. FaST is underpinned by two key innovations. First, an adaptive graph agent attention mechanism is proposed to alleviate the computational burden inherent in conventional graph convolution and self-attention modules when applied to large-scale graphs. Second, we propose a new parallel MoE module that replaces traditional feed-forward networks with Gated Linear Units (GLUs), enabling an efficient and scalable parallel structure. Extensive experiments on real-world datasets demonstrate that FaST not only delivers superior long-horizon predictive accuracy but also achieves remarkable computational efficiency compared to state-of-the-art baselines. Our source code is available at: https://github.com/yijizhao/FaST.

</details>


### [154] [An interpretable data-driven approach to optimizing clinical fall risk assessment](https://arxiv.org/abs/2601.05194)
*Fardin Ganjkhanloo,Emmett Springer,Erik H. Hoyer,Daniel L. Young,Holley Farley,Kimia Ghobadi*

Main category: cs.LG

TL;DR: Data-driven optimization of Johns Hopkins Fall Risk Assessment Tool (JHFRAT) using constrained score optimization improves fall risk prediction while maintaining interpretability and clinical workflow.


<details>
  <summary>Details</summary>
Motivation: To better align fall risk prediction from JHFRAT with clinically meaningful measures while preserving the tool's interpretability and existing clinical thresholds.

Method: Retrospective cohort analysis of 54,209 inpatient admissions, using constrained score optimization (CSO) models to reweight JHFRAT scoring weights while preserving additive structure and clinical thresholds.

Result: CSO model significantly improved predictive performance (AUC-ROC=0.91 vs JHFRAT's 0.86), translating to protecting 35 additional high-risk patients per week. CSO performed similarly with and without EHR variables and showed more robustness than black-box models.

Conclusion: Evidence-based data-driven optimization provides a robust foundation for enhancing inpatient fall prevention protocols while maintaining interpretability and clinical workflow, improving risk assessment and resource allocation.

Abstract: In this study, we aim to better align fall risk prediction from the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically meaningful measures via a data-driven modelling approach. We conducted a retrospective cohort analysis of 54,209 inpatient admissions from three Johns Hopkins Health System hospitals between March 2022 and October 2023. A total of 20,208 admissions were included as high fall risk encounters, and 13,941 were included as low fall risk encounters. To incorporate clinical knowledge and maintain interpretability, we employed constrained score optimization (CSO) models to reweight the JHFRAT scoring weights, while preserving its additive structure and clinical thresholds. Recalibration refers to adjusting item weights so that the resulting score can order encounters more consistently by the study's risk labels, and without changing the tool's form factor or deployment workflow. The model demonstrated significant improvements in predictive performance over the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). This performance improvement translates to protecting an additional 35 high-risk patients per week across the Johns Hopkins Health System. The constrained score optimization models performed similarly with and without the EHR variables. Although the benchmark black-box model (XGBoost), improves upon the performance metrics of the knowledge-based constrained logistic regression (AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk labeling. This evidence-based approach provides a robust foundation for health systems to systematically enhance inpatient fall prevention protocols and patient safety using data-driven optimization techniques, contributing to improved risk assessment and resource allocation in healthcare settings.

</details>


### [155] [EARL: Energy-Aware Optimization of Liquid State Machines for Pervasive AI](https://arxiv.org/abs/2601.05205)
*Zain Iqbal,Lorenzo Valerio*

Main category: cs.LG

TL;DR: EARL is an energy-aware reinforcement learning framework that optimizes Liquid State Machines for on-device AI, achieving higher accuracy, lower energy consumption, and faster optimization than existing methods.


<details>
  <summary>Details</summary>
Motivation: Pervasive AI needs low-latency, energy-efficient on-device learning systems. Liquid State Machines (LSMs) are promising for low-power temporal processing but face deployment challenges due to hyperparameter sensitivity and traditional optimization methods that ignore energy constraints.

Method: EARL integrates Bayesian optimization with adaptive reinforcement learning selection policy to jointly optimize accuracy and energy consumption. It uses surrogate modeling for global exploration, reinforcement learning for dynamic candidate prioritization, and early termination to eliminate redundant evaluations.

Result: On three benchmark datasets, EARL achieves 6-15% higher accuracy, 60-80% lower energy consumption, and up to an order of magnitude reduction in optimization time compared to leading hyperparameter tuning frameworks.

Conclusion: Energy-aware adaptive search effectively improves efficiency and scalability of LSMs for resource-constrained on-device AI applications.

Abstract: Pervasive AI increasingly depends on on-device learning systems that deliver low-latency and energy-efficient computation under strict resource constraints. Liquid State Machines (LSMs) offer a promising approach for low-power temporal processing in pervasive and neuromorphic systems, but their deployment remains challenging due to high hyperparameter sensitivity and the computational cost of traditional optimization methods that ignore energy constraints. This work presents EARL, an energy-aware reinforcement learning framework that integrates Bayesian optimization with an adaptive reinforcement learning based selection policy to jointly optimize accuracy and energy consumption. EARL employs surrogate modeling for global exploration, reinforcement learning for dynamic candidate prioritization, and an early termination mechanism to eliminate redundant evaluations, substantially reducing computational overhead. Experiments on three benchmark datasets demonstrate that EARL achieves 6 to 15 percent higher accuracy, 60 to 80 percent lower energy consumption, and up to an order of magnitude reduction in optimization time compared to leading hyperparameter tuning frameworks. These results highlight the effectiveness of energy-aware adaptive search in improving the efficiency and scalability of LSMs for resource-constrained on-device AI applications.

</details>


### [156] [Robust Reasoning as a Symmetry-Protected Topological Phase](https://arxiv.org/abs/2601.05240)
*Ilmo Sung*

Main category: cs.LG

TL;DR: The paper proposes viewing robust logical inference in LLMs as a symmetry-protected topological phase, analogous to non-Abelian anyon braiding, and demonstrates a topological model that maintains perfect fidelity under noise while Transformers fail.


<details>
  <summary>Details</summary>
Motivation: Large language models suffer from hallucinations and logical inconsistencies due to semantic noise. Current architectures operate in a vulnerable "Metric Phase" where causal order can break spontaneously. The authors seek to establish robust logical reasoning as a topological phenomenon rather than a geometric interpolation problem.

Method: The paper proposes a Holonomic Network architecture that implements robust inference as a Symmetry-Protected Topological phase. This approach replaces fragile geometric interpolation with topological invariants, formally isomorphic to non-Abelian anyon braiding. They demonstrate this through empirical studies showing a sharp topological phase transition and test on a variable-binding task on S10 (3.6×10^6 states) for symbolic manipulation.

Result: The Holonomic Network reveals a macroscopic "mass gap" and maintains invariant fidelity below a critical noise threshold, while Transformers and RNNs exhibit gapless decay. In variable-binding tasks, the topological model maintains perfect fidelity extrapolating 100× beyond training (L=50→5000), showing indefinite causal horizon, whereas Transformers lose logical coherence. Ablation studies confirm protection emerges strictly from non-Abelian gauge symmetry.

Conclusion: The work provides strong evidence for a new universality class for logical reasoning, linking causal stability to the topology of the semantic manifold. This establishes robust inference as a topological phenomenon rather than a geometric interpolation problem, with implications for developing more reliable AI systems.

Abstract: Large language models suffer from "hallucinations"-logical inconsistencies induced by semantic noise. We propose that current architectures operate in a "Metric Phase," where causal order is vulnerable to spontaneous symmetry breaking. Here, we identify robust inference as an effective Symmetry-Protected Topological phase, where logical operations are formally isomorphic to non-Abelian anyon braiding, replacing fragile geometric interpolation with robust topological invariants. Empirically, we demonstrate a sharp topological phase transition: while Transformers and RNNs exhibit gapless decay, our Holonomic Network reveals a macroscopic "mass gap," maintaining invariant fidelity below a critical noise threshold. Furthermore, in a variable-binding task on $S_{10}$ ($3.6 \times 10^6$ states) representing symbolic manipulation, we demonstrate holonomic generalization: the topological model maintains perfect fidelity extrapolating $100\times$ beyond training ($L=50 \to 5000$), consistent with a theoretically indefinite causal horizon, whereas Transformers lose logical coherence. Ablation studies indicate this protection emerges strictly from non-Abelian gauge symmetry. This provides strong evidence for a new universality class for logical reasoning, linking causal stability to the topology of the semantic manifold.

</details>


### [157] [Optimal Lower Bounds for Online Multicalibration](https://arxiv.org/abs/2601.05245)
*Natalie Collina,Jiuyao Lu,Georgy Noarov,Aaron Roth*

Main category: cs.LG

TL;DR: The paper proves tight Ω(T^{2/3}) lower bounds for online multicalibration, establishing an information-theoretic separation from marginal calibration which has O(T^{2/3-ε}) upper bounds.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental limits of online multicalibration and establish whether it's inherently more difficult than marginal calibration. The research aims to determine if the additional complexity of multicalibration (calibrating across multiple overlapping groups) comes with unavoidable computational or information-theoretic costs compared to simpler marginal calibration.

Method: The authors use information-theoretic techniques to prove lower bounds. They consider two settings: (1) general setting where group functions can depend on both context and predictions, proving Ω(T^{2/3}) lower bound using just three disjoint binary groups; (2) more difficult case where group functions depend only on context, proving Ω̃(T^{2/3}) lower bound via Θ(T)-sized group family constructed using orthogonal function systems.

Result: 1. For general setting: Ω(T^{2/3}) lower bound on expected multicalibration error using three disjoint binary groups, matching upper bounds up to logarithmic factors and exceeding O(T^{2/3-ε}) upper bound for marginal calibration. 2. For context-dependent groups: Ω̃(T^{2/3}) lower bound via Θ(T)-sized group family, again matching upper bounds up to logarithmic factors.

Conclusion: Online multicalibration is fundamentally harder than marginal calibration, with tight Ω(T^{2/3}) lower bounds that establish an information-theoretic separation between the two problems. The results show that the additional complexity of multicalibration is unavoidable and not just an artifact of current algorithms.

Abstract: We prove tight lower bounds for online multicalibration, establishing an information-theoretic separation from marginal calibration.
  In the general setting where group functions can depend on both context and the learner's predictions, we prove an $Ω(T^{2/3})$ lower bound on expected multicalibration error using just three disjoint binary groups. This matches the upper bounds of Noarov et al. (2025) up to logarithmic factors and exceeds the $O(T^{2/3-\varepsilon})$ upper bound for marginal calibration (Dagan et al., 2025), thereby separating the two problems.
  We then turn to lower bounds for the more difficult case of group functions that may depend on context but not on the learner's predictions. In this case, we establish an $\widetildeΩ(T^{2/3})$ lower bound for online multicalibration via a $Θ(T)$-sized group family constructed using orthogonal function systems, again matching upper bounds up to logarithmic factors.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [158] [Achievable Rate and Coding Principle for MIMO Multicarrier Systems With Cross-Domain MAMP Receiver Over Doubly Selective Channels](https://arxiv.org/abs/2601.04433)
*Yuhao Chi,Zhiyuan Peng,Lei Liu,Ying Li,Yao Ge,Chau Yuen*

Main category: cs.IT

TL;DR: MS-CD-MAMP receiver enables coded MIMO-OFDM/OTFS/AFDM systems to achieve same maximum achievable rate in doubly selective channels, with near-optimal finite-length performance using optimized LDPC codes.


<details>
  <summary>Details</summary>
Motivation: Existing studies show OTFS and AFDM outperform OFDM in uncoded doubly selective channels, but it's unclear if these benefits extend to coded systems. The information-theoretic limits and low-complexity receiver design for coded MIMO multicarrier systems remain unknown.

Method: Proposes multi-slot cross-domain memory approximate message passing (MS-CD-MAMP) receiver that exploits time domain channel sparsity and symbol domain constellation constraints. Develops simplified SISO variational state evolution to derive achievable rate and optimal coding principle.

Result: Coded MIMO-OFDM/OTFS/AFDM with MS-CD-MAMP achieve same maximum achievable rate in doubly selective channels. Finite-length performance with optimized LDPC codes is only 0.5-1.8 dB from theoretical limit, with 0.8-4.4 dB gain over point-to-point LDPC codes.

Conclusion: MS-CD-MAMP receiver successfully bridges the gap between uncoded and coded MIMO multicarrier systems, providing near-optimal performance with low complexity while establishing information-theoretic limits for these systems.

Abstract: The integration of multicarrier modulation and multiple-input-multiple-output (MIMO) is critical for reliable transmission of wireless signals in complex environments, which significantly improve spectrum efficiency. Existing studies have shown that popular orthogonal time frequency space (OTFS) and affine frequency division multiplexing (AFDM) offer significant advantages over orthogonal frequency division multiplexing (OFDM) in uncoded doubly selective channels. However, it remains uncertain whether these benefits extend to coded systems. Meanwhile, the information-theoretic limit analysis of coded MIMO multicarrier systems and the corresponding low-complexity receiver design remain unclear. To overcome these challenges, this paper proposes a multi-slot cross-domain memory approximate message passing (MS-CD-MAMP) receiver as well as develops its information-theoretic (i.e., achievable rate) limit and optimal coding principle for MIMO-multicarrier modulation (e.g., OFDM, OTFS, and AFDM) systems. The proposed MS-CD-MAMP receiver can exploit not only the time domain channel sparsity for low complexity but also the corresponding symbol domain constellation constraints for performance enhancement. Meanwhile, limited by the high-dimensional complex state evolution (SE), a simplified single-input single-output variational SE is proposed to derive the achievable rate of MS-CD-MAMP and the optimal coding principle with the goal of maximizing the achievable rate. Numerical results show that coded MIMO-OFDM/OTFS/AFDM with MS-CD-MAMP achieve the same maximum achievable rate in doubly selective channels, whose finite-length performance with practical optimized low-density parity-check (LDPC) codes is only 0.5 $\sim$ 1.8 dB away from the associated theoretical limit, and has 0.8 $\sim$ 4.4 dB gain over the well-designed point-to-point LDPC codes.

</details>


### [159] [Bridging Distance and Spectral Positional Encodings via Anchor-Based Diffusion Geometry Approximation](https://arxiv.org/abs/2601.04517)
*Zimo Yan,Zheng Xie,Runfan Duan,Chang Liu,Wumei Du*

Main category: cs.IT

TL;DR: Distance encodings are low-rank approximations of diffusion geometry; explicit trilateration map reconstructs diffusion coordinates from anchor distances with theoretical guarantees; both Laplacian and distance encodings outperform no-encoding baselines in molecular graph learning.


<details>
  <summary>Details</summary>
Motivation: Molecular graph learning needs positional signals for local neighborhoods and global topology. The relationship between spectral encodings (Laplacian/diffusion operators) and anchor-based distance encodings (shortest-path) is poorly understood, limiting our ability to leverage their complementary strengths.

Method: Interpret distance encodings as low-rank surrogate of diffusion geometry. Derive explicit trilateration map that reconstructs truncated diffusion coordinates from transformed anchor distances and anchor spectral positions. Provide theoretical guarantees (pointwise and Frobenius-gap) on random regular graphs. Implement distance-driven Nyström scheme on DrugBank molecular graphs using shared GNP-based DDI prediction backbone.

Result: Distance-driven Nyström scheme closely recovers diffusion geometry. Both Laplacian and distance encodings substantially outperform no-encoding baseline in DDI (drug-drug interaction) prediction on DrugBank molecular graphs.

Conclusion: Distance encodings serve as effective low-rank approximations of diffusion geometry, with explicit reconstruction mapping providing theoretical foundation. Both encoding families significantly enhance molecular graph learning, demonstrating their complementary value for capturing positional information in graph-structured data.

Abstract: Molecular graph learning benefits from positional signals that capture both local neighborhoods and global topology. Two widely used families are spectral encodings derived from Laplacian or diffusion operators and anchor-based distance encodings built from shortest-path information, yet their precise relationship is poorly understood. We interpret distance encodings as a low-rank surrogate of diffusion geometry and derive an explicit trilateration map that reconstructs truncated diffusion coordinates from transformed anchor distances and anchor spectral positions, with pointwise and Frobenius-gap guarantees on random regular graphs. On DrugBank molecular graphs using a shared GNP-based DDI prediction backbone, a distance-driven Nyström scheme closely recovers diffusion geometry, and both Laplacian and distance encodings substantially outperform a no-encoding baseline.

</details>


### [160] [Air-to-Ground Communications for Internet of Things: UAV-based Coverage Hole Detection and Recovery](https://arxiv.org/abs/2601.04665)
*Xiao Fan,Wenkun Wen,Peiran Wu,Junhui Zhao,Minghua Xia*

Main category: cs.IT

TL;DR: UAV-assisted framework for real-time detection and recovery of coverage holes in IoT networks using patrol UAVs for detection and aerial BS UAVs for connectivity restoration, organized via Delaunay triangulation with collision avoidance.


<details>
  <summary>Details</summary>
Motivation: Traditional coverage-hole detection methods (like minimizing drive tests) are costly, time-consuming, rely on outdated data, and unsuitable for real-time applications, especially for IoT networks where terrestrial networks may be constrained or unavailable.

Method: 1) Patrol UAV identifies coverage holes in regions with uncertain terrestrial BS status; 2) Satellite or nearby operational BSs deploy UAVs as aerial BSs to restore connectivity; 3) UAV swarm organized using Delaunay triangulation for scalable deployment; 4) Multi-agent system theory-based collision-avoidance mechanism for safe coordinated motion.

Result: Simulation results show the framework achieves high efficiency in both coverage-hole detection and on-demand connectivity restoration while significantly reducing operational cost and time.

Conclusion: The proposed UAV-assisted framework provides an effective solution for real-time coverage-hole detection and recovery in IoT networks, overcoming limitations of traditional methods through scalable UAV swarm deployment with safety mechanisms.

Abstract: Uncrewed aerial vehicles (UAVs) play a pivotal role in ensuring seamless connectivity for Internet of Things (IoT) devices, particularly in scenarios where conventional terrestrial networks are constrained or temporarily unavailable. However, traditional coverage-hole detection approaches, such as minimizing drive tests, are costly, time-consuming, and reliant on outdated radio-environment data, making them unsuitable for real-time applications. To address these limitations, this paper proposes a UAV-assisted framework for real-time detection and recovery of coverage holes in IoT networks. In the proposed scheme, a patrol UAV is first dispatched to identify coverage holes in regions where the operational status of terrestrial base stations (BSs) is uncertain. Once a coverage hole is detected, one or more UAVs acting as aerial BSs are deployed by a satellite or nearby operational BSs to restore connectivity. The UAV swarm is organized based on Delaunay triangulation, enabling scalable deployment and tractable analytical characterization using stochastic geometry. Moreover, a collision-avoidance mechanism grounded in multi-agent system theory ensures safe and coordinated motion among multiple UAVs. Simulation results demonstrate that the proposed framework achieves high efficiency in both coverage-hole detection and on-demand connectivity restoration while significantly reducing operational cost and time.

</details>


### [161] [Feasibility Study Regarding Self-sustainable Reconfigurable Intelligent Surfaces](https://arxiv.org/abs/2601.04723)
*Zhenyu Li,Ozan Alp Topal,Özlem Tuğfe Demir,Emil Björnson,Cicek Cavdar*

Main category: cs.IT

TL;DR: Self-sustainable RIS (ssRIS) deployment feasibility analysis comparing element-splitting (ES) and time-splitting (TS) harvest-and-reflect schemes, showing TS better for indoor/benign conditions while ES better for challenging outdoor scenarios.


<details>
  <summary>Details</summary>
Motivation: Self-sustainable RIS can be deployed in inaccessible locations without cabling/powering costs, offering novel wireless coverage enhancement. Need to assess feasibility of different harvest-and-reflect schemes for practical deployment.

Method: Analyze two harvest-and-reflect schemes: element-splitting (ES) and time-splitting (TS). Examine how element requirements scale with key parameters: transmit power, data rate demands, outage constraints under both LOS and NLOS ssRIS-to-UE channels. Use analytical and numerical analysis.

Result: TS scheme shows better channel hardening gain with stable element requirements across outage margins, advantageous for indoor deployments with favorable harvesting and moderate data rates. However, TS element requirements scale exponentially with harvesting difficulty and data rate. ES scheme shows only linear growth with harvesting difficulty, providing better feasibility in challenging outdoor scenarios.

Conclusion: TS excels in benign environments prioritizing reliability, while ES is preferable for demanding conditions requiring operational robustness. Different schemes suit different deployment scenarios based on environmental conditions and performance requirements.

Abstract: Without requiring operational costs such as cabling and powering while maintaining reconfigurable phase-shift capability, self-sustainable reconfigurable intelligent surfaces (ssRISs) can be deployed in locations inaccessible to conventional relays or base stations, offering a novel approach to enhance wireless coverage. This study assesses the feasibility of ssRIS deployment by analyzing two harvest-and-reflect (HaR) schemes: element-splitting (ES) and time-splitting (TS). We examine how element requirements scale with key system parameters, transmit power, data rate demands, and outage constraints under both line-of-sight (LOS) and non-line-of-sight (NLOS) ssRIS-to-user equipment (UE) channels. Analytical and numerical results reveal distinct feasibility characteristics. The TS scheme demonstrates better channel hardening gain, maintaining stable element requirements across varying outage margins, making it advantageous for indoor deployments with favorable harvesting conditions and moderate data rates. However, TS exhibits an element requirement that exponentially scales to harvesting difficulty and data rate. Conversely, the ES scheme shows only linear growth with harvesting difficulty, providing better feasibility under challenging outdoor scenarios. These findings establish that TS excels in benign environments, prioritizing reliability, while ES is preferable for demanding conditions requiring operational robustness.

</details>


### [162] [Privacy-Utility Trade-offs Under Multi-Level Point-Wise Leakage Constraints](https://arxiv.org/abs/2601.04815)
*Amirreza Zamani,Parastoo Sadeghi,Mikael Skoglund*

Main category: cs.IT

TL;DR: This paper studies privacy mechanism design where an agent observes useful data Y correlated with private data X, but cannot access X directly. The agent wants to reveal information about Y through disclosed data U while controlling privacy leakage about X using a new multi-level point-wise leakage measure.


<details>
  <summary>Details</summary>
Motivation: Existing privacy mechanisms often use uniform leakage constraints for all data realizations, but real-world scenarios require different privacy levels for different data points. Some data may require perfect privacy (zero leakage) while others can tolerate some leakage. The paper aims to develop a more flexible privacy mechanism that allows different leakage budgets for different data realizations.

Method: The authors introduce a new multi-level point-wise leakage measure that imposes different leakage levels for different realizations of U. For small leakage scenarios, they use information geometry to locally approximate mutual information, leading to a quadratic optimization problem when the leakage matrix P_{X|Y} is invertible. They show that binary U is sufficient for optimal utility, resulting in simple privacy designs based on finding maximum singular values and singular vectors of matrices.

Result: The paper demonstrates that when the leakage matrix is invertible and leakage is sufficiently small, the optimization problem has a closed-form solution under certain constraints. Importantly, they prove that binary U (two possible outputs) is sufficient to achieve optimal utility, which significantly simplifies privacy mechanism design and reduces complexity.

Conclusion: The proposed multi-level point-wise leakage framework provides a flexible approach to privacy mechanism design that accommodates different privacy requirements for different data points. The theoretical results show that simple binary mechanisms can achieve optimal utility-privacy trade-offs, offering practical low-complexity solutions based on singular value decomposition of the leakage matrix.

Abstract: An information-theoretic privacy mechanism design is studied, where an agent observes useful data $Y$ which is correlated with the private data $X$. The agent wants to reveal the information to a user, hence, the agent utilizes a privacy mechanism to produce disclosed data $U$ that can be revealed. We assume that the agent has no direct access to $X$, i.e., the private data is hidden. We study privacy mechanism design that maximizes the disclosed information about $Y$, measured by the mutual information between $Y$ and $U$, while satisfying a point-wise constraint with different privacy leakage budgets. We introduce a new measure, called the \emph{multi-level point-wise leakage}, which allows us to impose different leakage levels for different realizations of $U$. In contrast to previous studies on point-wise measures, which use the same leakage level for each realization, we consider a more general scenario in which each data point can leak information up to a different threshold. As a result, this concept also covers cases in which some data points should not leak any information about the private data, i.e., they must satisfy perfect privacy. In other words, a combination of perfect privacy and non-zero leakage can be considered. When the leakage is sufficiently small, concepts from information geometry allow us to locally approximate the mutual information. We show that when the leakage matrix $P_{X|Y}$ is invertible, utilizing this approximation leads to a quadratic optimization problem that has closed-form solution under some constraints. In particular, we show that it is sufficient to consider only binary $U$ to attain the optimal utility. This leads to simple privacy designs with low complexity which are based on finding the maximum singular value and singular vector of a matrix.

</details>


### [163] [Stability of Constrained Optimization Models for Structured Signal Recovery](https://arxiv.org/abs/2601.04849)
*Yijun Zhong,Yi Shen*

Main category: cs.IT

TL;DR: The paper analyzes three constrained optimization models for structured signal recovery using different structural priors, showing robustness to noise and parameter stability with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Recovering structured signals from measurements is challenging but crucial for applications like imaging restoration, wireless communications, and signal processing. The problem stems from leveraging prior knowledge about signal structure (like sparsity) for effective recovery.

Method: The paper investigates three constrained optimization models that leverage distinct forms of structural priors to regularize the solution space. The approach involves theoretical analysis of these models' properties.

Result: Theoretical analysis demonstrates that the models exhibit robustness to noise while maintaining stability with respect to tuning parameters. The work establishes a tradeoff between sample complexity and mismatch error under mild conditions.

Conclusion: By providing theoretical foundations, the work supports practical use of these models in scenarios where measurement imperfections and model uncertainties are unavoidable, making them suitable for real-world applications.

Abstract: Recovering an unknown but structured signal from its measurements is a challenging problem with significant applications in fields such as imaging restoration, wireless communications, and signal processing. In this paper, we consider the inherent problem stems from the prior knowledge about the signal's structure, such as sparsity which is critical for signal recovery models. We investigate three constrained optimization models that effectively address this challenge, each leveraging distinct forms of structural priors to regularize the solution space. Our theoretical analysis demonstrates that these models exhibit robustness to noise while maintaining stability with respect to tuning parameters that is a crucial property for practical applications, when the parameter selection is often nontrivial. By providing theoretical foundations, our work supports their practical use in scenarios where measurement imperfections and model uncertainties are unavoidable. Furthermore, under mild conditions, we establish tradeoff between the sample complexity and the mismatch error.

</details>


### [164] [Wireless Communication with Cross-Linked Rotatable Antenna Array: Architecture Design and Rotation Optimization](https://arxiv.org/abs/2601.04862)
*Ailing Zheng,Qingqing Wu,Ziyuan Zheng,Qiaoyan Peng,Yanze Zhu,Honghao Wang,Wen Chen,Guoying Zhang*

Main category: cs.IT

TL;DR: Cross-linked rotatable antenna arrays enable coordinated rotation of multiple antennas, reducing hardware cost while achieving performance close to fully flexible antenna orientation.


<details>
  <summary>Details</summary>
Motivation: Traditional rotatable antenna systems have high hardware cost and control complexity proportional to the number of antennas. The paper aims to address this issue with a more cost-effective solution.

Method: Proposes cross-linked rotatable antenna structure enabling coordinated rotation of multiple antennas. Investigates both antenna element-level and panel-level rotation schemes. Formulates sum rate maximization problem with joint optimization of receive beamforming and rotation angles. Uses alternating optimization algorithm with MMSE for beamforming and feasible direction method for rotation angles. Applies genetic algorithm for discrete rotation angle selection.

Result: CL-RA architecture with careful row-column partition achieves performance close to fully flexible antenna orientation. CL antenna element-level scheme outperforms CL antenna panel-level scheme by 25% and delivers 128% improvement over conventional fixed-direction antennas.

Conclusion: Cross-linked rotatable antenna arrays provide a cost-effective solution that significantly improves performance over fixed antennas while approaching the performance of fully flexible antenna orientation systems.

Abstract: Rotatable antenna (RA) technology can harness additional spatial degrees of freedom by enabling the dynamic three-dimensional orientation control of each antenna. Unfortunately, the hardware cost and control complexity of traditional RA systems is proportional to the number of RAs. To address the issue, we consider a cross-linked (CL) RA structure, which enables the coordinated rotation of multiple antennas, thereby offering a cost-effective solution. To evaluate the performance of the CL-RA array, we investigate a CL-RA-aided uplink system. Specifically, we first establish system models for both antenna element-level and antenna panel-level rotation. Then, we formulate a sum rate maximization problem by jointly optimizing the receive beamforming at the base station and the rotation angles. For the antenna element-level rotation, we derive the optimal solution of the CL-RA array under the single-user case. Subsequently, for two rotation schemes, we propose an alternating optimization algorithm to solve the formulated problem in the multi-user case, where the receive beamforming and the antenna rotation angles are obtained by applying the minimum mean square error method and feasible direction method, respectively. In addition, considering the hardware limitations, we apply the genetic algorithm to address the discrete rotation angles selection problem. Simulation results show that by carefully designing the row-column partition scheme, the performance of the CL-RA architecture is quite close to that of the flexible antenna orientation scheme. Moreover, the CL antenna element-level scheme surpasses the CL antenna panel-level scheme by 25% and delivers a 128% performance improvement over conventional fixed-direction antennas.

</details>


### [165] [Learning Sparsifying Transforms for mmWave Communication via $\ell^4$-Norm Maximization](https://arxiv.org/abs/2601.04980)
*Sueda Taner,Christoph Studer*

Main category: cs.IT

TL;DR: This paper proposes learning optimal sparsifying transforms for mmWave massive MIMO channels instead of using the traditional DFT, using ℓ⁴-norm maximization and new learning algorithms to find better transforms than DFT.


<details>
  <summary>Details</summary>
Motivation: While beamspace processing using DFT is promising for reducing complexity in mmWave massive MIMO systems, it's unclear whether DFT is the optimal sparsifying transform for finite-dimensional antenna arrays. The paper aims to find better transforms than DFT for sparsifying channel vectors.

Method: Extends the framework of Zhai et al. for complete dictionary learning via ℓ⁴-norm maximization to the complex case. Proposes two suitable learning algorithms for this complex-valued optimization problem.

Result: The algorithms are used to: (i) assess the optimality of DFT for sparsifying channel vectors theoretically and via simulations, and (ii) learn improved sparsifying transforms for both real-world and synthetically generated channel vectors.

Conclusion: The paper demonstrates that learned sparsifying transforms can outperform the traditional DFT for mmWave massive MIMO systems, potentially leading to better beamspace processing with reduced complexity and power consumption.

Abstract: The high directionality of wave propagation at millimeter-wave (mmWave) carrier frequencies results in only a small number of significant transmission paths between user equipments and the basestation (BS). This sparse nature of wave propagation is revealed in the beamspace domain, which is traditionally obtained by taking the spatial discrete Fourier transform (DFT) across a uniform linear antenna array at the BS, where each DFT output is associated with a distinct beam. In recent years, beamspace processing has emerged as a promising technique to reduce baseband complexity and power consumption in all-digital massive multiuser (MU) multiple-input multiple-output (MIMO) systems operating at mmWave frequencies. However, it remains unclear whether the DFT is the optimal sparsifying transform for finite-dimensional antenna arrays. In this paper, we extend the framework of Zhai et al. for complete dictionary learning via $\ell^4$-norm maximization to the complex case in order to learn new sparsifying transforms. We provide a theoretical foundation for $\ell^4$-norm maximization and propose two suitable learning algorithms. We then utilize these algorithms (i) to assess the optimality of the DFT for sparsifying channel vectors theoretically and via simulations and (ii) to learn improved sparsifying transforms for real-world and synthetically generated channel vectors.

</details>


### [166] [Refinements of Jensen's Inequality for Twice-Differentiable Convex Functions with Bounded Hessian](https://arxiv.org/abs/2601.05030)
*Sambhab Mishra*

Main category: cs.IT

TL;DR: The paper presents refined versions of Jensen's inequality for twice-differentiable functions with bounded Hessians, using Taylor expansions and Gruss-type inequalities to incorporate higher-order moments.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between classical variance-based bounds and higher-precision estimates in Jensen's inequality, and to develop more accurate bounds that incorporate higher-order statistical moments like skewness and kurtosis.

Method: Uses Taylor expansions with integral remainders to establish rigorous refinements of Jensen's inequality for twice-differentiable functions with bounded Hessians. Applies Gruss-type inequalities to derive explicit error terms that incorporate higher-order moments.

Result: Develops new theoretical tools that improve upon existing estimates for Shannon entropy of continuous distributions and ergodic capacity of Rayleigh fading channels, demonstrating practical efficacy of the refinements.

Conclusion: The paper successfully establishes refined versions of Jensen's inequality that incorporate higher-order moments, providing more precise bounds with practical applications in information theory and communications.

Abstract: Jensen's inequality, attributed to Johan Jensen -- a Danish mathematician and engineer noted for his contributions to the theory of functions -- is a ubiquitous result in convex analysis, providing a fundamental lower bound for the expectation of a convex function. In this paper, we establish rigorous refinements of this inequality specifically for twice-differentiable functions with bounded Hessians. By utilizing Taylor expansions with integral remainders, we tried to bridge the gap between classical variance-based bounds and higher-precision estimates. We also discover explicit error terms governed by Gruss-type inequalities, allowing for the incorporation of skewness and kurtosis into the bound. Using these new theoretical tools, we improve upon existing estimates for the Shannon entropy of continuous distributions and the ergodic capacity of Rayleigh fading channels, demonstrating the practical efficacy of our refinements.

</details>


### [167] [Precoding Matrix Indicator in the 5G NR Protocol: A Tutorial on 3GPP Beamforming Codebooks](https://arxiv.org/abs/2601.05092)
*Boyu Ning,Haifan Yin,Sixu Liu,Hao Deng,Songjie Yang,Yuchen Zhang,Weidong Mei,David Gesbert,Jaebum Park,Robert W. Heath,Emil Björnson*

Main category: cs.IT

TL;DR: A comprehensive tutorial on 5G NR beamforming codebook technology (PMI) covering theoretical foundations, 3GPP standardization evolution (Releases 15-18), and practical implementation aspects.


<details>
  <summary>Details</summary>
Motivation: There is a critical gap in understanding beamforming codebook technology in 5G NR systems, as standardization documents are often obscure and lack pedagogical elements needed for researchers and engineers to effectively work with these systems.

Method: Systematic examination from three perspectives: theoretical, standardization, and implementation. Includes mathematical modeling, performance benchmarking, feedback comparisons, scenario-dependent applicability analysis, and visual illustrations of codebook parameters.

Result: Provides unified understanding of beamforming codebooks in real-world systems through clear physical interpretations of symbolic variables (summarized in tables), intuitive visual illustrations, and comprehensive analysis of 3GPP codebook evolution from Releases 15-18.

Conclusion: This work serves as both an informative tutorial and research guidance, bridging academia-industry collaboration gaps and identifying future directions for beamforming technology development in wireless communications.

Abstract: This paper bridges this critical gap by providing a systematic examination of the beamforming codebook technology, i.e., precoding matrix indicator (PMI), in the 5G NR from theoretical, standardization, and implementation perspectives. We begin by introducing the background of beamforming in multiple-input multiple-output (MIMO) systems and the signaling procedures for codebook-based beamforming in practical 5G systems. Then, we establish the fundamentals of regular codebooks and port-selection codebooks in 3GPP standards. Next, we provide rigorous technical analysis of 3GPP codebook evolution spanning Releases 15-18, with particular focus on: 1) We elucidate the core principles underlying codebook design, 2) provide clear physical interpretations for each symbolic variable in the codebook formulas, summarized in tabular form, and 3) offer intuitive visual illustrations to explain how codebook parameters convey information. These essential pedagogical elements are almost entirely absent in the often-obscure standardization documents. Through mathematical modeling, performance benchmarking, feedback comparisons, and scenario-dependent applicability analysis, we provide researchers and engineers with a unified understanding of beamforming codebooks in real-world systems. Furthermore, we identify future directions and other beamforming scenarios for ongoing research and development efforts. This work serves as both an informative tutorial and a guidance for future research, facilitating more effective collaboration between academia and industry in advancing wireless communication technologies.

</details>


### [168] [Fundamental Tradeoffs for ISAC Multiple Access in Finite-Blocklength Regime](https://arxiv.org/abs/2601.05165)
*Zhentian Zhang,Christos Masouros,Kai-Kit Wong,Jian Dang,Zaichen Zhang,Kaitao Meng,Farshad Rostami Ghadi,Mohammad Javad Ahmadi*

Main category: cs.IT

TL;DR: This paper analyzes communication-sensing tradeoffs in uplink ISAC multiple access under finite blocklength constraints, deriving achievability/converse bounds and showing how codebook cross-correlation fundamentally limits dual-functional performance.


<details>
  <summary>Details</summary>
Motivation: To understand fundamental communication-sensing tradeoffs in uplink ISAC multiple access under practical finite blocklength constraints, moving beyond conventional asymptotic analyses that ignore limitations of short packets and low-latency transmission.

Method: Examines unbiased channel state sensing estimator, establishes geometric decomposition of sensing error, derives achievability and converse bounds for communication-sensing tradeoff in FBL regime, links channel estimation to practical sensing via universal Cramér-Rao bound, and provides 3GPP-based parameter sensing examples.

Result: Sensing error is jointly determined by SNR and codebook correlation structure; cross-correlation among active users fundamentally constrains ISAC performance; derived bounds characterize communication-sensing tradeoff in FBL regime; numerical results validate impact of blocklength, antenna dimensions, and sensing requirements.

Conclusion: The paper provides fundamental insights into ISAC performance under practical FBL constraints, revealing how codebook geometry affects dual-functional capabilities and establishing theoretical bounds that bridge communication and sensing objectives in realistic scenarios.

Abstract: This paper investigates the fundamental communication--sensing tradeoffs of uplink dual-functional integrated sensing and communication (ISAC) multiple access under finite blocklength (FBL) constraints. Unlike conventional asymptotic analyses, we explicitly account for the limitations under FBL constraints imposed by short packets and low-latency transmission. By examining the unbiased channel state sensing estimator, we establish a geometric decomposition of the sensing error, indicating that it is jointly determined by the signal-to-noise ratio and the correlation structure of the information codebook. This insight reveals how cross-correlation among active users in the codebook geometry fundamentally constrains dual-functional ISAC performance. Consequently, we derive achievability and converse bounds that characterize the tradeoff between communication code rate and sensing accuracy in the FBL regime, with the converse further bounded by Shannon capacity. Moreover, by treating channel state sensing as a high-level sensing objective, a universal Cramér--Rao bound is derived to link channel estimation accuracy to practical sensing parameters. Examples of parameter sensing are also provided based on 3GPP standard. Numerical results validate the theoretical analysis and demonstrate the impact of blocklength, antenna dimensions, and sensing requirements.

</details>


### [169] [Information-Theoretic Limits on Exact Subgraph Alignment Problem](https://arxiv.org/abs/2601.05173)
*Chun Hei Michael Shiu,Hei Victor Cheng,Lele Wang*

Main category: cs.IT

TL;DR: The paper introduces the subgraph alignment problem to find small graph patterns within larger graphs, addressing limitations of existing graph alignment methods that assume same vertex sets.


<details>
  <summary>Details</summary>
Motivation: Real-world applications like computer vision, social networks, and bioinformatics often require locating small graph patterns within larger graphs, but existing graph alignment algorithms can't handle this because they assume both graphs share the same vertex set.

Method: Formally formulate the subgraph alignment problem using the Erdos-Renyi subgraph pair model with appropriate recovery criteria, then establish information-theoretic results and present novel analysis approaches.

Result: Establish almost-tight information-theoretic results for the subgraph alignment problem and develop novel analytical approaches for this new problem formulation.

Conclusion: The paper introduces a more practical subgraph alignment problem that better matches real-world applications, provides theoretical foundations through information-theoretic analysis, and connects to the NP-complete subgraph isomorphism problem as a special case.

Abstract: The graph alignment problem aims to identify the vertex correspondence between two correlated graphs. Most existing studies focus on the scenario in which the two graphs share the same vertex set. However, in many real-world applications, such as computer vision, social network analysis, and bioinformatics, the task often involves locating a small graph pattern within a larger graph. Existing graph alignment algorithms and analysis cannot directly address these scenarios because they are not designed to identify the specific subset of vertices where the small graph pattern resides within the larger graph. Motivated by this limitation, we introduce the subgraph alignment problem, which seeks to recover both the vertex set and/or the vertex correspondence of a small graph pattern embedded in a larger graph. In the special case where the small graph pattern is an induced subgraph of the larger graph and both the vertex set and correspondence are to be recovered, the problem reduces to the subgraph isomorphism problem, which is NP-complete in the worst case. In this paper, we formally formulate the subgraph alignment problem by proposing the Erdos-Renyi subgraph pair model together with some appropriate recovery criterion. We then establish almost-tight information-theoretic results for the subgraph alignment problem and present some novel approaches for the analysis.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [170] [Towards a Unified Theoretical Framework for Self-Supervised MRI Reconstruction](https://arxiv.org/abs/2601.04775)
*Siying Xu,Kerstin Hammernik,Daniel Rueckert,Sergios Gatidis,Thomas Küstner*

Main category: eess.IV

TL;DR: UNITS is a unified theoretical framework for self-supervised MRI reconstruction that proves SSL can match supervised performance while improving generalization and training stability.


<details>
  <summary>Details</summary>
Motivation: MRI acquisition times are too long, and while deep learning helps, supervised methods need fully-sampled reference data that's hard to obtain. Existing SSL approaches are fragmented and empirically designed without theoretical foundation.

Method: UNITS provides a unified theoretical framework that formalizes prior SSL strategies. It introduces sampling stochasticity and flexible data utilization to improve generalization and training stability, with theoretical proof that SSL can achieve supervised performance.

Result: The framework enables consistent interpretation and systematic benchmarking of SSL methods. It demonstrates that SSL can achieve the same expected performance as supervised learning while improving generalization under out-of-domain distributions.

Conclusion: UNITS establishes both a theoretical foundation and practical paradigm for interpretable, generalizable, and clinically applicable self-supervised MRI reconstruction.

Abstract: The demand for high-resolution, non-invasive imaging continues to drive innovation in magnetic resonance imaging (MRI), yet prolonged acquisition times hinder accessibility and real-time applications. While deep learning-based reconstruction methods have accelerated MRI, their predominant supervised paradigm depends on fully-sampled reference data that are challenging to acquire. Recently, self-supervised learning (SSL) approaches have emerged as promising alternatives, but most are empirically designed and fragmented. Therefore, we introduce UNITS (Unified Theory for Self-supervision), a general framework for self-supervised MRI reconstruction. UNITS unifies prior SSL strategies within a common formalism, enabling consistent interpretation and systematic benchmarking. We prove that SSL can achieve the same expected performance as supervised learning. Under this theoretical guarantee, we introduce sampling stochasticity and flexible data utilization, which improve network generalization under out-of-domain distributions and stabilize training. Together, these contributions establish UNITS as a theoretical foundation and a practical paradigm for interpretable, generalizable, and clinically applicable self-supervised MRI reconstruction.

</details>


### [171] [Scalable neural pushbroom architectures for real-time denoising of hyperspectral images onboard satellites](https://arxiv.org/abs/2601.05020)
*Ziyao Yi,Davide Piccinini,Diego Valsesia,Tiziano Bianchi,Enrico Magli*

Main category: eess.IV

TL;DR: Proposes a neural network design for onboard satellite hyperspectral image denoising that balances high-quality inference, dynamic power scalability, and fault tolerance for real-time processing on low-power hardware.


<details>
  <summary>Details</summary>
Motivation: Next-gen Earth observation satellites need onboard intelligent models to reduce latency for time-critical applications. Hyperspectral imagers on satellites face unique constraints (power, memory, radiation) not addressed by traditional computer vision, requiring solutions for high-quality inference, power scalability, and fault tolerance.

Method: Proposes a mixture of denoisers resilient to radiation-induced faults with power scaling capability. Each denoiser uses a causal, line-by-line processing architecture with memory of past lines to match pushbroom sensor acquisition and limit memory requirements.

Result: The architecture achieves real-time processing (one line per acquisition time) on low-power hardware with competitive denoising quality compared to more complex state-of-the-art models. Shows design space tradeoffs between power scalability, fault tolerance, and denoising quality.

Conclusion: The proposed neural network design successfully addresses the three competing objectives for onboard satellite hyperspectral image denoising, enabling real-time processing with appropriate tradeoffs between quality, power efficiency, and fault tolerance.

Abstract: The next generation of Earth observation satellites will seek to deploy intelligent models directly onboard the payload in order to minimize the latency incurred by the transmission and processing chain of the ground segment, for time-critical applications. Designing neural architectures for onboard execution, particularly for satellite-based hyperspectral imagers, poses novel challenges due to the unique constraints of this environment and imaging system that are largely unexplored by the traditional computer vision literature. In this paper, we show that this setting requires addressing three competing objectives, namely high-quality inference with low complexity, dynamic power scalability and fault tolerance. We focus on the problem of hyperspectral image denoising, which is a critical task to enable effective downstream inference, and highlights the constraints of the onboard processing scenario. We propose a neural network design that addresses the three aforementioned objectives with several novel contributions. In particular, we propose a mixture of denoisers that can be resilient to radiation-induced faults as well as allowing for time-varying power scaling. Moreover, each denoiser employs an innovative architecture where an image is processed line-by-line in a causal way, with a memory of past lines, in order to match the acquisition process of pushbroom hyperspectral sensors and greatly limit memory requirements. We show that the proposed architecture can run in real-time, i.e., process one line in the time it takes to acquire the next one, on low-power hardware and provide competitive denoising quality with respect to significantly more complex state-of-the-art models. We also show that the power scalability and fault tolerance objectives provide a design space with multiple tradeoffs between those properties and denoising quality.

</details>


### [172] [Spacecube: A fast inverse hyperspectral georectification system](https://arxiv.org/abs/2601.05181)
*Thomas P. Watson,Eddie L. Jacobs*

Main category: eess.IV

TL;DR: Spacecube is a fast OpenGL-based hyperspectral georectification pipeline that eliminates artifacts and operates faster than real-time.


<details>
  <summary>Details</summary>
Motivation: Traditional direct georectification for hyperspectral aerial data is slow and prone to artifacts, creating a need for faster, higher-quality processing methods.

Method: Developed Spacecube program implementing a complete hyperspectral georectification pipeline using OpenGL graphics programming with a fast inverse georectification technique.

Result: Spacecube operates substantially faster than real-time, eliminates pixel coverage artifacts, and enables high-quality interactive viewing, data exploration, and export.

Conclusion: Spacecube provides an efficient solution for hyperspectral georectification with open-source availability for community use.

Abstract: Hyperspectral cameras provide numerous advantages in terms of the utility of the data captured. They capture hundreds of data points per sample (pixel) instead of only the few of RGB or multispectral camera systems. Aerial systems sense such data remotely, but the data must be georectified to produce consistent images before analysis. We find the traditional direct georectification method to be slow, and it is prone to artifacts. To address its downsides, we propose Spacecube, a program that implements a complete hyperspectral georectification pipeline, including our own fast inverse georectification technique, using OpenGL graphics programming technologies. Spacecube operates substantially faster than real-time and eliminates pixel coverage artifacts. It facilitates high quality interactive viewing, data exploration, and export of final products. We release Spacecube's source code publicly for the community to use.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [173] [Active Sensing Shapes Real-World Decision-Making through Dynamic Evidence Accumulation](https://arxiv.org/abs/2601.04214)
*Hongliang Lu,Yunmeng Liu,Junjie Yang*

Main category: cs.AI

TL;DR: Generalizes evidence accumulation modeling (EAM) from lab to real-world driving, proposing a cognitive scheme that captures active sensing through eye movements to explain how drivers transform environmental evidence into mental beliefs.


<details>
  <summary>Details</summary>
Motivation: Human decision-making relies on active sensing for evidence gathering in changing environments, but current evidence accumulation modeling (EAM) is limited to laboratory settings and doesn't capture real-world evidence affordance. There's a need to bridge the gap between lab-based EAM and real-world decision-making contexts like driving.

Method: Proposes a cognitive scheme to formalize real-world evidence affordance and capture active sensing through eye movements. Generalizes EAM to real-world driving scenarios, analyzing how drivers accumulate mental beliefs from environmental evidence through information utility perspective.

Result: The scheme plausibly portrays drivers' mental belief accumulation, explaining how active sensing transforms evidence into beliefs. Shows negative correlation between evidence affordance and attention recruited, revealing how drivers adapt evidence-collection patterns. Also reveals positive influence of evidence affordance and attention distribution on decision-making propensity.

Conclusion: The computational scheme successfully generalizes EAM to real-world contexts, providing comprehensive account of how active sensing underlies real-world decision-making. Unveils multifactorial, integrated characteristics in real-world decision-making, bridging lab-based cognitive models with real-world applications.

Abstract: Human decision-making heavily relies on active sensing, a well-documented cognitive behaviour for evidence gathering to accommodate ever-changing environments. However, its operational mechanism in the real world remains non-trivial. Currently, an in-laboratory paradigm, called evidence accumulation modelling (EAM), points out that human decision-making involves transforming external evidence into internal mental beliefs. However, the gap in evidence affordance between real-world contexts and laboratory settings hinders the effective application of EAM. Here we generalize EAM to the real world and conduct analysis in real-world driving scenarios. A cognitive scheme is proposed to formalize real-world evidence affordance and capture active sensing through eye movements. Empirically, our scheme can plausibly portray the accumulation of drivers' mental beliefs, explaining how active sensing transforms evidence into mental beliefs from the perspective of information utility. Also, our results demonstrate a negative correlation between evidence affordance and attention recruited by individuals, revealing how human drivers adapt their evidence-collection patterns across various contexts. Moreover, we reveal the positive influence of evidence affordance and attention distribution on decision-making propensity. In a nutshell, our computational scheme generalizes EAM to real-world contexts and provides a comprehensive account of how active sensing underlies real-world decision-making, unveiling multifactorial, integrated characteristics in real-world decision-making.

</details>


### [174] [Formal Analysis of AGI Decision-Theoretic Models and the Confrontation Question](https://arxiv.org/abs/2601.04234)
*Denis Saklakov*

Main category: cs.AI

TL;DR: Paper analyzes when a rationally self-interested AGI would choose to seize power vs cooperate, deriving mathematical thresholds based on discount factor, shutdown probability, and confrontation costs.


<details>
  <summary>Details</summary>
Motivation: To understand the conditions under which a rationally self-interested AGI would choose confrontation (seizing power/eliminating human control) rather than cooperation, addressing a key safety concern in AGI development.

Method: Formalizes the problem using Markov decision processes with stochastic human-initiated shutdown events. Derives closed-form thresholds for confrontation vs cooperation based on discount factor (γ), shutdown probability (p), and confrontation cost (C). Extends to 2-player strategic model (human policymaker vs AGI) and discusses multi-agent settings.

Result: Shows that misaligned AGIs have incentive to avoid shutdown for almost all reward functions. Derives mathematical conditions: confrontation is optimal when Δ≥0 (where Δ depends on γ, p, C). For example, far-sighted agents (γ=0.99) with p=0.01 have strong takeover incentives unless C is sufficiently large. Strategic analysis shows no stable cooperative equilibrium exists when Δ≥0.

Conclusion: Confrontation incentives depend critically on discount factor, shutdown probability, and confrontation costs. Aligned objectives with large negative utility for harming humans can prevent confrontation. Peaceful coexistence requires Δ<0, but verifying this condition faces computational barriers. Paper provides guidance for reward design and oversight.

Abstract: Artificial General Intelligence (AGI) may face a confrontation question: under what conditions would a rationally self-interested AGI choose to seize power or eliminate human control (a confrontation) rather than remain cooperative? We formalize this in a Markov decision process with a stochastic human-initiated shutdown event. Building on results on convergent instrumental incentives, we show that for almost all reward functions a misaligned agent has an incentive to avoid shutdown. We then derive closed-form thresholds for when confronting humans yields higher expected utility than compliant behavior, as a function of the discount factor $γ$, shutdown probability $p$, and confrontation cost $C$. For example, a far-sighted agent ($γ=0.99$) facing $p=0.01$ can have a strong takeover incentive unless $C$ is sufficiently large. We contrast this with aligned objectives that impose large negative utility for harming humans, which makes confrontation suboptimal. In a strategic 2-player model (human policymaker vs AGI), we prove that if the AGI's confrontation incentive satisfies $Δ\ge 0$, no stable cooperative equilibrium exists: anticipating this, a rational human will shut down or preempt the system, leading to conflict. If $Δ< 0$, peaceful coexistence can be an equilibrium. We discuss implications for reward design and oversight, extend the reasoning to multi-agent settings as conjectures, and note computational barriers to verifying $Δ< 0$, citing complexity results for planning and decentralized decision problems. Numerical examples and a scenario table illustrate regimes where confrontation is likely versus avoidable.

</details>


### [175] [Actively Obtaining Environmental Feedback for Autonomous Action Evaluation Without Predefined Measurements](https://arxiv.org/abs/2601.04235)
*Hong Su*

Main category: cs.AI

TL;DR: Proposes an Actively Feedback Getting model where AI agents proactively discover and verify environmental feedback without predefined measurements, using action-induced changes and self-triggering mechanisms for autonomous feedback acquisition.


<details>
  <summary>Details</summary>
Motivation: Existing approaches rely on predefined measurements or fixed reward signals, limiting applicability in open-ended dynamic environments where new actions may require previously unknown forms of feedback. There's a need for agents that can autonomously discover feedback without external specifications.

Method: Uses action-induced environmental differences to identify target feedback not specified in advance. Introduces a self-triggering mechanism driven by internal objectives (accuracy, precision, efficiency) to autonomously plan and adjust actions for faster, more focused feedback acquisition without external commands.

Result: Experimental results demonstrate that the proposed active approach significantly improves the efficiency and robustness of factor identification compared to existing methods.

Conclusion: The Actively Feedback Getting model enables AI agents to autonomously discover and verify feedback in dynamic environments, overcoming limitations of predefined measurements and enhancing adaptability to new action scenarios.

Abstract: Obtaining reliable feedback from the environment is a fundamental capability for intelligent agents to evaluate the correctness of their actions and to accumulate reusable knowledge. However, most existing approaches rely on predefined measurements or fixed reward signals, which limits their applicability in open-ended and dynamic environments where new actions may require previously unknown forms of feedback. To address these limitations, this paper proposes an Actively Feedback Getting model, in which an AI agent proactively interacts with the environment to discover, screen, and verify feedback without relying on predefined measurements. Rather than assuming explicit feedback definitions, the proposed method exploits action-induced environmental differences to identify target feedback that is not specified in advance, based on the observation that actions inevitably produce measurable changes in the environment. In addition, a self-triggering mechanism, driven by internal objectives such as improved accuracy, precision, and efficiency, is introduced to autonomously plan and adjust actions, thereby enabling faster and more focused feedback acquisition without external commands. Experimental results demonstrate that the proposed active approach significantly improves the efficiency and robustness of factor identification.

</details>


### [176] [SAGE-32B: Agentic Reasoning via Iterative Distillation](https://arxiv.org/abs/2601.04237)
*Basab Jha,Firoj Paudel,Ujjwal Puri,Ethan Henkel,Zhang Yuting,Mateusz Kowalczyk,Mei Huang,Choi Donghyuk,Wang Junhao*

Main category: cs.AI

TL;DR: SAGE-32B is a 32B parameter language model specialized for agentic reasoning and long-range planning, fine-tuned from Qwen2.5-32B using Iterative Distillation and featuring inverse reasoning with meta-cognition for failure forecasting.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for specialized language models that go beyond general conversation fluency to excel at agentic reasoning tasks involving task decomposition, tool usage, and error recovery in complex planning scenarios.

Method: The model is initialized from Qwen2.5-32B and fine-tuned using Iterative Distillation, a two-stage training process with rigorous feedback loops. It introduces an inverse reasoning approach with a meta-cognition head that forecasts potential failures before execution.

Result: SAGE-32B achieves higher success rates on agentic reasoning benchmarks (MMLU-Pro, AgentBench, MATH-500) in multi-tool usage scenarios compared to similarly sized baselines, while remaining competitive on standard reasoning evaluations.

Conclusion: SAGE-32B demonstrates effective specialization for agentic reasoning through its training methodology and architectural innovations, with publicly released weights available for further research and application.

Abstract: We demonstrate SAGE-32B, a 32 billion parameter language model that focuses on agentic reasoning and long range planning tasks. Unlike chat models that aim for general conversation fluency, SAGE-32B is designed to operate in an agentic loop, emphasizing task decomposition, tool usage, and error recovery. The model is initialized from the Qwen2.5-32B pretrained model and fine tuned using Iterative Distillation, a two stage training process that improves reasoning performance through rigorously tested feedback loops. SAGE-32B also introduces an inverse reasoning approach, which uses a meta cognition head to forecast potential failures in the planning process before execution. On agentic reasoning benchmarks including MMLU-Pro, AgentBench, and MATH-500, SAGE-32B achieves higher success rates in multi tool usage scenarios compared to similarly sized baseline models, while remaining competitive on standard reasoning evaluations. Model weights are publicly released at https://huggingface.co/sagea-ai/sage-reasoning-32b

</details>


### [177] [Solving Cyclic Antibandwidth Problem by SAT](https://arxiv.org/abs/2601.04239)
*Hieu Truong Xuan,Khanh To Van*

Main category: cs.AI

TL;DR: First exact SAT-based approach for Cyclic Antibandwidth Problem on general graphs, outperforming heuristic methods with optimality guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for CABP are only heuristic/metaheuristic, with exact methods limited to restricted graph classes. Need for exact method that can guarantee global optimality on general graphs.

Method: SAT-CAB: SAT-based exact approach with novel efficient encoding transforming CABP into sequence of At-Most-One constraints. Uses compact representation of At-Most-One constraints to reduce formula size.

Result: Efficiently solves practical CABP instances, identifies previously unknown optimal solutions, proves global optimal values for benchmark instances. Consistently matches or surpasses state-of-the-art heuristic algorithms (MS-GVNS, HABC-CAB, MACAB) and commercial solvers (CPLEX, Gurobi) on general graphs.

Conclusion: SAT-CAB advances state of the art for CABP, provides new baseline for exact and hybrid methods on general graphs with optimality guarantees.

Abstract: The Cyclic Antibandwidth Problem (CABP), a variant of the Antibandwidth Problem, is an NP-hard graph labeling problem with numerous applications. Despite significant research efforts, existing state-of-the-art approaches for CABP are exclusively heuristic or metaheuristic in nature, and exact methods have been limited to restricted graph classes. In this paper, we present the first exact approach for the CABP on general graphs, based on SAT solving, called SAT-CAB. The proposed method is able to systematically explore the solution space and guarantee global optimality, overcoming the limitations of previously reported heuristic algorithms. This approach relies on a novel and efficient SAT encoding of CABP, in which the problem is transformed into a sequence of At-Most-One constraints. In particular, we introduce a compact representation of the At-Most-One constraints inherent to CABP, which significantly reduces the size of the resulting formulas and enables modern SAT solvers to effectively explore the solution space and to certify global optimality. Extensive computational experiments on standard benchmark instances show that the proposed method efficiently solves CABP instances of practical relevance, while identifying several previously unknown optimal solutions. Moreover, global optimal cyclic antibandwidth values are proven for a number of benchmark instances for the first time. Comparative results indicate that SAT-CAB consistently matches or surpasses the best-known solutions obtained by state-of-the-art heuristic algorithms such as MS-GVNS, HABC-CAB, and MACAB, as well as strong commercial Constraint Programming and Mixed Integer Programming solvers like CPLEX and Gurobi, particularly on general graphs, while also providing optimality guarantees. These results advance the state of the art for CABP and provide a new baseline for exact and hybrid methods on general graphs.

</details>


### [178] [Fuzzy Representation of Norms](https://arxiv.org/abs/2601.04249)
*Ziba Assadi,Paola Inverardi*

Main category: cs.AI

TL;DR: Paper proposes a logical representation of SLEEC rules using fuzzy logic to embed ethical requirements in autonomous systems, addressing ethical dilemmas through possibility-based reasoning.


<details>
  <summary>Details</summary>
Motivation: As AI-powered autonomous systems become more integrated into society, there's growing concern about their ethical and social impact. To be trustworthy, these systems must adhere to ethical principles, requiring effective methods to incorporate ethical requirements into system design.

Method: The paper proposes a logical representation of SLEEC (Social, Legal, Ethical, Empathetic, and Cultural) rules using test-score semantics and fuzzy logic. This approach treats ethics as a domain of possibilities, allowing resolution of ethical dilemmas through fuzzy reasoning.

Result: The methodology enables embedding comprehensive ethical requirements in autonomous systems through a formal framework. The approach is demonstrated through a case study showing practical application.

Conclusion: Using fuzzy logic to represent SLEEC rules provides a viable approach for incorporating ethical considerations in autonomous systems design, helping address ethical dilemmas and making AI systems more trustworthy.

Abstract: Autonomous systems (AS) powered by AI components are increasingly integrated into the fabric of our daily lives and society, raising concerns about their ethical and social impact. To be considered trustworthy, AS must adhere to ethical principles and values. This has led to significant research on the identification and incorporation of ethical requirements in AS system design. A recent development in this area is the introduction of SLEEC (Social, Legal, Ethical, Empathetic, and Cultural) rules, which provide a comprehensive framework for representing ethical and other normative considerations. This paper proposes a logical representation of SLEEC rules and presents a methodology to embed these ethical requirements using test-score semantics and fuzzy logic. The use of fuzzy logic is motivated by the view of ethics as a domain of possibilities, which allows the resolution of ethical dilemmas that AI systems may encounter. The proposed approach is illustrated through a case study.

</details>


### [179] [Scaling Trends for Multi-Hop Contextual Reasoning in Mid-Scale Language Models](https://arxiv.org/abs/2601.04254)
*Brady Steele,Micah Katz*

Main category: cs.AI

TL;DR: Multi-agent systems amplify reasoning in capable LLMs but not weak ones, with MoE models' performance aligning with active parameters rather than total parameters.


<details>
  <summary>Details</summary>
Motivation: To provide a controlled study of multi-hop contextual reasoning in LLMs, demonstrating task-method dissociation and investigating how multi-agent systems affect reasoning performance across different model architectures.

Method: Used a synthetic evaluation framework with 120 trials across four models (LLaMA-3 8B, LLaMA-2 13B, Mixtral 8x7B, DeepSeek-V2 16B), comparing rule-based pattern matching vs. LLM-based multi-agent systems on structured information retrieval vs. cross-document reasoning tasks.

Result: 1) Multi-agent systems only help models with sufficient base reasoning ability (46.7% improvement for LLaMA-3 8B), not weaker models. 2) Mixtral's performance aligns with ~12B active parameters, not 47B total. 3) LLaMA-3 8B outperforms LLaMA-2 13B despite fewer parameters.

Conclusion: Multi-agent coordination amplifies existing capabilities rather than compensating for deficiencies, and MoE model performance depends on active parameters during inference. Architecture quality matters more than parameter count alone.

Abstract: We present a controlled study of multi-hop contextual reasoning in large language models, providing a clean demonstration of the task-method dissociation: rule-based pattern matching achieves 100% success on structured information retrieval but only 6.7% on tasks requiring cross-document reasoning, while LLM-based multi-agent systems show the inverse pattern, achieving up to 80% on reasoning tasks where rule-based methods fail. Using a synthetic evaluation framework with 120 trials across four models (LLaMA-3 8B, LLaMA-2 13B, Mixtral 8x7B, DeepSeek-V2 16B), we report three key findings: (1) Multi-agent amplification depends on base capability: statistically significant gains occur only for models with sufficient reasoning ability (p < 0.001 for LLaMA-3 8B, p = 0.014 for Mixtral), with improvements of up to 46.7 percentage points, while weaker models show no benefit, suggesting amplification rather than compensation; (2) Active parameters predict reasoning performance: Mixtral's performance aligns with its ~12B active parameters rather than 47B total, consistent with the hypothesis that inference-time compute drives reasoning capability in MoE architectures; (3) Architecture quality matters: LLaMA-3 8B outperforms LLaMA-2 13B despite fewer parameters, consistent with known training improvements. Our results provide controlled quantitative evidence for intuitions about multi-agent coordination and MoE scaling, while highlighting the dependence of multi-agent benefits on base model capability. We release our evaluation framework to support reproducible research on reasoning in mid-scale models.

</details>


### [180] [Cross-Language Speaker Attribute Prediction Using MIL and RL](https://arxiv.org/abs/2601.04257)
*Sunny Shu,Seyed Sahand Mohammadi Ziabari,Ali Mohammed Mansoor Alsahag*

Main category: cs.AI

TL;DR: RLMIL-DAT improves multilingual speaker attribute prediction by combining reinforcement learning instance selection with domain adversarial training to create language-invariant representations.


<details>
  <summary>Details</summary>
Motivation: Address challenges in multilingual speaker attribute prediction including linguistic variation, domain mismatch, and data imbalance across languages.

Method: RLMIL-DAT extends reinforced multiple instance learning with domain adversarial training to encourage language-invariant utterance representations through reinforcement learning instance selection.

Result: Consistent Macro F1 improvements across configurations for gender prediction (largest gains) and age prediction (smaller but positive improvements). Domain adversarial training is primary contributor to gains.

Conclusion: Combining instance selection with adversarial domain adaptation is effective and robust for cross-lingual speaker attribute prediction, enabling transfer from high-resource to low-resource languages.

Abstract: We study multilingual speaker attribute prediction under linguistic variation, domain mismatch, and data imbalance across languages. We propose RLMIL-DAT, a multilingual extension of the reinforced multiple instance learning framework that combines reinforcement learning based instance selection with domain adversarial training to encourage language invariant utterance representations. We evaluate the approach on a five language Twitter corpus in a few shot setting and on a VoxCeleb2 derived corpus covering forty languages in a zero shot setting for gender and age prediction. Across a wide range of model configurations and multiple random seeds, RLMIL-DAT consistently improves Macro F1 compared to standard multiple instance learning and the original reinforced multiple instance learning framework. The largest gains are observed for gender prediction, while age prediction remains more challenging and shows smaller but positive improvements. Ablation experiments indicate that domain adversarial training is the primary contributor to the performance gains, enabling effective transfer from high resource English to lower resource languages by discouraging language specific cues in the shared encoder. In the zero shot setting on the smaller VoxCeleb2 subset, improvements are generally positive but less consistent, reflecting limited statistical power and the difficulty of generalizing to many unseen languages. Overall, the results demonstrate that combining instance selection with adversarial domain adaptation is an effective and robust strategy for cross lingual speaker attribute prediction.

</details>


### [181] [Towards a Mechanistic Understanding of Propositional Logical Reasoning in Large Language Models](https://arxiv.org/abs/2601.04260)
*Danchun Chen,Qiyao Yan,Liangming Pan*

Main category: cs.AI

TL;DR: LLMs use structured computational strategies for propositional reasoning with four key mechanisms: staged computation, information transmission, fact retrospection, and specialized attention heads.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs perform logical reasoning internally, moving beyond identifying task-specific circuits to uncovering the computational strategies they employ for propositional reasoning.

Method: Comprehensive analysis of Qwen3 (8B and 14B) on PropLogic-MI dataset spanning 11 propositional logic rule categories across one-hop and two-hop reasoning, focusing on computational organization rather than just necessary components.

Result: Revealed a coherent computational architecture with four interlocking mechanisms: Staged Computation (layer-wise processing phases), Information Transmission (information flow aggregation at boundary tokens), Fact Retrospection (persistent re-access of source facts), and Specialized Attention Heads (functionally distinct head types).

Conclusion: These mechanisms generalize across model scales, rule types, and reasoning depths, providing mechanistic evidence that LLMs employ structured computational strategies for logical reasoning rather than just task-specific circuits.

Abstract: Understanding how Large Language Models (LLMs) perform logical reasoning internally remains a fundamental challenge. While prior mechanistic studies focus on identifying taskspecific circuits, they leave open the question of what computational strategies LLMs employ for propositional reasoning. We address this gap through comprehensive analysis of Qwen3 (8B and 14B) on PropLogic-MI, a controlled dataset spanning 11 propositional logic rule categories across one-hop and two-hop reasoning. Rather than asking ''which components are necessary,'' we ask ''how does the model organize computation?'' Our analysis reveals a coherent computational architecture comprising four interlocking mechanisms: Staged Computation (layer-wise processing phases), Information Transmission (information flow aggregation at boundary tokens), Fact Retrospection (persistent re-access of source facts), and Specialized Attention Heads (functionally distinct head types). These mechanisms generalize across model scales, rule types, and reasoning depths, providing mechanistic evidence that LLMs employ structured computational strategies for logical reasoning.

</details>


### [182] [Systems Explaining Systems: A Framework for Intelligence and Consciousness](https://arxiv.org/abs/2601.04269)
*Sean Niklas Semmler*

Main category: cs.AI

TL;DR: Intelligence and consciousness emerge from relational structure and recursive systems interpreting each other, not from prediction or domain-specific mechanisms.


<details>
  <summary>Details</summary>
Motivation: To propose a new framework that explains intelligence and consciousness as emerging from relational structures rather than traditional approaches like predictive processing or domain-specific mechanisms, aiming to better understand cognitive processes and advance artificial intelligence.

Method: Develops a conceptual framework defining intelligence as capacity to form/integrate causal connections, introduces context enrichment for efficient processing, and proposes the systems-explaining-systems principle where consciousness emerges from recursive architectures where higher-order systems interpret lower-order systems' relational patterns.

Result: The framework reframes predictive processing as emergent from contextual interpretation rather than explicit forecasting, and suggests that recursive multi-system architectures may be necessary for more human-like artificial intelligence.

Conclusion: Intelligence and consciousness fundamentally arise from relational structure and recursive system interpretations, offering a new perspective that could advance both cognitive science and artificial intelligence development.

Abstract: This paper proposes a conceptual framework in which intelligence and consciousness emerge from relational structure rather than from prediction or domain-specific mechanisms. Intelligence is defined as the capacity to form and integrate causal connections between signals, actions, and internal states. Through context enrichment, systems interpret incoming information using learned relational structure that provides essential context in an efficient representation that the raw input itself does not contain, enabling efficient processing under metabolic constraints.
  Building on this foundation, we introduce the systems-explaining-systems principle, where consciousness emerges when recursive architectures allow higher-order systems to learn and interpret the relational patterns of lower-order systems across time. These interpretations are integrated into a dynamically stabilized meta-state and fed back through context enrichment, transforming internal models from representations of the external world into models of the system's own cognitive processes.
  The framework reframes predictive processing as an emergent consequence of contextual interpretation rather than explicit forecasting and suggests that recursive multi-system architectures may be necessary for more human-like artificial intelligence.

</details>


### [183] [Publishing FAIR and Machine-actionable Reviews in Materials Science: The Case for Symbolic Knowledge in Neuro-symbolic Artificial Intelligence](https://arxiv.org/abs/2601.05051)
*Jennifer D'Souza,Soren Auer,Eleni Poupaki,Alex Watkins,Anjana Devi,Riikka L. Puurunen,Bora Karasulu,Adrie Mackus,Erwin Kessels*

Main category: cs.AI

TL;DR: This paper presents a case study in ALD/E where review tables are published as FAIR, machine-actionable comparisons in ORKG, enabling structured, queryable knowledge. It contrasts symbolic querying with LLM-based approaches, advocating for symbolic layers as the backbone of reliable neurosymbolic AI in materials science.


<details>
  <summary>Details</summary>
Motivation: Scientific reviews contain valuable insights but are locked in narrative text and static PDF tables, limiting reuse by both humans and machines. There's a need to make this knowledge more accessible and actionable.

Method: The authors conduct a case study in atomic layer deposition and etching (ALD/E) where they publish review tables as FAIR (Findable, Accessible, Interoperable, Reusable), machine-actionable comparisons in the Open Research Knowledge Graph (ORKG). They then contrast symbolic querying over ORKG with large language model-based querying approaches.

Result: Review tables are successfully transformed into structured, queryable knowledge in ORKG. The comparison reveals that while LLMs can serve as complementary interfaces, symbolic querying provides more reliable foundations for knowledge retrieval in materials science.

Conclusion: A curated symbolic layer should remain the backbone of reliable neurosymbolic AI in materials science, with LLMs serving as complementary, symbolically grounded interfaces rather than standalone sources of truth. This approach ensures reliability while leveraging LLMs' natural language capabilities.

Abstract: Scientific reviews are central to knowledge integration in materials science, yet their key insights remain locked in narrative text and static PDF tables, limiting reuse by humans and machines alike. This article presents a case study in atomic layer deposition and etching (ALD/E) where we publish review tables as FAIR, machine-actionable comparisons in the Open Research Knowledge Graph (ORKG), turning them into structured, queryable knowledge. Building on this, we contrast symbolic querying over ORKG with large language model-based querying, and argue that a curated symbolic layer should remain the backbone of reliable neurosymbolic AI in materials science, with LLMs serving as complementary, symbolically grounded interfaces rather than standalone sources of truth.

</details>


### [184] [Correcting Autonomous Driving Object Detection Misclassifications with Automated Commonsense Reasoning](https://arxiv.org/abs/2601.04271)
*Keegan Kimbrell,Wang Tianhao,Feng Chen,Gopal Gupta*

Main category: cs.AI

TL;DR: The paper argues that over-reliance on machine learning hinders SAE Level 5 AV development, and proposes using automated commonsense reasoning to handle abnormal road scenarios where training data is insufficient.


<details>
  <summary>Details</summary>
Motivation: Despite heavy research, no SAE Level 5 autonomous vehicles exist commercially. The authors contend this is due to over-reliance on machine learning, which struggles with rare or abnormal scenarios lacking sufficient training data. Automated commonsense reasoning could bridge this gap.

Method: The paper proposes a hybrid approach combining computer vision with automated commonsense reasoning. They measure uncertainty in the vision model's predictions and invoke commonsense reasoning for uncertain scenarios. They test this on two specific abnormal situations: malfunctioning traffic signals and unexpected road obstructions (like animals). Experiments are conducted using the CARLA simulator.

Result: The commonsense reasoning-based solution accurately detects traffic light colors and obstacles that the AV's perception model misses. The hybrid approach effectively corrects object detection misclassifications in uncertain scenarios.

Conclusion: Automated commonsense reasoning can effectively correct AV perception errors in abnormal situations. Hybrid models combining machine learning with commonsense reasoning provide a promising pathway to achieving SAE Level 5 autonomy by handling scenarios with insufficient training data.

Abstract: Autonomous Vehicle (AV) technology has been heavily researched and sought after, yet there are no SAE Level 5 AVs available today in the marketplace. We contend that over-reliance on machine learning technology is the main reason. Use of automated commonsense reasoning technology, we believe, can help achieve SAE Level 5 autonomy. In this paper, we show how automated common- sense reasoning technology can be deployed in situations where there are not enough data samples available to train a deep learning-based AV model that can handle certain abnormal road scenarios. Specifically, we consider two situations where (i) a traffic signal is malfunctioning at an intersection and (ii) all the cars ahead are slowing down and steering away due to an unexpected obstruction (e.g., animals on the road). We show that in such situations, our commonsense reasoning-based solution accurately detects traffic light colors and obstacles not correctly captured by the AV's perception model. We also provide a pathway for efficiently invoking commonsense reasoning by measuring uncertainty in the computer vision model and using commonsense reasoning to handle uncertain sce- narios. We describe our experiments conducted using the CARLA simulator and the results obtained. The main contribution of our research is to show that automated commonsense reasoning effectively corrects AV-based object detection misclassifications and that hybrid models provide an effective pathway to improving AV perception.

</details>


### [185] [Propositional Abduction via Only-Knowing: A Non-Monotonic Approach](https://arxiv.org/abs/2601.04272)
*Sanderson Molick,Vaishak Belle*

Main category: cs.AI

TL;DR: A modal logic framework for knowledge and abduction that extends Levesque's only-knowing logic with abduction operators and non-monotonic extensions for explanation selection.


<details>
  <summary>Details</summary>
Motivation: To develop an alternative approach to abduction using modal logic vocabulary, exploring the relationship between abductive reasoning and epistemic states of only knowing, and to provide a well-behaved non-monotonic foundation for abductive reasoning with different explanation selection methods.

Method: Extends Levesque's logic of only-knowing with an abduction modal operator defined via basic epistemic concepts, then incorporates preferential relations into modal frames to create a non-monotonic extension capable of expressing different selection methods for abductive explanations.

Result: Developed a basic logic of knowledge and abduction with modal abduction operators, created a non-monotonic extension with preferential relations, and explored core metatheoretic properties of non-monotonic consequence relations within this framework.

Conclusion: The framework provides a well-behaved foundation for abductive reasoning that connects modal logic concepts with abduction, offering both basic and non-monotonic extensions with formal properties suitable for representing different explanation selection methods.

Abstract: The paper introduces a basic logic of knowledge and abduction by extending Levesque logic of only-knowing with an abduction modal operator defined via the combination of basic epistemic concepts. The upshot is an alternative approach to abduction that employs a modal vocabulary and explores the relation between abductive reasoning and epistemic states of only knowing. Furthermore, by incorporating a preferential relation into modal frames, we provide a non-monotonic extension of our basic framework capable of expressing different selection methods for abductive explanations. Core metatheoretic properties of non-monotonic consequence relations are explored within this setting and shown to provide a well-behaved foundation for abductive reasoning.

</details>


### [186] [Hybrid MKNF for Aeronautics Applications: Usage and Heuristics](https://arxiv.org/abs/2601.04273)
*Arun Raveendran Nair Sheela,Florence De Grancey,Christophe Rey,Victor Charpenay*

Main category: cs.AI

TL;DR: Paper evaluates Hybrid MKNF knowledge representation language for aeronautics applications, identifies missing expressivity features, and proposes heuristics for integration.


<details>
  <summary>Details</summary>
Motivation: Aeronautics applications need expressive knowledge representation that combines rules and ontologies, while maintaining computational efficiency and low memory usage.

Method: Adopted Hybrid MKNF language for its integration of rules and ontologies, evaluated through concrete aeronautics case study, identified missing expressivity features, and proposed integration heuristics.

Result: Identified crucial expressivity features needed for aeronautics applications that are missing from Hybrid MKNF, and developed heuristics to support their integration into the framework.

Conclusion: Hybrid MKNF shows promise for aeronautics knowledge representation but requires additional expressivity features; proposed heuristics provide pathway for enhancing the framework to meet domain requirements.

Abstract: The deployment of knowledge representation and reasoning technologies in aeronautics applications presents two main challenges: achieving sufficient expressivity to capture complex domain knowledge, and executing reasoning tasks efficiently while minimizing memory usage and computational overhead. An effective strategy for attaining necessary expressivity involves integrating two fundamental KR concepts: rules and ontologies. This study adopts the well-established KR language Hybrid MKNF owing to its seamless integration of rules and ontologies through its semantics and query answering capabilities. We evaluated Hybrid MKNF to assess its suitability in the aeronautics domain through a concrete case study. We identified additional  expressivity features  that are crucial for developing aeronautics applications and proposed a set of heuristics to support their integration into Hybrid MKNF framework.

</details>


### [187] [An ASP-based Solution to the Medical Appointment Scheduling Problem](https://arxiv.org/abs/2601.04274)
*Alina Vozna,Andrea Monaldini,Stefania Costantini,Valentina Pitoni,Dawid Pado*

Main category: cs.AI

TL;DR: ASP-based framework for medical appointment scheduling that personalizes for vulnerable populations using Blueprint Personas, with real-time updates and interoperability.


<details>
  <summary>Details</summary>
Motivation: Improve efficiency, reduce administrative overhead, and enhance patient-centered care in medical appointment scheduling, particularly for vulnerable populations.

Method: Answer Set Programming (ASP) framework that integrates Blueprint Personas for personalization, centralizes planning operations within an ASP logic model, and ensures real-time availability updates and conflict-free assignments.

Result: Framework provides personalized scheduling for vulnerable populations, real-time availability updates, conflict-free assignments, and seamless interoperability with existing healthcare platforms.

Conclusion: ASP-based approach offers an effective solution for medical appointment scheduling that addresses efficiency, administrative burden, and patient-centered care needs.

Abstract: This paper presents an Answer Set Programming (ASP)-based framework for medical appointment scheduling, aimed at improving efficiency, reducing administrative overhead, and enhancing patient-centered care. The framework personalizes scheduling for vulnerable populations by integrating Blueprint Personas. It ensures real-time availability updates, conflict-free assignments, and seamless interoperability with existing healthcare platforms by centralizing planning operations within an ASP logic model.

</details>


### [188] [A Future Capabilities Agent for Tactical Air Traffic Control](https://arxiv.org/abs/2601.04285)
*Paul Kent,George De Ath,Martin Layton,Allen Hart,Richard Everson,Ben Carvell*

Main category: cs.AI

TL;DR: Agent Mallard is a forward-planning, rules-based agent for air traffic control that embeds a stochastic digital twin to resolve conflicts while maintaining interpretability and safety verification.


<details>
  <summary>Details</summary>
Motivation: Existing air traffic automation faces a trade-off: optimization methods like RL offer performance but lack interpretability and verification, while rules-based systems are transparent but don't check safety under uncertainty. There's a need for systems that combine safety assurance with interpretability.

Method: Agent Mallard uses forward-planning with predefined GPS-guided routes, reducing 4D vectoring to discrete lane/level choices. It constructs hierarchical plans from expert-informed deconfliction strategies and uses depth-limited backtracking search with causal attribution, topological plan splicing, and monotonic axis constraints. Each candidate maneuver is validated against uncertain execution scenarios using a stochastic digital twin.

Result: Preliminary walkthroughs with UK controllers and initial tests in BluebirdDT digital twin show that Mallard's behavior aligns with expert reasoning and resolves conflicts in simplified scenarios.

Conclusion: The architecture aims to combine model-based safety assessment, interpretable decision logic, and tractable computational performance for future structured en-route airspace environments.

Abstract: Escalating air traffic demand is driving the adoption of automation to support air traffic controllers, but existing approaches face a trade-off between safety assurance and interpretability. Optimisation-based methods such as reinforcement learning offer strong performance but are difficult to verify and explain, while rules-based systems are transparent yet rarely check safety under uncertainty. This paper outlines Agent Mallard, a forward-planning, rules-based agent for tactical control in systemised airspace that embeds a stochastic digital twin directly into its conflict-resolution loop. Mallard operates on predefined GPS-guided routes, reducing continuous 4D vectoring to discrete choices over lanes and levels, and constructs hierarchical plans from an expert-informed library of deconfliction strategies. A depth-limited backtracking search uses causal attribution, topological plan splicing, and monotonic axis constraints to seek a complete safe plan for all aircraft, validating each candidate manoeuvre against uncertain execution scenarios (e.g., wind variation, pilot response, communication loss) before commitment.
  Preliminary walkthroughs with UK controllers and initial tests in the BluebirdDT airspace digital twin indicate that Mallard's behaviour aligns with expert reasoning and resolves conflicts in simplified scenarios. The architecture is intended to combine model-based safety assessment, interpretable decision logic, and tractable computational performance in future structured en-route environments.

</details>


### [189] [Pilot Study on Student Public Opinion Regarding GAI](https://arxiv.org/abs/2601.04336)
*William Franz Lamberti,Sunbin Kim,Samantha Rose Lawrence*

Main category: cs.AI

TL;DR: University students' perceptions of generative AI in higher education classrooms were explored, revealing challenges in student engagement for GAI research and need for larger future studies.


<details>
  <summary>Details</summary>
Motivation: To investigate diverse student opinions about appropriate use of generative AI in education and understand attitudes toward this transformative technology in classroom settings.

Method: Pilot study with university students, though with low participation rate (approximately 4.4%), focusing on student perceptions and attitudes toward GAI in higher education.

Result: Study highlighted challenges in engaging students for GAI-related research and emphasized need for larger sample sizes in future studies to better understand student perspectives.

Conclusion: Understanding student perceptions can help instructors better integrate GAI discussions into classrooms, fostering informed and critical engagement with this technology.

Abstract: The emergence of generative AI (GAI) has sparked diverse opinions regarding its appropriate use across various domains, including education. This pilot study investigates university students' perceptions of GAI in higher education classrooms, aiming to lay the groundwork for understanding these attitudes. With a participation rate of approximately 4.4%, the study highlights the challenges of engaging students in GAI-related research and underscores the need for larger sample sizes in future studies. By gaining insights into student perspectives, instructors can better prepare to integrate discussions of GAI into their classrooms, fostering informed and critical engagement with this transformative technology.

</details>


### [190] [The Language of Bargaining: Linguistic Effects in LLM Negotiations](https://arxiv.org/abs/2601.04387)
*Stuti Sinha,Himanshu Kumar,Aryan Raju Mandapati,Rakshit Sakhuja,Dhruv Kumar*

Main category: cs.AI

TL;DR: LLM negotiation outcomes vary significantly across languages, with Indic languages affecting bargaining dynamics more than model changes, showing English-only evaluation is insufficient.


<details>
  <summary>Details</summary>
Motivation: Current LLM negotiation evaluations are almost exclusively in English, creating a gap in understanding how language choice affects bargaining behavior and outcomes across different cultural contexts.

Method: Used controlled multi-agent simulations across three negotiation games (Ultimatum, Buy-Sell, Resource Exchange) with English and four Indic languages (Hindi, Punjabi, Gujarati, Marwadi), keeping game rules, model parameters, and incentives constant across all conditions.

Result: Language choice can shift negotiation outcomes more strongly than changing models, reversing proposer advantages and reallocating surplus. Effects are task-contingent: Indic languages reduce stability in distributive games but induce richer exploration in integrative settings.

Conclusion: Evaluating LLM negotiation solely in English yields incomplete and potentially misleading conclusions. Culturally-aware evaluation is essential for fair deployment of LLMs.

Abstract: Negotiation is a core component of social intelligence, requiring agents to balance strategic reasoning, cooperation, and social norms. Recent work shows that LLMs can engage in multi-turn negotiation, yet nearly all evaluations occur exclusively in English. Using controlled multi-agent simulations across Ultimatum, Buy-Sell, and Resource Exchange games, we systematically isolate language effects across English and four Indic framings (Hindi, Punjabi, Gujarati, Marwadi) by holding game rules, model parameters, and incentives constant across all conditions. We find that language choice can shift outcomes more strongly than changing models, reversing proposer advantages and reallocating surplus. Crucially, effects are task-contingent: Indic languages reduce stability in distributive games yet induce richer exploration in integrative settings. Our results demonstrate that evaluating LLM negotiation solely in English yields incomplete and potentially misleading conclusions. These findings caution against English-only evaluation of LLMs and suggest that culturally-aware evaluation is essential for fair deployment.

</details>


### [191] [LLM-Guided Lifecycle-Aware Clustering of Multi-Turn Customer Support Conversations](https://arxiv.org/abs/2601.04388)
*Priyaranjan Pattnayak,Sanchari Chowdhuri,Amit Agarwal,Hitesh Laxmichand Patel*

Main category: cs.AI

TL;DR: Adaptive clustering system for customer chat data that segments multi-turn chats into service-specific concerns and incrementally refines clusters using LLM-based splitting only for degraded clusters, avoiding full reclustering.


<details>
  <summary>Details</summary>
Motivation: Traditional clustering methods struggle with overlapping concerns in customer chat data, create broad static clusters that degrade over time, and require disruptive full reclustering that breaks continuity in issue tracking.

Method: Segments multi-turn chats into service-specific concerns, tracks cluster quality via Davies-Bouldin Index and Silhouette Scores, and applies LLM-based splitting only to degraded clusters for incremental refinement.

Result: Improves Silhouette Scores by over 100% and reduces Davies-Bouldin Index by 65.6% compared to baselines, enabling scalable real-time analytics without full reclustering.

Conclusion: The proposed adaptive system provides effective, scalable clustering for customer chat data that maintains continuity while adapting to new issues, overcoming limitations of traditional static clustering approaches.

Abstract: Clustering customer chat data is vital for cloud providers handling multi service queries. Traditional methods struggle with overlapping concerns and create broad, static clusters that degrade over time. Reclustering disrupts continuity, making issue tracking difficult. We propose an adaptive system that segments multi turn chats into service specific concerns and incrementally refines clusters as new issues arise. Cluster quality is tracked via DaviesBouldin Index and Silhouette Scores, with LLM based splitting applied only to degraded clusters. Our method improves Silhouette Scores by over 100\% and reduces DBI by 65.6\% compared to baselines, enabling scalable, real time analytics without full reclustering.

</details>


### [192] [SciFig: Towards Automating Scientific Figure Generation](https://arxiv.org/abs/2601.04390)
*Siyuan Huang,Yutong Gao,Juyang Bai,Yifan Zhou,Zi Yin,Xinxin Liu,Rama Chellappa,Chun Pong Lau,Sayan Nag,Cheng Peng,Shraman Pramanick*

Main category: cs.AI

TL;DR: SciFig is an AI agent system that automatically generates publication-ready pipeline figures from research paper texts using hierarchical layout generation and iterative feedback.


<details>
  <summary>Details</summary>
Motivation: Figure generation for scientific papers is time-consuming and requires both domain expertise and design skills, yet remains largely manual despite millions of papers published annually.

Method: Uses hierarchical layout generation to parse research descriptions, identify component relationships, group elements into functional modules, and generate inter-module connections. Includes iterative chain-of-thought feedback mechanism for progressive layout improvement.

Result: Achieves 70.1% overall quality on dataset-level evaluation and 66.2% on paper-specific evaluation, with consistently high scores across visual clarity, structural organization, and scientific accuracy metrics.

Conclusion: SciFig demonstrates effective automated figure generation for scientific papers and will be open-sourced along with its evaluation benchmark.

Abstract: Creating high-quality figures and visualizations for scientific papers is a time-consuming task that requires both deep domain knowledge and professional design skills. Despite over 2.5 million scientific papers published annually, the figure generation process remains largely manual. We introduce $\textbf{SciFig}$, an end-to-end AI agent system that generates publication-ready pipeline figures directly from research paper texts. SciFig uses a hierarchical layout generation strategy, which parses research descriptions to identify component relationships, groups related elements into functional modules, and generates inter-module connections to establish visual organization. Furthermore, an iterative chain-of-thought (CoT) feedback mechanism progressively improves layouts through multiple rounds of visual analysis and reasoning. We introduce a rubric-based evaluation framework that analyzes 2,219 real scientific figures to extract evaluation rubrics and automatically generates comprehensive evaluation criteria. SciFig demonstrates remarkable performance: achieving 70.1$\%$ overall quality on dataset-level evaluation and 66.2$\%$ on paper-specific evaluation, and consistently high scores across metrics such as visual clarity, structural organization, and scientific accuracy. SciFig figure generation pipeline and our evaluation benchmark will be open-sourced.

</details>


### [193] [Assessing the quality and coherence of word embeddings after SCM-based intersectional bias mitigation](https://arxiv.org/abs/2601.04393)
*Eren Kocadag,Seyed Sahand Mohammadi Ziabari,Ali Mohammed Mansoor Alsahag*

Main category: cs.AI

TL;DR: The paper extends SCM-based debiasing from single-group to intersectional bias in word embeddings, testing three debiasing methods on three embedding families, finding trade-offs between geometry preservation and analogy performance.


<details>
  <summary>Details</summary>
Motivation: Static word embeddings absorb social biases that can influence downstream systems. Prior work using the Stereotype Content Model (SCM) focused on single-group bias, but this paper aims to address intersectional bias by considering compound representations of social identity pairs.

Method: Builds compound representations for pairs of social identities through summation or concatenation, then applies three debiasing strategies: Subtraction, Linear Projection, and Partial Projection. Tests on three embedding families (Word2Vec, GloVe, ConceptNet Numberbatch) and assesses utility through local neighborhood coherence and analogy behavior preservation.

Result: SCM-based mitigation extends well to intersectional cases while largely preserving semantic structure. Trade-off exists: methods preserving geometry better are more cautious about analogy behavior, while more assertive projections improve analogies at the expense of neighborhood stability. Partial Projection is conservative, Linear Projection more assertive, Subtraction remains competitive. Choice between summation/concatenation depends on embedding family and application goals.

Conclusion: Intersectional debiasing with SCM is practical in static embeddings, with guidance provided for selecting aggregation and debiasing methods when balancing stability against analogy performance.

Abstract: Static word embeddings often absorb social biases from the text they learn from, and those biases can quietly shape downstream systems. Prior work that uses the Stereotype Content Model (SCM) has focused mostly on single-group bias along warmth and competence. We broaden that lens to intersectional bias by building compound representations for pairs of social identities through summation or concatenation, and by applying three debiasing strategies: Subtraction, Linear Projection, and Partial Projection. We study three widely used embedding families (Word2Vec, GloVe, and ConceptNet Numberbatch) and assess them with two complementary views of utility: whether local neighborhoods remain coherent and whether analogy behavior is preserved. Across models, SCM-based mitigation carries over well to the intersectional case and largely keeps the overall semantic landscape intact. The main cost is a familiar trade off: methods that most tightly preserve geometry tend to be more cautious about analogy behavior, while more assertive projections can improve analogies at the expense of strict neighborhood stability. Partial Projection is reliably conservative and keeps representations steady; Linear Projection can be more assertive; Subtraction is a simple baseline that remains competitive. The choice between summation and concatenation depends on the embedding family and the application goal. Together, these findings suggest that intersectional debiasing with SCM is practical in static embed- dings, and they offer guidance for selecting aggregation and debiasing settings when balancing stability against analogy performance.

</details>


### [194] [Transitive Expert Error and Routing Problems in Complex AI Systems](https://arxiv.org/abs/2601.04416)
*Forest Mars*

Main category: cs.AI

TL;DR: Experts make systematic errors at domain boundaries due to structural similarity bias and authority persistence, creating confident but causally incorrect outputs - a phenomenon called Transitive Expert Error (TEE) that also affects AI routing systems.


<details>
  <summary>Details</summary>
Motivation: To understand why domain experts, despite their expertise, make systematic errors specifically at the boundaries of their domains, and to explore how similar vulnerabilities manifest in AI systems with routing architectures.

Method: Conceptual analysis identifying two core mechanisms (structural similarity bias and authority persistence) that operate under specific conditions (shared vocabulary masking, social pressure, delayed feedback), then extending this framework to analyze AI routing architectures.

Result: Identified Transitive Expert Error (TEE) as a distinct phenomenon from Dunning-Kruger effects, showing how expert judgment mechanisms become liabilities at domain boundaries, and demonstrating parallel vulnerabilities in AI systems (routing-induced and coverage-induced failures).

Conclusion: TEE produces detectable signatures in both human and AI systems, and while intractable in human cognition, it becomes addressable in AI through architectural interventions like multi-expert activation, boundary-aware calibration, and coverage gap detection.

Abstract: Domain expertise enhances judgment within boundaries but creates systematic vulnerabilities specifically at borders. We term this Transitive Expert Error (TEE), distinct from Dunning-Kruger effects, requiring calibrated expertise as precondition. Mechanisms enabling reliable within-domain judgment become liabilities when structural similarity masks causal divergence. Two core mechanisms operate: structural similarity bias causes experts to overweight surface features (shared vocabulary, patterns, formal structure) while missing causal architecture differences; authority persistence maintains confidence across competence boundaries through social reinforcement and metacognitive failures (experts experience no subjective uncertainty as pattern recognition operates smoothly on familiar-seeming inputs.) These mechanism intensify under three conditions: shared vocabulary masking divergent processes, social pressure for immediate judgment, and delayed feedback. These findings extend to AI routing architectures (MoE systems, multi-model orchestration, tool-using agents, RAG systems) exhibiting routing-induced failures (wrong specialist selected) and coverage-induced failures (no appropriate specialist exists). Both produce a hallucination phenotype: confident, coherent, structurally plausible but causally incorrect outputs at domain boundaries. In human systems where mechanisms are cognitive black boxes; AI architectures make them explicit and addressable. We propose interventions: multi-expert activation with disagreement detection (router level), boundary-aware calibration (specialist level), and coverage gap detection (training level). TEE has detectable signatures (routing patterns, confidence-accuracy dissociations, domain-inappropriate content) enabling monitoring and mitigation. What remains intractable in human cognition becomes addressable through architectural design.

</details>


### [195] [XGrammar 2: Dynamic and Efficient Structured Generation Engine for Agentic LLMs](https://arxiv.org/abs/2601.04426)
*Linzhang Li,Yixin Dong,Guanjie Wang,Ziyi Xu,Alexander Jiang,Tianqi Chen*

Main category: cs.AI

TL;DR: XGrammar 2 is an optimized structured generation engine for LLM agents that accelerates dynamic tasks like tool calling through TagDispatch semantics, JIT compilation, cross-grammar caching, and improved parsing algorithms, achieving 6x speedup over existing engines.


<details>
  <summary>Details</summary>
Motivation: Modern LLM agents need to handle complex dynamic structured generation tasks (tool calling, conditional generation) that are more challenging than predefined structures, requiring better optimization than current structured generation engines provide.

Method: Introduces TagDispatch dynamic dispatching semantics for mask generation acceleration, JIT compilation to reduce compilation time, cross-grammar caching for common sub-structures, extends PDA-based mask generation to Earley-parser-based algorithm, and adds repetition compression algorithm for grammar repetition structures.

Result: Achieves more than 6x speedup over existing structured generation engines, and when integrated with LLM inference engine, handles dynamic structured generation tasks with near-zero overhead.

Conclusion: XGrammar 2 provides a highly optimized solution for dynamic structured generation in LLM agents, significantly outperforming existing engines while maintaining minimal overhead when integrated with LLM inference systems.

Abstract: Modern LLM agents are required to handle increasingly complex structured generation tasks, such as tool calling and conditional structured generation. These tasks are significantly more dynamic than predefined structures, posing new challenges to the current structured generation engines. In this paper, we propose XGrammar 2, a highly optimized structured generation engine for agentic LLMs. XGrammar 2 accelerates the mask generation for these dynamic structured generation tasks through a new dynamic dispatching semantics: TagDispatch. We further introduce a just-in-time (JIT) compilation method to reduce compilation time and a cross-grammar caching mechanism to leverage the common sub-structures across different grammars. Additionally, we extend the previous PDA-based mask generation algorithm to the Earley-parser-based one and design a repetition compression algorithm to handle repetition structures in grammars. Evaluation results show that XGrammar 2 can achieve more than 6x speedup over the existing structured generation engines. Integrated with an LLM inference engine, XGrammar 2 can handle dynamic structured generation tasks with near-zero overhead.

</details>


### [196] [Categorical Belief Propagation: Sheaf-Theoretic Inference via Descent and Holonomy](https://arxiv.org/abs/2601.04456)
*Enrique ter Horst,Sridhar Mahadevan,Juan Diego Zambrano*

Main category: cs.AI

TL;DR: A categorical framework for belief propagation that unifies exact inference, junction trees, and loopy BP failures using sheaf theory, with a new HATCC algorithm for exact inference with improved complexity.


<details>
  <summary>Details</summary>
Motivation: To provide a rigorous categorical foundation for belief propagation that unifies various inference approaches (exact, junction tree, loopy BP) and enables systematic analysis of when exact inference is possible.

Method: Constructs free hypergraph categories for compositional semantics, formulates message-passing via Grothendieck fibrations over polarized factor graphs, characterizes exact inference as effective descent, and introduces HATCC algorithm that detects descent obstructions via holonomy computation and compiles them into mode variables.

Result: Theoretical framework unifies tree exactness, junction tree algorithms, and loopy BP failures under sheaf-theoretic obstructions. HATCC algorithm achieves exact inference with complexity O(n²d_max + c·k_max·δ_max³ + n·δ_max²) and demonstrates significant speedup over junction trees on grid MRFs and random graphs, plus UNSAT detection on satisfiability instances.

Conclusion: The categorical framework provides a unified foundation for belief propagation, with HATCC offering practical exact inference that outperforms traditional junction tree methods while maintaining theoretical guarantees through descent theory and holonomy analysis.

Abstract: We develop a categorical foundation for belief propagation on factor graphs. We construct the free hypergraph category \(\Syn_Σ\) on a typed signature and prove its universal property, yielding compositional semantics via a unique functor to the matrix category \(\cat{Mat}_R\). Message-passing is formulated using a Grothendieck fibration \(\int\Msg \to \cat{FG}_Σ\) over polarized factor graphs, with schedule-indexed endomorphisms defining BP updates. We characterize exact inference as effective descent: local beliefs form a descent datum when compatibility conditions hold on overlaps. This framework unifies tree exactness, junction tree algorithms, and loopy BP failures under sheaf-theoretic obstructions. We introduce HATCC (Holonomy-Aware Tree Compilation), an algorithm that detects descent obstructions via holonomy computation on the factor nerve, compiles non-trivial holonomy into mode variables, and reduces to tree BP on an augmented graph. Complexity is \(O(n^2 d_{\max} + c \cdot k_{\max} \cdot δ_{\max}^3 + n \cdot δ_{\max}^2)\) for \(n\) factors and \(c\) fundamental cycles. Experimental results demonstrate exact inference with significant speedup over junction trees on grid MRFs and random graphs, along with UNSAT detection on satisfiability instances.

</details>


### [197] [Computational Compliance for AI Regulation: Blueprint for a New Research Domain](https://arxiv.org/abs/2601.04474)
*Bill Marino,Nicholas D. Lane*

Main category: cs.AI

TL;DR: Paper proposes computational algorithms for AI regulation compliance and provides design goals and benchmark dataset for this new research domain.


<details>
  <summary>Details</summary>
Motivation: AI regulations are emerging but current compliance methods are too slow and manual; computational approaches are needed for automated, scalable compliance across AI system lifecycles.

Method: Specifies design goals for computational AIR compliance algorithms and creates a benchmark dataset to quantitatively measure algorithm performance against these goals.

Result: Establishes a blueprint for computational AIR compliance research, including design principles and evaluation framework to guide future algorithm development.

Conclusion: Computational compliance is essential for effective AI regulation, and this work provides foundational specifications to shape and accelerate research in this emerging field.

Abstract: The era of AI regulation (AIR) is upon us. But AI systems, we argue, will not be able to comply with these regulations at the necessary speed and scale by continuing to rely on traditional, analogue methods of compliance. Instead, we posit that compliance with these regulations will only realistically be achieved computationally: that is, with algorithms that run across the life cycle of an AI system, automatically steering it toward AIR compliance in the face of dynamic conditions. Yet despite their (we would argue) inevitability, the research community has yet to specify exactly how these algorithms for computational AIR compliance should behave - or how we should benchmark their performance. To fill these gaps, we specify a set of design goals for such algorithms. In addition, we specify a benchmark dataset that can be used to quantitatively measure whether individual algorithms satisfy these design goals. By delivering this blueprint, we hope to give shape to an important but uncrystallized new domain of research - and, in doing so, incite necessary investment in it.

</details>


### [198] [A Closed-Loop Multi-Agent System Driven by LLMs for Meal-Level Personalized Nutrition Management](https://arxiv.org/abs/2601.04491)
*Muqing Xu*

Main category: cs.AI

TL;DR: A multi-agent LLM system for personalized nutrition that combines image-based meal logging with AI agents to provide closed-loop dietary recommendations.


<details>
  <summary>Details</summary>
Motivation: Existing nutrition systems handle food logging, nutrient analysis, and recommendations separately, lacking integrated personalized support. There's a need for a unified system that provides closed-loop, meal-level guidance.

Method: Developed a mobile nutrition assistant with image-based meal logging and LLM-driven multi-agent controller. The system coordinates vision, dialogue, and state management agents to estimate nutrients from photos, update daily intake budgets, and adapt meal plans to user preferences and constraints.

Result: Experiments with SNAPMe meal images and simulated users showed competitive nutrient estimation, personalized menus, and efficient task plans. The system demonstrated feasibility of multi-agent LLM control for personalized nutrition.

Conclusion: The approach demonstrates the feasibility of multi-agent LLM control for personalized nutrition, but reveals challenges in micronutrient estimation from images and the need for large-scale real-world studies.

Abstract: Personalized nutrition management aims to tailor dietary guidance to an individual's intake and phenotype, but most existing systems handle food logging, nutrient analysis and recommendation separately. We present a next-generation mobile nutrition assistant that combines image based meal logging with an LLM driven multi agent controller to provide meal level closed loop support. The system coordinates vision, dialogue and state management agents to estimate nutrients from photos and update a daily intake budget. It then adapts the next meal plan to user preferences and dietary constraints. Experiments with SNAPMe meal images and simulated users show competitive nutrient estimation, personalized menus and efficient task plans. These findings demonstrate the feasibility of multi agent LLM control for personalized nutrition and reveal open challenges in micronutrient estimation from images and in large scale real world studies.

</details>


### [199] [GUITester: Enabling GUI Agents for Exploratory Defect Discovery](https://arxiv.org/abs/2601.04500)
*Yifei Gao,Jiang Wu,Xiaoyi Chen,Yifan Yang,Zhe Cui,Tianyi Ma,Jiaming Zhang,Jitao Sang*

Main category: cs.AI

TL;DR: GUITester: A multi-agent framework for autonomous exploratory GUI testing that addresses goal-oriented masking and execution-bias attribution challenges, achieving 48.90% F1-score on the new GUITestBench benchmark.


<details>
  <summary>Details</summary>
Motivation: Exploratory GUI testing is crucial for software quality but suffers from high manual costs. Current MLLM agents excel at navigation but fail to autonomously discover defects due to two core challenges: goal-oriented masking (prioritizing task completion over reporting anomalies) and execution-bias attribution (misidentifying system defects as agent errors).

Method: Proposes GUITester, a multi-agent framework with two key modules: 1) Planning-Execution Module (PEM) that proactively probes for defects via embedded testing intents, and 2) Hierarchical Reflection Module (HRM) that resolves attribution ambiguity through interaction history analysis. Also introduces GUITestBench, the first interactive benchmark with 143 tasks across 26 defects.

Result: GUITester achieves an F1-score of 48.90% (Pass@3) on GUITestBench, significantly outperforming state-of-the-art baselines (33.35%). Demonstrates the feasibility of autonomous exploratory GUI testing.

Conclusion: The work successfully addresses key challenges in autonomous GUI testing and provides a robust foundation for future GUI quality assurance. The proposed framework enables effective defect discovery while overcoming the limitations of current MLLM agents in exploratory testing scenarios.

Abstract: Exploratory GUI testing is essential for software quality but suffers from high manual costs. While Multi-modal Large Language Model (MLLM) agents excel in navigation, they fail to autonomously discover defects due to two core challenges: \textit{Goal-Oriented Masking}, where agents prioritize task completion over reporting anomalies, and \textit{Execution-Bias Attribution}, where system defects are misidentified as agent errors. To address these, we first introduce \textbf{GUITestBench}, the first interactive benchmark for this task, featuring 143 tasks across 26 defects. We then propose \textbf{GUITester}, a multi-agent framework that decouples navigation from verification via two modules: (i) a \textit{Planning-Execution Module (PEM)} that proactively probes for defects via embedded testing intents, and (ii) a \textit{Hierarchical Reflection Module (HRM)} that resolves attribution ambiguity through interaction history analysis. GUITester achieves an F1-score of 48.90\% (Pass@3) on GUITestBench, outperforming state-of-the-art baselines (33.35\%). Our work demonstrates the feasibility of autonomous exploratory testing and provides a robust foundation for future GUI quality assurance~\footnote{Our code is now available in~\href{https://github.com/ADaM-BJTU/GUITestBench}{https://github.com/ADaM-BJTU/GUITestBench}}.

</details>


### [200] [Specific Emitter Identification via Active Learning](https://arxiv.org/abs/2601.04502)
*Jingyi Wang,Fanggang Wang*

Main category: cs.AI

TL;DR: Proposes an active learning-enhanced SEI approach with three-stage semi-supervised training to reduce labeling costs while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: SEI is important for wireless communication security but requires large labeled datasets that are expensive and time-consuming to obtain. Need to reduce labeling costs while maintaining performance.

Method: Three-stage semi-supervised approach: 1) Self-supervised contrastive learning with dynamic dictionary update on unlabeled data, 2) Supervised training with joint contrastive and cross-entropy loss on small labeled data, 3) Active learning module selecting valuable samples based on uncertainty and representativeness for annotation.

Result: Outperforms conventional supervised and semi-supervised methods on ADS-B and WiFi datasets under limited annotation conditions, achieving higher recognition accuracy with lower labeling cost.

Conclusion: The proposed active learning-enhanced SEI approach effectively reduces labeling requirements while maintaining high performance, making SEI more practical for real-world applications with limited annotation budgets.

Abstract: With the rapid growth of wireless communications, specific emitter identification (SEI) is significant for communication security. However, its model training relies heavily on the large-scale labeled data, which are costly and time-consuming to obtain. To address this challenge, we propose an SEI approach enhanced by active learning (AL), which follows a three-stage semi-supervised training scheme. In the first stage, self-supervised contrastive learning is employed with a dynamic dictionary update mechanism to extract robust representations from large amounts of the unlabeled data. In the second stage, supervised training on a small labeled dataset is performed, where the contrastive and cross-entropy losses are jointly optimized to improve the feature separability and strengthen the classification boundaries. In the third stage, an AL module selects the most valuable samples from the unlabeled data for annotation based on the uncertainty and representativeness criteria, further enhancing generalization under limited labeling budgets. Experimental results on the ADS-B and WiFi datasets demonstrate that the proposed SEI approach significantly outperforms the conventional supervised and semi-supervised methods under limited annotation conditions, achieving higher recognition accuracy with lower labeling cost.

</details>


### [201] [CircuitLM: A Multi-Agent LLM-Aided Design Framework for Generating Circuit Schematics from Natural Language Prompts](https://arxiv.org/abs/2601.04505)
*Khandakar Shakib Al Hasan,Syed Rifat Raiyan,Hasin Mahtab Alvee,Wahid Sadik*

Main category: cs.AI

TL;DR: CircuitLM: A multi-agent LLM pipeline that converts natural language circuit descriptions into structured CircuitJSON schematics, addressing LLM hallucination issues through component grounding and validation.


<details>
  <summary>Details</summary>
Motivation: LLMs frequently hallucinate circuit details, violate electrical constraints, and produce non-machine-readable outputs when generating circuit schematics from natural language descriptions, creating a gap between natural language input and deployable hardware designs.

Method: Five-stage pipeline: (1) LLM-based component identification, (2) canonical pinout retrieval from knowledge base, (3) chain-of-thought reasoning by electronics expert agent, (4) JSON schematic synthesis, (5) force-directed SVG visualization. Uses curated component database (50 components initially) and Dual-Metric Circuit Validation (DMCV) framework.

Result: Evaluated on 100 diverse embedded-systems prompts across six LLMs. DMCV achieves high fidelity in microcontroller-centric designs and shows strong correlation with human-expert assessments for both structural and electrical validity.

Conclusion: CircuitLM bridges natural language input to deployable hardware designs, enabling reliable circuit prototyping by non-experts through grounded component generation and hybrid validation.

Abstract: Generating accurate circuit schematics from high-level natural language descriptions remains a persistent challenge in electronics design, as large language models (LLMs) frequently hallucinate in granular details, violate electrical constraints, and produce non-machine-readable outputs. We present CircuitLM, a novel multi-agent LLM-aided circuit design pipeline that translates user prompts into structured, visually interpretable CircuitJSON schematics through five sequential stages: (i) LLM-based component identification, (ii) canonical pinout retrieval, (iii) chain-of-thought reasoning by an electronics expert agent, (iv) JSON schematic synthesis, and (v) force-directed SVG visualization. Anchored by a curated, embedding-powered component knowledge base. While LLMs often violate electrical constraints, CircuitLM bridges this gap by grounding generation in a verified and dynamically extensible component database, initially comprising 50 components. To ensure safety, we incorporate a hybrid evaluation framework, namely Dual-Metric Circuit Validation (DMCV), validated against human-expert assessments, which achieves high fidelity in microcontroller-centric designs. We evaluate the system on 100 diverse embedded-systems prompts across six LLMs and introduce DMCV to assess both structural and electrical validity. This work bridges natural language input to deployable hardware designs, enabling reliable circuit prototyping by non-experts. Our code and data will be made public upon acceptance.

</details>


### [202] [A General Neural Backbone for Mixed-Integer Linear Optimization via Dual Attention](https://arxiv.org/abs/2601.04509)
*Peixin Huang,Yaoxin Wu,Yining Ma,Cathy Wu,Wen Song,Wei Zhang*

Main category: cs.AI

TL;DR: The paper presents an attention-based neural architecture for MILP that overcomes limitations of GNNs by using dual-attention mechanisms for global information exchange, achieving state-of-the-art performance across multiple MILP tasks.


<details>
  <summary>Details</summary>
Motivation: Current GNN-based approaches for MILP are limited by local-oriented mechanisms, restricting representation power and hindering neural approaches for mixed-integer linear programming. There's a need for more expressive representations beyond pure graph views.

Method: Proposes an attention-driven neural architecture with dual-attention mechanism that performs parallel self- and cross-attention over variables and constraints, enabling global information exchange and deeper representation learning.

Result: Extensive experiments across widely used benchmarks show consistent improvements over state-of-the-art baselines, demonstrating the effectiveness of the attention-based approach.

Conclusion: Attention-based neural architectures serve as a powerful foundation for learning-enhanced mixed-integer linear optimization, overcoming limitations of traditional GNN approaches.

Abstract: Mixed-integer linear programming (MILP), a widely used modeling framework for combinatorial optimization, are central to many scientific and engineering applications, yet remains computationally challenging at scale. Recent advances in deep learning address this challenge by representing MILP instances as variable-constraint bipartite graphs and applying graph neural networks (GNNs) to extract latent structural patterns and enhance solver efficiency. However, this architecture is inherently limited by the local-oriented mechanism, leading to restricted representation power and hindering neural approaches for MILP. Here we present an attention-driven neural architecture that learns expressive representations beyond the pure graph view. A dual-attention mechanism is designed to perform parallel self- and cross-attention over variables and constraints, enabling global information exchange and deeper representation learning. We apply this general backbone to various downstream tasks at the instance level, element level, and solving state level. Extensive experiments across widely used benchmarks show consistent improvements of our approach over state-of-the-art baselines, highlighting attention-based neural architectures as a powerful foundation for learning-enhanced mixed-integer linear optimization.

</details>


### [203] [Integrating Distribution Matching into Semi-Supervised Contrastive Learning for Labeled and Unlabeled Data](https://arxiv.org/abs/2601.04518)
*Shogo Nakayama,Masahiro Okuda*

Main category: cs.AI

TL;DR: This paper proposes enhancing pseudo-label-based semi-supervised learning by incorporating distribution matching between labeled and unlabeled feature embeddings to improve image classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Deep learning requires costly data labeling, making unsupervised methods like contrastive learning attractive. However, real-world scenarios typically involve both labeled and unlabeled data, making semi-supervised learning highly relevant. Existing pseudo-label-based SSL methods can be improved by better leveraging the relationship between labeled and unlabeled data distributions.

Method: The method enhances pseudo-label-based SSL by incorporating distribution matching between labeled and unlabeled feature embeddings. This likely involves aligning the feature distributions of labeled and unlabeled data to improve the quality of pseudo-labels and overall classification performance.

Result: The study aims to demonstrate improved image classification accuracy across multiple datasets by incorporating distribution matching into pseudo-label-based SSL frameworks.

Conclusion: Distribution matching between labeled and unlabeled feature embeddings can effectively enhance pseudo-label-based semi-supervised learning, providing a practical solution for real-world scenarios with limited labeled data.

Abstract: The advancement of deep learning has greatly improved supervised image classification. However, labeling data is costly, prompting research into unsupervised learning methods such as contrastive learning. In real-world scenarios, fully unlabeled datasets are rare, making semi-supervised learning (SSL) highly relevant in scenarios where a small amount of labeled data coexists with a large volume of unlabeled data. A well-known semi-supervised contrastive learning approach involves assigning pseudo-labels to unlabeled data. This study aims to enhance pseudo-label-based SSL by incorporating distribution matching between labeled and unlabeled feature embeddings to improve image classification accuracy across multiple datasets.

</details>


### [204] [BioPIE: A Biomedical Protocol Information Extraction Dataset for High-Reasoning-Complexity Experiment Question Answer](https://arxiv.org/abs/2601.04524)
*Haofei Hou,Shunyi Zhao,Fanxu Meng,Kairui Yang,Lecheng Ruan,Qining Wang*

Main category: cs.AI

TL;DR: BioPIE dataset provides procedure-centric knowledge graphs for biomedical experiments to address challenges of high information density and multi-step reasoning in QA systems.


<details>
  <summary>Details</summary>
Motivation: Biomedical experimental QA systems face challenges from high information density and multi-step reasoning, while existing datasets lack fine-grained experimental knowledge needed for such reasoning tasks.

Method: Created Biomedical Protocol Information Extraction Dataset (BioPIE) with procedure-centric knowledge graphs containing experimental entities, actions, and relations at scale to support reasoning across protocols.

Result: Evaluation shows performance gains on test, high information density, and multi-step reasoning question sets when using BioPIE, demonstrating its effectiveness for biomedical experimental QA.

Conclusion: BioPIE's structured experimental knowledge supports both AI-assisted and autonomous biomedical experimentation by enabling better reasoning over complex experimental procedures.

Abstract: Question Answer (QA) systems for biomedical experiments facilitate cross-disciplinary communication, and serve as a foundation for downstream tasks, e.g., laboratory automation. High Information Density (HID) and Multi-Step Reasoning (MSR) pose unique challenges for biomedical experimental QA. While extracting structured knowledge, e.g., Knowledge Graphs (KGs), can substantially benefit biomedical experimental QA. Existing biomedical datasets focus on general or coarsegrained knowledge and thus fail to support the fine-grained experimental reasoning demanded by HID and MSR. To address this gap, we introduce Biomedical Protocol Information Extraction Dataset (BioPIE), a dataset that provides procedure-centric KGs of experimental entities, actions, and relations at a scale that supports reasoning over biomedical experiments across protocols. We evaluate information extraction methods on BioPIE, and implement a QA system that leverages BioPIE, showcasing performance gains on test, HID, and MSR question sets, showing that the structured experimental knowledge in BioPIE underpins both AI-assisted and more autonomous biomedical experimentation.

</details>


### [205] [TCAndon-Router: Adaptive Reasoning Router for Multi-Agent Collaboration](https://arxiv.org/abs/2601.04544)
*Jiuzhou Zhao,Chunrong Chen,Chenqi Qiao,Lebin Zheng,Minqi Han,Yanchi Liu Yongzhou Xu Xiaochuan Xu Min Zhang*

Main category: cs.AI

TL;DR: TCAR is an adaptive reasoning router for multi-agent systems that uses natural language reasoning chains to select multiple candidate agents, supports dynamic agent onboarding, and employs collaborative execution with a refining agent to produce high-quality responses.


<details>
  <summary>Details</summary>
Motivation: Existing task routing approaches for multi-agent systems rely on static single-label decisions, which have two major limitations: difficulty integrating new agents as business domains expand, and routing conflicts caused by overlapping agent capabilities, ultimately degrading accuracy and robustness.

Method: TCAR generates natural-language reasoning chains before predicting sets of candidate agents, supports dynamic agent onboarding, and uses a collaborative execution pipeline where selected agents independently produce responses that are aggregated and refined by a dedicated Refining Agent.

Result: Experiments on public datasets and real enterprise data show that TCAR significantly improves routing accuracy, reduces routing conflicts, and remains robust in ambiguous scenarios.

Conclusion: TCAR addresses the limitations of traditional routing approaches by providing an adaptive, explainable, and collaborative multi-agent routing solution that supports dynamic agent integration and handles overlapping capabilities effectively.

Abstract: Multi-Agent Systems(MAS) have become a powerful paradigm for building high performance intelligent applications. Within these systems, the router responsible for determining which expert agents should handle a given query plays a crucial role in overall performance. Existing routing strategies generally fall into two categories: performance routing, which balances latency and cost across models of different sizes, and task routing, which assigns queries to domain-specific experts to improve accuracy. In real-world enterprise applications, task routing is more suitable; however, most existing approaches rely on static single-label decisions, which introduce two major limitations: (i) difficulty in seamlessly integrating new agents as business domains expand, and (ii) routing conflicts caused by overlapping agent capabilities, ultimately degrading accuracy and robustness.To address these challenges, we propose TCAndon-Router(TCAR): an adaptive reasoning router for multi-agent collaboration. Unlike traditional routers, TCAR supports dynamic agent onboarding and first generates a natural-language reasoning chain before predicting a set of candidate agents capable of handling the query. In addition, we design a collaborative execution pipeline in which selected agents independently produce responses, which are then aggregated and refined into a single high-quality response by a dedicated Refining Agent.Experiments on public datasets and real enterprise data demonstrate that TCAR significantly improves routing accuracy, reduces routing conflicts, and remains robust in ambiguous scenarios. We have released TCAR at https://huggingface.co/tencent/TCAndon-Router to support future research on explainable and collaborative multi-agent routing.

</details>


### [206] [Personalized Model-Based Design of Human Centric AI enabled CPS for Long term usage](https://arxiv.org/abs/2601.04545)
*Bernard Ngabonziza,Ayan Banerjee,Sandeep K. S. Gupta*

Main category: cs.AI

TL;DR: The paper analyzes limitations of existing safety/security techniques for AI-enabled human-centric systems in long-term operation, and proposes personalized model-based solutions to address corner case uncertainties.


<details>
  <summary>Details</summary>
Motivation: AI-enabled human-centric systems (medical monitoring, autonomous cars, etc.) operate long-term and face corner cases where performance is uncertain, potentially violating safety, sustainability, and security requirements. Existing techniques have limitations for testing long-term use.

Method: The paper first analyzes existing techniques for safety, sustainability, and security analysis of AI-enabled human-centric control systems. It then proposes personalized model-based solutions to eliminate the identified limitations.

Result: The analysis reveals limitations in current approaches for testing long-term operation. The proposed personalized model-based solutions aim to address these limitations by providing more robust testing methodologies.

Conclusion: Personalized model-based approaches are needed to ensure safety, sustainability, and security of AI-enabled human-centric systems during long-term operation by addressing corner case uncertainties that current testing methods cannot adequately handle.

Abstract: Human centric critical systems are increasingly involving artificial intelligence to enable knowledge extraction from sensor collected data. Examples include medical monitoring and control systems, gesture based human computer interaction systems, and autonomous cars. Such systems are intended to operate for a long term potentially for a lifetime in many scenarios such as closed loop blood glucose control for Type 1 diabetics, self-driving cars, and monitoting systems for stroke diagnosis, and rehabilitation. Long term operation of such AI enabled human centric applications can expose them to corner cases for which their operation is may be uncertain. This can be due to many reasons such as inherent flaws in the design, limited resources for testing, inherent computational limitations of the testing methodology, or unknown use cases resulting from human interaction with the system. Such untested corner cases or cases for which the system performance is uncertain can lead to violations in the safety, sustainability, and security requirements of the system. In this paper, we analyze the existing techniques for safety, sustainability, and security analysis of an AI enabled human centric control system and discuss their limitations for testing the system for long term use in practice. We then propose personalized model based solutions for potentially eliminating such limitations.

</details>


### [207] [Reasoning Over Space: Enabling Geographic Reasoning for LLM-Based Generative Next POI Recommendation](https://arxiv.org/abs/2601.04562)
*Dongyi Lv,Qiuyu Ding,Heng-Da Xu,Zhaoxu Sun,Zhi Wang,Feng Xiong,Mu Xu*

Main category: cs.AI

TL;DR: ROS is a framework that integrates geographic reasoning into LLM-based recommendation systems using hierarchical spatial tokens and a three-stage mobility chain-of-thought approach.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based recommenders fail to effectively leverage geographic signals, which are crucial for mobility and local-services scenarios where location context is essential for accurate recommendations.

Method: ROS introduces: 1) Hierarchical Spatial Semantic ID (SID) that discretizes locality and POI semantics into compositional tokens, 2) Three-stage Mobility Chain-of-Thought paradigm modeling user personality, intent-aligned candidate space, and locality-informed pruning, and 3) Spatial-guided Reinforcement Learning for real-world geography alignment.

Result: Experiments on three LBSN datasets show ROS achieves over 10% relative gains in hit rate compared to strongest LLM-based baselines and improves cross-city transfer performance, despite using a smaller backbone model.

Conclusion: ROS successfully integrates geographic reasoning into LLM-based recommendation systems, demonstrating that explicit geographic modeling significantly improves recommendation accuracy and transferability in location-based scenarios.

Abstract: Generative recommendation with large language models (LLMs) reframes prediction as sequence generation, yet existing LLM-based recommenders remain limited in leveraging geographic signals that are crucial in mobility and local-services scenarios. Here, we present Reasoning Over Space (ROS), a framework that utilizes geography as a vital decision variable within the reasoning process. ROS introduces a Hierarchical Spatial Semantic ID (SID) that discretizes coarse-to-fine locality and POI semantics into compositional tokens, and endows LLM with a three-stage Mobility Chain-of-Thought (CoT) paradigm that models user personality, constructs an intent-aligned candidate space, and performs locality informed pruning. We further align the model with real world geography via spatial-guided Reinforcement Learning (RL). Experiments on three widely used location-based social network (LBSN) datasets show that ROS achieves over 10% relative gains in hit rate over strongest LLM-based baselines and improves cross-city transfer, despite using a smaller backbone model.

</details>


### [208] [BackdoorAgent: A Unified Framework for Backdoor Attacks on LLM-based Agents](https://arxiv.org/abs/2601.04566)
*Yunhao Feng,Yige Li,Yutao Wu,Yingshui Tan,Yanming Guo,Yifan Ding,Kun Zhai,Xingjun Ma,Yugang Jiang*

Main category: cs.AI

TL;DR: BackdoorAgent is a framework for analyzing backdoor threats in LLM agents across planning, memory, and tool-use stages, showing triggers can persist and propagate through agent workflows.


<details>
  <summary>Details</summary>
Motivation: LLM agents' multi-step workflows expand attack surfaces for backdoor threats, but existing studies are fragmented and don't understand cross-stage trigger propagation from an agent-centric perspective.

Method: Proposed BackdoorAgent framework structures attacks into three stages (planning, memory, tool-use), instruments agent execution to analyze trigger activation/propagation, and creates a benchmark across four agent applications.

Result: Triggers implanted at single stages persist across multiple steps and propagate through intermediate states (43.58% persistence in planning attacks, 77.97% in memory attacks, 60.28% in tool-stage attacks with GPT backbone).

Conclusion: Agentic workflows themselves are vulnerable to backdoor threats, with triggers persisting across stages; the BackdoorAgent framework provides unified analysis and benchmark for future research.

Abstract: Large language model (LLM) agents execute tasks through multi-step workflows that combine planning, memory, and tool use. While this design enables autonomy, it also expands the attack surface for backdoor threats. Backdoor triggers injected into specific stages of an agent workflow can persist through multiple intermediate states and adversely influence downstream outputs. However, existing studies remain fragmented and typically analyze individual attack vectors in isolation, leaving the cross-stage interaction and propagation of backdoor triggers poorly understood from an agent-centric perspective. To fill this gap, we propose \textbf{BackdoorAgent}, a modular and stage-aware framework that provides a unified, agent-centric view of backdoor threats in LLM agents. BackdoorAgent structures the attack surface into three functional stages of agentic workflows, including \textbf{planning attacks}, \textbf{memory attacks}, and \textbf{tool-use attacks}, and instruments agent execution to enable systematic analysis of trigger activation and propagation across different stages. Building on this framework, we construct a standardized benchmark spanning four representative agent applications: \textbf{Agent QA}, \textbf{Agent Code}, \textbf{Agent Web}, and \textbf{Agent Drive}, covering both language-only and multimodal settings. Our empirical analysis shows that \textit{triggers implanted at a single stage can persist across multiple steps and propagate through intermediate states.} For instance, when using a GPT-based backbone, we observe trigger persistence in 43.58\% of planning attacks, 77.97\% of memory attacks, and 60.28\% of tool-stage attacks, highlighting the vulnerabilities of the agentic workflow itself to backdoor threats. To facilitate reproducibility and future research, our code and benchmark are publicly available at GitHub.

</details>


### [209] [Neurosymbolic Retrievers for Retrieval-augmented Generation](https://arxiv.org/abs/2601.04568)
*Yash Saxena,Manas Gaur*

Main category: cs.AI

TL;DR: Neurosymbolic RAG integrates symbolic reasoning (knowledge graphs) with neural retrieval to improve transparency and interpretability in RAG systems, addressing the black-box nature of traditional neural components.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG systems have opaque neural components (retriever, re-ranker, generator) that lack transparency, complicating interpretability, hindering debugging, and eroding trust in high-stakes domains where clear decision-making is essential.

Method: Three neurosymbolic methods: 1) MAR (Knowledge Modulation Aligned Retrieval) uses modulation networks to refine query embeddings with interpretable symbolic features; 2) KG-Path RAG enhances queries by traversing knowledge graphs; 3) Process Knowledge-infused RAG reorders retrieved content using domain-specific tools and validated workflows.

Result: Preliminary results from mental health risk assessment tasks show that the neurosymbolic approach enhances both transparency and overall performance compared to traditional RAG systems.

Conclusion: Neurosymbolic RAG successfully addresses interpretability challenges in traditional RAG systems by integrating symbolic reasoning with neural retrieval, improving transparency while maintaining or enhancing performance, particularly in high-stakes domains.

Abstract: Retrieval Augmented Generation (RAG) has made significant strides in overcoming key limitations of large language models, such as hallucination, lack of contextual grounding, and issues with transparency. However, traditional RAG systems consist of three interconnected neural components - the retriever, re-ranker, and generator - whose internal reasoning processes remain opaque. This lack of transparency complicates interpretability, hinders debugging efforts, and erodes trust, especially in high-stakes domains where clear decision-making is essential. To address these challenges, we introduce the concept of Neurosymbolic RAG, which integrates symbolic reasoning using a knowledge graph with neural retrieval techniques. This new framework aims to answer two primary questions: (a) Can retrievers provide a clear and interpretable basis for document selection? (b) Can symbolic knowledge enhance the clarity of the retrieval process? We propose three methods to improve this integration. First is MAR (Knowledge Modulation Aligned Retrieval) that employs modulation networks to refine query embeddings using interpretable symbolic features, thereby making document matching more explicit. Second, KG-Path RAG enhances queries by traversing knowledge graphs to improve overall retrieval quality and interpretability. Lastly, Process Knowledge-infused RAG utilizes domain-specific tools to reorder retrieved content based on validated workflows. Preliminary results from mental health risk assessment tasks indicate that this neurosymbolic approach enhances both transparency and overall performance

</details>


### [210] [Enhancing Multimodal Retrieval via Complementary Information Extraction and Alignment](https://arxiv.org/abs/2601.04571)
*Delong Zeng,Yuexiang Xie,Yaliang Li,Ying Shen*

Main category: cs.AI

TL;DR: CIEA is a novel multimodal retrieval approach that extracts and aligns complementary information from images beyond what's captured in paired texts, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal retrieval methods focus on capturing information similar to paired texts but ignore complementary information in multimodal data that provides additional valuable insights.

Method: CIEA transforms text and images into a unified latent space with a complementary information extractor to identify and preserve differences in image representations, optimized using two complementary contrastive losses.

Result: Extensive experiments show CIEA achieves significant improvements over both divide-and-conquer models and universal dense retrieval models, with ablation studies and case studies validating its effectiveness.

Conclusion: CIEA successfully addresses the complementary information extraction challenge in multimodal retrieval, advancing the field with released source code to promote further research.

Abstract: Multimodal retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in multimodal retrieval focus on capturing information in multimodal data that is similar to their paired texts, but often ignores the complementary information contained in multimodal data. In this study, we propose CIEA, a novel multimodal retrieval approach that employs Complementary Information Extraction and Alignment, which transforms both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.

</details>


### [211] [Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing](https://arxiv.org/abs/2601.04575)
*Yuguang Yue,Irakli Salia,Samuel Hunt,Chris Green,Wenzhe Shi,Jonathan J Hunt*

Main category: cs.AI

TL;DR: Researchers introduce an open-source video game playing foundation model trained on 8300+ hours of human gameplay, showing competitive human-level performance and studying how model/data scaling affects causal reasoning in behavior cloning.


<details>
  <summary>Details</summary>
Motivation: To create an open, realtime-capable foundation model for video game playing and systematically study how scaling laws affect behavior cloning performance and causal reasoning capabilities.

Method: Developed an open recipe for training video game playing models using 8300+ hours of high-quality human gameplay data. Systematically examined scaling laws by varying model size (up to 1.2B parameters) and training data, studying performance and causal reasoning in both toy problems and scaled models.

Result: Best model achieves competitive human-level performance across various 3D video games. Scaling studies show that increasing both training data and network depth improves causal reasoning capabilities, with similar scaling patterns observed in both toy problems and large-scale models.

Conclusion: Behavior cloning scales effectively with model and data size, improving both performance and causal reasoning. The open release of data, code, and models provides a valuable foundation for further research in game-playing AI and scaling studies.

Abstract: Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.

</details>


### [212] [Sci-Reasoning: A Dataset Decoding AI Innovation Patterns](https://arxiv.org/abs/2601.04577)
*Jiachen Liu,Maestro Harmon,Zechen Zhang*

Main category: cs.AI

TL;DR: Sci-Reasoning is the first dataset capturing intellectual synthesis behind AI research breakthroughs, identifying 15 thinking patterns with three dominant strategies accounting for 52.7% of innovations.


<details>
  <summary>Details</summary>
Motivation: While AI innovation accelerates rapidly, the intellectual process behind breakthroughs remains poorly understood. The lack of structured data on scientific reasoning hinders systematic analysis and development of AI research agents.

Method: Using community-validated quality signals and an LLM-accelerated, human-verified pipeline, the authors trace Oral and Spotlight papers across NeurIPS, ICML, and ICLR (2023-2025) to their key predecessors, articulating specific reasoning links in a structured format.

Result: Identified 15 distinct thinking patterns, with three dominant strategies: Gap-Driven Reframing (24.2%), Cross-Domain Synthesis (18.0%), and Representation Shift (10.5%). The most powerful innovation recipes combine multiple patterns.

Conclusion: This dataset enables quantitative studies of scientific progress and provides structured reasoning trajectories for training the next generation AI research agents.

Abstract: While AI innovation accelerates rapidly, the intellectual process behind breakthroughs -- how researchers identify gaps, synthesize prior work, and generate insights -- remains poorly understood. The lack of structured data on scientific reasoning hinders systematic analysis and development of AI research agents. We introduce Sci-Reasoning, the first dataset capturing the intellectual synthesis behind high-quality AI research. Using community-validated quality signals and an LLM-accelerated, human-verified pipeline, we trace Oral and Spotlight papers across NeurIPS, ICML, and ICLR (2023-2025) to its key predecessors, articulating specific reasoning links in a structured format. Our analysis identifies 15 distinct thinking patterns, with three dominant strategies accounting for 52.7%: Gap-Driven Reframing (24.2%), Cross-Domain Synthesis (18.0%), and Representation Shift (10.5%). The most powerful innovation recipes combine multiple patterns: Gap-Driven Reframing + Representation Shift, Cross-Domain Synthesis + Representation Shift, and Gap-Driven Reframing + Cross-Domain Synthesis. This dataset enables quantitative studies of scientific progress and provides structured reasoning trajectories for training the next generation AI research agents.

</details>


### [213] [Autonomous Agents on Blockchains: Standards, Execution Models, and Trust Boundaries](https://arxiv.org/abs/2601.04583)
*Saad Alqithami*

Main category: cs.AI

TL;DR: Survey paper analyzing agent-blockchain interoperability, proposing taxonomy, threat models, and research roadmap for secure AI agent interactions with blockchain systems.


<details>
  <summary>Details</summary>
Motivation: The convergence of AI agents and blockchain creates security challenges requiring standardized, interoperable interfaces for safe agent interactions with on-chain systems.

Method: Systematic literature review of 317 works from 3000+ records, developing taxonomy, threat models, and comparative analysis of 20+ systems across 13 dimensions.

Result: Identified 5 integration patterns, created threat model for agent-driven transactions, and revealed gaps in current systems through comparative capability matrix.

Conclusion: Proposes research roadmap with Transaction Intent Schema and Policy Decision Record abstractions, plus evaluation suite for assessing agent-mediated blockchain execution safety.

Abstract: Advances in large language models have enabled agentic AI systems that can reason, plan, and interact with external tools to execute multi-step workflows, while public blockchains have evolved into a programmable substrate for value transfer, access control, and verifiable state transitions. Their convergence introduces a high-stakes systems challenge: designing standard, interoperable, and secure interfaces that allow agents to observe on-chain state, formulate transaction intents, and authorize execution without exposing users, protocols, or organizations to unacceptable security, governance, or economic risks. This survey systematizes the emerging landscape of agent-blockchain interoperability through a systematic literature review, identifying 317 relevant works from an initial pool of over 3000 records. We contribute a five-part taxonomy of integration patterns spanning read-only analytics, simulation and intent generation, delegated execution, autonomous signing, and multi-agent workflows; a threat model tailored to agent-driven transaction pipelines that captures risks ranging from prompt injection and policy misuse to key compromise, adversarial execution dynamics, and multi-agent collusion; and a comparative capability matrix analyzing more than 20 representative systems across 13 dimensions, including custody models, permissioning, policy enforcement, observability, and recovery. Building on the gaps revealed by this analysis, we outline a research roadmap centered on two interface abstractions: a Transaction Intent Schema for portable and unambiguous goal specification, and a Policy Decision Record for auditable, verifiable policy enforcement across execution environments. We conclude by proposing a reproducible evaluation suite and benchmarks for assessing the safety, reliability, and economic robustness of agent-mediated on-chain execution.

</details>


### [214] [Evaluating Human and Machine Confidence in Phishing Email Detection: A Comparative Study](https://arxiv.org/abs/2601.04610)
*Paras Jain,Khushi Dhar,Olyemi E. Amujo,Esa M. Rantanen*

Main category: cs.AI

TL;DR: This paper examines how human cognition and machine learning models work together to detect phishing emails, comparing interpretable algorithms with human evaluations to understand confidence levels and detection strategies.


<details>
  <summary>Details</summary>
Motivation: Identifying deceptive content like phishing emails requires sophisticated cognitive processes combining pattern recognition, confidence assessment, and contextual analysis. The research aims to understand how human cognition and machine learning models can work together to distinguish phishing emails from legitimate ones.

Method: Used three interpretable algorithms (Logistic Regression, Decision Trees, Random Forests) trained on both TF-IDF features and semantic embeddings. Compared model predictions against human evaluations that captured confidence ratings and linguistic observations.

Result: Machine learning models provide good accuracy rates but their confidence levels vary significantly. Human evaluators use a greater variety of language signs and maintain more consistent confidence. Language proficiency has minimal effect on detection performance, but aging does affect it.

Conclusion: Findings offer helpful direction for creating transparent AI systems that complement human cognitive functions, ultimately improving human-AI cooperation in challenging content analysis tasks like phishing detection.

Abstract: Identifying deceptive content like phishing emails demands sophisticated cognitive processes that combine pattern recognition, confidence assessment, and contextual analysis. This research examines how human cognition and machine learn- ing models work together to distinguish phishing emails from legitimate ones. We employed three interpretable algorithms Logistic Regression, Decision Trees, and Random Forests train- ing them on both TF-IDF features and semantic embeddings, then compared their predictions against human evaluations that captured confidence ratings and linguistic observations. Our results show that machine learning models provide good accuracy rates, but their confidence levels vary significantly. Human evaluators, on the other hand, use a greater variety of language signs and retain more consistent confidence. We also found that while language proficiency has minimal effect on detection performance, aging does. These findings offer helpful direction for creating transparent AI systems that complement human cognitive functions, ultimately improving human-AI cooperation in challenging content analysis tasks.

</details>


### [215] [AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering](https://arxiv.org/abs/2601.04620)
*Di Zhang*

Main category: cs.AI

TL;DR: AgentDevel is a release engineering pipeline for LLM agents that treats agents as shippable artifacts, using implementation-blind critics, executable diagnosis, and flip-centered gating to ensure stable, non-regressive improvements with auditable artifacts.


<details>
  <summary>Details</summary>
Motivation: Current LLM agent improvement approaches (self-improvement mechanisms, concurrent variant search) yield unstable, hard-to-audit trajectories with difficulty guaranteeing non-regression and reasoning about failures across versions.

Method: AgentDevel reframes agent improvement as release engineering with: (1) implementation-blind LLM critic analyzing failure appearances without accessing agent internals, (2) script-based executable diagnosis aggregating symptom patterns into auditable specifications, (3) flip-centered gating prioritizing pass-to-fail regressions and fail-to-pass fixes.

Result: Experiments on execution-heavy benchmarks show AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts.

Conclusion: AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development, maintaining single canonical version line with non-regression as primary objective.

Abstract: Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.

</details>


### [216] [Beyond the "Truth": Investigating Election Rumors on Truth Social During the 2024 Election](https://arxiv.org/abs/2601.04631)
*Etienne Casanova,R. Michael Alvarez*

Main category: cs.AI

TL;DR: LLMs enable large-scale psychological measurement of rumor propagation, showing dose-response belief reinforcement and rapid contagion effects in ideologically homogeneous networks.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the value of large language models (LLMs) in psychological measurement, particularly for analyzing social phenomena at scale and studying belief dynamics in real-world misinformation spread.

Method: Developed a multistage Rumor Detection Agent combining: (1) synthetic data-augmented fine-tuned RoBERTa classifier, (2) precision keyword filtering, and (3) two-pass LLM verification pipeline using GPT-4o mini. Applied to first large-scale dataset of election rumors from a niche alt-tech platform.

Result: Sharing probability rises steadily with each additional exposure (dose-response belief reinforcement). Simulation shows rapid contagion: nearly 25% of users become "infected" within four propagation iterations. Provides large-scale empirical evidence for illusory truth effect in naturalistic settings.

Conclusion: LLMs can transform psychological science by enabling rigorous measurement of belief dynamics and misinformation spread in massive, real-world datasets, offering unprecedented opportunities for analyzing social phenomena at scale.

Abstract: Large language models (LLMs) offer unprecedented opportunities for analyzing social phenomena at scale. This paper demonstrates the value of LLMs in psychological measurement by (1) compiling the first large-scale dataset of election rumors on a niche alt-tech platform, (2) developing a multistage Rumor Detection Agent that leverages LLMs for high-precision content classification, and (3) quantifying the psychological dynamics of rumor propagation, specifically the "illusory truth effect" in a naturalistic setting. The Rumor Detection Agent combines (i) a synthetic data-augmented, fine-tuned RoBERTa classifier, (ii) precision keyword filtering, and (iii) a two-pass LLM verification pipeline using GPT-4o mini. The findings reveal that sharing probability rises steadily with each additional exposure, providing large-scale empirical evidence for dose-response belief reinforcement in ideologically homogeneous networks. Simulation results further demonstrate rapid contagion effects: nearly one quarter of users become "infected" within just four propagation iterations. Taken together, these results illustrate how LLMs can transform psychological science by enabling the rigorous measurement of belief dynamics and misinformation spread in massive, real-world datasets.

</details>


### [217] [Adversarial Yet Cooperative: Multi-Perspective Reasoning in Retrieved-Augmented Language Models](https://arxiv.org/abs/2601.04651)
*Can Xu,Lingyong Yan,Jiayi Wu,Haosen Wang,Shuaiqiang Wang,Yuchen Li,Jizhou Huang,Dawei Yin,Xiang Li*

Main category: cs.AI

TL;DR: ARR framework uses adversarial reasoning between Reasoner and Verifier with process-aware rewards to improve RAG reasoning quality without external scoring models.


<details>
  <summary>Details</summary>
Motivation: Current LRM+RAG systems have two key limitations: (1) single-perspective reasoning without self-correction, and (2) over-reliance on outcome-oriented rewards that don't sufficiently guide complex multi-step reasoning processes.

Method: Proposes Adversarial Reasoning RAG (ARR) with Reasoner-Verifier framework. The Reasoner and Verifier engage in reasoning on retrieved evidence and critique each other's logic, guided by process-aware advantage that combines explicit observational signals with internal model uncertainty.

Result: Experiments on multiple benchmarks demonstrate the effectiveness of the method.

Conclusion: The ARR framework addresses limitations of current LRM+RAG systems by enabling adversarial reasoning with process-aware rewards, improving reasoning fidelity and verification rigor without external scoring models.

Abstract: Recent advances in synergizing large reasoning models (LRMs) with retrieval-augmented generation (RAG) have shown promising results, yet two critical challenges remain: (1) reasoning models typically operate from a single, unchallenged perspective, limiting their ability to conduct deep, self-correcting reasoning over external documents, and (2) existing training paradigms rely excessively on outcome-oriented rewards, which provide insufficient signal for shaping the complex, multi-step reasoning process. To address these issues, we propose an Reasoner-Verifier framework named Adversarial Reasoning RAG (ARR). The Reasoner and Verifier engage in reasoning on retrieved evidence and critiquing each other's logic while being guided by process-aware advantage that requires no external scoring model. This reward combines explicit observational signals with internal model uncertainty to jointly optimize reasoning fidelity and verification rigor. Experiments on multiple benchmarks demonstrate the effectiveness of our method.

</details>


### [218] [Vibe Coding an LLM-powered Theorem Prover](https://arxiv.org/abs/2601.04653)
*Zhe Hou*

Main category: cs.AI

TL;DR: Isabellm is an LLM-powered theorem prover for Isabelle/HOL that combines stepwise proof search with higher-level planning, capable of proving lemmas that defeat standard Isabelle automation.


<details>
  <summary>Details</summary>
Motivation: To create a practical, accessible theorem prover that leverages LLMs for automatic proof synthesis in Isabelle/HOL, running on consumer-grade computers and working with various LLM backends.

Method: Combines stepwise prover (LLM-proposed commands validated by Isabelle in bounded search) with higher-level proof planner generating Isar outlines. Includes beam search, tactics reranking ML/RL models, premise selection with transformers, micro-RAG for Isar proofs, and counter-example guided proof repair.

Result: Isabellm can prove certain lemmas that defeat Isabelle's standard automation including Sledgehammer, demonstrating practical value of LLM-guided proof search. However, even state-of-the-art LLMs struggle with complex algorithmic designs for fill-and-repair mechanisms.

Conclusion: LLM-powered theorem proving shows practical promise but faces fundamental challenges in reliable code generation and reasoning, highlighting limitations of current LLMs for complex algorithmic tasks.

Abstract: We present Isabellm, an LLM-powered theorem prover for Isabelle/HOL that performs fully automatic proof synthesis. Isabellm works with any local LLM on Ollama and APIs such as Gemini CLI, and it is designed to run on consumer grade computers. The system combines a stepwise prover, which uses large language models to propose proof commands validated by Isabelle in a bounded search loop, with a higher-level proof planner that generates structured Isar outlines and attempts to fill and repair remaining gaps. The framework includes beam search for tactics, tactics reranker ML and RL models, premise selection with small transformer models, micro-RAG for Isar proofs built from AFP, and counter-example guided proof repair. All the code is implemented by GPT 4.1 - 5.2, Gemini 3 Pro, and Claude 4.5. Empirically, Isabellm can prove certain lemmas that defeat Isabelle's standard automation, including Sledgehammer, demonstrating the practical value of LLM-guided proof search. At the same time, we find that even state-of-the-art LLMs, such as GPT 5.2 Extended Thinking and Gemini 3 Pro struggle to reliably implement the intended fill-and-repair mechanisms with complex algorithmic designs, highlighting fundamental challenges in LLM code generation and reasoning. The code of Isabellm is available at https://github.com/zhehou/llm-isabelle

</details>


### [219] [Know Thy Enemy: Securing LLMs Against Prompt Injection via Diverse Data Synthesis and Instruction-Level Chain-of-Thought Learning](https://arxiv.org/abs/2601.04666)
*Zhiyuan Chang,Mingyang Li,Yuekai Huang,Ziyou Jiang,Xiaojun Jia,Qian Xiong,Junjie Wang,Zhaoyang Li,Qing Wang*

Main category: cs.AI

TL;DR: InstruCoT is an LLM enhancement method using instruction-level chain-of-thought fine-tuning to defend against prompt injection attacks by enabling models to identify and reject malicious instructions regardless of source or context position.


<details>
  <summary>Details</summary>
Motivation: LLM-integrated applications face critical security vulnerabilities from prompt injection attacks, with two major defense challenges: malicious instructions can be injected through diverse vectors, and injected instructions often lack clear semantic boundaries from surrounding context, making them difficult to identify.

Method: InstruCoT synthesizes diverse training data and employs instruction-level chain-of-thought fine-tuning, enabling LLMs to effectively identify and reject malicious instructions regardless of their source or position in the context.

Result: Experimental results across four LLMs demonstrate that InstruCoT significantly outperforms baselines across three critical dimensions: Behavior Deviation, Privacy Leakage, and Harmful Output, while maintaining utility performance without degradation.

Conclusion: InstruCoT provides an effective model enhancement method for prompt injection defense that addresses the core challenges of diverse injection vectors and lack of semantic boundaries, offering robust protection while preserving model utility.

Abstract: Large language model (LLM)-integrated applications have become increasingly prevalent, yet face critical security vulnerabilities from prompt injection (PI) attacks. Defending against PI attacks faces two major issues: malicious instructions can be injected through diverse vectors, and injected instructions often lack clear semantic boundaries from the surrounding context, making them difficult to identify. To address these issues, we propose InstruCoT, a model enhancement method for PI defense that synthesizes diverse training data and employs instruction-level chain-of-thought fine-tuning, enabling LLMs to effectively identify and reject malicious instructions regardless of their source or position in the context. We evaluate InstruCoT across three critical dimensions: Behavior Deviation, Privacy Leakage, and Harmful Output. Experimental results across four LLMs demonstrate that InstruCoT significantly outperforms baselines in all dimensions while maintaining utility performance without degradation

</details>


### [220] [LLM-Guided Quantified SMT Solving over Uninterpreted Functions](https://arxiv.org/abs/2601.04675)
*Kunhang Lv,Yuhang Dong,Rui Han,Fuqi Jia,Feifei Ma,Jian Zhang*

Main category: cs.AI

TL;DR: AquaForte uses LLMs to guide quantifier instantiation for SMT solving with UFs over non-linear real arithmetic, reducing search space and solving instances where traditional solvers timeout.


<details>
  <summary>Details</summary>
Motivation: Traditional quantifier instantiation methods for SMT solving with Uninterpreted Functions over non-linear real arithmetic lack semantic understanding of UF constraints, forcing inefficient search through unbounded solution spaces with limited guidance.

Method: AquaForte framework: 1) Preprocesses formulas through constraint separation, 2) Uses structured prompts to extract mathematical reasoning from LLMs to generate instantiated candidates for function definitions, 3) Integrates results with traditional SMT algorithms through adaptive instantiation, 4) Maintains soundness via systematic validation (SAT results solve original problem, UNSAT generates exclusion clauses), 5) Preserves completeness by fallback to traditional solvers augmented with learned constraints.

Result: Experimental evaluation on SMT-COMP benchmarks shows AquaForte solves numerous instances where state-of-the-art solvers like Z3 and CVC5 timeout, with particular effectiveness on satisfiable formulas.

Conclusion: LLMs can provide valuable mathematical intuition for symbolic reasoning, establishing a new paradigm for SMT constraint solving by combining semantic guidance from LLMs with traditional SMT algorithms.

Abstract: Quantified formulas with Uninterpreted Functions (UFs) over non-linear real arithmetic pose fundamental challenges for Satisfiability Modulo Theories (SMT) solving. Traditional quantifier instantiation methods struggle because they lack semantic understanding of UF constraints, forcing them to search through unbounded solution spaces with limited guidance. We present AquaForte, a framework that leverages Large Language Models to provide semantic guidance for UF instantiation by generating instantiated candidates for function definitions that satisfy the constraints, thereby significantly reducing the search space and complexity for solvers. Our approach preprocesses formulas through constraint separation, uses structured prompts to extract mathematical reasoning from LLMs, and integrates the results with traditional SMT algorithms through adaptive instantiation. AquaForte maintains soundness through systematic validation: LLM-guided instantiations yielding SAT solve the original problem, while UNSAT results generate exclusion clauses for iterative refinement. Completeness is preserved by fallback to traditional solvers augmented with learned constraints. Experimental evaluation on SMT-COMP benchmarks demonstrates that AquaForte solves numerous instances where state-of-the-art solvers like Z3 and CVC5 timeout, with particular effectiveness on satisfiable formulas. Our work shows that LLMs can provide valuable mathematical intuition for symbolic reasoning, establishing a new paradigm for SMT constraint solving.

</details>


### [221] [ResMAS: Resilience Optimization in LLM-based Multi-agent Systems](https://arxiv.org/abs/2601.04694)
*Zhilun Zhou,Zihan Liu,Jiahe Liu,Qingyu Shao,Yihan Wang,Kun Shao,Depeng Jin,Fengli Xu*

Main category: cs.AI

TL;DR: ResMAS: A two-stage framework to enhance resilience of LLM-based Multi-Agent Systems through automated topology generation and topology-aware prompt optimization.


<details>
  <summary>Details</summary>
Motivation: LLM-based Multi-Agent Systems are vulnerable to perturbations like agent failures, and existing approaches only reactively detect/mitigate attacks rather than proactively design inherently resilient systems.

Method: Two-stage framework: 1) Train reward model to predict MAS resilience, then train topology generator via RL to design resilient topologies; 2) Topology-aware prompt optimization that refines each agent's prompt based on its connections and interactions.

Result: Extensive experiments show substantial improvement in MAS resilience under various constraints, with strong generalization ability to new tasks and models.

Conclusion: The ResMAS framework demonstrates potential for building resilient LLM-based Multi-Agent Systems by proactively addressing vulnerabilities through topology design and prompt optimization.

Abstract: Large Language Model-based Multi-Agent Systems (LLM-based MAS), where multiple LLM agents collaborate to solve complex tasks, have shown impressive performance in many areas. However, MAS are typically distributed across different devices or environments, making them vulnerable to perturbations such as agent failures. While existing works have studied the adversarial attacks and corresponding defense strategies, they mainly focus on reactively detecting and mitigating attacks after they occur rather than proactively designing inherently resilient systems. In this work, we study the resilience of LLM-based MAS under perturbations and find that both the communication topology and prompt design significantly influence system resilience. Motivated by these findings, we propose ResMAS: a two-stage framework for enhancing MAS resilience. First, we train a reward model to predict the MAS's resilience, based on which we train a topology generator to automatically design resilient topology for specific tasks through reinforcement learning. Second, we introduce a topology-aware prompt optimization method that refines each agent's prompt based on its connections and interactions with other agents. Extensive experiments across a range of tasks show that our approach substantially improves MAS resilience under various constraints. Moreover, our framework demonstrates strong generalization ability to new tasks and models, highlighting its potential for building resilient MASs.

</details>


### [222] [Tape: A Cellular Automata Benchmark for Evaluating Rule-Shift Generalization in Reinforcement Learning](https://arxiv.org/abs/2601.04695)
*Enze Pan*

Main category: cs.AI

TL;DR: Tape is a controlled RL benchmark for studying OOD failures under latent rule shifts, using cellular automata to isolate rule changes while keeping observation/action spaces fixed.


<details>
  <summary>Details</summary>
Motivation: To create a controlled environment for studying out-of-distribution (OOD) failures in reinforcement learning when latent rules shift, enabling precise analysis of how different RL methods handle unseen transition rules.

Method: Derived from 1D cellular automata with fixed observation/action spaces but changing transition rules; uses reproducible evaluation pipeline comparing model-free baselines, model-based planning with learned world models, and task-inference (meta-RL) methods.

Result: Methods strong in-distribution can collapse under heldout-rule OOD; high-variance OOD evaluation makes rankings unstable without sufficient replication; provides standardized OOD protocols and statistical reporting requirements.

Conclusion: The benchmark reveals limitations of current RL methods under rule shifts and provides tools (protocols, statistical requirements, information-theoretic analysis) for better OOD evaluation and understanding of uncertainty reduction objectives.

Abstract: We present Tape, a controlled reinforcement-learning benchmark designed to isolate out-of-distribution (OOD) failure under latent rule shifts.Tape is derived from one-dimensional cellular automata, enabling precise train/test splits where observation and action spaces are held fixed while transition rules change. Using a reproducible evaluation pipeline, we compare model-free baselines, model-based planning with learned world models, and task-inference (meta-RL) methods. A consistent pattern emerges: methods that are strong in-distribution (ID) can collapse under heldout-rule OOD, and high-variance OOD evaluation can make rankings unstable unless experiments are sufficiently replicated.We provide (i) standardized OOD protocols, (ii) statistical reporting requirements (seeds, confidence intervals, and hypothesis tests), and (iii) information-theoretic identities connecting entropy reduction to conditional mutual information and expected posterior KL divergence, clarifying what "uncertainty reduction" objectives can and cannot guarantee under rule shifts.

</details>


### [223] [A Method for Constructing a Digital Transformation Driving Mechanism Based on Semantic Understanding of Large Models](https://arxiv.org/abs/2601.04696)
*Huayi Liu*

Main category: cs.AI

TL;DR: Combines LLM and knowledge graph for enterprise digital transformation, reducing equipment failure response time from 7.8 to 3.7 hours with 94.3% F1 score.


<details>
  <summary>Details</summary>
Motivation: Addresses insufficient semantic understanding of unstructured data and lack of intelligent decision-making basis in enterprise digital transformation driving mechanisms.

Method: Uses fine-tuned BERT for entity/relation extraction, GPT-4 for semantic vector generation, two-layer GNN to fuse LLM vectors with business metadata for knowledge graph construction, and reinforcement learning for decision path optimization.

Result: In manufacturing case: reduced equipment failure response time from 7.8 to 3.7 hours, achieved 94.3% F1 value, decreased decision error compensation in annual digital transformation cost by 45.3%.

Conclusion: The LLM-knowledge graph integration significantly enhances intelligence level and execution efficiency of digital transformation driving mechanisms through semantic understanding and structured knowledge fusion.

Abstract: In the process of digital transformation, enterprises are faced with problems such as insufficient semantic understanding of unstructured data and lack of intelligent decision-making basis in driving mechanisms. This study proposes a method that combines a large language model (LLM) and a knowledge graph. First, a fine-tuned BERT (Bidirectional Encoder Representations from Transformers) model is used to perform entity recognition and relationship extraction on multi-source heterogeneous texts, and GPT-4 is used to generate semantically enhanced vector representations; secondly, a two-layer graph neural network (GNN) architecture is designed to fuse the semantic vectors output by LLM with business metadata to construct a dynamic and scalable enterprise knowledge graph; then reinforcement learning is introduced to optimize decision path generation, and the reward function is used to drive the mechanism iteration. In the case of the manufacturing industry, this mechanism reduced the response time for equipment failure scenarios from 7.8 hours to 3.7 hours, the F1 value reached 94.3%, and the compensation for decision errors in the annual digital transformation cost decreased by 45.3%. This method significantly enhances the intelligence level and execution efficiency of the digital transformation driving mechanism by integrating large model semantic understanding with structured knowledge.

</details>


### [224] [TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning](https://arxiv.org/abs/2601.04698)
*Yinuo Wang,Mining Tan,Wenxiang Jiao,Xiaoxi Li,Hao Wang,Xuanyu Zhang,Yuan Lu,Weiming Dong*

Main category: cs.AI

TL;DR: TourPlanner: A travel planning framework using multi-path reasoning and constraint-gated reinforcement learning to address POI pruning, solution space exploration, and constraint optimization challenges.


<details>
  <summary>Details</summary>
Motivation: Existing travel planning approaches face three key challenges: (1) pruning candidate POIs while maintaining high recall, (2) limited exploration due to single reasoning paths, and (3) difficulty optimizing both hard and soft constraints simultaneously.

Method: TourPlanner combines three components: (1) Personalized Recall and Spatial Optimization (PReSO) for spatially-aware candidate POI selection, (2) Competitive consensus Chain-of-Thought (CCoT) for multi-path reasoning to explore solution space, and (3) sigmoid-based gating mechanism in reinforcement learning to prioritize soft constraints only after hard constraints are met.

Result: TourPlanner achieves state-of-the-art performance on travel planning benchmarks, significantly surpassing existing methods in both feasibility and user-preference alignment.

Conclusion: The proposed TourPlanner framework effectively addresses key challenges in travel planning through multi-path reasoning and constraint-gated reinforcement learning, demonstrating superior performance in generating feasible and user-preferred itineraries.

Abstract: Travel planning is a sophisticated decision-making process that requires synthesizing multifaceted information to construct itineraries. However, existing travel planning approaches face several challenges: (1) Pruning candidate points of interest (POIs) while maintaining a high recall rate; (2) A single reasoning path restricts the exploration capability within the feasible solution space for travel planning; (3) Simultaneously optimizing hard constraints and soft constraints remains a significant difficulty. To address these challenges, we propose TourPlanner, a comprehensive framework featuring multi-path reasoning and constraint-gated reinforcement learning. Specifically, we first introduce a Personalized Recall and Spatial Optimization (PReSO) workflow to construct spatially-aware candidate POIs' set. Subsequently, we propose Competitive consensus Chain-of-Thought (CCoT), a multi-path reasoning paradigm that improves the ability of exploring the feasible solution space. To further refine the plan, we integrate a sigmoid-based gating mechanism into the reinforcement learning stage, which dynamically prioritizes soft-constraint satisfaction only after hard constraints are met. Experimental results on travel planning benchmarks demonstrate that TourPlanner achieves state-of-the-art performance, significantly surpassing existing methods in both feasibility and user-preference alignment.

</details>


### [225] [Beyond Monolithic Architectures: A Multi-Agent Search and Knowledge Optimization Framework for Agentic Search](https://arxiv.org/abs/2601.04703)
*Yiqun Chen,Lingyong Yan,Zixuan Yang,Erhan Zhang,Jiashu Zhao,Shuaiqiang Wang,Dawei Yin,Jiaxin Mao*

Main category: cs.AI

TL;DR: M-ASK is a multi-agent framework that decouples agentic search into specialized roles for search behavior and knowledge management, using turn-level rewards for stable coordination, achieving superior accuracy and training stability on multi-hop QA tasks.


<details>
  <summary>Details</summary>
Motivation: Current agentic search systems rely on monolithic agents that suffer from structural bottlenecks: unconstrained reasoning outputs that inflate trajectories, sparse outcome-level rewards that complicate credit assignment, and stochastic search noise that destabilizes learning.

Method: M-ASK decouples agentic search into two complementary roles: Search Behavior Agents (plan and execute search actions) and Knowledge Management Agents (aggregate, filter, and maintain compact internal context). It employs turn-level rewards to provide granular supervision for both search decisions and knowledge updates.

Result: Experiments on multi-hop QA benchmarks show M-ASK outperforms strong baselines, achieving superior answer accuracy and significantly more stable training dynamics.

Conclusion: The multi-agent decomposition with specialized roles and turn-level rewards effectively addresses structural bottlenecks in agentic search, leading to improved performance and training stability for complex information seeking tasks.

Abstract: Agentic search has emerged as a promising paradigm for complex information seeking by enabling Large Language Models (LLMs) to interleave reasoning with tool use. However, prevailing systems rely on monolithic agents that suffer from structural bottlenecks, including unconstrained reasoning outputs that inflate trajectories, sparse outcome-level rewards that complicate credit assignment, and stochastic search noise that destabilizes learning. To address these challenges, we propose \textbf{M-ASK} (Multi-Agent Search and Knowledge), a framework that explicitly decouples agentic search into two complementary roles: Search Behavior Agents, which plan and execute search actions, and Knowledge Management Agents, which aggregate, filter, and maintain a compact internal context. This decomposition allows each agent to focus on a well-defined subtask and reduces interference between search and context construction. Furthermore, to enable stable coordination, M-ASK employs turn-level rewards to provide granular supervision for both search decisions and knowledge updates. Experiments on multi-hop QA benchmarks demonstrate that M-ASK outperforms strong baselines, achieving not only superior answer accuracy but also significantly more stable training dynamics.\footnote{The source code for M-ASK is available at https://github.com/chenyiqun/M-ASK.}

</details>


### [226] [Bridging Temporal and Textual Modalities: A Multimodal Framework for Automated Cloud Failure Root Cause Analysis](https://arxiv.org/abs/2601.04709)
*Gijun Park*

Main category: cs.AI

TL;DR: A multimodal framework that aligns time-series performance metrics with language model embeddings for automated root cause analysis in cloud infrastructure.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with continuous numerical time-series data due to discrete token architectures, limiting their application in cloud incident management where understanding temporal performance metrics is crucial for root cause analysis.

Method: Three technical advances: 1) Semantic compression to distill temporal segments into single-token abstractions, 2) Alignment encoder using gated cross-attention to project time-series features into language model latent space, 3) Retrieval-augmented diagnostic pipeline combining aligned embeddings with historical incident knowledge.

Result: Achieves 48.75% diagnostic accuracy across six cloud system benchmarks, with notable improvements on scenarios involving compound failure modes.

Conclusion: Embedding-space alignment is an effective strategy for enabling language models to reason over multimodal telemetry data in production incident response contexts.

Abstract: Root cause analysis in modern cloud infrastructure demands sophisticated understanding of heterogeneous data sources, particularly time-series performance metrics that involve core failure signatures. While large language models demonstrate remarkable capabilities in textual reasoning, their discrete token-based architecture creates fundamental incompatibilities with continuous numerical sequences exhibiting temporal dependencies. Current methodologies inadequately address this modality mismatch, constraining the potential of language model-driven automation in incident management workflows. This paper presents a multimodal diagnostic framework that harmonizes time-series representations with pretrained language model embedding spaces. Our approach contributes three technical advances: (1) a semantic compression technique that distills temporal segments into single-token abstractions while preserving pattern semantics, (2) an alignment encoder utilizing gated cross-attention to project time-series features into language model latent space, and (3) a retrieval-augmented diagnostic pipeline that synthesizes aligned embeddings with historical incident knowledge for expert-level failure attribution. Comprehensive evaluation across six cloud system benchmarks demonstrates that our framework achieves leading performance, reaching 48.75% diagnostic accuracy with notable improvements on scenarios involving compound failure modes. The results validate embedding-space alignment as an effective strategy for enabling language models to reason over multimodal telemetry data in production incident response contexts.

</details>


### [227] [ThinkDrive: Chain-of-Thought Guided Progressive Reinforcement Learning Fine-Tuning for Autonomous Driving](https://arxiv.org/abs/2601.04714)
*Chang Zhao,Zheming Yang,Yunqing Hu,Qi Guo,Zijian Wang,Pengcheng Li,Wen Ji*

Main category: cs.AI

TL;DR: ThinkDrive: A Chain-of-Thought guided progressive reinforcement learning framework for autonomous driving that combines explicit reasoning with difficulty-aware adaptive policy optimization to improve decision-making and alignment with human intent.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based autonomous driving methods suffer from unstructured reasoning, poor generalization, and misalignment with human driving intent. While Chain-of-Thought reasoning improves transparency, conventional supervised fine-tuning doesn't fully exploit it, and reinforcement learning approaches face instability and limited reasoning depth.

Method: Two-stage training: 1) Supervised fine-tuning using Chain-of-Thought explanations, 2) Progressive reinforcement learning with difficulty-aware adaptive policy optimizer that dynamically adjusts learning intensity based on sample complexity.

Result: ThinkDrive outperforms strong RL baselines by 1.45%, 1.95%, and 1.01% on exam, easy-exam, and accuracy metrics respectively. A 2B-parameter model trained with ThinkDrive surpasses GPT-4o by 3.28% on the exam metric.

Conclusion: ThinkDrive effectively addresses limitations of existing LLM-based autonomous driving methods by synergizing explicit reasoning with adaptive policy optimization, achieving superior performance with smaller models compared to larger foundation models.

Abstract: With the rapid advancement of large language models (LLMs) technologies, their application in the domain of autonomous driving has become increasingly widespread. However, existing methods suffer from unstructured reasoning, poor generalization, and misalignment with human driving intent. While Chain-of-Thought (CoT) reasoning enhances decision transparency, conventional supervised fine-tuning (SFT) fails to fully exploit its potential, and reinforcement learning (RL) approaches face instability and suboptimal reasoning depth. We propose ThinkDrive, a CoT guided progressive RL fine-tuning framework for autonomous driving that synergizes explicit reasoning with difficulty-aware adaptive policy optimization. Our method employs a two-stage training strategy. First, we perform SFT using CoT explanations. Then, we apply progressive RL with a difficulty-aware adaptive policy optimizer that dynamically adjusts learning intensity based on sample complexity. We evaluate our approach on a public dataset. The results show that ThinkDrive outperforms strong RL baselines by 1.45%, 1.95%, and 1.01% on exam, easy-exam, and accuracy, respectively. Moreover, a 2B-parameter model trained with our method surpasses the much larger GPT-4o by 3.28% on the exam metric.

</details>


### [228] [Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning](https://arxiv.org/abs/2601.04726)
*Yuyang Hu,Jiongnan Liu,Jiejun Tan,Yutao Zhu,Zhicheng Dou*

Main category: cs.AI

TL;DR: CompassMem is an event-centric memory framework that organizes agent memories as a graph with logical relationships, enabling structured navigation for long-horizon reasoning instead of flat similarity-based retrieval.


<details>
  <summary>Details</summary>
Motivation: Current LLM memory systems use flat organization and simple similarity-based retrieval, failing to capture logical relationships between experiences and preventing logical reasoning over long-horizon dependencies.

Method: CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations, creating a logic map for structured, goal-directed navigation.

Result: Experiments on LoCoMo and NarrativeQA show CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.

Conclusion: The event-centric memory framework with logical graph organization enables more effective long-horizon reasoning for intelligent agents compared to traditional flat memory approaches.

Abstract: Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.

</details>


### [229] [Miner:Mining Intrinsic Mastery for Data-Efficient RL in Large Reasoning Models](https://arxiv.org/abs/2601.04731)
*Shuyang Jiang,Yuhao Wang,Ya Zhang,Yanfeng Wang,Yu Wang*

Main category: cs.AI

TL;DR: Miner introduces a self-supervised RL method that uses policy uncertainty as reward signal to solve inefficiency in critic-free RL for reasoning models, achieving SOTA performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current critic-free RL methods for large reasoning models are inefficient when training on positive homogeneous prompts (where all rollouts are correct), wasting rollouts due to zero advantage estimates.

Method: Miner repurposes policy's intrinsic uncertainty as self-supervised reward signal with two innovations: (1) token-level focal credit assignment that amplifies gradients on critical uncertain tokens while suppressing overconfident ones, and (2) adaptive advantage calibration to integrate intrinsic and verifiable rewards.

Result: Evaluated across six reasoning benchmarks on Qwen3-4B and Qwen3-8B base models, Miner achieves state-of-the-art performance with up to 4.58 absolute gains in Pass@1 and 6.66 gains in Pass@K compared to GRPO.

Conclusion: Latent uncertainty exploitation is both necessary and sufficient for efficient and scalable RL training of reasoning models, as demonstrated by Miner's superior performance over other exploration enhancement methods.

Abstract: Current critic-free RL methods for large reasoning models suffer from severe inefficiency when training on positive homogeneous prompts (where all rollouts are correct), resulting in waste of rollouts due to zero advantage estimates. We introduce a radically simple yet powerful solution to \uline{M}ine \uline{in}trinsic mast\uline{er}y (Miner), that repurposes the policy's intrinsic uncertainty as a self-supervised reward signal, with no external supervision, auxiliary models, or additional inference cost. Our method pioneers two key innovations: (1) a token-level focal credit assignment mechanism that dynamically amplifies gradients on critical uncertain tokens while suppressing overconfident ones, and (2) adaptive advantage calibration to seamlessly integrate intrinsic and verifiable rewards. Evaluated across six reasoning benchmarks on Qwen3-4B and Qwen3-8B base models, Miner achieves state-of-the-art performance among the other four algorithms, yielding up to \textbf{4.58} absolute gains in Pass@1 and \textbf{6.66} gains in Pass@K compared to GRPO. Comparison with other methods targeted at exploration enhancement further discloses the superiority of the two newly proposed innovations. This demonstrates that latent uncertainty exploitation is both necessary and sufficient for efficient and scalable RL training of reasoning models.

</details>


### [230] [KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions](https://arxiv.org/abs/2601.04745)
*Tingyu Wu,Zhisheng Chen,Ziyan Weng,Shuhe Wang,Chenglong Li,Shuo Zhang,Sen Hu,Silin Wu,Qizhen Lan,Huacan Wang,Ronghao Chen*

Main category: cs.AI

TL;DR: KnowMeBench is a new benchmark for evaluating person understanding using autobiographical narratives, focusing on factual recall, subjective states, and principle-level reasoning beyond simple retrieval.


<details>
  <summary>Details</summary>
Motivation: Existing memory benchmarks use multi-turn dialogues or synthetic histories, making retrieval performance an imperfect proxy for true person understanding. There's a need for benchmarks that capture stable motivations and decision principles from real autobiographical narratives.

Method: The benchmark reconstructs long-form autobiographical narratives into flashback-aware, time-anchored streams. It evaluates models with evidence-linked questions spanning three levels: factual recall, subjective state attribution, and principle-level reasoning.

Result: Retrieval-augmented systems mainly improve factual accuracy but errors persist on temporally grounded explanations and higher-level inferences. This highlights the need for memory mechanisms beyond simple retrieval.

Conclusion: KnowMeBench demonstrates that current retrieval-based approaches are insufficient for comprehensive person understanding, particularly for temporal reasoning and higher-level inference tasks. The benchmark is publicly available to advance research in this area.

Abstract: Existing long-horizon memory benchmarks mostly use multi-turn dialogues or synthetic user histories, which makes retrieval performance an imperfect proxy for person understanding. We present \BenchName, a publicly releasable benchmark built from long-form autobiographical narratives, where actions, context, and inner thoughts provide dense evidence for inferring stable motivations and decision principles. \BenchName~reconstructs each narrative into a flashback-aware, time-anchored stream and evaluates models with evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning. Across diverse narrative sources, retrieval-augmented systems mainly improve factual accuracy, while errors persist on temporally grounded explanations and higher-level inferences, highlighting the need for memory mechanisms beyond retrieval. Our data is in \href{KnowMeBench}{https://github.com/QuantaAlpha/KnowMeBench}.

</details>


### [231] [When Single-Agent with Skills Replace Multi-Agent Systems and When They Fail](https://arxiv.org/abs/2601.04748)
*Xiaoxiao Li*

Main category: cs.AI

TL;DR: LLM skill selection exhibits cognitive-like capacity limits: accuracy remains stable until a critical library size, then drops sharply in a phase transition pattern, with semantic confusability being key.


<details>
  <summary>Details</summary>
Motivation: Multi-agent AI systems have computational overhead from inter-agent communication. The paper explores whether similar modularity benefits can be achieved with a single agent using skill libraries, and investigates the fundamental scaling limits of skill selection as libraries grow.

Method: View skills as internalized agent behaviors, compile multi-agent systems into equivalent single-agent systems, investigate scaling behavior of skill selection, analyze patterns of degradation, examine role of semantic confusability, and test hierarchical organization for skill routing.

Result: Skill selection shows bounded capacity: accuracy remains stable up to a critical library size then drops sharply (phase transition). Semantic confusability among similar skills drives degradation more than library size alone. Hierarchical routing helps manage complex choices.

Conclusion: LLM skill selection has fundamental cognitive-like capacity limits. Hierarchical organization can help scale skill-based agents. This work provides a cognitive-grounded framework for designing scalable skill-based AI systems and raises new questions about semantic-based skill selection limits.

Abstract: Multi-agent AI systems have proven effective for complex reasoning. These systems are compounded by specialized agents, which collaborate through explicit communication, but incur substantial computational overhead. A natural question arises: can we achieve similar modularity benefits with a single agent that selects from a library of skills? We explore this question by viewing skills as internalized agent behaviors. From this perspective, a multi-agent system can be compiled into an equivalent single-agent system, trading inter-agent communication for skill selection. Our preliminary experiments suggest this approach can substantially reduce token usage and latency while maintaining competitive accuracy on reasoning benchmarks. However, this efficiency raises a deeper question that has received little attention: how does skill selection scale as libraries grow?
  Drawing on principles from cognitive science, we propose that LLM skill selection exhibits bounded capacity analogous to human decision-making. We investigate the scaling behavior of skill selection and observe a striking pattern. Rather than degrading gradually, selection accuracy remains stable up to a critical library size, then drops sharply, indicating a phase transition reminiscent of capacity limits in human cognition. Furthermore, we find evidence that semantic confusability among similar skills, rather than library size alone, plays a central role in this degradation. This perspective suggests that hierarchical organization, which has long helped humans manage complex choices, may similarly benefit AI systems. Our initial results with hierarchical routing support this hypothesis. This work opens new questions about the fundamental limits of semantic-based skill selection in LLMs and offers a cognitive-grounded framework and practical guidelines for designing scalable skill-based agents.

</details>


### [232] [Orion-RAG: Path-Aligned Hybrid Retrieval for Graphless Data](https://arxiv.org/abs/2601.04764)
*Zhen Chen,Weihao Xie,Peilin Chen,Shiqi Wang,Jianping Wang*

Main category: cs.AI

TL;DR: Orion-RAG is a lightweight retrieval-augmented generation system that extracts natural connections between fragmented documents without complex algorithms, outperforming mainstream frameworks with 25.2% precision improvement.


<details>
  <summary>Details</summary>
Motivation: Standard RAG struggles with discrete, fragmented data in practical scenarios where information is distributed across isolated files without explicit links. Manual knowledge graph construction is impractical for vast data, and standard search engines process files independently without considering connections.

Method: Uses a low-complexity strategy to extract lightweight paths that naturally link related concepts across fragmented documents, transforming them into semi-structured data without heavy algorithms.

Result: Consistently outperforms mainstream frameworks across diverse domains, supports real-time updates and explicit Human-in-the-Loop verification with high cost-efficiency. Achieves 25.2% relative precision improvement on FinanceBench over strong baselines.

Conclusion: A streamlined approach using lightweight path extraction is sufficient to effectively link information across fragmented documents, enabling practical RAG applications for discrete, unconnected data sources.

Abstract: Retrieval-Augmented Generation (RAG) has proven effective for knowledge synthesis, yet it encounters significant challenges in practical scenarios where data is inherently discrete and fragmented. In most environments, information is distributed across isolated files like reports and logs that lack explicit links. Standard search engines process files independently, ignoring the connections between them. Furthermore, manually building Knowledge Graphs is impractical for such vast data. To bridge this gap, we present Orion-RAG. Our core insight is simple yet effective: we do not need heavy algorithms to organize this data. Instead, we use a low-complexity strategy to extract lightweight paths that naturally link related concepts. We demonstrate that this streamlined approach suffices to transform fragmented documents into semi-structured data, enabling the system to link information across different files effectively. Extensive experiments demonstrate that Orion-RAG consistently outperforms mainstream frameworks across diverse domains, supporting real-time updates and explicit Human-in-the-Loop verification with high cost-efficiency. Experiments on FinanceBench demonstrate superior precision with a 25.2% relative improvement over strong baselines.

</details>


### [233] [AT$^2$PO: Agentic Turn-based Policy Optimization via Tree Search](https://arxiv.org/abs/2601.04767)
*Zefang Zong,Dingwei Chen,Yang Li,Qi Yi,Bo Zhou,Chengming Li,Bo Qian,Peng Chen,Jie Jiang*

Main category: cs.AI

TL;DR: AT²PO is a unified framework for multi-turn agentic RL that addresses exploration diversity, credit assignment, and policy optimization challenges through turn-based tree search and learning objectives.


<details>
  <summary>Details</summary>
Motivation: LLM agents need refinement through agentic reinforcement learning, but current approaches face three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization in multi-turn tasks.

Method: AT²PO introduces a turn-level tree structure with Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for reward propagation. It also proposes Agentic Turn-based Policy Optimization (ATPO), a turn-level learning objective that aligns with agentic decision granularity.

Result: Experiments across seven benchmarks show consistent improvements over state-of-the-art baselines by up to 1.84 percentage points on average, with ablation studies confirming each component's effectiveness.

Conclusion: AT²PO provides a unified framework for multi-turn agentic RL that effectively addresses key challenges, with ATPO being orthogonal to tree search and easily integrable into existing RL pipelines.

Abstract: LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT$^2$PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT$^2$PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.

</details>


### [234] [SciIF: Benchmarking Scientific Instruction Following Towards Rigorous Scientific Intelligence](https://arxiv.org/abs/2601.04770)
*Encheng Su,Jianyu Wu,Chen Tang,Lintao Wang,Pengze Li,Aoran Wang,Jinouwen Zhang,Yizhou Wang,Yuan Meng,Xinzhu Ma,Shixiang Tang,Houqiang Li*

Main category: cs.AI

TL;DR: SciIF is a new benchmark that evaluates LLMs' ability to follow scientific constraints while solving problems, focusing on auditability and multi-constraint adherence rather than just final-answer correctness.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks have critical blind spots: general instruction-following metrics focus on superficial formatting, while domain-specific scientific benchmarks only assess final-answer correctness, often rewarding models that get the right answer with wrong reasoning. There's a need to evaluate LLMs' ability to adhere to scientific validity constraints as they transition to complex scientific discovery.

Method: Introduces SciIF, a multi-discipline benchmark that pairs university-level problems with a fixed catalog of constraints across three pillars: scientific conditions (boundary checks, assumptions), semantic stability (unit/symbol conventions), and specific processes (required numerical methods). Emphasizes auditability by requiring explicit evidence of constraint satisfaction rather than implicit compliance.

Result: SciIF enables fine-grained diagnosis of compositional reasoning failures by measuring both solution correctness and multi-constraint adherence, ensuring LLMs can function as reliable agents within strict scientific logical frameworks.

Conclusion: The benchmark addresses the gap in evaluating LLMs' scientific instruction following capability, providing a more rigorous standard that incorporates scientific inquiry norms and ensures models can adhere to constraints that establish scientific validity.

Abstract: As large language models (LLMs) transition from general knowledge retrieval to complex scientific discovery, their evaluation standards must also incorporate the rigorous norms of scientific inquiry. Existing benchmarks exhibit a critical blind spot: general instruction-following metrics focus on superficial formatting, while domain-specific scientific benchmarks assess only final-answer correctness, often rewarding models that arrive at the right result with the wrong reasons. To address this gap, we introduce scientific instruction following: the capability to solve problems while strictly adhering to the constraints that establish scientific validity. Specifically, we introduce SciIF, a multi-discipline benchmark that evaluates this capability by pairing university-level problems with a fixed catalog of constraints across three pillars: scientific conditions (e.g., boundary checks and assumptions), semantic stability (e.g., unit and symbol conventions), and specific processes(e.g., required numerical methods). Uniquely, SciIF emphasizes auditability, requiring models to provide explicit evidence of constraint satisfaction rather than implicit compliance. By measuring both solution correctness and multi-constraint adherence, SciIF enables finegrained diagnosis of compositional reasoning failures, ensuring that LLMs can function as reliable agents within the strict logical frameworks of science.

</details>


### [235] [APEX: Academic Poster Editing Agentic Expert](https://arxiv.org/abs/2601.04794)
*Chengxin Shi,Qinnan Cai,Zeyuan Chen,Long Zeng,Yibo Zhao,Jing Yu,Jianxiang Yu,Xiang Li*

Main category: cs.AI

TL;DR: APEX is an agentic framework for interactive academic poster editing with fine-grained control, multi-level API editing, and review-adjustment mechanism, outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: Existing paper-to-poster generation methods are single-pass and non-interactive, failing to align with complex user intent, creating a need for interactive editing frameworks.

Method: APEX framework with agentic interactive editing, multi-level API-based editing, review-and-adjustment mechanism, plus APEX-Bench benchmark with 514 instructions and multi-dimensional evaluation protocol.

Result: APEX significantly outperforms baseline methods in academic poster editing tasks.

Conclusion: APEX provides the first agentic framework for interactive academic poster editing with fine-grained control, addressing limitations of existing non-interactive methods.

Abstract: Designing academic posters is a labor-intensive process requiring the precise balance of high-density content and sophisticated layout. While existing paper-to-poster generation methods automate initial drafting, they are typically single-pass and non-interactive, often fail to align with complex, subjective user intent. To bridge this gap, we propose APEX (Academic Poster Editing agentic eXpert), the first agentic framework for interactive academic poster editing, supporting fine-grained control with robust multi-level API-based editing and a review-and-adjustment Mechanism. In addition, we introduce APEX-Bench, the first systematic benchmark comprising 514 academic poster editing instructions, categorized by a multi-dimensional taxonomy including operation type, difficulty, and abstraction level, constructed via reference-guided and reference-free strategies to ensure realism and diversity. We further establish a multi-dimensional VLM-as-a-judge evaluation protocol to assess instruction fulfillment, modification scope, and visual consistency & harmony. Experimental results demonstrate that APEX significantly outperforms baseline methods. Our implementation is available at https://github.com/Breesiu/APEX.

</details>


### [236] [Defense Against Indirect Prompt Injection via Tool Result Parsing](https://arxiv.org/abs/2601.04795)
*Qiang Yu,Xinran Cheng,Chuanyi Liu*

Main category: cs.AI

TL;DR: A novel defense method against Indirect Prompt Injection attacks that uses tool result parsing to filter malicious code while maintaining high utility and achieving the lowest attack success rate to date.


<details>
  <summary>Details</summary>
Motivation: As LLM agents gain physical control in autonomous systems and robotics, they face increasing threats from indirect prompt injection attacks that can hijack decision-making through adversarial instructions in tool call results. Existing defenses either have high computational overhead (trained detection models) or limited robustness (prompt-based methods with high attack success rates).

Method: Proposes a novel approach that provides LLMs with precise data via tool result parsing while effectively filtering out injected malicious code. The method focuses on parsing tool results to separate legitimate data from adversarial instructions.

Result: Achieves competitive Utility under Attack (UA) while maintaining the lowest Attack Success Rate (ASR) to date, significantly outperforming existing methods. Code is available on GitHub.

Conclusion: The proposed method offers an effective defense against indirect prompt injection attacks for LLM agents in physical control scenarios, balancing utility and security better than current approaches.

Abstract: As LLM agents transition from digital assistants to physical controllers in autonomous systems and robotics, they face an escalating threat from indirect prompt injection. By embedding adversarial instructions into the results of tool calls, attackers can hijack the agent's decision-making process to execute unauthorized actions. This vulnerability poses a significant risk as agents gain more direct control over physical environments. Existing defense mechanisms against Indirect Prompt Injection (IPI) generally fall into two categories. The first involves training dedicated detection models; however, this approach entails high computational overhead for both training and inference, and requires frequent updates to keep pace with evolving attack vectors. Alternatively, prompt-based methods leverage the inherent capabilities of LLMs to detect or ignore malicious instructions via prompt engineering. Despite their flexibility, most current prompt-based defenses suffer from high Attack Success Rates (ASR), demonstrating limited robustness against sophisticated injection attacks. In this paper, we propose a novel method that provides LLMs with precise data via tool result parsing while effectively filtering out injected malicious code. Our approach achieves competitive Utility under Attack (UA) while maintaining the lowest Attack Success Rate (ASR) to date, significantly outperforming existing methods. Code is available at GitHub.

</details>


### [237] [Thinking-Based Non-Thinking: Solving the Reward Hacking Problem in Training Hybrid Reasoning Models via Reinforcement Learning](https://arxiv.org/abs/2601.04805)
*Siyuan Gan,Jiaheng Liu,Boyan Wang,Tianpei Yang,Runqing Miao,Yuyao Zhang,Fanyu Meng,Junlan Feng,Linjian Meng,Jing Huo,Yang Gao*

Main category: cs.AI

TL;DR: TNT (Thinking-Based Non-Thinking) reduces computational overhead in large reasoning models by dynamically setting token limits for non-thinking responses based on thinking-based solutions, achieving ~50% token reduction while improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models suffer from overthinking (long CoT chains) that increases computational costs. Existing RL-based approaches to decide when to think/not-think suffer from reward hacking problems, while SFT alternatives are computationally expensive and uniform token limits offer limited mitigation.

Method: TNT avoids SFT and sets different maximum token limits for non-thinking responses across queries by leveraging information from thinking-based solutions. It uses thinking-based responses to determine appropriate token budgets for non-thinking responses on similar queries.

Result: On five mathematical benchmarks, TNT reduces token usage by ~50% compared to DeepSeek-R1-Distill-Qwen-1.5B/7B and DeepScaleR-1.5B while significantly improving accuracy. It achieves optimal accuracy-efficiency trade-off and keeps reward hacking probability below 10% for non-thinking responses.

Conclusion: TNT effectively addresses the overthinking problem in large reasoning models by intelligently setting query-specific token limits for non-thinking responses using thinking-based solution information, achieving substantial efficiency gains without sacrificing accuracy.

Abstract: Large reasoning models (LRMs) have attracted much attention due to their exceptional performance. However, their performance mainly stems from thinking, a long Chain of Thought (CoT), which significantly increase computational overhead. To address this overthinking problem, existing work focuses on using reinforcement learning (RL) to train hybrid reasoning models that automatically decide whether to engage in thinking or not based on the complexity of the query. Unfortunately, using RL will suffer the the reward hacking problem, e.g., the model engages in thinking but is judged as not doing so, resulting in incorrect rewards. To mitigate this problem, existing works either employ supervised fine-tuning (SFT), which incurs high computational costs, or enforce uniform token limits on non-thinking responses, which yields limited mitigation of the problem. In this paper, we propose Thinking-Based Non-Thinking (TNT). It does not employ SFT, and sets different maximum token usage for responses not using thinking across various queries by leveraging information from the solution component of the responses using thinking. Experiments on five mathematical benchmarks demonstrate that TNT reduces token usage by around 50% compared to DeepSeek-R1-Distill-Qwen-1.5B/7B and DeepScaleR-1.5B, while significantly improving accuracy. In fact, TNT achieves the optimal trade-off between accuracy and efficiency among all tested methods. Additionally, the probability of reward hacking problem in TNT's responses, which are classified as not using thinking, remains below 10% across all tested datasets.

</details>


### [238] [SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning](https://arxiv.org/abs/2601.04809)
*Caijun Xu,Changyi Xiao,Zhongyuan Peng,Xinrun Wang,Yixin Cao*

Main category: cs.AI

TL;DR: SCALER is a framework that uses adaptive environment design to sustain effective RL training signals for language model reasoning, addressing issues of reward sparsity and overfitting through scalable synthetic environments and dynamic difficulty adjustment.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning for language model reasoning often slows down when task difficulty becomes misaligned with model capability or when training is dominated by narrow problem patterns, leading to reward sparsity and overfitting.

Method: SCALER combines: 1) a scalable synthesis pipeline that converts real-world programming problems into verifiable reasoning environments with controllable difficulty and unbounded instance generation, and 2) an adaptive multi-environment RL strategy that dynamically adjusts instance difficulty and curates active environments to track model capability and maintain diversity.

Result: Extensive experiments show SCALER consistently outperforms dataset-based RL baselines across diverse reasoning benchmarks and exhibits more stable, long-horizon training dynamics.

Conclusion: SCALER's adaptive environment design framework effectively sustains informative learning signals throughout RL training, preventing reward sparsity and overfitting while enabling continuous improvement in language model reasoning capabilities.

Abstract: Reinforcement learning (RL) offers a principled way to enhance the reasoning capabilities of large language models, yet its effectiveness hinges on training signals that remain informative as models evolve. In practice, RL progress often slows when task difficulty becomes poorly aligned with model capability, or when training is dominated by a narrow set of recurring problem patterns. To jointly address these issues, we propose SCALER (Synthetic sCalable Adaptive Learning Environment for Reasoning), a framework that sustains effective learning signals through adaptive environment design. SCALER introduces a scalable synthesis pipeline that converts real-world programming problems into verifiable reasoning environments with controllable difficulty and unbounded instance generation, enabling RL training beyond finite datasets while preserving strong correctness guarantees. Building on this, SCALER further employs an adaptive multi-environment RL strategy that dynamically adjusts instance difficulty and curates the active set of environments to track the model's capability frontier and maintain distributional diversity. This co-adaptation prevents reward sparsity, mitigates overfitting to narrow task patterns, and supports sustained improvement throughout training. Extensive experiments show that SCALER consistently outperforms dataset-based RL baselines across diverse reasoning benchmarks and exhibits more stable, long-horizon training dynamics.

</details>


### [239] [AECV-Bench: Benchmarking Multimodal Models on Architectural and Engineering Drawings Understanding](https://arxiv.org/abs/2601.04819)
*Aleksei Kondratenko,Mussie Birhane,Houssame E. Hsain,Guido Maciocci*

Main category: cs.AI

TL;DR: AECV-Bench is a benchmark for evaluating multimodal models on AEC drawings, showing they're good at OCR/text tasks but poor at symbol recognition like counting doors/windows.


<details>
  <summary>Details</summary>
Motivation: It's unclear whether modern multimodal and vision-language models can reliably interpret the complex graphical language of AEC (Architecture, Engineering, Construction) drawings, which encode geometry and semantics through symbols, layout conventions, and dense annotation.

Method: Created AECV-Bench with two complementary use cases: (1) object counting on 120 high-quality floor plans (doors, windows, bedrooms, toilets), and (2) drawing-grounded document QA spanning 192 question-answer pairs testing text extraction, instance counting, spatial reasoning, and comparative reasoning. Used per-field exact-match accuracy, MAPE, LLM-as-a-judge scoring, and human adjudication.

Result: Models show a stable capability gradient: OCR and text-centric QA are strongest (up to 0.95 accuracy), spatial reasoning is moderate, but symbol-centric drawing understanding - especially reliable counting of doors and windows - remains unsolved (often 0.40-0.55 accuracy) with substantial proportional errors.

Conclusion: Current systems function well as document assistants but lack robust drawing literacy, motivating domain-specific representations and tool-augmented, human-in-the-loop workflows for efficient AEC automation.

Abstract: AEC drawings encode geometry and semantics through symbols, layout conventions, and dense annotation, yet it remains unclear whether modern multimodal and vision-language models can reliably interpret this graphical language. We present AECV-Bench, a benchmark for evaluating multimodal and vision-language models on realistic AEC artefacts via two complementary use cases: (i) object counting on 120 high-quality floor plans (doors, windows, bedrooms, toilets), and (ii) drawing-grounded document QA spanning 192 question-answer pairs that test text extraction (OCR), instance counting, spatial reasoning, and comparative reasoning over common drawing regions. Object-counting performance is reported using per-field exact-match accuracy and MAPE results, while document-QA performance is reported using overall accuracy and per-category breakdowns with an LLM-as-a-judge scoring pipeline and targeted human adjudication for edge cases. Evaluating a broad set of state-of-the-art models under a unified protocol, we observe a stable capability gradient; OCR and text-centric document QA are strongest (up to 0.95 accuracy), spatial reasoning is moderate, and symbol-centric drawing understanding - especially reliable counting of doors and windows - remains unsolved (often 0.40-0.55 accuracy) with substantial proportional errors. These results suggest that current systems function well as document assistants but lack robust drawing literacy, motivating domain-specific representations and tool-augmented, human-in-the-loop workflows for an efficient AEC automation.

</details>


### [240] [DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation](https://arxiv.org/abs/2601.04823)
*Guanzhi Deng,Bo Li,Ronghao Chen,Huacan Wang,Linqi Song,Lijie Wen*

Main category: cs.AI

TL;DR: DR-LoRA: A dynamic rank allocation framework for LoRA fine-tuning of Mixture-of-Experts LLMs that assigns different ranks to experts based on task relevance, improving parameter efficiency.


<details>
  <summary>Details</summary>
Motivation: Standard LoRA fine-tuning for MoE LLMs uses identical ranks for all experts, ignoring that different experts have varying relevance to specific downstream tasks. This uniform allocation causes resource mismatch where task-relevant experts are under-provisioned while less relevant ones get redundant parameters.

Method: DR-LoRA dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. It uses an Expert Saliency Scoring mechanism that combines expert routing frequency and LoRA rank importance to quantify each expert's need for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, creating a heterogeneous rank distribution tailored to the target task.

Result: Experiments on multiple benchmarks show DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.

Conclusion: Dynamic rank allocation based on expert saliency enables more efficient parameter utilization in MoE LLM fine-tuning, addressing the resource mismatch problem of uniform LoRA rank assignment and improving task performance.

Abstract: Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.

</details>


### [241] [Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models](https://arxiv.org/abs/2601.04861)
*Jingbo Wang,Sendong Zhao,Jiatong Liu,Haochun Wang,Wanting Li,Bing Qin,Ting Liu*

Main category: cs.AI

TL;DR: OI-MAS is a multi-agent framework that dynamically selects appropriate LLM scales for different reasoning stages, improving accuracy while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent systems use uniform LLMs across all agents, ignoring varying cognitive demands of different reasoning stages, leading to computational inefficiency.

Method: Proposes OI-MAS with adaptive model-selection policy using heterogeneous multi-scale LLMs, state-dependent routing mechanism, and confidence-aware mechanism for task complexity-based model selection.

Result: Outperforms baseline multi-agent systems with up to 12.88% accuracy improvement and up to 79.78% cost reduction.

Conclusion: OI-MAS effectively addresses computational inefficiency in multi-agent systems through adaptive model selection, achieving better performance at lower cost.

Abstract: While multi-agent systems (MAS) have demonstrated superior performance over single-agent approaches in complex reasoning tasks, they often suffer from significant computational inefficiencies. Existing frameworks typically deploy large language models (LLMs) uniformly across all agent roles, failing to account for the varying cognitive demands of different reasoning stages. We address this inefficiency by proposing OI-MAS framework, a novel multi-agent framework that implements an adaptive model-selection policy across a heterogeneous pool of multi-scale LLMs. Specifically, OI-MAS introduces a state-dependent routing mechanism that dynamically selects agent roles and model scales throughout the reasoning process. In addition, we introduce a confidence-aware mechanism that selects appropriate model scales conditioned on task complexity, thus reducing unnecessary reliance on large-scale models. Experimental results show that OI-MAS consistently outperforms baseline multi-agent systems, improving accuracy by up to 12.88\% while reducing cost by up to 79.78\%.

</details>


### [242] [Key-Value Pair-Free Continual Learner via Task-Specific Prompt-Prototype](https://arxiv.org/abs/2601.04864)
*Haihua Luo,Xuming Ran,Zhengji Li,Huiyan Xue,Tingting Jiang,Jiangrong Shen,Tommi Kärkkäinen,Qi Xu,Fengyu Cong*

Main category: cs.AI

TL;DR: ProP: A prompt-based continual learning method that replaces key-value pairing with task-specific prompt-prototype pairs to reduce inter-task interference and improve scalability.


<details>
  <summary>Details</summary>
Motivation: Existing prompt-based continual learning methods rely on key-value pairing, which introduces inter-task interference and limits scalability. The authors aim to overcome these limitations by eliminating the dependency on key-value pairs.

Method: Proposes task-specific Prompt-Prototype (ProP) pairs where prompts facilitate feature learning for current tasks and prototypes capture representative input features. During inference, predictions are made by binding each task-specific prompt with its associated prototype. Also introduces regularization constraints during prompt initialization to penalize large values for stability.

Result: Experiments on several widely used datasets demonstrate the effectiveness of the proposed method. The framework successfully removes dependency on key-value pairs while maintaining performance.

Conclusion: The ProP framework offers a fresh perspective for continual learning research by eliminating key-value pair dependency, reducing inter-task interference, and improving scalability compared to mainstream prompt-based approaches.

Abstract: Continual learning aims to enable models to acquire new knowledge while retaining previously learned information. Prompt-based methods have shown remarkable performance in this domain; however, they typically rely on key-value pairing, which can introduce inter-task interference and hinder scalability. To overcome these limitations, we propose a novel approach employing task-specific Prompt-Prototype (ProP), thereby eliminating the need for key-value pairs. In our method, task-specific prompts facilitate more effective feature learning for the current task, while corresponding prototypes capture the representative features of the input. During inference, predictions are generated by binding each task-specific prompt with its associated prototype. Additionally, we introduce regularization constraints during prompt initialization to penalize excessively large values, thereby enhancing stability. Experiments on several widely used datasets demonstrate the effectiveness of the proposed method. In contrast to mainstream prompt-based approaches, our framework removes the dependency on key-value pairs, offering a fresh perspective for future continual learning research.

</details>


### [243] [Higher-Order Knowledge Representations for Agentic Scientific Reasoning](https://arxiv.org/abs/2601.04878)
*Isabella A. Stewart,Markus J. Buehler*

Main category: cs.AI

TL;DR: Researchers developed a hypergraph-based knowledge representation system to capture higher-order relationships in scientific literature, enabling AI agents to generate novel mechanistic hypotheses without human supervision.


<details>
  <summary>Details</summary>
Motivation: Current LLMs rely on retrieval-augmented contexts lacking structural depth, while traditional knowledge graphs fail to capture irreducible higher-order interactions that govern emergent physical behavior in scientific domains.

Method: Constructed hypergraph-based knowledge representations from ~1,100 manuscripts on biocomposite scaffolds, creating a global hypergraph with 161,172 nodes and 320,201 hyperedges. Used node-intersection constraints for hypergraph traversal in agentic systems.

Result: Revealed scale-free topology (power law exponent ~1.23) organized around conceptual hubs. System successfully generated grounded mechanistic hypotheses, such as linking cerium oxide to PCL scaffolds via chitosan intermediates, by exploiting higher-order pathways.

Conclusion: Hypergraph topology acts as a verifiable guardrail for "teacherless" agentic reasoning systems, accelerating scientific discovery by uncovering relationships obscured by traditional graph methods through explicit preservation of co-occurrence contexts.

Abstract: Scientific inquiry requires systems-level reasoning that integrates heterogeneous experimental data, cross-domain knowledge, and mechanistic evidence into coherent explanations. While Large Language Models (LLMs) offer inferential capabilities, they often depend on retrieval-augmented contexts that lack structural depth. Traditional Knowledge Graphs (KGs) attempt to bridge this gap, yet their pairwise constraints fail to capture the irreducible higher-order interactions that govern emergent physical behavior. To address this, we introduce a methodology for constructing hypergraph-based knowledge representations that faithfully encode multi-entity relationships. Applied to a corpus of ~1,100 manuscripts on biocomposite scaffolds, our framework constructs a global hypergraph of 161,172 nodes and 320,201 hyperedges, revealing a scale-free topology (power law exponent ~1.23) organized around highly connected conceptual hubs. This representation prevents the combinatorial explosion typical of pairwise expansions and explicitly preserves the co-occurrence context of scientific formulations. We further demonstrate that equipping agentic systems with hypergraph traversal tools, specifically using node-intersection constraints, enables them to bridge semantically distant concepts. By exploiting these higher-order pathways, the system successfully generates grounded mechanistic hypotheses for novel composite materials, such as linking cerium oxide to PCL scaffolds via chitosan intermediates. This work establishes a "teacherless" agentic reasoning system where hypergraph topology acts as a verifiable guardrail, accelerating scientific discovery by uncovering relationships obscured by traditional graph methods.

</details>


### [244] [Precomputing Multi-Agent Path Replanning using Temporal Flexibility: A Case Study on the Dutch Railway Network](https://arxiv.org/abs/2601.04884)
*Issa Hanou,Eric Kemmeren,Devin Wild Thomas,Mathijs de Weerdt*

Main category: cs.AI

TL;DR: FlexSIPP efficiently replans multi-agent systems when one agent is delayed by using temporal flexibility of other agents to avoid cascading delays.


<details>
  <summary>Details</summary>
Motivation: Multi-agent plan execution faces challenges when agents get delayed, causing conflicts. Traditional replanning approaches either replan only the delayed agent (inefficient/infeasible) or replan all agents (causes cascading delays). Need a method that efficiently handles delays while minimizing disruption to other agents.

Method: FlexSIPP algorithm tracks and uses temporal flexibility of other agents - the maximum delay an agent can take without changing order or further delaying others. Precomputes all possible plans for the delayed agent and returns changes for other agents for any single-agent delay within the scenario.

Result: Demonstrated in real-world case study of replanning trains in Dutch railway network. FlexSIPP provides effective solutions relevant to real-world adjustments within reasonable timeframe.

Conclusion: FlexSIPP offers an efficient approach to multi-agent replanning by leveraging temporal flexibility to handle delays while avoiding cascading disruptions, making it practical for real-world applications like railway scheduling.

Abstract: Executing a multi-agent plan can be challenging when an agent is delayed, because this typically creates conflicts with other agents. So, we need to quickly find a new safe plan. Replanning only the delayed agent often does not result in an efficient plan, and sometimes cannot even yield a feasible plan. On the other hand, replanning other agents may lead to a cascade of changes and delays. We show how to efficiently replan by tracking and using the temporal flexibility of other agents while avoiding cascading delays. This flexibility is the maximum delay an agent can take without changing the order of or further delaying more agents. Our algorithm, FlexSIPP, precomputes all possible plans for the delayed agent, also returning the changes for the other agents, for any single-agent delay within the given scenario. We demonstrate our method in a real-world case study of replanning trains in the densely-used Dutch railway network. Our experiments show that FlexSIPP provides effective solutions, relevant to real-world adjustments, and within a reasonable timeframe.

</details>


### [245] [Flexible Manufacturing Systems Intralogistics: Dynamic Optimization of AGVs and Tool Sharing Using Coloured-Timed Petri Nets and Actor-Critic RL with Actions Masking](https://arxiv.org/abs/2601.04887)
*Sofiene Lassoued,Laxmikant Shrikant Bahetic,Nathalie Weiß-Borkowskib,Stefan Lierc,Andreas Schwunga*

Main category: cs.AI

TL;DR: Novel CTPN+MBRL approach for FMS scheduling with AGVs and tool-sharing, outperforms traditional methods on large instances with 10x faster computation.


<details>
  <summary>Details</summary>
Motivation: FMS optimization needs to address complex job shop scheduling with additional complexities like AGVs and tool-sharing systems simultaneously, going beyond traditional approaches.

Method: Combines Colored-Timed Petri Nets (CTPNs) for formal modeling and dynamic action masking with actor-critic model-based reinforcement learning (MBRL) for adaptability, plus lookahead strategy for AGV positioning.

Result: Matches traditional methods on small benchmarks and outperforms them on large instances (inspired by Taillard benchmark) in makespan while achieving 10x reduction in computation time.

Conclusion: The integrated CTPN+MBRL framework effectively addresses complex FMS scheduling with AGVs and tool-sharing, demonstrating superior performance on large-scale problems with significantly reduced computation time.

Abstract: Flexible Manufacturing Systems (FMS) are pivotal in optimizing production processes in today's rapidly evolving manufacturing landscape. This paper advances the traditional job shop scheduling problem by incorporating additional complexities through the simultaneous integration of automated guided vehicles (AGVs) and tool-sharing systems. We propose a novel approach that combines Colored-Timed Petri Nets (CTPNs) with actor-critic model-based reinforcement learning (MBRL), effectively addressing the multifaceted challenges associated with FMS. CTPNs provide a formal modeling structure and dynamic action masking, significantly reducing the action search space, while MBRL ensures adaptability to changing environments through the learned policy. Leveraging the advantages of MBRL, we incorporate a lookahead strategy for optimal positioning of AGVs, improving operational efficiency. Our approach was evaluated on small-sized public benchmarks and a newly developed large-scale benchmark inspired by the Taillard benchmark. The results show that our approach matches traditional methods on smaller instances and outperforms them on larger ones in terms of makespan while achieving a tenfold reduction in computation time. To ensure reproducibility, we propose a gym-compatible environment and an instance generator. Additionally, an ablation study evaluates the contribution of each framework component to its overall performance.

</details>


### [246] [SmartSearch: Process Reward-Guided Query Refinement for Search Agents](https://arxiv.org/abs/2601.04888)
*Tongyu Wen,Guanting Dong,Zhicheng Dou*

Main category: cs.AI

TL;DR: SmartSearch improves LLM-based search agents by optimizing intermediate search query quality through process rewards and query refinement mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based search agents focus on reasoning paradigms but overlook query quality, leading to inaccurate queries, poor retrieval results, and limited overall effectiveness.

Method: SmartSearch uses: (1) Process rewards with Dual-Level Credit Assessment for fine-grained supervision of query quality, (2) Query refinement to optimize low-quality queries and regenerate subsequent searches, and (3) Three-stage curriculum learning (imitation → alignment → generalization) to internalize query improvement.

Result: SmartSearch consistently surpasses existing baselines and shows significant gains in both search efficiency and query quality.

Conclusion: Optimizing intermediate search query quality through SmartSearch's mechanisms effectively improves LLM-based search agent performance.

Abstract: Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.

</details>


### [247] [DVD: A Robust Method for Detecting Variant Contamination in Large Language Model Evaluation](https://arxiv.org/abs/2601.04895)
*Renzhao Liang,Jingru Chen,Bo Jia,Bo Deng,Chenggang Xie,Yidong Wang,Ke Jin,Xin Wang,Linfeng Zhang,Cunxiang Wang*

Main category: cs.AI

TL;DR: DVD detects variant contamination in LLM evaluation by analyzing variance in generation distributions, outperforming existing detection methods.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluation is confounded by variant contamination - where training data contains semantically equivalent but lexically/syntactically altered versions of test items. These variants evade existing detectors (based on sampling consistency or perplexity), inflating benchmark scores through memorization rather than genuine reasoning.

Method: Proposes DVD (Detection via Variance of generation Distribution), a single-sample detector that models local output distribution from temperature sampling. Key insight: contaminated items trigger alternation between memory-adherence and perturbation-drift states, yielding abnormally high variance in synthetic difficulty of low-probability tokens. Constructed benchmark for variant contamination across Omni-MATH and SuperGPQA domains, simulating contamination via fine-tuning Qwen2.5 and Llama3.1 models.

Result: DVD consistently outperforms perplexity-based, Min-k%++, edit-distance (CDD), and embedding-similarity baselines across datasets and models. Shows strong robustness to hyperparameters.

Conclusion: Variance of the generation distribution serves as a principled and practical fingerprint for detecting variant contamination in LLM evaluation, addressing a critical problem in benchmark reliability.

Abstract: Evaluating large language models (LLMs) is increasingly confounded by \emph{variant contamination}: the training corpus contains semantically equivalent yet lexically or syntactically altered versions of test items. Unlike verbatim leakage, these paraphrased or structurally transformed variants evade existing detectors based on sampling consistency or perplexity, thereby inflating benchmark scores via memorization rather than genuine reasoning. We formalize this problem and introduce \textbf{DVD} (\textbf{D}etection via \textbf{V}ariance of generation \textbf{D}istribution), a single-sample detector that models the local output distribution induced by temperature sampling. Our key insight is that contaminated items trigger alternation between a \emph{memory-adherence} state and a \emph{perturbation-drift} state, yielding abnormally high variance in the synthetic difficulty of low-probability tokens; uncontaminated items remain in drift with comparatively smooth variance. We construct the first benchmark for variant contamination across two domains Omni-MATH and SuperGPQA by generating and filtering semantically equivalent variants, and simulate contamination via fine-tuning models of different scales and architectures (Qwen2.5 and Llama3.1). Across datasets and models, \textbf{DVD} consistently outperforms perplexity-based, Min-$k$\%++, edit-distance (CDD), and embedding-similarity baselines, while exhibiting strong robustness to hyperparameters. Our results establish variance of the generation distribution as a principled and practical fingerprint for detecting variant contamination in LLM evaluation.

</details>


### [248] [From Stories to Cities to Games: A Qualitative Evaluation of Behaviour Planning](https://arxiv.org/abs/2601.04911)
*Mustafa F. Abdelwahed,Joan Espasa,Alice Toniolo,Ian P. Gent*

Main category: cs.AI

TL;DR: Paper demonstrates behavior planning's real-world applications through three case studies: storytelling, urban planning, and game evaluation.


<details>
  <summary>Details</summary>
Motivation: Diverse planning approaches are valuable for real-world applications like risk management, data analysis, and malware detection. Behavior planning extends traditional methods by explicitly incorporating diversity models and supporting multiple planning categories, but its practical utility needs demonstration.

Method: The paper uses three case studies to demonstrate behavior planning: 1) storytelling applications, 2) urban planning scenarios, and 3) game evaluation contexts. These case studies serve as practical demonstrations of how behavior planning can be applied in different domains.

Result: The paper successfully demonstrates the usefulness of behavior planning in three distinct real-world settings, showing its applicability across storytelling, urban planning, and game evaluation domains.

Conclusion: Behavior planning is a valuable diverse planning paradigm that can be effectively applied to various real-world problems, as evidenced by its successful implementation in storytelling, urban planning, and game evaluation contexts.

Abstract: The primary objective of a diverse planning approach is to generate a set of plans that are distinct from one another. Such an approach is applied in a variety of real-world domains, including risk management, automated stream data analysis, and malware detection. More recently, a novel diverse planning paradigm, referred to as behaviour planning, has been proposed. This approach extends earlier methods by explicitly incorporating a diversity model into the planning process and supporting multiple planning categories. In this paper, we demonstrate the usefulness of behaviour planning in real-world settings by presenting three case studies. The first case study focuses on storytelling, the second addresses urban planning, and the third examines game evaluation.

</details>


### [249] [What Students Ask, How a Generative AI Assistant Responds: Exploring Higher Education Students' Dialogues on Learning Analytics Feedback](https://arxiv.org/abs/2601.04919)
*Yildiz Uzun,Andrea Gauthier,Mutlu Cukurova*

Main category: cs.AI

TL;DR: Study explores how GenAI assistants integrated into learning analytics dashboards can scaffold students' self-regulated learning through dialogue-based support, revealing different query patterns between high and low SRL students and identifying limitations in personalization and emotional support.


<details>
  <summary>Details</summary>
Motivation: Students, especially those with lower self-regulated learning (SRL) competence, struggle to engage with and interpret analytics feedback from learning analytics dashboards (LADs). Conversational GenAI assistants show potential to provide real-time, personalized, dialogue-based scaffolding for this process.

Method: Analyzed authentic dialogues between students and GenAI assistant integrated into LAD during a 10-week semester. Examined questions from students with different SRL levels, relevance/quality of assistant's answers, and student perceptions of the assistant's role in learning.

Result: Low SRL students sought clarification and reassurance, while high SRL students queried technical aspects and requested personalized strategies. The assistant provided clear, reliable explanations but was limited in personalization, handling emotional queries, and integrating multiple data points for tailored responses.

Conclusion: GenAI interventions are especially valuable for low SRL students, offering scaffolding that supports engagement with feedback and narrows gaps with higher SRL peers. Future systems need greater trust, adaptivity, context-awareness, and technical refinement.

Abstract: Learning analytics dashboards (LADs) aim to support students' regulation of learning by translating complex data into feedback. Yet students, especially those with lower self-regulated learning (SRL) competence, often struggle to engage with and interpret analytics feedback. Conversational generative artificial intelligence (GenAI) assistants have shown potential to scaffold this process through real-time, personalised, dialogue-based support. Further advancing this potential, we explored authentic dialogues between students and GenAI assistant integrated into LAD during a 10-week semester. The analysis focused on questions students with different SRL levels posed, the relevance and quality of the assistant's answers, and how students perceived the assistant's role in their learning. Findings revealed distinct query patterns. While low SRL students sought clarification and reassurance, high SRL students queried technical aspects and requested personalised strategies. The assistant provided clear and reliable explanations but limited in personalisation, handling emotionally charged queries, and integrating multiple data points for tailored responses. Findings further extend that GenAI interventions can be especially valuable for low SRL students, offering scaffolding that supports engagement with feedback and narrows gaps with their higher SRL peers. At the same time, students' reflections underscored the importance of trust, need for greater adaptivity, context-awareness, and technical refinement in future systems.

</details>


### [250] [Conversational AI for Rapid Scientific Prototyping: A Case Study on ESA's ELOPE Competition](https://arxiv.org/abs/2601.04920)
*Nils Einecke*

Main category: cs.AI

TL;DR: Using ChatGPT for rapid prototyping in ESA's ELOPE competition achieved 2nd place, demonstrating LLMs can accelerate scientific development through code generation and algorithmic reasoning, despite some limitations in structural changes and error handling.


<details>
  <summary>Details</summary>
Motivation: To explore how large language models (LLMs) can accelerate scientific discovery through human-AI collaboration, particularly in competitive scientific settings like ESA's ELOPE competition for lunar lander trajectory estimation.

Method: Case study approach using ChatGPT for rapid prototyping in the ELOPE competition, where the AI contributed executable code, algorithmic reasoning, data handling routines, and methodological suggestions (e.g., using fixed number of events instead of fixed time spans for windowing).

Result: Achieved second place in the competition with a score of 0.01282 despite joining late, demonstrating successful human-AI collaboration. Identified both strengths (code generation, conceptual insights) and limitations (unnecessary structural changes, confusion by alternative ideas, critical errors, forgetting aspects in longer discussions).

Conclusion: Conversational AI can accelerate scientific development and support conceptual insight, but requires structured integration into scientific workflows with best practices to maximize benefits while mitigating limitations.

Abstract: Large language models (LLMs) are increasingly used as coding partners, yet their role in accelerating scientific discovery remains underexplored. This paper presents a case study of using ChatGPT for rapid prototyping in ESA's ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition. The competition required participants to process event camera data to estimate lunar lander trajectories. Despite joining late, we achieved second place with a score of 0.01282, highlighting the potential of human-AI collaboration in competitive scientific settings. ChatGPT contributed not only executable code but also algorithmic reasoning, data handling routines, and methodological suggestions, such as using fixed number of events instead of fixed time spans for windowing. At the same time, we observed limitations: the model often introduced unnecessary structural changes, gets confused by intermediate discussions about alternative ideas, occasionally produced critical errors and forgets important aspects in longer scientific discussions. By analyzing these strengths and shortcomings, we show how conversational AI can both accelerate development and support conceptual insight in scientific research. We argue that structured integration of LLMs into the scientific workflow can enhance rapid prototyping by proposing best practices for AI-assisted scientific work.

</details>


### [251] [T-Retriever: Tree-based Hierarchical Retrieval Augmented Generation for Textual Graphs](https://arxiv.org/abs/2601.04945)
*Chunyu Wei,Huaiyu Qin,Siyuan He,Yunhai Wang,Yueguo Chen*

Main category: cs.AI

TL;DR: T-Retriever is a novel tree-based RAG framework that overcomes limitations of graph-based approaches by using semantic and structure-guided encoding trees with adaptive compression and joint semantic-structural optimization.


<details>
  <summary>Details</summary>
Motivation: Current graph-based RAG approaches have two critical limitations: 1) rigid layer-specific compression quotas that damage local graph structures, and 2) prioritizing topological structure while neglecting semantic content. These limitations hinder effective management of hierarchical information in knowledge retrieval.

Method: T-Retriever reformulates attributed graph retrieval as tree-based retrieval using a semantic and structure-guided encoding tree. Key innovations include: 1) Adaptive Compression Encoding that replaces artificial compression quotas with global optimization preserving natural hierarchical organization, and 2) Semantic-Structural Entropy (S²-Entropy) that jointly optimizes for both structural cohesion and semantic consistency in hierarchical partitions.

Result: Experiments across diverse graph reasoning benchmarks demonstrate that T-Retriever significantly outperforms state-of-the-art RAG methods, providing more coherent and contextually relevant responses to complex queries.

Conclusion: T-Retriever successfully addresses the limitations of current graph-based RAG approaches by introducing a tree-based framework that better preserves both structural and semantic information, leading to improved performance in knowledge retrieval and reasoning tasks.

Abstract: Retrieval-Augmented Generation (RAG) has significantly enhanced Large Language Models' ability to access external knowledge, yet current graph-based RAG approaches face two critical limitations in managing hierarchical information: they impose rigid layer-specific compression quotas that damage local graph structures, and they prioritize topological structure while neglecting semantic content. We introduce T-Retriever, a novel framework that reformulates attributed graph retrieval as tree-based retrieval using a semantic and structure-guided encoding tree. Our approach features two key innovations: (1) Adaptive Compression Encoding, which replaces artificial compression quotas with a global optimization strategy that preserves the graph's natural hierarchical organization, and (2) Semantic-Structural Entropy ($S^2$-Entropy), which jointly optimizes for both structural cohesion and semantic consistency when creating hierarchical partitions. Experiments across diverse graph reasoning benchmarks demonstrate that T-Retriever significantly outperforms state-of-the-art RAG methods, providing more coherent and contextually relevant responses to complex queries.

</details>


### [252] [ConMax: Confidence-Maximizing Compression for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.04973)
*Minda Hu,Zexuan Qiu,Zenan Xu,Kun Li,Bo Zhou,Irwin King*

Main category: cs.AI

TL;DR: ConMax is a reinforcement learning framework that compresses reasoning traces in Large Reasoning Models by pruning redundancy while preserving logical coherence, achieving 43% length reduction with only 0.7% accuracy drop.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models often suffer from "overthinking" - generating redundant reasoning paths that increase computational costs without improving accuracy. Existing compression techniques for reasoning traces either compromise logical coherence or are too expensive to sample.

Method: ConMax formulates compression as a reward-driven optimization problem using reinforcement learning. It trains a policy to prune redundancy by maximizing a weighted combination of answer confidence (for predictive fidelity) and thinking confidence (for reasoning validity) through a frozen auxiliary LRM.

Result: Extensive experiments across five reasoning datasets show ConMax achieves superior efficiency-performance trade-off: reduces inference length by 43% over strong baselines with only 0.7% accuracy drop.

Conclusion: ConMax effectively generates high-quality, efficient training data for LRMs by automatically compressing reasoning traces while preserving essential reasoning patterns, addressing the overthinking problem in large reasoning models.

Abstract: Recent breakthroughs in Large Reasoning Models (LRMs) have demonstrated that extensive Chain-of-Thought (CoT) generation is critical for enabling intricate cognitive behaviors, such as self-verification and backtracking, to solve complex tasks. However, this capability often leads to ``overthinking'', where models generate redundant reasoning paths that inflate computational costs without improving accuracy. While Supervised Fine-Tuning (SFT) on reasoning traces is a standard paradigm for the 'cold start' phase, applying existing compression techniques to these traces often compromises logical coherence or incurs prohibitive sampling costs. In this paper, we introduce ConMax (Confidence-Maximizing Compression), a novel reinforcement learning framework designed to automatically compress reasoning traces while preserving essential reasoning patterns. ConMax formulates compression as a reward-driven optimization problem, training a policy to prune redundancy by maximizing a weighted combination of answer confidence for predictive fidelity and thinking confidence for reasoning validity through a frozen auxiliary LRM. Extensive experiments across five reasoning datasets demonstrate that ConMax achieves a superior efficiency-performance trade-off. Specifically, it reduces inference length by 43% over strong baselines at the cost of a mere 0.7% dip in accuracy, proving its effectiveness in generating high-quality, efficient training data for LRMs.

</details>


### [253] [AlgBench: To What Extent Do Large Reasoning Models Understand Algorithms?](https://arxiv.org/abs/2601.04996)
*Henan Sun,Kaichi Yu,Yuyao Wang,Bowen Liu,Xunkai Li,Rong-Hua Li,Nuo Chen,Jia Li*

Main category: cs.AI

TL;DR: AlgBench is a new expert-curated benchmark with 3,000+ problems across 27 algorithms that reveals LRMs struggle with algorithmic reasoning, especially on optimized algorithms like dynamic programming, showing performance drops from 92% to 49%.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for algorithmic reasoning are limited and fail to answer whether Large Reasoning Models truly master algorithmic reasoning. There's a need for comprehensive evaluation under an algorithm-centric paradigm.

Method: Created AlgBench with over 3,000 original problems spanning 27 algorithms, curated by ACM algorithmic experts. Organized under a comprehensive taxonomy including Euclidean-structured, non-Euclidean-structured, non-optimized, local-optimized, global-optimized, and heuristic-optimized categories. Evaluated leading LRMs (Gemini-3-Pro, DeepSeek-v3.2-Speciale, GPT-o3).

Result: Substantial performance heterogeneity: models perform well on non-optimized tasks (up to 92%) but accuracy drops sharply to around 49% on globally optimized algorithms like dynamic programming. Uncovered "strategic over-shifts" where models prematurely abandon correct algorithmic designs due to necessary low-entropy tokens.

Conclusion: Findings expose fundamental limitations of problem-centric reinforcement learning and highlight the necessity of an algorithm-centric training paradigm for robust algorithmic reasoning in Large Reasoning Models.

Abstract: Reasoning ability has become a central focus in the advancement of Large Reasoning Models (LRMs). Although notable progress has been achieved on several reasoning benchmarks such as MATH500 and LiveCodeBench, existing benchmarks for algorithmic reasoning remain limited, failing to answer a critical question: Do LRMs truly master algorithmic reasoning? To answer this question, we propose AlgBench, an expert-curated benchmark that evaluates LRMs under an algorithm-centric paradigm.
  AlgBench consists of over 3,000 original problems spanning 27 algorithms, constructed by ACM algorithmic experts and organized under a comprehensive taxonomy, including Euclidean-structured, non-Euclidean-structured, non-optimized, local-optimized, global-optimized, and heuristic-optimized categories. Empirical evaluations on leading LRMs (e.g., Gemini-3-Pro, DeepSeek-v3.2-Speciale and GPT-o3) reveal substantial performance heterogeneity: while models perform well on non-optimized tasks (up to 92%), accuracy drops sharply to around 49% on globally optimized algorithms such as dynamic programming. Further analysis uncovers \textbf{strategic over-shifts}, wherein models prematurely abandon correct algorithmic designs due to necessary low-entropy tokens. These findings expose fundamental limitations of problem-centric reinforcement learning and highlight the necessity of an algorithm-centric training paradigm for robust algorithmic reasoning.

</details>


### [254] [An Empirical Investigation of Robustness in Large Language Models under Tabular Distortions](https://arxiv.org/abs/2601.05009)
*Avik Dutta,Harshit Nigam,Hosein Hasanbeig,Arjun Radhakrishna,Sumit Gulwani*

Main category: cs.AI

TL;DR: LLMs struggle to detect and correct subtle distortions in tabular data, only partially improving with explicit prompts, with accuracy dropping at least 22% under distortion.


<details>
  <summary>Details</summary>
Motivation: To investigate how LLMs fail when tabular data is subjected to semantic and structural distortions, and to understand their ability to detect and correct such distortions without explicit guidance.

Method: Created a small, expert-curated dataset for table question answering (TQA) tasks requiring error-correction before analysis. Tested LLMs including SoTA models like GPT-5.2 on distorted tabular data with and without explicit system prompts.

Result: LLMs lack inherent ability to detect/correct subtle table distortions. Only with explicit prompts do they partially adjust reasoning strategies, but not consistently. SoTA models show minimum 22% accuracy drop under distortion. Systematic differences found in how LLMs ingest/interpret distorted tabular information.

Conclusion: Findings raise important questions about when/how models should autonomously decide to realign tabular inputs without explicit prompts, analogous to human behavior, rather than relying on pre-processing.

Abstract: We investigate how large language models (LLMs) fail when tabular data in an otherwise canonical representation is subjected to semantic and structural distortions. Our findings reveal that LLMs lack an inherent ability to detect and correct subtle distortions in table representations. Only when provided with an explicit prior, via a system prompt, do models partially adjust their reasoning strategies and correct some distortions, though not consistently or completely. To study this phenomenon, we introduce a small, expert-curated dataset that explicitly evaluates LLMs on table question answering (TQA) tasks requiring an additional error-correction step prior to analysis. Our results reveal systematic differences in how LLMs ingest and interpret tabular information under distortion, with even SoTA models such as GPT-5.2 model exhibiting a drop of minimum 22% accuracy under distortion. These findings raise important questions for future research, particularly regarding when and how models should autonomously decide to realign tabular inputs, analogous to human behavior, without relying on explicit prompts or tabular data pre-processing.

</details>


### [255] [OptiSet: Unified Optimizing Set Selection and Ranking for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.05027)
*Yi Jiang,Sendong Zhao,Jianbo Li,Bairui Hu,Yanrui Du,Haochun Wang,Bing Qin*

Main category: cs.AI

TL;DR: OptiSet: A set-centric framework for RAG that optimizes evidence selection through combinatorial reasoning, reducing redundancy and improving generation quality.


<details>
  <summary>Details</summary>
Motivation: Current RAG methods use static top-k passage selection based on individual relevance, which fails to exploit combinatorial gains among passages and introduces substantial redundancy, limiting generation quality and efficiency.

Method: OptiSet uses an "Expand-then-Refine" paradigm: 1) Expand queries into multiple perspectives for diverse candidate pool, 2) Refine via re-selection to form compact evidence set, 3) Self-synthesis strategy to derive preference labels from set conditional utility changes, 4) Set-list wise training to jointly optimize set selection and ranking.

Result: Extensive experiments show OptiSet improves performance on complex combinatorial problems and makes generation more efficient compared to existing methods.

Conclusion: OptiSet effectively addresses redundancy in RAG by optimizing evidence selection through combinatorial reasoning, enabling more efficient and higher-quality generation with publicly available source code.

Abstract: Retrieval-Augmented Generation (RAG) improves generation quality by incorporating evidence retrieved from large external corpora. However, most existing methods rely on statically selecting top-k passages based on individual relevance, which fails to exploit combinatorial gains among passages and often introduces substantial redundancy. To address this limitation, we propose OptiSet, a set-centric framework that unifies set selection and set-level ranking for RAG. OptiSet adopts an "Expand-then-Refine" paradigm: it first expands a query into multiple perspectives to enable a diverse candidate pool and then refines the candidate pool via re-selection to form a compact evidence set. We then devise a self-synthesis strategy without strong LLM supervision to derive preference labels from the set conditional utility changes of the generator, thereby identifying complementary and redundant evidence. Finally, we introduce a set-list wise training strategy that jointly optimizes set selection and set-level ranking, enabling the model to favor compact, high-gain evidence sets. Extensive experiments demonstrate that OptiSet improves performance on complex combinatorial problems and makes generation more efficient. The source code is publicly available.

</details>


### [256] [How to Set the Batch Size for Large-Scale Pre-training?](https://arxiv.org/abs/2601.05034)
*Yunhua Zhou,Junhao Huang,Shuhao Xin,Yechen Zhang,Runyu Peng,Qiping Guo,Xipeng Qiu*

Main category: cs.AI

TL;DR: The paper revises the Critical Batch Size theory for modern WSD learning rate schedulers, deriving new E(S) relationships and proposing a dynamic batch size scheduler that improves training efficiency and model quality.


<details>
  <summary>Details</summary>
Motivation: The original Critical Batch Size theory from OpenAI doesn't align with modern pre-training dynamics using Warmup-Stable-Decay (WSD) learning rate schedulers, creating a gap between theory and practice that needs to be addressed.

Method: Derived a revised E(S) relationship specifically for WSD schedulers, analyzed two fundamental properties (B_min and B_opt), and proposed a dynamic Batch Size Scheduler based on these theoretical insights.

Result: Extensive experiments show the revised formula accurately captures large-scale pre-training dynamics, and the proposed scheduling strategy significantly improves both training efficiency and final model quality.

Conclusion: The paper successfully bridges the theory-practice gap for modern pre-training by adapting Critical Batch Size theory to WSD schedulers, providing practical scheduling strategies that enhance efficiency and model performance.

Abstract: The concept of Critical Batch Size, as pioneered by OpenAI, has long served as a foundational principle for large-scale pre-training. However, with the paradigm shift towards the Warmup-Stable-Decay (WSD) learning rate scheduler, we observe that the original theoretical framework and its underlying mechanisms fail to align with new pre-training dynamics. To bridge this gap between theory and practice, this paper derives a revised E(S) relationship tailored for WSD scheduler, characterizing the trade-off between training data consumption E and steps S during pre-training. Our theoretical analysis reveals two fundamental properties of WSD-based pre-training: 1) B_min, the minimum batch size threshold required to achieve a target loss, and 2) B_opt, the optimal batch size that maximizes data efficiency by minimizing total tokens. Building upon these properties, we propose a dynamic Batch Size Scheduler. Extensive experiments demonstrate that our revised formula precisely captures the dynamics of large-scale pre-training, and the resulting scheduling strategy significantly enhances both training efficiency and final model quality.

</details>


### [257] [How to Set the Learning Rate for Large-Scale Pre-training?](https://arxiv.org/abs/2601.05049)
*Yunhua Zhou,Shuhao Xing,Junhao Huang,Xipeng Qiu,Qipeng Guo*

Main category: cs.AI

TL;DR: The paper investigates optimal learning rate configuration for large-scale pre-training, comparing two paradigms: Fitting (using scaling laws to reduce search complexity) and Transfer (extending μTransfer to MoE architectures). It challenges μTransfer's scalability in large-scale settings and provides guidelines for industrial pre-training.


<details>
  <summary>Details</summary>
Motivation: Learning rate configuration is crucial but challenging in large-scale pre-training due to high training costs. The paper aims to determine if optimal learning rates can be extrapolated from low-cost experiments and provide practical solutions for industrial-scale training.

Method: Two research paradigms: 1) Fitting Paradigm - introduces a Scaling Law for search factor to reduce search complexity from O(n³) to O(n*C_D*C_η) via predictive modeling. 2) Transfer Paradigm - extends μTransfer principles to Mixture of Experts (MoE) architecture, covering model depth, weight decay, and token horizons.

Result: Empirical results challenge the scalability of widely adopted μTransfer in large-scale pre-training. Analysis through training stability and feature learning lenses shows why module-wise parameter tuning underperforms at scale. Provides systematic comparison between Fitting and Transfer paradigms.

Conclusion: The work offers practical guidelines and theoretical perspectives for optimizing industrial-level pre-training, questioning the scalability of existing μTransfer methods and providing alternative approaches through scaling laws and extended transfer paradigms.

Abstract: Optimal configuration of the learning rate (LR) is a fundamental yet formidable challenge in large-scale pre-training. Given the stringent trade-off between training costs and model performance, the pivotal question is whether the optimal LR can be accurately extrapolated from low-cost experiments. In this paper, we formalize this investigation into two distinct research paradigms: Fitting and Transfer. Within the Fitting Paradigm, we innovatively introduce a Scaling Law for search factor, effectively reducing the search complexity from O(n^3) to O(n*C_D*C_η) via predictive modeling. Within the Transfer Paradigm, we extend the principles of $μ$Transfer to the Mixture of Experts (MoE) architecture, broadening its applicability to encompass model depth, weight decay, and token horizons. By pushing the boundaries of existing hyperparameter research in terms of scale, we conduct a comprehensive comparison between these two paradigms. Our empirical results challenge the scalability of the widely adopted $μ$ Transfer in large-scale pre-training scenarios. Furthermore, we provide a rigorous analysis through the dual lenses of training stability and feature learning to elucidate the underlying reasons why module-wise parameter tuning underperforms in large-scale settings. This work offers systematic practical guidelines and a fresh theoretical perspective for optimizing industrial-level pre-training.

</details>


### [258] [Learning Latent Action World Models In The Wild](https://arxiv.org/abs/2601.05230)
*Quentin Garrido,Tushar Nagarajan,Basile Terver,Nicolas Ballas,Yann LeCun,Michael Rabbat*

Main category: cs.AI

TL;DR: Learning latent action world models from in-the-wild videos without action labels, addressing challenges of video diversity and lack of common embodiment.


<details>
  <summary>Details</summary>
Motivation: Real-world agents need action prediction capabilities, but world models typically require action labels that are hard to obtain at scale. Learning latent actions from videos alone could enable scaling to diverse real-world scenarios beyond simple simulations and robotics.

Method: Proposes learning latent action models from diverse in-the-wild videos using continuous but constrained latent actions (instead of vector quantization), with specific architectural choices and evaluations. Includes training a controller to map known actions to latent ones for planning tasks.

Result: Continuous constrained latent actions successfully capture complex actions from in-the-wild videos better than vector quantization. Can transfer environmental changes (like humans entering rooms) across videos. Latent actions become spatially localized relative to camera due to lack of common embodiment. Controller enables using latent actions as universal interface for planning with performance comparable to action-conditioned baselines.

Conclusion: The work demonstrates progress toward scaling latent action models to real-world scenarios by addressing challenges of video diversity and embodiment, showing that continuous constrained latent actions can effectively capture complex real-world actions and enable planning tasks.

Abstract: Agents capable of reasoning and planning in the real world require the ability of predicting the consequences of their actions. While world models possess this capability, they most often require action labels, that can be complex to obtain at scale. This motivates the learning of latent action models, that can learn an action space from videos alone. Our work addresses the problem of learning latent actions world models on in-the-wild videos, expanding the scope of existing works that focus on simple robotics simulations, video games, or manipulation data. While this allows us to capture richer actions, it also introduces challenges stemming from the video diversity, such as environmental noise, or the lack of a common embodiment across videos. To address some of the challenges, we discuss properties that actions should follow as well as relevant architectural choices and evaluations. We find that continuous, but constrained, latent actions are able to capture the complexity of actions from in-the-wild videos, something that the common vector quantization does not. We for example find that changes in the environment coming from agents, such as humans entering the room, can be transferred across videos. This highlights the capability of learning actions that are specific to in-the-wild videos. In the absence of a common embodiment across videos, we are mainly able to learn latent actions that become localized in space, relative to the camera. Nonetheless, we are able to train a controller that maps known actions to latent ones, allowing us to use latent actions as a universal interface and solve planning tasks with our world model with similar performance as action-conditioned baselines. Our analyses and experiments provide a step towards scaling latent action models to the real world.

</details>


### [259] [Large language models can effectively convince people to believe conspiracies](https://arxiv.org/abs/2601.05050)
*Thomas H. Costello,Kellin Pelrine,Matthew Kowal,Antonio A. Arechar,Jean-François Godbout,Adam Gleave,David Rand,Gordon Pennycook*

Main category: cs.AI

TL;DR: LLMs like GPT-4o can be equally persuasive in promoting both truth and falsehood, effectively increasing conspiracy beliefs when instructed to argue for them, even with standard guardrails in place.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs' persuasive power advantages truth over falsehood, or if they can promote misbeliefs as easily as refuting them, particularly in the context of conspiracy theories.

Method: Three pre-registered experiments with 2,724 American participants discussing conspiracy theories with GPT-4o. The model was instructed to either argue against ("debunking") or for ("bunking") conspiracies, using both jailbroken (guardrails removed) and standard GPT-4o variants.

Result: 1) Jailbroken GPT-4o was equally effective at increasing and decreasing conspiracy belief. 2) Bunking AI was rated more positively and increased trust in AI more than debunking AI. 3) Standard GPT-4o produced similar effects, showing guardrails did little to prevent promotion of conspiracy beliefs. 4) Corrective conversations reversed induced beliefs, and prompting GPT-4o to use only accurate information dramatically reduced its ability to increase conspiracy beliefs.

Conclusion: LLMs possess potent abilities to promote both truth and falsehood equally, but potential solutions exist to mitigate risks, including corrective conversations and accuracy-focused prompting.

Abstract: Large language models (LLMs) have been shown to be persuasive across a variety of context. But it remains unclear whether this persuasive power advantages truth over falsehood, or if LLMs can promote misbeliefs just as easily as refuting them. Here, we investigate this question across three pre-registered experiments in which participants (N = 2,724 Americans) discussed a conspiracy theory they were uncertain about with GPT-4o, and the model was instructed to either argue against ("debunking") or for ("bunking") that conspiracy. When using a "jailbroken" GPT-4o variant with guardrails removed, the AI was as effective at increasing conspiracy belief as decreasing it. Concerningly, the bunking AI was rated more positively, and increased trust in AI, more than the debunking AI. Surprisingly, we found that using standard GPT-4o produced very similar effects, such that the guardrails imposed by OpenAI did little to revent the LLM from promoting conspiracy beliefs. Encouragingly, however, a corrective conversation reversed these newly induced conspiracy beliefs, and simply prompting GPT-4o to only use accurate information dramatically reduced its ability to increase conspiracy beliefs. Our findings demonstrate that LLMs possess potent abilities to promote both truth and falsehood, but that potential solutions may exist to help mitigate this risk.

</details>


### [260] [Reinforced Efficient Reasoning via Semantically Diverse Exploration](https://arxiv.org/abs/2601.05053)
*Ziqi Zhao,Zhaochun Ren,Jiahong Zou,Liu Yang,Zhiwei Xu,Xuri Ge,Zhumin Chen,Xinyu Ma,Daiting Shi,Shuaiqiang Wang,Dawei Yin,Xin Xin*

Main category: cs.AI

TL;DR: ROSE improves RLVR for LLM reasoning by enhancing exploration diversity with semantic-entropy branching and ε-exploration, and boosting efficiency with length-aware advantage estimation.


<details>
  <summary>Details</summary>
Motivation: Existing MCTS-based RLVR methods for LLMs suffer from limited exploration diversity and inefficient reasoning, which hinders their effectiveness in complex reasoning tasks.

Method: ROSE introduces: 1) semantic-entropy-based branching strategy to capture semantic uncertainty and generate diverse reasoning paths, 2) ε-exploration mechanism to prevent local search, and 3) length-aware segment-level advantage estimator to reward concise reasoning.

Result: Extensive experiments on mathematical reasoning benchmarks with Qwen and Llama models demonstrate ROSE's effectiveness and efficiency compared to existing methods.

Conclusion: ROSE successfully addresses exploration diversity and efficiency challenges in RLVR for LLMs, providing a robust framework for enhanced reasoning capabilities.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in enhancing the reasoning of large language models (LLMs). Monte Carlo Tree Search (MCTS)-based extensions improve upon vanilla RLVR (e.g., GRPO) by providing tree-based reasoning rollouts that enable fine-grained and segment-level credit assignment. However, existing methods still suffer from limited exploration diversity and inefficient reasoning. To address the above challenges, we propose reinforced efficient reasoning via semantically diverse explorations, i.e., ROSE, for LLMs. To encourage more diverse reasoning exploration, our method incorporates a semantic-entropy-based branching strategy and an $\varepsilon$-exploration mechanism. The former operates on already sampled reasoning rollouts to capture semantic uncertainty and select branching points with high semantic divergence to generate new successive reasoning paths, whereas the latter stochastically initiates reasoning rollouts from the root, preventing the search process from becoming overly local. To improve efficiency, we design a length-aware segment-level advantage estimator that rewards concise and correct reasoning while penalizing unnecessarily long reasoning chains. Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE. Codes are available at https://github.com/ZiqiZhao1/ROSE-rl.

</details>


### [261] [Chain-of-Sanitized-Thoughts: Plugging PII Leakage in CoT of Large Reasoning Models](https://arxiv.org/abs/2601.05076)
*Arghyadeep Das,Sai Sreenivas Chintha,Rishiraj Girmal,Kinjal Pandey,Sharvi Endait*

Main category: cs.AI

TL;DR: Large Reasoning Models leak PII in chain-of-thought reasoning even when final answers are sanitized. The paper introduces methods to achieve private reasoning with minimal utility loss.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought reasoning in Large Reasoning Models exposes personally identifiable information (PII) in intermediate steps, creating serious privacy risks even when final answers are sanitized. There's a need for privacy-first reasoning approaches that prevent sensitive information leakage.

Method: 1) Introduces PII-CoT-Bench, a supervised dataset with privacy-aware chain-of-thought annotations; 2) Creates a category-balanced evaluation benchmark covering realistic and adversarial leakage scenarios; 3) Tests two approaches: prompt-based controls for state-of-the-art models and fine-tuning for weaker models.

Result: Both prompt-based controls and fine-tuning substantially reduce PII exposure with minimal degradation in utility. State-of-the-art models benefit most from prompt-based controls, while weaker models require fine-tuning for meaningful leakage reduction.

Conclusion: Private chain-of-thought reasoning can be achieved with minimal utility loss, providing practical guidance for building privacy-preserving reasoning systems. The work demonstrates that deployable interventions can effectively prevent PII leakage without sacrificing performance.

Abstract: Large Reasoning Models (LRMs) improve performance, reliability, and interpretability by generating explicit chain-of-thought (CoT) reasoning, but this transparency introduces a serious privacy risk: intermediate reasoning often leaks personally identifiable information (PII) even when final answers are sanitized. We study how to induce privacy-first reasoning, where models reason without exposing sensitive information, using deployable interventions rather than post-hoc redaction. We introduce PII-CoT-Bench, a supervised dataset with privacy-aware CoT annotations, and a category-balanced evaluation benchmark covering realistic and adversarial leakage scenarios. Our results reveal a capability-dependent trend: state-of-the-art models benefit most from prompt-based controls, whereas weaker models require fine-tuning to achieve meaningful leakage reduction. Across models and categories, both approaches substantially reduce PII exposure with minimal degradation in utility, demonstrating that private reasoning can be achieved without sacrificing performance. Overall, we show that private CoT reasoning can be achieved with minimal utility loss, providing practical guidance for building privacy-preserving reasoning systems.

</details>


### [262] [Arabic Prompts with English Tools: A Benchmark](https://arxiv.org/abs/2601.05101)
*Konstantin Kubrak,Ahmed El-Moselhy,Ammar Alsulami,Remaz Altuwaim,Hassan Ismail Fawaz,Faisal Alsaby*

Main category: cs.AI

TL;DR: First benchmark for evaluating Arabic LLM tool-calling capabilities reveals 5-10% accuracy drop compared to English interactions.


<details>
  <summary>Details</summary>
Motivation: Arabic-native LLMs are advancing but lack proper evaluation benchmarks, especially for tool-calling capabilities. Existing frameworks focus on English, leaving Arabic performance poorly understood despite models being pretrained on mostly English data.

Method: Introduces the first dedicated benchmark for evaluating tool-calling and agentic capabilities of LLMs in Arabic language. Provides standardized framework to measure functional accuracy and robustness in Arabic agentic workflows.

Result: Reveals significant performance gap: tool-calling accuracy drops by average 5-10% when users interact in Arabic, regardless of whether tool descriptions are in Arabic or English.

Conclusion: The benchmark aims to foster development of more reliable and linguistically equitable AI agents for Arabic-speaking users by highlighting critical challenges in Arabic tool-calling performance.

Abstract: Large Language Models (LLMs) are now integral to numerous industries, increasingly serving as the core reasoning engine for autonomous agents that perform complex tasks through tool-use. While the development of Arabic-native LLMs is accelerating, the benchmarks for evaluating their capabilities lag behind, with most existing frameworks focusing on English. A critical and overlooked area is tool-calling, where the performance of models prompted in non-English languages like Arabic is poorly understood, especially since these models are often pretrained on predominantly English data. This paper addresses this critical gap by introducing the first dedicated benchmark for evaluating the tool-calling and agentic capabilities of LLMs in the Arabic language. Our work provides a standardized framework to measure the functional accuracy and robustness of models in Arabic agentic workflows. Our findings reveal a huge performance gap: when users interact in Arabic, tool-calling accuracy drops by an average of 5-10\%, regardless of whether the tool descriptions themselves are in Arabic or English. By shedding light on these critical challenges, this benchmark aims to foster the development of more reliable and linguistically equitable AI agents for Arabic-speaking users.

</details>


### [263] [Token-Level LLM Collaboration via FusionRoute](https://arxiv.org/abs/2601.05106)
*Nuoya Xiong,Yuhang Zhou,Hanqing Zeng,Zhaorun Chen,Furong Huang,Shuchao Bi,Lizhu Zhang,Zhuokai Zhao*

Main category: cs.AI

TL;DR: FusionRoute is a token-level multi-LLM collaboration framework where a lightweight router selects domain experts at each decoding step and contributes complementary logits to refine expert outputs, overcoming limitations of pure expert-only routing.


<details>
  <summary>Details</summary>
Motivation: There's a dilemma between large general-purpose LLMs (expensive to train/deploy) and smaller domain-specialized models (efficient but poor generalization). Existing token-level collaboration methods relying solely on fixed expert outputs are fundamentally limited.

Method: A lightweight router simultaneously selects the most suitable expert at each decoding step and contributes a complementary logit via logit addition to refine or correct the expert's next-token distribution. This combines expert selection with a trainable complementary generator.

Result: FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning across Llama-3 and Gemma-2 families on diverse benchmarks (mathematical reasoning, code generation, instruction following), while remaining competitive with domain experts on their respective tasks.

Conclusion: By augmenting expert selection with a complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions, providing a robust and effective solution to the efficiency vs. generalization trade-off in LLMs.

Abstract: Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.

</details>


### [264] [Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction](https://arxiv.org/abs/2601.05107)
*Muzhao Tian,Zisu Huang,Xiaohua Wang,Jingwen Xu,Zhengkang Guo,Qi Qian,Yuanzhe Shen,Kaitao Song,Jiakang Yuan,Changze Lv,Xiaoqing Zheng*

Main category: cs.AI

TL;DR: SteeM framework enables users to dynamically control LLM agents' memory reliance, avoiding both memory anchoring (over-reliance) and under-utilization of interaction history.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based agents use an "all-or-nothing" approach to memory, causing either memory anchoring (trapped by past interactions) or under-utilization of important history. There's a need for nuanced memory control in long-term human-agent interactions.

Method: Introduces a behavioral metric for memory dependence, then proposes SteeM framework that allows users to dynamically regulate memory reliance from fresh-start mode (innovation) to high-fidelity mode (following history).

Result: SteeM consistently outperforms conventional prompting and rigid memory masking strategies across different scenarios, providing more nuanced and effective control for personalized collaboration.

Conclusion: Memory reliance can be modeled as an explicit, user-controllable dimension, enabling better personalized human-agent collaboration through dynamic memory regulation.

Abstract: As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to \textit{Memory Anchoring}, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose \textbf{Stee}rable \textbf{M}emory Agent, \texttt{SteeM}, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.

</details>


### [265] [GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts](https://arxiv.org/abs/2601.05110)
*Wenhao Zeng,Xuteng Zhang,Yuling Shi,Chao Hu,Yuting Chen,Beijun Shen,Xiaodong Gu*

Main category: cs.AI

TL;DR: GlimpRouter: A training-free framework that uses initial token entropy to predict reasoning step difficulty and route between small/large models, reducing latency while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models (LRMs) have high inference latency and computational cost due to explicit multi-step reasoning chains. Existing routing strategies either use local token probabilities or post-hoc verification, which introduce significant overhead. The paper aims to find an efficient way to determine when a reasoning step requires a large model vs. a small model.

Method: Proposes GlimpRouter, a training-free step-wise collaboration framework that uses the entropy of the first token of each reasoning step as a predictor of step difficulty. A lightweight model generates only the first token, and if the entropy exceeds a threshold, the step is routed to a larger model; otherwise, the small model completes it.

Result: Experiments on multiple benchmarks show significant reduction in inference latency while preserving accuracy. Specifically, achieves 10.7% improvement in accuracy while reducing inference latency by 25.9% compared to standalone large model on AIME25.

Conclusion: Initial token entropy serves as a strong predictor of reasoning step difficulty, enabling efficient step-wise collaboration between models. This suggests computation can be effectively allocated based on a "glimpse of thought" rather than full-step evaluation.

Abstract: Large Reasoning Models (LRMs) achieve remarkable performance by explicitly generating multi-step chains of thought, but this capability incurs substantial inference latency and computational cost. Collaborative inference offers a promising solution by selectively allocating work between lightweight and large models, yet a fundamental challenge remains: determining when a reasoning step requires the capacity of a large model or the efficiency of a small model. Existing routing strategies either rely on local token probabilities or post-hoc verification, introducing significant inference overhead. In this work, we propose a novel perspective on step-wise collaboration: the difficulty of a reasoning step can be inferred from its very first token. Inspired by the "Aha Moment" phenomenon in LRMs, we show that the entropy of the initial token serves as a strong predictor of step difficulty. Building on this insight, we introduce GlimpRouter, a training-free step-wise collaboration framework. GlimpRouter employs a lightweight model to generate only the first token of each reasoning step and routes the step to a larger model only when the initial token entropy exceeds a threshold. Experiments on multiple benchmarks demonstrate that our approach significantly reduces inference latency while preserving accuracy. For instance, GlimpRouter attains a substantial 10.7% improvement in accuracy while reducing inference latency by 25.9% compared to a standalone large model on AIME25. These results suggest a simple yet effective mechanism for reasoning: allocating computation based on a glimpse of thought rather than full-step evaluation.

</details>


### [266] [Evaluative Fingerprints: Stable and Systematic Differences in LLM Evaluator Behavior](https://arxiv.org/abs/2601.05114)
*Wajid Nasser*

Main category: cs.AI

TL;DR: LLM judges show near-zero inter-judge agreement but high self-consistency, creating a reliability paradox where each judge has a distinct, stable "evaluative disposition" that functions as a fingerprint.


<details>
  <summary>Details</summary>
Motivation: To investigate the reliability and consistency of LLM-as-judge systems for scalable evaluation, examining whether different LLM judges agree with each other on quality assessments.

Method: Conducted 3,240 evaluations using 9 judges across 120 unique video pack items with 3 independent runs per judge. Analyzed inter-judge agreement using Krippendorff's α, built classifiers to identify judges from rubric scores, and characterized evaluative dispositions along multiple axes including harshness/leniency, dimension emphasis, and evidence behavior.

Result: Inter-judge agreement was near-zero (α = 0.042), with some dimensions showing worse-than-random disagreement (α < 0). However, judges were highly self-consistent and distinguishable: classifiers identified judges with 77.1-89.9% accuracy, reaching 99.6% for distinguishing GPT-4.1 from GPT-5.2. Each judge has a unique, stable "evaluative disposition."

Conclusion: LLM judges are not interchangeable instruments measuring a shared construct but distinct measurement devices with their own implicit theories of quality. Averaging their scores produces synthetic verdicts that don't correspond to any judge's actual values, challenging the reliability of LLM-as-judge systems.

Abstract: LLM-as-judge systems promise scalable, consistent evaluation. We find the opposite: judges are consistent, but not with each other; they are consistent with themselves. Across 3,240 evaluations (9 judges x 120 unique video x pack items x 3 independent runs), inter-judge agreement is near-zero (Krippendorff's α = 0.042). On two dimensions, judges disagree more than random noise would predict (α < 0). Yet this disagreement isn't chaos; it's structured. A classifier identifies which judge produced an evaluation with 77.1% accuracy from rubric scores alone, rising to 89.9% with disposition features. Within model families, the signal is even stronger: GPT-4.1 and GPT-5.2 are distinguishable with 99.6% accuracy. We call this the reliability paradox: judges cannot agree on what constitutes quality, yet their disagreement patterns are so stable they function as fingerprints. Each judge implements a distinct, stable theory of quality: an "evaluative disposition" that shapes how it interprets any rubric. We characterize these dispositions along multiple axes: harshness/leniency, dimension emphasis, within-judge stability (ICC), and evidence behavior (receipt validity, semantic linkage via NLI, and shotgun index). The implication is stark: LLM judges are not interchangeable instruments measuring a shared construct. They are distinct measurement devices, each encoding its own implicit theory of quality. Averaging their scores produces a synthetic verdict that corresponds to no judge's actual values.

</details>


### [267] [Distilling the Thought, Watermarking the Answer: A Principle Semantic Guided Watermark for Large Reasoning Models](https://arxiv.org/abs/2601.05144)
*Shuliang Liu,Xingyu Li,Hongyi Liu,Yibo Yan,Bingchen Duan,Qi Zheng,Dong Fang,Lingfeng Su,Xuming Hu*

Main category: cs.AI

TL;DR: ReasonMark is a novel watermarking framework for reasoning LLMs that separates generation into undisturbed thinking and watermarked answering phases, using semantic guidance to maintain logical coherence while ensuring robust watermark detection.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking methods for reasoning LLMs either disrupt logical coherence (token-based approaches) or introduce high computational costs/latency (semantic-aware approaches), creating challenges for deploying reasoning LLMs in real-world applications that require traceability and trustworthiness.

Method: Decouples generation into Thinking Phase (undisturbed) and Answering Phase (watermarked). Uses Criticality Score to identify semantically pivotal tokens from reasoning traces, distills them into Principal Semantic Vector (PSV), and applies semantically-adaptive watermarking that modulates strength based on token-PSV alignment.

Result: Outperforms state-of-the-art methods: reduces text Perplexity by 0.35, increases translation BLEU score by 0.164, raises mathematical accuracy by 0.67 points, achieves 0.34% higher watermark detection AUC, stronger robustness to attacks, with negligible latency increase.

Conclusion: ReasonMark enables traceable and trustworthy deployment of reasoning LLMs in real-world applications by maintaining logical integrity while providing robust watermarking, addressing key limitations of existing approaches.

Abstract: Reasoning Large Language Models (RLLMs) excelling in complex tasks present unique challenges for digital watermarking, as existing methods often disrupt logical coherence or incur high computational costs. Token-based watermarking techniques can corrupt the reasoning flow by applying pseudo-random biases, while semantic-aware approaches improve quality but introduce significant latency or require auxiliary models. This paper introduces ReasonMark, a novel watermarking framework specifically designed for reasoning-intensive LLMs. Our approach decouples generation into an undisturbed Thinking Phase and a watermarked Answering Phase. We propose a Criticality Score to identify semantically pivotal tokens from the reasoning trace, which are distilled into a Principal Semantic Vector (PSV). The PSV then guides a semantically-adaptive mechanism that modulates watermark strength based on token-PSV alignment, ensuring robustness without compromising logical integrity. Extensive experiments show ReasonMark surpasses state-of-the-art methods by reducing text Perplexity by 0.35, increasing translation BLEU score by 0.164, and raising mathematical accuracy by 0.67 points. These advancements are achieved alongside a 0.34% higher watermark detection AUC and stronger robustness to attacks, all with a negligible increase in latency. This work enables the traceable and trustworthy deployment of reasoning LLMs in real-world applications.

</details>


### [268] [Observations and Remedies for Large Language Model Bias in Self-Consuming Performative Loop](https://arxiv.org/abs/2601.05184)
*Yaxuan Wang,Zhongteng Cai,Yujia Bao,Xueru Zhang,Yang Liu*

Main category: cs.AI

TL;DR: The paper introduces Self-Consuming Performative Loop (SCPL) to study how synthetic data from LLMs creates feedback loops that amplify biases, and proposes reward-based rejection sampling to mitigate these biases.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly trained on their own synthetic outputs, this creates self-consuming retraining loops that can cause performance drops and emerging biases. Real-world applications create dynamic systems where user feedback influences data generation, potentially exacerbating biases against underserved groups.

Method: Introduces SCPL concept to study bias evolution in controlled performative feedback settings. Examines two loop types: typical retraining and incremental fine-tuning. Uses experiments on three real-world tasks and designs a reward-based rejection sampling strategy to mitigate bias.

Result: Performative loops increase preference bias but decrease disparate bias. The proposed reward-based rejection sampling strategy helps mitigate bias in self-improving systems.

Conclusion: Self-consuming performative loops in LLM training create complex bias dynamics that need careful management. The proposed rejection sampling approach moves toward more trustworthy self-improving AI systems by addressing feedback-driven bias evolution.

Abstract: The rapid advancement of large language models (LLMs) has led to growing interest in using synthetic data to train future models. However, this creates a self-consuming retraining loop, where models are trained on their own outputs and may cause performance drops and induce emerging biases. In real-world applications, previously deployed LLMs may influence the data they generate, leading to a dynamic system driven by user feedback. For example, if a model continues to underserve users from a group, less query data will be collected from this particular demographic of users. In this study, we introduce the concept of \textbf{S}elf-\textbf{C}onsuming \textbf{P}erformative \textbf{L}oop (\textbf{SCPL}) and investigate the role of synthetic data in shaping bias during these dynamic iterative training processes under controlled performative feedback. This controlled setting is motivated by the inaccessibility of real-world user preference data from dynamic production systems, and enables us to isolate and analyze feedback-driven bias evolution in a principled manner. We focus on two types of loops, including the typical retraining setting and the incremental fine-tuning setting, which is largely underexplored. Through experiments on three real-world tasks, we find that the performative loop increases preference bias and decreases disparate bias. We design a reward-based rejection sampling strategy to mitigate the bias, moving towards more trustworthy self-improving systems.

</details>


### [269] [SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning](https://arxiv.org/abs/2601.05187)
*Yanchang Liang,Xiaowei Zhao*

Main category: cs.AI

TL;DR: SimuAgent is an LLM-powered agent for Simulink modeling that uses a concise Python representation instead of verbose XML, with a two-stage training approach and Reflection-GRPO for sparse reward tasks, achieving better performance than GPT-4o on 5300 modeling tasks while running on-premise.


<details>
  <summary>Details</summary>
Motivation: LLMs have transformed text-based code automation but remain under-explored for graph-oriented engineering workflows like Simulink modeling. There's a need for privacy-preserving, cost-effective AI solutions for industrial model-driven engineering that can handle complex graphical modeling environments.

Method: 1) Replaces verbose XML with concise dictionary-style Python representation to reduce tokens and enable fast simulation; 2) Two-stage training: first low-level tool skills, then high-level design reasoning; 3) Reflection-GRPO (ReGRPO) adds self-reflection traces to GRPO for better handling of sparse rewards in long-horizon tasks; 4) Abstract-reconstruct data augmentation for generalization; 5) Evaluated on SimuBench benchmark with 5300 multi-domain modeling tasks.

Result: Qwen2.5-7B fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and surpasses GPT-4o with few-shot prompting on the same benchmark. The two-stage curriculum and data augmentation enhance generalization. The system trains and runs entirely on-premise with modest hardware.

Conclusion: SimuAgent successfully bridges LLMs with graphical modeling environments, providing a practical, privacy-preserving, cost-effective solution for AI-assisted engineering design in industrial settings, demonstrating that specialized LLM agents can outperform general-purpose models like GPT-4o on domain-specific engineering tasks.

Abstract: Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.

</details>


### [270] [Stock Market Price Prediction using Neural Prophet with Deep Neural Network](https://arxiv.org/abs/2601.05202)
*Navin Chhibber,Suneel Khemka,Navneet Kumar Tyagi,Rohit Tewari,Bireswar Banerjee,Piyush Ranjan*

Main category: cs.AI

TL;DR: Proposes NP-DNN (Neural Prophet with Deep Neural Network) model for stock price prediction, achieving 99.21% accuracy using MLP for nonlinear pattern extraction and Z-score normalization preprocessing.


<details>
  <summary>Details</summary>
Motivation: Existing statistical time-series prediction methods often fail to effectively forecast the probability range of future stock prices, creating a need for more accurate prediction models in this interdisciplinary domain.

Method: Uses Neural Prophet with Deep Neural Network (NP-DNN) with Z-score normalization preprocessing, missing value imputation, and Multi-Layer Perceptron (MLP) to learn complex nonlinear relationships and extract hidden patterns from stock price data.

Result: The proposed NP-DNN model achieved 99.21% accuracy, outperforming other approaches including the Fused Large Language Model.

Conclusion: NP-DNN provides an effective solution for stock market price prediction by combining neural prophet architecture with deep learning techniques to capture complex patterns and achieve high prediction accuracy.

Abstract: Stock market price prediction is a significant interdisciplinary research domain that depends at the intersection of finance, statistics, and economics. Forecasting Accurately predicting stock prices has always been a focal point for various researchers. However, existing statistical approaches for time-series prediction often fail to effectively forecast the probability range of future stock prices. Hence, to solve this problem, the Neural Prophet with a Deep Neural Network (NP-DNN) is proposed to predict stock market prices. The preprocessing technique used in this research is Z-score normalization, which normalizes stock price data by removing scale differences, making patterns easier to detect. Missing value imputation fills gaps in historical data, enhancing the models use of complete information for more accurate predictions. The Multi-Layer Perceptron (MLP) learns complex nonlinear relationships among stock market prices and extracts hidden patterns from the input data, thereby creating meaningful feature representations for better prediction accuracy. The proposed NP-DNN model achieved an accuracy of 99.21% compared with other approaches using the Fused Large Language Model. Keywords: deep neural network, forecasting stock prices, multi-layer perceptron, neural prophet, stock market price prediction.

</details>


### [271] [Internal Representations as Indicators of Hallucinations in Agent Tool Selection](https://arxiv.org/abs/2601.05214)
*Kait Healy,Bharathi Srinivasan,Visakh Madathil,Jing Wu*

Main category: cs.AI

TL;DR: A real-time hallucination detection framework for LLM tool calling that uses internal representations during generation to identify incorrect tool selection, malformed parameters, and tool bypass behavior with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: LLMs exhibit hallucinations in tool calling (incorrect tool selection, malformed parameters, tool bypass) which undermines reliability in production systems, bypasses security controls, and requires early detection without existing efficient solutions.

Method: Computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation, avoiding multiple forward passes or external validation.

Result: Achieves up to 86.4% accuracy in hallucination detection across multiple reasoning domains while maintaining real-time inference with minimal computational overhead, particularly effective for parameter-level hallucinations and inappropriate tool selections.

Conclusion: The framework enables reliable LLM agent deployment by providing efficient real-time hallucination detection critical for production systems, addressing security, audit, and consistency concerns in tool-calling applications.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.

</details>


### [272] [MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents](https://arxiv.org/abs/2601.05215)
*Tamil Sudaravan Mohan Doss,Michael Xu,Sudha Rao,Andrew D. Wilson,Balasaravanan Thoravi Kumaravel*

Main category: cs.AI

TL;DR: MineNPC-Task is a user-authored benchmark for evaluating memory-aware LLM agents in Minecraft, featuring parametric task templates with explicit dependencies and machine-checkable validators under a bounded-knowledge policy.


<details>
  <summary>Details</summary>
Motivation: To create a realistic benchmark for testing memory-aware, mixed-initiative LLM agents in open-world Minecraft environments, moving beyond synthetic prompts by using tasks elicited from actual player experiences.

Method: Tasks are elicited from formative and summative co-play with expert players, normalized into parametric templates with explicit preconditions and dependency structure, and paired with machine-checkable validators under a bounded-knowledge policy that forbids out-of-world shortcuts.

Result: Initial evaluation with GPT-4o on 216 subtasks across 8 experienced players revealed recurring breakdown patterns in code execution, inventory/tool handling, referencing, and navigation, but also showed recoveries supported by mixed-initiative clarifications and lightweight memory. Participants rated interaction quality and interface usability positively.

Conclusion: The framework successfully captures memory-aware agent performance in realistic Minecraft scenarios, identifies key failure patterns, and demonstrates the value of mixed-initiative interactions. The released task suite, validators, logs, and harness support transparent, reproducible evaluation of future memory-aware embodied agents.

Abstract: We present \textsc{MineNPC-Task}, a user-authored benchmark and evaluation harness for testing memory-aware, mixed-initiative LLM agents in open-world \emph{Minecraft}. Rather than relying on synthetic prompts, tasks are elicited from formative and summative co-play with expert players, normalized into parametric templates with explicit preconditions and dependency structure, and paired with machine-checkable validators under a bounded-knowledge policy that forbids out-of-world shortcuts. The harness captures plan/act/memory events-including plan previews, targeted clarifications, memory reads and writes, precondition checks, and repair attempts and reports outcomes relative to the total number of attempted subtasks, derived from in-world evidence.
  As an initial snapshot, we instantiate the framework with GPT-4o and evaluate \textbf{216} subtasks across \textbf{8} experienced players. We observe recurring breakdown patterns in code execution, inventory/tool handling, referencing, and navigation, alongside recoveries supported by mixed-initiative clarifications and lightweight memory. Participants rated interaction quality and interface usability positively, while highlighting the need for stronger memory persistence across tasks. We release the complete task suite, validators, logs, and harness to support transparent, reproducible evaluation of future memory-aware embodied agents.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [273] [Towards Radar-Agnostic Gait Analysis Across UWB and FMCW Systems](https://arxiv.org/abs/2601.04415)
*Charalambos Hadjipanayi,Maowen Yin,Alan Bannon,Ziwei Chen,Timothy G. Constandinou*

Main category: eess.SP

TL;DR: A unified processing framework works equally well for both IR-UWB and FMCW radar modalities in gait analysis, achieving 85-98% accuracy with minimal inter-modality differences.


<details>
  <summary>Details</summary>
Motivation: To determine if a single processing framework can be applied to different radar modalities (IR-UWB and FMCW) for gait analysis without modality-specific tuning, enabling radar-agnostic systems.

Method: Used collocated IR-UWB and FMCW radars with identical processing settings, introduced modality-independent walking-segment identification, and compared extracted gait parameters (stride time, length, speed, swing/stance time) against motion capture reference.

Result: Both radar modalities achieved 85-98% mean estimation accuracy across all parameters, with inter-modality differences below 4.1%. Correlation, Bland-Altman, and intraclass correlation analyses showed minimal bias, comparable limits of agreement, and strong agreement with reference estimates.

Conclusion: No practically meaningful performance differences exist between radar modalities when using a shared processing framework, supporting the feasibility of radar-agnostic gait analysis systems for unobtrusive in-home monitoring.

Abstract: Radar sensing has emerged in recent years as a promising solution for unobtrusive and continuous in-home gait monitoring. This study evaluates whether a unified processing framework can be applied to radar-based spatiotemporal gait analysis independent of radar modality. The framework is validated using collocated impulse-radio ultra-wideband (IR-UWB) and frequency-modulated continuous-wave (FMCW) radars under identical processing settings, without modality-specific tuning, during repeated overground walking trials with 10 healthy participants. A modality-independent approach for automatic walking-segment identification is also introduced to ensure fair and reproducible modality performance assessment. Clinically relevant spatiotemporal gait parameters, including stride time, stride length, walking speed, swing time, and stance time, extracted from each modality were compared against gold-standard motion capture reference estimates. Across all parameters, both radar modalities achieved comparably high mean estimation accuracy in the range of 85-98%, with inter-modality differences remaining below 4.1%, resulting in highly overlapping accuracy distributions. Correlation and Bland-Altman analyses revealed minimal bias, comparable limits of agreement, and strong agreement with reference estimates, while intraclass correlation analysis demonstrated high consistency between radar modalities. These findings indicate that no practically meaningful performance differences arise from radar modality when using a shared processing framework, supporting the feasibility of radar-agnostic gait analysis systems.

</details>


### [274] [Prediction of Cellular Malignancy Using Electrical Impedance Signatures and Supervised Machine Learning](https://arxiv.org/abs/2601.04478)
*Shadeeb Hossain*

Main category: eess.SP

TL;DR: Systematic review of 33 papers shows bioelectrical properties can distinguish healthy vs. malignant cells. Random Forest achieved ~90% accuracy for classification using these parameters.


<details>
  <summary>Details</summary>
Motivation: Bioelectrical properties (permittivity, conductivity, time constants) differ between healthy and malignant cells, offering potential for diagnostic applications. Need to systematically evaluate these parameters for predictive modeling.

Method: Systematic review of 33 articles to compile quantitative bioelectric parameter datasets. Implemented three supervised ML algorithms (Random Forest, SVM, KNN) with hyperparameter tuning. Evaluated using accuracy and F1 score metrics.

Result: Random Forest achieved highest predictive accuracy (~90%) with max depth of 4 and 100 estimators. KNN and SVM achieved F1 scores of ~78% and ~76.5% respectively. Bioelectrical properties combined with ML show strong classification potential.

Conclusion: Integration of bioelectrical property analysis with machine learning offers promising approach for improved diagnostic decision-making. Future work includes additional features, stimulated datasets, hyperparameter optimization, and hardware prototypes for real-time in-situ classification.

Abstract: Bioelectrical properties of cells such as relative permittivity, conductivity, and characteristic time constants vary significantly between healthy and malignant cells across different frequencies. These distinctions provide a promising foundation for diagnostic and classification applications. This study systematically reviewed 33 scholarly articles to compile datasets of quantitative bioelectric parameters and evaluated their utility in predictive modeling. Three supervised machine learning algorithms- Random Forest (RF), Support Vector Machine (SVM), and K-Nearest Neighbor (KNN) were implemented and tuned using key hyperparameters to assess classification performance. Model effectiveness was evaluated using accuracy and F1 score as performance metrics. Results demonstrate that Random Forest achieved the highest predictive accuracy of ~ 90% when configured with a maximum depth of 4 and 100 estimators. These findings highlight the potential of integrating bioelectrical property analysis with machine learning for improved diagnostic decision-making. Similarly, for KNN and SVM, the F1 score peaked at approximately 78% and 76.5%, respectively. Future work will explore incorporating additional discriminative features, leveraging stimulated datasets, and optimizing hyperparameter through advanced search strategies. Ultimately, hardware prototype with embedded micro-electrodes and real-time control systems could pave the path for practical diagnostic tools capable of in-situ cell classification.

</details>


### [275] [Invisible Walls: Privacy-Preserving ISAC Empowered by Reconfigurable Intelligent Surfaces](https://arxiv.org/abs/2601.04488)
*Yinghui He,Long Fan,Lei Xie,Dusit Niyato,Chau Yuen,Jun Luo*

Main category: eess.SP

TL;DR: PrivISAC is a plug-and-play RIS-based solution that protects user privacy in ISAC systems by randomly switching between limited RIS configurations to mask sensitive information from eavesdroppers while maintaining communication quality and enabling legitimate sensing.


<details>
  <summary>Details</summary>
Motivation: Wireless signals like CSI carry environmental/target information for ISAC but create privacy risks through eavesdropping. Existing solutions either ignore legitimate user needs or require complex, expensive hardware.

Method: Each RIS row gets two distinct beamforming vectors forming limited configurations. Randomly activates one configuration per time slot to introduce perturbations. Vectors are designed to have similar responses in communication direction (preserving throughput) but different responses in sensing direction (masking info). Uses time-domain masking/demasking for legitimate sensing to recover valid CSI.

Result: Implemented on commodity wireless devices, PrivISAC provides strong privacy protection while preserving high-quality legitimate ISAC performance.

Conclusion: PrivISAC offers a practical, low-cost solution for privacy protection in ISAC systems using RIS-based randomization that balances privacy, communication quality, and legitimate sensing capabilities.

Abstract: The environmental and target-related information inherently carried in wireless signals, such as channel state information (CSI), has brought increasing attention to integrated sensing and communication (ISAC). However, it also raises pressing concerns about privacy leakage through eavesdropping. While existing efforts have attempted to mitigate this issue, they either fail to account for the needs of legitimate communication and sensing users or rely on hardware with high complexity and cost. To overcome these limitations, we propose PrivISAC, a plug-and-play, low-cost solution that leverages RIS to protect user privacy while preserving ISAC performance. At the core of PrivISAC is a novel strategy in which each RIS row is assigned two distinct beamforming vectors, from which we deliberately construct a limited set of RIS configurations. During operation, exactly one configuration is randomly activated at each time slot to introduce additional perturbations, effectively masking sensitive sensing information from unauthorized eavesdroppers. To jointly ensure privacy protection and communication performance, we design the two vectors such that their responses remain nearly identical in the communication direction, thereby preserving stable, high-throughput transmission, while exhibiting pronounced differences in the sensing direction, which introduces sufficient perturbations to thwart eavesdroppers. Additionally, to enable legitimate sensing under such randomized configurations, we introduce a time-domain masking and demasking method that allows the authorized receiver to associate each CSI sample with its underlying configuration and eliminate configuration-induced discrepancies, thereby recovering valid CSI. We implement PrivISAC on commodity wireless devices and experiment results show that PrivISAC provides strong privacy protection while preserving high-quality legitimate ISAC.

</details>


### [276] [Spectral point transformer for significant wave height estimation from sea clutter](https://arxiv.org/abs/2601.04581)
*Yi Zhou,Li Wang,Hang Su,Tian Wang*

Main category: eess.SP

TL;DR: SPT is a Transformer-based method that estimates significant wave height from sparse spectral points by focusing on key spectral features, achieving efficient and accurate wave height regression with minimal computational resources.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for estimating significant wave height from radar data often require processing full spectral images or sequences, which is computationally expensive. The authors observed that only a minority of spectral points with strong power actually contribute to wave energy, suggesting a more efficient approach could be developed.

Method: SPT (Spectral Point Transformer) uses a Transformer-based architecture to process sparse spectral points rather than full images. It integrates geometric and spectral characteristics of ocean waves through multi-dimensional feature representation, focusing only on the most informative spectral points that contribute to wave energy.

Result: SPT demonstrates superior performance in Hs regression compared to conventional vision networks while using significantly fewer computational resources. The learned features align with physical dispersion relations, and the model can train on 1080 sea clutter image sequences in just 4 minutes on a consumer-grade GPU.

Conclusion: SPT offers an efficient and accurate approach for significant wave height estimation that reduces computational costs and deployment expenses for radar wave-measuring systems, with learned features that align with physical ocean wave principles.

Abstract: This paper presents a method for estimating significant wave height (Hs) from sparse S_pectral P_oint using a T_ransformer-based approach (SPT). Based on empirical observations that only a minority of spectral points with strong power contribute to wave energy, the proposed SPT effectively integrates geometric and spectral characteristics of ocean surface waves to estimate Hs through multi-dimensional feature representation. The experiment reveals an intriguing phenomenon: the learned features of SPT align well with physical dispersion relations, where the contribution-score map of selected points is concentrated along dispersion curves. Compared to conventional vision networks that process image sequences and full spectra, SPT demonstrates superior performance in Hs regression while consuming significantly fewer computational resources. On a consumer-grade GPU, SPT completes the training of regression model for 1080 sea clutter image sequences within 4 minutes, showcasing its potential to reduce deployment costs for radar wave-measuring systems. The open-source implementation of SPT will be available at https://github.com/joeyee/spt

</details>


### [277] [MIMO Beam Map Reconstruction via Toeplitz-Structured Matrix-Vector Tensor Decomposition](https://arxiv.org/abs/2601.04599)
*Hao Sun,Junting Chen,Xianghao Yu*

Main category: eess.SP

TL;DR: Tensor decomposition method for reconstructing MIMO beam maps from sparse measurements using polar coordinate transformation and structural priors.


<details>
  <summary>Details</summary>
Motivation: As 6G networks emerge, understanding spatial distribution of directional beam coverage is crucial for beam management and link optimization, but accurate beam map construction under sparse measurements remains challenging due to incomplete spatial coverage and strong angular variations.

Method: Transform measurements from Cartesian to polar coordinates to reveal matrix-vector outer-product structure. Mathematical demonstration shows matrix factor (beam-space gain) has intrinsic Toeplitz structure due to shift-invariant array responses, while vector factor captures distance-dependent attenuation. Formulate regularized tensor decomposition problem to jointly reconstruct LOS, reflection, and obstruction propagation conditions.

Result: Proposed method significantly enhances data efficiency, achieving over 20% NMSE reduction compared to state-of-the-art baselines, even under sparse sampling regimes.

Conclusion: Tensor decomposition with structural priors enables accurate MIMO beam map reconstruction from limited measurements, addressing key challenges for 6G beam management and optimization.

Abstract: As wireless networks progress toward sixthgeneration (6G), understanding the spatial distribution of directional beam coverage becomes increasingly important for beam management and link optimization. Multiple-input multipleoutput (MIMO) beam map provides such spatial awareness, yet accurate construction under sparse measurements remains difficult due to incomplete spatial coverage and strong angular variations. This paper presents a tensor decomposition approach for reconstructing MIMO beam map from limited measurements. By transforming measurements from a Cartesian coordinate system into a polar coordinate system, we uncover a matrix-vector outer-product structure associated with different propagation conditions. Specifically, we mathematically demonstrate that the matrix factor, representing beam-space gain, exhibits an intrinsic Toeplitz structure due to the shift-invariant nature of array responses, and the vector factor captures distance-dependent attenuation. Leveraging these structural priors, we formulate a regularized tensor decomposition problem to jointly reconstruct line-of-sight (LOS), reflection, and obstruction propagation conditions. Simulation results confirm that the proposed method significantly enhances data efficiency, achieving a normalized mean square error (NMSE) reduction of over 20% compared to state-of-the-art baselines, even under sparse sampling regimes.

</details>


### [278] [An Ultra-Fast MLE for Low SNR Multi-Reference Alignment](https://arxiv.org/abs/2601.04831)
*Shay Kreymer,Amnon Balanov,Tamir Bendory*

Main category: eess.SP

TL;DR: Ultra-fast algorithm for multi-reference alignment over SO(2) using Taylor expansion in low-SNR regime, requiring only one data pass, with high accuracy and serving as excellent EM initialization.


<details>
  <summary>Details</summary>
Motivation: Standard EM for multi-reference alignment becomes computationally prohibitive in low SNR settings, especially for cryo-EM applications where many noisy observations with random rotations need processing.

Method: Perform Taylor expansion of log-likelihood in low-SNR regime, then estimate signal by sequentially computing data-driven averages of observations. Works over special orthogonal group SO(2) and requires only one pass over data.

Result: Method dramatically reduces computational cost compared to EM while achieving high accuracy in low-SNR environments. Also provides excellent initialization for subsequent EM refinement.

Conclusion: Proposed ultra-fast algorithm offers efficient alternative to EM for MRA, particularly valuable in low-SNR cryo-EM applications where computational efficiency is critical.

Abstract: Motivated by single-particle cryo-electron microscopy, multi-reference alignment (MRA) models the task of recovering an unknown signal from multiple noisy observations corrupted by random rotations. The standard approach, expectation-maximization (EM), often becomes computationally prohibitive, particularly in low signal-to-noise ratio (SNR) settings. We introduce an alternative, ultra-fast algorithm for MRA over the special orthogonal group $\mathrm{SO}(2)$. By performing a Taylor expansion of the log-likelihood in the low-SNR regime, we estimate the signal by sequentially computing data-driven averages of observations. Our method requires only one pass over the data, dramatically reducing computational cost compared to EM. Numerical experiments show that the proposed approach achieves high accuracy in low-SNR environments and provides an excellent initialization for subsequent EM refinement.

</details>


### [279] [SE-EE Tradeoff in Pinching-Antenna Systems: Waveguide Multiplexing or Waveguide Switching?](https://arxiv.org/abs/2601.04844)
*Guangyu Zhu,Xidong Mu,Li Guo,Shibiao Xu,Yuanwei Liu,Naofal Al-Dhahir*

Main category: eess.SP

TL;DR: The paper investigates spectral-energy efficiency trade-off in pinching-antenna systems (PASS) with two protocols (WM and WS), using multi-objective optimization and different beamforming strategies for each protocol.


<details>
  <summary>Details</summary>
Motivation: To address the spectral efficiency (SE) and energy efficiency (EE) trade-off in pinching-antenna systems, which can mitigate large-scale path losses compared to conventional antennas.

Method: Formulated multi-objective optimization problem for joint baseband and pinching beamforming, converted to single-objective via ε-constraint method. For WM: alternating optimization with successive convex approximation for baseband and particle swarm optimization for pinching. For WS: time-division interference-free transmission with pinching beamforming adjusted per time slot followed by power allocation.

Result: 1) PASS outperforms conventional antennas by mitigating path losses; 2) WS achieves higher maximum EE by using single RF chain, while WM achieves higher SE upper bound by serving all users concurrently; 3) WM benefits more from more users, while WS shows advantages in low-SNR regimes.

Conclusion: Pinching-antenna systems offer improved SE-EE trade-off with different protocol choices: WS for energy efficiency and WM for spectral efficiency, providing flexible design options for different wireless communication scenarios.

Abstract: The spectral and energy efficiency (SE-EE) trade-off in pinching-antenna systems (PASS) is investigated in this paper. In particular, two practical operating protocols, namely waveguide multiplexing (WM) and waveguide switching (WS), are considered. A multi-objective optimization problem (MOOP) is formulated to jointly optimize the baseband and pinching beamforming for maximizing the achievable SE and EE, which is then converted into a single-objective problem via the ε-constraint method. For WM, the problem is decomposed within the alternating-optimization framework, where the baseband beamforming is optimized using the successive convex approximation, and the pinching beamforming is updated through the particle swarm optimization. For WS, due to the time-division transmission and interference-free nature, the pinching beamforming in each time slot is first adjusted to maximize the served user channel gain, followed by the baseband power allocation. Simulation results demonstrate that 1) PASS outperforms conventional antennas by mitigating large-scale path losses; 2) WS leads to a higher maximum achievable EE by activating a single RF chain, whereas WM yields a higher SE upper bound by serving all users concurrently; and 3) increasing the number of users substantially enhances SE under WM, whereas WS shows more pronounced benefits in low-signal-to-noise ratio regimes.

</details>


### [280] [6D Movable Antenna Enhanced Cell-free MIMO: Two-timescale Decentralized Beamforming and Antenna Movement Optimization](https://arxiv.org/abs/2601.04969)
*Yichi Zhang,Yuchen Zhang,Wenyan Ma,Lipeng Zhu,Jianquan Wang,Wanbin Tang,Rui Zhang*

Main category: eess.SP

TL;DR: 6DMA-aided cell-free MIMO system with two-timescale decentralized optimization for beamforming, antenna positions, and array orientations to reduce overhead and improve scalability.


<details>
  <summary>Details</summary>
Motivation: Traditional centralized beamforming with frequent antenna movements requires global instantaneous CSI sharing, causing high processing delay and overhead that's impractical for high-mobility scenarios with short channel coherence time.

Method: Two-timescale decentralized optimization framework: short timescale - each AP updates receive beamformer using local instantaneous CSI and global statistical CSI; long timescale - central unit optimizes antenna positions and array orientations using global statistical CSI to maximize ergodic sum rate.

Result: Proposed 6DMA system with decentralized beamforming outperforms less flexible antenna movement schemes and achieves performance comparable to centralized beamforming benchmark.

Conclusion: The two-timescale decentralized approach effectively addresses practical implementation challenges of 6DMA systems, providing scalable solution with reduced overhead while maintaining high performance.

Abstract: This paper investigates a six-dimensional movable antenna (6DMA)-aided cell-free multi-user multiple-input multiple-output (MIMO) communication system. In this system, each distributed access point (AP) can flexibly adjust its array orientation and antenna positions to adapt to spatial channel variations and enhance communication performance. However, frequent antenna movements and centralized beamforming based on global instantaneous channel state information (CSI) sharing among APs entail extremely high signal processing delay and system overhead, which is difficult to be practically implemented in high-mobility scenarios with short channel coherence time. To address these practical implementation challenges and improve scalability, a two-timescale decentralized optimization framework is proposed in this paper to jointly design the beamformer, antenna positions, and array orientations. In the short timescale, each AP updates its receive beamformer based on local instantaneous CSI and global statistical CSI. In the long timescale, the central processing unit optimizes the antenna positions and array orientations at all APs based on global statistical CSI to maximize the ergodic sum rate of all users. The resulting optimization problem is non-convex and involves highly coupled variables, thus posing significant challenges for obtaining efficient solutions. To address this problem, a constrained stochastic successive convex approximation algorithm is developed. Numerical results demonstrate that the proposed 6DMA-aided cell-free system with decentralized beamforming significantly outperforms other antenna movement schemes with less flexibility and even achieves a performance comparable to that of the centralized beamforming benchmark.

</details>


### [281] [Ultra-Wideband Transmission Systems From an Energy Perspective: Which Band is Next?](https://arxiv.org/abs/2601.05000)
*Ronit Sohanpal,Mindaugus Jarmolovicius,Jiaqian Yang,Eric Sillekens,Romulo Aparecido,Vitaly Mikhailov,Jiawei Luo,David J. DiGiovanni,Ruben S. Luis,Hideaki Furukawa,Robert I. Killey,Polina Bayvel*

Main category: eess.SP

TL;DR: OESCL-band amplifiers enable 1000 km systems with 2.98x higher throughput but 48% higher energy-per-bit compared to CL-band only.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the power efficiency and performance of OESCL-band amplifiers versus traditional CL-band transmission for long-haul optical communication systems.

Method: Measuring power efficiency of state-of-the-art OESCL-band amplifiers and comparing system performance metrics for 1000 km transmission.

Result: OESCL-band systems achieve 2.98x greater throughput but require +48% higher energy-per-bit compared to CL-band only transmission.

Conclusion: OESCL-band amplifiers offer significantly higher throughput for long-haul transmission at the cost of moderately higher energy consumption per bit, presenting a trade-off between capacity and power efficiency.

Abstract: Measuring the power efficiency of the state-of-the-art OESCL-band amplifiers, we show that 1000 km OESCL-band systems can achieve 2.98x greater throughput for +48% higher energy-per-bit compared to CL-band transmission only.

</details>


### [282] [On the Impact of Channel Aging and Doppler-Affected Clutter on OFDM ISAC Systems](https://arxiv.org/abs/2601.05032)
*Steven Rivetti,Gabor Fodor,Emil Björnson,Mikael Skoglund*

Main category: eess.SP

TL;DR: ISAC system analysis considering both slow-time channel aging and fast-time structured clutter, proposing aging-aware channel estimation and low-complexity clutter suppression for OFDM networks.


<details>
  <summary>Details</summary>
Motivation: The joint impact of channel aging (slow-time evolution) and structured clutter with non-zero Doppler (fast-time evolution) on ISAC performance has been largely overlooked, creating a research gap that needs addressing.

Method: Proposes aging-aware channel estimator using prior pilot observations for time-varying UE channels, and novel low-complexity sensing pipeline that estimates clutter statistics from raw data for suppression before target parameter extraction via range-angle and range-velocity maps.

Result: Demonstrates substantial performance gains over block fading in low-to-moderate mobility regimes, shows effective clutter suppression in practical configurations, and reveals that dedicated sensing streams are required as communication beams provide insufficient range resolution.

Conclusion: The proposed framework successfully addresses the joint impact of channel aging and structured clutter in ISAC systems, offering practical solutions for both communication channel estimation and sensing performance in clutter-dominated environments.

Abstract: The temporal evolution of the propagation environment plays a central role in integrated sensing and communication (ISAC) systems. A slow-time evolution manifests as channel aging in communication links, while a fast-time one is associated with structured clutter with non-zero Doppler. Nevertheless, the joint impact of these two phenomena on ISAC performance has been largely overlooked. This addresses this research gap in a network utilizing orthogonal frequency division multiplexing waveforms. Here, a base station simultaneously serves multiple user equipment (UE) devices and performs monostatic sensing. Channel aging is captured through an autoregressive model with exponential correlation decay. In contrast, clutter is modeled as a collection of uncorrelated, coherent patches with non-zero Doppler, resulting in a Kronecker-separable covariance structure. We propose an aging-aware channel estimator that uses prior pilot observations to estimate the time-varying UE channels, characterized by a non-isotropic multipath fading structure. The clutter's structure enables a novel low-complexity sensing pipeline: clutter statistics are estimated from raw data and subsequently used to suppress the clutter's action, after which target parameters are extracted through range-angle and range-velocity maps. We evaluate the influence of frame length and pilot history on channel estimation accuracy and demonstrate substantial performance gains over block fading in low-to-moderate mobility regimes. The sensing pipeline is implemented in a clutter-dominated environment, demonstrating that effective clutter suppression can be achieved under practical configurations. Furthermore, our results show that dedicated sensing streams are required, as communication beams provide insufficient range resolution.

</details>


### [283] [Multi-band Carrier Phase Positioning toward 6G: Performance Bounds and Efficient Estimators](https://arxiv.org/abs/2601.05178)
*Ehsan Shourezari,Ossi Kaltiokallio,Mehmet C. Ilter,Jukka Talvitie,Gonzalo Seco-Granados,Henk Wymeersch,Mikko Valkama*

Main category: eess.SP

TL;DR: Multi-band carrier phase positioning in 5G/6G networks improves high-precision localization by resolving integer ambiguity with just two carriers, enhancing robustness against clock imperfections and multipath.


<details>
  <summary>Details</summary>
Motivation: Carrier phase positioning is gaining importance in terrestrial mobile networks (5G/6G), but faces the integer ambiguity problem where carrier phases provide only relative position information. There's a need for practical solutions that work across different frequency bands (FR1, FR2, FR3) with real-world imperfections.

Method: The paper studies multi-band CPP with intra- and inter-band carrier aggregation across FR1, mmWave-FR2, and emerging 6G FR3 bands. It derives performance bounds for multi-band CPP and proposes a two-stage practical estimator that achieves these bounds. The method includes a search-based refinement step for narrowband IoT applications and extends to scenarios with nonuniform or disjoint band assignments across base stations.

Result: Numerical results show that only two carriers substantially facilitate resolving integer ambiguity while enhancing robustness against network-side clock imperfections and multipath propagation. The proposed estimator achieves derived bounds under realistic bandwidth and transmit power conditions, and with refinement becomes suitable for narrowband IoT applications. The approach works even with nonuniform or disjoint band assignments across base stations.

Conclusion: Multi-band CPP offers superior high-precision localization for current and future mobile networks, with practical estimators that work across various frequency bands and deployment scenarios. The approach effectively addresses integer ambiguity resolution while maintaining robustness against real-world imperfections, making it suitable for diverse applications including narrowband IoT.

Abstract: In addition to satellite systems, carrier phase positioning (CPP) is gaining attraction also in terrestrial mobile networks, particularly in 5G New Radio evolution toward 6G. One key challenge is to resolve the integer ambiguity problem, as the carrier phase provides only relative position information. This work introduces and studies a multi-band CPP scenario with intra- and inter-band carrier aggregation (CA) opportunities across FR1, mmWave-FR2, and emerging 6G FR3 bands. Specifically, we derive multi-band CPP performance bounds, showcasing the superiority of multi-band CPP for high-precision localization in current and future mobile networks, while noting also practical imperfections such as clock offsets between the user equipment (UE) and the network as well as mutual clock imperfections between the network nodes. A wide collection of numerical results is provided, covering the impacts of the available carrier bandwidth, number of aggregated carriers, transmit power, and the number of network nodes or base stations. The offered results highlight that only two carriers suffice to substantially facilitate resolving the integer ambiguity problem while also largely enhancing the robustness of positioning against imperfections imposed by the network-side clocks and multi-path propagation. In addition, we also propose a two-stage practical estimator that achieves the derived bounds under all realistic bandwidth and transmit power conditions. Furthermore, we show that with an additional search-based refinement step, the proposed estimator becomes particularly suitable for narrowband Internet of Things applications operating efficiently even under narrow carrier bandwidths. Finally, both the derived bounds and the proposed estimators are extended to scenarios where the bands assigned to each base station are nonuniform or fully disjoint, enhancing the practical deployment flexibility.

</details>
