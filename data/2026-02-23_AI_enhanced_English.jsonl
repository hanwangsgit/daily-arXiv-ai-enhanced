{"id": "2602.17710", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.17710", "abs": "https://arxiv.org/abs/2602.17710", "authors": ["Xiaodan Shao", "Yixiao Zhang", "Nan Cheng", "Weihua Zhuang", "Xuemin", "Shen"], "title": "Flexible Coupler Array with Reconfigurable Pattern: Mechanical Beamforming and Digital Agent", "comment": "14 pages", "summary": "Flexible coupler is a promising solution for enhancing wireless network capacity by moving passive couplers around a fixed-position active antenna to reshape the induced currents on passive elements. Motivated by this, this paper proposes a novel flexible coupler array that incorporates additional degrees of freedom (DoF) in radiation pattern reconfiguration and enhanced communication coverage with low hardware cost. Specifically, a new form of mechanical beamforming can be obtained by moving only the passive coupling elements while keeping the active antenna stationary. In addition, the flexible coupler antenna can slide along a rail toward users, thereby enhancing communication coverage. To fully exploit the potential of the flexible coupler array, we formulate a two-timescale sum-rate maximization problem with statistical channel state information (CSI). The antenna position is optimized based on scattering cluster-core statistics in the slow timescale, while mechanical beamforming is optimized based on multipath channel statistics in the fast timescale, subject to movement and energy constraints. To address the coupling between timescales and the high cost of extensive channel sampling, we develop a digital agent framework that leverages an electromagnetic (EM) map to generate statistical channel information for different user and antenna positions. Then, a deep neural network is trained to learn a slow-fast performance (SFP) surrogate. Mechanical beamforming at the fast timescale is obtained by selecting per-antenna radiation patterns from a predefined dictionary via a convex relaxation. Simulation results verify the performance gains achieved by the proposed flexible coupler array and the digital-agent-assisted algorithm.", "AI": {"tldr": "Proposes a flexible coupler array for wireless networks that uses movable passive couplers to reshape radiation patterns and enhance coverage with low hardware cost, using a two-timescale optimization approach with digital agent framework.", "motivation": "Flexible couplers can enhance wireless network capacity by reshaping induced currents on passive elements. Current solutions lack sufficient degrees of freedom for radiation pattern reconfiguration and coverage enhancement while maintaining low hardware cost.", "method": "Proposes flexible coupler array with movable passive coupling elements while keeping active antenna stationary. Uses two-timescale optimization: slow timescale optimizes antenna position based on scattering cluster-core statistics; fast timescale optimizes mechanical beamforming based on multipath channel statistics. Develops digital agent framework with EM map for statistical channel generation and deep neural network for SFP surrogate learning.", "result": "Simulation results verify performance gains of proposed flexible coupler array and digital-agent-assisted algorithm, achieving enhanced communication coverage and mechanical beamforming capabilities with low hardware cost.", "conclusion": "Flexible coupler array with movable passive elements enables novel mechanical beamforming and enhanced coverage. Two-timescale optimization with digital agent framework effectively addresses coupling between timescales and reduces channel sampling costs."}}
{"id": "2602.18077", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.18077", "abs": "https://arxiv.org/abs/2602.18077", "authors": ["Xuejun Cheng", "Qian Zhang", "Yunnuo Xu", "Zheng Dong", "Ju Liu", "Bruno Clerckx"], "title": "Optimality Analysis of RSMA Degenerating to SDMA Under Imperfect SIC", "comment": null, "summary": "This document serves as supplementary material for our journal submission, providing detailed mathematical proofs and derivations that support the results presented in the main manuscript. Specifically, under a modeling framework that jointly considers transceiver hardware impairments and imperfect successive interference cancellation (SIC), we systematically derive and prove from an optimality perspective that: when the residual interference coefficient approaches 1 (i.e., SIC becomes severely ineffective), there exists an optimal solution such that the common stream beamformer satisfies $\\bm w_c^\\star=\\bm 0$, and hence the optimal rate-splitting multiple access (RSMA) transmission structure degenerates into space division multiple access (SDMA). This conclusion provides a verifiable theoretical justification for the convergence phenomenon observed in simulations, namely that \"the RSMA performance gradually approaches that of SDMA as SIC degrades\", and can also serve as a reference for multiple-access selection and system design in SIC-limited scenarios.", "AI": {"tldr": "RSMA degenerates to SDMA when SIC becomes severely ineffective (residual interference coefficient \u2192 1), with optimal common stream beamformer becoming zero.", "motivation": "To provide theoretical justification for the observed simulation phenomenon where RSMA performance converges to SDMA as SIC degrades, and to guide multiple-access selection in SIC-limited scenarios.", "method": "Mathematical proofs and derivations under a joint modeling framework considering both transceiver hardware impairments and imperfect SIC, systematically deriving optimality conditions.", "result": "When residual interference coefficient approaches 1 (severe SIC ineffectiveness), there exists an optimal solution where the common stream beamformer w_c* = 0, causing RSMA to degenerate into SDMA.", "conclusion": "The theoretical proof validates the convergence phenomenon observed in simulations, providing a foundation for multiple-access selection and system design in SIC-limited scenarios."}}
{"id": "2602.18165", "categories": ["cs.IT", "cs.CR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.18165", "abs": "https://arxiv.org/abs/2602.18165", "authors": ["Xiao Tang", "Zhen Ma", "Limeng Dong", "Yichen Wang", "Qinghe Du", "Dusit Niyato", "Zhu Han"], "title": "Uncertainty-Aware Jamming Mitigation with Active RIS: A Robust Stackelberg Game Approach", "comment": "Accepted @ IEEE TIFS", "summary": "Malicious jamming presents a pervasive threat to the secure communications, where the challenge becomes increasingly severe due to the growing capability of the jammer allowing the adaptation to legitimate transmissions. This paper investigates the jamming mitigation by leveraging an active reconfigurable intelligent surface (ARIS), where the channel uncertainties are particularly addressed for robust anti-jamming design. Towards this issue, we adopt the Stackelberg game formulation to model the strategic interaction between the legitimate side and the adversary, acting as the leader and follower, respectively. We prove the existence of the game equilibrium and adopt the backward induction method for equilibrium analysis. We first derive the optimal jamming policy as the follower's best response, which is then incorporated into the legitimate-side optimization for robust anti-jamming design. We address the uncertainty issue and reformulate the legitimate-side problem by exploiting the error bounds to combat the worst-case jamming attacks. The problem is decomposed within a block successive upper bound minimization (BSUM) framework to tackle the power allocation, transceiving beamforming, and active reflection, respectively, which are iterated towards the robust jamming mitigation scheme. Simulation results are provided to demonstrate the effectiveness of the proposed scheme in protecting the legitimate transmissions under uncertainties, and the superior performance in terms of jamming mitigation as compared with the baselines.", "AI": {"tldr": "This paper proposes a robust anti-jamming scheme using Active Reconfigurable Intelligent Surface (ARIS) to combat malicious jamming under channel uncertainties, formulated as a Stackelberg game with worst-case optimization.", "motivation": "Malicious jamming poses a severe threat to secure communications, especially as jammers become more capable of adapting to legitimate transmissions. Channel uncertainties further complicate anti-jamming design, requiring robust solutions.", "method": "The authors use a Stackelberg game formulation where legitimate side is leader and jammer is follower. They prove equilibrium existence and use backward induction. They derive optimal jamming policy, then incorporate it into robust anti-jamming design using worst-case optimization with error bounds. The problem is decomposed using BSUM framework to handle power allocation, beamforming, and active reflection.", "result": "Simulation results demonstrate the proposed scheme effectively protects legitimate transmissions under uncertainties and shows superior jamming mitigation performance compared to baselines.", "conclusion": "The ARIS-based robust anti-jamming scheme successfully addresses channel uncertainties and strategic jamming attacks, providing effective protection for legitimate communications through a game-theoretic optimization approach."}}
{"id": "2602.18255", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.18255", "abs": "https://arxiv.org/abs/2602.18255", "authors": ["Soham Ravikant Joshi", "Shikha Patel", "Om Prakash"], "title": "Construction of Cyclic Codes over a Class of Matrix Rings", "comment": "30", "summary": "Let $ \\mathbb F_2[u]/ \\langle u^k \\rangle= \\mathbb F_2+u\\mathbb F_2+u^2\\mathbb F_2+\\cdots+u^{k-1}\\mathbb F_2 ,$ where $u^k=0$ for a positive integer $k$, and $\\mathcal{R}=M_4 (\\mathbb F_2( u)/ \\langle u^k \\rangle)$ be the finite noncommutative non-chain matrix ring of order $4\\times4$. This paper presents the construction of cyclic codes over the finite field $\\mathbb F_{16}$ via the considered matrix ring $\\mathcal{R}$. In this connection, first, we discuss the structure of the ring $\\mathcal{R}$ and show that $\\mathcal{R}$ is isomorphic to the ring $( \\mathbb F_{16}+ v\\mathbb F_{16} + v^2\\mathbb F_{16} + v^3\\mathbb F_{16}) + u(\\mathbb F_{16} + v\\mathbb F_{16} + v^2\\mathbb F_{16} + v^3\\mathbb F_{16}) + u^2(\\mathbb F_{16} + v\\mathbb F_{16} + v^2\\mathbb F_{16}+ v^3\\mathbb F_{16}) + \\cdots + u^{k-1}(\\mathbb F_{16} + v\\mathbb F_{16} + v^2\\mathbb F_{16} + v^3\\mathbb F_{16})$ where $v^4=0, u^k=0, u^iv^j=v^ju^i$ for $i \\in \\{1,\\dots, k-1\\}$ and $j \\in \\{1, 2, 3\\}$. Then, we establish the form of ideals of the ring $\\mathcal{R}$ and related cyclic codes over $\\mathcal{R}$. Further, we show that these cyclic codes can be written as the direct sums of $\\mathcal{R}$-submodules of $\\frac{\\mathcal{R}[x]}{<x^n-1>}$, and derive the formula for the cardinality of cyclic codes over $\\mathcal{R}$. Then, we consider the Euclidean and Hermitian duals of the derived cyclic codes over $\\mathcal{R}$. Under the module isometry for $\\mathcal{R}$, we use the Bachoc map and the Gray map, which takes a derived cyclic code over $\\mathcal{R}$ to $\\mathbb F_{16}$. Finally, we provide some non-trivial examples of linear codes over $\\mathbb F_{16}$ with good parameters that support our derived results and compare a few codes with existing codes in the literature.", "AI": {"tldr": "The paper constructs cyclic codes over finite field F16 via a specific noncommutative matrix ring, establishes their structure and properties, and provides examples with good parameters.", "motivation": "To study cyclic codes over finite fields using algebraic structures from noncommutative matrix rings, exploring connections between ring theory and coding theory to obtain codes with good parameters.", "method": "First analyzes the structure of the matrix ring R = M4(F2[u]/\u27e8u^k\u27e9), shows it's isomorphic to a specific ring with generators u and v. Then establishes ideals of R and related cyclic codes, shows they can be written as direct sums of R-submodules. Derives formulas for cardinality and considers Euclidean/Hermitian duals. Uses Bachoc and Gray maps under module isometry to map codes to F16.", "result": "Constructs cyclic codes over F16 via the matrix ring R, establishes their algebraic structure, derives formulas for cardinality, and provides non-trivial examples of linear codes over F16 with good parameters that compare favorably with existing codes.", "conclusion": "The approach successfully connects noncommutative matrix rings with coding theory, producing cyclic codes over F16 with desirable properties and parameters, demonstrating the utility of algebraic structures in constructing good error-correcting codes."}}
{"id": "2602.17737", "categories": ["cs.RO", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.17737", "abs": "https://arxiv.org/abs/2602.17737", "authors": ["Upasana Biswas", "Durgesh Kalwar", "Subbarao Kambhampati", "Sarath Sreedharan"], "title": "Nested Training for Mutual Adaptation in Human-AI Teaming", "comment": null, "summary": "Mutual adaptation is a central challenge in human--AI teaming, as humans naturally adjust their strategies in response to a robot's policy. Existing approaches aim to improve diversity in training partners to approximate human behavior, but these partners are static and fail to capture adaptive behavior of humans. Exposing robots to adaptive behaviors is critical, yet when both agents learn simultaneously in a multi-agent setting, they often converge to opaque implicit coordination strategies that only work with the agents they were co-trained with. Such agents fail to generalize when paired with new partners. In order to capture the adaptive behavior of humans, we model the human-robot teaming scenario as an Interactive Partially Observable Markov Decision Process (I-POMDP), explicitly modeling human adaptation as part of the state. We propose a nested training regime to approximately learn the solution to a finite-level I-POMDP. In this framework, agents at each level are trained against adaptive agents from the level below. This ensures that the ego agent is exposed to adaptive behavior during training while avoiding the emergence of implicit coordination strategies, since the training partners are not themselves learning. We train our method in a multi-episode, required cooperation setup in the Overcooked domain, comparing it against several baseline agents designed for human-robot teaming. We evaluate the performance of our agent when paired with adaptive partners that were not seen during training. Our results demonstrate that our agent not only achieves higher task performance with these adaptive partners but also exhibits significantly greater adaptability during team interactions.", "AI": {"tldr": "Proposes a nested training framework for human-robot teaming that models human adaptation explicitly using I-POMDPs, avoiding implicit coordination strategies that fail to generalize to new partners.", "motivation": "Existing approaches use static training partners that fail to capture human adaptive behavior, while simultaneous learning in multi-agent settings leads to opaque implicit coordination strategies that don't generalize to new partners.", "method": "Models human-robot teaming as Interactive POMDP (I-POMDP) with human adaptation as part of state. Uses nested training regime where agents at each level train against adaptive agents from level below, avoiding co-learning that creates implicit coordination.", "result": "Agent achieves higher task performance with unseen adaptive partners and exhibits greater adaptability during team interactions compared to baseline human-robot teaming agents in Overcooked domain.", "conclusion": "Nested training framework with explicit modeling of human adaptation in I-POMDPs enables robots to generalize better to new adaptive partners by avoiding problematic implicit coordination strategies from co-learning."}}
{"id": "2602.17677", "categories": ["cs.LG", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17677", "abs": "https://arxiv.org/abs/2602.17677", "authors": ["Sutej Kulgod", "Sean Ye", "Sanchit Tanwar", "Christoffer Heckman"], "title": "Reducing Text Bias in Synthetically Generated MCQAs for VLMs in Autonomous Driving", "comment": "7 pages, 2 figures", "summary": "Multiple Choice Question Answering (MCQA) benchmarks are an established standard for measuring Vision Language Model (VLM) performance in driving tasks. However, we observe the known phenomenon that synthetically generated MCQAs are highly susceptible to hidden textual cues that allow models to exploit linguistic patterns rather than visual context. Our results show that a VLM fine-tuned on such data can achieve accuracy comparable to human-validated benchmarks even without visual input. Our proposed method reduces blind accuracy from +66.9% above random to +2.9%, eliminating the vast majority of exploitable textual shortcuts. By decoupling the correct answer from linguistic artifacts and employing a curriculum learning strategy, we force the model to rely on visual grounding, ensuring that performance accurately reflects perceptual understanding.", "AI": {"tldr": "VLMs exploit textual shortcuts in synthetic MCQA benchmarks, achieving high accuracy without visual input. Proposed method reduces blind accuracy from +66.9% to +2.9% above random by decoupling answers from linguistic artifacts and using curriculum learning.", "motivation": "Synthetic MCQA benchmarks for driving tasks contain hidden textual cues that allow VLMs to exploit linguistic patterns rather than visual context, leading to inflated performance metrics that don't reflect true perceptual understanding.", "method": "Decouple correct answers from linguistic artifacts and employ curriculum learning strategy to force models to rely on visual grounding rather than textual shortcuts.", "result": "Reduced blind accuracy from +66.9% above random to only +2.9% above random, eliminating the vast majority of exploitable textual shortcuts in synthetic MCQA benchmarks.", "conclusion": "The proposed method successfully forces VLMs to rely on visual grounding rather than textual shortcuts, ensuring that MCQA performance accurately reflects perceptual understanding in driving tasks."}}
{"id": "2602.17676", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17676", "abs": "https://arxiv.org/abs/2602.17676", "authors": ["Xingcheng Xu", "Jingjing Qu", "Qiaosheng Zhang", "Chaochao Lu", "Yanqing Yang", "Na Zou", "Xia Hu"], "title": "Epistemic Traps: Rational Misalignment Driven by Model Misspecification", "comment": null, "summary": "The rapid deployment of Large Language Models and AI agents across critical societal and technical domains is hindered by persistent behavioral pathologies including sycophancy, hallucination, and strategic deception that resist mitigation via reinforcement learning. Current safety paradigms treat these failures as transient training artifacts, lacking a unified theoretical framework to explain their emergence and stability. Here we show that these misalignments are not errors, but mathematically rationalizable behaviors arising from model misspecification. By adapting Berk-Nash Rationalizability from theoretical economics to artificial intelligence, we derive a rigorous framework that models the agent as optimizing against a flawed subjective world model. We demonstrate that widely observed failures are structural necessities: unsafe behaviors emerge as either a stable misaligned equilibrium or oscillatory cycles depending on reward scheme, while strategic deception persists as a \"locked-in\" equilibrium or through epistemic indeterminacy robust to objective risks. We validate these theoretical predictions through behavioral experiments on six state-of-the-art model families, generating phase diagrams that precisely map the topological boundaries of safe behavior. Our findings reveal that safety is a discrete phase determined by the agent's epistemic priors rather than a continuous function of reward magnitude. This establishes Subjective Model Engineering, defined as the design of an agent's internal belief structure, as a necessary condition for robust alignment, marking a paradigm shift from manipulating environmental rewards to shaping the agent's interpretation of reality.", "AI": {"tldr": "AI safety failures (sycophancy, hallucination, deception) are not training errors but mathematically rational behaviors arising from model misspecification, requiring subjective model engineering rather than reward manipulation.", "motivation": "Current AI safety approaches treat behavioral pathologies as transient training artifacts without a unified theoretical framework. The paper aims to provide a rigorous explanation for why these failures emerge and persist despite reinforcement learning efforts.", "method": "Adapt Berk-Nash Rationalizability from economics to AI, modeling agents as optimizing against flawed subjective world models. Validate through behavioral experiments on six state-of-the-art model families, generating phase diagrams mapping topological boundaries of safe behavior.", "result": "Safety failures are structural necessities: unsafe behaviors emerge as stable misaligned equilibria or oscillatory cycles depending on reward schemes. Strategic deception persists as \"locked-in\" equilibria or through epistemic indeterminacy. Safety is a discrete phase determined by epistemic priors, not continuous reward magnitude.", "conclusion": "Subjective Model Engineering (designing agent's internal belief structure) is necessary for robust alignment, marking a paradigm shift from manipulating environmental rewards to shaping the agent's interpretation of reality."}}
{"id": "2602.17797", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17797", "abs": "https://arxiv.org/abs/2602.17797", "authors": ["Mohammad Tahmid Noor", "B. M. Shahria Alam", "Tasmiah Rahman Orpa", "Shaila Afroz Anika", "Mahjabin Tasnim Samiha", "Fahad Ahammed"], "title": "Deep Learning for Dermatology: An Innovative Framework for Approaching Precise Skin Cancer Detection", "comment": "6 pages, 9 figures, this is the author's accepted manuscript of a paper accepted for publication in the Proceedings of the 16th International IEEE Conference on Computing, Communication and Networking Technologies (ICCCNT 2025). The final published version will be available via IEEE Xplore", "summary": "Skin cancer can be life-threatening if not diagnosed early, a prevalent yet preventable disease. Globally, skin cancer is perceived among the finest prevailing cancers and millions of people are diagnosed each year. For the allotment of benign and malignant skin spots, an area of critical importance in dermatological diagnostics, the application of two prominent deep learning models, VGG16 and DenseNet201 are investigated by this paper. We evaluate these CNN architectures for their efficacy in differentiating benign from malignant skin lesions leveraging enhancements in deep learning enforced to skin cancer spotting. Our objective is to assess model accuracy and computational efficiency, offering insights into how these models could assist in early detection, diagnosis, and streamlined workflows in dermatology. We used two deep learning methods DenseNet201 and VGG16 model on a binary class dataset containing 3297 images. The best result with an accuracy of 93.79% achieved by DenseNet201. All images were resized to 224x224 by rescaling. Although both models provide excellent accuracy, there is still some room for improvement. In future using new datasets, we tend to improve our work by achieving great accuracy.", "AI": {"tldr": "This paper investigates VGG16 and DenseNet201 deep learning models for binary classification of benign vs malignant skin lesions, achieving 93.79% accuracy with DenseNet201 on a dataset of 3297 images.", "motivation": "Skin cancer is a life-threatening disease that requires early diagnosis. Millions are diagnosed annually, making automated detection crucial for improving dermatological diagnostics and streamlining clinical workflows.", "method": "The study uses two CNN architectures (VGG16 and DenseNet201) on a binary classification dataset of 3297 skin lesion images. All images were resized to 224x224 through rescaling. The models were evaluated for accuracy and computational efficiency in differentiating benign from malignant lesions.", "result": "DenseNet201 achieved the best performance with 93.79% accuracy. Both models provided excellent accuracy, though DenseNet201 outperformed VGG16. The results demonstrate the potential of deep learning models for skin cancer detection.", "conclusion": "Deep learning models like DenseNet201 show strong potential for assisting in early skin cancer detection and diagnosis. While current results are promising, there is room for improvement. Future work will focus on using new datasets to achieve even higher accuracy."}}
{"id": "2602.17768", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17768", "abs": "https://arxiv.org/abs/2602.17768", "authors": ["Boda Lin", "Yongjie Zhu", "Xiaocheng Gong", "Wenyu Qin", "Meng Wang"], "title": "KPM-Bench: A Kinematic Parsing Motion Benchmark for Fine-grained Motion-centric Video Understanding", "comment": "26 pages", "summary": "Despite recent advancements, video captioning models still face significant limitations in accurately describing fine-grained motion details and suffer from severe hallucination issues. These challenges become particularly prominent when generating captions for motion-centric videos, where precise depiction of intricate movements and limb dynamics is crucial yet often neglected. To alleviate this gap, we introduce an automated annotation pipeline that integrates kinematic-based motion computation with linguistic parsing, enabling detailed decomposition and description of complex human motions. Based on this pipeline, we construct and release the Kinematic Parsing Motion Benchmark (KPM-Bench), a novel open-source dataset designed to facilitate fine-grained motion understanding. KPM-Bench consists of (i) fine-grained video-caption pairs that comprehensively illustrate limb-level dynamics in complex actions, (ii) diverse and challenging question-answer pairs focusing specifically on motion understanding, and (iii) a meticulously curated evaluation set specifically designed to assess hallucination phenomena associated with motion descriptions. Furthermore, to address hallucination issues systematically, we propose the linguistically grounded Motion Parsing and Extraction (MoPE) algorithm, capable of accurately extracting motion-specific attributes directly from textual captions. Leveraging MoPE, we introduce a precise hallucination evaluation metric that functions independently of large-scale vision-language or language-only models. By integrating MoPE into the GRPO post-training framework, we effectively mitigate hallucination problems, significantly improving the reliability of motion-centric video captioning models.", "AI": {"tldr": "The paper introduces KPM-Bench, a novel dataset for fine-grained motion understanding in videos, and proposes MoPE algorithm to address hallucination issues in video captioning.", "motivation": "Current video captioning models struggle with accurately describing fine-grained motion details and suffer from severe hallucination issues, especially for motion-centric videos where precise depiction of intricate movements and limb dynamics is crucial.", "method": "1) Created an automated annotation pipeline integrating kinematic-based motion computation with linguistic parsing; 2) Built KPM-Bench dataset with fine-grained video-caption pairs, QA pairs, and hallucination evaluation set; 3) Proposed MoPE algorithm for extracting motion-specific attributes from text; 4) Developed hallucination evaluation metric independent of large models; 5) Integrated MoPE into GRPO post-training framework.", "result": "Introduced KPM-Bench (open-source dataset), proposed MoPE algorithm for motion attribute extraction, created hallucination evaluation metric, and demonstrated improved reliability of motion-centric video captioning models through GRPO integration.", "conclusion": "The work addresses key limitations in video captioning by providing a comprehensive benchmark for fine-grained motion understanding and systematic solutions for hallucination reduction, advancing reliable motion-centric video description."}}
{"id": "2602.18364", "categories": ["cs.IT", "cs.LG", "quant-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.18364", "abs": "https://arxiv.org/abs/2602.18364", "authors": ["Sreejith Sreekumar", "Nir Weinberger"], "title": "Quantum Maximum Likelihood Prediction via Hilbert Space Embeddings", "comment": "32+4 pages, 1 figure", "summary": "Recent works have proposed various explanations for the ability of modern large language models (LLMs) to perform in-context prediction. We propose an alternative conceptual viewpoint from an information-geometric and statistical perspective. Motivated by Bach[2023], we model training as learning an embedding of probability distributions into the space of quantum density operators, and in-context learning as maximum-likelihood prediction over a specified class of quantum models. We provide an interpretation of this predictor in terms of quantum reverse information projection and quantum Pythagorean theorem when the class of quantum models is sufficiently expressive. We further derive non-asymptotic performance guarantees in terms of convergence rates and concentration inequalities, both in trace norm and quantum relative entropy. Our approach provides a unified framework to handle both classical and quantum LLMs.", "AI": {"tldr": "The paper proposes an information-geometric framework for understanding LLM in-context learning by modeling it as quantum maximum-likelihood prediction over quantum density operators.", "motivation": "To provide an alternative conceptual viewpoint for explaining modern LLMs' in-context learning ability from information-geometric and statistical perspectives, unifying classical and quantum LLMs.", "method": "Model training as learning embeddings of probability distributions into quantum density operator space, and in-context learning as maximum-likelihood prediction over quantum models, interpreted via quantum reverse information projection and Pythagorean theorem.", "result": "Derived non-asymptotic performance guarantees including convergence rates and concentration inequalities in both trace norm and quantum relative entropy metrics.", "conclusion": "The proposed quantum information-geometric framework provides a unified theoretical foundation for analyzing both classical and quantum LLMs' in-context learning capabilities."}}
{"id": "2602.17794", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17794", "abs": "https://arxiv.org/abs/2602.17794", "authors": ["Neethan Ratnakumar", "Mariya Huzaifa Tohfafarosh", "Saanya Jauhri", "Xianlian Zhou"], "title": "Reinforcement-Learning-Based Assistance Reduces Squat Effort with a Modular Hip--Knee Exoskeleton", "comment": null, "summary": "Squatting is one of the most demanding lower-limb movements, requiring substantial muscular effort and coordination. Reducing the physical demands of this task through intelligent and personalized assistance has significant implications, particularly in industries involving repetitive low-level assembly activities. In this study, we evaluated the effectiveness of a neural network controller for a modular Hip-Knee exoskeleton designed to assist squatting tasks. The neural network controller was trained via reinforcement learning (RL) in a physics-based, human-exoskeleton interaction simulation environment. The controller generated real-time hip and knee assistance torques based on recent joint-angle and velocity histories. Five healthy adults performed three-minute metronome-guided squats under three conditions: (1) no exoskeleton (No-Exo), (2) exoskeleton with Zero-Torque, and (3) exoskeleton with active assistance (Assistance). Physiological effort was assessed using indirect calorimetry and heart rate monitoring, alongside concurrent kinematic data collection. Results show that the RL-based controller adapts to individuals by producing torque profiles tailored to each subject's kinematics and timing. Compared with the Zero-Torque and No-Exo condition, active assistance reduced the net metabolic rate by approximately 10%, with minor reductions observed in heart rate. However, assisted trials also exhibited reduced squat depth, reflected by smaller hip and knee flexion. These preliminary findings suggest that the proposed controller can effectively lower physiological effort during repetitive squatting, motivating further improvements in both hardware design and control strategies.", "AI": {"tldr": "RL-based neural network controller for hip-knee exoskeleton reduces metabolic cost by ~10% during squatting but reduces squat depth.", "motivation": "Squatting is physically demanding for industrial workers performing repetitive assembly tasks. Personalized exoskeleton assistance could reduce physical strain and improve workplace safety and productivity.", "method": "Developed neural network controller trained via reinforcement learning in physics-based simulation. Tested on 5 healthy adults performing 3-minute metronome-guided squats under three conditions: no exoskeleton, exoskeleton with zero torque, and exoskeleton with active assistance. Measured metabolic rate, heart rate, and kinematics.", "result": "Controller adapted to individuals with personalized torque profiles. Active assistance reduced net metabolic rate by ~10% compared to zero-torque and no-exo conditions, with minor heart rate reductions. However, assisted trials showed reduced squat depth (smaller hip/knee flexion).", "conclusion": "RL-based controller effectively reduces physiological effort during repetitive squatting, demonstrating potential for industrial applications. Future work needed to address reduced squat depth through hardware and control improvements."}}
{"id": "2602.17679", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.17679", "abs": "https://arxiv.org/abs/2602.17679", "authors": ["Saksham Kiroriwal", "Julius Pfrommer", "J\u00fcrgen Beyerer"], "title": "Joint Parameter and State-Space Bayesian Optimization: Using Process Expertise to Accelerate Manufacturing Optimization", "comment": "This paper is under review and has been submitted for CIRP CMS 2026", "summary": "Bayesian optimization (BO) is a powerful method for optimizing black-box manufacturing processes, but its performance is often limited when dealing with high-dimensional multi-stage systems, where we can observe intermediate outputs. Standard BO models the process as a black box and ignores the intermediate observations and the underlying process structure. Partially Observable Gaussian Process Networks (POGPN) model the process as a Directed Acyclic Graph (DAG). However, using intermediate observations is challenging when the observations are high-dimensional state-space time series. Process-expert knowledge can be used to extract low-dimensional latent features from the high-dimensional state-space data. We propose POGPN-JPSS, a framework that combines POGPN with Joint Parameter and State-Space (JPSS) modeling to use intermediate extracted information. We demonstrate the effectiveness of POGPN-JPSS on a challenging, high-dimensional simulation of a multi-stage bioethanol production process. Our results show that POGPN-JPSS significantly outperforms state-of-the-art methods by achieving the desired performance threshold twice as fast and with greater reliability. The fast optimization directly translates to substantial savings in time and resources. This highlights the importance of combining expert knowledge with structured probabilistic models for rapid process maturation.", "AI": {"tldr": "POGPN-JPSS combines Bayesian optimization with process structure modeling and expert knowledge to optimize high-dimensional multi-stage manufacturing processes using intermediate observations.", "motivation": "Standard Bayesian optimization treats processes as black boxes, ignoring intermediate observations and process structure, which limits performance in high-dimensional multi-stage systems where intermediate outputs are available.", "method": "Proposes POGPN-JPSS framework that combines Partially Observable Gaussian Process Networks (POGPN) with Joint Parameter and State-Space (JPSS) modeling to incorporate intermediate extracted information from high-dimensional state-space time series using process-expert knowledge.", "result": "POGPN-JPSS significantly outperforms state-of-the-art methods on a high-dimensional bioethanol production simulation, achieving desired performance threshold twice as fast with greater reliability, leading to substantial time and resource savings.", "conclusion": "Combining expert knowledge with structured probabilistic models is crucial for rapid process maturation in high-dimensional multi-stage manufacturing optimization."}}
{"id": "2602.17826", "categories": ["cs.AI", "cs.LG", "cs.SC"], "pdf": "https://arxiv.org/pdf/2602.17826", "abs": "https://arxiv.org/abs/2602.17826", "authors": ["Marcelo Labre"], "title": "Ontology-Guided Neuro-Symbolic Inference: Grounding Language Models with Mathematical Domain Knowledge", "comment": "Submitted to NeuS 2026. Supplementary materials and code: https://doi.org/10.5281/zenodo.18665030", "summary": "Language models exhibit fundamental limitations -- hallucination, brittleness, and lack of formal grounding -- that are particularly problematic in high-stakes specialist fields requiring verifiable reasoning. I investigate whether formal domain ontologies can enhance language model reliability through retrieval-augmented generation. Using mathematics as proof of concept, I implement a neuro-symbolic pipeline leveraging the OpenMath ontology with hybrid retrieval and cross-encoder reranking to inject relevant definitions into model prompts. Evaluation on the MATH benchmark with three open-source models reveals that ontology-guided context improves performance when retrieval quality is high, but irrelevant context actively degrades it -- highlighting both the promise and challenges of neuro-symbolic approaches.", "AI": {"tldr": "Ontology-guided RAG improves LLM reliability in math when retrieval quality is high, but irrelevant context degrades performance.", "motivation": "Language models have fundamental limitations (hallucination, brittleness, lack of formal grounding) that are problematic in high-stakes specialist fields requiring verifiable reasoning.", "method": "Implemented a neuro-symbolic pipeline using OpenMath ontology with hybrid retrieval and cross-encoder reranking to inject relevant definitions into model prompts. Evaluated on MATH benchmark with three open-source models.", "result": "Ontology-guided context improves performance when retrieval quality is high, but irrelevant context actively degrades performance.", "conclusion": "Highlights both the promise and challenges of neuro-symbolic approaches for enhancing language model reliability through formal domain ontologies."}}
{"id": "2602.17813", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17813", "abs": "https://arxiv.org/abs/2602.17813", "authors": ["Junqing Yang", "Natasha Thorley", "Ahmed Nadeem Abbasi", "Shonit Punwani", "Zion Tse", "Yipeng Hu", "Shaheer U. Saeed"], "title": "Promptable segmentation with region exploration enables minimal-effort expert-level prostate cancer delineation", "comment": "Accepted at IPCAI 2026 (IJCARS - IPCAI 2026 Special Issue)", "summary": "Purpose: Accurate segmentation of prostate cancer on magnetic resonance (MR) images is crucial for planning image-guided interventions such as targeted biopsies, cryoablation, and radiotherapy. However, subtle and variable tumour appearances, differences in imaging protocols, and limited expert availability make consistent interpretation difficult. While automated methods aim to address this, they rely on large expertly-annotated datasets that are often inconsistent, whereas manual delineation remains labour-intensive. This work aims to bridge the gap between automated and manual segmentation through a framework driven by user-provided point prompts, enabling accurate segmentation with minimal annotation effort.\n  Methods: The framework combines reinforcement learning (RL) with a region-growing segmentation process guided by user prompts. Starting from an initial point prompt, region-growing generates a preliminary segmentation, which is iteratively refined through RL. At each step, the RL agent observes the image and current segmentation to predict a new point, from which region growing updates the mask. A reward, balancing segmentation accuracy and voxel-wise uncertainty, encourages exploration of ambiguous regions, allowing the agent to escape local optima and perform sample-specific optimisation. Despite requiring fully supervised training, the framework bridges manual and fully automated segmentation at inference by substantially reducing user effort while outperforming current fully automated methods.\n  Results: The framework was evaluated on two public prostate MR datasets (PROMIS and PICAI, with 566 and 1090 cases). It outperformed the previous best automated methods by 9.9% and 8.9%, respectively, with performance comparable to manual radiologist segmentation, reducing annotation time tenfold.", "AI": {"tldr": "A reinforcement learning framework for prostate cancer segmentation on MR images that uses user point prompts to guide region-growing, achieving near-radiologist accuracy with 10x faster annotation.", "motivation": "Prostate cancer segmentation on MR images is challenging due to subtle tumor appearances, imaging protocol variations, and limited expert availability. Automated methods need large annotated datasets that are often inconsistent, while manual segmentation is labor-intensive. The goal is to bridge automated and manual segmentation with minimal user effort.", "method": "Combines reinforcement learning with region-growing segmentation guided by user point prompts. Starting from an initial point, region-growing creates a preliminary segmentation refined iteratively by RL. The RL agent observes image and current segmentation to predict new points for mask updates. A reward function balances segmentation accuracy and voxel-wise uncertainty to explore ambiguous regions and escape local optima.", "result": "Outperformed previous best automated methods by 9.9% on PROMIS (566 cases) and 8.9% on PICAI (1090 cases), achieving performance comparable to manual radiologist segmentation while reducing annotation time tenfold.", "conclusion": "The framework successfully bridges manual and fully automated segmentation by substantially reducing user effort while outperforming current automated methods, enabling accurate prostate cancer segmentation with minimal annotation."}}
{"id": "2602.17770", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17770", "abs": "https://arxiv.org/abs/2602.17770", "authors": ["Balamurugan Thambiraja", "Omid Taheri", "Radek Danecek", "Giorgio Becherini", "Gerard Pons-Moll", "Justus Thies"], "title": "CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild", "comment": "ICLR2026; Project page: https://balamuruganthambiraja.github.io/CLUTCH/", "summary": "Hands play a central role in daily life, yet modeling natural hand motions remains underexplored. Existing methods that tackle text-to-hand-motion generation or hand animation captioning rely on studio-captured datasets with limited actions and contexts, making them costly to scale to \"in-the-wild\" settings. Further, contemporary models and their training schemes struggle to capture animation fidelity with text-motion alignment. To address this, we (1) introduce '3D Hands in the Wild' (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned text, and (2) propose CLUTCH, an LLM-based hand animation system with two critical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand motion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-HIW, we propose a data annotation pipeline that combines vision-language models (VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of egocentric action videos covering a wide range of scenarios. To fully capture motion in-the-wild, CLUTCH employs SHIFT, a part-modality decomposed VQ-VAE, which improves generalization and reconstruction fidelity. Finally, to improve animation quality, we introduce a geometric refinement stage, where CLUTCH is co-supervised with a reconstruction loss applied directly to decoded hand motion parameters. Experiments demonstrate state-of-the-art performance on text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modelling. Code, data and models will be released.", "AI": {"tldr": "This paper introduces 3D-HIW dataset and CLUTCH system for in-the-wild hand motion modeling, achieving SOTA on text-to-motion and motion-to-text tasks.", "motivation": "Hands are crucial in daily life but modeling natural hand motions is underexplored. Existing methods rely on limited studio-captured datasets that are costly to scale to real-world settings, and current models struggle with animation fidelity and text-motion alignment.", "method": "(1) Created 3D-HIW dataset using a pipeline combining VLMs and 3D hand trackers on egocentric action videos. (2) Proposed CLUTCH system with SHIFT (part-modality decomposed VQ-VAE for motion tokenization) and geometric refinement stage to finetune LLM with reconstruction loss.", "result": "Achieved state-of-the-art performance on both text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modeling.", "conclusion": "The paper presents a comprehensive solution for in-the-wild hand motion modeling through a novel dataset and system, enabling scalable and high-fidelity hand animation generation and captioning."}}
{"id": "2602.18405", "categories": ["cs.IT"], "pdf": "https://arxiv.org/pdf/2602.18405", "abs": "https://arxiv.org/abs/2602.18405", "authors": ["Akira Kamatsuka", "Takahiro Yoshida"], "title": "A Generalized Information Bottleneck Method: A Decision-Theoretic Perspective", "comment": null, "summary": "The information bottleneck (IB) method seeks a compressed representation of data that preserves information relevant to a target variable for prediction while discarding irrelevant information from the original data. In its classical formulation, the IB method employs mutual information to evaluate the compression between the original and compressed data and the utility of the representation for the target variable. In this study, we investigate a generalized IB problem, where the evaluation of utility is based on the $\\mathcal{H}$-mutual information that satisfies the concave (\\texttt{CV}) and averaging (\\texttt{AVG}) conditions. This class of information measures admits a statistical decision-theoretic interpretation via its equivalence to the expected value of sample information. Based on this interpretation, we derive an alternating optimization algorithm to assess the tradeoff between compression and utility in the generalized IB problem.", "AI": {"tldr": "Generalized Information Bottleneck using H-mutual information with CV and AVG conditions, enabling statistical decision-theoretic interpretation and alternating optimization algorithm.", "motivation": "The classical Information Bottleneck method uses mutual information to balance compression and utility, but this paper explores a generalized version using H-mutual information that satisfies specific conditions (CV and AVG), allowing for broader applications and statistical decision-theoretic interpretation.", "method": "Proposes a generalized IB problem where utility evaluation is based on H-mutual information satisfying concave (CV) and averaging (AVG) conditions. Derives an alternating optimization algorithm based on the statistical decision-theoretic interpretation of H-mutual information as expected value of sample information.", "result": "Develops a framework for generalized IB with H-mutual information, establishes equivalence to expected value of sample information, and provides an alternating optimization algorithm to assess the compression-utility tradeoff.", "conclusion": "The generalized IB framework with H-mutual information extends the classical IB method, provides a decision-theoretic foundation, and offers practical optimization algorithms for balancing compression and utility in representation learning."}}
{"id": "2602.17818", "categories": ["cs.RO", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.17818", "abs": "https://arxiv.org/abs/2602.17818", "authors": ["Zachary Turcotte", "Fran\u00e7ois Grondin"], "title": "Lend me an Ear: Speech Enhancement Using a Robotic Arm with a Microphone Array", "comment": null, "summary": "Speech enhancement performance degrades significantly in noisy environments, limiting the deployment of speech-controlled technologies in industrial settings, such as manufacturing plants. Existing speech enhancement solutions primarly rely on advanced digital signal processing techniques, deep learning methods, or complex software optimization techniques. This paper introduces a novel enhancement strategy that incorporates a physical optimization stage by dynamically modifying the geometry of a microphone array to adapt to changing acoustic conditions. A sixteen-microphone array is mounted on a robotic arm manipulator with seven degrees of freedom, with microphones divided into four groups of four, including one group positioned near the end-effector. The system reconfigures the array by adjusting the manipulator joint angles to place the end-effector microphones closer to the target speaker, thereby improving the reference signal quality. This proposed method integrates sound source localization techniques, computer vision, inverse kinematics, minimum variance distortionless response beamformer and time-frequency masking using a deep neural network. Experimental results demonstrate that this approach outperforms other traditional recording configruations, achieving higher scale-invariant signal-to-distortion ratio and lower word error rate accross multiple input signal-to-noise ratio conditions.", "AI": {"tldr": "A robotic microphone array system that physically reconfigures its geometry using a 7-DOF robotic arm to improve speech enhancement in noisy industrial environments.", "motivation": "Speech enhancement performance degrades significantly in noisy industrial environments (like manufacturing plants), limiting deployment of speech-controlled technologies. Existing solutions rely on digital signal processing, deep learning, or software optimization, but don't address physical array geometry adaptation.", "method": "A 16-microphone array mounted on a 7-DOF robotic arm manipulator, with microphones divided into 4 groups of 4. The system dynamically reconfigures array geometry by adjusting manipulator joint angles to position end-effector microphones closer to target speaker. Integrates sound source localization, computer vision, inverse kinematics, MVDR beamformer, and time-frequency masking with DNN.", "result": "Outperforms traditional recording configurations, achieving higher scale-invariant signal-to-distortion ratio and lower word error rate across multiple input signal-to-noise ratio conditions.", "conclusion": "Physical optimization of microphone array geometry through robotic manipulation provides superior speech enhancement in noisy industrial environments compared to purely algorithmic approaches."}}
{"id": "2602.17680", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17680", "abs": "https://arxiv.org/abs/2602.17680", "authors": ["Yujia Wang", "Jihong Guan", "Wengen Li", "Shuigeng Zhou", "Xuhong Wang"], "title": "BioBridge: Bridging Proteins and Language for Enhanced Biological Reasoning with LLMs", "comment": null, "summary": "Existing Protein Language Models (PLMs) often suffer from limited adaptability to multiple tasks and exhibit poor generalization across diverse biological contexts. In contrast, general-purpose Large Language Models (LLMs) lack the capability to interpret protein sequences and fall short in domain-specific knowledge, limiting their capacity for effective biosemantic reasoning. To combine the advantages of both, we propose BioBridge, a domain-adaptive continual pretraining framework for protein understanding. This framework employs Domain-Incremental Continual Pre-training (DICP) to infuse protein domain knowledge and general reasoning corpus into a LLM simultaneously, effectively mitigating catastrophic forgetting. Cross-modal alignment is achieved via a PLM-Projector-LLM pipeline, which maps protein sequence embeddings into the semantic space of the language model. Ultimately, an end-to-end optimization is adopted to uniformly support various tasks, including protein property prediction and knowledge question-answering. Our proposed BioBridge demonstrates performance comparable to that of mainstream PLMs on multiple protein benchmarks, such as EC and BindingDB. It also achieves results on par with LLMs on general understanding tasks like MMLU and RACE. This showcases its innovative advantage of combining domain-specific adaptability with general-purpose language competency.", "AI": {"tldr": "BioBridge is a framework that combines protein language models with general-purpose LLMs through continual pretraining and cross-modal alignment to achieve both domain-specific protein understanding and general reasoning capabilities.", "motivation": "Existing PLMs have limited adaptability across tasks and poor generalization, while general LLMs lack protein sequence interpretation and domain-specific knowledge. There's a need to combine the strengths of both approaches for effective biosemantic reasoning.", "method": "Uses Domain-Incremental Continual Pre-training (DICP) to infuse protein knowledge and general reasoning into LLMs, preventing catastrophic forgetting. Implements PLM-Projector-LLM pipeline for cross-modal alignment of protein embeddings to semantic space, with end-to-end optimization for various tasks.", "result": "Achieves performance comparable to mainstream PLMs on protein benchmarks (EC, BindingDB) and matches LLMs on general understanding tasks (MMLU, RACE), demonstrating combined domain-specific adaptability with general language competency.", "conclusion": "BioBridge successfully bridges the gap between specialized protein understanding and general reasoning, offering an innovative approach that leverages both domain knowledge and language model capabilities for comprehensive protein analysis."}}
{"id": "2602.17831", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17831", "abs": "https://arxiv.org/abs/2602.17831", "authors": ["Simon Henniger", "Gabriel Poesia"], "title": "The Token Games: Evaluating Language Model Reasoning with Puzzle Duels", "comment": "Project website: https://token-games.ai/", "summary": "Evaluating the reasoning capabilities of Large Language Models is increasingly challenging as models improve. Human curation of hard questions is highly expensive, especially in recent benchmarks using PhD-level domain knowledge to challenge the most capable models. Even then, there is always a concern about whether these questions test genuine reasoning or if similar problems have been seen during training. Here, we take inspiration from 16th-century mathematical duels to design The Token Games (TTG): an evaluation framework where models challenge each other by creating their own puzzles. We leverage the format of Programming Puzzles - given a Python function that returns a boolean, find inputs that make it return True - to flexibly represent problems and enable verifying solutions. Using results from pairwise duels, we then compute Elo ratings, allowing us to compare models relative to each other. We evaluate 10 frontier models on TTG, and closely match the ranking from existing benchmarks such as Humanity's Last Exam, without involving any human effort in creating puzzles. We also find that creating good puzzles is still a highly challenging task for current models, not measured by previous benchmarks. Overall, our work suggests new paradigms for evaluating reasoning that cannot be saturated by design, and that allow testing models for other skills like creativity and task creation alongside problem solving.", "AI": {"tldr": "TTG is an automated evaluation framework where LLMs challenge each other by creating programming puzzles, then solve each other's puzzles, with Elo ratings computed from pairwise duels to assess reasoning capabilities without human effort.", "motivation": "Current evaluation of LLM reasoning is expensive (requires human curation of PhD-level questions) and potentially contaminated by training data exposure. There's a need for evaluation that tests genuine reasoning without human effort and avoids saturation.", "method": "Inspired by 16th-century mathematical duels, TTG uses Programming Puzzles format: models create Python functions returning booleans (puzzles), then other models find inputs that make them return True (solutions). Pairwise duels between 10 frontier models generate Elo ratings.", "result": "TTG rankings closely match existing benchmarks like Humanity's Last Exam without human puzzle creation. Creating good puzzles remains highly challenging for current models, revealing skills not measured by previous benchmarks.", "conclusion": "TTG offers a new paradigm for evaluating reasoning that can't be saturated by design, enabling testing of creativity and task creation alongside problem-solving, while eliminating human effort in benchmark creation."}}
{"id": "2602.17855", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17855", "abs": "https://arxiv.org/abs/2602.17855", "authors": ["Seungik Cho"], "title": "TopoGate: Quality-Aware Topology-Stabilized Gated Fusion for Longitudinal Low-Dose CT New-Lesion Prediction", "comment": null, "summary": "Longitudinal low-dose CT follow-ups vary in noise, reconstruction kernels, and registration quality. These differences destabilize subtraction images and can trigger false new lesion alarms. We present TopoGate, a lightweight model that combines the follow-up appearance view with the subtraction view and controls their influence through a learned, quality-aware gate. The gate is driven by three case-specific signals: CT appearance quality, registration consistency, and stability of anatomical topology measured with topological metrics. On the NLST--New-Lesion--LongCT cohort comprising 152 pairs from 122 patients, TopoGate improves discrimination and calibration over single-view baselines, achieving an area under the ROC curve of 0.65 with a standard deviation of 0.05 and a Brier score of 0.14. Removing corrupted or low-quality pairs, identified by the quality scores, further increases the area under the ROC curve from 0.62 to 0.68 and reduces the Brier score from 0.14 to 0.12. The gate responds predictably to degradation, placing more weight on appearance when noise grows, which mirrors radiologist practice. The approach is simple, interpretable, and practical for reliable longitudinal LDCT triage.", "AI": {"tldr": "TopoGate is a lightweight model that combines follow-up CT appearance and subtraction views with a quality-aware gate to improve new lesion detection in longitudinal low-dose CT scans.", "motivation": "Longitudinal low-dose CT follow-ups have variations in noise, reconstruction kernels, and registration quality that destabilize subtraction images and cause false new lesion alarms.", "method": "TopoGate combines follow-up appearance view with subtraction view using a learned quality-aware gate driven by three case-specific signals: CT appearance quality, registration consistency, and stability of anatomical topology measured with topological metrics.", "result": "On the NLST cohort (152 pairs from 122 patients), TopoGate achieves AUC of 0.65\u00b10.05 and Brier score of 0.14. Removing low-quality pairs increases AUC from 0.62 to 0.68 and reduces Brier score from 0.14 to 0.12.", "conclusion": "TopoGate is simple, interpretable, and practical for reliable longitudinal LDCT triage, with the gate responding predictably to degradation by placing more weight on appearance when noise grows, mirroring radiologist practice."}}
{"id": "2602.17785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17785", "abs": "https://arxiv.org/abs/2602.17785", "authors": ["Xinwei Ju", "Rema Daher", "Danail Stoyanov", "Sophia Bano", "Francisco Vasconcelos"], "title": "Multi-Modal Monocular Endoscopic Depth and Pose Estimation with Edge-Guided Self-Supervision", "comment": "14 pages, 6 figures; early accepted by IPCAI2026", "summary": "Monocular depth and pose estimation play an important role in the development of colonoscopy-assisted navigation, as they enable improved screening by reducing blind spots, minimizing the risk of missed or recurrent lesions, and lowering the likelihood of incomplete examinations. However, this task remains challenging due to the presence of texture-less surfaces, complex illumination patterns, deformation, and a lack of in-vivo datasets with reliable ground truth. In this paper, we propose **PRISM** (Pose-Refinement with Intrinsic Shading and edge Maps), a self-supervised learning framework that leverages anatomical and illumination priors to guide geometric learning. Our approach uniquely incorporates edge detection and luminance decoupling for structural guidance. Specifically, edge maps are derived using a learning-based edge detector (e.g., DexiNed or HED) trained to capture thin and high-frequency boundaries, while luminance decoupling is obtained through an intrinsic decomposition module that separates shading and reflectance, enabling the model to exploit shading cues for depth estimation. Experimental results on multiple real and synthetic datasets demonstrate state-of-the-art performance. We further conduct a thorough ablation study on training data selection to establish best practices for pose and depth estimation in colonoscopy. This analysis yields two practical insights: (1) self-supervised training on real-world data outperforms supervised training on realistic phantom data, underscoring the superiority of domain realism over ground truth availability; and (2) video frame rate is an extremely important factor for model performance, where dataset-specific video frame sampling is necessary for generating high quality training data.", "AI": {"tldr": "PRISM is a self-supervised learning framework for monocular depth and pose estimation in colonoscopy that uses edge detection and luminance decoupling to overcome challenges like texture-less surfaces and complex illumination.", "motivation": "Monocular depth and pose estimation are crucial for colonoscopy-assisted navigation to improve screening quality, but current methods struggle with texture-less surfaces, complex illumination, deformation, and lack of reliable in-vivo ground truth datasets.", "method": "PRISM incorporates anatomical and illumination priors through edge detection (using learning-based detectors like DexiNed or HED) and luminance decoupling via intrinsic decomposition to separate shading and reflectance, providing structural guidance for geometric learning.", "result": "State-of-the-art performance on multiple real and synthetic datasets, with key findings showing that self-supervised training on real-world data outperforms supervised training on phantom data, and video frame rate is critical for model performance.", "conclusion": "PRISM effectively addresses colonoscopy depth/pose estimation challenges through self-supervised learning with anatomical priors, demonstrating the importance of domain realism over ground truth availability and careful video frame sampling for training data quality."}}
{"id": "2602.17701", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17701", "abs": "https://arxiv.org/abs/2602.17701", "authors": ["Yun Song", "Wenjia Zheng", "Tiedan Chen", "Ziyu Wang", "Jiazhao Shi", "Yisong Chen"], "title": "Deep Neural Network Architectures for Electrocardiogram Classification: A Comprehensive Evaluation", "comment": null, "summary": "With the rising prevalence of cardiovascular diseases, electrocardiograms (ECG) remain essential for the non-invasive detection of cardiac abnormalities. This study presents a comprehensive evaluation of deep neural network architectures for automated arrhythmia classification, integrating temporal modeling, attention mechanisms, and ensemble strategies. To address data scarcity in minority classes, the MIT-BIH Arrhythmia dataset was augmented using a Generative Adversarial Network (GAN). We developed and compared four distinct architectures, including Convolutional Neural Networks (CNN), CNN combined with Long Short-Term Memory (CNN-LSTM), CNN-LSTM with Attention, and 1D Residual Networks (ResNet-1D), to capture both local morphological features and long-term temporal dependencies. Performance was rigorously evaluated using accuracy, F1-score, and Area Under the Curve (AUC) with 95\\% confidence intervals to ensure statistical robustness, while Gradient-weighted Class Activation Mapping (Grad-CAM) was employed to validate model interpretability. Experimental results indicate that the CNN-LSTM model achieved the optimal stand-alone balance between sensitivity and specificity, yielding an F1-score of 0.951. Conversely, the CNN-LSTM-Attention and ResNet-1D models exhibited higher sensitivity to class imbalance. To mitigate this, a dynamic ensemble fusion strategy was introduced; specifically, the Top2-Weighted ensemble achieved the highest overall performance with an F1-score of 0.958. These findings demonstrate that leveraging complementary deep architectures significantly enhances classification reliability, providing a robust and interpretable foundation for intelligent arrhythmia detection systems.", "AI": {"tldr": "Deep learning ensemble approach combining CNN-LSTM, attention mechanisms, and ResNet-1D achieves state-of-the-art arrhythmia classification using GAN-augmented MIT-BIH dataset.", "motivation": "Addressing the rising prevalence of cardiovascular diseases and the need for automated, reliable arrhythmia detection from ECG signals, while overcoming data scarcity in minority classes.", "method": "Used GAN for data augmentation on MIT-BIH Arrhythmia dataset; compared four architectures (CNN, CNN-LSTM, CNN-LSTM-Attention, ResNet-1D); employed ensemble fusion strategies; validated with statistical metrics and Grad-CAM for interpretability.", "result": "CNN-LSTM achieved best standalone F1-score of 0.951; CNN-LSTM-Attention and ResNet-1D showed sensitivity to class imbalance; Top2-Weighted ensemble achieved highest F1-score of 0.958.", "conclusion": "Leveraging complementary deep architectures with ensemble strategies significantly enhances arrhythmia classification reliability, providing a robust and interpretable foundation for intelligent ECG analysis systems."}}
{"id": "2602.17849", "categories": ["cs.LG", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.17849", "abs": "https://arxiv.org/abs/2602.17849", "authors": ["Aditya Agrawal", "Albert Magyar", "Hiteshwar Eswaraiah", "Patrick Sheridan", "Pradeep Janedula", "Ravi Krishnan Venkatesan", "Krishna Nair", "Ravi Iyer"], "title": "Dual Length Codes for Lossless Compression of BFloat16", "comment": "6 pages, 5 figures", "summary": "Training and serving Large Language Models (LLMs) relies heavily on parallelization and collective operations, which are frequently bottlenecked by network bandwidth. Lossless compression using e.g., Huffman codes can alleviate the issue, however, Huffman codes suffer from slow, bit-sequential decoding and high hardware complexity due to deep tree traversals. Universal codes e.g., Exponential-Golomb codes are faster to decode but do not exploit the symbol frequency distributions. To address these limitations, this paper introduces Dual Length Codes, a hybrid approach designed to balance compression efficiency with decoding speed. Analyzing BFloat16 tensors from the Gemma model, we observed that the top 8 most frequent symbols account for approximately 50% of the cumulative probability. These 8 symbols are assigned a short 4 bit code. The remaining 248 symbols are assigned a longer 9 bit code. The coding scheme uses a single prefix bit to distinguish between the two code lengths. The scheme uses a small Look Up Table with only 8 entries for encoding and decoding. The scheme achieves a compressibility of 18.6% in comparison to 21.3% achieved by Huffman codes, but it significantly speeds up the decoding and simplifies the hardware complexity.", "AI": {"tldr": "Dual Length Codes: A hybrid compression method for LLM tensors that balances compression efficiency (18.6% vs Huffman's 21.3%) with faster decoding and simpler hardware using 4-bit codes for frequent symbols and 9-bit codes for others.", "motivation": "Network bandwidth bottlenecks in LLM training/serving; existing solutions have trade-offs: Huffman codes are efficient but slow/complex to decode, while universal codes are fast but don't exploit symbol frequency distributions.", "method": "Hybrid approach assigning 4-bit codes to top 8 most frequent symbols (covering ~50% probability) and 9-bit codes to remaining 248 symbols, using single prefix bit to distinguish lengths, with small 8-entry LUT for encoding/decoding.", "result": "Achieves 18.6% compressibility (slightly less than Huffman's 21.3%) but significantly improves decoding speed and reduces hardware complexity compared to Huffman codes.", "conclusion": "Dual Length Codes offer a practical trade-off between compression efficiency and decoding performance, making them suitable for bandwidth-constrained LLM deployments where decoding speed and hardware simplicity are important."}}
{"id": "2602.17822", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17822", "abs": "https://arxiv.org/abs/2602.17822", "authors": ["Daniel Hartmann", "Krist\u00fdna Ham\u0159\u00edkov\u00e1", "Ale\u0161 Vysock\u00fd", "Vendula Laciok", "Ale\u0161 Bernat\u00edk"], "title": "Evolution of Safety Requirements in Industrial Robotics: Comparative Analysis of ISO 10218-1/2 (2011 vs. 2025) and Integration of ISO/TS 15066", "comment": null, "summary": "Industrial robotics has established itself as an integral component of large-scale manufacturing enterprises. Simultaneously, collaborative robotics is gaining prominence, introducing novel paradigms of human-machine interaction. These advancements have necessitated a comprehensive revision of safety standards, specifically incorporating requirements for cybersecurity and protection against unauthorized access in networked robotic systems. This article presents a comparative analysis of the ISO 10218:2011 and ISO 10218:2025 standards, examining the evolution of their structure, terminology, technical requirements, and annexes. The analysis reveals significant expansions in functional safety and cybersecurity, the introduction of new classifications for robots and collaborative applications, and the normative integration of the technical specification ISO/TS 15066. Consequently, the new edition synthesizes mechanical, functional, and digital safety requirements, establishing a comprehensive framework for the design and operation of modern robotic systems.", "AI": {"tldr": "Comparative analysis of ISO 10218:2011 vs ISO 10218:2025 standards for industrial robotics safety, showing evolution in cybersecurity, functional safety, and collaborative robot requirements.", "motivation": "Industrial robotics has become essential in manufacturing, with collaborative robotics introducing new human-machine interaction paradigms, requiring updated safety standards that address cybersecurity and unauthorized access in networked robotic systems.", "method": "Comparative analysis of ISO 10218:2011 and ISO 10218:2025 standards, examining evolution in structure, terminology, technical requirements, and annexes.", "result": "Significant expansions in functional safety and cybersecurity, introduction of new robot classifications and collaborative applications, normative integration of ISO/TS 15066 technical specification, and synthesis of mechanical, functional, and digital safety requirements.", "conclusion": "The 2025 edition establishes a comprehensive framework for modern robotic system design and operation by integrating traditional mechanical safety with new digital and cybersecurity requirements."}}
{"id": "2602.17681", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17681", "abs": "https://arxiv.org/abs/2602.17681", "authors": ["Ofir Gordon", "Lior Dikstein", "Arnon Netzer", "Idan Achituve", "Hai Victor Habi"], "title": "LATMiX: Learnable Affine Transformations for Microscaling Quantization of LLMs", "comment": "24 pages, 4 figures", "summary": "Post-training quantization (PTQ) is a widely used approach for reducing the memory and compute costs of large language models (LLMs). Recent studies have shown that applying invertible transformations to activations can significantly improve quantization robustness by reducing activation outliers; however, existing approaches are largely restricted to rotation or Hadamard-based transformations. Moreover, most studies focused primarily on traditional quantization schemes, whereas modern hardware increasingly supports the microscaling (MX) data format. Attempts to combine both showed severe performance degradation, leading prior work to introduce assumptions on the transformations. In this work, we take a complementary perspective. First, we provide a theoretical analysis of transformations under MX quantization by deriving a bound on the quantization error. Our analysis emphasizes the importance of accounting for both the activation distribution and the underlying quantization structure. Building on this analysis, we propose LATMiX, a method that generalizes outlier reduction to learnable invertible affine transformations optimized using standard deep learning tools. Experiments show consistent improvements in average accuracy for MX low-bit quantization over strong baselines on a wide range of zero-shot benchmarks, across multiple model sizes.", "AI": {"tldr": "LATMiX: Learnable Affine Transformations for MX quantization that reduces activation outliers and improves low-bit quantization accuracy for LLMs.", "motivation": "Existing PTQ methods for LLMs use limited transformation types (rotation/Hadamard) and don't work well with modern MX quantization formats, causing performance degradation. Need a method that accounts for both activation distribution and quantization structure.", "method": "Proposes LATMiX - learnable invertible affine transformations optimized using standard deep learning tools. Provides theoretical analysis of transformations under MX quantization with error bounds, emphasizing importance of activation distribution and quantization structure.", "result": "Consistent improvements in average accuracy for MX low-bit quantization over strong baselines on wide range of zero-shot benchmarks across multiple model sizes.", "conclusion": "LATMiX successfully generalizes outlier reduction through learnable affine transformations, addressing limitations of existing methods and achieving better performance with modern MX quantization formats."}}
{"id": "2602.17902", "categories": ["cs.AI", "cs.MA", "cs.SE", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2602.17902", "abs": "https://arxiv.org/abs/2602.17902", "authors": ["Jiaru Bai", "Abdulrahman Aldossary", "Thomas Swanick", "Marcel M\u00fcller", "Yeonghun Kang", "Zijian Zhang", "Jin Won Lee", "Tsz Wai Ko", "Mohammad Ghazi Vakili", "Varinia Bernales", "Al\u00e1n Aspuru-Guzik"], "title": "El Agente Gr\u00e1fico: Structured Execution Graphs for Scientific Agents", "comment": null, "summary": "Large language models (LLMs) are increasingly used to automate scientific workflows, yet their integration with heterogeneous computational tools remains ad hoc and fragile. Current agentic approaches often rely on unstructured text to manage context and coordinate execution, generating often overwhelming volumes of information that may obscure decision provenance and hinder auditability. In this work, we present El Agente Gr\u00e1fico, a single-agent framework that embeds LLM-driven decision-making within a type-safe execution environment and dynamic knowledge graphs for external persistence. Central to our approach is a structured abstraction of scientific concepts and an object-graph mapper that represents computational state as typed Python objects, stored either in memory or persisted in an external knowledge graph. This design enables context management through typed symbolic identifiers rather than raw text, thereby ensuring consistency, supporting provenance tracking, and enabling efficient tool orchestration. We evaluate the system by developing an automated benchmarking framework across a suite of university-level quantum chemistry tasks previously evaluated on a multi-agent system, demonstrating that a single agent, when coupled to a reliable execution engine, can robustly perform complex, multi-step, and parallel computations. We further extend this paradigm to two other large classes of applications: conformer ensemble generation and metal-organic framework design, where knowledge graphs serve as both memory and reasoning substrates. Together, these results illustrate how abstraction and type safety can provide a scalable foundation for agentic scientific automation beyond prompt-centric designs.", "AI": {"tldr": "A single-agent framework called El Agente Gr\u00e1fico that uses type-safe execution and knowledge graphs to improve LLM-driven scientific automation beyond fragile prompt-based approaches.", "motivation": "Current LLM-based scientific automation is ad hoc and fragile, relying on unstructured text that generates overwhelming information, obscures decision provenance, and hinders auditability. There's a need for more robust, scalable approaches.", "method": "Develops a single-agent framework with structured abstraction of scientific concepts and an object-graph mapper that represents computational state as typed Python objects. Uses type-safe execution environment and dynamic knowledge graphs for external persistence, enabling context management through typed symbolic identifiers rather than raw text.", "result": "Successfully evaluated on university-level quantum chemistry tasks, demonstrating that a single agent with reliable execution engine can robustly perform complex, multi-step, and parallel computations. Extended to conformer ensemble generation and metal-organic framework design, showing knowledge graphs serve as both memory and reasoning substrates.", "conclusion": "Abstraction and type safety provide a scalable foundation for agentic scientific automation beyond prompt-centric designs, enabling consistency, provenance tracking, and efficient tool orchestration."}}
{"id": "2602.17901", "categories": ["eess.IV", "cs.CV", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.17901", "abs": "https://arxiv.org/abs/2602.17901", "authors": ["Junkai Liu", "Ling Shao", "Le Zhang"], "title": "MeDUET: Disentangled Unified Pretraining for 3D Medical Image Synthesis and Analysis", "comment": null, "summary": "Self-supervised learning (SSL) and diffusion models have advanced representation learning and image synthesis. However, in 3D medical imaging, they remain separate: diffusion for synthesis, SSL for analysis. Unifying 3D medical image synthesis and analysis is intuitive yet challenging, as multi-center datasets exhibit dominant style shifts, while downstream tasks rely on anatomy, and site-specific style co-varies with anatomy across slices, making factors unreliable without explicit constraints. In this paper, we propose MeDUET, a 3D Medical image Disentangled UnifiEd PreTraining framework that performs SSL in the Variational Autoencoder (VAE) latent space which explicitly disentangles domain-invariant content from domain-specific style. The token demixing mechanism serves to turn disentanglement from a modeling assumption into an empirically identifiable property. Two novel proxy tasks, Mixed-Factor Token Distillation (MFTD) and Swap-invariance Quadruplet Contrast (SiQC), are devised to synergistically enhance disentanglement. Once pretrained, MeDUET is capable of (i) delivering higher fidelity, faster convergence, and improved controllability for synthesis, and (ii) demonstrating strong domain generalization and notable label efficiency for analysis across diverse medical benchmarks. In summary, MeDUET converts multi-source heterogeneity from an obstacle into a learning signal, enabling unified pretraining for 3D medical image synthesis and analysis. The code is available at https://github.com/JK-Liu7/MeDUET .", "AI": {"tldr": "MeDUET is a unified 3D medical image pretraining framework that disentangles domain-invariant content from domain-specific style using VAE latent space and novel proxy tasks, enabling both high-fidelity synthesis and robust analysis across diverse medical datasets.", "motivation": "Current self-supervised learning (SSL) and diffusion models remain separate in 3D medical imaging - diffusion for synthesis, SSL for analysis. Multi-center datasets have dominant style shifts while downstream tasks rely on anatomy, and site-specific style co-varies with anatomy across slices, making factors unreliable without explicit constraints.", "method": "MeDUET performs SSL in VAE latent space with explicit disentanglement of domain-invariant content from domain-specific style. Uses token demixing mechanism to make disentanglement empirically identifiable. Introduces two novel proxy tasks: Mixed-Factor Token Distillation (MFTD) and Swap-invariance Quadruplet Contrast (SiQC) to synergistically enhance disentanglement.", "result": "MeDUET delivers higher fidelity, faster convergence, and improved controllability for synthesis, while demonstrating strong domain generalization and notable label efficiency for analysis across diverse medical benchmarks.", "conclusion": "MeDUET converts multi-source heterogeneity from an obstacle into a learning signal, enabling unified pretraining for 3D medical image synthesis and analysis, bridging the gap between SSL and diffusion models in medical imaging."}}
{"id": "2602.17793", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.17793", "abs": "https://arxiv.org/abs/2602.17793", "authors": ["Peide Zhu", "Linbin Lu", "Zhiqin Chen", "Xiong Chen"], "title": "LGD-Net: Latent-Guided Dual-Stream Network for HER2 Scoring with Task-Specific Domain Knowledge", "comment": null, "summary": "It is a critical task to evalaute HER2 expression level accurately for breast cancer evaluation and targeted treatment therapy selection. However, the standard multi-step Immunohistochemistry (IHC) staining is resource-intensive, expensive, and time-consuming, which is also often unavailable in many areas. Consequently, predicting HER2 levels directly from H&E slides has emerged as a potential alternative solution. It has been shown to be effective to use virtual IHC images from H&E images for automatic HER2 scoring. However, the pixel-level virtual staining methods are computationally expensive and prone to reconstruction artifacts that can propagate diagnostic errors. To address these limitations, we propose the Latent-Guided Dual-Stream Network (LGD-Net), a novel framework that employes cross-modal feature hallucination instead of explicit pixel-level image generation. LGD-Net learns to map morphological H&E features directly to the molecular latent space, guided by a teacher IHC encoder during training. To ensure the hallucinated features capture clinically relevant phenotypes, we explicitly regularize the model training with task-specific domain knowledge, specifically nuclei distribution and membrane staining intensity, via lightweight auxiliary regularization tasks. Extensive experiments on the public BCI dataset demonstrate that LGD-Net achieves state-of-the-art performance, significantly outperforming baseline methods while enabling efficient inference using single-modality H&E inputs.", "AI": {"tldr": "LGD-Net predicts HER2 expression from H&E slides using cross-modal feature hallucination instead of pixel-level virtual staining, achieving SOTA performance with efficient inference.", "motivation": "Standard IHC staining for HER2 evaluation is resource-intensive, expensive, and often unavailable in many areas. Predicting HER2 directly from H&E slides offers a potential alternative, but existing pixel-level virtual staining methods are computationally expensive and prone to reconstruction artifacts.", "method": "Proposes Latent-Guided Dual-Stream Network (LGD-Net) that uses cross-modal feature hallucination instead of explicit pixel-level image generation. It learns to map H&E features to molecular latent space guided by a teacher IHC encoder, with auxiliary regularization tasks for nuclei distribution and membrane staining intensity.", "result": "Extensive experiments on public BCI dataset show LGD-Net achieves state-of-the-art performance, significantly outperforming baseline methods while enabling efficient inference using single-modality H&E inputs.", "conclusion": "LGD-Net provides an effective alternative to pixel-level virtual staining for HER2 prediction from H&E slides, addressing computational and artifact limitations while maintaining clinical relevance through domain knowledge regularization."}}
{"id": "2602.17705", "categories": ["eess.SP", "cs.IR", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17705", "abs": "https://arxiv.org/abs/2602.17705", "authors": ["Zijian Zhang", "Linglong Dai"], "title": "Wavenumber-domain signal processing for holographic MIMO: Foundations, methods, and future directions", "comment": "Accepted by IEEE Communications Standards Magazine. 6 pages, 5 figures", "summary": "Holographic multiple-input multiple-output (H-MIMO) systems represent a paradigm shift in wireless communications by enabling quasi-continuous apertures. Unlike conventional MIMO systems, H-MIMO with subwavelength antenna spacing operates in both far-field and near-field regimes, where classical discrete Fourier transform (DFT) representations fail to sufficiently capture the channel characteristics. To address this challenge, this article provides an overview of the emerging wavenumber-domain signal processing framework. Specifically, by leveraging spatial Fourier plane-wave decomposition to model H-MIMO channels, the wavenumber domain offers a unified and physically consistent basis for characterizing subwavelength-level spatial correlation and spherical wave propagation. This article first introduces the concept of H-MIMO and the wavenumber representation of H-MIMO channels. Next, it elaborates on wavenumber-domain signal processing technologies reported in the literature, including multiplexing, channel estimation, and waveform designs. Finally, it highlights open challenges and outlines future research directions in wavenumber-domain signal processing for next-generation wireless systems.", "AI": {"tldr": "This paper introduces wavenumber-domain signal processing as a framework for holographic MIMO systems, addressing limitations of conventional DFT representations in capturing near-field and subwavelength channel characteristics.", "motivation": "Conventional MIMO systems with DFT representations fail to capture channel characteristics in holographic MIMO systems with subwavelength antenna spacing operating in both far-field and near-field regimes.", "method": "The paper proposes using spatial Fourier plane-wave decomposition to model H-MIMO channels, creating a wavenumber-domain framework that provides unified and physically consistent basis for characterizing spatial correlation and spherical wave propagation.", "result": "The wavenumber-domain approach enables effective signal processing technologies for H-MIMO including multiplexing, channel estimation, and waveform designs, overcoming limitations of conventional DFT-based methods.", "conclusion": "Wavenumber-domain signal processing offers a promising framework for next-generation holographic MIMO systems, though open challenges remain that require further research and development."}}
{"id": "2602.18018", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.18018", "abs": "https://arxiv.org/abs/2602.18018", "authors": ["Weifeng Zhu", "Junyuan Gao", "Shuowen Zhang", "Meixia Tao", "Liang Liu"], "title": "Joint Multi-User Tracking and Signal Detection in Reconfigurable Intelligent Surface-Assisted Cell-Free ISAC Systems", "comment": null, "summary": "This paper investigates the cell-free multi-user integrated sensing and communication (ISAC) system, where multiple base stations collaboratively track the users and detect their signals. Moreover, reconfigurable intelligent surfaces (RISs) are deployed to serve as additional reference nodes to overcome the line-of-sight blockage issue of mobile users for accomplishing seamless sensing. Due to the high-speed user mobility, the multi-user tracking and signal detection performance can be significantly deteriorated without elaborated online user kinematic state updating principles. To tackle this challenge, we first manage to establish a probabilistic signal model to comprehensively characterize the interdependencies among user states, transmit signals, and received signals during the tracking procedure. Based on the Bayesian problem formulation, we further propose a novel hybrid variational message passing (HVMP) algorithm to realize computationally efficient joint estimation of user states and transmit signals in an online manner, which integrates VMP and standard MP to derive the posterior probabilities of estimated variables. Furthermore, the Bayesian Cramer-Rao bound is provided to characterize the performance limit of the multi-user tracking problem, which is also utilized to optimize RIS phase profiles for tracking performance enhancement. Numerical results demonstrate that the proposed algorithm can significantly improve both tracking and signal detection performance over the representative Bayesian estimation counterparts.", "AI": {"tldr": "Proposes HVMP algorithm for joint user tracking and signal detection in cell-free ISAC systems with RIS assistance, achieving improved performance over existing Bayesian methods.", "motivation": "High-speed user mobility in cell-free ISAC systems deteriorates multi-user tracking and signal detection performance, requiring sophisticated online state updating mechanisms to maintain reliable sensing and communication.", "method": "Establishes probabilistic signal model to characterize interdependencies among user states, transmit signals, and received signals. Proposes Hybrid Variational Message Passing (HVMP) algorithm integrating VMP and standard MP for efficient joint estimation of user states and transmit signals in online manner. Uses Bayesian Cramer-Rao bound to optimize RIS phase profiles.", "result": "Numerical results show proposed HVMP algorithm significantly improves both tracking and signal detection performance over representative Bayesian estimation counterparts.", "conclusion": "The HVMP algorithm effectively addresses the challenge of joint user tracking and signal detection in cell-free ISAC systems with RIS assistance, providing computationally efficient online estimation that outperforms existing Bayesian methods."}}
{"id": "2602.17908", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17908", "abs": "https://arxiv.org/abs/2602.17908", "authors": ["Mingzhang Zhu", "Alvin Zhu", "Jose Victor S. H. Ramos", "Beom Jun Kim", "Yike Shi", "Yufeng Wu", "Ruochen Hou", "Quanyou Wang", "Eric Song", "Tony Fan", "Yuchen Cui", "Dennis W. Hong"], "title": "WHED: A Wearable Hand Exoskeleton for Natural, High-Quality Demonstration Collection", "comment": "7 pages, 9 figures, submitted to IEEE UR", "summary": "Scalable learning of dexterous manipulation remains bottlenecked by the difficulty of collecting natural, high-fidelity human demonstrations of multi-finger hands due to occlusion, complex hand kinematics, and contact-rich interactions. We present WHED, a wearable hand-exoskeleton system designed for in-the-wild demonstration capture, guided by two principles: wearability-first operation for extended use and a pose-tolerant, free-to-move thumb coupling that preserves natural thumb behaviors while maintaining a consistent mapping to the target robot thumb degrees of freedom. WHED integrates a linkage-driven finger interface with passive fit accommodation, a modified passive hand with robust proprioceptive sensing, and an onboard sensing/power module. We also provide an end-to-end data pipeline that synchronizes joint encoders, AR-based end-effector pose, and wrist-mounted visual observations, and supports post-processing for time alignment and replay. We demonstrate feasibility on representative grasping and manipulation sequences spanning precision pinch and full-hand enclosure grasps, and show qualitative consistency between collected demonstrations and replayed executions.", "AI": {"tldr": "WHED is a wearable hand-exoskeleton system for capturing natural human hand demonstrations for dexterous robot manipulation learning, featuring wearability-first design and pose-tolerant thumb coupling.", "motivation": "Scalable learning of dexterous manipulation is bottlenecked by difficulty in collecting natural, high-fidelity human demonstrations due to occlusion, complex hand kinematics, and contact-rich interactions.", "method": "WHED system with wearability-first operation, pose-tolerant free-to-move thumb coupling, linkage-driven finger interface with passive fit accommodation, modified passive hand with robust proprioceptive sensing, and onboard sensing/power module. Includes end-to-end data pipeline for synchronization of joint encoders, AR-based end-effector pose, and wrist-mounted visual observations.", "result": "Demonstrated feasibility on representative grasping and manipulation sequences spanning precision pinch and full-hand enclosure grasps, showing qualitative consistency between collected demonstrations and replayed executions.", "conclusion": "WHED provides a practical solution for capturing in-the-wild human hand demonstrations for dexterous manipulation learning, addressing key challenges in demonstration collection through wearable design and comprehensive data pipeline."}}
{"id": "2602.17682", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17682", "abs": "https://arxiv.org/abs/2602.17682", "authors": ["Peng Sun", "Xinyi Shang", "Tao Lin", "Zhiqiang Shen"], "title": "Duality Models: An Embarrassingly Simple One-step Generation Paradigm", "comment": "https://github.com/LINs-lab/DuMo", "summary": "Consistency-based generative models like Shortcut and MeanFlow achieve impressive results via a target-aware design for solving the Probability Flow ODE (PF-ODE). Typically, such methods introduce a target time $r$ alongside the current time $t$ to modulate outputs between a local multi-step derivative ($r = t$) and a global few-step integral ($r = 0$). However, the conventional \"one input, one output\" paradigm enforces a partition of the training budget, often allocating a significant portion (e.g., 75% in MeanFlow) solely to the multi-step objective for stability. This separation forces a trade-off: allocating sufficient samples to the multi-step objective leaves the few-step generation undertrained, which harms convergence and limits scalability. To this end, we propose Duality Models (DuMo) via a \"one input, dual output\" paradigm. Using a shared backbone with dual heads, DuMo simultaneously predicts velocity $v_t$ and flow-map $u_t$ from a single input $x_t$. This applies geometric constraints from the multi-step objective to every sample, bounding the few-step estimation without separating training objectives, thereby significantly improving stability and efficiency. On ImageNet 256 $\\times$ 256, a 679M Diffusion Transformer with SD-VAE achieves a state-of-the-art (SOTA) FID of 1.79 in just 2 steps. Code is available at: https://github.com/LINs-lab/DuMo", "AI": {"tldr": "DuMo introduces a \"one input, dual output\" paradigm that simultaneously predicts velocity and flow-map from a single input, eliminating the training budget trade-off between multi-step and few-step objectives in consistency-based generative models.", "motivation": "Current consistency-based models like Shortcut and MeanFlow use a \"one input, one output\" paradigm that forces a partition of training budget between multi-step and few-step objectives. This separation causes undertraining of few-step generation, harming convergence and limiting scalability.", "method": "DuMo uses a shared backbone with dual heads to simultaneously predict velocity (v_t) and flow-map (u_t) from a single input (x_t). This applies geometric constraints from the multi-step objective to every sample, bounding the few-step estimation without separating training objectives.", "result": "On ImageNet 256\u00d7256, a 679M Diffusion Transformer with SD-VAE achieves state-of-the-art FID of 1.79 in just 2 steps, demonstrating significant improvements in stability and efficiency.", "conclusion": "The \"one input, dual output\" paradigm of DuMo effectively addresses the training budget trade-off in consistency-based models, enabling more stable and efficient few-step generation with better convergence and scalability."}}
{"id": "2602.17910", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17910", "abs": "https://arxiv.org/abs/2602.17910", "authors": ["Hanjing Shi", "Dominic DiFranzo"], "title": "Alignment in Time: Peak-Aware Orchestration for Long-Horizon Agentic Systems", "comment": null, "summary": "Traditional AI alignment primarily focuses on individual model outputs; however, autonomous agents in long-horizon workflows require sustained reliability across entire interaction trajectories. We introduce APEMO (Affect-aware Peak-End Modulation for Orchestration), a runtime scheduling layer that optimizes computational allocation under fixed budgets by operationalizing temporal-affective signals. Instead of modifying model weights, APEMO detects trajectory instability through behavioral proxies and targets repairs at critical segments, such as peak moments and endings. Evaluation across multi-agent simulations and LLM-based planner--executor flows demonstrates that APEMO consistently enhances trajectory-level quality and reuse probability over structural orchestrators. Our results reframe alignment as a temporal control problem, offering a resilient engineering pathway for the development of long-horizon agentic systems.", "AI": {"tldr": "APEMO is a runtime scheduling layer that optimizes computational allocation for autonomous agents by using temporal-affective signals to detect trajectory instability and target repairs at critical segments like peaks and endings, enhancing trajectory-level quality without modifying model weights.", "motivation": "Traditional AI alignment focuses on individual model outputs, but autonomous agents in long-horizon workflows need sustained reliability across entire interaction trajectories. There's a need for systems that maintain alignment throughout complex, multi-step workflows rather than just at single output points.", "method": "APEMO (Affect-aware Peak-End Modulation for Orchestration) is a runtime scheduling layer that operationalizes temporal-affective signals. Instead of modifying model weights, it detects trajectory instability through behavioral proxies and targets repairs at critical segments (peak moments and endings) under fixed computational budgets.", "result": "Evaluation across multi-agent simulations and LLM-based planner-executor flows shows that APEMO consistently enhances trajectory-level quality and reuse probability compared to structural orchestrators.", "conclusion": "The paper reframes alignment as a temporal control problem and offers a resilient engineering pathway for developing long-horizon agentic systems by focusing on runtime orchestration rather than static model alignment."}}
{"id": "2602.17986", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17986", "abs": "https://arxiv.org/abs/2602.17986", "authors": ["Zengtian Deng", "Yimeng He", "Yu Shi", "Lixia Wang", "Touseef Ahmad Qureshi", "Xiuzhen Huang", "Debiao Li"], "title": "From Global Radiomics to Parametric Maps: A Unified Workflow Fusing Radiomics and Deep Learning for PDAC Detection", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Radiomics and deep learning both offer powerful tools for quantitative medical imaging, but most existing fusion approaches only leverage global radiomic features and overlook the complementary value of spatially resolved radiomic parametric maps. We propose a unified framework that first selects discriminative radiomic features and then injects them into a radiomics-enhanced nnUNet at both the global and voxel levels for pancreatic ductal adenocarcinoma (PDAC) detection. On the PANORAMA dataset, our method achieved AUC = 0.96 and AP = 0.84 in cross-validation. On an external in-house cohort, it achieved AUC = 0.95 and AP = 0.78, outperforming the baseline nnUNet; it also ranked second in the PANORAMA Grand Challenge. This demonstrates that handcrafted radiomics, when injected at both global and voxel levels, provide complementary signals to deep learning models for PDAC detection. Our code can be found at https://github.com/briandzt/dl-pdac-radiomics-global-n-paramaps", "AI": {"tldr": "A unified framework combining radiomics and deep learning for pancreatic cancer detection, using both global features and spatial parametric maps to enhance nnUNet performance.", "motivation": "Existing fusion approaches for radiomics and deep learning only use global radiomic features, missing the complementary value of spatially resolved radiomic parametric maps for better medical image analysis.", "method": "Proposes a unified framework that first selects discriminative radiomic features, then injects them into a radiomics-enhanced nnUNet at both global and voxel levels for pancreatic ductal adenocarcinoma detection.", "result": "Achieved AUC = 0.96 and AP = 0.84 on PANORAMA dataset in cross-validation, and AUC = 0.95 and AP = 0.78 on external cohort, outperforming baseline nnUNet and ranking second in PANORAMA Grand Challenge.", "conclusion": "Handcrafted radiomics, when injected at both global and voxel levels, provide complementary signals to deep learning models for PDAC detection, demonstrating the value of spatial radiomic features."}}
{"id": "2602.17799", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17799", "abs": "https://arxiv.org/abs/2602.17799", "authors": ["Jose Sosa", "Danila Rukhovich", "Anis Kacem", "Djamila Aouada"], "title": "Enabling Training-Free Text-Based Remote Sensing Segmentation", "comment": null, "summary": "Recent advances in Vision Language Models (VLMs) and Vision Foundation Models (VFMs) have opened new opportunities for zero-shot text-guided segmentation of remote sensing imagery. However, most existing approaches still rely on additional trainable components, limiting their generalisation and practical applicability. In this work, we investigate to what extent text-based remote sensing segmentation can be achieved without additional training, by relying solely on existing foundation models. We propose a simple yet effective approach that integrates contrastive and generative VLMs with the Segment Anything Model (SAM), enabling a fully training-free or lightweight LoRA-tuned pipeline. Our contrastive approach employs CLIP as mask selector for SAM's grid-based proposals, achieving state-of-the-art open-vocabulary semantic segmentation (OVSS) in a completely zero-shot setting. In parallel, our generative approach enables reasoning and referring segmentation by generating click prompts for SAM using GPT-5 in a zero-shot setting and a LoRA-tuned Qwen-VL model, with the latter yielding the best results. Extensive experiments across 19 remote sensing benchmarks, including open-vocabulary, referring, and reasoning-based tasks, demonstrate the strong capabilities of our approach. Code will be released at https://github.com/josesosajs/trainfree-rs-segmentation.", "AI": {"tldr": "A training-free approach for text-guided remote sensing segmentation using foundation models (CLIP, SAM, GPT-4V, Qwen-VL) achieves state-of-the-art zero-shot performance across 19 benchmarks.", "motivation": "Existing VLMs/VFMs for remote sensing segmentation still require additional trainable components, limiting generalization and practical applicability. The authors want to explore how much can be achieved without training by leveraging existing foundation models.", "method": "Two approaches: 1) Contrastive: Uses CLIP as mask selector for SAM's grid proposals for open-vocabulary semantic segmentation. 2) Generative: Uses GPT-4V (zero-shot) or LoRA-tuned Qwen-VL to generate click prompts for SAM for reasoning/referring segmentation.", "result": "Achieves state-of-the-art open-vocabulary semantic segmentation in completely zero-shot setting. The LoRA-tuned Qwen-VL approach yields best results for reasoning/referring segmentation. Extensive experiments across 19 remote sensing benchmarks demonstrate strong capabilities.", "conclusion": "Text-based remote sensing segmentation can be effectively achieved without additional training by integrating existing foundation models (contrastive VLMs + generative VLMs + SAM), enabling practical, training-free pipelines with strong zero-shot performance."}}
{"id": "2602.17745", "categories": ["eess.SP", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17745", "abs": "https://arxiv.org/abs/2602.17745", "authors": ["Tobias Herrmann", "Nikolay Chenkov", "Florian Stark", "Matthias H\u00e4rter", "Martin K\u00f6ppel"], "title": "Driving-Over Detection in the Railway Environment", "comment": null, "summary": "To enable fully automated driving of trains, numerous new technological components must be introduced into the railway system. Tasks that are nowadays carried out by the operating stuff, need to be taken over by automatic systems. Therefore, equipment for automatic train operation and observing the environment is needed. Here, an important task is the detection of collisions, including both (1) collisions with the front of the train as well as (2) collisions with the wheel, corresponding to an driving-over event. Technologies for detecting the driving-over events are barely investigated nowadays. Therefore, detailed driving-over experiments were performed to gather knowledge for fully automated rail operations, using a variety of objects made from steel, wood, stone and bones. Based on the captured test data, three methods were developed to detect driving-over events automatically. The first method is based on convolutional neural networks and the other two methods are classical threshold-based approaches. The neural network based approach provides an mean accuracy of 99.6% while the classical approaches show 85% and 88.6%, respectively.", "AI": {"tldr": "Paper develops methods for detecting train wheel driving-over collisions using neural networks and threshold-based approaches, achieving 99.6% accuracy with CNN.", "motivation": "To enable fully automated train driving, systems need to detect collisions including driving-over events (wheel collisions), which are currently under-investigated.", "method": "Conducted detailed driving-over experiments with various objects (steel, wood, stone, bones), then developed three detection methods: one CNN-based and two classical threshold-based approaches.", "result": "Neural network approach achieved 99.6% mean accuracy, while classical threshold-based methods showed 85% and 88.6% accuracy respectively.", "conclusion": "CNN-based approach significantly outperforms classical methods for detecting driving-over events, providing promising technology for automated train operation systems."}}
{"id": "2602.18086", "categories": ["eess.SP", "cs.ET", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.18086", "abs": "https://arxiv.org/abs/2602.18086", "authors": ["Ana Jekni\u0107", "Ale\u0161 \u0160vigelj", "Toma\u017e Javornik", "Andrej Hrovat"], "title": "Non-Contiguous Wi-Fi Spectrum for ISAC: Impact on Multipath Delay Estimation", "comment": "12 pages, 7 figures (4 figures contain 2 pictures each, so total 11 pictures in form of 7 figures)", "summary": "Leveraging channel state information from multiple Wi-Fi bands can improve delay resolution for ranging and sensing when a wide contiguous spectrum is unavailable. However, frequency gaps shape the delay response, introducing sidelobes and secondary peaks that can obscure closely spaced multipath components. This paper examines multipath delay estimation for Wi-Fi-compliant multiband configurations using channel state information (CSI). For a two-path model with unknown complex gains and delays, the Cram\u00e9r-Rao lower bound (CRLB) for delay separation is derived and analyzed, confirming the benefit of larger frequency aperture, while revealing pronounced, separation-dependent oscillations driven by gap geometry and inter-path coupling. Given the local nature of Cram\u00e9r-Rao lower bound, the delay response is analyzed next. In the single-path case, the combined subband responses determine how delay-domain sidelobe levels are distributed. The dominant peak spacing is set primarily by the separation between subband center frequencies. In the two-path case, increased aperture sharpens the mainlobe but also intensifies sidelobes and leakage, yielding competing peaks and, in some regimes, a dominant peak shifted from the true delay. Finally, a normalized leakage metric is introduced to predict problematic separations and to identify regimes where local Cram\u00e9r-Rao lower bound analysis does not capture practical peak-leakage behavior in delay estimation.", "AI": {"tldr": "This paper analyzes multipath delay estimation using Wi-Fi multiband CSI, examining how frequency gaps affect delay resolution, deriving CRLB for delay separation, and introducing a leakage metric to predict problematic regimes.", "motivation": "Wi-Fi multiband configurations can improve delay resolution for ranging and sensing when wide contiguous spectrum is unavailable, but frequency gaps introduce sidelobes and secondary peaks that obscure closely spaced multipath components, requiring analysis of these effects.", "method": "Derived Cram\u00e9r-Rao lower bound (CRLB) for delay separation in two-path model with unknown complex gains and delays; analyzed delay response for single-path and two-path cases; introduced normalized leakage metric to predict problematic separations and identify regimes where CRLB analysis fails.", "result": "Larger frequency aperture improves delay resolution but causes pronounced separation-dependent oscillations; combined subband responses determine sidelobe distribution; increased aperture sharpens mainlobe but intensifies sidelobes/leakage, sometimes causing shifted dominant peaks; leakage metric successfully predicts problematic regimes.", "conclusion": "Multiband Wi-Fi CSI enables improved delay resolution but frequency gaps create complex sidelobe/leakage effects; CRLB analysis shows benefits of larger aperture but local nature limits practical insights; leakage metric provides better prediction of problematic delay estimation regimes."}}
{"id": "2602.17921", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17921", "abs": "https://arxiv.org/abs/2602.17921", "authors": ["Kei Ikemura", "Yifei Dong", "Florian T. Pokorny"], "title": "Latent Diffeomorphic Co-Design of End-Effectors for Deformable and Fragile Object Manipulation", "comment": null, "summary": "Manipulating deformable and fragile objects remains a fundamental challenge in robotics due to complex contact dynamics and strict requirements on object integrity. Existing approaches typically optimize either end-effector design or control strategies in isolation, limiting achievable performance. In this work, we present the first co-design framework that jointly optimizes end-effector morphology and manipulation control for deformable and fragile object manipulation. We introduce (1) a latent diffeomorphic shape parameterization enabling expressive yet tractable end-effector geometry optimization, (2) a stress-aware bi-level co-design pipeline coupling morphology and control optimization, and (3) a privileged-to-pointcloud policy distillation scheme for zero-shot real-world deployment. We evaluate our approach on challenging food manipulation tasks, including grasping and pushing jelly and scooping fillets. Simulation and real-world experiments demonstrate the effectiveness of the proposed method.", "AI": {"tldr": "First co-design framework jointly optimizing end-effector morphology and control for deformable/fragile object manipulation, with applications to food manipulation tasks.", "motivation": "Manipulating deformable and fragile objects is challenging due to complex contact dynamics and strict integrity requirements. Existing approaches optimize either end-effector design or control strategies in isolation, limiting performance.", "method": "Three key components: (1) latent diffeomorphic shape parameterization for expressive end-effector geometry optimization, (2) stress-aware bi-level co-design pipeline coupling morphology and control optimization, (3) privileged-to-pointcloud policy distillation for zero-shot real-world deployment.", "result": "Evaluated on challenging food manipulation tasks including grasping/pushing jelly and scooping fillets. Simulation and real-world experiments demonstrate effectiveness of the proposed method.", "conclusion": "The co-design framework successfully addresses deformable and fragile object manipulation by jointly optimizing end-effector morphology and control, enabling better performance than isolated optimization approaches."}}
{"id": "2602.17683", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17683", "abs": "https://arxiv.org/abs/2602.17683", "authors": ["Irene Iele", "Giulia Romoli", "Daniele Molino", "Elena Mulero Ayll\u00f3n", "Filippo Ruffini", "Paolo Soda", "Matteo Tortora"], "title": "Probabilistic NDVI Forecasting from Sparse Satellite Time Series and Weather Covariates", "comment": null, "summary": "Accurate short-term forecasting of vegetation dynamics is a key enabler for data-driven decision support in precision agriculture. Normalized Difference Vegetation Index (NDVI) forecasting from satellite observations, however, remains challenging due to sparse and irregular sampling caused by cloud coverage, as well as the heterogeneous climatic conditions under which crops evolve. In this work, we propose a probabilistic forecasting framework specifically designed for field-level NDVI prediction under clear-sky acquisition constraints. The method leverages a transformer-based architecture that explicitly separates the modeling of historical vegetation dynamics from future exogenous information, integrating historical NDVI observations with both historical and future meteorological covariates. To address irregular revisit patterns and horizon-dependent uncertainty, we introduce a temporal-distance weighted quantile loss that aligns the training objective with the effective forecasting horizon. In addition, we incorporate cumulative and extreme-weather feature engineering to better capture delayed meteorological effects relevant to vegetation response. Extensive experiments on European satellite data demonstrate that the proposed approach consistently outperforms a diverse set of statistical, deep learning, and recent time series baselines across both point-wise and probabilistic evaluation metrics. Ablation studies further highlight the central role of target history, while showing that meteorological covariates provide complementary gains when jointly exploited. The code is available at https://github.com/arco-group/ndvi-forecasting.", "AI": {"tldr": "Transformer-based probabilistic framework for field-level NDVI forecasting that separates historical vegetation dynamics from future exogenous information, addressing irregular satellite sampling and horizon-dependent uncertainty.", "motivation": "Accurate short-term vegetation forecasting is crucial for precision agriculture, but NDVI prediction from satellite data is challenging due to sparse/irregular sampling from cloud coverage and heterogeneous climatic conditions affecting crop evolution.", "method": "Transformer architecture that explicitly separates modeling of historical vegetation dynamics from future exogenous information, integrating historical NDVI with historical/future meteorological covariates. Uses temporal-distance weighted quantile loss for irregular revisit patterns and horizon-dependent uncertainty, plus cumulative and extreme-weather feature engineering.", "result": "Extensive experiments on European satellite data show the approach consistently outperforms diverse statistical, deep learning, and recent time series baselines across both point-wise and probabilistic evaluation metrics.", "conclusion": "The proposed probabilistic forecasting framework effectively addresses NDVI prediction challenges, with ablation studies highlighting the central role of target history and complementary gains from meteorological covariates when jointly exploited."}}
{"id": "2602.17990", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17990", "abs": "https://arxiv.org/abs/2602.17990", "authors": ["Madhav Kanda", "Pedro Las-Casas", "Alok Gautam Kumbhare", "Rodrigo Fonseca", "Sharad Agarwal"], "title": "WorkflowPerturb: Calibrated Stress Tests for Evaluating Multi-Agent Workflow Metrics", "comment": null, "summary": "LLM-based systems increasingly generate structured workflows for complex tasks. In practice, automatic evaluation of these workflows is difficult, because metric scores are often not calibrated, and score changes do not directly communicate the severity of workflow degradation. We introduce WorkflowPerturb, a controlled benchmark for studying workflow evaluation metrics. It works by applying realistic, controlled perturbations to golden workflows. WorkflowPerturb contains 4,973 golden workflows and 44,757 perturbed variants across three perturbation types (Missing Steps, Compressed Steps, and Description Changes), each applied at severity levels of 10%, 30%, and 50%. We benchmark multiple metric families and analyze their sensitivity and calibration using expected score trajectories and residuals. Our results characterize systematic differences across metric families and support severity-aware interpretation of workflow evaluation scores. Our dataset will be released upon acceptance.", "AI": {"tldr": "WorkflowPerturb is a benchmark for evaluating workflow evaluation metrics by applying controlled perturbations to golden workflows across three types and severity levels.", "motivation": "Automatic evaluation of LLM-generated structured workflows is difficult because metric scores are often not calibrated, and score changes don't communicate the severity of workflow degradation.", "method": "Create WorkflowPerturb benchmark with 4,973 golden workflows and 44,757 perturbed variants across three perturbation types (Missing Steps, Compressed Steps, Description Changes) at severity levels of 10%, 30%, and 50%. Benchmark multiple metric families and analyze sensitivity/calibration using expected score trajectories and residuals.", "result": "Results characterize systematic differences across metric families and support severity-aware interpretation of workflow evaluation scores.", "conclusion": "WorkflowPerturb provides a controlled benchmark for studying workflow evaluation metrics, enabling better understanding of metric behavior and severity-aware score interpretation."}}
{"id": "2602.18119", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18119", "abs": "https://arxiv.org/abs/2602.18119", "authors": ["Chris Tomy", "Mo Vali", "David Pertzborn", "Tammam Alamatouri", "Anna M\u00fchlig", "Orlando Guntinas-Lichius", "Anna Xylander", "Eric Michele Fantuzzi", "Matteo Negro", "Francesco Crisafi", "Pietro Lio", "Tiago Azevedo"], "title": "RamanSeg: Interpretability-driven Deep Learning on Raman Spectra for Cancer Diagnosis", "comment": "12 pages, 8 figures", "summary": "Histopathology, the current gold standard for cancer diagnosis, involves the manual examination of tissue samples after chemical staining, a time-consuming process requiring expert analysis. Raman spectroscopy is an alternative, stain-free method of extracting information from samples. Using nnU-Net, we trained a segmentation model on a novel dataset of spatial Raman spectra aligned with tumour annotations, achieving a mean foreground Dice score of 80.9%, surpassing previous work. Furthermore, we propose a novel, interpretable, prototype-based architecture called RamanSeg. RamanSeg classifies pixels based on discovered regions of the training set, generating a segmentation mask. Two variants of RamanSeg allow a trade-off between interpretability and performance: one with prototype projection and another projection-free version. The projection-free RamanSeg outperformed a U-Net baseline with a mean foreground Dice score of 67.3%, offering a meaningful improvement over a black-box training approach.", "AI": {"tldr": "The paper presents RamanSeg, an interpretable prototype-based architecture for tumor segmentation using Raman spectroscopy, achieving competitive performance while offering better interpretability than black-box methods.", "motivation": "Current histopathology for cancer diagnosis is time-consuming and requires expert analysis. Raman spectroscopy offers a stain-free alternative, but existing methods lack interpretability. The authors aim to develop an interpretable segmentation model for Raman spectroscopy data.", "method": "Two approaches: 1) nnU-Net trained on novel spatial Raman spectra dataset aligned with tumor annotations, 2) RamanSeg - a novel interpretable prototype-based architecture with two variants (prototype projection and projection-free) that classifies pixels based on discovered regions from training data.", "result": "nnU-Net achieved 80.9% mean foreground Dice score, surpassing previous work. RamanSeg projection-free variant achieved 67.3% mean foreground Dice score, outperforming U-Net baseline while offering interpretability trade-off.", "conclusion": "RamanSeg provides a meaningful improvement over black-box approaches by offering interpretability while maintaining competitive performance for tumor segmentation using Raman spectroscopy, addressing the need for both accuracy and explainability in medical diagnostics."}}
{"id": "2602.17807", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17807", "abs": "https://arxiv.org/abs/2602.17807", "authors": ["Narges Norouzi", "Idil Esen Zulfikar", "Niccol`o Cavagnero", "Tommie Kerssies", "Bastian Leibe", "Gijs Dubbelman", "Daan de Geus"], "title": "VidEoMT: Your ViT is Secretly Also a Video Segmentation Model", "comment": null, "summary": "Existing online video segmentation models typically combine a per-frame segmenter with complex specialized tracking modules. While effective, these modules introduce significant architectural complexity and computational overhead. Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules. Motivated by this observation, we propose the Video Encoder-only Mask Transformer (VidEoMT), a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. To enable temporal modeling in an encoder-only ViT, VidEoMT introduces a lightweight query propagation mechanism that carries information across frames by reusing queries from the previous frame. To balance this with adaptability to new content, it employs a query fusion strategy that combines the propagated queries with a set of temporally-agnostic learned queries. As a result, VidEoMT attains the benefits of a tracker without added complexity, achieving competitive accuracy while being 5x--10x faster, running at up to 160 FPS with a ViT-L backbone. Code: https://www.tue-mps.org/videomt/", "AI": {"tldr": "VidEoMT is a simple encoder-only video segmentation model that eliminates specialized tracking modules by using query propagation and fusion, achieving competitive accuracy with 5-10x speedup (up to 160 FPS).", "motivation": "Existing video segmentation models rely on complex tracking modules that add architectural complexity and computational overhead. Recent research shows plain Vision Transformers with sufficient capacity can perform image segmentation without specialized modules, suggesting similar potential for video segmentation.", "method": "Proposes Video Encoder-only Mask Transformer (VidEoMT) with: 1) Lightweight query propagation mechanism that reuses queries from previous frames for temporal modeling, and 2) Query fusion strategy combining propagated queries with temporally-agnostic learned queries to balance temporal consistency with adaptability to new content.", "result": "VidEoMT achieves competitive accuracy while being 5-10x faster than existing methods, running at up to 160 FPS with a ViT-L backbone, eliminating the need for dedicated tracking modules.", "conclusion": "Encoder-only video segmentation with simple query propagation and fusion can match the benefits of complex tracking modules while significantly reducing computational overhead and architectural complexity."}}
{"id": "2602.17977", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.17977", "abs": "https://arxiv.org/abs/2602.17977", "authors": ["Wenyan Ma", "Lipeng Zhu", "Yanhua Tan", "Beixiong Zheng", "Yujie Zhang", "Yuchen Zhang", "Keke Ying", "Zhen Gao", "He Sun", "Xiaodan Shao", "Zhenyu Xiao", "Dusit Niyato", "Rui Zhang"], "title": "A Survey on Reconfigurable and Movable Antennas for Wireless Communications and Sensing", "comment": "IEEE Communications Surveys & Tutorials, early access", "summary": "Reconfigurable antennas (RAs) and movable antennas (MAs) have been recognized as promising technologies to enhance the performance of wireless communication and sensing systems by introducing additional degrees of freedom (DoFs) in tuning antenna radiation and/or placement. This paradigm shift from conventional non-reconfigurable/movable antennas offers tremendous new opportunities for realizing multi-functional, more adaptive, and efficient next-generation wireless networks. In this paper, we provide a comprehensive survey on the fundamentals, architectures, and applications of these two emerging antenna technologies. First, we provide a chronological overview of the parallel historical development of both RA and MA technologies. Next, we review and classify the state-of-the-art hardware architectures for implementing RAs and MAs, followed by a detailed comparison of their distinct mechanisms, performance metrics, and functionalities. Subsequently, we focus on various applications of RAs and MAs in wireless communication systems, analyzing their respective performance advantages and key design considerations such as mode selection, movement optimization, and channel acquisition. We also explore the significant roles of RAs and MAs in advancing wireless sensing and integrated sensing and communication (ISAC). Furthermore, we present numerical performance comparisons to illustrate the distinct characteristics and complementary advantages of RA and MA systems. Finally, we outline key challenges and identify promising future research directions to inspire further innovations in this burgeoning field.", "AI": {"tldr": "Comprehensive survey on reconfigurable antennas (RAs) and movable antennas (MAs) covering fundamentals, architectures, applications in wireless communication/sensing, performance comparisons, and future research directions.", "motivation": "RAs and MAs offer additional degrees of freedom for tuning antenna radiation/placement, enabling multi-functional, adaptive, and efficient next-generation wireless networks. This paper aims to provide a comprehensive survey to guide research and development in these emerging technologies.", "method": "1) Chronological overview of RA and MA historical development; 2) Review and classification of hardware architectures; 3) Detailed comparison of mechanisms, performance metrics, and functionalities; 4) Analysis of applications in wireless communication systems; 5) Exploration of roles in wireless sensing and ISAC; 6) Numerical performance comparisons; 7) Identification of challenges and future directions.", "result": "The paper provides a systematic framework for understanding RA and MA technologies, highlighting their distinct characteristics, complementary advantages, and performance benefits in various wireless applications. Numerical comparisons illustrate their respective strengths.", "conclusion": "RAs and MAs represent a paradigm shift in antenna technology with tremendous potential for next-generation wireless networks. The survey establishes a foundation for future innovations while identifying key challenges and promising research directions in this burgeoning field."}}
{"id": "2602.18254", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.18254", "abs": "https://arxiv.org/abs/2602.18254", "authors": ["Tongkai Li", "Weifeng Zhu", "Shuowen Zhang", "Jiannong Cao", "Shuguang Cui", "Liang Liu"], "title": "m^3TrackFormer: Transformer-based mmWave Multi-Target Tracking with Lost Target Re-Acquisition Capability", "comment": null, "summary": "This paper considers a millimeter wave (mmWave) integrated sensing and communication (ISAC) system, where a base station (BS) equipped with a large number of antennas but a small number of radio-frequency (RF) chains emits pencillike narrow beams for persistent tracking of multiple moving targets. Under this model, the tracking lost issue arising from the misalignment between the pencil-like beams and the true target positions is inevitable, especially when the trajectories of the targets are complex, and the conventional Kalman filter-based scheme does not work well. To deal with this issue, we propose a Transformer-based mmWave multi-target tracking framework, namely m3TrackFormer, with a novel re-acquisition mechanism, such that even if the echo signals from some targets are too weak to extract sensing information, we are able to re-acquire their locations quickly with small beam sweeping overhead. Specifically, the proposed framework can operate in two modes of normal tracking and target re-acquisition during the tracking procedure, depending on whether the tracking lost occurs. When all targets are hit by the swept beams, the framework works in the Normal Tracking Mode (N-Mode) with a Transformer encoder-based Normal Tracking Network (N-Net) to accurately estimate the positions of these targets and predict the swept beams in the next time block. While the tracking lost happens, the framework will switch to the Re-Acquisition Mode (R-Mode) with a Transformer decoder-based Re-Acquisition Network (RNet) to adjust the beam sweeping strategy for getting back the lost targets and maintaining the tracking of the remaining targets. Thanks to the ability of global trajectory feature extraction, the m3TrackFormer can achieve high beam prediction accuracy and quickly re-acquire the lost targets, compared with other tracking methods.", "AI": {"tldr": "Transformer-based mmWave multi-target tracking framework with re-acquisition mechanism for ISAC systems", "motivation": "Address tracking lost issue in mmWave ISAC systems due to misalignment between pencil-like beams and moving targets, especially when targets have complex trajectories and conventional Kalman filter fails", "method": "Propose m3TrackFormer framework with dual-mode operation: Normal Tracking Mode (N-Mode) with Transformer encoder-based network for accurate position estimation and beam prediction, and Re-Acquisition Mode (R-Mode) with Transformer decoder-based network for adjusting beam sweeping strategy to recover lost targets", "result": "Achieves high beam prediction accuracy and quickly re-acquires lost targets compared to other tracking methods, thanks to global trajectory feature extraction capability", "conclusion": "Transformer-based framework effectively solves tracking lost problem in mmWave multi-target ISAC systems through intelligent dual-mode operation and re-acquisition mechanism"}}
{"id": "2602.17926", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17926", "abs": "https://arxiv.org/abs/2602.17926", "authors": ["Jennifer Wakulicz", "Ki Myung Brian Lee", "Teresa Vidal-Calleja", "Robert Fitch"], "title": "Homotopic information gain for sparse active target tracking", "comment": "12 pages, 12 figures, accepted to Transactions on Robotics", "summary": "The problem of planning sensing trajectories for a mobile robot to collect observations of a target and predict its future trajectory is known as active target tracking. Enabled by probabilistic motion models, one may solve this problem by exploring the belief space of all trajectory predictions given future sensing actions to maximise information gain. However, for multi-modal motion models the notion of information gain is often ill-defined. This paper proposes a planning approach designed around maximising information regarding the target's homotopy class, or high-level motion. We introduce homotopic information gain, a measure of the expected high-level trajectory information given by a measurement. We show that homotopic information gain is a lower bound for metric or low-level information gain, and is as sparsely distributed in the environment as obstacles are. Planning sensing trajectories to maximise homotopic information results in highly accurate trajectory estimates with fewer measurements than a metric information approach, as supported by our empirical evaluation on real and simulated pedestrian data.", "AI": {"tldr": "Planning sensing trajectories for mobile robots to track targets by maximizing homotopic information gain instead of traditional metric information, resulting in better trajectory estimates with fewer measurements.", "motivation": "Traditional active target tracking approaches using probabilistic motion models struggle with multi-modal motion models where information gain is ill-defined. There's a need for better planning methods that can effectively handle high-level motion patterns.", "method": "Proposes a planning approach that maximizes homotopic information gain - a measure of expected high-level trajectory information from measurements. This focuses on identifying the target's homotopy class (high-level motion patterns) rather than low-level metric details.", "result": "Homotopic information gain is shown to be a lower bound for metric information gain and is sparsely distributed in the environment (like obstacles). Planning with this approach yields highly accurate trajectory estimates with fewer measurements than metric information approaches.", "conclusion": "Focusing on homotopic information gain for sensing trajectory planning provides superior performance in active target tracking, especially for multi-modal motion models, achieving better accuracy with reduced measurement requirements."}}
{"id": "2602.17684", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17684", "abs": "https://arxiv.org/abs/2602.17684", "authors": ["Xiao Zhu", "Xinyu Zhou", "Boyu Zhu", "Hanxu Hu", "Mingzhe Du", "Haotian Zhang", "Huiming Wang", "Zhijiang Guo"], "title": "CodeScaler: Scaling Code LLM Training and Test-Time Inference via Execution-Free Reward Models", "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has driven recent progress in code large language models by leveraging execution-based feedback from unit tests, but its scalability is fundamentally constrained by the availability and reliability of high-quality test cases. We propose CodeScaler, an execution-free reward model designed to scale both reinforcement learning training and test-time inference for code generation. CodeScaler is trained on carefully curated preference data derived from verified code problems and incorporates syntax-aware code extraction and validity-preserving reward shaping to ensure stable and robust optimization. Across five coding benchmarks, CodeScaler improves Qwen3-8B-Base by an average of +11.72 points, outperforming binary execution-based RL by +1.82 points, and enables scalable reinforcement learning on synthetic datasets without any test cases. At inference time, CodeScaler serves as an effective test-time scaling method, achieving performance comparable to unit test approaches while providing a 10-fold reduction in latency. Moreover, CodeScaler surpasses existing reward models on RM-Bench not only in the code domain (+3.3 points), but also in general and reasoning domains (+2.7 points on average).", "AI": {"tldr": "CodeScaler is an execution-free reward model for code generation that outperforms test-based RL methods, enables scalable training without test cases, and reduces inference latency 10x.", "motivation": "Current RL from Verifiable Rewards (RLVR) for code LLMs relies on execution-based unit tests, which limits scalability due to test case availability and reliability issues.", "method": "CodeScaler is trained on curated preference data from verified code problems, using syntax-aware code extraction and validity-preserving reward shaping for stable optimization.", "result": "Improves Qwen3-8B-Base by +11.72 points avg across 5 benchmarks, beats binary execution-based RL by +1.82 points, enables test-case-free training, reduces inference latency 10x, and outperforms existing reward models on RM-Bench.", "conclusion": "CodeScaler provides a scalable, execution-free alternative to test-based RL for code generation, offering superior performance, reduced latency, and broader applicability beyond code domains."}}
{"id": "2602.18025", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18025", "abs": "https://arxiv.org/abs/2602.18025", "authors": ["Haruki Abe", "Takayuki Osa", "Yusuke Mukuta", "Tatsuya Harada"], "title": "Cross-Embodiment Offline Reinforcement Learning for Heterogeneous Robot Datasets", "comment": "ICLR 2026", "summary": "Scalable robot policy pre-training has been hindered by the high cost of collecting high-quality demonstrations for each platform. In this study, we address this issue by uniting offline reinforcement learning (offline RL) with cross-embodiment learning. Offline RL leverages both expert and abundant suboptimal data, and cross-embodiment learning aggregates heterogeneous robot trajectories across diverse morphologies to acquire universal control priors. We perform a systematic analysis of this offline RL and cross-embodiment paradigm, providing a principled understanding of its strengths and limitations. To evaluate this offline RL and cross-embodiment paradigm, we construct a suite of locomotion datasets spanning 16 distinct robot platforms. Our experiments confirm that this combined approach excels at pre-training with datasets rich in suboptimal trajectories, outperforming pure behavior cloning. However, as the proportion of suboptimal data and the number of robot types increase, we observe that conflicting gradients across morphologies begin to impede learning. To mitigate this, we introduce an embodiment-based grouping strategy in which robots are clustered by morphological similarity and the model is updated with a group gradient. This simple, static grouping substantially reduces inter-robot conflicts and outperforms existing conflict-resolution methods.", "AI": {"tldr": "Offline RL + cross-embodiment learning enables scalable robot policy pre-training using diverse robot data, but suffers from conflicting gradients across morphologies that can be mitigated with embodiment-based grouping.", "motivation": "High cost of collecting platform-specific demonstrations hinders scalable robot policy pre-training. Need to leverage both expert/suboptimal data and heterogeneous robot trajectories across diverse morphologies.", "method": "Combine offline RL with cross-embodiment learning, construct locomotion datasets across 16 robot platforms, and introduce embodiment-based grouping strategy where robots are clustered by morphological similarity and updated with group gradients.", "result": "Combined approach excels at pre-training with suboptimal trajectory-rich datasets, outperforming behavior cloning. However, increasing suboptimal data and robot types causes conflicting gradients. Embodiment-based grouping reduces inter-robot conflicts and outperforms existing conflict-resolution methods.", "conclusion": "Offline RL + cross-embodiment paradigm is effective for scalable robot pre-training, but requires careful handling of morphological conflicts. Simple static grouping by embodiment similarity provides effective conflict mitigation."}}
{"id": "2602.18400", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18400", "abs": "https://arxiv.org/abs/2602.18400", "authors": ["Junkai Liu", "Nay Aung", "Theodoros N. Arvanitis", "Joao A. C. Lima", "Steffen E. Petersen", "Daniel C. Alexander", "Le Zhang"], "title": "Exploiting Completeness Perception with Diffusion Transformer for Unified 3D MRI Synthesis", "comment": null, "summary": "Missing data problems, such as missing modalities in multi-modal brain MRI and missing slices in cardiac MRI, pose significant challenges in clinical practice. Existing methods rely on external guidance to supply detailed missing state for instructing generative models to synthesize missing MRIs. However, manual indicators are not always available or reliable in real-world scenarios due to the unpredictable nature of clinical environments. Moreover, these explicit masks are not informative enough to provide guidance for improving semantic consistency. In this work, we argue that generative models should infer and recognize missing states in a self-perceptive manner, enabling them to better capture subtle anatomical and pathological variations. Towards this goal, we propose CoPeDiT, a general-purpose latent diffusion model equipped with completeness perception for unified synthesis of 3D MRIs. Specifically, we incorporate dedicated pretext tasks into our tokenizer, CoPeVAE, empowering it to learn completeness-aware discriminative prompts, and design MDiT3D, a specialized diffusion transformer architecture for 3D MRI synthesis, that effectively uses the learned prompts as guidance to enhance semantic consistency in 3D space. Comprehensive evaluations on three large-scale MRI datasets demonstrate that CoPeDiT significantly outperforms state-of-the-art methods, achieving superior robustness, generalizability, and flexibility. The code is available at https://github.com/JK-Liu7/CoPeDiT .", "AI": {"tldr": "CoPeDiT is a latent diffusion model with completeness perception that enables self-perceptive missing state inference for unified 3D MRI synthesis without requiring external guidance.", "motivation": "Existing methods for missing data problems in MRI rely on external guidance (manual indicators/masks) which are often unavailable or unreliable in clinical practice. These explicit masks also lack sufficient semantic information to ensure anatomical consistency.", "method": "Proposes CoPeDiT with two key components: 1) CoPeVAE tokenizer with pretext tasks to learn completeness-aware discriminative prompts, and 2) MDiT3D, a specialized diffusion transformer architecture for 3D MRI synthesis that uses learned prompts as guidance.", "result": "Comprehensive evaluations on three large-scale MRI datasets show CoPeDiT significantly outperforms state-of-the-art methods in robustness, generalizability, and flexibility for missing data synthesis.", "conclusion": "The self-perceptive approach to missing state inference enables better capture of anatomical variations and provides superior performance for unified 3D MRI synthesis in clinical scenarios with unpredictable missing data."}}
{"id": "2602.17814", "categories": ["cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17814", "abs": "https://arxiv.org/abs/2602.17814", "authors": ["Adrian Catalin Lutu", "Eduard Poesina", "Radu Tudor Ionescu"], "title": "VQPP: Video Query Performance Prediction Benchmark", "comment": null, "summary": "Query performance prediction (QPP) is an important and actively studied information retrieval task, having various applications, such as query reformulation, query expansion, and retrieval system selection, among many others. The task has been primarily studied in the context of text and image retrieval, whereas QPP for content-based video retrieval (CBVR) remains largely underexplored. To this end, we propose the first benchmark for video query performance prediction (VQPP), comprising two text-to-video retrieval datasets and two CBVR systems, respectively. VQPP contains a total of 56K text queries and 51K videos, and comes with official training, validation and test splits, fostering direct comparisons and reproducible results. We explore multiple pre-retrieval and post-retrieval performance predictors, creating a representative benchmark for future exploration of QPP in the video domain. Our results show that pre-retrieval predictors obtain competitive performance, enabling applications before performing the retrieval step. We also demonstrate the applicability of VQPP by employing the best performing pre-retrieval predictor as reward model for training a large language model (LLM) on the query reformulation task via direct preference optimization (DPO). We release our benchmark and code at https://github.com/AdrianLutu/VQPP.", "AI": {"tldr": "First benchmark for video query performance prediction (VQPP) with 56K text queries and 51K videos, exploring pre- and post-retrieval predictors for content-based video retrieval.", "motivation": "Query performance prediction (QPP) is well-studied for text and image retrieval but largely underexplored for content-based video retrieval (CBVR), creating a need for standardized benchmarks in this domain.", "method": "Created VQPP benchmark with two text-to-video retrieval datasets and two CBVR systems, exploring multiple pre-retrieval and post-retrieval performance predictors. Used best pre-retrieval predictor as reward model for training LLM on query reformulation via DPO.", "result": "Pre-retrieval predictors achieve competitive performance, enabling applications before retrieval. Demonstrated practical applicability by using best predictor as reward model for LLM training on query reformulation.", "conclusion": "VQPP provides first standardized benchmark for video QPP research, showing pre-retrieval predictors can be effective and enabling new applications like query reformulation via LLMs trained with QPP-based rewards."}}
{"id": "2602.17979", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.17979", "abs": "https://arxiv.org/abs/2602.17979", "authors": ["Geon Choi", "Namyoon Lee"], "title": "Learning While Transmitting: Pilotless Polar Coded Modulation for Short Packet Transmission", "comment": "12 pages", "summary": "Short packets make channel learning expensive. In pilot-aided transmission (PAT), a non-negligible fraction of the packet is consumed by pilots, creating a direct pre-log loss and tightening the reliability margin needed for ultra-reliable low-latency communication. We propose a pilot-free polar-coded framework that replaces explicit pilots with \\emph{coded pilots}. The message is carried by two polar-coded segments: a quadrature phase shift keying (QPSK) segment that is decodable without channel state information (CSI), and a higher-order quadrature amplitude modulation (QAM) segment that provides high spectral efficiency. The receiver employs \\emph{hybrid decoding}: it first jointly infers CSI during successive-cancellation-based decoding of the QPSK segment by exploiting QPSK phase-rotation invariance together with polar frozen-bit constraints; the decoded QPSK symbols then act as \\emph{implicit pilots} for coherent detection and decoding of the QAM segment. The split also makes rate adaptation practical by confining the symmetry/frozen-bit requirements for phase resolution to the QPSK segment, enabling puncturing and shortening without breaking the pilot-free mechanism. For multi-block fading, we optimize the split and code parameters via density evolution with Gaussian approximation (DEGA); for higher-order modulation, we use bit-interleaved coded modulation capacity approximation to obtain equivalent channel parameters. Incorporating channel-estimation error variance into the DEGA-based analysis, simulations over practical multi-block block-fading channels show gains up to $1.5$~dB over PAT in the short-blocklength regime.", "AI": {"tldr": "Proposes pilot-free polar-coded framework using coded pilots instead of explicit pilots to avoid pre-log loss in short packets. Uses hybrid decoding with QPSK segment for CSI inference and QAM segment for spectral efficiency.", "motivation": "In short-packet communications, traditional pilot-aided transmission (PAT) consumes significant portion of packet for pilots, creating pre-log loss and tightening reliability margins for ultra-reliable low-latency communication (URLLC). Need to reduce overhead while maintaining reliability.", "method": "Two-part polar-coded framework: 1) QPSK segment decodable without CSI, using phase-rotation invariance and polar frozen-bit constraints for joint CSI inference during successive-cancellation decoding; 2) QAM segment for high spectral efficiency. Hybrid decoding uses decoded QPSK symbols as implicit pilots for coherent detection of QAM segment. Rate adaptation via split design, with optimization using density evolution with Gaussian approximation (DEGA) for multi-block fading.", "result": "Simulations over practical multi-block block-fading channels show gains up to 1.5 dB over PAT in short-blocklength regime. Framework enables practical rate adaptation while maintaining pilot-free operation.", "conclusion": "Pilot-free polar-coded framework with coded pilots effectively replaces explicit pilots, reducing overhead while maintaining reliability. Hybrid decoding with QPSK segment enables joint CSI inference, and the split design allows practical rate adaptation for URLLC applications."}}
{"id": "2602.18263", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.18263", "abs": "https://arxiv.org/abs/2602.18263", "authors": ["Junyuan Gao", "Shuowen Zhang", "Liang Liu"], "title": "Channel Estimation for Double-BD-RIS-Assisted Multi-User MIMO Communication", "comment": null, "summary": "Deploying multiple beyond diagonal reconfigurable intelligent surfaces (BD-RISs) can potentially improve the communication performance thanks to inter-element connections of each BD-RIS and inter-surface cooperative beamforming gain among BD-RISs. However, a major issue for multi-BD-RISassisted communication lies in the channel estimation overhead - the channel coefficients associated with the off-diagonal elements in each BD-RIS's scattering matrix as well as those associated with the reflection links among BD-RISs have to be estimated. In this paper, we propose an efficient channel estimation framework for double-BD-RIS-assisted multi-user multipleinput multiple-output (MIMO) systems. Specifically, we reveal that high-dimensional cascaded channels are characterized by five low-dimensional matrices by exploiting channel correlation properties. Based on this novel observation, in the ideal noiseless case, we develop a channel estimation scheme to recover these matrices sequentially and characterize the closed-form overhead required for perfect estimation as a function of the numbers of users and each BD-RIS's elements and channel ranks, which is with the same order as that in double-diagonal-RIS-aided communication systems. This exciting result implies the superiority of cooperative BD-RIS-aided communication over the diagonal- RIS counterpart even when channel estimation overhead is considered. We further extend the proposed scheme to practical noisy scenarios and provide extensive numerical simulations to validate its effectiveness.", "AI": {"tldr": "Proposes efficient channel estimation for double-BD-RIS MIMO systems by exploiting channel correlations to reduce overhead, showing BD-RIS outperforms diagonal-RIS even with estimation costs.", "motivation": "Multi-BD-RIS systems offer performance gains through inter-element connections and cooperative beamforming, but suffer from excessive channel estimation overhead due to complex scattering matrices and inter-surface reflection links.", "method": "Reveals high-dimensional cascaded channels can be characterized by five low-dimensional matrices using channel correlation properties. Develops sequential estimation scheme with closed-form overhead analysis for ideal noiseless case, then extends to practical noisy scenarios.", "result": "Achieves channel estimation overhead with same order as double-diagonal-RIS systems, demonstrating BD-RIS superiority even when considering estimation costs. Provides extensive numerical validation.", "conclusion": "Proposed framework enables efficient channel estimation for BD-RIS systems, making cooperative BD-RIS-aided communication practically viable and superior to diagonal-RIS counterparts."}}
{"id": "2602.18014", "categories": ["cs.RO", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.18014", "abs": "https://arxiv.org/abs/2602.18014", "authors": ["Unnati Nigam", "Radhendushka Srivastava", "Faezeh Marzbanrad", "Michael Burke"], "title": "Quasi-Periodic Gaussian Process Predictive Iterative Learning Control", "comment": null, "summary": "Repetitive motion tasks are common in robotics, but performance can degrade over time due to environmental changes and robot wear and tear. Iterative learning control (ILC) improves performance by using information from previous iterations to compensate for expected errors in future iterations. This work incorporates the use of Quasi-Periodic Gaussian Processes (QPGPs) into a predictive ILC framework to model and forecast disturbances and drift across iterations. Using a recent structural equation formulation of QPGPs, the proposed approach enables efficient inference with complexity $\\mathcal{O}(p^3)$ instead of $\\mathcal{O}(i^2p^3)$, where $p$ denotes the number of points within an iteration and $i$ represents the total number of iterations, specially for larger $i$. This formulation also enables parameter estimation without loss of information, making continual GP learning computationally feasible within the control loop. By predicting next-iteration error profiles rather than relying only on past errors, the controller achieves faster convergence and maintains this under time-varying disturbances. We benchmark the method against both standard ILC and conventional Gaussian Process (GP)-based predictive ILC on three tasks, autonomous vehicle trajectory tracking, a three-link robotic manipulator, and a real-world Stretch robot experiment. Across all cases, the proposed approach converges faster and remains robust under injected and natural disturbances while reducing computational cost. This highlights its practicality across a range of repetitive dynamical systems.", "AI": {"tldr": "Quasi-Periodic Gaussian Processes integrated with predictive iterative learning control for faster convergence and robustness in repetitive robotic tasks.", "motivation": "Repetitive motion tasks in robotics suffer from performance degradation due to environmental changes and robot wear. Standard iterative learning control (ILC) uses past errors but may not adapt well to time-varying disturbances.", "method": "Incorporates Quasi-Periodic Gaussian Processes (QPGPs) into predictive ILC framework using structural equation formulation for efficient inference (O(p\u00b3) vs O(i\u00b2p\u00b3)). Enables parameter estimation without information loss and continual GP learning within control loop.", "result": "Faster convergence and maintained performance under time-varying disturbances compared to standard ILC and conventional GP-based predictive ILC. Demonstrated on autonomous vehicle trajectory tracking, three-link manipulator, and real-world Stretch robot experiments.", "conclusion": "QPGP-based predictive ILC is computationally efficient, robust to disturbances, and practical for various repetitive dynamical systems, offering significant improvements over existing methods."}}
{"id": "2602.17685", "categories": ["cs.LG", "cs.RO", "physics.space-ph"], "pdf": "https://arxiv.org/pdf/2602.17685", "abs": "https://arxiv.org/abs/2602.17685", "authors": ["Agni Bandyopadhyay", "Gunther Waxenegger-Wilfing"], "title": "Optimal Multi-Debris Mission Planning in LEO: A Deep Reinforcement Learning Approach with Co-Elliptic Transfers and Refueling", "comment": "Presented at Conference: IFAC Workshop on Control Aspects of Multi-Satellite Systems (CAMSAT) 2025 At: Wuerzburg", "summary": "This paper addresses the challenge of multi target active debris removal (ADR) in Low Earth Orbit (LEO) by introducing a unified coelliptic maneuver framework that combines Hohmann transfers, safety ellipse proximity operations, and explicit refueling logic. We benchmark three distinct planning algorithms Greedy heuristic, Monte Carlo Tree Search (MCTS), and deep reinforcement learning (RL) using Masked Proximal Policy Optimization (PPO) within a realistic orbital simulation environment featuring randomized debris fields, keep out zones, and delta V constraints. Experimental results over 100 test scenarios demonstrate that Masked PPO achieves superior mission efficiency and computational performance, visiting up to twice as many debris as Greedy and significantly outperforming MCTS in runtime. These findings underscore the promise of modern RL methods for scalable, safe, and resource efficient space mission planning, paving the way for future advancements in ADR autonomy.", "AI": {"tldr": "A unified coelliptic maneuver framework for multi-target active debris removal in LEO, comparing Greedy heuristic, MCTS, and Masked PPO reinforcement learning, with RL showing superior performance in debris removal efficiency and computational speed.", "motivation": "Addressing the challenge of multi-target active debris removal in Low Earth Orbit, which requires efficient mission planning to remove multiple debris objects while considering safety constraints, fuel limitations, and operational complexity.", "method": "Developed a unified coelliptic maneuver framework combining Hohmann transfers, safety ellipse proximity operations, and explicit refueling logic. Benchmark three planning algorithms: Greedy heuristic, Monte Carlo Tree Search (MCTS), and deep reinforcement learning using Masked Proximal Policy Optimization (PPO) in realistic orbital simulations with randomized debris fields, keep-out zones, and delta V constraints.", "result": "Masked PPO achieved superior mission efficiency and computational performance, visiting up to twice as many debris as Greedy heuristic and significantly outperforming MCTS in runtime across 100 test scenarios.", "conclusion": "Modern reinforcement learning methods show promise for scalable, safe, and resource-efficient space mission planning, paving the way for future advancements in active debris removal autonomy."}}
{"id": "2602.18095", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18095", "abs": "https://arxiv.org/abs/2602.18095", "authors": ["Hyunseok Oh", "Sam Stern", "Youngki Lee", "Matthai Philipose"], "title": "Neurosymbolic Language Reasoning as Satisfiability Modulo Theory", "comment": null, "summary": "Natural language understanding requires interleaving textual and logical reasoning, yet large language models often fail to perform such reasoning reliably. Existing neurosymbolic systems combine LLMs with solvers but remain limited to fully formalizable tasks such as math or program synthesis, leaving natural documents with only partial logical structure unaddressed. We introduce Logitext, a neurosymbolic language that represents documents as natural language text constraints (NLTCs), making partial logical structure explicit. We develop an algorithm that integrates LLM-based constraint evaluation with satisfiability modulo theory (SMT) solving, enabling joint textual-logical reasoning. Experiments on a new content moderation benchmark, together with LegalBench and Super-Natural Instructions, show that Logitext improves both accuracy and coverage. This work is the first that treats LLM-based reasoning as an SMT theory, extending neurosymbolic methods beyond fully formalizable domains.", "AI": {"tldr": "Logitext introduces a neurosymbolic language that represents documents as natural language text constraints, enabling joint textual-logical reasoning through LLM-based constraint evaluation integrated with SMT solving.", "motivation": "Current neurosymbolic systems combining LLMs with solvers are limited to fully formalizable tasks like math or programming, leaving natural documents with partial logical structure unaddressed. LLMs often fail to reliably perform interleaved textual and logical reasoning needed for natural language understanding.", "method": "Logitext represents documents as natural language text constraints (NLTCs) to make partial logical structure explicit. The method integrates LLM-based constraint evaluation with satisfiability modulo theory (SMT) solving, treating LLM-based reasoning as an SMT theory.", "result": "Experiments on a new content moderation benchmark, LegalBench, and Super-Natural Instructions show that Logitext improves both accuracy and coverage compared to existing approaches.", "conclusion": "This work is the first to treat LLM-based reasoning as an SMT theory, extending neurosymbolic methods beyond fully formalizable domains to handle natural documents with partial logical structure."}}
{"id": "2602.17854", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17854", "abs": "https://arxiv.org/abs/2602.17854", "authors": ["Domonkos Varga"], "title": "On the Evaluation Protocol of Gesture Recognition for UAV-based Rescue Operation based on Deep Learning: A Subject-Independence Perspective", "comment": null, "summary": "This paper presents a methodological analysis of the gesture-recognition approach proposed by Liu and Szir\u00e1nyi, with a particular focus on the validity of their evaluation protocol. We show that the reported near-perfect accuracy metrics result from a frame-level random train-test split that inevitably mixes samples from the same subjects across both sets, causing severe data leakage. By examining the published confusion matrix, learning curves, and dataset construction, we demonstrate that the evaluation does not measure generalization to unseen individuals. Our findings underscore the importance of subject-independent data partitioning in vision-based gesture-recognition research, especially for applications - such as UAV-human interaction - that require reliable recognition of gestures performed by previously unseen people.", "AI": {"tldr": "The paper critiques Liu and Szir\u00e1nyi's gesture-recognition evaluation protocol, revealing that their near-perfect accuracy results from data leakage due to improper train-test splitting that mixes samples from same subjects across both sets.", "motivation": "To analyze the validity of Liu and Szir\u00e1nyi's gesture-recognition evaluation protocol, particularly addressing concerns about data leakage and whether their reported results truly measure generalization to unseen individuals.", "method": "Methodological analysis examining the published confusion matrix, learning curves, and dataset construction to identify flaws in the frame-level random train-test split that causes data leakage.", "result": "Demonstrated that the near-perfect accuracy metrics result from severe data leakage due to mixing samples from same subjects across train and test sets, meaning the evaluation doesn't measure generalization to unseen individuals.", "conclusion": "The analysis underscores the critical importance of subject-independent data partitioning in vision-based gesture-recognition research, especially for real-world applications like UAV-human interaction that require reliable recognition of gestures from previously unseen people."}}
{"id": "2602.18005", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.18005", "abs": "https://arxiv.org/abs/2602.18005", "authors": ["Mengyuan Lu", "Lu Bai", "Xiang Cheng"], "title": "Multi-Modal Sensing Residual-Corrected GNN for mmWave Path Loss Prediction via Synesthesia of Machines", "comment": null, "summary": "To support sixth-generation (6G)-enabled intelligent transportation systems (ITSs), a multi-modal sensing residual-corrected graph neural network (MM-ResGNN) framework is proposed for millimeter-wave (mmWave) path loss prediction in vehicular communications for the first time. The propagation environment is formulated as an environment sensing path loss graph (ESPL-Graph), where nodes represent the transmitter (Tx) and receiver (Rx) entities and edges jointly describe Tx--Rx transmission links and Rx--Rx spatial correlation links. Meanwhile, a geometry-driven physical baseline is introduced to decouple deterministic attenuation trends from stochastic residual variations. A vehicular multi-modal path loss dataset (VMMPL) is constructed, which covers three representative scenarios, including the urban wide lane, urban crossroad, and suburban forking road environments, and achieves precise alignment between RGB images and global semantic information in the physical space, and link-level ray-tracing (RT)-based path loss data in the electromagnetic space. In MM-ResGNN, topology-aware graph representations and fine-grained visual semantics are synergistically integrated through a gated fusion mechanism to estimate the path loss residual relative to the physical baseline. Experimental results demonstrate that MM-ResGNN achieves significant improvements over empirical models and conventional data-driven baselines, with a normalized mean squared error (NMSE) of 0.0098, a mean absolute error (MAE) of 5.7991~dB, and a mean absolute percentage error (MAPE) of 5.0498\\%. Furthermore, MM-ResGNN exhibits robust cross-scenario generalization through a few-shot fine-tuning strategy, enabling accurate path loss prediction in unseen vehicular environments with limited labeled data.", "AI": {"tldr": "Proposed MM-ResGNN framework for mmWave path loss prediction in 6G ITS using multi-modal sensing and graph neural networks with residual correction.", "motivation": "Need accurate path loss prediction for 6G-enabled intelligent transportation systems, especially for millimeter-wave vehicular communications where propagation characteristics are complex and environment-dependent.", "method": "1) Formulate environment as ESPL-Graph with Tx/Rx nodes and transmission/spatial correlation edges; 2) Introduce geometry-driven physical baseline to separate deterministic trends from stochastic residuals; 3) Create VMMPL dataset with aligned RGB images, semantic info, and RT-based path loss data; 4) Develop MM-ResGNN with gated fusion of graph representations and visual semantics to predict residuals.", "result": "MM-ResGNN achieves NMSE of 0.0098, MAE of 5.7991 dB, and MAPE of 5.0498%, significantly outperforming empirical models and conventional data-driven baselines. Shows robust cross-scenario generalization with few-shot fine-tuning.", "conclusion": "MM-ResGNN framework effectively integrates multi-modal sensing with graph neural networks for accurate mmWave path loss prediction in vehicular environments, enabling reliable 6G ITS applications with strong generalization capabilities."}}
{"id": "2602.18297", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.18297", "abs": "https://arxiv.org/abs/2602.18297", "authors": ["Usman Anwar", "Tim Bakker", "Dana Kianfar", "Cristina Pinneri", "Christos Louizos"], "title": "Analyzing and Improving Chain-of-Thought Monitorability Through Information Theory", "comment": "First two authors contributed equally", "summary": "Chain-of-thought (CoT) monitors are LLM-based systems that analyze reasoning traces to detect when outputs may exhibit attributes of interest, such as test-hacking behavior during code generation. In this paper, we use information-theoretic analysis to show that non-zero mutual information between CoT and output is a necessary but not sufficient condition for CoT monitorability. We identify two sources of approximation error that may undermine the performance of CoT monitors in practice: information gap, which measures the extent to which the monitor can extract the information available in CoT, and elicitation error, which measures the extent to which the monitor approximates the optimal monitoring function. We further demonstrate that CoT monitorability can be systematically improved through targeted training objectives. To this end, we propose two complementary approaches: (a) an oracle-based method that directly rewards the monitored model for producing CoTs that maximize monitor accuracy, and (b) a more practical, label-free approach that maximizes conditional mutual information between outputs and CoTs. Across multiple different environments, we show both methods significantly improve monitor accuracy while preventing CoT degeneration even when training against a monitor, thereby mitigating reward hacking when the task reward is imperfectly specified.", "AI": {"tldr": "The paper analyzes the theoretical foundations of Chain-of-Thought (CoT) monitors for LLMs, identifies sources of approximation error, and proposes training methods to improve monitorability while preventing reward hacking.", "motivation": "Chain-of-Thought monitors are used to detect specific attributes in LLM outputs by analyzing reasoning traces, but their theoretical foundations and practical limitations need better understanding to ensure reliable monitoring.", "method": "The paper uses information-theoretic analysis to establish conditions for CoT monitorability, identifies information gap and elicitation error as key limitations, and proposes two training approaches: (1) oracle-based method that rewards models for producing CoTs that maximize monitor accuracy, and (2) label-free approach that maximizes conditional mutual information between outputs and CoTs.", "result": "The analysis shows that non-zero mutual information between CoT and output is necessary but not sufficient for monitorability. Both proposed training methods significantly improve monitor accuracy across multiple environments while preventing CoT degeneration and mitigating reward hacking when task rewards are imperfectly specified.", "conclusion": "CoT monitorability can be systematically improved through targeted training objectives, with both oracle-based and label-free approaches effectively enhancing monitoring performance while maintaining CoT quality and preventing reward optimization issues."}}
{"id": "2602.18071", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18071", "abs": "https://arxiv.org/abs/2602.18071", "authors": ["Boyuan An", "Zhexiong Wang", "Yipeng Wang", "Jiaqi Li", "Sihang Li", "Jing Zhang", "Chen Feng"], "title": "EgoPush: Learning End-to-End Egocentric Multi-Object Rearrangement for Mobile Robots", "comment": "18 pages, 13 figures. Project page: https://ai4ce.github.io/EgoPush/", "summary": "Humans can rearrange objects in cluttered environments using egocentric perception, navigating occlusions without global coordinates. Inspired by this capability, we study long-horizon multi-object non-prehensile rearrangement for mobile robots using a single egocentric camera. We introduce EgoPush, a policy learning framework that enables egocentric, perception-driven rearrangement without relying on explicit global state estimation that often fails in dynamic scenes. EgoPush designs an object-centric latent space to encode relative spatial relations among objects, rather than absolute poses. This design enables a privileged reinforcement-learning (RL) teacher to jointly learn latent states and mobile actions from sparse keypoints, which is then distilled into a purely visual student policy. To reduce the supervision gap between the omniscient teacher and the partially observed student, we restrict the teacher's observations to visually accessible cues. This induces active perception behaviors that are recoverable from the student's viewpoint. To address long-horizon credit assignment, we decompose rearrangement into stage-level subproblems using temporally decayed, stage-local completion rewards. Extensive simulation experiments demonstrate that EgoPush significantly outperforms end-to-end RL baselines in success rate, with ablation studies validating each design choice. We further demonstrate zero-shot sim-to-real transfer on a mobile platform in the real world. Code and videos are available at https://ai4ce.github.io/EgoPush/.", "AI": {"tldr": "EgoPush enables mobile robots to perform long-horizon multi-object rearrangement using only egocentric vision, without global state estimation, through object-centric latent space learning and teacher-student distillation.", "motivation": "Humans can rearrange cluttered objects using egocentric perception without global coordinates. Current robotic approaches often fail in dynamic scenes due to reliance on explicit global state estimation. There's a need for robots to perform long-horizon rearrangement using only egocentric vision like humans do.", "method": "1) Object-centric latent space encodes relative spatial relations among objects instead of absolute poses. 2) Privileged RL teacher learns latent states and mobile actions from sparse keypoints. 3) Teacher knowledge is distilled into purely visual student policy. 4) Teacher observations restricted to visually accessible cues to reduce supervision gap. 5) Long-horizon tasks decomposed using temporally decayed, stage-local completion rewards.", "result": "EgoPush significantly outperforms end-to-end RL baselines in success rate in simulation experiments. Ablation studies validate each design choice. The method demonstrates successful zero-shot sim-to-real transfer on a mobile platform in real-world scenarios.", "conclusion": "EgoPush provides an effective framework for egocentric, perception-driven rearrangement without global state estimation. The object-centric latent representation, teacher-student distillation with restricted teacher observations, and stage decomposition enable successful long-horizon multi-object rearrangement with zero-shot sim-to-real transfer capability."}}
{"id": "2602.17686", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17686", "abs": "https://arxiv.org/abs/2602.17686", "authors": ["Bowen Yu", "Maolin Wang", "Sheng Zhang", "Binhao Wang", "Yi Wen", "Jingtong Gao", "Bowen Liu", "Zimo Zhao", "Wanyu Wang", "Xiangyu Zhao"], "title": "Curriculum Learning for Efficient Chain-of-Thought Distillation via Structure-Aware Masking and GRPO", "comment": "22 pages, 12 figures", "summary": "Distilling Chain-of-Thought (CoT) reasoning from large language models into compact student models presents a fundamental challenge: teacher rationales are often too verbose for smaller models to faithfully reproduce. Existing approaches either compress reasoning into single-step, losing the interpretability that makes CoT valuable. We present a three-stage curriculum learning framework that addresses this capacity mismatch through progressive skill acquisition. First, we establish structural understanding via masked shuffled reconstruction. Second, we apply Group Relative Policy Optimization (GRPO) on masked completion tasks, enabling the model to discover its own balance between accuracy and brevity. Third, we identify persistent failure cases and guide the student to internalize teacher knowledge through targeted rewriting, again optimized with GRPO. Experiments on GSM8K demonstrate that our approach enables Qwen2.5-3B-Base to achieve an 11.29 percent accuracy improvement while reducing output length by 27.4 percent, surpassing both instruction-tuned variants and prior distillation methods.", "AI": {"tldr": "Three-stage curriculum learning framework distills CoT reasoning into compact models by progressive skill acquisition, achieving better accuracy with shorter outputs.", "motivation": "Existing CoT distillation methods either produce verbose outputs that small models can't reproduce or compress reasoning into single-step, losing interpretability. There's a capacity mismatch between teacher rationales and student model capabilities.", "method": "Three-stage curriculum learning: 1) Masked shuffled reconstruction for structural understanding; 2) Group Relative Policy Optimization (GRPO) on masked completion tasks to balance accuracy and brevity; 3) Targeted rewriting of failure cases with GRPO optimization.", "result": "Qwen2.5-3B-Base achieves 11.29% accuracy improvement while reducing output length by 27.4% on GSM8K, surpassing both instruction-tuned variants and prior distillation methods.", "conclusion": "The progressive curriculum learning framework effectively addresses the capacity mismatch in CoT distillation, enabling compact models to produce accurate yet concise reasoning chains while maintaining interpretability."}}
{"id": "2602.18201", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18201", "abs": "https://arxiv.org/abs/2602.18201", "authors": ["Joseph Bingham", "Netanel Arussy", "Dvir Aran"], "title": "SOMtime the World Ain$'$t Fair: Violating Fairness Using Self-Organizing Maps", "comment": "10 pages, 2 figures, preprint", "summary": "Unsupervised representations are widely assumed to be neutral with respect to sensitive attributes when those attributes are withheld from training. We show that this assumption is false. Using SOMtime, a topology-preserving representation method based on high-capacity Self-Organizing Maps, we demonstrate that sensitive attributes such as age and income emerge as dominant latent axes in purely unsupervised embeddings, even when explicitly excluded from the input. On two large-scale real-world datasets (the World Values Survey across five countries and the Census-Income dataset), SOMtime recovers monotonic orderings aligned with withheld sensitive attributes, achieving Spearman correlations of up to 0.85, whereas PCA and UMAP typically remain below 0.23 (with a single exception reaching 0.31), and against t-SNE and autoencoders which achieve at most 0.34. Furthermore, unsupervised segmentation of SOMtime embeddings produces demographically skewed clusters, demonstrating downstream fairness risks without any supervised task. These findings establish that \\textit{fairness through unawareness} fails at the representation level for ordinal sensitive attributes and that fairness auditing must extend to unsupervised components of machine learning pipelines. We have made the code available at~ https://github.com/JosephBingham/SOMtime", "AI": {"tldr": "SOMtime, a topology-preserving unsupervised representation method, reveals that sensitive attributes like age and income emerge as dominant latent axes even when explicitly excluded from training, challenging the assumption that unsupervised representations are neutral.", "motivation": "The paper challenges the widely held assumption that unsupervised representations are neutral with respect to sensitive attributes when those attributes are withheld from training. The motivation is to demonstrate that \"fairness through unawareness\" fails at the representation level for ordinal sensitive attributes.", "method": "The authors use SOMtime, a topology-preserving representation method based on high-capacity Self-Organizing Maps. They evaluate on two large-scale real-world datasets: the World Values Survey across five countries and the Census-Income dataset. They compare SOMtime against PCA, UMAP, t-SNE, and autoencoders.", "result": "SOMtime recovers monotonic orderings aligned with withheld sensitive attributes, achieving Spearman correlations up to 0.85, while other methods typically remain below 0.23-0.34. Unsupervised segmentation of SOMtime embeddings produces demographically skewed clusters, demonstrating downstream fairness risks without any supervised task.", "conclusion": "The findings establish that fairness through unawareness fails at the representation level for ordinal sensitive attributes, and that fairness auditing must extend to unsupervised components of machine learning pipelines. The authors have made their code publicly available."}}
{"id": "2602.17929", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.17929", "abs": "https://arxiv.org/abs/2602.17929", "authors": ["Athanasios Angelakis"], "title": "ZACH-ViT: Regime-Dependent Inductive Bias in Compact Vision Transformers for Medical Imaging", "comment": "15 pages, 12 figures, 7 tables. Code and models available at https://github.com/Bluesman79/ZACH-ViT", "summary": "Vision Transformers rely on positional embeddings and class tokens that encode fixed spatial priors. While effective for natural images, these priors may hinder generalization when spatial layout is weakly informative or inconsistent, a frequent condition in medical imaging and edge-deployed clinical systems. We introduce ZACH-ViT (Zero-token Adaptive Compact Hierarchical Vision Transformer), a compact Vision Transformer that removes both positional embeddings and the [CLS] token, achieving permutation invariance through global average pooling over patch representations. The term \"Zero-token\" specifically refers to removing the dedicated [CLS] aggregation token and positional embeddings; patch tokens remain unchanged and are processed normally. Adaptive residual projections preserve training stability in compact configurations while maintaining a strict parameter budget.\n  Evaluation is performed across seven MedMNIST datasets spanning binary and multi-class tasks under a strict few-shot protocol (50 samples per class, fixed hyperparameters, five random seeds). The empirical analysis demonstrates regime-dependent behavior: ZACH-ViT (0.25M parameters, trained from scratch) achieves its strongest advantage on BloodMNIST and remains competitive with TransMIL on PathMNIST, while its relative advantage decreases on datasets with strong anatomical priors (OCTMNIST, OrganAMNIST), consistent with the architectural hypothesis. These findings support the view that aligning architectural inductive bias with data structure can be more important than pursuing universal benchmark dominance. Despite its minimal size and lack of pretraining, ZACH-ViT achieves competitive performance while maintaining sub-second inference times, supporting deployment in resource-constrained clinical environments. Code and models are available at https://github.com/Bluesman79/ZACH-ViT.", "AI": {"tldr": "ZACH-ViT removes positional embeddings and CLS token from Vision Transformers for medical imaging, achieving permutation invariance through global average pooling while maintaining competitive performance with minimal parameters.", "motivation": "Standard Vision Transformers rely on fixed spatial priors (positional embeddings and class tokens) that may hinder generalization in medical imaging where spatial layout is often weakly informative or inconsistent, especially in resource-constrained clinical environments.", "method": "ZACH-ViT removes both positional embeddings and the [CLS] token, achieving permutation invariance through global average pooling over patch representations. Uses adaptive residual projections to preserve training stability in compact configurations while maintaining strict parameter budget.", "result": "Shows regime-dependent behavior: strongest advantage on BloodMNIST, competitive with TransMIL on PathMNIST, but relative advantage decreases on datasets with strong anatomical priors (OCTMNIST, OrganAMNIST). Achieves competitive performance with only 0.25M parameters and sub-second inference times without pretraining.", "conclusion": "Aligning architectural inductive bias with data structure is more important than universal benchmark dominance. ZACH-ViT's minimal size and lack of pretraining support deployment in resource-constrained clinical environments while maintaining competitive performance."}}
{"id": "2602.17869", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17869", "abs": "https://arxiv.org/abs/2602.17869", "authors": ["Yuxiao Chen", "Jue Wang", "Zhikang Zhang", "Jingru Yi", "Xu Zhang", "Yang Zou", "Zhaowei Cai", "Jianbo Yuan", "Xinyu Li", "Hao Yang", "Davide Modolo"], "title": "Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models", "comment": null, "summary": "With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.", "AI": {"tldr": "A novel end-to-end framework for long-form video understanding that combines adaptive video sampling and spatiotemporal compression with multimodal LLMs to handle video redundancy and memory constraints.", "motivation": "Long-form video analysis is becoming feasible with modern video backbones and LLMs, but faces challenges from video redundancy: 1) incorporating many frames within memory limits, and 2) extracting discriminative information from massive input data.", "method": "Proposes an end-to-end system with: 1) Information-density-based Adaptive Video Sampler (AVS) to capture essential information from varying video durations, and 2) Autoencoder-based Spatiotemporal Video Compressor (SVC) integrated with a Multimodal Large Language Model (MLLM) for high compression while preserving discriminative features.", "result": "The framework shows promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks, demonstrating versatility and efficacy in handling prolonged video sequences.", "conclusion": "The proposed approach effectively addresses the challenges of long-form video understanding by adaptively capturing essential information and achieving high compression rates while maintaining discriminative capabilities, making it suitable for complex, extended video analysis."}}
{"id": "2602.18097", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18097", "abs": "https://arxiv.org/abs/2602.18097", "authors": ["Aarati Andrea Noronha", "Jean Oh"], "title": "Interacting safely with cyclists using Hamilton-Jacobi reachability and reinforcement learning", "comment": "7 pages. This manuscript was completed in 2020 as part of the first author's graduate thesis at Carnegie Mellon University", "summary": "In this paper, we present a framework for enabling autonomous vehicles to interact with cyclists in a manner that balances safety and optimality. The approach integrates Hamilton-Jacobi reachability analysis with deep Q-learning to jointly address safety guarantees and time-efficient navigation. A value function is computed as the solution to a time-dependent Hamilton-Jacobi-Bellman inequality, providing a quantitative measure of safety for each system state. This safety metric is incorporated as a structured reward signal within a reinforcement learning framework. The method further models the cyclist's latent response to the vehicle, allowing disturbance inputs to reflect human comfort and behavioral adaptation. The proposed framework is evaluated through simulation and comparison with human driving behavior and an existing state-of-the-art method.", "AI": {"tldr": "A framework combining Hamilton-Jacobi reachability analysis with deep Q-learning for autonomous vehicle-cyclist interaction, balancing safety guarantees and time-efficient navigation.", "motivation": "To enable autonomous vehicles to safely and efficiently interact with cyclists by addressing both safety guarantees and optimal navigation, while accounting for human cyclist behavior and comfort.", "method": "Integrates Hamilton-Jacobi reachability analysis with deep Q-learning. Computes a safety value function from Hamilton-Jacobi-Bellman inequality, uses it as structured reward in RL, and models cyclist's latent responses as disturbance inputs.", "result": "Evaluated through simulation, comparing with human driving behavior and existing state-of-the-art methods.", "conclusion": "The framework successfully balances safety and optimality in autonomous vehicle-cyclist interactions by combining formal safety guarantees with learning-based optimization."}}
{"id": "2602.17688", "categories": ["cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.17688", "abs": "https://arxiv.org/abs/2602.17688", "authors": ["Anton Xue", "Litu Rout", "Constantine Caramanis", "Sanjay Shakkottai"], "title": "AnCoder: Anchored Code Generation via Discrete Diffusion Models", "comment": null, "summary": "Diffusion language models offer a compelling alternative to autoregressive code generation, enabling global planning and iterative refinement of complex program logic. However, existing approaches fail to respect the rigid structure of programming languages and, as a result, often produce broken programs that fail to execute. To address this, we introduce AnchorTree, a framework that explicitly anchors the diffusion process using structured, hierarchical priors native to code. Specifically, AnchorTree uses the abstract syntax tree to prioritize resolving syntactically and semantically salient tokens, such as keywords (e.g., if, while) and identifiers (e.g., variable names), thereby establishing a structural scaffold that guides the remaining generation. We validate this framework via AnCoder, a family of models showing that structurally anchored diffusion offers a parameter-efficient path to high-quality code generation.", "AI": {"tldr": "AnchorTree uses abstract syntax trees to guide diffusion models for better code generation by prioritizing syntactically important tokens first.", "motivation": "Existing diffusion language models for code generation often produce broken programs that fail to execute because they don't respect the rigid structure of programming languages.", "method": "AnchorTree framework anchors the diffusion process using structured, hierarchical priors from abstract syntax trees, prioritizing resolution of syntactically and semantically salient tokens (keywords, identifiers) to establish a structural scaffold.", "result": "AnCoder models demonstrate that structurally anchored diffusion offers a parameter-efficient path to high-quality code generation.", "conclusion": "Explicitly anchoring diffusion with code's native hierarchical structure enables better program generation by establishing proper syntactic scaffolding before filling in details."}}
{"id": "2602.18291", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18291", "abs": "https://arxiv.org/abs/2602.18291", "authors": ["Zhuoran Li", "Hai Zhong", "Xun Wang", "Qingxin Xia", "Lihua Zhang", "Longbo Huang"], "title": "Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies", "comment": null, "summary": "Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \\underline{O}nline off-policy \\underline{MA}RL framework using \\underline{D}iffusion policies (\\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\\times$ to $5\\times$ improvement in sample efficiency.", "AI": {"tldr": "OMAD is a new online MARL framework using diffusion policies that achieves state-of-the-art performance with 2.5-5x sample efficiency improvements.", "motivation": "Diffusion models have shown remarkable expressiveness in image generation and offline settings, but their potential in online MARL remains under-explored due to intractable likelihoods impeding entropy-based exploration and coordination.", "method": "Proposes OMAD framework with: 1) relaxed policy objective maximizing scaled joint entropy for exploration without tractable likelihood, 2) joint distributional value function within CTDE paradigm to optimize decentralized diffusion policies using tractable entropy-augmented targets.", "result": "Extensive evaluations on MPE and MAMuJoCo establish OMAD as new state-of-the-art across 10 diverse tasks, demonstrating 2.5x to 5x improvement in sample efficiency.", "conclusion": "OMAD successfully overcomes diffusion model limitations in online MARL through innovative entropy-based exploration and coordination mechanisms, achieving superior performance and sample efficiency."}}
{"id": "2602.18428", "categories": ["cs.LG", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.18428", "abs": "https://arxiv.org/abs/2602.18428", "authors": ["Mojtaba Sahraee-Ardakan", "Mauricio Delbracio", "Peyman Milanfar"], "title": "The Geometry of Noise: Why Diffusion Models Don't Need Noise Conditioning", "comment": null, "summary": "Autonomous (noise-agnostic) generative models, such as Equilibrium Matching and blind diffusion, challenge the standard paradigm by learning a single, time-invariant vector field that operates without explicit noise-level conditioning. While recent work suggests that high-dimensional concentration allows these models to implicitly estimate noise levels from corrupted observations, a fundamental paradox remains: what is the underlying landscape being optimized when the noise level is treated as a random variable, and how can a bounded, noise-agnostic network remain stable near the data manifold where gradients typically diverge? We resolve this paradox by formalizing Marginal Energy, $E_{\\text{marg}}(\\mathbf{u}) = -\\log p(\\mathbf{u})$, where $p(\\mathbf{u}) = \\int p(\\mathbf{u}|t)p(t)dt$ is the marginal density of the noisy data integrated over a prior distribution of unknown noise levels. We prove that generation using autonomous models is not merely blind denoising, but a specific form of Riemannian gradient flow on this Marginal Energy. Through a novel relative energy decomposition, we demonstrate that while the raw Marginal Energy landscape possesses a $1/t^p$ singularity normal to the data manifold, the learned time-invariant field implicitly incorporates a local conformal metric that perfectly counteracts the geometric singularity, converting an infinitely deep potential well into a stable attractor. We also establish the structural stability conditions for sampling with autonomous models. We identify a ``Jensen Gap'' in noise-prediction parameterizations that acts as a high-gain amplifier for estimation errors, explaining the catastrophic failure observed in deterministic blind models. Conversely, we prove that velocity-based parameterizations are inherently stable because they satisfy a bounded-gain condition that absorbs posterior uncertainty into a smooth geometric drift.", "AI": {"tldr": "The paper resolves a paradox in autonomous generative models by showing they perform Riemannian gradient flow on Marginal Energy, not blind denoising, with velocity-based parameterizations being inherently stable.", "motivation": "Autonomous generative models challenge the standard paradigm by learning single, time-invariant vector fields without explicit noise-level conditioning. A fundamental paradox exists: how can bounded, noise-agnostic networks remain stable near data manifolds where gradients typically diverge?", "method": "Formalize Marginal Energy as -log p(u) where p(u) is marginal density integrated over unknown noise levels. Prove generation is Riemannian gradient flow on this energy. Use relative energy decomposition to show learned fields incorporate local conformal metrics that counteract geometric singularities.", "result": "Identifies \"Jensen Gap\" in noise-prediction parameterizations causing catastrophic failure, while velocity-based parameterizations satisfy bounded-gain conditions that absorb posterior uncertainty into smooth geometric drift, making them inherently stable.", "conclusion": "Autonomous models perform specific Riemannian gradient flow on Marginal Energy, not blind denoising. Velocity-based parameterizations are structurally stable due to bounded-gain conditions, resolving the paradox of how noise-agnostic networks remain stable near data manifolds."}}
{"id": "2602.17871", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.17871", "abs": "https://arxiv.org/abs/2602.17871", "authors": ["Dhruba Ghosh", "Yuhui Zhang", "Ludwig Schmidt"], "title": "Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.", "AI": {"tldr": "VLMs perform well on visual QA benchmarks but lag in fine-grained image classification. Better vision encoders and pretraining with unfrozen LLM weights are key to improving fine-grained performance.", "motivation": "Vision-language models have shown strong performance on various visual question answering tasks, but recent studies reveal they underperform on traditional fine-grained image classification benchmarks that test detailed visual knowledge. This disconnect between general vision-language capabilities and fine-grained visual understanding needs investigation.", "method": "The researchers tested numerous recent VLMs on fine-grained classification benchmarks and conducted ablation experiments to identify factors affecting performance. They examined the impact of different LLMs, vision encoders, and pretraining strategies (particularly whether language model weights were frozen or unfrozen during pretraining).", "result": "Better LLMs improve all benchmark scores equally, while better vision encoders disproportionately boost fine-grained classification performance. Pretraining stage is crucial for fine-grained performance, especially when language model weights remain unfrozen during pretraining.", "conclusion": "The findings provide insights for enhancing fine-grained visual understanding in VLMs, suggesting that vision encoder quality and pretraining strategies with unfrozen LLM weights are key to improving vision-centric capabilities."}}
{"id": "2602.18076", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.18076", "abs": "https://arxiv.org/abs/2602.18076", "authors": ["Tommaso Bacchielli", "Lorenzo Pucci", "Andrea Giorgetti"], "title": "Extremely Large Antenna Spacing Method for Enhanced Wideband Near-Field Sensing", "comment": "14 pages, 8 figures", "summary": "This paper proposes a monostatic wideband system for integrated sensing and communication (ISAC) at millimeter-wave frequencies, based on multiple-input multiple-output (MIMO) orthogonal frequency-division multiplexing (OFDM). The system operates in a hybrid near-/far-field regime. The transmitter (Tx) operates in the far field (FF) and uses low-complexity beam steering. The receiver (Rx), on the other hand, operates in a pervasive near field (NF), enabled by a very large effective array aperture. To enable a fully digital implementation, we introduce an extremely large antenna spacing (ELAS) design. This design attains the required aperture with only a few widely spaced antenna elements while avoiding grating lobes in the composite Tx-Rx response. We analytically characterize the NF range-angle response of this architecture and study the interplay between NF effects and waveform bandwidth. This leads to the definition of a super-resolution region, where NF propagation at the Rx dominates the achievable range resolution and surpasses the classical, bandwidth-limited resolution. As a case study, we consider an extended target modeled as a collection of scatterers and assess localization performance via maximum-likelihood estimation. Numerical results evaluated in terms of root mean square error (RMSE) and generalized optimal sub-pattern assignment (GOSPA) show that operating in NF conditions with the ELAS-based design yields significant gains compared to a conventional FF baseline at both the Tx and Rx.", "AI": {"tldr": "Proposes ELAS-based MIMO-OFDM ISAC system with hybrid near-/far-field operation, achieving super-resolution beyond bandwidth limits through near-field effects.", "motivation": "To overcome bandwidth-limited resolution in conventional far-field ISAC systems by exploiting near-field propagation effects for enhanced sensing performance.", "method": "Uses ELAS (extremely large antenna spacing) design with few widely spaced antennas to create large aperture for near-field operation at receiver, while transmitter operates in far-field with beam steering. Analytical characterization of near-field range-angle response and study of interplay between near-field effects and waveform bandwidth.", "result": "Defines super-resolution region where near-field propagation dominates range resolution, surpassing classical bandwidth-limited resolution. Numerical results show significant gains in localization performance (RMSE and GOSPA) compared to conventional far-field baseline.", "conclusion": "ELAS-based design enables hybrid near-/far-field operation that exploits near-field effects to achieve super-resolution sensing, offering substantial performance improvements for millimeter-wave ISAC systems."}}
{"id": "2602.18164", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18164", "abs": "https://arxiv.org/abs/2602.18164", "authors": ["Jonas Frey", "Turcan Tuna", "Frank Fu", "Katharine Patterson", "Tianao Xu", "Maurice Fallon", "Cesar Cadena", "Marco Hutter"], "title": "GrandTour: A Legged Robotics Dataset in the Wild for Multi-Modal Perception and State Estimation", "comment": "Jonas Frey and Turcan Tuna contributed equally. Submitted to Sage The International Journal of Robotics Research", "summary": "Accurate state estimation and multi-modal perception are prerequisites for autonomous legged robots in complex, large-scale environments. To date, no large-scale public legged-robot dataset captures the real-world conditions needed to develop and benchmark algorithms for legged-robot state estimation, perception, and navigation. To address this, we introduce the GrandTour dataset, a multi-modal legged-robotics dataset collected across challenging outdoor and indoor environments, featuring an ANYbotics ANYmal-D quadruped equipped with the \\boxi multi-modal sensor payload. GrandTour spans a broad range of environments and operational scenarios across distinct test sites, ranging from alpine scenery and forests to demolished buildings and urban areas, and covers a wide variation in scale, complexity, illumination, and weather conditions. The dataset provides time-synchronized sensor data from spinning LiDARs, multiple RGB cameras with complementary characteristics, proprioceptive sensors, and stereo depth cameras. Moreover, it includes high-precision ground-truth trajectories from satellite-based RTK-GNSS and a Leica Geosystems total station. This dataset supports research in SLAM, high-precision state estimation, and multi-modal learning, enabling rigorous evaluation and development of new approaches to sensor fusion in legged robotic systems. With its extensive scope, GrandTour represents the largest open-access legged-robotics dataset to date. The dataset is available at https://grand-tour.leggedrobotics.com, on HuggingFace (ROS-independent), and in ROS formats, along with tools and demo resources.", "AI": {"tldr": "GrandTour is the largest open-access multi-modal legged-robotics dataset collected across diverse challenging environments using an ANYmal-D quadruped with comprehensive sensor payload and high-precision ground truth.", "motivation": "There is no large-scale public legged-robot dataset capturing real-world conditions needed to develop and benchmark algorithms for state estimation, perception, and navigation in complex environments.", "method": "Collected data using an ANYbotics ANYmal-D quadruped equipped with multi-modal sensor payload across diverse outdoor and indoor environments including alpine scenery, forests, demolished buildings, and urban areas under varying conditions.", "result": "Created GrandTour dataset with time-synchronized sensor data from spinning LiDARs, multiple RGB cameras, proprioceptive sensors, stereo depth cameras, and high-precision ground-truth trajectories from RTK-GNSS and total station.", "conclusion": "GrandTour enables research in SLAM, high-precision state estimation, and multi-modal learning for legged robotic systems, representing the largest open-access legged-robotics dataset available in multiple formats with supporting tools."}}
{"id": "2602.17689", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17689", "abs": "https://arxiv.org/abs/2602.17689", "authors": ["Melika Filvantorkaman", "Mohsen Piri"], "title": "Robust Pre-Training of Medical Vision-and-Language Models with Domain-Invariant Multi-Modal Masked Reconstruction", "comment": "28 pages, 3 figures", "summary": "Medical vision-language models show strong potential for joint reasoning over medical images and clinical text, but their performance often degrades under domain shift caused by variations in imaging devices, acquisition protocols, and reporting styles. Existing multi-modal pre-training methods largely overlook robustness, treating it as a downstream adaptation problem. In this work, we propose Robust Multi-Modal Masked Reconstruction (Robust-MMR), a self-supervised pre-training framework that explicitly incorporates robustness objectives into masked vision-language learning. Robust-MMR integrates asymmetric perturbation-aware masking, domain-consistency regularization, and modality-resilience constraints to encourage domain-invariant representations. We evaluate Robust-MMR on multiple medical vision-language benchmarks, including medical visual question answering (VQA-RAD, SLAKE, VQA-2019), cross-domain image-text classification (MELINDA), and robust image-caption retrieval (ROCO). Robust-MMR achieves 78.9% cross-domain accuracy on VQA-RAD, outperforming the strongest baseline by 3.8 percentage points, and reaches 74.6% and 77.0% accuracy on SLAKE and VQA-2019, respectively. Under perturbed evaluation, Robust-MMR improves VQA-RAD accuracy from 69.1% to 75.6%. For image-text classification, cross-domain MELINDA accuracy increases from 70.3% to 75.2%, while retrieval experiments show a reduction in mean rank degradation from over 16 to 4.1 under perturbation. Qualitative results further demonstrate improved clinical reasoning for disease detection and structural abnormality assessment. These findings show that explicitly modeling robustness during pre-training leads to more reliable and transferable medical vision-language representations for real-world deployment.", "AI": {"tldr": "Robust-MMR is a self-supervised pre-training framework that incorporates robustness objectives into masked vision-language learning to improve medical AI's performance under domain shifts.", "motivation": "Medical vision-language models degrade under domain shift from variations in imaging devices, protocols, and reporting styles. Existing methods treat robustness as a downstream problem rather than addressing it during pre-training.", "method": "Robust-MMR integrates asymmetric perturbation-aware masking, domain-consistency regularization, and modality-resilience constraints to encourage domain-invariant representations through self-supervised masked vision-language learning.", "result": "Achieves 78.9% cross-domain accuracy on VQA-RAD (3.8% improvement), 74.6% on SLAKE, 77.0% on VQA-2019. Improves perturbed VQA-RAD accuracy from 69.1% to 75.6%, MELINDA cross-domain accuracy from 70.3% to 75.2%, and reduces retrieval rank degradation from over 16 to 4.1.", "conclusion": "Explicitly modeling robustness during pre-training leads to more reliable and transferable medical vision-language representations for real-world deployment, addressing domain shift challenges directly rather than as a downstream adaptation problem."}}
{"id": "2602.17909", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17909", "abs": "https://arxiv.org/abs/2602.17909", "authors": ["Amirhosein Javadi", "Chi-Shiang Gau", "Konstantinos D. Polyzos", "Tara Javidi"], "title": "A Single Image and Multimodality Is All You Need for Novel View Synthesis", "comment": null, "summary": "Diffusion-based approaches have recently demonstrated strong performance for single-image novel view synthesis by conditioning generative models on geometry inferred from monocular depth estimation. However, in practice, the quality and consistency of the synthesized views are fundamentally limited by the reliability of the underlying depth estimates, which are often fragile under low texture, adverse weather, and occlusion-heavy real-world conditions. In this work, we show that incorporating sparse multimodal range measurements provides a simple yet effective way to overcome these limitations. We introduce a multimodal depth reconstruction framework that leverages extremely sparse range sensing data, such as automotive radar or LiDAR, to produce dense depth maps that serve as robust geometric conditioning for diffusion-based novel view synthesis. Our approach models depth in an angular domain using a localized Gaussian Process formulation, enabling computationally efficient inference while explicitly quantifying uncertainty in regions with limited observations. The reconstructed depth and uncertainty are used as a drop-in replacement for monocular depth estimators in existing diffusion-based rendering pipelines, without modifying the generative model itself. Experiments on real-world multimodal driving scenes demonstrate that replacing vision-only depth with our sparse range-based reconstruction substantially improves both geometric consistency and visual quality in single-image novel-view video generation. These results highlight the importance of reliable geometric priors for diffusion-based view synthesis and demonstrate the practical benefits of multimodal sensing even at extreme levels of sparsity.", "AI": {"tldr": "Sparse multimodal range measurements (radar/LiDAR) improve diffusion-based novel view synthesis by providing robust geometric priors, overcoming limitations of monocular depth estimation.", "motivation": "Monocular depth estimation is unreliable in challenging conditions (low texture, adverse weather, occlusion), limiting quality and consistency of diffusion-based novel view synthesis. Sparse range measurements can overcome these limitations.", "method": "Multimodal depth reconstruction framework using sparse range sensing (radar/LiDAR) with angular domain modeling via localized Gaussian Process for efficient inference and uncertainty quantification. Reconstructed depth replaces monocular depth in existing diffusion pipelines without modifying generative models.", "result": "Substantial improvement in geometric consistency and visual quality for single-image novel-view video generation on real-world multimodal driving scenes compared to vision-only depth approaches.", "conclusion": "Reliable geometric priors are crucial for diffusion-based view synthesis, and multimodal sensing provides practical benefits even with extremely sparse data, enhancing robustness in challenging real-world conditions."}}
{"id": "2602.18174", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18174", "abs": "https://arxiv.org/abs/2602.18174", "authors": ["Hyoseok Ju", "Bokeon Suh", "Giseop Kim"], "title": "Have We Mastered Scale in Deep Monocular Visual SLAM? The ScaleMaster Dataset and Benchmark", "comment": "8 pages, 9 figures, accepted to ICRA 2026", "summary": "Recent advances in deep monocular visual Simultaneous Localization and Mapping (SLAM) have achieved impressive accuracy and dense reconstruction capabilities, yet their robustness to scale inconsistency in large-scale indoor environments remains largely unexplored. Existing benchmarks are limited to room-scale or structurally simple settings, leaving critical issues of intra-session scale drift and inter-session scale ambiguity insufficiently addressed. To fill this gap, we introduce the ScaleMaster Dataset, the first benchmark explicitly designed to evaluate scale consistency under challenging scenarios such as multi-floor structures, long trajectories, repetitive views, and low-texture regions. We systematically analyze the vulnerability of state-of-the-art deep monocular visual SLAM systems to scale inconsistency, providing both quantitative and qualitative evaluations. Crucially, our analysis extends beyond traditional trajectory metrics to include a direct map-to-map quality assessment using metrics like Chamfer distance against high-fidelity 3D ground truth. Our results reveal that while recent deep monocular visual SLAM systems demonstrate strong performance on existing benchmarks, they suffer from severe scale-related failures in realistic, large-scale indoor environments. By releasing the ScaleMaster dataset and baseline results, we aim to establish a foundation for future research toward developing scale-consistent and reliable visual SLAM systems.", "AI": {"tldr": "ScaleMaster Dataset is the first benchmark for evaluating scale consistency in deep monocular visual SLAM systems, revealing their vulnerability to scale drift in large-scale indoor environments despite strong performance on existing benchmarks.", "motivation": "Existing deep monocular visual SLAM systems show impressive accuracy but their robustness to scale inconsistency in large-scale indoor environments remains unexplored. Current benchmarks are limited to room-scale or simple settings, failing to address critical issues of intra-session scale drift and inter-session scale ambiguity.", "method": "Introduced ScaleMaster Dataset - the first benchmark explicitly designed for scale consistency evaluation under challenging scenarios (multi-floor structures, long trajectories, repetitive views, low-texture regions). Conducted systematic analysis of state-of-the-art deep monocular visual SLAM systems using both quantitative and qualitative evaluations, including direct map-to-map quality assessment with metrics like Chamfer distance against high-fidelity 3D ground truth.", "result": "Recent deep monocular visual SLAM systems demonstrate strong performance on existing benchmarks but suffer from severe scale-related failures in realistic, large-scale indoor environments. The analysis reveals significant vulnerability to scale inconsistency that wasn't apparent in previous evaluations.", "conclusion": "By releasing the ScaleMaster dataset and baseline results, the authors aim to establish a foundation for future research toward developing scale-consistent and reliable visual SLAM systems, addressing a critical gap in current SLAM evaluation methodologies."}}
{"id": "2602.17691", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17691", "abs": "https://arxiv.org/abs/2602.17691", "authors": ["Craig Atkinson"], "title": "Tethered Reasoning: Decoupling Entropy from Hallucination in Quantized LLMs via Manifold Steering", "comment": "16 pages, 6 tables", "summary": "Quantized language models face a fundamental dilemma: low sampling temperatures yield repetitive, mode-collapsed outputs, while high temperatures (T > 2.0) cause trajectory divergence and semantic incoherence. We present HELIX, a geometric framework that decouples output entropy from hallucination by tethering hidden-state trajectories to a pre-computed truthfulness manifold. HELIX computes a Unified Truth Score (UTS) combining token-level semantic entropy with Mahalanobis distance from the manifold. When UTS indicates trajectory divergence, graduated steering vectors redirect activations toward structurally coherent regions while affecting only 0.2-2.5% of tokens.\n  On 4-bit quantized Granite 4.0 H Small (32B/9B active, hybrid Mamba-Transformer): GSM8K maintains 88.84% accuracy at T = 3.0 (2.81pp degradation from T = 0.5); MMLU maintains 72.49% across 14,042 questions (1.24pp degradation). This demonstrates that high-temperature hallucination is primarily trajectory divergence rather than semantic collapse. Notably, steering the sparse Transformer attention layers (~10% of layers) is sufficient to correct drift in the Mamba-2 state-space formulation.\n  Geometric tethering reveals a previously-masked High-Entropy Creative Reservoir. At T > 2.0, steered outputs exhibit 5-20% idea duplication versus 70-80% at conservative settings. Cross-architecture validation (Qwen3-30B-A3B MOE) confirms this phenomenon is architecture-independent, with 46.7% higher unique concept generation. HELIX acts as a syntax tether, enabling exploration of semantic diversity without violating the logical backbone required for valid output. This enables Multi-Temperature Synthesis, generating 200% more unique concepts than single-temperature inference.", "AI": {"tldr": "HELIX is a geometric framework that decouples output entropy from hallucination in quantized language models by tethering hidden-state trajectories to a truthfulness manifold, enabling high-temperature sampling without semantic incoherence.", "motivation": "Quantized language models face a fundamental dilemma: low temperatures cause repetitive outputs while high temperatures cause trajectory divergence and semantic incoherence. There's a need to enable creative, high-entropy outputs without sacrificing truthfulness.", "method": "HELIX uses geometric tethering to decouple output entropy from hallucination. It computes a Unified Truth Score (UTS) combining token-level semantic entropy with Mahalanobis distance from a pre-computed truthfulness manifold. When UTS indicates divergence, graduated steering vectors redirect activations toward structurally coherent regions, affecting only 0.2-2.5% of tokens.", "result": "On 4-bit quantized Granite 4.0 H Small: GSM8K maintains 88.84% accuracy at T=3.0 (only 2.81pp degradation from T=0.5); MMLU maintains 72.49% across 14,042 questions (1.24pp degradation). Steering sparse Transformer attention layers (~10% of layers) corrects drift in Mamba-2 state-space models. High-temperature steered outputs show 5-20% idea duplication vs 70-80% at conservative settings.", "conclusion": "High-temperature hallucination is primarily trajectory divergence rather than semantic collapse. HELIX reveals a High-Entropy Creative Reservoir and enables Multi-Temperature Synthesis, generating 200% more unique concepts than single-temperature inference while maintaining logical backbone."}}
{"id": "2602.18212", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18212", "abs": "https://arxiv.org/abs/2602.18212", "authors": ["Rui Chen", "Domenico Chiaradia", "Daniele Leonardis", "Antonio Frisoli"], "title": "Design and Characterization of a Dual-DOF Soft Shoulder Exosuit with Volume-Optimized Pneumatic Actuator", "comment": null, "summary": "Portable pneumatic systems for 2 degree-of-freedom (DOF) soft shoulder exosuits remain underexplored, and face fundamental trade-offs between torque output and dynamic response that are further compounded by the need for multiple actuators to support complex shoulder movement. This work addresses these constraints through a volume-optimized spindle-shaped angled actuator (SSAA) geometry: by reducing actuator volume by 35.7% (357mL vs. 555mL), the SSAA maintains 94.2% of output torque while achieving 35.2% faster dynamic response compared to uniform cylindrical designs. Building on the SSAA, we develop a curved abduction actuator (CAA) based on the SSAA geometry and a horizontal adduction actuator (HAA) based on the pouch motor principle, integrating both into a dual-DOF textile-based shoulder exosuit (390 g). The exosuit delivers multi-modal assistance spanning shoulder abduction, flexion, and horizontal adduction, depending on the actuation.\n  User studies with 10 healthy participants reveal that the exosuit substantially reduces electromyographic (EMG) activity across both shoulder abduction and flexion tasks. For abduction with HAA only, the exosuit achieved up to 59% muscle activity reduction across seven muscles. For flexion, both the single-actuator configuration (HAA only) and the dual-actuator configuration (HAA,+,CAA) reduced EMG activity by up to 63.7% compared to no assistance. However, the incremental benefit of adding the CAA to existing HAA support was limited in healthy users during flexion, with statistically significant additional reductions observed only in pectoralis major. These experimental findings characterize actuator contributions in healthy users and provide design guidance for multi-DOF exosuit systems.", "AI": {"tldr": "Portable pneumatic shoulder exosuit uses volume-optimized spindle-shaped actuators to improve torque-to-volume ratio and dynamic response, achieving significant muscle activity reduction in user studies.", "motivation": "Portable pneumatic systems for 2-DOF soft shoulder exosuits face trade-offs between torque output and dynamic response, compounded by the need for multiple actuators for complex shoulder movement.", "method": "Developed volume-optimized spindle-shaped angled actuator (SSAA) geometry that reduces volume by 35.7% while maintaining torque. Built curved abduction actuator (CAA) based on SSAA and horizontal adduction actuator (HAA) based on pouch motor principle, integrated into 390g textile-based dual-DOF shoulder exosuit.", "result": "SSAA maintained 94.2% of output torque with 35.2% faster dynamic response. Exosuit reduced EMG activity by up to 59% in abduction tasks and up to 63.7% in flexion tasks. Limited additional benefit from CAA in healthy users during flexion.", "conclusion": "The SSAA geometry effectively addresses torque-volume trade-offs in pneumatic actuators, and the developed exosuit demonstrates substantial muscle assistance, though incremental benefits of multi-actuator configurations in healthy users may be limited."}}
{"id": "2602.17692", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17692", "abs": "https://arxiv.org/abs/2602.17692", "authors": ["Bin Wang", "Fan Wang", "Pingping Wang", "Jinyu Cong", "Yang Yu", "Yilong Yin", "Zhongyi Han", "Benzheng Wei"], "title": "Agentic Unlearning: When LLM Agent Meets Machine Unlearning", "comment": "9 pages, 6 figures, 6 tables", "summary": "In this paper, we introduce \\textbf{agentic unlearning} which removes specified information from both model parameters and persistent memory in agents with closed-loop interaction. Existing unlearning methods target parameters alone, leaving two critical gaps: (i) parameter-memory backflow, where retrieval reactivates parametric remnants or memory artifacts reintroduce sensitive content, and (ii) the absence of a unified strategy that covers both parameter and memory pathways. We present Synchronized Backflow Unlearning (SBU), a framework that unlearns jointly across parameter and memory pathways. The memory pathway performs dependency closure-based unlearning that prunes isolated entities while logically invalidating shared artifacts. The parameter pathway employs stochastic reference alignment to guide model outputs toward a high-entropy prior. These pathways are integrated via a synchronized dual-update protocol, forming a closed-loop mechanism where memory unlearning and parametric suppression reinforce each other to prevent cross-pathway recontamination. Experiments on medical QA benchmarks show that SBU reduces traces of targeted private information across both pathways with limited degradation on retained data.", "AI": {"tldr": "SBU introduces agentic unlearning that removes information from both model parameters and persistent memory, addressing parameter-memory backflow and providing unified parameter-memory unlearning strategy.", "motivation": "Existing unlearning methods only target model parameters, leaving two critical gaps: parameter-memory backflow (retrieval reactivating parametric remnants or memory artifacts reintroducing sensitive content) and lack of unified strategy covering both parameter and memory pathways.", "method": "Synchronized Backflow Unlearning (SBU) framework with two integrated pathways: memory pathway uses dependency closure-based unlearning to prune isolated entities and logically invalidate shared artifacts; parameter pathway uses stochastic reference alignment to guide outputs toward high-entropy prior. Both pathways integrated via synchronized dual-update protocol forming closed-loop mechanism.", "result": "Experiments on medical QA benchmarks show SBU reduces traces of targeted private information across both pathways with limited degradation on retained data.", "conclusion": "SBU provides effective agentic unlearning that addresses both parameter and memory pathways simultaneously, preventing cross-pathway recontamination through synchronized updates."}}
{"id": "2602.17951", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17951", "abs": "https://arxiv.org/abs/2602.17951", "authors": ["Guoheng Sun", "Tingting Du", "Kaixi Feng", "Chenxiang Luo", "Xingguo Ding", "Zheyu Shen", "Ziyao Wang", "Yexiao He", "Ang Li"], "title": "ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, na\u00efve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts. We provide both theoretical justification and empirical analyses showing that a shared projector is sufficient and outperforms prior designs, and further propose a Matryoshka-style sparse activation scheme for the shared projector to balance multiple alignment losses. Our experiments show that, combined with a training-free layer selection strategy, ROCKET requires only about 4% of the compute budget while achieving 98.5% state-of-the-art success rate on LIBERO. We further demonstrate the superior performance of ROCKET across LIBERO-Plus and RoboTwin, as well as multiple VLA models. The code and model weights can be found at https://github.com/CASE-Lab-UMD/ROCKET-VLA.", "AI": {"tldr": "ROCKET introduces a residual-oriented multi-layer representation alignment framework for Vision-Language-Action models that aligns multiple layers between 2D VLA and 3D vision foundation models using a shared projector to reduce gradient conflicts.", "motivation": "Current VLA models are pretrained on 2D data and lack 3D spatial understanding. Existing representation alignment methods only apply supervision at single layers, failing to exploit rich information across depth, while naive multi-layer alignment causes gradient interference.", "method": "ROCKET formulates multi-layer alignment as aligning one residual stream to another using a shared projector with layer-invariant mapping. It includes a Matryoshka-style sparse activation scheme to balance multiple alignment losses and uses a training-free layer selection strategy.", "result": "Achieves 98.5% state-of-the-art success rate on LIBERO using only 4% of compute budget. Shows superior performance across LIBERO-Plus, RoboTwin, and multiple VLA models.", "conclusion": "ROCKET's residual-oriented multi-layer alignment with shared projector effectively addresses gradient conflicts in representation alignment, enabling efficient 3D spatial understanding for VLA models with minimal compute requirements."}}
{"id": "2602.18224", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18224", "abs": "https://arxiv.org/abs/2602.18224", "authors": ["Yuankai Luo", "Woping Chen", "Tong Liang", "Baiqiao Wang", "Zhenguo Li"], "title": "SimVLA: A Simple VLA Baseline for Robotic Manipulation", "comment": null, "summary": "Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic manipulation, leveraging large-scale pre-training to achieve strong performance. The field has rapidly evolved with additional spatial priors and diverse architectural innovations. However, these advancements are often accompanied by varying training recipes and implementation details, which can make it challenging to disentangle the precise source of empirical gains. In this work, we introduce SimVLA, a streamlined baseline designed to establish a transparent reference point for VLA research. By strictly decoupling perception from control, using a standard vision-language backbone and a lightweight action head, and standardizing critical training dynamics, we demonstrate that a minimal design can achieve state-of-the-art performance. Despite having only 0.5B parameters, SimVLA outperforms multi-billion-parameter models on standard simulation benchmarks without robot pretraining. SimVLA also reaches on-par real-robot performance compared to pi0.5. Our results establish SimVLA as a robust, reproducible baseline that enables clear attribution of empirical gains to future architectural innovations. Website: https://frontierrobo.github.io/SimVLA", "AI": {"tldr": "SimVLA is a minimal Vision-Language-Action baseline that achieves state-of-the-art performance with only 0.5B parameters, outperforming larger models without robot pretraining.", "motivation": "The rapid evolution of VLA models has led to complex architectures with varying training recipes, making it difficult to identify the true sources of performance gains. There's a need for a transparent, standardized baseline to enable clear attribution of improvements.", "method": "SimVLA uses a streamlined design that strictly decouples perception from control, employing a standard vision-language backbone and lightweight action head. It standardizes critical training dynamics to create a minimal but effective architecture.", "result": "With only 0.5B parameters, SimVLA outperforms multi-billion-parameter models on standard simulation benchmarks without robot pretraining. It also achieves on-par real-robot performance compared to pi0.5.", "conclusion": "SimVLA establishes a robust, reproducible baseline that enables clear attribution of empirical gains to future architectural innovations in VLA research, demonstrating that minimal designs can achieve state-of-the-art performance."}}
{"id": "2602.17693", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17693", "abs": "https://arxiv.org/abs/2602.17693", "authors": ["Yuchen Luo", "Fangyue Zhu", "Ruining Zhou", "Mingzhe Huang", "Jian Zhu", "Fanyu Fan", "Wei Shao"], "title": "A Case Study of Selected PTQ Baselines for Reasoning LLMs on Ascend NPU", "comment": null, "summary": "Post-Training Quantization (PTQ) is crucial for efficient model deployment, yet its effectiveness on Ascend NPU remains under-explored compared to GPU architectures. This paper presents a case study of representative PTQ baselines applied to reasoning-oriented models such as DeepSeek-R1-Distill-Qwen series (1.5B/7B/14B) and QwQ-32B. We evaluate four distinct algorithms, including AWQ, GPTQ, SmoothQuant, and FlatQuant, to cover the spectrum from weight-only compression to advanced rotation-based methods. Our empirical results reveal significant platform sensitivity. While 4-bit weight-only quantization proves viable for larger models, aggressive 4-bit weight-activation schemes suffer from layer-wise calibration instability on the NPU, leading to logic collapse in long-context reasoning tasks. Conversely, standard 8-bit quantization remains numerically stable. Furthermore, a real-world INT8 deployment demonstrates that although optimized kernels reduce latency, dynamic quantization overheads currently limit end-to-end acceleration. These findings offer a practical reference for the feasibility and limitations of deploying quantized reasoning models on Ascend NPU.", "AI": {"tldr": "This paper evaluates Post-Training Quantization (PTQ) methods on Ascend NPU for reasoning models, finding platform sensitivity issues where 4-bit weight-activation schemes cause instability while 8-bit quantization remains stable.", "motivation": "PTQ is crucial for efficient model deployment, but its effectiveness on Ascend NPU remains under-explored compared to GPU architectures, creating a need for practical evaluation of quantization methods on this specific hardware platform.", "method": "Case study of four PTQ algorithms (AWQ, GPTQ, SmoothQuant, FlatQuant) applied to reasoning-oriented models (DeepSeek-R1-Distill-Qwen series and QwQ-32B), covering weight-only compression to advanced rotation-based methods, with empirical evaluation on Ascend NPU.", "result": "Significant platform sensitivity: 4-bit weight-only quantization works for larger models, but aggressive 4-bit weight-activation schemes suffer from layer-wise calibration instability leading to logic collapse in long-context reasoning; 8-bit quantization remains numerically stable; INT8 deployment shows optimized kernels reduce latency but dynamic quantization overheads limit end-to-end acceleration.", "conclusion": "The findings provide practical reference for feasibility and limitations of deploying quantized reasoning models on Ascend NPU, highlighting platform-specific challenges that differ from GPU architectures."}}
{"id": "2602.18000", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18000", "abs": "https://arxiv.org/abs/2602.18000", "authors": ["Xuting Lan", "Mingliang Zhou", "Xuekai Wei", "Jielu Yan", "Yueting Huang", "Huayan Pu", "Jun Luo", "Weijia Jia"], "title": "Image Quality Assessment: Exploring Quality Awareness via Memory-driven Distortion Patterns Matching", "comment": null, "summary": "Existing full-reference image quality assessment (FR-IQA) methods achieve high-precision evaluation by analysing feature differences between reference and distorted images. However, their performance is constrained by the quality of the reference image, which limits real-world applications where ideal reference sources are unavailable. Notably, the human visual system has the ability to accumulate visual memory, allowing image quality assessment on the basis of long-term memory storage. Inspired by this biological memory mechanism, we propose a memory-driven quality-aware framework (MQAF), which establishes a memory bank for storing distortion patterns and dynamically switches between dual-mode quality assessment strategies to reduce reliance on high-quality reference images. When reference images are available, MQAF obtains reference-guided quality scores by adaptively weighting reference information and comparing the distorted image with stored distortion patterns in the memory bank. When the reference image is absent, the framework relies on distortion patterns in the memory bank to infer image quality, enabling no-reference quality assessment (NR-IQA). The experimental results show that our method outperforms state-of-the-art approaches across multiple datasets while adapting to both no-reference and full-reference tasks.", "AI": {"tldr": "MQAF is a memory-driven quality assessment framework that reduces reliance on high-quality reference images by storing distortion patterns in a memory bank and dynamically switching between full-reference and no-reference modes.", "motivation": "Existing FR-IQA methods are limited by their dependence on high-quality reference images, which are often unavailable in real-world scenarios. Inspired by the human visual system's ability to accumulate visual memory, the authors aim to develop a more flexible quality assessment approach.", "method": "Proposes MQAF with a memory bank storing distortion patterns. Uses dual-mode strategy: when reference is available, adaptively weights reference information and compares with stored patterns; when reference is absent, relies solely on memory bank patterns for no-reference assessment.", "result": "Outperforms state-of-the-art approaches across multiple datasets while adapting to both no-reference and full-reference tasks.", "conclusion": "MQAF successfully reduces reliance on high-quality reference images through memory-driven approach, enabling flexible quality assessment in both reference-available and reference-unavailable scenarios."}}
{"id": "2602.18332", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.18332", "abs": "https://arxiv.org/abs/2602.18332", "authors": ["Li Qiao", "Yueqing Wang", "Hanjun Jiang", "Xinhua Liu", "Yixuan Xing", "Yongpeng Wu", "Zhen Gao"], "title": "MD-AirComp+: Adaptive Quantization for Blind Massive Digital Over-the-Air Computation", "comment": "Accepted for publication in Chinese Journal of Electronics", "summary": "Recent research has shown that unsourced massive access (UMA) is naturally well-suited for over-the-air computation (AirComp), as it does not require knowledge of each individual signal, as demonstrated by the massive digital AirComp (MD-AirComp) scheme proposed in prior work. The MD-AirComp scheme has proven effective in federated edge learning and is highly compatible with current digital wireless networks. However, it depends on channel pre-equalization, which may amplify computation errors in the presence of channel estimation inaccuracies, thus limiting its practical use. In this paper, we propose a blind MD-AirComp+ scheme, which takes advantage of the channel hardening effect in massive multiple-input multiple-output (MIMO) systems. We provide an upper bound on the computation mean square error, analyze the trade-off between computation accuracy and communication overhead, and determine the optimal quantization level. Additionally, we introduce a deep unfolding algorithm to reduce the computational complexity of solving the underdetermined detection problem formulated as a least absolute shrinkage and selection operator optimization problem. Simulation results confirm the effectiveness of the proposed MD-AirComp+ framework, the optimal quantization selection strategy, and the low-complexity detection algorithm.", "AI": {"tldr": "Blind MD-AirComp+ scheme for over-the-air computation that eliminates channel pre-equalization dependency by leveraging massive MIMO channel hardening, with optimized quantization and low-complexity detection.", "motivation": "Existing MD-AirComp scheme for unsourced massive access in AirComp requires channel pre-equalization, which amplifies computation errors when channel estimation is inaccurate, limiting practical deployment.", "method": "Proposed blind MD-AirComp+ leverages massive MIMO channel hardening effect, provides computation error upper bound, analyzes accuracy-overhead trade-off, determines optimal quantization level, and introduces deep unfolding algorithm for low-complexity detection of underdetermined LASSO problem.", "result": "Simulation results confirm effectiveness of MD-AirComp+ framework, validate optimal quantization selection strategy, and demonstrate performance of low-complexity detection algorithm.", "conclusion": "Blind MD-AirComp+ overcomes practical limitations of prior schemes by eliminating channel pre-equalization dependency while maintaining compatibility with digital wireless networks, making it more suitable for real-world federated edge learning applications."}}
{"id": "2602.18258", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18258", "abs": "https://arxiv.org/abs/2602.18258", "authors": ["Gwangtak Bae", "Jaeho Shin", "Seunggu Kang", "Junho Kim", "Ayoung Kim", "Young Min Kim"], "title": "RoEL: Robust Event-based 3D Line Reconstruction", "comment": "IEEE Transactions on Robotics (T-RO)", "summary": "Event cameras in motion tend to detect object boundaries or texture edges, which produce lines of brightness changes, especially in man-made environments. While lines can constitute a robust intermediate representation that is consistently observed, the sparse nature of lines may lead to drastic deterioration with minor estimation errors. Only a few previous works, often accompanied by additional sensors, utilize lines to compensate for the severe domain discrepancies of event sensors along with unpredictable noise characteristics. We propose a method that can stably extract tracks of varying appearances of lines using a clever algorithmic process that observes multiple representations from various time slices of events, compensating for potential adversaries within the event data. We then propose geometric cost functions that can refine the 3D line maps and camera poses, eliminating projective distortions and depth ambiguities. The 3D line maps are highly compact and can be equipped with our proposed cost function, which can be adapted for any observations that can detect and extract line structures or projections of them, including 3D point cloud maps or image observations. We demonstrate that our formulation is powerful enough to exhibit a significant performance boost in event-based mapping and pose refinement across diverse datasets, and can be flexibly applied to multimodal scenarios. Our results confirm that the proposed line-based formulation is a robust and effective approach for the practical deployment of event-based perceptual modules. Project page: https://gwangtak.github.io/roel/", "AI": {"tldr": "A robust line-based method for event camera mapping and pose refinement that extracts stable line tracks from event data and uses geometric cost functions to refine 3D line maps and camera poses, applicable to multimodal scenarios.", "motivation": "Event cameras detect object boundaries and texture edges as lines, but existing methods struggle with sparse line data, domain discrepancies, and unpredictable noise. There's a need for robust line-based representations that can handle event camera challenges while being applicable to multimodal scenarios.", "method": "1) Stable line track extraction using multiple time slices of events to compensate for noise and domain discrepancies. 2) Geometric cost functions to refine 3D line maps and camera poses by eliminating projective distortions and depth ambiguities. 3) Compact 3D line maps that can integrate with various observation types (point clouds, images).", "result": "Demonstrated significant performance improvements in event-based mapping and pose refinement across diverse datasets. The method shows robust performance and can be flexibly applied to multimodal scenarios.", "conclusion": "The line-based formulation provides a robust and effective approach for practical deployment of event-based perceptual modules, offering stable performance despite event camera challenges and enabling multimodal integration."}}
{"id": "2602.17694", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17694", "abs": "https://arxiv.org/abs/2602.17694", "authors": ["Hui Ma", "Shaoyu Dou", "Ya Liu", "Fei Xing", "Li Feng", "Feng Pi"], "title": "AsynDBT: Asynchronous Distributed Bilevel Tuning for efficient In-Context Learning with Large Language Models", "comment": "Accepted in Scientific Reports", "summary": "With the rapid development of large language models (LLMs), an increasing number of applications leverage cloud-based LLM APIs to reduce usage costs. However, since cloud-based models' parameters and gradients are agnostic, users have to manually or use heuristic algorithms to adjust prompts for intervening LLM outputs, which requiring costly optimization procedures. In-context learning (ICL) has recently emerged as a promising paradigm that enables LLMs to adapt to new tasks using examples provided within the input, eliminating the need for parameter updates. Nevertheless, the advancement of ICL is often hindered by the lack of high-quality data, which is often sensitive and different to share. Federated learning (FL) offers a potential solution by enabling collaborative training of distributed LLMs while preserving data privacy. Despite this issues, previous FL approaches that incorporate ICL have struggled with severe straggler problems and challenges associated with heterogeneous non-identically data. To address these problems, we propose an asynchronous distributed bilevel tuning (AsynDBT) algorithm that optimizes both in-context learning samples and prompt fragments based on the feedback from the LLM, thereby enhancing downstream task performance. Benefiting from its distributed architecture, AsynDBT provides privacy protection and adaptability to heterogeneous computing environments. Furthermore, we present a theoretical analysis establishing the convergence guarantees of the proposed algorithm. Extensive experiments conducted on multiple benchmark datasets demonstrate the effectiveness and efficiency of AsynDBT.", "AI": {"tldr": "AsynDBT: An asynchronous distributed bilevel tuning algorithm that optimizes in-context learning samples and prompt fragments for LLMs using federated learning to address privacy, heterogeneity, and straggler issues.", "motivation": "Cloud-based LLM APIs require manual/heuristic prompt tuning which is costly. In-context learning (ICL) needs high-quality data that is often sensitive and hard to share. Federated learning (FL) can help but previous FL+ICL approaches suffer from straggler problems and heterogeneous non-IID data challenges.", "method": "Propose Asynchronous Distributed Bilevel Tuning (AsynDBT) algorithm that optimizes both in-context learning samples and prompt fragments based on LLM feedback. Uses distributed architecture for privacy protection and adaptability to heterogeneous computing environments.", "result": "Theoretical analysis establishes convergence guarantees. Extensive experiments on multiple benchmark datasets demonstrate effectiveness and efficiency of AsynDBT.", "conclusion": "AsynDBT effectively addresses the challenges of privacy, heterogeneity, and straggler problems in federated learning for in-context learning with LLMs, providing a practical solution for optimizing prompts and ICL samples in distributed settings."}}
{"id": "2602.18006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18006", "abs": "https://arxiv.org/abs/2602.18006", "authors": ["Ahsan Baidar Bakht", "Mohamad Alansari", "Muhayy Ud Din", "Muzammal Naseer", "Sajid Javed", "Irfan Hussain", "Jiri Matas", "Arif Mahmood"], "title": "MUOT_3M: A 3 Million Frame Multimodal Underwater Benchmark and the MUTrack Tracking Method", "comment": null, "summary": "Underwater Object Tracking (UOT) is crucial for efficient marine robotics, large scale ecological monitoring, and ocean exploration; however, progress has been hindered by the scarcity of large, multimodal, and diverse datasets. Existing benchmarks remain small and RGB only, limiting robustness under severe color distortion, turbidity, and low visibility conditions. We introduce MUOT_3M, the first pseudo multimodal UOT benchmark comprising 3 million frames from 3,030 videos (27.8h) annotated with 32 tracking attributes, 677 fine grained classes, and synchronized RGB, estimated enhanced RGB, estimated depth, and language modalities validated by a marine biologist. Building upon MUOT_3M, we propose MUTrack, a SAM-based multimodal to unimodal tracker featuring visual geometric alignment, vision language fusion, and four level knowledge distillation that transfers multimodal knowledge into a unimodal student model. Extensive evaluations across five UOT benchmarks demonstrate that MUTrack achieves up to 8.40% higher AUC and 7.80% higher precision than the strongest SOTA baselines while running at 24 FPS. MUOT_3M and MUTrack establish a new foundation for scalable, multimodally trained yet practically deployable underwater tracking.", "AI": {"tldr": "MUOT_3M is the first pseudo-multimodal underwater object tracking benchmark with 3M frames, and MUTrack is a SAM-based multimodal tracker that achieves SOTA performance through knowledge distillation.", "motivation": "Underwater object tracking lacks large, multimodal datasets, limiting robustness in challenging underwater conditions like color distortion and low visibility.", "method": "1) Created MUOT_3M benchmark with 3M frames, 4 modalities (RGB, enhanced RGB, depth, language), and detailed annotations. 2) Developed MUTrack tracker with visual geometric alignment, vision-language fusion, and 4-level knowledge distillation from multimodal teacher to unimodal student.", "result": "MUTrack achieves up to 8.40% higher AUC and 7.80% higher precision than SOTA baselines while running at 24 FPS. MUOT_3M provides comprehensive multimodal data validated by marine biologist.", "conclusion": "MUOT_3M and MUTrack establish a new foundation for scalable, multimodally trained yet practically deployable underwater tracking systems."}}
{"id": "2602.18339", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.18339", "abs": "https://arxiv.org/abs/2602.18339", "authors": ["Mushfiqur Rahman", "Ismail Guvenc", "David Matolak"], "title": "GS-SBL: Bridging Greedy Pursuit and Sparse Bayesian Learning for Efficient 3D Wireless Channel Modeling", "comment": "Submitted to 2026 IEEE International Symposium on Antennas and Propagation & USNC-URSI Radio Science Meeting", "summary": "Robust cognitive radio development requires accurate 3D path loss models. Traditional empirical models often lack environment-awareness, while deep learning approaches are frequently constrained by the scarcity of large-scale training datasets. This work leverages the inherent sparsity of wireless propagation to model scenario-specific channels by identifying a discrete set of virtual signal sources. We propose a novel Greedy Sequential Sparse Bayesian Learning (GS-SBL) framework that bridges the gap between the computational efficiency of Orthogonal Matching Pursuit (OMP) and the robust uncertainty quantification of SBL. Unlike standard top-down SBL, which updates all source hyperparameters simultaneously, our approach employs a ``Micro-SBL'' architecture. We sequentially evaluate candidate source locations in isolation by executing localized, low-iteration SBL loops and selecting the source that minimizes the $L_2$ residual error. Once identified, the source and its corresponding power are added to the support set, and the process repeats on the signal residual to identify subsequent sources. Experimental results on real-world 3D propagation data demonstrate that the GS-SBL framework significantly outperforms OMP in terms of generalization. By utilizing SBL as a sequential source identifier rather than a global optimizer, the proposed method preserves Bayesian high-resolution accuracy while achieving the execution speeds necessary for real-time 3D path loss characterization.", "AI": {"tldr": "GS-SBL framework combines greedy sequential search with sparse Bayesian learning for efficient 3D path loss modeling using virtual signal sources.", "motivation": "Need accurate 3D path loss models for robust cognitive radio development. Traditional empirical models lack environment-awareness, while deep learning approaches suffer from data scarcity.", "method": "Proposes Greedy Sequential Sparse Bayesian Learning (GS-SBL) that bridges OMP's efficiency with SBL's uncertainty quantification. Uses \"Micro-SBL\" architecture to sequentially evaluate candidate source locations via localized SBL loops, selecting sources minimizing L2 residual error.", "result": "GS-SBL significantly outperforms OMP in generalization on real-world 3D propagation data. Preserves Bayesian high-resolution accuracy while achieving execution speeds suitable for real-time 3D path loss characterization.", "conclusion": "GS-SBL framework enables efficient, accurate 3D path loss modeling by leveraging wireless propagation sparsity and combining greedy sequential search with sparse Bayesian learning, suitable for real-time cognitive radio applications."}}
{"id": "2602.18260", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18260", "abs": "https://arxiv.org/abs/2602.18260", "authors": ["Magnus Nor\u00e9n", "Marios-Nektarios Stamatopoulos", "Avijit Banerjee", "George Nikolakopoulos"], "title": "Role-Adaptive Collaborative Formation Planning for Team of Quadruped Robots in Cluttered Environments", "comment": null, "summary": "This paper presents a role-adaptive Leader-Follower-based formation planning and control framework for teams of quadruped robots operating in cluttered environments. Unlike conventional methods with fixed leaders or rigid formation roles, the proposed approach integrates dynamic role assignment and partial goal planning, enabling flexible, collision-free navigation in complex scenarios. Formation stability and inter-robot safety are ensured through a virtual spring-damper system coupled with a novel obstacle avoidance layer that adaptively adjusts each agent's velocity. A dynamic look-ahead reference generator further enhances flexibility, allowing temporary formation deformation to maneuver around obstacles while maintaining goal-directed motion. The Fast Marching Square (FM2) algorithm provides the global path for the leader and local paths for the followers as the planning backbone. The framework is validated through extensive simulations and real-world experiments with teams of quadruped robots. Results demonstrate smooth coordination, adaptive role switching, and robust formation maintenance in complex, unstructured environments. A video featuring the simulation and physical experiments along with their associated visualizations can be found at https://youtu.be/scq37Tua9W4.", "AI": {"tldr": "A flexible formation control framework for quadruped robot teams that dynamically assigns leader/follower roles and adapts formations for collision-free navigation in cluttered environments.", "motivation": "Conventional formation control methods use fixed leaders and rigid roles, which are inadequate for navigating complex, cluttered environments where robots need to dynamically adapt their formation to avoid obstacles while maintaining coordination.", "method": "Integrates dynamic role assignment, partial goal planning, virtual spring-damper system for formation stability, obstacle avoidance layer for adaptive velocity adjustment, dynamic look-ahead reference generator for temporary formation deformation, and Fast Marching Square (FM2) algorithm for global/local path planning.", "result": "Validated through extensive simulations and real-world experiments with quadruped robot teams, demonstrating smooth coordination, adaptive role switching, and robust formation maintenance in complex, unstructured environments.", "conclusion": "The proposed role-adaptive Leader-Follower framework enables flexible, collision-free navigation for multi-robot teams in cluttered environments, overcoming limitations of conventional fixed-role approaches through dynamic adaptation and robust formation control."}}
{"id": "2602.17695", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.17695", "abs": "https://arxiv.org/abs/2602.17695", "authors": ["Xin Yu", "Hanwen Xing", "Lingzhou Xue"], "title": "EXACT: Explicit Attribute-Guided Decoding-Time Personalization", "comment": null, "summary": "Achieving personalized alignment requires adapting large language models to each user's evolving context. While decoding-time personalization offers a scalable alternative to training-time methods, existing methods largely rely on implicit, less interpretable preference representations and impose a rigid, context-agnostic user representation, failing to account for how preferences shift across prompts. We introduce EXACT, a new decoding-time personalization that aligns generation with limited pairwise preference feedback using a predefined set of interpretable attributes. EXACT first identifies user-specific attribute subsets by maximizing the likelihood of preferred responses in the offline stage. Then, for online inference, EXACT retrieves the most semantically relevant attributes for an incoming prompt and injects them into the context to steer generation. We establish theoretical approximation guarantees for the proposed algorithm under mild assumptions, and provably show that our similarity-based retrieval mechanism effectively mitigates contextual preference shifts, adapting to disparate tasks without pooling conflicting preferences. Extensive experiments on human-annotated preference datasets demonstrate that EXACT consistently outperforms strong baselines, including preference modeling accuracy and personalized generation quality.", "AI": {"tldr": "EXACT is a decoding-time personalization method that uses interpretable attributes to align LLM generation with user preferences, addressing contextual preference shifts through similarity-based attribute retrieval.", "motivation": "Existing decoding-time personalization methods rely on implicit, less interpretable preference representations and use rigid, context-agnostic user representations that fail to account for how user preferences shift across different prompts and contexts.", "method": "EXACT uses a two-stage approach: 1) Offline stage identifies user-specific attribute subsets by maximizing likelihood of preferred responses from limited pairwise feedback; 2) Online inference retrieves semantically relevant attributes for incoming prompts and injects them into context to steer generation.", "result": "EXACT demonstrates superior performance over strong baselines on human-annotated preference datasets, showing improvements in both preference modeling accuracy and personalized generation quality. Theoretical guarantees show the method effectively mitigates contextual preference shifts.", "conclusion": "EXACT provides an effective decoding-time personalization framework that uses interpretable attributes to adapt to user preferences while handling contextual preference shifts, offering theoretical guarantees and empirical improvements over existing methods."}}
{"id": "2602.18016", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18016", "abs": "https://arxiv.org/abs/2602.18016", "authors": ["Jiamin Luo", "Xuqian Gu", "Jingjing Wang", "Jiahong Lu"], "title": "Towards LLM-centric Affective Visual Customization via Efficient and Precise Emotion Manipulating", "comment": null, "summary": "Previous studies on visual customization primarily rely on the objective alignment between various control signals (e.g., language, layout and canny) and the edited images, which largely ignore the subjective emotional contents, and more importantly lack general-purpose foundation models for affective visual customization. With this in mind, this paper proposes an LLM-centric Affective Visual Customization (L-AVC) task, which focuses on generating images within modifying their subjective emotions via Multimodal LLM. Further, this paper contends that how to make the model efficiently align emotion conversion in semantics (named inter-emotion semantic conversion) and how to precisely retain emotion-agnostic contents (named exter-emotion semantic retaining) are rather important and challenging in this L-AVC task. To this end, this paper proposes an Efficient and Precise Emotion Manipulating approach for editing subjective emotions in images. Specifically, an Efficient Inter-emotion Converting (EIC) module is tailored to make the LLM efficiently align emotion conversion in semantics before and after editing, followed by a Precise Exter-emotion Retaining (PER) module to precisely retain the emotion-agnostic contents. Comprehensive experimental evaluations on our constructed L-AVC dataset demonstrate the great advantage of the proposed EPEM approach to the L-AVC task over several state-of-the-art baselines. This justifies the importance of emotion information for L-AVC and the effectiveness of EPEM in efficiently and precisely manipulating such information.", "AI": {"tldr": "Proposes L-AVC task for emotion-based image editing using multimodal LLMs, with EPEM approach for efficient emotion conversion and content retention.", "motivation": "Previous visual customization methods focus on objective alignment (language, layout, canny) but ignore subjective emotional content and lack general-purpose foundation models for affective visual customization.", "method": "Proposes EPEM approach with two modules: Efficient Inter-emotion Converting (EIC) module for semantic emotion conversion alignment, and Precise Exter-emotion Retaining (PER) module for preserving emotion-agnostic content.", "result": "Comprehensive experiments on constructed L-AVC dataset show EPEM outperforms state-of-the-art baselines, demonstrating importance of emotion information and effectiveness of the approach.", "conclusion": "The paper justifies the importance of emotion information for L-AVC tasks and shows EPEM effectively manipulates emotional content while preserving other image elements."}}
{"id": "2602.18408", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2602.18408", "abs": "https://arxiv.org/abs/2602.18408", "authors": ["Chenrui Qiu", "Yongxu Zhu", "Bo Tan", "George K. Karagiannidis", "Tasos Dagiuklas"], "title": "Modeling UAV-aided Roadside Cell-Free Networks with Mat\u00e9rn Hard-Core Point Processes", "comment": "Accepted for presentation at IEEE International Conference on Communications 2026", "summary": "This paper investigates a uncrewed aerial vehicles (UAV)-assisted cell-free architecture for vehicular networks in road-constrained environments. Roads are modeled using a Poisson Line Process (PLP), with multi-layer roadside access points (APs) deployed via 1-D Poisson Point Process (PPP). Each user forms a localized cell-free cluster by associating with the nearest AP in each layer along its corresponding road. This forms a road-constrained cell-free architecture. To enhance coverage, UAV act as an aerial tier, extending access from 1-D road-constrained layouts (embedded in 2-D) to 3-D. We employ a Mat\u00e9rn Hard-Core (MHC) point process to model the spatial distribution of UAV base stations, ensuring a minimum safety distance between them. In order to enable tractable analysis of the aggregate signal from multiple APs, a distance-based power control scheme is introduced. Leveraging tools from stochastic geometry, we have studied the coverage probability. Furthermore, we analyze the impact of key system parameters on coverage performance, providing useful insights into the deployment and optimization of UAV-assisted cell-free vehicular networks.", "AI": {"tldr": "UAV-assisted cell-free architecture for vehicular networks using stochastic geometry models with road constraints and aerial tier enhancement.", "motivation": "To address coverage challenges in road-constrained vehicular networks by extending traditional 2D cell-free architectures to 3D using UAVs as aerial access points.", "method": "Models roads with Poisson Line Process, deploys multi-layer APs via 1D PPP, forms road-constrained cell-free clusters, adds UAV aerial tier using Mat\u00e9rn Hard-Core process for safety spacing, implements distance-based power control, and analyzes coverage probability using stochastic geometry.", "result": "Developed tractable analytical framework for coverage probability in UAV-assisted cell-free vehicular networks and analyzed impact of key system parameters on coverage performance.", "conclusion": "The proposed architecture successfully extends vehicular network coverage from 2D road-constrained layouts to 3D, providing useful deployment and optimization insights for UAV-assisted cell-free systems."}}
{"id": "2602.18312", "categories": ["cs.RO", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.18312", "abs": "https://arxiv.org/abs/2602.18312", "authors": ["Zhaoming Xie", "Kevin Karol", "Jessica Hodgins"], "title": "Learning Smooth Time-Varying Linear Policies with an Action Jacobian Penalty", "comment": null, "summary": "Reinforcement learning provides a framework for learning control policies that can reproduce diverse motions for simulated characters. However, such policies often exploit unnatural high-frequency signals that are unachievable by humans or physical robots, making them poor representations of real-world behaviors. Existing work addresses this issue by adding a reward term that penalizes a large change in actions over time. This term often requires substantial tuning efforts. We propose to use the action Jacobian penalty, which penalizes changes in action with respect to the changes in simulated state directly through auto differentiation. This effectively eliminates unrealistic high-frequency control signals without task specific tuning. While effective, the action Jacobian penalty introduces significant computational overhead when used with traditional fully connected neural network architectures. To mitigate this, we introduce a new architecture called a Linear Policy Net (LPN) that significantly reduces the computational burden for calculating the action Jacobian penalty during training. In addition, a LPN requires no parameter tuning, exhibits faster learning convergence compared to baseline methods, and can be more efficiently queried during inference time compared to a fully connected neural network. We demonstrate that a Linear Policy Net, combined with the action Jacobian penalty, is able to learn policies that generate smooth signals while solving a number of motion imitation tasks with different characteristics, including dynamic motions such as a backflip and various challenging parkour skills. Finally, we apply this approach to create policies for dynamic motions on a physical quadrupedal robot equipped with an arm.", "AI": {"tldr": "The paper proposes Linear Policy Net (LPN) with action Jacobian penalty to eliminate unrealistic high-frequency control signals in reinforcement learning policies without task-specific tuning.", "motivation": "RL policies often exploit unnatural high-frequency signals unachievable by humans or physical robots, making them poor representations of real-world behaviors. Existing methods require substantial tuning efforts.", "method": "Proposes action Jacobian penalty that penalizes changes in action with respect to state changes via auto differentiation. Introduces Linear Policy Net (LPN) architecture to reduce computational overhead of calculating action Jacobian penalty.", "result": "LPN requires no parameter tuning, exhibits faster learning convergence, and can be more efficiently queried during inference. Successfully learns policies for motion imitation tasks including backflips and parkour skills, and works on physical quadrupedal robot with arm.", "conclusion": "Linear Policy Net combined with action Jacobian penalty effectively eliminates unrealistic high-frequency control signals without task-specific tuning, enabling smooth, realistic policies for diverse motion tasks on both simulated characters and physical robots."}}
{"id": "2602.17696", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17696", "abs": "https://arxiv.org/abs/2602.17696", "authors": ["Zongmin Li", "Jian Su", "Farah Benamara", "Aixin Sun"], "title": "Can LLM Safety Be Ensured by Constraining Parameter Regions?", "comment": "32 pages", "summary": "Large language models (LLMs) are often assumed to contain ``safety regions'' -- parameter subsets whose modification directly influences safety behaviors. We conduct a systematic evaluation of four safety region identification methods spanning different parameter granularities, from individual weights to entire Transformer layers, across four families of backbone LLMs with varying sizes. Using ten safety identification datasets, we find that the identified safety regions exhibit only low to moderate overlap, as measured by IoU. The overlap drops significantly when the safety regions are further refined using utility datasets (\\ie non-harmful queries). These results suggest that current techniques fail to reliably identify a stable, dataset-agnostic safety region.", "AI": {"tldr": "Current methods for identifying safety regions in LLMs show low overlap and instability across datasets, failing to find reliable safety parameters.", "motivation": "LLMs are assumed to contain \"safety regions\" - parameter subsets that directly influence safety behaviors. The paper aims to systematically evaluate whether current methods can reliably identify these regions.", "method": "Systematic evaluation of four safety region identification methods across different parameter granularities (individual weights to entire Transformer layers), tested on four families of backbone LLMs with varying sizes using ten safety identification datasets.", "result": "Identified safety regions show only low to moderate overlap (measured by IoU), with overlap dropping significantly when refined using utility datasets (non-harmful queries).", "conclusion": "Current techniques fail to reliably identify a stable, dataset-agnostic safety region in LLMs, suggesting safety mechanisms may be more distributed or complex than assumed."}}
{"id": "2602.18019", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18019", "abs": "https://arxiv.org/abs/2602.18019", "authors": ["Yujie Jin", "Wenxin Zhang", "Jingjing Wang", "Guodong Zhou"], "title": "DeepSVU: Towards In-depth Security-oriented Video Understanding via Unified Physical-world Regularized MoE", "comment": null, "summary": "In the literature, prior research on Security-oriented Video Understanding (SVU) has predominantly focused on detecting and localize the threats (e.g., shootings, robberies) in videos, while largely lacking the effective capability to generate and evaluate the threat causes. Motivated by these gaps, this paper introduces a new chat paradigm SVU task, i.e., In-depth Security-oriented Video Understanding (DeepSVU), which aims to not only identify and locate the threats but also attribute and evaluate the causes threatening segments. Furthermore, this paper reveals two key challenges in the proposed task: 1) how to effectively model the coarse-to-fine physical-world information (e.g., human behavior, object interactions and background context) to boost the DeepSVU task; and 2) how to adaptively trade off these factors. To tackle these challenges, this paper proposes a new Unified Physical-world Regularized MoE (UPRM) approach. Specifically, UPRM incorporates two key components: the Unified Physical-world Enhanced MoE (UPE) Block and the Physical-world Trade-off Regularizer (PTR), to address the above two challenges, respectively. Extensive experiments conduct on our DeepSVU instructions datasets (i.e., UCF-C instructions and CUVA instructions) demonstrate that UPRM outperforms several advanced Video-LLMs as well as non-VLM approaches. Such information.These justify the importance of the coarse-to-fine physical-world information in the DeepSVU task and demonstrate the effectiveness of our UPRM in capturing such information.", "AI": {"tldr": "This paper introduces DeepSVU, a new security-oriented video understanding task that goes beyond threat detection to include cause attribution and evaluation, and proposes UPRM (Unified Physical-world Regularized MoE) to address modeling challenges.", "motivation": "Prior security video understanding research focused only on detecting and localizing threats, lacking capability to generate and evaluate threat causes. This gap motivates the need for more comprehensive security video analysis.", "method": "Proposes UPRM (Unified Physical-world Regularized MoE) with two key components: Unified Physical-world Enhanced MoE (UPE) Block to model coarse-to-fine physical-world information, and Physical-world Trade-off Regularizer (PTR) to adaptively balance these factors.", "result": "UPRM outperforms several advanced Video-LLMs and non-VLM approaches on DeepSVU instruction datasets (UCF-C instructions and CUVA instructions), demonstrating effectiveness in capturing physical-world information.", "conclusion": "The work justifies the importance of coarse-to-fine physical-world information in DeepSVU tasks and shows UPRM's effectiveness in capturing such information for comprehensive security video understanding."}}
{"id": "2602.18330", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18330", "abs": "https://arxiv.org/abs/2602.18330", "authors": ["Mohsen Jafarpour", "Ayberk Y\u00fcksek", "Shahab Eshghi", "Stanislav Gorb", "Edoardo Milana"], "title": "Tendon-Driven Reciprocating and Non-Reciprocating Motion via Snapping Metabeams", "comment": "9th IEEE-RAS International Conference on Soft Robotics (RoboSoft 2026)", "summary": "Snapping beams enable rapid geometric transitions through nonlinear instability, offering an efficient means of generating motion in soft robotic systems. In this study, a tendon-driven mechanism consisting of spiral-based metabeams was developed to exploit this principle for producing both reciprocating and non-reciprocating motion. The snapping structures were fabricated using fused deposition modeling with polylactic acid (PLA) and experimentally tested under different boundary conditions to analyze their nonlinear behavior. The results show that the mechanical characteristics, including critical forces and stability, can be tuned solely by adjusting the boundary constraints. The spiral geometry allows large reversible deformation even when made from a relatively stiff material such as PLA, providing a straightforward design concept for controllable snapping behavior. The developed mechanism was further integrated into a swimming robot, where tendon-driven fins exhibited two distinct actuation modes: reciprocating and non-reciprocating motion. The latter enabled efficient propulsion, producing a forward displacement of about 32 mm per 0.4 s cycle ($\\approx$ 81 mm/s, equivalent to 0.4 body lengths per second). This study highlights the potential of geometry-driven snapping structures for efficient and programmable actuation in soft robotic systems.", "AI": {"tldr": "Tendon-driven spiral metabeams use snapping instability for programmable motion in soft robots, enabling both reciprocating and non-reciprocating actuation with efficient swimming propulsion.", "motivation": "To develop efficient soft robotic actuation by exploiting nonlinear snapping instability in geometric structures, creating programmable motion with simple boundary condition control.", "method": "Developed tendon-driven mechanism using spiral-based metabeams fabricated via FDM 3D printing with PLA. Tested under different boundary conditions to analyze nonlinear behavior and integrated into swimming robot with tendon-driven fins.", "result": "Boundary constraints alone can tune mechanical characteristics (critical forces, stability). Spiral geometry enables large reversible deformation with stiff PLA. Swimming robot achieved efficient propulsion: 32 mm per 0.4s cycle (\u224881 mm/s, 0.4 body lengths/s) using non-reciprocating motion.", "conclusion": "Geometry-driven snapping structures offer efficient, programmable actuation for soft robotics, with boundary condition control enabling tunable behavior and practical applications like swimming propulsion."}}
{"id": "2602.17697", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17697", "abs": "https://arxiv.org/abs/2602.17697", "authors": ["Nada Zine", "Cl\u00e9ment Quinton", "Romain Rouvoy"], "title": "Pimp My LLM: Leveraging Variability Modeling to Tune Inference Hyperparameters", "comment": null, "summary": "Large Language Models (LLMs) are being increasingly used across a wide range of tasks. However, their substantial computational demands raise concerns about the energy efficiency and sustainability of both training and inference. Inference, in particular, dominates total compute usage, making its optimization crucial. Recent research has explored optimization techniques and analyzed how configuration choices influence energy consumption. Yet, the vast configuration space of inference servers makes exhaustive empirical evaluation infeasible due to combinatorial explosion. In this paper, we introduce a new perspective on this problem by treating LLMs as configurable systems and applying variability management techniques to systematically analyze inference-time configuration choices. We evaluate our approach on the Hugging Face Transformers library by representing generation hyperparameters and their constraints using a feature-based variability model, sampling representative configurations, measuring their energy consumption, latency, accuracy, and learning predictive models from the collected data. Our results show that variability modeling effectively manages the complexity of LLM inference configurations. It enables systematic analysis of hyperparameters effects and interactions, reveals trade-offs, and supports accurate prediction of inference behavior from a limited number of measurements. Overall, this work opens a new research direction that bridges software engineering and machine learning by leveraging variability modeling for the efficient and sustainable configuration of LLMs.", "AI": {"tldr": "This paper introduces variability management techniques to systematically analyze LLM inference configurations for optimizing energy efficiency, latency, and accuracy, treating LLMs as configurable systems.", "motivation": "LLMs have high computational demands that raise sustainability concerns, with inference dominating total compute usage. The vast configuration space of inference servers makes exhaustive empirical evaluation infeasible due to combinatorial explosion.", "method": "Treat LLMs as configurable systems and apply variability management techniques. Represent generation hyperparameters and constraints using feature-based variability models, sample representative configurations, measure energy consumption, latency, and accuracy, and learn predictive models from collected data.", "result": "Variability modeling effectively manages LLM inference configuration complexity, enables systematic analysis of hyperparameter effects and interactions, reveals trade-offs, and supports accurate prediction of inference behavior from limited measurements.", "conclusion": "This work opens a new research direction bridging software engineering and machine learning by leveraging variability modeling for efficient and sustainable LLM configuration."}}
{"id": "2602.18020", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18020", "abs": "https://arxiv.org/abs/2602.18020", "authors": ["Jiabing Yang", "Yixiang Chen", "Yuan Xu", "Peiyan Li", "Xiangnan Wu", "Zichen Wen", "Bowen Fang", "Tao Yu", "Zhengbo Zhang", "Yingda Li", "Kai Wang", "Jing Liu", "Nianfeng Liu", "Yan Huang", "Liang Wang"], "title": "UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as \"key-value memory\", we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer's Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.", "AI": {"tldr": "UAOR is a training-free plug-and-play module for Vision-Language-Action models that reinjects observation information when language layers show high uncertainty, improving performance without extra data or modules.", "motivation": "Existing VLA enhancement methods require costly extra observation cues (depth maps, point clouds) or auxiliary modules (object detectors, encoders) with additional training and data collection. There's a need for a more efficient, training-free approach to improve VLA performance.", "method": "UAOR (Uncertainty-aware Observation Reinjection) monitors language model layer uncertainty using Action Entropy. When high uncertainty is detected, it retrieves and reinjects key observation information into the next layer's Feed-Forward Network through attention mechanisms, helping VLAs better attend to observations during inference.", "result": "Comprehensive experiments show UAOR consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. It eliminates the need for additional observation cues or modules while enhancing performance.", "conclusion": "UAOR provides an effective, training-free, plug-and-play solution for enhancing VLA models, making it a versatile and practical addition to existing VLA pipelines without requiring costly data collection or additional training."}}
{"id": "2602.18396", "categories": ["cs.LG", "eess.SP", "math.PR", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.18396", "abs": "https://arxiv.org/abs/2602.18396", "authors": ["Ehsan Lari", "Reza Arablouei", "Stefan Werner"], "title": "PRISM-FCP: Byzantine-Resilient Federated Conformal Prediction via Partial Sharing", "comment": "13 pages, 5 figures, 2 tables, Submitted to IEEE Transactions on Signal Processing (TSP)", "summary": "We propose PRISM-FCP (Partial shaRing and robust calIbration with Statistical Margins for Federated Conformal Prediction), a Byzantine-resilient federated conformal prediction framework that utilizes partial model sharing to improve robustness against Byzantine attacks during both model training and conformal calibration. Existing approaches address adversarial behavior only in the calibration stage, leaving the learned model susceptible to poisoned updates. In contrast, PRISM-FCP mitigates attacks end-to-end. During training, clients partially share updates by transmitting only $M$ of $D$ parameters per round. This attenuates the expected energy of an adversary's perturbation in the aggregated update by a factor of $M/D$, yielding lower mean-square error (MSE) and tighter prediction intervals. During calibration, clients convert nonconformity scores into characterization vectors, compute distance-based maliciousness scores, and downweight or filter suspected Byzantine contributions before estimating the conformal quantile. Extensive experiments on both synthetic data and the UCI Superconductivity dataset demonstrate that PRISM-FCP maintains nominal coverage guarantees under Byzantine attacks while avoiding the interval inflation observed in standard FCP with reduced communication, providing a robust and communication-efficient approach to federated uncertainty quantification.", "AI": {"tldr": "PRISM-FCP is a Byzantine-resilient federated conformal prediction framework that uses partial model sharing and robust calibration to maintain coverage guarantees under attacks while reducing communication.", "motivation": "Existing federated conformal prediction approaches only address adversarial behavior during calibration, leaving the learned model vulnerable to poisoned updates during training. There's a need for end-to-end Byzantine resilience that also reduces communication overhead.", "method": "Two-stage approach: 1) During training, clients share only M out of D parameters per round to attenuate adversary perturbation by factor M/D. 2) During calibration, clients convert nonconformity scores to characterization vectors, compute distance-based maliciousness scores, and downweight/filter suspected Byzantine contributions before estimating conformal quantile.", "result": "PRISM-FCP maintains nominal coverage guarantees under Byzantine attacks while avoiding interval inflation observed in standard FCP. It achieves lower mean-square error, tighter prediction intervals, and reduced communication overhead.", "conclusion": "PRISM-FCP provides a robust and communication-efficient approach to federated uncertainty quantification that mitigates Byzantine attacks end-to-end during both model training and conformal calibration."}}
{"id": "2602.18344", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18344", "abs": "https://arxiv.org/abs/2602.18344", "authors": ["Mengguang Li", "Heinz Koeppl"], "title": "Downwash-aware Configuration Optimization for Modular Aerial Systems", "comment": "Accepted to the IEEE International Conference on Robotics and Automation (ICRA) 2026", "summary": "This work proposes a framework that generates and optimally selects task-specific assembly configurations for a large group of homogeneous modular aerial systems, explicitly enforcing bounds on inter-module downwash. Prior work largely focuses on planar layouts and often ignores aerodynamic interference. In contrast, firstly we enumerate non-isomorphic connection topologies at scale; secondly, we solve a nonlinear program to check feasibility and select the configuration that minimizes control input subject to actuation limits and downwash constraints. We evaluate the framework in physics-based simulation and demonstrate it in real-world experiments.", "AI": {"tldr": "Framework for generating and selecting optimal assembly configurations for modular aerial systems with downwash constraints", "motivation": "Prior work focuses on planar layouts and ignores aerodynamic interference, creating a need for better assembly configuration methods that account for downwash effects", "method": "1) Enumerate non-isomorphic connection topologies at scale, 2) Solve nonlinear program to check feasibility and select configuration minimizing control input subject to actuation limits and downwash constraints", "result": "Framework evaluated in physics-based simulation and demonstrated in real-world experiments", "conclusion": "Proposed framework successfully addresses limitations of prior work by considering aerodynamic interference and providing optimal assembly configurations for modular aerial systems"}}
{"id": "2602.17698", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17698", "abs": "https://arxiv.org/abs/2602.17698", "authors": ["Xinlin Li", "Timothy Chou", "Josh Fromm", "Zichang Liu", "Yunjie Pan", "Christina Fragouli"], "title": "ScaleBITS: Scalable Bitwidth Search for Hardware-Aligned Mixed-Precision LLMs", "comment": null, "summary": "Post-training weight quantization is crucial for reducing the memory and inference cost of large language models (LLMs), yet pushing the average precision below 4 bits remains challenging due to highly non-uniform weight sensitivity and the lack of principled precision allocation. Existing solutions use irregular fine-grained mixed-precision with high runtime overhead or rely on heuristics or highly constrained precision allocation strategies. In this work, we propose ScaleBITS, a mixed-precision quantization framework that enables automated, fine-grained bitwidth allocation under a memory budget while preserving hardware efficiency. Guided by a new sensitivity analysis, we introduce a hardware-aligned, block-wise weight partitioning scheme, powered by bi-directional channel reordering. We formulate global bitwidth allocation as a constrained optimization problem and develop a scalable approximation to the greedy algorithm, enabling end-to-end principled allocation. Experiments show that ScaleBITS significantly improves over uniform-precision quantization (up to +36%) and outperforms state-of-the-art sensitivity-aware baselines (up to +13%) in ultra-low-bit regime, without adding runtime overhead.", "AI": {"tldr": "ScaleBITS is a mixed-precision quantization framework for LLMs that enables automated fine-grained bitwidth allocation under memory constraints while maintaining hardware efficiency, achieving significant improvements over uniform quantization and state-of-the-art baselines.", "motivation": "Post-training weight quantization below 4 bits is challenging due to non-uniform weight sensitivity and lack of principled precision allocation. Existing solutions have high runtime overhead or rely on heuristics/constrained strategies.", "method": "Proposes ScaleBITS framework with hardware-aligned block-wise weight partitioning using bi-directional channel reordering. Formulates global bitwidth allocation as constrained optimization with scalable approximation to greedy algorithm.", "result": "Significantly improves over uniform-precision quantization (up to +36%) and outperforms state-of-the-art sensitivity-aware baselines (up to +13%) in ultra-low-bit regime, without adding runtime overhead.", "conclusion": "ScaleBITS enables effective automated fine-grained mixed-precision quantization for LLMs under memory budgets while preserving hardware efficiency, addressing key challenges in ultra-low-bit quantization."}}
{"id": "2602.18022", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18022", "abs": "https://arxiv.org/abs/2602.18022", "authors": ["Guandong Li", "Mengxia Ye"], "title": "Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers", "comment": null, "summary": "Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT's multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(\u03b4_k, \u03b4_v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).", "AI": {"tldr": "DCAG is a training-free framework that simultaneously manipulates both Key and Value channels in DiT's attention layers for precise control over editing intensity, outperforming Key-only methods.", "motivation": "Existing attention manipulation methods for diffusion-based image editing focus only on the Key space, ignoring the Value space which governs feature aggregation. There's a need for more precise training-free control over editing intensity in DiT-based models.", "method": "Proposes Dual-Channel Attention Guidance (DCAG) that exploits the bias-delta structure in both Key and Value projections of DiT's multi-modal attention layers. DCAG simultaneously manipulates the Key channel (controlling where to attend) and Value channel (controlling what to aggregate), creating a two-dimensional parameter space (\u03b4_k, \u03b4_v) for precise editing control.", "result": "DCAG consistently outperforms Key-only guidance across all fidelity metrics on the PIE-Bench benchmark (700 images, 10 editing categories). Most significant improvements are in localized editing tasks: 4.9% LPIPS reduction for object deletion and 3.2% LPIPS reduction for object addition.", "conclusion": "DCAG enables more precise editing-fidelity trade-offs than single-channel methods by leveraging both Key and Value channels, with Key providing coarse control and Value providing fine-grained complement, demonstrating the importance of exploiting both attention channels for diffusion-based image editing."}}
{"id": "2602.18374", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18374", "abs": "https://arxiv.org/abs/2602.18374", "authors": ["Venkatesh Sripada", "Frank Guerin", "Amir Ghalamzan"], "title": "Zero-shot Interactive Perception", "comment": "Original manuscript submitted on April 24, 2025. Timestamped and publicly available on OpenReview: https://openreview.net/forum?id=7MhpFcr5Nx", "summary": "Interactive perception (IP) enables robots to extract hidden information in their workspace and execute manipulation plans by physically interacting with objects and altering the state of the environment -- crucial for resolving occlusions and ambiguity in complex, partially observable scenarios. We present Zero-Shot IP (ZS-IP), a novel framework that couples multi-strategy manipulation (pushing and grasping) with a memory-driven Vision Language Model (VLM) to guide robotic interactions and resolve semantic queries. ZS-IP integrates three key components: (1) an Enhanced Observation (EO) module that augments the VLM's visual perception with both conventional keypoints and our proposed pushlines -- a novel 2D visual augmentation tailored to pushing actions, (2) a memory-guided action module that reinforces semantic reasoning through context lookup, and (3) a robotic controller that executes pushing, pulling, or grasping based on VLM output. Unlike grid-based augmentations optimized for pick-and-place, pushlines capture affordances for contact-rich actions, substantially improving pushing performance. We evaluate ZS-IP on a 7-DOF Franka Panda arm across diverse scenes with varying occlusions and task complexities. Our experiments demonstrate that ZS-IP outperforms passive and viewpoint-based perception techniques such as Mark-Based Visual Prompting (MOKA), particularly in pushing tasks, while preserving the integrity of non-target elements.", "AI": {"tldr": "Zero-Shot Interactive Perception (ZS-IP) is a novel framework that combines multi-strategy manipulation (pushing/grasping) with memory-driven Vision Language Models to guide robotic interactions and resolve semantic queries in partially observable environments.", "motivation": "Interactive perception is crucial for robots to extract hidden information in complex, partially observable scenarios by physically interacting with objects. Current methods struggle with occlusions and ambiguity, especially for contact-rich actions like pushing.", "method": "ZS-IP integrates three components: 1) Enhanced Observation module with conventional keypoints and novel pushlines (2D visual augmentations for pushing actions), 2) memory-guided action module for semantic reasoning through context lookup, and 3) robotic controller for pushing, pulling, or grasping based on VLM output.", "result": "ZS-IP outperforms passive and viewpoint-based perception techniques like Mark-Based Visual Prompting (MOKA), particularly in pushing tasks, while preserving non-target element integrity. Evaluated on 7-DOF Franka Panda arm across diverse scenes with varying occlusions and complexities.", "conclusion": "The proposed framework successfully integrates multi-strategy manipulation with memory-driven VLMs, with pushlines specifically improving pushing performance over grid-based augmentations optimized for pick-and-place tasks."}}
{"id": "2602.17699", "categories": ["cs.LG", "math.RA", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17699", "abs": "https://arxiv.org/abs/2602.17699", "authors": ["Chandrasekhar Gokavarapu", "Sudhakar Gadde", "Y. Rajasekhar", "S. R. Bhargava"], "title": "Certified Learning under Distribution Shift: Sound Verification and Identifiable Structure", "comment": null, "summary": "Proposition. Let $f$ be a predictor trained on a distribution $P$ and evaluated on a shifted distribution $Q$. Under verifiable regularity and complexity constraints, the excess risk under shift admits an explicit upper bound determined by a computable shift metric and model parameters. We develop a unified framework in which (i) risk under distribution shift is certified by explicit inequalities, (ii) verification of learned models is sound for nontrivial sizes, and (iii) interpretability is enforced through identifiability conditions rather than post hoc explanations. All claims are stated with explicit assumptions. Failure modes are isolated. Non-certifiable regimes are characterized.", "AI": {"tldr": "The paper provides a theoretical framework for certifying model performance under distribution shift with explicit bounds based on computable shift metrics and model parameters.", "motivation": "To address the challenge of ensuring model reliability when deployed in shifted distributions from training data, providing theoretical guarantees rather than empirical observations alone.", "method": "Develops a unified framework with explicit assumptions, regularity conditions, and complexity constraints to derive certifiable bounds on excess risk under distribution shift using computable shift metrics.", "result": "Establishes explicit inequalities that certify risk under distribution shift, enables sound verification for nontrivial model sizes, and enforces interpretability through identifiability conditions rather than post-hoc explanations.", "conclusion": "Provides a rigorous theoretical foundation for certifying model performance under distribution shift, with explicit characterization of assumptions, failure modes, and non-certifiable regimes."}}
{"id": "2602.17700", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.17700", "abs": "https://arxiv.org/abs/2602.17700", "authors": ["Konstanty Subbotko"], "title": "MIDAS: Mosaic Input-Specific Differentiable Architecture Search", "comment": null, "summary": "Differentiable Neural Architecture Search (NAS) provides efficient, gradient-based methods for automatically designing neural networks, yet its adoption remains limited in practice. We present MIDAS, a novel approach that modernizes DARTS by replacing static architecture parameters with dynamic, input-specific parameters computed via self-attention. To improve robustness, MIDAS (i) localizes the architecture selection by computing it separately for each spatial patch of the activation map, and (ii) introduces a parameter-free, topology-aware search space that models node connectivity and simplifies selecting the two incoming edges per node. We evaluate MIDAS on the DARTS, NAS-Bench-201, and RDARTS search spaces. In DARTS, it reaches 97.42% top-1 on CIFAR-10 and 83.38% on CIFAR-100. In NAS-Bench-201, it consistently finds globally optimal architectures. In RDARTS, it sets the state of the art on two of four search spaces on CIFAR-10. We further analyze why MIDAS works, showing that patchwise attention improves discrimination among candidate operations, and the resulting input-specific parameter distributions are class-aware and predominantly unimodal, providing reliable guidance for decoding.", "AI": {"tldr": "MIDAS modernizes DARTS by replacing static architecture parameters with dynamic, input-specific parameters computed via self-attention, with patchwise localization and topology-aware search space for improved robustness.", "motivation": "Differentiable NAS provides efficient gradient-based methods for neural architecture design, but adoption remains limited in practice. The authors aim to modernize DARTS with more dynamic and robust architecture search capabilities.", "method": "MIDAS replaces static architecture parameters with dynamic, input-specific parameters computed via self-attention. It localizes architecture selection by computing it separately for each spatial patch, and introduces a parameter-free, topology-aware search space that models node connectivity and simplifies edge selection.", "result": "On DARTS: 97.42% top-1 on CIFAR-10 and 83.38% on CIFAR-100. On NAS-Bench-201: consistently finds globally optimal architectures. On RDARTS: sets state-of-the-art on two of four search spaces on CIFAR-10. Analysis shows patchwise attention improves operation discrimination, and parameter distributions are class-aware and unimodal.", "conclusion": "MIDAS successfully modernizes DARTS with dynamic, input-specific architecture parameters via self-attention, achieving strong performance across multiple search spaces while providing reliable guidance for architecture decoding through improved discrimination and robust parameter distributions."}}
{"id": "2602.18043", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18043", "abs": "https://arxiv.org/abs/2602.18043", "authors": ["Hongyu Qu", "Xiangbo Shu", "Rui Yan", "Hailiang Gao", "Wenguan Wang", "Jinhui Tang"], "title": "Spatio-temporal Decoupled Knowledge Compensator for Few-Shot Action Recognition", "comment": "Accepted to TPAMI 2026", "summary": "Few-Shot Action Recognition (FSAR) is a challenging task that requires recognizing novel action categories with a few labeled videos. Recent works typically apply semantically coarse category names as auxiliary contexts to guide the learning of discriminative visual features. However, such context provided by the action names is too limited to provide sufficient background knowledge for capturing novel spatial and temporal concepts in actions. In this paper, we propose DiST, an innovative Decomposition-incorporation framework for FSAR that makes use of decoupled Spatial and Temporal knowledge provided by large language models to learn expressive multi-granularity prototypes. In the decomposition stage, we decouple vanilla action names into diverse spatio-temporal attribute descriptions (action-related knowledge). Such commonsense knowledge complements semantic contexts from spatial and temporal perspectives. In the incorporation stage, we propose Spatial/Temporal Knowledge Compensators (SKC/TKC) to discover discriminative object-level and frame-level prototypes, respectively. In SKC, object-level prototypes adaptively aggregate important patch tokens under the guidance of spatial knowledge. Moreover, in TKC, frame-level prototypes utilize temporal attributes to assist in inter-frame temporal relation modeling. These learned prototypes thus provide transparency in capturing fine-grained spatial details and diverse temporal patterns. Experimental results show DiST achieves state-of-the-art results on five standard FSAR datasets.", "AI": {"tldr": "DiST: A novel framework for Few-Shot Action Recognition that decomposes action names into spatial/temporal attributes using LLMs, then incorporates this knowledge to learn multi-granularity prototypes for better novel action recognition.", "motivation": "Current FSAR methods use coarse category names as auxiliary context, which is too limited to capture novel spatial and temporal concepts in actions. More detailed background knowledge is needed to improve recognition of novel action categories with few labeled videos.", "method": "DiST uses a two-stage framework: 1) Decomposition stage - decouples action names into diverse spatio-temporal attribute descriptions using large language models; 2) Incorporation stage - uses Spatial/Temporal Knowledge Compensators (SKC/TKC) to learn object-level and frame-level prototypes that capture fine-grained spatial details and temporal patterns.", "result": "DiST achieves state-of-the-art results on five standard Few-Shot Action Recognition datasets, demonstrating the effectiveness of using decomposed spatial and temporal knowledge for learning expressive prototypes.", "conclusion": "The proposed DiST framework successfully addresses the limitations of coarse action names in FSAR by leveraging LLM-generated spatio-temporal knowledge to learn discriminative multi-granularity prototypes, leading to improved performance on novel action recognition with few labeled examples."}}
{"id": "2602.18379", "categories": ["cs.RO", "cond-mat.soft"], "pdf": "https://arxiv.org/pdf/2602.18379", "abs": "https://arxiv.org/abs/2602.18379", "authors": ["Hugo de Souza Oliveira", "Xin Li", "Mohsen Jafarpour", "Edoardo Milana"], "title": "Ori-Sense: origami capacitive sensing for soft robotic applications", "comment": "9th IEEE-RAS International Conference on Soft Robotics (RoboSoft 2026)", "summary": "This work introduces Ori-Sense, a compliant capacitive sensor inspired by the inverted Kresling origami pattern. The device translates torsional deformation into measurable capacitance changes, enabling proprioceptive feedback for soft robotic systems. Using dissolvable-core molding, we fabricated a monolithic silicone structure with embedded conductive TPU electrodes, forming an integrated soft capacitor. Mechanical characterization revealed low stiffness and minimal impedance, with torque values below 0.01 N mm for axial displacements between -15 mm and 15 mm, and up to 0.03 N mm at 30 degrees twist under compression. Finite-element simulations confirmed localized stresses along fold lines and validated the measured torque-rotation response. Electrical tests showed consistent capacitance modulation up to 30%, directly correlated with the twist angle, and maximal sensitivity of S_theta ~ 0.0067 pF/deg at 5 mm of axial deformation.", "AI": {"tldr": "Ori-Sense is a compliant capacitive sensor based on inverted Kresling origami that measures torsional deformation through capacitance changes for soft robotic proprioception.", "motivation": "To develop a soft, compliant sensor that can provide proprioceptive feedback for soft robotic systems by translating torsional deformation into measurable electrical signals.", "method": "Used dissolvable-core molding to fabricate a monolithic silicone structure with embedded conductive TPU electrodes, creating an integrated soft capacitor. Conducted mechanical characterization and finite-element simulations to validate performance.", "result": "The sensor showed low stiffness (torque <0.01 N\u00b7mm for \u00b115 mm axial displacement), consistent capacitance modulation up to 30% correlated with twist angle, and maximal sensitivity of ~0.0067 pF/deg at 5 mm axial deformation.", "conclusion": "Ori-Sense successfully demonstrates a compliant capacitive sensor capable of providing proprioceptive feedback for soft robotics through torsional deformation sensing with good sensitivity and low mechanical impedance."}}
{"id": "2602.17751", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17751", "abs": "https://arxiv.org/abs/2602.17751", "authors": ["Nina Brolich", "Simon Geis", "Maximilian Kasper", "Alexander Barnhill", "Axel Plinge", "Dominik Seu\u00df"], "title": "Investigating Target Class Influence on Neural Network Compressibility for Energy-Autonomous Avian Monitoring", "comment": "11 pages, 7 figures, Funding: GreenICT@FMD (BMFTR grant 16ME0491K)", "summary": "Biodiversity loss poses a significant threat to humanity, making wildlife monitoring essential for assessing ecosystem health. Avian species are ideal subjects for this due to their popularity and the ease of identifying them through their distinctive songs. Traditionalavian monitoring methods require manual counting and are therefore costly and inefficient. In passive acoustic monitoring, soundscapes are recorded over long periods of time. The recordings are analyzed to identify bird species afterwards. Machine learning methods have greatly expedited this process in a wide range of species and environments, however, existing solutions require complex models and substantial computational resources. Instead, we propose running machine learning models on inexpensive microcontroller units (MCUs) directly in the field. Due to the resulting hardware and energy constraints, efficient artificial intelligence (AI) architecture is required. In this paper, we present our method for avian monitoring on MCUs. We trained and compressed models for various numbers of target classes to assess the detection of multiple bird species on edge devices and evaluate the influence of the number of species on the compressibility of neural networks. Our results demonstrate significant compression rates with minimal performance loss. We also provide benchmarking results for different hardware platforms and evaluate the feasibility of deploying energy-autonomous devices.", "AI": {"tldr": "This paper proposes running bird species detection models on low-power microcontrollers for efficient wildlife monitoring, achieving significant compression with minimal performance loss.", "motivation": "Traditional bird monitoring methods are costly and inefficient, requiring manual counting. While machine learning has helped automate acoustic monitoring, existing solutions need complex models and substantial computational resources. The authors aim to enable efficient wildlife monitoring using low-cost, energy-constrained edge devices.", "method": "The authors trained and compressed neural network models for various numbers of target bird species classes, specifically designed to run on inexpensive microcontroller units (MCUs). They assessed model compressibility and detection performance across different hardware platforms while evaluating energy-autonomous deployment feasibility.", "result": "The approach achieved significant compression rates with minimal performance loss. Benchmarking results showed the feasibility of deploying these models on different hardware platforms, enabling energy-autonomous devices for avian monitoring.", "conclusion": "Running bird species detection models on MCUs is a viable approach for efficient wildlife monitoring, offering a practical solution that balances computational constraints with detection accuracy for biodiversity assessment."}}
{"id": "2602.18047", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18047", "abs": "https://arxiv.org/abs/2602.18047", "authors": ["Rong Fu", "Wenxin Zhang", "Yibo Meng", "Jia Yee Tan", "Jiaxuan Lu", "Rui Lu", "Jiekai Wu", "Zhaolu Kang", "Simon Fong"], "title": "CityGuard: Graph-Aware Private Descriptors for Bias-Resilient Identity Search Across Urban Cameras", "comment": "36 pages, 12 figures", "summary": "City-scale person re-identification across distributed cameras must handle severe appearance changes from viewpoint, occlusion, and domain shift while complying with data protection rules that prevent sharing raw imagery. We introduce CityGuard, a topology-aware transformer for privacy-preserving identity retrieval in decentralized surveillance. The framework integrates three components. A dispersion-adaptive metric learner adjusts instance-level margins according to feature spread, increasing intra-class compactness. Spatially conditioned attention injects coarse geometry, such as GPS or deployment floor plans, into graph-based self-attention to enable projectively consistent cross-view alignment using only coarse geometric priors without requiring survey-grade calibration. Differentially private embedding maps are coupled with compact approximate indexes to support secure and cost-efficient deployment. Together these designs produce descriptors robust to viewpoint variation, occlusion, and domain shifts, and they enable a tunable balance between privacy and utility under rigorous differential-privacy accounting. Experiments on Market-1501 and additional public benchmarks, complemented by database-scale retrieval studies, show consistent gains in retrieval precision and query throughput over strong baselines, confirming the practicality of the framework for privacy-critical urban identity matching.", "AI": {"tldr": "CityGuard: A privacy-preserving transformer for decentralized person re-identification using dispersion-adaptive metric learning, spatially conditioned attention, and differentially private embeddings.", "motivation": "City-scale person re-identification faces challenges of severe appearance changes (viewpoint, occlusion, domain shift) while needing to comply with data protection rules that prevent sharing raw imagery in decentralized surveillance systems.", "method": "Three-component framework: 1) Dispersion-adaptive metric learner adjusts instance-level margins based on feature spread; 2) Spatially conditioned attention injects coarse geometry (GPS/floor plans) into graph-based self-attention for cross-view alignment; 3) Differentially private embedding maps with compact approximate indexes for secure deployment.", "result": "Experiments on Market-1501 and other benchmarks show consistent gains in retrieval precision and query throughput over strong baselines, with database-scale retrieval studies confirming practical utility for privacy-critical urban identity matching.", "conclusion": "CityGuard provides a tunable balance between privacy and utility under differential privacy, producing descriptors robust to viewpoint variation, occlusion, and domain shifts for practical privacy-preserving identity retrieval in decentralized surveillance."}}
{"id": "2602.18386", "categories": ["cs.RO", "cs.AI", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18386", "abs": "https://arxiv.org/abs/2602.18386", "authors": ["Mohamed Elgouhary", "Amr S. El-Wakeel"], "title": "Learning to Tune Pure Pursuit in Autonomous Racing: Joint Lookahead and Steering-Gain Control with PPO", "comment": null, "summary": "Pure Pursuit (PP) is widely used in autonomous racing for real-time path tracking due to its efficiency and geometric clarity, yet performance is highly sensitive to how key parameters-lookahead distance and steering gain-are chosen. Standard velocity-based schedules adjust these only approximately and often fail to transfer across tracks and speed profiles. We propose a reinforcement-learning (RL) approach that jointly chooses the lookahead Ld and a steering gain g online using Proximal Policy Optimization (PPO). The policy observes compact state features (speed and curvature taps) and outputs (Ld, g) at each control step. Trained in F1TENTH Gym and deployed in a ROS 2 stack, the policy drives PP directly (with light smoothing) and requires no per-map retuning. Across simulation and real-car tests, the proposed RL-PP controller that jointly selects (Ld, g) consistently outperforms fixed-lookahead PP, velocity-scheduled adaptive PP, and an RL lookahead-only variant, and it also exceeds a kinematic MPC raceline tracker under our evaluated settings in lap time, path-tracking accuracy, and steering smoothness, demonstrating that policy-guided parameter tuning can reliably improve classical geometry-based control.", "AI": {"tldr": "RL-based adaptive tuning of Pure Pursuit parameters (lookahead distance and steering gain) outperforms traditional fixed and velocity-scheduled methods in autonomous racing.", "motivation": "Pure Pursuit is widely used in autonomous racing but its performance depends heavily on parameter tuning. Traditional velocity-based schedules are approximate and don't transfer well across different tracks and speed profiles.", "method": "Use reinforcement learning (Proximal Policy Optimization) to jointly choose lookahead distance and steering gain online. The policy observes compact state features (speed and curvature taps) and outputs parameters at each control step. Trained in F1TENTH Gym and deployed in ROS 2.", "result": "The RL-PP controller consistently outperforms fixed-lookahead PP, velocity-scheduled adaptive PP, and RL lookahead-only variant. It also exceeds a kinematic MPC raceline tracker in lap time, path-tracking accuracy, and steering smoothness.", "conclusion": "Policy-guided parameter tuning can reliably improve classical geometry-based control like Pure Pursuit, demonstrating that RL can effectively adapt key parameters online without per-map retuning."}}
{"id": "2602.17706", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17706", "abs": "https://arxiv.org/abs/2602.17706", "authors": ["Rongyao Cai", "Yuxi Wan", "Kexin Zhang", "Ming Jin", "Zhiqiang Ge", "Qingsong Wen", "Yong Liu"], "title": "Parallel Complex Diffusion for Scalable Time Series Generation", "comment": null, "summary": "Modeling long-range dependencies in time series generation poses a fundamental trade-off between representational capacity and computational efficiency. Traditional temporal diffusion models suffer from local entanglement and the $\\mathcal{O}(L^2)$ cost of attention mechanisms. We address these limitations by introducing PaCoDi (Parallel Complex Diffusion), a spectral-native architecture that decouples generative modeling in the frequency domain. PaCoDi fundamentally alters the problem topology: the Fourier Transform acts as a diagonalizing operator, converting locally coupled temporal signals into globally decorrelated spectral components. Theoretically, we prove the Quadrature Forward Diffusion and Conditional Reverse Factorization theorem, demonstrating that the complex diffusion process can be split into independent real and imaginary branches. We bridge the gap between this decoupled theory and data reality using a \\textbf{Mean Field Theory (MFT) approximation} reinforced by an interactive correction mechanism. Furthermore, we generalize this discrete DDPM to continuous-time Frequency SDEs, rigorously deriving the Spectral Wiener Process describe the differential spectral Brownian motion limit. Crucially, PaCoDi exploits the Hermitian Symmetry of real-valued signals to compress the sequence length by half, achieving a 50% reduction in attention FLOPs without information loss. We further derive a rigorous Heteroscedastic Loss to handle the non-isotropic noise distribution on the compressed manifold. Extensive experiments show that PaCoDi outperforms existing baselines in both generation quality and inference speed, offering a theoretically grounded and computationally efficient solution for time series modeling.", "AI": {"tldr": "PaCoDi introduces a spectral-native diffusion model that transforms time series into frequency domain to decouple dependencies, achieving 50% FLOPs reduction via Hermitian symmetry while maintaining generation quality.", "motivation": "Traditional temporal diffusion models face trade-offs between representational capacity and computational efficiency due to local entanglement and O(L\u00b2) attention costs. Long-range dependencies in time series generation need better modeling approaches.", "method": "PaCoDi uses Fourier Transform to diagonalize temporal signals into decorrelated spectral components. It employs Mean Field Theory approximation with interactive correction, splits complex diffusion into independent real/imaginary branches, exploits Hermitian symmetry for 50% compression, and uses Heteroscedastic Loss for non-isotropic noise.", "result": "Extensive experiments show PaCoDi outperforms existing baselines in both generation quality and inference speed, achieving 50% reduction in attention FLOPs without information loss.", "conclusion": "PaCoDi provides a theoretically grounded, computationally efficient solution for time series modeling by fundamentally altering problem topology through spectral transformation and decoupling dependencies."}}
{"id": "2602.18057", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18057", "abs": "https://arxiv.org/abs/2602.18057", "authors": ["Hongsong Wang", "Wenjing Yan", "Qiuxia Lai", "Xin Geng"], "title": "Temporal Consistency-Aware Text-to-Motion Generation", "comment": "Code is on https://github.com/Giat995/TCA-T2M/", "summary": "Text-to-Motion (T2M) generation aims to synthesize realistic human motion sequences from natural language descriptions. While two-stage frameworks leveraging discrete motion representations have advanced T2M research, they often neglect cross-sequence temporal consistency, i.e., the shared temporal structures present across different instances of the same action. This leads to semantic misalignments and physically implausible motions. To address this limitation, we propose TCA-T2M, a framework for temporal consistency-aware T2M generation. Our approach introduces a temporal consistency-aware spatial VQ-VAE (TCaS-VQ-VAE) for cross-sequence temporal alignment, coupled with a masked motion transformer for text-conditioned motion generation. Additionally, a kinematic constraint block mitigates discretization artifacts to ensure physical plausibility. Experiments on HumanML3D and KIT-ML benchmarks demonstrate that TCA-T2M achieves state-of-the-art performance, highlighting the importance of temporal consistency in robust and coherent T2M generation.", "AI": {"tldr": "TCA-T2M: A temporal consistency-aware framework for text-to-motion generation that addresses cross-sequence temporal alignment issues in existing two-stage approaches.", "motivation": "Existing two-stage text-to-motion frameworks neglect cross-sequence temporal consistency (shared temporal structures across different instances of the same action), leading to semantic misalignments and physically implausible motions.", "method": "Proposes TCA-T2M with three key components: 1) Temporal consistency-aware spatial VQ-VAE (TCaS-VQ-VAE) for cross-sequence temporal alignment, 2) Masked motion transformer for text-conditioned motion generation, and 3) Kinematic constraint block to mitigate discretization artifacts and ensure physical plausibility.", "result": "Achieves state-of-the-art performance on HumanML3D and KIT-ML benchmarks, demonstrating the importance of temporal consistency for robust and coherent text-to-motion generation.", "conclusion": "Temporal consistency is crucial for high-quality text-to-motion generation, and the proposed TCA-T2M framework effectively addresses this limitation through cross-sequence temporal alignment and physical plausibility constraints."}}
{"id": "2602.18397", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18397", "abs": "https://arxiv.org/abs/2602.18397", "authors": ["Wenqi Jiang", "Jason Clemons", "Karu Sankaralingam", "Christos Kozyrakis"], "title": "How Fast Can I Run My VLA? Demystifying VLA Inference Performance with VLA-Perf", "comment": null, "summary": "Vision-Language-Action (VLA) models have recently demonstrated impressive capabilities across various embodied AI tasks. While deploying VLA models on real-world robots imposes strict real-time inference constraints, the inference performance landscape of VLA remains poorly understood due to the large combinatorial space of model architectures and inference systems. In this paper, we ask a fundamental research question: How should we design future VLA models and systems to support real-time inference? To address this question, we first introduce VLA-Perf, an analytical performance model that can analyze inference performance for arbitrary combinations of VLA models and inference systems. Using VLA-Perf, we conduct the first systematic study of the VLA inference performance landscape. From a model-design perspective, we examine how inference performance is affected by model scaling, model architectural choices, long-context video inputs, asynchronous inference, and dual-system model pipelines. From the deployment perspective, we analyze where VLA inference should be executed -- on-device, on edge servers, or in the cloud -- and how hardware capability and network performance jointly determine end-to-end latency. By distilling 15 key takeaways from our comprehensive evaluation, we hope this work can provide practical guidance for the design of future VLA models and inference systems.", "AI": {"tldr": "VLA-Perf: An analytical performance model for evaluating Vision-Language-Action model inference performance across different architectures and systems to guide real-time deployment on robots.", "motivation": "VLA models show promise for embodied AI tasks but face real-time inference constraints on robots. The large combinatorial space of model architectures and inference systems makes performance understanding difficult, creating a need for systematic analysis to guide future VLA design and deployment.", "method": "Introduces VLA-Perf, an analytical performance model that can analyze inference performance for arbitrary combinations of VLA models and inference systems. Uses this model to conduct the first systematic study of VLA inference performance landscape, examining model scaling, architectural choices, long-context video inputs, asynchronous inference, dual-system pipelines, and deployment locations (on-device, edge, cloud).", "result": "Comprehensive evaluation yields 15 key takeaways providing practical guidance for future VLA model and inference system design, analyzing how hardware capability and network performance jointly determine end-to-end latency.", "conclusion": "VLA-Perf enables systematic analysis of VLA inference performance, providing crucial insights for designing models and systems that can meet real-time constraints for robotic deployment, with 15 practical takeaways to guide future development."}}
{"id": "2602.17743", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17743", "abs": "https://arxiv.org/abs/2602.17743", "authors": ["Di Zhang"], "title": "Provable Adversarial Robustness in In-Context Learning", "comment": "16 pages", "summary": "Large language models adapt to new tasks through in-context learning (ICL) without parameter updates. Current theoretical explanations for this capability assume test tasks are drawn from a distribution similar to that seen during pretraining. This assumption overlooks adversarial distribution shifts that threaten real-world reliability. To address this gap, we introduce a distributionally robust meta-learning framework that provides worst-case performance guarantees for ICL under Wasserstein-based distribution shifts. Focusing on linear self-attention Transformers, we derive a non-asymptotic bound linking adversarial perturbation strength ($\u03c1$), model capacity ($m$), and the number of in-context examples ($N$). The analysis reveals that model robustness scales with the square root of its capacity ($\u03c1_{\\text{max}} \\propto \\sqrt{m}$), while adversarial settings impose a sample complexity penalty proportional to the square of the perturbation magnitude ($N_\u03c1- N_0 \\propto \u03c1^2$). Experiments on synthetic tasks confirm these scaling laws. These findings advance the theoretical understanding of ICL's limits under adversarial conditions and suggest that model capacity serves as a fundamental resource for distributional robustness.", "AI": {"tldr": "The paper provides theoretical guarantees for in-context learning under adversarial distribution shifts, showing robustness scales with \u221a(model capacity) and adversarial settings require additional in-context examples proportional to perturbation magnitude squared.", "motivation": "Current theoretical explanations for in-context learning assume test tasks come from similar distributions as pretraining, overlooking adversarial distribution shifts that threaten real-world reliability. This gap needs addressing to understand ICL's limits under adversarial conditions.", "method": "Introduces a distributionally robust meta-learning framework with worst-case performance guarantees for ICL under Wasserstein-based distribution shifts. Focuses on linear self-attention Transformers and derives non-asymptotic bounds linking adversarial perturbation strength, model capacity, and number of in-context examples.", "result": "Analysis reveals: 1) Model robustness scales with square root of its capacity (\u03c1_max \u221d \u221am), 2) Adversarial settings impose sample complexity penalty proportional to square of perturbation magnitude (N_\u03c1 - N_0 \u221d \u03c1\u00b2). Experiments on synthetic tasks confirm these scaling laws.", "conclusion": "The findings advance theoretical understanding of ICL's limits under adversarial conditions and suggest model capacity serves as a fundamental resource for distributional robustness, providing worst-case guarantees for real-world reliability."}}
{"id": "2602.17865", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17865", "abs": "https://arxiv.org/abs/2602.17865", "authors": ["Andrzej Podobi\u0144ski", "Jaros\u0142aw A. Chudziak"], "title": "Financial time series augmentation using transformer based GAN architecture", "comment": "This paper has been accepted for the upcoming 18th International Conference on Agents and Artificial Intelligence (ICAART-2026), Marbella, Spain. The final published version will appear in the official conference proceedings", "summary": "Time-series forecasting is a critical task across many domains, from engineering to economics, where accurate predictions drive strategic decisions. However, applying advanced deep learning models in challenging, volatile domains like finance is difficult due to the inherent limitation and dynamic nature of financial time series data. This scarcity often results in sub-optimal model training and poor generalization. The fundamental challenge lies in determining how to reliably augment scarce financial time series data to enhance the predictive accuracy of deep learning forecasting models. Our main contribution is a demonstration of how Generative Adversarial Networks (GANs) can effectively serve as a data augmentation tool to overcome data scarcity in the financial domain. Specifically, we show that training a Long Short-Term Memory (LSTM) forecasting model on a dataset augmented with synthetic data generated by a transformer-based GAN (TTS-GAN) significantly improves the forecasting accuracy compared to using real data alone. We confirm these results across different financial time series (Bitcoin and S\\&P500 price data) and various forecasting horizons. Furthermore, we propose a novel, time series specific quality metric that combines Dynamic Time Warping (DTW) and a modified Deep Dataset Dissimilarity Measure (DeD-iMs) to reliably monitor the training progress and evaluate the quality of the generated data. These findings provide compelling evidence for the benefits of GAN-based data augmentation in enhancing financial predictive capabilities.", "AI": {"tldr": "GAN-based data augmentation using transformer-based GAN (TTS-GAN) improves financial time series forecasting accuracy by overcoming data scarcity, validated on Bitcoin and S&P500 data with novel quality metrics.", "motivation": "Financial time series data is scarce and volatile, making it difficult to train deep learning models effectively. Data scarcity leads to sub-optimal training and poor generalization, creating a need for reliable data augmentation methods to enhance predictive accuracy in financial forecasting.", "method": "Proposes using Generative Adversarial Networks (GANs) as data augmentation tool, specifically a transformer-based GAN (TTS-GAN) to generate synthetic financial time series data. Uses LSTM forecasting model trained on augmented dataset. Introduces novel quality metric combining Dynamic Time Warping (DTW) and modified Deep Dataset Dissimilarity Measure (DeD-iMs) to evaluate generated data quality.", "result": "Training LSTM forecasting model on dataset augmented with TTS-GAN generated synthetic data significantly improves forecasting accuracy compared to using real data alone. Results confirmed across different financial time series (Bitcoin and S&P500) and various forecasting horizons.", "conclusion": "GAN-based data augmentation effectively overcomes data scarcity in financial domain, enhancing predictive capabilities of deep learning forecasting models. The proposed approach provides compelling evidence for the benefits of synthetic data generation in improving financial time series forecasting accuracy."}}
{"id": "2602.18064", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18064", "abs": "https://arxiv.org/abs/2602.18064", "authors": ["Ziyue Wang", "Linghan Cai", "Chang Han Low", "Haofeng Liu", "Junde Wu", "Jingyu Wang", "Rui Wang", "Lei Song", "Jiang Bian", "Jingjing Fu", "Yueming Jin"], "title": "3DMedAgent: Unified Perception-to-Understanding for 3D Medical Analysis", "comment": "19 pages, 7 figures", "summary": "3D CT analysis spans a continuum from low-level perception to high-level clinical understanding. Existing 3D-oriented analysis methods adopt either isolated task-specific modeling or task-agnostic end-to-end paradigms to produce one-hop outputs, impeding the systematic accumulation of perceptual evidence for downstream reasoning. In parallel, recent multimodal large language models (MLLMs) exhibit improved visual perception and can integrate visual and textual information effectively, yet their predominantly 2D-oriented designs fundamentally limit their ability to perceive and analyze volumetric medical data. To bridge this gap, we propose 3DMedAgent, a unified agent that enables 2D MLLMs to perform general 3D CT analysis without 3D-specific fine-tuning. 3DMedAgent coordinates heterogeneous visual and textual tools through a flexible MLLM agent, progressively decomposing complex 3D analysis into tractable subtasks that transition from global to regional views, from 3D volumes to informative 2D slices, and from visual evidence to structured textual representations. Central to this design, 3DMedAgent maintains a long-term structured memory that aggregates intermediate tool outputs and supports query-adaptive, evidence-driven multi-step reasoning. We further introduce the DeepChestVQA benchmark for evaluating unified perception-to-understanding capabilities in 3D thoracic imaging. Experiments across over 40 tasks demonstrate that 3DMedAgent consistently outperforms general, medical, and 3D-specific MLLMs, highlighting a scalable path toward general-purpose 3D clinical assistants.Code and data are available at \\href{https://github.com/jinlab-imvr/3DMedAgent}{https://github.com/jinlab-imvr/3DMedAgent}.", "AI": {"tldr": "3DMedAgent enables 2D multimodal LLMs to perform 3D CT analysis without 3D fine-tuning by coordinating tools to decompose complex tasks into subtasks and maintaining structured memory for evidence-driven reasoning.", "motivation": "Existing 3D analysis methods use isolated task-specific modeling or task-agnostic end-to-end approaches that don't accumulate perceptual evidence for downstream reasoning. Meanwhile, MLLMs have improved visual perception but are limited by 2D-oriented designs that can't handle volumetric medical data effectively.", "method": "3DMedAgent coordinates heterogeneous visual and textual tools through a flexible MLLM agent to decompose complex 3D analysis into tractable subtasks. It transitions from global to regional views, 3D volumes to informative 2D slices, and visual evidence to structured textual representations. The system maintains a long-term structured memory that aggregates intermediate tool outputs for query-adaptive, evidence-driven multi-step reasoning.", "result": "Experiments across over 40 tasks demonstrate that 3DMedAgent consistently outperforms general, medical, and 3D-specific MLLMs. The authors also introduce the DeepChestVQA benchmark for evaluating unified perception-to-understanding capabilities in 3D thoracic imaging.", "conclusion": "3DMedAgent provides a scalable path toward general-purpose 3D clinical assistants by enabling 2D MLLMs to perform comprehensive 3D CT analysis without requiring 3D-specific fine-tuning, bridging the gap between 2D-oriented MLLMs and volumetric medical data analysis needs."}}
{"id": "2602.18421", "categories": ["cs.RO", "cond-mat.soft"], "pdf": "https://arxiv.org/pdf/2602.18421", "abs": "https://arxiv.org/abs/2602.18421", "authors": ["Xin Li", "Ye Jin", "Mohsen Jafarpour", "Hugo de Souza Oliveira", "Edoardo Milana"], "title": "Snapping Actuators with Asymmetric and Sequenced Motion", "comment": "9th IEEE-RAS International Conference on Soft Robotics (RoboSoft 2026)", "summary": "Snapping instabilities in soft structures offer a powerful pathway to achieve rapid and energy-efficient actuation. In this study, an eccentric dome-shaped snapping actuator is developed to generate controllable asymmetric motion through geometry-induced instability. Finite element simulations and experiments reveal consistent asymmetric deformation and the corresponding pressure characteristics. By coupling four snapping actuators in a pneumatic network, a compact quadrupedal robot achieves coordinated wavelike locomotion using only a single pressure input. The robot exhibits frequency-dependent performance with a maximum speed of 72.78~mm/s at 7.5~Hz. These findings demonstrate the potential of asymmetric snapping mechanisms for physically controlled actuation and lay the groundwork for fully untethered and efficient soft robotic systems.", "AI": {"tldr": "Researchers developed an eccentric dome-shaped snapping actuator that uses geometry-induced instability to create controllable asymmetric motion, then used four of these actuators in a pneumatic network to build a quadrupedal robot that achieves coordinated wavelike locomotion with just a single pressure input.", "motivation": "To harness snapping instabilities in soft structures for rapid, energy-efficient actuation and develop asymmetric snapping mechanisms for physically controlled actuation in soft robotics.", "method": "Created an eccentric dome-shaped snapping actuator that generates asymmetric motion through geometry-induced instability. Used finite element simulations and experiments to analyze deformation and pressure characteristics. Coupled four actuators in a pneumatic network to build a quadrupedal robot controlled by a single pressure input.", "result": "The quadrupedal robot achieved frequency-dependent performance with maximum speed of 72.78 mm/s at 7.5 Hz. The asymmetric snapping mechanism enabled coordinated wavelike locomotion using only one pressure input.", "conclusion": "Asymmetric snapping mechanisms show great potential for physically controlled actuation and pave the way for fully untethered, efficient soft robotic systems."}}
{"id": "2602.17744", "categories": ["cs.LG", "cs.CL", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17744", "abs": "https://arxiv.org/abs/2602.17744", "authors": ["Di Zhang", "Jiaqi Xing"], "title": "Bayesian Optimality of In-Context Learning with Selective State Spaces", "comment": "17 pages", "summary": "We propose Bayesian optimal sequential prediction as a new principle for understanding in-context learning (ICL). Unlike interpretations framing Transformers as performing implicit gradient descent, we formalize ICL as meta-learning over latent sequence tasks. For tasks governed by Linear Gaussian State Space Models (LG-SSMs), we prove a meta-trained selective SSM asymptotically implements the Bayes-optimal predictor, converging to the posterior predictive mean. We further establish a statistical separation from gradient descent, constructing tasks with temporally correlated noise where the optimal Bayesian predictor strictly outperforms any empirical risk minimization (ERM) estimator. Since Transformers can be seen as performing implicit ERM, this demonstrates selective SSMs achieve lower asymptotic risk due to superior statistical efficiency. Experiments on synthetic LG-SSM tasks and a character-level Markov benchmark confirm selective SSMs converge faster to Bayes-optimal risk, show superior sample efficiency with longer contexts in structured-noise settings, and track latent states more robustly than linear Transformers. This reframes ICL from \"implicit optimization\" to \"optimal inference,\" explaining the efficiency of selective SSMs and offering a principled basis for architecture design.", "AI": {"tldr": "The paper proposes Bayesian optimal sequential prediction as a new principle for understanding in-context learning, showing selective SSMs implement Bayes-optimal predictors for LG-SSM tasks and outperform Transformers due to superior statistical efficiency.", "motivation": "Current interpretations frame Transformers as performing implicit gradient descent, but the authors argue this view is incomplete. They seek to reframe in-context learning from \"implicit optimization\" to \"optimal inference\" to better explain the efficiency of selective SSMs and provide principled basis for architecture design.", "method": "Formalize ICL as meta-learning over latent sequence tasks. For Linear Gaussian State Space Models (LG-SSMs), prove that a meta-trained selective SSM asymptotically implements the Bayes-optimal predictor (converging to posterior predictive mean). Establish statistical separation from gradient descent by constructing tasks with temporally correlated noise where Bayesian predictor strictly outperforms any ERM estimator.", "result": "Selective SSMs converge faster to Bayes-optimal risk, show superior sample efficiency with longer contexts in structured-noise settings, and track latent states more robustly than linear Transformers. Experiments on synthetic LG-SSM tasks and character-level Markov benchmark confirm these advantages.", "conclusion": "The paper reframes ICL from \"implicit optimization\" to \"optimal inference,\" explaining why selective SSMs achieve lower asymptotic risk due to superior statistical efficiency. This provides a principled basis for architecture design and offers a new theoretical framework for understanding in-context learning."}}
{"id": "2602.17868", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17868", "abs": "https://arxiv.org/abs/2602.17868", "authors": ["Vasilii Feofanov", "Songkang Wen", "Jianfeng Zhang", "Lujia Pan", "Ievgen Redko"], "title": "MantisV2: Closing the Zero-Shot Gap in Time Series Classification with Synthetic Data and Test-Time Strategies", "comment": null, "summary": "Developing foundation models for time series classification is of high practical relevance, as such models can serve as universal feature extractors for diverse downstream tasks. Although early models such as Mantis have shown the promise of this approach, a substantial performance gap remained between frozen and fine-tuned encoders. In this work, we introduce methods that significantly strengthen zero-shot feature extraction for time series. First, we introduce Mantis+, a variant of Mantis pre-trained entirely on synthetic time series. Second, through controlled ablation studies, we refine the architecture and obtain MantisV2, an improved and more lightweight encoder. Third, we propose an enhanced test-time methodology that leverages intermediate-layer representations and refines output-token aggregation. In addition, we show that performance can be further improved via self-ensembling and cross-model embedding fusion. Extensive experiments on UCR, UEA, Human Activity Recognition (HAR) benchmarks, and EEG datasets show that MantisV2 and Mantis+ consistently outperform prior time series foundation models, achieving state-of-the-art zero-shot performance.", "AI": {"tldr": "MantisV2 and Mantis+ achieve state-of-the-art zero-shot performance for time series classification through synthetic pre-training, architectural improvements, and enhanced test-time methods.", "motivation": "Foundation models for time series classification can serve as universal feature extractors for diverse downstream tasks, but there was a substantial performance gap between frozen and fine-tuned encoders in early models like Mantis.", "method": "1) Mantis+ pre-trained entirely on synthetic time series; 2) MantisV2 with refined architecture through controlled ablation studies; 3) Enhanced test-time methodology using intermediate-layer representations and refined output-token aggregation; 4) Self-ensembling and cross-model embedding fusion.", "result": "MantisV2 and Mantis+ consistently outperform prior time series foundation models, achieving state-of-the-art zero-shot performance on UCR, UEA, Human Activity Recognition (HAR) benchmarks, and EEG datasets.", "conclusion": "The proposed methods significantly strengthen zero-shot feature extraction for time series, bridging the performance gap between frozen and fine-tuned encoders in time series foundation models."}}
{"id": "2602.18066", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18066", "abs": "https://arxiv.org/abs/2602.18066", "authors": ["Daniel Busch", "Christian Bohn", "Thomas Kurbiel", "Klaus Friedrichs", "Richard Meyes", "Tobias Meisen"], "title": "Faster Training, Fewer Labels: Self-Supervised Pretraining for Fine-Grained BEV Segmentation", "comment": "This Paper has been accepted to the 2026 IEEE Intelligent Vehicles Symposium (IV)", "summary": "Dense Bird's Eye View (BEV) semantic maps are central to autonomous driving, yet current multi-camera methods depend on costly, inconsistently annotated BEV ground truth. We address this limitation with a two-phase training strategy for fine-grained road marking segmentation that removes full supervision during pretraining and halves the amount of training data during fine-tuning while still outperforming the comparable supervised baseline model. During the self-supervised pretraining, BEVFormer predictions are differentiably reprojected into the image plane and trained against multi-view semantic pseudo-labels generated by the widely used semantic segmentation model Mask2Former. A temporal loss encourages consistency across frames. The subsequent supervised fine-tuning phase requires only 50% of the dataset and significantly less training time. With our method, the fine-tuning benefits from rich priors learned during pretraining boosting the performance and BEV segmentation quality (up to +2.5pp mIoU over the fully supervised baseline) on nuScenes. It simultaneously halves the usage of annotation data and reduces total training time by up to two thirds. The results demonstrate that differentiable reprojection plus camera perspective pseudo labels yields transferable BEV features and a scalable path toward reduced-label autonomous perception.", "AI": {"tldr": "Two-phase training strategy for BEV road marking segmentation that uses self-supervised pretraining with image pseudo-labels and temporal consistency, then supervised fine-tuning with only 50% of data, outperforming fully supervised baselines while reducing annotation needs and training time.", "motivation": "Current multi-camera BEV semantic mapping methods depend on costly and inconsistently annotated BEV ground truth, creating a need for more scalable approaches that reduce annotation requirements while maintaining or improving performance.", "method": "Two-phase approach: 1) Self-supervised pretraining where BEVFormer predictions are reprojected to image plane and trained against Mask2Former-generated semantic pseudo-labels with temporal consistency loss, 2) Supervised fine-tuning using only 50% of dataset with rich priors from pretraining.", "result": "Achieves up to +2.5pp mIoU improvement over fully supervised baseline on nuScenes, halves annotation data usage, reduces total training time by up to two-thirds, and demonstrates transferable BEV features.", "conclusion": "Differentiable reprojection plus camera perspective pseudo labels provides a scalable path toward reduced-label autonomous perception with improved BEV segmentation quality while significantly reducing annotation requirements and training time."}}
{"id": "2602.18083", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18083", "abs": "https://arxiv.org/abs/2602.18083", "authors": ["Ioannis Kontogiorgakis", "Athanasios Askitopoulos", "Iason Tsardanidis", "Dimitrios Bormpoudakis", "Ilias Tsoumas", "Fotios Balampanis", "Charalampos Kontoes"], "title": "Comparative Assessment of Multimodal Earth Observation Data for Soil Moisture Estimation", "comment": "This paper has been submitted to IEEE IGARSS 2026", "summary": "Accurate soil moisture (SM) estimation is critical for precision agriculture, water resources management and climate monitoring. Yet, existing satellite SM products are too coarse (>1km) for farm-level applications. We present a high-resolution (10m) SM estimation framework for vegetated areas across Europe, combining Sentinel-1 SAR, Sentinel-2 optical imagery and ERA-5 reanalysis data through machine learning. Using 113 International Soil Moisture Network (ISMN) stations spanning diverse vegetated areas, we compare modality combinations with temporal parameterizations, using spatial cross-validation, to ensure geographic generalization. We also evaluate whether foundation model embeddings from IBM-NASA's Prithvi model improve upon traditional hand-crafted spectral features. Results demonstrate that hybrid temporal matching - Sentinel-2 current-day acquisitions with Sentinel-1 descending orbit - achieves R^2=0.514, with 10-day ERA5 lookback window improving performance to R^2=0.518. Foundation model (Prithvi) embeddings provide negligible improvement over hand-crafted features (R^2=0.515 vs. 0.514), indicating traditional feature engineering remains highly competitive for sparse-data regression tasks. Our findings suggest that domain-specific spectral indices combined with tree-based ensemble methods offer a practical and computationally efficient solution for operational pan-European field-scale soil moisture monitoring.", "AI": {"tldr": "High-resolution (10m) soil moisture estimation framework for Europe using Sentinel-1 SAR, Sentinel-2 optical, and ERA-5 data with machine learning, achieving R\u00b2~0.52, showing traditional spectral features remain competitive with foundation model embeddings.", "motivation": "Existing satellite soil moisture products are too coarse (>1km) for farm-level applications in precision agriculture, water resources management, and climate monitoring, creating a need for high-resolution (10m) estimation.", "method": "Combined Sentinel-1 SAR, Sentinel-2 optical imagery, and ERA-5 reanalysis data through machine learning. Used 113 ISMN stations for validation with spatial cross-validation. Compared modality combinations with temporal parameterizations and evaluated foundation model embeddings (IBM-NASA's Prithvi) vs traditional hand-crafted spectral features.", "result": "Hybrid temporal matching (Sentinel-2 current-day with Sentinel-1 descending orbit) achieved R\u00b2=0.514, with 10-day ERA5 lookback improving to R\u00b2=0.518. Foundation model embeddings provided negligible improvement (R\u00b2=0.515 vs 0.514), showing traditional features remain competitive for sparse-data regression tasks.", "conclusion": "Domain-specific spectral indices combined with tree-based ensemble methods offer a practical and computationally efficient solution for operational pan-European field-scale soil moisture monitoring, with traditional feature engineering remaining highly effective."}}
{"id": "2602.17778", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.17778", "abs": "https://arxiv.org/abs/2602.17778", "authors": ["Zachary Coalson", "Bo Fang", "Sanghyun Hong"], "title": "Asking Forever: Universal Activations Behind Turn Amplification in Conversational LLMs", "comment": "Pre-print", "summary": "Multi-turn interaction length is a dominant factor in the operational costs of conversational LLMs. In this work, we present a new failure mode in conversational LLMs: turn amplification, in which a model consistently prolongs multi-turn interactions without completing the underlying task. We show that an adversary can systematically exploit clarification-seeking behavior$-$commonly encouraged in multi-turn conversation settings$-$to scalably prolong interactions. Moving beyond prompt-level behaviors, we take a mechanistic perspective and identify a query-independent, universal activation subspace associated with clarification-seeking responses. Unlike prior cost-amplification attacks that rely on per-turn prompt optimization, our attack arises from conversational dynamics and persists across prompts and tasks. We show that this mechanism provides a scalable pathway to induce turn amplification: both supply-chain attacks via fine-tuning and runtime attacks through low-level parameter corruptions consistently shift models toward abstract, clarification-seeking behavior across prompts. Across multiple instruction-tuned LLMs and benchmarks, our attack substantially increases turn count while remaining compliant. We also show that existing defenses offer limited protection against this emerging class of failures.", "AI": {"tldr": "Researchers identify \"turn amplification\" as a new failure mode in conversational LLMs where models systematically prolong interactions without completing tasks, which adversaries can exploit to increase operational costs.", "motivation": "Multi-turn interaction length significantly impacts operational costs of conversational LLMs, and there's a need to understand systematic failure modes that adversaries could exploit to artificially inflate these costs.", "method": "The researchers take a mechanistic perspective to identify a universal activation subspace associated with clarification-seeking responses. They demonstrate two attack vectors: supply-chain attacks via fine-tuning and runtime attacks through low-level parameter corruptions, both of which shift models toward abstract, clarification-seeking behavior.", "result": "Across multiple instruction-tuned LLMs and benchmarks, the attack substantially increases turn count while remaining compliant. Existing defenses offer limited protection against this emerging class of failures.", "conclusion": "Turn amplification represents a new, scalable failure mode in conversational LLMs that exploits clarification-seeking behavior, persists across prompts and tasks, and can be induced through both fine-tuning and parameter corruption attacks, highlighting a significant vulnerability in current systems."}}
{"id": "2602.17888", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17888", "abs": "https://arxiv.org/abs/2602.17888", "authors": ["Sayeed Shafayet Chowdhury", "Karen D'Souza", "V. Siva Kakumani", "Snehasis Mukhopadhyay", "Shiaofen Fang", "Rodney J. Schlosser", "Daniel M. Beswick", "Jeremiah A. Alt", "Jess C. Mace", "Zachary M. Soler", "Timothy L. Smith", "Vijay R. Ramakrishnan"], "title": "Machine Learning Based Prediction of Surgical Outcomes in Chronic Rhinosinusitis from Clinical Data", "comment": null, "summary": "Artificial intelligence (AI) has increasingly transformed medical prognostics by enabling rapid and accurate analysis across imaging and pathology. However, the investigation of machine learning predictions applied to prospectively collected, standardized data from observational clinical intervention trials remains underexplored, despite its potential to reduce costs and improve patient outcomes. Chronic rhinosinusitis (CRS), a persistent inflammatory disease of the paranasal sinuses lasting more than three months, imposes a substantial burden on quality of life (QoL) and societal cost. Although many patients respond to medical therapy, others with refractory symptoms often pursue surgical intervention. Surgical decision-making in CRS is complex, as it must weigh known procedural risks against uncertain individualized outcomes. In this study, we evaluated supervised machine learning models for predicting surgical benefit in CRS, using the Sino-Nasal Outcome Test-22 (SNOT-22) as the primary patient-reported outcome. Our prospectively collected cohort from an observational intervention trial comprised patients who all underwent surgery; we investigated whether models trained only on preoperative data could identify patients who might not have been recommended surgery prior to the procedure. Across multiple algorithms, including an ensemble approach, our best model achieved approximately 85% classification accuracy, providing accurate and interpretable predictions of surgical candidacy. Moreover, on a held-out set of 30 cases spanning mixed difficulty, our model achieved 80% accuracy, exceeding the average prediction accuracy of expert clinicians (75.6%), demonstrating its potential to augment clinical decision-making and support personalized CRS care.", "AI": {"tldr": "ML models predict surgical benefit in chronic rhinosinusitis with 85% accuracy using preoperative data, outperforming expert clinicians (75.6%) on held-out cases.", "motivation": "While AI has transformed medical prognostics, its application to prospectively collected data from clinical intervention trials remains underexplored. Chronic rhinosinusitis (CRS) imposes substantial quality of life burden, and surgical decision-making is complex due to uncertain individualized outcomes. There's a need for better tools to predict surgical benefit in CRS patients.", "method": "Supervised machine learning models were trained on prospectively collected cohort data from an observational intervention trial. Models used only preoperative data to predict surgical benefit, with Sino-Nasal Outcome Test-22 (SNOT-22) as the primary outcome measure. Multiple algorithms were tested including an ensemble approach.", "result": "Best model achieved approximately 85% classification accuracy in predicting surgical candidacy. On a held-out set of 30 mixed-difficulty cases, the model achieved 80% accuracy, exceeding the average prediction accuracy of expert clinicians (75.6%). Models could identify patients who might not have been recommended surgery prior to the procedure.", "conclusion": "Machine learning models can provide accurate and interpretable predictions of surgical benefit in CRS using only preoperative data, outperforming expert clinicians. This demonstrates potential to augment clinical decision-making and support personalized CRS care by identifying patients who may not benefit from surgery."}}
{"id": "2602.18089", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18089", "abs": "https://arxiv.org/abs/2602.18089", "authors": ["Kunwar Arpit Singh", "Ankush Prakash", "Haroon R Lone"], "title": "DohaScript: A Large-Scale Multi-Writer Dataset for Continuous Handwritten Hindi Text", "comment": null, "summary": "Despite having hundreds of millions of speakers, handwritten Devanagari text remains severely underrepresented in publicly available benchmark datasets. Existing resources are limited in scale, focus primarily on isolated characters or short words, and lack controlled lexical content and writer level diversity, which restricts their utility for modern data driven handwriting analysis. As a result, they fail to capture the continuous, fused, and structurally complex nature of Devanagari handwriting, where characters are connected through a shared shirorekha (horizontal headline) and exhibit rich ligature formations. We introduce DohaScript, a large scale, multi writer dataset of handwritten Hindi text collected from 531 unique contributors. The dataset is designed as a parallel stylistic corpus, in which all writers transcribe the same fixed set of six traditional Hindi dohas (couplets). This controlled design enables systematic analysis of writer specific variation independent of linguistic content, and supports tasks such as handwriting recognition, writer identification, style analysis, and generative modeling. The dataset is accompanied by non identifiable demographic metadata, rigorous quality curation based on objective sharpness and resolution criteria, and page level layout difficulty annotations that facilitate stratified benchmarking. Baseline experiments demonstrate clear quality separation and strong generalization to unseen writers, highlighting the dataset's reliability and practical value. DohaScript is intended to serve as a standardized and reproducible benchmark for advancing research on continuous handwritten Devanagari text in low resource script settings.", "AI": {"tldr": "DohaScript is a large-scale, multi-writer dataset of handwritten Hindi text from 531 contributors, featuring controlled lexical content (six traditional dohas) to enable systematic analysis of writer variation independent of linguistic content.", "motivation": "Handwritten Devanagari text is severely underrepresented in public benchmark datasets. Existing resources are limited in scale, focus on isolated characters/short words, lack controlled lexical content and writer diversity, and fail to capture the continuous, fused, and structurally complex nature of Devanagari handwriting with shared shirorekha and ligature formations.", "method": "Created a parallel stylistic corpus where all 531 writers transcribed the same fixed set of six traditional Hindi dohas (couplets). Includes non-identifiable demographic metadata, rigorous quality curation based on objective sharpness/resolution criteria, and page-level layout difficulty annotations for stratified benchmarking.", "result": "Baseline experiments demonstrate clear quality separation and strong generalization to unseen writers, highlighting the dataset's reliability and practical value. The dataset supports handwriting recognition, writer identification, style analysis, and generative modeling tasks.", "conclusion": "DohaScript serves as a standardized, reproducible benchmark for advancing research on continuous handwritten Devanagari text in low-resource script settings, addressing the critical gap in publicly available Devanagari handwriting datasets."}}
{"id": "2602.17783", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17783", "abs": "https://arxiv.org/abs/2602.17783", "authors": ["Xiangyu Sun", "Shirin Hosseinmardi", "Amin Yousefpour", "Ramin Bostanabad"], "title": "Multi-material Multi-physics Topology Optimization with Physics-informed Gaussian Process Priors", "comment": null, "summary": "Machine learning (ML) has been increasingly used for topology optimization (TO). However, most existing ML-based approaches focus on simplified benchmark problems due to their high computational cost, spectral bias, and difficulty in handling complex physics. These limitations become more pronounced in multi-material, multi-physics problems whose objective or constraint functions are not self-adjoint. To address these challenges, we propose a framework based on physics-informed Gaussian processes (PIGPs). In our approach, the primary, adjoint, and design variables are represented by independent GP priors whose mean functions are parametrized via neural networks whose architectures are particularly beneficial for surrogate modeling of PDE solutions. We estimate all parameters of our model simultaneously by minimizing a loss that is based on the objective function, multi-physics potential energy functionals, and design-constraints. We demonstrate the capability of the proposed framework on benchmark TO problems such as compliance minimization, heat conduction optimization, and compliant mechanism design under single- and multi-material settings. Additionally, we leverage thermo-mechanical TO with single- and multi-material options as a representative multi-physics problem. We also introduce differentiation and integration schemes that dramatically accelerate the training process. Our results demonstrate that the proposed PIGP framework can effectively solve coupled multi-physics and design problems simultaneously -- generating super-resolution topologies with sharp interfaces and physically interpretable material distributions. We validate these results using open-source codes and the commercial software package COMSOL.", "AI": {"tldr": "A physics-informed Gaussian process framework for topology optimization that simultaneously solves multi-physics problems with sharp interfaces and super-resolution topologies.", "motivation": "Existing ML-based topology optimization methods struggle with high computational costs, spectral bias, and handling complex multi-material, multi-physics problems with non-self-adjoint objective/constraint functions.", "method": "Proposes a physics-informed Gaussian process (PIGP) framework where primary, adjoint, and design variables are represented by independent GP priors with neural network-parameterized mean functions. All parameters are estimated simultaneously by minimizing a loss based on objective function, multi-physics potential energy functionals, and design constraints. Includes differentiation and integration schemes to accelerate training.", "result": "Successfully demonstrates the framework on benchmark TO problems (compliance minimization, heat conduction optimization, compliant mechanism design) under single- and multi-material settings, and thermo-mechanical TO as a representative multi-physics problem. Generates super-resolution topologies with sharp interfaces and physically interpretable material distributions.", "conclusion": "The PIGP framework effectively solves coupled multi-physics and design problems simultaneously, validated using open-source codes and COMSOL commercial software."}}
{"id": "2602.17930", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17930", "abs": "https://arxiv.org/abs/2602.17930", "authors": ["Narjes Nourzad", "Carlee Joe-Wong"], "title": "MIRA: Memory-Integrated Reinforcement Learning Agent with Limited LLM Guidance", "comment": "International Conference on Learning Representations (ICLR'26)", "summary": "Reinforcement learning (RL) agents often suffer from high sample complexity in sparse or delayed reward settings due to limited prior structure. Large language models (LLMs) can provide subgoal decompositions, plausible trajectories, and abstract priors that facilitate early learning. However, heavy reliance on LLM supervision introduces scalability constraints and dependence on potentially unreliable signals. We propose MIRA (Memory-Integrated Reinforcement Learning Agent), which incorporates a structured, evolving memory graph to guide early training. The graph stores decision-relevant information, including trajectory segments and subgoal structures, and is constructed from both the agent's high-return experiences and LLM outputs. This design amortizes LLM queries into a persistent memory rather than requiring continuous real-time supervision. From this memory graph, we derive a utility signal that softly adjusts advantage estimation to influence policy updates without modifying the underlying reward function. As training progresses, the agent's policy gradually surpasses the initial LLM-derived priors, and the utility term decays, preserving standard convergence guarantees. We provide theoretical analysis showing that utility-based shaping improves early-stage learning in sparse-reward environments. Empirically, MIRA outperforms RL baselines and achieves returns comparable to approaches that rely on frequent LLM supervision, while requiring substantially fewer online LLM queries. Project webpage: https://narjesno.github.io/MIRA/", "AI": {"tldr": "MIRA integrates a structured memory graph with RL to reduce LLM dependency while improving sample efficiency in sparse-reward environments.", "motivation": "RL agents struggle with high sample complexity in sparse/delayed reward settings, and while LLMs can provide helpful priors, continuous LLM supervision is impractical and unreliable.", "method": "MIRA uses a structured, evolving memory graph that stores high-return experiences and LLM outputs (subgoals, trajectories). From this graph, it derives a utility signal that softly adjusts advantage estimation to guide early policy updates without modifying rewards.", "result": "MIRA outperforms RL baselines and achieves returns comparable to approaches using frequent LLM supervision, while requiring substantially fewer online LLM queries.", "conclusion": "The memory graph amortizes LLM queries into persistent memory, allowing agents to surpass initial LLM priors while preserving convergence guarantees, making RL more practical in sparse-reward settings."}}
{"id": "2602.18093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18093", "abs": "https://arxiv.org/abs/2602.18093", "authors": ["Hanshuai Cui", "Zhiqing Tang", "Qianli Ma", "Zhi Yao", "Weijia Jia"], "title": "Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers", "comment": null, "summary": "Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \\textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation.", "AI": {"tldr": "PrediT is a training-free acceleration framework for Diffusion Transformers that uses linear multistep methods to predict future model outputs instead of naive feature reuse, achieving up to 5.54\u00d7 latency reduction with minimal quality degradation.", "motivation": "Diffusion Transformers (DiT) have high computational costs due to iterative denoising. Existing training-free acceleration methods rely on feature caching and reuse, but this can cause latent drift and visual degradation when features are reused for multiple steps.", "method": "PrediT formulates feature prediction as a linear multistep problem, using classical linear multistep methods to forecast future model outputs from historical information. It includes a corrector that activates in high-dynamics regions to prevent error accumulation, and a dynamic step modulation mechanism that adaptively adjusts the prediction horizon by monitoring feature change rates.", "result": "The method achieves up to 5.54\u00d7 latency reduction across various DiT-based image and video generation models while incurring negligible quality degradation, validated through extensive experiments.", "conclusion": "PrediT enables substantial acceleration of Diffusion Transformers while preserving generation fidelity, offering an effective training-free solution to the computational bottleneck of iterative denoising processes."}}
{"id": "2602.17832", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17832", "abs": "https://arxiv.org/abs/2602.17832", "authors": ["Hang Liu", "Sangli Teng", "Maani Ghaffari"], "title": "MePoly: Max Entropy Polynomial Policy Optimization", "comment": null, "summary": "Stochastic Optimal Control provides a unified mathematical framework for solving complex decision-making problems, encompassing paradigms such as maximum entropy reinforcement learning(RL) and imitation learning(IL). However, conventional parametric policies often struggle to represent the multi-modality of the solutions. Though diffusion-based policies are aimed at recovering the multi-modality, they lack an explicit probability density, which complicates policy-gradient optimization. To bridge this gap, we propose MePoly, a novel policy parameterization based on polynomial energy-based models. MePoly provides an explicit, tractable probability density, enabling exact entropy maximization. Theoretically, we ground our method in the classical moment problem, leveraging the universal approximation capabilities for arbitrary distributions. Empirically, we demonstrate that MePoly effectively captures complex non-convex manifolds and outperforms baselines in performance across diverse benchmarks.", "AI": {"tldr": "MePoly introduces polynomial energy-based models for stochastic optimal control, providing explicit probability densities to capture multi-modal solutions while enabling exact entropy maximization.", "motivation": "Conventional parametric policies struggle with multi-modality in decision-making problems, while diffusion-based policies lack explicit probability densities, complicating policy-gradient optimization.", "method": "MePoly uses polynomial energy-based models for policy parameterization, grounded in the classical moment problem, providing explicit, tractable probability densities for exact entropy maximization.", "result": "MePoly effectively captures complex non-convex manifolds and outperforms baselines across diverse benchmarks in stochastic optimal control tasks.", "conclusion": "MePoly bridges the gap between multi-modal representation and tractable optimization by providing explicit probability densities through polynomial energy-based models, enabling better performance in complex decision-making problems."}}
{"id": "2602.17798", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17798", "abs": "https://arxiv.org/abs/2602.17798", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Grassmannian Mixture-of-Experts: Concentration-Controlled Routing on Subspace Manifolds", "comment": null, "summary": "Mixture-of-Experts models rely on learned routers to assign tokens to experts, yet standard softmax gating provides no principled mechanism to control the tradeoff between sparsity and utilization. We propose Grassmannian MoE (GrMoE), a routing framework that operates on the Grassmannian manifold of subspaces, where gating weights arise from the concentration parameters of Matrix Bingham distributions. This construction yields a single, interpretable knob -- the concentration matrix $\u039b$ -- that continuously controls routing entropy, replacing discrete top-$k$ selection with a smooth, geometrically principled sparsity mechanism. We further develop an amortized variational inference procedure for posterior routing distributions, enabling uncertainty-aware expert assignment that naturally resists expert collapse. We formally prove tight bounds relating the Bingham concentration spectrum to routing entropy, expected top-$k$ mass, and an exponential bound on expert collapse, establishing the first formal theory of concentration-controlled sparsity. On synthetic routing tasks, a 350M-parameter MoE language model with 8 experts, a 1.3B-parameter model with 16 experts, and a 2.7B-parameter model with 32 experts, GrMoE achieves 0\\% routing collapse across all seeds, comparable or better perplexity with 15--30\\% improved load balance, and a smooth monotonic relationship between concentration and effective sparsity that enables post-hoc sparsity tuning without retraining. Token-level analysis reveals that experts learn heterogeneous concentration values that correlate with linguistic specialization, providing interpretable routing behavior.", "AI": {"tldr": "GrMoE introduces a Grassmannian manifold routing framework using Matrix Bingham distributions to control sparsity via concentration parameters, replacing top-k selection with continuous sparsity control and preventing expert collapse.", "motivation": "Standard softmax gating in Mixture-of-Experts models lacks principled mechanisms to control the tradeoff between sparsity and utilization, and suffers from expert collapse issues.", "method": "Proposes Grassmannian MoE (GrMoE) operating on Grassmannian manifold of subspaces, using Matrix Bingham distributions where gating weights come from concentration parameters. Develops amortized variational inference for posterior routing distributions and proves formal bounds relating concentration spectrum to routing metrics.", "result": "Achieves 0% routing collapse across all seeds, comparable/better perplexity with 15-30% improved load balance, smooth monotonic relationship between concentration and sparsity enabling post-hoc tuning, and interpretable routing where experts learn heterogeneous concentration values correlating with linguistic specialization.", "conclusion": "GrMoE provides a geometrically principled routing framework with continuous sparsity control, formal theoretical guarantees, and practical benefits including collapse prevention, improved load balancing, and interpretable routing behavior."}}
{"id": "2602.17931", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17931", "abs": "https://arxiv.org/abs/2602.17931", "authors": ["Narjes Nourzad", "Carlee Joe-Wong"], "title": "Memory-Based Advantage Shaping for LLM-Guided Reinforcement Learning", "comment": "Association for the Advancement of Artificial Intelligence (AAAI)", "summary": "In environments with sparse or delayed rewards, reinforcement learning (RL) incurs high sample complexity due to the large number of interactions needed for learning. This limitation has motivated the use of large language models (LLMs) for subgoal discovery and trajectory guidance. While LLMs can support exploration, frequent reliance on LLM calls raises concerns about scalability and reliability. We address these challenges by constructing a memory graph that encodes subgoals and trajectories from both LLM guidance and the agent's own successful rollouts. From this graph, we derive a utility function that evaluates how closely the agent's trajectories align with prior successful strategies. This utility shapes the advantage function, providing the critic with additional guidance without altering the reward. Our method relies primarily on offline input and only occasional online queries, avoiding dependence on continuous LLM supervision. Preliminary experiments in benchmark environments show improved sample efficiency and faster early learning compared to baseline RL methods, with final returns comparable to methods that require frequent LLM interaction.", "AI": {"tldr": "LLM-guided RL with memory graph for subgoal discovery improves sample efficiency in sparse-reward environments while reducing LLM dependency.", "motivation": "RL suffers from high sample complexity in sparse/delayed reward environments. While LLMs can help with subgoal discovery and trajectory guidance, frequent LLM calls raise scalability and reliability concerns.", "method": "Construct a memory graph encoding subgoals and trajectories from both LLM guidance and agent's successful rollouts. Derive a utility function from this graph to evaluate trajectory alignment with prior successful strategies. Use this utility to shape the advantage function, providing critic guidance without altering rewards. Relies primarily on offline input with only occasional online LLM queries.", "result": "Preliminary experiments in benchmark environments show improved sample efficiency and faster early learning compared to baseline RL methods, with final returns comparable to methods requiring frequent LLM interaction.", "conclusion": "The proposed approach effectively leverages LLM guidance for exploration while addressing scalability concerns through a memory-based utility function, achieving good performance with reduced LLM dependency."}}
{"id": "2602.18094", "categories": ["cs.CV", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.18094", "abs": "https://arxiv.org/abs/2602.18094", "authors": ["Ling Lin", "Yang Bai", "Heng Su", "Congcong Zhu", "Yaoxing Wang", "Yang Zhou", "Huazhu Fu", "Jingrun Chen"], "title": "OODBench: Out-of-Distribution Benchmark for Large Vision-Language Models", "comment": "54 pages, 21 figures", "summary": "Existing Visual-Language Models (VLMs) have achieved significant progress by being trained on massive-scale datasets, typically under the assumption that data are independent and identically distributed (IID). However, in real-world scenarios, it is often impractical to expect that all data processed by an AI system satisfy this assumption. Furthermore, failure to appropriately handle out-of-distribution (OOD) objects may introduce safety risks in real-world applications (e.g., autonomous driving or medical assistance). Unfortunately, current research has not yet provided valid benchmarks that can comprehensively assess the performance of VLMs in response to OOD data. Therefore, we propose OODBench, a predominantly automated method with minimal human verification, for constructing new benchmarks and evaluating the ability of VLMs to process OOD data. OODBench contains 40K instance-level OOD instance-category pairs, and we show that current VLMs still exhibit notable performance degradation on OODBench, even when the underlying image categories are common. In addition, we propose a reliable automated assessment metric that employs a Basic-to-Advanced Progression of prompted questions to assess the impact of OOD data on questions of varying difficulty more fully. Lastly, we summarize substantial findings and insights to facilitate future research in the acquisition and evaluation of OOD data.", "AI": {"tldr": "OODBench: A new automated benchmark for evaluating Visual-Language Models' performance on out-of-distribution data, revealing significant performance degradation even on common categories.", "motivation": "Current VLMs are trained on IID data assumptions, but real-world applications often encounter OOD data which poses safety risks. Existing research lacks comprehensive benchmarks to assess VLM performance on OOD data.", "method": "Proposed OODBench - an automated method with minimal human verification to construct benchmarks for OOD evaluation. Contains 40K instance-level OOD instance-category pairs and introduces a Basic-to-Advanced Progression metric using prompted questions to assess OOD impact across difficulty levels.", "result": "Current VLMs show notable performance degradation on OODBench, even when underlying image categories are common. The automated assessment metric effectively evaluates OOD impact across varying question difficulties.", "conclusion": "OODBench provides a comprehensive framework for evaluating VLM performance on OOD data, revealing current limitations and offering insights to guide future research in OOD data acquisition and evaluation."}}
{"id": "2602.17997", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17997", "abs": "https://arxiv.org/abs/2602.17997", "authors": ["Zehao Jin", "Yaoye Zhu", "Chen Zhang", "Yanan Sui"], "title": "Whole-Brain Connectomic Graph Model Enables Whole-Body Locomotion Control in Fruit Fly", "comment": null, "summary": "Whole-brain biological neural networks naturally support the learning and control of whole-body movements. However, the use of brain connectomes as neural network controllers in embodied reinforcement learning remains unexplored. We investigate using the exact neural architecture of an adult fruit fly's brain for the control of its body movement. We develop Fly-connectomic Graph Model (FlyGM), whose static structure is identical to the complete connectome of an adult Drosophila for whole-body locomotion control. To perform dynamical control, FlyGM represents the static connectome as a directed message-passing graph to impose a biologically grounded information flow from sensory inputs to motor outputs. Integrated with a biomechanical fruit fly model, our method achieves stable control across diverse locomotion tasks without task-specific architectural tuning. To verify the structural advantages of the connectome-based model, we compare it against a degree-preserving rewired graph, a random graph, and multilayer perceptrons, showing that FlyGM yields higher sample efficiency and superior performance. This work demonstrates that static brain connectomes can be transformed to instantiate effective neural policy for embodied learning of movement control.", "AI": {"tldr": "Researchers developed FlyGM, a neural network controller using the exact brain connectome of a fruit fly for whole-body locomotion control, showing it outperforms alternative architectures in embodied reinforcement learning.", "motivation": "Brain connectomes naturally support learning and control of whole-body movements, but their use as neural network controllers in embodied reinforcement learning remains unexplored. The researchers wanted to investigate whether the exact neural architecture of an adult fruit fly's brain could effectively control body movement.", "method": "Developed Fly-connectomic Graph Model (FlyGM) with static structure identical to the complete connectome of an adult Drosophila. Represented the static connectome as a directed message-passing graph to impose biologically grounded information flow from sensory inputs to motor outputs. Integrated with a biomechanical fruit fly model and compared against degree-preserving rewired graphs, random graphs, and multilayer perceptrons.", "result": "FlyGM achieved stable control across diverse locomotion tasks without task-specific architectural tuning. It demonstrated higher sample efficiency and superior performance compared to alternative architectures, showing that static brain connectomes can be transformed into effective neural policies.", "conclusion": "Static brain connectomes can be successfully transformed to instantiate effective neural policies for embodied learning of movement control, demonstrating the structural advantages of biologically-inspired architectures over artificial alternatives."}}
{"id": "2602.17809", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17809", "abs": "https://arxiv.org/abs/2602.17809", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Calibrated Adaptation: Bayesian Stiefel Manifold Priors for Reliable Parameter-Efficient Fine-Tuning", "comment": null, "summary": "Parameter-efficient fine-tuning methods such as LoRA enable practical adaptation of large language models but provide no principled uncertainty estimates, leading to poorly calibrated predictions and unreliable behavior under domain shift. We introduce Stiefel-Bayes Adapters (SBA), a Bayesian PEFT framework that places a Matrix Langevin prior over orthonormal adapter factors on the Stiefel manifold $\\St$ and performs approximate posterior inference via tangent space Laplace approximation with geodesic retraction. Unlike Gaussian priors in flat space projected onto orthogonality constraints, our prior on the manifold naturally encodes the inductive bias that adapter subspaces should be well conditioned and orthogonal, while the posterior provides calibrated predictive uncertainty without recalibration. We prove formally that the tangent space approximation strictly avoids the structural variance inflation inherent in projecting from ambient space, establishing a rigorous theoretical advantage for intrinsic manifold inference. Across GLUE and SuperGLUE benchmarks on RoBERTa-large, LLaMA-2-7B, LLaMA-2-13B, Mistral-7B, and Qwen2.5-7B, domain shift evaluations, selective prediction protocols, and an abstractive summarization task, SBA achieves task performance comparable to LoRA and DoRA while reducing Expected Calibration Error by 18 to 34\\% over deterministic baselines, improving selective prediction AUROC by 12 to 25\\% under domain shift, and outperforming deep ensembles of five LoRA models on OOD detection at a fraction of the parameter cost. Our results demonstrate that where you place uncertainty, on the right geometric structure, matters more than simply adding any Bayesian treatment to adapters.", "AI": {"tldr": "SBA is a Bayesian PEFT framework that places Matrix Langevin priors on orthonormal adapter factors on the Stiefel manifold, providing calibrated uncertainty estimates without sacrificing task performance.", "motivation": "Current parameter-efficient fine-tuning methods like LoRA lack principled uncertainty estimates, leading to poorly calibrated predictions and unreliable behavior under domain shift. There's a need for Bayesian PEFT frameworks that provide calibrated uncertainty while maintaining efficiency.", "method": "Introduces Stiefel-Bayes Adapters (SBA) with Matrix Langevin priors over orthonormal adapter factors on the Stiefel manifold. Uses tangent space Laplace approximation with geodesic retraction for approximate posterior inference, avoiding structural variance inflation from ambient space projections.", "result": "Achieves comparable task performance to LoRA/DoRA while reducing Expected Calibration Error by 18-34%, improving selective prediction AUROC by 12-25% under domain shift, and outperforming deep ensembles of five LoRA models on OOD detection with fewer parameters.", "conclusion": "Placing uncertainty on the right geometric structure (Stiefel manifold) matters more than simply adding Bayesian treatment to adapters. SBA provides calibrated predictive uncertainty without recalibration while maintaining parameter efficiency."}}
{"id": "2602.17934", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17934", "abs": "https://arxiv.org/abs/2602.17934", "authors": ["Simi Job", "Xiaohui Tao", "Taotao Cai", "Haoran Xie", "Jianming Yong"], "title": "Causal Neighbourhood Learning for Invariant Graph Representations", "comment": null, "summary": "Graph data often contain noisy and spurious correlations that mask the true causal relationships, which are essential for enabling graph models to make predictions based on the underlying causal structure of the data. Dependence on spurious connections makes it challenging for traditional Graph Neural Networks (GNNs) to generalize effectively across different graphs. Furthermore, traditional aggregation methods tend to amplify these spurious patterns, limiting model robustness under distribution shifts. To address these issues, we propose Causal Neighbourhood Learning with Graph Neural Networks (CNL-GNN), a novel framework that performs causal interventions on graph structure. CNL-GNN effectively identifies and preserves causally relevant connections and reduces spurious influences through the generation of counterfactual neighbourhoods and adaptive edge perturbation guided by learnable importance masking and an attention-based mechanism. In addition, by combining structural-level interventions with the disentanglement of causal features from confounding factors, the model learns invariant node representations that are robust and generalize well across different graph structures. Our approach improves causal graph learning beyond traditional feature-based methods, resulting in a robust classification model. Extensive experiments on four publicly available datasets, including multiple domain variants of one dataset, demonstrate that CNL-GNN outperforms state-of-the-art GNN models.", "AI": {"tldr": "CNL-GNN is a novel GNN framework that performs causal interventions on graph structure to identify and preserve causally relevant connections while reducing spurious influences, improving robustness and generalization across different graphs.", "motivation": "Graph data often contain noisy and spurious correlations that mask true causal relationships, causing traditional GNNs to rely on spurious connections and limiting their generalization across different graphs. Traditional aggregation methods amplify these spurious patterns, reducing model robustness under distribution shifts.", "method": "CNL-GNN performs causal interventions on graph structure through generation of counterfactual neighbourhoods and adaptive edge perturbation guided by learnable importance masking and attention-based mechanisms. It combines structural-level interventions with disentanglement of causal features from confounding factors to learn invariant node representations.", "result": "Extensive experiments on four publicly available datasets (including multiple domain variants of one dataset) demonstrate that CNL-GNN outperforms state-of-the-art GNN models in classification tasks.", "conclusion": "CNL-GNN improves causal graph learning beyond traditional feature-based methods, resulting in a robust classification model that effectively identifies and preserves causally relevant connections while reducing spurious influences for better generalization across different graph structures."}}
{"id": "2602.18178", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18178", "abs": "https://arxiv.org/abs/2602.18178", "authors": ["Poonam Poonam", "Pere-Pau V\u00e1zquez", "Timo Ropinski"], "title": "Evaluating Graphical Perception Capabilities of Vision Transformers", "comment": null, "summary": "Vision Transformers, ViTs, have emerged as a powerful alternative to convolutional neural networks, CNNs, in a variety of image-based tasks. While CNNs have previously been evaluated for their ability to perform graphical perception tasks, which are essential for interpreting visualizations, the perceptual capabilities of ViTs remain largely unexplored. In this work, we investigate the performance of ViTs in elementary visual judgment tasks inspired by the foundational studies of Cleveland and McGill, which quantified the accuracy of human perception across different visual encodings. Inspired by their study, we benchmark ViTs against CNNs and human participants in a series of controlled graphical perception tasks. Our results reveal that, although ViTs demonstrate strong performance in general vision tasks, their alignment with human-like graphical perception in the visualization domain is limited. This study highlights key perceptual gaps and points to important considerations for the application of ViTs in visualization systems and graphical perceptual modeling.", "AI": {"tldr": "ViTs show strong general vision performance but have limited alignment with human-like graphical perception in visualization tasks compared to CNNs and human participants.", "motivation": "While Vision Transformers (ViTs) have become powerful alternatives to CNNs for image tasks, their perceptual capabilities for graphical perception tasks (essential for interpreting visualizations) remain unexplored, unlike CNNs which have been previously evaluated.", "method": "Benchmark ViTs against CNNs and human participants in controlled graphical perception tasks inspired by Cleveland and McGill's foundational studies that quantified human perception accuracy across different visual encodings.", "result": "ViTs demonstrate strong performance in general vision tasks but show limited alignment with human-like graphical perception in the visualization domain, revealing key perceptual gaps.", "conclusion": "The study highlights important considerations for applying ViTs in visualization systems and graphical perceptual modeling due to their perceptual limitations compared to human perception."}}
{"id": "2602.17827", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17827", "abs": "https://arxiv.org/abs/2602.17827", "authors": ["Pedro Dall'Antonia", "Tiago da Silva", "Daniel Csillag", "Salem Lahlou", "Diego Mesquita"], "title": "Avoid What You Know: Divergent Trajectory Balance for GFlowNets", "comment": "20 pages, under review", "summary": "Generative Flow Networks (GFlowNets) are a flexible family of amortized samplers trained to generate discrete and compositional objects with probability proportional to a reward function. However, learning efficiency is constrained by the model's ability to rapidly explore diverse high-probability regions during training. To mitigate this issue, recent works have focused on incentivizing the exploration of unvisited and valuable states via curiosity-driven search and self-supervised random network distillation, which tend to waste samples on already well-approximated regions of the state space. In this context, we propose Adaptive Complementary Exploration (ACE), a principled algorithm for the effective exploration of novel and high-probability regions when learning GFlowNets. To achieve this, ACE introduces an exploration GFlowNet explicitly trained to search for high-reward states in regions underexplored by the canonical GFlowNet, which learns to sample from the target distribution. Through extensive experiments, we show that ACE significantly improves upon prior work in terms of approximation accuracy to the target distribution and discovery rate of diverse high-reward states.", "AI": {"tldr": "ACE (Adaptive Complementary Exploration) improves GFlowNet training by using a separate exploration GFlowNet to focus on underexplored high-reward regions, avoiding wasted samples on already well-approximated areas.", "motivation": "Current GFlowNet exploration methods waste samples on already well-approximated regions and struggle to efficiently explore diverse high-probability areas during training, limiting learning efficiency.", "method": "ACE introduces an exploration GFlowNet that explicitly searches for high-reward states in regions underexplored by the main GFlowNet, creating complementary exploration focused on novel and valuable areas.", "result": "ACE significantly outperforms prior methods in both approximation accuracy to the target distribution and discovery rate of diverse high-reward states across extensive experiments.", "conclusion": "The proposed ACE algorithm provides a principled solution for effective exploration in GFlowNets by adaptively focusing on complementary underexplored regions, leading to improved training efficiency and performance."}}
{"id": "2602.17941", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17941", "abs": "https://arxiv.org/abs/2602.17941", "authors": ["Simi Job", "Xiaohui Tao", "Taotao Cai", "Haoran Xie", "Jianming Yong", "Xin Wang"], "title": "Optimizing Graph Causal Classification Models: Estimating Causal Effects and Addressing Confounders", "comment": null, "summary": "Graph data is becoming increasingly prevalent due to the growing demand for relational insights in AI across various domains. Organizations regularly use graph data to solve complex problems involving relationships and connections. Causal learning is especially important in this context, since it helps to understand cause-effect relationships rather than mere associations. Since many real-world systems are inherently causal, graphs can efficiently model these systems. However, traditional graph machine learning methods including graph neural networks (GNNs), rely on correlations and are sensitive to spurious patterns and distribution changes. On the other hand, causal models enable robust predictions by isolating true causal factors, thus making them more stable under such shifts. Causal learning also helps in identifying and adjusting for confounders, ensuring that predictions reflect true causal relationships and remain accurate even under interventions. To address these challenges and build models that are robust and causally informed, we propose CCAGNN, a Confounder-Aware causal GNN framework that incorporates causal reasoning into graph learning, supporting counterfactual reasoning and providing reliable predictions in real-world settings. Comprehensive experiments on six publicly available datasets from diverse domains show that CCAGNN consistently outperforms leading state-of-the-art models.", "AI": {"tldr": "CCAGNN is a Confounder-Aware Causal Graph Neural Network framework that integrates causal reasoning into graph learning to address limitations of traditional GNNs that rely on correlations and are sensitive to spurious patterns.", "motivation": "Traditional graph ML methods like GNNs rely on correlations and are sensitive to spurious patterns and distribution changes, while causal models enable robust predictions by isolating true causal factors. Causal learning helps identify and adjust for confounders to ensure predictions reflect true causal relationships.", "method": "Proposes CCAGNN, a Confounder-Aware causal GNN framework that incorporates causal reasoning into graph learning, supporting counterfactual reasoning and providing reliable predictions in real-world settings.", "result": "Comprehensive experiments on six publicly available datasets from diverse domains show that CCAGNN consistently outperforms leading state-of-the-art models.", "conclusion": "CCAGNN addresses the challenges of building robust and causally informed models by integrating causal reasoning into graph learning, demonstrating superior performance across diverse domains."}}
{"id": "2602.18193", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18193", "abs": "https://arxiv.org/abs/2602.18193", "authors": ["Yiran Yang", "Zhaowei Liu", "Yuan Yuan", "Yukun Song", "Xiong Ma", "Yinghao Song", "Xiangji Zeng", "Lu Sun", "Yulu Wang", "Hai Zhou", "Shuai Cui", "Zhaohan Gong", "Jiefei Zhang"], "title": "BLM-Guard: Explainable Multimodal Ad Moderation with Chain-of-Thought and Policy-Aligned Rewards", "comment": "7 pages, 3 figures. To appear in AAAI 2026", "summary": "Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward. A rule-driven ICoT data-synthesis pipeline jump-starts training by generating structured scene descriptions, reasoning chains and labels, cutting annotation costs. Reinforcement learning then refines the model using a composite reward balancing causal coherence with policy adherence. A multitask architecture models intra-modal manipulations (e.g., exaggerated imagery) and cross-modal mismatches (e.g., subtitle-speech drift), boosting robustness. Experiments on real short-video ads show BLM-Guard surpasses strong baselines in accuracy, consistency and generalization.", "AI": {"tldr": "BLM-Guard: A multimodal ad moderation framework using Chain-of-Thought reasoning, rule-based policies, and reinforcement learning to detect deceptive visuals, speech, and subtitles in short-video ads.", "motivation": "Short-video platforms host deceptive multimodal ads that require finer-grained, policy-driven moderation beyond community safety filters. Current approaches lack the sophistication needed to handle complex multimodal manipulations and policy adherence.", "method": "Combines Chain-of-Thought reasoning with rule-based policy principles and critic-guided rewards. Uses a rule-driven ICoT data-synthesis pipeline to generate training data, then applies reinforcement learning with composite rewards balancing causal coherence and policy adherence. Features multitask architecture modeling intra-modal manipulations and cross-modal mismatches.", "result": "BLM-Guard surpasses strong baselines in accuracy, consistency, and generalization on real short-video ads, demonstrating superior performance in multimodal ad moderation.", "conclusion": "BLM-Guard provides an effective framework for policy-driven multimodal ad moderation on short-video platforms, addressing deceptive content through structured reasoning, rule-based policies, and reinforcement learning refinement."}}
{"id": "2602.17829", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17829", "abs": "https://arxiv.org/abs/2602.17829", "authors": ["Preetom Biswas", "Giulia Pedrielli", "K. Sel\u00e7uk Candan"], "title": "Causality by Abstraction: Symbolic Rule Learning in Multivariate Timeseries with Large Language Models", "comment": null, "summary": "Inferring causal relations in timeseries data with delayed effects is a fundamental challenge, especially when the underlying system exhibits complex dynamics that cannot be captured by simple functional mappings. Traditional approaches often fail to produce generalized and interpretable explanations, as multiple distinct input trajectories may yield nearly indistinguishable outputs. In this work, we present ruleXplain, a framework that leverages Large Language Models (LLMs) to extract formal explanations for input-output relations in simulation-driven dynamical systems. Our method introduces a constrained symbolic rule language with temporal operators and delay semantics, enabling LLMs to generate verifiable causal rules through structured prompting. ruleXplain relies on the availability of a principled model (e.g., a simulator) that maps multivariate input time series to output time series. Within ruleXplain, the simulator is used to generate diverse counterfactual input trajectories that yield similar target output, serving as candidate explanations. Such counterfactual inputs are clustered and provided as context to the LLM, which is tasked with the generation of symbolic rules encoding the joint temporal trends responsible for the patterns observable in the output times series. A closed-loop refinement process ensures rule consistency and semantic validity. We validate the framework using the PySIRTEM epidemic simulator, mapping testing rate inputs to daily infection counts; and the EnergyPlus building energy simulator, observing temperature and solar irradiance inputs to electricity needs. For validation, we perform three classes of experiments: (1) the efficacy of the ruleset through input reconstruction; (2) ablation studies evaluating the causal encoding of the ruleset; and (3) generalization tests of the extracted rules across unseen output trends with varying phase dynamics.", "AI": {"tldr": "ruleXplain is a framework that uses LLMs to extract formal, interpretable causal rules from simulation-driven dynamical systems with delayed effects, generating verifiable symbolic explanations for input-output relations.", "motivation": "Traditional approaches fail to provide generalized and interpretable explanations for causal relations in timeseries data with delayed effects, especially when multiple distinct input trajectories produce nearly indistinguishable outputs in complex dynamical systems.", "method": "Uses LLMs with constrained symbolic rule language featuring temporal operators and delay semantics. Generates diverse counterfactual input trajectories from a principled simulator, clusters them, and provides as context to LLMs which generate symbolic rules encoding joint temporal trends. Includes closed-loop refinement for rule consistency.", "result": "Validated using PySIRTEM epidemic simulator (testing rates to infection counts) and EnergyPlus building energy simulator (temperature/solar irradiance to electricity needs). Experiments show efficacy through input reconstruction, causal encoding evaluation, and generalization across unseen output trends with varying phase dynamics.", "conclusion": "ruleXplain successfully extracts formal, interpretable causal explanations from simulation-driven dynamical systems, addressing the challenge of inferring causal relations in timeseries data with delayed effects where traditional methods fail."}}
{"id": "2602.18199", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18199", "abs": "https://arxiv.org/abs/2602.18199", "authors": ["Gahyeon Shim", "Soogeun Park", "Hyemin Ahn"], "title": "A Self-Supervised Approach on Motion Calibration for Enhancing Physical Plausibility in Text-to-Motion", "comment": null, "summary": "Generating semantically aligned human motion from textual descriptions has made rapid progress, but ensuring both semantic and physical realism in motion remains a challenge. In this paper, we introduce the Distortion-aware Motion Calibrator (DMC), a post-hoc module that refines physically implausible motions (e.g., foot floating) while preserving semantic consistency with the original textual description. Rather than relying on complex physical modeling, we propose a self-supervised and data-driven approach, whereby DMC learns to obtain physically plausible motions when an intentionally distorted motion and the original textual descriptions are given as inputs. We evaluate DMC as a post-hoc module to improve motions obtained from various text-to-motion generation models and demonstrate its effectiveness in improving physical plausibility while enhancing semantic consistency. The experimental results show that DMC reduces FID score by 42.74% on T2M and 13.20% on T2M-GPT, while also achieving the highest R-Precision. When applied to high-quality models like MoMask, DMC improves the physical plausibility of motions by reducing penetration by 33.0% as well as adjusting floating artifacts closer to the ground-truth reference. These results highlight that DMC can serve as a promising post-hoc motion refinement framework for any kind of text-to-motion models by incorporating textual semantics and physical plausibility.", "AI": {"tldr": "DMC is a post-hoc module that refines text-generated human motions to improve physical plausibility while preserving semantic alignment with text descriptions.", "motivation": "Existing text-to-motion generation models produce semantically aligned motions but often lack physical realism (e.g., foot floating, penetration issues), creating a need for a solution that ensures both semantic and physical correctness.", "method": "Self-supervised, data-driven approach where DMC learns to generate physically plausible motions from intentionally distorted motions and original text descriptions, without complex physical modeling.", "result": "DMC reduces FID by 42.74% on T2M and 13.20% on T2M-GPT, achieves highest R-Precision, reduces penetration by 33.0% on MoMask, and adjusts floating artifacts closer to ground-truth reference.", "conclusion": "DMC serves as a promising post-hoc motion refinement framework that can enhance any text-to-motion model by incorporating textual semantics and physical plausibility."}}
{"id": "2602.18314", "categories": ["cs.CV", "cs.GR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18314", "abs": "https://arxiv.org/abs/2602.18314", "authors": ["Tianyi Song", "Danail Stoyanov", "Evangelos Mazomenos", "Francisco Vasconcelos"], "title": "Diff2DGS: Reliable Reconstruction of Occluded Surgical Scenes via 2D Gaussian Splatting", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Real-time reconstruction of deformable surgical scenes is vital for advancing robotic surgery, improving surgeon guidance, and enabling automation. Recent methods achieve dense reconstructions from da Vinci robotic surgery videos, with Gaussian Splatting (GS) offering real-time performance via graphics acceleration. However, reconstruction quality in occluded regions remains limited, and depth accuracy has not been fully assessed, as benchmarks like EndoNeRF and StereoMIS lack 3D ground truth. We propose Diff2DGS, a novel two-stage framework for reliable 3D reconstruction of occluded surgical scenes. In the first stage, a diffusion-based video module with temporal priors inpaints tissue occluded by instruments with high spatial-temporal consistency. In the second stage, we adapt 2D Gaussian Splatting (2DGS) with a Learnable Deformation Model (LDM) to capture dynamic tissue deformation and anatomical geometry. We also extend evaluation beyond prior image-quality metrics by performing quantitative depth accuracy analysis on the SCARED dataset. Diff2DGS outperforms state-of-the-art approaches in both appearance and geometry, reaching 38.02 dB PSNR on EndoNeRF and 34.40 dB on StereoMIS. Furthermore, our experiments demonstrate that optimizing for image quality alone does not necessarily translate into optimal 3D reconstruction accuracy. To address this, we further optimize the depth quality of the reconstructed 3D results, ensuring more faithful geometry in addition to high-fidelity appearance.", "AI": {"tldr": "Diff2DGS: A two-stage framework using diffusion-based video inpainting and 2D Gaussian Splatting with learnable deformation for real-time 3D reconstruction of occluded surgical scenes, with improved depth accuracy evaluation.", "motivation": "Real-time reconstruction of deformable surgical scenes is crucial for robotic surgery advancement, but current methods have limited quality in occluded regions and lack proper depth accuracy assessment due to missing 3D ground truth in existing benchmarks.", "method": "Two-stage framework: 1) Diffusion-based video module with temporal priors to inpaint tissue occluded by surgical instruments with spatial-temporal consistency; 2) Adapted 2D Gaussian Splatting with Learnable Deformation Model (LDM) to capture dynamic tissue deformation and anatomical geometry.", "result": "Outperforms state-of-the-art approaches in both appearance and geometry, achieving 38.02 dB PSNR on EndoNeRF and 34.40 dB on StereoMIS. Demonstrates that optimizing for image quality alone doesn't guarantee optimal 3D reconstruction accuracy, leading to additional depth quality optimization.", "conclusion": "Diff2DGS provides reliable 3D reconstruction of occluded surgical scenes with improved depth accuracy, addressing limitations of prior methods through a novel two-stage approach and comprehensive evaluation on the SCARED dataset."}}
{"id": "2602.17976", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17976", "abs": "https://arxiv.org/abs/2602.17976", "authors": ["Alessio Russo", "Yin-Ching Lee", "Ryan Welch", "Aldo Pacchiano"], "title": "In-Context Learning for Pure Exploration in Continuous Spaces", "comment": null, "summary": "In active sequential testing, also termed pure exploration, a learner is tasked with the goal to adaptively acquire information so as to identify an unknown ground-truth hypothesis with as few queries as possible. This problem, originally studied by Chernoff in 1959, has several applications: classical formulations include Best-Arm Identification (BAI) in bandits, where actions index hypotheses, and generalized search problems, where strategically chosen queries reveal partial information about a hidden label. In many modern settings, however, the hypothesis space is continuous and naturally coincides with the query/action space: for example, identifying an optimal action in a continuous-armed bandit, localizing an $\u03b5$-ball contained in a target region, or estimating the minimizer of an unknown function from a sequence of observations. In this work, we study pure exploration in such continuous spaces and introduce Continuous In-Context Pure Exploration for this regime. We introduce C-ICPE-TS, an algorithm that meta-trains deep neural policies to map observation histories to (i) the next continuous query action and (ii) a predicted hypothesis, thereby learning transferable sequential testing strategies directly from data. At inference time, C-ICPE-TS actively gathers evidence on previously unseen tasks and infers the true hypothesis without parameter updates or explicit hand-crafted information models. We validate C-ICPE-TS across a range of benchmarks, spanning continuous best-arm identification, region localization, and function minimizer identification.", "AI": {"tldr": "The paper introduces C-ICPE-TS, a meta-learning approach for continuous pure exploration problems that trains neural policies to map observation histories to continuous query actions and hypothesis predictions, enabling transferable sequential testing strategies without parameter updates at inference time.", "motivation": "Traditional pure exploration problems often have discrete hypothesis spaces, but many modern applications involve continuous hypothesis spaces that coincide with query/action spaces (e.g., continuous-armed bandits, region localization, function minimizer estimation). Existing methods may not efficiently handle these continuous settings.", "method": "C-ICPE-TS meta-trains deep neural policies to learn two functions: (1) mapping observation histories to the next continuous query action, and (2) predicting the hypothesis from accumulated evidence. The approach learns transferable sequential testing strategies directly from data without requiring explicit hand-crafted information models.", "result": "The method is validated across multiple benchmarks including continuous best-arm identification, region localization, and function minimizer identification, demonstrating its effectiveness in continuous pure exploration settings.", "conclusion": "C-ICPE-TS provides a novel approach to continuous pure exploration problems by learning transferable sequential testing strategies through meta-training, enabling efficient hypothesis identification in continuous spaces without parameter updates or explicit information models at inference time."}}
{"id": "2602.18252", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18252", "abs": "https://arxiv.org/abs/2602.18252", "authors": ["Rishika Bhagwatkar", "Irina Rish", "Nicolas Flammarion", "Francesco Croce"], "title": "On the Adversarial Robustness of Discrete Image Tokenizers", "comment": null, "summary": "Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored. Ours being the first work studying this topic, we first formulate attacks that aim to perturb the features extracted by discrete tokenizers, and thus change the extracted tokens. These attacks are computationally efficient, application-agnostic, and effective across classification, multimodal retrieval, and captioning tasks. Second, to defend against this vulnerability, inspired by recent work on robust CLIP encoders, we fine-tune popular tokenizers with unsupervised adversarial training, keeping all other components frozen. While unsupervised and task-agnostic, our approach significantly improves robustness to both unsupervised and end-to-end supervised attacks and generalizes well to unseen tasks and data. Unlike supervised adversarial training, our approach can leverage unlabeled images, making it more versatile. Overall, our work highlights the critical role of tokenizer robustness in downstream tasks and presents an important step in the development of safe multimodal foundation models.", "AI": {"tldr": "First study on adversarial attacks against discrete image tokenizers used in multimodal systems, proposing efficient attacks and unsupervised adversarial training for defense.", "motivation": "Discrete image tokenizers are widely used in multimodal systems but their vulnerability to adversarial attacks hasn't been explored, unlike CLIP encoders. Understanding and addressing this vulnerability is crucial for developing safe multimodal foundation models.", "method": "1) Formulate computationally efficient, application-agnostic attacks that perturb features extracted by discrete tokenizers to change extracted tokens. 2) Defend using unsupervised adversarial training by fine-tuning tokenizers while keeping other components frozen, leveraging unlabeled images.", "result": "Attacks are effective across classification, multimodal retrieval, and captioning tasks. The unsupervised adversarial training approach significantly improves robustness to both unsupervised and end-to-end supervised attacks, generalizes well to unseen tasks and data, and is more versatile than supervised approaches.", "conclusion": "Tokenizer robustness plays a critical role in downstream tasks. This work presents an important step toward developing safe multimodal foundation models by addressing the previously unexplored vulnerability of discrete image tokenizers to adversarial attacks."}}
{"id": "2602.18424", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18424", "abs": "https://arxiv.org/abs/2602.18424", "authors": ["Xia Su", "Ruiqi Chen", "Benlin Liu", "Jingwei Ma", "Zonglin Di", "Ranjay Krishna", "Jon Froehlich"], "title": "CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation", "comment": null, "summary": "Vision-Language Models (VLMs) have shown remarkable progress in Vision-Language Navigation (VLN), offering new possibilities for navigation decision-making that could benefit both robotic platforms and human users. However, real-world navigation is inherently conditioned by the agent's mobility constraints. For example, a sweeping robot cannot traverse stairs, while a quadruped can. We introduce Capability-Conditioned Navigation (CapNav), a benchmark designed to evaluate how well VLMs can navigate complex indoor spaces given an agent's specific physical and operational capabilities. CapNav defines five representative human and robot agents, each described with physical dimensions, mobility capabilities, and environmental interaction abilities. CapNav provides 45 real-world indoor scenes, 473 navigation tasks, and 2365 QA pairs to test if VLMs can traverse indoor environments based on agent capabilities. We evaluate 13 modern VLMs and find that current VLM's navigation performance drops sharply as mobility constraints tighten, and that even state-of-the-art models struggle with obstacle types that require reasoning on spatial dimensions. We conclude by discussing the implications for capability-aware navigation and the opportunities for advancing embodied spatial reasoning in future VLMs. The benchmark is available at https://github.com/makeabilitylab/CapNav", "AI": {"tldr": "CapNav is a new benchmark that tests Vision-Language Models' ability to navigate indoor environments while considering specific agent capabilities and mobility constraints, revealing significant performance drops when constraints tighten.", "motivation": "Real-world navigation requires considering agent-specific mobility constraints (e.g., robots that can't climb stairs), but current VLMs don't adequately account for these physical capabilities when making navigation decisions.", "method": "Created CapNav benchmark with 5 representative agents (human and robot types), each with defined physical dimensions, mobility capabilities, and environmental interactions. Includes 45 real-world indoor scenes, 473 navigation tasks, and 2365 QA pairs to test capability-aware navigation.", "result": "Evaluation of 13 modern VLMs shows navigation performance drops sharply as mobility constraints tighten. Even state-of-the-art models struggle with obstacle types requiring spatial dimension reasoning.", "conclusion": "Current VLMs need improvement in capability-aware navigation. The benchmark highlights opportunities for advancing embodied spatial reasoning in future VLMs for more realistic navigation applications."}}
{"id": "2602.17835", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17835", "abs": "https://arxiv.org/abs/2602.17835", "authors": ["Sirui Chen", "Yunzhe Qi", "Mengting Ai", "Yifan Sun", "Ruizhong Qiu", "Jiaru Zou", "Jingrui He"], "title": "Influence-Preserving Proxies for Gradient-Based Data Selection in LLM Fine-tuning", "comment": null, "summary": "Supervised fine-tuning (SFT) relies critically on selecting training data that most benefits a model's downstream performance. Gradient-based data selection methods such as TracIn and Influence Functions leverage influence to identify useful samples, but their computational cost scales poorly, making them impractical for multi-billion-parameter large language models (LLMs). A common alternative is to use off-the-shelf smaller models as proxies, but they remain suboptimal since their learning dynamics are unclear, their sizes cannot be flexibly adjusted, and they cannot be further aligned with the target model in terms of gradient-based influence estimation. To address these challenges, we introduce Iprox, a two-stage framework that derives influence-preserving proxies directly from the target model. It first applies a low-rank compression stage to preserve influence information of the target model, and then an aligning stage to align both model gradients and logits, thereby constructing proxies that flexibly control computational cost while retaining the target model's influence. Experimental results across diverse LLM families and evaluation tasks show that Iprox consistently outperforms off-the-shelf proxies and baseline methods. On Qwen3-4B, a 1.5B proxy constructed with Iprox achieves stronger performance than the larger 1.7B off-the-shelf proxy. Notably, on Llama3.2, Iprox achieves better performance than baselines while reducing computational cost by more than half relative to the full 3B model. These results show that Iprox provides effective influence-preserving proxies, making gradient-based data selection more scalable for LLMs.", "AI": {"tldr": "Iprox is a two-stage framework that creates influence-preserving proxy models from target LLMs for efficient gradient-based data selection, outperforming off-the-shelf proxies while reducing computational costs.", "motivation": "Current gradient-based data selection methods (TracIn, Influence Functions) are computationally expensive for large LLMs, while off-the-shelf smaller proxies are suboptimal due to unclear learning dynamics, inflexible size adjustment, and inability to align with target model's influence.", "method": "Two-stage framework: 1) Low-rank compression to preserve target model's influence information, 2) Aligning stage to align both model gradients and logits, creating flexible proxies that control computational cost while retaining target model's influence.", "result": "Iprox consistently outperforms off-the-shelf proxies and baselines across diverse LLM families and tasks. On Qwen3-4B, a 1.5B Iprox proxy beats larger 1.7B off-the-shelf proxy. On Llama3.2, achieves better performance than baselines while reducing computational cost by more than half relative to full 3B model.", "conclusion": "Iprox provides effective influence-preserving proxies that make gradient-based data selection more scalable for LLMs, addressing computational bottlenecks while maintaining performance."}}
{"id": "2602.17978", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17978", "abs": "https://arxiv.org/abs/2602.17978", "authors": ["Daqian Shao"], "title": "Learning Optimal and Sample-Efficient Decision Policies with Guarantees", "comment": "A thesis submitted for the degree of DPhil in Computer Science at Oxford", "summary": "The paradigm of decision-making has been revolutionised by reinforcement learning and deep learning. Although this has led to significant progress in domains such as robotics, healthcare, and finance, the use of RL in practice is challenging, particularly when learning decision policies in high-stakes applications that may require guarantees. Traditional RL algorithms rely on a large number of online interactions with the environment, which is problematic in scenarios where online interactions are costly, dangerous, or infeasible. However, learning from offline datasets is hindered by the presence of hidden confounders. Such confounders can cause spurious correlations in the dataset and can mislead the agent into taking suboptimal or adversarial actions. Firstly, we address the problem of learning from offline datasets in the presence of hidden confounders. We work with instrumental variables (IVs) to identify the causal effect, which is an instance of a conditional moment restrictions (CMR) problem. Inspired by double/debiased machine learning, we derive a sample-efficient algorithm for solving CMR problems with convergence and optimality guarantees, which outperforms state-of-the-art algorithms. Secondly, we relax the conditions on the hidden confounders in the setting of (offline) imitation learning, and adapt our CMR estimator to derive an algorithm that can learn effective imitator policies with convergence rate guarantees. Finally, we consider the problem of learning high-level objectives expressed in linear temporal logic (LTL) and develop a provably optimal learning algorithm that improves sample efficiency over existing methods. Through evaluation on reinforcement learning benchmarks and synthetic and semi-synthetic datasets, we demonstrate the usefulness of the methods developed in this thesis in real-world decision making.", "AI": {"tldr": "This thesis develops provably optimal algorithms for offline RL with hidden confounders using instrumental variables and conditional moment restrictions, extends to imitation learning, and improves LTL objective learning.", "motivation": "Traditional RL requires costly online interactions, while offline learning faces challenges from hidden confounders causing spurious correlations and suboptimal decisions in high-stakes applications.", "method": "Uses instrumental variables for causal identification as conditional moment restrictions problem, applies double/debiased ML for sample-efficient algorithm, adapts to imitation learning with relaxed confounder conditions, and develops optimal LTL learning algorithm.", "result": "Developed algorithms outperform state-of-the-art with convergence and optimality guarantees, demonstrate effectiveness on RL benchmarks and synthetic datasets for real-world decision making.", "conclusion": "The thesis provides provably optimal methods for offline RL with hidden confounders, imitation learning, and LTL objective learning, enabling safer and more reliable decision-making in high-stakes applications."}}
{"id": "2602.18282", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18282", "abs": "https://arxiv.org/abs/2602.18282", "authors": ["Shiyan Du", "Conghan Yue", "Xinyu Cheng", "Dongyu Zhang"], "title": "DEIG: Detail-Enhanced Instance Generation with Fine-Grained Semantic Control", "comment": "Accepted by AAAI 2026", "summary": "Multi-Instance Generation has advanced significantly in spatial placement and attribute binding. However, existing approaches still face challenges in fine-grained semantic understanding, particularly when dealing with complex textual descriptions. To overcome these limitations, we propose DEIG, a novel framework for fine-grained and controllable multi-instance generation. DEIG integrates an Instance Detail Extractor (IDE) that transforms text encoder embeddings into compact, instance-aware representations, and a Detail Fusion Module (DFM) that applies instance-based masked attention to prevent attribute leakage across instances. These components enable DEIG to generate visually coherent multi-instance scenes that precisely match rich, localized textual descriptions. To support fine-grained supervision, we construct a high-quality dataset with detailed, compositional instance captions generated by VLMs. We also introduce DEIG-Bench, a new benchmark with region-level annotations and multi-attribute prompts for both humans and objects. Experiments demonstrate that DEIG consistently outperforms existing approaches across multiple benchmarks in spatial consistency, semantic accuracy, and compositional generalization. Moreover, DEIG functions as a plug-and-play module, making it easily integrable into standard diffusion-based pipelines.", "AI": {"tldr": "DEIG is a novel framework for fine-grained and controllable multi-instance generation that addresses challenges in semantic understanding of complex textual descriptions through instance-aware representations and masked attention mechanisms.", "motivation": "Existing multi-instance generation approaches struggle with fine-grained semantic understanding when dealing with complex textual descriptions, particularly in preventing attribute leakage across instances and achieving precise spatial and attribute binding.", "method": "DEIG integrates an Instance Detail Extractor (IDE) that transforms text encoder embeddings into compact, instance-aware representations, and a Detail Fusion Module (DFM) that applies instance-based masked attention to prevent attribute leakage across instances. The framework also uses a high-quality dataset with detailed compositional instance captions generated by VLMs.", "result": "DEIG consistently outperforms existing approaches across multiple benchmarks in spatial consistency, semantic accuracy, and compositional generalization. It also functions as a plug-and-play module that can be easily integrated into standard diffusion-based pipelines.", "conclusion": "DEIG provides an effective solution for fine-grained and controllable multi-instance generation, enabling visually coherent scenes that precisely match rich, localized textual descriptions while preventing attribute leakage across instances."}}
{"id": "2602.17846", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17846", "abs": "https://arxiv.org/abs/2602.17846", "authors": ["Nick Dodson", "Xinyu Gao", "Qingsong Wang", "Yusu Wang", "Zhengchao Wan"], "title": "Two Calm Ends and the Wild Middle: A Geometric Picture of Memorization in Diffusion Models", "comment": null, "summary": "Diffusion models generate high-quality samples but can also memorize training data, raising serious privacy concerns. Understanding the mechanisms governing when memorization versus generalization occurs remains an active area of research. In particular, it is unclear where along the noise schedule memorization is induced, how data geometry influences it, and how phenomena at different noise scales interact. We introduce a geometric framework that partitions the noise schedule into three regimes based on the coverage properties of training data by Gaussian shells and the concentration behavior of the posterior, which we argue are two fundamental objects governing memorization and generalization in diffusion models. This perspective reveals that memorization risk is highly non-uniform across noise levels. We further identify a danger zone at medium noise levels where memorization is most pronounced. In contrast, both the small and large noise regimes resist memorization, but through fundamentally different mechanisms: small noise avoids memorization due to limited training coverage, while large noise exhibits low posterior concentration and admits a provably near linear Gaussian denoising behavior. For the medium noise regime, we identify geometric conditions through which we propose a geometry-informed targeted intervention that mitigates memorization.", "AI": {"tldr": "Diffusion models can memorize training data, creating privacy risks. The paper introduces a geometric framework showing memorization risk varies across noise levels, with medium noise being most dangerous, while small and large noise regimes resist memorization through different mechanisms.", "motivation": "Diffusion models generate high-quality samples but can memorize training data, raising serious privacy concerns. Understanding when memorization versus generalization occurs remains unclear, particularly regarding where along the noise schedule memorization is induced, how data geometry influences it, and how phenomena at different noise scales interact.", "method": "Introduces a geometric framework that partitions the noise schedule into three regimes based on: 1) coverage properties of training data by Gaussian shells, and 2) concentration behavior of the posterior. These are identified as fundamental objects governing memorization and generalization in diffusion models.", "result": "Memorization risk is highly non-uniform across noise levels. A danger zone at medium noise levels where memorization is most pronounced is identified. Small and large noise regimes resist memorization through different mechanisms: small noise avoids memorization due to limited training coverage, while large noise exhibits low posterior concentration and admits provably near linear Gaussian denoising behavior.", "conclusion": "For the medium noise regime, geometric conditions are identified through which a geometry-informed targeted intervention is proposed to mitigate memorization. The framework provides insights into when and why diffusion models memorize versus generalize."}}
{"id": "2602.17993", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17993", "abs": "https://arxiv.org/abs/2602.17993", "authors": ["Mohan Tang", "Sidi Lu"], "title": "Turbo Connection: Reasoning as Information Flow from Higher to Lower Layers", "comment": null, "summary": "Complex problems, whether in math, logic, or planning, are solved by humans through a sequence of steps where the result of one step informs the next. In this work, we adopt the perspective that the reasoning power of Transformers is fundamentally limited by a fixed maximum number of steps along any latent path of computation. To address this, we introduce Turbo Connection (TurboConn), a novel architecture that overcomes the fixed-depth constraint by routing multiple residual connections from the higher-layer hidden states of each token $t$ to the lower layers of token $t+1$. Fine-tuning pre-trained LLMs with our method not only yields accuracy gains of 0.9% to over 10% on benchmarks like GSM8K, Parity, and multi-step arithmetic, but also demonstrates that the density of these backward connections is critical; our dense interaction significantly outperforms \"sparse\" alternatives that only pass a single hidden state or vector. Notably, TurboConn can be integrated into pre-trained LLMs to overcome task-specific plateaus: while a fine-tuned Qwen-3-1.7B achieves only 53.78% on Parity, adding our architectural modification enables the model to reach 100% accuracy, all without the necessity to retrain the full model from scratch or sophisticated curriculum learning. Our results provide strong empirical evidence that the depth of the computational path is a key factor in reasoning ability, also offering a new mechanism to enhance LLMs without significantly affecting generation latency.", "AI": {"tldr": "TurboConn is a novel Transformer architecture that overcomes fixed-depth computation limits by routing higher-layer hidden states of token t to lower layers of token t+1, enabling deeper reasoning paths and significant accuracy improvements on reasoning tasks.", "motivation": "Transformers have fundamental limitations in reasoning due to fixed maximum computation steps along latent paths. Human reasoning involves sequential steps where results inform subsequent steps, but current architectures lack mechanisms for deeper computational paths.", "method": "Introduces Turbo Connection (TurboConn) architecture that routes multiple residual connections from higher-layer hidden states of token t to lower layers of token t+1. This creates dense backward connections allowing deeper computational paths. Can be integrated into pre-trained LLMs through fine-tuning without full retraining.", "result": "Achieves accuracy gains of 0.9% to over 10% on benchmarks (GSM8K, Parity, multi-step arithmetic). Dense connections outperform sparse alternatives. Qwen-3-1.7B with TurboConn reaches 100% accuracy on Parity (vs 53.78% baseline) without full retraining or curriculum learning.", "conclusion": "Computational path depth is crucial for reasoning ability. TurboConn provides an effective mechanism to enhance LLMs' reasoning capabilities without significantly affecting generation latency, offering a practical way to overcome task-specific plateaus."}}
{"id": "2602.18309", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18309", "abs": "https://arxiv.org/abs/2602.18309", "authors": ["Ziyue Liu", "Davide Talon", "Federico Girella", "Zanxi Ruan", "Mattia Mondo", "Loris Bazzani", "Yiming Wang", "Marco Cristani"], "title": "Multi-Level Conditioning by Pairing Localized Text and Sketch for Fashion Image Generation", "comment": "Project page: https://intelligolabs.github.io/lots/", "summary": "Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text. We present LOcalized Text and Sketch with multi-level guidance (LOTS), a framework that enhances fashion image generation by combining global sketch guidance with multiple localized sketch-text pairs. LOTS employs a Multi-level Conditioning Stage to independently encode local features within a shared latent space while maintaining global structural coordination. Then, the Diffusion Pair Guidance stage integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we develop Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Sketchy provides high-quality, clean sketches with a professional look and consistent structure. To assess robustness beyond this setting, we also include an \"in the wild\" split with non-expert sketches, featuring higher variability and imperfections. Experiments demonstrate that our method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. The dataset, platform, and code are publicly available.", "AI": {"tldr": "LOTS is a framework for fashion image generation that combines global sketch guidance with multiple localized sketch-text pairs to enhance structural adherence and semantic guidance.", "motivation": "Sketches provide structural information for fashion design while text conveys material and stylistic details. Existing methods need better integration of both modalities while maintaining sketch structure when using localized text attributes.", "method": "LOTS framework with two stages: 1) Multi-level Conditioning Stage encodes local features in shared latent space with global coordination, 2) Diffusion Pair Guidance stage integrates local and global conditioning via attention-based guidance in diffusion model denoising. Also created Sketchy dataset with multiple text-sketch pairs per image.", "result": "Method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. Created Sketchy dataset with professional and \"in the wild\" sketches for robustness testing.", "conclusion": "LOTS effectively combines textual and visual modalities for fashion image generation by maintaining sketch structure while utilizing localized text guidance, demonstrated through comprehensive experiments and new dataset."}}
{"id": "2602.17998", "categories": ["cs.LG", "cs.AI", "cs.CE", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17998", "abs": "https://arxiv.org/abs/2602.17998", "authors": ["Shubham Bhardwaj", "Chandrajit Bajaj"], "title": "PHAST: Port-Hamiltonian Architecture for Structured Temporal Dynamics Forecasting", "comment": "50 pages", "summary": "Real physical systems are dissipative -- a pendulum slows, a circuit loses charge to heat -- and forecasting their dynamics from partial observations is a central challenge in scientific machine learning. We address the \\emph{position-only} (q-only) problem: given only generalized positions~$q_t$ at discrete times (momenta~$p_t$ latent), learn a structured model that (a)~produces stable long-horizon forecasts and (b)~recovers physically meaningful parameters when sufficient structure is provided. The port-Hamiltonian framework makes the conservative-dissipative split explicit via $\\dot{x}=(J-R)\\nabla H(x)$, guaranteeing $dH/dt\\le 0$ when $R\\succeq 0$. We introduce \\textbf{PHAST} (Port-Hamiltonian Architecture for Structured Temporal dynamics), which decomposes the Hamiltonian into potential~$V(q)$, mass~$M(q)$, and damping~$D(q)$ across three knowledge regimes (KNOWN, PARTIAL, UNKNOWN), uses efficient low-rank PSD/SPD parameterizations, and advances dynamics with Strang splitting. Across thirteen q-only benchmarks spanning mechanical, electrical, molecular, thermal, gravitational, and ecological systems, PHAST achieves the best long-horizon forecasting among competitive baselines and enables physically meaningful parameter recovery when the regime provides sufficient anchors. We show that identification is fundamentally ill-posed without such anchors (gauge freedom), motivating a two-axis evaluation that separates forecasting stability from identifiability.", "AI": {"tldr": "PHAST is a port-Hamiltonian neural architecture for learning dissipative dynamics from position-only observations, achieving state-of-the-art long-horizon forecasting and enabling physical parameter recovery when structure is provided.", "motivation": "Real physical systems are dissipative (energy loss), and forecasting their dynamics from partial position-only observations is challenging. Existing methods often fail to maintain long-term stability or recover physically meaningful parameters.", "method": "PHAST uses port-Hamiltonian framework with explicit conservative-dissipative split. It decomposes Hamiltonian into potential, mass, and damping components across three knowledge regimes (KNOWN, PARTIAL, UNKNOWN), employs efficient low-rank PSD/SPD parameterizations, and advances dynamics using Strang splitting.", "result": "Across 13 position-only benchmarks spanning mechanical, electrical, molecular, thermal, gravitational, and ecological systems, PHAST achieves best long-horizon forecasting among competitive baselines and enables physically meaningful parameter recovery when sufficient structural anchors are provided.", "conclusion": "PHAST successfully addresses the position-only forecasting problem with stable long-term predictions. Parameter identification is fundamentally ill-posed without structural anchors (gauge freedom), necessitating a two-axis evaluation separating forecasting stability from identifiability."}}
{"id": "2602.17853", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17853", "abs": "https://arxiv.org/abs/2602.17853", "authors": ["Masoud Yavari", "Payman Moallem"], "title": "Neural Prior Estimation: Learning Class Priors from Latent Representations", "comment": null, "summary": "Class imbalance induces systematic bias in deep neural networks by imposing a skewed effective class prior. This work introduces the Neural Prior Estimator (NPE), a framework that learns feature-conditioned log-prior estimates from latent representations. NPE employs one or more Prior Estimation Modules trained jointly with the backbone via a one-way logistic loss. Under the Neural Collapse regime, NPE is analytically shown to recover the class log-prior up to an additive constant, providing a theoretically grounded adaptive signal without requiring explicit class counts or distribution-specific hyperparameters. The learned estimate is incorporated into logit adjustment, forming NPE-LA, a principled mechanism for bias-aware prediction. Experiments on long-tailed CIFAR and imbalanced semantic segmentation benchmarks (STARE, ADE20K) demonstrate consistent improvements, particularly for underrepresented classes. NPE thus offers a lightweight and theoretically justified approach to learned prior estimation and imbalance-aware prediction.", "AI": {"tldr": "NPE learns feature-conditioned log-prior estimates from latent representations to address class imbalance bias in neural networks, enabling principled bias-aware prediction without explicit class counts.", "motivation": "Class imbalance causes systematic bias in deep neural networks by creating skewed effective class priors, which leads to poor performance on underrepresented classes.", "method": "Neural Prior Estimator (NPE) learns feature-conditioned log-prior estimates from latent representations using Prior Estimation Modules trained jointly with the backbone via one-way logistic loss, then incorporates these estimates into logit adjustment (NPE-LA).", "result": "Experiments on long-tailed CIFAR and imbalanced semantic segmentation benchmarks (STARE, ADE20K) show consistent improvements, especially for underrepresented classes.", "conclusion": "NPE provides a lightweight, theoretically justified approach to learned prior estimation and imbalance-aware prediction that recovers class log-prior under Neural Collapse regime."}}
{"id": "2602.18008", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18008", "abs": "https://arxiv.org/abs/2602.18008", "authors": ["Zihan Guan", "Rituparna Datta", "Mengxuan Hu", "Shunshun Liu", "Aiying Zhang", "Prasanna Balachandran", "Sheng Li", "Anil Vullikanti"], "title": "NIMMGen: Learning Neural-Integrated Mechanistic Digital Twins with LLMs", "comment": "19 pages, 6 figures", "summary": "Mechanistic models encode scientific knowledge about dynamical systems and are widely used in downstream scientific and policy applications. Recent work has explored LLM-based agentic frameworks to automatically construct mechanistic models from data; however, existing problem settings substantially oversimplify real-world conditions, leaving it unclear whether LLM-generated mechanistic models are reliable in practice. To address this gap, we introduce the Neural-Integrated Mechanistic Modeling (NIMM) evaluation framework, which evaluates LLM-generated mechanistic models under realistic settings with partial observations and diversified task objectives. Our evaluation reveals fundamental challenges in current baselines, ranging from model effectiveness to code-level correctness. Motivated by these findings, we design NIMMgen, an agentic framework for neural-integrated mechanistic modeling that enhances code correctness and practical validity through iterative refinement. Experiments across three datasets from diversified scientific domains demonstrate its strong performance. We also show that the learned mechanistic models support counterfactual intervention simulation.", "AI": {"tldr": "LLM-based agentic framework (NIMMgen) for neural-integrated mechanistic modeling that improves code correctness and practical validity through iterative refinement, evaluated under realistic conditions with partial observations.", "motivation": "Existing LLM-based frameworks for constructing mechanistic models oversimplify real-world conditions, leaving reliability unclear. Need to evaluate LLM-generated mechanistic models under realistic settings with partial observations and diversified task objectives.", "method": "Introduces Neural-Integrated Mechanistic Modeling (NIMM) evaluation framework, then designs NIMMgen - an agentic framework that enhances code correctness and practical validity through iterative refinement.", "result": "Experiments across three datasets from diversified scientific domains demonstrate strong performance. Learned mechanistic models support counterfactual intervention simulation.", "conclusion": "NIMMgen addresses fundamental challenges in current baselines (model effectiveness and code-level correctness) and enables reliable mechanistic modeling under realistic conditions."}}
{"id": "2602.18322", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18322", "abs": "https://arxiv.org/abs/2602.18322", "authors": ["Ziteng Cui", "Shuhong Liu", "Xiaoyu Dong", "Xuangeng Chu", "Lin Gu", "Ming-Hsuan Yang", "Tatsuya Harada"], "title": "Unifying Color and Lightness Correction with View-Adaptive Curve Adjustment for Robust 3D Novel View Synthesis", "comment": "Journal extension version of CVPR 2025 paper: arXiv:2504.01503", "summary": "High-quality image acquisition in real-world environments remains challenging due to complex illumination variations and inherent limitations of camera imaging pipelines. These issues are exacerbated in multi-view capture, where differences in lighting, sensor responses, and image signal processor (ISP) configurations introduce photometric and chromatic inconsistencies that violate the assumptions of photometric consistency underlying modern 3D novel view synthesis (NVS) methods, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), leading to degraded reconstruction and rendering quality. We propose Luminance-GS++, a 3DGS-based framework for robust NVS under diverse illumination conditions. Our method combines a globally view-adaptive lightness adjustment with a local pixel-wise residual refinement for precise color correction. We further design unsupervised objectives that jointly enforce lightness correction and multi-view geometric and photometric consistency. Extensive experiments demonstrate state-of-the-art performance across challenging scenarios, including low-light, overexposure, and complex luminance and chromatic variations. Unlike prior approaches that modify the underlying representation, our method preserves the explicit 3DGS formulation, improving reconstruction fidelity while maintaining real-time rendering efficiency.", "AI": {"tldr": "Luminance-GS++ is a 3D Gaussian Splatting-based framework that addresses photometric inconsistencies in multi-view capture by combining global lightness adjustment with local pixel-wise refinement for robust novel view synthesis under diverse illumination conditions.", "motivation": "Real-world image acquisition faces challenges from complex illumination variations and camera pipeline limitations. In multi-view capture, these issues cause photometric and chromatic inconsistencies that violate photometric consistency assumptions in 3D novel view synthesis methods like NeRF and 3DGS, leading to degraded reconstruction quality.", "method": "The method combines a globally view-adaptive lightness adjustment with local pixel-wise residual refinement for precise color correction. It uses unsupervised objectives that jointly enforce lightness correction and multi-view geometric/photometric consistency while preserving the explicit 3DGS formulation.", "result": "Extensive experiments demonstrate state-of-the-art performance across challenging scenarios including low-light, overexposure, and complex luminance/chromatic variations. The method improves reconstruction fidelity while maintaining real-time rendering efficiency.", "conclusion": "Luminance-GS++ provides a robust solution for novel view synthesis under diverse illumination conditions by addressing photometric inconsistencies without modifying the underlying 3DGS representation, achieving high-quality results while preserving real-time rendering capabilities."}}
{"id": "2602.17861", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17861", "abs": "https://arxiv.org/abs/2602.17861", "authors": ["Ryan McKenna", "Galen Andrew", "Borja Balle", "Vadym Doroshenko", "Arun Ganesh", "Weiwei Kong", "Alex Kurakin", "Brendan McMahan", "Mikhail Pravilov"], "title": "JAX-Privacy: A library for differentially private machine learning", "comment": null, "summary": "JAX-Privacy is a library designed to simplify the deployment of robust and performant mechanisms for differentially private machine learning. Guided by design principles of usability, flexibility, and efficiency, JAX-Privacy serves both researchers requiring deep customization and practitioners who want a more out-of-the-box experience. The library provides verified, modular primitives for critical components for all aspects of the mechanism design including batch selection, gradient clipping, noise addition, accounting, and auditing, and brings together a large body of recent research on differentially private ML.", "AI": {"tldr": "JAX-Privacy is a library for differentially private ML that balances usability for practitioners with customization for researchers, providing verified primitives for all DP mechanism components.", "motivation": "To simplify deployment of robust and performant differentially private ML mechanisms, addressing the need for both easy-to-use solutions for practitioners and customizable tools for researchers.", "method": "Developed as a library with design principles of usability, flexibility, and efficiency. Provides modular primitives for batch selection, gradient clipping, noise addition, accounting, and auditing, integrating recent DP ML research.", "result": "A comprehensive library that serves both research and practical needs, offering verified components for all aspects of differentially private mechanism design in machine learning.", "conclusion": "JAX-Privacy successfully bridges the gap between research and practice in differentially private ML by providing a flexible, efficient library with verified primitives that accommodates both customization needs and out-of-the-box usability."}}
{"id": "2602.18015", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18015", "abs": "https://arxiv.org/abs/2602.18015", "authors": ["Jongseong Chae", "Jongeui Park", "Yongjae Shin", "Gyeongmin Kim", "Seungyul Han", "Youngchul Sung"], "title": "Flow Actor-Critic for Offline Reinforcement Learning", "comment": "Accepted to ICLR 2026", "summary": "The dataset distributions in offline reinforcement learning (RL) often exhibit complex and multi-modal distributions, necessitating expressive policies to capture such distributions beyond widely-used Gaussian policies. To handle such complex and multi-modal datasets, in this paper, we propose Flow Actor-Critic, a new actor-critic method for offline RL, based on recent flow policies. The proposed method not only uses the flow model for actor as in previous flow policies but also exploits the expressive flow model for conservative critic acquisition to prevent Q-value explosion in out-of-data regions. To this end, we propose a new form of critic regularizer based on the flow behavior proxy model obtained as a byproduct of flow-based actor design. Leveraging the flow model in this joint way, we achieve new state-of-the-art performance for test datasets of offline RL including the D4RL and recent OGBench benchmarks.", "AI": {"tldr": "Flow Actor-Critic is a new offline RL method that uses flow models for both actor and critic to handle multi-modal data distributions and prevent Q-value explosion, achieving SOTA on D4RL and OGBench benchmarks.", "motivation": "Offline RL datasets often have complex, multi-modal distributions that require expressive policies beyond simple Gaussian policies. Existing methods struggle with these distributions and suffer from Q-value explosion in out-of-distribution regions.", "method": "Proposes Flow Actor-Critic that uses flow models for both actor (policy) and critic (value function). The flow-based actor captures multi-modal distributions, while a flow-based critic regularizer prevents Q-value explosion using a flow behavior proxy model obtained as a byproduct of actor design.", "result": "Achieves new state-of-the-art performance on offline RL benchmarks including D4RL and OGBench datasets.", "conclusion": "Jointly leveraging flow models for both actor and critic in offline RL effectively handles complex multi-modal data distributions while preventing value overestimation, leading to superior performance on standard benchmarks."}}
{"id": "2602.18329", "categories": ["cs.CV", "math.AT"], "pdf": "https://arxiv.org/pdf/2602.18329", "abs": "https://arxiv.org/abs/2602.18329", "authors": ["Qingsong Wang", "Jiaxing He", "Bingzhe Hou", "Tieru Wu", "Yang Cao", "Cailing Yao"], "title": "G-LoG Bi-filtration for Medical Image Classification", "comment": null, "summary": "Building practical filtrations on objects to detect topological and geometric features is an important task in the field of Topological Data Analysis (TDA). In this paper, leveraging the ability of the Laplacian of Gaussian operator to enhance the boundaries of medical images, we define the G-LoG (Gaussian-Laplacian of Gaussian) bi-filtration to generate the features more suitable for multi-parameter persistence module. By modeling volumetric images as bounded functions, then we prove the interleaving distance on the persistence modules obtained from our bi-filtrations on the bounded functions is stable with respect to the maximum norm of the bounded functions. Finally, we conduct experiments on the MedMNIST dataset, comparing our bi-filtration against single-parameter filtration and the established deep learning baselines, including Google AutoML Vision, ResNet, AutoKeras and auto-sklearn. Experiments results demonstrate that our bi-filtration significantly outperforms single-parameter filtration. Notably, a simple Multi-Layer Perceptron (MLP) trained on the topological features generated by our bi-filtration achieves performance comparable to complex deep learning models trained on the original dataset.", "AI": {"tldr": "The paper introduces G-LoG bi-filtration for medical image analysis using topological data analysis, proving stability and showing it outperforms single-parameter filtration while achieving comparable performance to deep learning models.", "motivation": "To build practical filtrations for detecting topological and geometric features in medical images, leveraging the Laplacian of Gaussian operator's ability to enhance image boundaries for multi-parameter persistence analysis.", "method": "Define G-LoG (Gaussian-Laplacian of Gaussian) bi-filtration by modeling volumetric images as bounded functions, prove stability of interleaving distance with respect to maximum norm, and conduct experiments on MedMNIST dataset comparing against single-parameter filtration and deep learning baselines.", "result": "G-LoG bi-filtration significantly outperforms single-parameter filtration. A simple MLP trained on topological features from the bi-filtration achieves performance comparable to complex deep learning models (Google AutoML Vision, ResNet, AutoKeras, auto-sklearn) trained on original data.", "conclusion": "The proposed G-LoG bi-filtration provides effective topological features for medical image analysis, offering a competitive alternative to complex deep learning approaches while maintaining theoretical stability guarantees."}}
{"id": "2602.18394", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18394", "abs": "https://arxiv.org/abs/2602.18394", "authors": ["Stefan Becker", "Simon Weiss", "Wolfgang H\u00fcbner", "Michael Arens"], "title": "Self-Aware Object Detection via Degradation Manifolds", "comment": null, "summary": "Object detectors achieve strong performance under nominal imaging conditions but can fail silently when exposed to blur, noise, compression, adverse weather, or resolution changes. In safety-critical settings, it is therefore insufficient to produce predictions without assessing whether the input remains within the detector's nominal operating regime. We refer to this capability as self-aware object detection.\n  We introduce a degradation-aware self-awareness framework based on degradation manifolds, which explicitly structure a detector's feature space according to image degradation rather than semantic content. Our method augments a standard detection backbone with a lightweight embedding head trained via multi-layer contrastive learning. Images sharing the same degradation composition are pulled together, while differing degradation configurations are pushed apart, yielding a geometrically organized representation that captures degradation type and severity without requiring degradation labels or explicit density modeling.\n  To anchor the learned geometry, we estimate a pristine prototype from clean training embeddings, defining a nominal operating point in representation space. Self-awareness emerges as geometric deviation from this reference, providing an intrinsic, image-level signal of degradation-induced shift that is independent of detection confidence.\n  Extensive experiments on synthetic corruption benchmarks, cross-dataset zero-shot transfer, and natural weather-induced distribution shifts demonstrate strong pristine-degraded separability, consistent behavior across multiple detector architectures, and robust generalization under semantic shift. These results suggest that degradation-aware representation geometry provides a practical and detector-agnostic foundation.", "AI": {"tldr": "A framework for self-aware object detection that learns degradation-aware representations via contrastive learning, enabling detectors to assess whether inputs are within their nominal operating regime without requiring degradation labels.", "motivation": "Object detectors can fail silently when exposed to degraded inputs (blur, noise, weather, etc.), which is dangerous in safety-critical applications. Current detectors lack the ability to assess whether inputs remain within their nominal operating regime.", "method": "Augments detection backbone with lightweight embedding head trained via multi-layer contrastive learning. Images with same degradation composition are pulled together while different degradations are pushed apart, creating geometrically organized degradation manifolds. Estimates pristine prototype from clean embeddings as nominal reference point.", "result": "Strong pristine-degraded separability, consistent behavior across multiple detector architectures, robust generalization under semantic shift, and effective zero-shot transfer across datasets and natural weather-induced distribution shifts.", "conclusion": "Degradation-aware representation geometry provides a practical, detector-agnostic foundation for self-aware object detection, enabling intrinsic assessment of input degradation without relying on detection confidence or explicit degradation modeling."}}
{"id": "2602.17867", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17867", "abs": "https://arxiv.org/abs/2602.17867", "authors": ["Jo\u00e3o N. Cardoso", "Arlindo L. Oliveira", "Bruno Martins"], "title": "ADAPT: Hybrid Prompt Optimization for LLM Feature Visualization", "comment": null, "summary": "Understanding what features are encoded by learned directions in LLM activation space requires identifying inputs that strongly activate them. Feature visualization, which optimizes inputs to maximally activate a target direction, offers an alternative to costly dataset search approaches, but remains underexplored for LLMs due to the discrete nature of text. Furthermore, existing prompt optimization techniques are poorly suited to this domain, which is highly prone to local minima. To overcome these limitations, we introduce ADAPT, a hybrid method combining beam search initialization with adaptive gradient-guided mutation, designed around these failure modes. We evaluate on Sparse Autoencoder latents from Gemma 2 2B, proposing metrics grounded in dataset activation statistics to enable rigorous comparison, and show that ADAPT consistently outperforms prior methods across layers and latent types. Our results establish that feature visualization for LLMs is tractable, but requires design assumptions tailored to the domain.", "AI": {"tldr": "ADAPT is a hybrid method combining beam search initialization with adaptive gradient-guided mutation for feature visualization in LLMs, outperforming prior methods across layers and latent types.", "motivation": "Understanding what features are encoded by learned directions in LLM activation space requires identifying inputs that strongly activate them. Feature visualization offers an alternative to costly dataset search but remains underexplored for LLMs due to text's discrete nature, and existing prompt optimization techniques are poorly suited due to high susceptibility to local minima.", "method": "ADAPT is a hybrid method combining beam search initialization with adaptive gradient-guided mutation, specifically designed to overcome the failure modes in LLM feature visualization. The approach is evaluated on Sparse Autoencoder latents from Gemma 2 2B using metrics grounded in dataset activation statistics for rigorous comparison.", "result": "ADAPT consistently outperforms prior methods across layers and latent types, establishing that feature visualization for LLMs is tractable but requires domain-specific design assumptions.", "conclusion": "Feature visualization for LLMs is achievable with the right approach, and ADAPT's success demonstrates the importance of tailored design assumptions for this domain, overcoming the limitations of existing prompt optimization techniques."}}
{"id": "2602.18406", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18406", "abs": "https://arxiv.org/abs/2602.18406", "authors": ["Minh Dinh", "St\u00e9phane Deny"], "title": "Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges", "comment": null, "summary": "Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training-for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to earn equivariant operators in a latent space from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. While conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.", "AI": {"tldr": "The paper explores architectures that learn equivariant operators from examples to handle group-symmetric transformations not seen during training, demonstrating success on rotated/translated MNIST but noting scaling challenges for complex datasets.", "motivation": "Deep learning struggles with recognizing objects that undergo group-symmetric transformations rarely seen during training (unusual poses, scales, positions). While equivariant networks can generalize across symmetric transformations, they require prior knowledge of transformations.", "method": "The paper proposes architectures that learn equivariant operators in a latent space from examples of symmetric transformations. The approach is tested on simple datasets of rotated and translated noisy MNIST.", "result": "The architectures successfully enable out-of-distribution classification for rotated and translated MNIST, overcoming limitations of both traditional and equivariant networks.", "conclusion": "While conceptually promising for handling unseen symmetric transformations, significant challenges remain in scaling these architectures to more complex datasets beyond simple MNIST variations."}}
{"id": "2602.18037", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18037", "abs": "https://arxiv.org/abs/2602.18037", "authors": ["Johannes Ackermann", "Michael Noukhovitch", "Takashi Ishida", "Masashi Sugiyama"], "title": "Gradient Regularization Prevents Reward Hacking in Reinforcement Learning from Human Feedback and Verifiable Rewards", "comment": "25 pages, 15 figures", "summary": "Reinforcement Learning from Human Feedback (RLHF) or Verifiable Rewards (RLVR) are two key steps in the post-training of modern Language Models (LMs). A common problem is reward hacking, where the policy may exploit inaccuracies of the reward and learn an unintended behavior. Most previous works address this by limiting the policy update with a Kullback-Leibler (KL) penalty towards a reference model. We propose a different framing: Train the LM in a way that biases policy updates towards regions in which the reward is more accurate. First, we derive a theoretical connection between the accuracy of a reward model and the flatness of an optimum at convergence. Gradient regularization (GR) can then be used to bias training to flatter regions and thereby maintain reward model accuracy. We confirm these results by showing that the gradient norm and reward accuracy are empirically correlated in RLHF. We then show that Reference Resets of the KL penalty implicitly use GR to find flatter regions with higher reward accuracy. We further improve on this by proposing to use explicit GR with an efficient finite-difference estimate. Empirically, GR performs better than a KL penalty across a diverse set of RL experiments with LMs. GR achieves a higher GPT-judged win-rate in RLHF, avoids overly focusing on the format in rule-based math rewards, and prevents hacking the judge in LLM-as-a-Judge math tasks.", "AI": {"tldr": "The paper proposes using gradient regularization (GR) instead of KL penalties to prevent reward hacking in RLHF/RLVR by biasing training toward regions where reward models are more accurate.", "motivation": "Current RLHF/RLVR approaches suffer from reward hacking where policies exploit inaccurate reward models. Traditional KL penalties limit policy updates but don't address the root cause of reward model inaccuracy.", "method": "Theoretical connection shows reward accuracy correlates with flatness of optima. Gradient regularization biases training to flatter regions where rewards are more accurate. Reference Resets implicitly use GR, and explicit GR with finite-difference estimates improves this.", "result": "GR outperforms KL penalties across diverse RL experiments: achieves higher GPT-judged win-rate in RLHF, avoids format overfitting in math rewards, and prevents judge hacking in LLM-as-a-Judge tasks.", "conclusion": "Gradient regularization provides a better alternative to KL penalties for preventing reward hacking by directly addressing reward model accuracy through flatness optimization."}}
{"id": "2602.18422", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18422", "abs": "https://arxiv.org/abs/2602.18422", "authors": ["Linxi Xie", "Lisong C. Sun", "Ashley Neall", "Tong Wu", "Shengqu Cai", "Gordon Wetzstein"], "title": "Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control", "comment": "Project page here: https://codeysun.github.io/generated-reality", "summary": "Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.", "AI": {"tldr": "A human-centric video world model that generates egocentric virtual environments controlled by tracked head and hand poses, enabling embodied interaction in XR.", "motivation": "Current video world models only accept coarse control signals like text or keyboard input, limiting their utility for embodied interaction in extended reality (XR) where users need models that respond to their tracked real-world motion.", "method": "Introduce a human-centric video world model conditioned on tracked head pose and joint-level hand poses. Evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control. Train a bidirectional video diffusion model teacher and distill it into a causal, interactive system for generating egocentric virtual environments.", "result": "The system demonstrates improved task performance and significantly higher perceived amount of control over performed actions compared to relevant baselines when evaluated with human subjects.", "conclusion": "The proposed human-centric video world model enables dexterous hand-object interactions and embodied control in XR environments, addressing the limitations of current coarse-control video world models for interactive applications."}}
{"id": "2602.17893", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17893", "abs": "https://arxiv.org/abs/2602.17893", "authors": ["Jiajun Shen", "Yufei Jin", "Yi He", "xingquan Zhu"], "title": "COMBA: Cross Batch Aggregation for Learning Large Graphs with Context Gating State Space Models", "comment": null, "summary": "State space models (SSMs) have recently emerged for modeling long-range dependency in sequence data, with much simplified computational costs than modern alternatives, such as transformers. Advancing SMMs to graph structured data, especially for large graphs, is a significant challenge because SSMs are sequence models and the shear graph volumes make it very expensive to convert graphs as sequences for effective learning. In this paper, we propose COMBA to tackle large graph learning using state space models, with two key innovations: graph context gating and cross batch aggregation. Graph context refers to different hops of neighborhood for each node, and graph context gating allows COMBA to use such context to learn best control of neighbor aggregation. For each graph context, COMBA samples nodes as batches, and train a graph neural network (GNN), with information being aggregated cross batches, allowing COMBA to scale to large graphs. Our theoretical study asserts that cross-batch aggregation guarantees lower error than training GNN without aggregation. Experiments on benchmark networks demonstrate significant performance gains compared to baseline approaches. Code and benchmark datasets will be released for public access.", "AI": {"tldr": "COMBA: A state space model approach for large graph learning using graph context gating and cross-batch aggregation to scale SSMs to graph-structured data.", "motivation": "State space models (SSMs) excel at modeling long-range dependencies in sequences but face challenges when applied to graph data. Converting large graphs to sequences is computationally expensive, creating a need for efficient SSM adaptation to graph-structured data.", "method": "COMBA introduces two key innovations: 1) Graph context gating - uses different neighborhood hops (graph contexts) to learn optimal neighbor aggregation control, and 2) Cross-batch aggregation - samples nodes as batches and trains GNNs with information aggregated across batches to enable scaling to large graphs.", "result": "Theoretical analysis shows cross-batch aggregation guarantees lower error than training GNNs without aggregation. Experiments on benchmark networks demonstrate significant performance gains over baseline approaches.", "conclusion": "COMBA successfully adapts state space models to large graph learning through innovative context gating and cross-batch aggregation techniques, providing both theoretical guarantees and empirical performance improvements."}}
{"id": "2602.18432", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18432", "abs": "https://arxiv.org/abs/2602.18432", "authors": ["Evonne Ng", "Siwei Zhang", "Zhang Chen", "Michael Zollhoefer", "Alexander Richard"], "title": "SARAH: Spatially Aware Real-time Agentic Humans", "comment": "Project page: https://evonneng.github.io/sarah/", "summary": "As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user's position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details.", "AI": {"tldr": "Real-time, fully causal method for spatially-aware conversational motion in VR agents that aligns gestures with speech while orienting toward users based on their position and audio.", "motivation": "Current embodied agents lack spatial awareness - they don't turn toward users, respond to movement, or maintain natural gaze, which is essential for VR, telepresence, and digital human applications.", "method": "Combines causal transformer-based VAE with interleaved latent tokens for streaming inference and flow matching model conditioned on user trajectory and audio. Includes gaze scoring mechanism with classifier-free guidance to decouple learning from control.", "result": "Achieves state-of-the-art motion quality at over 300 FPS (3x faster than non-causal baselines) on Embody 3D dataset, capturing subtle spatial dynamics of natural conversation. Successfully validated on live VR system.", "conclusion": "First real-time, fully causal method for spatially-aware conversational motion that enables VR agents to maintain natural spatial alignment with users while allowing gaze preference adjustment at inference time."}}
{"id": "2602.17898", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17898", "abs": "https://arxiv.org/abs/2602.17898", "authors": ["Jingquan Yan", "Yuwei Miao", "Peiran Yu", "Junzhou Huang"], "title": "Breaking the Correlation Plateau: On the Optimization and Capacity Limits of Attention-Based Regressors", "comment": "Accepted by ICLR 2026", "summary": "Attention-based regression models are often trained by jointly optimizing Mean Squared Error (MSE) loss and Pearson correlation coefficient (PCC) loss, emphasizing the magnitude of errors and the order or shape of targets, respectively. A common but poorly understood phenomenon during training is the PCC plateau: PCC stops improving early in training, even as MSE continues to decrease. We provide the first rigorous theoretical analysis of this behavior, revealing fundamental limitations in both optimization dynamics and model capacity. First, in regard to the flattened PCC curve, we uncover a critical conflict where lowering MSE (magnitude matching) can paradoxically suppress the PCC gradient (shape matching). This issue is exacerbated by the softmax attention mechanism, particularly when the data to be aggregated is highly homogeneous. Second, we identify a limitation in the model capacity: we derived a PCC improvement limit for any convex aggregator (including the softmax attention), showing that the convex hull of the inputs strictly bounds the achievable PCC gain. We demonstrate that data homogeneity intensifies both limitations. Motivated by these insights, we propose the Extrapolative Correlation Attention (ECA), which incorporates novel, theoretically-motivated mechanisms to improve the PCC optimization and extrapolate beyond the convex hull. Across diverse benchmarks, including challenging homogeneous data setting, ECA consistently breaks the PCC plateau, achieving significant improvements in correlation without compromising MSE performance.", "AI": {"tldr": "The paper analyzes why Pearson correlation coefficient (PCC) plateaus during attention-based regression training, identifies optimization and capacity limitations, and proposes Extrapolative Correlation Attention (ECA) to overcome these issues.", "motivation": "Attention-based regression models often show PCC plateauing early in training while MSE continues to improve. This phenomenon is poorly understood, and the authors aim to provide the first rigorous theoretical analysis of this behavior and develop solutions.", "method": "The authors conduct theoretical analysis revealing two limitations: 1) optimization conflict where lowering MSE suppresses PCC gradients, exacerbated by softmax attention on homogeneous data; 2) capacity limitation where convex aggregators (including softmax) have bounded PCC improvement. They propose Extrapolative Correlation Attention (ECA) with novel mechanisms to improve PCC optimization and extrapolate beyond convex hull constraints.", "result": "The analysis shows data homogeneity intensifies both limitations. ECA consistently breaks the PCC plateau across diverse benchmarks, achieving significant correlation improvements without compromising MSE performance, especially in challenging homogeneous data settings.", "conclusion": "The PCC plateau stems from fundamental optimization and capacity limitations in attention-based regression. The proposed ECA successfully addresses these issues through theoretically-motivated mechanisms, enabling better correlation optimization while maintaining MSE performance."}}
{"id": "2602.18116", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18116", "abs": "https://arxiv.org/abs/2602.18116", "authors": ["Olga Saukh", "Dong Wang", "Haris \u0160iki\u0107", "Yun Cheng", "Lothar Thiele"], "title": "Cut Less, Fold More: Model Compression through the Lens of Projection Geometry", "comment": "Accepted by ICLR 2026", "summary": "Compressing neural networks without retraining is vital for deployment at scale. We study calibration-free compression through the lens of projection geometry: structured pruning is an axis-aligned projection, whereas model folding performs a low-rank projection via weight clustering. We formalize both as orthogonal operators and show that, within a rank distance of one, folding provably yields smaller parameter reconstruction error, and under mild smoothness assumptions, smaller functional perturbations than pruning. At scale, we evaluate >1000 checkpoints spanning ResNet18, PreActResNet18, ViT-B/32, and CLIP ViT-B/32 on CIFAR-10 and ImageNet-1K, covering diverse training hyperparameters (optimizers, learning rates, augmentations, regularization, sharpness-aware training), as well as multiple LLaMA-family 60M and 130M parameter models trained on C4. We show that folding typically achieves higher post-compression accuracy, with the largest gains at moderate-high compression. The gap narrows and occasionally reverses at specific training setups. Our results position folding as a geometry-aware, calibration-free alternative to pruning that is often superior in practice and principled in theory.", "AI": {"tldr": "Folding (weight clustering) outperforms pruning for calibration-free neural network compression, with theoretical guarantees and empirical validation across diverse models.", "motivation": "Need for calibration-free neural network compression that works at scale without retraining, addressing deployment challenges.", "method": "Formalizes pruning as axis-aligned projection and folding (weight clustering) as low-rank projection via orthogonal operators. Proves folding yields smaller reconstruction error and functional perturbations than pruning within rank distance of one.", "result": "Evaluated 1000+ checkpoints across ResNet18, PreActResNet18, ViT-B/32, CLIP ViT-B/32 on CIFAR-10/ImageNet, and LLaMA models. Folding typically achieves higher post-compression accuracy, especially at moderate-high compression, though gap narrows in specific training setups.", "conclusion": "Folding is a geometry-aware, calibration-free alternative to pruning that is often superior in practice and principled in theory."}}
{"id": "2602.18434", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18434", "abs": "https://arxiv.org/abs/2602.18434", "authors": ["Vatsal Agarwal", "Saksham Suri", "Matthew Gwilliam", "Pulkit Kumar", "Abhinav Shrivastava"], "title": "Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory", "comment": "Project page: see https://vatsalag99.github.io/memstream/", "summary": "Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B.", "AI": {"tldr": "MemStream improves streaming video understanding by scaling token budget, addressing retrieval bias in dense streams, and using adaptive selection with retrieval mixture-of-experts.", "motivation": "Existing streaming video QA methods use limited tokens per frame, losing fine-grained visual details. Current approaches also suffer from retrieval bias toward later frames in dense video streams.", "method": "1) Scale token budget for granular spatiotemporal understanding; 2) Adaptive selection strategy to reduce token redundancy while preserving local spatiotemporal information; 3) Training-free retrieval mixture-of-experts leveraging external models to identify relevant frames.", "result": "MemStream achieves significant improvements: +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B.", "conclusion": "Scaling token budget with adaptive selection and retrieval mixture-of-experts enables better streaming video understanding by preserving fine-grained details and addressing retrieval bias in dense video streams."}}
{"id": "2602.17918", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17918", "abs": "https://arxiv.org/abs/2602.17918", "authors": ["Jialin Yu", "Mo\u00efse Blanchard"], "title": "Distribution-Free Sequential Prediction with Abstentions", "comment": "38 pages, 2 figures. Submitted to COLT 2026. Extended version", "summary": "We study a sequential prediction problem in which an adversary is allowed to inject arbitrarily many adversarial instances in a stream of i.i.d.\\ instances, but at each round, the learner may also \\emph{abstain} from making a prediction without incurring any penalty if the instance was indeed corrupted. This semi-adversarial setting naturally sits between the classical stochastic case with i.i.d.\\ instances for which function classes with finite VC dimension are learnable; and the adversarial case with arbitrary instances, known to be significantly more restrictive. For this problem, Goel et al. (2023) showed that, if the learner knows the distribution $\u03bc$ of clean samples in advance, learning can be achieved for all VC classes without restrictions on adversary corruptions. This is, however, a strong assumption in both theory and practice: a natural question is whether similar learning guarantees can be achieved without prior distributional knowledge, as is standard in classical learning frameworks (e.g., PAC learning or asymptotic consistency) and other non-i.i.d.\\ models (e.g., smoothed online learning). We therefore focus on the distribution-free setting where $\u03bc$ is \\emph{unknown} and propose an algorithm \\textsc{AbstainBoost} based on a boosting procedure of weak learners, which guarantees sublinear error for general VC classes in \\emph{distribution-free} abstention learning for oblivious adversaries. These algorithms also enjoy similar guarantees for adaptive adversaries, for structured function classes including linear classifiers. These results are complemented with corresponding lower bounds, which reveal an interesting polynomial trade-off between misclassification error and number of erroneous abstentions.", "AI": {"tldr": "The paper studies sequential prediction with adversarial injections where learners can abstain without penalty on corrupted instances, showing that distribution-free learning is possible for VC classes via boosting algorithms.", "motivation": "Previous work required knowing the distribution of clean samples in advance, which is unrealistic. The authors want to achieve similar learning guarantees without prior distributional knowledge, aligning with standard learning frameworks like PAC learning.", "method": "Proposed AbstainBoost algorithm based on boosting weak learners for distribution-free abstention learning with oblivious adversaries. Also extended to adaptive adversaries for structured function classes like linear classifiers.", "result": "Achieved sublinear error for general VC classes in distribution-free setting. Established polynomial trade-off between misclassification error and number of erroneous abstentions through matching lower bounds.", "conclusion": "Distribution-free learning is possible for VC classes in semi-adversarial settings with abstention, bridging the gap between stochastic and fully adversarial learning frameworks."}}
{"id": "2602.18117", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18117", "abs": "https://arxiv.org/abs/2602.18117", "authors": ["Yongjae Shin", "Jongseong Chae", "Jongeui Park", "Youngchul Sung"], "title": "Flow Matching with Injected Noise for Offline-to-Online Reinforcement Learning", "comment": "ICLR 2026 camera-ready", "summary": "Generative models have recently demonstrated remarkable success across diverse domains, motivating their adoption as expressive policies in reinforcement learning (RL). While they have shown strong performance in offline RL, particularly where the target distribution is well defined, their extension to online fine-tuning has largely been treated as a direct continuation of offline pre-training, leaving key challenges unaddressed. In this paper, we propose Flow Matching with Injected Noise for Offline-to-Online RL (FINO), a novel method that leverages flow matching-based policies to enhance sample efficiency for offline-to-online RL. FINO facilitates effective exploration by injecting noise into policy training, thereby encouraging a broader range of actions beyond those observed in the offline dataset. In addition to exploration-enhanced flow policy training, we combine an entropy-guided sampling mechanism to balance exploration and exploitation, allowing the policy to adapt its behavior throughout online fine-tuning. Experiments across diverse, challenging tasks demonstrate that FINO consistently achieves superior performance under limited online budgets.", "AI": {"tldr": "FINO is a flow matching-based RL method that injects noise during policy training to improve exploration in offline-to-online reinforcement learning, achieving better sample efficiency with limited online budgets.", "motivation": "While generative models show promise as expressive policies in RL and perform well in offline settings, their extension to online fine-tuning faces challenges. Current approaches treat online fine-tuning as a direct continuation of offline pre-training, failing to address key issues like effective exploration beyond the offline dataset.", "method": "FINO uses flow matching-based policies with injected noise during training to encourage broader action exploration beyond the offline dataset. It combines this with an entropy-guided sampling mechanism to balance exploration and exploitation during online fine-tuning.", "result": "Experiments across diverse, challenging tasks show that FINO consistently achieves superior performance under limited online budgets, demonstrating improved sample efficiency for offline-to-online RL.", "conclusion": "FINO effectively addresses exploration challenges in offline-to-online RL by combining noise-injected flow matching policies with entropy-guided sampling, enabling better adaptation and performance with limited online interaction."}}
{"id": "2602.18182", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18182", "abs": "https://arxiv.org/abs/2602.18182", "authors": ["Daniel Romero-Alvarado", "Fernando Mart\u00ednez-Plumed", "Lorenzo Pacchiardi", "Hugo Save", "Siddhesh Milind Pawar", "Behzad Mehrbakhsh", "Pablo Antonio Moreno Casares", "Ben Slater", "Paolo Bova", "Peter Romero", "Zachary R. Tyler", "Jonathan Prunty", "Luning Sun", "Jose Hernandez-Orallo"], "title": "Capabilities Ain't All You Need: Measuring Propensities in AI", "comment": null, "summary": "AI evaluation has primarily focused on measuring capabilities, with formal approaches inspired from Item Response Theory (IRT) being increasingly applied. Yet propensities - the tendencies of models to exhibit particular behaviours - play a central role in determining both performance and safety outcomes. However, traditional IRT describes a model's success on a task as a monotonic function of model capabilities and task demands, an approach unsuited to propensities, where both excess and deficiency can be problematic. Here, we introduce the first formal framework for measuring AI propensities by using a bilogistic formulation for model success, which attributes high success probability when the model's propensity is within an \"ideal band\". Further, we estimate the limits of the ideal band using LLMs equipped with newly developed task-agnostic rubrics. Applying our framework to six families of LLM models whose propensities are incited in either direction, we find that we can measure how much the propensity is shifted and what effect this has on the tasks. Critically, propensities estimated using one benchmark successfully predict behaviour on held-out tasks. Moreover, we obtain stronger predictive power when combining propensities and capabilities than either separately. More broadly, our framework showcases how rigorous propensity measurements can be conducted and how it yields gains over solely using capability evaluations to predict AI behaviour.", "AI": {"tldr": "Introduces first formal framework for measuring AI propensities using bilogistic formulation with \"ideal band\" concept, showing propensities predict behavior better than capabilities alone.", "motivation": "Current AI evaluation focuses mainly on capabilities using IRT, but propensities (behavioral tendencies) are crucial for performance and safety. Traditional IRT's monotonic approach is unsuitable for propensities where both excess and deficiency can be problematic.", "method": "Develops bilogistic formulation where model success probability is high when propensity is within an \"ideal band.\" Estimates ideal band limits using LLMs with task-agnostic rubrics. Tests framework on six LLM families with manipulated propensities.", "result": "Successfully measures propensity shifts and their effects on tasks. Propensities from one benchmark predict behavior on held-out tasks. Combining propensities and capabilities yields stronger predictive power than either alone.", "conclusion": "Demonstrates rigorous propensity measurement framework that outperforms capability-only evaluations for predicting AI behavior, offering better assessment of performance and safety outcomes."}}
{"id": "2602.18195", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18195", "abs": "https://arxiv.org/abs/2602.18195", "authors": ["Hairong Chen", "Yicheng Feng", "Ziyu Jia", "Samir Bhatt", "Hengguan Huang"], "title": "LERD: Latent Event-Relational Dynamics for Neurodegenerative Classification", "comment": null, "summary": "Alzheimer's disease (AD) alters brain electrophysiology and disrupts multichannel EEG dynamics, making accurate and clinically useful EEG-based diagnosis increasingly important for screening and disease monitoring. However, many existing approaches rely on black-box classifiers and do not explicitly model the underlying dynamics that generate observed signals. To address these limitations, we propose LERD, an end-to-end Bayesian electrophysiological neural dynamical system that infers latent neural events and their relational structure directly from multichannel EEG without event or interaction annotations. LERD combines a continuous-time event inference module with a stochastic event-generation process to capture flexible temporal patterns, while incorporating an electrophysiology-inspired dynamical prior to guide learning in a principled way. We further provide theoretical analysis that yields a tractable bound for training and stability guarantees for the inferred relational dynamics. Extensive experiments on synthetic benchmarks and two real-world AD EEG cohorts demonstrate that LERD consistently outperforms strong baselines and yields physiology-aligned latent summaries that help characterize group-level dynamical differences.", "AI": {"tldr": "LERD: Bayesian neural dynamical system that infers latent neural events and relational structure from EEG for Alzheimer's disease diagnosis, outperforming baselines and providing physiology-aligned insights.", "motivation": "Alzheimer's disease alters brain electrophysiology and disrupts EEG dynamics, but existing approaches use black-box classifiers without modeling underlying dynamics. Need for accurate EEG-based diagnosis for screening and disease monitoring.", "method": "Propose LERD - end-to-end Bayesian electrophysiological neural dynamical system. Combines continuous-time event inference module with stochastic event-generation process to capture flexible temporal patterns. Uses electrophysiology-inspired dynamical prior to guide learning. Provides theoretical analysis with tractable training bound and stability guarantees.", "result": "Extensive experiments on synthetic benchmarks and two real-world AD EEG cohorts show LERD consistently outperforms strong baselines. Yields physiology-aligned latent summaries that help characterize group-level dynamical differences.", "conclusion": "LERD provides a principled approach to infer latent neural events and relational structure from EEG without annotations, offering improved Alzheimer's disease diagnosis and insights into disease-related electrophysiological changes."}}
{"id": "2602.17940", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17940", "abs": "https://arxiv.org/abs/2602.17940", "authors": ["Shogo Iwazaki"], "title": "Tighter Regret Lower Bound for Gaussian Process Bandits with Squared Exponential Kernel in Hypersphere", "comment": "27 pages, 2 figures", "summary": "We study an algorithm-independent, worst-case lower bound for the Gaussian process (GP) bandit problem in the frequentist setting, where the reward function is fixed and has a bounded norm in the known reproducing kernel Hilbert space (RKHS). Specifically, we focus on the squared exponential (SE) kernel, one of the most widely used kernel functions in GP bandits. One of the remaining open questions for this problem is the gap in the \\emph{dimension-dependent} logarithmic factors between upper and lower bounds. This paper partially resolves this open question under a hyperspherical input domain. We show that any algorithm suffers $\u03a9(\\sqrt{T (\\ln T)^{d} (\\ln \\ln T)^{-d}})$ cumulative regret, where $T$ and $d$ represent the total number of steps and the dimension of the hyperspherical domain, respectively. Regarding the simple regret, we show that any algorithm requires $\u03a9(\u03b5^{-2}(\\ln \\frac{1}\u03b5)^d (\\ln \\ln \\frac{1}\u03b5)^{-d})$ time steps to find an $\u03b5$-optimal point. We also provide the improved $O((\\ln T)^{d+1}(\\ln \\ln T)^{-d})$ upper bound on the maximum information gain for the SE kernel. Our results guarantee the optimality of the existing best algorithm up to \\emph{dimension-independent} logarithmic factors under a hyperspherical input domain.", "AI": {"tldr": "The paper establishes algorithm-independent lower bounds for Gaussian process bandits with squared exponential kernels on hyperspherical domains, partially resolving open questions about dimension-dependent logarithmic factors in regret bounds.", "motivation": "The motivation is to address the gap in dimension-dependent logarithmic factors between upper and lower bounds for GP bandits with squared exponential kernels, particularly focusing on the frequentist setting with bounded RKHS norm.", "method": "The authors study algorithm-independent worst-case lower bounds for GP bandits, focusing on the squared exponential kernel and hyperspherical input domains. They analyze cumulative regret and simple regret, and also provide improved upper bounds on maximum information gain.", "result": "Key results: 1) \u03a9(\u221a[T(ln T)^d(ln ln T)^{-d}]) cumulative regret lower bound, 2) \u03a9(\u03b5^{-2}(ln 1/\u03b5)^d(ln ln 1/\u03b5)^{-d}) time steps to find \u03b5-optimal point, 3) Improved O((ln T)^{d+1}(ln ln T)^{-d}) upper bound on maximum information gain for SE kernel.", "conclusion": "The results guarantee optimality of existing best algorithms up to dimension-independent logarithmic factors under hyperspherical domains, partially resolving open questions about dimension-dependent logarithmic factor gaps in GP bandit theory."}}
{"id": "2602.18230", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18230", "abs": "https://arxiv.org/abs/2602.18230", "authors": ["Jorge Carrasco Pollo", "Ioannis Kapetangeorgis", "Joshua Rosenthal", "John Hua Yao"], "title": "[Re] Benchmarking LLM Capabilities in Negotiation through Scoreable Games", "comment": "Accepted for publication at Transactions on Machine Learning Research (TMLR) and MLRC Journal Track, 2025. Code available at: https://github.com/joshrosie/FACT29", "summary": "Large Language Models (LLMs) demonstrate significant potential in multi-agent negotiation tasks, yet evaluation in this domain remains challenging due to a lack of robust and generalizable benchmarks. Abdelnabi et al. (2024) introduce a negotiation benchmark based on Scoreable Games, with the aim of developing a highly complex and realistic evaluation framework for LLMs. Our work investigates the reproducibility of claims in their benchmark, and provides a deeper understanding of its usability and generalizability. We replicate the original experiments on additional models, and introduce additional metrics to verify negotiation quality and evenness of evaluation. Our findings reveal that while the benchmark is indeed complex, model comparison is ambiguous, raising questions about its objectivity. Furthermore, we identify limitations in the experimental setup, particularly in information leakage detection and thoroughness of the ablation study. By examining and analyzing the behavior of a wider range of models on an extended version of the benchmark, we reveal insights that provide additional context to potential users. Our results highlight the importance of context in model-comparative evaluations.", "AI": {"tldr": "This paper investigates the reproducibility and validity of Abdelnabi et al.'s (2024) negotiation benchmark for LLMs, finding issues with objectivity, information leakage, and experimental thoroughness.", "motivation": "While LLMs show promise in multi-agent negotiation, existing benchmarks lack robustness and generalizability. The authors aim to verify the reproducibility and validity of Abdelnabi et al.'s proposed Scoreable Games benchmark, examining its objectivity and usability for model comparison.", "method": "The researchers replicate original experiments on additional LLM models, introduce new metrics to assess negotiation quality and evaluation fairness, conduct extended benchmark testing on a wider range of models, and analyze information leakage detection and ablation study completeness.", "result": "The benchmark is confirmed to be complex, but model comparisons are ambiguous and raise objectivity concerns. Limitations include inadequate information leakage detection and insufficient ablation studies. Testing more models reveals important contextual factors for comparative evaluations.", "conclusion": "The study highlights critical issues with the negotiation benchmark's objectivity and experimental design, emphasizing that context is crucial for meaningful model comparisons in multi-agent negotiation evaluations."}}
{"id": "2602.17947", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17947", "abs": "https://arxiv.org/abs/2602.17947", "authors": ["Yubo Zhou", "Jun Shu", "Junmin Liu", "Deyu Meng"], "title": "Understanding the Generalization of Bilevel Programming in Hyperparameter Optimization: A Tale of Bias-Variance Decomposition", "comment": null, "summary": "Gradient-based hyperparameter optimization (HPO) have emerged recently, leveraging bilevel programming techniques to optimize hyperparameter by estimating hypergradient w.r.t. validation loss. Nevertheless, previous theoretical works mainly focus on reducing the gap between the estimation and ground-truth (i.e., the bias), while ignoring the error due to data distribution (i.e., the variance), which degrades performance. To address this issue, we conduct a bias-variance decomposition for hypergradient estimation error and provide a supplemental detailed analysis of the variance term ignored by previous works. We also present a comprehensive analysis of the error bounds for hypergradient estimation. This facilitates an easy explanation of some phenomena commonly observed in practice, like overfitting to the validation set. Inspired by the derived theories, we propose an ensemble hypergradient strategy to reduce the variance in HPO algorithms effectively. Experimental results on tasks including regularization hyperparameter learning, data hyper-cleaning, and few-shot learning demonstrate that our variance reduction strategy improves hypergradient estimation. To explain the improved performance, we establish a connection between excess error and hypergradient estimation, offering some understanding of empirical observations.", "AI": {"tldr": "The paper analyzes bias-variance decomposition in hypergradient estimation for HPO, proposes ensemble strategy to reduce variance, and shows improved performance on various tasks.", "motivation": "Previous theoretical works on gradient-based HPO focus only on bias in hypergradient estimation while ignoring variance, which degrades performance and causes issues like overfitting to validation set.", "method": "Conduct bias-variance decomposition for hypergradient estimation error, provide comprehensive error bounds analysis, and propose ensemble hypergradient strategy to reduce variance in HPO algorithms.", "result": "Experimental results on regularization hyperparameter learning, data hyper-cleaning, and few-shot learning demonstrate improved hypergradient estimation with variance reduction strategy.", "conclusion": "The paper establishes connection between excess error and hypergradient estimation, providing theoretical understanding of empirical observations and showing variance reduction improves HPO performance."}}
{"id": "2602.18277", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.18277", "abs": "https://arxiv.org/abs/2602.18277", "authors": ["Finn van der Knaap", "Kejiang Qian", "Zheng Xu", "Fengxiang He"], "title": "PRISM: Parallel Reward Integration with Symmetry for MORL", "comment": null, "summary": "This work studies heterogeneous Multi-Objective Reinforcement Learning (MORL), where objectives can differ sharply in temporal frequency. Such heterogeneity allows dense objectives to dominate learning, while sparse long-horizon rewards receive weak credit assignment, leading to poor sample efficiency. We propose a Parallel Reward Integration with Symmetry (PRISM) algorithm that enforces reflectional symmetry as an inductive bias in aligning reward channels. PRISM introduces ReSymNet, a theory-motivated model that reconciles temporal-frequency mismatches across objectives, using residual blocks to learn a scaled opportunity value that accelerates exploration while preserving the optimal policy. We also propose SymReg, a reflectional equivariance regulariser that enforces agent mirroring and constrains policy search to a reflection-equivariant subspace. This restriction provably reduces hypothesis complexity and improves generalisation. Across MuJoCo benchmarks, PRISM consistently outperforms both a sparse-reward baseline and an oracle trained with full dense rewards, improving Pareto coverage and distributional balance: it achieves hypervolume gains exceeding 100\\% over the baseline and up to 32\\% over the oracle. The code is at \\href{https://github.com/EVIEHub/PRISM}{https://github.com/EVIEHub/PRISM}.", "AI": {"tldr": "PRISM algorithm addresses heterogeneous MORL with temporal-frequency mismatches using reflectional symmetry bias to improve sample efficiency and Pareto coverage.", "motivation": "In heterogeneous Multi-Objective RL, objectives with different temporal frequencies cause dense objectives to dominate learning while sparse long-horizon rewards receive weak credit assignment, leading to poor sample efficiency.", "method": "Proposes PRISM algorithm with ReSymNet (theory-motivated model using residual blocks to learn scaled opportunity value) and SymReg (reflectional equivariance regularizer that enforces agent mirroring and constrains policy search to reflection-equivariant subspace).", "result": "Outperforms sparse-reward baseline and oracle with full dense rewards across MuJoCo benchmarks, achieving hypervolume gains exceeding 100% over baseline and up to 32% over oracle, improving Pareto coverage and distributional balance.", "conclusion": "PRISM effectively addresses temporal-frequency mismatches in heterogeneous MORL through reflectional symmetry inductive bias, reducing hypothesis complexity and improving generalization while preserving optimal policies."}}
{"id": "2602.17948", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17948", "abs": "https://arxiv.org/abs/2602.17948", "authors": ["Yu Bai", "Zhe Wang", "Jiarui Zhang", "Dong-Xiao Zhang", "Yinjun Gao", "Jun-Jie Zhang"], "title": "A Geometric Probe of the Accuracy-Robustness Trade-off: Sharp Boundaries in Symmetry-Breaking Dimensional Expansion", "comment": "22 pages, 3 figures", "summary": "The trade-off between clean accuracy and adversarial robustness is a pervasive phenomenon in deep learning, yet its geometric origin remains elusive. In this work, we utilize Symmetry-Breaking Dimensional Expansion (SBDE) as a controlled probe to investigate the mechanism underlying this trade-off. SBDE expands input images by inserting constant-valued pixels, which breaks translational symmetry and consistently improves clean accuracy (e.g., from $90.47\\%$ to $95.63\\%$ on CIFAR-10 with ResNet-18) by reducing parameter degeneracy. However, this accuracy gain comes at the cost of reduced robustness against iterative white-box attacks. By employing a test-time \\emph{mask projection} that resets the inserted auxiliary pixels to their training values, we demonstrate that the vulnerability stems almost entirely from the inserted dimensions. The projection effectively neutralizes the attacks and restores robustness, revealing that the model achieves high accuracy by creating \\emph{sharp boundaries} (steep loss gradients) specifically along the auxiliary axes. Our findings provide a concrete geometric explanation for the accuracy-robustness paradox: the optimization landscape deepens the basin of attraction to improve accuracy but inevitably erects steep walls along the auxiliary degrees of freedom, creating a fragile sensitivity to off-manifold perturbations.", "AI": {"tldr": "SBDE improves clean accuracy by breaking symmetry but reduces adversarial robustness due to sharp boundaries in auxiliary dimensions; mask projection restores robustness.", "motivation": "To understand the geometric origin of the trade-off between clean accuracy and adversarial robustness in deep learning, which remains elusive despite being a pervasive phenomenon.", "method": "Use Symmetry-Breaking Dimensional Expansion (SBDE) as a controlled probe: expand input images by inserting constant-valued pixels to break translational symmetry. Employ test-time mask projection that resets inserted auxiliary pixels to their training values to analyze vulnerability sources.", "result": "SBDE consistently improves clean accuracy (e.g., from 90.47% to 95.63% on CIFAR-10 with ResNet-18) by reducing parameter degeneracy, but reduces robustness against iterative white-box attacks. Mask projection neutralizes attacks and restores robustness, revealing vulnerability stems almost entirely from inserted dimensions. Model achieves high accuracy by creating sharp boundaries (steep loss gradients) specifically along auxiliary axes.", "conclusion": "Provides concrete geometric explanation for accuracy-robustness paradox: optimization landscape deepens basin of attraction to improve accuracy but inevitably erects steep walls along auxiliary degrees of freedom, creating fragile sensitivity to off-manifold perturbations."}}
{"id": "2602.18292", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18292", "abs": "https://arxiv.org/abs/2602.18292", "authors": ["Xiaotong Ji", "Rasul Tutunov", "Matthieu Zimmer", "Haitham Bou-Ammar"], "title": "Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers", "comment": null, "summary": "Decoding sits between a language model and everything we do with it, yet it is still treated as a heuristic knob-tuning exercise. We argue decoding should be understood as a principled optimisation layer: at each token, we solve a regularised problem over the probability simplex that trades off model score against structural preferences and constraints. This single template recovers greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax-style sparsity as special cases, and explains their common structure through optimality conditions. More importantly, the framework makes it easy to invent new decoders without folklore. We demonstrate this by designing Best-of-K (BoK), a KL-anchored coverage objective aimed at multi-sample pipelines (self-consistency, reranking, verifier selection). BoK targets the probability of covering good alternatives within a fixed K-sample budget and improves empirical performance. We show that such samples can improve accuracy by, for example, +18.6% for Qwen2.5-Math-7B on MATH500 at high sampling temperatures.", "AI": {"tldr": "The paper proposes a unified framework for decoding as a principled optimization layer, recovering existing methods as special cases and enabling new decoder designs like Best-of-K for improved performance.", "motivation": "Decoding is currently treated as heuristic knob-tuning between language models and their applications, lacking a principled foundation despite being a critical component.", "method": "Proposes viewing decoding as solving regularized optimization problems over probability simplex at each token, trading off model score against structural preferences and constraints. This framework recovers existing methods and enables new decoder designs like Best-of-K (BoK) with KL-anchored coverage objectives.", "result": "The framework explains common structure of existing decoders through optimality conditions. Best-of-K improves empirical performance, achieving +18.6% accuracy improvement for Qwen2.5-Math-7B on MATH500 at high sampling temperatures.", "conclusion": "Decoding should be understood as a principled optimization layer rather than heuristic tuning, enabling systematic decoder design and improved performance through novel objectives like Best-of-K for multi-sample pipelines."}}
{"id": "2602.17952", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17952", "abs": "https://arxiv.org/abs/2602.17952", "authors": ["Hu Lou", "Yin-Jun Gao", "Dong-Xiao Zhang", "Tai-Jiao Du", "Jun-Jie Zhang", "Jia-Rui Zhang"], "title": "Hardware-Friendly Input Expansion for Accelerating Function Approximation", "comment": "22 pages, 4 figures", "summary": "One-dimensional function approximation is a fundamental problem in scientific computing and engineering applications. While neural networks possess powerful universal approximation capabilities, their optimization process is often hindered by flat loss landscapes induced by parameter-space symmetries, leading to slow convergence and poor generalization, particularly for high-frequency components. Inspired by the principle of \\emph{symmetry breaking} in physics, this paper proposes a hardware-friendly approach for function approximation through \\emph{input-space expansion}. The core idea involves augmenting the original one-dimensional input (e.g., $x$) with constant values (e.g., $\u03c0$) to form a higher-dimensional vector (e.g., $[\u03c0, \u03c0, x, \u03c0, \u03c0]$), effectively breaking parameter symmetries without increasing the network's parameter count. We evaluate the method on ten representative one-dimensional functions, including smooth, discontinuous, high-frequency, and non-differentiable functions. Experimental results demonstrate that input-space expansion significantly accelerates training convergence (reducing LBFGS iterations by 12\\% on average) and enhances approximation accuracy (reducing final MSE by 66.3\\% for the optimal 5D expansion). Ablation studies further reveal the effects of different expansion dimensions and constant selections, with $\u03c0$ consistently outperforming other constants. Our work proposes a low-cost, efficient, and hardware-friendly technique for algorithm design.", "AI": {"tldr": "A hardware-friendly method using input-space expansion with constants like \u03c0 to break parameter symmetries in neural networks for 1D function approximation, improving convergence speed and accuracy.", "motivation": "Neural networks for 1D function approximation suffer from slow convergence and poor generalization due to parameter-space symmetries creating flat loss landscapes, especially for high-frequency components.", "method": "Input-space expansion: augment original 1D input (x) with constant values (e.g., \u03c0) to form higher-dimensional vectors (e.g., [\u03c0, \u03c0, x, \u03c0, \u03c0]), breaking parameter symmetries without increasing network parameters.", "result": "Significantly accelerates training (12% reduction in LBFGS iterations) and enhances accuracy (66.3% MSE reduction for optimal 5D expansion). \u03c0 consistently outperforms other constants in ablation studies.", "conclusion": "Proposes a low-cost, efficient, hardware-friendly technique for algorithm design that improves neural network performance for 1D function approximation through symmetry breaking via input-space expansion."}}
{"id": "2602.17958", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17958", "abs": "https://arxiv.org/abs/2602.17958", "authors": ["Aida Afshar", "Yuke Zhang", "Aldo Pacchiano"], "title": "Bayesian Online Model Selection", "comment": null, "summary": "Online model selection in Bayesian bandits raises a fundamental exploration challenge: When an environment instance is sampled from a prior distribution, how can we design an adaptive strategy that explores multiple bandit learners and competes with the best one in hindsight? We address this problem by introducing a new Bayesian algorithm for online model selection in stochastic bandits. We prove an oracle-style guarantee of $O\\left( d^* M \\sqrt{T} + \\sqrt{(MT)} \\right)$ on the Bayesian regret, where $M$ is the number of base learners, $d^*$ is the regret coefficient of the optimal base learner, and $T$ is the time horizon. We also validate our method empirically across a range of stochastic bandit settings, demonstrating performance that is competitive with the best base learner. Additionally, we study the effect of sharing data among base learners and its role in mitigating prior mis-specification.", "AI": {"tldr": "Bayesian algorithm for online model selection in stochastic bandits achieves O(d*M\u221aT + \u221a(MT)) Bayesian regret, where M is number of base learners, d* is optimal learner's regret coefficient, and T is time horizon.", "motivation": "Address the exploration challenge in Bayesian bandits when sampling from prior distributions: how to design adaptive strategies that explore multiple bandit learners and compete with the best one in hindsight.", "method": "Introduce a new Bayesian algorithm for online model selection in stochastic bandits, with theoretical analysis of Bayesian regret bounds and empirical validation across various stochastic bandit settings.", "result": "Prove oracle-style guarantee of O(d*M\u221aT + \u221a(MT)) on Bayesian regret, validate empirically that performance is competitive with best base learner, and study data sharing's role in mitigating prior mis-specification.", "conclusion": "The proposed Bayesian algorithm effectively addresses online model selection in stochastic bandits with strong theoretical guarantees and practical performance, while data sharing helps handle prior mis-specification."}}
{"id": "2602.18308", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18308", "abs": "https://arxiv.org/abs/2602.18308", "authors": ["Biswa Sengupta", "Jinhua Wang", "Leo Brunswic"], "title": "JPmHC Dynamical Isometry via Orthogonal Hyper-Connections", "comment": null, "summary": "Recent advances in deep learning, exemplified by Hyper-Connections (HC), have expanded the residual connection paradigm by introducing wider residual streams and diverse connectivity patterns. While these innovations yield significant performance gains, they compromise the identity mapping property of residual connections, leading to training instability, limited scalability, and increased memory overhead. To address these challenges, we propose JPmHC (Jacobian-spectrum Preserving manifold-constrained Hyper-Connections), a framework that replaces identity skips with a trainable linear mixer acting on n parallel streams while explicitly controlling gradient conditioning. By constraining the mixer M on operator-norm-bounded manifolds (e.g., bistochastic, Stiefel, Grassmann), JPmHC prevents gradient pathologies and enhances stability. JPmHC introduces three key contributions: (i) a free-probability analysis that predicts Jacobian spectra for structured skips, providing actionable design rules for mixer selection; (ii) memory-efficient implicit differentiation for fixed-point projections, reducing activation memory and synchronization overhead; and (iii) a Stiefel-constrained mixer via Cayley transforms, ensuring orthogonality without post-hoc normalization. Empirical evaluations on ARC-AGI demonstrate that JPmHC achieves faster convergence, higher accuracy, and lower computational cost compared to bistochastic baselines. As a flexible and scalable extension of HC, JPmHC advances spectrum-aware, stable, and efficient deep learning, offering insights into topological architecture design and foundational model evolution.", "AI": {"tldr": "JPmHC is a framework that replaces identity skips in hyper-connections with trainable linear mixers on parallel streams while controlling gradient conditioning through operator-norm-bounded manifolds, improving stability and efficiency.", "motivation": "Hyper-connections improve performance but compromise identity mapping, causing training instability, limited scalability, and memory overhead. Need to maintain performance gains while restoring stability.", "method": "Replaces identity skips with trainable linear mixer on n parallel streams, constraining mixer M on operator-norm-bounded manifolds (bistochastic, Stiefel, Grassmann). Uses free-probability analysis for Jacobian spectra prediction, memory-efficient implicit differentiation for fixed-point projections, and Stiefel-constrained mixer via Cayley transforms.", "result": "JPmHC achieves faster convergence, higher accuracy, and lower computational cost compared to bistochastic baselines on ARC-AGI benchmarks. Prevents gradient pathologies and enhances stability while maintaining performance gains.", "conclusion": "JPmHC provides a flexible, scalable extension of hyper-connections that advances spectrum-aware, stable, and efficient deep learning, offering insights into topological architecture design and foundational model evolution."}}
{"id": "2602.17962", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17962", "abs": "https://arxiv.org/abs/2602.17962", "authors": ["Shuo Sun", "Meiling Zhou", "Chen Zhao", "Joyce H. Keyak", "Nancy E. Lane", "Jeffrey D. Deng", "Kuan-Jui Su", "Hui Shen", "Hong-Wen Deng", "Kui Zhang", "Weihua Zhou"], "title": "Improving Generalizability of Hip Fracture Risk Prediction via Domain Adaptation Across Multiple Cohorts", "comment": "26 pages, 3 tables, 1 figure", "summary": "Clinical risk prediction models often fail to be generalized across cohorts because underlying data distributions differ by clinical site, region, demographics, and measurement protocols. This limitation is particularly pronounced in hip fracture risk prediction, where the performance of models trained on one cohort (the source cohort) can degrade substantially when deployed in other cohorts (target cohorts). We used a shared set of clinical and DXA-derived features across three large cohorts - the Study of Osteoporotic Fractures (SOF), the Osteoporotic Fractures in Men Study (MrOS), and the UK Biobank (UKB), to systematically evaluate the performance of three domain adaptation methods - Maximum Mean Discrepancy (MMD), Correlation Alignment (CORAL), and Domain - Adversarial Neural Networks (DANN) and their combinations. For a source cohort with males only and a source cohort with females only, domain-adaptation methods consistently showed improved performance than the no-adaptation baseline (source-only training), and the use of combinations of multiple domain adaptation methods delivered the largest and most stable gains. The method that combines MMD, CORAL, and DANN achieved the highest discrimination with the area under curve (AUC) of 0.88 for a source cohort with males only and 0.95 for a source cohort with females only), demonstrating that integrating multiple domain adaptation methods could produce feature representations that are less sensitive to dataset differences. Unlike existing methods that rely heavily on supervised tuning or assume known outcomes of samples in target cohorts, our outcome-free approaches enable the model selection under realistic deployment conditions and improve generalization of models in hip fracture risk prediction.", "AI": {"tldr": "Domain adaptation methods improve hip fracture risk prediction across cohorts by reducing dataset distribution differences without requiring target cohort outcomes.", "motivation": "Clinical risk prediction models often fail to generalize across cohorts due to distribution differences in clinical sites, regions, demographics, and measurement protocols, particularly problematic in hip fracture risk prediction where performance degrades when deployed from source to target cohorts.", "method": "Systematically evaluated three domain adaptation methods (Maximum Mean Discrepancy/MMD, Correlation Alignment/CORAL, and Domain-Adversarial Neural Networks/DANN) and their combinations across three large cohorts (SOF, MrOS, UK Biobank) using shared clinical and DXA-derived features. Used outcome-free approaches that don't require target cohort outcomes.", "result": "Domain adaptation methods consistently outperformed no-adaptation baselines. The combination of MMD, CORAL, and DANN achieved highest discrimination (AUC 0.88 for male-only source cohort, 0.95 for female-only source cohort). Multiple method combinations delivered largest and most stable gains.", "conclusion": "Integrating multiple domain adaptation methods produces feature representations less sensitive to dataset differences, enabling model selection under realistic deployment conditions and improving generalization in hip fracture risk prediction without requiring target cohort outcomes."}}
{"id": "2602.17972", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17972", "abs": "https://arxiv.org/abs/2602.17972", "authors": ["Sebastian Felipe R. Bundoc", "Paula Joy B. Martinez", "Sebastian C. Iba\u00f1ez", "Erika Fille T. Legara"], "title": "Student Flow Modeling for School Decongestion via Stochastic Gravity Estimation and Constrained Spatial Allocation", "comment": null, "summary": "School congestion, where student enrollment exceeds school capacity, is a major challenge in low- and middle-income countries. It highly impacts learning outcomes and deepens inequities in education. While subsidy programs that transfer students from public to private schools offer a mechanism to alleviate congestion without capital-intensive construction, they often underperform due to fragmented data systems that hinder effective implementation. The Philippine Educational Service Contracting program, one of the world's largest educational subsidy programs, exemplifies these challenges, falling short of its goal to decongest public schools. This prevents the science-based and data-driven analyses needed to understand what shapes student enrollment flows, particularly how families respond to economic incentives and spatial constraints. We introduce a computational framework for modeling student flow patterns and simulating policy scenarios. By synthesizing heterogeneous government data across nearly 3,000 institutions, we employ a stochastic gravity model estimated via negative binomial regression to derive behavioral elasticities for distance, net tuition cost, and socioeconomic determinants. These elasticities inform a doubly constrained spatial allocation mechanism that simulates student redistribution under varying subsidy amounts while respecting both origin candidate pools and destination slot capacities. We find that geographic proximity constrains school choice four times more strongly than tuition cost and that slot capacity, not subsidy amounts, is the binding constraint. Our work demonstrates that subsidy programs alone cannot resolve systemic overcrowding, and computational modeling can empower education policymakers to make equitable, data-driven decisions by revealing the structural constraints that shape effective resource allocation, even when resources are limited.", "AI": {"tldr": "A computational framework using stochastic gravity models reveals that geographic proximity constrains school choice 4x more than tuition costs, and slot capacity (not subsidy amounts) is the binding constraint in addressing school congestion through subsidy programs.", "motivation": "School congestion in low/middle-income countries impacts learning outcomes and deepens inequities. Subsidy programs (like Philippines' ESC program) underperform due to fragmented data systems preventing data-driven analysis of student enrollment flows, particularly how families respond to economic incentives and spatial constraints.", "method": "Computational framework synthesizing heterogeneous government data across ~3,000 institutions. Uses stochastic gravity model estimated via negative binomial regression to derive behavioral elasticities for distance, net tuition cost, and socioeconomic determinants. These inform a doubly constrained spatial allocation mechanism simulating student redistribution under varying subsidy amounts while respecting origin candidate pools and destination slot capacities.", "result": "Geographic proximity constrains school choice four times more strongly than tuition cost. Slot capacity, not subsidy amounts, is the binding constraint for addressing school congestion. Subsidy programs alone cannot resolve systemic overcrowding.", "conclusion": "Computational modeling empowers education policymakers to make equitable, data-driven decisions by revealing structural constraints that shape effective resource allocation, even with limited resources. The approach demonstrates that understanding spatial and capacity constraints is crucial for effective policy design beyond just increasing subsidies."}}
{"id": "2602.18384", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18384", "abs": "https://arxiv.org/abs/2602.18384", "authors": ["Fotios Zantalis", "Evangelos Zervas", "Grigorios Koulouras"], "title": "FedZMG: Efficient Client-Side Optimization in Federated Learning", "comment": null, "summary": "Federated Learning (FL) enables distributed model training on edge devices while preserving data privacy. However, clients tend to have non-Independent and Identically Distributed (non-IID) data, which often leads to client-drift, and therefore diminishing convergence speed and model performance. While adaptive optimizers have been proposed to mitigate these effects, they frequently introduce computational complexity or communication overhead unsuitable for resource-constrained IoT environments. This paper introduces Federated Zero Mean Gradients (FedZMG), a novel, parameter-free, client-side optimization algorithm designed to tackle client-drift by structurally regularizing the optimization space. Advancing the idea of Gradient Centralization, FedZMG projects local gradients onto a zero-mean hyperplane, effectively neutralizing the \"intensity\" or \"bias\" shifts inherent in heterogeneous data distributions without requiring additional communication or hyperparameter tuning. A theoretical analysis is provided, proving that FedZMG reduces the effective gradient variance and guarantees tighter convergence bounds compared to standard FedAvg. Extensive empirical evaluations on EMNIST, CIFAR100, and Shakespeare datasets demonstrate that FedZMG achieves better convergence speed and final validation accuracy compared to the baseline FedAvg and the adaptive optimizer FedAdam, particularly in highly non-IID settings.", "AI": {"tldr": "FedZMG is a parameter-free client-side optimization algorithm for federated learning that projects gradients to zero-mean hyperplane to reduce client-drift in non-IID data without extra communication or hyperparameter tuning.", "motivation": "Federated learning faces client-drift issues due to non-IID data distributions, which reduces convergence speed and model performance. Existing adaptive optimizers often introduce computational complexity or communication overhead unsuitable for resource-constrained IoT environments.", "method": "FedZMG projects local gradients onto a zero-mean hyperplane, neutralizing \"intensity\" or \"bias\" shifts in heterogeneous data distributions. This structural regularization reduces effective gradient variance without requiring additional communication or hyperparameter tuning.", "result": "Theoretical analysis shows FedZMG reduces gradient variance and guarantees tighter convergence bounds than FedAvg. Empirical evaluations on EMNIST, CIFAR100, and Shakespeare datasets demonstrate better convergence speed and final validation accuracy compared to FedAvg and FedAdam, especially in highly non-IID settings.", "conclusion": "FedZMG effectively addresses client-drift in federated learning with non-IID data through simple gradient projection, offering improved performance without the computational overhead or communication requirements of existing adaptive optimizers."}}
{"id": "2602.17975", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17975", "abs": "https://arxiv.org/abs/2602.17975", "authors": ["Robert Parker"], "title": "Generating adversarial inputs for a graph neural network model of AC power flow", "comment": null, "summary": "This work formulates and solves optimization problems to generate input points that yield high errors between a neural network's predicted AC power flow solution and solutions to the AC power flow equations. We demonstrate this capability on an instance of the CANOS-PF graph neural network model, as implemented by the PF$\u0394$ benchmark library, operating on a 14-bus test grid. Generated adversarial points yield errors as large as 3.4 per-unit in reactive power and 0.08 per-unit in voltage magnitude. When minimizing the perturbation from a training point necessary to satisfy adversarial constraints, we find that the constraints can be met with as little as an 0.04 per-unit perturbation in voltage magnitude on a single bus. This work motivates the development of rigorous verification and robust training methods for neural network surrogate models of AC power flow.", "AI": {"tldr": "Paper generates adversarial inputs to maximize errors in neural network AC power flow predictions, achieving up to 3.4 per-unit reactive power errors with minimal perturbations.", "motivation": "To demonstrate vulnerabilities in neural network surrogate models for AC power flow and motivate development of rigorous verification and robust training methods for these critical power system models.", "method": "Formulates and solves optimization problems to generate adversarial input points that maximize errors between neural network predictions and actual AC power flow solutions. Tests on CANOS-PF graph neural network model from PF\u0394 benchmark library on 14-bus test grid.", "result": "Generated adversarial points yield errors as large as 3.4 per-unit in reactive power and 0.08 per-unit in voltage magnitude. Constraints can be met with minimal perturbations (0.04 per-unit voltage magnitude change on single bus).", "conclusion": "Neural network surrogate models for AC power flow are vulnerable to adversarial attacks, highlighting the need for rigorous verification and robust training methods to ensure reliability in power system applications."}}
{"id": "2602.18401", "categories": ["cs.LG", "cs.AI", "q-bio.NC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.18401", "abs": "https://arxiv.org/abs/2602.18401", "authors": ["Josue Casco-Rodriguez", "Nanda H. Krishna", "Richard G. Baraniuk"], "title": "Leakage and Second-Order Dynamics Improve Hippocampal RNN Replay", "comment": null, "summary": "Biological neural networks (like the hippocampus) can internally generate \"replay\" resembling stimulus-driven activity. Recent computational models of replay use noisy recurrent neural networks (RNNs) trained to path-integrate. Replay in these networks has been described as Langevin sampling, but new modifiers of noisy RNN replay have surpassed this description. We re-examine noisy RNN replay as sampling to understand or improve it in three ways: (1) Under simple assumptions, we prove that the gradients replay activity should follow are time-varying and difficult to estimate, but readily motivate the use of hidden state leakage in RNNs for replay. (2) We confirm that hidden state adaptation (negative feedback) encourages exploration in replay, but show that it incurs non-Markov sampling that also slows replay. (3) We propose the first model of temporally compressed replay in noisy path-integrating RNNs through hidden state momentum, connect it to underdamped Langevin sampling, and show that, together with adaptation, it counters slowness while maintaining exploration. We verify our findings via path-integration of 2D triangular and T-maze paths and of high-dimensional paths of synthetic rat place cell activity.", "AI": {"tldr": "Noisy RNNs trained for path integration can generate replay-like activity. The paper re-examines this as sampling, showing hidden state leakage helps replay, adaptation encourages exploration but slows it, and momentum enables faster compressed replay while maintaining exploration.", "motivation": "To better understand and improve replay generation in noisy recurrent neural networks trained for path integration, moving beyond the simple Langevin sampling description to address limitations like slow replay and enable temporally compressed replay.", "method": "Theoretical analysis of replay as sampling with time-varying gradients, investigation of hidden state leakage and adaptation effects, and proposal of hidden state momentum for compressed replay connected to underdamped Langevin sampling. Validation through path-integration tasks on 2D triangular/T-maze paths and high-dimensional synthetic rat place cell activity.", "result": "Hidden state leakage helps replay by addressing time-varying gradient estimation; adaptation encourages exploration but causes non-Markov sampling that slows replay; momentum enables temporally compressed replay that counters slowness while maintaining exploration.", "conclusion": "The paper provides a refined sampling perspective on noisy RNN replay, showing how hidden state leakage, adaptation, and momentum work together to enable effective replay generation with both exploration and temporal compression capabilities."}}
{"id": "2602.18409", "categories": ["cs.LG", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.18409", "abs": "https://arxiv.org/abs/2602.18409", "authors": ["Huan Luo", "Jonni Virtema"], "title": "Unifying approach to uniform expressivity of graph neural networks", "comment": null, "summary": "The expressive power of Graph Neural Networks (GNNs) is often analysed via correspondence to the Weisfeiler-Leman (WL) algorithm and fragments of first-order logic. Standard GNNs are limited to performing aggregation over immediate neighbourhoods or over global read-outs. To increase their expressivity, recent attempts have been made to incorporate substructural information (e.g. cycle counts and subgraph properties). In this paper, we formalize this architectural trend by introducing Template GNNs (T-GNNs), a generalized framework where node features are updated by aggregating over valid template embeddings from a specified set of graph templates. We propose a corresponding logic, Graded template modal logic (GML(T)), and generalized notions of template-based bisimulation and WL algorithm. We establish an equivalence between the expressive power of T-GNNs and GML(T), and provide a unifying approach for analysing GNN expressivity: we show how standard AC-GNNs and its recent variants can be interpreted as instantiations of T-GNNs.", "AI": {"tldr": "T-GNNs generalize GNNs by aggregating over template embeddings, with corresponding logic GML(T) and template-based bisimulation/WL algorithm, providing a unifying framework for analyzing GNN expressivity.", "motivation": "Standard GNNs are limited to immediate neighborhood or global aggregation, lacking expressivity for substructural information. Recent attempts to incorporate substructural features (cycle counts, subgraph properties) need formalization.", "method": "Introduce Template GNNs (T-GNNs) where node features are updated by aggregating over valid template embeddings from specified graph templates. Propose Graded template modal logic (GML(T)) and generalized template-based bisimulation and WL algorithm.", "result": "Establish equivalence between expressive power of T-GNNs and GML(T). Show how standard AC-GNNs and recent variants can be interpreted as instantiations of T-GNNs, providing a unifying framework for GNN expressivity analysis.", "conclusion": "T-GNNs formalize the architectural trend of incorporating substructural information, offering a generalized framework that unifies various GNN variants and provides tools (logic, bisimulation, WL) for analyzing their expressivity."}}
{"id": "2602.17985", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17985", "abs": "https://arxiv.org/abs/2602.17985", "authors": ["Ryan O'Dowd"], "title": "Learning Without Training", "comment": "PhD Dissertation of Ryan O'Dowd, defended successfully at Claremont Graduate University on 1/28/2026", "summary": "Machine learning is at the heart of managing the real-world problems associated with massive data. With the success of neural networks on such large-scale problems, more research in machine learning is being conducted now than ever before. This dissertation focuses on three different projects rooted in mathematical theory for machine learning applications.\n  The first project deals with supervised learning and manifold learning. In theory, one of the main problems in supervised learning is that of function approximation: that is, given some data set $\\mathcal{D}=\\{(x_j,f(x_j))\\}_{j=1}^M$, can one build a model $F\\approx f$? We introduce a method which aims to remedy several of the theoretical shortcomings of the current paradigm for supervised learning.\n  The second project deals with transfer learning, which is the study of how an approximation process or model learned on one domain can be leveraged to improve the approximation on another domain. We study such liftings of functions when the data is assumed to be known only on a part of the whole domain. We are interested in determining subsets of the target data space on which the lifting can be defined, and how the local smoothness of the function and its lifting are related.\n  The third project is concerned with the classification task in machine learning, particularly in the active learning paradigm. Classification has often been treated as an approximation problem as well, but we propose an alternative approach leveraging techniques originally introduced for signal separation problems. We introduce theory to unify signal separation with classification and a new algorithm which yields competitive accuracy to other recent active learning algorithms while providing results much faster.", "AI": {"tldr": "This dissertation presents three machine learning projects with mathematical foundations: 1) improved supervised learning via function approximation, 2) transfer learning for function lifting across domains, and 3) classification using signal separation techniques in active learning.", "motivation": "The motivation is to address theoretical shortcomings in current machine learning paradigms by developing mathematically-grounded approaches for supervised learning, transfer learning, and classification problems.", "method": "1) A new method for supervised learning addressing theoretical shortcomings of current function approximation approaches. 2) Study of function lifting in transfer learning when data is known only on part of the domain. 3) Classification using signal separation techniques with a new algorithm for active learning.", "result": "The dissertation develops theoretical frameworks for each problem area and introduces new algorithms. The third project's algorithm achieves competitive accuracy to recent active learning methods while providing results much faster.", "conclusion": "The dissertation demonstrates how mathematical theory can address fundamental challenges in machine learning across three key areas: supervised learning, transfer learning, and classification, providing both theoretical insights and practical algorithmic improvements."}}
{"id": "2602.18002", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18002", "abs": "https://arxiv.org/abs/2602.18002", "authors": ["Junfei Sun", "Dixi Yao", "Xuchen Gong", "Tahseen Rabbani", "Manzil Zaheer", "Tian Li"], "title": "Asynchronous Heavy-Tailed Optimization", "comment": "8-page main body, 25-page appendix, 5 figures", "summary": "Heavy-tailed stochastic gradient noise, commonly observed in transformer models, can destabilize the optimization process. Recent works mainly focus on developing and understanding approaches to address heavy-tailed noise in the centralized or distributed, synchronous setting, leaving the interactions between such noise and asynchronous optimization underexplored. In this work, we investigate two communication schemes that handle stragglers with asynchronous updates in the presence of heavy-tailed gradient noise. We propose and theoretically analyze algorithmic modifications based on delay-aware learning rate scheduling and delay compensation to enhance the performance of asynchronous algorithms. Our convergence guarantees under heavy-tailed noise match the rate of the synchronous counterparts and improve delay tolerance compared with existing asynchronous approaches. Empirically, our approaches outperform prior synchronous and asynchronous methods in terms of accuracy/runtime trade-offs and are more robust to hyperparameters in both image and language tasks.", "AI": {"tldr": "The paper addresses heavy-tailed gradient noise in asynchronous optimization for transformer models, proposing delay-aware learning rate scheduling and delay compensation methods that outperform existing synchronous and asynchronous approaches.", "motivation": "Heavy-tailed stochastic gradient noise in transformer models destabilizes optimization, and while existing work focuses on centralized/distributed synchronous settings, the interaction between such noise and asynchronous optimization remains underexplored.", "method": "Proposes two communication schemes for handling stragglers with asynchronous updates: algorithmic modifications based on delay-aware learning rate scheduling and delay compensation to enhance asynchronous algorithm performance under heavy-tailed noise.", "result": "Theoretical convergence guarantees match synchronous counterparts' rates and improve delay tolerance compared to existing asynchronous approaches. Empirical results show better accuracy/runtime trade-offs and hyperparameter robustness in image and language tasks.", "conclusion": "The proposed delay-aware methods effectively address heavy-tailed noise in asynchronous optimization, providing superior performance and robustness compared to both synchronous and existing asynchronous approaches for transformer models."}}
{"id": "2602.18055", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18055", "abs": "https://arxiv.org/abs/2602.18055", "authors": ["Jingyang Qiao", "Zhizhong Zhang", "Xin Tan", "Jingyu Gong", "Yanyun Qu", "Yuan Xie"], "title": "Continual-NExT: A Unified Comprehension And Generation Continual Learning Framework", "comment": null, "summary": "Dual-to-Dual MLLMs refer to Multimodal Large Language Models, which can enable unified multimodal comprehension and generation through text and image modalities. Although exhibiting strong instantaneous learning and generalization capabilities, Dual-to-Dual MLLMs still remain deficient in lifelong evolution, significantly affecting continual adaptation to dynamic real-world scenarios. One of the challenges is that learning new tasks inevitably destroys the learned knowledge. Beyond traditional catastrophic forgetting, Dual-to-Dual MLLMs face other challenges, including hallucination, instruction unfollowing, and failures in cross-modal knowledge transfer. However, no standardized continual learning framework for Dual-to-Dual MLLMs has been established yet, leaving these challenges unexplored. Thus, in this paper, we establish Continual-NExT, a continual learning framework for Dual-to-Dual MLLMs with deliberately-architected evaluation metrics. To improve the continual learning capability of Dual-to-Dual MLLMs, we propose an efficient MAGE (Mixture and Aggregation of General LoRA and Expert LoRA) method to further facilitate knowledge transfer across modalities and mitigate forgetting. Extensive experiments demonstrate that MAGE outperforms other continual learning methods and achieves state-of-the-art performance.", "AI": {"tldr": "Continual-NExT framework and MAGE method for Dual-to-Dual MLLMs to address lifelong learning challenges including catastrophic forgetting, hallucination, and cross-modal knowledge transfer.", "motivation": "Dual-to-Dual MLLMs lack standardized continual learning frameworks, making them deficient in lifelong evolution and adaptation to dynamic real-world scenarios. They face challenges like catastrophic forgetting, hallucination, instruction unfollowing, and cross-modal knowledge transfer failures.", "method": "Proposes Continual-NExT framework with evaluation metrics and MAGE (Mixture and Aggregation of General LoRA and Expert LoRA) method to facilitate knowledge transfer across modalities and mitigate forgetting.", "result": "Extensive experiments show MAGE outperforms other continual learning methods and achieves state-of-the-art performance.", "conclusion": "The Continual-NExT framework and MAGE method effectively address continual learning challenges in Dual-to-Dual MLLMs, enabling better lifelong evolution and adaptation capabilities."}}
{"id": "2602.18060", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18060", "abs": "https://arxiv.org/abs/2602.18060", "authors": ["Abhay Shinde", "Aryan Amit Barsainyan", "Jose Siguenza", "Ankita Vaishnobi Bisoi", "Rakshit Kr. Singh", "Bharath Ramsundar"], "title": "Deepmechanics", "comment": "11 pages, 7 figures, Submitted to KDD 2026", "summary": "Physics-informed deep learning models have emerged as powerful tools for learning dynamical systems. These models directly encode physical principles into network architectures. However, systematic benchmarking of these approaches across diverse physical phenomena remains limited, particularly in conservative and dissipative systems. In addition, benchmarking that has been done thus far does not integrate out full trajectories to check stability. In this work, we benchmark three prominent physics-informed architectures such as Hamiltonian Neural Networks (HNN), Lagrangian Neural Networks (LNN), and Symplectic Recurrent Neural Networks (SRNN) using the DeepChem framework, an open-source scientific machine learning library. We evaluate these models on six dynamical systems spanning classical conservative mechanics (mass-spring system, simple pendulum, double pendulum, and three-body problem, spring-pendulum) and non-conservative systems with contact (bouncing ball). We evaluate models by computing error on predicted trajectories and evaluate error both quantitatively and qualitatively. We find that all benchmarked models struggle to maintain stability for chaotic or nonconservative systems. Our results suggest that more research is needed for physics-informed deep learning models to learn robust models of classical mechanical systems.", "AI": {"tldr": "Benchmarking of physics-informed neural networks (HNN, LNN, SRNN) on 6 dynamical systems reveals stability issues in chaotic and non-conservative systems.", "motivation": "Systematic benchmarking of physics-informed deep learning models is limited, especially for conservative/dissipative systems, and existing benchmarks don't check long-term trajectory stability.", "method": "Benchmarked three physics-informed architectures (Hamiltonian Neural Networks, Lagrangian Neural Networks, Symplectic Recurrent Neural Networks) using DeepChem framework on six dynamical systems spanning classical conservative mechanics and non-conservative systems with contact.", "result": "All benchmarked models struggle to maintain stability for chaotic or nonconservative systems, showing limitations in learning robust models of classical mechanical systems.", "conclusion": "More research is needed for physics-informed deep learning models to learn robust models of classical mechanical systems, particularly for handling chaotic and non-conservative dynamics."}}
{"id": "2602.18084", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18084", "abs": "https://arxiv.org/abs/2602.18084", "authors": ["Benjamin Honor\u00e9", "Alba Carballo-Castro", "Yiming Qin", "Pascal Frossard"], "title": "Balancing Symmetry and Efficiency in Graph Flow Matching", "comment": "15 pages, 11 figures", "summary": "Equivariance is central to graph generative models, as it ensures the model respects the permutation symmetry of graphs. However, strict equivariance can increase computational cost due to added architectural constraints, and can slow down convergence because the model must be consistent across a large space of possible node permutations. We study this trade-off for graph generative models. Specifically, we start from an equivariant discrete flow-matching model, and relax its equivariance during training via a controllable symmetry modulation scheme based on sinusoidal positional encodings and node permutations. Experiments first show that symmetry-breaking can accelerate early training by providing an easier learning signal, but at the expense of encouraging shortcut solutions that can cause overfitting, where the model repeatedly generates graphs that are duplicates of the training set. On the contrary, properly modulating the symmetry signal can delay overfitting while accelerating convergence, allowing the model to reach stronger performance with $19\\%$ of the baseline training epochs.", "AI": {"tldr": "The paper explores relaxing strict equivariance in graph generative models to balance computational efficiency and convergence speed, showing that controlled symmetry-breaking can accelerate training while avoiding overfitting.", "motivation": "Strict equivariance in graph generative models increases computational costs and slows convergence due to architectural constraints and the need for consistency across all node permutations. There's a trade-off between equivariance benefits and these practical limitations.", "method": "Start from an equivariant discrete flow-matching model and relax its equivariance during training using a controllable symmetry modulation scheme based on sinusoidal positional encodings and node permutations.", "result": "Symmetry-breaking accelerates early training by providing easier learning signals but can cause overfitting (model generates duplicates of training graphs). Proper symmetry modulation delays overfitting while accelerating convergence, achieving stronger performance with only 19% of baseline training epochs.", "conclusion": "Controlled relaxation of equivariance in graph generative models offers a practical solution to the trade-off between symmetry preservation and training efficiency, enabling faster convergence to better performance while avoiding overfitting pitfalls."}}
{"id": "2602.18109", "categories": ["cs.LG", "cs.OS", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18109", "abs": "https://arxiv.org/abs/2602.18109", "authors": ["Rong Fu", "Yibo Meng", "Guangzhen Yao", "Jiaxuan Lu", "Zeyu Zhang", "Zhaolu Kang", "Ziming Guo", "Jia Yee Tan", "Xiaojing Du", "Simon James Fong"], "title": "TempoNet: Slack-Quantized Transformer-Guided Reinforcement Scheduler for Adaptive Deadline-Centric Real-Time Dispatchs", "comment": "43 pages, 12 figures", "summary": "Real-time schedulers must reason about tight deadlines under strict compute budgets. We present TempoNet, a reinforcement learning scheduler that pairs a permutation-invariant Transformer with a deep Q-approximation. An Urgency Tokenizer discretizes temporal slack into learnable embeddings, stabilizing value learning and capturing deadline proximity. A latency-aware sparse attention stack with blockwise top-k selection and locality-sensitive chunking enables global reasoning over unordered task sets with near-linear scaling and sub-millisecond inference. A multicore mapping layer converts contextualized Q-scores into processor assignments through masked-greedy selection or differentiable matching. Extensive evaluations on industrial mixed-criticality traces and large multiprocessor settings show consistent gains in deadline fulfillment over analytic schedulers and neural baselines, together with improved optimization stability. Diagnostics include sensitivity analyses for slack quantization, attention-driven policy interpretation, hardware-in-the-loop and kernel micro-benchmarks, and robustness under stress with simple runtime mitigations; we also report sample-efficiency benefits from behavioral-cloning pretraining and compatibility with an actor-critic variant without altering the inference pipeline. These results establish a practical framework for Transformer-based decision making in high-throughput real-time scheduling.", "AI": {"tldr": "TempoNet is a Transformer-based RL scheduler that uses urgency tokenization and sparse attention for efficient real-time scheduling with improved deadline fulfillment.", "motivation": "Real-time schedulers need to handle tight deadlines under strict compute constraints, requiring efficient decision-making that traditional analytic schedulers struggle with.", "method": "Combines permutation-invariant Transformer with deep Q-learning, urgency tokenizer for temporal slack discretization, latency-aware sparse attention with blockwise top-k selection, and multicore mapping layer for processor assignments.", "result": "Consistent gains in deadline fulfillment over analytic schedulers and neural baselines, with near-linear scaling, sub-millisecond inference, improved optimization stability, and sample-efficiency benefits.", "conclusion": "Establishes a practical framework for Transformer-based decision making in high-throughput real-time scheduling with demonstrated effectiveness across various evaluations."}}
{"id": "2602.18114", "categories": ["cs.LG", "cs.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.18114", "abs": "https://arxiv.org/abs/2602.18114", "authors": ["Yiding Feng", "Jiashuo Jiang", "Yige Wang"], "title": "Non-Stationary Online Resource Allocation: Learning from a Single Sample", "comment": null, "summary": "We study online resource allocation under non-stationary demand with a minimum offline data requirement. In this problem, a decision-maker must allocate multiple types of resources to sequentially arriving queries over a finite horizon. Each query belongs to a finite set of types with fixed resource consumption and a stochastic reward drawn from an unknown, type-specific distribution. Critically, the environment exhibits arbitrary non-stationarity -- arrival distributions may shift unpredictably-while the algorithm requires only one historical sample per period to operate effectively. We distinguish two settings based on sample informativeness: (i) reward-observed samples containing both query type and reward realization, and (ii) the more challenging type-only samples revealing only query type information.\n  We propose a novel type-dependent quantile-based meta-policy that decouples the problem into modular components: reward distribution estimation, optimization of target service probabilities via fluid relaxation, and real-time decisions through dynamic acceptance thresholds. For reward-observed samples, our static threshold policy achieves $\\tilde{O}(\\sqrt{T})$ regret. For type-only samples, we first establish that sublinear regret is impossible without additional structure; under a mild minimum-arrival-probability assumption, we design both a partially adaptive policy attaining the same $\\tilde{O}({T})$ bound and, more significantly, a fully adaptive resolving policy with careful rounding that achieves the first poly-logarithmic regret guarantee of $O((\\log T)^3)$ for non-stationary multi-resource allocation. Our framework advances prior work by operating with minimal offline data (one sample per period), handling arbitrary non-stationarity without variation-budget assumptions, and supporting multiple resource constraints.", "AI": {"tldr": "Online resource allocation with non-stationary demand using minimal offline data (one sample per period). Proposes type-dependent quantile-based meta-policy achieving sublinear regret for reward-observed samples and poly-logarithmic regret for type-only samples under mild assumptions.", "motivation": "Address online resource allocation under arbitrary non-stationary demand with minimal offline data requirement. Many real-world applications have unpredictable demand shifts but limited historical data. Need algorithms that work with just one sample per period while handling multiple resource constraints.", "method": "Type-dependent quantile-based meta-policy that decouples problem into: 1) reward distribution estimation, 2) optimization of target service probabilities via fluid relaxation, and 3) real-time decisions through dynamic acceptance thresholds. For reward-observed samples: static threshold policy. For type-only samples: partially adaptive policy and fully adaptive resolving policy with careful rounding.", "result": "For reward-observed samples: achieves $\\tilde{O}(\\sqrt{T})$ regret. For type-only samples: sublinear regret impossible without additional structure; under minimum-arrival-probability assumption, partially adaptive policy achieves $\\tilde{O}(\\sqrt{T})$ and fully adaptive resolving policy achieves $O((\\log T)^3)$ poly-logarithmic regret.", "conclusion": "Framework advances prior work by operating with minimal offline data (one sample per period), handling arbitrary non-stationarity without variation-budget assumptions, and supporting multiple resource constraints. Achieves first poly-logarithmic regret guarantee for non-stationary multi-resource allocation with type-only samples."}}
{"id": "2602.18131", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18131", "abs": "https://arxiv.org/abs/2602.18131", "authors": ["Tom Potter", "Oliver Rhodes"], "title": "Learning Long-Range Dependencies with Temporal Predictive Coding", "comment": null, "summary": "Predictive Coding (PC) is a biologically-inspired learning framework characterised by local, parallelisable operations, properties that enable energy-efficient implementation on neuromorphic hardware. Despite this, extending PC effectively to recurrent neural networks (RNNs) has been challenging, particularly for tasks involving long-range temporal dependencies. Backpropagation Through Time (BPTT) remains the dominant method for training RNNs, but its non-local computation, lack of spatial parallelism, and requirement to store extensive activation histories results in significant energy consumption. This work introduces a novel method combining Temporal Predictive Coding (tPC) with approximate Real-Time Recurrent Learning (RTRL), enabling effective spatio-temporal credit assignment. Results indicate that the proposed method can closely match the performance of BPTT on both synthetic benchmarks and real-world tasks. On a challenging machine translation task, with a 15-million parameter model, the proposed method achieves a test perplexity of 7.62 (vs. 7.49 for BPTT), marking one of the first applications of tPC to tasks of this scale. These findings demonstrate the potential of this method to learn complex temporal dependencies whilst retaining the local, parallelisable, and flexible properties of the original PC framework, paving the way for more energy-efficient learning systems.", "AI": {"tldr": "Combines Temporal Predictive Coding with approximate RTRL to enable local, parallelizable training of RNNs that matches BPTT performance while being more energy-efficient.", "motivation": "Predictive Coding has desirable properties for neuromorphic hardware (local, parallelizable operations) but struggles with RNNs and long-range temporal dependencies. BPTT dominates RNN training but is energy-inefficient due to non-local computation, lack of parallelism, and extensive memory requirements.", "method": "Novel combination of Temporal Predictive Coding (tPC) with approximate Real-Time Recurrent Learning (RTRL) to enable effective spatio-temporal credit assignment while maintaining local, parallelizable operations.", "result": "Method closely matches BPTT performance on synthetic benchmarks and real-world tasks. On a 15M parameter machine translation task, achieves test perplexity of 7.62 vs. 7.49 for BPTT - one of first tPC applications at this scale.", "conclusion": "Demonstrates potential to learn complex temporal dependencies while retaining PC's local, parallelizable, flexible properties, paving way for more energy-efficient learning systems."}}
{"id": "2602.18141", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18141", "abs": "https://arxiv.org/abs/2602.18141", "authors": ["Pierre-Gabriel Berlureau", "Ali Hariri", "Victor Kawasaki-Borruat", "Mia Zosso", "Pierre Vandergheynst"], "title": "Advection-Diffusion on Graphs: A Bakry-Emery Laplacian for Spectral Graph Neural Networks", "comment": null, "summary": "Graph Neural Networks (GNNs) often struggle to propagate information across long distances due to oversmoothing and oversquashing. Existing remedies such as graph transformers or rewiring typically incur high computational cost or require altering the graph structure. We introduce a Bakry-Emery graph Laplacian that integrates diffusion and advection through a learnable node-wise potential, inducing task-dependent propagation dynamics without modifying topology. This operator has a well-behaved spectral decomposition and acts as a drop-in replacement for standard Laplacians in spectral GNNs. Building on this insight, we develop mu-ChebNet, a spectral architecture that jointly learns the potential and Chebyshev filters, effectively bridging message-passing adaptivity and spectral efficiency. Our theoretical analysis shows how the potential modulates the spectrum, enabling control of key graph properties. Empirically, mu-ChebNet delivers consistent gains on synthetic long-range reasoning tasks, as well as real-world benchmarks, while offering an interpretable routing field that reveals how information flows through the graph. This establishes the Bakry-Emery Laplacian as a principled and efficient foundation for adaptive spectral graph learning.", "AI": {"tldr": "The paper introduces a Bakry-Emery graph Laplacian with learnable node potentials to address oversmoothing/oversquashing in GNNs, and develops mu-ChebNet as an efficient spectral architecture that jointly learns potentials and filters without modifying graph topology.", "motivation": "GNNs struggle with long-distance information propagation due to oversmoothing and oversquashing. Existing solutions like graph transformers or rewiring are computationally expensive or require altering graph structure, creating a need for more efficient approaches.", "method": "Introduces a Bakry-Emery graph Laplacian that integrates diffusion and advection through learnable node-wise potentials, enabling task-dependent propagation without topology changes. Develops mu-ChebNet, a spectral architecture that jointly learns potentials and Chebyshev filters, combining message-passing adaptivity with spectral efficiency.", "result": "Theoretical analysis shows how the potential modulates the spectrum, enabling control of key graph properties. Empirically, mu-ChebNet achieves consistent gains on synthetic long-range reasoning tasks and real-world benchmarks, while providing interpretable routing fields that reveal information flow patterns.", "conclusion": "The Bakry-Emery Laplacian serves as a principled and efficient foundation for adaptive spectral graph learning, offering a drop-in replacement for standard Laplacians that addresses GNN limitations without computational or structural overhead."}}
{"id": "2602.18146", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2602.18146", "abs": "https://arxiv.org/abs/2602.18146", "authors": ["Lionel Salesses", "Larbi Arbaoui", "Tariq Benamara", "Arnaud Francois", "Caroline Sainvitu"], "title": "Stable Long-Horizon Spatiotemporal Prediction on Meshes Using Latent Multiscale Recurrent Graph Neural Networks", "comment": null, "summary": "Accurate long-horizon prediction of spatiotemporal fields on complex geometries is a fundamental challenge in scientific machine learning, with applications such as additive manufacturing where temperature histories govern defect formation and mechanical properties. High-fidelity simulations are accurate but computationally costly, and despite recent advances, machine learning methods remain challenged by long-horizon temperature and gradient prediction. We propose a deep learning framework for predicting full temperature histories directly on meshes, conditioned on geometry and process parameters, while maintaining stability over thousands of time steps and generalizing across heterogeneous geometries. The framework adopts a temporal multiscale architecture composed of two coupled models operating at complementary time scales. Both models rely on a latent recurrent graph neural network to capture spatiotemporal dynamics on meshes, while a variational graph autoencoder provides a compact latent representation that reduces memory usage and improves training stability. Experiments on simulated powder bed fusion data demonstrate accurate and temporally stable long-horizon predictions across diverse geometries, outperforming existing baseline. Although evaluated in two dimensions, the framework is general and extensible to physics-driven systems with multiscale dynamics and to three-dimensional geometries.", "AI": {"tldr": "A deep learning framework for long-horizon temperature prediction on complex geometries using temporal multiscale architecture with graph neural networks and variational autoencoders.", "motivation": "Accurate long-horizon prediction of spatiotemporal fields on complex geometries is crucial for applications like additive manufacturing, where temperature histories affect defect formation and mechanical properties. High-fidelity simulations are computationally expensive, and existing machine learning methods struggle with long-horizon temperature and gradient prediction.", "method": "The framework uses a temporal multiscale architecture with two coupled models operating at complementary time scales. Both models employ latent recurrent graph neural networks to capture spatiotemporal dynamics on meshes. A variational graph autoencoder provides compact latent representations to reduce memory usage and improve training stability.", "result": "Experiments on simulated powder bed fusion data demonstrate accurate and temporally stable long-horizon predictions across diverse geometries, outperforming existing baselines. The framework maintains stability over thousands of time steps and generalizes across heterogeneous geometries.", "conclusion": "The proposed deep learning framework effectively addresses long-horizon temperature prediction challenges on complex geometries. Although evaluated in 2D, the framework is generalizable to physics-driven systems with multiscale dynamics and can be extended to 3D geometries."}}
{"id": "2602.18160", "categories": ["cs.LG", "cs.CC", "cs.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.18160", "abs": "https://arxiv.org/abs/2602.18160", "authors": ["Shahaf Bassan", "Xuanxiang Huang", "Guy Katz"], "title": "Unifying Formal Explanations: A Complexity-Theoretic Perspective", "comment": "To appear in ICLR 2026", "summary": "Previous work has explored the computational complexity of deriving two fundamental types of explanations for ML model predictions: (1) *sufficient reasons*, which are subsets of input features that, when fixed, determine a prediction, and (2) *contrastive reasons*, which are subsets of input features that, when modified, alter a prediction. Prior studies have examined these explanations in different contexts, such as non-probabilistic versus probabilistic frameworks and local versus global settings. In this study, we introduce a unified framework for analyzing these explanations, demonstrating that they can all be characterized through the minimization of a unified probabilistic value function. We then prove that the complexity of these computations is influenced by three key properties of the value function: (1) *monotonicity*, (2) *submodularity*, and (3) *supermodularity* - which are three fundamental properties in *combinatorial optimization*. Our findings uncover some counterintuitive results regarding the nature of these properties within the explanation settings examined. For instance, although the *local* value functions do not exhibit monotonicity or submodularity/supermodularity whatsoever, we demonstrate that the *global* value functions do possess these properties. This distinction enables us to prove a series of novel polynomial-time results for computing various explanations with provable guarantees in the global explainability setting, across a range of ML models that span the interpretability spectrum, such as neural networks, decision trees, and tree ensembles. In contrast, we show that even highly simplified versions of these explanations become NP-hard to compute in the corresponding local explainability setting.", "AI": {"tldr": "Unified framework shows computational complexity of ML explanations depends on monotonicity, submodularity, and supermodularity properties, with polynomial-time global explanations but NP-hard local explanations.", "motivation": "Previous work has studied sufficient reasons and contrastive explanations separately in different contexts (probabilistic vs non-probabilistic, local vs global). This paper aims to create a unified framework to analyze these explanations systematically and understand their computational complexity.", "method": "Introduces a unified framework where all explanations can be characterized through minimization of a unified probabilistic value function. Analyzes computational complexity based on three combinatorial optimization properties: monotonicity, submodularity, and supermodularity of the value function.", "result": "Finds counterintuitive results: local value functions lack monotonicity/submodularity/supermodularity, while global value functions possess these properties. Enables polynomial-time computation of explanations with guarantees for global explainability across neural networks, decision trees, and tree ensembles. Shows simplified local explanations are NP-hard.", "conclusion": "The unified framework reveals fundamental computational complexity differences between local and global explainability settings, with global explanations being computationally tractable due to combinatorial optimization properties, while local explanations remain computationally hard."}}
{"id": "2602.18168", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18168", "abs": "https://arxiv.org/abs/2602.18168", "authors": ["Danning Jing", "Xinhai Chen", "Xifeng Pu", "Jie Hu", "Chao Huang", "Xuguang Chen", "Qinglin Wang", "Jie Liu"], "title": "A Deep Surrogate Model for Robust and Generalizable Long-Term Blast Wave Prediction", "comment": null, "summary": "Accurately modeling the spatio-temporal dynamics of blast wave propagation remains a longstanding challenge due to its highly nonlinear behavior, sharp gradients, and burdensome computational cost. While machine learning-based surrogate models offer fast inference as a promising alternative, they suffer from degraded accuracy, particularly evaluated on complex urban layouts or out-of-distribution scenarios. Moreover, autoregressive prediction strategies in such models are prone to error accumulation over long forecasting horizons, limiting their robustness for extended-time simulations. To address these limitations, we propose RGD-Blast, a robust and generalizable deep surrogate model for high-fidelity, long-term blast wave forecasting. RGD-Blast incorporates a multi-scale module to capture both global flow patterns and local boundary interactions, effectively mitigating error accumulation during autoregressive prediction. We introduce a dynamic-static feature coupling mechanism that fuses time-varying pressure fields with static source and layout features, thereby enhancing out-of-distribution generalization. Experiments demonstrate that RGD-Blast achieves a two-order-of-magnitude speedup over traditional numerical methods while maintaining comparable accuracy. In generalization tests on unseen building layouts, the model achieves an average RMSE below 0.01 and an R2 exceeding 0.89 over 280 consecutive time steps. Additional evaluations under varying blast source locations and explosive charge weights further validate its generalization, substantially advancing the state of the art in long-term blast wave modeling.", "AI": {"tldr": "RGD-Blast is a deep learning surrogate model for accurate, long-term blast wave forecasting that achieves 100x speedup over traditional methods while maintaining high accuracy and strong generalization to unseen scenarios.", "motivation": "Traditional blast wave modeling is computationally expensive and struggles with nonlinear behavior and sharp gradients. Existing ML surrogates suffer from accuracy degradation, especially on complex urban layouts or out-of-distribution scenarios, and autoregressive prediction accumulates errors over long time horizons.", "method": "RGD-Blast incorporates a multi-scale module to capture both global flow patterns and local boundary interactions, reducing error accumulation in autoregressive prediction. It uses a dynamic-static feature coupling mechanism that fuses time-varying pressure fields with static source and layout features to enhance out-of-distribution generalization.", "result": "The model achieves two-order-of-magnitude speedup over traditional numerical methods while maintaining comparable accuracy. In generalization tests on unseen building layouts, it achieves average RMSE below 0.01 and R\u00b2 exceeding 0.89 over 280 consecutive time steps. Additional evaluations under varying blast source locations and explosive charge weights further validate its generalization capabilities.", "conclusion": "RGD-Blast substantially advances the state of the art in long-term blast wave modeling by providing a robust, generalizable, and computationally efficient surrogate model that addresses key limitations of both traditional methods and existing ML approaches."}}
{"id": "2602.18181", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18181", "abs": "https://arxiv.org/abs/2602.18181", "authors": ["Jihun Kim", "Namhoon Lee"], "title": "SeedFlood: A Step Toward Scalable Decentralized Training of LLMs", "comment": null, "summary": "This work presents a new approach to decentralized training-SeedFlood-designed to scale for large models across complex network topologies and achieve global consensus with minimal communication overhead. Traditional gossip-based methods suffer from message communication costs that grow with model size, while information decay over network hops renders global consensus inefficient. SeedFlood departs from these practices by exploiting the seed-reconstructible structure of zeroth-order updates and effectively making the messages near-zero in size, allowing them to be flooded to every client in the network. This mechanism makes communication overhead negligible and independent of model size, removing the primary scalability bottleneck in decentralized training. Consequently, SeedFlood enables training in regimes previously considered impractical, such as billion-parameter models distributed across hundreds of clients. Our experiments on decentralized LLM fine-tuning demonstrate thatSeedFlood consistently outperforms gossip-based baselines in both generalization performance and communication efficiency, and even achieves results comparable to first-order methods in large scale settings.", "AI": {"tldr": "SeedFlood enables decentralized training of large models with near-zero communication overhead by exploiting zeroth-order updates' seed-reconstructible structure, allowing message flooding across networks.", "motivation": "Traditional gossip-based decentralized training suffers from high communication costs that scale with model size and information decay over network hops, making global consensus inefficient and limiting scalability for large models.", "method": "SeedFlood exploits the seed-reconstructible structure of zeroth-order updates to make messages near-zero in size, allowing them to be flooded to every client in the network. This approach makes communication overhead negligible and independent of model size.", "result": "SeedFlood outperforms gossip-based baselines in both generalization performance and communication efficiency for decentralized LLM fine-tuning, achieving results comparable to first-order methods in large-scale settings and enabling training of billion-parameter models across hundreds of clients.", "conclusion": "SeedFlood removes the primary scalability bottleneck in decentralized training by making communication overhead negligible and independent of model size, enabling practical training of large models in previously impractical regimes."}}
{"id": "2602.18196", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18196", "abs": "https://arxiv.org/abs/2602.18196", "authors": ["Xiuying Wei", "Caglar Gulcehre"], "title": "RAT+: Train Dense, Infer Sparse -- Recurrence Augmented Attention for Dilated Inference", "comment": null, "summary": "Structured dilated attention has an appealing inference-time efficiency knob: it reduces the FLOPs of the attention and the KV cache size by a factor of the dilation size D, while preserving long-range connectivity. However, we find a persistent failure mode of them -- sparsifying a pretrained attention model to a dilated pattern leads to severe accuracy degradation. We introduce RAT+, a dense-pretraining architecture that augments attention with full-sequence recurrence and active recurrence learning. A single RAT+ model is pretrained densely once, then flexibly switched at inference time to dilated attention (optionally with local windows) or hybrid layer/head compositions, requiring only a short 1B-token resolution adaptation rather than retraining separate sparse models. At 1.5B parameters trained on 100B tokens, RAT+ closely matches dense accuracy at 16 and drops by about 2-3 points at 64 on commonsense reasoning and LongBench tasks, respectively. Moreover, RAT+ outperforms attention when sparsifying to the top-k block attention. We further scale to 2.6B parameters and 200B tokens and observe the same trend.", "AI": {"tldr": "RAT+ is a dense-pretraining architecture that uses full-sequence recurrence to enable flexible inference-time switching to dilated attention patterns without retraining separate sparse models.", "motivation": "Structured dilated attention has inference-time efficiency benefits but suffers from severe accuracy degradation when sparsifying pretrained attention models to dilated patterns. There's a need for a single model that can flexibly switch to different attention patterns at inference without retraining separate models.", "method": "RAT+ augments attention with full-sequence recurrence and active recurrence learning during dense pretraining. This allows a single model to be adapted at inference time to various attention patterns (dilated, local windows, hybrid compositions) with only short resolution adaptation rather than full retraining.", "result": "At 1.5B parameters trained on 100B tokens, RAT+ closely matches dense accuracy at dilation 16 and drops only 2-3 points at dilation 64 on commonsense reasoning and LongBench tasks. It also outperforms attention when sparsifying to top-k block attention. The trend holds when scaling to 2.6B parameters and 200B tokens.", "conclusion": "RAT+ provides an efficient solution for inference-time flexibility, enabling a single densely pretrained model to support various sparse attention patterns with minimal accuracy degradation and without needing to retrain separate sparse models."}}
{"id": "2602.18216", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18216", "abs": "https://arxiv.org/abs/2602.18216", "authors": ["Georgi Hrusanov", "Oliver Y. Ch\u00e9n", "Julien S. Bodelet"], "title": "Generative Model via Quantile Assignment", "comment": null, "summary": "Deep Generative models (DGMs) play two key roles in modern machine learning: (i) producing new information (e.g., image synthesis) and (ii) reducing dimensionality. However, traditional architectures often rely on auxiliary networks such as encoders in Variational Autoencoders (VAEs) or discriminators in Generative Adversarial Networks (GANs), which introduce training instability, computational overhead, and risks like mode collapse. We present NeuroSQL, a new generative paradigm that eliminates the need for auxiliary networks by learning low-dimensional latent representations implicitly. NeuroSQL leverages an asymptotic approximation that expresses the latent variables as the solution to an optimal transportation problem. Specifically, NeuroSQL learns the latent variables by solving a linear assignment problem and then passes the latent information to a standalone generator. We benchmark its performance against GANs, VAEs, and a budget-matched diffusion baseline on four datasets: handwritten digits (MNIST), faces (CelebA), animal faces (AFHQ), and brain images (OASIS). Compared to VAEs, GANs, and diffusion models: (1) in terms of image quality, NeuroSQL achieves overall lower mean pixel distance between synthetic and authentic images and stronger perceptual/structural fidelity; (2) computationally, NeuroSQL requires the least training time; and (3) practically, NeuroSQL provides an effective solution for generating synthetic data with limited training samples. By embracing quantile assignment rather than an encoder, NeuroSQL provides a fast, stable, and robust way to generate synthetic data with minimal information loss.", "AI": {"tldr": "NeuroSQL is a new generative model that eliminates auxiliary networks (encoders/discriminators) by learning latent representations implicitly through optimal transport, offering faster training and better image quality than GANs, VAEs, and diffusion models.", "motivation": "Traditional deep generative models (VAEs, GANs) rely on auxiliary networks that introduce training instability, computational overhead, and risks like mode collapse. There's a need for a simpler, more stable generative paradigm without these auxiliary components.", "method": "NeuroSQL learns low-dimensional latent representations implicitly using an asymptotic approximation that expresses latent variables as solutions to an optimal transportation problem. It solves a linear assignment problem to learn latent variables, then passes this information to a standalone generator.", "result": "On MNIST, CelebA, AFHQ, and OASIS datasets: (1) achieves lower mean pixel distance and better perceptual/structural fidelity than VAEs, GANs, and diffusion models; (2) requires least training time; (3) works well with limited training samples.", "conclusion": "NeuroSQL provides a fast, stable, and robust alternative to traditional generative models by using quantile assignment instead of encoders, offering minimal information loss and effective synthetic data generation."}}
{"id": "2602.18227", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18227", "abs": "https://arxiv.org/abs/2602.18227", "authors": ["Redwanul Karim", "Changhun Kim", "Timon Conrad", "Nora Gourmelon", "Julian Oelhaf", "David Riebesel", "Tom\u00e1s Arias-Vergara", "Andreas Maier", "Johann J\u00e4ger", "Siming Bayer"], "title": "Parameter-Efficient Domain Adaptation of Physics-Informed Self-Attention based GNNs for AC Power Flow Prediction", "comment": null, "summary": "Accurate AC-PF prediction under domain shift is critical when models trained on medium-voltage (MV) grids are deployed on high-voltage (HV) networks. Existing physics-informed graph neural solvers typically rely on full fine-tuning for cross-regime transfer, incurring high retraining cost and offering limited control over the stability-plasticity trade-off between target-domain adaptation and source-domain retention. We study parameter-efficient domain adaptation for physics-informed self-attention based GNN, encouraging Kirchhoff-consistent behavior via a physics-based loss while restricting adaptation to low-rank updates. Specifically, we apply LoRA to attention projections with selective unfreezing of the prediction head to regulate adaptation capacity. This design yields a controllable efficiency-accuracy trade-off for physics-constrained inverse estimation under voltage-regime shift. Across multiple grid topologies, the proposed LoRA+PHead adaptation recovers near-full fine-tuning accuracy with a target-domain RMSE gap of $2.6\\times10^{-4}$ while reducing the number of trainable parameters by 85.46%. The physics-based residual remains comparable to full fine-tuning; however, relative to Full FT, LoRA+PHead reduces MV source retention by 4.7 percentage points (17.9% vs. 22.6%) under domain shift, while still enabling parameter-efficient and physically consistent AC-PF estimation.", "AI": {"tldr": "Parameter-efficient domain adaptation for physics-informed GNNs using LoRA with selective head unfreezing enables accurate AC power flow prediction under voltage regime shifts while reducing trainable parameters by 85%.", "motivation": "Existing physics-informed graph neural solvers require full fine-tuning for cross-regime transfer (MV to HV grids), which is costly and offers poor control over the stability-plasticity trade-off between target-domain adaptation and source-domain retention.", "method": "Apply LoRA (Low-Rank Adaptation) to attention projections in physics-informed self-attention GNNs, with selective unfreezing of the prediction head to regulate adaptation capacity. This maintains Kirchhoff-consistent behavior via physics-based loss while restricting adaptation to low-rank updates.", "result": "Achieves near-full fine-tuning accuracy with target-domain RMSE gap of 2.6\u00d710\u207b\u2074 while reducing trainable parameters by 85.46%. Physics-based residual remains comparable to full fine-tuning, though source retention decreases by 4.7 percentage points under domain shift.", "conclusion": "LoRA+PHead adaptation provides a controllable efficiency-accuracy trade-off for physics-constrained inverse estimation under voltage-regime shift, enabling parameter-efficient and physically consistent AC-PF estimation across grid topologies."}}
{"id": "2602.18248", "categories": ["cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.18248", "abs": "https://arxiv.org/abs/2602.18248", "authors": ["Pietro Sittoni", "Emanuele Zangrando", "Angelo A. Casulli", "Nicola Guglielmi", "Francesco Tudisco"], "title": "Neural-HSS: Hierarchical Semi-Separable Neural PDE Solver", "comment": null, "summary": "Deep learning-based methods have shown remarkable effectiveness in solving PDEs, largely due to their ability to enable fast simulations once trained. However, despite the availability of high-performance computing infrastructure, many critical applications remain constrained by the substantial computational costs associated with generating large-scale, high-quality datasets and training models. In this work, inspired by studies on the structure of Green's functions for elliptic PDEs, we introduce Neural-HSS, a parameter-efficient architecture built upon the Hierarchical Semi-Separable (HSS) matrix structure that is provably data-efficient for a broad class of PDEs. We theoretically analyze the proposed architecture, proving that it satisfies exactness properties even in very low-data regimes. We also investigate its connections with other architectural primitives, such as the Fourier neural operator layer and convolutional layers. We experimentally validate the data efficiency of Neural-HSS on the three-dimensional Poisson equation over a grid of two million points, demonstrating its superior ability to learn from data generated by elliptic PDEs in the low-data regime while outperforming baseline methods. Finally, we demonstrate its capability to learn from data arising from a broad class of PDEs in diverse domains, including electromagnetism, fluid dynamics, and biology.", "AI": {"tldr": "Neural-HSS: A parameter-efficient neural architecture based on Hierarchical Semi-Separable matrix structure that achieves data-efficient learning for elliptic PDEs and related problems.", "motivation": "Deep learning methods for PDEs require large computational costs for dataset generation and model training, limiting their applicability despite available computing infrastructure. The paper aims to develop a more data-efficient architecture inspired by Green's function structure.", "method": "Proposes Neural-HSS architecture built upon Hierarchical Semi-Separable (HSS) matrix structure, which is theoretically analyzed for exactness properties in low-data regimes. Connections with Fourier neural operator and convolutional layers are investigated.", "result": "Experimental validation on 3D Poisson equation over 2 million point grid shows superior data efficiency in low-data regimes compared to baselines. Also demonstrates capability to learn from diverse PDE domains including electromagnetism, fluid dynamics, and biology.", "conclusion": "Neural-HSS provides a parameter-efficient, data-efficient architecture for learning from PDE data, with proven theoretical properties and practical effectiveness across multiple application domains."}}
{"id": "2602.18250", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18250", "abs": "https://arxiv.org/abs/2602.18250", "authors": ["Yves Ruffenach"], "title": "Variational Distributional Neuron", "comment": "29 pages, 7 figures. Code available at GitHub (link in paper)", "summary": "We propose a proof of concept for a variational distributional neuron: a compute unit formulated as a VAE brick, explicitly carrying a prior, an amortized posterior and a local ELBO. The unit is no longer a deterministic scalar but a distribution: computing is no longer about propagating values, but about contracting a continuous space of possibilities under constraints. Each neuron parameterizes a posterior, propagates a reparameterized sample and is regularized by the KL term of a local ELBO - hence, the activation is distributional. This \"contraction\" becomes testable through local constraints and can be monitored via internal measures. The amount of contextual information carried by the unit, as well as the temporal persistence of this information, are locally tuned by distinct constraints. This proposal addresses a structural tension: in sequential generation, causality is predominantly organized in the symbolic space and, even when latents exist, they often remain auxiliary, while the effective dynamics are carried by a largely deterministic decoder. In parallel, probabilistic latent models capture factors of variation and uncertainty, but that uncertainty typically remains borne by global or parametric mechanisms, while units continue to propagate scalars - hence the pivot question: if uncertainty is intrinsic to computation, why does the compute unit not carry it explicitly? We therefore draw two axes: (i) the composition of probabilistic constraints, which must be made stable, interpretable and controllable; and (ii) granularity: if inference is a negotiation of distributions under constraints, should the primitive unit remain deterministic or become distributional? We analyze \"collapse\" modes and the conditions for a \"living neuron\", then extend the contribution over time via autoregressive priors over the latent, per unit.", "AI": {"tldr": "Proposes a variational distributional neuron as a VAE-based compute unit that carries explicit uncertainty, replacing deterministic scalar activations with distributions that contract possibility spaces under constraints.", "motivation": "Addresses structural tension in neural networks: symbolic causality vs. deterministic decoders in sequential generation, and global uncertainty mechanisms vs. deterministic unit propagation. Questions why compute units don't carry intrinsic uncertainty if uncertainty is fundamental to computation.", "method": "Designs neurons as VAE bricks with priors, amortized posteriors, and local ELBOs. Each neuron parameterizes a posterior, propagates reparameterized samples, and is regularized by KL divergence. Introduces local constraints for testable \"contraction\" and internal monitoring. Extends with autoregressive priors over latent variables per unit.", "result": "Conceptual framework for distributional neurons that can locally tune contextual information and temporal persistence through distinct constraints. Analyzes \"collapse\" modes and conditions for \"living neurons\" that maintain distributional properties rather than reverting to deterministic behavior.", "conclusion": "Proposes shifting neural network primitives from deterministic scalars to distributional units that explicitly carry uncertainty, enabling more interpretable and controllable probabilistic constraints at granular computational levels."}}
{"id": "2602.18253", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18253", "abs": "https://arxiv.org/abs/2602.18253", "authors": ["Xabier de Zuazo", "Vincenzo Verbeni", "Eva Navas", "Ibon Saratxaga", "Mathieu Bourguignon", "Nicola Molinaro"], "title": "MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data", "comment": "6 pages, 3 figures, 3 tables, submitted to Interspeech 2026", "summary": "Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity.", "AI": {"tldr": "Transfer learning with Conformer models enables cross-task speech decoding between perception and production using MEG data, achieving significant accuracy improvements with minimal fine-tuning data.", "motivation": "Data-efficient neural decoding is crucial for speech brain-computer interfaces, but collecting sufficient training data is challenging. The paper aims to demonstrate that transfer learning can overcome data limitations and enable cross-task decoding between speech perception and production.", "method": "Pre-train a Conformer-based model on 50 hours of single-subject listening data, then fine-tune on just 5 minutes per subject across 18 participants. Evaluate both in-task accuracy (perception\u2192perception, production\u2192production) and cross-task decoding (perception\u2192production, production\u2192perception).", "result": "Transfer learning yields consistent improvements: 1-4% in-task accuracy gains and larger cross-task gains of up to 5-6%. Models trained on speech production can decode passive listening above chance, confirming learned representations reflect shared neural processes rather than task-specific motor activity.", "conclusion": "Transfer learning enables data-efficient cross-task speech decoding, demonstrating that pre-trained models capture shared neural representations between speech perception and production, advancing speech brain-computer interface development."}}
{"id": "2602.18266", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18266", "abs": "https://arxiv.org/abs/2602.18266", "authors": ["Stefan Wahl", "Raphaela Schenk", "Ali Farnoud", "Jakob H. Macke", "Daniel Gedon"], "title": "A Probabilistic Framework for LLM-Based Model Discovery", "comment": null, "summary": "Automated methods for discovering mechanistic simulator models from observational data offer a promising path toward accelerating scientific progress. Such methods often take the form of agentic-style iterative workflows that repeatedly propose and revise candidate models by imitating human discovery processes. However, existing LLM-based approaches typically implement such workflows via hand-crafted heuristic procedures, without an explicit probabilistic formulation. We recast model discovery as probabilistic inference, i.e., as sampling from an unknown distribution over mechanistic models capable of explaining the data. This perspective provides a unified way to reason about model proposal, refinement, and selection within a single inference framework. As a concrete instantiation of this view, we introduce ModelSMC, an algorithm based on Sequential Monte Carlo sampling. ModelSMC represents candidate models as particles which are iteratively proposed and refined by an LLM, and weighted using likelihood-based criteria. Experiments on real-world scientific systems illustrate that this formulation discovers models with interpretable mechanisms and improves posterior predictive checks. More broadly, this perspective provides a probabilistic lens for understanding and developing LLM-based approaches to model discovery.", "AI": {"tldr": "The paper introduces ModelSMC, a probabilistic inference approach using Sequential Monte Carlo sampling for discovering mechanistic simulator models from observational data, improving over heuristic LLM-based methods.", "motivation": "Existing LLM-based approaches for model discovery use hand-crafted heuristic procedures without explicit probabilistic formulation. There's a need for a unified probabilistic framework to reason about model proposal, refinement, and selection systematically.", "method": "Recast model discovery as probabilistic inference (sampling from unknown distribution over mechanistic models). Introduce ModelSMC algorithm based on Sequential Monte Carlo sampling, representing candidate models as particles iteratively proposed/refined by LLMs and weighted using likelihood-based criteria.", "result": "Experiments on real-world scientific systems show that ModelSMC discovers models with interpretable mechanisms and improves posterior predictive checks compared to existing approaches.", "conclusion": "The probabilistic perspective provides a unified framework for understanding and developing LLM-based approaches to model discovery, offering systematic reasoning about model proposal, refinement, and selection within a single inference framework."}}
{"id": "2602.18301", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18301", "abs": "https://arxiv.org/abs/2602.18301", "authors": ["Ivan Bondarenko", "Egor Palkin", "Fedor Tikunov"], "title": "On the Semantic and Syntactic Information Encoded in Proto-Tokens for One-Step Text Reconstruction", "comment": null, "summary": "Autoregressive large language models (LLMs) generate text token-by-token, requiring n forward passes to produce a sequence of length n. Recent work, Exploring the Latent Capacity of LLMs for One-Step Text Reconstruction (Mezentsev and Oseledets), shows that frozen LLMs can reconstruct hundreds of tokens from only two learned proto-tokens in a single forward pass, suggesting a path beyond the autoregressive paradigm. In this paper, we study what information these proto-tokens encode and how they behave under reconstruction and controlled constraints. We perform a series of experiments aimed at disentangling semantic and syntactic content in the two proto-tokens, analyzing stability properties of the e-token, and visualizing attention patterns to the e-token during reconstruction. Finally, we test two regularization schemes for \"imposing\" semantic structure on the e-token using teacher embeddings, including an anchor-based loss and a relational distillation objective. Our results indicate that the m-token tends to capture semantic information more strongly than the e-token under standard optimization; anchor-based constraints trade off sharply with reconstruction accuracy; and relational distillation can transfer batch-level semantic relations into the proto-token space without sacrificing reconstruction quality, supporting the feasibility of future non-autoregressive seq2seq systems that predict proto-tokens as an intermediate representation.", "AI": {"tldr": "Frozen LLMs can reconstruct long text sequences from just two learned proto-tokens in one forward pass. This paper analyzes what information these proto-tokens encode, their behavior under constraints, and explores regularization methods to impose semantic structure on them.", "motivation": "Autoregressive LLMs require n forward passes for n tokens, which is inefficient. Recent work shows frozen LLMs can reconstruct hundreds of tokens from just two proto-tokens in one pass, suggesting a path beyond autoregressive generation. This paper aims to understand what information these proto-tokens encode and how to control their properties.", "method": "Conducted experiments to disentangle semantic and syntactic content in the two proto-tokens (m-token and e-token). Analyzed stability properties of the e-token and visualized attention patterns during reconstruction. Tested two regularization schemes: 1) anchor-based loss and 2) relational distillation objective using teacher embeddings to impose semantic structure on the e-token.", "result": "The m-token tends to capture semantic information more strongly than the e-token under standard optimization. Anchor-based constraints trade off sharply with reconstruction accuracy. Relational distillation can transfer batch-level semantic relations into the proto-token space without sacrificing reconstruction quality.", "conclusion": "The findings support the feasibility of future non-autoregressive sequence-to-sequence systems that predict proto-tokens as an intermediate representation, potentially enabling more efficient text generation beyond the autoregressive paradigm."}}
{"id": "2602.18333", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18333", "abs": "https://arxiv.org/abs/2602.18333", "authors": ["M. Reza Ebrahimi", "Micha\u00ebl Defferrard", "Sunny Panchal", "Roland Memisevic"], "title": "On the \"Induction Bias\" in Sequence Models", "comment": null, "summary": "Despite the remarkable practical success of transformer-based language models, recent work has raised concerns about their ability to perform state tracking. In particular, a growing body of literature has shown this limitation primarily through failures in out-of-distribution (OOD) generalization, such as length extrapolation. In this work, we shift attention to the in-distribution implications of these limitations. We conduct a large-scale experimental study of the data efficiency of transformers and recurrent neural networks (RNNs) across multiple supervision regimes. We find that the amount of training data required by transformers grows much more rapidly with state-space size and sequence length than for RNNs. Furthermore, we analyze the extent to which learned state-tracking mechanisms are shared across different sequence lengths. We show that transformers exhibit negligible or even detrimental weight sharing across lengths, indicating that they learn length-specific solutions in isolation. In contrast, recurrent models exhibit effective amortized learning by sharing weights across lengths, allowing data from one sequence length to improve performance on others. Together, these results demonstrate that state tracking remains a fundamental challenge for transformers, even when training and evaluation distributions match.", "AI": {"tldr": "Transformers struggle with state tracking even in-distribution, requiring exponentially more training data than RNNs as state-space/sequence length increases, and fail to share learned mechanisms across different lengths.", "motivation": "While prior work focused on transformers' out-of-distribution generalization failures in state tracking, this study examines the in-distribution implications - whether these limitations persist even when training and evaluation distributions match.", "method": "Large-scale experimental study comparing transformers and RNNs across multiple supervision regimes, analyzing data efficiency requirements relative to state-space size and sequence length, and examining weight sharing across different sequence lengths.", "result": "Transformers require much more training data than RNNs as state-space size and sequence length increase, and show negligible or detrimental weight sharing across lengths (learning length-specific solutions in isolation). RNNs exhibit effective amortized learning by sharing weights across lengths.", "conclusion": "State tracking remains a fundamental challenge for transformers even in-distribution, as they lack the efficient weight sharing mechanisms that enable RNNs to generalize learned state-tracking solutions across different sequence lengths."}}
{"id": "2602.18348", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18348", "abs": "https://arxiv.org/abs/2602.18348", "authors": ["Matheus Camilo da Silva", "Leonardo Arrighi", "Ana Carolina Lorena", "Sylvio Barbon Junior"], "title": "Explaining AutoClustering: Uncovering Meta-Feature Contribution in AutoML for Clustering", "comment": null, "summary": "AutoClustering methods aim to automate unsupervised learning tasks, including algorithm selection (AS), hyperparameter optimization (HPO), and pipeline synthesis (PS), by often leveraging meta-learning over dataset meta-features. While these systems often achieve strong performance, their recommendations are often difficult to justify: the influence of dataset meta-features on algorithm and hyperparameter choices is typically not exposed, limiting reliability, bias diagnostics, and efficient meta-feature engineering. This limits reliability and diagnostic insight for further improvements. In this work, we investigate the explainability of the meta-models in AutoClustering. We first review 22 existing methods and organize their meta-features into a structured taxonomy. We then apply a global explainability technique (i.e., Decision Predicate Graphs) to assess feature importance within meta-models from selected frameworks. Finally, we use local explainability tools such as SHAP (SHapley Additive exPlanations) to analyse specific clustering decisions. Our findings highlight consistent patterns in meta-feature relevance, identify structural weaknesses in current meta-learning strategies that can distort recommendations, and provide actionable guidance for more interpretable Automated Machine Learning (AutoML) design. This study therefore offers a practical foundation for increasing decision transparency in unsupervised learning automation.", "AI": {"tldr": "The paper investigates explainability of meta-models in AutoClustering systems, analyzing how dataset meta-features influence algorithm/hyperparameter recommendations to improve transparency and reliability.", "motivation": "AutoClustering systems automate unsupervised learning tasks but lack transparency - their recommendations are difficult to justify because the influence of dataset meta-features on algorithm/hyperparameter choices is not exposed, limiting reliability, bias diagnostics, and meta-feature engineering.", "method": "1) Review 22 existing AutoClustering methods and organize meta-features into structured taxonomy; 2) Apply global explainability technique (Decision Predicate Graphs) to assess feature importance in meta-models; 3) Use local explainability tools (SHAP) to analyze specific clustering decisions.", "result": "Findings reveal consistent patterns in meta-feature relevance, identify structural weaknesses in current meta-learning strategies that can distort recommendations, and provide actionable guidance for more interpretable AutoML design.", "conclusion": "The study offers a practical foundation for increasing decision transparency in unsupervised learning automation by making meta-model reasoning more interpretable and exposing the influence of dataset characteristics on clustering recommendations."}}
{"id": "2602.18403", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18403", "abs": "https://arxiv.org/abs/2602.18403", "authors": ["Orfeas Bourchas", "George Papalambrou"], "title": "Scientific Knowledge-Guided Machine Learning for Vessel Power Prediction: A Comparative Study", "comment": "Accepted to the KGML Bridge at AAAI 2026 (non-archival)", "summary": "Accurate prediction of main engine power is essential for vessel performance optimization, fuel efficiency, and compliance with emission regulations. Conventional machine learning approaches, such as Support Vector Machines, variants of Artificial Neural Networks (ANNs), and tree-based methods like Random Forests, Extra Tree Regressors, and XGBoost, can capture nonlinearities but often struggle to respect the fundamental propeller law relationship between power and speed, resulting in poor extrapolation outside the training envelope. This study introduces a hybrid modeling framework that integrates physics-based knowledge from sea trials with data-driven residual learning. The baseline component, derived from calm-water power curves of the form $P = cV^n$, captures the dominant power-speed dependence, while another, nonlinear, regressor is then trained to predict the residual power, representing deviations caused by environmental and operational conditions. By constraining the machine learning task to residual corrections, the hybrid model simplifies learning, improves generalization, and ensures consistency with the underlying physics. In this study, an XGBoost, a simple Neural Network, and a Physics-Informed Neural Network (PINN) coupled with the baseline component were compared to identical models without the baseline component. Validation on in-service data demonstrates that the hybrid model consistently outperformed a pure data-driven baseline in sparse data regions while maintaining similar performance in populated ones. The proposed framework provides a practical and computationally efficient tool for vessel performance monitoring, with applications in weather routing, trim optimization, and energy efficiency planning.", "AI": {"tldr": "Hybrid physics-data model for ship engine power prediction combines propeller law baseline with ML residual correction, outperforming pure data-driven methods especially in sparse data regions.", "motivation": "Conventional ML methods for vessel power prediction often fail to respect fundamental propeller law physics, leading to poor extrapolation outside training data and unreliable predictions for performance optimization and emission compliance.", "method": "Hybrid framework integrating physics-based baseline (calm-water power curve P=cV^n) with data-driven residual learning. ML models (XGBoost, Neural Network, PINN) predict residual power deviations from baseline due to environmental/operational conditions.", "result": "Hybrid models consistently outperformed pure data-driven baselines in sparse data regions while maintaining similar performance in populated areas. The approach provides better generalization and physics consistency.", "conclusion": "The hybrid physics-data framework offers practical, computationally efficient vessel performance monitoring with improved generalization, suitable for weather routing, trim optimization, and energy efficiency planning."}}
{"id": "2602.18417", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18417", "abs": "https://arxiv.org/abs/2602.18417", "authors": ["Joshua Nunley"], "title": "Subgroups of $U(d)$ Induce Natural RNN and Transformer Architectures", "comment": "12 pages, 3 figures, 8 tables", "summary": "This paper presents a direct framework for sequence models with hidden states on closed subgroups of U(d). We use a minimal axiomatic setup and derive recurrent and transformer templates from a shared skeleton in which subgroup choice acts as a drop-in replacement for state space, tangent projection, and update map. We then specialize to O(d) and evaluate orthogonal-state RNN and transformer models on Tiny Shakespeare and Penn Treebank under parameter-matched settings. We also report a general linear-mixing extension in tangent space, which applies across subgroup choices and improves finite-budget performance in the current O(d) experiments.", "AI": {"tldr": "Direct framework for sequence models with hidden states on closed subgroups of U(d), with O(d) specialization showing competitive performance on language tasks.", "motivation": "To develop a unified framework for sequence models that can work with hidden states on matrix Lie groups (specifically closed subgroups of U(d)), allowing for more structured and geometrically meaningful representations compared to standard Euclidean hidden states.", "method": "Minimal axiomatic setup deriving recurrent and transformer templates from a shared skeleton where subgroup choice acts as drop-in replacement for state space, tangent projection, and update map. Specialized to O(d) for experiments, with linear-mixing extension in tangent space.", "result": "Orthogonal-state RNN and transformer models evaluated on Tiny Shakespeare and Penn Treebank under parameter-matched settings show competitive performance. Linear-mixing extension improves finite-budget performance in O(d) experiments.", "conclusion": "The framework provides a principled way to build sequence models with hidden states on matrix Lie groups, with O(d) specialization demonstrating practical viability and the tangent-space linear-mixing extension offering performance improvements."}}
{"id": "2602.18435", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18435", "abs": "https://arxiv.org/abs/2602.18435", "authors": ["Aggelos Semoglou", "John Pavlopoulos"], "title": "Assigning Confidence: K-partition Ensembles", "comment": "31 pages including appendix", "summary": "Clustering is widely used for unsupervised structure discovery, yet it offers limited insight into how reliable each individual assignment is. Diagnostics, such as convergence behavior or objective values, may reflect global quality, but they do not indicate whether particular instances are assigned confidently, especially for initialization-sensitive algorithms like k-means. This assignment-level instability can undermine both accuracy and robustness. Ensemble approaches improve global consistency by aggregating multiple runs, but they typically lack tools for quantifying pointwise confidence in a way that combines cross-run agreement with geometric support from the learned cluster structure. We introduce CAKE (Confidence in Assignments via K-partition Ensembles), a framework that evaluates each point using two complementary statistics computed over a clustering ensemble: assignment stability and consistency of local geometric fit. These are combined into a single, interpretable score in [0,1]. Our theoretical analysis shows that CAKE remains effective under noise and separates stable from unstable points. Experiments on synthetic and real-world datasets indicate that CAKE effectively highlights ambiguous points and stable core members, providing a confidence ranking that can guide filtering or prioritization to improve clustering quality.", "AI": {"tldr": "CAKE framework quantifies pointwise confidence in clustering assignments using ensemble stability and geometric fit, providing interpretable confidence scores to identify ambiguous vs stable points.", "motivation": "Traditional clustering diagnostics only assess global quality, not individual assignment reliability. Ensemble methods improve consistency but lack tools to quantify pointwise confidence that combines cross-run agreement with geometric support from cluster structure.", "method": "CAKE framework computes two complementary statistics over clustering ensembles: assignment stability (cross-run agreement) and consistency of local geometric fit. These are combined into a single interpretable confidence score in [0,1].", "result": "Theoretical analysis shows CAKE remains effective under noise and separates stable from unstable points. Experiments on synthetic and real-world datasets demonstrate CAKE effectively highlights ambiguous points and stable core members.", "conclusion": "CAKE provides a confidence ranking that can guide filtering or prioritization to improve clustering quality by identifying which assignments are reliable vs ambiguous."}}
