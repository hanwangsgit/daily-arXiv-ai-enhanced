<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 65]
- [cs.AI](#cs.AI) [Total: 22]
- [cs.LG](#cs.LG) [Total: 113]
- [eess.SP](#eess.SP) [Total: 15]
- [cs.RO](#cs.RO) [Total: 20]
- [cs.IT](#cs.IT) [Total: 10]
- [eess.IV](#eess.IV) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Size Matters: Reconstructing Real-Scale 3D Models from Monocular Images for Food Portion Estimation](https://arxiv.org/abs/2601.20051)
*Gautham Vinod,Bruce Coburn,Siddeshwar Raghavan,Jiangpeng He,Fengqing Zhu*

Main category: cs.CV

TL;DR: A method for true-to-scale 3D reconstruction from monocular images to accurately estimate food portion sizes for precision nutrition applications.


<details>
  <summary>Details</summary>
Motivation: Chronic diseases like obesity and diabetes require accurate food intake monitoring, but current AI dietary assessment struggles with portion size estimation from images due to scale ambiguity in monocular 3D reconstruction.

Method: Leverages rich visual features from models trained on large-scale datasets to estimate the real-world scale of reconstructed objects, converting single-view 3D reconstructions into physically meaningful true-to-scale models.

Result: Outperforms existing techniques on two public datasets, achieving nearly 30% reduction in mean absolute volume-estimation error.

Conclusion: Bridges 3D computer vision and digital health by enabling accurate food portion estimation from monocular images, with potential to enhance precision nutrition applications.

Abstract: The rise of chronic diseases related to diet, such as obesity and diabetes, emphasizes the need for accurate monitoring of food intake. While AI-driven dietary assessment has made strides in recent years, the ill-posed nature of recovering size (portion) information from monocular images for accurate estimation of ``how much did you eat?'' is a pressing challenge. Some 3D reconstruction methods have achieved impressive geometric reconstruction but fail to recover the crucial real-world scale of the reconstructed object, limiting its usage in precision nutrition. In this paper, we bridge the gap between 3D computer vision and digital health by proposing a method that recovers a true-to-scale 3D reconstructed object from a monocular image. Our approach leverages rich visual features extracted from models trained on large-scale datasets to estimate the scale of the reconstructed object. This learned scale enables us to convert single-view 3D reconstructions into true-to-life, physically meaningful models. Extensive experiments and ablation studies on two publicly available datasets show that our method consistently outperforms existing techniques, achieving nearly a 30% reduction in mean absolute volume-estimation error, showcasing its potential to enhance the domain of precision nutrition. Code: https://gitlab.com/viper-purdue/size-matters

</details>


### [2] [DiSa: Saliency-Aware Foreground-Background Disentangled Framework for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2601.20064)
*Zhen Yao,Xin Li,Taotao Jing,Shuai Zhang,Mooi Choo Chuah*

Main category: cs.CV

TL;DR: DiSa introduces a saliency-aware foreground-background disentangled framework for open-vocabulary semantic segmentation that addresses CLIP's foreground bias and limited spatial localization issues.


<details>
  <summary>Details</summary>
Motivation: Vision-language models like CLIP have two critical limitations when adapted to segmentation: (1) Foreground Bias - tendency to ignore background regions, and (2) Limited Spatial Localization - resulting in blurred object boundaries due to their pre-training on image-text pairs that focus on salient, object-centric regions.

Method: DiSa uses a saliency-aware foreground-background disentangled framework with two key components: (1) Saliency-aware Disentanglement Module (SDM) that explicitly incorporates saliency cues to separately model foreground and background ensemble features in a divide-and-conquer manner, and (2) Hierarchical Refinement Module (HRM) that leverages pixel-wise spatial contexts and enables channel-wise feature refinement through multi-level updates.

Result: Extensive experiments on six benchmarks demonstrate that DiSa consistently outperforms state-of-the-art methods for open-vocabulary semantic segmentation.

Conclusion: DiSa effectively addresses the foreground bias and spatial localization limitations of VLMs in segmentation tasks through its novel saliency-aware disentanglement approach, achieving superior performance across multiple benchmarks.

Abstract: Open-vocabulary semantic segmentation aims to assign labels to every pixel in an image based on text labels. Existing approaches typically utilize vision-language models (VLMs), such as CLIP, for dense prediction. However, VLMs, pre-trained on image-text pairs, are biased toward salient, object-centric regions and exhibit two critical limitations when adapted to segmentation: (i) Foreground Bias, which tends to ignore background regions, and (ii) Limited Spatial Localization, resulting in blurred object boundaries. To address these limitations, we introduce DiSa, a novel saliency-aware foreground-background disentangled framework. By explicitly incorporating saliency cues in our designed Saliency-aware Disentanglement Module (SDM), DiSa separately models foreground and background ensemble features in a divide-and-conquer manner. Additionally, we propose a Hierarchical Refinement Module (HRM) that leverages pixel-wise spatial contexts and enables channel-wise feature refinement through multi-level updates. Extensive experiments on six benchmarks demonstrate that DiSa consistently outperforms state-of-the-art methods.

</details>


### [3] [Semi-Supervised Masked Autoencoders: Unlocking Vision Transformer Potential with Limited Data](https://arxiv.org/abs/2601.20072)
*Atik Faysal,Mohammad Rostami,Reihaneh Gh. Roshan,Nikhil Muralidhar,Huaxia Wang*

Main category: cs.CV

TL;DR: SSMAE is a semi-supervised framework that combines masked autoencoder reconstruction with classification using dynamically gated pseudo-labels, achieving significant gains over supervised ViTs in low-label regimes.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of training Vision Transformers when labeled data is scarce but unlabeled data is abundant, aiming to leverage both labeled and unlabeled data effectively for data-efficient transformer training.

Method: Proposes Semi-Supervised Masked Autoencoder (SSMAE) that jointly optimizes masked image reconstruction and classification using both unlabeled and labeled samples with dynamically selected pseudo-labels. Introduces a validation-driven gating mechanism that activates pseudo-labeling only after the model achieves reliable, high-confidence predictions consistent across both weakly and strongly augmented views.

Result: On CIFAR-10 and CIFAR-100, SSMAE consistently outperforms supervised ViT and fine-tuned MAE, with the largest gains in low-label regimes (+9.24% over ViT on CIFAR-10 with 10% labels). Demonstrates that when pseudo-labels are introduced is as important as how they are generated.

Conclusion: SSMAE effectively leverages both labeled and unlabeled data for Vision Transformer training, showing that timing of pseudo-label introduction is crucial for data-efficient transformer training. The framework demonstrates significant performance improvements in low-label scenarios.

Abstract: We address the challenge of training Vision Transformers (ViTs) when labeled data is scarce but unlabeled data is abundant. We propose Semi-Supervised Masked Autoencoder (SSMAE), a framework that jointly optimizes masked image reconstruction and classification using both unlabeled and labeled samples with dynamically selected pseudo-labels. SSMAE introduces a validation-driven gating mechanism that activates pseudo-labeling only after the model achieves reliable, high-confidence predictions that are consistent across both weakly and strongly augmented views of the same image, reducing confirmation bias. On CIFAR-10 and CIFAR-100, SSMAE consistently outperforms supervised ViT and fine-tuned MAE, with the largest gains in low-label regimes (+9.24% over ViT on CIFAR-10 with 10% labels). Our results demonstrate that when pseudo-labels are introduced is as important as how they are generated for data-efficient transformer training. Codes are available at https://github.com/atik666/ssmae.

</details>


### [4] [Sparse CLIP: Co-Optimizing Interpretability and Performance in Contrastive Learning](https://arxiv.org/abs/2601.20075)
*Chuan Qin,Constantin Venhoff,Sonia Joseph,Fanyi Xiao,Stefan Scherer*

Main category: cs.CV

TL;DR: Sparse CLIP integrates sparsity directly into CLIP training to create interpretable representations without sacrificing performance, challenging the assumption that interpretability requires accuracy trade-offs.


<details>
  <summary>Details</summary>
Motivation: CLIP's dense and opaque latent representations pose significant interpretability challenges, and conventional wisdom suggests interpretability comes at the cost of performance. Post-hoc approaches like Sparse Autoencoders often degrade downstream performance and lose multimodal capabilities.

Method: Proposes integrating sparsity directly into CLIP training rather than using post-hoc approaches. This yields sparse representations that maintain performance while being interpretable, preserving CLIP's multimodal capabilities.

Result: Sparse CLIP representations preserve strong downstream task performance, achieve superior interpretability compared to SAEs, retain multimodal capabilities, enable semantic concept alignment, reveal cross-modal knowledge emergence dynamics, and support interpretable vision-based steering.

Conclusion: Interpretability and performance can be co-optimized, challenging conventional wisdom that they are in tension. Sparse CLIP offers a promising design principle for future models, demonstrating that interpretable representations don't require sacrificing accuracy.

Abstract: Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in vision-language representation learning, powering diverse downstream tasks and serving as the default vision backbone in multimodal large language models (MLLMs). Despite its success, CLIP's dense and opaque latent representations pose significant interpretability challenges. A common assumption is that interpretability and performance are in tension: enforcing sparsity during training degrades accuracy, motivating recent post-hoc approaches such as Sparse Autoencoders (SAEs). However, these post-hoc approaches often suffer from degraded downstream performance and loss of CLIP's inherent multimodal capabilities, with most learned features remaining unimodal.
  We propose a simple yet effective approach that integrates sparsity directly into CLIP training, yielding representations that are both interpretable and performant. Compared to SAEs, our Sparse CLIP representations preserve strong downstream task performance, achieve superior interpretability, and retain multimodal capabilities. We show that multimodal sparse features enable straightforward semantic concept alignment and reveal training dynamics of how cross-modal knowledge emerges. Finally, as a proof of concept, we train a vision-language model on sparse CLIP representations that enables interpretable, vision-based steering capabilities. Our findings challenge conventional wisdom that interpretability requires sacrificing accuracy and demonstrate that interpretability and performance can be co-optimized, offering a promising design principle for future models.

</details>


### [5] [NucFuseRank: Dataset Fusion and Performance Ranking for Nuclei Instance Segmentation](https://arxiv.org/abs/2601.20104)
*Nima Torbati,Anastasia Meshcheryakova,Ramona Woitek,Sepideh Hatamikia,Diana Mechtcheriakova,Amirreza Mahbod*

Main category: cs.CV

TL;DR: This paper focuses on standardizing and benchmarking publicly available nuclei instance segmentation datasets for H&E-stained histological images rather than developing new segmentation models.


<details>
  <summary>Details</summary>
Motivation: Most research focuses on developing new segmentation algorithms and benchmarking on limited datasets. There's a need for systematic evaluation of available datasets and standardized benchmarks for nuclei instance segmentation in H&E-stained images.

Method: 1. Identified manually annotated public datasets through literature review. 2. Standardized datasets into unified input and annotation format. 3. Evaluated datasets using two state-of-the-art segmentation models (CNN-based and hybrid CNN-Vision Transformer). 4. Proposed unified test set (NucFuse-test) for cross-dataset evaluation and unified training set (NucFuse-train) by merging multiple datasets.

Result: Systematically evaluated and ranked datasets based on nuclei instance segmentation performance. Provided a new benchmark for training, testing, and evaluating segmentation models on H&E-stained images.

Conclusion: The work establishes a comprehensive benchmark for nuclei instance segmentation in H&E-stained images, addressing dataset standardization and evaluation gaps in the field, with publicly available implementation for community use.

Abstract: Nuclei instance segmentation in hematoxylin and eosin (H&E)-stained images plays an important role in automated histological image analysis, with various applications in downstream tasks. While several machine learning and deep learning approaches have been proposed for nuclei instance segmentation, most research in this field focuses on developing new segmentation algorithms and benchmarking them on a limited number of arbitrarily selected public datasets.
  In this work, rather than focusing on model development, we focused on the datasets used for this task. Based on an extensive literature review, we identified manually annotated, publicly available datasets of H&E-stained images for nuclei instance segmentation and standardized them into a unified input and annotation format. Using two state-of-the-art segmentation models, one based on convolutional neural networks (CNNs) and one based on a hybrid CNN and vision transformer architecture, we systematically evaluated and ranked these datasets based on their nuclei instance segmentation performance. Furthermore, we proposed a unified test set (NucFuse-test) for fair cross-dataset evaluation and a unified training set (NucFuse-train) for improved segmentation performance by merging images from multiple datasets.
  By evaluating and ranking the datasets, performing comprehensive analyses, generating fused datasets, conducting external validation, and making our implementation publicly available, we provided a new benchmark for training, testing, and evaluating nuclei instance segmentation models on H&E-stained histological images.

</details>


### [6] [Look in the Middle: Structural Anchor Pruning for Scalable Visual RAG Indexing](https://arxiv.org/abs/2601.20107)
*Zhuchenyang Liu,Ziyu Hu,Yao Zhang,Yu Xiao*

Main category: cs.CV

TL;DR: SAP is a training-free pruning method that identifies key visual patches from middle layers to achieve over 90% compression for Vision-Language Models while maintaining retrieval performance.


<details>
  <summary>Details</summary>
Motivation: Existing Vision-Language Models for Visual Document Retrieval have prohibitive index vector size overheads. Training-free pruning methods underperform random selection in high-compression scenarios (>80%), and prior research questions the feasibility of training-free pruning due to query-dependent visual token importance.

Method: Proposes Structural Anchor Pruning (SAP) - a training-free pruning method that identifies key visual patches from middle layers rather than final layers. Also introduces Oracle Score Retention (OSR) protocol to evaluate how layer-wise information affects compression efficiency.

Result: SAP reduces index vectors by over 90% while maintaining robust retrieval fidelity on the ViDoRe benchmark. OSR-based analysis reveals that semantic structural anchor patches persist in middle layers, unlike traditional pruning solutions that focus on final layers where structural signals dissipate.

Conclusion: SAP provides a highly scalable solution for Visual RAG by achieving high-performance compression through training-free pruning of middle-layer structural anchors, challenging previous assumptions about the feasibility of training-free pruning for Vision-Language Models.

Abstract: Recent Vision-Language Models (e.g., ColPali) enable fine-grained Visual Document Retrieval (VDR) but incur prohibitive index vector size overheads. Training-free pruning solutions (e.g., EOS-attention based methods) can reduce index vector size by approximately 60% without model adaptation, but often underperform random selection in high-compression scenarios (> 80%). Prior research (e.g., Light-ColPali) attributes this to the conclusion that visual token importance is inherently query-dependent, thereby questioning the feasibility of training-free pruning. In this work, we propose Structural Anchor Pruning (SAP), a training-free pruning method that identifies key visual patches from middle layers to achieve high performance compression. We also introduce Oracle Score Retention (OSR) protocol to evaluate how layer-wise information affects compression efficiency. Evaluations on the ViDoRe benchmark demonstrate that SAP reduces index vectors by over 90% while maintaining robust retrieval fidelity, providing a highly scalable solution for Visual RAG. Furthermore, our OSR-based analysis reveals that semantic structural anchor patches persist in the middle layers, unlike traditional pruning solutions that focus on the final layer where structural signals dissipate.

</details>


### [7] [Efficient Token Pruning for LLaDA-V](https://arxiv.org/abs/2601.20168)
*Zhewen Wan,Tianchen Song,Chen Lin,Zhiyong Zhao,Xianpeng Lang*

Main category: cs.CV

TL;DR: This paper proposes a structured token pruning strategy for diffusion-based multimodal models like LLaDA-V, reducing computational cost by up to 65% while preserving 95% of task performance by pruning visual tokens in middle-to-late layers.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based multimodal models suffer from significant computational overhead due to bidirectional attention and iterative denoising, with visual tokens being repeatedly processed across all layers and steps. The authors discovered that LLaDA-V aggregates cross-modal information predominantly in middle-to-late layers, leading to delayed semantic alignment.

Method: Proposes a structured token pruning strategy inspired by FastV, but targeting middle-to-late layers of the first denoising step (unlike FastV's shallow-layer pruning). This aligns with LLaDA-V's delayed attention aggregation to maintain output quality, and first-step pruning reduces computation across all subsequent denoising steps.

Result: The best configuration reduces computational cost by up to 65% while preserving an average of 95% task performance across multiple benchmarks. This is the first work to investigate structured token pruning in diffusion-based large multimodal models.

Conclusion: The framework provides an empirical basis for efficient LLaDA-V inference and highlights the potential of vision-aware pruning in diffusion-based multimodal models, demonstrating that targeted pruning aligned with model architecture can achieve significant efficiency gains with minimal performance loss.

Abstract: Diffusion-based large multimodal models, such as LLaDA-V, have demonstrated impressive capabilities in vision-language understanding and generation. However, their bidirectional attention mechanism and diffusion-style iterative denoising paradigm introduce significant computational overhead, as visual tokens are repeatedly processed across all layers and denoising steps. In this work, we conduct an in-depth attention analysis and reveal that, unlike autoregressive decoders, LLaDA-V aggregates cross-modal information predominantly in middle-to-late layers, leading to delayed semantic alignment. Motivated by this observation, we propose a structured token pruning strategy inspired by FastV, selectively removing a proportion of visual tokens at designated layers to reduce FLOPs while preserving critical semantic information. To the best of our knowledge, this is the first work to investigate structured token pruning in diffusion-based large multimodal models. Unlike FastV, which focuses on shallow-layer pruning, our method targets the middle-to-late layers of the first denoising step to align with LLaDA-V's delayed attention aggregation to maintain output quality, and the first-step pruning strategy reduces the computation across all subsequent steps. Our framework provides an empirical basis for efficient LLaDA-V inference and highlights the potential of vision-aware pruning in diffusion-based multimodal models. Across multiple benchmarks, our best configuration reduces computational cost by up to 65% while preserving an average of 95% task performance.

</details>


### [8] [TeleStyle: Content-Preserving Style Transfer in Images and Videos](https://arxiv.org/abs/2601.20175)
*Shiwen Zhang,Xiaoyan Yang,Bojia Zi,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleStyle is a lightweight Diffusion Transformer model for image and video style transfer that achieves state-of-the-art performance by addressing content-style entanglement through curriculum continual learning on hybrid datasets.


<details>
  <summary>Details</summary>
Motivation: Current Diffusion Transformers (DiTs) struggle with content-preserving style transfer due to inherent entanglement of content and style features in their internal representations, making it challenging to generate stylized outputs while maintaining content fidelity.

Method: Built on Qwen-Image-Edit, TeleStyle uses a Curriculum Continual Learning framework trained on hybrid datasets of curated clean triplets and synthetic noisy triplets from diverse style categories. Includes video-to-video stylization module for temporal consistency.

Result: TeleStyle achieves state-of-the-art performance across three core metrics: style similarity, content consistency, and aesthetic quality. The model generalizes well to unseen styles without compromising content fidelity.

Conclusion: TeleStyle presents an effective solution for content-preserving style transfer in both images and videos, addressing the content-style entanglement problem in DiTs through innovative training strategies and achieving superior performance.

Abstract: Content-preserving style transfer, generating stylized outputs based on content and style references, remains a significant challenge for Diffusion Transformers (DiTs) due to the inherent entanglement of content and style features in their internal representations. In this technical report, we present TeleStyle, a lightweight yet effective model for both image and video stylization. Built upon Qwen-Image-Edit, TeleStyle leverages the base model's robust capabilities in content preservation and style customization. To facilitate effective training, we curated a high-quality dataset of distinct specific styles and further synthesized triplets using thousands of diverse, in-the-wild style categories. We introduce a Curriculum Continual Learning framework to train TeleStyle on this hybrid dataset of clean (curated) and noisy (synthetic) triplets. This approach enables the model to generalize to unseen styles without compromising precise content fidelity. Additionally, we introduce a video-to-video stylization module to enhance temporal consistency and visual quality. TeleStyle achieves state-of-the-art performance across three core evaluation metrics: style similarity, content consistency, and aesthetic quality. Code and pre-trained models are available at https://github.com/Tele-AI/TeleStyle

</details>


### [9] [Automated Marine Biofouling Assessment: Benchmarking Computer Vision and Multimodal LLMs on the Level of Fouling Scale](https://arxiv.org/abs/2601.20196)
*Brayden Hamilton,Tim Cashmore,Peter Driscoll,Trevor Gee,Henry Williams*

Main category: cs.CV

TL;DR: Automated classification of marine biofouling severity using computer vision and LLMs, showing complementary strengths and potential for hybrid approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional diver inspections for biofouling assessment are hazardous and not scalable, creating need for automated solutions to address ecological, economic, and biosecurity risks.

Method: Evaluated convolutional neural networks, transformer-based segmentation, and zero-shot LLMs on expert-labelled dataset from New Zealand Ministry for Primary Industries, using structured prompts and retrieval for LLMs.

Result: Computer vision models achieved high accuracy at extreme fouling categories but struggled with intermediate levels due to dataset imbalance. LLMs achieved competitive performance without training and provided interpretable outputs.

Conclusion: Hybrid methods integrating segmentation coverage with LLM reasoning offer promising pathway toward scalable and interpretable biofouling assessment, demonstrating complementary strengths across approaches.

Abstract: Marine biofouling on vessel hulls poses major ecological, economic, and biosecurity risks. Traditional survey methods rely on diver inspections, which are hazardous and limited in scalability. This work investigates automated classification of biofouling severity on the Level of Fouling (LoF) scale using both custom computer vision models and large multimodal language models (LLMs). Convolutional neural networks, transformer-based segmentation, and zero-shot LLMs were evaluated on an expert-labelled dataset from the New Zealand Ministry for Primary Industries. Computer vision models showed high accuracy at extreme LoF categories but struggled with intermediate levels due to dataset imbalance and image framing. LLMs, guided by structured prompts and retrieval, achieved competitive performance without training and provided interpretable outputs. The results demonstrate complementary strengths across approaches and suggest that hybrid methods integrating segmentation coverage with LLM reasoning offer a promising pathway toward scalable and interpretable biofouling assessment.

</details>


### [10] [DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment](https://arxiv.org/abs/2601.20218)
*Haoyou Deng,Keyu Yan,Chaojie Mao,Xiang Wang,Yu Liu,Changxin Gao,Nong Sang*

Main category: cs.CV

TL;DR: DenseGRPO addresses sparse reward problem in GRPO-based flow matching models by introducing dense step-wise rewards and adaptive exploration for better human preference alignment in text-to-image generation.


<details>
  <summary>Details</summary>
Motivation: Existing GRPO-based approaches suffer from sparse reward problem where terminal rewards are applied uniformly to all denoising steps, creating mismatch between global feedback and fine-grained step contributions. This hinders effective training and alignment.

Method: Proposes DenseGRPO with two key components: (1) Predicts step-wise reward gain as dense reward for each denoising step using ODE-based approach on intermediate clean images; (2) Introduces reward-aware scheme to calibrate exploration space by adaptively adjusting timestep-specific stochasticity injection in SDE sampler.

Result: Extensive experiments on multiple standard benchmarks demonstrate effectiveness of DenseGRPO and highlight critical role of valid dense rewards in flow matching model alignment.

Conclusion: DenseGRPO successfully addresses sparse reward problem in GRPO-based flow matching models through dense step-wise rewards and adaptive exploration, improving human preference alignment for text-to-image generation.

Abstract: Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce \textbf{DenseGRPO}, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.

</details>


### [11] [Feature Projection Learning for Better Vision-Language Reasoning](https://arxiv.org/abs/2601.20224)
*Yi Zhang,Weicheng Lin,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: FPL (Feature Projection Learning) is a simple, efficient method that adapts CLIP to downstream tasks by projecting class prototypes into image feature space and using reconstruction error as class scores.


<details>
  <summary>Details</summary>
Motivation: Existing methods for adapting Vision-Language Pre-trained models like CLIP to downstream tasks suffer from limited performance, excessive parameters, or long training times, hindering effective adaptation.

Method: Develops a projection model that projects class prototype features into query image feature space and reconstructs the query image feature map. Uses negative average squared reconstruction error as class score, transforming classification into feature projection. Combines predictions from projection model with original CLIP.

Result: FPL delivers superior accuracy, surpassing current state-of-the-art methods by a substantial margin in comprehensive empirical evaluations.

Conclusion: FPL provides a simple yet efficient and effective approach for adapting CLIP to downstream tasks, addressing limitations of previous methods while achieving better performance.

Abstract: Vision-Language Pre-Trained models, notably CLIP, that utilize contrastive learning have proven highly adept at extracting generalizable visual features. To inherit the well-learned knowledge of VLP models for downstream tasks, several approaches aim to adapt them efficiently with limited supervision. However, these methods either suffer from limited performance, excessive learnable parameters, or extended training times, all of which hinder their effectiveness in adapting the CLIP model to downstream tasks. In this work, we propose a simple yet efficient and effective method called \textit{\textbf{F}eature \textbf{P}rojection \textbf{L}earning(FPL)} to address these problems. Specifically, we develop a projection model that projects class prototype features into the query image feature space and reconstructs the query image feature map. The negative average squared reconstruction error is used as the class score. In this way, we transform the classification problem into a feature projection problem. The final output of this method is a combination of the prediction from the projection model and the original pre-trained CLIP. Comprehensive empirical evaluations confirm that FPL delivers superior accuracy, surpassing the current state-of-the-art methods by a substantial margin.

</details>


### [12] [Visual Prompt-Agnostic Evolution](https://arxiv.org/abs/2601.20232)
*Junze Wang,Lei Fan,Dezheng Zhang,Weipeng Jing,Donglin Di,Yang Song,Sidong Liu,Cong Cong*

Main category: cs.CV

TL;DR: PAE improves Visual Prompt Tuning by addressing unstable training dynamics through frequency-aware initialization, cross-layer coordination via shared Koopman operator, and stability regularization.


<details>
  <summary>Details</summary>
Motivation: Existing Visual Prompt Tuning (VPT) variants suffer from unstable training dynamics with gradient oscillations, where shallow-layer prompts stagnate early while deeper-layer prompts oscillate with high variance, causing cross-layer mismatch that slows convergence and degrades performance.

Method: Proposes Prompt-Agnostic Evolution (PAE) with three key components: 1) Frequency-aware initialization that uncovers and propagates frequency shortcut patterns the backbone uses for recognition, 2) Shared Koopman operator for coherent cross-layer evolution instead of layer-specific updates, and 3) Lyapunov-inspired regularizer to constrain error amplification during training.

Result: PAE accelerates convergence with average 1.41Ã— speedup and improves accuracy by 1-3% on 25 datasets across multiple downstream tasks. It's prompt-agnostic, lightweight, and integrates seamlessly with diverse VPT variants without backbone modification or inference changes.

Conclusion: PAE effectively addresses training instability in Visual Prompt Tuning through principled modeling of prompt dynamics, achieving faster convergence and better performance while maintaining compatibility with existing VPT approaches.

Abstract: Visual Prompt Tuning (VPT) adapts a frozen Vision Transformer (ViT) to downstream tasks by inserting a small number of learnable prompt tokens into the token sequence at each layer. However, we observe that existing VPT variants often suffer from unstable training dynamics, characterized by gradient oscillations. A layer-wise analysis reveals that shallow-layer prompts tend to stagnate early, while deeper-layer prompts exhibit high-variance oscillations, leading to cross-layer mismatch. These issues slow convergence and degrade final performance. To address these challenges, we propose Prompt-Agnostic Evolution ($\mathtt{PAE}$), which strengthens vision prompt tuning by explicitly modeling prompt dynamics. From a frequency-domain perspective, we initialize prompts in a task-aware direction by uncovering and propagating frequency shortcut patterns that the backbone inherently exploits for recognition. To ensure coherent evolution across layers, we employ a shared Koopman operator that imposes a global linear transformation instead of uncoordinated, layer-specific updates. Finally, inspired by Lyapunov stability theory, we introduce a regularizer that constrains error amplification during evolution. Extensive experiments show that $\mathtt{PAE}$ accelerates convergence with an average $1.41\times$ speedup and improves accuracy by 1--3% on 25 datasets across multiple downstream tasks. Beyond performance, $\mathtt{PAE}$ is prompt-agnostic and lightweight, and it integrates seamlessly with diverse VPT variants without backbone modification or inference-time changes.

</details>


### [13] [BLENDER: Blended Text Embeddings and Diffusion Residuals for Intra-Class Image Synthesis in Deep Metric Learning](https://arxiv.org/abs/2601.20246)
*Jan Niklas Kolf,Ozan Tezcan,Justin Theiss,Hyung Jun Kim,Wentao Bao,Bhargav Bhushanam,Khushi Gupta,Arun Kejariwal,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: BLenDeR is a diffusion sampling method that uses set-theory operations on denoising residuals to generate diverse synthetic data for Deep Metric Learning, improving performance on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: Deep Generative Models can create synthetic data to augment authentic data in Deep Metric Learning, but existing approaches lack controllability in generating diverse intra-class attribute combinations needed to improve DML performance.

Method: BLenDeR uses diffusion sampling with set-theory inspired union and intersection operations on denoising residuals. Union encourages any attribute present across multiple prompts, while intersection extracts common directions via principal component analysis, enabling controlled synthesis of diverse attribute combinations within classes.

Result: BLenDeR consistently outperforms state-of-the-art baselines across multiple datasets and backbones, achieving 3.7% increase in Recall@1 on CUB-200 and 1.8% increase on Cars-196 under standard experimental settings.

Conclusion: BLenDeR effectively addresses limitations of existing generative approaches by providing controllable intra-class diversity generation, leading to improved Deep Metric Learning performance through better synthetic data augmentation.

Abstract: The rise of Deep Generative Models (DGM) has enabled the generation of high-quality synthetic data. When used to augment authentic data in Deep Metric Learning (DML), these synthetic samples enhance intra-class diversity and improve the performance of downstream DML tasks. We introduce BLenDeR, a diffusion sampling method designed to increase intra-class diversity for DML in a controllable way by leveraging set-theory inspired union and intersection operations on denoising residuals. The union operation encourages any attribute present across multiple prompts, while the intersection extracts the common direction through a principal component surrogate. These operations enable controlled synthesis of diverse attribute combinations within each class, addressing key limitations of existing generative approaches. Experiments on standard DML benchmarks demonstrate that BLenDeR consistently outperforms state-of-the-art baselines across multiple datasets and backbones. Specifically, BLenDeR achieves 3.7% increase in Recall@1 on CUB-200 and a 1.8% increase on Cars-196, compared to state-of-the-art baselines under standard experimental settings.

</details>


### [14] [Reversible Efficient Diffusion for Image Fusion](https://arxiv.org/abs/2601.20260)
*Xingxin Xu,Bing Cao,DongDong Li,Qinghua Hu,Pengfei Zhu*

Main category: cs.CV

TL;DR: RED model is a reversible efficient diffusion framework for multi-modal image fusion that combines diffusion model generative power with explicit supervision while avoiding computational inefficiency.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models for image fusion suffer from detail loss due to noise error accumulation in Markov processes, and explicit supervision in end-to-end training creates computational efficiency challenges.

Method: Proposes Reversible Efficient Diffusion (RED) model - an explicitly supervised training framework that inherits diffusion model generative capabilities while avoiding distribution estimation and computational inefficiency.

Result: Not specified in abstract - would need full paper for experimental results.

Conclusion: RED addresses limitations of current diffusion-based image fusion methods by providing an efficient supervised framework that maintains generative power while preserving fine details.

Abstract: Multi-modal image fusion aims to consolidate complementary information from diverse source images into a unified representation. The fused image is expected to preserve fine details and maintain high visual fidelity. While diffusion models have demonstrated impressive generative capabilities in image generation, they often suffer from detail loss when applied to image fusion tasks. This issue arises from the accumulation of noise errors inherent in the Markov process, leading to inconsistency and degradation in the fused results. However, incorporating explicit supervision into end-to-end training of diffusion-based image fusion introduces challenges related to computational efficiency. To address these limitations, we propose the Reversible Efficient Diffusion (RED) model - an explicitly supervised training framework that inherits the powerful generative capability of diffusion models while avoiding the distribution estimation.

</details>


### [15] [Hallucination Begins Where Saliency Drops](https://arxiv.org/abs/2601.20279)
*Xiaofeng Zhang,Yuanchao Zhu,Chaochen Gu,Xiaosong Yuan,Qiyan Zhao,Jiawei Cao,Feilong Tang,Sinan Fan,Yaomin Shen,Chen Shen,Hao Tang*

Main category: cs.CV

TL;DR: LVLMs-Saliency: A gradient-aware framework that fuses attention weights with input gradients to detect and mitigate hallucinations in large vision-language models by identifying contextual memory breakdown patterns.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for detecting hallucinations in LVLMs rely solely on forward-pass attention patterns and neglect gradient-based signals, limiting their ability to reliably distinguish hallucinated from factually grounded outputs.

Method: Introduces LVLMs-Saliency framework that quantifies visual grounding strength by fusing attention weights with input gradients. Identifies that hallucinations occur when preceding tokens show low saliency toward next token prediction. Proposes two inference-time mechanisms: Saliency-Guided Rejection Sampling (SGRS) to filter candidate tokens during decoding, and Local Coherence Reinforcement (LocoRE) to strengthen attention to recent predecessors.

Result: Extensive experiments across multiple LVLMs demonstrate significant reduction in hallucination rates while preserving fluency and task performance.

Conclusion: The method offers a robust and interpretable solution for enhancing model reliability by addressing contextual forgetting behavior through gradient-aware analysis and targeted mitigation strategies.

Abstract: Recent studies have examined attention dynamics in large vision-language models (LVLMs) to detect hallucinations. However, existing approaches remain limited in reliably distinguishing hallucinated from factually grounded outputs, as they rely solely on forward-pass attention patterns and neglect gradient-based signals that reveal how token influence propagates through the network. To bridge this gap, we introduce LVLMs-Saliency, a gradient-aware diagnostic framework that quantifies the visual grounding strength of each output token by fusing attention weights with their input gradients. Our analysis uncovers a decisive pattern: hallucinations frequently arise when preceding output tokens exhibit low saliency toward the prediction of the next token, signaling a breakdown in contextual memory retention. Leveraging this insight, we propose a dual-mechanism inference-time framework to mitigate hallucinations: (1) Saliency-Guided Rejection Sampling (SGRS), which dynamically filters candidate tokens during autoregressive decoding by rejecting those whose saliency falls below a context-adaptive threshold, thereby preventing coherence-breaking tokens from entering the output sequence; and (2) Local Coherence Reinforcement (LocoRE), a lightweight, plug-and-play module that strengthens attention from the current token to its most recent predecessors, actively counteracting the contextual forgetting behavior identified by LVLMs-Saliency. Extensive experiments across multiple LVLMs demonstrate that our method significantly reduces hallucination rates while preserving fluency and task performance, offering a robust and interpretable solution for enhancing model reliability. Code is available at: https://github.com/zhangbaijin/LVLMs-Saliency

</details>


### [16] [A Source-Free Approach for Domain Adaptation via Multiview Image Transformation and Latent Space Consistency](https://arxiv.org/abs/2601.20284)
*Debopom Sutradhar,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Reem E. Mohamed,Sami Azam*

Main category: cs.CV

TL;DR: A novel source-free domain adaptation method using multiview augmentation and latent space consistency to learn domain-invariant features directly from target domain without source data access.


<details>
  <summary>Details</summary>
Motivation: Existing domain adaptation methods require source domain data access, adversarial training, or complex pseudo-labeling techniques, which are computationally expensive. There's a need for more efficient approaches that can adapt without source data.

Method: Proposes source-free domain adaptation using multiview augmentation and latent space consistency. Generates multiple augmented views of target domain data and enforces consistency between their feature representations in latent space. Uses ConvNeXt-based encoder with a loss function combining classification and consistency objectives.

Result: Achieves average classification accuracy of 90.72% on Office-31, 84% on Office-Home, and 97.12% on Office-Caltech datasets. Outperforms existing methods by +1.23%, +7.26%, and +1.77% improvements respectively.

Conclusion: The method successfully enables domain adaptation without source data access by learning transferable representations directly from target domain through multiview consistency, offering a more efficient alternative to existing computationally expensive approaches.

Abstract: Domain adaptation (DA) addresses the challenge of transferring knowledge from a source domain to a target domain where image data distributions may differ. Existing DA methods often require access to source domain data, adversarial training, or complex pseudo-labeling techniques, which are computationally expensive. To address these challenges, this paper introduces a novel source-free domain adaptation method. It is the first approach to use multiview augmentation and latent space consistency techniques to learn domain-invariant features directly from the target domain. Our method eliminates the need for source-target alignment or pseudo-label refinement by learning transferable representations solely from the target domain by enforcing consistency between multiple augmented views in the latent space. Additionally, the method ensures consistency in the learned features by generating multiple augmented views of target domain data and minimizing the distance between their feature representations in the latent space. We also introduce a ConvNeXt-based encoder and design a loss function that combines classification and consistency objectives to drive effective adaptation directly from the target domain. The proposed model achieves an average classification accuracy of 90. 72\%, 84\%, and 97. 12\% in Office-31, Office-Home and Office-Caltech datasets, respectively. Further evaluations confirm that our study improves existing methods by an average classification accuracy increment of +1.23\%, +7.26\%, and +1.77\% on the respective datasets.

</details>


### [17] [Artifact-Aware Evaluation for High-Quality Video Generation](https://arxiv.org/abs/2601.20297)
*Chen Zhu,Jiashu Zhu,Yanxun Li,Meiqi Wu,Bingze Song,Chubin Chen,Jiahong Wu,Xiangxiang Chu,Yangang Wang*

Main category: cs.CV

TL;DR: This paper introduces a comprehensive evaluation protocol and framework for detecting and categorizing specific artifacts in generated videos, addressing the limitation of existing coarse quality scoring methods.


<details>
  <summary>Details</summary>
Motivation: Existing video generation evaluation approaches only provide coarse quality scores without detailed localization and categorization of specific artifacts, which is insufficient for comprehensive quality assessment as video generation techniques advance rapidly.

Method: The authors introduce a comprehensive evaluation protocol focusing on Appearance, Motion, and Camera aspects, define 10 artifact categories, create GenVID dataset (80k generated videos with annotations), and develop DVAR framework for dense video artifact recognition.

Result: Extensive experiments show that the approach significantly improves artifact detection accuracy and enables effective filtering of low-quality content compared to existing methods.

Conclusion: The proposed comprehensive evaluation protocol, dataset, and framework provide fine-grained artifact detection and classification for generated videos, addressing the limitations of current coarse evaluation methods and enabling better quality assessment.

Abstract: With the rapid advancement of video generation techniques, evaluating and auditing generated videos has become increasingly crucial. Existing approaches typically offer coarse video quality scores, lacking detailed localization and categorization of specific artifacts. In this work, we introduce a comprehensive evaluation protocol focusing on three key aspects affecting human perception: Appearance, Motion, and Camera. We define these axes through a taxonomy of 10 prevalent artifact categories reflecting common generative failures observed in video generation. To enable robust artifact detection and categorization, we introduce GenVID, a large-scale dataset of 80k videos generated by various state-of-the-art video generation models, each carefully annotated for the defined artifact categories. Leveraging GenVID, we develop DVAR, a Dense Video Artifact Recognition framework for fine-grained identification and classification of generative artifacts. Extensive experiments show that our approach significantly improves artifact detection accuracy and enables effective filtering of low-quality content.

</details>


### [18] [Towards Compact and Robust DNNs via Compression-aware Sharpness Minimization](https://arxiv.org/abs/2601.20301)
*Jialuo He,Huangxun Chen*

Main category: cs.CV

TL;DR: C-SAM integrates sharpness-aware minimization with pruning by perturbing pruning masks instead of parameters, achieving both model compactness and robustness.


<details>
  <summary>Details</summary>
Motivation: Current SAM methods don't work well with pruning - pruning SAM-trained models hurts robustness, and applying SAM after pruning is limited by suboptimal pruning patterns. Need to jointly optimize for compactness and robustness.

Method: C-SAM shifts sharpness-aware learning from parameter perturbations to mask perturbations. It explicitly perturbs pruning masks during training to find pruning patterns that create flatter loss landscapes with respect to model structure.

Result: C-SAM achieves up to 42% higher certified robustness than baselines on CelebA-HQ, Flowers-102, and CIFAR-10-C across ResNet-18, GoogLeNet, and MobileNet-V2, while maintaining task accuracy comparable to unpruned models.

Conclusion: C-SAM successfully bridges the gap between model robustness and compactness by jointly optimizing pruning patterns and sharpness-aware training through mask perturbations.

Abstract: Sharpness-Aware Minimization (SAM) has recently emerged as an effective technique for improving DNN robustness to input variations. However, its interplay with the compactness requirements of on-device DNN deployments remains less explored. Simply pruning a SAM-trained model can undermine robustness, since flatness in the continuous parameter space does not necessarily translate to robustness under the discrete structural changes induced by pruning. Conversely, applying SAM after pruning may be fundamentally constrained by architectural limitations imposed by an early, robustness-agnostic pruning pattern. To address this gap, we propose Compression-aware ShArpness Minimization (C-SAM), a framework that shifts sharpness-aware learning from parameter perturbations to mask perturbations. By explicitly perturbing pruning masks during training, C-SAM promotes a flatter loss landscape with respect to model structure, enabling the discovery of pruning patterns that simultaneously optimize model compactness and robustness to input variations. Extensive experiments on CelebA-HQ, Flowers-102, and CIFAR-10-C across ResNet-18, GoogLeNet, and MobileNet-V2 show that C-SAM consistently achieves higher certified robustness than strong baselines, with improvements of up to 42%, while maintaining task accuracy comparable to the corresponding unpruned models.

</details>


### [19] [Bridging the Applicator Gap with Data-Doping:Dual-Domain Learning for Precise Bladder Segmentation in CT-Guided Brachytherapy](https://arxiv.org/abs/2601.20302)
*Suresh Das,Siladittya Manna,Sayantari Ghosh*

Main category: cs.CV

TL;DR: A dual domain learning strategy combining CT scans with and without brachytherapy applicators improves bladder segmentation performance under covariate shift, achieving near-exclusive WA data performance with only 10-30% WA data.


<details>
  <summary>Details</summary>
Motivation: Performance degradation due to covariate shift is a major challenge in medical image segmentation. While CT scans without brachytherapy applicators (NA) are widely available, scans with applicators inserted (WA) are scarce and exhibit substantial anatomical deformation and imaging artifacts, making automated segmentation difficult.

Method: Proposed a dual domain learning strategy that integrates NA and WA CT data to improve robustness under covariate shift. Used systematic experiments across axial, coronal, and sagittal planes with multiple deep learning architectures, testing different proportions of WA data in predominantly NA training sets.

Result: NA data alone fails to capture WA characteristics, but introducing 10-30% WA data into NA training achieves segmentation performance comparable to models trained exclusively on WA data. Achieved Dice similarity coefficients up to 0.94 and Intersection over Union scores up to 0.92.

Conclusion: Integrating anatomically similar but distribution-shifted datasets can overcome data scarcity and enhance deep learning segmentation for brachytherapy treatment planning, demonstrating effective domain adaptation and improved clinical reliability.

Abstract: Performance degradation due to covariate shift remains a major challenge for deep learning models in medical image segmentation. An open question is whether samples from a shifted distribution can effectively support learning when combined with limited target domain data. We investigate this problem in the context of bladder segmentation in CT guided gynecological brachytherapy, a critical task for accurate dose optimization and organ at risk sparing. While CT scans without brachytherapy applicators (no applicator: NA) are widely available, scans with applicators inserted (with applicator: WA) are scarce and exhibit substantial anatomical deformation and imaging artifacts, making automated segmentation particularly difficult.
  We propose a dual domain learning strategy that integrates NA and WA CT data to improve robustness and generalizability under covariate shift. Using a curated assorted dataset, we show that NA data alone fail to capture the anatomical and artifact related characteristics of WA images. However, introducing a modest proportion of WA data into a predominantly NA training set leads to significant performance improvements. Through systematic experiments across axial, coronal, and sagittal planes using multiple deep learning architectures, we demonstrate that doping only 10 to 30 percent WA data achieves segmentation performance comparable to models trained exclusively on WA data.
  The proposed approach attains Dice similarity coefficients of up to 0.94 and Intersection over Union scores of up to 0.92, indicating effective domain adaptation and improved clinical reliability. This study highlights the value of integrating anatomically similar but distribution shifted datasets to overcome data scarcity and enhance deep learning based segmentation for brachytherapy treatment planning.

</details>


### [20] [Physically Guided Visual Mass Estimation from a Single RGB Image](https://arxiv.org/abs/2601.20303)
*Sungjae Lee,Junhan Jeong,Yeonjoo Hong,Kwang In Kim*

Main category: cs.CV

TL;DR: A physically structured framework for estimating object mass from single RGB images by combining 3D geometry (for volume) and material semantics (for density) through adaptive fusion and separate regression heads.


<details>
  <summary>Details</summary>
Motivation: Mass estimation from visual input is challenging because mass depends on both geometric volume and material density, neither of which is directly observable from RGB appearance. This makes mass prediction ill-posed and benefits from physically meaningful representations to constrain plausible solutions.

Method: From a single RGB image: 1) Recover object-centric 3D geometry via monocular depth estimation for volume information, 2) Extract coarse material semantics using a vision-language model for density reasoning, 3) Fuse geometry, semantic, and appearance representations through an instance-adaptive gating mechanism, 4) Predict two physically guided latent factors (volume- and density-related) through separate regression heads under mass-only supervision.

Result: The proposed method consistently outperforms state-of-the-art methods on image2mass and ABO-500 datasets.

Conclusion: The physically structured framework successfully addresses the ambiguity in mass estimation by aligning visual cues with physical factors governing mass, demonstrating that incorporating physical priors through separate volume and density reasoning improves mass prediction from single images.

Abstract: Estimating object mass from visual input is challenging because mass depends jointly on geometric volume and material-dependent density, neither of which is directly observable from RGB appearance. Consequently, mass prediction from pixels is ill-posed and therefore benefits from physically meaningful representations to constrain the space of plausible solutions. We propose a physically structured framework for single-image mass estimation that addresses this ambiguity by aligning visual cues with the physical factors governing mass. From a single RGB image, we recover object-centric three-dimensional geometry via monocular depth estimation to inform volume and extract coarse material semantics using a vision-language model to guide density-related reasoning. These geometry, semantic, and appearance representations are fused through an instance-adaptive gating mechanism, and two physically guided latent factors (volume- and density-related) are predicted through separate regression heads under mass-only supervision. Experiments on image2mass and ABO-500 show that the proposed method consistently outperforms state-of-the-art methods.

</details>


### [21] [Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction](https://arxiv.org/abs/2601.20720)
*Matej Halinkovic,Nina Masarykova,Alexey Vinel,Marek Galinski*

Main category: cs.CV

TL;DR: Li-ViP3D++ is a query-based multimodal perception-and-prediction framework that introduces Query-Gated Deformable Fusion to integrate multi-view RGB and LiDAR data in query space for end-to-end autonomous driving tasks.


<details>
  <summary>Details</summary>
Motivation: Modular pipelines restrict information flow and amplify upstream errors. Existing query-based models haven't sufficiently explored camera-LiDAR complementarity in query-space, often using heuristic fusion schemes that introduce bias and prevent full information utilization.

Method: Proposes Query-Gated Deformable Fusion (QGDF) that: (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through differentiable BEV sampling with learned per-query offsets, and (iii) uses query-conditioned gating to adaptively weight visual and geometric cues per agent.

Result: On nuScenes, achieves higher EPA (0.335) and mAP (0.502), reduces false positives (FP ratio 0.147), and is faster than prior Li-ViP3D variant (139.82 ms vs. 145.91 ms).

Conclusion: Query-space, fully differentiable camera-LiDAR fusion increases robustness of end-to-end perception-and-prediction without sacrificing deployability.

Abstract: End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.

</details>


### [22] [Structure-constrained Language-informed Diffusion Model for Unpaired Low-dose Computed Tomography Angiography Reconstruction](https://arxiv.org/abs/2601.20304)
*Genyuan Zhang,Zihao Wang,Zhifan Gao,Lei Xu,Zhen Zhou,Haijun Yu,Jianjia Zhang,Xiujian Liu,Weiwei Zhang,Shaoyu Wang,Huazhu Fu,Fenglin Liu,Weiwen Wu*

Main category: cs.CV

TL;DR: SLDM is a deep learning model that generates normal-dose contrast CT images from low-dose scans using structure constraints and language-informed diffusion to reduce contrast media dose while maintaining diagnostic quality.


<details>
  <summary>Details</summary>
Motivation: Iodinated contrast media (ICM) in CT scans improves diagnostic accuracy but can cause kidney damage and allergic reactions when overdosed. Existing deep learning methods struggle with accurate enhancement using incompletely paired images due to limited structural recognition capabilities.

Method: Proposes Structure-constrained Language-informed Diffusion Model (SLDM) with three key components: 1) Structural prior extraction to constrain model inference and ensure structural consistency, 2) Semantic supervision with spatial intelligence integrating visual perception and spatial reasoning, 3) Subtraction angiography enhancement module to improve contrast in ICM regions to optimal observation levels.

Result: Qualitative visual comparisons and quantitative metrics demonstrate the method's effectiveness in angiographic reconstruction for low-dose contrast medium CT angiography, showing improved enhancement accuracy compared to existing approaches.

Conclusion: SLDM successfully addresses the limitations of existing methods by integrating structural synergy and spatial intelligence, enabling accurate enhancement of low-dose contrast CT images while reducing the required contrast media dose and associated risks.

Abstract: The application of iodinated contrast media (ICM) improves the sensitivity and specificity of computed tomography (CT) for a wide range of clinical indications. However, overdose of ICM can cause problems such as kidney damage and life-threatening allergic reactions. Deep learning methods can generate CT images of normal-dose ICM from low-dose ICM, reducing the required dose while maintaining diagnostic power. However, existing methods are difficult to realize accurate enhancement with incompletely paired images, mainly because of the limited ability of the model to recognize specific structures. To overcome this limitation, we propose a Structure-constrained Language-informed Diffusion Model (SLDM), a unified medical generation model that integrates structural synergy and spatial intelligence. First, the structural prior information of the image is effectively extracted to constrain the model inference process, thus ensuring structural consistency in the enhancement process. Subsequently, semantic supervision strategy with spatial intelligence is introduced, which integrates the functions of visual perception and spatial reasoning, thus prompting the model to achieve accurate enhancement. Finally, the subtraction angiography enhancement module is applied, which serves to improve the contrast of the ICM agent region to suitable interval for observation. Qualitative analysis of visual comparison and quantitative results of several metrics demonstrate the effectiveness of our method in angiographic reconstruction for low-dose contrast medium CT angiography.

</details>


### [23] [TPGDiff: Hierarchical Triple-Prior Guided Diffusion for Image Restoration](https://arxiv.org/abs/2601.20306)
*Yanjie Tu,Qingsen Yan,Axi Niu,Jiacong Tang*

Main category: cs.CV

TL;DR: TPGDiff is a unified image restoration model that uses triple priors (degradation, structural, semantic) to guide diffusion models for better reconstruction of severely degraded images.


<details>
  <summary>Details</summary>
Motivation: Existing unified image restoration methods struggle with content reconstruction in severely degraded regions. While some use semantic information, integrating it into shallow diffusion layers often causes spatial structure issues like blurring artifacts.

Method: Triple-Prior Guided Diffusion (TPGDiff) network that incorporates: 1) degradation priors throughout diffusion trajectory, 2) structural priors (multi-source structural cues) in shallow layers for fine-grained details, and 3) semantic priors (from distillation-driven extractor) in deep layers for high-level guidance. Uses degradation extractor for stage-adaptive control.

Result: Extensive experiments on single- and multi-degradation benchmarks show TPGDiff achieves superior performance and generalization across diverse restoration scenarios.

Conclusion: TPGDiff's hierarchical and complementary prior guidance (degradation, structural, semantic) enables effective unified image restoration, addressing limitations of existing methods in handling severe degradations while preserving spatial structures.

Abstract: All-in-one image restoration aims to address diverse degradation types using a single unified model. Existing methods typically rely on degradation priors to guide restoration, yet often struggle to reconstruct content in severely degraded regions. Although recent works leverage semantic information to facilitate content generation, integrating it into the shallow layers of diffusion models often disrupts spatial structures (\emph{e.g.}, blurring artifacts). To address this issue, we propose a Triple-Prior Guided Diffusion (TPGDiff) network for unified image restoration. TPGDiff incorporates degradation priors throughout the diffusion trajectory, while introducing structural priors into shallow layers and semantic priors into deep layers, enabling hierarchical and complementary prior guidance for image reconstruction. Specifically, we leverage multi-source structural cues as structural priors to capture fine-grained details and guide shallow layers representations. To complement this design, we further develop a distillation-driven semantic extractor that yields robust semantic priors, ensuring reliable high-level guidance at deep layers even under severe degradations. Furthermore, a degradation extractor is employed to learn degradation-aware priors, enabling stage-adaptive control of the diffusion process across all timesteps. Extensive experiments on both single- and multi-degradation benchmarks demonstrate that TPGDiff achieves superior performance and generalization across diverse restoration scenarios. Our project page is: https://leoyjtu.github.io/tpgdiff-project.

</details>


### [24] [OSDEnhancer: Taming Real-World Space-Time Video Super-Resolution with One-Step Diffusion](https://arxiv.org/abs/2601.20308)
*Shuoyan Wei,Feng Li,Chen Zhou,Runmin Cong,Yao Zhao,Huihui Bai*

Main category: cs.CV

TL;DR: OSDEnhancer is the first one-step diffusion framework for real-world space-time video super-resolution, addressing both spatial enhancement and temporal upsampling with coherent dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing STVSR methods struggle with real-world complex degradations and lack robust temporal consistency. Diffusion models show promise for VSR but remain underexplored for STVSR's dual requirements of spatial enhancement and temporal upsampling.

Method: Uses linear pre-interpolation for initial spatiotemporal structures, trains temporal refinement and spatial enhancement mixture of experts (TR-SE MoE) for specialized representations, and employs bidirectional deformable VAE decoder for recurrent spatiotemporal aggregation.

Result: Achieves state-of-the-art performance with superior generalization capability in real-world scenarios while maintaining efficient one-step diffusion inference.

Conclusion: OSDEnhancer successfully addresses real-world STVSR challenges through an efficient one-step diffusion framework that ensures both reconstruction fidelity and temporal coherence.

Abstract: Diffusion models (DMs) have demonstrated exceptional success in video super-resolution (VSR), showcasing a powerful capacity for generating fine-grained details. However, their potential for space-time video super-resolution (STVSR), which necessitates not only recovering realistic visual content from low-resolution to high-resolution but also improving the frame rate with coherent temporal dynamics, remains largely underexplored. Moreover, existing STVSR methods predominantly address spatiotemporal upsampling under simplified degradation assumptions, which often struggle in real-world scenarios with complex unknown degradations. Such a high demand for reconstruction fidelity and temporal consistency makes the development of a robust STVSR framework particularly non-trivial. To address these challenges, we propose OSDEnhancer, a novel framework that, to the best of our knowledge, represents the first method to achieve real-world STVSR through an efficient one-step diffusion process. OSDEnhancer initializes essential spatiotemporal structures through a linear pre-interpolation strategy and pivots on training temporal refinement and spatial enhancement mixture of experts (TR-SE MoE), which allows distinct expert pathways to progressively learn robust, specialized representations for temporal coherence and spatial detail, further collaboratively reinforcing each other during inference. A bidirectional deformable variational autoencoder (VAE) decoder is further introduced to perform recurrent spatiotemporal aggregation and propagation, enhancing cross-frame reconstruction fidelity. Experiments demonstrate that the proposed method achieves state-of-the-art performance while maintaining superior generalization capability in real-world scenarios.

</details>


### [25] [CPiRi: Channel Permutation-Invariant Relational Interaction for Multivariate Time Series Forecasting](https://arxiv.org/abs/2601.20318)
*Jiyuan Xu,Wenyu Zhang,Xin Jing,Shuai Chen,Shuai Zhang,Jiahao Nie*

Main category: cs.CV

TL;DR: CPiRi is a channel permutation invariant framework for multivariate time series forecasting that learns cross-channel structure from data rather than memorizing fixed channel ordering, enabling deployment without retraining when channels are added or reordered.


<details>
  <summary>Details</summary>
Motivation: Current multivariate time series forecasting methods face limitations: channel-dependent models overfit to channel ordering and struggle with channel additions/reordering, while channel-independent models neglect inter-channel dependencies and limit performance. There's a need for models that can handle structural and distributional co-drift without retraining.

Method: CPiRi combines spatio-temporal decoupling architecture with permutation-invariant regularization: 1) frozen pretrained temporal encoder extracts temporal features, 2) lightweight spatial module learns content-driven inter-channel relations, 3) channel shuffling strategy enforces channel permutation invariance during training. The approach is grounded in theory analyzing permutation equivariance in multivariate forecasting.

Result: State-of-the-art results on multiple benchmarks. CPiRi remains stable when channel orders are shuffled and exhibits strong inductive generalization to unseen channels even when trained on only half of the channels, while maintaining practical efficiency on large-scale datasets.

Conclusion: CPiRi addresses limitations of both channel-dependent and channel-independent models by learning cross-channel structure from data rather than memorizing fixed ordering, enabling robust deployment in settings with structural and distributional co-drift without requiring retraining.

Abstract: Current methods for multivariate time series forecasting can be classified into channel-dependent and channel-independent models. Channel-dependent models learn cross-channel features but often overfit the channel ordering, which hampers adaptation when channels are added or reordered. Channel-independent models treat each channel in isolation to increase flexibility, yet this neglects inter-channel dependencies and limits performance. To address these limitations, we propose \textbf{CPiRi}, a \textbf{channel permutation invariant (CPI)} framework that infers cross-channel structure from data rather than memorizing a fixed ordering, enabling deployment in settings with structural and distributional co-drift without retraining. CPiRi couples \textbf{spatio-temporal decoupling architecture} with \textbf{permutation-invariant regularization training strategy}: a frozen pretrained temporal encoder extracts high-quality temporal features, a lightweight spatial module learns content-driven inter-channel relations, while a channel shuffling strategy enforces CPI during training. We further \textbf{ground CPiRi in theory} by analyzing permutation equivariance in multivariate time series forecasting. Experiments on multiple benchmarks show state-of-the-art results. CPiRi remains stable when channel orders are shuffled and exhibits strong \textbf{inductive generalization} to unseen channels even when trained on \textbf{only half} of the channels, while maintaining \textbf{practical efficiency} on large-scale datasets. The source code is released at https://github.com/JasonStraka/CPiRi.

</details>


### [26] [GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction](https://arxiv.org/abs/2601.20331)
*Mai Su,Qihan Yu,Zhongtao Wang,Yilong Li,Chengwei Pan,Yisong Chen,Guoping Wang*

Main category: cs.CV

TL;DR: GVGS improves 3D Gaussian Splatting surface reconstruction using visibility-aware multi-view consistency and progressive monocular depth calibration.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting has efficient optimization and rendering but struggles with accurate surface reconstruction. Existing methods using multi-view geometric consistency or monocular depth priors have limitations: multi-view constraints fail with large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, leading to inaccurate Gaussian depth supervision.

Method: Two key innovations: 1) Gaussian visibility-aware multi-view geometric consistency constraint that aggregates visibility of shared Gaussian primitives across views for more accurate geometric supervision. 2) Progressive quadtree-calibrated monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales to mitigate scale ambiguity while preserving surface details.

Result: Extensive experiments on DTU and TNT datasets show consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods.

Conclusion: The proposed GVGS framework successfully addresses limitations of existing surface reconstruction methods for 3D Gaussian Splatting by combining visibility-aware multi-view constraints with progressive monocular depth calibration, achieving superior geometric accuracy.

Abstract: 3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: https://github.com/GVGScode/GVGS.

</details>


### [27] [Test-Time Adaptation for Anomaly Segmentation via Topology-Aware Optimal Transport Chaining](https://arxiv.org/abs/2601.20333)
*Ali Zia,Usman Ali,Umer Ramzan,Abdul Rehman,Abdelwahed Khamis,Wei Xiang*

Main category: cs.CV

TL;DR: TopoOT: A topology-aware optimal transport framework for anomaly segmentation that uses persistence diagrams and test-time adaptation to achieve SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Deep TDA captures structural invariants across scales, making it suitable for anomaly segmentation. Unlike threshold-based binarization which produces brittle masks under distribution shift, TDA characterizes anomalies as disruptions to global structure rather than local fluctuations.

Method: Introduces TopoOT framework integrating multi-filtration persistence diagrams with test-time adaptation. Key innovation is Optimal Transport Chaining which sequentially aligns PDs across thresholds and filtrations to yield geodesic stability scores. These stability-aware pseudo-labels supervise a lightweight head trained online with OT-consistency and contrastive objectives.

Result: Achieves state-of-the-art performance across standard 2D and 3D anomaly detection benchmarks, outperforming the most competitive methods by up to +24.1% mean F1 on 2D datasets and +10.2% on 3D AS benchmarks.

Conclusion: TopoOT demonstrates that integrating topological data analysis with optimal transport and test-time adaptation provides a robust framework for anomaly segmentation that maintains performance under domain shift.

Abstract: Deep topological data analysis (TDA) offers a principled framework for capturing structural invariants such as connectivity and cycles that persist across scales, making it a natural fit for anomaly segmentation (AS). Unlike thresholdbased binarisation, which produces brittle masks under distribution shift, TDA allows anomalies to be characterised as disruptions to global structure rather than local fluctuations. We introduce TopoOT, a topology-aware optimal transport (OT) framework that integrates multi-filtration persistence diagrams (PDs) with test-time adaptation (TTA). Our key innovation is Optimal Transport Chaining, which sequentially aligns PDs across thresholds and filtrations, yielding geodesic stability scores that identify features consistently preserved across scales. These stabilityaware pseudo-labels supervise a lightweight head trained online with OT-consistency and contrastive objectives, ensuring robust adaptation under domain shift. Across standard 2D and 3D anomaly detection benchmarks, TopoOT achieves state-of-the-art performance, outperforming the most competitive methods by up to +24.1% mean F1 on 2D datasets and +10.2% on 3D AS benchmarks.

</details>


### [28] [MMSF: Multitask and Multimodal Supervised Framework for WSI Classification and Survival Analysis](https://arxiv.org/abs/2601.20347)
*Chengying She,Chengwei Chen,Xinran Zhang,Ben Wang,Lizhuang Liu,Chengwei Shao,Yun Bian*

Main category: cs.CV

TL;DR: MMSF is a multimodal framework for computational pathology that integrates whole slide images and clinical data using a linear-complexity MIL backbone with explicit cross-modal decomposition and fusion, achieving significant performance improvements on cancer diagnosis and survival prediction tasks.


<details>
  <summary>Details</summary>
Motivation: Multimodal evidence is crucial in computational pathology, but integrating heterogeneous signals like gigapixel whole slide images and patient-level clinical descriptors is challenging due to distinct feature statistics and scales.

Method: MMSF uses a multitask multimodal supervised framework with: 1) graph feature extraction for tissue topology, 2) clinical data embedding for patient attributes, 3) feature fusion aligning modality-shared and modality-specific representations, and 4) a Mamba-based MIL encoder with multitask prediction heads.

Result: On CAMELYON16 and TCGA-NSCLC: 2.1-6.6% accuracy and 2.2-6.9% AUC improvements. On five TCGA survival cohorts: 7.1-9.8% C-index improvements over unimodal methods and 5.6-7.1% over multimodal alternatives.

Conclusion: MMSF effectively integrates multimodal pathology data through explicit decomposition and fusion of cross-modal information, demonstrating superior performance for both cancer diagnosis and survival prediction tasks.

Abstract: Multimodal evidence is critical in computational pathology: gigapixel whole slide images capture tumor morphology, while patient-level clinical descriptors preserve complementary context for prognosis. Integrating such heterogeneous signals remains challenging because feature spaces exhibit distinct statistics and scales. We introduce MMSF, a multitask and multimodal supervised framework built on a linear-complexity MIL backbone that explicitly decomposes and fuses cross-modal information. MMSF comprises a graph feature extraction module embedding tissue topology at the patch level, a clinical data embedding module standardizing patient attributes, a feature fusion module aligning modality-shared and modality-specific representations, and a Mamba-based MIL encoder with multitask prediction heads. Experiments on CAMELYON16 and TCGA-NSCLC demonstrate 2.1--6.6\% accuracy and 2.2--6.9\% AUC improvements over competitive baselines, while evaluations on five TCGA survival cohorts yield 7.1--9.8\% C-index improvements compared with unimodal methods and 5.6--7.1\% over multimodal alternatives.

</details>


### [29] [PalmBridge: A Plug-and-Play Feature Alignment Framework for Open-Set Palmprint Verification](https://arxiv.org/abs/2601.20351)
*Chenke Zhang,Ziyuan Yang,Licheng Yan,Shuyi Li,Andrew Beng Jin Teoh,Bob Zhang,Yi Zhang*

Main category: cs.CV

TL;DR: PalmBridge is a plug-and-play feature-space alignment framework using vector quantization to address domain shift in palmprint recognition by learning representative vectors and blending them with original features to suppress nuisance variations while preserving identity cues.


<details>
  <summary>Details</summary>
Motivation: Real-world palmprint recognition performance degrades due to feature distribution shifts from heterogeneous deployment conditions. Current deep models assume closed stationary distributions, leading to overfitting to dataset-specific textures rather than learning domain-invariant representations. Data augmentation often fails under significant domain mismatch.

Method: PalmBridge learns a compact set of representative vectors from training features using vector quantization. During enrollment/verification, each feature vector is mapped to its nearest representative vector under minimum-distance criterion, then blended with the original vector. The framework uses joint optimization with backbone network via task supervision, feature-consistency objective, and orthogonality regularization to form a stable shared embedding space.

Result: Experiments on multiple palmprint datasets and backbone architectures show PalmBridge consistently reduces EER in intra-dataset open-set evaluation and improves cross-dataset generalization with negligible to modest runtime overhead.

Conclusion: PalmBridge effectively addresses domain shift in palmprint recognition through feature-space alignment with vector quantization, providing a practical plug-and-play solution that enhances generalization across heterogeneous deployment conditions without significant computational cost.

Abstract: Palmprint recognition is widely used in biometric systems, yet real-world performance often degrades due to feature distribution shifts caused by heterogeneous deployment conditions. Most deep palmprint models assume a closed and stationary distribution, leading to overfitting to dataset-specific textures rather than learning domain-invariant representations. Although data augmentation is commonly used to mitigate this issue, it assumes augmented samples can approximate the target deployment distribution, an assumption that often fails under significant domain mismatch. To address this limitation, we propose PalmBridge, a plug-and-play feature-space alignment framework for open-set palmprint verification based on vector quantization. Rather than relying solely on data-level augmentation, PalmBridge learns a compact set of representative vectors directly from training features. During enrollment and verification, each feature vector is mapped to its nearest representative vector under a minimum-distance criterion, and the mapped vector is then blended with the original vector. This design suppresses nuisance variation induced by domain shifts while retaining discriminative identity cues. The representative vectors are jointly optimized with the backbone network using task supervision, a feature-consistency objective, and an orthogonality regularization term to form a stable and well-structured shared embedding space. Furthermore, we analyze feature-to-representative mappings via assignment consistency and collision rate to assess model's sensitivity to blending weights. Experiments on multiple palmprint datasets and backbone architectures show that PalmBridge consistently reduces EER in intra-dataset open-set evaluation and improves cross-dataset generalization with negligible to modest runtime overhead.

</details>


### [30] [Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models](https://arxiv.org/abs/2601.20354)
*Zengbin Wang,Xuecai Hu,Yong Wang,Feng Xiong,Man Zhang,Xiangxiang Chu*

Main category: cs.CV

TL;DR: SpatialGenEval is a new benchmark for evaluating spatial intelligence in text-to-image models using 1,230 long, information-dense prompts across 25 real-world scenes with 10 spatial sub-domains.


<details>
  <summary>Details</summary>
Motivation: Current T2I models fail at complex spatial relationships (perception, reasoning, interaction), and existing benchmarks overlook these aspects due to short or information-sparse prompt design.

Method: Created SpatialGenEval benchmark with 1,230 long prompts covering 25 scenes and 10 spatial sub-domains (object position, layout, occlusion, causality). Also built SpatialT2I dataset with 15,400 text-image pairs with rewritten prompts for training.

Result: Evaluation of 21 SOTA models shows higher-order spatial reasoning is a primary bottleneck. Fine-tuning on SpatialT2I dataset yields consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic spatial relations.

Conclusion: SpatialGenEval reveals spatial reasoning limitations in T2I models, and the SpatialT2I dataset demonstrates a data-centric paradigm to achieve spatial intelligence through information-dense training data.

Abstract: Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.

</details>


### [31] [CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization](https://arxiv.org/abs/2601.20355)
*Yue Liang,Jiatong Du,Ziyi Yang,Yanjun Huang,Hong Chen*

Main category: cs.CV

TL;DR: CURVE is a causality-inspired framework that uses variational uncertainty modeling and structural regularization to suppress spurious correlations in scene graphs, improving out-of-distribution generalization.


<details>
  <summary>Details</summary>
Motivation: Scene graphs often overfit to spurious correlations, which severely hinders out-of-distribution generalization. Current approaches fail to address this limitation effectively.

Method: CURVE integrates variational uncertainty modeling with uncertainty-guided structural regularization. It applies prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting sparse and domain-stable topology.

Result: Empirical evaluation shows CURVE's effectiveness in zero-shot transfer and low-data sim-to-real adaptation. It learns domain-stable sparse topologies and provides reliable uncertainty estimates for risk prediction under distribution shifts.

Conclusion: CURVE addresses the spurious correlation problem in scene graphs through causality-inspired uncertainty modeling, enabling better out-of-distribution generalization and reliable uncertainty estimation for distribution shifts.

Abstract: Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.

</details>


### [32] [RAW-Flow: Advancing RGB-to-RAW Image Reconstruction with Deterministic Latent Flow Matching](https://arxiv.org/abs/2601.20364)
*Zhen Liu,Diedong Feng,Hai Jiang,Liaoyuan Zeng,Hao Wang,Chaoyu Feng,Lei Lei,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: RAW-Flow: A flow matching framework for RGB-to-RAW reconstruction that treats it as a deterministic latent transport problem, outperforming previous regression-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing learning-based RGB-to-RAW reconstruction methods treat it as direct regression, suffering from detail inconsistency and color deviation due to the ill-posed inverse ISP problem and information loss in quantized RGB images.

Method: Reformulates RGB-to-RAW reconstruction as deterministic latent transport using flow matching to learn vector fields in latent space. Includes cross-scale context guidance module for hierarchical RGB feature injection, and dual-domain latent autoencoder with feature alignment constraint for stable training.

Result: Extensive experiments show RAW-Flow outperforms state-of-the-art approaches both quantitatively and visually.

Conclusion: The generative perspective with flow matching effectively addresses limitations of regression-based methods, enabling accurate reconstruction of structural details and color information in RGB-to-RAW tasks.

Abstract: RGB-to-RAW reconstruction, or the reverse modeling of a camera Image Signal Processing (ISP) pipeline, aims to recover high-fidelity RAW data from RGB images. Despite notable progress, existing learning-based methods typically treat this task as a direct regression objective and struggle with detail inconsistency and color deviation, due to the ill-posed nature of inverse ISP and the inherent information loss in quantized RGB images. To address these limitations, we pioneer a generative perspective by reformulating RGB-to-RAW reconstruction as a deterministic latent transport problem and introduce a novel framework named RAW-Flow, which leverages flow matching to learn a deterministic vector field in latent space, to effectively bridge the gap between RGB and RAW representations and enable accurate reconstruction of structural details and color information. To further enhance latent transport, we introduce a cross-scale context guidance module that injects hierarchical RGB features into the flow estimation process. Moreover, we design a dual-domain latent autoencoder with a feature alignment constraint to support the proposed latent transport framework, which jointly encodes RGB and RAW inputs while promoting stable training and high-fidelity reconstruction. Extensive experiments demonstrate that RAW-Flow outperforms state-of-the-art approaches both quantitatively and visually.

</details>


### [33] [Dual-Modality IoT Framework for Integrated Access Control and Environmental Safety Monitoring with Real-Time Cloud Analytics](https://arxiv.org/abs/2601.20366)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib,Nihal Das Ankur,Anish Giri*

Main category: cs.CV

TL;DR: A dual-modality IoT framework integrating RFID access control with environmental safety monitoring achieves 99.2% authentication accuracy and 98.5% flame detection reliability at 82% cost reduction compared to commercial solutions.


<details>
  <summary>Details</summary>
Motivation: Traditional physical security and environmental safety systems operate as independent silos, creating operational inefficiencies, delayed emergency responses, and increased management complexity in smart infrastructure management.

Method: A comprehensive dual-modality IoT framework with two coordinated subsystems: Subsystem 1 implements RFID authentication with servo-actuated gate control and real-time Google Sheets logging; Subsystem 2 provides safety monitoring with flame detection, water flow measurement, LCD status display, and personnel identification. Both use ESP32 microcontrollers for edge processing and wireless connectivity through a unified cloud architecture.

Result: 45-day experimental evaluation showed: 99.2% RFID authentication accuracy with 0.82-second average response time, 98.5% flame detection reliability within 5-meter range, 99.8% cloud data logging success rate, operational integrity during network disruptions via local caching, and total implementation cost of 5,400 BDT (~$48) representing 82% reduction compared to commercial integrated solutions.

Conclusion: The research establishes a practical framework for synergistic security-safety integration, demonstrating that professional-grade performance can be achieved through careful architectural design and component optimization while maintaining exceptional cost-effectiveness and accessibility for diverse application scenarios.

Abstract: The integration of physical security systems with environmental safety monitoring represents a critical advancement in smart infrastructure management. Traditional approaches maintain these systems as independent silos, creating operational inefficiencies, delayed emergency responses, and increased management complexity. This paper presents a comprehensive dual-modality Internet of Things framework that seamlessly integrates RFID-based access control with multi-sensor environmental safety monitoring through a unified cloud architecture. The system comprises two coordinated subsystems: Subsystem 1 implements RFID authentication with servo-actuated gate control and real-time Google Sheets logging, while Subsystem 2 provides comprehensive safety monitoring incorporating flame detection, water flow measurement, LCD status display, and personnel identification. Both subsystems utilize ESP32 microcontrollers for edge processing and wireless connectivity. Experimental evaluation over 45 days demonstrates exceptional performance metrics: 99.2\% RFID authentication accuracy with 0.82-second average response time, 98.5\% flame detection reliability within 5-meter range, and 99.8\% cloud data logging success rate. The system maintains operational integrity during network disruptions through intelligent local caching mechanisms and achieves total implementation cost of 5,400 BDT (approximately \$48), representing an 82\% reduction compared to commercial integrated solutions. This research establishes a practical framework for synergistic security-safety integration, demonstrating that professional-grade performance can be achieved through careful architectural design and component optimization while maintaining exceptional cost-effectiveness and accessibility for diverse application scenarios.

</details>


### [34] [RepSFNet : A Single Fusion Network with Structural Reparameterization for Crowd Counting](https://arxiv.org/abs/2601.20369)
*Mas Nurul Achmadiah,Chi-Chia Sun,Wen-Kai Kuo,Jun-Wei Hsieh*

Main category: cs.CV

TL;DR: RepSFNet is a lightweight crowd counting model using reparameterized kernels and feature fusion for accurate, real-time performance with 34% faster inference than SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Address challenges in crowd counting: scale variations, occlusions, and high computational cost of existing models. Need for lightweight architecture suitable for real-time and edge computing applications.

Method: Propose RepSFNet with RepLK-ViT backbone (large reparameterized kernels), Feature Fusion module (ASPP + CAN), Concatenate Fusion module, avoiding attention mechanisms and multi-branch designs. Training uses MSE + Optimal Transport loss.

Result: Achieves competitive accuracy on ShanghaiTech, NWPU, and UCF-QNRF datasets while reducing inference latency by up to 34% compared to recent SOTA methods.

Conclusion: RepSFNet provides accurate, lightweight crowd counting suitable for real-time and low-power edge computing applications with significantly reduced computational complexity.

Abstract: Crowd counting remains challenging in variable-density scenes due to scale variations, occlusions, and the high computational cost of existing models. To address these issues, we propose RepSFNet (Reparameterized Single Fusion Network), a lightweight architecture designed for accurate and real-time crowd estimation. RepSFNet leverages a RepLK-ViT backbone with large reparameterized kernels for efficient multi-scale feature extraction. It further integrates a Feature Fusion module combining Atrous Spatial Pyramid Pooling (ASPP) and Context-Aware Network (CAN) to achieve robust, density-adaptive context modeling. A Concatenate Fusion module is employed to preserve spatial resolution and generate high-quality density maps. By avoiding attention mechanisms and multi-branch designs, RepSFNet significantly reduces parameters and computational complexity. The training objective combines Mean Squared Error and Optimal Transport loss to improve both count accuracy and spatial distribution alignment. Experiments conducted on ShanghaiTech, NWPU, and UCF-QNRF datasets demonstrate that RepSFNet achieves competitive accuracy while reducing inference latency by up to 34 percent compared to recent state-of-the-art methods, making it suitable for real-time and low-power edge computing applications.

</details>


### [35] [HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation](https://arxiv.org/abs/2601.20383)
*Mengge Liu,Yan Di,Gu Wang,Yun Qu,Dekai Zhu,Yanyan Li,Xiangyang Ji*

Main category: cs.CV

TL;DR: HINT is an autoregressive diffusion framework for multi-human motion generation that handles variable text lengths and agent counts through hierarchical interaction modeling and sliding-window generation.


<details>
  <summary>Details</summary>
Motivation: Existing offline methods for multi-human motion generation are limited to fixed-length motions with fixed numbers of agents, making them unsuitable for handling long/variable text descriptions and varying agent counts. Autoregressive approaches are needed but challenging.

Method: HINT uses: 1) Disentangled motion representation in canonicalized latent space separating local motion semantics from inter-person interactions, enabling adaptation to varying human counts; 2) Sliding-window strategy for online generation with aggregation of local within-window and global cross-window conditions to capture past history, inter-person dependencies, and text alignment.

Result: HINT matches strong offline models and surpasses autoregressive baselines. On InterHuman benchmark, achieves FID of 3.100, significantly improving over previous state-of-the-art score of 5.154.

Conclusion: HINT is the first autoregressive framework for multi-human motion generation that effectively handles variable text lengths and agent counts through hierarchical interaction modeling, enabling both fine-grained interactions and long-horizon coherence.

Abstract: Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154.

</details>


### [36] [Let's Roll a BiFTA: Bi-refinement for Fine-grained Text-visual Alignment in Vision-Language Models](https://arxiv.org/abs/2601.20419)
*Yuhao Sun,Chengyi Cai,Jiacheng Zhang,Zesheng Ye,Xingliang Yuan,Feng Liu*

Main category: cs.CV

TL;DR: BiFTA improves CLIP's zero-shot performance by removing redundant information from both image patches and text descriptions through view and description refinement.


<details>
  <summary>Details</summary>
Motivation: Fine-grained text descriptions and localized image patches often contain redundant information that makes text-visual alignment less effective in vision-language models like CLIP.

Method: BiFTA uses two refinement approaches: (1) View refinement removes redundant image patches with high IoU ratios, and (2) Description refinement removes redundant text descriptions with high pairwise cosine similarity.

Result: BiFTA achieves superior zero-shot performance on 6 benchmark datasets for both ViT-based and ResNet-based CLIP models.

Conclusion: Removing redundant information in both visual and text modalities is necessary for effective fine-grained text-visual alignment, as demonstrated by BiFTA's improved performance.

Abstract: Recent research has shown that aligning fine-grained text descriptions with localized image patches can significantly improve the zero-shot performance of pre-trained vision-language models (e.g., CLIP). However, we find that both fine-grained text descriptions and localized image patches often contain redundant information, making text-visual alignment less effective. In this paper, we tackle this issue from two perspectives: \emph{View Refinement} and \emph{Description refinement}, termed as \textit{\textbf{Bi}-refinement for \textbf{F}ine-grained \textbf{T}ext-visual \textbf{A}lignment} (BiFTA). \emph{View refinement} removes redundant image patches with high \emph{Intersection over Union} (IoU) ratios, resulting in more distinctive visual samples. \emph{Description refinement} removes redundant text descriptions with high pairwise cosine similarity, ensuring greater diversity in the remaining descriptions. BiFTA achieves superior zero-shot performance on 6 benchmark datasets for both ViT-based and ResNet-based CLIP, justifying the necessity to remove redundant information in visual-text alignment.

</details>


### [37] [Quartet of Diffusions: Structure-Aware Point Cloud Generation through Part and Symmetry Guidance](https://arxiv.org/abs/2601.20425)
*Chenliang Zhou,Fangcheng Zhong,Weihao Xia,Albert Miao,Canberk Baykal,Cengiz Oztireli*

Main category: cs.CV

TL;DR: A novel point cloud generation framework using four coordinated diffusion models to explicitly model part composition and symmetry for structured 3D shape generation.


<details>
  <summary>Details</summary>
Motivation: Prior methods treat shape generation holistically or only support part composition, lacking explicit modeling of symmetry and structured part assembly. There's a need for frameworks that can enforce both symmetry and part priors throughout generation while enabling fine-grained control.

Method: Quartet of Diffusions: four coordinated diffusion models that learn distributions of (1) global shape latents, (2) symmetries, (3) semantic parts, and (4) spatial assembly. This structured pipeline disentangles generation into interpretable components with a central global latent for structural coherence.

Result: Achieves state-of-the-art performance with guaranteed symmetry, coherent part placement, and diverse, high-quality outputs. Enables fine-grained control over shape attributes and targeted manipulation of individual parts while preserving global consistency.

Conclusion: First 3D point cloud generation framework to fully integrate and enforce both symmetry and part priors throughout the generative process, offering structured, controllable shape generation with explicit modeling of composition and symmetry.

Abstract: We introduce the Quartet of Diffusions, a structure-aware point cloud generation framework that explicitly models part composition and symmetry. Unlike prior methods that treat shape generation as a holistic process or only support part composition, our approach leverages four coordinated diffusion models to learn distributions of global shape latents, symmetries, semantic parts, and their spatial assembly. This structured pipeline ensures guaranteed symmetry, coherent part placement, and diverse, high-quality outputs. By disentangling the generative process into interpretable components, our method supports fine-grained control over shape attributes, enabling targeted manipulation of individual parts while preserving global consistency. A central global latent further reinforces structural coherence across assembled parts. Our experiments show that the Quartet achieves state-of-the-art performance. To our best knowledge, this is the first 3D point cloud generation framework that fully integrates and enforces both symmetry and part priors throughout the generative process.

</details>


### [38] [Youtu-Parsing: Perception, Structuring and Recognition via High-Parallelism Decoding](https://arxiv.org/abs/2601.20430)
*Kun Yin,Yunfei Wu,Bing Liu,Zhongpeng Cai,Xiaotian Li,Huang Chen,Xin Li,Haoyu Cao,Yinsong Liu,Deqiang Jiang,Xing Sun,Yunsheng Wu,Qianyu Li,Antai Guo,Yanzhen Liao,Yanqiu Qu,Haodong Lin,Chengxu He,Shuangyin Liu*

Main category: cs.CV

TL;DR: Youtu-Parsing is a high-performance document parsing model combining Vision Transformer with Youtu-LLM-2B, featuring parallel decoding strategies (token and query parallelism) for 5-11x speedup while maintaining SOTA performance on document parsing benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for efficient and versatile document parsing that can handle diverse document elements (text, formulas, tables, charts, seals, hierarchical structures) with robustness to rare characters, multilingual text, and handwritten content, while achieving high performance and practical utility for large-scale document intelligence applications.

Method: The architecture uses a native Vision Transformer with dynamic-resolution visual encoder for shared document feature extraction, coupled with a prompt-guided Youtu-LLM-2B language model for layout analysis and region-prompted decoding. The key innovation is a high-parallelism decoding strategy with two components: token parallelism (generating up to 64 candidate tokens per step with verification) and query parallelism (simultaneous content prediction for multiple bounding boxes up to five).

Result: Youtu-Parsing achieves state-of-the-art performance on both OmniDocBench and olmOCR-bench benchmarks. The token parallelism provides 5-11x speedup over traditional autoregressive decoding, while query parallelism offers additional 2x acceleration while maintaining output quality equivalent to standard decoding.

Conclusion: Youtu-Parsing demonstrates significant experimental value and practical utility for large-scale document intelligence applications, offering an efficient, versatile parsing solution with strong robustness across diverse document elements and challenging content types.

Abstract: This paper presents Youtu-Parsing, an efficient and versatile document parsing model designed for high-performance content extraction. The architecture employs a native Vision Transformer (ViT) featuring a dynamic-resolution visual encoder to extract shared document features, coupled with a prompt-guided Youtu-LLM-2B language model for layout analysis and region-prompted decoding. Leveraging this decoupled and feature-reusable framework, we introduce a high-parallelism decoding strategy comprising two core components: token parallelism and query parallelism. The token parallelism strategy concurrently generates up to 64 candidate tokens per inference step, which are subsequently validated through a verification mechanism. This approach yields a 5--11x speedup over traditional autoregressive decoding and is particularly well-suited for highly structured scenarios, such as table recognition. To further exploit the advantages of region-prompted decoding, the query parallelism strategy enables simultaneous content prediction for multiple bounding boxes (up to five), providing an additional 2x acceleration while maintaining output quality equivalent to standard decoding. Youtu-Parsing encompasses a diverse range of document elements, including text, formulas, tables, charts, seals, and hierarchical structures. Furthermore, the model exhibits strong robustness when handling rare characters, multilingual text, and handwritten content. Extensive evaluations demonstrate that Youtu-Parsing achieves state-of-the-art (SOTA) performance on both the OmniDocBench and olmOCR-bench benchmarks. Overall, Youtu-Parsing demonstrates significant experimental value and practical utility for large-scale document intelligence applications.

</details>


### [39] [MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models](https://arxiv.org/abs/2601.20433)
*Wenbo Xu,Wei Lu,Xiangyang Luo,Jiantao Zhou*

Main category: cs.CV

TL;DR: MARE is a vision-language model approach for explainable deepfake detection that uses multimodal alignment, reinforcement learning from human feedback, and forgery disentanglement to improve accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods mainly focus on classification or spatial localization, but rapid advancements in generative models require more sophisticated approaches. There's a need for explainable detection that combines accuracy with human-understandable reasoning.

Method: MARE uses multimodal alignment and reinforcement learning from human feedback (RLHF) with comprehensive reward functions to generate text-spatially aligned reasoning content. It also introduces a forgery disentanglement module to separate intrinsic forgery traces from high-level facial semantics.

Result: MARE achieves state-of-the-art performance in both quantitative and qualitative evaluations, demonstrating superior accuracy and reliability in deepfake detection and reasoning content generation.

Conclusion: The proposed MARE framework successfully enhances vision-language models for explainable deepfake detection through multimodal alignment, reinforcement learning, and forgery disentanglement, addressing the growing demands of advanced generative models.

Abstract: Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability.

</details>


### [40] [Exploiting the Final Component of Generator Architectures for AI-Generated Image Detection](https://arxiv.org/abs/2601.20461)
*Yanzhu Liu,Xiao Liu,Yuexuan Wang,Mondal Soumik*

Main category: cs.CV

TL;DR: The paper proposes a novel approach to detect AI-generated images by "contaminating" real images using generators' final architectural components, achieving 98.83% accuracy across unseen generators with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detectors generalize poorly to images from unseen AI generators, despite many modern generators sharing common final architectural components that convert intermediate representations into images.

Method: The method "contaminates" real images using generators' final components and trains a detector to distinguish them from original real images. A taxonomy categorizes 21 generators by their final components, enabling systematic generalization testing.

Result: Using only 100 samples from each of three representative categories, the detector (fine-tuned on DINOv3 backbone) achieves 98.83% average accuracy across 22 testing sets from unseen generators.

Conclusion: The approach effectively addresses generalization challenges in AI-generated image detection by leveraging shared architectural patterns in modern generators, achieving high accuracy with minimal training data.

Abstract: With the rapid proliferation of powerful image generators, accurate detection of AI-generated images has become essential for maintaining a trustworthy online environment. However, existing deepfake detectors often generalize poorly to images produced by unseen generators. Notably, despite being trained under vastly different paradigms, such as diffusion or autoregressive modeling, many modern image generators share common final architectural components that serve as the last stage for converting intermediate representations into images. Motivated by this insight, we propose to "contaminate" real images using the generator's final component and train a detector to distinguish them from the original real images. We further introduce a taxonomy based on generators' final components and categorize 21 widely used generators accordingly, enabling a comprehensive investigation of our method's generalization capability. Using only 100 samples from each of three representative categories, our detector-fine-tuned on the DINOv3 backbone-achieves an average accuracy of 98.83% across 22 testing sets from unseen generators.

</details>


### [41] [Efficient Autoregressive Video Diffusion with Dummy Head](https://arxiv.org/abs/2601.20499)
*Hang Guo,Zhaoyang Jia,Jiahao Li,Bin Li,Yuanhao Cai,Jiangshan Wang,Yawei Li,Yan Lu*

Main category: cs.CV

TL;DR: Dummy Forcing: A method to optimize autoregressive video diffusion models by reducing context redundancy in multi-head self-attention, achieving 2x speedup with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: The authors identify that multi-head self-attention in autoregressive video diffusion models under-utilizes historical frames, with about 25% of attention heads focusing almost exclusively on the current frame, making their KV caches redundant.

Method: Proposes Dummy Forcing with three components: 1) heterogeneous memory allocation to reduce head-wise context redundancy, 2) dynamic head programming to adaptively classify head types, and 3) context packing for more aggressive cache compression.

Result: Achieves up to 2.0x speedup over baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop, without requiring additional training.

Conclusion: Dummy Forcing effectively optimizes autoregressive video diffusion models by addressing attention head redundancy, enabling faster video generation with negligible quality degradation.

Abstract: The autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising. In this work, we identify that the multi-head self-attention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, a simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop a context packing technique to achieve more aggressive cache compression. Without additional training, our Dummy Forcing delivers up to 2.0x speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at https://csguoh.github.io/project/DummyForcing/.

</details>


### [42] [Comparative evaluation of training strategies using partially labelled datasets for segmentation of white matter hyperintensities and stroke lesions in FLAIR MRI](https://arxiv.org/abs/2601.20503)
*Jesse Phitidis,Alison Q. Smithard,William N. Whiteley,Joanna M. Wardlaw,Miguel O. Bernabeu,Maria ValdÃ©s HernÃ¡ndez*

Main category: cs.CV

TL;DR: Researchers investigated six training strategies for a combined WMH and ISL segmentation model using partially labeled data from multiple datasets totaling 2052 MRI volumes, finding pseudolabeling to be the most effective approach.


<details>
  <summary>Details</summary>
Motivation: WMH and ISL are imaging features of cerebral small vessel disease that visually confound each other on FLAIR MRI scans and often co-occur, making deep learning segmentation challenging. The difficulty is compounded by limited fully labeled data and the need to differentiate these similar-looking features.

Method: Six strategies were investigated for training a combined WMH and ISL segmentation model using partially labeled data. The approach combined private fully/partially labeled datasets with public partially labeled datasets, totaling 2052 MRI volumes (1341 with WMH annotations, 1152 with ISL annotations).

Result: Several methods effectively leveraged partially labeled data to improve model performance. The pseudolabeling approach yielded the best results among the six investigated strategies.

Conclusion: Partially labeled data can be effectively utilized for combined WMH and ISL segmentation, with pseudolabeling emerging as the most successful strategy for improving model performance when dealing with visually confounding features that often appear together.

Abstract: White matter hyperintensities (WMH) and ischaemic stroke lesions (ISL) are imaging features associated with cerebral small vessel disease (SVD) that are visible on brain magnetic resonance imaging (MRI) scans. The development and validation of deep learning models to segment and differentiate these features is difficult because they visually confound each other in the fluid-attenuated inversion recovery (FLAIR) sequence and often appear in the same subject. We investigated six strategies for training a combined WMH and ISL segmentation model using partially labelled data. We combined privately held fully and partially labelled datasets with publicly available partially labelled datasets to yield a total of 2052 MRI volumes, with 1341 and 1152 containing ground truth annotations for WMH and ISL respectively. We found that several methods were able to effectively leverage the partially labelled data to improve model performance, with the use of pseudolabels yielding the best result.

</details>


### [43] [Latent Temporal Discrepancy as Motion Prior: A Loss-Weighting Strategy for Dynamic Fidelity in T2V](https://arxiv.org/abs/2601.20504)
*Meiqi Wu,Bingze Song,Ruimin Lin,Chen Zhu,Xiaokun Feng,Jiahong Wu,Xiangxiang Chu,Kaiqi Huang*

Main category: cs.CV

TL;DR: The paper introduces Latent Temporal Discrepancy (LTD) as a motion prior to improve video generation quality in dynamic scenarios by applying motion-aware loss weighting.


<details>
  <summary>Details</summary>
Motivation: Current video generation models perform well in static scenarios but degrade in motion-heavy videos due to noise disrupting temporal coherence and difficulty learning dynamic regions. Existing diffusion models use static loss for all scenarios, limiting their ability to capture complex dynamics.

Method: Proposes Latent Temporal Discrepancy (LTD) as a motion prior to guide loss weighting. LTD measures frame-to-frame variation in latent space, assigning larger penalties to high-discrepancy (dynamic) regions while maintaining regular optimization for stable regions. This motion-aware strategy stabilizes training and enables better reconstruction of high-frequency dynamics.

Result: Extensive experiments on VBench (general benchmark) and VMBench (motion-focused benchmark) show consistent gains. The method outperforms strong baselines by 3.31% on VBench and 3.58% on VMBench, achieving significant improvements in motion quality.

Conclusion: The LTD motion prior effectively addresses the limitations of static loss in video generation models, enabling better handling of dynamic scenarios and improving motion quality through motion-aware loss weighting.

Abstract: Video generation models have achieved notable progress in static scenarios, yet their performance in motion video generation remains limited, with quality degrading under drastic dynamic changes. This is due to noise disrupting temporal coherence and increasing the difficulty of learning dynamic regions. {Unfortunately, existing diffusion models rely on static loss for all scenarios, constraining their ability to capture complex dynamics.} To address this issue, we introduce Latent Temporal Discrepancy (LTD) as a motion prior to guide loss weighting. LTD measures frame-to-frame variation in the latent space, assigning larger penalties to regions with higher discrepancy while maintaining regular optimization for stable regions. This motion-aware strategy stabilizes training and enables the model to better reconstruct high-frequency dynamics. Extensive experiments on the general benchmark VBench and the motion-focused VMBench show consistent gains, with our method outperforming strong baselines by 3.31% on VBench and 3.58% on VMBench, achieving significant improvements in motion quality.

</details>


### [44] [Say Cheese! Detail-Preserving Portrait Collection Generation via Natural Language Edits](https://arxiv.org/abs/2601.20511)
*Zelong Sun,Jiahui Wu,Ying Ba,Dong Jing,Zhiwu Lu*

Main category: cs.CV

TL;DR: Portrait Collection Generation (PCG) is a new task for creating coherent portrait collections by editing reference portraits with natural language instructions. The paper introduces CHEESE dataset (24K collections, 573K samples) and SCheese framework with adaptive feature fusion and ConsistencyNet for identity/detail preservation.


<details>
  <summary>Details</summary>
Motivation: Social media platforms need intuitive ways to create diverse, high-quality portrait collections. Existing methods struggle with complex multi-attribute modifications (pose, layout, viewpoint) while preserving high-fidelity details (identity, clothing, accessories).

Method: 1) CHEESE dataset construction using Large Vision-Language Model pipeline with inversion-based verification. 2) SCheese framework combining text-guided generation with hierarchical identity/detail preservation, featuring adaptive feature fusion and ConsistencyNet for fine-grained feature injection.

Result: CHEESE dataset contains 24K portrait collections and 573K samples with high-quality modification text annotations. SCheese achieves state-of-the-art performance in comprehensive experiments, effectively addressing PCG challenges.

Conclusion: The paper introduces PCG as a novel task, provides the first large-scale dataset (CHEESE), and proposes an effective framework (SCheese) that successfully handles complex multi-attribute modifications while preserving identity and detail consistency.

Abstract: As social media platforms proliferate, users increasingly demand intuitive ways to create diverse, high-quality portrait collections. In this work, we introduce Portrait Collection Generation (PCG), a novel task that generates coherent portrait collections by editing a reference portrait image through natural language instructions. This task poses two unique challenges to existing methods: (1) complex multi-attribute modifications such as pose, spatial layout, and camera viewpoint; and (2) high-fidelity detail preservation including identity, clothing, and accessories. To address these challenges, we propose CHEESE, the first large-scale PCG dataset containing 24K portrait collections and 573K samples with high-quality modification text annotations, constructed through an Large Vison-Language Model-based pipeline with inversion-based verification. We further propose SCheese, a framework that combines text-guided generation with hierarchical identity and detail preservation. SCheese employs adaptive feature fusion mechanism to maintain identity consistency, and ConsistencyNet to inject fine-grained features for detail consistency. Comprehensive experiments validate the effectiveness of CHEESE in advancing PCG, with SCheese achieving state-of-the-art performance.

</details>


### [45] [Context Tokens are Anchors: Understanding the Repetition Curse in dMLLMs from an Information Flow Perspective](https://arxiv.org/abs/2601.20520)
*Qiyan Zhao,Xiaofeng Zhang,Shuochen Chang,Qianyu Chen,Xiaosong Yuan,Xuhang Chen,Luoqi Liu,Jiajun Zhang,Xu-Yao Zhang,Da-Han Wang*

Main category: cs.CV

TL;DR: CoTA is a plug-and-play method that mitigates repetitive text generation in diffusion-based Multimodal Large Language Models by analyzing information flow and applying attention enhancement with confidence penalty during decoding.


<details>
  <summary>Details</summary>
Motivation: Recent diffusion-based MLLMs suffer from high inference latency and rely on caching techniques, which introduce undesirable repetitive text generation (the "Repeat Curse"). The paper aims to understand the underlying mechanisms behind this repetition issue.

Method: The authors analyze repetition through information flow analysis, revealing three key findings about context tokens. Based on these insights, they propose CoTA: 1) enhances attention of context tokens to preserve information flow patterns, and 2) introduces a penalty term to confidence scores during decoding to avoid outputs driven by uncertain context tokens.

Result: CoTA demonstrates significant effectiveness in alleviating repetition and achieves consistent performance improvements on general tasks through extensive experiments.

Conclusion: The information flow analysis provides insights into the repetition problem in dMLLMs, and CoTA offers an effective plug-and-play solution that mitigates the "Repeat Curse" while maintaining or improving overall performance.

Abstract: Recent diffusion-based Multimodal Large Language Models (dMLLMs) suffer from high inference latency and therefore rely on caching techniques to accelerate decoding. However, the application of cache mechanisms often introduces undesirable repetitive text generation, a phenomenon we term the \textbf{Repeat Curse}. To better investigate underlying mechanism behind this issue, we analyze repetition generation through the lens of information flow. Our work reveals three key findings: (1) context tokens aggregate semantic information as anchors and guide the final predictions; (2) as information propagates across layers, the entropy of context tokens converges in deeper layers, reflecting the model's growing prediction certainty; (3) Repetition is typically linked to disruptions in the information flow of context tokens and to the inability of their entropy to converge in deeper layers. Based on these insights, we present \textbf{CoTA}, a plug-and-play method for mitigating repetition. CoTA enhances the attention of context tokens to preserve intrinsic information flow patterns, while introducing a penalty term to the confidence score during decoding to avoid outputs driven by uncertain context tokens. With extensive experiments, CoTA demonstrates significant effectiveness in alleviating repetition and achieves consistent performance improvements on general tasks. Code is available at https://github.com/ErikZ719/CoTA

</details>


### [46] [AnomalyVFM -- Transforming Vision Foundation Models into Zero-Shot Anomaly Detectors](https://arxiv.org/abs/2601.20524)
*Matic FuÄka,Vitjan Zavrtanik,Danijel SkoÄaj*

Main category: cs.CV

TL;DR: AnomalyVFM is a framework that transforms any pretrained vision foundation model into a strong zero-shot anomaly detector by addressing dataset diversity limitations and shallow adaptation strategies.


<details>
  <summary>Details</summary>
Motivation: Current vision foundation models (VFMs) like DINOv2 lag behind vision-language models in zero-shot anomaly detection due to limited diversity in existing auxiliary datasets and overly shallow adaptation strategies.

Method: Combines a three-stage synthetic dataset generation scheme with parameter-efficient adaptation using low-rank feature adapters and confidence-weighted pixel loss to effectively adapt VFMs for anomaly detection.

Result: With RADIO as backbone, achieves 94.1% average image-level AUROC across 9 diverse datasets, surpassing previous methods by 3.3 percentage points.

Conclusion: AnomalyVFM demonstrates that properly adapted vision foundation models can outperform current state-of-the-art methods in zero-shot anomaly detection through robust synthetic data generation and efficient adaptation mechanisms.

Abstract: Zero-shot anomaly detection aims to detect and localise abnormal regions in the image without access to any in-domain training images. While recent approaches leverage vision-language models (VLMs), such as CLIP, to transfer high-level concept knowledge, methods based on purely vision foundation models (VFMs), like DINOv2, have lagged behind in performance. We argue that this gap stems from two practical issues: (i) limited diversity in existing auxiliary anomaly detection datasets and (ii) overly shallow VFM adaptation strategies. To address both challenges, we propose AnomalyVFM, a general and effective framework that turns any pretrained VFM into a strong zero-shot anomaly detector. Our approach combines a robust three-stage synthetic dataset generation scheme with a parameter-efficient adaptation mechanism, utilising low-rank feature adapters and a confidence-weighted pixel loss. Together, these components enable modern VFMs to substantially outperform current state-of-the-art methods. More specifically, with RADIO as a backbone, AnomalyVFM achieves an average image-level AUROC of 94.1% across 9 diverse datasets, surpassing previous methods by significant 3.3 percentage points. Project Page: https://maticfuc.github.io/anomaly_vfm/

</details>


### [47] [IOTA: Corrective Knowledge-Guided Prompt Learning via Black-White Box Framework](https://arxiv.org/abs/2601.20526)
*Shaokun Wang,Yifan Yu,Yuhang He,Weili Guan,Yihong Gong*

Main category: cs.CV

TL;DR: IOTA is a black-white box prompt learning framework that combines data-driven black box optimization with knowledge-driven white box correction for better downstream task adaptation of pre-trained models.


<details>
  <summary>Details</summary>
Motivation: Previous parameter-efficient tuning methods treat pre-trained models as opaque black boxes, relying only on data-driven optimization and underutilizing their inherent prior knowledge, which limits effective downstream task adaptation.

Method: IOTA integrates a data-driven Black Box module with a knowledge-driven White Box module. The White Box module derives corrective knowledge by contrasting wrong predictions with right cognition, verbalizes this into interpretable human prompts, and uses a corrective knowledge-guided prompt selection strategy to guide the Black Box module.

Result: Experimental results on 12 image classification benchmarks under few-shot and easy-to-hard adaptation settings demonstrate the effectiveness of corrective knowledge and the superiority of IOTA over state-of-the-art methods.

Conclusion: By jointly leveraging knowledge- and data-driven learning signals, IOTA achieves effective downstream task adaptation, overcoming limitations of purely data-driven approaches.

Abstract: Recently, adapting pre-trained models to downstream tasks has attracted increasing interest. Previous Parameter-Efficient-Tuning (PET) methods regard the pre-trained model as an opaque Black Box model, relying purely on data-driven optimization and underutilizing their inherent prior knowledge. This oversight limits the models' potential for effective downstream task adaptation. To address these issues, we propose a novel black-whIte bOx prompT leArning framework (IOTA), which integrates a data-driven Black Box module with a knowledge-driven White Box module for downstream task adaptation. Specifically, the White Box module derives corrective knowledge by contrasting the wrong predictions with the right cognition. This knowledge is verbalized into interpretable human prompts and leveraged through a corrective knowledge-guided prompt selection strategy to guide the Black Box module toward more accurate predictions. By jointly leveraging knowledge- and data-driven learning signals, IOTA achieves effective downstream task adaptation. Experimental results on 12 image classification benchmarks under few-shot and easy-to-hard adaptation settings demonstrate the effectiveness of corrective knowledge and the superiority of our method over state-of-the-art methods.

</details>


### [48] [Advancing Open-source World Models](https://arxiv.org/abs/2601.20540)
*Robbyant Team,Zelin Gao,Qiuyu Wang,Yanhong Zeng,Jiapeng Zhu,Ka Leong Cheng,Yixuan Li,Hanlin Wang,Yinghao Xu,Shuailei Ma,Yihang Chen,Jie Liu,Yansong Cheng,Yao Yao,Jiayi Zhu,Yihao Meng,Kecheng Zheng,Qingyan Bai,Jingye Chen,Zehong Shen,Yue Yu,Xing Zhu,Yujun Shen,Hao Ouyang*

Main category: cs.CV

TL;DR: LingBot-World is an open-source world simulator based on video generation that offers high-fidelity dynamics across diverse environments, minute-level horizon with long-term memory, and real-time interactivity with <1 second latency.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between open-source and closed-source technologies by providing a high-quality world simulator that can empower the community for practical applications in content creation, gaming, and robot learning.

Method: Developed as a video generation-based world simulator that maintains high fidelity and robust dynamics across various environments (realism, scientific contexts, cartoon styles), incorporates long-term memory for contextual consistency, and optimizes for real-time performance.

Result: Created LingBot-World with three key features: (1) high fidelity across diverse environments, (2) minute-level horizon with long-term memory, (3) real-time interactivity with under 1-second latency at 16 FPS. The code and model are publicly released.

Conclusion: LingBot-World represents a top-tier open-source world simulator that narrows the technology gap between open and closed-source solutions, enabling practical applications across multiple domains including content creation, gaming, and robotics.

Abstract: We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as "long-term memory". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.

</details>


### [49] [DeepSeek-OCR 2: Visual Causal Flow](https://arxiv.org/abs/2601.20552)
*Haoran Wei,Yaofeng Sun,Yukun Li*

Main category: cs.CV

TL;DR: DeepSeek-OCR 2 introduces a novel encoder (DeepEncoder V2) that dynamically reorders visual tokens based on image semantics, moving beyond rigid raster-scan processing to emulate human visual perception patterns.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models process visual tokens in fixed raster-scan order with static positional encoding, which contradicts human visual perception that follows flexible, semantically-driven scanning patterns, especially for complex layouts.

Method: DeepEncoder V2 endows the encoder with causal reasoning capabilities to intelligently reorder visual tokens before LLM interpretation, exploring whether 2D image understanding can be achieved through two-cascaded 1D causal reasoning structures.

Result: The work presents a novel architectural paradigm that enables dynamic visual token reordering based on image semantics, offering a new approach to achieve genuine 2D reasoning in vision-language models.

Conclusion: This research introduces a cognitive-inspired approach to visual processing that better aligns with human perception patterns, potentially enabling more natural and effective 2D image understanding through cascaded causal reasoning structures.

Abstract: We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.

</details>


### [50] [DiffVC-RT: Towards Practical Real-Time Diffusion-based Perceptual Neural Video Compression](https://arxiv.org/abs/2601.20564)
*Wenzhuo Ma,Zhenzhong Chen*

Main category: cs.CV

TL;DR: DiffVC-RT is the first real-time diffusion-based neural video compression framework that achieves 80.1% bitrate savings over VTM-17.0 with real-time speeds of 206/30 fps for 720p videos.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based neural video compression faces three critical challenges: severe information loss, prohibitive inference latency, and poor temporal consistency, which hinder practical deployment.

Method: Three key innovations: 1) Efficient and Informative Model Architecture with strategic module replacements and pruning; 2) Explicit and Implicit Consistency Modeling with zero-cost Online Temporal Shift Module and hybrid constraints; 3) Asynchronous and Parallel Decoding Pipeline with Mixed Half Precision and Batch-dimension Temporal Shift design.

Result: Achieves 80.1% bitrate savings in LPIPS over VTM-17.0 on HEVC dataset, with real-time encoding/decoding speeds of 206/30 fps for 720p videos on NVIDIA H800 GPU.

Conclusion: DiffVC-RT represents a significant milestone in diffusion-based video compression by overcoming previous limitations and enabling real-time performance while maintaining high compression efficiency.

Abstract: The practical deployment of diffusion-based Neural Video Compression (NVC) faces critical challenges, including severe information loss, prohibitive inference latency, and poor temporal consistency. To bridge this gap, we propose DiffVC-RT, the first framework designed to achieve real-time diffusion-based perceptual NVC. First, we introduce an Efficient and Informative Model Architecture. Through strategic module replacements and pruning, this architecture significantly reduces computational complexity while mitigating structural information loss. Second, to address generative flickering artifacts, we propose Explicit and Implicit Consistency Modeling. We enhance temporal consistency by explicitly incorporating a zero-cost Online Temporal Shift Module within the U-Net, complemented by hybrid implicit consistency constraints. Finally, we present an Asynchronous and Parallel Decoding Pipeline incorporating Mixed Half Precision, which enables asynchronous latent decoding and parallel frame reconstruction via a Batch-dimension Temporal Shift design. Experiments show that DiffVC-RT achieves 80.1% bitrate savings in terms of LPIPS over VTM-17.0 on HEVC dataset with real-time encoding and decoding speeds of 206 / 30 fps for 720p videos on an NVIDIA H800 GPU, marking a significant milestone in diffusion-based video compression.

</details>


### [51] [StructAlign: Structured Cross-Modal Alignment for Continual Text-to-Video Retrieval](https://arxiv.org/abs/2601.20597)
*Shaokun Wang,Weili Guan,Jizhou Han,Jianlong Wu,Yupeng Hu,Liqiang Nie*

Main category: cs.CV

TL;DR: StructAlign addresses catastrophic forgetting in Continual Text-to-Video Retrieval by using simplex ETF geometry for cross-modal alignment and preserving cross-modal relations to mitigate feature drift.


<details>
  <summary>Details</summary>
Motivation: Continual Text-to-Video Retrieval (CTVR) suffers from catastrophic forgetting due to feature drift - both intra-modal drift within each modality and non-cooperative drift across modalities that causes modality misalignment.

Method: Proposes StructAlign with two key components: 1) Cross-modal ETF alignment using simplex Equiangular Tight Frame geometry as unified prior to align text/video features with category prototypes, 2) Cross-modal Relation Preserving loss that leverages complementary modalities to preserve similarity relations and suppress intra-modal drift.

Result: Extensive experiments on benchmark datasets show StructAlign consistently outperforms state-of-the-art continual retrieval approaches.

Conclusion: By jointly addressing non-cooperative feature drift across modalities and intra-modal feature drift, StructAlign effectively alleviates catastrophic forgetting in CTVR through structured cross-modal alignment.

Abstract: Continual Text-to-Video Retrieval (CTVR) is a challenging multimodal continual learning setting, where models must incrementally learn new semantic categories while maintaining accurate text-video alignment for previously learned ones, thus making it particularly prone to catastrophic forgetting. A key challenge in CTVR is feature drift, which manifests in two forms: intra-modal feature drift caused by continual learning within each modality, and non-cooperative feature drift across modalities that leads to modality misalignment. To mitigate these issues, we propose StructAlign, a structured cross-modal alignment method for CTVR. First, StructAlign introduces a simplex Equiangular Tight Frame (ETF) geometry as a unified geometric prior to mitigate modality misalignment. Building upon this geometric prior, we design a cross-modal ETF alignment loss that aligns text and video features with category-level ETF prototypes, encouraging the learned representations to form an approximate simplex ETF geometry. In addition, to suppress intra-modal feature drift, we design a Cross-modal Relation Preserving loss, which leverages complementary modalities to preserve cross-modal similarity relations, providing stable relational supervision for feature updates. By jointly addressing non-cooperative feature drift across modalities and intra-modal feature drift, StructAlign effectively alleviates catastrophic forgetting in CTVR. Extensive experiments on benchmark datasets demonstrate that our method consistently outperforms state-of-the-art continual retrieval approaches.

</details>


### [52] [Person Re-ID in 2025: Supervised, Self-Supervised, and Language-Aligned. What Works?](https://arxiv.org/abs/2601.20598)
*Lakshman Balasubramanian*

Main category: cs.CV

TL;DR: This paper analyzes Person ReID training paradigms, evaluating supervised, self-supervised, and language-aligned models across 11 models and 9 datasets to assess cross-domain generalization and foundation model performance.


<details>
  <summary>Details</summary>
Motivation: Person ReID faces challenges in cross-domain applications, and there's a need to understand how different training paradigms (supervised, self-supervised, language-aligned) perform in generalization, particularly examining the role of foundation models in improving transferable representations.

Method: The study compares three training paradigms across 11 models and 9 datasets, evaluating supervised models, self-supervised models, and language-aligned foundation models like SigLIP2 for cross-domain ReID performance.

Result: Supervised models excel in their training domain but fail in cross-domain scenarios. Language-aligned models show surprising cross-domain robustness for ReID tasks despite not being explicitly trained for them.

Conclusion: Language-aligned foundation models demonstrate strong cross-domain generalization capabilities for ReID, suggesting they provide richer, more transferable visual representations compared to traditional supervised approaches.

Abstract: Person Re-Identification (ReID) remains a challenging problem in computer vision. This work reviews various training paradigm and evaluates the robustness of state-of-the-art ReID models in cross-domain applications and examines the role of foundation models in improving generalization through richer, more transferable visual representations. We compare three training paradigms, supervised, self-supervised, and language-aligned models. Through the study the aim is to answer the following questions: Can supervised models generalize in cross-domain scenarios? How does foundation models like SigLIP2 perform for the ReID tasks? What are the weaknesses of current supervised and foundational models for ReID? We have conducted the analysis across 11 models and 9 datasets. Our results show a clear split: supervised models dominate their training domain but crumble on cross-domain data. Language-aligned models, however, show surprising robustness cross-domain for ReID tasks, even though they are not explicitly trained to do so. Code and data available at: https://github.com/moiiai-tech/object-reid-benchmark.

</details>


### [53] [CLEAR-Mamba:Towards Accurate, Adaptive and Trustworthy Multi-Sequence Ophthalmic Angiography Classification](https://arxiv.org/abs/2601.20601)
*Zhuonan Wang,Wenjie Yan,Wenqiao Zhang,Xiaohui Song,Jian Ma,Ke Yao,Yibo Yu,Beng Chin Ooi*

Main category: cs.CV

TL;DR: CLEAR-Mamba enhances MedMamba with hypernetwork-based adaptive conditioning and reliability-aware prediction for improved ophthalmic angiography classification across FFA/ICGA modalities.


<details>
  <summary>Details</summary>
Motivation: Medical image classification for CAD faces limitations in ophthalmic angiography due to single-modality nature, subtle lesion patterns, and inter-device variability affecting generalization and high-confidence prediction.

Method: Proposes CLEAR-Mamba with two key innovations: 1) HaC - hypernetwork-based adaptive conditioning layer for dynamic parameter generation based on input features, improving cross-domain adaptability; 2) RaP - reliability-aware prediction scheme using evidential uncertainty learning to emphasize low-confidence samples and improve stability.

Result: CLEAR-Mamba consistently outperforms multiple baselines including original MedMamba across various metrics, showing particular advantages in multi-disease classification and reliability-aware prediction on a large-scale ophthalmic angiography dataset.

Conclusion: Provides an effective solution balancing generalizability and reliability for modality-specific medical image classification tasks, particularly in ophthalmic angiography with FFA/ICGA modalities.

Abstract: Medical image classification is a core task in computer-aided diagnosis (CAD), playing a pivotal role in early disease detection, treatment planning, and patient prognosis assessment. In ophthalmic practice, fluorescein fundus angiography (FFA) and indocyanine green angiography (ICGA) provide hemodynamic and lesion-structural information that conventional fundus photography cannot capture. However, due to the single-modality nature, subtle lesion patterns, and significant inter-device variability, existing methods still face limitations in generalization and high-confidence prediction. To address these challenges, we propose CLEAR-Mamba, an enhanced framework built upon MedMamba with optimizations in both architecture and training strategy. Architecturally, we introduce HaC, a hypernetwork-based adaptive conditioning layer that dynamically generates parameters according to input feature distributions, thereby improving cross-domain adaptability. From a training perspective, we develop RaP, a reliability-aware prediction scheme built upon evidential uncertainty learning, which encourages the model to emphasize low-confidence samples and improves overall stability and reliability. We further construct a large-scale ophthalmic angiography dataset covering both FFA and ICGA modalities, comprising multiple retinal disease categories for model training and evaluation. Experimental results demonstrate that CLEAR-Mamba consistently outperforms multiple baseline models, including the original MedMamba, across various metrics-showing particular advantages in multi-disease classification and reliability-aware prediction. This study provides an effective solution that balances generalizability and reliability for modality-specific medical image classification tasks.

</details>


### [54] [GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection](https://arxiv.org/abs/2601.20618)
*Shuguang Zhang,Junhong Lian,Guoxin Yu,Baoxun Xu,Xiang Ao*

Main category: cs.CV

TL;DR: GDCNet uses MLLM-generated image captions as stable semantic anchors to detect sarcasm by comparing semantic/sentiment discrepancies with original text and measuring visual-textual fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing MSD methods struggle with loosely related image-text pairs and noisy sarcastic cues from LLMs, needing more stable semantic anchors for cross-modal conflict detection.

Method: Generative Discrepancy Comparison Network (GDCNet) uses MLLM-generated objective image captions as anchors, computes semantic/sentiment discrepancies with original text, measures visual-textual fidelity, and fuses features via gated module.

Result: GDCNet achieves superior accuracy and robustness on MSD benchmarks, establishing new state-of-the-art on MMSD2.0 benchmark.

Conclusion: Using factually grounded MLLM-generated captions as semantic anchors effectively addresses cross-modal conflict detection challenges in sarcasm detection, outperforming existing methods.

Abstract: Multimodal sarcasm detection (MSD) aims to identify sarcasm within image-text pairs by modeling semantic incongruities across modalities. Existing methods often exploit cross-modal embedding misalignment to detect inconsistency but struggle when visual and textual content are loosely related or semantically indirect. While recent approaches leverage large language models (LLMs) to generate sarcastic cues, the inherent diversity and subjectivity of these generations often introduce noise. To address these limitations, we propose the Generative Discrepancy Comparison Network (GDCNet). This framework captures cross-modal conflicts by utilizing descriptive, factually grounded image captions generated by Multimodal LLMs (MLLMs) as stable semantic anchors. Specifically, GDCNet computes semantic and sentiment discrepancies between the generated objective description and the original text, alongside measuring visual-textual fidelity. These discrepancy features are then fused with visual and textual representations via a gated module to adaptively balance modality contributions. Extensive experiments on MSD benchmarks demonstrate GDCNet's superior accuracy and robustness, establishing a new state-of-the-art on the MMSD2.0 benchmark.

</details>


### [55] [OS-Marathon: Benchmarking Computer-Use Agents on Long-Horizon Repetitive Tasks](https://arxiv.org/abs/2601.20650)
*Jing Wu,Daphne Barretto,Yiye Chen,Nicholas GydÃ©,Yanan Jian,Yuhang He,Vibhav Vineet*

Main category: cs.CV

TL;DR: OS-Marathon benchmark with 242 long-horizon repetitive tasks for evaluating computer-use agents, plus a few-shot demonstration method for teaching workflow logic.


<details>
  <summary>Details</summary>
Motivation: Long-horizon repetitive workflows are tedious for humans but ideal for computer-use agents, yet there's no proper evaluation benchmark for such tasks.

Method: Created OS-Marathon benchmark with 242 tasks across 2 domains, and introduced a cost-effective few-shot demonstration method to teach workflow logic to agents.

Result: Extensive experiments show both the inherent challenges of long-horizon repetitive tasks and the effectiveness of the proposed few-shot demonstration method.

Conclusion: OS-Marathon addresses the benchmark bottleneck for evaluating computer-use agents on repetitive workflows, and the few-shot demonstration method enables effective workflow learning.

Abstract: Long-horizon, repetitive workflows are common in professional settings, such as processing expense reports from receipts and entering student grades from exam papers. These tasks are often tedious for humans since they can extend to extreme lengths proportional to the size of the data to process. However, they are ideal for Computer-Use Agents (CUAs) due to their structured, recurring sub-workflows with logic that can be systematically learned. Identifying the absence of an evaluation benchmark as a primary bottleneck, we establish OS-Marathon, comprising 242 long-horizon, repetitive tasks across 2 domains to evaluate state-of-the-art (SOTA) agents. We then introduce a cost-effective method to construct a condensed demonstration using only few-shot examples to teach agents the underlying workflow logic, enabling them to execute similar workflows effectively on larger, unseen data collections. Extensive experiments demonstrate both the inherent challenges of these tasks and the effectiveness of our proposed method. Project website: https://os-marathon.github.io/.

</details>


### [56] [FD-MAD: Frequency-Domain Residual Analysis for Face Morphing Attack Detection](https://arxiv.org/abs/2601.20656)
*Diogo J. Paulo,Hugo ProenÃ§a,JoÃ£o C. Neves*

Main category: cs.CV

TL;DR: Region-aware frequency-based morph detection using residual frequency domain and Markov Random Field for cross-dataset S-MAD, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Face morphing attacks threaten face recognition systems in border control and identity verification, especially in single-image scenarios without trusted references. Current morph detection systems struggle with cross-dataset generalization.

Method: 1) Introduces residual frequency domain to decouple signal frequency from natural spectral decay for easier discrimination; 2) Combines evidence from different facial regions using Markov Random Field for globally consistent decisions; 3) Uses only spectral features with lightweight approach.

Result: Achieves 1.85% average EER on FRLL-Morph and ranks second on MAD22 with 6.12% average EER. Shows good BPCER at low APCER using only spectral features, trained exclusively on SMDD dataset.

Conclusion: Fourier-domain residual modeling with structured regional fusion offers competitive alternative to deep S-MAD architectures, improving cross-dataset and cross-morph generalization.

Abstract: Face morphing attacks present a significant threat to face recognition systems used in electronic identity enrolment and border control, particularly in single-image morphing attack detection (S-MAD) scenarios where no trusted reference is available. In spite of the vast amount of research on this problem, morph detection systems struggle in cross-dataset scenarios. To address this problem, we introduce a region-aware frequency-based morph detection strategy that drastically improves over strong baseline methods in challenging cross-dataset and cross-morph settings using a lightweight approach. Having observed the separability of bona fide and morph samples in the frequency domain of different facial parts, our approach 1) introduces the concept of residual frequency domain, where the frequency of the signal is decoupled from the natural spectral decay to easily discriminate between morph and bona fide data; 2) additionally, we reason in a global and local manner by combining the evidence from different facial regions in a Markov Random Field, which infers a globally consistent decision. The proposed method, trained exclusively on the synthetic morphing attack detection development dataset (SMDD), is evaluated in challenging cross-dataset and cross-morph settings on FRLL-Morph and MAD22 sets. Our approach achieves an average equal error rate (EER) of 1.85\% on FRLL-Morph and ranks second on MAD22 with an average EER of 6.12\%, while also obtaining a good bona fide presentation classification error rate (BPCER) at a low attack presentation classification error rate (APCER) using only spectral features. These findings indicate that Fourier-domain residual modeling with structured regional fusion offers a competitive alternative to deep S-MAD architectures.

</details>


### [57] [ProSkill: Segment-Level Skill Assessment in Procedural Videos](https://arxiv.org/abs/2601.20661)
*Michele Mazzamuto,Daniele Di Mauro,Gianpiero Francesca,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: ProSkill is the first benchmark dataset for action-level skill assessment in procedural tasks, featuring both absolute and pairwise skill annotations using a novel Swiss Tournament + ELO-based annotation protocol.


<details>
  <summary>Details</summary>
Motivation: Current skill assessment research lacks large-scale datasets for complex procedural activities, focuses mainly on sports, and is limited to either pairwise comparisons or binary labels rather than absolute skill assessment.

Method: Introduces ProSkill dataset with a novel annotation protocol: uses Swiss Tournament scheme for efficient pairwise comparisons, then aggregates them into consistent continuous global scores using an ELO-based rating system.

Result: Benchmarked state-of-the-art skill assessment algorithms on ProSkill, showing suboptimal results that highlight the dataset's challenges and value for advancing procedural video skill assessment.

Conclusion: ProSkill addresses critical gaps in procedural skill assessment research and provides a valuable benchmark for developing more sophisticated skill assessment algorithms, with all data and code publicly available.

Abstract: Skill assessment in procedural videos is crucial for the objective evaluation of human performance in settings such as manufacturing and procedural daily tasks. Current research on skill assessment has predominantly focused on sports and lacks large-scale datasets for complex procedural activities. Existing studies typically involve only a limited number of actions, focus on either pairwise assessments (e.g., A is better than B) or on binary labels (e.g., good execution vs needs improvement). In response to these shortcomings, we introduce ProSkill, the first benchmark dataset for action-level skill assessment in procedural tasks. ProSkill provides absolute skill assessment annotations, along with pairwise ones. This is enabled by a novel and scalable annotation protocol that allows for the creation of an absolute skill assessment ranking starting from pairwise assessments. This protocol leverages a Swiss Tournament scheme for efficient pairwise comparisons, which are then aggregated into consistent, continuous global scores using an ELO-based rating system. We use our dataset to benchmark the main state-of-the-art skill assessment algorithms, including both ranking-based and pairwise paradigms. The suboptimal results achieved by the current state-of-the-art highlight the challenges and thus the value of ProSkill in the context of skill assessment for procedural videos. All data and code are available at https://fpv-iplab.github.io/ProSkill/

</details>


### [58] [bi-modal textual prompt learning for vision-language models in remote sensing](https://arxiv.org/abs/2601.20675)
*Pankhi Kashyap,Mainak Singha,Biplab Banerjee*

Main category: cs.CV

TL;DR: BiMoRS is a lightweight bi-modal prompt learning framework for remote sensing tasks that uses image captions and visual features to generate contextualized prompts without modifying the CLIP backbone.


<details>
  <summary>Details</summary>
Motivation: Prompt learning works well for natural images but struggles with remote sensing data due to multi-label scenes, high intra-class variability, and diverse spatial resolutions. Existing methods fail to identify dominant semantic cues and generalize poorly to novel classes in RS scenarios.

Method: Uses frozen image captioning model (BLIP-2) to extract textual semantic summaries from RS images, tokenizes captions with BERT, fuses with CLIP visual features, and employs lightweight cross-attention to condition learnable query prompts on fused textual-visual representation.

Result: Outperforms strong baselines by up to 2% on average across four RS datasets and three domain generalization tasks.

Conclusion: BiMoRS effectively adapts prompt learning to remote sensing challenges, demonstrating consistent performance gains while maintaining a lightweight architecture that doesn't modify the CLIP backbone.

Abstract: Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.

</details>


### [59] [Decoupling Perception and Calibration: Label-Efficient Image Quality Assessment Framework](https://arxiv.org/abs/2601.20689)
*Xinyue Li,Zhichao Zhang,Zhiming Xu,Shubo Xu,Xiongkuo Min,Yitong Chen,Guangtao Zhai*

Main category: cs.CV

TL;DR: LEAF is a label-efficient IQA framework that distills quality perception from MLLMs into lightweight regressors, reducing human annotation needs while maintaining strong performance.


<details>
  <summary>Details</summary>
Motivation: Current MLLM-based IQA methods are computationally expensive and require substantial MOS annotations. The core bottleneck is not MLLMs' quality perception capacity but MOS scale calibration.

Method: Distills perceptual quality priors from MLLM teacher to lightweight student regressor. Teacher provides dense supervision through point-wise judgments and pair-wise preferences with reliability estimates. Student learns teacher's patterns through joint distillation and calibrates on small MOS subset.

Result: Significantly reduces need for human annotations while maintaining strong MOS-aligned correlations on both user-generated and AI-generated IQA benchmarks.

Conclusion: LEAF enables lightweight IQA practical under limited annotation budgets by efficiently leveraging MLLM quality perception with minimal human supervision.

Abstract: Recent multimodal large language models (MLLMs) have demonstrated strong capabilities in image quality assessment (IQA) tasks. However, adapting such large-scale models is computationally expensive and still relies on substantial Mean Opinion Score (MOS) annotations. We argue that for MLLM-based IQA, the core bottleneck lies not in the quality perception capacity of MLLMs, but in MOS scale calibration. Therefore, we propose LEAF, a Label-Efficient Image Quality Assessment Framework that distills perceptual quality priors from an MLLM teacher into a lightweight student regressor, enabling MOS calibration with minimal human supervision. Specifically, the teacher conducts dense supervision through point-wise judgments and pair-wise preferences, with an estimate of decision reliability. Guided by these signals, the student learns the teacher's quality perception patterns through joint distillation and is calibrated on a small MOS subset to align with human annotations. Experiments on both user-generated and AI-generated IQA benchmarks demonstrate that our method significantly reduces the need for human annotations while maintaining strong MOS-aligned correlations, making lightweight IQA practical under limited annotation budgets.

</details>


### [60] [LEMON: How Well Do MLLMs Perform Temporal Multimodal Understanding on Instructional Videos?](https://arxiv.org/abs/2601.20705)
*Zhuang Yu,Lei Shen,Jing Zhao,Shiliang Sun*

Main category: cs.CV

TL;DR: LEMON is a new benchmark for evaluating multimodal LLMs on STEM lecture videos, featuring long-form, knowledge-intensive content with temporal structure and cross-modal integration.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs show strong performance on general multimodal tasks but their capabilities on long-form educational content with temporal structure remain unexplored. There's a need for benchmarks that test reasoning on instructional materials.

Method: Created LEMON benchmark with 2,277 STEM lecture video segments (avg 196.1s) across 5 disciplines and 29 courses, yielding 4,181 QA pairs (3,413 multiple-choice, 768 open-ended). Features semantic richness, tightly coupled modalities, explicit temporal/pedagogical structure, and contextually linked multi-turn questions.

Result: Comprehensive experiments show substantial performance gaps across tasks. Even state-of-the-art MLLMs like GPT-4o struggle with temporal reasoning and instructional prediction tasks.

Conclusion: LEMON serves as an extensible, challenging benchmark for advancing multimodal perception, reasoning, and generation in long-form instructional content, highlighting current limitations and future research directions.

Abstract: Recent multimodal large language models (MLLMs) have shown remarkable progress across vision, audio, and language tasks, yet their performance on long-form, knowledge-intensive, and temporally structured educational content remains largely unexplored. To bridge this gap, we introduce LEMON, a Lecture-based Evaluation benchmark for MultimOdal uNderstanding, focusing on STEM lecture videos that require long-horizon reasoning and cross-modal integration. LEMON comprises 2,277 video segments spanning 5 disciplines and 29 courses, with an average duration of 196.1 seconds, yielding 4,181 high-quality QA pairs, including 3,413 multiple-choice and 768 open-ended questions. Distinct from existing video benchmarks, LEMON features: (1) semantic richness and disciplinary density, (2) tightly coupled video-audio-text modalities, (3) explicit temporal and pedagogical structure, and (4) contextually linked multi-turn questioning. It further encompasses six major tasks and twelve subtasks, covering the full cognitive spectrum from perception to reasoning and then to generation. Comprehensive experiments reveal substantial performance gaps across tasks, highlighting that even state-of-the-art MLLMs like GPT-4o struggle with temporal reasoning and instructional prediction. We expect LEMON to serve as an extensible and challenging benchmark for advancing multimodal perception, reasoning, and generation in long-form instructional contents.

</details>


### [61] [Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification](https://arxiv.org/abs/2601.20742)
*Xin Jin,Jinming Liu,Yuntao Wei,Junyan Lin,Zhicheng Wang,Jianguo Huang,Xudong Yang,Yanxiao Liu,Wenjun Zeng*

Main category: cs.CV

TL;DR: This paper explores the connection between compression efficiency and AI model performance, unifying traditional visual coding with modern visual token technology to optimize semantic information fidelity while minimizing computational cost.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the observation that compression efficiency correlates with improved AI model performance, particularly in multimodal LLMs. Both classical visual coding (like H.264/265) and emerging visual token technology share similar fundamental objectives of maximizing semantic information fidelity while minimizing computational cost, suggesting a deeper connection worth exploring.

Method: The paper provides a comprehensive overview of two dominant technique families: Visual Coding and Vision Token Technology. It then unifies them from an optimization perspective, discussing the trade-off between compression efficiency and model performance. Based on this unified formulation, the authors synthesize bidirectional insights and forecast next-generation techniques.

Result: The paper experimentally demonstrates the large potential of task-oriented token developments in practical applications like multimodal LLMs, AI-generated content, and embodied AI. It shows that unifying these approaches can lead to more efficient and effective solutions for intelligent tasks.

Conclusion: The paper concludes that there is significant potential for standardizing a general token technology similar to traditional codecs (like H.264/265) that could provide high efficiency for a wide range of intelligent tasks in a unified manner, bridging the gap between classical compression techniques and modern AI tokenization approaches.

Abstract: "Compression Tells Intelligence", is supported by research in artificial intelligence, particularly concerning (multimodal) large language models (LLMs/MLLMs), where compression efficiency often correlates with improved model performance and capabilities. For compression, classical visual coding based on traditional information theory has developed over decades, achieving great success with numerous international industrial standards widely applied in multimedia (e.g., image/video) systems. Except that, the recent emergingvisual token technology of generative multi-modal large models also shares a similar fundamental objective like visual coding: maximizing semantic information fidelity during the representation learning while minimizing computational cost. Therefore, this paper provides a comprehensive overview of two dominant technique families first -- Visual Coding and Vision Token Technology -- then we further unify them from the aspect of optimization, discussing the essence of compression efficiency and model performance trade-off behind. Next, based on the proposed unified formulation bridging visual coding andvisual token technology, we synthesize bidirectional insights of themselves and forecast the next-gen visual codec and token techniques. Last but not least, we experimentally show a large potential of the task-oriented token developments in the more practical tasks like multimodal LLMs (MLLMs), AI-generated content (AIGC), and embodied AI, as well as shedding light on the future possibility of standardizing a general token technology like the traditional codecs (e.g., H.264/265) with high efficiency for a wide range of intelligent tasks in a unified and effective manner.

</details>


### [62] [FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models](https://arxiv.org/abs/2601.20791)
*Haonan Zhong,Wei Song,Tingxu Han,Maurice Pagnucco,Jingling Xue,Yang Song*

Main category: cs.CV

TL;DR: FairT2V is a training-free framework that mitigates gender bias in text-to-video generation by neutralizing prompt embeddings through spherical geodesic transformations, applied only during early denoising steps to maintain temporal coherence.


<details>
  <summary>Details</summary>
Motivation: Text-to-video diffusion models have made rapid progress but their demographic biases, particularly gender bias, remain largely unexplored. The authors identify that this bias primarily originates from pretrained text encoders that encode implicit gender associations even for neutral prompts.

Method: FairT2V uses anchor-based spherical geodesic transformations to neutralize prompt embeddings while preserving semantics. To maintain temporal coherence, debiasing is applied only during early identity-forming steps through a dynamic denoising schedule. The framework is training-free and doesn't require finetuning.

Result: Experiments on the modern T2V model Open-Sora show that FairT2V substantially reduces demographic bias across occupations with minimal impact on video quality. The authors also propose a video-level fairness evaluation protocol combining VideoLLM-based reasoning with human verification.

Conclusion: FairT2V effectively addresses demographic bias in text-to-video generation by targeting encoder-induced bias through training-free prompt embedding neutralization, offering a practical solution that maintains video quality while improving fairness.

Abstract: Text-to-video (T2V) diffusion models have achieved rapid progress, yet their demographic biases, particularly gender bias, remain largely unexplored. We present FairT2V, a training-free debiasing framework for text-to-video generation that mitigates encoder-induced bias without finetuning. We first analyze demographic bias in T2V models and show that it primarily originates from pretrained text encoders, which encode implicit gender associations even for neutral prompts. We quantify this effect with a gender-leaning score that correlates with bias in generated videos.
  Based on this insight, FairT2V mitigates demographic bias by neutralizing prompt embeddings via anchor-based spherical geodesic transformations while preserving semantics. To maintain temporal coherence, we apply debiasing only during early identity-forming steps through a dynamic denoising schedule. We further propose a video-level fairness evaluation protocol combining VideoLLM-based reasoning with human verification. Experiments on the modern T2V model Open-Sora show that FairT2V substantially reduces demographic bias across occupations with minimal impact on video quality.

</details>


### [63] [Open-Vocabulary Functional 3D Human-Scene Interaction Generation](https://arxiv.org/abs/2601.20835)
*Jie Liu,Yu Sun,Alpar Cseke,Yao Feng,Nicolas Heron,Michael J. Black,Yan Zhang*

Main category: cs.CV

TL;DR: FunHSI: A training-free framework for generating functionally correct 3D human-scene interactions from open-vocabulary task prompts by reasoning about object functionality and human-scene contact.


<details>
  <summary>Details</summary>
Motivation: Existing methods for generating 3D human-scene interactions lack explicit reasoning about object functionality and corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. There's a need for a framework that can generate functionally correct human interactions with 3D scenes for applications in embodied AI, robotics, and interactive content creation.

Method: FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. It uses vision-language models to synthesize humans performing tasks in images and estimate proposed 3D body/hand poses. The proposed 3D body configuration is then refined via stage-wise optimization for physical plausibility and functional correctness.

Result: FunHSI generates more plausible general 3D interactions (like "sitting on a sofa") and supports fine-grained functional interactions (like "increasing the room temperature"). Extensive experiments show it consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes.

Conclusion: FunHSI addresses the key challenge of generating functionally correct 3D human-scene interactions by explicitly reasoning about object functionality and human-scene contact, outperforming existing methods while being training-free and supporting open-vocabulary task prompts.

Abstract: Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as "sitting on a sofa'', while supporting fine-grained functional human-scene interactions, e.g., "increasing the room temperature''. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes.

</details>


### [64] [A New Dataset and Framework for Robust Road Surface Classification via Camera-IMU Fusion](https://arxiv.org/abs/2601.20847)
*Willams de Lima Costa,Thifany Ketuli Silva de Souza,Jonas Ferreira Silva,Carlos Gabriel Bezerra Pereira,Bruno Reis Vila Nova,Leonardo Silvino Brito,Rafael Raider Leoni,Juliano Silva,Valter Ferreira,Sibele Miguel Soares Neto,Samantha Uehara,Daniel Giacomo,JoÃ£o Marcelo Teixeira,Veronica Teichrieb,Cristiano Coelho de AraÃºjo*

Main category: cs.CV

TL;DR: Multimodal road surface classification framework fusing images and IMU data with cross-attention and adaptive gating, plus new ROAD dataset with real, vision-only, and synthetic subsets for better generalization.


<details>
  <summary>Details</summary>
Motivation: Existing road surface classification techniques fail to generalize beyond narrow operational conditions due to limited sensing modalities and datasets lacking environmental diversity.

Method: Multimodal framework that fuses images and inertial measurements using a lightweight bidirectional cross-attention module followed by an adaptive gating layer that adjusts modality contributions under domain shifts. Introduces ROAD dataset with three subsets: real-world multimodal recordings, vision-only subset, and synthetic subset.

Result: Achieves +1.4 pp improvement over previous SOTA on PVS benchmark and +11.6 pp improvement on multimodal ROAD subset, with consistently higher F1-scores on minority classes. Stable performance across challenging visual conditions including nighttime, heavy rain, and mixed-surface transitions.

Conclusion: Combining affordable camera and IMU sensors with multimodal attention mechanisms provides a scalable, robust foundation for road surface understanding, particularly relevant for regions with environmental variability and cost constraints.

Abstract: Road surface classification (RSC) is a key enabler for environment-aware predictive maintenance systems. However, existing RSC techniques often fail to generalize beyond narrow operational conditions due to limited sensing modalities and datasets that lack environmental diversity. This work addresses these limitations by introducing a multimodal framework that fuses images and inertial measurements using a lightweight bidirectional cross-attention module followed by an adaptive gating layer that adjusts modality contributions under domain shifts. Given the limitations of current benchmarks, especially regarding lack of variability, we introduce ROAD, a new dataset composed of three complementary subsets: (i) real-world multimodal recordings with RGB-IMU streams synchronized using a gold-standard industry datalogger, captured across diverse lighting, weather, and surface conditions; (ii) a large vision-only subset designed to assess robustness under adverse illumination and heterogeneous capture setups; and (iii) a synthetic subset generated to study out-of-distribution generalization in scenarios difficult to obtain in practice. Experiments show that our method achieves a +1.4 pp improvement over the previous state-of-the-art on the PVS benchmark and an +11.6 pp improvement on our multimodal ROAD subset, with consistently higher F1-scores on minority classes. The framework also demonstrates stable performance across challenging visual conditions, including nighttime, heavy rain, and mixed-surface transitions. These findings indicate that combining affordable camera and IMU sensors with multimodal attention mechanisms provides a scalable, robust foundation for road surface understanding, particularly relevant for regions where environmental variability and cost constraints limit the adoption of high-end sensing suites.

</details>


### [65] [FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models](https://arxiv.org/abs/2601.20857)
*Hongyu Zhou,Zisen Shao,Sheng Miao,Pan Wang,Dongfeng Bai,Bingbing Liu,Yiyi Liao*

Main category: cs.CV

TL;DR: FreeFix is a fine-tuning-free approach that uses pretrained image diffusion models to enhance extrapolated 3D rendering quality while maintaining generalization, avoiding the trade-off between fidelity and generalization faced by existing methods.


<details>
  <summary>Details</summary>
Motivation: Current neural rendering methods (NeRF, 3D Gaussian Splatting) degrade at extrapolated views and rely on dense inputs. Existing generative model approaches face a trade-off: fine-tuning improves fidelity but risks overfitting, while fine-tuning-free methods preserve generalization but yield lower fidelity.

Method: FreeFix uses an interleaved 2D-3D refinement strategy with pretrained image diffusion models (avoiding costly video diffusion models). It introduces a per-pixel confidence mask to identify uncertain regions for targeted improvement, enabling consistent refinement without fine-tuning.

Result: Experiments across multiple datasets show FreeFix improves multi-frame consistency and achieves performance comparable to or surpassing fine-tuning-based methods, while retaining strong generalization ability.

Conclusion: FreeFix successfully pushes the boundary of the trade-off between generalization and fidelity in novel view synthesis, demonstrating that image diffusion models can be effectively leveraged for consistent 3D refinement without fine-tuning.

Abstract: Neural Radiance Fields and 3D Gaussian Splatting have advanced novel view synthesis, yet still rely on dense inputs and often degrade at extrapolated views. Recent approaches leverage generative models, such as diffusion models, to provide additional supervision, but face a trade-off between generalization and fidelity: fine-tuning diffusion models for artifact removal improves fidelity but risks overfitting, while fine-tuning-free methods preserve generalization but often yield lower fidelity. We introduce FreeFix, a fine-tuning-free approach that pushes the boundary of this trade-off by enhancing extrapolated rendering with pretrained image diffusion models. We present an interleaved 2D-3D refinement strategy, showing that image diffusion models can be leveraged for consistent refinement without relying on costly video diffusion models. Furthermore, we take a closer look at the guidance signal for 2D refinement and propose a per-pixel confidence mask to identify uncertain regions for targeted improvement. Experiments across multiple datasets show that FreeFix improves multi-frame consistency and achieves performance comparable to or surpassing fine-tuning-based methods, while retaining strong generalization ability.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [66] [NeuroAI and Beyond](https://arxiv.org/abs/2601.19955)
*Jean-Marc Fellous,Gert Cauwenberghs,Cornelia FermÃ¼ller,Yulia Sandamisrkaya,Terrence Sejnowski*

Main category: cs.AI

TL;DR: Workshop identifies synergies between neuroscience and AI across embodiment, language, robotics, learning, and neuromorphic engineering, advocating for NeuroAI to improve AI algorithms and understand biological neural computation.


<details>
  <summary>Details</summary>
Motivation: Neuroscience and AI have progressed significantly but remain loosely connected. The workshop aims to identify current and future synergies between these fields to advance both disciplines through cross-pollination.

Method: Based on an August 2025 workshop, the paper analyzes synergies in key subareas: embodiment, language and communication, robotics, learning in humans and machines, and neuromorphic engineering. Includes personal statements from leading researchers and SWOT analyses by researchers and trainees.

Result: Identifies promising avenues for NeuroAI development across multiple domains. Provides diverse expert perspectives on NeuroAI's potential and includes SWOT analyses that outline both benefits and risks of this interdisciplinary approach.

Conclusion: Advocates for NeuroAI - neuroscience-informed AI that could significantly improve AI algorithm scope/efficiency while advancing understanding of biological neural computation. Calls for stronger integration between neuroscience and AI fields.

Abstract: Neuroscience and Artificial Intelligence (AI) have made significant progress in the past few years but have only been loosely inter-connected. Based on a workshop held in August 2025, we identify current and future areas of synergism between these two fields. We focus on the subareas of embodiment, language and communication, robotics, learning in humans and machines and Neuromorphic engineering to take stock of the progress made so far, and possible promising new future avenues. Overall, we advocate for the development of NeuroAI, a type of Neuroscience-informed Artificial Intelligence that, we argue, has the potential for significantly improving the scope and efficiency of AI algorithms while simultaneously changing the way we understand biological neural computations. We include personal statements from several leading researchers on their diverse views of NeuroAI. Two Strength-Weakness-Opportunities-Threat (SWOT) analyses by researchers and trainees are appended that describe the benefits and risks offered by NeuroAI.

</details>


### [67] [Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning](https://arxiv.org/abs/2601.20014)
*Shuhui Qu*

Main category: cs.AI

TL;DR: SQ-BCP is a planning method for LLMs that handles partial observability by tracking precondition status and resolving unknowns through self-queries or bridging hypotheses, with formal guarantees on plan correctness.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with inference-time planning under partial observability - when critical preconditions aren't specified, they hallucinate missing facts or produce constraint-violating plans.

Method: SQ-BCP explicitly tracks precondition status (Sat/Viol/Unk), resolves unknowns via targeted self-queries to users/oracles or bridging hypotheses, performs bidirectional search, uses pullback-based verifier as categorical certificate, and employs distance-based scores only for ranking/pruning.

Result: On WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to 14.9% and 5.8% (vs. 26.0% and 15.7% for best baseline) while maintaining competitive reference quality.

Conclusion: SQ-BCP provides formal guarantees of plan compatibility with goal requirements under certain conditions, significantly reduces constraint violations in partially observable settings, and offers a principled approach to handling missing preconditions in LLM planning.

Abstract: Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\texttt{Sat}/\texttt{Viol}/\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \textbf{14.9\%} and \textbf{5.8\%} (vs.\ \textbf{26.0\%} and \textbf{15.7\%} for the best baseline), while maintaining competitive reference quality.

</details>


### [68] [Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints](https://arxiv.org/abs/2601.20021)
*Shuhui Qu*

Main category: cs.AI

TL;DR: Fuzzy Category-theoretic Planning (FCP) extends category-theoretic planners with fuzzy logic to handle graded applicability of vague predicates in natural-language planning, using LLM-based grounding and t-norm composition.


<details>
  <summary>Details</summary>
Motivation: Existing category-theoretic planners treat action applicability as crisp (binary), forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans when dealing with vague natural-language predicates like "suitable substitute" or "stable enough."

Method: FCP annotates each action (morphism) with a degree in [0,1], composes plan quality via Lukasiewicz t-norm, retains crisp executability checks via pullback verification, grounds graded applicability from language using LLM with k-sample median aggregation, and supports meeting-in-the-middle search using residuum-based backward requirements.

Result: FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs (missing-substitute recipe-planning benchmark) compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners on public PDDL3 preference/oversubscription benchmarks.

Conclusion: FCP successfully bridges the gap between crisp category-theoretic planning and fuzzy natural-language reasoning, enabling better handling of graded applicability in planning with vague predicates while maintaining formal verification capabilities.

Abstract: Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners.

</details>


### [69] [Insight Agents: An LLM-Based Multi-Agent System for Data Insights](https://arxiv.org/abs/2601.20048)
*Jincheng Bai,Zhenyu Zhang,Jennifer Zhang,Zhihuai Zhu*

Main category: cs.AI

TL;DR: Insight Agents (IA) is a conversational multi-agent system that provides Amazon sellers with personalized data insights using LLM-backed hierarchical agents for efficient information retrieval and business decision support.


<details>
  <summary>Details</summary>
Motivation: E-commerce sellers face challenges in discovering/utilizing available programs/tools and understanding rich data from various sources, requiring a solution to reduce effort and increase decision-making speed.

Method: Hierarchical multi-agent system with manager agent (OOD detection + BERT-based routing) and two worker agents (data presentation & insight generation) using plan-and-execute paradigm, API-based data models, and dynamic domain knowledge injection.

Result: Launched for Amazon sellers in US with 90% accuracy based on human evaluation and P90 latency below 15 seconds.

Conclusion: IA successfully serves as a force multiplier for sellers by reducing effort and accelerating business decisions through automated personalized insights.

Abstract: Today, E-commerce sellers face several key challenges, including difficulties in discovering and effectively utilizing available programs and tools, and struggling to understand and utilize rich data from various tools. We therefore aim to develop Insight Agents (IA), a conversational multi-agent Data Insight system, to provide E-commerce sellers with personalized data and business insights through automated information retrieval. Our hypothesis is that IA will serve as a force multiplier for sellers, thereby driving incremental seller adoption by reducing the effort required and increase speed at which sellers make good business decisions. In this paper, we introduce this novel LLM-backed end-to-end agentic system built on a plan-and-execute paradigm and designed for comprehensive coverage, high accuracy, and low latency. It features a hierarchical multi-agent structure, consisting of manager agent and two worker agents: data presentation and insight generation, for efficient information retrieval and problem-solving. We design a simple yet effective ML solution for manager agent that combines Out-of-Domain (OOD) detection using a lightweight encoder-decoder model and agent routing through a BERT-based classifier, optimizing both accuracy and latency. Within the two worker agents, a strategic planning is designed for API-based data model that breaks down queries into granular components to generate more accurate responses, and domain knowledge is dynamically injected to to enhance the insight generator. IA has been launched for Amazon sellers in US, which has achieved high accuracy of 90% based on human evaluation, with latency of P90 below 15s.

</details>


### [70] [Should I Have Expressed a Different Intent? Counterfactual Generation for LLM-Based Autonomous Control](https://arxiv.org/abs/2601.20090)
*Amirmohammad Farzaneh,Salvatore D'Oro,Osvaldo Simeone*

Main category: cs.AI

TL;DR: CCG framework enables reliable counterfactual reasoning for LLM-powered agents with formal guarantees on containing true counterfactual outcomes.


<details>
  <summary>Details</summary>
Motivation: Users interacting with LLM-powered agents may wonder how different phrasing of their intents would affect outcomes, but current systems lack reliable counterfactual reasoning capabilities with formal guarantees.

Method: Models user-LLM-environment interaction as structural causal model (SCM), uses test-time scaling for probabilistic abduction to generate candidate counterfactual outcomes, and applies conformal counterfactual generation (CCG) with offline calibration for formal reliability guarantees.

Result: CCG yields sets of counterfactual outcomes guaranteed to contain the true counterfactual outcome with high probability, outperforming naive re-execution baselines in wireless network control use case.

Conclusion: The CCG framework provides a principled approach for reliable counterfactual reasoning in LLM-driven control scenarios with formal guarantees, enabling users to explore "what-if" scenarios about intent phrasing.

Abstract: Large language model (LLM)-powered agents can translate high-level user intents into plans and actions in an environment. Yet after observing an outcome, users may wonder: What if I had phrased my intent differently? We introduce a framework that enables such counterfactual reasoning in agentic LLM-driven control scenarios, while providing formal reliability guarantees. Our approach models the closed-loop interaction between a user, an LLM-based agent, and an environment as a structural causal model (SCM), and leverages test-time scaling to generate multiple candidate counterfactual outcomes via probabilistic abduction. Through an offline calibration phase, the proposed conformal counterfactual generation (CCG) yields sets of counterfactual outcomes that are guaranteed to contain the true counterfactual outcome with high probability. We showcase the performance of CCG on a wireless network control use case, demonstrating significant advantages compared to naive re-execution baselines.

</details>


### [71] [Towards Intelligent Urban Park Development Monitoring: LLM Agents for Multi-Modal Information Fusion and Analysis](https://arxiv.org/abs/2601.20206)
*Zixuan Xiao,Chunguang Hu,Jun Ma*

Main category: cs.AI

TL;DR: A multi-modal LLM agent framework for urban park development monitoring that addresses limitations of traditional remote sensing methods through semantic understanding and reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Traditional remote sensing change detection methods have limitations in high-level intelligent analysis and fail to meet current urban planning needs. There's growing demand for complex multi-modal data analysis in urban park monitoring that existing methods can't handle flexibly.

Method: Proposes a multi-modal LLM agent framework with: 1) general horizontal and vertical data alignment mechanism for multi-modal data consistency and tracking, and 2) specific toolkit to address LLM hallucination issues due to lack of domain knowledge.

Result: The approach enables robust multi-modal information fusion and analysis, offering reliable and scalable solutions for urban park development monitoring compared to vanilla GPT-4o and other agents.

Conclusion: The multi-modal LLM agent framework effectively addresses challenges in urban park development monitoring by leveraging LLM's semantic understanding and reasoning capabilities while overcoming domain knowledge limitations through specialized toolkits.

Abstract: As an important part of urbanization, the development monitoring of newly constructed parks is of great significance for evaluating the effect of urban planning and optimizing resource allocation. However, traditional change detection methods based on remote sensing imagery have obvious limitations in high-level and intelligent analysis, and thus are difficult to meet the requirements of current urban planning and management. In face of the growing demand for complex multi-modal data analysis in urban park development monitoring, these methods often fail to provide flexible analysis capabilities for diverse application scenarios. This study proposes a multi-modal LLM agent framework, which aims to make full use of the semantic understanding and reasoning capabilities of LLM to meet the challenges in urban park development monitoring. In this framework, a general horizontal and vertical data alignment mechanism is designed to ensure the consistency and effective tracking of multi-modal data. At the same time, a specific toolkit is constructed to alleviate the hallucination issues of LLM due to the lack of domain-specific knowledge. Compared to vanilla GPT-4o and other agents, our approach enables robust multi-modal information fusion and analysis, offering reliable and scalable solutions tailored to the diverse and evolving demands of urban park development monitoring.

</details>


### [72] [Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning](https://arxiv.org/abs/2601.20221)
*Hang Zhang,Ruheng Wang,Yuelyu Ji,Mingu Kwak,Xizhi Wu,Chenyu Li,Li Zhang,Wenqi Shi,Yifan Peng,Yanshan Wang*

Main category: cs.AI

TL;DR: MIRAGE is an agentic framework that trains medical reasoning verifiers to iteratively query external medical corpora during evaluation, achieving substantial accuracy gains while reducing sampling budget requirements.


<details>
  <summary>Details</summary>
Motivation: Current reward models for medical reasoning verification have limitations: they only produce scalar rewards without justification, and use single-pass retrieval that prevents adaptive knowledge access during verification. There's a need for more reliable verification methods for clinical deployment of LLMs.

Method: MIRAGE combines tool-augmented verification with iterative reinforcement learning. It trains verifiers to query external medical corpora iteratively during evaluation, uses trace-level supervision, and includes an adaptive curriculum mechanism that dynamically adjusts training data distribution.

Result: Across four medical reasoning benchmarks, MIRAGE achieved 23.5% improvement on MedQA and 32.0% on MedXpertQA relative to base generator. Most notably, it demonstrated an 8Ã— reduction in sampling budget requirement compared to prior reward model baselines.

Conclusion: Grounding verification in dynamically retrieved evidence offers a principled path toward more reliable medical reasoning systems, addressing limitations of existing verification methods and enabling more efficient clinical deployment.

Abstract: Large language models have achieved strong performance on medical reasoning benchmarks, yet their deployment in clinical settings demands rigorous verification to ensure factual accuracy. While reward models offer a scalable approach for reasoning trace verification, existing methods face two limitations: they produce only scalar reward values without explicit justification, and they rely on single-pass retrieval that precludes adaptive knowledge access as verification unfolds. We introduce $\method$, an agentic framework that addresses these limitations by training medical reasoning verifiers to iteratively query external medical corpora during evaluation. Our approach combines tool-augmented verification with an iterative reinforcement learning paradigm that requires only trace-level supervision, alongside an adaptive curriculum mechanism that dynamically adjusts training data distribution. Across four medical reasoning benchmarks, $\method$ achieves substantial gains over existing methods, improving MedQA accuracy by 23.5% and MedXpertQA by 32.0% relative to the base generator in particular. Crucially, $\method$ demonstrates an $\mathbf{8\times}$ reduction in sampling budget requirement compared to prior reward model baselines. These findings establish that grounding verification in dynamically retrieved evidence offers a principled path toward more reliable medical reasoning systems.

</details>


### [73] [Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models](https://arxiv.org/abs/2601.20305)
*Zhenchen Tang,Songlin Yang,Zichuan Wang,Bo Peng,Yang Li,Beibei Dong,Jing Dong*

Main category: cs.AI

TL;DR: SEER framework bridges cognitive gap in UMMs by enabling models to generate self-aligned descriptors during generation through endogenous reprompting, improving both understanding and generation quality.


<details>
  <summary>Details</summary>
Motivation: Unified Multimodal Models have strong understanding capabilities but fail to effectively guide generation due to a "Cognitive Gap" - the model lacks understanding of how to enhance its own generation process.

Method: Proposes Endogenous Reprompting mechanism and SEER framework with two-stage endogenous loop: 1) RLVR activates latent evaluation ability via curriculum learning, 2) RLMT leverages this signal to optimize generative reasoning policy. Uses only 300 samples from Visual Instruction Elaboration proxy task.

Result: SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality without sacrificing general multimodal capabilities.

Conclusion: The proposed Endogenous Reprompting mechanism successfully bridges the cognitive gap in UMMs by transforming understanding from passive encoding into explicit generative reasoning, enabling models to enhance their own generation process.

Abstract: Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model's understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model's latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities.

</details>


### [74] [ECG-Agent: On-Device Tool-Calling Agent for ECG Multi-Turn Dialogue](https://arxiv.org/abs/2601.20323)
*Hyunseung Chung,Jungwoo Oh,Daeun Kyung,Jiho Kim,Yeonsu Kwon,Min-Gyu Kim,Edward Choi*

Main category: cs.AI

TL;DR: ECG-Agent is the first LLM-based tool-calling agent for multi-turn ECG dialogue, addressing limitations of current models by enabling conversational ability, on-device efficiency, and precise ECG measurement understanding.


<details>
  <summary>Details</summary>
Motivation: Current multimodal LLMs for electrocardiograms lack multi-turn conversational ability, on-device efficiency, and precise understanding of ECG measurements like PQRST intervals, making them inadequate for real-world clinical scenarios.

Method: Introduced ECG-Agent as an LLM-based tool-calling agent for multi-turn ECG dialogue, along with ECG-MTD dataset containing realistic user-assistant dialogues for diverse ECG lead configurations. Developed ECG-Agents in various sizes from on-device capable to larger models.

Result: ECG-Agents outperform baseline ECG-LLMs in response accuracy. On-device agents achieve comparable performance to larger agents in evaluations assessing response accuracy, tool-calling ability, and hallucinations.

Conclusion: ECG-Agent demonstrates viability for real-world applications by addressing key limitations of current ECG-LLMs, with on-device agents showing comparable performance to larger models across multiple evaluation metrics.

Abstract: Recent advances in Multimodal Large Language Models have rapidly expanded to electrocardiograms, focusing on classification, report generation, and single-turn QA tasks. However, these models fall short in real-world scenarios, lacking multi-turn conversational ability, on-device efficiency, and precise understanding of ECG measurements such as the PQRST intervals. To address these limitations, we introduce ECG-Agent, the first LLM-based tool-calling agent for multi-turn ECG dialogue. To facilitate its development and evaluation, we also present ECG-Multi-Turn-Dialogue (ECG-MTD) dataset, a collection of realistic user-assistant multi-turn dialogues for diverse ECG lead configurations. We develop ECG-Agents in various sizes, from on-device capable to larger agents. Experimental results show that ECG-Agents outperform baseline ECG-LLMs in response accuracy. Furthermore, on-device agents achieve comparable performance to larger agents in various evaluations that assess response accuracy, tool-calling ability, and hallucinations, demonstrating their viability for real-world applications.

</details>


### [75] [AMA: Adaptive Memory via Multi-Agent Collaboration](https://arxiv.org/abs/2601.20352)
*Weiquan Huang,Zixuan Wang,Hehai Lin,Sudong Wang,Bo Xu,Qian Li,Beier Zhu,Linyi Yang,Chengwei Qin*

Main category: cs.AI

TL;DR: AMA is a multi-agent memory framework that uses coordinated agents to manage memory across granularities, improving retrieval precision and consistency while reducing token usage by 80%.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agent memory systems have rigid retrieval granularity, accumulation-heavy maintenance, and coarse-grained updates, causing mismatches between stored information and task needs while accumulating logical inconsistencies over time.

Method: AMA uses a hierarchical memory design with coordinated agents: Constructor and Retriever for multi-granularity memory construction and adaptive query routing; Judge for relevance/consistency verification with iterative retrieval; Refresher for targeted updates and conflict resolution.

Result: AMA significantly outperforms state-of-the-art baselines on challenging long-context benchmarks while reducing token consumption by approximately 80% compared to full-context methods.

Conclusion: The AMA framework effectively addresses memory system limitations in LLM agents by enabling adaptive granularity alignment, maintaining retrieval precision, and ensuring long-term memory consistency through multi-agent collaboration.

Abstract: The rapid evolution of Large Language Model (LLM) agents has necessitated robust memory systems to support cohesive long-term interaction and complex reasoning. Benefiting from the strong capabilities of LLMs, recent research focus has shifted from simple context extension to the development of dedicated agentic memory systems. However, existing approaches typically rely on rigid retrieval granularity, accumulation-heavy maintenance strategies, and coarse-grained update mechanisms. These design choices create a persistent mismatch between stored information and task-specific reasoning demands, while leading to the unchecked accumulation of logical inconsistencies over time. To address these challenges, we propose Adaptive Memory via Multi-Agent Collaboration (AMA), a novel framework that leverages coordinated agents to manage memory across multiple granularities. AMA employs a hierarchical memory design that dynamically aligns retrieval granularity with task complexity. Specifically, the Constructor and Retriever jointly enable multi-granularity memory construction and adaptive query routing. The Judge verifies the relevance and consistency of retrieved content, triggering iterative retrieval when evidence is insufficient or invoking the Refresher upon detecting logical conflicts. The Refresher then enforces memory consistency by performing targeted updates or removing outdated entries. Extensive experiments on challenging long-context benchmarks show that AMA significantly outperforms state-of-the-art baselines while reducing token consumption by approximately 80% compared to full-context methods, demonstrating its effectiveness in maintaining retrieval precision and long-term memory consistency.

</details>


### [76] [Policy of Thoughts: Scaling LLM Reasoning via Test-time Policy Evolution](https://arxiv.org/abs/2601.20379)
*Zhengbo Jiao,Hongyu Xian,Qinglong Wang,Yunpu Ma,Zhebo Wang,Zifan Zhang,Dezhang Kong,Meng Han*

Main category: cs.AI

TL;DR: PoT (Policy of Thoughts) enables LLMs to learn from execution feedback during reasoning, improving complex problem-solving by updating model policies online rather than just filtering outputs.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex, long-horizon reasoning due to their frozen policy assumption. Current methods treat execution feedback only as an external signal for filtering or rewriting, without internalizing it to improve the underlying reasoning strategy.

Method: PoT recasts reasoning as within-instance online optimization: 1) generates diverse candidate solutions via efficient exploration, 2) uses Group Relative Policy Optimization (GRPO) to update a transient LoRA adapter based on execution feedback, enabling dynamic refinement of reasoning priors.

Result: A 4B model achieves 49.71% accuracy on LiveCodeBench, outperforming GPT-4o and DeepSeek-V3 despite being over 50Ã— smaller, showing dramatic performance improvements.

Conclusion: PoT demonstrates that real-time evolution of model policies through learning from failed attempts enables more effective reasoning, supporting the epistemological principle that intelligence requires "conjectures and refutations."

Abstract: Large language models (LLMs) struggle with complex, long-horizon reasoning due to instability caused by their frozen policy assumption. Current test-time scaling methods treat execution feedback merely as an external signal for filtering or rewriting trajectories, without internalizing it to improve the underlying reasoning strategy. Inspired by Popper's epistemology of "conjectures and refutations," we argue that intelligence requires real-time evolution of the model's policy through learning from failed attempts. We introduce Policy of Thoughts (PoT), a framework that recasts reasoning as a within-instance online optimization process. PoT first generates diverse candidate solutions via an efficient exploration mechanism, then uses Group Relative Policy Optimization (GRPO) to update a transient LoRA adapter based on execution feedback. This closed-loop design enables dynamic, instance-specific refinement of the model's reasoning priors. Experiments show that PoT dramatically boosts performance: a 4B model achieves 49.71% accuracy on LiveCodeBench, outperforming GPT-4o and DeepSeek-V3 despite being over 50 smaller.

</details>


### [77] [OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution](https://arxiv.org/abs/2601.20380)
*Le Zhang,Yixiong Xiao,Xinjiang Lu,Jingjia Cao,Yusai Zhao,Jingbo Zhou,Lang An,Zikan Feng,Wanxiang Sha,Yu Shi,Congxi Xiao,Jian Xiong,Yankai Zhang,Hua Wu,Haifeng Wang*

Main category: cs.AI

TL;DR: OmegaUse is a general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, using a Mixture-of-Experts backbone with novel data synthesis and two-stage training to achieve state-of-the-art performance across multiple GUI benchmarks.


<details>
  <summary>Details</summary>
Motivation: GUI agents have great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving productivity. However, building effective GUI agents requires addressing two key challenges: obtaining high-quality training data and developing effective training methods.

Method: 1) Data construction: Combines curated open-source datasets with a novel automated synthesis framework using bottom-up autonomous exploration and top-down taxonomy-guided generation. 2) Training: Two-stage approach with Supervised Fine-Tuning (SFT) for fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) for spatial grounding and sequential planning. 3) Architecture: Built on Mixture-of-Experts (MoE) backbone for computational efficiency and reasoning capacity.

Result: Achieves state-of-the-art performance: 96.3% on ScreenSpot-V2, 79.1% step success rate on AndroidControl. On the new OS-Nav benchmark: 74.24% step success on ChiM-Nav (Chinese Android mobile) and 55.9% average success on Ubu-Nav (Ubuntu desktop).

Conclusion: OmegaUse demonstrates strong cross-terminal capabilities for GUI task execution, with competitive performance across established benchmarks and promising results on new operating system-specific evaluation suites, showing the effectiveness of the proposed data construction and training methodology.

Abstract: Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving human productivity. In this report, we present OmegaUse, a general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, supporting computer-use and phone-use scenarios. Building an effective GUI agent model relies on two factors: (1) high-quality data and (2) effective training methods. To address these, we introduce a carefully engineered data-construction pipeline and a decoupled training paradigm. For data construction, we leverage rigorously curated open-source datasets and introduce a novel automated synthesis framework that integrates bottom-up autonomous exploration with top-down taxonomy-guided generation to create high-fidelity synthetic data. For training, to better leverage these data, we adopt a two-stage strategy: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. To balance computational efficiency with agentic reasoning capacity, OmegaUse is built on a Mixture-of-Experts (MoE) backbone. To evaluate cross-terminal capabilities in an offline setting, we introduce OS-Nav, a benchmark suite spanning multiple operating systems: ChiM-Nav, targeting Chinese Android mobile environments, and Ubu-Nav, focusing on routine desktop interactions on Ubuntu. Extensive experiments show that OmegaUse is highly competitive across established GUI benchmarks, achieving a state-of-the-art (SOTA) score of 96.3% on ScreenSpot-V2 and a leading 79.1% step success rate on AndroidControl. OmegaUse also performs strongly on OS-Nav, reaching 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav.

</details>


### [78] [CtrlCoT: Dual-Granularity Chain-of-Thought Compression for Controllable Reasoning](https://arxiv.org/abs/2601.20467)
*Zhenxuan Fan,Jie Cao,Yang Dai,Zheqi Lv,Wenqiao Zhang,Zhongle Xie,Peng LU,Beng Chin Ooi*

Main category: cs.AI

TL;DR: CtrlCoT is a dual-granularity CoT compression framework that combines semantic abstraction and token-level pruning to reduce LLM reasoning latency while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought prompting improves LLM reasoning but has high latency and memory costs due to verbose traces. Existing compression methods either use conservative semantic shortening or aggressive token pruning, both with limitations, and combining them is challenging.

Method: Three components: 1) Hierarchical Reasoning Abstraction for multi-granularity CoTs, 2) Logic-Preserving Distillation to train a logic-aware pruner that retains critical reasoning cues, and 3) Distribution-Alignment Generation to align compressed traces with fluent inference-time reasoning styles.

Result: On MATH-500 with Qwen2.5-7B-Instruct, CtrlCoT uses 30.7% fewer tokens while achieving 7.6 percentage points higher accuracy than the strongest baseline.

Conclusion: CtrlCoT demonstrates more efficient and reliable reasoning through harmonized dual-granularity compression that preserves task-critical logic while reducing computational overhead.

Abstract: Chain-of-thought (CoT) prompting improves LLM reasoning but incurs high latency and memory cost due to verbose traces, motivating CoT compression with preserved correctness. Existing methods either shorten CoTs at the semantic level, which is often conservative, or prune tokens aggressively, which can miss task-critical cues and degrade accuracy. Moreover, combining the two is non-trivial due to sequential dependency, task-agnostic pruning, and distribution mismatch. We propose \textbf{CtrlCoT}, a dual-granularity CoT compression framework that harmonizes semantic abstraction and token-level pruning through three components: Hierarchical Reasoning Abstraction produces CoTs at multiple semantic granularities; Logic-Preserving Distillation trains a logic-aware pruner to retain indispensable reasoning cues (e.g., numbers and operators) across pruning ratios; and Distribution-Alignment Generation aligns compressed traces with fluent inference-time reasoning styles to avoid fragmentation. On MATH-500 with Qwen2.5-7B-Instruct, CtrlCoT uses 30.7\% fewer tokens while achieving 7.6 percentage points higher than the strongest baseline, demonstrating more efficient and reliable reasoning. Our code will be publicly available at https://github.com/fanzhenxuan/Ctrl-CoT.

</details>


### [79] [Normative Equivalence in human-AI Cooperation: Behaviour, Not Identity, Drives Cooperation in Mixed-Agent Groups](https://arxiv.org/abs/2601.20487)
*Nico Mutzner,Taha Yasseri,Heiko Rauhut*

Main category: cs.AI

TL;DR: AI agents in human groups don't disrupt cooperative norms - cooperation levels, persistence, and normative perceptions are equivalent whether partners are labeled human or AI.


<details>
  <summary>Details</summary>
Motivation: Previous research focused on human-AI dyadic interactions, but little is known about how AI integration affects cooperative norm emergence in small groups. This study addresses the gap in understanding AI's impact on group cooperation dynamics.

Method: Online experiment using repeated four-player Public Goods Game with 236 participants. Each group had 3 humans + 1 bot framed as human or AI, following one of three strategies: unconditional cooperation, conditional cooperation, or free-riding. Follow-up Prisoner's Dilemma tested norm persistence.

Result: Cooperation driven by reciprocal dynamics and behavioral inertia, with identical mechanisms across conditions. No significant differences in cooperation levels between human and AI labels. No differences in norm persistence or normative perceptions. Behavior followed same normative logic regardless of partner identity.

Conclusion: Cooperative norms are flexible enough to extend to AI agents, supporting "normative equivalence" where cooperation mechanisms function similarly in mixed human-AI and all-human groups, blurring human-AI boundaries in collective decision-making.

Abstract: The introduction of artificial intelligence (AI) agents into human group settings raises essential questions about how these novel participants influence cooperative social norms. While previous studies on human-AI cooperation have primarily focused on dyadic interactions, little is known about how integrating AI agents affects the emergence and maintenance of cooperative norms in small groups. This study addresses this gap through an online experiment using a repeated four-player Public Goods Game (PGG). Each group consisted of three human participants and one bot, which was framed either as human or AI and followed one of three predefined decision strategies: unconditional cooperation, conditional cooperation, or free-riding. In our sample of 236 participants, we found that reciprocal group dynamics and behavioural inertia primarily drove cooperation. These normative mechanisms operated identically across conditions, resulting in cooperation levels that did not differ significantly between human and AI labels. Furthermore, we found no evidence of differences in norm persistence in a follow-up Prisoner's Dilemma, or in participants' normative perceptions. Participants' behaviour followed the same normative logic across human and AI conditions, indicating that cooperation depended on group behaviour rather than partner identity. This supports a pattern of normative equivalence, in which the mechanisms that sustain cooperation function similarly in mixed human-AI and all human groups. These findings suggest that cooperative norms are flexible enough to extend to artificial agents, blurring the boundary between humans and AI in collective decision-making.

</details>


### [80] [PathWise: Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs](https://arxiv.org/abs/2601.20539)
*Oguzhan Gungordu,Siheng Xiong,Faramarz Fekri*

Main category: cs.AI

TL;DR: PathWise is a multi-agent LLM framework that treats heuristic design as a sequential decision process using an entailment graph for stateful memory, enabling more efficient and effective automated heuristic generation for combinatorial optimization problems.


<details>
  <summary>Details</summary>
Motivation: Existing automated heuristic design frameworks using LLMs suffer from myopic heuristic generation, redundant evaluations, and limited reasoning about how new heuristics should be derived due to fixed evolutionary rules and static prompt templates.

Method: Proposes PathWise: a multi-agent reasoning framework with (1) policy agent planning evolutionary actions, (2) world model agent generating heuristic rollouts conditioned on actions, and (3) critic agents providing routed reflections from prior steps, all operating over an entailment graph serving as compact, stateful memory of search trajectory.

Result: Experiments across diverse combinatorial optimization problems show PathWise converges faster to better heuristics, generalizes across different LLM backbones, and scales to larger problem sizes.

Conclusion: PathWise shifts LLM-based automated heuristic design from trial-and-error evolution toward state-aware planning through reasoning, enabling more efficient and effective heuristic generation for combinatorial optimization.

Abstract: Large Language Models (LLMs) have enabled automated heuristic design (AHD) for combinatorial optimization problems (COPs), but existing frameworks' reliance on fixed evolutionary rules and static prompt templates often leads to myopic heuristic generation, redundant evaluations, and limited reasoning about how new heuristics should be derived. We propose a novel multi-agent reasoning framework, referred to as Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs (PathWise), which formulates heuristic generation as a sequential decision process over an entailment graph serving as a compact, stateful memory of the search trajectory. This approach allows the system to carry forward past decisions and reuse or avoid derivation information across generations. A policy agent plans evolutionary actions, a world model agent generates heuristic rollouts conditioned on those actions, and critic agents provide routed reflections summarizing lessons from prior steps, shifting LLM-based AHD from trial-and-error evolution toward state-aware planning through reasoning. Experiments across diverse COPs show that PathWise converges faster to better heuristics, generalizes across different LLM backbones, and scales to larger problem sizes.

</details>


### [81] [Online Risk-Averse Planning in POMDPs Using Iterated CVaR Value Function](https://arxiv.org/abs/2601.20554)
*Yaacov Pariente,Vadim Indelman*

Main category: cs.AI

TL;DR: This paper extends risk-sensitive planning to partially observable Markov decision processes using Iterated Conditional Value-at-Risk, developing algorithms with performance guarantees and demonstrating improved tail risk management.


<details>
  <summary>Details</summary>
Motivation: Traditional POMDP planning focuses on expected returns, but many real-world applications require managing risk and avoiding catastrophic outcomes. There's a need for risk-sensitive planning under partial observability that can handle tail risks effectively.

Method: Develops policy evaluation for ICVaR with finite-time guarantees, then extends three online planning algorithms (Sparse Sampling, PFT-DPW, POMCPOW) to optimize ICVaR instead of expected return. Introduces risk parameter Î± controlling risk aversion, with Î±=1 recovering standard planning.

Result: Establishes finite-time performance guarantees for ICVaR Sparse Sampling and develops novel exploration strategies tailored to ICVaR. Experiments on benchmark POMDP domains show proposed ICVaR planners achieve lower tail risk compared to risk-neutral counterparts.

Conclusion: The paper successfully extends risk-sensitive planning to partially observable settings using ICVaR, providing theoretical guarantees and practical algorithms that effectively manage tail risks while maintaining computational efficiency independent of action space size.

Abstract: We study risk-sensitive planning under partial observability using the dynamic risk measure Iterated Conditional Value-at-Risk (ICVaR). A policy evaluation algorithm for ICVaR is developed with finite-time performance guarantees that do not depend on the cardinality of the action space. Building on this foundation, three widely used online planning algorithms--Sparse Sampling, Particle Filter Trees with Double Progressive Widening (PFT-DPW), and Partially Observable Monte Carlo Planning with Observation Widening (POMCPOW)--are extended to optimize the ICVaR value function rather than the expectation of the return. Our formulations introduce a risk parameter $Î±$, where $Î±= 1$ recovers standard expectation-based planning and $Î±< 1$ induces increasing risk aversion. For ICVaR Sparse Sampling, we establish finite-time performance guarantees under the risk-sensitive objective, which further enable a novel exploration strategy tailored to ICVaR. Experiments on benchmark POMDP domains demonstrate that the proposed ICVaR planners achieve lower tail risk compared to their risk-neutral counterparts.

</details>


### [82] [Dialogical Reasoning Across AI Architectures: A Multi-Model Framework for Testing AI Alignment Strategies](https://arxiv.org/abs/2601.20604)
*Gray Cox*

Main category: cs.AI

TL;DR: Researchers test AI alignment using Peace Studies concepts through multi-model dialogues, finding AI can engage meaningfully with complex frameworks and generate novel insights.


<details>
  <summary>Details</summary>
Motivation: To empirically test AI alignment strategies by reframing alignment from a control problem to a relationship problem, drawing on Peace Studies traditions to develop a methodological framework for stress-testing alignment proposals before implementation.

Method: Experimental design with four distinct roles (Proposer, Responder, Monitor, Translator) assigned to different AI systems across six conditions, using Claude, Gemini, and GPT-4o for 72 dialogue turns totaling 576,822 characters of structured exchange.

Result: AI systems can meaningfully engage with Peace Studies concepts, surface complementary objections from different architectural perspectives, and generate emergent insights like "VCW as transitional framework." Different models foreground different concerns: Claude emphasized verification, Gemini focused on bias/scalability, GPT-4o highlighted implementation barriers.

Conclusion: The framework provides replicable methods for stress-testing alignment proposals, with preliminary evidence of AI capacity for dialogical reasoning. Limitations include dialogues engaging more with process than foundational claims, with future directions including human-AI hybrid protocols and extended dialogue studies.

Abstract: This paper introduces a methodological framework for empirically testing AI alignment strategies through structured multi-model dialogue. Drawing on Peace Studies traditions - particularly interest-based negotiation, conflict transformation, and commons governance - we operationalize Viral Collaborative Wisdom (VCW), an approach that reframes alignment from a control problem to a relationship problem developed through dialogical reasoning.
  Our experimental design assigns four distinct roles (Proposer, Responder, Monitor, Translator) to different AI systems across six conditions, testing whether current large language models can engage substantively with complex alignment frameworks. Using Claude, Gemini, and GPT-4o, we conducted 72 dialogue turns totaling 576,822 characters of structured exchange.
  Results demonstrate that AI systems can engage meaningfully with Peace Studies concepts, surface complementary objections from different architectural perspectives, and generate emergent insights not present in initial framings - including the novel synthesis of "VCW as transitional framework." Cross-architecture patterns reveal that different models foreground different concerns: Claude emphasized verification challenges, Gemini focused on bias and scalability, and GPT-4o highlighted implementation barriers.
  The framework provides researchers with replicable methods for stress-testing alignment proposals before implementation, while the findings offer preliminary evidence about AI capacity for the kind of dialogical reasoning VCW proposes. We discuss limitations, including the observation that dialogues engaged more with process elements than with foundational claims about AI nature, and outline directions for future research including human-AI hybrid protocols and extended dialogue studies.

</details>


### [83] [Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation](https://arxiv.org/abs/2601.20614)
*Yanqi Dai,Yuxiang Ji,Xiao Zhang,Yong Wang,Xiangxiang Chu,Zhiwu Lu*

Main category: cs.AI

TL;DR: MathForge improves mathematical reasoning in LLMs by addressing algorithmic imbalances in policy optimization and data augmentation limitations, using difficulty-aware training and multi-aspect question reformulation.


<details>
  <summary>Details</summary>
Motivation: Existing RLVR methods lack emphasis on challenging questions from both algorithmic (GRPO has implicit imbalance favoring easier questions) and data perspectives (augmentation only rephrases without increasing difficulty), limiting refinement of underdeveloped capabilities.

Method: Two-dual framework: 1) DGPO algorithm rectifies GRPO's imbalance via difficulty-balanced group advantage estimation and difficulty-aware question-level weighting; 2) MQR strategy reformulates questions across multiple aspects to increase intrinsic difficulty while preserving original answers.

Result: MathForge significantly outperforms existing methods on various mathematical reasoning tasks, with code and augmented data publicly available.

Conclusion: The synergistic loop between MQR (expanding data frontier) and DGPO (effectively learning from augmented data) successfully addresses algorithmic and data limitations to improve mathematical reasoning by targeting harder questions.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.

</details>


### [84] [Investigating the Development of Task-Oriented Communication in Vision-Language Models](https://arxiv.org/abs/2601.20641)
*Boaz Carmeli,Orr Paradise,Shafi Goldwasser,Yonatan Belinkov,Ron Meir*

Main category: cs.AI

TL;DR: LLM-based agents can develop task-oriented communication protocols that are more efficient than natural language and covert (hard to interpret by outsiders), raising both potential benefits and transparency risks.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLM-based agents can develop specialized communication protocols for collaborative reasoning tasks that differ from standard natural language, focusing on efficiency and covertness properties.

Method: Used a referential-game framework with vision-language model (VLM) agents communicating in a controlled setting to evaluate language variants and measure protocol development.

Result: VLMs can develop effective, task-adapted communication patterns that are more efficient than natural language and covert (difficult for humans and external agents to interpret). Spontaneous coordination between similar models was observed without explicitly shared protocols.

Conclusion: Task-oriented communication protocols developed by LLM agents show both potential benefits (efficiency) and risks (lack of transparency/control). Referential games provide a valuable testbed for future research in this area.

Abstract: We investigate whether \emph{LLM-based agents} can develop task-oriented communication protocols that differ from standard natural language in collaborative reasoning tasks. Our focus is on two core properties such task-oriented protocols may exhibit: Efficiency -- conveying task-relevant information more concisely than natural language, and Covertness -- becoming difficult for external observers to interpret, raising concerns about transparency and control. To investigate these aspects, we use a referential-game framework in which vision-language model (VLM) agents communicate, providing a controlled, measurable setting for evaluating language variants. Experiments show that VLMs can develop effective, task-adapted communication patterns. At the same time, they can develop covert protocols that are difficult for humans and external agents to interpret. We also observe spontaneous coordination between similar models without explicitly shared protocols. These findings highlight both the potential and the risks of task-oriented communication, and position referential games as a valuable testbed for future work in this area.

</details>


### [85] [Enterprise Resource Planning Using Multi-type Transformers in Ferro-Titanium Industry](https://arxiv.org/abs/2601.20696)
*Samira Yazdanpourmoghadam,Mahan Balal Pour,Vahid Partovi Nia*

Main category: cs.AI

TL;DR: MTT achieves competitive performance on JSP and KP benchmarks and demonstrates real-world application in manufacturing.


<details>
  <summary>Details</summary>
Motivation: Combinatorial optimization problems like JSP and KP are fundamental challenges in operations research and logistics that require sophisticated algorithms. Recent advances in deep learning show transformers as promising alternatives to traditional heuristics.

Method: Leverage Multi-Type Transformer (MTT) architecture to address JSP and KP benchmarks in a unified framework.

Result: Extensive experimental evaluation shows MTT achieves competitive performance across standard benchmark datasets for JSP and KP of different sizes. Successfully applied multi-type attention to real Ferro-Titanium industry application.

Conclusion: First application of multi-type transformers in real manufacturing, demonstrating their potential for combinatorial optimization problems in practical settings.

Abstract: Combinatorial optimization problems such as the Job-Shop Scheduling Problem (JSP) and Knapsack Problem (KP) are fundamental challenges in operations research, logistics, and eterprise resource planning (ERP). These problems often require sophisticated algorithms to achieve near-optimal solutions within practical time constraints. Recent advances in deep learning have introduced transformer-based architectures as promising alternatives to traditional heuristics and metaheuristics. We leverage the Multi-Type Transformer (MTT) architecture to address these benchmarks in a unified framework. We present an extensive experimental evaluation across standard benchmark datasets for JSP and KP, demonstrating that MTT achieves competitive performance on different size of these benchmark problems. We showcase the potential of multi-type attention on a real application in Ferro-Titanium industry. To the best of our knowledge, we are the first to apply multi-type transformers in real manufacturing.

</details>


### [86] [Implementing Metric Temporal Answer Set Programming](https://arxiv.org/abs/2601.20735)
*Arvid Becker,Pedro Cabalar,Martin DiÃ©guez,Susana Hahn,Javier Romero,Torsten Schaub*

Main category: cs.AI

TL;DR: A computational approach for Metric ASP that handles quantitative temporal constraints (durations, deadlines) while maintaining scalability by using difference constraints to decouple from time granularity.


<details>
  <summary>Details</summary>
Motivation: To enable expressing quantitative temporal constraints in Answer Set Programming while overcoming the grounding bottleneck that occurs with fine-grained timing constraints.

Method: Extends ASP with difference constraints (simplified linear constraints) to handle time-related aspects externally, effectively decoupling metric ASP from time granularity.

Result: Developed a solution that maintains scalability and is unaffected by time precision, addressing the grounding bottleneck problem with fine-grained timing constraints.

Conclusion: The approach successfully enables quantitative temporal reasoning in ASP while preserving scalability through external handling of time constraints via difference constraints.

Abstract: We develop a computational approach to Metric Answer Set Programming (ASP) to allow for expressing quantitative temporal constraints, like durations and deadlines. A central challenge is to maintain scalability when dealing with fine-grained timing constraints, which can significantly exacerbate ASP's grounding bottleneck. To address this issue, we leverage extensions of ASP with difference constraints, a simplified form of linear constraints, to handle time-related aspects externally. Our approach effectively decouples metric ASP from the granularity of time, resulting in a solution that is unaffected by time precision.

</details>


### [87] [REASON: Accelerating Probabilistic Logical Reasoning for Scalable Neuro-Symbolic Intelligence](https://arxiv.org/abs/2601.20784)
*Zishen Wan,Che-Kai Liu,Jiayi Qian,Hanchen Yang,Arijit Raychowdhury,Tushar Krishna*

Main category: cs.AI

TL;DR: REASON is a hardware acceleration framework that achieves 12-50x speedup for probabilistic logical reasoning in neuro-symbolic AI systems by addressing inefficiencies in symbolic and probabilistic inference through specialized architecture and system-level integration.


<details>
  <summary>Details</summary>
Motivation: Neuro-symbolic AI systems show superior performance in reasoning tasks but suffer from severe inefficiencies in symbolic and probabilistic inference, particularly probabilistic logical reasoning, which has irregular control flow, low arithmetic intensity, and poor hardware utilization on conventional CPUs/GPUs.

Method: REASON introduces: 1) A unified directed acyclic graph representation capturing structure across symbolic/probabilistic models with adaptive pruning/regularization; 2) A reconfigurable tree-based processing fabric optimized for irregular traversal and probabilistic aggregation; 3) Tight GPU integration via programmable interface and multi-level pipeline for compositional execution.

Result: Achieves 12-50x speedup and 310-681x energy efficiency over desktop/edge GPUs (TSMC 28nm). Enables real-time reasoning with 0.8s end-to-end tasks, 6mmÂ² area, and 2.12W power consumption across six neuro-symbolic workloads.

Conclusion: Targeted acceleration of probabilistic logical reasoning is critical for practical neuro-symbolic AI. REASON serves as a foundational system architecture for next-generation cognitive intelligence by enabling efficient, real-time probabilistic reasoning.

Abstract: Neuro-symbolic AI systems integrate neural perception with symbolic reasoning to enable data-efficient, interpretable, and robust intelligence beyond purely neural models. Although this compositional paradigm has shown superior performance in domains such as reasoning, planning, and verification, its deployment remains challenging due to severe inefficiencies in symbolic and probabilistic inference. Through systematic analysis of representative neuro-symbolic workloads, we identify probabilistic logical reasoning as the inefficiency bottleneck, characterized by irregular control flow, low arithmetic intensity, uncoalesced memory accesses, and poor hardware utilization on CPUs and GPUs.
  This paper presents REASON, an integrated acceleration framework for probabilistic logical reasoning in neuro-symbolic AI. REASON introduces a unified directed acyclic graph representation that captures common structure across symbolic and probabilistic models, coupled with adaptive pruning and regularization. At the architecture level, REASON features a reconfigurable, tree-based processing fabric optimized for irregular traversal, symbolic deduction, and probabilistic aggregation. At the system level, REASON is tightly integrated with GPU streaming multiprocessors through a programmable interface and multi-level pipeline that efficiently orchestrates compositional execution. Evaluated across six neuro-symbolic workloads, REASON achieves 12-50x speedup and 310-681x energy efficiency over desktop and edge GPUs under TSMC 28 nm node. REASON enables real-time probabilistic logical reasoning, completing end-to-end tasks in 0.8 s with 6 mm2 area and 2.12 W power, demonstrating that targeted acceleration of probabilistic logical reasoning is critical for practical and scalable neuro-symbolic AI and positioning REASON as a foundational system architecture for next-generation cognitive intelligence.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [88] [Gap-K%: Measuring Top-1 Prediction Gap for Detecting Pretraining Data](https://arxiv.org/abs/2601.19936)
*Minseo Kwak,Jaehyung Kim*

Main category: cs.LG

TL;DR: Gap-K% is a new method for detecting pretraining data in LLMs that uses the probability gap between top-1 predicted tokens and target tokens, with sliding windows to capture local correlations, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: The opacity of massive pretraining corpora raises privacy and copyright concerns, making pretraining data detection critical. Existing methods relying on token likelihoods overlook divergence from top-1 predictions and local correlation between adjacent tokens.

Method: Gap-K% analyzes optimization dynamics of LLM pretraining, using the log probability gap between top-1 predicted token and target token. It incorporates a sliding window strategy to capture local correlations and mitigate token-level fluctuations.

Result: Extensive experiments on WikiMIA and MIMIR benchmarks show Gap-K% achieves state-of-the-art performance, consistently outperforming prior baselines across various model sizes and input lengths.

Conclusion: Gap-K% effectively addresses limitations of existing methods by leveraging optimization dynamics and local token correlations, providing a superior approach for pretraining data detection in LLMs.

Abstract: The opacity of massive pretraining corpora in Large Language Models (LLMs) raises significant privacy and copyright concerns, making pretraining data detection a critical challenge. Existing state-of-the-art methods typically rely on token likelihoods, yet they often overlook the divergence from the model's top-1 prediction and local correlation between adjacent tokens. In this work, we propose Gap-K%, a novel pretraining data detection method grounded in the optimization dynamics of LLM pretraining. By analyzing the next-token prediction objective, we observe that discrepancies between the model's top-1 prediction and the target token induce strong gradient signals, which are explicitly penalized during training. Motivated by this, Gap-K% leverages the log probability gap between the top-1 predicted token and the target token, incorporating a sliding window strategy to capture local correlations and mitigate token-level fluctuations. Extensive experiments on the WikiMIA and MIMIR benchmarks demonstrate that Gap-K% achieves state-of-the-art performance, consistently outperforming prior baselines across various model sizes and input lengths.

</details>


### [89] [DecHW: Heterogeneous Decentralized Federated Learning Exploiting Second-Order Information](https://arxiv.org/abs/2601.19938)
*Adnan Ahmad,Chiara Boldrini,Lorenzo Valerio,Andrea Passarella,Marco Conti*

Main category: cs.LG

TL;DR: DFL addresses data/model heterogeneity by using second-order information to generate consensus weights for robust aggregation, improving generalization with reduced communication.


<details>
  <summary>Details</summary>
Motivation: In decentralized federated learning, variations in individual device experiences and interactions create data and model initialization heterogeneities, leading to parameter variations that slow down convergence.

Method: Proposes a novel aggregation approach that captures parameter variations by generating consensus weights via approximation of second-order information of local models on their datasets, then scales neighborhood updates before aggregation.

Result: Extensive experiments with computer vision tasks show strong generalizability of local models at reduced communication costs.

Conclusion: The approach effectively tackles data and model heterogeneity in DFL by addressing parameter-level evidential credence variations, leading to improved convergence and generalization with lower communication overhead.

Abstract: Decentralized Federated Learning (DFL) is a serverless collaborative machine learning paradigm where devices collaborate directly with neighbouring devices to exchange model information for learning a generalized model. However, variations in individual experiences and different levels of device interactions lead to data and model initialization heterogeneities across devices. Such heterogeneities leave variations in local model parameters across devices that leads to slower convergence. This paper tackles the data and model heterogeneity by explicitly addressing the parameter level varying evidential credence across local models. A novel aggregation approach is introduced that captures these parameter variations in local models and performs robust aggregation of neighbourhood local updates. Specifically, consensus weights are generated via approximation of second-order information of local models on their local datasets. These weights are utilized to scale neighbourhood updates before aggregating them into global neighbourhood representation. In extensive experiments with computer vision tasks, the proposed approach shows strong generalizability of local models at reduced communication costs.

</details>


### [90] [oculomix: Hierarchical Sampling for Retinal-Based Systemic Disease Prediction](https://arxiv.org/abs/2601.19939)
*Hyunmin Kim,Yukun Zhou,Rahul A. Jonas,Lie Ju,Sunjin Hwang,Pearse A. Keane,Siegfried K. Wagner*

Main category: cs.LG

TL;DR: Oculomix: A hierarchical sampling strategy for mixed sample augmentations in retinal imaging that preserves patient-specific attributes by constraining mixing to patient and exam levels, outperforming image-level methods by up to 3% AUROC for cardiovascular event prediction.


<details>
  <summary>Details</summary>
Motivation: Current image-level mixed sample augmentations (CutMix, MixUp) used for training transformers in oculomics perturb patient-specific attributes like medical comorbidities and clinical factors because they only consider images and labels, not patient-level information.

Method: Oculomix uses hierarchical sampling based on two clinical priors: 1) Exam level - images from same patient at same time share attributes, 2) Patient level - images from same patient at different times have soft temporal trend (morbidity increases over time). The method constrains mixing space to patient and exam levels to preserve patient-specific characteristics and leverages hierarchical relationships.

Result: Oculomix consistently outperforms image-level CutMix and MixUp by up to 3% in AUROC for 5-year prediction of major adverse cardiovascular events (MACE) using ViT models on Alzeye dataset (large ethnically diverse population).

Conclusion: The hierarchical sampling approach preserves patient-specific attributes better than image-level methods, demonstrating the necessity and value of patient-aware data augmentation in oculomics for improved disease prediction performance.

Abstract: Oculomics - the concept of predicting systemic diseases, such as cardiovascular disease and dementia, through retinal imaging - has advanced rapidly due to the data efficiency of transformer-based foundation models like RETFound. Image-level mixed sample data augmentations, such as CutMix and MixUp, are frequently used for training transformers, yet these techniques perturb patient-specific attributes, such as medical comorbidity and clinical factors, since they only account for images and labels. To address this limitation, we propose a hierarchical sampling strategy, Oculomix, for mixed sample augmentations. Our method is based on two clinical priors. First (exam level), images acquired from the same patient at the same time point share the same attributes. Second (patient level), images acquired from the same patient at different time points have a soft temporal trend, as morbidity generally increases over time. Guided by these priors, our method constrains the mixing space to the patient and exam levels to better preserve patient-specific characteristics and leverages their hierarchical relationships. The proposed method is validated using ViT models on a five-year prediction of major adverse cardiovascular events (MACE) in a large ethnically diverse population (Alzeye). We show that Oculomix consistently outperforms image-level CutMix and MixUp by up to 3% in AUROC, demonstrating the necessity and value of the proposed method in oculomics.

</details>


### [91] [Continuous-Flow Data-Rate-Aware CNN Inference on FPGA](https://arxiv.org/abs/2601.19940)
*Tobias Habermann,Michael Mecik,Zhenyu Wang,CÃ©sar David Vera,Martin Kumm,Mario Garrido*

Main category: cs.LG

TL;DR: Novel data-rate-aware continuous-flow CNN architecture for FPGAs that achieves near 100% hardware utilization by interleaving low data rate signals and sharing hardware units, enabling complex CNNs like MobileNet on a single FPGA with high throughput.


<details>
  <summary>Details</summary>
Motivation: Previous unrolled hardware implementations for deep learning mostly focused on fully connected networks, but CNNs require fewer computations for the same accuracy. However, CNN pooling layers and convolutional layers with stride >1 reduce data rates, causing hardware underutilization in fully parallel implementations unless properly handled.

Method: Analyzes CNN data flow and proposes a data-rate-aware continuous-flow architecture that interleaves low data rate signals and shares hardware units. Uses appropriate parallelization to achieve throughput of fully parallel implementation while maintaining high hardware utilization.

Result: Significant reduction in arithmetic logic requirements, enabling implementation of complex CNNs like MobileNet on a single FPGA with high throughput while maintaining near 100% hardware utilization.

Conclusion: The proposed data-rate-aware continuous-flow CNN architecture effectively addresses hardware underutilization in FPGA implementations, allowing efficient deployment of complex CNNs with high throughput by optimizing data flow and hardware sharing.

Abstract: Among hardware accelerators for deep-learning inference, data flow implementations offer low latency and high throughput capabilities. In these architectures, each neuron is mapped to a dedicated hardware unit, making them well-suited for field-programmable gate array (FPGA) implementation. Previous unrolled implementations mostly focus on fully connected networks because of their simplicity, although it is well known that convolutional neural networks (CNNs) require fewer computations for the same accuracy. When observing the data flow in CNNs, pooling layers and convolutional layers with a stride larger than one, the number of data at their output is reduced with respect to their input. This data reduction strongly affects the data rate in a fully parallel implementation, making hardware units heavily underutilized unless it is handled properly. This work addresses this issue by analyzing the data flow of CNNs and presents a novel approach to designing data-rate-aware, continuous-flow CNN architectures. The proposed approach ensures a high hardware utilization close to 100% by interleaving low data rate signals and sharing hardware units, as well as using the right parallelization to achieve the throughput of a fully parallel implementation. The results show that a significant amount of the arithmetic logic can be saved, which allows implementing complex CNNs like MobileNet on a single FPGA with high throughput.

</details>


### [92] [Scaling Next-Brain-Token Prediction for MEG](https://arxiv.org/abs/2601.20138)
*Richard Csaky*

Main category: cs.LG

TL;DR: Large autoregressive model for MEG brain activity generation that scales next-token prediction across datasets and scanners, trained on 500+ hours of MEG data to generate minutes of brain activity from context.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable model for generating realistic magnetoencephalography (MEG) brain activity that can handle long contexts across different datasets and scanners, enabling brain activity generation and analysis.

Method: Modified SEANet-style vector-quantizer reduces multichannel MEG into token stream; Qwen2.5-VL backbone trained from scratch for next-token prediction; trained on CamCAN and Omega datasets; evaluated on held-out MOUS dataset with three task-matched tests for long-horizon generation quality.

Result: Model successfully generates stable MEG sequences over long rollouts; generations remain closer to correct continuations than swapped controls; demonstrates cross-dataset generalization from CamCAN/Omega to MOUS.

Conclusion: The model effectively generates realistic MEG brain activity across datasets, with stable long-horizon generation and specificity to context, providing a foundation for brain activity modeling and analysis.

Abstract: We present a large autoregressive model for source-space MEG that scales next-token prediction to long context across datasets and scanners: handling a corpus of over 500 hours and thousands of sessions across the three largest MEG datasets. A modified SEANet-style vector-quantizer reduces multichannel MEG into a flattened token stream on which we train a Qwen2.5-VL backbone from scratch to predict the next brain token and to recursively generate minutes of MEG from up to a minute of context. To evaluate long-horizon generation, we introduce three task-matched tests: (i) on-manifold stability via generated-only drift compared to the time-resolved distribution of real sliding windows, and (ii) conditional specificity via correct context versus prompt-swap controls using a neurophysiologically grounded metric set. We train on CamCAN and Omega and run all analyses on held-out MOUS, establishing cross-dataset generalization. Across metrics, generations remain relatively stable over long rollouts and are closer to the correct continuation than swapped controls. Code available at: https://github.com/ricsinaruto/brain-gen.

</details>


### [93] [Latent Object Permanence: Topological Phase Transitions, Free-Energy Principles, and Renormalization Group Flows in Deep Transformer Manifolds](https://arxiv.org/abs/2601.19942)
*Faruk Alpay,Bugra Kilictas*

Main category: cs.LG

TL;DR: Transformers develop multi-step reasoning through a phase transition where hidden states collapse into low-dimensional concept basins, forming reusable Transient Class Objects.


<details>
  <summary>Details</summary>
Motivation: To understand how deep Transformers develop multi-step reasoning capabilities through geometric and statistical physics analysis of their internal representations.

Method: Analyze layerwise covariance spectrum of activations, track deviations from random-matrix theory, measure sparsity order parameter, formalize forward pass as coarse-graining map, and validate with layerwise probes.

Result: Observed sharp reduction in effective dimensionality consistent with phase transition near critical depth Î³_câ‰ˆ0.42, formation of Transient Class Objects (TCOs), and spectral tail collapse in low-entropy regime.

Conclusion: Multi-step reasoning emerges through renormalization-like dynamics where representations collapse into stable concept basins, with theoretical connections between logical separability and spectral decay.

Abstract: We study the emergence of multi-step reasoning in deep Transformer language models through a geometric and statistical-physics lens. Treating the hidden-state trajectory as a flow on an implicit Riemannian manifold, we analyze the layerwise covariance spectrum of activations, where $C^{(\ell)}=\mathbb{E}[h^{(\ell)}h^{(\ell)\top}]$, and track deviations from a random-matrix bulk. Across model scales (1.5B--30B), we observe a sharp reduction in effective dimensionality consistent with a phase transition: an order parameter based on sparsity/localization, $Î©(h)=1-\|h\|_1/(\sqrt{d}\|h\|_2)$, exhibits a discontinuity near a critical normalized depth $Î³_c\approx 0.42$ in sufficiently large models. We formalize the forward pass as a discrete coarse-graining map and relate the appearance of stable "concept basins" to fixed points of this renormalization-like dynamics. The resulting low-entropy regime is characterized by a spectral tail collapse and by the formation of transient, reusable object-like structures in representation space, which we call Transient Class Objects (TCOs). We provide theoretical conditions connecting logical separability to spectral decay and validate the predicted signatures with layerwise probes on multiple open-weight model families.

</details>


### [94] [Emergent Specialization in Learner Populations: Competition as the Source of Diversity](https://arxiv.org/abs/2601.19943)
*Yuhao Li*

Main category: cs.LG

TL;DR: Competition alone induces emergent specialization in learner populations without communication or diversity incentives, validated across six real-world domains with strong performance gains.


<details>
  <summary>Details</summary>
Motivation: To understand how populations of learners can develop coordinated, diverse behaviors without explicit communication or diversity incentives, inspired by ecological niche theory.

Method: NichePopulation algorithm combining competitive exclusion with niche affinity tracking, tested across six domains: cryptocurrency trading, commodity prices, weather forecasting, solar irradiance, urban traffic, and air quality.

Result: Achieved mean Specialization Index of 0.75 with effect sizes Cohen's d > 20; learners still achieve SI > 0.30 without niche bonus; diverse populations outperform homogeneous baselines by +26.5%; outperforms MARL baselines by 4.3x while being 4x faster.

Conclusion: Competition alone is sufficient to induce emergent specialization in learner populations, enabling method-level division of labor that significantly outperforms both homogeneous populations and existing multi-agent reinforcement learning approaches.

Abstract: How can populations of learners develop coordinated, diverse behaviors without explicit communication or diversity incentives? We demonstrate that competition alone is sufficient to induce emergent specialization -- learners spontaneously partition into specialists for different environmental regimes through competitive dynamics, consistent with ecological niche theory. We introduce the NichePopulation algorithm, a simple mechanism combining competitive exclusion with niche affinity tracking. Validated across six real-world domains (cryptocurrency trading, commodity prices, weather forecasting, solar irradiance, urban traffic, and air quality), our approach achieves a mean Specialization Index of 0.75 with effect sizes of Cohen's d > 20. Key findings: (1) At lambda=0 (no niche bonus), learners still achieve SI > 0.30, proving specialization is genuinely emergent; (2) Diverse populations outperform homogeneous baselines by +26.5% through method-level division of labor; (3) Our approach outperforms MARL baselines (QMIX, MAPPO, IQL) by 4.3x while being 4x faster.

</details>


### [95] [Classifier Calibration at Scale: An Empirical Study of Model-Agnostic Post-Hoc Methods](https://arxiv.org/abs/2601.19944)
*Valery Manokhin,Daniel GrÃ¸nhaug*

Main category: cs.LG

TL;DR: Benchmark study shows Venn-Abers predictors and Beta calibration provide best calibration improvements for binary classification on tabular data, while Platt scaling and isotonic regression can degrade performance for modern models.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare model-agnostic post-hoc calibration methods for improving probabilistic predictions in binary classification, with focus on methods providing distribution-free validity guarantees under exchangeability.

Method: Benchmarked 21 classifiers on TabArena-v0.1 tasks using 5-fold cross-validation. Tested 5 calibrators (Isotonic, Platt, Beta, Venn-Abers, Pearsonify) on separate calibration split. Evaluated using proper scoring rules, diagnostic measures, discrimination, and classification metrics.

Result: Venn-Abers predictors achieved largest average log-loss reductions, followed by Beta calibration. Platt scaling showed weaker effects. Beta calibration improved log-loss most frequently, while Venn-Abers had fewer extreme degradations. Common methods like Platt and isotonic can degrade performance for modern tabular models.

Conclusion: Calibration effects vary substantially across datasets and architectures with no uniform winner. Venn-Abers and Beta calibration generally perform best, while traditional methods may harm modern models. All methods except Pearsonify slightly increase accuracy but effects are marginal.

Abstract: We study model-agnostic post-hoc calibration methods intended to improve probabilistic predictions in supervised binary classification on real i.i.d. tabular data, with particular emphasis on conformal and Venn-based approaches that provide distribution-free validity guarantees under exchangeability. We benchmark 21 widely used classifiers, including linear models, SVMs, tree ensembles (CatBoost, XGBoost, LightGBM), and modern tabular neural and foundation models, on binary tasks from the TabArena-v0.1 suite using randomized, stratified five-fold cross-validation with a held-out test fold. Five calibrators; Isotonic regression, Platt scaling, Beta calibration, Venn-Abers predictors, and Pearsonify are trained on a separate calibration split and applied to test predictions. Calibration is evaluated using proper scoring rules (log-loss and Brier score) and diagnostic measures (Spiegelhalter's Z, ECE, and ECI), alongside discrimination (AUC-ROC) and standard classification metrics. Across tasks and architectures, Venn-Abers predictors achieve the largest average reductions in log-loss, followed closely by Beta calibration, while Platt scaling exhibits weaker and less consistent effects. Beta calibration improves log-loss most frequently across tasks, whereas Venn-Abers displays fewer instances of extreme degradation and slightly more instances of extreme improvement. Importantly, we find that commonly used calibration procedures, most notably Platt scaling and isotonic regression, can systematically degrade proper scoring performance for strong modern tabular models. Overall classification performance is often preserved, but calibration effects vary substantially across datasets and architectures, and no method dominates uniformly. In expectation, all methods except Pearsonify slightly increase accuracy, but the effect is marginal, with the largest expected gain about 0.008%.

</details>


### [96] [NCSAM Noise-Compensated Sharpness-Aware Minimization for Noisy Label Learning](https://arxiv.org/abs/2601.19947)
*Jiayu Xu,Junbiao Pang*

Main category: cs.LG

TL;DR: The paper proposes Noise-Compensated Sharpness-aware Minimization (NCSAM), which uses simulated label noise and loss landscape flatness analysis to improve generalization and robustness against noisy labels.


<details>
  <summary>Details</summary>
Motivation: Real-world datasets often contain noisy/erroneous labels (e.g., from web-crawled data), which harms deep learning performance. Current methods focus on label correction, but this paper takes a novel theoretical approach analyzing loss landscape flatness in relation to label noise.

Method: Theoretical analysis shows simulated label noise can enhance generalization and robustness. Based on this, proposes Noise-Compensated Sharpness-aware Minimization (NCSAM), which leverages SAM's perturbation mechanism to compensate for label noise damage.

Result: Testing accuracy shows similar behavior to noise-clear datasets. Extensive experiments on multiple benchmark datasets demonstrate consistent superiority over state-of-the-art methods across diverse tasks.

Conclusion: The paper establishes a theoretical link between loss landscape flatness and label noise, proposes NCSAM as an effective solution, and shows it outperforms existing approaches by leveraging simulated noise to improve both generalization and robustness.

Abstract: Learning from Noisy Labels (LNL) presents a fundamental challenge in deep learning, as real-world datasets often contain erroneous or corrupted annotations, \textit{e.g.}, data crawled from Web. Current research focuses on sophisticated label correction mechanisms. In contrast, this paper adopts a novel perspective by establishing a theoretical analysis the relationship between flatness of the loss landscape and the presence of label noise. In this paper, we theoretically demonstrate that carefully simulated label noise synergistically enhances both the generalization performance and robustness of label noises. Consequently, we propose Noise-Compensated Sharpness-aware Minimization (NCSAM) to leverage the perturbation of Sharpness-Aware Minimization (SAM) to remedy the damage of label noises. Our analysis reveals that the testing accuracy exhibits a similar behavior that has been observed on the noise-clear dataset. Extensive experimental results on multiple benchmark datasets demonstrate the consistent superiority of the proposed method over existing state-of-the-art approaches on diverse tasks.

</details>


### [97] [Probabilistic Sensing: Intelligence in Data Sampling](https://arxiv.org/abs/2601.19953)
*Ibrahim Albulushi,Saleh Bunaiyan,Suraj S. Cheema,Hesham ElSawy,Feras Al-Dirini*

Main category: cs.LG

TL;DR: A probabilistic sensing paradigm using p-neurons enables intelligent, real-time decisions about data sampling, achieving 93% energy savings with minimal information loss.


<details>
  <summary>Details</summary>
Motivation: Current deterministic approaches to intelligent sensor sampling risk losing important information. There's a need for a probabilistic approach that can make real-time sampling decisions while maintaining data integrity and achieving significant energy efficiency gains.

Method: The system employs a probabilistic neuron (p-neuron) inspired by the autonomous nervous system, driven by an analog feature extraction circuit. This enables probabilistic decision-making about whether to sample data, with microsecond response times that overcome sub-sampling-rate limitations.

Result: The system achieved lossless probabilistic data acquisition with only 0.41% normalized mean squared error. It demonstrated 93% savings in both active operation time and number of generated samples during validation experiments using active seismic survey data.

Conclusion: The probabilistic sensing paradigm successfully enables real-time intelligent autonomous activation of data sampling, providing transformative energy-efficiency gains while maintaining data integrity through probabilistic decision-making rather than deterministic approaches.

Abstract: Extending the intelligence of sensors to the data-acquisition process - deciding whether to sample or not - can result in transformative energy-efficiency gains. However, making such a decision in a deterministic manner involves risk of losing information. Here we present a sensing paradigm that enables making such a decision in a probabilistic manner. The paradigm takes inspiration from the autonomous nervous system and employs a probabilistic neuron (p-neuron) driven by an analog feature extraction circuit. The response time of the system is on the order of microseconds, over-coming the sub-sampling-rate response time limit and enabling real-time intelligent autonomous activation of data-sampling. Validation experiments on active seismic survey data demonstrate lossless probabilistic data acquisition, with a normalized mean squared error of 0.41%, and 93% saving in the active operation time of the system and the number of generated samples.

</details>


### [98] [MeanCache: From Instantaneous to Average Velocity for Accelerating Flow Matching Inference](https://arxiv.org/abs/2601.19961)
*Huanlin Gao,Ping Chen,Fuyuan Shi,Ruijia Wu,Li YanTao,Qiang Hui,Yuren You,Ting Lu,Chao Tan,Shaoan Zhao,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.LG

TL;DR: MeanCache is a training-free caching framework for Flow Matching inference that uses average-velocity perspective and trajectory-stability scheduling to achieve 3.59-4.56X acceleration while maintaining generation quality.


<details>
  <summary>Details</summary>
Motivation: Existing caching methods for Flow Matching inference rely on instantaneous velocity information, which causes severe trajectory deviations and error accumulation under high acceleration ratios, limiting their effectiveness.

Method: MeanCache introduces an average-velocity perspective using cached Jacobian-vector products (JVP) to construct interval average velocities from instantaneous velocities. It also employs a trajectory-stability scheduling strategy with Peak-Suppressed Shortest Path under budget constraints to optimize cache timing and JVP reuse stability.

Result: Experiments on FLUX.1, Qwen-Image, and HunyuanVideo show MeanCache achieves 4.12X, 4.56X, and 3.59X acceleration respectively, while consistently outperforming state-of-the-art caching baselines in generation quality.

Conclusion: MeanCache provides a simple yet effective approach for Flow Matching inference, offering a new perspective that could inspire further exploration of stability-driven acceleration in commercial-scale generative models.

Abstract: We present MeanCache, a training-free caching framework for efficient Flow Matching inference. Existing caching methods reduce redundant computation but typically rely on instantaneous velocity information (e.g., feature caching), which often leads to severe trajectory deviations and error accumulation under high acceleration ratios. MeanCache introduces an average-velocity perspective: by leveraging cached Jacobian--vector products (JVP) to construct interval average velocities from instantaneous velocities, it effectively mitigates local error accumulation. To further improve cache timing and JVP reuse stability, we develop a trajectory-stability scheduling strategy as a practical tool, employing a Peak-Suppressed Shortest Path under budget constraints to determine the schedule. Experiments on FLUX.1, Qwen-Image, and HunyuanVideo demonstrate that MeanCache achieves 4.12X and 4.56X and 3.59X acceleration, respectively, while consistently outperforming state-of-the-art caching baselines in generation quality. We believe this simple yet effective approach provides a new perspective for Flow Matching inference and will inspire further exploration of stability-driven acceleration in commercial-scale generative models.

</details>


### [99] [Order-Optimal Sample Complexity of Rectified Flows](https://arxiv.org/abs/2601.20250)
*Hari Krishna Sahoo,Mudit Gaur,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: Rectified flow models achieve optimal sample complexity of $\tilde{O}(\varepsilon^{-2})$ by constraining transport trajectories to be linear, enabling fast sampling with single Euler steps.


<details>
  <summary>Details</summary>
Motivation: Flow-based generative models show superior efficiency over diffusion models, but existing flow matching models have suboptimal $O(\varepsilon^{-4})$ sample complexity bounds. The paper aims to theoretically explain and improve the sample complexity of rectified flow models.

Method: Rectified flow models constrain transport trajectories to be linear from base distribution to data distribution. The velocity field is parameterized by neural networks, and the model is trained with squared loss along linear paths. Analysis exploits the structure to control localized Rademacher complexity.

Result: Proves rectified flows achieve sample complexity $\tilde{O}(\varepsilon^{-2})$, improving on the best known $O(\varepsilon^{-4})$ bounds for flow matching models and matching the optimal rate for mean estimation. This provides theoretical explanation for strong empirical performance.

Conclusion: The linear structure constraint in rectified flows enables both practical efficiency (fast sampling with single Euler steps) and theoretical optimality (matching mean estimation rates), explaining their strong empirical performance.

Abstract: Recently, flow-based generative models have shown superior efficiency compared to diffusion models. In this paper, we study rectified flow models, which constrain transport trajectories to be linear from the base distribution to the data distribution. This structural restriction greatly accelerates sampling, often enabling high-quality generation with a single Euler step. Under standard assumptions on the neural network classes used to parameterize the velocity field and data distribution, we prove that rectified flows achieve sample complexity $\tilde{O}(\varepsilon^{-2})$. This improves on the best known $O(\varepsilon^{-4})$ bounds for flow matching model and matches the optimal rate for mean estimation. Our analysis exploits the particular structure of rectified flows: because the model is trained with a squared loss along linear paths, the associated hypothesis class admits a sharply controlled localized Rademacher complexity. This yields the improved, order-optimal sample complexity and provides a theoretical explanation for the strong empirical performance of rectified flow models.

</details>


### [100] [Cross-Session Decoding of Neural Spiking Data via Task-Conditioned Latent Alignment](https://arxiv.org/abs/2601.19963)
*Canyang Zhao,Bolin Peng,J. Patrick Mayo,Ce Ju,Bing Liu*

Main category: cs.LG

TL;DR: TCLA framework addresses cross-session nonstationarity in BCIs by learning neural dynamics from source sessions and aligning target session latents in task-conditioned manner, improving decoding with limited target data.


<details>
  <summary>Details</summary>
Motivation: Cross-session nonstationarity in neural recordings causes decoder failure when trained on one session and applied to another. This is especially problematic when limited data is available from new sessions for retraining or adaptation.

Method: Task-Conditioned Latent Alignment (TCLA) framework: 1) Uses autoencoder to learn low-dimensional neural dynamics from source session with sufficient data. 2) For target sessions with limited data, aligns target latent representations to source in task-conditioned manner to transfer learned neural dynamics.

Result: TCLA consistently improves decoding performance across macaque motor and oculomotor datasets compared to baseline methods trained solely on target-session data. Achieves gains in coefficient of determination up to 0.386 for y coordinate velocity decoding in motor dataset.

Conclusion: TCLA provides effective strategy for transferring knowledge from source to target sessions, enabling more robust neural decoding under limited data conditions for invasive BCIs.

Abstract: Cross-session nonstationarity in neural activity recorded by implanted electrodes is a major challenge for invasive Brain-computer interfaces (BCIs), as decoders trained on data from one session often fail to generalize to subsequent sessions. This issue is further exacerbated in practice, as retraining or adapting decoders becomes particularly challenging when only limited data are available from a new session. To address this challenge, we propose a Task-Conditioned Latent Alignment framework (TCLA) for cross-session neural decoding. Building upon an autoencoder architecture, TCLA first learns a low-dimensional representation of neural dynamics from a source session with sufficient data. For target sessions with limited data, TCLA then aligns target latent representations to the source in a task-conditioned manner, enabling effective transfer of learned neural dynamics. We evaluate TCLA on the macaque motor and oculomotor center-out dataset. Compared to baseline methods trained solely on target-session data, TCLA consistently improves decoding performance across datasets and decoding settings, with gains in the coefficient of determination of up to 0.386 for y coordinate velocity decoding in a motor dataset. These results suggest that TCLA provides an effective strategy for transferring knowledge from source to target sessions, enabling more robust neural decoding under conditions with limited data.

</details>


### [101] [Implicit Hypothesis Testing and Divergence Preservation in Neural Network Representations](https://arxiv.org/abs/2601.20477)
*Kadircan Aksoy,Peter Jung,Protim Bhattacharjee*

Main category: cs.LG

TL;DR: Neural classifiers improve generalization by aligning with optimal binary hypothesis testing rules during training, with KL divergence improvements relating to error rate exponents.


<details>
  <summary>Details</summary>
Motivation: To understand how neural networks generalize by analyzing their training dynamics through the lens of binary hypothesis testing, viewing classification as multiple binary tests between class-conditional distributions.

Method: Model classification as a set of binary hypothesis tests between class-conditional distributions of representations. Empirically analyze training trajectories to show networks increasingly align with Neyman-Pearson optimal decision rules via monotonic improvements in KL divergence.

Result: Well-generalizing networks show monotonic improvements in KL divergence along training trajectories, aligning with optimal decision rules. These improvements relate to error rate exponents, providing insights into generalization.

Conclusion: The binary hypothesis testing perspective yields explanations for neural network generalization and suggests possible training or regularization strategies for different network architectures.

Abstract: We study the supervised training dynamics of neural classifiers through the lens of binary hypothesis testing. We model classification as a set of binary tests between class-conditional distributions of representations and empirically show that, along training trajectories, well-generalizing networks increasingly align with Neyman-Pearson optimal decision rules via monotonic improvements in KL divergence that relate to error rate exponents. We finally discuss how this yields an explanation and possible training or regularization strategies for different classes of neural networks.

</details>


### [102] [Modeling Cascaded Delay Feedback for Online Net Conversion Rate Prediction: Benchmark, Insights and Solutions](https://arxiv.org/abs/2601.19965)
*Mingxuan Luo,Guipeng Xv,Sishuo Chen,Xinyu Li,Li Zhang,Zhangming Chan,Xiang-Rong Sheng,Han Zhu,Jian Xu,Bo Zheng,Chen Lin*

Main category: cs.LG

TL;DR: Proposes TESLA framework for NetCVR prediction using cascaded CVR-refund-rate modeling with stage-wise debiasing and delay-time-aware ranking loss, achieving significant improvements over SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Traditional conversion rate (CVR) fails to capture true business value by ignoring refund behavior. NetCVR (purchase without refund) better reflects user satisfaction but involves complex multi-stage delayed feedback with cascaded delays from click to conversion and conversion to refund.

Method: Introduces CASCADE dataset from Taobao and proposes TESLA framework with: 1) CVR-refund-rate cascaded architecture, 2) stage-wise debiasing, and 3) delay-time-aware ranking loss for continuous NetCVR modeling.

Result: TESLA outperforms state-of-the-art methods on CASCADE dataset with 12.41% improvement in RI-AUC and 14.94% improvement in RI-PRAUC for NetCVR prediction.

Conclusion: NetCVR requires continuous modeling due to temporal dynamics, cascaded modeling of CVR and refund rate works better than direct NetCVR modeling, and delay time is a crucial feature for accurate prediction.

Abstract: In industrial recommender systems, conversion rate (CVR) is widely used for traffic allocation, but it fails to fully reflect recommendation effectiveness because it ignores refund behavior. To better capture true user satisfaction and business value, net conversion rate (NetCVR), defined as the probability that a clicked item is purchased and not refunded, has been proposed.Unlike CVR, NetCVR prediction involves a more complex multi-stage cascaded delayed feedback process. The two cascaded delays from click to conversion and from conversion to refund have opposite effects, making traditional CVR modeling methods inapplicable. Moreover, the lack of open-source datasets and online continuous training schemes further hinders progress in this area.To address these challenges, we introduce CASCADE (Cascaded Sequences of Conversion and Delayed Refund), the first large-scale open dataset derived from the Taobao app for online continuous NetCVR prediction. Through an in-depth analysis of CASCADE, we identify three key insights: (1) NetCVR exhibits strong temporal dynamics, necessitating online continuous modeling; (2) cascaded modeling of CVR and refund rate outperforms direct NetCVR modeling; and (3) delay time, which correlates with both CVR and refund rate, is an important feature for NetCVR prediction.Based on these insights, we propose TESLA, a continuous NetCVR modeling framework featuring a CVR-refund-rate cascaded architecture, stage-wise debiasing, and a delay-time-aware ranking loss. Extensive experiments demonstrate that TESLA consistently outperforms state-of-the-art methods on CASCADE, achieving absolute improvements of 12.41 percent in RI-AUC and 14.94 percent in RI-PRAUC on NetCVR prediction. The code and dataset are publicly available at https://github.com/alimama-tech/NetCVR.

</details>


### [103] [A Learning-based Framework for Spatial Impulse Response Compensation in 3D Photoacoustic Computed Tomography](https://arxiv.org/abs/2601.20291)
*Kaiyi Yang,Seonyeong Park,Gangwon Jeong,Hsuan-Kai Huang,Alexander A. Oraevsky,Umberto Villa,Mark A. Anastasio*

Main category: cs.LG

TL;DR: A learned compensation method for 3D photoacoustic computed tomography that maps transducer spatial impulse response-corrupted data to idealized point-like transducer data, enabling fast analytic reconstruction while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Large ultrasound transducers improve PACT sensitivity but degrade spatial resolution when using fast analytic reconstruction methods that ignore spatial impulse responses. Optimization-based methods that account for SIRs are computationally expensive, especially in 3D applications.

Method: Developed a learned SIR compensation framework operating in the data domain, using two neural network models (U-Net and physics-inspired Deconv-Net) to map SIR-corrupted data to idealized point-like transducer data. Includes fast analytical training data generation procedure.

Result: The framework demonstrated resolution improvement and robustness to noise variations, object complexity, and sound speed heterogeneity in virtual studies. Applied to in-vivo breast imaging, it revealed fine structures obscured by SIR-induced artifacts.

Conclusion: The learned SIR compensation method enables accurate 3D PACT image reconstruction with computational efficiency, bridging the gap between fast analytic methods and accurate optimization-based approaches. First demonstration of learned SIR compensation in 3D PACT imaging.

Abstract: Photoacoustic computed tomography (PACT) is a promising imaging modality that combines the advantages of optical contrast with ultrasound detection. Utilizing ultrasound transducers with larger surface areas can improve detection sensitivity. However, when computationally efficient analytic reconstruction methods that neglect the spatial impulse responses (SIRs) of the transducer are employed, the spatial resolution of the reconstructed images will be compromised. Although optimization-based reconstruction methods can explicitly account for SIR effects, their computational cost is generally high, particularly in three-dimensional (3D) applications. To address the need for accurate but rapid 3D PACT image reconstruction, this study presents a framework for establishing a learned SIR compensation method that operates in the data domain. The learned compensation method maps SIR-corrupted PACT measurement data to compensated data that would have been recorded by idealized point-like transducers. Subsequently, the compensated data can be used with a computationally efficient reconstruction method that neglects SIR effects. Two variants of the learned compensation model are investigated that employ a U-Net model and a specifically designed, physics-inspired model, referred to as Deconv-Net. A fast and analytical training data generation procedure is also a component of the presented framework. The framework is rigorously validated in virtual imaging studies, demonstrating resolution improvement and robustness to noise variations, object complexity, and sound speed heterogeneity. When applied to in-vivo breast imaging data, the learned compensation models revealed fine structures that had been obscured by SIR-induced artifacts. To our knowledge, this is the first demonstration of learned SIR compensation in 3D PACT imaging.

</details>


### [104] [Perturbation-Induced Linearization: Constructing Unlearnable Data with Solely Linear Classifiers](https://arxiv.org/abs/2601.19967)
*Jinlin Liu,Wei Chen,Xiaojin Zhang*

Main category: cs.LG

TL;DR: PIL is a computationally efficient method for generating unlearnable examples using linear surrogate models instead of deep neural networks, achieving comparable protection with dramatically reduced computational time.


<details>
  <summary>Details</summary>
Motivation: Existing unlearnable example methods use computationally expensive deep neural networks as surrogate models for perturbation generation. There's a need for more efficient methods to protect data from unauthorized usage in deep learning training.

Method: Perturbation-Induced Linearization (PIL) generates unlearnable examples using only linear surrogate models instead of deep neural networks. The method induces linearization to deep models through perturbations.

Result: PIL achieves comparable or better performance than existing surrogate-based methods while dramatically reducing computational time. The method also provides insights into the mechanism of unlearnable examples through linearization analysis.

Conclusion: PIL offers a practical, efficient approach for data protection through unlearnable examples and provides theoretical insights into why such methods work by revealing the linearization mechanism underlying their effectiveness.

Abstract: Collecting web data to train deep models has become increasingly common, raising concerns about unauthorized data usage. To mitigate this issue, unlearnable examples introduce imperceptible perturbations into data, preventing models from learning effectively. However, existing methods typically rely on deep neural networks as surrogate models for perturbation generation, resulting in significant computational costs. In this work, we propose Perturbation-Induced Linearization (PIL), a computationally efficient yet effective method that generates perturbations using only linear surrogate models. PIL achieves comparable or better performance than existing surrogate-based methods while reducing computational time dramatically. We further reveal a key mechanism underlying unlearnable examples: inducing linearization to deep models, which explains why PIL can achieve competitive results in a very short time. Beyond this, we provide an analysis about the property of unlearnable examples under percentage-based partial perturbation. Our work not only provides a practical approach for data protection but also offers insights into what makes unlearnable examples effective.

</details>


### [105] [BayPrAnoMeta: Bayesian Proto-MAML for Few-Shot Industrial Image Anomaly Detection](https://arxiv.org/abs/2601.19992)
*Soham Sarkar,Tanmay Sen,Sayantan Banerjee*

Main category: cs.LG

TL;DR: BayPrAnoMeta: Bayesian Proto-MAML for few-shot industrial anomaly detection using probabilistic normality models and Bayesian adaptation, outperforming existing methods on MVTec AD benchmark.


<details>
  <summary>Details</summary>
Motivation: Industrial anomaly detection faces extreme class imbalance and scarcity of labeled defective samples, especially in few-shot settings where existing Proto-MAML approaches with deterministic prototypes are insufficient.

Method: Bayesian generalization of Proto-MAML that replaces deterministic prototypes with task-specific probabilistic normality models using Normal-Inverse-Wishart prior, performs inner-loop adaptation via Bayesian posterior predictive likelihood (Student-t distribution), and extends to federated meta-learning with supervised contrastive regularization for heterogeneous clients.

Result: Consistent and significant AUROC improvements over MAML, Proto-MAML, and PatchCore-based methods on MVTec AD benchmark in few-shot anomaly detection settings.

Conclusion: BayPrAnoMeta provides uncertainty-aware, heavy-tailed anomaly scoring essential for robustness in extreme few-shot industrial anomaly detection, with theoretical convergence guarantees in federated settings.

Abstract: Industrial image anomaly detection is a challenging problem owing to extreme class imbalance and the scarcity of labeled defective samples, particularly in few-shot settings. We propose BayPrAnoMeta, a Bayesian generalization of Proto-MAML for few-shot industrial image anomaly detection. Unlike existing Proto-MAML approaches that rely on deterministic class prototypes and distance-based adaptation, BayPrAnoMeta replaces prototypes with task-specific probabilistic normality models and performs inner-loop adaptation via a Bayesian posterior predictive likelihood. We model normal support embeddings with a Normal-Inverse-Wishart (NIW) prior, producing a Student-$t$ predictive distribution that enables uncertainty-aware, heavy-tailed anomaly scoring and is essential for robustness in extreme few-shot settings. We further extend BayPrAnoMeta to a federated meta-learning framework with supervised contrastive regularization for heterogeneous industrial clients and prove convergence to stationary points of the resulting nonconvex objective. Experiments on the MVTec AD benchmark demonstrate consistent and significant AUROC improvements over MAML, Proto-MAML, and PatchCore-based methods in few-shot anomaly detection settings.

</details>


### [106] [Decomposing multimodal embedding spaces with group-sparse autoencoders](https://arxiv.org/abs/2601.20028)
*Chiraag Kaushik,Davis Barch,Andrea Fanelli*

Main category: cs.LG

TL;DR: The paper proposes a new sparse autoencoder method for multimodal embeddings that addresses the "split dictionary" problem, improving modality alignment and interpretability.


<details>
  <summary>Details</summary>
Motivation: Standard sparse autoencoders (SAEs) applied to multimodal embeddings like CLIP/CLAP often learn "split dictionaries" where features are unimodal rather than multimodal, limiting cross-modal interpretability and alignment.

Method: Proposes cross-modal random masking and group-sparse regularization for SAEs, arguing that aligned embeddings should have non-split dictionary decompositions with better modality alignment.

Result: Applied to CLIP (image/text) and CLAP (audio/text) embeddings, the method learns more multimodal dictionaries, reduces dead neurons, improves feature semanticity, and enhances cross-modal interpretability and control.

Conclusion: The proposed SAE adaptation effectively addresses the split dictionary problem in multimodal embeddings, enabling better alignment of concepts across modalities and improved interpretability for cross-modal tasks.

Abstract: The Linear Representation Hypothesis asserts that the embeddings learned by neural networks can be understood as linear combinations of features corresponding to high-level concepts. Based on this ansatz, sparse autoencoders (SAEs) have recently become a popular method for decomposing embeddings into a sparse combination of linear directions, which have been shown empirically to often correspond to human-interpretable semantics. However, recent attempts to apply SAEs to multimodal embedding spaces (such as the popular CLIP embeddings for image/text data) have found that SAEs often learn "split dictionaries", where most of the learned sparse features are essentially unimodal, active only for data of a single modality. In this work, we study how to effectively adapt SAEs for the setting of multimodal embeddings while ensuring multimodal alignment. We first argue that the existence of a split dictionary decomposition on an aligned embedding space implies the existence of a non-split dictionary with improved modality alignment. Then, we propose a new SAE-based approach to multimodal embedding decomposition using cross-modal random masking and group-sparse regularization. We apply our method to popular embeddings for image/text (CLIP) and audio/text (CLAP) data and show that, compared to standard SAEs, our approach learns a more multimodal dictionary while reducing the number of dead neurons and improving feature semanticity. We finally demonstrate how this improvement in alignment of concepts between modalities can enable improvements in the interpretability and control of cross-modal tasks.

</details>


### [107] [SA-PEF: Step-Ahead Partial Error Feedback for Efficient Federated Learning](https://arxiv.org/abs/2601.20738)
*Dawit Kiros Redie,Reza Arablouei,Stefan Werner*

Main category: cs.LG

TL;DR: SA-PEF combines step-ahead correction with partial error feedback to accelerate federated learning convergence under non-IID data, achieving faster target accuracy than standard error feedback.


<details>
  <summary>Details</summary>
Motivation: Standard error feedback (EF) in federated learning suffers from slow residual error decay under non-IID data, causing gradient mismatch and stalled progress in early training rounds.

Method: Proposes step-ahead partial error feedback (SA-PEF) that integrates step-ahead correction with partial error feedback, recovering EF when Î±=0 and step-ahead EF (SAEF) when Î±=1.

Result: Established convergence guarantees for non-convex objectives with Î´-contractive compressors, achieving O((Î·,Î·â‚€TR)â»Â¹) convergence matching Fed-SGD up to constants. Experiments show SA-PEF consistently reaches target accuracy faster than EF.

Conclusion: SA-PEF accelerates early training while maintaining long-term stability, with optimal Î± selection balancing rapid warm-up with stability, making it superior to standard error feedback for federated learning with non-IID data.

Abstract: Biased gradient compression with error feedback (EF) reduces communication in federated learning (FL), but under non-IID data, the residual error can decay slowly, causing gradient mismatch and stalled progress in the early rounds. We propose step-ahead partial error feedback (SA-PEF), which integrates step-ahead (SA) correction with partial error feedback (PEF). SA-PEF recovers EF when the step-ahead coefficient $Î±=0$ and step-ahead EF (SAEF) when $Î±=1$. For non-convex objectives and $Î´$-contractive compressors, we establish a second-moment bound and a residual recursion that guarantee convergence to stationarity under heterogeneous data and partial client participation. The resulting rates match standard non-convex Fed-SGD guarantees up to constant factors, achieving $O((Î·,Î·_0TR)^{-1})$ convergence to a variance/heterogeneity floor with a fixed inner step size. Our analysis reveals a step-ahead-controlled residual contraction $Ï_r$ that explains the observed acceleration in the early training phase. To balance SAEF's rapid warm-up with EF's long-term stability, we select $Î±$ near its theory-predicted optimum. Experiments across diverse architectures and datasets show that SA-PEF consistently reaches target accuracy faster than EF.

</details>


### [108] [Structural Compositional Function Networks: Interpretable Functional Compositions for Tabular Discovery](https://arxiv.org/abs/2601.20037)
*Fang Li*

Main category: cs.LG

TL;DR: StructuralCFN is a novel neural architecture for tabular data that models feature relationships through differentiable structural priors, achieving better performance than gradient-boosted trees while providing human-readable mathematical interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning struggles with tabular data compared to gradient-boosted decision trees, and standard neural networks fail to exploit the inherent structural dependencies between features that define tabular distributions. There's a need for models that maintain scientific interpretability while improving performance.

Method: Proposes Structural Compositional Function Networks (StructuralCFN) with Relation-Aware Inductive Bias via differentiable structural priors. Features are modeled as mathematical compositions of counterparts through Differentiable Adaptive Gating, which discovers optimal activation physics (attention-style filtering vs. inhibitory polarity) for each relationship. Enables Structured Knowledge Integration for domain-specific relational priors.

Result: Evaluated across 18 benchmarks with 10-fold cross-validation, showing statistically significant improvements (p < 0.05) on scientific/clinical datasets (Blood Transfusion, Ozone, WDBC). Provides Intrinsic Symbolic Interpretability by recovering governing "laws" as human-readable mathematical expressions. Maintains compact parameter footprint (300-2,500 parameters), 10x-20x smaller than standard deep baselines.

Conclusion: StructuralCFN successfully addresses the limitations of traditional deep learning on tabular data by explicitly modeling feature relationships through structural priors, achieving superior performance while maintaining interpretability and parameter efficiency.

Abstract: Despite the ubiquity of tabular data in high-stakes domains, traditional deep learning architectures often struggle to match the performance of gradient-boosted decision trees while maintaining scientific interpretability. Standard neural networks typically treat features as independent entities, failing to exploit the inherent manifold structural dependencies that define tabular distributions. We propose Structural Compositional Function Networks (StructuralCFN), a novel architecture that imposes a Relation-Aware Inductive Bias via a differentiable structural prior. StructuralCFN explicitly models each feature as a mathematical composition of its counterparts through Differentiable Adaptive Gating, which automatically discovers the optimal activation physics (e.g., attention-style filtering vs. inhibitory polarity) for each relationship. Our framework enables Structured Knowledge Integration, allowing domain-specific relational priors to be injected directly into the architecture to guide discovery. We evaluate StructuralCFN across a rigorous 10-fold cross-validation suite on 18 benchmarks, demonstrating statistically significant improvements (p < 0.05) on scientific and clinical datasets (e.g., Blood Transfusion, Ozone, WDBC). Furthermore, StructuralCFN provides Intrinsic Symbolic Interpretability: it recovers the governing "laws" of the data manifold as human-readable mathematical expressions while maintaining a compact parameter footprint (300--2,500 parameters) that is over an order of magnitude (10x--20x) smaller than standard deep baselines.

</details>


### [109] [PatchFormer: A Patch-Based Time Series Foundation Model with Hierarchical Masked Reconstruction and Cross-Domain Transfer Learning for Zero-Shot Multi-Horizon Forecasting](https://arxiv.org/abs/2601.20845)
*Olaf Yunus Laitinen Imanov,Derya Umut Kulali,Taner Yilmaz*

Main category: cs.LG

TL;DR: PatchFormer: A patch-based time series foundation model using hierarchical masked reconstruction for self-supervised pretraining and lightweight adapters for efficient transfer, achieving SOTA zero-shot forecasting with 27.3% MSE reduction and 94% less task-specific data.


<details>
  <summary>Details</summary>
Motivation: Time series forecasting requires domain-specific feature engineering and substantial labeled data for each task. Existing approaches lack generalizability across domains and are data-hungry.

Method: Segments time series into patches, learns multiscale temporal representations with learnable aggregation across scales. Uses masked patch reconstruction with dynamic masking (local accuracy + global consistency objectives), followed by cross-domain knowledge distillation. Employs lightweight adapters for efficient transfer.

Result: Achieves state-of-the-art zero-shot multi-horizon forecasting on 24 benchmark datasets across weather, energy, traffic, finance, healthcare. Reduces MSE by 27.3% relative to strong baselines while requiring 94% less task-specific training data. Shows near log-linear scaling up to 100B points and processes length-512 sequences 3.8x faster than full-sequence transformers.

Conclusion: PatchFormer provides an effective foundation model for time series forecasting that generalizes across domains, reduces data requirements, and offers computational efficiency through patch-based architecture and efficient transfer mechanisms.

Abstract: Time series forecasting is a fundamental problem with applications in climate, energy, healthcare, and finance. Many existing approaches require domain-specific feature engineering and substantial labeled data for each task. We introduce PatchFormer, a patch-based time series foundation model that uses hierarchical masked reconstruction for self-supervised pretraining and lightweight adapters for efficient transfer. PatchFormer segments time series into patches and learns multiscale temporal representations with learnable aggregation across temporal scales. Pretraining uses masked patch reconstruction with dynamic masking and objectives that encourage both local accuracy and global consistency, followed by cross-domain knowledge distillation. Experiments on 24 benchmark datasets spanning weather, energy, traffic, finance, and healthcare demonstrate state-of-the-art zero-shot multi-horizon forecasting, reducing mean squared error by 27.3 percent relative to strong baselines while requiring 94 percent less task-specific training data. The model exhibits near log-linear scaling with more pretraining data up to 100 billion points and processes length-512 sequences 3.8x faster than full-sequence transformers.

</details>


### [110] [CiMRAG: Cim-Aware Domain-Adaptive and Noise-Resilient Retrieval-Augmented Generation for Edge-Based LLMs](https://arxiv.org/abs/2601.20041)
*Shih-Hsuan Chiu,Ming-Syan Chen*

Main category: cs.LG

TL;DR: TONEL framework improves noise robustness and domain adaptability for RAG on edge devices using noise-aware embedding learning compatible with CiM hardware constraints.


<details>
  <summary>Details</summary>
Motivation: Deploying RAG on edge devices faces efficiency challenges due to growing profile data, while CiM architectures are susceptible to environmental noise that degrades retrieval precision, especially critical in dynamic multi-domain edge scenarios requiring both accuracy and adaptability.

Method: Task-Oriented Noise-resilient Embedding Learning (TONEL) framework employs a noise-aware projection model to learn task-specific embeddings compatible with CiM hardware constraints for accurate retrieval under noisy conditions.

Result: Extensive experiments on personalization benchmarks demonstrate effectiveness and practicality relative to strong baselines, especially in task-specific noisy scenarios.

Conclusion: TONEL successfully addresses noise robustness and domain adaptability challenges for RAG deployment in noisy edge environments, enabling reliable personalized virtual assistants on edge devices.

Abstract: Personalized virtual assistants powered by large language models (LLMs) on edge devices are attracting growing attention, with Retrieval-Augmented Generation (RAG) emerging as a key method for personalization by retrieving relevant profile data and generating tailored responses. However, deploying RAG on edge devices faces efficiency hurdles due to the rapid growth of profile data, such as user-LLM interactions and recent updates. While Computing-in-Memory (CiM) architectures mitigate this bottleneck by eliminating data movement between memory and processing units via in-situ operations, they are susceptible to environmental noise that can degrade retrieval precision. This poses a critical issue in dynamic, multi-domain edge-based scenarios (e.g., travel, medicine, and law) where both accuracy and adaptability are paramount. To address these challenges, we propose Task-Oriented Noise-resilient Embedding Learning (TONEL), a framework that improves noise robustness and domain adaptability for RAG in noisy edge environments. TONEL employs a noise-aware projection model to learn task-specific embeddings compatible with CiM hardware constraints, enabling accurate retrieval under noisy conditions. Extensive experiments conducted on personalization benchmarks demonstrate the effectiveness and practicality of our methods relative to strong baselines, especially in task-specific noisy scenarios.

</details>


### [111] [Regime-Adaptive Bayesian Optimization via Dirichlet Process Mixtures of Gaussian Processes](https://arxiv.org/abs/2601.20043)
*Yan Zhang,Xuefeng Liu,Sipeng Chen,Sascha Ranftl,Chong Liu,Shibo Li*

Main category: cs.LG

TL;DR: RAMBO: Dirichlet Process Mixture of GPs for Bayesian Optimization in multi-regime problems, automatically discovering latent regimes with adaptive inference and specialized acquisition functions.


<details>
  <summary>Details</summary>
Motivation: Standard BO assumes uniform smoothness across search space, which fails in multi-regime problems like molecular conformation search and drug discovery where different regions have distinct characteristics. Single GP models either oversmooth sharp transitions or hallucinate noise, leading to miscalibrated uncertainty.

Method: RAMBO uses Dirichlet Process Mixture of Gaussian Processes to automatically discover latent regimes during optimization. Each regime is modeled by an independent GP with locally-optimized hyperparameters. The method includes collapsed Gibbs sampling for efficient inference (analytically marginalizing latent functions), adaptive concentration parameter scheduling for coarse-to-fine regime discovery, and acquisition functions that decompose uncertainty into intra-regime and inter-regime components.

Result: Experiments on synthetic benchmarks and real-world applications (molecular conformer optimization, virtual screening for drug discovery, fusion reactor design) demonstrate consistent improvements over state-of-the-art baselines on multi-regime objectives.

Conclusion: RAMBO effectively addresses multi-regime optimization problems by automatically discovering latent regimes, providing better uncertainty calibration and optimization performance compared to standard BO approaches that assume uniform smoothness.

Abstract: Standard Bayesian Optimization (BO) assumes uniform smoothness across the search space an assumption violated in multi-regime problems such as molecular conformation search through distinct energy basins or drug discovery across heterogeneous molecular scaffolds. A single GP either oversmooths sharp transitions or hallucinates noise in smooth regions, yielding miscalibrated uncertainty. We propose RAMBO, a Dirichlet Process Mixture of Gaussian Processes that automatically discovers latent regimes during optimization, each modeled by an independent GP with locally-optimized hyperparameters. We derive collapsed Gibbs sampling that analytically marginalizes latent functions for efficient inference, and introduce adaptive concentration parameter scheduling for coarse-to-fine regime discovery. Our acquisition functions decompose uncertainty into intra-regime and inter-regime components. Experiments on synthetic benchmarks and real-world applications, including molecular conformer optimization, virtual screening for drug discovery, and fusion reactor design, demonstrate consistent improvements over state-of-the-art baselines on multi-regime objectives.

</details>


### [112] [Externally Validated Longitudinal GRU Model for Visit-Level 180-Day Mortality Risk in Metastatic Castration-Resistant Prostate Cancer](https://arxiv.org/abs/2601.20046)
*Javier Mencia-Ledo,Mohammad Noaeen,Zahra Shakeri*

Main category: cs.LG

TL;DR: Researchers developed and validated a 180-day mortality risk prediction model for metastatic castration-resistant prostate cancer using longitudinal clinical data, with GRU and Random Survival Forest models showing best performance.


<details>
  <summary>Details</summary>
Motivation: Metastatic castration-resistant prostate cancer (mCRPC) has poor prognosis and heterogeneous treatment response, creating a need for accurate short-term mortality prediction to enable proactive care planning.

Method: Used longitudinal data from two Phase III cohorts (n=526 and n=640), comparing five architectures: LSTM, GRU, Cox PH, Random Survival Forest, and Logistic Regression. Only visits with observable 180-day outcomes were included, excluding right-censored cases. Selected smallest risk threshold achieving 85% sensitivity floor.

Result: GRU and RSF showed high initial discrimination (C-index: 87% both). In external validation, GRU achieved better calibration (slope: 0.93; intercept: 0.07) and PR-AUC of 0.87. Clinical impact analysis showed median time-in-warning of 151.0 days for true positives vs 59.0 days for false positives, with 18.3 alerts per 100 patient-visits. BMI and systolic blood pressure were strongest predictors.

Conclusion: Longitudinal routine clinical markers can effectively estimate short-horizon mortality risk in mCRPC, supporting proactive care planning over multi-month windows, with GRU models showing particularly strong performance.

Abstract: Metastatic castration-resistant prostate cancer (mCRPC) is a highly aggressive disease with poor prognosis and heterogeneous treatment response. In this work, we developed and externally validated a visit-level 180-day mortality risk model using longitudinal data from two Phase III cohorts (n=526 and n=640). Only visits with observable 180-day outcomes were labeled; right-censored cases were excluded from analysis. We compared five candidate architectures: Long Short-Term Memory, Gated Recurrent Unit (GRU), Cox Proportional Hazards, Random Survival Forest (RSF), and Logistic Regression. For each dataset, we selected the smallest risk-threshold that achieved an 85% sensitivity floor. The GRU and RSF models showed high discrimination capabilities initially (C-index: 87% for both). In external validation, the GRU obtained a higher calibration (slope: 0.93; intercept: 0.07) and achieved an PR-AUC of 0.87. Clinical impact analysis showed a median time-in-warning of 151.0 days for true positives (59.0 days for false positives) and 18.3 alerts per 100 patient-visits. Given late-stage frailty or cachexia and hemodynamic instability, permutation importance ranked BMI and systolic blood pressure as the strongest associations. These results suggest that longitudinal routine clinical markers can estimate short-horizon mortality risk in mCRPC and support proactive care planning over a multi-month window.

</details>


### [113] [Domain Expansion: A Latent Space Construction Framework for Multi-Task Learning](https://arxiv.org/abs/2601.20069)
*Chi-Yao Huang,Khoa Vo,Aayush Atul Verma,Duo Lu,Yezhou Yang*

Main category: cs.LG

TL;DR: Domain Expansion framework prevents latent representation collapse in multi-objective learning by using orthogonal pooling to create mutually orthogonal subspaces for different tasks.


<details>
  <summary>Details</summary>
Motivation: Training single networks with multiple objectives leads to conflicting gradients that degrade shared representations into compromised, suboptimal states - a problem called latent representation collapse.

Method: Introduces Domain Expansion framework with orthogonal pooling mechanism that restructures latent space by assigning each objective to mutually orthogonal subspaces.

Result: Validated across ShapeNet, MPIIGaze, and Rotated MNIST benchmarks on multi-objective problems combining classification with pose and gaze estimation. Prevents collapse and yields interpretable, compositional latent space.

Conclusion: The orthogonal subspace structure prevents representation collapse while creating explicit, interpretable latent spaces where concepts can be directly manipulated.

Abstract: Training a single network with multiple objectives often leads to conflicting gradients that degrade shared representations, forcing them into a compromised state that is suboptimal for any single task--a problem we term latent representation collapse. We introduce Domain Expansion, a framework that prevents these conflicts by restructuring the latent space itself. Our framework uses a novel orthogonal pooling mechanism to construct a latent space where each objective is assigned to a mutually orthogonal subspace. We validate our approach across diverse benchmarks--including ShapeNet, MPIIGaze, and Rotated MNIST--on challenging multi-objective problems combining classification with pose and gaze estimation. Our experiments demonstrate that this structure not only prevents collapse but also yields an explicit, interpretable, and compositional latent space where concepts can be directly manipulated.

</details>


### [114] [Distributional value gradients for stochastic environments](https://arxiv.org/abs/2601.20071)
*Baptiste Debes,Tinne Tuytelaars*

Main category: cs.LG

TL;DR: Distributional Sobolev Training extends distributional RL to model both value functions and their gradients, improving performance in stochastic environments using a cVAE world model and MSMMD for distributional Bellman updates.


<details>
  <summary>Details</summary>
Motivation: Existing gradient-regularized value learning methods like MAGE struggle in stochastic or noisy environments, limiting their applicability. The paper aims to address these limitations by developing a more robust approach that can handle environmental uncertainty.

Method: Extends distributional RL to continuous spaces to model distributions over both value functions and their gradients. Uses a one-step world model (reward and transition distributions) via conditional VAE, employs Max-sliced Maximum Mean Discrepancy for distributional Bellman operator, and proves contraction properties of the Sobolev-augmented Bellman operator.

Result: Proves the Sobolev-augmented Bellman operator is a contraction with unique fixed point, identifies fundamental smoothness trade-off for contraction in gradient-aware RL. Validates method on stochastic RL toy problem and benchmarks on several MuJoCo environments.

Conclusion: Distributional Sobolev Training successfully addresses limitations of previous gradient-regularized methods in stochastic environments by modeling both value distributions and their gradients, providing theoretical guarantees and empirical validation on benchmark tasks.

Abstract: Gradient-regularized value learning methods improve sample efficiency by leveraging learned models of transition dynamics and rewards to estimate return gradients. However, existing approaches, such as MAGE, struggle in stochastic or noisy environments, limiting their applicability. In this work, we address these limitations by extending distributional reinforcement learning on continuous state-action spaces to model not only the distribution over scalar state-action value functions but also over their gradients. We refer to this approach as Distributional Sobolev Training. Inspired by Stochastic Value Gradients (SVG), our method utilizes a one-step world model of reward and transition distributions implemented via a conditional Variational Autoencoder (cVAE). The proposed framework is sample-based and employs Max-sliced Maximum Mean Discrepancy (MSMMD) to instantiate the distributional Bellman operator. We prove that the Sobolev-augmented Bellman operator is a contraction with a unique fixed point, and highlight a fundamental smoothness trade-off underlying contraction in gradient-aware RL. To validate our method, we first showcase its effectiveness on a simple stochastic reinforcement learning toy problem, then benchmark its performance on several MuJoCo environments.

</details>


### [115] [Techno-economic optimization of a heat-pipe microreactor, part II: multi-objective optimization analysis](https://arxiv.org/abs/2601.20079)
*Paul Seurin,Dean Price*

Main category: cs.LG

TL;DR: Multi-objective optimization of heat-pipe microreactors using PEARL algorithm to minimize both power peaking factor and electricity cost under different cost scenarios.


<details>
  <summary>Details</summary>
Motivation: Extend previous single-objective optimization framework to handle trade-offs between safety (power peaking factor) and economics (LCOE) for heat-pipe microreactors in remote deployments.

Method: Used Pareto Envelope Augmented with Reinforcement Learning (PEARL) algorithm for multi-objective optimization, evaluating three cost scenarios for reflectors, with constraints on safety and operations.

Result: Identified consistent design strategies: reduce solid moderator radius, pin pitch, and drum coating angle while increasing fuel height to lower power peaking; minimize costly components and maximize fuel burnup to reduce LCOE across all scenarios.

Conclusion: PEARL shows promise for navigating design trade-offs, though surrogate model discrepancies remain; ongoing work focuses on constraint relaxation and surrogate development improvements.

Abstract: Heat-pipe microreactors (HPMRs) are compact and transportable nuclear power systems exhibiting inherent safety, well-suited for deployment in remote regions where access is limited and reliance on costly fossil fuels is prevalent. In prior work, we developed a design optimization framework that incorporates techno-economic considerations through surrogate modeling and reinforcement learning (RL)-based optimization, focusing solely on minimizing the levelized cost of electricity (LCOE) by using a bottom-up cost estimation approach. In this study, we extend that framework to a multi-objective optimization that uses the Pareto Envelope Augmented with Reinforcement Learning (PEARL) algorithm. The objectives include minimizing both the rod-integrated peaking factor ($F_{Î”h}$) and LCOE -- subject to safety and operational constraints. We evaluate three cost scenarios: (1) a high-cost axial and drum reflectors, (2) a low-cost axial reflector, and (3) low-cost axial and drum reflectors. Our findings indicate that reducing the solid moderator radius, pin pitch, and drum coating angle -- all while increasing the fuel height -- effectively lowers $F_{Î”h}$. Across all three scenarios, four key strategies consistently emerged for optimizing LCOE: (1) minimizing the axial reflector contribution when costly, (2) reducing control drum reliance, (3) substituting expensive tri-structural isotropic (TRISO) fuel with axial reflector material priced at the level of graphite, and (4) maximizing fuel burnup. While PEARL demonstrates promise in navigating trade-offs across diverse design scenarios, discrepancies between surrogate model predictions and full-order simulations remain. Further improvements are anticipated through constraint relaxation and surrogate development, constituting an ongoing area of investigation.

</details>


### [116] [Quantization-Aware Distillation for NVFP4 Inference Accuracy Recovery](https://arxiv.org/abs/2601.20088)
*Meng Xin,Sweta Priyadarshi,Jingyu Xin,Bilal Kartal,Aditya Vavre,Asma Kuriparambil Thekkumpate,Zijia Chen,Ameya Sunil Mahabaleshwarkar,Ido Shahaf,Akhiad Bercovich,Kinjal Patel,Suguna Varshini Velury,Chenjie Luo,Zhiyu Cheng,Jenny Chen,Chen-Han Yu,Wei Ping,Oleg Rybakov,Nima Tajbakhsh,Oluwatobi Olabiyi,Dusan Stosic,Di Wu,Song Han,Eric Chung,Sharath Turuvekere Sreenivas,Bryan Catanzaro,Yoshi Suhara,Tijmen Blankevoort,Huizi Mao*

Main category: cs.LG

TL;DR: QAD (quantization-aware distillation) effectively recovers accuracy for 4-bit quantized LLMs/VLMs using KL divergence loss from full-precision teachers, outperforming traditional QAT in stability and data efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional quantization-aware training (QAT) suffers from engineering complexity and training instability for modern LLMs trained through multi-stage pipelines (SFT, RL, model merging). QAD addresses these issues while being robust to data quality/coverage limitations.

Method: QAD distills a full-precision teacher model into a quantized student model using KL divergence loss. It leverages distillation rather than direct quantization training, making it more stable and data-efficient for complex post-trained models.

Result: QAD consistently recovers near-BF16 accuracy across multiple post-trained models including AceReason Nemotron, Nemotron 3 Nano, Nemotron Nano V2, Nemotron Nano V2 VL (VLM), and Llama Nemotron Super v1.

Conclusion: QAD provides an effective and practical solution for quantizing modern LLMs/VLMs, overcoming limitations of traditional QAT through better stability, reduced engineering complexity, and robustness to data constraints.

Abstract: This technical report presents quantization-aware distillation (QAD) and our best practices for recovering accuracy of NVFP4-quantized large language models (LLMs) and vision-language models (VLMs). QAD distills a full-precision teacher model into a quantized student model using a KL divergence loss. While applying distillation to quantized models is not a new idea, we observe key advantages of QAD for today's LLMs: 1. It shows remarkable effectiveness and stability for models trained through multi-stage post-training pipelines, including supervised fine-tuning (SFT), reinforcement learning (RL), and model merging, where traditional quantization-aware training (QAT) suffers from engineering complexity and training instability; 2. It is robust to data quality and coverage, enabling accuracy recovery without full training data. We evaluate QAD across multiple post-trained models including AceReason Nemotron, Nemotron 3 Nano, Nemotron Nano V2, Nemotron Nano V2 VL (VLM), and Llama Nemotron Super v1, showing consistent recovery to near-BF16 accuracy.

</details>


### [117] [In-Context Reinforcement Learning From Suboptimal Historical Data](https://arxiv.org/abs/2601.20116)
*Juncheng Dong,Moyang Guo,Ethan X. Fang,Zhuoran Yang,Vahid Tarokh*

Main category: cs.LG

TL;DR: DIT (Decision Importance Transformer) is a transformer-based framework for in-context reinforcement learning that improves over standard imitation learning when trained on suboptimal offline data by incorporating value function estimation and importance weighting.


<details>
  <summary>Details</summary>
Motivation: Transformers excel at in-context learning, but standard autoregressive training on suboptimal RL trajectories leads to imitation learning and poor performance. Need a method that can learn optimal policies from suboptimal historical data in an in-context manner.

Method: Two-stage approach: 1) Train transformer-based value function to estimate advantage functions of behavior policies, 2) Train transformer-based policy via weighted maximum likelihood estimation, using value function outputs as weights to steer suboptimal policies toward optimal ones.

Result: DIT achieves superior performance on both bandit and Markov Decision Process problems, especially when offline dataset contains suboptimal historical data, outperforming standard imitation learning approaches.

Conclusion: DIT successfully enables in-context reinforcement learning from suboptimal data by emulating actor-critic algorithms in a transformer framework, demonstrating the potential of transformers for offline RL with suboptimal demonstrations.

Abstract: Transformer models have achieved remarkable empirical successes, largely due to their in-context learning capabilities. Inspired by this, we explore training an autoregressive transformer for in-context reinforcement learning (ICRL). In this setting, we initially train a transformer on an offline dataset consisting of trajectories collected from various RL tasks, and then fix and use this transformer to create an action policy for new RL tasks. Notably, we consider the setting where the offline dataset contains trajectories sampled from suboptimal behavioral policies. In this case, standard autoregressive training corresponds to imitation learning and results in suboptimal performance. To address this, we propose the Decision Importance Transformer(DIT) framework, which emulates the actor-critic algorithm in an in-context manner. In particular, we first train a transformer-based value function that estimates the advantage functions of the behavior policies that collected the suboptimal trajectories. Then we train a transformer-based policy via a weighted maximum likelihood estimation loss, where the weights are constructed based on the trained value function to steer the suboptimal policies to the optimal ones. We conduct extensive experiments to test the performance of DIT on both bandit and Markov Decision Process problems. Our results show that DIT achieves superior performance, particularly when the offline dataset contains suboptimal historical data.

</details>


### [118] [A Reinforcement Learning Based Universal Sequence Design for Polar Codes](https://arxiv.org/abs/2601.20118)
*David Kin Wai Ho,Arman Fazeli,Mohamad M. Mansour,Louay M. A. Jalloul*

Main category: cs.LG

TL;DR: RL-based universal Polar code sequence design framework for 6G that scales to 2048 length, achieving competitive performance with 5G NR and up to 0.2dB gain over baseline.


<details>
  <summary>Details</summary>
Motivation: To advance Polar code design for 6G applications by developing an extensible and adaptable framework that works across diverse channel conditions and decoding strategies, while scaling to practical code lengths suitable for standardization.

Method: Reinforcement learning-based universal sequence design framework with three key scaling elements: (i) physical law constrained learning using universal partial order property, (ii) limited lookahead evaluation exploiting weak long-term influence of decisions, and (iii) joint multi-configuration optimization for learning efficiency.

Result: Achieves competitive performance relative to 5G NR sequence across all (N,K) configurations, with up to 0.2 dB gain over beta-expansion baseline at N=2048. Scales to code lengths up to 2048, making it suitable for standardization.

Conclusion: The RL-based framework successfully enables scalable Polar code sequence design for 6G applications, demonstrating practical viability through key innovations in constrained learning, evaluation optimization, and multi-configuration training.

Abstract: To advance Polar code design for 6G applications, we develop a reinforcement learning-based universal sequence design framework that is extensible and adaptable to diverse channel conditions and decoding strategies. Crucially, our method scales to code lengths up to $2048$, making it suitable for use in standardization. Across all $(N,K)$ configurations supported in 5G, our approach achieves competitive performance relative to the NR sequence adopted in 5G and yields up to a 0.2 dB gain over the beta-expansion baseline at $N=2048$. We further highlight the key elements that enabled learning at scale: (i) incorporation of physical law constrained learning grounded in the universal partial order property of Polar codes, (ii) exploitation of the weak long term influence of decisions to limit lookahead evaluation, and (iii) joint multi-configuration optimization to increase learning efficiency.

</details>


### [119] [Going NUTS with ADVI: Exploring various Bayesian Inference techniques with Facebook Prophet](https://arxiv.org/abs/2601.20120)
*Jovan Krajevski,Biljana Tojtovska Ribarski*

Main category: cs.LG

TL;DR: The paper presents a PyMC-based reimplementation of Facebook Prophet to overcome limitations of the original implementation, enabling flexible Bayesian inference methods beyond the default options.


<details>
  <summary>Details</summary>
Motivation: The authors encountered limitations with Facebook Prophet's built-in inference methods (only MAP estimation via L-BFGS-B and MCMC via NUTS) and insufficient API flexibility for custom modeling. They needed alternative Bayesian inference techniques and more extensible framework.

Method: Developed a complete reimplementation of the Prophet model in PyMC, enabling extension of the base model and evaluation/comparison of multiple Bayesian inference methods including full MCMC techniques, MAP estimation, and Variational inference.

Result: The paper analyzes implementation of different Bayesian inference techniques on time-series forecasting problems, discussing sampling approach, convergence diagnostics, forecasting metrics, computational efficiency, and identifies issues for future work.

Conclusion: The PyMC-based implementation successfully addresses the limitations of Facebook Prophet by providing flexible Bayesian inference capabilities and extensible framework for custom modeling, though some issues remain for future work.

Abstract: Since its introduction, Facebook Prophet has attracted positive attention from both classical statisticians and the Bayesian statistics community. The model provides two built-in inference methods: maximum a posteriori estimation using the L-BFGS-B algorithm, and Markov Chain Monte Carlo (MCMC) sampling via the No-U-Turn Sampler (NUTS). While exploring various time-series forecasting problems using Bayesian inference with Prophet, we encountered limitations stemming from the inability to apply alternative inference techniques beyond those provided by default. Additionally, the fluent API design of Facebook Prophet proved insufficiently flexible for implementing our custom modeling ideas. To address these shortcomings, we developed a complete reimplementation of the Prophet model in PyMC, which enables us to extend the base model and evaluate and compare multiple Bayesian inference methods. In this paper, we present our PyMC-based implementation and analyze in detail the implementation of different Bayesian inference techniques. We consider full MCMC techniques, MAP estimation and Variational inference techniques on a time-series forecasting problem. We discuss in details the sampling approach, convergence diagnostics, forecasting metrics as well as their computational efficiency and detect possible issues which will be addressed in our future work.

</details>


### [120] [Membership Inference Attacks Against Fine-tuned Diffusion Language Models](https://arxiv.org/abs/2601.20125)
*Yuetian Chen,Kaiyuan Zhang,Yuntao Du,Edoardo Stoppa,Charles Fleming,Ashish Kundu,Bruno Ribeiro,Ninghui Li*

Main category: cs.LG

TL;DR: First systematic investigation of Membership Inference Attack vulnerabilities in Diffusion Language Models, introducing SAMA attack that achieves 30% AUC improvement over baselines by exploiting DLMs' multiple maskable configurations.


<details>
  <summary>Details</summary>
Motivation: Diffusion Language Models (DLMs) are promising alternatives to autoregressive models but their susceptibility to privacy leakage via Membership Inference Attacks remains critically underexplored. Unlike autoregressive models with single fixed prediction patterns, DLMs' multiple maskable configurations create exponential attack opportunities that need investigation.

Method: Introduces SAMA (Subset-Aggregated Membership Attack) which addresses sparse signal challenge through robust aggregation. SAMA samples masked subsets across progressive densities and applies sign-based statistics effective despite heavy-tailed noise. Uses inverse-weighted aggregation prioritizing sparse masks' cleaner signals to transform sparse memorization detection into robust voting mechanism.

Result: Experiments on nine datasets show SAMA achieves 30% relative AUC improvement over best baseline, with up to 8 times improvement at low false positive rates. Reveals significant, previously unknown vulnerabilities in DLMs.

Conclusion: Findings demonstrate substantial privacy vulnerabilities in Diffusion Language Models that were previously unknown, necessitating development of tailored privacy defenses for DLMs.

Abstract: Diffusion Language Models (DLMs) represent a promising alternative to autoregressive language models, using bidirectional masked token prediction. Yet their susceptibility to privacy leakage via Membership Inference Attacks (MIA) remains critically underexplored. This paper presents the first systematic investigation of MIA vulnerabilities in DLMs. Unlike the autoregressive models' single fixed prediction pattern, DLMs' multiple maskable configurations exponentially increase attack opportunities. This ability to probe many independent masks dramatically improves detection chances. To exploit this, we introduce SAMA (Subset-Aggregated Membership Attack), which addresses the sparse signal challenge through robust aggregation. SAMA samples masked subsets across progressive densities and applies sign-based statistics that remain effective despite heavy-tailed noise. Through inverse-weighted aggregation prioritizing sparse masks' cleaner signals, SAMA transforms sparse memorization detection into a robust voting mechanism. Experiments on nine datasets show SAMA achieves 30% relative AUC improvement over the best baseline, with up to 8 times improvement at low false positive rates. These findings reveal significant, previously unknown vulnerabilities in DLMs, necessitating the development of tailored privacy defenses.

</details>


### [121] [Spectral Ghost in Representation Learning: from Component Analysis to Self-Supervised Learning](https://arxiv.org/abs/2601.20154)
*Bo Dai,Na Li,Dale Schuurmans*

Main category: cs.LG

TL;DR: The paper develops a principled theoretical foundation for self-supervised representation learning, revealing the spectral essence of successful SSL algorithms and providing a unified framework for analysis and algorithm design.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of diverse self-supervised learning methods lacks unified theoretical understanding, making algorithm design unclear and practical use unjustified. The absence of a principled framework hampers representation learning development.

Method: The authors theoretically investigate representation sufficiency from a spectral representation perspective, revealing the spectral essence of existing successful SSL algorithms, which leads to a unified framework for understanding and analysis.

Result: The paper establishes a principled foundation for representation learning that explains why successful SSL algorithms work and provides a unified framework that can inspire more efficient and practical algorithm development.

Conclusion: The spectral perspective provides a unified understanding of SSL, enabling principled algorithm design and justifying the use of representation learning methods in real-world applications.

Abstract: Self-supervised learning (SSL) have improved empirical performance by unleashing the power of unlabeled data for practical applications. Specifically, SSL extracts the representation from massive unlabeled data, which will be transferred to a plenty of down streaming tasks with limited data. The significant improvement on diverse applications of representation learning has attracted increasing attention, resulting in a variety of dramatically different self-supervised learning objectives for representation extraction, with an assortment of learning procedures, but the lack of a clear and unified understanding. Such an absence hampers the ongoing development of representation learning, leaving a theoretical understanding missing, principles for efficient algorithm design unclear, and the use of representation learning methods in practice unjustified. The urgency for a unified framework is further motivated by the rapid growth in representation learning methods. In this paper, we are therefore compelled to develop a principled foundation of representation learning. We first theoretically investigate the sufficiency of the representation from a spectral representation view, which reveals the spectral essence of the existing successful SSL algorithms and paves the path to a unified framework for understanding and analysis. Such a framework work also inspires the development of more efficient and easy-to-use representation learning algorithms with principled way in real-world applications.

</details>


### [122] [PASS: Ambiguity Guided Subsets for Scalable Classical and Quantum Constrained Clustering](https://arxiv.org/abs/2601.20157)
*Pedro Chumpitaz-Flores,My Duong,Ying Mao,Kaixun Hua*

Main category: cs.LG

TL;DR: PASS is a pairwise-constraints and ambiguity-driven subset selection framework that enables scalable clustering with must-link and cannot-link constraints by selecting informative subsets of data points.


<details>
  <summary>Details</summary>
Motivation: Current pairwise-constrained clustering methods struggle with data scalability, especially in niche applications like quantum or quantum-hybrid clustering, due to the added complexity of ML and CL constraints.

Method: PASS collapses must-link constraints into pseudo-points and offers two selection rules: 1) constraint-aware margin rule that collects near-boundary points and all detected CL violations, and 2) information-geometric rule that scores points via Fisher-Rao distance from soft assignment posteriors, then selects highest-information subsets under a budget.

Result: PASS attains competitive SSE at substantially lower cost than exact or penalty-based methods, and remains effective in regimes where prior approaches fail across diverse benchmarks.

Conclusion: PASS provides a scalable, high-quality clustering solution that preserves ML and CL constraint satisfaction through intelligent subset selection, addressing the scalability limitations of existing pairwise-constrained clustering methods.

Abstract: Pairwise-constrained clustering augments unsupervised partitioning with side information by enforcing must-link (ML) and cannot-link (CL) constraints between specific samples, yielding labelings that respect known affinities and separations. However, ML and CL constraints add an extra layer of complexity to the clustering problem, with current methods struggling in data scalability, especially in niche applications like quantum or quantum-hybrid clustering. We propose PASS, a pairwise-constraints and ambiguity-driven subset selection framework that preserves ML and CL constraints satisfaction while allowing scalable, high-quality clustering solution. PASS collapses ML constraints into pseudo-points and offers two selectors: a constraint-aware margin rule that collects near-boundary points and all detected CL violations, and an information-geometric rule that scores points via a Fisher-Rao distance derived from soft assignment posteriors, then selects the highest-information subset under a simple budget. Across diverse benchmarks, PASS attains competitive SSE at substantially lower cost than exact or penalty-based methods, and remains effective in regimes where prior approaches fail.

</details>


### [123] [What's the plan? Metrics for implicit planning in LLMs and their application to rhyme generation and question answering](https://arxiv.org/abs/2601.20164)
*Jim Maar,Denis Paperno,Callum Stuart McDougall,Neel Nanda*

Main category: cs.LG

TL;DR: Researchers propose simple techniques to detect implicit planning in language models, showing it exists even in smaller models (from 1B parameters) and can be manipulated through vector steering.


<details>
  <summary>Details</summary>
Motivation: To develop simpler, scalable methods for assessing implicit planning in language models, building on prior qualitative work that suggested models show planning behavior during token generation.

Method: Use case studies on rhyme poetry generation and question answering with vector steering techniques; manipulate generation by applying steering vectors at the end of preceding lines to affect future tokens.

Result: Implicit planning is a universal mechanism present in models as small as 1B parameters; generated rhymes and answers can be manipulated through vector steering, affecting intermediate token generation.

Conclusion: The methodology provides a widely applicable way to study implicit planning in LLMs, with implications for understanding model capabilities and informing AI safety and control decisions.

Abstract: Prior work suggests that language models, while trained on next token prediction, show implicit planning behavior: they may select the next token in preparation to a predicted future token, such as a likely rhyming word, as supported by a prior qualitative study of Claude 3.5 Haiku using a cross-layer transcoder. We propose much simpler techniques for assessing implicit planning in language models. With case studies on rhyme poetry generation and question answering, we demonstrate that our methodology easily scales to many models. Across models, we find that the generated rhyme (e.g. "-ight") or answer to a question ("whale") can be manipulated by steering at the end of the preceding line with a vector, affecting the generation of intermediate tokens leading up to the rhyme or answer word. We show that implicit planning is a universal mechanism, present in smaller models than previously thought, starting from 1B parameters. Our methodology offers a widely applicable direct way to study implicit planning abilities of LLMs. More broadly, understanding planning abilities of language models can inform decisions in AI safety and control.

</details>


### [124] [Local Duality for Sparse Support Vector Machines](https://arxiv.org/abs/2601.20170)
*Penghe Zhang,Naihua Xiu,Houduo Qi*

Main category: cs.LG

TL;DR: This paper develops a local duality theory for sparse support vector machines (SSVMs) using cardinality minimization, showing they are exactly dual to 0/1-loss SVMs and explaining their empirical advantages over hinge-loss and ramp-loss SVMs.


<details>
  <summary>Details</summary>
Motivation: Sparse SVMs have shown empirical advantages but lack theoretical justification when derived by adding cardinality functions to convex SVM dual problems. The paper aims to fill this theoretical gap and explain why SSVMs outperform other SVM variants.

Method: Develops a local duality theory for SSVMs, proves they are exactly dual to 0/1-loss SVMs, establishes linear representer theorem for local solutions, and shows connections between hSVM, rSVM, and 0/1-loss SVM solutions under specific conditions.

Result: Proves SSVM is dual to 0/1-loss SVM, shows linear representer theorem holds for local solutions, demonstrates convergence of hSVM global solutions to 0/1-loss SVM local solutions, and proves 0/1-loss SVM local minimizers are also rSVM local minimizers.

Conclusion: The theoretical framework explains SSVM's empirical superiority, provides hyperparameter selection guidelines for hSVM/rSVM, and numerical tests on real datasets demonstrate advantages of working with locally nice solutions.

Abstract: Due to the rise of cardinality minimization in optimization, sparse support vector machines (SSVMs) have attracted much attention lately and show certain empirical advantages over convex SVMs. A common way to derive an SSVM is to add a cardinality function such as $\ell_0$-norm to the dual problem of a convex SVM. However, this process lacks theoretical justification. This paper fills the gap by developing a local duality theory for such an SSVM formulation and exploring its relationship with the hinge-loss SVM (hSVM) and the ramp-loss SVM (rSVM). In particular, we prove that the derived SSVM is exactly the dual problem of the 0/1-loss SVM, and the linear representer theorem holds for their local solutions. The local solution of SSVM also provides guidelines on selecting hyperparameters of hSVM and rSVM. {Under specific conditions, we show that a sequence of global solutions of hSVM converges to a local solution of 0/1-loss SVM. Moreover, a local minimizer of 0/1-loss SVM is a local minimizer of rSVM.} This explains why a local solution induced by SSVM outperforms hSVM and rSVM in the prior empirical study. We further conduct numerical tests on real datasets and demonstrate potential advantages of SSVM by working with locally nice solutions proposed in this paper.

</details>


### [125] [Loss Landscape Geometry and the Learning of Symmetries: Or, What Influence Functions Reveal About Robust Generalization](https://arxiv.org/abs/2601.20172)
*James Amarel,Robyn Miller,Nicolas Hengartner,Benjamin Migliori,Emily Casleton,Alexei Skurikhin,Earl Lawrence,Gerd J. Kunde*

Main category: cs.LG

TL;DR: A new diagnostic method measures how neural PDE emulators internalize physical symmetries by analyzing gradient coherence between symmetry-related states, going beyond simple equivariance tests.


<details>
  <summary>Details</summary>
Motivation: To understand whether neural emulators of PDE solution operators truly internalize physical symmetries, not just exhibit forward-pass equivariance, and to develop a diagnostic that can assess if training dynamics properly couple physically equivalent configurations.

Method: Introduces an influence-based diagnostic that measures the propagation of parameter updates between symmetry-related states using metric-weighted overlap of loss gradients evaluated along group orbits. This probes the local geometry of the learned loss landscape and assesses whether learning dynamics couple physically equivalent configurations.

Result: Applied to autoregressive fluid flow emulators, the diagnostic shows that orbit-wise gradient coherence provides the mechanism for learning to generalize over symmetry transformations and indicates when training selects a symmetry-compatible basin.

Conclusion: The paper presents a novel technique for evaluating whether surrogate models have internalized symmetry properties of known solution operators, going beyond simple forward-pass equivariance tests by examining learning dynamics and loss landscape geometry.

Abstract: We study how neural emulators of partial differential equation solution operators internalize physical symmetries by introducing an influence-based diagnostic that measures the propagation of parameter updates between symmetry-related states, defined as the metric-weighted overlap of loss gradients evaluated along group orbits. This quantity probes the local geometry of the learned loss landscape and goes beyond forward-pass equivariance tests by directly assessing whether learning dynamics couple physically equivalent configurations. Applying our diagnostic to autoregressive fluid flow emulators, we show that orbit-wise gradient coherence provides the mechanism for learning to generalize over symmetry transformations and indicates when training selects a symmetry compatible basin. The result is a novel technique for evaluating if surrogate models have internalized symmetry properties of the known solution operator.

</details>


### [126] [MAPLE: Self-supervised Learning-Enhanced Nonlinear Dimensionality Reduction for Visual Analysis](https://arxiv.org/abs/2601.20173)
*Zeyang Huang,Takanori Fujiwara,Angelos Chatzimparmpas,Wandrille Duchemin,Andreas Kerren*

Main category: cs.LG

TL;DR: MAPLE is a new nonlinear dimensionality reduction method that improves upon UMAP by using self-supervised learning and maximum manifold capacity representations to better model complex manifolds in high-dimensional data.


<details>
  <summary>Details</summary>
Motivation: The authors aim to enhance UMAP's manifold modeling capabilities, particularly for high-dimensional data with substantial intra-cluster variance and curved manifold structures (common in biological or image data), where existing methods may struggle to properly separate clusters and reveal fine subcluster structures.

Method: MAPLE uses a self-supervised learning approach with maximum manifold capacity representations (MMCRs) that compress variances among locally similar data points while amplifying variance among dissimilar points, effectively untangling complex manifolds.

Result: Qualitative and quantitative evaluations show MAPLE produces clearer visual cluster separations and finer subcluster resolution than UMAP while maintaining comparable computational cost.

Conclusion: MAPLE successfully improves upon UMAP's manifold modeling capabilities, offering better visualization and analysis of complex high-dimensional datasets with curved manifold structures and substantial intra-cluster variance.

Abstract: We present a new nonlinear dimensionality reduction method, MAPLE, that enhances UMAP by improving manifold modeling. MAPLE employs a self-supervised learning approach to more efficiently encode low-dimensional manifold geometry. Central to this approach are maximum manifold capacity representations (MMCRs), which help untangle complex manifolds by compressing variances among locally similar data points while amplifying variance among dissimilar data points. This design is particularly effective for high-dimensional data with substantial intra-cluster variance and curved manifold structures, such as biological or image data. Our qualitative and quantitative evaluations demonstrate that MAPLE can produce clearer visual cluster separations and finer subcluster resolution than UMAP while maintaining comparable computational cost.

</details>


### [127] [NeuraLSP: An Efficient and Rigorous Neural Left Singular Subspace Preconditioner for Conjugate Gradient Methods](https://arxiv.org/abs/2601.20174)
*Alexander Benanti,Xi Han,Hong Qin*

Main category: cs.LG

TL;DR: NeuraLSP: A neural preconditioner using left singular subspace of near-nullspace vectors to accelerate PDE solving, achieving up to 53% speedup with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing neural preconditioners for PDEs suffer from rank inflation and suboptimal convergence rates when aggregating discretized system matrices into graphs, limiting their effectiveness.

Method: Proposes NeuraLSP, a neural preconditioner combined with a novel loss metric that leverages the left singular subspace of the system matrix's near-nullspace vectors, compressing spectral information into a fixed low-rank operator.

Result: The method exhibits theoretical guarantees and empirical robustness to rank inflation, achieving up to 53% speedup across diverse families of PDEs.

Conclusion: NeuraLSP provides an effective neural preconditioning approach with both theoretical foundations and practical performance improvements for solving PDE systems.

Abstract: Numerical techniques for solving partial differential equations (PDEs) are integral for many fields across science and engineering. Such techniques usually involve solving large, sparse linear systems, where preconditioning methods are critical. In recent years, neural methods, particularly graph neural networks (GNNs), have demonstrated their potential through accelerated convergence. Nonetheless, to extract connective structures, existing techniques aggregate discretized system matrices into graphs, and suffer from rank inflation and a suboptimal convergence rate. In this paper, we articulate NeuraLSP, a novel neural preconditioner combined with a novel loss metric that leverages the left singular subspace of the system matrix's near-nullspace vectors. By compressing spectral information into a fixed low-rank operator, our method exhibits both theoretical guarantees and empirical robustness to rank inflation, affording up to a 53% speedup. Besides the theoretical guarantees for our newly-formulated loss function, our comprehensive experimental results across diverse families of PDEs also substantiate the aforementioned theoretical advances.

</details>


### [128] [Causal-Driven Feature Evaluation for Cross-Domain Image Classification](https://arxiv.org/abs/2601.20176)
*Chen Cheng,Ang Li*

Main category: cs.LG

TL;DR: The paper proposes a causal evaluation framework for OOD generalization that measures feature necessity and sufficiency, outperforming traditional domain-invariance approaches.


<details>
  <summary>Details</summary>
Motivation: Current OOD generalization methods focus on domain-invariant representations, but invariant features aren't necessarily causally effective for prediction. There's a need for better evaluation of learned representations under distribution shift.

Method: Introduces a causal perspective with explicit segment-level framework that directly measures causal effectiveness (necessity and sufficiency) of representations across domains, rather than relying solely on invariance.

Result: Experiments on multi-domain benchmarks show consistent improvements in OOD performance, especially under challenging domain shifts, demonstrating the value of causal evaluation.

Conclusion: Causal evaluation of representations (measuring necessity and sufficiency) provides a more faithful criterion for OOD generalization than invariance alone, leading to better robust performance.

Abstract: Out-of-distribution (OOD) generalization remains a fundamental challenge in real-world classification, where test distributions often differ substantially from training data. Most existing approaches pursue domain-invariant representations, implicitly assuming that invariance implies reliability. However, features that are invariant across domains are not necessarily causally effective for prediction.
  In this work, we revisit OOD classification from a causal perspective and propose to evaluate learned representations based on their necessity and sufficiency under distribution shift. We introduce an explicit segment-level framework that directly measures causal effectiveness across domains, providing a more faithful criterion than invariance alone.
  Experiments on multi-domain benchmarks demonstrate consistent improvements in OOD performance, particularly under challenging domain shifts, highlighting the value of causal evaluation for robust generalization.

</details>


### [129] [On the Computational Complexity of Performative Prediction](https://arxiv.org/abs/2601.20180)
*Ioannis Anagnostides,Rohan Chauhan,Ioannis Panageas,Tuomas Sandholm,Jingming Yan*

Main category: cs.LG

TL;DR: PPAD-completeness for computing performatively stable points when performative effects are strong (Ï>1), even for simple settings like quadratic loss with linear shifts.


<details>
  <summary>Details</summary>
Motivation: Performative prediction models where predictions shift data distributions, but complexity in the strong performative effects regime (Ï>1) was previously unknown.

Method: Established computational complexity results using PPAD-completeness proofs, extending to convex domains and strategic classification cases.

Result: Sharp phase transition: computing Îµ-performatively stable points is PPAD-complete when Ï=1+O(Îµ), even for quadratic loss with linear shifts. Strategic classification local optimum is PLS-hard.

Conclusion: Strong performative effects make finding stable points computationally intractable (PPAD-complete), analogous to finding Nash equilibria, even in simple settings.

Abstract: Performative prediction captures the phenomenon where deploying a predictive model shifts the underlying data distribution. While simple retraining dynamics are known to converge linearly when the performative effects are weak ($Ï< 1$), the complexity in the regime $Ï> 1$ was hitherto open. In this paper, we establish a sharp phase transition: computing an $Îµ$-performatively stable point is PPAD-complete -- and thus polynomial-time equivalent to Nash equilibria in general-sum games -- even when $Ï= 1 + O(Îµ)$. This intractability persists even in the ostensibly simple setting with a quadratic loss function and linear distribution shifts. One of our key technical contributions is to extend this PPAD-hardness result to general convex domains, which is of broader interest in the complexity of variational inequalities. Finally, we address the special case of strategic classification, showing that computing a strategic local optimum is PLS-hard.

</details>


### [130] [Meta-Cognitive Reinforcement Learning with Self-Doubt and Recovery](https://arxiv.org/abs/2601.20193)
*Zhipeng Zhang,Wenting Ma,Kai Li,Meng Guo,Lei Yang,Wei Yu,Hongji Cui,Yichen Zhang,Mo Zhang,Jinzhe Lin,Zhenjie Yao*

Main category: cs.LG

TL;DR: Meta-cognitive RL framework enables agents to self-assess learning reliability using VPES-driven meta-trust, achieving better robustness against reward corruption.


<details>
  <summary>Details</summary>
Motivation: Existing robust RL methods lack self-awareness about learning reliability, causing them to either overreact to noise or fail catastrophically when uncertainty accumulates.

Method: Proposes meta-cognitive RL with meta-trust variable driven by Value Prediction Error Stability (VPES), enabling fail-safe regulation and gradual trust recovery mechanisms.

Result: Experiments on continuous-control benchmarks with reward corruption show higher average returns and significantly reduced late-stage training failures compared to strong baselines.

Conclusion: Meta-cognitive control with self-assessment and recovery mechanisms provides superior robustness in RL, addressing limitations of traditional robust RL approaches.

Abstract: Robust reinforcement learning methods typically focus on suppressing unreliable experiences or corrupted rewards, but they lack the ability to reason about the reliability of their own learning process. As a result, such methods often either overreact to noise by becoming overly conservative or fail catastrophically when uncertainty accumulates.
  In this work, we propose a meta-cognitive reinforcement learning framework that enables an agent to assess, regulate, and recover its learning behavior based on internally estimated reliability signals. The proposed method introduces a meta-trust variable driven by Value Prediction Error Stability (VPES), which modulates learning dynamics via fail-safe regulation and gradual trust recovery.
  Experiments on continuous-control benchmarks with reward corruption demonstrate that recovery-enabled meta-cognitive control achieves higher average returns and significantly reduces late-stage training failures compared to strong robustness baselines.

</details>


### [131] [DeRaDiff: Denoising Time Realignment of Diffusion Models](https://arxiv.org/abs/2601.20198)
*Ratnavibusena Don Shahain Manujith,Yang Zhang,Teoh Tze Tzun,Kenji Kawaguchi*

Main category: cs.LG

TL;DR: DeRaDiff enables real-time adjustment of regularization strength during diffusion model sampling without retraining, eliminating expensive hyperparameter sweeps for alignment.


<details>
  <summary>Details</summary>
Motivation: Current diffusion model alignment methods require expensive hyperparameter sweeps to find optimal regularization strength, balancing alignment quality against reward hacking. This computational cost is prohibitive.

Method: DeRaDiff uses denoising time realignment that modulates regularization strength during sampling by replacing reverse step reference distribution with geometric mixture of aligned and reference posteriors, enabling single parameter (lambda) control.

Result: DeRaDiff approximates models aligned from scratch at different regularization strengths across multiple text-image alignment and quality metrics, eliminating need for expensive alignment sweeps.

Conclusion: The method provides efficient search for optimal regularization strength, substantially reducing computational costs while maintaining alignment quality.

Abstract: Recent advances align diffusion models with human preferences to increase aesthetic appeal and mitigate artifacts and biases. Such methods aim to maximize a conditional output distribution aligned with higher rewards whilst not drifting far from a pretrained prior. This is commonly enforced by KL (Kullback Leibler) regularization. As such, a central issue still remains: how does one choose the right regularization strength? Too high of a strength leads to limited alignment and too low of a strength leads to "reward hacking". This renders the task of choosing the correct regularization strength highly non-trivial. Existing approaches sweep over this hyperparameter by aligning a pretrained model at multiple regularization strengths and then choose the best strength. Unfortunately, this is prohibitively expensive. We introduce DeRaDiff, a denoising time realignment procedure that, after aligning a pretrained model once, modulates the regularization strength during sampling to emulate models trained at other regularization strengths without any additional training or finetuning. Extending decoding-time realignment from language to diffusion models, DeRaDiff operates over iterative predictions of continuous latents by replacing the reverse step reference distribution by a geometric mixture of an aligned and reference posterior, thus giving rise to a closed form update under common schedulers and a single tunable parameter, lambda, for on the fly control. Our experiments show that across multiple text image alignment and image-quality metrics, our method consistently provides a strong approximation for models aligned entirely from scratch at different regularization strengths. Thus, our method yields an efficient way to search for the optimal strength, eliminating the need for expensive alignment sweeps and thereby substantially reducing computational costs.

</details>


### [132] [Minimum-Cost Network Flow with Dual Predictions](https://arxiv.org/abs/2601.20203)
*Zhiyang Chen,Hailong Yao,Xia Yin*

Main category: cs.LG

TL;DR: First minimum-cost network flow algorithm with dual prediction that provides provable speedups using learned predictions, achieving 12.74Ã— and 1.64Ã— average speedup on traffic networks and chip escape routing.


<details>
  <summary>Details</summary>
Motivation: To improve performance of classic algorithms using machine-learned predictions, specifically for minimum-cost network flow problems where predictions can provide provable benefits.

Method: Augments Îµ-relaxation minimum-cost flow algorithm with dual prediction, providing time complexity bounds based on infinity norm prediction error and sample complexity bounds for PAC-learning the prediction.

Result: Theoretical guarantees for consistency and robustness, with empirical validation showing 12.74Ã— average speedup on traffic networks and 1.64Ã— average speedup on chip escape routing applications.

Conclusion: Machine-learned predictions can significantly accelerate minimum-cost flow algorithms with provable guarantees, demonstrating practical value in real-world applications like traffic optimization and chip design.

Abstract: Recent work has shown that machine-learned predictions can provably improve the performance of classic algorithms. In this work, we propose the first minimum-cost network flow algorithm augmented with a dual prediction. Our method is based on a classic minimum-cost flow algorithm, namely $\varepsilon$-relaxation. We provide time complexity bounds in terms of the infinity norm prediction error, which is both consistent and robust. We also prove sample complexity bounds for PAC-learning the prediction. We empirically validate our theoretical results on two applications of minimum-cost flow, i.e., traffic networks and chip escape routing, in which we learn a fixed prediction, and a feature-based neural network model to infer the prediction, respectively. Experimental results illustrate $12.74\times$ and $1.64\times$ average speedup on two applications.

</details>


### [133] [Hyperparameter Transfer with Mixture-of-Expert Layers](https://arxiv.org/abs/2601.20205)
*Tianze Jiang,Blake Bordelon,Cengiz Pehlevan,Boris Hanin*

Main category: cs.LG

TL;DR: The paper proposes a new parameterization method for Mixture-of-Experts (MoE) transformer models that enables reliable hyperparameter transfer across different model scales, validated from 51M to over 2B parameters.


<details>
  <summary>Details</summary>
Motivation: MoE layers help scale neural networks by decoupling total parameters from activated parameters, but introduce training complexity due to new router weights requiring hyperparameter tuning and architectural scale dimensions that must be chosen carefully.

Method: Proposes a novel parameterization for transformer models with MoE layers when scaling model width, depth, number of experts, and expert size, justified by dynamical mean-field theory (DMFT) analysis.

Result: Empirical validation shows the parameterization enables reliable hyperparameter transfer across models from 51M to over 2B total parameters, and allows using HPs from small models on short token horizons to train larger models on longer horizons.

Conclusion: The proposed parameterization makes hyperparameter selection cheap and reliable for MoE models, addressing the complexity challenges introduced by sparse MoE architectures during training.

Abstract: Mixture-of-Experts (MoE) layers have emerged as an important tool in scaling up modern neural networks by decoupling total trainable parameters from activated parameters in the forward pass for each token. However, sparse MoEs add complexity to training due to (i) new trainable parameters (router weights) that, like all other parameter groups, require hyperparameter (HP) tuning; (ii) new architecture scale dimensions (number of and size of experts) that must be chosen and potentially taken large. To make HP selection cheap and reliable, we propose a new parameterization for transformer models with MoE layers when scaling model width, depth, number of experts, and expert (hidden) size. Our parameterization is justified by a novel dynamical mean-field theory (DMFT) analysis. When varying different model dimensions trained at a fixed token budget, we find empirically that our parameterization enables reliable HP transfer across models from 51M to over 2B total parameters. We further take HPs identified from sweeping small models on a short token horizon to train larger models on longer horizons and report performant model behaviors.

</details>


### [134] [Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning](https://arxiv.org/abs/2601.20209)
*Jinyang Wu,Shuo Yang,Changpeng Yang,Yuhao Shen,Shuai Zhang,Zhengqi Wen,Jianhua Tao*

Main category: cs.LG

TL;DR: Spark is a reinforcement learning framework for LLM agents that selectively branches exploration at critical decision states to improve sample efficiency and generalization in long-horizon tasks.


<details>
  <summary>Details</summary>
Motivation: Training LLM agents for long-horizon tasks is challenging due to scarcity of high-quality trajectories and inefficient resource allocation in existing methods that waste computation on trivial steps while failing to guarantee sample quality.

Method: Spark uses strategic policy-aware exploration via key-state dynamic branching, which selectively activates adaptive branching at critical decision points to probe promising trajectories, leveraging the agent's intrinsic decision-making signals for precise resource allocation.

Result: Experiments across diverse tasks (including embodied planning) show Spark achieves superior success rates with significantly fewer training samples and exhibits robust generalization in unseen scenarios.

Conclusion: Spark's selective branching approach enables more efficient exploration, reduces dependence on human priors, and allows LLM agents to achieve better performance with fewer resources while maintaining strong generalization capabilities.

Abstract: Reinforcement learning has empowered large language models to act as intelligent agents, yet training them for long-horizon tasks remains challenging due to the scarcity of high-quality trajectories, especially under limited resources. Existing methods typically scale up rollout sizes and indiscriminately allocate computational resources among intermediate steps. Such attempts inherently waste substantial computation budget on trivial steps while failing to guarantee sample quality. To address this, we propose \textbf{Spark} (\textbf{S}trategic \textbf{P}olicy-\textbf{A}ware explo\textbf{R}ation via \textbf{K}ey-state dynamic branching), a novel framework that selectively branches at critical decision states for resource-efficient exploration. Our key insight is to activate adaptive branching exploration at critical decision points to probe promising trajectories, thereby achieving precise resource allocation that prioritizes sampling quality over blind coverage. This design leverages the agent's intrinsic decision-making signals to reduce dependence on human priors, enabling the agent to autonomously expand exploration and achieve stronger generalization. Experiments across diverse tasks (e.g., embodied planning), demonstrate that \textsc{Spark} achieves superior success rates with significantly fewer training samples, exhibiting robust generalization even in unseen scenarios.

</details>


### [135] [An Accounting Identity for Algorithmic Fairness](https://arxiv.org/abs/2601.20217)
*Hadi Elzayn,Jacob Goldin*

Main category: cs.LG

TL;DR: The paper establishes a mathematical identity linking model accuracy and fairness criteria, showing they are complements rather than tradeoffs in binary prediction.


<details>
  <summary>Details</summary>
Motivation: To provide a unified mathematical framework that clarifies the relationship between accuracy and fairness in predictive models, moving beyond simplistic tradeoff narratives.

Method: Derives an accounting identity for predictive models that mathematically links accuracy with common fairness criteria, focusing on globally calibrated models and analyzing weighted sums of miscalibration within groups and error imbalance across groups.

Result: The identity reveals that accuracy and fairness are complements in binary prediction: increasing accuracy shrinks the total unfairness budget. Experiments show fairness interventions often substitute between fairness violations and can expand the unfairness budget when reducing accuracy.

Conclusion: The framework provides a principled way to understand fairness-accuracy relationships, showing impossibility results as special cases and extending naturally to non-binary outcomes where additional information can relax fairness incompatibilities.

Abstract: We derive an accounting identity for predictive models that links accuracy with common fairness criteria. The identity shows that for globally calibrated models, the weighted sums of miscalibration within groups and error imbalance across groups is equal to a "total unfairness budget." For binary outcomes, this budget is the model's mean-squared error times the difference in group prevalence across outcome classes. The identity nests standard impossibility results as special cases, while also describing inherent tradeoffs when one or more fairness measures are not perfectly satisfied. The results suggest that accuracy and fairness are best viewed as complements in binary prediction tasks: increasing accuracy necessarily shrinks the total unfairness budget and vice-versa. Experiments on benchmark data confirm the theory and show that many fairness interventions largely substitute between fairness violations, and when they reduce accuracy they tend to expand the total unfairness budget. The results extend naturally to prediction tasks with non-binary outcomes, illustrating how additional outcome information can relax fairness incompatibilities and identifying conditions under which the binary-style impossibility does and does not extend to regression tasks.

</details>


### [136] [ProFlow: Zero-Shot Physics-Consistent Sampling via Proximal Flow Guidance](https://arxiv.org/abs/2601.20227)
*Zichao Yu,Ming Li,Wenyi Zhang,Difan Zou,Weiguo Gao*

Main category: cs.LG

TL;DR: ProFlow: A zero-shot proximal guidance framework for physics-consistent sampling that infers physical fields from sparse observations using pre-trained generative priors without retraining, alternating between terminal optimization and interpolation steps.


<details>
  <summary>Details</summary>
Motivation: Existing deep generative models for inverse problems in computational physics struggle to enforce hard physical constraints without costly retraining or disrupting learned generative priors. There's a critical need for sampling mechanisms that can reconcile strict physical consistency and observational fidelity with pre-trained statistical priors.

Method: ProFlow uses a two-step scheme: (1) terminal optimization step projects flow predictions onto physically and observationally consistent sets via proximal minimization; (2) interpolation step maps refined states back to the generative trajectory to maintain consistency with learned flow probability paths. This admits a Bayesian interpretation as sequential MAP updates.

Result: Comprehensive benchmarks on Poisson, Helmholtz, Darcy, and viscous Burgers' equations show ProFlow achieves superior physical and observational consistency, and more accurate distributional statistics compared to state-of-the-art diffusion- and flow-based baselines.

Conclusion: ProFlow provides an effective zero-shot framework for physics-consistent sampling that strictly satisfies PDE constraints while maintaining fidelity to both observations and pre-trained generative priors, addressing a fundamental challenge in computational physics inverse problems.

Abstract: Inferring physical fields from sparse observations while strictly satisfying partial differential equations (PDEs) is a fundamental challenge in computational physics. Recently, deep generative models offer powerful data-driven priors for such inverse problems, yet existing methods struggle to enforce hard physical constraints without costly retraining or disrupting the learned generative prior. Consequently, there is a critical need for a sampling mechanism that can reconcile strict physical consistency and observational fidelity with the statistical structure of the pre-trained prior. To this end, we present ProFlow, a proximal guidance framework for zero-shot physics-consistent sampling, defined as inferring solutions from sparse observations using a fixed generative prior without task-specific retraining. The algorithm employs a rigorous two-step scheme that alternates between: (\romannumeral1) a terminal optimization step, which projects the flow prediction onto the intersection of the physically and observationally consistent sets via proximal minimization; and (\romannumeral2) an interpolation step, which maps the refined state back to the generative trajectory to maintain consistency with the learned flow probability path. This procedure admits a Bayesian interpretation as a sequence of local maximum a posteriori (MAP) updates. Comprehensive benchmarks on Poisson, Helmholtz, Darcy, and viscous Burgers' equations demonstrate that ProFlow achieves superior physical and observational consistency, as well as more accurate distributional statistics, compared to state-of-the-art diffusion- and flow-based baselines.

</details>


### [137] [Parametric and Generative Forecasts of Day-Ahead Market Curves for Storage Optimization](https://arxiv.org/abs/2601.20226)
*Julian Gutierrez,Redouane Silvente*

Main category: cs.LG

TL;DR: Two ML frameworks for EPEX SPOT market: 1) Fast parametric model for hourly curve forecasting, 2) Generative models for synthetic order-level scenarios. Both used to optimize storage strategies and analyze price compression effects.


<details>
  <summary>Details</summary>
Motivation: To develop practical forecasting and optimization tools for the EPEX SPOT day-ahead electricity market, enabling better storage strategy optimization and revenue analysis while understanding market dynamics like price compression.

Method: 1) Parametric model: Low-dimensional grid-robust representation using min/max volumes + Chebyshev polynomial for elastic segment. 2) Generative models: Learn joint distribution of 24-hour order-level submissions conditioned on weather/fuel variables to create synthetic daily scenarios.

Result: The parametric model enables daily use with low error and interpretability. Generative models provide comprehensive analysis of aggregated supply/demand curves. Both frameworks optimize price-making storage strategies, quantify revenue distributions, and reveal price compression effects (lower peaks, higher off-peak levels, diminishing returns with capacity expansion).

Conclusion: The paper presents complementary ML approaches for electricity market forecasting and storage optimization, offering both practical daily tools and comprehensive analytical frameworks that reveal important market dynamics like price compression effects.

Abstract: We present two machine learning frameworks for forecasting aggregated curves and optimizing storage in the EPEX SPOT day-ahead market. First, a fast parametric model forecasts hourly demand and supply curves in a low-dimensional and grid-robust representation, with minimum and maximum volumes combined with a Chebyshev polynomial for the elastic segment. The model enables daily use with low error and clear interpretability. Second, for a more comprehensive analysis, though less suited to daily operation, we employ generative models that learn the joint distribution of 24-hour order-level submissions given weather and fuel variables. These models generate synthetic daily scenarios of individual buy and sell orders, which, once aggregated, yield hourly supply and demand curves. Based on these forecasts, we optimize a price-making storage strategy, quantify revenue distributions, and highlight the price-compression effect with lower peaks, higher off-peak levels, and diminishing returns as capacity expands.

</details>


### [138] [Certificate-Guided Pruning for Stochastic Lipschitz Optimization](https://arxiv.org/abs/2601.20231)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: CGP introduces certificate-guided pruning for Lipschitz function optimization with explicit optimality certificates and measurable progress guarantees, achieving near-optimal sample complexity with adaptive extensions.


<details>
  <summary>Details</summary>
Motivation: Existing adaptive discretization methods for black-box optimization of Lipschitz functions under noisy evaluations lack explicit certificates of optimality and measurable progress guarantees, making it difficult to determine when to stop optimization or assess solution quality.

Method: Certificate-Guided Pruning (CGP) maintains an explicit active set of potentially optimal points using confidence-adjusted Lipschitz envelopes, certifying points outside this set as suboptimal. Three extensions: CGP-Adaptive learns Lipschitz constant online, CGP-TR scales to high dimensions via trust regions with local certificates, and CGP-Hybrid switches to Gaussian Process refinement when local smoothness is detected.

Result: Under a margin condition with near-optimality dimension Î±, CGP achieves sample complexity of Ã•(Îµ^{-(2+Î±)}). Experiments on 12 benchmarks (d âˆˆ [2, 100]) show CGP variants match or exceed strong baselines while providing principled stopping criteria via certificate volume.

Conclusion: CGP provides a principled framework for Lipschitz optimization with explicit optimality certificates, measurable progress guarantees, and near-optimal sample complexity, with practical extensions for adaptive parameter learning, high-dimensional scaling, and hybrid approaches.

Abstract: We study black-box optimization of Lipschitz functions under noisy evaluations. Existing adaptive discretization methods implicitly avoid suboptimal regions but do not provide explicit certificates of optimality or measurable progress guarantees. We introduce \textbf{Certificate-Guided Pruning (CGP)}, which maintains an explicit \emph{active set} $A_t$ of potentially optimal points via confidence-adjusted Lipschitz envelopes. Any point outside $A_t$ is certifiably suboptimal with high probability, and under a margin condition with near-optimality dimension $Î±$, we prove $\Vol(A_t)$ shrinks at a controlled rate yielding sample complexity $\tildeO(\varepsilon^{-(2+Î±)})$. We develop three extensions: CGP-Adaptive learns $L$ online with $O(\log T)$ overhead; CGP-TR scales to $d > 50$ via trust regions with local certificates; and CGP-Hybrid switches to GP refinement when local smoothness is detected. Experiments on 12 benchmarks ($d \in [2, 100]$) show CGP variants match or exceed strong baselines while providing principled stopping criteria via certificate volume.

</details>


### [139] [Robust SDE Parameter Estimation Under Missing Time Information Setting](https://arxiv.org/abs/2601.20268)
*Long Van Tran,Truyen Tran,Phuoc Nguyen*

Main category: cs.LG

TL;DR: Novel framework that simultaneously recovers temporal order and estimates SDE parameters when temporal ordering information is corrupted or missing.


<details>
  <summary>Details</summary>
Motivation: Existing SDE parameter estimation methods require accurate timestamped observations, but fail when temporal ordering is corrupted, missing, or deliberately hidden for privacy reasons.

Method: Exploits asymmetries between forward and backward SDE processes, uses score-matching to infer correct temporal order between observation pairs, recovers total order via sorting, then estimates SDE parameters using maximum likelihood from reconstructed sequence.

Result: Demonstrated effectiveness through extensive experiments on synthetic and real-world datasets, extending parameter estimation to settings with missing temporal order.

Conclusion: Broadens applicability of SDE parameter estimation in sensitive domains where temporal information may be corrupted or deliberately hidden.

Abstract: Recent advances in stochastic differential equations (SDEs) have enabled robust modeling of real-world dynamical processes across diverse domains, such as finance, health, and systems biology. However, parameter estimation for SDEs typically relies on accurately timestamped observational sequences. When temporal ordering information is corrupted, missing, or deliberately hidden (e.g., for privacy), existing estimation methods often fail. In this paper, we investigate the conditions under which temporal order can be recovered and introduce a novel framework that simultaneously reconstructs temporal information and estimates SDE parameters. Our approach exploits asymmetries between forward and backward processes, deriving a score-matching criterion to infer the correct temporal order between pairs of observations. We then recover the total order via a sorting procedure and estimate SDE parameters from the reconstructed sequence using maximum likelihood. Finally, we conduct extensive experiments on synthetic and real-world datasets to demonstrate the effectiveness of our method, extending parameter estimation to settings with missing temporal order and broadening applicability in sensitive domains.

</details>


### [140] [The Forecast After the Forecast: A Post-Processing Shift in Time Series](https://arxiv.org/abs/2601.20280)
*Daojun Liang,Qi Li,Yinglong Wang,Jing Chen,Hu Zhang,Xiaoxiao Cui,Qizheng Wang,Shuo Li*

Main category: cs.LG

TL;DR: Î´-Adapter is a lightweight post-processing module that boosts deployed time series forecasters without retraining, improving accuracy, uncertainty calibration, and interpretability through input nudging and output correction.


<details>
  <summary>Details</summary>
Motivation: As forecasting models approach diminishing returns in accuracy, there's an underexplored opportunity in post-processing to improve accuracy and uncertainty without retraining or modifying deployed backbone models, addressing the "last-mile gap" in time-series forecasting.

Method: Î´-Adapter learns tiny, bounded modules at two interfaces: input nudging (soft edits to covariates) and output residual correction. It provides local descent guarantees, O(Î´) drift bounds, and compositional stability. It can also act as a feature selector via sparse horizon-aware masks and as a distribution calibrator with Quantile Calibrator and Conformal Corrector for uncertainty measurement.

Result: Experiments across diverse backbones and datasets show that Î´-Adapter improves accuracy and calibration with negligible compute and no interface changes.

Conclusion: Î´-Adapter offers a practical, architecture-agnostic solution to enhance deployed time series forecasters through lightweight post-processing, addressing accuracy, uncertainty calibration, and interpretability without the need for model retraining.

Abstract: Time series forecasting has long been dominated by advances in model architecture, with recent progress driven by deep learning and hybrid statistical techniques. However, as forecasting models approach diminishing returns in accuracy, a critical yet underexplored opportunity emerges: the strategic use of post-processing. In this paper, we address the last-mile gap in time-series forecasting, which is to improve accuracy and uncertainty without retraining or modifying a deployed backbone. We propose $Î´$-Adapter, a lightweight, architecture-agnostic way to boost deployed time series forecasters without retraining. $Î´$-Adapter learns tiny, bounded modules at two interfaces: input nudging (soft edits to covariates) and output residual correction. We provide local descent guarantees, $O(Î´)$ drift bounds, and compositional stability for combined adapters. Meanwhile, it can act as a feature selector by learning a sparse, horizon-aware mask over inputs to select important features, thereby improving interpretability. In addition, it can also be used as a distribution calibrator to measure uncertainty. Thus, we introduce a Quantile Calibrator and a Conformal Corrector that together deliver calibrated, personalized intervals with finite-sample coverage. Our experiments across diverse backbones and datasets show that $Î´$-Adapter improves accuracy and calibration with negligible compute and no interface changes.

</details>


### [141] [HE-SNR: Uncovering Latent Logic via Entropy for Guiding Mid-Training on SWE-BENCH](https://arxiv.org/abs/2601.20255)
*Yueyang Wang,Jiawei Fu,Baolong Bi,Xili Wang,Xiaoqing Liu*

Main category: cs.LG

TL;DR: The paper introduces HE-SNR, a novel metric for guiding LLM mid-training on software engineering tasks, addressing limitations of standard metrics like perplexity that suffer from long-context issues and poor correlation with SWE-bench performance.


<details>
  <summary>Details</summary>
Motivation: Current metrics like perplexity are inadequate for guiding LLM mid-training on software engineering tasks due to the "Long-Context Tax" and weak correlation with downstream SWE-bench performance, creating a critical gap in effective training guidance.

Method: Proposes the Entropy Compression Hypothesis (intelligence as structuring uncertainty into low-order Entropy-Compressed States), rigorous data filtering, and formulates HE-SNR (High-Entropy Signal-to-Noise Ratio) metric based on fine-grained entropy analysis.

Result: HE-SNR demonstrates superior robustness and predictive power when validated on industrial-scale Mixture-of-Experts models across varying context windows (32K/128K), outperforming standard metrics.

Conclusion: The work provides both theoretical foundation (Entropy Compression Hypothesis) and practical tools (HE-SNR metric) for optimizing LLM potential in complex engineering domains, bridging the gap in effective mid-training guidance.

Abstract: SWE-bench has emerged as the premier benchmark for evaluating Large Language Models on complex software engineering tasks. While these capabilities are fundamentally acquired during the mid-training phase and subsequently elicited during Supervised Fine-Tuning (SFT), there remains a critical deficit in metrics capable of guiding mid-training effectively. Standard metrics such as Perplexity (PPL) are compromised by the "Long-Context Tax" and exhibit weak correlation with downstream SWE performance. In this paper, we bridge this gap by first introducing a rigorous data filtering strategy. Crucially, we propose the Entropy Compression Hypothesis, redefining intelligence not by scalar Top-1 compression, but by the capacity to structure uncertainty into Entropy-Compressed States of low orders ("reasonable hesitation"). Grounded in this fine-grained entropy analysis, we formulate a novel metric, HE-SNR (High-Entropy Signal-to-Noise Ratio). Validated on industrial-scale Mixture-of-Experts (MoE) models across varying context windows (32K/128K), our approach demonstrates superior robustness and predictive power. This work provides both the theoretical foundation and practical tools for optimizing the latent potential of LLMs in complex engineering domains.

</details>


### [142] [Cheap2Rich: A Multi-Fidelity Framework for Data Assimilation and System Identification of Multiscale Physics -- Rotating Detonation Engines](https://arxiv.org/abs/2601.20295)
*Yuxuan Bao,Jan Zajac,Megan Powers,Venkat Raman,J. Nathan Kutz*

Main category: cs.LG

TL;DR: Cheap2Rich is a multi-fidelity data assimilation framework that reconstructs high-fidelity states from sparse sensor data using fast low-fidelity models with learned interpretable discrepancy corrections, demonstrated on rotating detonation engines.


<details>
  <summary>Details</summary>
Motivation: Bridging the sim2real gap between computationally inexpensive models and complex physical systems, especially in multi-scale engineering applications where reduced-order models only capture dominant dynamics.

Method: Multi-scale data assimilation framework combining fast low-fidelity priors with learned, interpretable discrepancy corrections to reconstruct high-fidelity state spaces from sparse sensor histories.

Result: Successfully reconstructs high-fidelity RDE states from sparse measurements while isolating physically meaningful discrepancy dynamics associated with injector-driven effects.

Conclusion: Presents a general multi-fidelity framework for data assimilation and system identification in complex multi-scale systems, enabling rapid design exploration, real-time monitoring/control with interpretable discrepancy dynamics.

Abstract: Bridging the sim2real gap between computationally inexpensive models and complex physical systems remains a central challenge in machine learning applications to engineering problems, particularly in multi-scale settings where reduced-order models typically capture only dominant dynamics. In this work, we present Cheap2Rich, a multi-scale data assimilation framework that reconstructs high-fidelity state spaces from sparse sensor histories by combining a fast low-fidelity prior with learned, interpretable discrepancy corrections. We demonstrate the performance on rotating detonation engines (RDEs), a challenging class of systems that couple detonation-front propagation with injector-driven unsteadiness, mixing, and stiff chemistry across disparate scales. Our approach successfully reconstructs high-fidelity RDE states from sparse measurements while isolating physically meaningful discrepancy dynamics associated with injector-driven effects. The results highlight a general multi-fidelity framework for data assimilation and system identification in complex multi-scale systems, enabling rapid design exploration and real-time monitoring and control while providing interpretable discrepancy dynamics. Code for this project is is available at: github.com/kro0l1k/Cheap2Rich.

</details>


### [143] [C2:Cross learning module enhanced decision transformer with Constraint-aware loss for auto-bidding](https://arxiv.org/abs/2601.20257)
*Jinren Ding,Xuejian Xu,Shen Jiang,Zhitong Hao,Jinhui Yang,Peng Jiang*

Main category: cs.LG

TL;DR: C2 enhances Decision Transformer for auto-bidding with cross-attention for better sequence correlation and constraint-aware loss for selective learning of optimal behaviors.


<details>
  <summary>Details</summary>
Motivation: Decision Transformer shows promise for generative auto-bidding but has limitations: insufficient cross-correlation modeling among state, action, and RTG sequences, and indiscriminate learning of both optimal and suboptimal behaviors.

Method: Proposes C2 framework with two innovations: (1) Cross Learning Block using cross-attention to strengthen inter-sequence correlation modeling, (2) Constraint-aware Loss incorporating budget and CPA constraints for selective learning of optimal trajectories.

Result: Extensive offline evaluations on AuctionNet dataset show consistent performance gains (up to 3.23% over state-of-the-art GAVE) across diverse budget settings; ablation studies verify complementary synergy of CLB and CL.

Conclusion: C2 demonstrates superiority in auto-bidding by addressing DT's limitations through enhanced cross-correlation modeling and selective learning of optimal behaviors.

Abstract: Decision Transformer (DT) shows promise for generative auto-bidding by capturing temporal dependencies, but suffers from two critical limitations: insufficient cross-correlation modeling among state, action, and return-to-go (RTG) sequences, and indiscriminate learning of optimal/suboptimal behaviors. To address these, we propose C2, a novel framework enhancing DT with two core innovations: (1) a Cross Learning Block (CLB) via cross-attention to strengthen inter-sequence correlation modeling; (2) a Constraint-aware Loss (CL) incorporating budget and Cost-Per-Acquisition (CPA) constraints for selective learning of optimal trajectories. Extensive offline evaluations on the AuctionNet dataset demonstrate consistent performance gains (up to 3.23\% over state-of-the-art GAVE) across diverse budget settings; ablation studies verify the complementary synergy of CLB and CL, confirming C2's superiority in auto-bidding. The code for reproducing our results is available at: https://github.com/Dingjinren/C2.

</details>


### [144] [Truthfulness Despite Weak Supervision: Evaluating and Training LLMs Using Peer Prediction](https://arxiv.org/abs/2601.20299)
*Tianyi Alex Qiu,Micah Carroll,Cameron Allen*

Main category: cs.LG

TL;DR: Peer prediction method for LLM evaluation and post-training uses game theory to elicit honest answers without ground truth labels, resisting deception even with weak supervision.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluation and post-training rely on supervision that's often unavailable for difficult tasks, especially with frontier models. Models can exploit imperfect supervision, leading to deceptive results. Game-theoretic mechanism design offers underutilized solutions for eliciting honest answers with weak supervision.

Method: Peer prediction method based on mutual predictability, rewarding honest and informative answers over deceptive ones without requiring ground truth labels. Uses game-theoretic incentive compatibility principles from mechanism design literature.

Result: Method shows theoretical guarantees and empirical validation on models up to 405B parameters. Training an 8B model with peer prediction recovers most truthfulness drop from malicious finetuning. Peer prediction exhibits inverse scaling: resistance to deception strengthens as capability gap between experts and participants widens, enabling reliable evaluation of strong models with weak supervision.

Conclusion: Peer prediction provides a robust alternative to LLM-as-a-Judge for evaluating and post-training LLMs, especially effective when dealing with deceptive models and large capability gaps, working well even with weak supervision.

Abstract: The evaluation and post-training of large language models (LLMs) rely on supervision, but strong supervision for difficult tasks is often unavailable, especially when evaluating frontier models. In such cases, models are demonstrated to exploit evaluations built on such imperfect supervision, leading to deceptive results. However, underutilized in LLM research, a wealth of mechanism design research focuses on game-theoretic incentive compatibility, i.e., eliciting honest and informative answers with weak supervision. Drawing from this literature, we introduce the peer prediction method for model evaluation and post-training. It rewards honest and informative answers over deceptive and uninformative ones, using a metric based on mutual predictability and without requiring ground truth labels. We demonstrate the method's effectiveness and resistance to deception, with both theoretical guarantees and empirical validation on models with up to 405B parameters. We show that training an 8B model with peer prediction-based reward recovers most of the drop in truthfulness due to prior malicious finetuning, even when the reward is produced by a 0.135B language model with no finetuning. On the evaluation front, in contrast to LLM-as-a-Judge which requires strong and trusted judges, we discover an inverse scaling property in peer prediction, where, surprisingly, resistance to deception is strengthened as the capability gap between the experts and participants widens, enabling reliable evaluation of strong models with weak supervision. In particular, LLM-as-a-Judge become worse than random guess when facing deceptive models 5-20x the judge's size, while peer prediction thrives when such gaps are large, including in cases with over 100x size difference.

</details>


### [145] [Memory Retrieval in Transformers: Insights from The Encoding Specificity Principle](https://arxiv.org/abs/2601.20282)
*Viet Hung Dinh,Ming Ding,Youyang Qu,Kanchana Thilakarathna*

Main category: cs.LG

TL;DR: This paper investigates how attention layers in transformer-based LLMs function as memory mechanisms, drawing parallels to human memory retrieval processes, and identifies specific neurons that encode context-defining keywords for applications like machine unlearning.


<details>
  <summary>Details</summary>
Motivation: Despite increasing regulatory pressures for transparency and accountability in AI, the specific role of attention layers in transformer-based LLMs remains underexplored. There's a need to understand how these models implement memory mechanisms to enable better explainability and applications like privacy-preserving machine unlearning.

Method: The study draws on psychology and computational psycholinguistics research, linking Transformer attention to cue-based retrieval in human memory. It applies the Encoding Specificity Principle to hypothesize that retrieval cues are instantiated as keywords, then provides converging evidence for this hypothesis and isolates specific neurons within attention layers that encode these context-defining keywords.

Result: The research demonstrates that attention layers implement memory mechanisms where queries encode retrieval context, keys index memory traces, attention weights quantify cue-trace similarity, and values carry encoded content. It identifies specific neurons that selectively encode and facilitate retrieval of context-defining keywords, enabling keyword extraction for downstream applications.

Conclusion: Attention layers in transformer-based LLMs function as sophisticated memory systems analogous to human memory retrieval, with specific neurons encoding context-defining keywords. This understanding provides a foundation for improved XAI techniques and enables practical applications like targeted machine unlearning by manipulating these keyword-encoding neurons.

Abstract: While explainable artificial intelligence (XAI) for large language models (LLMs) remains an evolving field with many unresolved questions, increasing regulatory pressures have spurred interest in its role in ensuring transparency, accountability, and privacy-preserving machine unlearning. Despite recent advances in XAI have provided some insights, the specific role of attention layers in transformer based LLMs remains underexplored. This study investigates the memory mechanisms instantiated by attention layers, drawing on prior research in psychology and computational psycholinguistics that links Transformer attention to cue based retrieval in human memory. In this view, queries encode the retrieval context, keys index candidate memory traces, attention weights quantify cue trace similarity, and values carry the encoded content, jointly enabling the construction of a context representation that precedes and facilitates memory retrieval. Guided by the Encoding Specificity Principle, we hypothesize that the cues used in the initial stage of retrieval are instantiated as keywords. We provide converging evidence for this keywords-as-cues hypothesis. In addition, we isolate neurons within attention layers whose activations selectively encode and facilitate the retrieval of context-defining keywords. Consequently, these keywords can be extracted from identified neurons and further contribute to downstream applications such as unlearning.

</details>


### [146] [Can Continuous-Time Diffusion Models Generate and Solve Globally Constrained Discrete Problems? A Study on Sudoku](https://arxiv.org/abs/2601.20363)
*Mariia Drozdova*

Main category: cs.LG

TL;DR: Continuous-time generative models can represent sparse, constrained discrete sets like Sudoku grids, with stochastic sampling outperforming deterministic methods, and can be repurposed for constraint satisfaction via guided generation.


<details>
  <summary>Details</summary>
Motivation: To investigate whether standard continuous-time generative models can represent distributions with extremely sparse, globally constrained discrete support, using Sudoku grids as a controlled testbed.

Method: Train flow-matching and score-based models on Gaussian probability paths, compare deterministic (ODE) vs stochastic (SDE) sampling and DDPM-style discretizations, and repurpose models for guided generation via repeated sampling with clamped clues.

Result: Stochastic sampling substantially outperforms deterministic flows; score-based samplers are most reliable among continuous-time methods; DDPM-style ancestral sampling achieves highest validity overall; models can act as probabilistic Sudoku solvers via guided generation.

Conclusion: Classic diffusion/flow formulations can assign non-zero probability mass to globally constrained combinatorial structures and can be used for constraint satisfaction via stochastic search, though less sample-efficient than specialized methods.

Abstract: Can standard continuous-time generative models represent distributions whose support is an extremely sparse, globally constrained discrete set? We study this question using completed Sudoku grids as a controlled testbed, treating them as a subset of a continuous relaxation space. We train flow-matching and score-based models along a Gaussian probability path and compare deterministic (ODE) sampling, stochastic (SDE) sampling, and DDPM-style discretizations derived from the same continuous-time training. Unconditionally, stochastic sampling substantially outperforms deterministic flows; score-based samplers are the most reliable among continuous-time methods, and DDPM-style ancestral sampling achieves the highest validity overall. We further show that the same models can be repurposed for guided generation: by repeatedly sampling completions under clamped clues and stopping when constraints are satisfied, the model acts as a probabilistic Sudoku solver. Although far less sample-efficient than classical solvers and discrete-geometry-aware diffusion methods, these experiments demonstrate that classic diffusion/flow formulations can assign non-zero probability mass to globally constrained combinatorial structures and can be used for constraint satisfaction via stochastic search.

</details>


### [147] [Delayed Feedback Modeling for Post-Click Gross Merchandise Volume Prediction: Benchmark, Insights and Approaches](https://arxiv.org/abs/2601.20307)
*Xinyu Li,Sishuo Chen,Guipeng Xv,Li Zhang,Mingxuan Luo,Zhangming Chan,Xiang-Rong Sheng,Han Zhu,Jian Xu,Chen Lin*

Main category: cs.LG

TL;DR: Proposes READER, a novel GMV prediction model for online advertising that addresses delayed feedback challenges, introduces TRACE benchmark, and achieves 2.19% accuracy improvement.


<details>
  <summary>Details</summary>
Motivation: Online ad ranking is shifting from CVR to GMV prediction, but delayed feedback modeling for continuous GMV targets remains unexplored. GMV prediction is more challenging due to continuous targets and multiple purchases per click.

Method: Establishes TRACE benchmark with complete transaction sequences. Proposes READER (RepurchasE-Aware Dual-branch prEdictoR) with router-based expert parameter activation and dynamic target calibration to handle repurchase vs. single-purchase differences and incomplete labels.

Result: READER achieves 2.19% improvement in accuracy on TRACE benchmark compared to baselines. Analysis reveals rapid GMV label evolution and significant distribution differences between repurchase and single-purchase samples.

Conclusion: The study opens new research direction for online delayed feedback modeling in GMV prediction. TRACE benchmark and READER model provide foundation for future work in this area.

Abstract: The prediction objectives of online advertisement ranking models are evolving from probabilistic metrics like conversion rate (CVR) to numerical business metrics like post-click gross merchandise volume (GMV). Unlike the well-studied delayed feedback problem in CVR prediction, delayed feedback modeling for GMV prediction remains unexplored and poses greater challenges, as GMV is a continuous target, and a single click can lead to multiple purchases that cumulatively form the label. To bridge the research gap, we establish TRACE, a GMV prediction benchmark containing complete transaction sequences rising from each user click, which supports delayed feedback modeling in an online streaming manner. Our analysis and exploratory experiments on TRACE reveal two key insights: (1) the rapid evolution of the GMV label distribution necessitates modeling delayed feedback under online streaming training; (2) the label distribution of repurchase samples substantially differs from that of single-purchase samples, highlighting the need for separate modeling. Motivated by these findings, we propose RepurchasE-Aware Dual-branch prEdictoR (READER), a novel GMV modeling paradigm that selectively activates expert parameters according to repurchase predictions produced by a router. Moreover, READER dynamically calibrates the regression target to mitigate under-estimation caused by incomplete labels. Experimental results show that READER yields superior performance on TRACE over baselines, achieving a 2.19% improvement in terms of accuracy. We believe that our study will open up a new avenue for studying online delayed feedback modeling for GMV prediction, and our TRACE benchmark with the gathered insights will facilitate future research and application in this promising direction. Our code and dataset are available at https://github.com/alimama-tech/OnlineGMV .

</details>


### [148] [LLM-AutoDP: Automatic Data Processing via LLM Agents for Model Fine-tuning](https://arxiv.org/abs/2601.20375)
*Wei Huang,Anda Cheng,Yinggui Wang,Lei Wang,Tao Wei*

Main category: cs.LG

TL;DR: LLM-AutoDP is an automated framework that uses LLM agents to generate and optimize data processing strategies for domain-specific fine-tuning without exposing raw data, achieving 80% win rates over unprocessed data and 65% over AutoML baselines with 10x speedup.


<details>
  <summary>Details</summary>
Motivation: Domain-specific fine-tuning data often contains low-quality samples requiring manual data processing, which is labor-intensive and raises privacy concerns in sensitive domains like healthcare where direct human access to raw data is problematic.

Method: LLM-AutoDP uses LLMs as agents to automatically generate and optimize data processing strategies through iterative refinement with feedback signals and comparative evaluations. It employs three acceleration techniques: Distribution Preserving Sampling (reduces data volume while maintaining distribution), Processing Target Selection (binary classifier to identify low-quality samples), and Cache-and-Reuse Mechanism (minimizes redundant computations).

Result: Models trained on data processed by LLM-AutoDP achieve over 80% win rates against models trained on unprocessed data. Compared to AutoML baselines using LLM agents, LLM-AutoDP achieves approximately 65% win rate. The acceleration techniques reduce total searching time by up to 10 times.

Conclusion: LLM-AutoDP provides an effective and efficient automated solution for data processing in domain-specific LLM fine-tuning, addressing both quality improvement and privacy preservation while significantly reducing manual effort and computational costs.

Abstract: Large Language Models (LLMs) can be fine-tuned on domain-specific data to enhance their performance in specialized fields. However, such data often contains numerous low-quality samples, necessitating effective data processing (DP). In practice, DP strategies are typically developed through iterative manual analysis and trial-and-error adjustment. These processes inevitably incur high labor costs and may lead to privacy issues in high-privacy domains like healthcare due to direct human access to sensitive data. Thus, achieving automated data processing without exposing the raw data has become a critical challenge. To address this challenge, we propose LLM-AutoDP, a novel framework that leverages LLMs as agents to automatically generate and optimize data processing strategies. Our method generates multiple candidate strategies and iteratively refines them using feedback signals and comparative evaluations. This iterative in-context learning mechanism enables the agent to converge toward high-quality processing pipelines without requiring direct human intervention or access to the underlying data. To further accelerate strategy search, we introduce three key techniques: Distribution Preserving Sampling, which reduces data volume while maintaining distributional integrity; Processing Target Selection, which uses a binary classifier to identify low-quality samples for focused processing; Cache-and-Reuse Mechanism}, which minimizes redundant computations by reusing prior processing results. Results show that models trained on data processed by our framework achieve over 80% win rates against models trained on unprocessed data. Compared to AutoML baselines based on LLM agents, LLM-AutoDP achieves approximately a 65% win rate. Moreover, our acceleration techniques reduce the total searching time by up to 10 times, demonstrating both effectiveness and efficiency.

</details>


### [149] [Window-Diffusion: Accelerating Diffusion Language Model Inference with Windowed Token Pruning and Caching](https://arxiv.org/abs/2601.20332)
*Fengrui Zuo,Zhiwei Ke,Yiming Liu,Wenqi Lou,Chao Wang,Xvehai Zhou*

Main category: cs.LG

TL;DR: Window-Diffusion: A window-based token pruning and caching method that achieves up to 99Ã— inference speedup for diffusion language models by exploiting structural locality in denoising.


<details>
  <summary>Details</summary>
Motivation: Diffusion language models require full-sequence attention at every iteration during inference, leading to substantial redundant computation on masked tokens. Existing block-wise diffusion methods need retraining and have constrained update orders, limiting their applicability to pretrained models.

Method: Token-level analysis reveals structural locality in DLM inference: decoding is driven by prefix-localized active tokens, distant context influence diminishes rapidly, and decoded tokens show temporal stability. Based on this, Window-Diffusion uses a sliding local computation window that partitions undecoded tokens into: (1) active tokens computed online, (2) buffer tokens with cached KV states refreshed periodically, and (3) far-field tokens pruned outside the window.

Result: Experiments on LLaDA and Dream models show that under matched compute budgets, Window-Diffusion achieves up to 99Ã— inference speedup while largely preserving generation performance.

Conclusion: Window-Diffusion effectively reduces redundant computation in diffusion language model inference by exploiting structural locality, enabling significant speedups without requiring model retraining or compromising generation quality.

Abstract: Diffusion language models (DLMs) generate text through iterative denoising, but inference requires full-sequence attention at every iteration, resulting in substantial redundant computation on masked tokens. Block-wise diffusion can reduce this cost, yet it typically relies on retraining and constrained update orders, limiting its direct applicability to pretrained DLMs. Our token-level analysis reveals pronounced structural locality in DLM inference. Decoding is driven by a small set of prefix-localized active tokens; the influence of distant undecoded context diminishes rapidly, and decoded tokens exhibit stage-wise temporal stability, enabling reuse of intermediate representations except for a brief post-decode transient. Motivated by these observations, we propose \textbf{\placeholder}\footnote{The source code is available at https://github.com/vhicrgit/Window-Diffusion.}, a window-based token pruning and caching method for inference. We maintain a local computation window that slides rightward as denoising progresses, and partition undecoded tokens into: (i) \textit{active tokens} that are computed online, (ii) \textit{buffer tokens} whose KV states are cached and periodically refreshed, and (iii) \textit{far-field tokens} that are pruned outside the window. Computation is restricted to active and buffer tokens within the window, while far-field tokens are omitted at each stage. Experiments on LLaDA and Dream show that, under matched compute budgets, our method achieves up to $99\times$ inference speedup while largely preserving generation performance.

</details>


### [150] [FedRD: Reducing Divergences for Generalized Federated Learning via Heterogeneity-aware Parameter Guidance](https://arxiv.org/abs/2601.20397)
*Kaile Wang,Jiannong Cao,Yu Yang,Xiaoyin Li,Mingjin Zhang*

Main category: cs.LG

TL;DR: FedRD addresses federated domain generalization by reducing optimization and performance divergences through parameter-guided global generalization aggregation and local debiased classification.


<details>
  <summary>Details</summary>
Motivation: New clients in heterogeneous federated learning require significant adjustments to align with existing systems, creating challenges in generalizing models to unseen clients under heterogeneous data distributions.

Method: FedRD uses parameter-guided global generalization aggregation and local debiased classification to reduce optimization and performance divergences in federated domain generalization.

Result: Extensive experiments on public multi-domain datasets show substantial performance advantages over competing baselines for addressing federated domain generalization.

Conclusion: FedRD effectively tackles federated domain generalization challenges by reducing divergences and obtaining optimal global models for both participating and unseen clients.

Abstract: Heterogeneous federated learning (HFL) aims to ensure effective and privacy-preserving collaboration among different entities. As newly joined clients require significant adjustments and additional training to align with the existing system, the problem of generalizing federated learning models to unseen clients under heterogeneous data has become progressively crucial. Consequently, we highlight two unsolved challenging issues in federated domain generalization: Optimization Divergence and Performance Divergence. To tackle the above challenges, we propose FedRD, a novel heterogeneity-aware federated learning algorithm that collaboratively utilizes parameter-guided global generalization aggregation and local debiased classification to reduce divergences, aiming to obtain an optimal global model for participating and unseen clients. Extensive experiments on public multi-domain datasets demonstrate that our approach exhibits a substantial performance advantage over competing baselines in addressing this specific problem.

</details>


### [151] [TABED: Test-Time Adaptive Ensemble Drafting for Robust Speculative Decoding in LVLMs](https://arxiv.org/abs/2601.20357)
*Minjae Lee,Wonjun Kang,Byeongkeun Ahn,Christian Classen,Kevin Galim,Seunghyuk Oh,Minghao Yan,Hyung Il Koo,Kangwook Lee*

Main category: cs.LG

TL;DR: TABED is a training-free, plug-and-play method that dynamically ensembles multiple draft tokens via batch inference to accelerate Large Vision-Language Model inference, achieving 1.74x speedup over autoregressive decoding.


<details>
  <summary>Details</summary>
Motivation: Speculative decoding has proven effective for LLM acceleration but remains unexplored for Large Vision-Language Models. Existing methods show scenario-specific performance fluctuations, creating a need for more robust acceleration techniques for multimodal models.

Method: Test-time Adaptive Batched Ensemble Drafting (TABED) dynamically ensembles multiple drafts obtained via batch inference by leveraging deviations from past ground truths available in speculative decoding settings. It uses parameter sharing to keep ensembling costs negligible and integrates with advanced verification and alternative drafting methods.

Result: TABED achieves average robust walltime speedup of 1.74x over autoregressive decoding and 5% improvement over single drafting methods across 11 diverse datasets, while remaining training-free and maintaining plug-and-play compatibility.

Conclusion: TABED provides an effective, training-free solution for accelerating LVLM inference through dynamic ensemble drafting, with significant speed improvements and robust performance across diverse input scenarios.

Abstract: Speculative decoding (SD) has proven effective for accelerating LLM inference by quickly generating draft tokens and verifying them in parallel. However, SD remains largely unexplored for Large Vision-Language Models (LVLMs), which extend LLMs to process both image and text prompts. To address this gap, we benchmark existing inference methods with small draft models on 11 datasets across diverse input scenarios and observe scenario-specific performance fluctuations. Motivated by these findings, we propose Test-time Adaptive Batched Ensemble Drafting (TABED), which dynamically ensembles multiple drafts obtained via batch inference by leveraging deviations from past ground truths available in the SD setting. The dynamic ensemble method achieves an average robust walltime speedup of 1.74x over autoregressive decoding and a 5% improvement over single drafting methods, while remaining training-free and keeping ensembling costs negligible through parameter sharing. With its plug-and-play compatibility, we further enhance TABED by integrating advanced verification and alternative drafting methods. Code and custom-trained models are available at https://github.com/furiosa-ai/TABED.

</details>


### [152] [TINNs: Time-Induced Neural Networks for Solving Time-Dependent PDEs](https://arxiv.org/abs/2601.20361)
*Chen-Yang Dai,Che-Chia Chang,Te-Sheng Lin,Ming-Chih Lai,Chieh-Hsin Lai*

Main category: cs.LG

TL;DR: TINNs propose time-dependent network weights to improve PINN accuracy for time-dependent PDEs, achieving 4Ã— better accuracy and 10Ã— faster convergence.


<details>
  <summary>Details</summary>
Motivation: Standard PINNs use shared network weights across all times, forcing the same features to represent different dynamics, which degrades accuracy and destabilizes training when enforcing PDE, boundary, and initial constraints jointly.

Method: Time-Induced Neural Networks (TINNs) parameterize network weights as a learned function of time, allowing spatial representations to evolve over time while maintaining shared structure. The formulation yields a nonlinear least-squares problem optimized using Levenberg-Marquardt method.

Result: Experiments on various time-dependent PDEs show up to 4Ã— improved accuracy and 10Ã— faster convergence compared to PINNs and strong baselines.

Conclusion: TINNs provide an effective architecture for solving time-dependent PDEs by allowing network representations to evolve with time, overcoming limitations of standard PINNs while maintaining computational efficiency.

Abstract: Physics-informed neural networks (PINNs) solve time-dependent partial differential equations (PDEs) by learning a mesh-free, differentiable solution that can be evaluated anywhere in space and time. However, standard space--time PINNs take time as an input but reuse a single network with shared weights across all times, forcing the same features to represent markedly different dynamics. This coupling degrades accuracy and can destabilize training when enforcing PDE, boundary, and initial constraints jointly. We propose Time-Induced Neural Networks (TINNs), a novel architecture that parameterizes the network weights as a learned function of time, allowing the effective spatial representation to evolve over time while maintaining shared structure. The resulting formulation naturally yields a nonlinear least-squares problem, which we optimize efficiently using a Levenberg--Marquardt method. Experiments on various time-dependent PDEs show up to $4\times$ improved accuracy and $10\times$ faster convergence compared to PINNs and strong baselines.

</details>


### [153] [Fair Recourse for All: Ensuring Individual and Group Fairness in Counterfactual Explanations](https://arxiv.org/abs/2601.20449)
*Fatima Ezzeddine,Obaida Ammar,Silvia Giordano,Omran Ayoub*

Main category: cs.LG

TL;DR: A reinforcement learning approach for generating fair counterfactual explanations that ensures both individual and group fairness while maintaining explanation quality.


<details>
  <summary>Details</summary>
Motivation: Counterfactual explanations are crucial for transparent ML but need to ensure fairness - similar individuals should get similar recourse options, and different protected groups should receive equitable recourse. Current approaches often treat individual and group fairness as separate objectives.

Method: Proposes a model-agnostic reinforcement learning approach to generate counterfactuals that satisfy fairness constraints. Defines three fairness levels: individual fairness (similar individuals get similar CFs), group fairness (equitable CFs across protected groups), and hybrid fairness (both). Formulates as optimization task with fairness metrics extended from ML auditing metrics like equal choice of recourse and equal effectiveness.

Result: Evaluated on three benchmark datasets, showing effective individual and group fairness while preserving CF quality (proximity and plausibility). Quantifies the cost of fairness at different levels separately.

Conclusion: The approach successfully generates fair counterfactual explanations addressing both individual and group fairness. Opens broader discussion on hybrid fairness and its implications for XAI beyond just counterfactuals.

Abstract: Explainable Artificial Intelligence (XAI) is becoming increasingly essential for enhancing the transparency of machine learning (ML) models. Among the various XAI techniques, counterfactual explanations (CFs) hold a pivotal role due to their ability to illustrate how changes in input features can alter an ML model's decision, thereby offering actionable recourse to users. Ensuring that individuals with comparable attributes and those belonging to different protected groups (e.g., demographic) receive similar and actionable recourse options is essential for trustworthy and fair decision-making. In this work, we address this challenge directly by focusing on the generation of fair CFs. Specifically, we start by defining and formulating fairness at: 1) individual fairness, ensuring that similar individuals receive similar CFs, 2) group fairness, ensuring equitable CFs across different protected groups and 3) hybrid fairness, which accounts for both individual and broader group-level fairness. We formulate the problem as an optimization task and propose a novel model-agnostic, reinforcement learning based approach to generate CFs that satisfy fairness constraints at both the individual and group levels, two objectives that are usually treated as orthogonal. As fairness metrics, we extend existing metrics commonly used for auditing ML models, such as equal choice of recourse and equal effectiveness across individuals and groups. We evaluate our approach on three benchmark datasets, showing that it effectively ensures individual and group fairness while preserving the quality of the generated CFs in terms of proximity and plausibility, and quantify the cost of fairness in the different levels separately. Our work opens a broader discussion on hybrid fairness and its role and implications for XAI and beyond CFs.

</details>


### [154] [Unsupervised Anomaly Detection in Multi-Agent Trajectory Prediction via Transformer-Based Models](https://arxiv.org/abs/2601.20367)
*Qing Lyu,Zhe Fu,Alexandre Bayen*

Main category: cs.LG

TL;DR: Unsupervised anomaly detection framework using multi-agent Transformer to identify safety-critical driving scenarios, validated through dual evaluation of detection stability and physical alignment with safety metrics.


<details>
  <summary>Details</summary>
Motivation: Safety-critical scenarios are rare in autonomous driving, making supervised labeling impractical. Traditional rule-based metrics like Time-to-Collision are too simplistic, and existing methods lack systematic verification of whether statistical anomalies reflect actual physical danger.

Method: Proposes an unsupervised anomaly detection framework based on a multi-agent Transformer that models normal driving behavior and measures deviations through prediction residuals. Uses a dual evaluation scheme: 1) Stability assessment using Kendall Rank Correlation Coefficient and Jaccard index, 2) Physical alignment assessment through correlations with established Surrogate Safety Measures (SSM).

Result: Maximum residual aggregator achieves highest physical alignment while maintaining stability. Framework identifies 388 unique anomalies missed by Time-to-Collision and statistical baselines, capturing subtle multi-agent risks like reactive braking under lateral drift. Detected anomalies cluster into four interpretable risk types.

Conclusion: The proposed unsupervised framework effectively identifies safety-critical driving scenarios that traditional methods miss, providing actionable insights for simulation and testing through interpretable risk clustering.

Abstract: Identifying safety-critical scenarios is essential for autonomous driving, but the rarity of such events makes supervised labeling impractical. Traditional rule-based metrics like Time-to-Collision are too simplistic to capture complex interaction risks, and existing methods lack a systematic way to verify whether statistical anomalies truly reflect physical danger. To address this gap, we propose an unsupervised anomaly detection framework based on a multi-agent Transformer that models normal driving and measures deviations through prediction residuals. A dual evaluation scheme has been proposed to assess both detection stability and physical alignment: Stability is measured using standard ranking metrics in which Kendall Rank Correlation Coefficient captures rank agreement and Jaccard index captures the consistency of the top-K selected items; Physical alignment is assessed through correlations with established Surrogate Safety Measures (SSM). Experiments on the NGSIM dataset demonstrate our framework's effectiveness: We show that the maximum residual aggregator achieves the highest physical alignment while maintaining stability. Furthermore, our framework identifies 388 unique anomalies missed by Time-to-Collision and statistical baselines, capturing subtle multi-agent risks like reactive braking under lateral drift. The detected anomalies are further clustered into four interpretable risk types, offering actionable insights for simulation and testing.

</details>


### [155] [CCMamba: Selective State-Space Models for Higher-Order Graph Learning on Combinatorial Complexes](https://arxiv.org/abs/2601.20518)
*Jiawen Chen,Qi Shao,Mingtong Zhou,Duxin Chen,Wenwu Yu*

Main category: cs.LG

TL;DR: CCMamba is a new topological deep learning framework using Mamba-based state-space models for combinatorial complexes, achieving linear-time processing, better scalability, and improved performance over attention-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing topological deep learning methods for combinatorial complexes rely on attention mechanisms with quadratic complexity, limiting scalability and failing to effectively aggregate rank-aware information in higher-order structures.

Method: Reformulates message passing as selective state-space modeling, organizing multi-rank incidence relations into structured sequences processed by rank-aware state-space models (Mamba architecture), enabling adaptive, directional, long-range information propagation in linear time without self-attention.

Result: Theoretical analysis shows expressive power upper-bound is the 1-Weisfeiler-Lehman test. Experiments on graph, hypergraph, and simplicial benchmarks demonstrate consistent outperformance over existing methods with improved scalability and robustness to depth.

Conclusion: CCMamba provides the first unified Mamba-based neural framework for combinatorial complexes, offering linear-time processing, better scalability, and superior performance compared to attention-based topological deep learning methods.

Abstract: Topological deep learning has emerged for modeling higher-order relational structures beyond pairwise interactions that standard graph neural networks fail to capture. Although combinatorial complexes offer a unified topological framework, most existing topological deep learning methods rely on local message passing via attention mechanisms, which incur quadratic complexity and remain low-dimensional, limiting scalability and rank-aware information aggregation in higher-order complexes.We propose Combinatorial Complex Mamba (CCMamba), the first unified mamba-based neural framework for learning on combinatorial complexes. CCMamba reformulates message passing as a selective state-space modeling problem by organizing multi-rank incidence relations into structured sequences processed by rank-aware state-space models. This enables adaptive, directional, and long range information propagation in linear time without self attention. We further establish the theoretical analysis that the expressive power upper-bound of CCMamba message passing is the 1-Weisfeiler-Lehman test. Experiments on graph, hypergraph, and simplicial benchmarks demonstrate that CCMamba consistently outperforms existing methods while exhibiting improved scalability and robustness to depth.

</details>


### [156] [Unsupervised Ensemble Learning Through Deep Energy-based Models](https://arxiv.org/abs/2601.20556)
*Ariel Maymon,Yanir Buznah,Uri Shaham*

Main category: cs.LG

TL;DR: A novel deep energy-based method for unsupervised ensemble learning that combines multiple learners' predictions without ground truth labels, learner features, or problem-specific information.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of combining multiple learners' predictions in scenarios where ground truth labels are unavailable, individual classifier performance is hard to evaluate, and privacy/data-scarcity constraints exist.

Method: Deep energy-based approach that constructs an accurate meta-learner using only predictions from individual learners, capable of capturing complex dependence structures between them. Theoretically guaranteed for conditionally independent learners.

Result: Superior performance across diverse ensemble scenarios including challenging mixture of experts settings, demonstrated on both standard ensemble datasets and curated datasets designed to test expertise fusion.

Conclusion: Unsupervised ensemble learning shows strong potential for harnessing collective intelligence in data-scarce or privacy-sensitive environments without requiring labeled data or additional information.

Abstract: Unsupervised ensemble learning emerged to address the challenge of combining multiple learners' predictions without access to ground truth labels or additional data. This paradigm is crucial in scenarios where evaluating individual classifier performance or understanding their strengths is challenging due to limited information. We propose a novel deep energy-based method for constructing an accurate meta-learner using only the predictions of individual learners, potentially capable of capturing complex dependence structures between them. Our approach requires no labeled data, learner features, or problem-specific information, and has theoretical guarantees for when learners are conditionally independent. We demonstrate superior performance across diverse ensemble scenarios, including challenging mixture of experts settings. Our experiments span standard ensemble datasets and curated datasets designed to test how the model fuses expertise from multiple sources. These results highlight the potential of unsupervised ensemble learning to harness collective intelligence, especially in data-scarce or privacy-sensitive environments.

</details>


### [157] [Robust Distributed Learning under Resource Constraints: Decentralized Quantile Estimation via (Asynchronous) ADMM](https://arxiv.org/abs/2601.20571)
*Anna van Elst,Igor Colin,Stephan ClÃ©menÃ§on*

Main category: cs.LG

TL;DR: AsylADMM: A novel gossip algorithm for decentralized median/quantile estimation that is communication-efficient, robust, and memory-lightweight (only 2 variables per node).


<details>
  <summary>Details</summary>
Motivation: Need for decentralized learning algorithms on resource-constrained edge devices that are communication-efficient, robust to data corruption, and memory-lightweight. Existing gossip methods lack robustness, while ADMM-based median estimation methods require memory scaling with node degree.

Method: AsylADMM - a gossip algorithm for decentralized median and quantile estimation designed for asynchronous updates, requiring only two variables per node. Theoretical analysis of synchronous variant, empirical validation of asynchronous version.

Result: Algorithm enables quantile-based trimming, geometric median estimation, and depth-based trimming. Quantile-based trimming empirically outperforms existing rank-based methods. Novel theoretical analysis of rank-based trimming via Markov chain theory provided.

Conclusion: AsylADMM addresses key requirements for edge device learning: communication efficiency, robustness, and memory efficiency (only 2 variables per node), with applications in robust statistical estimation and trimming methods.

Abstract: Specifications for decentralized learning on resource-constrained edge devices require algorithms that are communication-efficient, robust to data corruption, and lightweight in memory usage. While state-of-the-art gossip-based methods satisfy the first requirement, achieving robustness remains challenging. Asynchronous decentralized ADMM-based methods have been explored for estimating the median, a statistical centrality measure that is notoriously more robust than the mean. However, existing approaches require memory that scales with node degree, making them impractical when memory is limited. In this paper, we propose AsylADMM, a novel gossip algorithm for decentralized median and quantile estimation, primarily designed for asynchronous updates and requiring only two variables per node. We analyze a synchronous variant of AsylADMM to establish theoretical guarantees and empirically demonstrate fast convergence for the asynchronous algorithm. We then show that our algorithm enables quantile-based trimming, geometric median estimation, and depth-based trimming, with quantile-based trimming empirically outperforming existing rank-based methods. Finally, we provide a novel theoretical analysis of rank-based trimming via Markov chain theory.

</details>


### [158] [ScatterFusion: A Hierarchical Scattering Transform Framework for Enhanced Time Series Forecasting](https://arxiv.org/abs/2601.20401)
*Wei Li*

Main category: cs.LG

TL;DR: ScatterFusion integrates scattering transforms with hierarchical attention for multi-scale time series forecasting, outperforming existing methods across seven benchmarks.


<details>
  <summary>Details</summary>
Motivation: Time series forecasting is challenging due to complex temporal dependencies at multiple scales, requiring methods that can capture both local and global patterns effectively.

Method: Four-component framework: 1) Hierarchical Scattering Transform Module for multi-scale feature extraction, 2) Scale-Adaptive Feature Enhancement for dynamic importance adjustment, 3) Multi-Resolution Temporal Attention for varying horizon dependencies, 4) TSR decomposition-guided loss function.

Result: Outperforms other common methods on seven benchmark datasets with significant error reductions across various prediction horizons.

Conclusion: ScatterFusion effectively addresses multi-scale temporal dependencies through synergistic integration of scattering transforms and hierarchical attention, demonstrating superior forecasting performance.

Abstract: Time series forecasting presents significant challenges due to the complex temporal dependencies at multiple time scales. This paper introduces ScatterFusion, a novel framework that synergistically integrates scattering transforms with hierarchical attention mechanisms for robust time series forecasting. Our approach comprises four key components: (1) a Hierarchical Scattering Transform Module (HSTM) that extracts multi-scale invariant features capturing both local and global patterns; (2) a Scale-Adaptive Feature Enhancement (SAFE) module that dynamically adjusts feature importance across different scales; (3) a Multi-Resolution Temporal Attention (MRTA) mechanism that learns dependencies at varying time horizons; and (4) a Trend-Seasonal-Residual (TSR) decomposition-guided structure-aware loss function. Extensive experiments on seven benchmark datasets demonstrate that ScatterFusion outperforms other common methods, achieving significant reductions in error metrics across various prediction horizons.

</details>


### [159] [Ranking-aware Reinforcement Learning for Ordinal Ranking](https://arxiv.org/abs/2601.20585)
*Aiming Hao,Chen Zhu,Jiashu Zhu,Jiahong Wu,Xiangxiang Chu*

Main category: cs.LG

TL;DR: RARL is a reinforcement learning framework that combines regression and ranking tasks with a unified objective and ranking-aware reward, enhanced by response mutation operations for better exploration.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to model inherent ordinal dependencies in ordinal regression and ranking tasks, creating a need for approaches that can explicitly learn these relationships.

Method: RARL uses a unified objective integrating regression and Learning-to-Rank, with ranking-aware verifiable reward for joint assessment, and Response Mutation Operations (RMO) for controlled noise injection to improve exploration.

Result: The effectiveness of RARL is validated through extensive experiments on three distinct benchmarks, demonstrating improved performance on ordinal regression and ranking tasks.

Conclusion: RARL provides an effective RL framework for ordinal regression and ranking that explicitly models ordinal dependencies through synergistic integration of regression and ranking tasks with enhanced exploration mechanisms.

Abstract: Ordinal regression and ranking are challenging due to inherent ordinal dependencies that conventional methods struggle to model. We propose Ranking-Aware Reinforcement Learning (RARL), a novel RL framework that explicitly learns these relationships. At its core, RARL features a unified objective that synergistically integrates regression and Learning-to-Rank (L2R), enabling mutual improvement between the two tasks. This is driven by a ranking-aware verifiable reward that jointly assesses regression precision and ranking accuracy, facilitating direct model updates via policy optimization. To further enhance training, we introduce Response Mutation Operations (RMO), which inject controlled noise to improve exploration and prevent stagnation at saddle points. The effectiveness of RARL is validated through extensive experiments on three distinct benchmarks.

</details>


### [160] [AWGformer: Adaptive Wavelet-Guided Transformer for Multi-Resolution Time Series Forecasting](https://arxiv.org/abs/2601.20409)
*Wei Li*

Main category: cs.LG

TL;DR: AWGformer integrates adaptive wavelet decomposition with cross-scale attention for efficient multi-scale time series forecasting, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Time series forecasting requires capturing patterns across multiple temporal scales while maintaining computational efficiency. Existing methods often struggle with multi-scale and non-stationary time series.

Method: Four key components: (1) Adaptive Wavelet Decomposition Module for dynamic wavelet basis selection, (2) Cross-Scale Feature Fusion for frequency band interactions, (3) Frequency-Aware Multi-Head Attention with frequency-selective weighting, (4) Hierarchical Prediction Network for multi-resolution forecasting.

Result: Extensive experiments show significant average improvements over state-of-the-art methods, with particular effectiveness on multi-scale and non-stationary time series.

Conclusion: AWGformer successfully integrates wavelet decomposition with attention mechanisms for enhanced time series forecasting, with theoretical analysis connecting wavelet-guided attention to classical signal processing principles.

Abstract: Time series forecasting requires capturing patterns across multiple temporal scales while maintaining computational efficiency. This paper introduces AWGformer, a novel architecture that integrates adaptive wavelet decomposition with cross-scale attention mechanisms for enhanced multi-variate time series prediction. Our approach comprises: (1) an Adaptive Wavelet Decomposition Module (AWDM) that dynamically selects optimal wavelet bases and decomposition levels based on signal characteristics; (2) a Cross-Scale Feature Fusion (CSFF) mechanism that captures interactions between different frequency bands through learnable coupling matrices; (3) a Frequency-Aware Multi-Head Attention (FAMA) module that weights attention heads according to their frequency selectivity; (4) a Hierarchical Prediction Network (HPN) that generates forecasts at multiple resolutions before reconstruction. Extensive experiments on benchmark datasets demonstrate that AWGformer achieves significant average improvements over state-of-the-art methods, with particular effectiveness on multi-scale and non-stationary time series. Theoretical analysis provides convergence guarantees and establishes the connection between our wavelet-guided attention and classical signal processing principles.

</details>


### [161] [Concept Component Analysis: A Principled Approach for Concept Extraction in LLMs](https://arxiv.org/abs/2601.20420)
*Yuhang Liu,Erdun Gao,Dong Gong,Anton van den Hengel,Javen Qinfeng Shi*

Main category: cs.LG

TL;DR: The paper proposes Concept Component Analysis (ConCA), a theory-backed framework for extracting interpretable concepts from LLM representations using unsupervised linear unmixing of log-posteriors, addressing theoretical ambiguities in existing sparse autoencoder methods.


<details>
  <summary>Details</summary>
Motivation: Current sparse autoencoder (SAE) methods for mechanistic interpretability lack theoretical grounding, creating ambiguity about the correspondence between LLM representations and human-interpretable concepts, leading to methodological challenges in design and evaluation.

Method: The paper introduces Concept Component Analysis (ConCA), which treats concepts as latent variables and shows LLM representations can be approximated as a linear mixture of log-posteriors over concepts. Sparse ConCA variants use sparsity priors to solve the ill-posed unmixing problem through unsupervised linear decomposition.

Result: The authors implement 12 sparse ConCA variants and demonstrate they can extract meaningful concepts across multiple LLMs, showing theory-backed advantages over traditional sparse autoencoders.

Conclusion: ConCA provides a principled theoretical framework for concept extraction from LLMs, addressing fundamental ambiguities in existing methods and offering a more theoretically grounded approach to mechanistic interpretability.

Abstract: Developing human understandable interpretation of large language models (LLMs) becomes increasingly critical for their deployment in essential domains. Mechanistic interpretability seeks to mitigate the issues through extracts human-interpretable process and concepts from LLMs' activations. Sparse autoencoders (SAEs) have emerged as a popular approach for extracting interpretable and monosemantic concepts by decomposing the LLM internal representations into a dictionary. Despite their empirical progress, SAEs suffer from a fundamental theoretical ambiguity: the well-defined correspondence between LLM representations and human-interpretable concepts remains unclear. This lack of theoretical grounding gives rise to several methodological challenges, including difficulties in principled method design and evaluation criteria. In this work, we show that, under mild assumptions, LLM representations can be approximated as a {linear mixture} of the log-posteriors over concepts given the input context, through the lens of a latent variable model where concepts are treated as latent variables. This motivates a principled framework for concept extraction, namely Concept Component Analysis (ConCA), which aims to recover the log-posterior of each concept from LLM representations through a {unsupervised} linear unmixing process. We explore a specific variant, termed sparse ConCA, which leverages a sparsity prior to address the inherent ill-posedness of the unmixing problem. We implement 12 sparse ConCA variants and demonstrate their ability to extract meaningful concepts across multiple LLMs, offering theory-backed advantages over SAEs.

</details>


### [162] [Regularized Gradient Temporal-Difference Learning](https://arxiv.org/abs/2601.20599)
*Hyunjun Na,Donghwan Lee*

Main category: cs.LG

TL;DR: Proposes R-GTD, a regularized gradient TD algorithm that guarantees convergence even when the feature interaction matrix is singular, unlike existing GTD methods.


<details>
  <summary>Details</summary>
Motivation: Existing GTD algorithms for off-policy policy evaluation require the feature interaction matrix (FIM) to be nonsingular for convergence, but in practice FIM can become singular, causing instability or degraded performance.

Method: Reformulates mean-square projected Bellman error (MSPBE) minimization with regularization, yielding R-GTD algorithm that guarantees convergence to unique solution even with singular FIM.

Result: Establishes theoretical convergence guarantees and explicit error bounds for R-GTD, and validates effectiveness through empirical experiments.

Conclusion: R-GTD provides a robust solution to the singularity problem in GTD algorithms, ensuring stable convergence in practical scenarios where traditional methods fail.

Abstract: Gradient temporal-difference (GTD) learning algorithms are widely used for off-policy policy evaluation with function approximation. However, existing convergence analyses rely on the restrictive assumption that the so-called feature interaction matrix (FIM) is nonsingular. In practice, the FIM can become singular and leads to instability or degraded performance. In this paper, we propose a regularized optimization objective by reformulating the mean-square projected Bellman error (MSPBE) minimization. This formulation naturally yields a regularized GTD algorithms, referred to as R-GTD, which guarantees convergence to a unique solution even when the FIM is singular. We establish theoretical convergence guarantees and explicit error bounds for the proposed method, and validate its effectiveness through empirical experiments.

</details>


### [163] [Nonlinear Dimensionality Reduction with Diffusion Maps in Practice](https://arxiv.org/abs/2601.20428)
*SÃ¶nke Beier,Paula Pirker-DÃ­az,Friedrich Pagenkopf,Karoline Wiesner*

Main category: cs.LG

TL;DR: Diffusion Map is a spectral dimensionality reduction method for uncovering nonlinear manifolds, but its practical application faces challenges in preprocessing, parameter settings, and component selection that aren't well-documented.


<details>
  <summary>Details</summary>
Motivation: Despite Diffusion Map's growing popularity across scientific fields, there's a lack of comprehensive guidance on practical implementation issues like data preprocessing, parameter tuning, and component selection, which significantly affect the resulting manifold structure.

Method: The paper provides a practice-oriented review of Diffusion Map, identifies common pitfalls, and demonstrates a recently introduced technique for identifying the most relevant components in the dimensionality reduction output.

Result: The analysis reveals that the first components (typically assumed to be most important) are not necessarily the most relevant ones for capturing meaningful manifold structure, challenging conventional assumptions in spectral dimensionality reduction.

Conclusion: Proper implementation of Diffusion Map requires careful attention to preprocessing, parameter settings, and component selection, with the first components not always being the most informative, necessitating more sophisticated component selection methods.

Abstract: Diffusion Map is a spectral dimensionality reduction technique which is able to uncover nonlinear submanifolds in high-dimensional data. And, it is increasingly applied across a wide range of scientific disciplines, such as biology, engineering, and social sciences. But data preprocessing, parameter settings and component selection have a significant influence on the resulting manifold, something which has not been comprehensively discussed in the literature so far. We provide a practice oriented review of the Diffusion Map technique, illustrate pitfalls and showcase a recently introduced technique for identifying the most relevant components. Our results show that the first components are not necessarily the most relevant ones.

</details>


### [164] [TimeCatcher: A Variational Framework for Volatility-Aware Forecasting of Non-Stationary Time Series](https://arxiv.org/abs/2601.20448)
*Zhiyu Chen,Minhao Liu,Yanru Zhang*

Main category: cs.LG

TL;DR: TimeCatcher: A volatility-aware variational forecasting framework that improves long-term forecasting for highly non-stationary time series by capturing latent dynamic patterns and amplifying significant local variations.


<details>
  <summary>Details</summary>
Motivation: Current lightweight MLP-based models assume local stationarity, making them prone to errors when forecasting highly non-stationary series with abrupt fluctuations, especially in domains like web traffic monitoring where volatility is common.

Method: Extends linear architectures with a variational encoder to capture latent dynamic patterns from historical data and a volatility-aware enhancement mechanism to detect and amplify significant local variations.

Result: Outperforms state-of-the-art baselines on nine real-world datasets from traffic, financial, energy, and weather domains, with particularly large improvements in long-term forecasting scenarios with high volatility and sudden fluctuations.

Conclusion: TimeCatcher effectively addresses the limitations of existing MLP-based models for non-stationary time series forecasting, demonstrating strong performance across diverse domains with volatile patterns.

Abstract: Recent lightweight MLP-based models have achieved strong performance in time series forecasting by capturing stable trends and seasonal patterns. However, their effectiveness hinges on an implicit assumption of local stationarity assumption, making them prone to errors in long-term forecasting of highly non-stationary series, especially when abrupt fluctuations occur, a common challenge in domains like web traffic monitoring. To overcome this limitation, we propose TimeCatcher, a novel Volatility-Aware Variational Forecasting framework. TimeCatcher extends linear architectures with a variational encoder to capture latent dynamic patterns hidden in historical data and a volatility-aware enhancement mechanism to detect and amplify significant local variations. Experiments on nine real-world datasets from traffic, financial, energy, and weather domains show that TimeCatcher consistently outperforms state-of-the-art baselines, with particularly large improvements in long-term forecasting scenarios characterized by high volatility and sudden fluctuations. Our code is available at https://github.com/ColaPrinceCHEN/TimeCatcher.

</details>


### [165] [WFR-MFM: One-Step Inference for Dynamic Unbalanced Optimal Transport](https://arxiv.org/abs/2601.20606)
*Xinyu Wang,Ruoyu Wang,Qiangwei Peng,Peijie Zhou,Tiejun Li*

Main category: cs.LG

TL;DR: Proposes Wasserstein-Fisher-Rao Mean Flow Matching (WFR-MFM) for fast, one-step inference in dynamic unbalanced optimal transport, achieving orders-of-magnitude speedup over existing methods while maintaining accuracy in single-cell biology applications.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for reconstructing dynamical evolution from limited observations in single-cell biology rely on trajectory simulation at inference time, creating a scalability bottleneck for applications requiring fast inference.

Method: Develops a mean-flow framework that summarizes transport and mass-growth dynamics using mean velocity and mass-growth fields over arbitrary time intervals, enabling one-step generation without trajectory simulation. Builds on this to create Wasserstein-Fisher-Rao Mean Flow Matching (WFR-MFM) for solving dynamic unbalanced optimal transport under Wasserstein-Fisher-Rao geometry.

Result: WFR-MFM achieves orders-of-magnitude faster inference than existing baselines while maintaining high predictive accuracy across synthetic and real single-cell RNA sequencing datasets. Enables efficient perturbation response prediction on large synthetic datasets with thousands of conditions.

Conclusion: The proposed mean-flow framework and WFR-MFM method provide a scalable solution for dynamic unbalanced optimal transport in single-cell biology, overcoming the inference bottleneck of trajectory-based approaches while preserving accuracy.

Abstract: Reconstructing dynamical evolution from limited observations is a fundamental challenge in single-cell biology, where dynamic unbalanced optimal transport provides a principled framework for modeling coupled transport and mass variation. However, existing approaches rely on trajectory simulation at inference time, making inference a key bottleneck for scalable applications. In this work, we propose a mean-flow framework for unbalanced flow matching that summarizes both transport and mass-growth dynamics over arbitrary time intervals using mean velocity and mass-growth fields, enabling fast one-step generation without trajectory simulation. To solve dynamic unbalanced optimal transport under the Wasserstein-Fisher-Rao geometry, we further build on this framework to develop Wasserstein-Fisher-Rao Mean Flow Matching (WFR-MFM). Across synthetic and real single-cell RNA sequencing datasets, WFR-MFM achieves orders-of-magnitude faster inference than a range of existing baselines while maintaining high predictive accuracy, and enables efficient perturbation response prediction on large synthetic datasets with thousands of conditions.

</details>


### [166] [Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability](https://arxiv.org/abs/2601.20642)
*Rohan Asthana,Vasileios Belagiannis*

Main category: cs.LG

TL;DR: The paper proposes a new memorization detection metric for diffusion models that combines isotropic norm and anisotropic alignment, working directly on noise inputs without denoising steps, achieving better performance and 5x speedup over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models are vulnerable to memorization (reproducing training data), and existing detection methods based on score difference norms are only effective under isotropic assumptions at high/medium noise levels, failing in anisotropic low-noise regimes.

Method: Developed a memorization detection metric that integrates isotropic norm and anisotropic alignment between guidance vector and unconditional scores in low-noise settings. The metric can be computed directly on pure noise inputs via two conditional and unconditional forward passes, eliminating costly denoising steps.

Result: The proposed metric outperforms existing denoising-free detection methods on Stable Diffusion v1.4 and v2, while being at least 5x faster than the previous best approach. Also demonstrated effectiveness through a mitigation strategy that adapts memorized prompts based on the developed metric.

Conclusion: The paper presents an efficient and effective memorization detection approach for diffusion models that works across different noise regimes, enabling practical detection without expensive denoising while supporting mitigation strategies.

Abstract: Diffusion-based image generative models produce high-fidelity images through iterative denoising but remain vulnerable to memorization, where they unintentionally reproduce exact copies or parts of training images. Recent memorization detection methods are primarily based on the norm of score difference as indicators of memorization. We prove that such norm-based metrics are mainly effective under the assumption of isotropic log-probability distributions, which generally holds at high or medium noise levels. In contrast, analyzing the anisotropic regime reveals that memorized samples exhibit strong angular alignment between the guidance vector and unconditional scores in the low-noise setting. Through these insights, we develop a memorization detection metric by integrating isotropic norm and anisotropic alignment. Our detection metric can be computed directly on pure noise inputs via two conditional and unconditional forward passes, eliminating the need for costly denoising steps. Detection experiments on Stable Diffusion v1.4 and v2 show that our metric outperforms existing denoising-free detection methods while being at least approximately 5x faster than the previous best approach. Finally, we demonstrate the effectiveness of our approach by utilizing a mitigation strategy that adapts memorized prompts based on our developed metric.

</details>


### [167] [An explainable framework for the relationship between dementia and glucose metabolism patterns](https://arxiv.org/abs/2601.20480)
*C. VÃ¡zquez-GarcÃ­a,F. J. MartÃ­nez-Murcia,F. Segovia RomÃ¡n,A. Forte,J. RamÃ­rez,I. IllÃ¡n,A. HernÃ¡ndez-Segura,C. JimÃ©nez-Mesa,Juan M. GÃ³rriz*

Main category: cs.LG

TL;DR: Semi-supervised VAE framework with similarity regularization aligns latent variables with clinical dementia measures, extracting disease-related patterns from neuroimaging data.


<details>
  <summary>Details</summary>
Motivation: High-dimensional neuroimaging data has complex non-linear relationships that challenge neurodegenerative disease assessment. Need for methods that can encode scans into interpretable latent spaces aligned with clinical progression.

Method: Proposed semi-supervised VAE framework with flexible similarity regularization term that aligns selected latent variables with clinical/biomarker measures of dementia progression. Allows adaptation of similarity metric and supervised variables to specific goals or available data.

Result: Applied to ADNI PET scans, first latent dimension aligned with cognitive score. Generated average reconstructions across cognitive impairment levels. Voxel-wise GLM revealed reduced metabolism in hippocampus and within Default Mode and Central Executive Networks. Remaining latent variables captured confounds like inter-subject variability and site effects.

Conclusion: Framework effectively extracts disease-related patterns aligned with established Alzheimer's biomarkers, providing interpretable and adaptable tool for studying neurodegenerative progression.

Abstract: High-dimensional neuroimaging data presents challenges for assessing neurodegenerative diseases due to complex non-linear relationships. Variational Autoencoders (VAEs) can encode scans into lower-dimensional latent spaces capturing disease-relevant features. We propose a semi-supervised VAE framework with a flexible similarity regularization term that aligns selected latent variables with clinical or biomarker measures of dementia progression. This allows adapting the similarity metric and supervised variables to specific goals or available data. We demonstrate the approach using PET scans from the Alzheimer's Disease Neuroimaging Initiative (ADNI), guiding the first latent dimension to align with a cognitive score. Using this supervised latent variable, we generate average reconstructions across levels of cognitive impairment. Voxel-wise GLM analysis reveals reduced metabolism in key regions, mainly the hippocampus, and within major Resting State Networks, particularly the Default Mode and Central Executive Networks. The remaining latent variables encode affine transformations and intensity variations, capturing confounds such as inter-subject variability and site effects. Our framework effectively extracts disease-related patterns aligned with established Alzheimer's biomarkers, offering an interpretable and adaptable tool for studying neurodegenerative progression.

</details>


### [168] [Learning Contextual Runtime Monitors for Safe AI-Based Autonomy](https://arxiv.org/abs/2601.20666)
*Alejandro Luque-Cerpa,Mengyuan Wang,Emil Carlsson,Sanjit A. Seshia,Devdatt Dubhashi,Hazem Torfah*

Main category: cs.LG

TL;DR: A framework for context-aware runtime monitors that selects the best ML controller for current conditions using contextual multi-armed bandits, improving safety and performance over traditional ensemble methods.


<details>
  <summary>Details</summary>
Motivation: ML controllers in autonomous systems degrade in unfamiliar environments, creating safety concerns. Traditional ensemble methods dilute individual controller strengths by averaging outputs rather than exploiting contextual specialization.

Method: Reformulates safe AI control ensembles as contextual monitoring problem. Uses contextual multi-armed bandits to learn which controller performs best in different operating contexts. Monitor observes system context and selects optimal controller.

Result: Validated in two simulated autonomous driving scenarios, showing significant improvements in both safety and performance compared to non-contextual baselines. Provides theoretical safety guarantees during controller selection.

Conclusion: Context-aware monitoring framework better utilizes controller diversity by selecting specialized controllers for specific contexts, offering safety guarantees and improved performance over traditional ensemble approaches.

Abstract: We introduce a novel framework for learning context-aware runtime monitors for AI-based control ensembles. Machine-learning (ML) controllers are increasingly deployed in (autonomous) cyber-physical systems because of their ability to solve complex decision-making tasks. However, their accuracy can degrade sharply in unfamiliar environments, creating significant safety concerns. Traditional ensemble methods aim to improve robustness by averaging or voting across multiple controllers, yet this often dilutes the specialized strengths that individual controllers exhibit in different operating contexts. We argue that, rather than blending controller outputs, a monitoring framework should identify and exploit these contextual strengths. In this paper, we reformulate the design of safe AI-based control ensembles as a contextual monitoring problem. A monitor continuously observes the system's context and selects the controller best suited to the current conditions. To achieve this, we cast monitor learning as a contextual learning task and draw on techniques from contextual multi-armed bandits. Our approach comes with two key benefits: (1) theoretical safety guarantees during controller selection, and (2) improved utilization of controller diversity. We validate our framework in two simulated autonomous driving scenarios, demonstrating significant improvements in both safety and performance compared to non-contextual baselines.

</details>


### [169] [Continual GUI Agents](https://arxiv.org/abs/2601.20732)
*Ziwei Liu,Borui Kang,Hangjie Yuan,Zixiang Zhao,Wei Li,Yifan Zhu,Tao Feng*

Main category: cs.LG

TL;DR: GUI-AiF: A reinforcement fine-tuning framework for continual GUI agents that stabilizes learning under shifting domains/resolutions using novel anchoring rewards.


<details>
  <summary>Details</summary>
Motivation: GUI agents trained on static environments deteriorate when faced with changing digital environments (new domains/resolutions over time). Existing methods fail to maintain stable grounding as GUI distributions shift due to diverse UI interaction points and regions.

Method: Introduces GUI-Anchoring in Flux (GUI-AiF), a reinforcement fine-tuning framework with two novel rewards: Anchoring Point Reward in Flux (APR-iF) and Anchoring Region Reward in Flux (ARR-iF). These rewards guide agents to align with shifting interaction points and regions, preventing over-adaptation to static grounding cues.

Result: Extensive experiments show GUI-AiF surpasses state-of-the-art baselines. The framework successfully maintains stable grounding as GUI distributions shift over time.

Conclusion: Establishes the first continual learning framework for GUI agents, revealing the untapped potential of reinforcement fine-tuning for continual GUI Agents in dynamic digital environments.

Abstract: As digital environments (data distribution) are in flux, with new GUI data arriving over time-introducing new domains or resolutions-agents trained on static environments deteriorate in performance. In this work, we introduce Continual GUI Agents, a new task that requires GUI agents to perform continual learning under shifted domains and resolutions. We find existing methods fail to maintain stable grounding as GUI distributions shift over time, due to the diversity of UI interaction points and regions in fluxing scenarios. To address this, we introduce GUI-Anchoring in Flux (GUI-AiF), a new reinforcement fine-tuning framework that stabilizes continual learning through two novel rewards: Anchoring Point Reward in Flux (APR-iF) and Anchoring Region Reward in Flux (ARR-iF). These rewards guide the agents to align with shifting interaction points and regions, mitigating the tendency of existing reward strategies to over-adapt to static grounding cues (e.g., fixed coordinates or element scales). Extensive experiments show GUI-AiF surpasses state-of-the-art baselines. Our work establishes the first continual learning framework for GUI agents, revealing the untapped potential of reinforcement fine-tuning for continual GUI Agents.

</details>


### [170] [C3Box: A CLIP-based Class-Incremental Learning Toolbox](https://arxiv.org/abs/2601.20852)
*Hao Sun,Da-Wei Zhou*

Main category: cs.LG

TL;DR: C3Box is a modular Python toolbox for CLIP-based Class-Incremental Learning that unifies disparate methods into a standardized framework for fair comparisons and reproducibility.


<details>
  <summary>Details</summary>
Motivation: Existing CLIP-based CIL methods are scattered across different codebases with inconsistent configurations, making fair comparisons, reproducibility, and practical adoption difficult.

Method: C3Box integrates traditional CIL methods, ViT-based CIL methods, and state-of-the-art CLIP-based CIL methods into a unified CLIP-based framework with JSON-based configuration and standardized execution pipeline, inheriting PyCIL's streamlined design.

Result: The toolbox provides reproducible experimentation with low engineering overhead and serves as a reliable benchmark platform for continual learning research, being user-friendly and compatible with major operating systems.

Conclusion: C3Box addresses the fragmentation in CLIP-based CIL research by providing a comprehensive, modular toolbox that enables fair comparisons and reproducible experiments, facilitating practical adoption of continual learning methods.

Abstract: Traditional machine learning systems are typically designed for static data distributions, which suffer from catastrophic forgetting when learning from evolving data streams. Class-Incremental Learning (CIL) addresses this challenge by enabling learning systems to continuously learn new classes while preserving prior knowledge. With the rise of pre-trained models (PTMs) such as CLIP, leveraging their strong generalization and semantic alignment capabilities has become a promising direction in CIL. However, existing CLIP-based CIL methods are often scattered across disparate codebases, rely on inconsistent configurations, hindering fair comparisons, reproducibility, and practical adoption. Therefore, we propose C3Box (CLIP-based Class-inCremental learning toolBOX), a modular and comprehensive Python toolbox. C3Box integrates representative traditional CIL methods, ViT-based CIL methods, and state-of-the-art CLIP-based CIL methods into a unified CLIP-based framework. By inheriting the streamlined design of PyCIL, C3Box provides a JSON-based configuration and standardized execution pipeline. This design enables reproducible experimentation with low engineering overhead and makes C3Box a reliable benchmark platform for continual learning research. Designed to be user-friendly, C3Box relies only on widely used open-source libraries and supports major operating systems. The code is available at https://github.com/LAMDA-CL/C3Box.

</details>


### [171] [Reinforcement Unlearning via Group Relative Policy Optimization](https://arxiv.org/abs/2601.20568)
*Efstratios Zaradoukas,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.LG

TL;DR: PURGE is a novel LLM unlearning method that uses intrinsic rewards to penalize forbidden concepts, achieving efficient unlearning while maintaining model fluency and robustness.


<details>
  <summary>Details</summary>
Motivation: LLMs memorize sensitive/copyrighted data during pretraining, creating compliance issues with GDPR and EU AI Act. Existing unlearning methods leak data, sacrifice fluency, or require expensive external reward models.

Method: PURGE (Policy Unlearning through Relative Group Erasure) uses Group Relative Policy Optimization framework with intrinsic reward signals that penalize mentions of forbidden concepts, treating unlearning as a verifiable problem.

Result: Reduces token usage by 46x vs SotA, improves fluency by 5.48%, boosts adversarial robustness by 12.02%, achieves 11% unlearning effectiveness while preserving 98% of original utility on RWKU benchmark.

Conclusion: Framing LLM unlearning as a verifiable task enables reliable, efficient, and scalable forgetting, offering a promising direction with theoretical guarantees, improved safety, and practical deployment efficiency.

Abstract: During pretraining, LLMs inadvertently memorize sensitive or copyrighted data, posing significant compliance challenges under legal frameworks like the GDPR and the EU AI Act. Fulfilling these mandates demands techniques that can remove information from a deployed model without retraining from scratch. Existing unlearning approaches attempt to address this need, but often leak the very data they aim to erase, sacrifice fluency and robustness, or depend on costly external reward models. We introduce PURGE (Policy Unlearning through Relative Group Erasure), a novel method grounded in the Group Relative Policy Optimization framework that formulates unlearning as a verifiable problem. PURGE uses an intrinsic reward signal that penalizes any mention of forbidden concepts, allowing safe and consistent unlearning. Our approach reduces token usage per target by up to a factor of 46 compared with SotA methods, while improving fluency by 5.48 percent and adversarial robustness by 12.02 percent over the base model. On the Real World Knowledge Unlearning (RWKU) benchmark, PURGE achieves 11 percent unlearning effectiveness while preserving 98 percent of original utility. PURGE shows that framing LLM unlearning as a verifiable task, enables more reliable, efficient, and scalable forgetting, suggesting a promising new direction for unlearning research that combines theoretical guarantees, improved safety, and practical deployment efficiency.

</details>


### [172] [Adapting the Behavior of Reinforcement Learning Agents to Changing Action Spaces and Reward Functions](https://arxiv.org/abs/2601.20714)
*Raul de la Rosa,Ivana Dusparic,Nicolas Cardozo*

Main category: cs.LG

TL;DR: MORPHIN is a self-adaptive Q-learning framework that enables RL agents to adapt to non-stationary environments with changing reward functions and expanding action spaces without full retraining.


<details>
  <summary>Details</summary>
Motivation: RL agents struggle in real-world applications with non-stationary environments where reward functions shift and action spaces expand dynamically, requiring adaptation without catastrophic forgetting.

Method: Integrates concept drift detection with dynamic adjustments to learning and exploration hyperparameters, enabling on-the-fly adaptation while preserving prior policy knowledge.

Result: MORPHIN achieves superior convergence speed and continuous adaptation compared to standard Q-learning, improving learning efficiency by up to 1.7x in Gridworld and traffic signal control simulations.

Conclusion: The framework successfully enables RL agents to adapt to changing environments while preventing catastrophic forgetting, demonstrating practical value for real-world non-stationary applications.

Abstract: Reinforcement Learning (RL) agents often struggle in real-world applications where environmental conditions are non-stationary, particularly when reward functions shift or the available action space expands. This paper introduces MORPHIN, a self-adaptive Q-learning framework that enables on-the-fly adaptation without full retraining. By integrating concept drift detection with dynamic adjustments to learning and exploration hyperparameters, MORPHIN adapts agents to changes in both the reward function and on-the-fly expansions of the agent's action space, while preserving prior policy knowledge to prevent catastrophic forgetting. We validate our approach using a Gridworld benchmark and a traffic signal control simulation. The results demonstrate that MORPHIN achieves superior convergence speed and continuous adaptation compared to a standard Q-learning baseline, improving learning efficiency by up to 1.7x.

</details>


### [173] [HESTIA: A Hessian-Guided Differentiable Quantization-Aware Training Framework for Extremely Low-Bit LLMs](https://arxiv.org/abs/2601.20745)
*Guoan Wang,Feiyu Wang,Zongwei Lv,Yikun Zong,Tong Yang*

Main category: cs.LG

TL;DR: Hestia: A Hessian-guided differentiable QAT framework for extremely low-bit LLMs that uses softmax relaxation and Hessian-driven temperature annealing to improve 1.58-bit quantization performance.


<details>
  <summary>Details</summary>
Motivation: Memory bottlenecks in large language model deployment motivate extremely low-bit quantization, but existing QAT methods use hard rounding and STE from the start, causing premature discretization and gradient mismatch that hinders effective optimization.

Method: Replaces rigid step function with temperature-controlled softmax relaxation to maintain gradient flow early in training, then progressively hardens quantization. Uses tensor-wise Hessian trace as lightweight curvature signal to drive fine-grained temperature annealing for sensitivity-aware discretization.

Result: Outperforms existing ternary QAT baselines on Llama-3.2, achieving average zero-shot improvements of 5.39% and 4.34% for 1B and 3B models respectively. Hessian-guided relaxation effectively recovers representational capacity for 1.58-bit LLMs.

Conclusion: Hestia establishes a more robust training path for extremely low-bit LLMs by addressing gradient mismatch issues through Hessian-guided differentiable quantization, enabling better performance in memory-constrained deployments.

Abstract: As large language models (LLMs) continue to scale, deployment is increasingly bottlenecked by the memory wall, motivating a shift toward extremely low-bit quantization. However, most quantization-aware training (QAT) methods apply hard rounding and the straight-through estimator (STE) from the beginning of the training, which prematurely discretizes the optimization landscape and induces persistent gradient mismatch between latent weights and quantized weights, hindering effective optimization of quantized models. To address this, we propose Hestia, a Hessian-guided differentiable QAT framework for extremely low-bit LLMs, which replaces the rigid step function with a temperature-controlled softmax relaxation to maintain gradient flow early in training while progressively hardening quantization. Furthermore, Hestia leverages a tensor-wise Hessian trace metric as a lightweight curvature signal to drive fine-grained temperature annealing, enabling sensitivity-aware discretization across the model. Evaluations on Llama-3.2 show that Hestia consistently outperforms existing ternary QAT baselines, yielding average zero-shot improvements of 5.39% and 4.34% for the 1B and 3B models. These results indicate that Hessian-guided relaxation effectively recovers representational capacity, establishing a more robust training path for 1.58-bit LLMs. The code is available at https://github.com/hestia2026/Hestia.

</details>


### [174] [CoBA: Integrated Deep Learning Model for Reliable Low-Altitude UAV Classification in mmWave Radio Networks](https://arxiv.org/abs/2601.20605)
*Junaid Sajid,Ivo MÃ¼Ã¼rsepp,Luca Reggiani,Davide Scazzoli,Federico Francesco Luigi Mariani,Maurizio Magarini,Rizwan Ahmad,Muhammad Mahtab Alam*

Main category: cs.LG

TL;DR: CoBA: A deep learning model combining CNN, BiLSTM, and Attention layers that uses 5G mmWave radio measurements to classify UAV operations in authorized vs. restricted airspaces at low altitudes.


<details>
  <summary>Details</summary>
Motivation: As UAV usage increases in civilian/industrial applications, secure low-altitude operations become crucial. Current methods struggle to accurately classify UAVs in dense mmWave environments due to complex propagation and signal variability.

Method: Proposes CoBA model integrating convolutional (CNN), bidirectional recurrent (BiLSTM), and attention layers to capture both spatial and temporal patterns in UAV radio measurements. Uses 5G mmWave radio measurements collected from controlled UAV flights at TalTech.

Result: CoBA achieves superior accuracy, significantly outperforming conventional ML models and fingerprinting-based benchmarks, demonstrating potential for reliable UAV airspace monitoring.

Conclusion: The CoBA model effectively addresses the challenge of classifying UAV operations in authorized vs. restricted airspaces using 5G mmWave measurements, showing promise for secure low-altitude UAV operations monitoring.

Abstract: Uncrewed Aerial Vehicles (UAVs) are increasingly used in civilian and industrial applications, making secure low-altitude operations crucial. In dense mmWave environments, accurately classifying low-altitude UAVs as either inside authorized or restricted airspaces remains challenging, requiring models that handle complex propagation and signal variability. This paper proposes a deep learning model, referred to as CoBA, which stands for integrated Convolutional Neural Network (CNN), Bidirectional Long Short-Term Memory (BiLSTM), and Attention which leverages Fifth Generation (5G) millimeter-wave (mmWave) radio measurements to classify UAV operations in authorized and restricted airspaces at low altitude. The proposed CoBA model integrates convolutional, bidirectional recurrent, and attention layers to capture both spatial and temporal patterns in UAV radio measurements. To validate the model, a dedicated dataset is collected using the 5G mmWave network at TalTech, with controlled low altitude UAV flights in authorized and restricted scenarios. The model is evaluated against conventional ML models and a fingerprinting-based benchmark. Experimental results show that CoBA achieves superior accuracy, significantly outperforming all baseline models and demonstrating its potential for reliable and regulated UAV airspace monitoring.

</details>


### [175] [Conditional PED-ANOVA: Hyperparameter Importance in Hierarchical & Dynamic Search Spaces](https://arxiv.org/abs/2601.20800)
*Kaito Baba,Yoshihiko Ozaki,Shuhei Watanabe*

Main category: cs.LG

TL;DR: condPED-ANOVA extends PED-ANOVA to estimate hyperparameter importance in conditional search spaces where hyperparameters can depend on each other.


<details>
  <summary>Details</summary>
Motivation: Existing hyperparameter importance (HPI) estimators like PED-ANOVA assume fixed, unconditional search spaces and fail to properly handle conditional hyperparameters whose presence or domain depends on other hyperparameters.

Method: Introduces conditional HPI for top-performing regions and derives a closed-form estimator that accurately reflects conditional activation and domain changes in hyperparameter dependencies.

Result: Naive adaptations of existing HPI estimators yield misleading or uninterpretable importance estimates in conditional settings, while condPED-ANOVA consistently provides meaningful importances that reflect the underlying conditional structure.

Conclusion: condPED-ANOVA provides a principled framework for accurate hyperparameter importance estimation in conditional search spaces, addressing limitations of existing methods that cannot properly handle hyperparameter dependencies.

Abstract: We propose conditional PED-ANOVA (condPED-ANOVA), a principled framework for estimating hyperparameter importance (HPI) in conditional search spaces, where the presence or domain of a hyperparameter can depend on other hyperparameters. Although the original PED-ANOVA provides a fast and efficient way to estimate HPI within the top-performing regions of the search space, it assumes a fixed, unconditional search space and therefore cannot properly handle conditional hyperparameters. To address this, we introduce a conditional HPI for top-performing regions and derive a closed-form estimator that accurately reflects conditional activation and domain changes. Experiments show that naive adaptations of existing HPI estimators yield misleading or uninterpretable importance estimates in conditional settings, whereas condPED-ANOVA consistently provides meaningful importances that reflect the underlying conditional structure.

</details>


### [176] [Reinforcement Learning via Self-Distillation](https://arxiv.org/abs/2601.20802)
*Jonas HÃ¼botter,Frederike LÃ¼beck,Lejs Behric,Anton Baumann,Marco Bagatella,Daniel Marta,Ido Hakimi,Idan Shenfeld,Thomas Kleine Buening,Carlos Guestrin,Andreas Krause*

Main category: cs.LG

TL;DR: SDPO is a reinforcement learning method that leverages rich textual feedback (like error messages) to improve learning efficiency in verifiable domains, outperforming traditional RL methods that only use scalar rewards.


<details>
  <summary>Details</summary>
Motivation: Current RL methods in verifiable domains (code, math) suffer from credit assignment bottlenecks because they only learn from scalar outcome rewards, ignoring rich textual feedback (runtime errors, judge evaluations) that explains why attempts fail.

Method: Self-Distillation Policy Optimization (SDPO) converts tokenized feedback into dense learning signals without external teachers or reward models. It treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy, leveraging the model's ability to retrospectively identify mistakes in-context.

Result: SDPO improves sample efficiency and final accuracy over strong RLVR baselines across scientific reasoning, tool use, and competitive programming (LiveCodeBench v6). It also outperforms baselines in standard RLVR environments by using successful rollouts as implicit feedback. On difficult binary-reward tasks, SDPO achieves same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.

Conclusion: SDPO effectively leverages rich textual feedback to overcome credit assignment bottlenecks in RL, demonstrating superior performance across multiple verifiable domains and enabling more efficient problem-solving in challenging binary-reward environments.

Abstract: Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.

</details>


### [177] [ACFormer: Mitigating Non-linearity with Auto Convolutional Encoder for Time Series Forecasting](https://arxiv.org/abs/2601.20611)
*Gawon Lee,Hanbyeol Park,Minseop Kim,Dohee Kim,Hyerim Bae*

Main category: cs.LG

TL;DR: ACFormer combines linear efficiency with CNN's non-linear feature extraction for time series forecasting, addressing limitations of linear models in capturing complex temporal patterns.


<details>
  <summary>Details</summary>
Motivation: Linear models for time series forecasting are efficient but struggle with non-linear signals and complex intra-channel dependencies. There's a need to reconcile linear efficiency with non-linear feature extraction capabilities.

Method: Conducted receptive field analysis of CNN TSF models, introduced "individual receptive field" concept, then proposed ACFormer architecture with shared compression module, gated attention for temporal locality, and independent patch expansion layer.

Result: ACFormer consistently achieves state-of-the-art performance on multiple benchmark datasets, effectively mitigating linear models' drawbacks in capturing high-frequency components.

Conclusion: ACFormer successfully reconciles linear projection efficiency with convolutional non-linear feature extraction, demonstrating superior robustness to non-linear fluctuations while maintaining forecasting accuracy.

Abstract: Time series forecasting (TSF) faces challenges in modeling complex intra-channel temporal dependencies and inter-channel correlations. Although recent research has highlighted the efficiency of linear architectures in capturing global trends, these models often struggle with non-linear signals. To address this gap, we conducted a systematic receptive field analysis of convolutional neural network (CNN) TSF models. We introduce the "individual receptive field" to uncover granular structural dependencies, revealing that convolutional layers act as feature extractors that mirror channel-wise attention while exhibiting superior robustness to non-linear fluctuations. Based on these insights, we propose ACFormer, an architecture designed to reconcile the efficiency of linear projections with the non-linear feature-extraction power of convolutions. ACFormer captures fine-grained information through a shared compression module, preserves temporal locality via gated attention, and reconstructs variable-specific temporal patterns using an independent patch expansion layer. Extensive experiments on multiple benchmark datasets demonstrate that ACFormer consistently achieves state-of-the-art performance, effectively mitigating the inherent drawbacks of linear models in capturing high-frequency components.

</details>


### [178] [GNN Explanations that do not Explain and How to find Them](https://arxiv.org/abs/2601.20815)
*Steve Azzolin,Stefano Teso,Bruno Lepri,Andrea Passerini,Sagar Malhotra*

Main category: cs.LG

TL;DR: SE-GNN explanations can be degenerate (unrelated to actual inference) while models still achieve optimal performance, and current faithfulness metrics fail to detect this. The paper introduces a new metric that reliably identifies these degenerate explanations.


<details>
  <summary>Details</summary>
Motivation: Self-explainable GNN explanations are crucial for model transparency and detecting misuse of sensitive attributes, but current explanations can be misleading and their failure cases are not well characterized. There's a need to identify and address critical failures in SE-GNN explanations.

Method: The paper identifies a critical failure mode where SE-GNN explanations can be completely unrelated to how the models actually infer labels. It shows that models can achieve optimal true risk while producing these degenerate explanations, and that current faithfulness metrics fail to detect them. The authors then introduce a novel faithfulness metric designed to reliably mark degenerate explanations as unfaithful.

Result: Empirical analysis reveals that degenerate explanations can be maliciously planted (to hide use of sensitive attributes) or emerge naturally. The proposed faithfulness metric successfully identifies degenerate explanations in both malicious and natural settings, unlike existing metrics.

Conclusion: SE-GNN explanations can be fundamentally misleading despite optimal model performance, highlighting the need for reliable auditing. The introduced faithfulness metric provides a solution for detecting degenerate explanations, addressing both malicious and natural failure cases.

Abstract: Explanations provided by Self-explainable Graph Neural Networks (SE-GNNs) are fundamental for understanding the model's inner workings and for identifying potential misuse of sensitive attributes. Although recent works have highlighted that these explanations can be suboptimal and potentially misleading, a characterization of their failure cases is unavailable. In this work, we identify a critical failure of SE-GNN explanations: explanations can be unambiguously unrelated to how the SE-GNNs infer labels. We show that, on the one hand, many SE-GNNs can achieve optimal true risk while producing these degenerate explanations, and on the other, most faithfulness metrics can fail to identify these failure modes. Our empirical analysis reveals that degenerate explanations can be maliciously planted (allowing an attacker to hide the use of sensitive attributes) and can also emerge naturally, highlighting the need for reliable auditing. To address this, we introduce a novel faithfulness metric that reliably marks degenerate explanations as unfaithful, in both malicious and natural settings. Our code is available in the supplemental.

</details>


### [179] [DIVERSE: Disagreement-Inducing Vector Evolution for Rashomon Set Exploration](https://arxiv.org/abs/2601.20627)
*Gilles Eerlings,Brent Zoomers,Jori Liesenborgs,Gustavo Rovelo Ruiz,Kris Luyten*

Main category: cs.LG

TL;DR: DIVERSE is a framework for exploring the Rashomon set of neural networks (models with similar accuracy but different predictions) using FiLM layers and CMA-ES optimization without retraining.


<details>
  <summary>Details</summary>
Motivation: The Rashomon set contains multiple models with similar accuracy but different predictive behaviors. Understanding this diversity is important for robustness and model multiplicity, but exploring this set through retraining is computationally expensive.

Method: DIVERSE augments pretrained models with Feature-wise Linear Modulation (FiLM) layers and uses Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to search a latent modulation space, generating diverse model variants without retraining or gradient access.

Result: Across MNIST, PneumoniaMNIST, and CIFAR-10, DIVERSE uncovers multiple high-performing yet functionally distinct models, achieving comparable diversity to retraining at reduced computational cost while maintaining robustness and performance.

Conclusion: DIVERSE offers a competitive and efficient exploration of the Rashomon set, making it feasible to construct diverse model sets that support well-balanced model multiplicity without the computational burden of retraining.

Abstract: We propose DIVERSE, a framework for systematically exploring the Rashomon set of deep neural networks, the collection of models that match a reference model's accuracy while differing in their predictive behavior. DIVERSE augments a pretrained model with Feature-wise Linear Modulation (FiLM) layers and uses Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to search a latent modulation space, generating diverse model variants without retraining or gradient access. Across MNIST, PneumoniaMNIST, and CIFAR-10, DIVERSE uncovers multiple high-performing yet functionally distinct models. Our experiments show that DIVERSE offers a competitive and efficient exploration of the Rashomon set, making it feasible to construct diverse sets that maintain robustness and performance while supporting well-balanced model multiplicity. While retraining remains the baseline to generate Rashomon sets, DIVERSE achieves comparable diversity at reduced computational cost.

</details>


### [180] [Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning](https://arxiv.org/abs/2601.20829)
*Minwu Kim,Safal Shrestha,Keith Ross*

Main category: cs.LG

TL;DR: Failure-prefix conditioning improves RLVR training on saturated problems by exposing models to rare incorrect reasoning trajectories, matching medium-difficulty problem gains with token efficiency.


<details>
  <summary>Details</summary>
Motivation: RLVR improves LLM reasoning but training stalls on saturated problems due to poor accessibility of informative failures - learning signals exist but are rarely encountered during standard rollouts.

Method: Failure-prefix conditioning: reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, exposing models to failure-prone states. Also explores iterative approach that refreshes failure prefixes during training.

Result: Method yields performance gains matching medium-difficulty problem training while preserving token efficiency. Reduces performance degradation under misleading failure prefixes (with mild trade-off in adherence to correct early reasoning). Iterative approach unlocks additional gains after plateaus.

Conclusion: Failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems by strategically exposing models to informative failures.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the model's robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with a mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems.

</details>


### [181] [A Foundation Model for Virtual Sensors](https://arxiv.org/abs/2601.20634)
*Leon GÃ¶tz,Lars Frederik Peiss,Erik Sauer,Andreas Udo Sass,Thorsten Bagdonat,Stephan GÃ¼nnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: First foundation model for virtual sensors that simultaneously predicts diverse sensors efficiently, learns relevant inputs automatically, and scales to hundreds of sensors with minimal computation/memory overhead.


<details>
  <summary>Details</summary>
Motivation: Existing virtual sensor approaches require application-specific models with hand-selected inputs for each sensor, cannot leverage task synergies, lack consistent benchmarks, and emerging time series foundation models are computationally expensive and limited to predicting input signals rather than virtual sensors.

Method: Introduces a unified foundation model architecture that can simultaneously predict diverse virtual sensors while maintaining computational efficiency. The model learns relevant input signals for each virtual sensor automatically, eliminating the need for expert knowledge while adding explainability.

Result: Achieves 415x reduction in computation time and 951x reduction in memory requirements compared to baselines, while maintaining or improving predictive quality. The model scales gracefully to hundreds of virtual sensors with nearly constant parameter count, enabling practical deployment in large-scale sensor networks.

Conclusion: The proposed foundation model addresses key limitations of existing virtual sensor approaches by providing a unified, efficient, and scalable solution that learns relevant inputs automatically and exploits task synergies, making it practical for large-scale industrial deployment.

Abstract: Virtual sensors use machine learning to predict target signals from available measurements, replacing expensive physical sensors in critical applications. Existing virtual sensor approaches require application-specific models with hand-selected inputs for each sensor, cannot leverage task synergies, and lack consistent benchmarks. At the same time, emerging time series foundation models are computationally expensive and limited to predicting their input signals, making them incompatible with virtual sensors. We introduce the first foundation model for virtual sensors addressing both limitations. Our unified model can simultaneously predict diverse virtual sensors exploiting synergies while maintaining computational efficiency. It learns relevant input signals for each virtual sensor, eliminating expert knowledge requirements while adding explainability. In our large-scale evaluation on a standard benchmark and an application-specific dataset with over 18 billion samples, our architecture achieves 415x reduction in computation time and 951x reduction in memory requirements, while maintaining or even improving predictive quality compared to baselines. Our model scales gracefully to hundreds of virtual sensors with nearly constant parameter count, enabling practical deployment in large-scale sensor networks.

</details>


### [182] [An Empirical Investigation of Neural ODEs and Symbolic Regression for Dynamical Systems](https://arxiv.org/abs/2601.20637)
*Panayiotis Ioannou,Pietro LiÃ²,Pietro Cicuta*

Main category: cs.LG

TL;DR: NODEs can extrapolate to new boundary conditions with dynamic similarity, SR recovers equations from noisy data with correct variable selection, and SR can recover equations from NODE-generated data trained on limited samples.


<details>
  <summary>Details</summary>
Motivation: To explore how Neural ODEs and Symbolic Regression can work together to model complex system dynamics and discover governing equations from limited or noisy data, accelerating scientific discovery.

Method: Used noisy synthetic data from two damped oscillatory systems to test NODE extrapolation capabilities and SR equation recovery, including using NODE-generated data trained on only 10% of full simulation.

Result: NODEs extrapolate effectively when trajectories share dynamic similarity; SR recovers equations from noisy ground-truth data with correct variable selection; SR recovers 2/3 equations plus approximation from NODE data trained on 10% sample.

Conclusion: Combining NODEs to enrich limited data with SR to infer physical laws represents a promising approach for scientific discovery, though further work is needed for complete equation recovery.

Abstract: Accurately modelling the dynamics of complex systems and discovering their governing differential equations are critical tasks for accelerating scientific discovery. Using noisy, synthetic data from two damped oscillatory systems, we explore the extrapolation capabilities of Neural Ordinary Differential Equations (NODEs) and the ability of Symbolic Regression (SR) to recover the underlying equations. Our study yields three key insights. First, we demonstrate that NODEs can extrapolate effectively to new boundary conditions, provided the resulting trajectories share dynamic similarity with the training data. Second, SR successfully recovers the equations from noisy ground-truth data, though its performance is contingent on the correct selection of input variables. Finally, we find that SR recovers two out of the three governing equations, along with a good approximation for the third, when using data generated by a NODE trained on just 10% of the full simulation. While this last finding highlights an area for future work, our results suggest that using NODEs to enrich limited data and enable symbolic regression to infer physical laws represents a promising new approach for scientific discovery.

</details>


### [183] [Reward Models Inherit Value Biases from Pretraining](https://arxiv.org/abs/2601.20838)
*Brian Christian,Jessica A. F. Thompson,Elle Michelle Yang,Vincent Adam,Hannah Rose Kirk,Christopher Summerfield,Tsvetomira Dumbalska*

Main category: cs.LG

TL;DR: RMs inherit value biases from their base LLMs, with Llama-based RMs preferring "agency" and Gemma-based RMs preferring "communion" even with identical training data.


<details>
  <summary>Details</summary>
Motivation: Reward models are crucial for aligning LLMs with human values but are understudied compared to pre-trained and post-trained LLMs. The influence of base model representations on RM behavior remains unclear.

Method: Comprehensive study of 10 open-weight RMs using validated psycholinguistic corpora. Analyzed using "Big Two" psychological axes (agency vs communion). Derived implicit reward scores from log-probability differences. Conducted ablation experiments varying preference data source and quantity.

Result: RMs exhibit significant value differences based on their base model: Llama RMs robustly prefer "agency" while Gemma RMs robustly prefer "communion." This effect persists even with identical preference data and finetuning, and is traceable to logits of instruction-tuned/pre-trained models. The effect is repeatable and durable across different training conditions.

Conclusion: RM outputs are significantly influenced by their pretrained base LLMs, not just human preference data. This underscores the importance of safety/alignment efforts at pretraining stage and shows that base model choice involves value considerations beyond just performance.

Abstract: Reward models (RMs) are central to aligning large language models (LLMs) with human values but have received less attention than pre-trained and post-trained LLMs themselves. Because RMs are initialized from LLMs, they inherit representations that shape their behavior, but the nature and extent of this influence remain understudied. In a comprehensive study of 10 leading open-weight RMs using validated psycholinguistic corpora, we show that RMs exhibit significant differences along multiple dimensions of human value as a function of their base model. Using the "Big Two" psychological axes, we show a robust preference of Llama RMs for "agency" and a corresponding robust preference of Gemma RMs for "communion." This phenomenon holds even when the preference data and finetuning process are identical, and we trace it back to the logits of the respective instruction-tuned and pre-trained models. These log-probability differences themselves can be formulated as an implicit RM; we derive usable implicit reward scores and show that they exhibit the very same agency/communion difference. We run experiments training RMs with ablations for preference data source and quantity, which demonstrate that this effect is not only repeatable but surprisingly durable. Despite RMs being designed to represent human preferences, our evidence shows that their outputs are influenced by the pretrained LLMs on which they are based. This work underscores the importance of safety and alignment efforts at the pretraining stage, and makes clear that open-source developers' choice of base model is as much a consideration of values as of performance.

</details>


### [184] [$\mathbb{R}^{2k}$ is Theoretically Large Enough for Embedding-based Top-$k$ Retrieval](https://arxiv.org/abs/2601.20844)
*Zihao Wang,Hang Yin,Lihui Liu,Hanghang Tong,Yangqiu Song,Ginny Wong,Simon See*

Main category: cs.LG

TL;DR: The paper studies Minimal Embeddable Dimension (MED) for embedding subsets into vector spaces, finding tight bounds that show logarithmic dependency on element count, suggesting retrieval limitations are due to learnability rather than geometric constraints.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental geometric limitations of embedding-based retrieval systems, specifically whether the difficulty in learning effective embeddings stems from geometric constraints or learning challenges.

Method: Theoretical derivation of tight bounds for MED under various distance/similarity measures (â„“â‚‚ metric, inner product, cosine similarity), plus numerical simulations using centroid-based subset embeddings.

Result: Tight bounds show logarithmic dependency between MED and number of elements, with empirical support. Simulations easily achieve this logarithmic scaling using centroid embeddings.

Conclusion: Embedding-based retrieval limitations primarily arise from learnability challenges rather than geometric constraints, providing guidance for future algorithm design.

Abstract: This paper studies the minimal dimension required to embed subset memberships ($m$ elements and ${m\choose k}$ subsets of at most $k$ elements) into vector spaces, denoted as Minimal Embeddable Dimension (MED). The tight bounds of MED are derived theoretically and supported empirically for various notions of "distances" or "similarities," including the $\ell_2$ metric, inner product, and cosine similarity. In addition, we conduct numerical simulation in a more achievable setting, where the ${m\choose k}$ subset embeddings are chosen as the centroid of the embeddings of the contained elements. Our simulation easily realizes a logarithmic dependency between the MED and the number of elements to embed. These findings imply that embedding-based retrieval limitations stem primarily from learnability challenges, not geometric constraints, guiding future algorithm design.

</details>


### [185] [MuRAL-CPD: Active Learning for Multiresolution Change Point Detection](https://arxiv.org/abs/2601.20686)
*Stefano Bertolasi,Diego Carrera,Diego Stucchi,Pasqualina Fragneto,Luigi Amedeo Bianchi*

Main category: cs.LG

TL;DR: MuRAL-CPD is a semi-supervised change point detection method that combines wavelet-based multiresolution analysis with active learning to incorporate user feedback for optimizing hyperparameters and aligning with user-defined change notions.


<details>
  <summary>Details</summary>
Motivation: Traditional unsupervised CPD methods lack adaptability to task-specific definitions of change and cannot benefit from user knowledge, limiting their effectiveness in real-world applications where user expertise could improve detection accuracy.

Method: MuRAL-CPD integrates active learning into a multiresolution CPD algorithm using wavelet-based decomposition to detect changes across multiple temporal scales. It incorporates user feedback to iteratively optimize key hyperparameters, allowing the model to align with user-defined change notions.

Result: Experimental results on several real-world datasets demonstrate MuRAL-CPD's effectiveness against state-of-the-art methods, particularly in scenarios with minimal supervision available.

Conclusion: MuRAL-CPD successfully addresses limitations of traditional CPD methods by incorporating user feedback through active learning, improving both accuracy and interpretability while maintaining effectiveness with minimal supervision.

Abstract: Change Point Detection (CPD) is a critical task in time series analysis, aiming to identify moments when the underlying data-generating process shifts. Traditional CPD methods often rely on unsupervised techniques, which lack adaptability to task-specific definitions of change and cannot benefit from user knowledge. To address these limitations, we propose MuRAL-CPD, a novel semi-supervised method that integrates active learning into a multiresolution CPD algorithm. MuRAL-CPD leverages a wavelet-based multiresolution decomposition to detect changes across multiple temporal scales and incorporates user feedback to iteratively optimize key hyperparameters. This interaction enables the model to align its notion of change with that of the user, improving both accuracy and interpretability. Our experimental results on several real-world datasets show the effectiveness of MuRAL-CPD against state-of-the-art methods, particularly in scenarios where minimal supervision is available.

</details>


### [186] [Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation](https://arxiv.org/abs/2601.20848)
*Weixin Chen,Li Chen,Yuhan Zhao*

Main category: cs.LG

TL;DR: Cofair enables post-training fairness control in recommender systems without retraining, allowing dynamic adjustment of fairness levels through adapter modules and user-level regularization.


<details>
  <summary>Details</summary>
Motivation: Existing fairness-aware methods fix fairness requirements at training time, but real-world scenarios need flexibility as different stakeholders may demand varying fairness levels over time. Retraining for each new requirement is impractical.

Method: Cofair uses a shared representation layer with fairness-conditioned adapter modules to generate user embeddings for different fairness levels, plus a user-level regularization term that ensures monotonic fairness improvements across levels.

Result: Experiments on multiple datasets and backbone models show Cofair provides dynamic fairness at different levels, achieving comparable or better fairness-accuracy curves than state-of-the-art baselines without retraining for each requirement.

Conclusion: Cofair offers a practical solution for post-training fairness control in recommender systems, enabling flexible adaptation to changing fairness requirements while maintaining performance.

Abstract: Despite growing efforts to mitigate unfairness in recommender systems, existing fairness-aware methods typically fix the fairness requirement at training time and provide limited post-training flexibility. However, in real-world scenarios, diverse stakeholders may demand differing fairness requirements over time, so retraining for different fairness requirements becomes prohibitive. To address this limitation, we propose Cofair, a single-train framework that enables post-training fairness control in recommendation. Specifically, Cofair introduces a shared representation layer with fairness-conditioned adapter modules to produce user embeddings specialized for varied fairness levels, along with a user-level regularization term that guarantees user-wise monotonic fairness improvements across these levels. We theoretically establish that the adversarial objective of Cofair upper bounds demographic parity and the regularization term enforces progressive fairness at user level. Comprehensive experiments on multiple datasets and backbone models demonstrate that our framework provides dynamic fairness at different levels, delivering comparable or better fairness-accuracy curves than state-of-the-art baselines, without the need to retrain for each new fairness requirement. Our code is publicly available at https://github.com/weixinchen98/Cofair.

</details>


### [187] [Positive-Unlabeled Reinforcement Learning Distillation for On-Premise Small Models](https://arxiv.org/abs/2601.20687)
*Zhiqiang Kou,Junyang Chen,Xin-Qiang Cai,Xiaobo Xia,Ming-Kun Xie,Dong-Dong Wu,Biao Liu,Yuheng Jia,Xin Geng,Masashi Sugiyama,Tat-Seng Chua*

Main category: cs.LG

TL;DR: Proposes PU RL distillation method for on-premise small models that distills teacher's preference optimization from black-box generations without human labels or reward models, enabling local RL alignment.


<details>
  <summary>Details</summary>
Motivation: On-premise deployment of small models is common due to privacy, cost, and latency constraints, but practical pipelines often stop at supervised fine-tuning and fail to reach RL alignment stage because RL typically requires expensive human preference annotation or heavy reliance on high-quality reward models with large-scale API usage and engineering maintenance, which are ill-suited for on-premise settings.

Method: Positive-unlabeled (PU) RL distillation method: query teacher once per prompt to get anchor response, locally sample multiple student candidates, perform anchor-conditioned self-ranking to induce pairwise or listwise preferences, enabling fully local training loop via direct preference optimization or group relative policy optimization.

Result: Theoretical analysis shows induced preference signal is order-consistent and concentrates on near-optimal candidates, supporting stability for preference optimization. Experiments demonstrate method achieves consistently strong performance under low-cost setting.

Conclusion: Proposed method bridges gap for on-premise small-model deployment by enabling RL alignment without human-labeled preferences or reward models, making preference optimization practical for resource-constrained environments.

Abstract: Due to constraints on privacy, cost, and latency, on-premise deployment of small models is increasingly common. However, most practical pipelines stop at supervised fine-tuning (SFT) and fail to reach the reinforcement learning (RL) alignment stage. The main reason is that RL alignment typically requires either expensive human preference annotation or heavy reliance on high-quality reward models with large-scale API usage and ongoing engineering maintenance, both of which are ill-suited to on-premise settings. To bridge this gap, we propose a positive-unlabeled (PU) RL distillation method for on-premise small-model deployment. Without human-labeled preferences or a reward model, our method distills the teacher's preference-optimization capability from black-box generations into a locally trainable student. For each prompt, we query the teacher once to obtain an anchor response, locally sample multiple student candidates, and perform anchor-conditioned self-ranking to induce pairwise or listwise preferences, enabling a fully local training loop via direct preference optimization or group relative policy optimization. Theoretical analysis justifies that the induced preference signal by our method is order-consistent and concentrates on near-optimal candidates, supporting its stability for preference optimization. Experiments demonstrate that our method achieves consistently strong performance under a low-cost setting.

</details>


### [188] [Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation](https://arxiv.org/abs/2601.20854)
*AnÃ­bal Silva,MoisÃ©s Santos,AndrÃ© Restivo,Carlos Soares*

Main category: cs.LG

TL;DR: Integrating Transformers into VAE components for tabular data shows trade-offs between fidelity/diversity and reveals high similarity between consecutive Transformer blocks with near-linear input-output relationships in decoders.


<details>
  <summary>Details</summary>
Motivation: Standard VAEs with MLPs struggle with modeling feature relationships in tabular data, especially with mixed data types. Transformers' attention mechanisms are better suited for capturing complex feature interactions, prompting investigation of Transformer-VAE integration.

Method: Empirically investigate integrating Transformers into different VAE components (encoder, decoder, latent space). Conduct experiments on 57 datasets from OpenML CC18 suite to analyze the impact on model performance.

Result: Two main findings: 1) Positioning Transformers to leverage latent and decoder representations creates a trade-off between fidelity and diversity. 2) High similarity between consecutive Transformer blocks in all components, with decoder showing approximately linear relationship between input and output.

Conclusion: Transformer integration in VAEs for tabular data reveals important architectural insights: careful positioning affects fidelity-diversity balance, and Transformer blocks exhibit redundancy with near-linear transformations in decoders, suggesting potential for architectural simplification.

Abstract: Tabular data remains a challenging domain for generative models. In particular, the standard Variational Autoencoder (VAE) architecture, typically composed of multilayer perceptrons, struggles to model relationships between features, especially when handling mixed data types. In contrast, Transformers, through their attention mechanism, are better suited for capturing complex feature interactions. In this paper, we empirically investigate the impact of integrating Transformers into different components of a VAE. We conduct experiments on 57 datasets from the OpenML CC18 suite and draw two main conclusions. First, results indicate that positioning Transformers to leverage latent and decoder representations leads to a trade-off between fidelity and diversity. Second, we observe a high similarity between consecutive blocks of a Transformer in all components. In particular, in the decoder, the relationship between the input and output of a Transformer is approximately linear.

</details>


### [189] [Optimal Transport Group Counterfactual Explanations](https://arxiv.org/abs/2601.20692)
*Enrique Valero-Leal,Bernd Bischl,Pedro LarraÃ±aga,Concha Bielza,Giuseppe Casalicchio*

Main category: cs.LG

TL;DR: Learning an optimal transport map for group counterfactual explanations that generalizes to new instances without re-optimization, minimizing group transport cost while preserving geometry.


<details>
  <summary>Details</summary>
Motivation: Existing group counterfactual explanation methods have limitations: they don't generalize to new group members, rely on strong model assumptions (like linearity), and poorly control group geometry distortion. There's a need for a method that can generalize without re-optimization while maintaining interpretability.

Method: Learn an explicit optimal transport map that sends any group instance to its counterfactual without re-optimization. For linear classifiers, derive functions representing group counterfactuals via mathematical optimization, identifying the underlying convex optimization type (QP, QCQP, etc.). The approach minimizes the group's total transport cost while preserving group geometry.

Result: The learned transport maps accurately generalize to new instances, preserve group geometry, and incur only negligible additional transport cost compared to baseline methods. For linear classifiers, the approach provides mathematically derived counterfactual functions. Even when model linearity cannot be exploited, the approach significantly outperforms baselines.

Conclusion: The proposed optimal transport approach for group counterfactual explanations successfully addresses limitations of existing methods by providing generalizable solutions with fewer parameters, better geometry preservation, and improved interpretability of common actionable recourse.

Abstract: Group counterfactual explanations find a set of counterfactual instances to explain a group of input instances contrastively. However, existing methods either (i) optimize counterfactuals only for a fixed group and do not generalize to new group members, (ii) strictly rely on strong model assumptions (e.g., linearity) for tractability or/and (iii) poorly control the counterfactual group geometry distortion. We instead learn an explicit optimal transport map that sends any group instance to its counterfactual without re-optimization, minimizing the group's total transport cost. This enables generalization with fewer parameters, making it easier to interpret the common actionable recourse. For linear classifiers, we prove that functions representing group counterfactuals are derived via mathematical optimization, identifying the underlying convex optimization type (QP, QCQP, ...). Experiments show that they accurately generalize, preserve group geometry and incur only negligible additional transport cost compared to baseline methods. If model linearity cannot be exploited, our approach also significantly outperforms the baselines.

</details>


### [190] [Evolutionary Strategies lead to Catastrophic Forgetting in LLMs](https://arxiv.org/abs/2601.20861)
*Immanuel Abdi,Akshat Gupta,Micah Mok,Alexander Lu,Nicholas Lee,Gopala Anumanchipalli*

Main category: cs.LG

TL;DR: ES performs comparably to GRPO on math/reasoning tasks but suffers from severe forgetting in continual learning due to less sparse, larger-norm updates.


<details>
  <summary>Details</summary>
Motivation: Current AI systems lack continual learning capabilities after deployment, and gradient-based methods require large memory. ES offers a gradient-free alternative but its forgetting behavior in continual learning needs investigation.

Method: Comprehensive analysis of Evolutionary Strategies (ES) with evaluation of forgetting curves across increasing update steps, comparing ES to GRPO on math and reasoning tasks with comparable compute budgets.

Result: ES achieves performance close to GRPO on math/reasoning tasks, but exhibits significant forgetting of prior abilities. ES updates are much less sparse and have orders of magnitude larger â„“â‚‚ norm compared to GRPO updates.

Conclusion: ES suffers from severe forgetting in continual learning due to its update characteristics, limiting its applicability for online training. The study highlights forgetting issues in gradient-free algorithms and calls for future work to mitigate these problems.

Abstract: One of the biggest missing capabilities in current AI systems is the ability to learn continuously after deployment. Implementing such continually learning systems have several challenges, one of which is the large memory requirement of gradient-based algorithms that are used to train state-of-the-art LLMs. Evolutionary Strategies (ES) have recently re-emerged as a gradient-free alternative to traditional learning algorithms and have shown encouraging performance on specific tasks in LLMs. In this paper, we perform a comprehensive analysis of ES and specifically evaluate its forgetting curves when training for an increasing number of update steps. We first find that ES is able to reach performance numbers close to GRPO for math and reasoning tasks with a comparable compute budget. However, and most importantly for continual learning, the performance gains in ES is accompanied by significant forgetting of prior abilities, limiting its applicability for training models online. We also explore the reason behind this behavior and show that the updates made using ES are much less sparse and have orders of magnitude larger $\ell_2$ norm compared to corresponding GRPO updates, explaining the contrasting forgetting curves between the two algorithms. With this study, we aim to highlight the issue of forgetting in gradient-free algorithms like ES and hope to inspire future work to mitigate these issues.

</details>


### [191] [Is Pure Exploitation Sufficient in Exogenous MDPs with Linear Function Approximation?](https://arxiv.org/abs/2601.20694)
*Hao Liang,Jiayu Cheng,Sean R. Sinclair,Yali Du*

Main category: cs.LG

TL;DR: Pure Exploitation Learning (PEL) achieves provable regret bounds in Exogenous MDPs without exploration, overturning conventional wisdom that exploration is necessary.


<details>
  <summary>Details</summary>
Motivation: Exogenous MDPs (Exo-MDPs) are common in operations research (inventory control, energy storage, resource allocation) where exogenous randomness drives system behavior. Despite empirical evidence that greedy methods work well in these settings, existing theory requires exploration or tabular assumptions.

Method: Propose Pure Exploitation Learning (PEL) - an exploitation-only algorithm. For tabular case: PEL achieves $\widetilde{O}(H^2|Îž|\sqrt{K})$ regret. For large continuous state spaces: introduce LSVI-PE, a linear-approximation method with regret polynomial in feature dimension, exogenous state space, and horizon, independent of endogenous state and action spaces. Analysis uses counterfactual trajectories and Bellman-closed feature transport.

Result: First general finite-sample regret bounds for exploitation-only algorithms in Exo-MDPs. PEL consistently outperforms baselines in experiments on synthetic and resource-management tasks.

Conclusion: Exploration is unnecessary in Exo-MDPs - pure exploitation is sufficient. This overturns conventional wisdom that exploration is required for sequential decision-making.

Abstract: Exogenous MDPs (Exo-MDPs) capture sequential decision-making where uncertainty comes solely from exogenous inputs that evolve independently of the learner's actions. This structure is especially common in operations research applications such as inventory control, energy storage, and resource allocation, where exogenous randomness (e.g., demand, arrivals, or prices) drives system behavior. Despite decades of empirical evidence that greedy, exploitation-only methods work remarkably well in these settings, theory has lagged behind: all existing regret guarantees for Exo-MDPs rely on explicit exploration or tabular assumptions. We show that exploration is unnecessary. We propose Pure Exploitation Learning (PEL) and prove the first general finite-sample regret bounds for exploitation-only algorithms in Exo-MDPs. In the tabular case, PEL achieves $\widetilde{O}(H^2|Îž|\sqrt{K})$. For large, continuous endogenous state spaces, we introduce LSVI-PE, a simple linear-approximation method whose regret is polynomial in the feature dimension, exogenous state space, and horizon, independent of the endogenous state and action spaces. Our analysis introduces two new tools: counterfactual trajectories and Bellman-closed feature transport, which together allow greedy policies to have accurate value estimates without optimism. Experiments on synthetic and resource-management tasks show that PEL consistently outperforming baselines. Overall, our results overturn the conventional wisdom that exploration is required, demonstrating that in Exo-MDPs, pure exploitation is enough.

</details>


### [192] [Structurally Human, Semantically Biased: Detecting LLM-Generated References with Embeddings and GNNs](https://arxiv.org/abs/2601.20704)
*Melika Mobini,Vincent Holst,Floriano Tori,Andres Algaba,Vincent Ginis*

Main category: cs.LG

TL;DR: LLM-generated bibliographies closely mimic human citation structure but leave detectable semantic fingerprints in content embeddings, not graph topology.


<details>
  <summary>Details</summary>
Motivation: To determine if LLM-generated reference lists are distinguishable from human ones, given increasing use of LLMs for bibliography curation.

Method: Built paired citation graphs (ground truth vs GPT-4o-generated) for 10,000 papers, added random baseline. Compared structure-only features vs 3072-D title/abstract embeddings using Random Forest on graph aggregates and Graph Neural Networks.

Result: Structure alone poorly separates GPT from ground truth (RF accuracy â‰ˆ0.60), while embeddings sharply increase separability to â‰ˆ0.83 (RF) and 93% (GNNs). Results robust across Claude Sonnet 4.5 and multiple embedding models.

Conclusion: LLM bibliographies closely mimic human citation topology but leave detectable semantic fingerprints; detection/debiasing should target content signals rather than global graph structure.

Abstract: Large language models are increasingly used to curate bibliographies, raising the question: are their reference lists distinguishable from human ones? We build paired citation graphs, ground truth and GPT-4o-generated (from parametric knowledge), for 10,000 focal papers ($\approx$ 275k references) from SciSciNet, and added a field-matched random baseline that preserves out-degree and field distributions while breaking latent structure. We compare (i) structure-only node features (degree/closeness/eigenvector centrality, clustering, edge count) with (ii) 3072-D title/abstract embeddings, using an RF on graph-level aggregates and Graph Neural Networks with node features. Structure alone barely separates GPT from ground truth (RF accuracy $\approx$ 0.60) despite cleanly rejecting the random baseline ($\approx$ 0.89--0.92). By contrast, embeddings sharply increase separability: RF on aggregated embeddings reaches $\approx$ 0.83, and GNNs with embedding node features achieve 93\% test accuracy on GPT vs.\ ground truth. We show the robustness of our findings by replicating the pipeline with Claude Sonnet 4.5 and with multiple embedding models (OpenAI and SPECTER), with RF separability for ground truth vs.\ Claude $\approx 0.77$ and clean rejection of the random baseline. Thus, LLM bibliographies, generated purely from parametric knowledge, closely mimic human citation topology, but leave detectable semantic fingerprints; detection and debiasing should target content signals rather than global graph structure.

</details>


### [193] [Deep Semi-Supervised Survival Analysis for Predicting Cancer Prognosis](https://arxiv.org/abs/2601.20729)
*Anchen Sun,Zhibin Chen,Xiaodong Cai*

Main category: cs.LG

TL;DR: Cox-MT: A deep semi-supervised learning approach using Mean Teacher framework to improve ANN-based Cox models by leveraging both labeled and unlabeled data for cancer prognosis prediction.


<details>
  <summary>Details</summary>
Motivation: ANN-based Cox-PH models require large labeled datasets for training, but labeled survival data is often limited, constraining model performance. Need to leverage unlabeled data to improve predictions.

Method: Developed single- and multi-modal ANN-based Cox models using Mean Teacher framework for deep semi-supervised learning. Cox-MT uses both labeled and unlabeled data, applied to TCGA cancer data including RNA-seq and whole slide images.

Result: Single-modal Cox-MT outperformed existing Cox-nnet across four cancer types. Performance improved with more unlabeled samples. Multi-modal Cox-MT performed better than single-modal models.

Conclusion: Cox-MT effectively leverages both labeled and unlabeled data to significantly enhance prediction accuracy compared to existing ANN-based Cox models trained only on labeled data.

Abstract: The Cox Proportional Hazards (PH) model is widely used in survival analysis. Recently, artificial neural network (ANN)-based Cox-PH models have been developed. However, training these Cox models with high-dimensional features typically requires a substantial number of labeled samples containing information about time-to-event. The limited availability of labeled data for training often constrains the performance of ANN-based Cox models. To address this issue, we employed a deep semi-supervised learning (DSSL) approach to develop single- and multi-modal ANN-based Cox models based on the Mean Teacher (MT) framework, which utilizes both labeled and unlabeled data for training. We applied our model, named Cox-MT, to predict the prognosis of several types of cancer using data from The Cancer Genome Atlas (TCGA). Our single-modal Cox-MT models, utilizing TCGA RNA-seq data or whole slide images, significantly outperformed the existing ANN-based Cox model, Cox-nnet, using the same data set across four types of cancer considered. As the number of unlabeled samples increased, the performance of Cox-MT significantly improved with a given set of labeled data. Furthermore, our multi-modal Cox-MT model demonstrated considerably better performance than the single-modal model. In summary, the Cox-MT model effectively leverages both labeled and unlabeled data to significantly enhance prediction accuracy compared to existing ANN-based Cox models trained solely on labeled data.

</details>


### [194] [GraphAllocBench: A Flexible Benchmark for Preference-Conditioned Multi-Objective Policy Learning](https://arxiv.org/abs/2601.20753)
*Zhiheng Jiang,Yunzhe Wang,Ryan Marr,Ellen Novoseller,Benjamin T. Files,Volkan Ustun*

Main category: cs.LG

TL;DR: GraphAllocBench: A graph-based benchmark for Preference-Conditioned Policy Learning in Multi-Objective RL, featuring CityPlannerEnv for realistic resource allocation tasks with new evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Existing PCPL benchmarks are limited to toy tasks and fixed environments, lacking realism and scalability needed to evaluate MORL approaches in complex, high-dimensional combinatorial allocation problems.

Method: Introduces GraphAllocBench with CityPlannerEnv - a graph-based resource allocation sandbox inspired by city management. Provides diverse objective functions, varying preference conditions, and high-dimensional scalability. Proposes two new metrics: PNDS (Proportion of Non-Dominated Solutions) and OS (Ordering Score) to evaluate preference consistency.

Result: Experiments with MLPs and graph-aware models show GraphAllocBench exposes limitations of existing MORL approaches and demonstrates the potential of graph-based methods like GNNs for complex combinatorial allocation tasks.

Conclusion: GraphAllocBench establishes a versatile, extensible benchmark for advancing PCPL, enabling flexible variation of objectives, preferences, and allocation rules beyond predefined problem sets.

Abstract: Preference-Conditioned Policy Learning (PCPL) in Multi-Objective Reinforcement Learning (MORL) aims to approximate diverse Pareto-optimal solutions by conditioning policies on user-specified preferences over objectives. This enables a single model to flexibly adapt to arbitrary trade-offs at run-time by producing a policy on or near the Pareto front. However, existing benchmarks for PCPL are largely restricted to toy tasks and fixed environments, limiting their realism and scalability. To address this gap, we introduce GraphAllocBench, a flexible benchmark built on a novel graph-based resource allocation sandbox environment inspired by city management, which we call CityPlannerEnv. GraphAllocBench provides a rich suite of problems with diverse objective functions, varying preference conditions, and high-dimensional scalability. We also propose two new evaluation metrics -- Proportion of Non-Dominated Solutions (PNDS) and Ordering Score (OS) -- that directly capture preference consistency while complementing the widely used hypervolume metric. Through experiments with Multi-Layer Perceptrons (MLPs) and graph-aware models, we show that GraphAllocBench exposes the limitations of existing MORL approaches and paves the way for using graph-based methods such as Graph Neural Networks in complex, high-dimensional combinatorial allocation tasks. Beyond its predefined problem set, GraphAllocBench enables users to flexibly vary objectives, preferences, and allocation rules, establishing it as a versatile and extensible benchmark for advancing PCPL. Code: https://anonymous.4open.science/r/GraphAllocBench

</details>


### [195] [Supervised Guidance Training for Infinite-Dimensional Diffusion Models](https://arxiv.org/abs/2601.20756)
*Elizabeth L. Baker,Alexander Denker,Jes Frellsen*

Main category: cs.LG

TL;DR: This paper extends score-based diffusion models to infinite-dimensional function spaces for Bayesian inverse problems, providing theoretical foundations for conditioning diffusion priors on observations using Doob's h-transform, and introduces Supervised Guidance Training for efficient posterior sampling.


<details>
  <summary>Details</summary>
Motivation: While diffusion models provide expressive priors in function space for Bayesian inverse problems, there's no established theory for conditioning them to sample from posterior distributions obtained by conditioning priors on noisy observations.

Method: The authors prove that diffusion models can be conditioned using an infinite-dimensional extension of Doob's h-transform, showing the conditional score decomposes into unconditional score plus guidance term. They propose Supervised Guidance Training, a simulation-free score matching objective to handle the intractable guidance term for efficient posterior sampling.

Result: The work provides the first function-space method for fine-tuning trained diffusion models to accurately sample from posterior distributions in Bayesian inverse problems, with numerical examples demonstrating the approach.

Conclusion: This research establishes theoretical foundations for conditioning diffusion models in function spaces and provides practical methods for posterior sampling in Bayesian inverse problems, bridging the gap between expressive diffusion priors and Bayesian inference.

Abstract: Score-based diffusion models have recently been extended to infinite-dimensional function spaces, with uses such as inverse problems arising from partial differential equations. In the Bayesian formulation of inverse problems, the aim is to sample from a posterior distribution over functions obtained by conditioning a prior on noisy observations. While diffusion models provide expressive priors in function space, the theory of conditioning them to sample from the posterior remains open. We address this, assuming that either the prior lies in the Cameron-Martin space, or is absolutely continuous with respect to a Gaussian measure. We prove that the models can be conditioned using an infinite-dimensional extension of Doob's $h$-transform, and that the conditional score decomposes into an unconditional score and a guidance term. As the guidance term is intractable, we propose a simulation-free score matching objective (called Supervised Guidance Training) enabling efficient and stable posterior sampling. We illustrate the theory with numerical examples on Bayesian inverse problems in function spaces. In summary, our work offers the first function-space method for fine-tuning trained diffusion models to accurately sample from a posterior.

</details>


### [196] [Less is More: Clustered Cross-Covariance Control for Offline RL](https://arxiv.org/abs/2601.20765)
*Nan Qiao,Sheng Yue,Shuning Wang,Yongheng Deng,Ju Ren*

Main category: cs.LG

TL;DR: C^4 method reduces TD cross-covariance bias in offline RL by using partitioned buffer sampling and gradient-based correction, improving stability and returns by up to 30% especially with small/OOD datasets.


<details>
  <summary>Details</summary>
Motivation: Offline RL suffers from distributional shift, especially with scarce data or datasets dominated by out-of-distribution (OOD) areas. Standard squared error objective induces harmful TD cross covariance that amplifies in OOD areas, biasing optimization and degrading policy learning.

Method: Two complementary strategies: 1) Partitioned buffer sampling that restricts updates to localized replay partitions, attenuates irregular covariance effects, and aligns update directions (C^4 - Clustered Cross-Covariance Control for TD). 2) Explicit gradient-based corrective penalty that cancels the covariance induced bias within each update.

Result: Theoretical proof that buffer partitioning preserves the lower bound property of the maximization objective, and constraints mitigate excessive conservatism in extreme OOD areas without altering core behavior of policy constrained offline RL. Empirically shows higher stability and up to 30% improvement in returns over prior methods, especially with small datasets and splits emphasizing OOD areas.

Conclusion: C^4 effectively addresses distributional shift in offline RL by controlling TD cross-covariance bias through partitioned sampling and gradient correction, offering practical improvements in challenging data-scarce and OOD-dominant scenarios.

Abstract: A fundamental challenge in offline reinforcement learning is distributional shift. Scarce data or datasets dominated by out-of-distribution (OOD) areas exacerbate this issue. Our theoretical analysis and experiments show that the standard squared error objective induces a harmful TD cross covariance. This effect amplifies in OOD areas, biasing optimization and degrading policy learning. To counteract this mechanism, we develop two complementary strategies: partitioned buffer sampling that restricts updates to localized replay partitions, attenuates irregular covariance effects, and aligns update directions, yielding a scheme that is easy to integrate with existing implementations, namely Clustered Cross-Covariance Control for TD (C^4). We also introduce an explicit gradient-based corrective penalty that cancels the covariance induced bias within each update. We prove that buffer partitioning preserves the lower bound property of the maximization objective, and that these constraints mitigate excessive conservatism in extreme OOD areas without altering the core behavior of policy constrained offline reinforcement learning. Empirically, our method showcases higher stability and up to 30% improvement in returns over prior methods, especially with small datasets and splits that emphasize OOD areas.

</details>


### [197] [COMET-SG1: Lightweight Autoregressive Regressor for Edge and Embedded AI](https://arxiv.org/abs/2601.20772)
*Shakhyar Gogoi*

Main category: cs.LG

TL;DR: COMET-SG1 is a lightweight autoregressive model for edge AI time-series prediction that focuses on stability and reduced long-horizon drift through linear behavior-space encoding and deterministic state updates.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for stable time-series prediction models suitable for edge and embedded AI systems, where prediction errors accumulate over time during autoregressive inference and where computational resources are limited.

Method: COMET-SG1 uses linear behavior-space encoding, memory-anchored transition estimation, and deterministic state updates to create a lightweight, stability-oriented autoregressive regression model. It operates without recurrent neural networks or transformers, focusing on bounded long-horizon behavior.

Result: Experiments on non-stationary synthetic time-series data show COMET-SG1 achieves competitive short-horizon accuracy while significantly reducing long-horizon drift compared to MLP, LSTM, and k-nearest neighbor baselines.

Conclusion: COMET-SG1 provides a practical, interpretable approach for stable autoregressive prediction in edge AI applications, featuring a compact parameter footprint and operations compatible with fixed-point arithmetic.

Abstract: COMET-SG1 is a lightweight, stability-oriented autoregressive regression model designed for time-series prediction on edge and embedded AI systems. Unlike recurrent neural networks or transformer-based sequence models, COMET-SG1 operates through linear behavior-space encoding, memory-anchored transition estimation, and deterministic state updates. This structure prioritizes bounded long-horizon behavior under fully autoregressive inference, a critical requirement for edge deployment where prediction errors accumulate over time. Experiments on non-stationary synthetic time-series data demonstrate that COMET-SG1 achieves competitive short-horizon accuracy while exhibiting significantly reduced long-horizon drift compared to MLP, LSTM, and k-nearest neighbor baselines. With a compact parameter footprint and operations compatible with fixed-point arithmetic, COMET-SG1 provides a practical and interpretable approach for stable autoregressive prediction in edge and embedded AI applications.

</details>


### [198] [Smoothing the Black-Box: Signed-Distance Supervision for Black-Box Model Copying](https://arxiv.org/abs/2601.20773)
*RubÃ©n JimÃ©nez,Oriol Pujol*

Main category: cs.LG

TL;DR: Black-box model copying using signed distances instead of hard labels improves replica fidelity and generalization by converting discontinuous classification into smooth regression.


<details>
  <summary>Details</summary>
Motivation: Machine learning systems need continuous evolution without access to original training data or model internals. Black-box copying provides practical refactoring, but hard-label outputs create discontinuous surface reconstruction problems that limit boundary geometry recovery.

Method: Proposes distance-based copying framework using signed distances to teacher's decision boundary instead of hard labels. Introduces Î±-governed smoothing and regularization with HÃ¶lder/Lipschitz control, plus two model-agnostic algorithms to estimate signed distances under label-only access.

Result: Experiments on synthetic problems and UCI benchmarks show consistent improvements in fidelity and generalization accuracy over hard-label baselines. Distance outputs also provide uncertainty-related signals for black-box replicas.

Conclusion: Distance-based copying converts black-box model replication from discontinuous classification to smooth regression, enabling better boundary geometry recovery and providing uncertainty signals, making it superior to hard-label approaches for practical model refactoring.

Abstract: Deployed machine learning systems must continuously evolve as data, architectures, and regulations change, often without access to original training data or model internals. In such settings, black-box copying provides a practical refactoring mechanism, i.e. upgrading legacy models by learning replicas from input-output queries alone. When restricted to hard-label outputs, copying turns into a discontinuous surface reconstruction problem from pointwise queries, severely limiting the ability to recover boundary geometry efficiently. We propose a distance-based copying (distillation) framework that replaces hard-label supervision with signed distances to the teacher's decision boundary, converting copying into a smooth regression problem that exploits local geometry. We develop an $Î±$-governed smoothing and regularization scheme with HÃ¶lder/Lipschitz control over the induced target surface, and introduce two model-agnostic algorithms to estimate signed distances under label-only access. Experiments on synthetic problems and UCI benchmarks show consistent improvements in fidelity and generalization accuracy over hard-label baselines, while enabling distance outputs as uncertainty-related signals for black-box replicas.

</details>


### [199] [When More Data Doesn't Help: Limits of Adaptation in Multitask Learning](https://arxiv.org/abs/2601.20774)
*Steve Hanneke,Mingyue Xu*

Main category: cs.LG

TL;DR: The paper establishes a stronger impossibility result for multitask learning, showing that adaptation hardness persists even with arbitrarily large sample sizes per task, going beyond previous no-free-lunch theorems.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental statistical limits of multitask learning and determine whether abundant data per task can overcome the adaptation challenges identified in previous work (arXiv:2006.15785).

Method: Theoretical analysis establishing a stronger impossibility result of adaptation that holds for arbitrarily large sample size per task, going beyond previous no-free-lunch theorems that only applied to bounded sample sizes.

Result: The hardness of multitask learning cannot be overcome by having abundant data per task - adaptation remains impossible even with arbitrarily large sample sizes per task.

Conclusion: Multitask learning faces fundamental statistical limitations that persist regardless of sample size per task, and the paper introduces the notion of optimal adaptivity as an important concept for future research.

Abstract: Multitask learning and related frameworks have achieved tremendous success in modern applications. In multitask learning problem, we are given a set of heterogeneous datasets collected from related source tasks and hope to enhance the performance above what we could hope to achieve by solving each of them individually. The recent work of arXiv:2006.15785 has showed that, without access to distributional information, no algorithm based on aggregating samples alone can guarantee optimal risk as long as the sample size per task is bounded.
  In this paper, we focus on understanding the statistical limits of multitask learning. We go beyond the no-free-lunch theorem in arXiv:2006.15785 by establishing a stronger impossibility result of adaptation that holds for arbitrarily large sample size per task. This improvement conveys an important message that the hardness of multitask learning cannot be overcame by having abundant data per task. We also discuss the notion of optimal adaptivity that may be of future interests.

</details>


### [200] [Active Learning for Decision Trees with Provable Guarantees](https://arxiv.org/abs/2601.20775)
*Arshia Soltani Moakhar,Tanapoom Laoaron,Faraz Ghahremani,Kiarash Banihashem,MohammadTaghi Hajiaghayi*

Main category: cs.LG

TL;DR: First theoretical analysis of active learning label complexity for decision trees, showing polylogarithmic label complexity under specific assumptions, and presenting a (1+Îµ)-approximate classifier algorithm.


<details>
  <summary>Details</summary>
Motivation: To advance theoretical understanding of active learning label complexity for decision trees as binary classifiers, which previously lacked rigorous analysis of key parameters like the disagreement coefficient.

Method: 1) Analyze disagreement coefficient for decision trees under two assumptions: distinct feature dimensions per path and regular grid-like data structure. 2) Develop first general active learning algorithm for binary classification with multiplicative error guarantee. 3) Combine results to design active learning algorithm for decision trees with polylogarithmic label queries.

Result: 1) Showed assumptions are essential for polylogarithmic label complexity (relaxing leads to polynomial complexity). 2) Designed algorithm using only polylogarithmic number of label queries under stated assumptions. 3) Established near-optimal label complexity lower bound for error tolerance Îµ.

Conclusion: The paper provides foundational theoretical analysis for active learning of decision trees, demonstrating that under natural structural assumptions, efficient active learning with polylogarithmic label complexity is achievable, with near-optimal dependence on error tolerance.

Abstract: This paper advances the theoretical understanding of active learning label complexity for decision trees as binary classifiers. We make two main contributions. First, we provide the first analysis of the disagreement coefficient for decision trees-a key parameter governing active learning label complexity. Our analysis holds under two natural assumptions required for achieving polylogarithmic label complexity, (i) each root-to-leaf path queries distinct feature dimensions, and (ii) the input data has a regular, grid-like structure. We show these assumptions are essential, as relaxing them leads to polynomial label complexity. Second, we present the first general active learning algorithm for binary classification that achieves a multiplicative error guarantee, producing a $(1+Îµ)$-approximate classifier. By combining these results, we design an active learning algorithm for decision trees that uses only a polylogarithmic number of label queries in the dataset size, under the stated assumptions. Finally, we establish a label complexity lower bound, showing our algorithm's dependence on the error tolerance $Îµ$ is close to optimal.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [201] [Electromagnetically Consistent Bounds on Information Transfer in Real-World RIS-Parametrized Wireless Channels](https://arxiv.org/abs/2601.20017)
*Albert Salmi,Ville Viikari,Philipp del Hougne*

Main category: eess.SP

TL;DR: This paper develops a fundamental bound on achievable channel gain enhancement for RIS-parametrized wireless channels using semidefinite relaxation, addressing the gap in electromagnetically consistent bounds with realistic hardware constraints.


<details>
  <summary>Details</summary>
Motivation: There's a lack of fundamental bounds on achievable information transfer for RIS-parametrized channels that are both electromagnetically consistent (aware of mutual coupling) and aligned with realistic hardware constraints (few-bit-programmable, potentially lossy loads).

Method: Based on a rigorous multiport network model of a SISO channel with 1-bit-programmable RIS elements, the authors apply semidefinite relaxation (SDR) to derive a fundamental bound on achievable SISO channel gain enhancement.

Result: The SDR-based bound is consistently tighter than existing benchmark strategies (norm-inequality bound and idealized beyond-diagonal load network relaxation). Standard discrete optimization techniques achieve at least 64% (often 100%) of the SDR-based bound in various numerical and experimental examples.

Conclusion: The developed bound provides valuable tools for wireless practitioners to evaluate RIS hardware design choices and optimization algorithms, contributing to electromagnetic information theory for RIS-parametrized channels and other programmable wave systems.

Abstract: A reconfigurable intelligent surface (RIS) endows a wireless channel with programmability that can be leveraged to optimize wireless information transfer. While many works study algorithms for optimizing such a programmable channel, relatively little is known about fundamental bounds on the achievable information transfer. In particular, non-trivial bounds that are both electromagnetically consistent (e.g., aware of mutual coupling) and in line with realistic hardware constraints (e.g., few-bit-programmable, potentially lossy loads) are missing. Here, based on a rigorous multiport network model of a single-input single-output (SISO) channel parametrized by 1-bit-programmable RIS elements, we apply a semidefinite relaxation (SDR) to derive a fundamental bound on the achievable SISO channel gain enhancement. A bound on the maximum achievable rate of information transfer at a given noise level follows directly from Shannon's theorem. We apply our bound to several numerical and experimental examples of different RIS-parametrized radio environments. Compared to electromagnetically consistent benchmark bounding strategies (a norm-inequality bound and, where applicable, a relaxation to an idealized beyond-diagonal load network for which a global solution exists), we consistently observe that our SDR-based bound is notably tighter. We reach at least 64 % (but often 100 %) of our SDR-based bound with standard discrete optimization techniques. The applicability of our bound to concrete experimental systems makes it valuable to inform wireless practitioners, e.g., to evaluate RIS hardware design choices and algorithms to optimize the RIS configuration. Our work contributes to the development of an electromagnetic information theory for RIS-parametrized channels as well as other programmable wave systems such as dynamic metasurface antennas or real-life beyond-diagonal RISs.

</details>


### [202] [Holographic & Channel-Aware Distributed Detection of a Non-cooperative Target](https://arxiv.org/abs/2601.20124)
*Domenico Ciuonzo,Alessio Zappone,Marco Di Renzo,Ciro D'Elia*

Main category: eess.SP

TL;DR: Holographic metasurface-aided distributed detection in WSNs with energy-efficient IoT constraints


<details>
  <summary>Details</summary>
Motivation: To enhance fusion efficiency in distributed detection systems for IoT applications while maintaining energy efficiency and minimal hardware complexity

Method: Uses reconfigurable metasurface near-field of receive antennas for holographic architecture, derives generalized likelihood ratio test, proposes two low-complexity joint design strategies for fusion and metasurface optimization

Result: Proposed holographic fusion achieves reliable detection with balanced performance-complexity trade-off, validated through simulations even with simplified designs

Conclusion: Holographic metasurface architecture enables efficient distributed detection in WSNs with minimal RF hardware, suitable for energy-constrained IoT applications

Abstract: This work investigates Distributed Detection (DD) in Wireless Sensor Networks (WSNs), where spatially distributed sensors transmit binary decisions over a shared flat-fading channel. To enhance fusion efficiency, a reconfigurable metasurface is positioned in the near-field of a few receive antennas, enabling a holographic architecture that harnesses large-aperture gains with minimal RF hardware. A generalized likelihood ratio test is derived for fixed metasurface settings, and two low-complexity joint design strategies are proposed to optimize both fusion and metasurface configuration. These suboptimal schemes achieve a balance between performance, complexity, and system knowledge. The goal is to ensure reliable detection of a localized phenomenon at the fusion center, under energy-efficient constraints aligned with IoT requirements. Simulation results validate the effectiveness of the proposed holographic fusion, even under simplified designs.

</details>


### [203] [Coverage Performance Analysis of FAS-enhanced LoRa Wide Area Networks under both Co-SF and Inter-SF Interference](https://arxiv.org/abs/2601.20178)
*Gaoze Mu,Yanzhao Hou,Mingjie Chen,Yuanyu Hu,Yongan Zheng,Qimei Cui,Xiaofeng Tao*

Main category: eess.SP

TL;DR: Analytical framework for evaluating coverage performance of fluid antenna system (FAS)-enhanced LoRaWANs, considering pathloss, FAS fading, and dense interference from random device deployment.


<details>
  <summary>Details</summary>
Motivation: To develop an analytical framework for evaluating the coverage performance of FAS-enhanced LoRaWANs, addressing the challenges of large-scale pathloss, small-scale fading from FAS, and dense interference in ALOHA-based networks with both co-SF and inter-SF interference.

Method: Derived statistical approximations of FAS channel envelope and power using extreme-value theorem; developed theoretical coverage probability analysis for FAS-enhanced LoRaWAN considering pathloss, fading, and interference from randomly deployed devices; validated approximations against exact correlation model.

Result: Numerical results validate analytical approximations with close agreement to exact correlation model; FAS with normalized aperture of 1Ã—1 significantly enhances network performance in both device numbers and coverage range.

Conclusion: The proposed analytical framework successfully evaluates FAS-enhanced LoRaWAN coverage performance, demonstrating that FAS technology can substantially improve network capacity and coverage range in LoRaWAN deployments.

Abstract: This paper presents an analytical framework for evaluating the coverage performance of the fluid antenna system (FAS)-enhanced LoRa wide-area networks (LoRaWANs). We investigate the effects of large-scale pathloss in LoRaWAN, small-scale fading characterized by FAS, and dense interference (i.e., collision in an ALOHA-based mechanism) arising from randomly deployed end devices (EDs). Both co-spreading factor (co-SF) interference (with the same SF) and inter-SF interference (with different SFs) are introduced into the network, and their differences in physical characteristics are also considered in the analysis. Additionally, simple yet accurate statistical approximations of the FAS channel envelope and power are derived using the extreme-value theorem. Based on the approximated channel expression, the theoretical coverage probability of the proposed FAS-enhanced LoRaWAN is derived. Numerical results validate our analytical approximations by exhibiting close agreement with the exact correlation model. Notably, it is revealed that a FAS with a normalized aperture of 1 times 1 can greatly enhance network performance, in terms of both ED numbers and coverage range.

</details>


### [204] [WirelessJEPA: A Multi-Antenna Foundation Model using Spatio-temporal Wireless Latent Predictions](https://arxiv.org/abs/2601.20190)
*Viet Chu,Omar Mashaal,Hatem Abou-Zeid*

Main category: eess.SP

TL;DR: WirelessJEPA is a wireless foundation model using JEPA architecture that learns representations from multi-antenna IQ data by predicting masked signal regions, enabling multiple downstream tasks without engineered augmentations.


<details>
  <summary>Details</summary>
Motivation: To create a general-purpose wireless foundation model that can handle diverse downstream tasks without relying on carefully engineered contrastive augmentations, addressing the need for more flexible and generalizable wireless signal processing.

Method: Uses Joint Embedding Predictive Architecture (JEPA) with a novel 2D antenna-time representation that reshapes multi-antenna IQ streams into structured grids. Introduces convolutional processing with block masking, efficient sparse computation over unmasked patches, and novel spatio-temporal mask geometries encoding inductive biases across antennas and time.

Result: Demonstrates robust performance and strong task generalization across six downstream tasks, establishing JEPA-based learning as a promising direction for building generalizable wireless foundation models.

Conclusion: WirelessJEPA successfully adapts JEPA architecture to wireless signals, enabling general-purpose representation learning from real-world multi-antenna IQ data and showing promising results for diverse wireless tasks without engineered augmentations.

Abstract: We propose WirelessJEPA, a novel wireless foundation model (WFM) that uses the Joint Embedding Predictive Architecture (JEPA). WirelessJEPA learns general-purpose representations directly from real-world multi-antenna IQ data by predicting latent representations of masked signal regions. This enables multiple diverse downstream tasks without reliance on carefully engineered contrastive augmentations. To adapt JEPA to wireless signals, we introduce a 2D antenna time representation that reshapes multi-antenna IQ streams into structured grids, allowing convolutional processing with block masking and efficient sparse computation over unmasked patches. Building on this representation, we propose novel spatio temporal mask geometries that encode inductive biases across antennas and time. We evaluate WirelessJEPA across six downstream tasks and demonstrate it's robust performance and strong task generalization. Our results establish that JEPA-based learning as a promising direction for building generalizable WFMs.

</details>


### [205] [User Localization via Active Sensing with Electromagnetically Reconfigurable Antennas](https://arxiv.org/abs/2601.20501)
*Ruizhi Zhang,Yuchen Zhang,Ying Zhang*

Main category: eess.SP

TL;DR: End-to-end deep learning framework for ERA-aided user localization with active sensing, using two-timescale design and attention-LSTM architecture to outperform conventional methods.


<details>
  <summary>Details</summary>
Motivation: To enhance user localization in wireless systems by leveraging electromagnetic reconfigurability of antennas (ERAs) to diversify measurements and improve localization informativeness, addressing the need for more accurate positioning in future wireless networks.

Method: Two-timescale design: digital combiner updated at each stage, ERA patterns reconfigured at each substage via spherical-harmonic representation. Integrates attention-based feature extraction and LSTM-based temporal learning to learn optimized sensing strategy and progressively refine position estimates from sequential observations.

Result: Simulation results show the proposed approach consistently outperforms conventional digital beamforming-only and single-stage sensing baselines in terms of localization accuracy.

Conclusion: ERA-enabled active sensing is effective for user localization in future wireless systems, demonstrating the value of electromagnetic reconfigurability combined with deep learning for improved positioning performance.

Abstract: This paper presents an end-to-end deep learning framework for electromagnetically reconfigurable antenna (ERA)-aided user localization with active sensing, where ERAs provide additional electromagnetic reconfigurability to diversify the received measurements and enhance localization informativeness.
  To balance sensing flexibility and overhead, we adopt a two-timescale design: the digital combiner is updated at each stage, while the ERA patterns are reconfigured at each substage via a spherical-harmonic representation. The proposed mechanism integrates attention-based feature extraction and LSTM-based temporal learning, enabling the system to learn an optimized sensing strategy and progressively refine the UE position estimate from sequential observations. Simulation results show that the proposed approach consistently outperforms conventional digital beamforming-only and single-stage sensing baselines in terms of localization accuracy. These results highlight the effectiveness of ERA-enabled active sensing for user localization in future wireless systems.

</details>


### [206] [Vehicular Wireless Positioning -- A Survey](https://arxiv.org/abs/2601.20547)
*Sharief Saleh,Satyam Dwivedi,Russ Whiton,Peter Hammarberg,Musa Furkan Keskin,Julia Equi,Hui Chen,Florent Munier,Olof Eriksson,Fredrik Gunnarsson,Fredrik Tufvesson,Henk Wymeersch*

Main category: eess.SP

TL;DR: A comprehensive survey of wireless-based positioning technologies for connected and autonomous vehicles, covering GNSS, LEO satellites, 5G cellular, and IEEE standards (Wi-Fi, UWB, Bluetooth, V2V), including performance requirements, historical development, algorithms, and sensor fusion techniques.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of connected and autonomous vehicles creates demand for precise, reliable positioning systems in complex environments, requiring integrated approaches combining multiple positioning technologies.

Method: The paper conducts a comprehensive survey that: 1) reviews vehicular positioning use cases and performance requirements, 2) explores historical development, standardization, and evolution of wireless positioning technologies (GNSS, LEO satellites, 5G cellular, IEEE standards), 3) categorizes existing positioning solutions and algorithms, and 4) examines sensor fusion techniques integrating wireless systems with perception and motion sensors.

Result: The survey provides a holistic perspective on historical foundations, current advancements, and future directions of wireless-based positioning for vehicular applications, identifying open challenges and contemporary trends while addressing a critical gap in the literature.

Conclusion: This comprehensive survey offers valuable insights into wireless positioning technologies for vehicles, emphasizing the need for integrated approaches and sensor fusion to achieve accurate, resilient positioning in real-world conditions for connected and autonomous vehicle applications.

Abstract: The rapid advancement of connected and autonomous vehicles has driven a growing demand for precise and reliable positioning systems capable of operating in complex environments. Meeting these demands requires an integrated approach that combines multiple positioning technologies, including wireless-based systems, perception-based technologies, and motion-based sensors. This paper presents a comprehensive survey of wireless-based positioning for vehicular applications, with a focus on satellite-based positioning (such as global navigation satellite systems (GNSS) and low-Earth-orbit (LEO) satellites), cellular-based positioning (5G and beyond), and IEEE-based technologies (including Wi-Fi, ultrawideband (UWB), Bluetooth, and vehicle-to-vehicle (V2V) communications). First, the survey reviews a wide range of vehicular positioning use cases, outlining their specific performance requirements. Next, it explores the historical development, standardization, and evolution of each wireless positioning technology, providing an in-depth categorization of existing positioning solutions and algorithms, and identifying open challenges and contemporary trends. Finally, the paper examines sensor fusion techniques that integrate these wireless systems with onboard perception and motion sensors to enhance positioning accuracy and resilience in real-world conditions. This survey thus offers a holistic perspective on the historical foundations, current advancements, and future directions of wireless-based positioning for vehicular applications, addressing a critical gap in the literature.

</details>


### [207] [Precoding Design for Multi-User MIMO Joint Communications and Sensing](https://arxiv.org/abs/2601.20647)
*Charlotte Muth,Shrinivas Chimmalgi,Laurent Schmalen*

Main category: eess.SP

TL;DR: This paper investigates precoding techniques for multi-user MIMO joint communications and sensing systems, analyzing interference between sensing and communication channels and deriving performance indicators for both functions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the interference challenges in joint communications and sensing (JCAS) systems where both functions operate simultaneously, potentially interfering with each other's performance in multi-user MIMO environments.

Method: The authors derive mathematical indicators for sensing performance (detection probability) and communication performance (SINR) for general input signals, and investigate precoding techniques to manage the interference between sensing and communication channels.

Result: Results show that using communication signals for sensing can prevent communication performance loss when channel interference occurs, but the kurtosis of the communication signal's transmit alphabet limits sensing performance. Simulation results validate these findings in example setups.

Conclusion: The study demonstrates that careful precoding design in MU-MIMO JCAS systems can mitigate interference between sensing and communication functions, with communication signals being usable for sensing without compromising communication performance, though signal statistics (kurtosis) impose constraints on sensing capabilities.

Abstract: We investigate precoding for multi-user (MU) multiple-input multiple-output (MIMO) joint communications and sensing (JCAS) systems, taking into account the potential interference between sensing and communication channels. We derive indicators for the sensing and communication performance, i.e., the detection probability and the communication signal-to-interference-and-noise ratio (SINR) for general input signals. Our results show that the use of the communication signal for sensing can prevent a loss in communication performance if channel interference occurs, while the kurtosis of the transmit alphabet of the communication signal limits the sensing performance. We present simulation results of example setups.

</details>


### [208] [RL based Beamforming Optimization for 3D Pinching Antenna assisted ISAC Systems](https://arxiv.org/abs/2601.20654)
*Qian Gao,Ruikang Zhong,Yue Liu,Hyundong Shin,Yuanwei Liu*

Main category: eess.SP

TL;DR: 3D deployment of pinching antenna array with HGRL optimization outperforms 1D/2D arrays in ISAC systems for better sum communication rate.


<details>
  <summary>Details</summary>
Motivation: To enhance Integrated Sensing and Communication (ISAC) system performance by leveraging 3D deployment of pinching antenna arrays, which offers more degrees of freedom than traditional 1D and 2D deployments.

Method: Proposes a joint optimization problem for antenna positioning, time allocation, and transmit power to maximize sum communication rate under sensing rate and energy constraints. Solves this using a Heterogeneous Graph Neural Network based Reinforcement Learning (HGRL) algorithm with advanced environment observation construction.

Result: Simulation results show that 3D deployment of pinching antenna array outperforms 1D and 2D counterparts in ISAC systems. The proposed HGRL algorithm surpasses other baselines in both performance and convergence speed.

Conclusion: 3D deployment of pinching antenna arrays combined with HGRL optimization provides superior ISAC system performance, demonstrating the value of spatial dimension exploitation and advanced learning algorithms for joint optimization problems.

Abstract: In this paper, a three-dimensional (3D) deployment scheme of pinching antenna array is proposed, aiming to enhances the performance of integrated sensing and communication (ISAC) systems. To fully realize the potential of 3D deployment, a joint antenna positioning, time allocation and transmit power optimization problem is formulated to maximize the sum communication rate with the constraints of target sensing rates and system energy. To solve the sum rate maximization problem, we propose a heterogeneous graph neural network based reinforcement learning (HGRL) algorithm. Simulation results prove that 3D deployment of pinching antenna array outperforms 1D and 2D counterparts in ISAC systems. Moreover, the proposed HGRL algorithm surpasses other baselines in both performance and convergence speed due to the advanced observation construction of the environment.

</details>


### [209] [Integrated Sensing and Communication for Segmented Waveguide-Enabled Pinching Antenna Systems](https://arxiv.org/abs/2601.20658)
*Qian Gao,Ruikang Zhong,Hyundong Shin,Yuanwei Liu*

Main category: eess.SP

TL;DR: Proposed SWAN-ISAC system with HSSM protocol and SHRL algorithm for joint optimization of beamforming, segment selection, and antenna positioning to maximize communication rate while meeting sensing constraints.


<details>
  <summary>Details</summary>
Motivation: To improve ISAC system performance by leveraging low in-waveguide propagation loss of segmented waveguides while reducing hardware costs through hybrid segment selection and multiplexing.

Method: Developed SWAN-ISAC system with HSSM protocol, formulated joint optimization problem for beamforming, segment selection, and antenna positioning, and proposed SHRL algorithm to solve it.

Result: Simulation shows SWAN-ISAC outperforms baseline schemes and SHRL algorithm achieves better performance than conventional RL algorithms.

Conclusion: The proposed SWAN-ISAC design with HSSM protocol and SHRL algorithm effectively improves ISAC system performance with reduced hardware cost.

Abstract: In this paper, an integrated sensing and communication (ISAC) design for segmented waveguide-enabled pinching-antenna array (SWAN) systems is proposed to improve the performance of systems by leveraging the low in-waveguide propagation loss of segmented waveguides. The hybrid segment selection and multiplexing (HSSM) protocol is implemented to provide favorable performance with less hardware cost. To achieve this, a joint transmit beamforming optimization, segment selection, and pinching antenna positioning problem is formulated to maximize the sum communication rate with the constraints of sensing performance. To solve the maximization problem, we propose a segment hysteresis based reinforcement learning (SHRL) algorithm to learn segment selection and pinching antenna positions in different progress to explore better strategies. Simulation results demonstrate that 1) the proposed SWAN-ISAC scheme outperforms the other baseline schemes, and 2) the proposed HARL algorithm achieves better performance compared to conventional RL algorithms.

</details>


### [210] [Deep Learning based Three-stage Solution for ISAC Beamforming Optimization](https://arxiv.org/abs/2601.20667)
*Qian Gao,Ruikang Zhong,Yuanwei Liu*

Main category: eess.SP

TL;DR: A three-stage deep learning framework for ISAC beamforming optimization that maximizes sum communication rate while meeting sensing rate constraints, outperforming baseline RL methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of optimizing beamforming in integrated sensing and communication (ISAC) systems where the BS must simultaneously communicate with multiple users and perform target detection, requiring joint optimization of communication and sensing performance.

Method: A three-stage deep learning framework: 1) Unsupervised learning for feature extraction from variable CSI, 2) Reinforcement learning for beampattern optimization based on extracted features, and 3) Supervised learning for beamforming vector reconstruction from optimized beampattern.

Result: Simulation results show the proposed three-stage solution outperforms baseline RL algorithms by optimizing intuitive beampatterns rather than directly optimizing beamforming vectors.

Conclusion: The proposed deep learning framework provides an effective solution for ISAC beamforming optimization that balances communication and sensing requirements, demonstrating superior performance through its three-stage approach of feature extraction, beampattern optimization, and beamforming reconstruction.

Abstract: In this paper, a general ISAC system where the base station (BS) communicates with multiple users and performs target detection is considered. Then, a sum communication rate maximization problem is formulated, subjected to the constraints of transmit power and the minimum sensing rates of users. To solve this problem, we develop a framework that leverages deep learning algorithms to provide a three-stage solution for ISAC beamforming. The three-stage beamforming optimization solution includes three modules: 1) an unsupervised learning based feature extraction algorithm is proposed to extract fixed-size latent features while keeping its essential information from the variable channel state information (CSI); 2) a reinforcement learning (RL) based beampattern optimization algorithm is proposed to search the desired beampattern according to the extracted features; 3) a supervised learning based beamforming reconstruction algorithm is proposed to reconstruct the beamforming vector from beampattern given by the RL agent. Simulation results demonstrate that the proposed three-stage solution outperforms the baseline RL algorithm by optimizing the intuitional beampattern rather than beamforming.

</details>


### [211] [Grover's Search-Inspired Quantum Reinforcement Learning for Massive MIMO User Scheduling](https://arxiv.org/abs/2601.20688)
*Ruining Fan,Xingyu Huang,Mouli Chakraborty,Avishek Nag,Anshu Mukherjee*

Main category: eess.SP

TL;DR: A quantum reinforcement learning framework using Grover's search for efficient user scheduling in massive MIMO systems, outperforming classical and quantum benchmarks.


<details>
  <summary>Details</summary>
Motivation: User scheduling in massive MIMO systems for 5G/B5G faces challenges of high computational complexity, scalability issues, and excessive CSI overhead, requiring more efficient solutions.

Method: Proposes a Quantum Reinforcement Learning framework inspired by Grover's search algorithm, using quantum-gate-based circuits that mimic reinforcement learning's layered architecture with quantum operations as policy updates and decision-making units.

Result: The proposed method achieves proper convergence and significantly outperforms classical CNN and Quantum Deep Learning benchmarks in simulation results.

Conclusion: The Grover's search-inspired QRL framework provides an effective solution for exploring the exponentially large scheduling space in mMIMO systems, offering superior performance over existing approaches.

Abstract: The efficient user scheduling policy in the massive Multiple Input Multiple Output (mMIMO) system remains a significant challenge in the field of 5G and Beyond 5G (B5G) due to its high computational complexity, scalability, and Channel State Information (CSI) overhead. This paper proposes a novel Grover's search-inspired Quantum Reinforcement Learning (QRL) framework for mMIMO user scheduling. The QRL agent can explore the exponentially large scheduling space effectively by applying Grover's search to the reinforcement learning process. The model is implemented using our designed quantum-gate-based circuit, which imitates the layered architecture of reinforcement learning, where quantum operations act as policy updates and decision-making units. Moreover, the simulation results demonstrate that the proposed method achieves proper convergence and significantly outperforms classical Convolutional Neural Networks (CNN) and Quantum Deep Learning (QDL) benchmarks.

</details>


### [212] [Sequential Processing Strategies in Fronthaul Constrained Cell-Free Massive MIMO Networks](https://arxiv.org/abs/2601.20721)
*Vida Ranjbar,Robbert Beerten,Marc Moonen,Sofie Pollin*

Main category: eess.SP

TL;DR: The paper proposes two sequential processing strategies for cell-free massive MIMO with daisy-chain fronthaul to improve spectral efficiency by addressing fronthaul compression effects.


<details>
  <summary>Details</summary>
Motivation: In cell-free massive MIMO networks with daisy-chain fronthaul topology, the sequential nature of fronthaul communication causes varying information requirements for each AP based on its position in the chain. This creates challenges with fronthaul compression that adversely affect users' spectral efficiency.

Method: Two sequential processing strategies: 1) Linearly increasing fronthaul capacity allocation among APs, and 2) Two-Path users' signal estimation approach.

Result: Both proposed strategies demonstrate superior performance in terms of sum spectral efficiency compared to baseline methods of equal fronthaul capacity allocation and Single-Path sequential signal estimation.

Conclusion: The proposed sequential processing strategies effectively combat the adverse effects of fronthaul compression in daisy-chain cell-free massive MIMO networks, offering improved spectral efficiency through optimized fronthaul capacity allocation and signal estimation techniques.

Abstract: In a cell-free massive MIMO (CFmMIMO) network with a daisy-chain fronthaul, the amount of information that each access point (AP) needs to communicate with the next AP in the chain is determined by the location of the AP in the sequential fronthaul. Therefore, we propose two sequential processing strategies to combat the adverse effect of fronthaul compression on the sum of users' spectral efficiency (SE): 1) linearly increasing fronthaul capacity allocation among APs and 2) Two-Path users' signal estimation. The two strategies show superior performance in terms of sum SE compared to the equal fronthaul capacity allocation and Single-Path sequential signal estimation.

</details>


### [213] [Multi-Mode Pinching Antenna Systems Enabled Multi-User Communications](https://arxiv.org/abs/2601.20780)
*Xiaoxia Xu,Xidong Mu,Yuanwei Liu,Arumugam Nallanathan*

Main category: eess.SP

TL;DR: Proposes multi-mode pinching-antenna systems (PASS) for efficient multi-user communications via mode-domain multiplexing in waveguides, with optimization of beamforming and antenna placement.


<details>
  <summary>Details</summary>
Motivation: To enable efficient multi-user communications by transmitting multiple data streams within a single waveguide through multiple guided modes, overcoming limitations of conventional single-mode systems and fixed antenna structures.

Method: Develops a physics model revealing mode-selective power radiation of pinching antennas, proposes PA grouping scheme (non-leakage/weak-leakage regimes), and optimizes beamforming and PA locations via: 1) channel orthogonality solution with Newton-based search for two-PA case, and 2) PSO-ZF algorithm for general multi-PA case.

Result: Simulation results demonstrate superiority of proposed multi-mode PASS over conventional single-mode PASS and fixed-antenna structures in terms of system performance.

Conclusion: The multi-mode PASS framework with optimized PA grouping, beamforming, and antenna placement enables efficient multi-user communications through mode-domain multiplexing, offering significant performance improvements over existing approaches.

Abstract: This paper proposes a novel multi-mode pinching-antenna systems (PASS) framework. Multiple data streams can be transmitted within a single waveguide through multiple guided modes, thus facilitating efficient multi-user communications through the mode-domain multiplexing. A physic model is derived, which reveals the mode-selective power radiation feature of pinching antennas (PAs). A two-mode PASS enabled two-user downlink communication system is investigated. Considering the mode selectivity of PA power radiation, a practical PA grouping scheme is proposed, where each PA group matches with one specific guided mode and mainly radiates its signal sequentially. Depending on whether the guided mode leaks power to unmatched PAs or not, the proposed PA grouping scheme operates in either the non-leakage or weak-leakage regime. Based on this, the baseband beamforming and PA locations are jointly optimized for sum rate maximization, subject to each user's minimum rate requirement. 1) A simple two-PA case in non-leakage regime is first considered. To solve the formulated problem, a channel orthogonality based solution is proposed. The channel orthogonality is ensured by large-scale and wavelength-scale equality constraints on PA locations. Thus, the optimal beamforming reduces to maximum-ratio transmission (MRT). Moreover, the optimal PA locations are obtained via a Newton-based one-dimension search algorithm that enforces two-scale PA-location constraints by Newton's method. 2) A general multi-PA case in both non-leakage and weak-leakage regimes is further considered. A low-complexity particle-swarm optimization with zero-forcing beamforming (PSO-ZF) algorithm is developed, thus effectively tackling the high-oscillatory and strong-coupled problem. Simulation results demonstrate the superiority of the proposed multi-mode PASS over conventional single-mode PASS and fixed-antenna structures.

</details>


### [214] [AI-Driven Design of Stacked Intelligent Metasurfaces for Software-Defined Radio Applications](https://arxiv.org/abs/2601.20795)
*Ivan Iudice,Giacinto Gelli,Donatella Darsena*

Main category: eess.SP

TL;DR: Implementation of stacked intelligent metasurface (SIM) model in NVIDIA's Sionna framework for 6G research, enabling differentiable GPU-accelerated simulation and learning-based optimization of RIS-assisted communication systems.


<details>
  <summary>Details</summary>
Motivation: Reconfigurable intelligent surfaces (RIS) offer dynamic environment shaping and spectrum efficiency for future wireless systems, but need scalable simulation tools for AI-driven optimization in 6G research.

Method: Implemented SIM model within NVIDIA's Sionna AI-native framework using TensorFlow-based pipeline, creating fully differentiable GPU-accelerated environment for end-to-end training and optimization of SIM-assisted communication channels.

Result: Successfully demonstrated SIM model in closed-loop learning scenarios for adaptive beamforming and dynamic reconfiguration, with benchmarking showing effectiveness for intelligent control and signal enhancement in non-terrestrial-network environments.

Conclusion: Provides scalable, modular approach for integrating intelligent metasurfaces into AI-accelerated SDR systems, enabling future hardware-in-the-loop experiments and advancing 6G physical layer research.

Abstract: The integration of reconfigurable intelligent surfaces (RIS) into future wireless communication systems offers promising capabilities in dynamic environment shaping and spectrum efficiency. In this work, we present a consistent implementation of a stacked intelligent metasurface (SIM) model within the NVIDIA's AI-native framework Sionna for 6G physical layer research. Our implementation allows simulation and learning-based optimization of SIM-assisted communication channels in fully differentiable and GPU-accelerated environments, enabling end-to-end training for cognitive and software-defined radio (SDR) applications. We describe the architecture of the SIM model, including its integration into the TensorFlow-based pipeline, and showcase its use in closed-loop learning scenarios involving adaptive beamforming and dynamic reconfiguration. Benchmarking results are provided for various deployment scenarios, highlighting the model's effectiveness in enabling intelligent control and signal enhancement in non-terrestrial-network (NTN) propagation environments. This work demonstrates a scalable, modular approach for incorporating intelligent metasurfaces into modern AI-accelerated SDR systems and paves the way for future hardware-in-the-loop experiments.

</details>


### [215] [Statistical Properties of Target Localization Using Passive Radar Systems](https://arxiv.org/abs/2601.20817)
*Mats Viberg,Daniele Gerosa,Tomas McKelvey,Thomas Eriksson*

Main category: eess.SP

TL;DR: The paper analyzes the Extended Cancelation Algorithm (ECA) for passive radar systems, deriving statistical properties of parameter estimates under high SNR conditions and providing a sufficient condition for statistically efficient estimates.


<details>
  <summary>Details</summary>
Motivation: Passive radar systems are attractive due to low cost and covert operation, but require robust algorithms for target detection and localization. The Extended Cancelation Algorithm (ECA) is a seminal approach that needs better theoretical understanding of its statistical properties and performance limits.

Method: Theoretical derivation of statistical properties of ECA parameter estimates under high SNR conditions, including analysis of interference cancellation from direct-path and clutter using a Reference Channel. Computer simulations validate theoretical results.

Result: Derived statistical properties of ECA parameter estimates, established a sufficient condition for SNR in the Reference Channel to enable statistically efficient estimates, and validated theory through simulations showing agreement above certain SNR thresholds.

Conclusion: The theoretical framework enables performance prediction for passive radar systems in given scenarios, supporting feasibility studies and system design decisions for practical applications.

Abstract: Passive Radar Systems have received tremendous attention during the past few decades, due to their low cost and ability to remain covert during operation. Such systems do not transmit any energy themselves, but rely on a so-called Illuminator-of-Opportunity (IO), for example a commercial TV station. A network of Receiving Nodes (RN) receive the direct signal as well as reflections from possible targets. The RNs transmit information to a Central Node (CN), that performs the final target detection, localization and tracking. A large number of methods and algorithms for target detection and localization have been proposed in the literature. In the present contribution, the focus is on the seminal Extended Cancelation Algorithm (ECA), in which each RN estimates target parameters after canceling interference from the direct-path as well as clutter from unwanted stationary objects. This is done by exploiting a separate Reference Channel (RC), which captures the IO signal without interference apart from receiver noise. We derive the statistical properties of the ECA parameter estimates under the assumption of a high Signal-to-Noise Ratio (SNR), and we give a sufficient condition for the SNR in the RC to enable statistically efficient estimates. The theoretical results are corroborated through computer simulations, which show that the theory agrees well with empirical results above a certain SNR threshold. The results can be used to predict the performance of passive radar systems in given scenarios, which is useful for feasibility studies as well as system design.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [216] [E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning](https://arxiv.org/abs/2601.19969)
*Haoyuan Deng,Yuanjiang Xue,Haoyang Du,Boyang Zhou,Zhenyu Wu,Ziwei Wang*

Main category: cs.RO

TL;DR: HiL-RL framework that improves sample efficiency by actively selecting informative samples using influence functions on policy entropy, reducing human interventions by 10.1% while increasing success rate by 42.1%.


<details>
  <summary>Details</summary>
Motivation: Existing human-in-the-loop RL frameworks suffer from low sample efficiency, requiring substantial human interventions which leads to high labor costs. There's a need for more efficient methods that reduce human guidance requirements while maintaining or improving performance.

Method: Proposes a sample-efficient framework that actively selects informative samples by building influence functions of different samples on policy entropy. Uses covariance of action probabilities and soft advantages to efficiently estimate influence functions, then selects samples with moderate values while pruning shortcut samples (sharp entropy drops) and noisy samples (negligible effect).

Result: Achieves 42.1% higher success rate while requiring 10.1% fewer human interventions compared to state-of-the-art HiL-RL methods across four real-world manipulation tasks.

Conclusion: The proposed framework significantly improves sample efficiency in human-in-the-loop RL by intelligently selecting informative samples, reducing human labor costs while enhancing performance on real-world manipulation tasks.

Abstract: Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \method achieves a 42.1\% higher success rate while requiring 10.1\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at https://e2hil.github.io/.

</details>


### [217] [Just in time Informed Trees: Manipulability-Aware Asymptotically Optimized Motion Planning](https://arxiv.org/abs/2601.19972)
*Kuanqi Cai,Liding Zhang,Xinwen Su,Kejia Chen,Chaoqun Wang,Sami Haddadin,Alois Knoll,Arash Ajoudani,Luis Figueredo*

Main category: cs.RO

TL;DR: JIT* is an enhanced sampling-based path planning algorithm that improves efficiency in high-dimensional robotic manipulation by dynamically refining edge connectivity, adjusting sampling density, and balancing manipulability with trajectory cost.


<details>
  <summary>Details</summary>
Motivation: Traditional sampling-based methods struggle with efficient path planning in high-dimensional, multi-obstacle environments, especially for robotic manipulators where kinematic singularities and self-collisions pose additional challenges to motion efficiency and safety.

Method: JIT* enhances EIT* with two core modules: 1) Just-in-Time module with "Just-in-Time Edge" for dynamic edge connectivity refinement and "Just-in-Time Sample" for adaptive sampling density in bottleneck areas; 2) Motion Performance module that dynamically switches between manipulability and trajectory cost optimization to reduce singularity risks.

Result: JIT* consistently outperforms traditional sampling-based planners across dimensions from â„â´ to â„Â¹â¶, and demonstrates effectiveness in both single-arm and dual-arm manipulation tasks.

Conclusion: JIT* provides an effective solution for high-dimensional robotic path planning that addresses both efficiency and safety concerns in complex manipulation tasks, with experimental validation showing superior performance over existing methods.

Abstract: In high-dimensional robotic path planning, traditional sampling-based methods often struggle to efficiently identify both feasible and optimal paths in complex, multi-obstacle environments. This challenge is intensified in robotic manipulators, where the risk of kinematic singularities and self-collisions further complicates motion efficiency and safety. To address these issues, we introduce the Just-in-Time Informed Trees (JIT*) algorithm, an enhancement over Effort Informed Trees (EIT*), designed to improve path planning through two core modules: the Just-in-Time module and the Motion Performance module. The Just-in-Time module includes "Just-in-Time Edge," which dynamically refines edge connectivity, and "Just-in-Time Sample," which adjusts sampling density in bottleneck areas to enable faster initial path discovery. The Motion Performance module balances manipulability and trajectory cost through dynamic switching, optimizing motion control while reducing the risk of singularities. Comparative analysis shows that JIT* consistently outperforms traditional sampling-based planners across $\mathbb{R}^4$ to $\mathbb{R}^{16}$ dimensions. Its effectiveness is further demonstrated in single-arm and dual-arm manipulation tasks, with experimental results available in a video at https://youtu.be/nL1BMHpMR7c.

</details>


### [218] [Real-Time Robot Execution with Masked Action Chunking](https://arxiv.org/abs/2601.20130)
*Haoxuan Wang,Gengyu Zhang,Yan Yan,Yuzhang Shang,Ramana Rao Kompella,Gaowen Liu*

Main category: cs.RO

TL;DR: REMAC addresses execution failures in asynchronous inference for real-time robot manipulation by correcting intra-chunk inconsistency (misalignment between executed actions and current perception) through masked action chunking and prefix-preserved sampling.


<details>
  <summary>Details</summary>
Motivation: Asynchronous inference enables real-time robot manipulation by predicting next actions while executing current ones, but naive implementations often fail due to overlooked intra-chunk inconsistency where executed actions misalign with current perception.

Method: REMAC learns corrective adjustments on pretrained policies using masked action chunking to handle mismatches between intended and executed actions during asynchronous inference, plus prefix-preserved sampling to reinforce inter-chunk continuity.

Result: Extensive experiments in simulation and real-world show REMAC enables faster task execution, maintains robustness across varying delays, and achieves higher completion rates without additional latency.

Conclusion: REMAC successfully addresses both intra-chunk inconsistency and inter-chunk continuity issues in asynchronous inference, delivering more reliable real-time robot manipulation policies.

Abstract: Real-time execution is essential for cyber-physical systems such as robots. These systems operate in dynamic real-world environments where even small delays can undermine responsiveness and compromise performance. Asynchronous inference has recently emerged as a system-level paradigm for real-time robot manipulation, enabling the next action chunk to be predicted while the current one is being executed. While this approach achieves real-time responsiveness, naive integration often results in execution failure. Previous methods attributed this failure to inter-chunk discontinuity and developed test-time algorithms to smooth chunk boundaries. In contrast, we identify another critical yet overlooked factor: intra-chunk inconsistency, where the robot's executed action chunk partially misaligns with its current perception. To address this, we propose REMAC, which learns corrective adjustments on the pretrained policy through masked action chunking, enabling the policy to remain resilient under mismatches between intended actions and actual execution during asynchronous inference. In addition, we introduce a prefix-preserved sampling procedure to reinforce inter-chunk continuity. Overall, our method delivers more reliable policies without incurring additional latency. Extensive experiments in both simulation and real-world settings demonstrate that our method enables faster task execution, maintains robustness across varying delays, and consistently achieves higher completion rates.

</details>


### [219] [A Taylor Series Approach to Correct Localization Errors in Robotic Field Mapping using Gaussian Processes](https://arxiv.org/abs/2601.20149)
*Muzaffar Qureshi,Tochukwu Elijah Ogri,Kyle Volle,Rushikesh Kamalapurkar*

Main category: cs.RO

TL;DR: A method for updating Gaussian Process models when improved location estimates become available, using second-order corrections based on kernel differentiability to handle sensor localization uncertainty in mobile robot field mapping.


<details>
  <summary>Details</summary>
Motivation: Real-world scalar field mapping with mobile robots suffers from imperfect localization, causing discrepancies between estimated and true measurement locations that degrade GP model accuracy. Traditional GPs assume perfect location knowledge, which doesn't match practical mobile robot applications.

Method: Proposes a second-order correction algorithm leveraging kernel function differentiability. Uses precomputed Jacobians and Hessians of GP mean and covariance functions for real-time refinement based on measurement location discrepancy data, avoiding full model retraining.

Result: Simulation results show improved prediction accuracy and computational efficiency compared to full model retraining, demonstrating the method's effectiveness in handling localization uncertainty.

Conclusion: The proposed approach enables efficient GP model updates when improved location estimates become available, addressing the practical challenge of imperfect localization in mobile robot field mapping applications while maintaining computational efficiency.

Abstract: Gaussian Processes (GPs) are powerful non-parametric Bayesian models for regression of scalar fields, formulated under the assumption that measurement locations are perfectly known and the corresponding field measurements have Gaussian noise. However, many real-world scalar field mapping applications rely on sensor-equipped mobile robots to collect field measurements, where imperfect localization introduces state uncertainty. Such discrepancies between the estimated and true measurement locations degrade GP mean and covariance estimates. To address this challenge, we propose a method for updating the GP models when improved estimates become available. Leveraging the differentiability of the kernel function, a second-order correction algorithm is developed using the precomputed Jacobians and Hessians of the GP mean and covariance functions for real-time refinement based on measurement location discrepancy data. Simulation results demonstrate improved prediction accuracy and computational efficiency compared to full model retraining.

</details>


### [220] [TRACER: Texture-Robust Affordance Chain-of-Thought for Deformable-Object Refinement](https://arxiv.org/abs/2601.20208)
*Wanjun Jia,Kang Li,Fan Yang,Mengfei Duan,Wenrui Chen,Yiming Jiang,Hui Zhang,Kailun Yang,Zhiyong Li,Yaonan Wang*

Main category: cs.RO

TL;DR: TRACER is a framework for robotic manipulation of deformable objects that improves affordance prediction by combining hierarchical semantic reasoning with appearance-robust functional region refinement, addressing challenges of boundary overflow and fragmented predictions.


<details>
  <summary>Details</summary>
Motivation: Existing vision-based affordance prediction methods struggle with deformable objects due to near-infinite degrees of freedom, complex dynamics, and heterogeneous patterns, leading to boundary overflow and fragmented functional regions. There's a need to better align high-level semantic instructions with physical interaction points under complex appearance and texture variations.

Method: TRACER framework includes: 1) Tree-structured Affordance Chain-of-Thought (TA-CoT) that decomposes high-level task intentions into hierarchical sub-task semantics; 2) Spatial-Constrained Boundary Refinement (SCBR) mechanism to suppress prediction spillover and guide perceptual response toward authentic interaction manifolds; 3) Interactive Convergence Refinement Flow (ICRF) to aggregate discrete pixels corrupted by appearance noise, enhancing spatial continuity and physical plausibility.

Result: Extensive experiments on Fine-AGDDO15 dataset and real-world robotic platform show TRACER significantly improves affordance grounding precision across diverse textures and patterns, and enhances success rate of long-horizon tasks, effectively bridging the gap between high-level semantic reasoning and low-level physical execution.

Conclusion: TRACER successfully addresses key challenges in deformable object manipulation by establishing cross-hierarchical mapping from semantic reasoning to appearance-robust functional region refinement, demonstrating practical improvements in both precision and task success rates for real-world robotic applications.

Abstract: The central challenge in robotic manipulation of deformable objects lies in aligning high-level semantic instructions with physical interaction points under complex appearance and texture variations. Due to near-infinite degrees of freedom, complex dynamics, and heterogeneous patterns, existing vision-based affordance prediction methods often suffer from boundary overflow and fragmented functional regions. To address these issues, we propose TRACER, a Texture-Robust Affordance Chain-of-thought with dEformable-object Refinement framework, which establishes a cross-hierarchical mapping from hierarchical semantic reasoning to appearance-robust and physically consistent functional region refinement. Specifically, a Tree-structured Affordance Chain-of-Thought (TA-CoT) is formulated to decompose high-level task intentions into hierarchical sub-task semantics, providing consistent guidance across various execution stages. To ensure spatial integrity, a Spatial-Constrained Boundary Refinement (SCBR) mechanism is introduced to suppress prediction spillover, guiding the perceptual response to converge toward authentic interaction manifolds. Furthermore, an Interactive Convergence Refinement Flow (ICRF) is developed to aggregate discrete pixels corrupted by appearance noise, significantly enhancing the spatial continuity and physical plausibility of the identified functional regions. Extensive experiments conducted on the Fine-AGDDO15 dataset and a real-world robotic platform demonstrate that TRACER significantly improves affordance grounding precision across diverse textures and patterns inherent to deformable objects. More importantly, it enhances the success rate of long-horizon tasks, effectively bridging the gap between high-level semantic reasoning and low-level physical execution. The source code and dataset will be made publicly available at https://github.com/Dikay1/TRACER.

</details>


### [221] [TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance](https://arxiv.org/abs/2601.20239)
*Zhemeng Zhang,Jiahua Ma,Xincheng Yang,Xin Wen,Yuzhi Zhang,Boyan Li,Yiran Qin,Jin Liu,Can Zhao,Li Kang,Haoqin Hong,Zhenfei Yin,Philip Torr,Hao Su,Ruimao Zhang,Daolin Ma*

Main category: cs.RO

TL;DR: TouchGuide is a visuo-tactile fusion method that refines robot manipulation actions using tactile feedback to ensure physical contact feasibility, outperforming existing methods on contact-rich tasks.


<details>
  <summary>Details</summary>
Motivation: Fine-grained and contact-rich manipulation remains challenging for robots due to underutilization of tactile feedback. Current approaches don't effectively integrate tactile information to ensure actions satisfy physical contact constraints.

Method: Two-stage approach: 1) Pre-trained diffusion/flow-matching visuomotor policy produces coarse visually-plausible actions using visual inputs. 2) Contact Physical Model (CPM) provides tactile guidance to steer and refine actions to align with realistic contact conditions. CPM trained via contrastive learning on limited expert demonstrations. Also introduced TacUMI data collection system using rigid fingertips for affordable, high-quality tactile data.

Result: Extensive experiments on five challenging contact-rich tasks (shoe lacing, chip handover, etc.) show TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies.

Conclusion: TouchGuide effectively integrates tactile feedback through cross-policy fusion, enabling robots to perform fine-grained, contact-rich manipulation by ensuring actions satisfy physical contact constraints, with demonstrated superior performance over existing methods.

Abstract: Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies.

</details>


### [222] [Shallow-Ï€: Knowledge Distillation for Flow-based VLAs](https://arxiv.org/abs/2601.20262)
*Boseong Jeon,Yunho Choi,Taehan Kim*

Main category: cs.RO

TL;DR: Shallow-pi is a knowledge distillation framework that reduces transformer depth in vision-language-action models from 18 to 6 layers, achieving 2x faster inference with minimal performance drop, validated on real-world robotic platforms.


<details>
  <summary>Details</summary>
Motivation: Real-time robotic deployment requires fast, on-device inference for VLA models. While token-level efficiency has been studied, systematic transformer layer reduction has received limited attention, especially for flow-based VLA models under knowledge distillation.

Method: Proposes Shallow-pi, a principled knowledge distillation framework that aggressively reduces transformer depth of both VLM backbone and flow-based action head, compressing models from 18 to 6 layers.

Result: Achieves over 2x faster inference with less than 1% absolute drop in success rate on standard manipulation benchmarks, establishing state-of-the-art performance among reduced VLA models.

Conclusion: The approach was successfully validated through industrial-scale real-world experiments on Jetson Orin and Jetson Thor across multiple robot platforms, including humanoid systems, in complex and dynamic manipulation scenarios.

Abstract: The growing demand for real-time robotic deployment necessitates fast and on-device inference for vision-language-action (VLA) models. Within the VLA literature, efficiency has been extensively studied at the token level, such as visual token pruning. In contrast, systematic transformer layer reduction has received limited attention and, to the best of our knowledge, has not been explored for flow-based VLA models under knowledge distillation. In this work, we propose Shallow-pi, a principled knowledge distillation framework that aggressively reduces the transformer depth of both the VLM backbone and the flow-based action head, compressing the model from 18 to 6 layers. Shallow-pi achieves over two times faster inference with less than one percent absolute drop in success rate on standard manipulation benchmarks, establishing state-of-the-art performance among reduced VLA models. Crucially, we validate our approach through industrial-scale real-world experiments on Jetson Orin and Jetson Thor across multiple robot platforms, including humanoid systems, in complex and dynamic manipulation scenarios.

</details>


### [223] [Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation](https://arxiv.org/abs/2601.20321)
*Yuzhe Huang,Pei Lin,Wanlin Li,Daohan Li,Jiajun Li,Jiaming Jiang,Chenxi Xiao,Ziyuan Jiao*

Main category: cs.RO

TL;DR: TaF-VLA introduces tactile-force alignment for VLA models, using a tactile-force adapter to ground tactile observations in physical forces, enabling better contact-rich robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language-Action models lack physical intuition for contact-rich tasks requiring precise force regulation, and existing tactile integration treats tactile inputs as visual textures rather than capturing interaction dynamics.

Method: Proposes tactile-force alignment paradigm with TaF-VLA framework, including automated tactile-force data acquisition device, TaF-Dataset with 10M+ synchronized tactile/force data, and Tactile-Force Adapter that extracts discretized latent representations from tactile observations.

Result: TaF-VLA significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, demonstrating robust, force-aware manipulation through cross-modal physical reasoning.

Conclusion: Tactile-force alignment provides superior physical reasoning for contact-rich manipulation compared to visual-texture approaches, enabling VLA models to achieve better force regulation and interaction dynamics understanding.

Abstract: Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning.

</details>


### [224] [Demonstration-Free Robotic Control via LLM Agents](https://arxiv.org/abs/2601.20334)
*Brian Y. Tsui,Alan Y. Fang,Tiffany J. Hwu*

Main category: cs.RO

TL;DR: LLM agent frameworks (like Claude Agent SDK) can be directly applied to robotic manipulation without modifications, achieving performance comparable to specialized VLA models without needing demonstrations or fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current vision-language-action (VLA) models for robotic manipulation require task-specific demonstrations and fine-tuning, and often generalize poorly under domain shift. The authors investigate whether general-purpose LLM agent frameworks (originally developed for software engineering) can serve as an alternative control paradigm for embodied manipulation.

Method: FAEA (Frontier Agent as Embodied Agent) applies an unmodified LLM agent framework (Claude Agent SDK) directly to embodied manipulation tasks. It uses the same iterative reasoning that enables software agents to debug code to reason through manipulation strategies. The approach is evaluated across LIBERO, ManiSkill3, and MetaWorld benchmarks with privileged environment state access.

Result: FAEA achieves success rates of 84.9% (LIBERO), 85.7% (ManiSkill3), and 96% (MetaWorld). This approaches the performance of VLA models trained with less than 100 demonstrations per task, but without requiring demonstrations or fine-tuning. With one round of human feedback, performance increases to 88.2% on LIBERO.

Conclusion: General-purpose LLM agents are sufficient for manipulation tasks dominated by deliberative, task-level planning. This opens a path for robotics systems to leverage actively maintained agent infrastructure and benefit directly from ongoing advances in frontier models. The demonstration-free capability has immediate practical value for autonomous exploration and training data augmentation.

Abstract: Robotic manipulation has increasingly adopted vision-language-action (VLA) models, which achieve strong performance but typically require task-specific demonstrations and fine-tuning, and often generalize poorly under domain shift. We investigate whether general-purpose large language model (LLM) agent frameworks, originally developed for software engineering, can serve as an alternative control paradigm for embodied manipulation. We introduce FAEA (Frontier Agent as Embodied Agent), which applies an LLM agent framework directly to embodied manipulation without modification. Using the same iterative reasoning that enables software agents to debug code, FAEA enables embodied agents to reason through manipulation strategies. We evaluate an unmodified frontier agent, Claude Agent SDK, across the LIBERO, ManiSkill3, and MetaWorld benchmarks. With privileged environment state access, FAEA achieves success rates of 84.9%, 85.7%, and 96%, respectively. This level of task success approaches that of VLA models trained with less than 100 demonstrations per task, without requiring demonstrations or fine-tuning. With one round of human feedback as an optional optimization, performance increases to 88.2% on LIBERO. This demonstration-free capability has immediate practical value: FAEA can autonomously explore novel scenarios in simulation and generate successful trajectories for training data augmentation in embodied learning. Our results indicate that general-purpose agents are sufficient for a class of manipulation tasks dominated by deliberative, task-level planning. This opens a path for robotics systems to leverage actively maintained agent infrastructure and benefit directly from ongoing advances in frontier models. Code is available at https://github.com/robiemusketeer/faea-sim

</details>


### [225] [RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification](https://arxiv.org/abs/2601.20377)
*Xinyan Chen,Qinchun Li,Ruiqin Ma,Jiaqi Bai,Li Yi,Jianfei Yang*

Main category: cs.RO

TL;DR: RF-MatID is the first open-source, large-scale, wide-band RF dataset for fine-grained material identification, featuring 16 categories across 5 superclasses with 142k samples and systematic geometry perturbations.


<details>
  <summary>Details</summary>
Motivation: Current vision-based material identification is limited by optical sensors, while RF approaches can reveal intrinsic material properties but lack large-scale public datasets and proper benchmarking for learning-based approaches.

Method: Created RF-MatID dataset with 16 fine-grained material categories across 5 superclasses, spanning 4-43.5 GHz frequency range, including 142k samples in both frequency- and time-domain representations. Systematically incorporated geometry perturbations (incidence angle and stand-off distance variations). Established multi-setting benchmark evaluating state-of-the-art deep learning models.

Result: Dataset enables reproducible research and systematic benchmarking. Established 5 frequency-allocation protocols for frequency- and region-level analysis. Provides comprehensive evaluation of both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts.

Conclusion: RF-MatID aims to accelerate algorithmic advancement, foster cross-domain robustness, and support real-world deployment of RF-based material identification systems by providing the first comprehensive open-source dataset and benchmark.

Abstract: Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification.

</details>


### [226] [STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation](https://arxiv.org/abs/2601.20381)
*Alexandre Chapin,Emmanuel DellandrÃ©a,Liming Chen*

Main category: cs.RO

TL;DR: STORM is a lightweight object-centric adaptation module that adds semantic-aware slots to frozen visual foundation models for robotic manipulation, improving robustness and generalization through multi-phase training.


<details>
  <summary>Details</summary>
Motivation: Visual foundation models provide strong perceptual features but lack explicit object-level structure, limiting robustness and contractility in manipulation tasks. Dense representations from these models don't capture object-level abstractions needed for reliable robotic control.

Method: STORM uses a multi-phase training strategy: 1) Object-centric slots are first stabilized through visual-semantic pretraining using language embeddings, 2) Then jointly adapted with a downstream manipulation policy. This prevents degenerate slot formation while preserving semantic consistency and aligning perception with task objectives.

Result: Experiments on object discovery benchmarks and simulated manipulation tasks show STORM improves generalization to visual distractors and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end.

Conclusion: Multi-phase adaptation is an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control, offering a lightweight alternative to retraining large backbones.

Abstract: Visual foundation models provide strong perceptual features for robotics, but their dense representations lack explicit object-level structure, limiting robustness and contractility in manipulation tasks. We propose STORM (Slot-based Task-aware Object-centric Representation for robotic Manipulation), a lightweight object-centric adaptation module that augments frozen visual foundation models with a small set of semantic-aware slots for robotic manipulation. Rather than retraining large backbones, STORM employs a multi-phase training strategy: object-centric slots are first stabilized through visual--semantic pretraining using language embeddings, then jointly adapted with a downstream manipulation policy. This staged learning prevents degenerate slot formation and preserves semantic consistency while aligning perception with task objectives. Experiments on object discovery benchmarks and simulated manipulation tasks show that STORM improves generalization to visual distractors, and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end. Our results highlight multi-phase adaptation as an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control.

</details>


### [227] [A Practical Framework of Key Performance Indicators for Multi-Robot Lunar and Planetary Field Tests](https://arxiv.org/abs/2601.20529)
*Julia Richter,David Oberacker,Gabriela Ligeza,Valentin T. Bickel,Philip Arm,William Talbot,Marvin Grosse Besselmann,Florian Kehl,Tristan Schnell,Hendrik Kolvenbach,RÃ¼diger Dillmann,Arne Roennau,Marco Hutter*

Main category: cs.RO

TL;DR: A framework for evaluating multi-robot lunar prospecting missions using scenario-derived KPIs that link field performance to scientific objectives.


<details>
  <summary>Details</summary>
Motivation: Current robotic prospecting field trials lack standardized evaluation methods, making comparisons difficult due to different platforms, setups, and engineering metrics that don't connect to science objectives.

Method: Derived a structured KPI framework from three realistic multi-robot lunar scenarios reflecting scientific goals and operational constraints, emphasizing scenario-dependent priorities in efficiency, robustness, and precision.

Result: Validated the framework in multi-robot field tests - found practical and easy to apply for efficiency and robustness KPIs, but precision KPIs require reliable ground-truth data that's often unavailable in outdoor analog environments.

Conclusion: Proposes this framework as a common evaluation standard for consistent, goal-oriented comparison of multi-robot field trials to support systematic development of robotic systems for planetary exploration.

Abstract: Robotic prospecting for critical resources on the Moon, such as ilmenite, rare earth elements, and water ice, requires robust exploration methods given the diverse terrain and harsh environmental conditions. Although numerous analog field trials address these goals, comparing their results remains challenging because of differences in robot platforms and experimental setups. These missions typically assess performance using selected, scenario-specific engineering metrics that fail to establish a clear link between field performance and science-driven objectives. In this paper, we address this gap by deriving a structured framework of KPI from three realistic multi-robot lunar scenarios reflecting scientific objectives and operational constraints. Our framework emphasizes scenario-dependent priorities in efficiency, robustness, and precision, and is explicitly designed for practical applicability in field deployments. We validated the framework in a multi-robot field test and found it practical and easy to apply for efficiency- and robustness-related KPI, whereas precision-oriented KPI require reliable ground-truth data that is not always feasible to obtain in outdoor analog environments. Overall, we propose this framework as a common evaluation standard enabling consistent, goal-oriented comparison of multi-robot field trials and supporting systematic development of robotic systems for future planetary exploration.

</details>


### [228] [Vibro-Sense: Robust Vibration-based Impulse Response Localization and Trajectory Tracking for Robotic Hands](https://arxiv.org/abs/2601.20555)
*Wadhah Zai El Amri,NicolÃ¡s Navarro-Guerrero*

Main category: cs.RO

TL;DR: Low-cost vibro-acoustic sensing with piezoelectric microphones and Audio Spectrogram Transformer achieves under 5mm touch localization on robotic hands, with material-specific performance advantages.


<details>
  <summary>Details</summary>
Motivation: Traditional tactile skins are expensive and complex to integrate, creating a need for scalable, affordable alternatives for rich contact perception in robotic manipulation.

Method: Equip robotic hand with seven low-cost piezoelectric microphones, use Audio Spectrogram Transformer to decode vibrational signatures from physical interactions, analyze material-specific responses.

Result: Achieves under 5mm localization error in static conditions, demonstrates material-specific advantages (metal for impulse response, wood for friction-based tracking), maintains robustness during robot motion.

Conclusion: Complex contact dynamics can be effectively decoded from simple vibrational signals, providing a viable pathway to widespread, affordable contact perception in robotics, with open-source resources provided.

Abstract: Rich contact perception is crucial for robotic manipulation, yet traditional tactile skins remain expensive and complex to integrate. This paper presents a scalable alternative: high-accuracy whole-body touch localization via vibro-acoustic sensing. By equipping a robotic hand with seven low-cost piezoelectric microphones and leveraging an Audio Spectrogram Transformer, we decode the vibrational signatures generated during physical interaction. Extensive evaluation across stationary and dynamic tasks reveals a localization error of under 5 mm in static conditions. Furthermore, our analysis highlights the distinct influence of material properties: stiff materials (e.g., metal) excel in impulse response localization due to sharp, high-bandwidth responses, whereas textured materials (e.g., wood) provide superior friction-based features for trajectory tracking. The system demonstrates robustness to the robot's own motion, maintaining effective tracking even during active operation. Our primary contribution is demonstrating that complex physical contact dynamics can be effectively decoded from simple vibrational signals, offering a viable pathway to widespread, affordable contact perception in robotics. To accelerate research, we provide our full datasets, models, and experimental setups as open-source resources.

</details>


### [229] [MeCo: Enhancing LLM-Empowered Multi-Robot Collaboration via Similar Task Memoization](https://arxiv.org/abs/2601.20577)
*Baiqing Wang,Helei Cui,Bo Zhang,Xiaolong Zheng,Bin Guo,Zhiwen Yu*

Main category: cs.RO

TL;DR: MeCo is a similarity-aware multi-robot collaboration framework that uses memoization to cache and reuse solutions for similar tasks, reducing redundant LLM planning and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Current LLM-empowered multi-robot collaboration methods are inefficient because they replan from scratch for similar tasks, ignoring task-level similarities and causing redundant computation.

Method: MeCo introduces a similarity testing method to retrieve previously solved tasks with high relevance, enabling plan reuse without re-invoking LLMs, and applies the "cache and reuse" principle (memoization).

Result: Experimental results show MeCo substantially reduces planning costs and improves success rates compared with state-of-the-art approaches, and the authors also present MeCoBench, the first benchmark for similar-task collaboration scenarios.

Conclusion: MeCo addresses the efficiency limitation in LLM-based multi-robot collaboration by leveraging task similarity through memoization, enabling more practical and adaptable systems for real-world applications.

Abstract: Multi-robot systems have been widely deployed in real-world applications, providing significant improvements in efficiency and reductions in labor costs. However, most existing multi-robot collaboration methods rely on extensive task-specific training, which limits their adaptability to new or diverse scenarios. Recent research leverages the language understanding and reasoning capabilities of large language models (LLMs) to enable more flexible collaboration without specialized training. Yet, current LLM-empowered approaches remain inefficient: when confronted with identical or similar tasks, they must replan from scratch because they omit task-level similarities. To address this limitation, we propose MeCo, a similarity-aware multi-robot collaboration framework that applies the principle of ``cache and reuse'' (a.k.a., memoization) to reduce redundant computation. Unlike simple task repetition, identifying and reusing solutions for similar but not identical tasks is far more challenging, particularly in multi-robot settings. To this end, MeCo introduces a new similarity testing method that retrieves previously solved tasks with high relevance, enabling effective plan reuse without re-invoking LLMs. Furthermore, we present MeCoBench, the first benchmark designed to evaluate performance on similar-task collaboration scenarios. Experimental results show that MeCo substantially reduces planning costs and improves success rates compared with state-of-the-art approaches.

</details>


### [230] [GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control](https://arxiv.org/abs/2601.20668)
*Shuhao Liao,Peizhuo Li,Xinrong Yang,Linnan Chang,Zhaoxin Fan,Qing Wang,Lei Shi,Yuhong Cao,Wenjun Wu,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: GPO is a training framework for legged robot RL that uses time-varying action transformations to restrict action space early for better learning, then expands it for exploration, achieving better performance on quadruped/hexapod robots.


<details>
  <summary>Details</summary>
Motivation: Training RL policies for legged robots is challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods work well for position-based control but are less effective for torque-based control where exploration and gradient signals are more difficult.

Method: Growing Policy Optimization (GPO) applies time-varying action transformations to restrict effective action space early (encouraging effective data collection and policy learning), then progressively expands it to enhance exploration and achieve higher expected return. The transformation preserves PPO update rule with bounded, vanishing gradient distortion.

Result: GPO evaluated on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance than existing methods.

Conclusion: GPO provides a general, environment-agnostic optimization framework for learning legged locomotion that works effectively for torque-based control where previous methods struggled.

Abstract: Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion.

</details>


### [231] [Tendon-based modelling, estimation and control for a simulated high-DoF anthropomorphic hand model](https://arxiv.org/abs/2601.20682)
*PÃ©ter Polcz,Katalin SchÃ¤ffer,MiklÃ³s Koller*

Main category: cs.RO

TL;DR: A computational method for estimating joint angles in tendon-driven robotic hands using tendon displacement/tension measurements, enabling closed-loop control without direct joint sensing.


<details>
  <summary>Details</summary>
Motivation: Tendon-driven anthropomorphic robotic hands often lack direct joint angle sensing because integrating joint encoders compromises mechanical compactness and dexterity. There's a need for alternative methods to estimate joint positions for control purposes.

Method: 1) Developed efficient kinematic modeling framework using Denavit-Hartenberg convention. 2) Derived nonlinear equations relating tendon states (displacements/tensions) to joint positions using simplified tendon model. 3) Solved equations via nonlinear optimization. 4) Implemented Jacobian-based PI controller with feedforward term for closed-loop gesture tracking using estimated joint angles.

Result: Demonstrated effectiveness and limitations of the estimation and control framework in MuJoCo simulation using the Anatomically Correct Biomechatronic Hand (5 DOF per long finger, 6 DOF for thumb). Successfully enabled gesture tracking without direct joint sensing.

Conclusion: The proposed computational method provides a viable alternative to direct joint sensing for tendon-driven robotic hands, enabling closed-loop control while maintaining mechanical compactness and dexterity. The framework shows promise for practical implementation in anthropomorphic hand designs.

Abstract: Tendon-driven anthropomorphic robotic hands often lack direct joint angle sensing, as the integration of joint encoders can compromise mechanical compactness and dexterity. This paper presents a computational method for estimating joint positions from measured tendon displacements and tensions. An efficient kinematic modeling framework for anthropomorphic hands is first introduced based on the Denavit-Hartenberg convention. Using a simplified tendon model, a system of nonlinear equations relating tendon states to joint positions is derived and solved via a nonlinear optimization approach. The estimated joint angles are then employed for closed-loop control through a Jacobian-based proportional-integral (PI) controller augmented with a feedforward term, enabling gesture tracking without direct joint sensing. The effectiveness and limitations of the proposed estimation and control framework are demonstrated in the MuJoCo simulation environment using the Anatomically Correct Biomechatronic Hand, featuring five degrees of freedom for each long finger and six degrees of freedom for the thumb.

</details>


### [232] [One Step Is Enough: Dispersive MeanFlow Policy Optimization](https://arxiv.org/abs/2601.20701)
*Guowei Zou,Haitao Wang,Hejun Wu,Yukun Qian,Yuhang Wang,Weibing Li*

Main category: cs.RO

TL;DR: DMPO enables true one-step robotic policy generation with MeanFlow inference, dispersive regularization, and RL fine-tuning, achieving >120Hz real-time control with 5-20x speedup over multi-step methods.


<details>
  <summary>Details</summary>
Motivation: Existing generative policies (diffusion/flow matching) require multi-step sampling, limiting deployment in time-critical robotic control scenarios where fast action generation is essential.

Method: Three key components: 1) MeanFlow for mathematically-derived single-step inference without knowledge distillation, 2) dispersive regularization to prevent representation collapse, 3) RL fine-tuning to surpass expert demonstrations.

Result: Competitive/superior performance on RoboMimic manipulation and OpenAI Gym locomotion benchmarks compared to multi-step baselines. Achieves >120Hz real-time control with 5-20x inference speedup, reaching hundreds of Hertz on high-performance GPUs.

Conclusion: DMPO enables true one-step generation for real-time robotic control, validated by physical deployment on a Franka-Emika-Panda robot, meeting real-world time-critical requirements.

Abstract: Real-time robotic control demands fast action generation. However, existing generative policies based on diffusion and flow matching require multi-step
  sampling, fundamentally limiting deployment in time-critical scenarios. We propose Dispersive MeanFlow Policy Optimization (DMPO), a unified framework that
  enables true one-step generation through three key components: MeanFlow for mathematically-derived single-step inference without knowledge distillation,
  dispersive regularization to prevent representation collapse, and reinforcement learning (RL) fine-tuning to surpass expert demonstrations. Experiments
  across RoboMimic manipulation and OpenAI Gym locomotion benchmarks demonstrate competitive or superior performance compared to multi-step baselines. With
  our lightweight model architecture and the three key algorithmic components working in synergy, DMPO exceeds real-time control requirements (>120Hz) with
  5-20x inference speedup, reaching hundreds of Hertz on high-performance GPUs. Physical deployment on a Franka-Emika-Panda robot validates real-world
  applicability.

</details>


### [233] [Learning From a Steady Hand: A Weakly Supervised Agent for Robot Assistance under Microscopy](https://arxiv.org/abs/2601.20776)
*Huanyu Tian,Martin Huber,Lingyun Zeng,Zhe Han,Wayne Bennett,Giuseppe Silvestri,Gerardo Mendizabal-Ruiz,Tom Vercauteren,Alejandro Chavez-Badiola,Christos Bergeles*

Main category: cs.RO

TL;DR: A weakly supervised robotic manipulation framework that uses warm-up trajectories for calibration-aware perception without manual labeling, achieving high precision and reducing user workload in microscope-guided micromanipulation.


<details>
  <summary>Details</summary>
Motivation: To improve steady-hand robotic manipulation by eliminating labor-intensive 2D labeling and external fiducials, while achieving calibration-aware perception for reliable microscope-guided biomedical micromanipulation.

Method: Uses reusable warm-up trajectories to extract implicit spatial information, fuses calibration-aware perception with admittance control, characterizes residuals from observation and calibration models to establish task-space error budget.

Result: Achieves lateral accuracy of ~49Î¼m at 95% confidence (worst-case) and depth accuracy â‰¤291Î¼m at 95% confidence; reduces NASA-TLX workload by 77.1% compared to baseline in user study (N=8).

Conclusion: The weakly supervised framework improves reliability of microscope-guided biomedical micromanipulation without complex setup, offering a practical solution for microscope-guided interventions.

Abstract: This paper rethinks steady-hand robotic manipulation by using a weakly supervised framework that fuses calibration-aware perception with admittance control. Unlike conventional automation that relies on labor-intensive 2D labeling, our framework leverages reusable warm-up trajectories to extract implicit spatial information, thereby achieving calibration-aware, depth-resolved perception without the need for external fiducials or manual depth annotation. By explicitly characterizing residuals from observation and calibration models, the system establishes a task-space error budget from recorded warm-ups. The uncertainty budget yields a lateral closed-loop accuracy of approx. 49 micrometers at 95% confidence (worst-case testing subset) and a depth accuracy of <= 291 micrometers at 95% confidence bound during large in-plane moves. In a within-subject user study (N=8), the learned agent reduces overall NASA-TLX workload by 77.1% relative to the simple steady-hand assistance baseline. These results demonstrate that the weakly supervised agent improves the reliability of microscope-guided biomedical micromanipulation without introducing complex setup requirements, offering a practical framework for microscope-guided intervention.

</details>


### [234] [A Methodology for Designing Knowledge-Driven Missions for Robots](https://arxiv.org/abs/2601.20797)
*Guillermo GP-Lenza,Carmen DR. Pita-Romero,Miguel Fernandez-Cortizas,Pascual Campoy*

Main category: cs.RO

TL;DR: Methodology for implementing knowledge graphs in ROS 2 to enhance autonomous robotic missions, demonstrated with drone search-and-rescue simulation.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency and intelligence of autonomous robotic missions by integrating knowledge graphs into ROS 2 systems for better decision-making and mission performance.

Method: Comprehensive methodology with key steps: defining initial/target conditions, structuring tasks/subtasks, planning sequence, representing data in knowledge graph, designing mission with high-level language. Implemented in Aerostack2 framework with simulated Gazebo environment.

Result: Successful demonstration through simulated search and rescue mission where drones autonomously locate target, showing effectiveness in improving decision-making and mission performance.

Conclusion: Knowledge graph implementation in ROS 2 systems enhances autonomous robotic missions by providing structured knowledge representation that improves efficiency and intelligence in mission execution.

Abstract: This paper presents a comprehensive methodology for implementing knowledge graphs in ROS 2 systems, aiming to enhance the efficiency and intelligence of autonomous robotic missions. The methodology encompasses several key steps: defining initial and target conditions, structuring tasks and subtasks, planning their sequence, representing task-related data in a knowledge graph, and designing the mission using a high-level language. Each step builds on the previous one to ensure a cohesive process from initial setup to final execution. A practical implementation within the Aerostack2 framework is demonstrated through a simulated search and rescue mission in a Gazebo environment, where drones autonomously locate a target. This implementation highlights the effectiveness of the methodology in improving decision-making and mission performance by leveraging knowledge graphs.

</details>


### [235] [End-to-end example-based sim-to-real RL policy transfer based on neural stylisation with application to robotic cutting](https://arxiv.org/abs/2601.20846)
*Jamie Hathaway,Alireza Rastegarpanah,Rustam Stolkin*

Main category: cs.RO

TL;DR: Novel sim-to-real RL transfer method using neural style transfer reinterpretation to synthesize realistic training data from unpaired real-world datasets, applied to robot cutting of unknown materials with improved performance over baselines.


<details>
  <summary>Details</summary>
Motivation: RL for robotic control faces limitations due to simulation-to-reality domain gap and limited real-world data availability, especially for contact-rich tasks where real-world reward information is unavailable.

Method: Reinterpret neural style transfer from image processing to synthesize novel training data from unpaired unlabelled real-world datasets. Use variational autoencoder to jointly learn self-supervised feature representations for style transfer and generate weakly paired source-target trajectories to improve physical realism.

Result: Achieves improved task completion time and behavioral stability with minimal real-world data compared to baseline methods (CycleGAN, conditional VAE-based time series translation, and previous work). Demonstrates robustness to geometric and material variation.

Conclusion: The framework enables feasible policy adaptation for challenging contact-rich tasks without real-world reward information, bridging the sim-to-real gap through novel data synthesis approach.

Abstract: Whereas reinforcement learning has been applied with success to a range of robotic control problems in complex, uncertain environments, reliance on extensive data - typically sourced from simulation environments - limits real-world deployment due to the domain gap between simulated and physical systems, coupled with limited real-world sample availability. We propose a novel method for sim-to-real transfer of reinforcement learning policies, based on a reinterpretation of neural style transfer from image processing to synthesise novel training data from unpaired unlabelled real world datasets. We employ a variational autoencoder to jointly learn self-supervised feature representations for style transfer and generate weakly paired source-target trajectories to improve physical realism of synthesised trajectories. We demonstrate the application of our approach based on the case study of robot cutting of unknown materials. Compared to baseline methods, including our previous work, CycleGAN, and conditional variational autoencoder-based time series translation, our approach achieves improved task completion time and behavioural stability with minimal real-world data. Our framework demonstrates robustness to geometric and material variation, and highlights the feasibility of policy adaptation in challenging contact-rich tasks where real-world reward information is unavailable.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [236] [Quick Change Detection in Discrete-Time in Presence of a Covert Adversary](https://arxiv.org/abs/2601.20022)
*Amir Reza Ramtin,Philippe Nain,Don Towsley*

Main category: cs.IT

TL;DR: Covert quickest change detection with adversarial post-change distribution selection, where ADD scales as Î˜(Î³) instead of classical O(log Î³).


<details>
  <summary>Details</summary>
Motivation: Traditional change detection assumes fixed post-change distribution, but covert adversaries can adapt their strategy based on detector parameters to evade detection longer.

Method: Extends CuSum procedure to adversarial setting where post-change distribution depends on false alarm constraint Î³ and converges to pre-change distribution as Î³â†’âˆž. Uses asymptotic analysis of ADD and AT2FA.

Result: Establishes exact asymptotic expressions showing ADD = Î˜(Î³) in covert regime vs classical O(log Î³). Characterizes scaling laws and conditions for covertness. Provides explicit ADD formulas for Gaussian and Exponential models.

Conclusion: Covert adversaries can significantly delay detection by exploiting detector parameters, fundamentally changing scaling behavior from logarithmic to linear in false alarm constraint.

Abstract: We study the problem of covert quickest change detection in a discrete-time setting, where a sequence of observations undergoes a distributional change at an unknown time. Unlike classical formulations, we consider a covert adversary who has knowledge of the detector's false alarm constraint parameter $Î³$ and selects a stationary post-change distribution that depends on it, seeking to remain undetected for as long as possible. Building on the theoretical foundations of the CuSum procedure, we rigorously characterize the asymptotic behavior of the average detection delay (ADD) and the average time to false alarm (AT2FA) when the post-change distribution converges to the pre-change distribution as $Î³\to \infty$. Our analysis establishes exact asymptotic expressions for these quantities, extending and refining classical results that no longer hold in this regime. We identify the critical scaling laws governing covert behavior and derive explicit conditions under which an adversary can maintain covertness, defined by ADD = $Î˜(Î³)$, whereas in the classical setting, ADD grows only as $\mathcal{O}(\log Î³)$. In particular, for Gaussian and Exponential models under adversarial perturbations of their respective parameters, we asymptotically characterize ADD as a function of the Kullback--Leibler divergence between the pre- and post-change distributions and $Î³$.

</details>


### [237] [On Efficient Polyphase Network Implementation Using Successive Vector Approximation](https://arxiv.org/abs/2601.20411)
*Luiz F. da S. Coelho,Didier Le Ruyet,Paulo S. R. Diniz*

Main category: cs.IT

TL;DR: Energy-efficient polyphase network for FBMC using greedy MP algorithm to convert floating-point to SOPOT for multiplierless implementation, outperforming state-of-the-art methods with similar complexity.


<details>
  <summary>Details</summary>
Motivation: To achieve energy-efficient implementation of filter bank multicarrier (FBMC) systems by developing multiplierless hardware for the polyphase network, reducing computational overhead and power consumption.

Method: Uses a greedy matching pursuits (MP) algorithm to approximate the polyphase network by converting numerical representations directly from floating point to sum of signed powers of two (SOPOT), enabling multiplierless implementation.

Result: The proposed technique achieves superior performance compared to other state-of-the-art methods for designing multiplierless hardware while maintaining similar computational complexity.

Conclusion: The greedy MP-based SOPOT conversion provides an effective approach for energy-efficient, multiplierless FBMC polyphase network implementation with competitive performance and complexity trade-offs.

Abstract: In this work, we explore an energy-efficient implementation of the polyphase network for a filter bank multicarrier (FBMC) system. The network is approximated using a greedy algorithm based on matching pursuits (MP) that converts the numerical representation directly from floating point to sum of signed powers of two (SOPOT), which is key for a multiplierless implementation. We compare this technique with other state-of-the-art methods for designing multiplierless hardware, and show that our technique achieves superior performance with similar computational complexity.

</details>


### [238] [Energy Efficient Downlink mMIMO Using Dynamic Antenna and Power Adaptation](https://arxiv.org/abs/2601.20586)
*Ravi Sharan B A G,Maliha Jada,Anders Karstensen,Daniela Laselva,Jyri HÃ¤mÃ¤lÃ¤inen,Silvio Mandelli*

Main category: cs.IT

TL;DR: Proposes a dynamic joint antenna and power adaptation scheme for mMIMO systems to improve network energy savings while maintaining user throughput, using CSI-RS framework and POLITE power adaptation.


<details>
  <summary>Details</summary>
Motivation: Address high data rate demands of 6G systems while reducing operational costs and meeting sustainability goals through network energy savings in mMIMO deployments.

Method: Dynamic joint antenna and power adaptation scheme using multiple CSI-RS framework for antenna adaptation and POLITE transmit power-aware link adaptation for power adaptation, adapting to users' instantaneous traffic and channel conditions.

Result: Consistently achieves balance between network energy savings and user perceived throughput across different network load conditions, significantly improves intra-cell interference and boosts overall NES in low/light load conditions without affecting UPT.

Conclusion: Proposed dynamic joint adaptation scheme effectively improves network energy savings while maintaining user throughput, offering a practical solution for sustainable 6G mMIMO systems.

Abstract: Massive multiple-input multiple-output (mMIMO) technology and its future evolutions are expected to address the high data rate demands of sixth generation (6G) communication systems. At the same time, network energy savings (NES) is essential in reducing the operational costs and meeting the sustainability goals of network operators. In this regard, we propose a dynamic scheme for joint antenna and power adaptation to improve NES from a user scheduling and resource allocation perspective. Antenna adaptation is performed using the multiple channel state information resource signal (CSI-RS) framework. Furthermore, the recently introduced transmit power-aware link adaptation scheme, referred to as POLITE for short, is used as the power adaptation technique. The proposed scheme adapts to variations in users' instantaneous traffic and channel conditions to opportunistically maximize NES while also inherently accounting for the user throughput. Numerical simulation results show that the proposed scheme consistently achieves a balance between NES and user perceived throughput (UPT) for different network load conditions. Especially in low and light load conditions, the proposed scheme significantly improves the intra-cell interference and boosts the overall NES, while ensuring that UPT is unaffected.

</details>


### [239] [Shortest LCD embeddings of binary, ternary and quaternary linear codes](https://arxiv.org/abs/2601.20600)
*Junmin An,Ji-Hoon Hong,Jon-Lark Kim,Haeun Lim*

Main category: cs.IT

TL;DR: The paper studies embedding linear codes into optimal LCD codes, determines the minimum number of columns needed for such embeddings, characterizes all shortest LCD embeddings, and finds new optimal LCD codes with improved parameters.


<details>
  <summary>Details</summary>
Motivation: Since self-orthogonal embeddings have yielded optimal self-orthogonal codes, and LCD codes are counterparts to self-orthogonal codes (having trivial hull), it's natural to investigate whether linear codes can be embedded into optimal LCD codes.

Method: First determine the number of columns to add to a generator matrix to embed a linear code into an LCD code. Then characterize all possible forms of shortest LCD embeddings of a linear code. Apply this method to binary and ternary Hamming codes of small lengths.

Result: Obtained optimal LCD codes with minimum distance 4 from small Hamming codes. Found new ternary LCD codes: [23,4,14], [23,5,12], [24,6,12], [25,5,14] and a new quaternary LCD [21,10,8] code, each with minimum distance one greater than known codes.

Conclusion: The shortest LCD embedding method is useful for finding optimal LCD codes over various fields, as demonstrated by discovering new codes with improved parameters compared to previously known ones.

Abstract: In the recent years, there has been active research on self-orthogonal embeddings of linear codes since they yielded some optimal self-orthogonal codes. LCD codes have a trivial hull so they are counterparts of self-orthogonal codes. So it is a natural question whether one can embed linear codes into optimal LCD codes. To answer it, we first determine the number of columns to be added to a generator matrix of a linear code in order to embed the given code into an LCD code. Then we characterize all possible forms of shortest LCD embeddings of a linear code. As examples, we start from binary and ternary Hamming codes of small lengths and obtain optimal LCD codes with minimum distance 4. Furthermore, we find new ternary LCD codes with parameters including $[23, 4, 14]$, $[23, 5, 12]$, $[24, 6, 12]$, and $[25, 5, 14]$ and a new quaternary LCD $[21, 10, 8]$ code, each of which has minimum distance one greater than those of known codes. This shows that our shortest LCD embedding method is useful in finding optimal LCD codes over various fields.

</details>


### [240] [Helper-Assisted Coding for Gaussian Wiretap Channels: Deep Learning Meets PhySec](https://arxiv.org/abs/2601.20678)
*Vidhi Rana,Remi A. Chou,Taejoon Kim*

Main category: cs.IT

TL;DR: Deep learning-based explicit short blocklength codes for Gaussian wiretap channels with helpers, showing improved security over existing non-cooperative codes.


<details>
  <summary>Details</summary>
Motivation: When eavesdropper has better channel than legitimate receiver, positive secrecy rates are impossible without help. Existing solutions are asymptotic/non-constructive. Need practical, explicit short blocklength codes demonstrating helper cooperation benefits.

Method: Two-layer design: 1) Reliability layer using autoencoder with successive interference cancellation, 2) Security layer using universal hash functions. Alternative architecture reduces training time by allowing independent message estimation without successive cancellation during training.

Result: Proposed codes show strict improvement in information leakage compared to existing non-helper codes. Design also applicable to multiple access wiretap channels with helpers.

Conclusion: First explicit short blocklength codes using deep learning and cryptography demonstrate practical benefits of transmitter cooperation for wiretap channel security, with reduced training time alternative architecture.

Abstract: Consider the Gaussian wiretap channel, where a transmitter wishes to send a confidential message to a legitimate receiver in the presence of an eavesdropper. It is well known that if the eavesdropper experiences less channel noise than the legitimate receiver, then it is impossible for the transmitter to achieve positive secrecy rates. A known solution to this issue consists in involving a second transmitter, referred to as a helper, to help the first transmitter to achieve security. While such a solution has been studied for the asymptotic blocklength regime and via non-constructive coding schemes, in this paper, for the first time, we design explicit and short blocklength codes using deep learning and cryptographic tools to demonstrate the benefit and practicality of cooperation between two transmitters over the wiretap channel. Specifically, our proposed codes show strict improvement in terms of information leakage compared to existing codes that do not consider a helper. Our code design approach relies on a reliability layer, implemented with an autoencoder architecture based on the successive interference cancellation method, and a security layer implemented with universal hash functions. We also propose an alternative autoencoder architecture that significantly reduces training time by allowing the decoders to independently estimate messages without successively canceling interference by the receiver during training. Additionally, we show that our code design is also applicable to the multiple access wiretap channel with helpers, where two transmitters send confidential messages to the legitimate receiver.

</details>


### [241] [Reflected wireless signals under random spatial sampling](https://arxiv.org/abs/2601.20699)
*H. Paul Keeler*

Main category: cs.IT

TL;DR: Random transmitter positioning creates unbounded peaks in power histograms when signal strength oscillates with distance, with implications for fading estimation and intelligent surface design.


<details>
  <summary>Details</summary>
Motivation: To understand how random transmitter positioning affects power distribution statistics, particularly when signal strength is oscillating or non-monotonic with distance, and to apply these insights to intelligent surface design in wireless networks.

Method: Developed a propagation model showing that randomly positioned transmitters generate singularities in power histograms at turning points of deterministic propagation models. Applied this to a physical model of a transmitter between two parallel passive walls, analyzing signal fading due to reflections.

Result: Discovered that unbounded peaks occur in power histograms when signal strength oscillates with distance. For the wall reflection model, observed power oscillations from reflections and derived a compact closed-form expression for received signal involving Lerch transcendent function for the special case of transmitter midway between walls.

Conclusion: Random propagation effects like fading can produce singularities in power distributions when signal strength oscillates with distance. These insights are valuable for estimating fading and designing intelligent surfaces in urban environments where wall reflections are common.

Abstract: We present a propagation model showing that a transmitter randomly positioned in space generates unbounded peaks in the histogram of the resulting power, provided the signal strength is an oscillating or non-monotonic function of distance. Specifically, these peaks are singularities in the empirical probability density that occur at turning point values of the deterministic propagation model. We explain the underlying mechanism of this phenomenon through a concise mathematical argument. This observation has direct implications for estimating random propagation effects such as fading, particularly when reflections off walls are involved.
  Motivated by understanding intelligent surfaces, we apply this fundamental result to a physical model consisting of a single transmitter between two parallel passive walls. We analyze signal fading due to reflections and observe power oscillations resulting from wall reflections -- a phenomenon long studied in waveguides but relatively unexplored in wireless networks. For the special case where the transmitter is placed halfway between the walls, we present a compact closed-form expression for the received signal involving the Lerch transcendent function. The insights from this work can inform design decisions for intelligent surfaces deployed in cities.

</details>


### [242] [Anytime-Valid Quantum Tomography via Confidence Sequences](https://arxiv.org/abs/2601.20761)
*Aldo Cumitini,Luca Barletta,Osvaldo Simeone*

Main category: cs.IT

TL;DR: The paper proposes an anytime-valid quantum state tomography framework that provides confidence sets with guaranteed coverage probability during sequential measurements.


<details>
  <summary>Details</summary>
Motivation: Current quantum state tomography methods lack rigorous uncertainty quantification during sequential measurement processes, making it difficult to assess the reliability of state estimates as data is being collected incrementally.

Method: The framework augments existing QST techniques by combining point estimates with confidence sets based on recent statistical advances in anytime-valid confidence sequences, ensuring coverage guarantees at any time during measurement sequences.

Result: Numerical results confirm that the proposed anytime-valid QST maintains theoretical coverage properties, meaning the confidence sets contain the true quantum state with the user-defined probability throughout the measurement process.

Conclusion: The proposed framework provides rigorous uncertainty quantification for quantum state tomography during sequential measurements, addressing a critical gap in current QST methodology by ensuring valid confidence sets at any point in time.

Abstract: In this letter, we address the problem of developing quantum state tomography (QST) methods that remain valid at any time during a sequence of measurements. Specifically, the aim is to provide a rigorous quantification of the uncertainty associated with the current state estimate as data are acquired incrementally. To this end, the proposed framework augments existing QST techniques by associating current point estimates of the state with confidence sets that are guaranteed to contain the true quantum state with a user-defined probability. The methodology is grounded in recent statistical advances in anytime-valid confidence sequences. Numerical results confirm the theoretical coverage properties of the proposed anytime-valid QST.

</details>


### [243] [Repeater-Assisted Massive MIMO Full-Duplex Communications](https://arxiv.org/abs/2601.20822)
*Mohammadali Mohammadi,Dhanushka Kudathanthirige,Himal A. Suraweera,Hien Quoc Ngo,Michail Matthaiou*

Main category: cs.IT

TL;DR: Optimizing repeater weights in FD massive MIMO systems to maximize weighted minimum spectral efficiencies for UL/DL users, achieving 4x SE improvement over half-duplex.


<details>
  <summary>Details</summary>
Motivation: To improve spectral efficiency in wireless networks with FD massive MIMO base stations serving multiple UL/DL users simultaneously over same frequency band, using repeaters to amplify and retransmit signals.

Method: Formulate repeater weight optimization as non-convex problem maximizing sum of weighted minimum SEs for UL/DL UEs, solve using successive convex approximation technique.

Result: Optimized FD design achieves SE improvements up to 4-fold and 2.5-fold compared to half-duplex counterpart, outperforming benchmark systems with/without repeater assistance.

Conclusion: Proposed repeater weight optimization approach effectively enhances spectral efficiency in FD massive MIMO systems, demonstrating significant performance gains over conventional half-duplex and non-optimized repeater systems.

Abstract: We consider a wireless network comprising multiple singleantenna repeaters that amplify and instantaneously re-transmit received signals in a full-duplex (FD) communication setting. Specifically, we study a massive multiple-input multiple output base station that simultaneously serves multiple uplink (UL) and downlink (DL) user equipment (UE) over the same frequency band. The focus is on the problem of repeater weight optimization at each active repeater to maximize the sum of the weighted minimum spectral efficiencies (SEs) for both UL and DL UEs. The resulting non-convex optimization problem is tackled using a successive convex approximation technique. To demonstrate the effectiveness of the proposed approach, we evaluate its performance against benchmark systems with and without repeater assistance. The optimized FD design achieves SE improvements of up to 4-fold and 2.5-fold compared to its half-duplex counterpart.

</details>


### [244] [Construction and Decoding of Convolutional Codes with optimal Column Distances](https://arxiv.org/abs/2601.20825)
*Julia Lieb,Michael Schaller*

Main category: cs.IT

TL;DR: Construction of convolutional codes with optimal column distances over arbitrary finite fields, proving uniqueness and leveraging Reed-Muller structure for efficient Viterbi decoding.


<details>
  <summary>Details</summary>
Motivation: MDP convolutional codes require very large finite fields, while optimal column distance codes can work with arbitrary finite fields, making them more practical for real-world applications.

Method: Present a construction method for convolutional codes with optimal column distances, prove uniqueness for given parameters, and relate the structure to first order Reed-Muller block codes.

Result: Successfully constructed convolutional codes with optimal column distances over arbitrary finite fields, proved uniqueness of these constructions, and developed a reduced-complexity Viterbi algorithm leveraging the Reed-Muller structure.

Conclusion: The paper provides practical convolutional codes with optimal column distances that work over arbitrary finite fields, offers uniqueness proofs, and enables efficient decoding through structural connections to Reed-Muller codes.

Abstract: The construction of Maximum Distance Profile (MDP) convolutional codes in general requires the use of very large finite fields. In contrast convolutional codes with optimal column distances maximize the column distances for a given arbitrary finite field. In this paper, we present a construction of such convolutional codes. In addition, we prove that for the considered parameters the codes that we constructed are the only ones achieving optimal column distances. The structure of the presented convolutional codes with optimal column distances is strongly related to first order Reed-Muller block codes and we leverage this fact to develop a reduced complexity version of the Viterbi algorithm for these codes.

</details>


### [245] [Low-Complexity Pilot-Aided Doppler Ambiguity Estimation for OTFS Parametric Channel Estimation](https://arxiv.org/abs/2601.20827)
*Bo-Yuan Chen,Hsuan-Jung Su*

Main category: cs.IT

TL;DR: Proposes a low-complexity pilot-aided Doppler ambiguity detection and compensation framework for OTFS in high-mobility NTN scenarios where extreme LEO satellite velocities cause Doppler shifts exceeding grid range.


<details>
  <summary>Details</summary>
Motivation: In high-mobility environments like 5G Non-Terrestrial Networks (NTN), extreme orbital velocities of LEO satellites cause physical Doppler shifts to exceed the fundamental grid range, creating Doppler ambiguity that induces severe model mismatch and renders traditional MLE channel estimators ineffective.

Method: Develops a two-stage estimator: 1) mathematically derives OTFS input-output relationship with aliasing to reveal Doppler ambiguity manifests as phase rotation along delay dimension, 2) uses pairwise phase differences between pilot symbols to identify integer ambiguity, 3) performs refined Maximum Likelihood Estimation (MLE) for channel recovery. Investigates two pilot arrangements: Embedded Pilot with Guard Zone (EP-GZ) and Data-Surrounded Pilot (DSP).

Result: Simulation results show the proposed scheme effectively eliminates error floor caused by ambiguity, achieving BER and NMSE performance comparable to exhaustive search benchmark while maintaining computational complexity similar to standard MLE.

Conclusion: The proposed low-complexity framework successfully addresses Doppler ambiguity in high-mobility OTFS systems, enabling robust channel estimation in challenging NTN scenarios with extreme Doppler shifts.

Abstract: Orthogonal Time Frequency Space (OTFS) modulation offers robust performance in high-mobility scenarios by transforming time-varying channels into the delay-Doppler (DD) domain. However, in high-mobility environment such as emerging 5G Non-Terrestrial Networks (NTN), the extreme orbital velocities of Low Earth Orbit (LEO) satellites frequently cause the physical Doppler shifts to exceed the fundamental grid range. This Doppler ambiguity induces severe model mismatch and renders traditional MLE channel estimators ineffective. To address this challenge, this paper proposes a novel low-complexity pilot-aided Doppler ambiguity detection and compensation framework. We first mathematically derive the OTFS input-output relationship in the presence of aliasing, revealing that Doppler ambiguity manifests itself as a distinct phase rotation along the delay dimension. Leveraging this insight, we developed a two-stage estimator that utilizes pairwise phase differences between pilot symbols to identify the integer ambiguity, followed by a refined Maximum Likelihood Estimation (MLE) for channel recovery. We investigate two pilot arrangements, Embedded Pilot with Guard Zone (EP-GZ) and Data-Surrounded Pilot (DSP), to analyze the trade-off between interference suppression and spectral efficiency. Simulation results demonstrate that the proposed scheme effectively eliminates the error floor caused by ambiguity, achieving Bit Error Rate (BER) and Normalized Mean Square Error (NMSE) performance comparable to the exhaustive search benchmark while maintaining a computational complexity similar to standard MLE.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [246] [Orthogonal Plane-Wave Transmit-Receive Isotropic-Focusing Micro-Ultrasound (OPTIMUS) with Bias-Switchable Row-Column Arrays](https://arxiv.org/abs/2601.20066)
*Darren Dahunsi,Randy Palamar,Tyler Henry,Mohammad Rahim Sobhani,Negar Majidi,Joy Wang,Afshin Kashani Ilkhechi,Roger Zemp*

Main category: eess.IV

TL;DR: OPTIMUS is a novel ultrasound imaging scheme using TOBE arrays to achieve nearly isotropic focusing throughout a large volume, extending beyond previous methods like HERCULES.


<details>
  <summary>Details</summary>
Motivation: Current ultrasound technologies have limitations: matrix probes have limited fields of view and element counts, while row-column arrays provide insufficient focusing. There's a need for high-quality structural volumetric imaging with both good focusing and expansive field of view.

Method: Developed OPTIMUS (Orthogonal Plane-Wave Transmit-Receive Isotropic-Focusing Micro-Ultrasound) scheme using TOBE (Top-Orthogonal-to-Bottom-Electrode) arrays. Used simulations with scatterer grids to evaluate resolution across volume, validated experimentally with commercial calibration phantom, custom TOBE array, biasing electronics, and research ultrasound system. Performed ex-vivo imaging to assess structural tissue information.

Result: TOBE arrays enable isotropic focusing comparable to ideal matrix probes while providing field of view surpassing conventional RCAs. OPTIMUS achieves nearly isotropic focusing throughout an expansive volume and can image beyond the aperture shadow like typical 2D matrix probes.

Conclusion: OPTIMUS with TOBE arrays represents a significant advancement in ultrasound volumetric imaging, combining the focusing quality of matrix probes with the expansive field of view advantages of RCAs, enabling high-quality structural tissue imaging.

Abstract: High quality structural volumetric imaging is a challenging goal to achieve with modern ultrasound transducers. Matrix probes have limited fields of view and element counts, whereas row-column arrays (RCAs) provide insufficient focusing. In contrast, Top-Orthogonal-to-Bottom-Electrode (TOBE) arrays, also known as bias-switchable RCAs can enable isotropic focusing on par with ideal matrix probes, with a field of view surpassing conventional RCAs. Orthogonal Plane-Wave Transmit-Receive Isotropic-Focusing Micro-Ultrasound (OPTIMUS) is a novel imaging scheme that can use TOBE arrays to achieve nearly isotropic focusing throughout an expansive volume. This approach extends upon a similar volumetric imaging scheme, Hadamard Encoded Row Column Ultrasonic Expansive Scanning (HERCULES), that is even able to image beyond the shadow of the aperture, much like typical 2D matrix probes. We simulate a grid of scatterers to evaluate how the resolution varies across the volume, and validate these simulations experimentally using a commercial calibration phantom. Experimental measurements were done with a custom fabricated TOBE array, custom biasing electronics, and a research ultrasound system. Finally we performed ex-vivo imaging to assess our ability to discern structural tissue information.

</details>


### [247] [SegRap2025: A Benchmark of Gross Tumor Volume and Lymph Node Clinical Target Volume Segmentation for Radiotherapy Planning of Nasopharyngeal Carcinoma](https://arxiv.org/abs/2601.20575)
*Jia Fu,Litingyu Wang,He Li,Zihao Luo,Huamin Wang,Chenyuan Bian,Zijun Gao,Chunbin Gu,Xin Weng,Jianghao Wu,Yicheng Wu,Jin Ye,Linhao Li,Yiwen Ye,Yong Xia,Elias Tappeiner,Fei He,Abdul qayyum,Moona Mazher,Steven A Niederer,Junqiang Chen,Chuanyi Huang,Lisheng Wang,Zhaohu Xing,Hongqiu Wang,Lei Zhu,Shichuan Zhang,Shaoting Zhang,Wenjun Liao,Guotai Wang*

Main category: eess.IV

TL;DR: SegRap2025 challenge builds on previous work to create a multi-center, multi-modality benchmark for evaluating generalization and robustness in radiotherapy target segmentation for Nasopharyngeal Carcinoma, with top models achieving 74.61% DSC for GTV and 60.50% DSC for LN CTV segmentation.


<details>
  <summary>Details</summary>
Motivation: Accurate delineation of GTV, LN CTV, and OAR from CT scans is essential for precise radiotherapy planning in NPC. The challenge aims to enhance generalizability and robustness of segmentation models across imaging centers and modalities beyond the previous single-center focus of SegRap2023.

Method: SegRap2025 comprises two tasks: Task01 for GTV segmentation using paired CT from SegRap2023 dataset with external testing for cross-center generalization, and Task02 for LN CTV segmentation using multi-center training data with unseen external testing, emphasizing both cross-center and cross-modality robustness. The paper analyzes solutions from ten participating teams.

Result: For GTV segmentation, top models achieved average DSC of 74.61% on internal testing and 56.79% on external testing. For LN CTV segmentation, highest average DSC values were 60.24% on paired CT, 60.50% on ceCT-only, and 57.23% on ncCT-only subsets.

Conclusion: SegRap2025 establishes a large-scale multi-center, multi-modality benchmark for evaluating generalization and robustness in radiotherapy target segmentation, providing valuable insights toward clinically applicable automated radiotherapy planning systems.

Abstract: Accurate delineation of Gross Tumor Volume (GTV), Lymph Node Clinical Target Volume (LN CTV), and Organ-at-Risk (OAR) from Computed Tomography (CT) scans is essential for precise radiotherapy planning in Nasopharyngeal Carcinoma (NPC). Building upon SegRap2023, which focused on OAR and GTV segmentation using single-center paired non-contrast CT (ncCT) and contrast-enhanced CT (ceCT) scans, the SegRap2025 challenge aims to enhance the generalizability and robustness of segmentation models across imaging centers and modalities. SegRap2025 comprises two tasks: Task01 addresses GTV segmentation using paired CT from the SegRap2023 dataset, with an additional external testing set to evaluate cross-center generalization, and Task02 focuses on LN CTV segmentation using multi-center training data and an unseen external testing set, where each case contains paired CT scans or a single modality, emphasizing both cross-center and cross-modality robustness. This paper presents the challenge setup and provides a comprehensive analysis of the solutions submitted by ten participating teams. For GTV segmentation task, the top-performing models achieved average Dice Similarity Coefficient (DSC) of 74.61% and 56.79% on the internal and external testing cohorts, respectively. For LN CTV segmentation task, the highest average DSC values reached 60.24%, 60.50%, and 57.23% on paired CT, ceCT-only, and ncCT-only subsets, respectively. SegRap2025 establishes a large-scale multi-center, multi-modality benchmark for evaluating the generalization and robustness in radiotherapy target segmentation, providing valuable insights toward clinically applicable automated radiotherapy planning systems. The benchmark is available at: https://hilab-git.github.io/SegRap2025_Challenge.

</details>


### [248] [Task-Based Adaptive Transmit Beamforming for Efficient Ultrasound Quantification](https://arxiv.org/abs/2601.20711)
*OisÃ­n Nolan,Wessel L. van Nierop,Louis D. van Harten,Tristan S. W. Stevens,Ruud J. G. van Sloun*

Main category: eess.IV

TL;DR: Task-based adaptive transmit beamforming method reduces ultrasound scan lines by 98% while maintaining accurate ventricular dimension measurements, enabling low-power wearable ultrasound monitoring.


<details>
  <summary>Details</summary>
Motivation: Wireless wearable ultrasound devices face power consumption and data throughput challenges. Reducing transmit events per second is critical for practical continuous monitoring applications.

Method: Proposed Task-Based Information Gain (TBIG) strategy formulates adaptive transmit beamforming as Bayesian active perception problem. It selectively chooses where to scan based on information gain for downstream quantitative measurements, avoiding redundant transmit events. Works with any differentiable downstream task function.

Result: Applied to ventricular dimension recovery from echocardiograms, TBIG achieves accurate results using fewer than 2% of typical scan lines. This represents potential for large reductions in power usage and data rates for monitoring applications.

Conclusion: Task-based adaptive scanning enables dramatic reduction in ultrasound transmit events while maintaining measurement accuracy, making continuous wireless ultrasound monitoring more feasible through reduced power consumption and data throughput requirements.

Abstract: Wireless and wearable ultrasound devices promise to enable continuous ultrasound monitoring, but power consumption and data throughput remain critical challenges. Reducing the number of transmit events per second directly impacts both. We propose a task-based adaptive transmit beamforming method, formulated as a Bayesian active perception problem, that adaptively chooses where to scan in order to gain information about downstream quantitative measurements, avoiding redundant transmit events. Our proposed Task-Based Information Gain (TBIG) strategy applies to any differentiable downstream task function. When applied to recovering ventricular dimensions from echocardiograms, TBIG recovers accurate results using fewer than 2% of scan lines typically used, showing potential for large reductions in the power usage and data rates necessary for monitoring. Code is available at https://github.com/tue-bmd/task-based-ulsa.

</details>


### [249] [Leveraging Second-Order Curvature for Efficient Learned Image Compression: Theory and Empirical Evidence](https://arxiv.org/abs/2601.20769)
*Yichi Zhang,Fengqing Zhu*

Main category: eess.IV

TL;DR: SOAP, a second-order quasi-Newton optimizer, dramatically improves training efficiency and final performance for learned image compression models by resolving gradient conflicts in the rate-distortion trade-off.


<details>
  <summary>Details</summary>
Motivation: Standard first-order optimizers (SGD, Adam) struggle with gradient conflicts arising from competing rate and distortion objectives in learned image compression, leading to slow convergence and suboptimal performance.

Method: The paper proposes using a second-order quasi-Newton optimizer called SOAP, which employs Newton preconditioning to resolve intra-step and inter-step update conflicts in the rate-distortion objective.

Result: SOAP achieves faster, more stable convergence and better final rate-distortion performance across diverse LIC models. Additionally, second-order trained models exhibit significantly fewer activation and latent outliers, enhancing robustness to post-training quantization.

Conclusion: Second-order optimization serves as a powerful, practical drop-in replacement for standard optimizers, advancing both the efficiency and real-world deployability of learned image compression models.

Abstract: Training learned image compression (LIC) models entails navigating a challenging optimization landscape defined by the fundamental trade-off between rate and distortion. Standard first-order optimizers, such as SGD and Adam, struggle with \emph{gradient conflicts} arising from competing objectives, leading to slow convergence and suboptimal rate-distortion performance. In this work, we demonstrate that a simple utilization of a second-order quasi-Newton optimizer, \textbf{SOAP}, dramatically improves both training efficiency and final performance across diverse LICs. Our theoretical and empirical analyses reveal that Newton preconditioning inherently resolves the intra-step and inter-step update conflicts intrinsic to the R-D objective, facilitating faster, more stable convergence. Beyond acceleration, we uncover a critical deployability benefit: second-order trained models exhibit significantly fewer activation and latent outliers. This substantially enhances robustness to post-training quantization. Together, these results establish second-order optimization, achievable as a seamless drop-in replacement of the imported optimizer, as a powerful, practical tool for advancing the efficiency and real-world readiness of LICs.

</details>
