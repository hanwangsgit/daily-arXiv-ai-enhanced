<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 111]
- [eess.IV](#eess.IV) [Total: 4]
- [eess.SP](#eess.SP) [Total: 19]
- [cs.AI](#cs.AI) [Total: 37]
- [cs.IT](#cs.IT) [Total: 6]
- [cs.LG](#cs.LG) [Total: 88]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Detection of Cyberbullying in GIF using AI](https://arxiv.org/abs/2512.07838)
*Pal Dave,Xiaohong Yuan,Madhuri Siddula,Kaushik Roy*

Main category: cs.CV

TL;DR: This paper presents a deep learning approach using VGG16 to detect cyberbullying in GIFs/stickers, achieving 97% accuracy on a newly collected Twitter dataset of over 4100 GIFs.


<details>
  <summary>Details</summary>
Motivation: Cyberbullying is a growing social issue on social media platforms, but most existing research focuses on textual data or images, with very few studies addressing cyberbullying detection in GIFs/stickers, which are increasingly used for communication.

Method: The researchers collected a GIF dataset from Twitter by extracting cyberbullying-related hashtags and downloading GIFs using the GIPHY API. They then applied the pre-trained VGG16 deep learning model for cyberbullying detection on the collected dataset of over 4100 GIFs (both cyberbullying and non-cyberbullying).

Result: The VGG16 deep learning model achieved 97% accuracy in detecting cyberbullying from GIFs. The paper also provides a valuable GIF dataset for future research in this area.

Conclusion: This work successfully demonstrates the feasibility of using deep learning models to detect cyberbullying in GIFs/stickers, addressing a significant gap in existing research and providing a valuable dataset for the research community.

Abstract: Cyberbullying is a well-known social issue, and it is escalating day by day. Due to the vigorous development of the internet, social media provide many different ways for the user to express their opinions and exchange information. Cyberbullying occurs on social media using text messages, comments, sharing images and GIFs or stickers, and audio and video. Much research has been done to detect cyberbullying on textual data; some are available for images. Very few studies are available to detect cyberbullying on GIFs/stickers. We collect a GIF dataset from Twitter and Applied a deep learning model to detect cyberbullying from the dataset. Firstly, we extracted hashtags related to cyberbullying using Twitter. We used these hashtags to download GIF file using publicly available API GIPHY. We collected over 4100 GIFs including cyberbullying and non cyberbullying. we applied deep learning pre-trained model VGG16 for the detection of the cyberbullying. The deep learning model achieved the accuracy of 97%. Our work provides the GIF dataset for researchers working in this area.

</details>


### [2] [Near-real time fires detection using satellite imagery in Sudan conflict](https://arxiv.org/abs/2512.07925)
*Kuldip Singh Atwal,Dieter Pfoser,Daniel Rothbart*

Main category: cs.CV

TL;DR: Deep learning with Planet Labs 4-band satellite imagery enables near real-time monitoring of fire damage in Sudan's armed conflicts, showing minimal benefit from 8-band or time series data.


<details>
  <summary>Details</summary>
Motivation: The ongoing war in Sudan creates urgent need for rapid conflict monitoring. Satellite imagery combined with deep learning offers potential for near real-time analysis of conflict-related damage.

Method: Uses 4-band imagery from Planet Labs with a deep learning model to monitor fire damage in armed conflicts. Tested approach on five case studies in Sudan, comparing against baseline methods.

Result: Automated method captures active fires and charred areas more accurately than baseline. 8-band imagery or time series provide only marginal gains over 4-band data.

Conclusion: Fire damage in armed conflicts can be effectively monitored with minimal delay using 4-band satellite imagery and deep learning, offering practical solution for rapid conflict monitoring in Sudan.

Abstract: The challenges of ongoing war in Sudan highlight the need for rapid moni- toring and analysis of such conflicts. Advances in deep learning and readily available satellite remote sensing imagery allow for near real-time monitor- ing. This paper uses 4-band imagery from Planet Labs with a deep learning model to show that fire damage in armed conflicts can be monitored with minimal delay. We demonstrate the effectiveness of our approach using five case studies in Sudan. We show that, compared to a baseline, the automated method captures the active fires and charred areas more accurately. Our re- sults indicate that using 8-band imagery or time series of such imagery only result in marginal gains.

</details>


### [3] [Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality](https://arxiv.org/abs/2512.07951)
*Zekai Luo,Zongze Du,Zhouhang Zhu,Hao Zhong,Muzhi Zhu,Wen Wang,Yuling Xi,Chenchen Jing,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: LivingSwap is the first video reference-guided face swapping model that uses keyframes as conditioning signals to achieve high fidelity and temporal consistency in long video sequences.


<details>
  <summary>Details</summary>
Motivation: Current video face swapping methods struggle with maintaining high fidelity and temporal consistency over long, complex video sequences in film and entertainment production. The paper explores whether rich visual attributes from source videos can be leveraged to enhance both fidelity and temporal coherence, similar to reference-guided image editing approaches.

Method: LivingSwap employs keyframes as conditioning signals to inject target identity, enabling flexible and controllable editing. It combines keyframe conditioning with video reference guidance to perform temporal stitching, ensuring stable identity preservation and high-fidelity reconstruction. The authors also construct a paired face-swapping dataset called Face2Face and reverse the data pairs to ensure reliable ground-truth supervision for training.

Result: Extensive experiments demonstrate state-of-the-art results, with the method seamlessly integrating target identity with source video's expressions, lighting, and motion while significantly reducing manual effort in production workflows.

Conclusion: LivingSwap successfully addresses the challenge of achieving high fidelity and temporal consistency in video face swapping by introducing video reference guidance and keyframe conditioning, making it a practical solution for film and entertainment production workflows.

Abstract: Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap

</details>


### [4] [Restrictive Hierarchical Semantic Segmentation for Stratified Tooth Layer Detection](https://arxiv.org/abs/2512.07984)
*Ryan Banks,Camila Lindoni Azevedo,Hongying Tang,Yunpeng Li*

Main category: cs.CV

TL;DR: Hierarchical semantic segmentation framework for dental anatomy using recurrent level-wise prediction with feature conditioning and consistency rules, validated on panoramic radiographs.


<details>
  <summary>Details</summary>
Motivation: Existing hierarchy-aware segmentation methods only encode anatomical structure through loss functions, providing weak and indirect supervision. Accurate anatomical understanding is essential for reliable dental disease staging.

Method: Framework embeds explicit anatomical hierarchy through recurrent level-wise prediction with restrictive output heads and top-down feature conditioning. Uses Feature-wise Linear Modulation to condition child class features on parent probabilities, probabilistic composition rules for consistency, and hierarchical loss combining per-level Dice/cross-entropy with consistency terms.

Result: Hierarchical variants consistently increase IoU, Dice, and recall, especially for fine-grained anatomies, producing more anatomically coherent masks. However, they show increased recall over precision (more false positives). Validated on TL-pano dataset (194 panoramic radiographs) using UNet and HRNet with 5-fold cross-validation.

Conclusion: Explicit hierarchical structuring improves both performance and clinical plausibility, particularly in low-data dental imaging regimes, though with trade-off of increased false positives.

Abstract: Accurate understanding of anatomical structures is essential for reliably staging certain dental diseases. A way of introducing this within semantic segmentation models is by utilising hierarchy-aware methodologies. However, existing hierarchy-aware segmentation methods largely encode anatomical structure through the loss functions, providing weak and indirect supervision. We introduce a general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. At each depth of the class tree, the backbone is re-run on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, to modulate child feature spaces for fine grained detection. A probabilistic composition rule enforces consistency between parent and descendant classes. Hierarchical loss combines per-level class weighted Dice and cross entropy loss and a consistency term loss, ensuring parent predictions are the sum of their children. We validate our approach on our proposed dataset, TL-pano, containing 194 panoramic radiographs with dense instance and semantic segmentation annotations, of tooth layers and alveolar bone. Utilising UNet and HRNet as donor models across a 5-fold cross validation scheme, the hierarchical variants consistently increase IoU, Dice, and recall, particularly for fine-grained anatomies, and produce more anatomically coherent masks. However, hierarchical variants also demonstrated increased recall over precision, implying increased false positives. The results demonstrate that explicit hierarchical structuring improves both performance and clinical plausibility, especially in low data dental imaging regimes.

</details>


### [5] [FRIEDA: Benchmarking Multi-Step Cartographic Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.08016)
*Jiyoon Pyo,Yuankun Jiao,Dongwon Jung,Zekun Li,Leeje Jang,Sofia Kirsanova,Jina Kim,Yijun Lin,Qin Liu,Junyi Xie,Hadi Askari,Nan Xu,Muhao Chen,Yao-Yi Chiang*

Main category: cs.CV

TL;DR: FRIEDA is a new benchmark for evaluating complex cartographic reasoning in large vision-language models, focusing on spatial relations across multiple maps, where current SOTA models perform poorly (38% accuracy) compared to humans (85%).


<details>
  <summary>Details</summary>
Motivation: Current map VQA benchmarks treat maps as simple charts, but real cartographic reasoning requires understanding layered symbology, spatial relations (topological, metric, directional), and cross-map reasoning that existing evaluations don't capture.

Method: Created FRIEDA benchmark using real map images from various domains and geographical areas, covering all three GIS spatial relation categories. Questions require multi-step inference and often cross-map grounding. Evaluated 11 LVLMs in two settings: direct (given relevant maps) and contextual (must identify relevant maps first).

Result: Even best models (Gemini-2.5-Pro and GPT-5-Think) achieved only 38.20% and 37.20% accuracy respectively, far below human performance of 84.87%. Shows significant gap in multi-step cartographic reasoning capabilities.

Conclusion: FRIEDA reveals persistent deficiencies in LVLMs' spatial intelligence and cartographic reasoning, positioning it as a rigorous benchmark to drive progress in this critical cognitive capability.

Abstract: Cartographic reasoning is the skill of interpreting geographic relationships by aligning legends, map scales, compass directions, map texts, and geometries across one or more map images. Although essential as a concrete cognitive capability and for critical tasks such as disaster response and urban planning, it remains largely unevaluated. Building on progress in chart and infographic understanding, recent large vision language model studies on map visual question-answering often treat maps as a special case of charts. In contrast, map VQA demands comprehension of layered symbology (e.g., symbols, geometries, and text labels) as well as spatial relations tied to orientation and distance that often span multiple maps and are not captured by chart-style evaluations. To address this gap, we introduce FRIEDA, a benchmark for testing complex open-ended cartographic reasoning in LVLMs. FRIEDA sources real map images from documents and reports in various domains and geographical areas. Following classifications in Geographic Information System (GIS) literature, FRIEDA targets all three categories of spatial relations: topological (border, equal, intersect, within), metric (distance), and directional (orientation). All questions require multi-step inference, and many require cross-map grounding and reasoning. We evaluate eleven state-of-the-art LVLMs under two settings: (1) the direct setting, where we provide the maps relevant to the question, and (2) the contextual setting, where the model may have to identify the maps relevant to the question before reasoning. Even the strongest models, Gemini-2.5-Pro and GPT-5-Think, achieve only 38.20% and 37.20% accuracy, respectively, far below human performance of 84.87%. These results reveal a persistent gap in multi-step cartographic reasoning, positioning FRIEDA as a rigorous benchmark to drive progress on spatial intelligence in LVLMs.

</details>


### [6] [SSplain: Sparse and Smooth Explainer for Retinopathy of Prematurity Classification](https://arxiv.org/abs/2512.08038)
*Elifnur Sunger,Tales Imbiriba,Peter Campbell,Deniz Erdogmus,Stratis Ioannidis,Jennifer Dy*

Main category: cs.CV

TL;DR: SSplain is a new explainer method for ROP classification that generates pixel-wise explanations while preserving image smoothness and sparsity, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing explainer methods for medical image classification fail to preserve important image structures like smoothness and sparsity, limiting their usefulness for clinicians who need realistic explanations to understand and trust black-box neural network outputs.

Method: SSplain formulates an optimization problem with combinatorial constraints to generate pixel-wise explanations that enforce smoothness and sparsity, solved using the Alternating Direction Method of Multipliers (ADMM).

Result: SSplain outperforms commonly used explainers in post-hoc accuracy and smoothness analyses, identifies clinically relevant features for ROP, and generalizes well to additional public datasets.

Conclusion: SSplain provides more realistic and clinically meaningful explanations for medical image classification by preserving image structures, enhancing clinician understanding and trust in black-box models.

Abstract: Neural networks are frequently used in medical diagnosis. However, due to their black-box nature, model explainers are used to help clinicians understand better and trust model outputs. This paper introduces an explainer method for classifying Retinopathy of Prematurity (ROP) from fundus images. Previous methods fail to generate explanations that preserve input image structures such as smoothness and sparsity. We introduce Sparse and Smooth Explainer (SSplain), a method that generates pixel-wise explanations while preserving image structures by enforcing smoothness and sparsity. This results in realistic explanations to enhance the understanding of the given black-box model. To achieve this goal, we define an optimization problem with combinatorial constraints and solve it using the Alternating Direction Method of Multipliers (ADMM). Experimental results show that SSplain outperforms commonly used explainers in terms of both post-hoc accuracy and smoothness analyses. Additionally, SSplain identifies features that are consistent with domain-understandable features that clinicians consider as discriminative factors for ROP. We also show SSplain's generalization by applying it to additional publicly available datasets. Code is available at https://github.com/neu-spiral/SSplain.

</details>


### [7] [Lost in Translation, Found in Embeddings: Sign Language Translation and Alignment](https://arxiv.org/abs/2512.08040)
*Youngjoon Jang,Liliane Momeni,Zifan Jiang,Joon Son Chung,GÃ¼l Varol,Andrew Zisserman*

Main category: cs.CV

TL;DR: A unified model for sign language understanding that performs both sign language translation (SLT) and sign-subtitle alignment (SSA) using privacy-preserving visual features, sliding perceiver mapping, and multi-task training, achieving SOTA results on BSL and strong performance on ASL.


<details>
  <summary>Details</summary>
Motivation: To develop a practical unified model for sign language understanding that enables conversion of signing videos to spoken language text and temporal alignment with subtitles, which are essential for communication, corpus construction, and educational applications.

Method: Three-component approach: (1) lightweight visual backbone using human keypoints and lip-region images for privacy preservation, (2) Sliding Perceiver mapping network to aggregate visual features into word-level embeddings, (3) multi-task scalable training jointly optimizing SLT and SSA.

Result: Achieves state-of-the-art results on BOBSL (BSL) dataset for both SLT and SSA, demonstrates robust zero-shot generalization and fine-tuned performance on How2Sign (ASL), showing potential for scalable cross-linguistic translation.

Conclusion: The unified model successfully addresses both sign language translation and alignment tasks with strong cross-linguistic generalization, highlighting the effectiveness of multilingual pretraining and the proposed architecture for practical sign language applications.

Abstract: Our aim is to develop a unified model for sign language understanding, that performs sign language translation (SLT) and sign-subtitle alignment (SSA). Together, these two tasks enable the conversion of continuous signing videos into spoken language text and also the temporal alignment of signing with subtitles -- both essential for practical communication, large-scale corpus construction, and educational applications. To achieve this, our approach is built upon three components: (i) a lightweight visual backbone that captures manual and non-manual cues from human keypoints and lip-region images while preserving signer privacy; (ii) a Sliding Perceiver mapping network that aggregates consecutive visual features into word-level embeddings to bridge the vision-text gap; and (iii) a multi-task scalable training strategy that jointly optimises SLT and SSA, reinforcing both linguistic and temporal alignment. To promote cross-linguistic generalisation, we pretrain our model on large-scale sign-text corpora covering British Sign Language (BSL) and American Sign Language (ASL) from the BOBSL and YouTube-SL-25 datasets. With this multilingual pretraining and strong model design, we achieve state-of-the-art results on the challenging BOBSL (BSL) dataset for both SLT and SSA. Our model also demonstrates robust zero-shot generalisation and finetuned SLT performance on How2Sign (ASL), highlighting the potential of scalable translation across different sign languages.

</details>


### [8] [Towards Sustainable Universal Deepfake Detection with Frequency-Domain Masking](https://arxiv.org/abs/2512.08042)
*Chandler Timm C. Doloriel,Habib Ullah,Kristian Hovde Liland,Fadi Al Machot,Ngai-Man Cheung*

Main category: cs.CV

TL;DR: Frequency-domain masking improves universal deepfake detection with better generalization to unseen generators and maintains performance under model pruning for sustainable AI.


<details>
  <summary>Details</summary>
Motivation: Universal deepfake detection needs robust generalization to new/unseen generative models while minimizing computational overhead for large-scale screening in the Green AI era.

Method: Introduces frequency-domain masking as a training strategy, using random masking and geometric transformations, with focus on frequency masking for superior generalization properties.

Result: Achieves state-of-the-art generalization on GAN- and diffusion-generated datasets, maintains performance under significant model pruning, and offers scalable resource-conscious solution.

Conclusion: Frequency-based masking is a practical step toward sustainable and generalizable deepfake detection, balancing performance with computational efficiency.

Abstract: Universal deepfake detection aims to identify AI-generated images across a broad range of generative models, including unseen ones. This requires robust generalization to new and unseen deepfakes, which emerge frequently, while minimizing computational overhead to enable large-scale deepfake screening, a critical objective in the era of Green AI. In this work, we explore frequency-domain masking as a training strategy for deepfake detectors. Unlike traditional methods that rely heavily on spatial features or large-scale pretrained models, our approach introduces random masking and geometric transformations, with a focus on frequency masking due to its superior generalization properties. We demonstrate that frequency masking not only enhances detection accuracy across diverse generators but also maintains performance under significant model pruning, offering a scalable and resource-conscious solution. Our method achieves state-of-the-art generalization on GAN- and diffusion-generated image datasets and exhibits consistent robustness under structured pruning. These results highlight the potential of frequency-based masking as a practical step toward sustainable and generalizable deepfake detection. Code and models are available at: [https://github.com/chandlerbing65nm/FakeImageDetection](https://github.com/chandlerbing65nm/FakeImageDetection).

</details>


### [9] [Mask to Adapt: Simple Random Masking Enables Robust Continual Test-Time Learning](https://arxiv.org/abs/2512.08048)
*Chandler Timm C. Doloriel*

Main category: cs.CV

TL;DR: M2A is a simple continual test-time adaptation method using random masking with consistency and entropy losses, achieving strong performance on corrupted datasets without complex uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: Existing CTTA methods rely on calibrated uncertainty or stable attention scores with complex masking designs. The authors question whether custom masking is necessary or if simple random masking can work under strong corruption.

Method: M2A generates random masked views (spatial or frequency) and adapts using two objectives: mask consistency loss aligning predictions across different views, and entropy minimization loss encouraging confident outputs. Explores spatial (patch/pixel) and frequency (all/low/high) masking types.

Result: On CIFAR10C/CIFAR100C/ImageNetC (severity 5), M2A (Spatial) achieves 8.3%/19.8%/39.2% mean error, outperforming or matching strong CTTA baselines. M2A (Frequency) performs worse. Ablations show simple random masking is effective and robust.

Conclusion: Simple random masking with consistency and entropy objectives is sufficient for effective test-time adaptation without relying on uncertainty or attention signals, challenging the need for complex custom masking designs.

Abstract: Distribution shifts at test time degrade image classifiers. Recent continual test-time adaptation (CTTA) methods use masking to regulate learning, but often depend on calibrated uncertainty or stable attention scores and introduce added complexity. We ask: do we need custom-made masking designs, or can a simple random masking schedule suffice under strong corruption? We introduce Mask to Adapt (M2A), a simple CTTA approach that generates a short sequence of masked views (spatial or frequency) and adapts with two objectives: a mask consistency loss that aligns predictions across different views and an entropy minimization loss that encourages confident outputs. Motivated by masked image modeling, we study two common masking families -- spatial masking and frequency masking -- and further compare subtypes within each (spatial: patch vs.\ pixel; frequency: all vs.\ low vs.\ high). On CIFAR10C/CIFAR100C/ImageNetC (severity~5), M2A (Spatial) attains 8.3\%/19.8\%/39.2\% mean error, outperforming or matching strong CTTA baselines, while M2A (Frequency) lags behind. Ablations further show that simple random masking is effective and robust. These results indicate that a simple random masking schedule, coupled with consistency and entropy objectives, is sufficient to drive effective test-time adaptation without relying on uncertainty or attention signals.

</details>


### [10] [Identification of Deforestation Areas in the Amazon Rainforest Using Change Detection Models](https://arxiv.org/abs/2512.08075)
*Christian Massao Konishi,Helio Pedrini*

Main category: cs.CV

TL;DR: This paper evaluates various change detection models for deforestation monitoring using PRODES data, addressing limitations of existing approaches through standardized evaluation, modern architectures, and preprocessing techniques to achieve improved performance.


<details>
  <summary>Details</summary>
Motivation: The preservation of the Amazon Rainforest is crucial for climate change, biodiversity, and indigenous cultures. Existing machine learning approaches for deforestation detection using PRODES data have limitations: unsatisfactory effectiveness, lack of modern architectures (like self-attention mechanisms), and absence of methodological standardization for direct comparisons between studies.

Method: The authors evaluate various change detection models on a unified dataset, including fully convolutional models and Transformer-based self-attention networks. They investigate different pre- and post-processing techniques such as filtering predicted deforested areas by connected component size, texture replacement, and image enhancements. They also test different model combination strategies.

Result: The preprocessing and postprocessing techniques significantly improve individual model effectiveness. Through model combination strategies, they achieve an F1-score of 80.41%, which is comparable to other recent works in the literature.

Conclusion: The work addresses key gaps in deforestation detection by providing standardized evaluation of modern architectures, demonstrating that preprocessing techniques and model combinations can substantially improve performance, contributing to more effective satellite-based monitoring of Amazon deforestation.

Abstract: The preservation of the Amazon Rainforest is one of the global priorities in combating climate change, protecting biodiversity, and safeguarding indigenous cultures. The Satellite-based Monitoring Project of Deforestation in the Brazilian Legal Amazon (PRODES), a project of the National Institute for Space Research (INPE), stands out as a fundamental initiative in this effort, annually monitoring deforested areas not only in the Amazon but also in other Brazilian biomes. Recently, machine learning models have been developed using PRODES data to support this effort through the comparative analysis of multitemporal satellite images, treating deforestation detection as a change detection problem. However, existing approaches present significant limitations: models evaluated in the literature still show unsatisfactory effectiveness, many do not incorporate modern architectures, such as those based on self-attention mechanisms, and there is a lack of methodological standardization that allows direct comparisons between different studies. In this work, we address these gaps by evaluating various change detection models in a unified dataset, including fully convolutional models and networks incorporating self-attention mechanisms based on Transformers. We investigate the impact of different pre- and post-processing techniques, such as filtering deforested areas predicted by the models based on the size of connected components, texture replacement, and image enhancements; we demonstrate that such approaches can significantly improve individual model effectiveness. Additionally, we test different strategies for combining the evaluated models to achieve results superior to those obtained individually, reaching an F1-score of 80.41%, a value comparable to other recent works in the literature.

</details>


### [11] [CVP: Central-Peripheral Vision-Inspired Multimodal Model for Spatial Reasoning](https://arxiv.org/abs/2512.08135)
*Zeyuan Chen,Xiang Zhang,Haiyang Xu,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: CVP is a multimodal model for spatial reasoning inspired by human central and peripheral vision, using target-affinity tokens and allocentric grids to achieve state-of-the-art 3D scene understanding.


<details>
  <summary>Details</summary>
Motivation: Existing approaches rely on unstructured representations (point clouds, voxels, patch features) with implicit scene context injection via coordinate embeddings, resulting in limited spatial reasoning capabilities due to lack of explicit, high-level structural understanding.

Method: Introduces a central-peripheral vision-inspired framework with two complementary components: 1) target-affinity token (analogous to central vision) that guides attention toward query-relevant objects, and 2) allocentric grid (akin to peripheral vision) that captures global scene context and spatial arrangements. These are integrated into a Large Multimodal Model-based architecture.

Result: CVP achieves state-of-the-art performance across a range of 3D scene understanding benchmarks, demonstrating superior spatial reasoning capabilities.

Conclusion: The biologically-inspired central-peripheral vision framework provides an effective approach for structured, context-aware understanding of complex 3D environments, addressing limitations of existing unstructured representations.

Abstract: We present a central-peripheral vision-inspired framework (CVP), a simple yet effective multimodal model for spatial reasoning that draws inspiration from the two types of human visual fields -- central vision and peripheral vision. Existing approaches primarily rely on unstructured representations, such as point clouds, voxels, or patch features, and inject scene context implicitly via coordinate embeddings. However, this often results in limited spatial reasoning capabilities due to the lack of explicit, high-level structural understanding. To address this limitation, we introduce two complementary components into a Large Multimodal Model-based architecture: target-affinity token, analogous to central vision, that guides the model's attention toward query-relevant objects; and allocentric grid, akin to peripheral vision, that captures global scene context and spatial arrangements. These components work in tandem to enable structured, context-aware understanding of complex 3D environments. Experiments show that CVP achieves state-of-the-art performance across a range of 3D scene understanding benchmarks.

</details>


### [12] [Fourier-RWKV: A Multi-State Perception Network for Efficient Image Dehazing](https://arxiv.org/abs/2512.08161)
*Lirong Zheng,Yanshan Li,Rui Yu,Kaihao Zhang*

Main category: cs.CV

TL;DR: Fourier-RWKV: A linear-complexity Transformer-based dehazing framework using Multi-State Perception for efficient non-uniform haze removal.


<details>
  <summary>Details</summary>
Motivation: Image dehazing is crucial for visual perception but challenging under real-world non-uniform haze conditions. Existing Transformer methods have quadratic complexity that hinders real-time deployment, creating a need for efficient dehazing solutions.

Method: Proposes Fourier-RWKV with Multi-State Perception: (1) Spatial-form Perception via Deformable Quad-directional Token Shift for local haze variations; (2) Frequency-domain Perception via Fourier Mix block extending RWKV's WKV attention to Fourier domain for global dependencies; (3) Semantic-relation Perception via Semantic Bridge Module with Dynamic Semantic Kernel Fusion for encoder-decoder alignment.

Result: Extensive experiments show state-of-the-art performance across diverse haze benchmarks while significantly reducing computational overhead, achieving favorable trade-off between restoration quality and efficiency.

Conclusion: Fourier-RWKV successfully addresses the efficiency limitations of Transformer-based dehazing methods through linear-complexity design with comprehensive haze modeling, enabling practical real-world deployment without sacrificing performance.

Abstract: Image dehazing is crucial for reliable visual perception, yet it remains highly challenging under real-world non-uniform haze conditions. Although Transformer-based methods excel at capturing global context, their quadratic computational complexity hinders real-time deployment. To address this, we propose Fourier Receptance Weighted Key Value (Fourier-RWKV), a novel dehazing framework based on a Multi-State Perception paradigm. The model achieves comprehensive haze degradation modeling with linear complexity by synergistically integrating three distinct perceptual states: (1) Spatial-form Perception, realized through the Deformable Quad-directional Token Shift (DQ-Shift) operation, which dynamically adjusts receptive fields to accommodate local haze variations; (2) Frequency-domain Perception, implemented within the Fourier Mix block, which extends the core WKV attention mechanism of RWKV from the spatial domain to the Fourier domain, preserving the long-range dependencies essential for global haze estimation while mitigating spatial attenuation; (3) Semantic-relation Perception, facilitated by the Semantic Bridge Module (SBM), which utilizes Dynamic Semantic Kernel Fusion (DSK-Fusion) to precisely align encoder-decoder features and suppress artifacts. Extensive experiments on multiple benchmarks demonstrate that Fourier-RWKV delivers state-of-the-art performance across diverse haze scenarios while significantly reducing computational overhead, establishing a favorable trade-off between restoration quality and practical efficiency. Code is available at: https://github.com/Dilizlr/Fourier-RWKV.

</details>


### [13] [Accuracy Does Not Guarantee Human-Likeness in Monocular Depth Estimators](https://arxiv.org/abs/2512.08163)
*Yuki Kubota,Taiki Fukiage*

Main category: cs.CV

TL;DR: This paper investigates the relationship between model accuracy and human similarity in monocular depth estimation, finding that improving accuracy doesn't necessarily lead to more human-like behavior.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand whether there's a trade-off between model accuracy and human-like behavior in monocular depth estimation, similar to what's observed in object recognition. Current benchmarks rely on sensor-based ground truth rather than human perceptual estimates, creating a gap in understanding how well models align with human perception.

Method: The researchers systematically investigated 69 monocular depth estimators using the KITTI dataset. They applied affine fitting to decompose prediction errors into interpretable components on a factor-by-factor basis, allowing them to dissect error patterns and analyze the relationship between model accuracy and human similarity.

Result: The results show that while humans and DNNs share certain estimation biases (positive error correlations), there are distinct trade-off relationships between model accuracy and human similarity. Improving accuracy does not necessarily lead to more human-like behavior in depth estimation.

Conclusion: The study concludes that developing multifaceted, human-centric evaluations beyond traditional accuracy metrics is necessary for monocular depth estimation. The findings indicate that accuracy improvements don't automatically translate to more human-like behavior, highlighting the need for new evaluation approaches that consider perceptual alignment.

Abstract: Monocular depth estimation is a fundamental capability for real-world applications such as autonomous driving and robotics. Although deep neural networks (DNNs) have achieved superhuman accuracy on physical-based benchmarks, a key challenge remains: aligning model representations with human perception, a promising strategy for enhancing model robustness and interpretability. Research in object recognition has revealed a complex trade-off between model accuracy and human-like behavior, raising a question whether a similar divergence exist in depth estimation, particularly for natural outdoor scenes where benchmarks rely on sensor-based ground truth rather than human perceptual estimates. In this study, we systematically investigated the relationship between model accuracy and human similarity across 69 monocular depth estimators using the KITTI dataset. To dissect the structure of error patterns on a factor-by-factor basis, we applied affine fitting to decompose prediction errors into interpretable components. Intriguingly, our results reveal while humans and DNNs share certain estimation biases (positive error correlations), we observed distinct trade-off relationships between model accuracy and human similarity. This finding indicates that improving accuracy does not necessarily lead to more human-like behavior, underscoring the necessity of developing multifaceted, human-centric evaluations beyond traditional accuracy.

</details>


### [14] [GeoLoom: High-quality Geometric Diagram Generation from Textual Input](https://arxiv.org/abs/2512.08180)
*Xiaojing Wei,Ting Zhang,Wei He,Jingdong Wang,Hua Huang*

Main category: cs.CV

TL;DR: GeoLoom is a text-to-geometric-diagram generation framework that uses formal language translation and Monte Carlo optimization for precise spatial accuracy.


<details>
  <summary>Details</summary>
Motivation: High-quality geometric diagram generation requires strict spatial accuracy while offering well-defined constraints, presenting both challenge and opportunity for guided generation.

Method: Two core components: 1) autoformalization module translating natural language to GeoLingua formal language, 2) coordinate solver using Monte Carlo optimization to map formal constraints to precise coordinates. Also introduces GeoNF dataset and constraint-based evaluation metric.

Result: GeoLoom significantly outperforms state-of-the-art baselines in structural fidelity, providing interpretable and scalable diagram generation.

Conclusion: The framework offers a principled foundation for interpretable and scalable geometric diagram generation with mathematically grounded supervision for iterative refinement.

Abstract: High-quality geometric diagram generation presents both a challenge and an opportunity: it demands strict spatial accuracy while offering well-defined constraints to guide generation. Inspired by recent advances in geometry problem solving that employ formal languages and symbolic solvers for enhanced correctness and interpretability, we propose GeoLoom, a novel framework for text-to-diagram generation in geometric domains. GeoLoom comprises two core components: an autoformalization module that translates natural language into a specifically designed generation-oriented formal language GeoLingua, and a coordinate solver that maps formal constraints to precise coordinates using the efficient Monte Carlo optimization. To support this framework, we introduce GeoNF, a dataset aligning natural language geometric descriptions with formal GeoLingua descriptions. We further propose a constraint-based evaluation metric that quantifies structural deviation, offering mathematically grounded supervision for iterative refinement. Empirical results demonstrate that GeoLoom significantly outperforms state-of-the-art baselines in structural fidelity, providing a principled foundation for interpretable and scalable diagram generation.

</details>


### [15] [Animal Re-Identification on Microcontrollers](https://arxiv.org/abs/2512.08198)
*Yubo Chen,Di Zhao,Yun Sing Koh,Talia Xu*

Main category: cs.CV

TL;DR: Proposes an on-device framework for animal re-identification that works on low-power microcontrollers, achieving competitive accuracy with 100x smaller models and adaptation with just 3 images per animal.


<details>
  <summary>Details</summary>
Motivation: Animal Re-ID is needed for wildlife monitoring and livestock management in remote areas with limited connectivity, but existing models are too large for microcontroller-based edge devices with memory and resolution constraints.

Method: 1) Analyzes gap between SOTA models and MCU hardware; 2) Designs compact CNN architecture by scaling MobileNetV2 for low-resolution inputs; 3) Introduces data-efficient fine-tuning strategy requiring only 3 images per animal identity for site adaptation.

Result: Model reduces size by over 100x while maintaining competitive accuracy on 6 public datasets. On cattle dataset, achieves fully on-device inference with minimal accuracy drop and unchanged Top-1 accuracy compared to cluster version.

Conclusion: Practical, adaptable Animal Re-ID is achievable on MCU-class devices, enabling scalable deployment in real field environments for wildlife monitoring and livestock management.

Abstract: Camera-based animal re-identification (Animal Re-ID) can support wildlife monitoring and precision livestock management in large outdoor environments with limited wireless connectivity. In these settings, inference must run directly on collar tags or low-power edge nodes built around microcontrollers (MCUs), yet most Animal Re-ID models are designed for workstations or servers and are too large for devices with small memory and low-resolution inputs. We propose an on-device framework. First, we characterise the gap between state-of-the-art Animal Re-ID models and MCU-class hardware, showing that straightforward knowledge distillation from large teachers offers limited benefit once memory and input resolution are constrained. Second, guided by this analysis, we design a high-accuracy Animal Re-ID architecture by systematically scaling a CNN-based MobileNetV2 backbone for low-resolution inputs. Third, we evaluate the framework with a real-world dataset and introduce a data-efficient fine-tuning strategy to enable fast adaptation with just three images per animal identity at a new site. Across six public Animal Re-ID datasets, our compact model achieves competitive retrieval accuracy while reducing model size by over two orders of magnitude. On a self-collected cattle dataset, the deployed model performs fully on-device inference with only a small accuracy drop and unchanged Top-1 accuracy relative to its cluster version. We demonstrate that practical, adaptable Animal Re-ID is achievable on MCU-class devices, paving the way for scalable deployment in real field environments.

</details>


### [16] [Blur2Sharp: Human Novel Pose and View Synthesis with Generative Prior Refinement](https://arxiv.org/abs/2512.08215)
*Chia-Hern Lai,I-Hsuan Lo,Yen-Ku Yeh,Thanh-Nguyen Truong,Ching-Chun Huang*

Main category: cs.CV

TL;DR: Blur2Sharp generates sharp, geometrically consistent novel-view human avatars from single reference images using 3D-aware neural rendering and diffusion models.


<details>
  <summary>Details</summary>
Motivation: Current human avatar generation methods produce either geometrically inconsistent multi-view images or blurry outputs under diverse viewing angles and complex motions, lacking both photorealism and geometric consistency.

Method: Dual-conditioning framework: 1) Human NeRF generates geometrically coherent multi-view renderings for target poses with 3D structural guidance, 2) Diffusion model refines these renderings while preserving details, 3) Hierarchical feature fusion incorporates texture, normal, and semantic priors from SMPL models.

Result: Blur2Sharp consistently surpasses state-of-the-art techniques in novel pose and view generation tasks, particularly excelling under challenging scenarios involving loose clothing and occlusions.

Conclusion: The proposed framework successfully addresses the trade-off between geometric consistency and photorealism in human avatar generation, enabling sharp, geometrically consistent novel-view images from single reference views.

Abstract: The creation of lifelike human avatars capable of realistic pose variation and viewpoint flexibility remains a fundamental challenge in computer vision and graphics. Current approaches typically yield either geometrically inconsistent multi-view images or sacrifice photorealism, resulting in blurry outputs under diverse viewing angles and complex motions. To address these issues, we propose Blur2Sharp, a novel framework integrating 3D-aware neural rendering and diffusion models to generate sharp, geometrically consistent novel-view images from only a single reference view. Our method employs a dual-conditioning architecture: initially, a Human NeRF model generates geometrically coherent multi-view renderings for target poses, explicitly encoding 3D structural guidance. Subsequently, a diffusion model conditioned on these renderings refines the generated images, preserving fine-grained details and structural fidelity. We further enhance visual quality through hierarchical feature fusion, incorporating texture, normal, and semantic priors extracted from parametric SMPL models to simultaneously improve global coherence and local detail accuracy. Extensive experiments demonstrate that Blur2Sharp consistently surpasses state-of-the-art techniques in both novel pose and view generation tasks, particularly excelling under challenging scenarios involving loose clothing and occlusions.

</details>


### [17] [VisKnow: Constructing Visual Knowledge Base for Object Understanding](https://arxiv.org/abs/2512.08221)
*Ziwei Yao,Qiyang Wan,Ruiping Wang,Xilin Chen*

Main category: cs.CV

TL;DR: Proposes Visual Knowledge Base (VisKnow) framework to structure multi-modal object knowledge as graphs, with AnimalKB as case study covering 406 animal categories with 22K textual triplets and 420K images.


<details>
  <summary>Details</summary>
Motivation: Current object understanding in computer vision is limited to simple category labels, lacking comprehensive perception of object components, attributes, relationships, and contextual knowledge. Existing multi-modal data is task-oriented and not systematically organized for holistic object understanding.

Method: VisKnow framework extracts multi-modal, object-level knowledge by integrating enriched aligned text and image-source knowledge with region annotations at object and part levels, combining expert design with large-scale model application.

Result: Built AnimalKB covering 406 animal categories with 22K textual knowledge triplets from encyclopedic documents, 420K images, and corresponding region annotations. Experiments show enhanced performance on zero-shot recognition, fine-grained VQA, and serves as benchmark for knowledge graph completion and part segmentation.

Conclusion: Visual knowledge bases like AnimalKB demonstrate potential to advance visual understanding and practical applications by providing structured multi-modal knowledge, enabling better object-level tasks and serving as challenging benchmarks for various computer vision tasks.

Abstract: Understanding objects is fundamental to computer vision. Beyond object recognition that provides only a category label as typical output, in-depth object understanding represents a comprehensive perception of an object category, involving its components, appearance characteristics, inter-category relationships, contextual background knowledge, etc. Developing such capability requires sufficient multi-modal data, including visual annotations such as parts, attributes, and co-occurrences for specific tasks, as well as textual knowledge to support high-level tasks like reasoning and question answering. However, these data are generally task-oriented and not systematically organized enough to achieve the expected understanding of object categories. In response, we propose the Visual Knowledge Base that structures multi-modal object knowledge as graphs, and present a construction framework named VisKnow that extracts multi-modal, object-level knowledge for object understanding. This framework integrates enriched aligned text and image-source knowledge with region annotations at both object and part levels through a combination of expert design and large-scale model application. As a specific case study, we construct AnimalKB, a structured animal knowledge base covering 406 animal categories, which contains 22K textual knowledge triplets extracted from encyclopedic documents, 420K images, and corresponding region annotations. A series of experiments showcase how AnimalKB enhances object-level visual tasks such as zero-shot recognition and fine-grained VQA, and serves as challenging benchmarks for knowledge graph completion and part segmentation. Our findings highlight the potential of automatically constructing visual knowledge bases to advance visual understanding and its practical applications. The project page is available at https://vipl-vsu.github.io/VisKnow.

</details>


### [18] [SOP^2: Transfer Learning with Scene-Oriented Prompt Pool on 3D Object Detection](https://arxiv.org/abs/2512.08223)
*Ching-Hung Cheng,Hsiu-Fu Wu,Bing-Chen Wu,Khanh-Phong Bui,Van-Tin Luu,Ching-Chun Huang*

Main category: cs.CV

TL;DR: This paper explores prompt tuning methods for 3D object detection, investigating whether models trained on Waymo dataset can serve as foundation models and adapt to other scenarios, proposing a Scene-Oriented Prompt Pool (SOPÂ²).


<details>
  <summary>Details</summary>
Motivation: To investigate the effectiveness of prompt tuning methods in 3D object detection, inspired by the success of prompt techniques in NLP with LLMs. The goal is to explore if models trained on large-scale datasets can serve as foundation models for adaptation to various 3D scenarios.

Method: The paper sequentially examines the impact of prompt tokens and prompt generators, then proposes a Scene-Oriented Prompt Pool (SOPÂ²) for 3D object detection adaptation. The approach investigates transfer learning techniques like fine-tuning and prompt tuning for 3D domain.

Result: The paper demonstrates the effectiveness of prompt pools in 3D object detection, showing that prompt tuning methods can successfully adapt foundation models trained on Waymo dataset to other 3D scenarios.

Conclusion: Prompt tuning methods are effective for 3D object detection adaptation, and the proposed SOPÂ² approach shows promise. The work aims to inspire further research into prompt-based techniques in the 3D computer vision domain.

Abstract: With the rise of Large Language Models (LLMs) such as GPT-3, these models exhibit strong generalization capabilities. Through transfer learning techniques such as fine-tuning and prompt tuning, they can be adapted to various downstream tasks with minimal parameter adjustments. This approach is particularly common in the field of Natural Language Processing (NLP). This paper aims to explore the effectiveness of common prompt tuning methods in 3D object detection. We investigate whether a model trained on the large-scale Waymo dataset can serve as a foundation model and adapt to other scenarios within the 3D object detection field. This paper sequentially examines the impact of prompt tokens and prompt generators, and further proposes a Scene-Oriented Prompt Pool (\textbf{SOP$^2$}). We demonstrate the effectiveness of prompt pools in 3D object detection, with the goal of inspiring future researchers to delve deeper into the potential of prompts in the 3D field.

</details>


### [19] [New VVC profiles targeting Feature Coding for Machines](https://arxiv.org/abs/2512.08227)
*Md Eimran Hossain Eimon,Ashan Perera,Juan Merlos,Velibor Adzic,Hari Kalva*

Main category: cs.CV

TL;DR: The paper proposes lightweight VVC profiles for compressing neural network features in split inference systems, achieving significant encoding speedups with minimal compression efficiency loss.


<details>
  <summary>Details</summary>
Motivation: Traditional video codecs optimize for perceptual quality using human visual system models, but these assumptions don't apply to intermediate neural network features in split inference systems where features are abstract, sparse, and task-specific.

Method: The authors investigate VVC for feature compression under MPEG-AI FCM standard, perform tool-level analysis to understand coding component impacts, and propose three lightweight VVC profiles: Fast, Faster, and Fastest.

Result: Fast profile provides 2.96% BD-Rate gain with 21.8% encoding time reduction; Faster achieves 1.85% BD-Rate gain with 51.5% speedup; Fastest reduces encoding time by 95.6% with only 1.71% BD-Rate loss.

Conclusion: Lightweight VVC profiles can effectively compress neural network features for split inference systems, offering substantial encoding speed improvements while maintaining compression efficiency and downstream task accuracy.

Abstract: Modern video codecs have been extensively optimized to preserve perceptual quality, leveraging models of the human visual system. However, in split inference systems-where intermediate features from neural network are transmitted instead of pixel data-these assumptions no longer apply. Intermediate features are abstract, sparse, and task-specific, making perceptual fidelity irrelevant. In this paper, we investigate the use of Versatile Video Coding (VVC) for compressing such features under the MPEG-AI Feature Coding for Machines (FCM) standard. We perform a tool-level analysis to understand the impact of individual coding components on compression efficiency and downstream vision task accuracy. Based on these insights, we propose three lightweight essential VVC profiles-Fast, Faster, and Fastest. The Fast profile provides 2.96% BD-Rate gain while reducing encoding time by 21.8%. Faster achieves a 1.85% BD-Rate gain with a 51.5% speedup. Fastest reduces encoding time by 95.6% with only a 1.71% loss in BD-Rate.

</details>


### [20] [MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models](https://arxiv.org/abs/2512.08228)
*Jusheng Zhang,Kaitong Cai,Xiaoyang Guo,Sidi Liu,Qinhan Lv,Ruiqi Chen,Jing Yang,Yijia Fan,Xiaofei Sun,Jian Wang,Ziliang Chen,Liang Lin,Keze Wang*

Main category: cs.CV

TL;DR: MM-CoT is a diagnostic benchmark that evaluates multimodal models' Chain-of-Thought reasoning by testing visual grounding and logical coherence through constrained event chain selection, revealing significant gaps in current models' reasoning fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on generating Chain-of-Thought reasoning but neglect verification of whether reasoning is genuinely grounded in visual evidence and logically coherent. There's a need to assess if multimodal models' reasoning is faithful to visual inputs and logically valid.

Method: MM-CoT requires models to select the sole event chain that satisfies two constraints: visual consistency (anchored in observable evidence) and logical coherence (causal and commonsense validity). The benchmark uses adversarial distractors engineered to violate one of these constraints, exposing distinct reasoning failures.

Result: Leading vision-language models struggle on MM-CoT, revealing a sharp discrepancy between generative fluency and true reasoning fidelity. The benchmark shows low correlation with existing benchmarks, confirming it measures a unique combination of visual grounding and logical reasoning.

Conclusion: MM-CoT provides a foundation for developing future multimodal models that reason not just plausibly, but faithfully and coherently within the visual world, addressing critical gaps in current evaluation methods.

Abstract: The ability to perform Chain-of-Thought (CoT) reasoning marks a major milestone for multimodal models (MMs), enabling them to solve complex visual reasoning problems. Yet a critical question remains: is such reasoning genuinely grounded in visual evidence and logically coherent? Existing benchmarks emphasize generation but neglect verification, i.e., the capacity to assess whether a reasoning chain is both visually consistent and logically valid. To fill this gap, we introduce MM-CoT, a diagnostic benchmark specifically designed to probe the visual grounding and logical coherence of CoT reasoning in MMs. Instead of generating free-form explanations, models must select the sole event chain that satisfies two orthogonal constraints: (i) visual consistency, ensuring all steps are anchored in observable evidence, and (ii) logical coherence, ensuring causal and commonsense validity. Adversarial distractors are engineered to violate one of these constraints, exposing distinct reasoning failures. We evaluate leading vision-language models on MM-CoT and find that even the most advanced systems struggle, revealing a sharp discrepancy between generative fluency and true reasoning fidelity. MM-CoT shows low correlation with existing benchmarks, confirming that it measures a unique combination of visual grounding and logical reasoning. This benchmark provides a foundation for developing future models that reason not just plausibly, but faithfully and coherently within the visual world.

</details>


### [21] [Geometry-Aware Sparse Depth Sampling for High-Fidelity RGB-D Depth Completion in Robotic Systems](https://arxiv.org/abs/2512.08229)
*Tony Salloom,Dandi Zhou,Xinhai Sun*

Main category: cs.CV

TL;DR: Proposed normal-guided sparse depth sampling for depth completion that mimics real sensor reliability patterns instead of uniform random sampling.


<details>
  <summary>Details</summary>
Motivation: Current depth completion methods use unrealistic uniform random sampling from ground truth depth, ignoring that real sensors have geometry-dependent and spatially nonuniform reliability patterns.

Method: Normal-guided sparse depth sampling using PCA-based surface normal estimation on RGB-D point clouds to compute per-pixel depth reliability measures, then sampling according to this reliability distribution.

Result: Improved accuracy, reduced artifacts near edges/discontinuities, and more realistic training conditions that better reflect real sensor behavior on NYU Depth v2 dataset.

Conclusion: Geometry-aware sparse depth sampling produces more realistic training data and better depth completion performance by mimicking real sensor reliability patterns.

Abstract: Accurate three-dimensional perception is essential for modern industrial robotic systems that perform manipulation, inspection, and navigation tasks. RGB-D and stereo vision sensors are widely used for this purpose, but the depth maps they produce are often noisy, incomplete, or biased due to sensor limitations and environmental conditions. Depth completion methods aim to generate dense, reliable depth maps from RGB images and sparse depth input. However, a key limitation in current depth completion pipelines is the unrealistic generation of sparse depth: sparse pixels are typically selected uniformly at random from dense ground-truth depth, ignoring the fact that real sensors exhibit geometry-dependent and spatially nonuniform reliability. In this work, we propose a normal-guided sparse depth sampling strategy that leverages PCA-based surface normal estimation on the RGB-D point cloud to compute a per-pixel depth reliability measure. The sparse depth samples are then drawn according to this reliability distribution. We integrate this sampling method with the Marigold-DC diffusion-based depth completion model and evaluate it on NYU Depth v2 using the standard metrics. Experiments show that our geometry-aware sparse depth improves accuracy, reduces artifacts near edges and discontinuities, and produces more realistic training conditions that better reflect real sensor behavior.

</details>


### [22] [FastBEV++: Fast by Algorithm, Deployable by Design](https://arxiv.org/abs/2512.08237)
*Yuanpeng Chen,Hui Song,Wei Tao,ShanHui Mo,Shuang Zhang,Xiao Hua,TianKun Zhao*

Main category: cs.CV

TL;DR: FastBEV++ is a camera-only BEV perception framework that achieves state-of-the-art performance while being fully deployable on automotive hardware without custom CUDA kernels.


<details>
  <summary>Details</summary>
Motivation: Current camera-only BEV perception systems face a fundamental tension between achieving state-of-the-art performance and maintaining practical on-vehicle deployment tractability, primarily due to reliance on computationally expensive view transformations and platform-specific kernels.

Method: Introduces two principles: 1) "Deployable by Design" via a novel view transformation paradigm that decomposes monolithic projection into a standard Index-Gather-Reshape pipeline using deterministic pre-sorting and elementary operator primitives; 2) "Fast by Algorithm" through end-to-end depth-aware fusion, temporal aggregation, and robust data augmentation.

Result: Achieves new state-of-the-art 0.359 NDS on nuScenes benchmark while maintaining exceptional real-time performance exceeding 134 FPS on Tesla T4 hardware, without requiring custom plugins.

Conclusion: FastBEV++ presents a mature and scalable design philosophy for production autonomous systems by reconciling the performance-deployment tension, offering a solution that is both highly accurate and fully TensorRT-native portable.

Abstract: The advancement of camera-only Bird's-Eye-View(BEV) perception is currently impeded by a fundamental tension between state-of-the-art performance and on-vehicle deployment tractability. This bottleneck stems from a deep-rooted dependency on computationally prohibitive view transformations and bespoke, platform-specific kernels. This paper introduces FastBEV++, a framework engineered to reconcile this tension, demonstrating that high performance and deployment efficiency can be achieved in unison via two guiding principles: Fast by Algorithm and Deployable by Design. We realize the "Deployable by Design" principle through a novel view transformation paradigm that decomposes the monolithic projection into a standard Index-Gather-Reshape pipeline. Enabled by a deterministic pre-sorting strategy, this transformation is executed entirely with elementary, operator native primitives (e.g Gather, Matrix Multiplication), which eliminates the need for specialized CUDA kernels and ensures fully TensorRT-native portability. Concurrently, our framework is "Fast by Algorithm", leveraging this decomposed structure to seamlessly integrate an end-to-end, depth-aware fusion mechanism. This jointly learned depth modulation, further bolstered by temporal aggregation and robust data augmentation, significantly enhances the geometric fidelity of the BEV representation.Empirical validation on the nuScenes benchmark corroborates the efficacy of our approach. FastBEV++ establishes a new state-of-the-art 0.359 NDS while maintaining exceptional real-time performance, exceeding 134 FPS on automotive-grade hardware (e.g Tesla T4). By offering a solution that is free of custom plugins yet highly accurate, FastBEV++ presents a mature and scalable design philosophy for production autonomous systems. The code is released at: https://github.com/ymlab/advanced-fastbev

</details>


### [23] [HybridToken-VLM: Hybrid Token Compression for Vision-Language Models](https://arxiv.org/abs/2512.08240)
*Jusheng Zhang,Xiaoyang Guo,Kaitong Cai,Qinhan Lv,Yijia Fan,Wenhao Chai,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: HTC-VLM is a hybrid vision-language model that uses dual channels (continuous for fine details, discrete for symbolic anchors) to compress visual information from 580 tokens to 1 token while maintaining 87.2% performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current VLMs face quadratic computational costs from processing hundreds of visual patch tokens, creating memory and context window strain. Traditional compression methods either dilute high-level semantics (continuous compression) or lose fine-grained details (discrete quantization).

Method: Hybrid framework with dual channels: continuous pathway for fine-grained details via ViT patches, and discrete pathway for symbolic anchors using MGVQ quantization projected to four tokens. These are fused into a 580-token hybrid sequence and compressed into a single voco token using disentanglement attention mask and bottleneck.

Result: Achieves 87.2% average performance retention across seven benchmarks (GQA, VQAv2, MMBench, MME, POPE, SEED-Bench, ScienceQA-Image), outperforming leading continuous baseline (81.0%) with 580-to-1 compression ratio. Attention analyses show compressed token prioritizes discrete anchor, validating semantic guidance.

Conclusion: A minimalist hybrid design can resolve the efficiency-fidelity dilemma in VLMs, enabling scalable multimodal reasoning by disentangling semantics and appearance through dual-channel compression.

Abstract: Vision-language models (VLMs) have transformed multimodal reasoning, but feeding hundreds of visual patch tokens into LLMs incurs quadratic computational costs, straining memory and context windows. Traditional approaches face a trade-off: continuous compression dilutes high-level semantics such as object identities, while discrete quantization loses fine-grained details such as textures. We introduce HTC-VLM, a hybrid framework that disentangles semantics and appearance through dual channels, i.e., a continuous pathway for fine-grained details via ViT patches and a discrete pathway for symbolic anchors using MGVQ quantization projected to four tokens. These are fused into a 580-token hybrid sequence and compressed into a single voco token via a disentanglement attention mask and bottleneck, ensuring efficient and grounded representations. HTC-VLM achieves an average performance retention of 87.2 percent across seven benchmarks (GQA, VQAv2, MMBench, MME, POPE, SEED-Bench, ScienceQA-Image), outperforming the leading continuous baseline at 81.0 percent with a 580-to-1 compression ratio. Attention analyses show that the compressed token prioritizes the discrete anchor, validating its semantic guidance. Our work demonstrates that a minimalist hybrid design can resolve the efficiency-fidelity dilemma and advance scalable VLMs.

</details>


### [24] [Residual-SwinCA-Net: A Channel-Aware Integrated Residual CNN-Swin Transformer for Malignant Lesion Segmentation in BUSI](https://arxiv.org/abs/2512.08243)
*Saeeda Naz,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: A hybrid Residual-SwinCA-Net framework combining CNN and Transformer achieves state-of-the-art breast lesion segmentation on BUSI dataset with 99.29% accuracy.


<details>
  <summary>Details</summary>
Motivation: To address challenges in breast ultrasound lesion segmentation by extracting both locally correlated robust features and global dependencies, while handling ultrasound noise and maintaining morphological integrity of malignant lesions.

Method: Proposes a deep hybrid framework with: 1) Residual CNN modules for local features, 2) Customized Swin Transformer blocks with internal residual pathways for global dependencies, 3) Laplacian-of-Gaussian operator for tissue continuity and noise suppression, 4) Boundary-oriented operator for morphological integrity, 5) Progressive contraction strategy for scale invariance, 6) Multi-Scale Channel Attention and Squeezing (MSCAS) module for selective feature emphasis, and 7) Pixel-Attention module for class-relevant spatial cues.

Result: Outperforms existing CNNs/ViTs on BUSI dataset with 99.29% mean accuracy, 98.74% IoU, and 0.9041 Dice score for breast lesion segmentation.

Conclusion: The Residual-SwinCA-Net framework significantly improves BUSI lesion diagnostic performance and strengthens timely clinical decision-making for breast cancer diagnosis.

Abstract: A novel deep hybrid Residual-SwinCA-Net segmentation framework is proposed in the study for addressing such challenges by extracting locally correlated and robust features, incorporating residual CNN modules. Furthermore, for learning global dependencies, Swin Transformer blocks are customized using internal residual pathways, which reinforce gradient stability, refine local patterns, and facilitate global feature fusion. Formerly, for enhancing tissue continuity, ultrasound noise suppressions, and accentuating fine structural transitions Laplacian-of-Gaussian regional operator is applied, and for maintaining the morphological integrity of malignant lesion contours, a boundary-oriented operator has been incorporated. Subsequently, a contraction strategy was applied stage-wise by progressively reducing features-map progressively for capturing scale invariance and enhancing the robustness of structural variability. In addition, each decoder level prior augmentation integrates a new Multi-Scale Channel Attention and Squeezing (MSCAS) module. The MSCAS selectively emphasizes encoder salient maps, retains discriminative global context, and complementary local structures with minimal computational cost while suppressing redundant activations. Finally, the Pixel-Attention module encodes class-relevant spatial cues by adaptively weighing malignant lesion pixels while suppressing background interference. The Residual-SwinCA-Net and existing CNNs/ViTs techniques have been implemented on the publicly available BUSI dataset. The proposed Residual-SwinCA-Net framework outperformed and achieved 99.29% mean accuracy, 98.74% IoU, and 0.9041 Dice for breast lesion segmentation. The proposed Residual-SwinCA-Net framework improves the BUSI lesion diagnostic performance and strengthens timely clinical decision-making.

</details>


### [25] [Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection](https://arxiv.org/abs/2512.08247)
*Haowen Zheng,Hu Zhu,Lu Deng,Weihao Gu,Yang Yang,Yanyan Liang*

Main category: cs.CV

TL;DR: FTKD is a knowledge distillation method that transfers future frame knowledge from offline to online 3D object detection models using sparse queries, improving accuracy without increasing inference cost.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge distillation methods for camera-based 3D object detection overlook future frames, focusing only on spatial features or temporal relations with strict frame alignment, making it hard for online models to learn future knowledge from offline teachers.

Method: Proposes Future Temporal Knowledge Distillation (FTKD) with two components: 1) future-aware feature reconstruction that helps students capture future features without strict frame alignment, and 2) future-guided logit distillation that leverages teacher's stable foreground/background context.

Result: Achieves up to 1.3 mAP and 1.3 NDS gains on nuScenes dataset, plus most accurate velocity estimation, without increasing inference cost when applied to two high-performing 3D detection baselines.

Conclusion: FTKD effectively transfers future knowledge from offline to online models, addressing limitations of existing KD methods and improving 3D object detection performance in autonomous driving applications.

Abstract: Camera-based temporal 3D object detection has shown impressive results in autonomous driving, with offline models improving accuracy by using future frames. Knowledge distillation (KD) can be an appealing framework for transferring rich information from offline models to online models. However, existing KD methods overlook future frames, as they mainly focus on spatial feature distillation under strict frame alignment or on temporal relational distillation, thereby making it challenging for online models to effectively learn future knowledge. To this end, we propose a sparse query-based approach, Future Temporal Knowledge Distillation (FTKD), which effectively transfers future frame knowledge from an offline teacher model to an online student model. Specifically, we present a future-aware feature reconstruction strategy to encourage the student model to capture future features without strict frame alignment. In addition, we further introduce future-guided logit distillation to leverage the teacher's stable foreground and background context. FTKD is applied to two high-performing 3D object detection baselines, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset, as well as the most accurate velocity estimation, without increasing inference cost.

</details>


### [26] [Query-aware Hub Prototype Learning for Few-Shot 3D Point Cloud Semantic Segmentation](https://arxiv.org/abs/2512.08253)
*YiLin Zhou,Lili Wei,Zheming Xu,Ziyi Chen,Congyan Lang*

Main category: cs.CV

TL;DR: QHP: Query-aware Hub Prototype learning for few-shot 3D point cloud segmentation that addresses prototype bias by modeling support-query correlations and optimizing prototype distributions.


<details>
  <summary>Details</summary>
Motivation: Existing metric-based prototype learning methods generate prototypes solely from support set without considering query relevance, causing prototype bias and poor generalization to query distribution, especially with distribution shifts.

Method: Proposes Query-aware Hub Prototype (QHP) with two modules: 1) Hub Prototype Generation (HPG) constructs bipartite graph connecting query/support points, identifies frequently linked support hubs to generate query-relevant prototypes; 2) Prototype Distribution Optimization (PDO) uses purity-reweighted contrastive loss to refine prototypes by pulling bad hubs and outliers toward class centers.

Result: Extensive experiments on S3DIS and ScanNet show QHP achieves substantial performance gains over state-of-the-art methods, effectively narrowing semantic gap between prototypes and query sets.

Conclusion: QHP successfully addresses prototype bias in FS-3DSeg by explicitly modeling support-query correlations and optimizing prototype distributions, leading to improved segmentation performance.

Abstract: Few-shot 3D point cloud semantic segmentation (FS-3DSeg) aims to segment novel classes with only a few labeled samples. However, existing metric-based prototype learning methods generate prototypes solely from the support set, without considering their relevance to query data. This often results in prototype bias, where prototypes overfit support-specific characteristics and fail to generalize to the query distribution, especially in the presence of distribution shifts, which leads to degraded segmentation performance. To address this issue, we propose a novel Query-aware Hub Prototype (QHP) learning method that explicitly models semantic correlations between support and query sets. Specifically, we propose a Hub Prototype Generation (HPG) module that constructs a bipartite graph connecting query and support points, identifies frequently linked support hubs, and generates query-relevant prototypes that better capture cross-set semantics. To further mitigate the influence of bad hubs and ambiguous prototypes near class boundaries, we introduce a Prototype Distribution Optimization (PDO) module, which employs a purity-reweighted contrastive loss to refine prototype representations by pulling bad hubs and outlier prototypes closer to their corresponding class centers. Extensive experiments on S3DIS and ScanNet demonstrate that QHP achieves substantial performance gains over state-of-the-art methods, effectively narrowing the semantic gap between prototypes and query sets in FS-3DSeg.

</details>


### [27] [SFP: Real-World Scene Recovery Using Spatial and Frequency Priors](https://arxiv.org/abs/2512.08254)
*Yun Liu,Tao Li,Cosmin Ancuti,Wenqi Ren,Weisi Lin*

Main category: cs.CV

TL;DR: SFP: Spatial and Frequency Priors for real-world scene recovery using spatial transmission estimation and adaptive frequency enhancement with novel priors, fused for final restoration.


<details>
  <summary>Details</summary>
Motivation: Existing scene recovery methods are insufficient: single priors can't handle multiple degradations, and complex networks trained on synthetic data generalize poorly to diverse real-world scenarios.

Method: Proposes Spatial and Frequency Priors (SFP): 1) Spatial prior: uses inverse degraded image's spectral projection to estimate transmission map for scattering degradation recovery; 2) Frequency prior: constructs adaptive frequency enhancement mask using two novel priors (DC component intensity similarity and low radial frequency magnitude proportion); 3) Weighted fusion integrates spatial restoration, frequency enhancement, and salient input features.

Result: Extensive evaluations demonstrate effectiveness and superiority of SFP for scene recovery under various degradation conditions.

Conclusion: SFP provides an effective approach for real-world scene recovery by leveraging complementary spatial and frequency priors, overcoming limitations of existing single-prior or synthetic-data-trained methods.

Abstract: Scene recovery serves as a critical task for various computer vision applications. Existing methods typically rely on a single prior, which is inherently insufficient to handle multiple degradations, or employ complex network architectures trained on synthetic data, which suffer from poor generalization for diverse real-world scenarios. In this paper, we propose Spatial and Frequency Priors (SFP) for real-world scene recovery. In the spatial domain, we observe that the inverse of the degraded image exhibits a projection along its spectral direction that resembles the scene transmission. Leveraging this spatial prior, the transmission map is estimated to recover the scene from scattering degradation. In the frequency domain, a mask is constructed for adaptive frequency enhancement, with two parameters estimated using our proposed novel priors. Specifically, one prior assumes that the mean intensity of the degraded image's direct current (DC) components across three channels in the frequency domain closely approximates that of each channel in the clear image. The second prior is based on the observation that, for clear images, the magnitude of low radial frequencies below 0.001 constitutes approximately 1% of the total spectrum. Finally, we design a weighted fusion strategy to integrate spatial-domain restoration, frequency-domain enhancement, and salient features from the input image, yielding the final recovered result. Extensive evaluations demonstrate the effectiveness and superiority of our proposed SFP for scene recovery under various degradation conditions.

</details>


### [28] [RLCNet: An end-to-end deep learning framework for simultaneous online calibration of LiDAR, RADAR, and Camera](https://arxiv.org/abs/2512.08262)
*Hafeez Husain Cholakkal,Stefano Arrigoni,Francesco Braghin*

Main category: cs.CV

TL;DR: RLCNet is an end-to-end deep learning framework for simultaneous online calibration of LiDAR, RADAR, and camera sensors in autonomous vehicles, featuring real-time operation with weighted moving average and outlier rejection for dynamic adjustment.


<details>
  <summary>Details</summary>
Motivation: Accurate extrinsic calibration of multimodal sensors (LiDAR, RADAR, camera) is essential for reliable perception in autonomous vehicles but remains challenging due to mechanical vibrations and cumulative sensor drift in dynamic environments.

Method: RLCNet: a novel end-to-end trainable deep learning framework for simultaneous online calibration of multimodal sensors, incorporating a weighted moving average and outlier rejection mechanism for real-time operation with dynamic parameter adjustment.

Result: Validated on real-world datasets, RLCNet demonstrates robust performance under diverse conditions with superior accuracy and robustness compared to existing methods, while ablation studies highlight the significance of architectural choices.

Conclusion: RLCNet provides a practical solution for online sensor calibration in autonomous vehicles, enabling dynamic adjustment of calibration parameters with reduced prediction noise and improved resilience to drift for reliable perception systems.

Abstract: Accurate extrinsic calibration of LiDAR, RADAR, and camera sensors is essential for reliable perception in autonomous vehicles. Still, it remains challenging due to factors such as mechanical vibrations and cumulative sensor drift in dynamic environments. This paper presents RLCNet, a novel end-to-end trainable deep learning framework for the simultaneous online calibration of these multimodal sensors. Validated on real-world datasets, RLCNet is designed for practical deployment and demonstrates robust performance under diverse conditions. To support real-time operation, an online calibration framework is introduced that incorporates a weighted moving average and outlier rejection, enabling dynamic adjustment of calibration parameters with reduced prediction noise and improved resilience to drift. An ablation study highlights the significance of architectural choices, while comparisons with existing methods demonstrate the superior accuracy and robustness of the proposed approach.

</details>


### [29] [EgoX: Egocentric Video Generation from a Single Exocentric Video](https://arxiv.org/abs/2512.08269)
*Taewoong Kang,Kinam Kim,Dohyeon Kim,Minho Park,Junha Hyung,Jaegul Choo*

Main category: cs.CV

TL;DR: EgoX is a framework that converts third-person (exocentric) videos to first-person (egocentric) videos using adapted video diffusion models with geometric consistency mechanisms.


<details>
  <summary>Details</summary>
Motivation: Egocentric perception provides direct, immersive understanding from one's own viewpoint. Converting third-person videos to first-person perspective enables new immersive applications but is challenging due to extreme camera pose differences and minimal view overlap between perspectives.

Method: EgoX leverages pretrained video diffusion models with lightweight LoRA adaptation. It uses a unified conditioning strategy combining exocentric and egocentric priors via width and channel-wise concatenation. A geometry-guided self-attention mechanism selectively attends to spatially relevant regions for geometric coherence.

Result: The approach achieves coherent and realistic egocentric video generation with strong scalability and robustness across unseen and in-the-wild videos.

Conclusion: EgoX successfully addresses the challenging task of converting exocentric to egocentric videos by combining adapted diffusion models with geometric consistency mechanisms, enabling new possibilities for immersive understanding.

Abstract: Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.

</details>


### [30] [PAVAS: Physics-Aware Video-to-Audio Synthesis](https://arxiv.org/abs/2512.08282)
*Oh Hyun-Bin,Yuhta Takida,Toshimitsu Uesaka,Tae-Hyun Oh,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: PAVAS introduces physics-aware video-to-audio generation using physical parameter estimation and a physics-driven adapter to create more realistic sounds based on object mass and motion.


<details>
  <summary>Details</summary>
Motivation: Current V2A models are appearance-driven and lack consideration of physical factors that shape real-world sounds, limiting their realism and physical plausibility.

Method: Uses Physics-Driven Audio Adapter (Phy-Adapter) with physical parameters estimated by Physical Parameter Estimator (PPE), which combines VLM for mass inference and segmentation-based 3D reconstruction for motion trajectory and velocity computation.

Result: Outperforms existing V2A models in quantitative and qualitative evaluations, produces physically plausible and perceptually coherent audio, validated on new VGG-Impact benchmark with Audio-Physics Correlation Coefficient (APCC) metric.

Conclusion: Incorporating physical reasoning into V2A generation significantly improves audio realism and physical plausibility, demonstrating the importance of considering underlying physical factors in audio synthesis.

Abstract: Recent advances in Video-to-Audio (V2A) generation have achieved impressive perceptual quality and temporal synchronization, yet most models remain appearance-driven, capturing visual-acoustic correlations without considering the physical factors that shape real-world sounds. We present Physics-Aware Video-to-Audio Synthesis (PAVAS), a method that incorporates physical reasoning into a latent diffusion-based V2A generation through the Physics-Driven Audio Adapter (Phy-Adapter). The adapter receives object-level physical parameters estimated by the Physical Parameter Estimator (PPE), which uses a Vision-Language Model (VLM) to infer the moving-object mass and a segmentation-based dynamic 3D reconstruction module to recover its motion trajectory for velocity computation. These physical cues enable the model to synthesize sounds that reflect underlying physical factors. To assess physical realism, we curate VGG-Impact, a benchmark focusing on object-object interactions, and introduce Audio-Physics Correlation Coefficient (APCC), an evaluation metric that measures consistency between physical and auditory attributes. Comprehensive experiments show that PAVAS produces physically plausible and perceptually coherent audio, outperforming existing V2A models in both quantitative and qualitative evaluations. Visit https://physics-aware-video-to-audio-synthesis.github.io for demo videos.

</details>


### [31] [OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation](https://arxiv.org/abs/2512.08294)
*Yexin Liu,Manyuan Zhang,Yueze Wang,Hongyu Li,Dian Zheng,Weiming Zhang,Changsheng Lu,Xunliang Cai,Yan Feng,Peng Pei,Harry Yang*

Main category: cs.CV

TL;DR: OpenSubject: A large-scale video-derived dataset (2.5M samples, 4.35M images) for improving subject-driven image generation and manipulation, especially in complex multi-subject scenes.


<details>
  <summary>Details</summary>
Motivation: Current subject-driven image generation models often fail to preserve reference identities and struggle with complex scenes containing multiple subjects. There's a need for better training data to address these limitations.

Method: Four-stage pipeline: 1) Video curation with quality filtering, 2) Cross-frame subject mining/pairing using VLM-based consensus and diversity-aware pairing, 3) Identity-preserving reference image synthesis via segmentation-guided outpainting and box-guided inpainting, 4) Verification/captioning with VLM validation and caption construction.

Result: Created OpenSubject dataset with 2.5M samples and 4.35M images. Training with this dataset improves generation and manipulation performance, particularly in complex scenes. Also introduced a comprehensive benchmark for evaluation.

Conclusion: OpenSubject addresses key limitations in subject-driven generation by providing high-quality training data that improves identity preservation and performance in complex multi-subject scenarios.

Abstract: Despite the promising progress in subject-driven image generation, current models often deviate from the reference identities and struggle in complex scenes with multiple subjects. To address this challenge, we introduce OpenSubject, a video-derived large-scale corpus with 2.5M samples and 4.35M images for subject-driven generation and manipulation. The dataset is built with a four-stage pipeline that exploits cross-frame identity priors. (i) Video Curation. We apply resolution and aesthetic filtering to obtain high-quality clips. (ii) Cross-Frame Subject Mining and Pairing. We utilize vision-language model (VLM)-based category consensus, local grounding, and diversity-aware pairing to select image pairs. (iii) Identity-Preserving Reference Image Synthesis. We introduce segmentation map-guided outpainting to synthesize the input images for subject-driven generation and box-guided inpainting to generate input images for subject-driven manipulation, together with geometry-aware augmentations and irregular boundary erosion. (iv) Verification and Captioning. We utilize a VLM to validate synthesized samples, re-synthesize failed samples based on stage (iii), and then construct short and long captions. In addition, we introduce a benchmark covering subject-driven generation and manipulation, and then evaluate identity fidelity, prompt adherence, manipulation consistency, and background consistency with a VLM judge. Extensive experiments show that training with OpenSubject improves generation and manipulation performance, particularly in complex scenes.

</details>


### [32] [Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation](https://arxiv.org/abs/2512.08309)
*Alexander Goslin*

Main category: cs.CV

TL;DR: Terrain Diffusion replaces Perlin noise with diffusion models for infinite, coherent procedural world generation with planetary-scale realism.


<details>
  <summary>Details</summary>
Motivation: Traditional procedural noise functions like Perlin noise are fast and infinite but limited in realism and large-scale coherence. The paper aims to bridge diffusion model fidelity with procedural noise's essential properties for next-generation world generation.

Method: Introduces Terrain Diffusion with InfiniteDiffusion algorithm for seamless infinite generation, hierarchical diffusion models for planetary context and local detail, compact Laplacian encoding for Earth-scale stability, open-source infinite-tensor framework, and few-step consistency distillation for efficiency.

Result: Establishes diffusion models as practical foundation for procedural world generation capable of synthesizing entire planets coherently, controllably, and without limits.

Conclusion: Terrain Diffusion represents an AI-era successor to Perlin noise, combining diffusion model fidelity with procedural noise's essential properties for limitless, realistic world generation.

Abstract: For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.

</details>


### [33] [GeoDM: Geometry-aware Distribution Matching for Dataset Distillation](https://arxiv.org/abs/2512.08317)
*Xuhui Li,Zhengquan Luo,Zihui Cui,Zhiqiang Xu*

Main category: cs.CV

TL;DR: GeoDM: A geometry-aware dataset distillation framework that operates in product manifolds (Euclidean, hyperbolic, spherical) to capture diverse data structures, outperforming Euclidean-only methods.


<details>
  <summary>Details</summary>
Motivation: Existing dataset distillation methods are confined to Euclidean spaces, capturing only linear structures and overlooking intrinsic data geometry like curvature. Real-world high-dimensional data often lie on low-dimensional manifolds, so distilled data should align with original data manifold geometry.

Method: Proposes GeoDM framework operating in Cartesian product of Euclidean, hyperbolic, and spherical manifolds. Introduces learnable curvature and weight parameters for different geometries. Designs optimal transport loss to enhance distribution fidelity. Theoretical analysis shows geometry-aware matching yields smaller generalization error bound.

Result: Extensive experiments on standard benchmarks demonstrate GeoDM outperforms state-of-the-art data distillation methods and remains effective across various distribution-matching strategies for single geometries.

Conclusion: Geometry-aware dataset distillation in product manifolds effectively captures diverse data structures (flat, hierarchical, cyclical) and achieves superior performance compared to Euclidean-only approaches.

Abstract: Dataset distillation aims to synthesize a compact subset of the original data, enabling models trained on it to achieve performance comparable to those trained on the original large dataset. Existing distribution-matching methods are confined to Euclidean spaces, making them only capture linear structures and overlook the intrinsic geometry of real data, e.g., curvature. However, high-dimensional data often lie on low-dimensional manifolds, suggesting that dataset distillation should have the distilled data manifold aligned with the original data manifold. In this work, we propose a geometry-aware distribution-matching framework, called \textbf{GeoDM}, which operates in the Cartesian product of Euclidean, hyperbolic, and spherical manifolds, with flat, hierarchical, and cyclical structures all captured by a unified representation. To adapt to the underlying data geometry, we introduce learnable curvature and weight parameters for three kinds of geometries. At the same time, we design an optimal transport loss to enhance the distribution fidelity. Our theoretical analysis shows that the geometry-aware distribution matching in a product space yields a smaller generalization error bound than the Euclidean counterparts. Extensive experiments conducted on standard benchmarks demonstrate that our algorithm outperforms state-of-the-art data distillation methods and remains effective across various distribution-matching strategies for the single geometries.

</details>


### [34] [Detecting Dental Landmarks from Intraoral 3D Scans: the 3DTeethLand challenge](https://arxiv.org/abs/2512.08323)
*Achraf Ben-Hamadou,Nour Neifar,Ahmed Rekik,Oussama Smaoui,Firas Bouzguenda,Sergi Pujades,Niels van Nistelrooij,Shankeeth Vinayahalingam,Kaibo Shi,Hairong Jin,Youyi Zheng,Tibor KubÃ­k,OldÅich Kodym,Petr Å illing,KateÅina TrÃ¡vnÃ­ÄkovÃ¡,TomÃ¡Å¡ MojÅ¾iÅ¡,Jan Matula,Jeffry Hartanto,Xiaoying Zhu,Kim-Ngan Nguyen,Tudor Dascalu,Huikai Wu,and Weijie Liu,Shaojie Zhuang,Guangshun Wei,Yuanfeng Zhou*

Main category: cs.CV

TL;DR: 3DTeethLand challenge introduces first public dataset for 3D teeth landmark detection from intraoral scans to advance orthodontic diagnostics and treatment planning.


<details>
  <summary>Details</summary>
Motivation: Precise teeth landmark detection is critical for advanced orthodontic diagnostics, personalized treatment strategies, and effective treatment monitoring, but faces challenges due to complex tooth geometry and individual variations.

Method: The 3DTeethLand challenge was organized in collaboration with MICCAI 2024, calling for algorithms focused on teeth landmark detection from intraoral 3D scans, and introduced the first publicly available dataset for this task.

Result: The challenge provides a valuable benchmark resource to assess state-of-the-art methods and encourages the research community to develop methodological contributions for this clinically significant problem.

Conclusion: The 3DTeethLand challenge addresses the need for advanced deep learning techniques in 3D teeth landmark detection by providing the first public dataset and fostering community collaboration to solve this clinically important problem.

Abstract: Teeth landmark detection is a critical task in modern clinical orthodontics. Their precise identification enables advanced diagnostics, facilitates personalized treatment strategies, and supports more effective monitoring of treatment progress in clinical dentistry. However, several significant challenges may arise due to the intricate geometry of individual teeth and the substantial variations observed across different individuals. To address these complexities, the development of advanced techniques, especially through the application of deep learning, is essential for the precise and reliable detection of 3D tooth landmarks. In this context, the 3DTeethLand challenge was held in collaboration with the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) in 2024, calling for algorithms focused on teeth landmark detection from intraoral 3D scans. This challenge introduced the first publicly available dataset for 3D teeth landmark detection, offering a valuable resource to assess the state-of-the-art methods in this task and encourage the community to provide methodological contributions towards the resolution of their problem with significant clinical implications.

</details>


### [35] [GeoDiffMM: Geometry-Guided Conditional Diffusion for Motion Magnification](https://arxiv.org/abs/2512.08325)
*Xuedeng Liu,Jiabao Guo,Zheng Zhang,Fei Wang,Zhi Liu,Dan Guo*

Main category: cs.CV

TL;DR: GeoDiffMM is a diffusion-based Lagrangian video motion magnification framework that uses optical flow as geometric conditioning to amplify subtle motions while suppressing noise.


<details>
  <summary>Details</summary>
Motivation: Existing Eulerian VMM approaches struggle to separate photon noise from true micro-motion when motion displacements are very small, despite using decoupling representation learning techniques.

Method: Three-stage approach: 1) Noise-free Optical Flow Augmentation synthesizes diverse nonrigid motion fields without photon noise for supervision, 2) Diffusion Motion Magnifier conditions denoising on optical flow geometry prior and learnable magnification factor, 3) Flow-based Video Synthesis maps amplified motion back to image domain.

Result: Extensive experiments on real and synthetic datasets show GeoDiffMM outperforms state-of-the-art methods and significantly improves motion magnification quality.

Conclusion: GeoDiffMM provides a novel diffusion-based Lagrangian framework that enables structurally consistent motion magnification by leveraging optical flow as geometric conditioning, effectively separating true motion from noise.

Abstract: Video Motion Magnification (VMM) amplifies subtle macroscopic motions to a perceptible level. Recently, existing mainstream Eulerian approaches address amplification-induced noise via decoupling representation learning such as texture, shape and frequancey schemes, but they still struggle to separate photon noise from true micro-motion when motion displacements are very small. We propose GeoDiffMM, a novel diffusion-based Lagrangian VMM framework conditioned on optical flow as a geometric cue, enabling structurally consistent motion magnification. Specifically, we design a Noise-free Optical Flow Augmentation strategy that synthesizes diverse nonrigid motion fields without photon noise as supervision, helping the model learn more accurate geometry-aware optial flow and generalize better. Next, we develop a Diffusion Motion Magnifier that conditions the denoising process on (i) optical flow as a geometry prior and (ii) a learnable magnification factor controlling magnitude, thereby selectively amplifying motion components consistent with scene semantics and structure while suppressing content-irrelevant perturbations. Finally, we perform Flow-based Video Synthesis to map the amplified motion back to the image domain with high fidelity. Extensive experiments on real and synthetic datasets show that GeoDiffMM outperforms state-of-the-art methods and significantly improves motion magnification.

</details>


### [36] [Low Rank Support Quaternion Matrix Machine](https://arxiv.org/abs/2512.08327)
*Wang Chen,Ziyan Luo,Shuangyue Wang*

Main category: cs.CV

TL;DR: LSQMM is a novel quaternion-based classification method for color images that treats RGB channels as pure quaternions and uses quaternion nuclear norm regularization for low-rank structure promotion.


<details>
  <summary>Details</summary>
Motivation: Conventional methods represent color image features as vectors/matrices/tensors in real field, but this doesn't effectively preserve the intrinsic coupling relationships among RGB channels. Quaternion algebra has shown success in image recovery/denoising, suggesting it could better model color channel relationships for classification.

Method: Proposes Low-rank Support Quaternion Matrix Machine (LSQMM) that treats RGB channels as pure quaternions. Adds quaternion nuclear norm regularization (extension of matrix nuclear norm to quaternion domain) to hinge loss to promote low-rank structures from correlated color channels. Uses ADMM-based iterative algorithm to solve the quaternion optimization model.

Result: Experimental results on multiple color image classification datasets show LSQMM has advantages in classification accuracy, robustness, and computational efficiency compared to state-of-the-art methods using support vector machines, support matrix machines, and support tensor machines.

Conclusion: LSQMM effectively preserves color channel relationships via quaternion algebra and low-rank regularization, leading to superior classification performance for color images compared to conventional real-field representations.

Abstract: Input features are conventionally represented as vectors, matrices, or third order tensors in the real field, for color image classification. Inspired by the success of quaternion data modeling for color images in image recovery and denoising tasks, we propose a novel classification method for color image classification, named as the Low-rank Support Quaternion Matrix Machine (LSQMM), in which the RGB channels are treated as pure quaternions to effectively preserve the intrinsic coupling relationships among channels via the quaternion algebra. For the purpose of promoting low-rank structures resulting from strongly correlated color channels, a quaternion nuclear norm regularization term, serving as a natural extension of the conventional matrix nuclear norm to the quaternion domain, is added to the hinge loss in our LSQMM model. An Alternating Direction Method of Multipliers (ADMM)-based iterative algorithm is designed to effectively resolve the proposed quaternion optimization model. Experimental results on multiple color image classification datasets demonstrate that our proposed classification approach exhibits advantages in classification accuracy, robustness and computational efficiency, compared to several state-of-the-art methods using support vector machines, support matrix machines, and support tensor machines.

</details>


### [37] [Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models](https://arxiv.org/abs/2512.08329)
*Michael R. Martin,Garrick Chan,Kwan-Liu Ma*

Main category: cs.CV

TL;DR: This paper provides a systematic explainable AI analysis of image protection mechanisms (Glaze and Nightshade), revealing they operate as structured, low-entropy perturbations tightly coupled to image content rather than semantic dislocation.


<details>
  <summary>Details</summary>
Motivation: While Glaze and Nightshade are empirically effective at disrupting text-to-image models, their internal structure, detectability, and representational behavior remain poorly understood. The paper aims to provide systematic analysis using explainable AI techniques.

Method: Uses a unified framework integrating white-box feature-space inspection and black-box signal-level probing. Techniques include latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization.

Result: Protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment.

Conclusion: Contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This advances interpretability of adversarial image protection and informs future defense and detection strategies for generative AI systems.

Abstract: Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems.

</details>


### [38] [MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs](https://arxiv.org/abs/2508.05502)
*Yufei Gao,Jiaying Fei,Nuo Chen,Ruirui Chen,Guohang Yan,Yunshi Lan,Botian Shi*

Main category: cs.CV

TL;DR: MELLA dataset improves MLLMs for low-resource languages by enhancing both linguistic capabilities and cultural groundedness through dual-source data collection.


<details>
  <summary>Details</summary>
Motivation: Current multilingual MLLM enhancement methods focus only on text modality or machine translation, producing "thin descriptions" that lack multimodal informativeness and cultural groundedness needed for effective low-resource language support.

Method: Proposed dual-source strategy: 1) native web alt-text for cultural groundedness, 2) MLLM-generated captions for linguistic capabilities. Implemented as MELLA multimodal multilingual dataset for fine-tuning.

Result: Fine-tuning on MELLA improves performance across eight languages on various MLLM backbones, producing "thick descriptions" with verified gains from both cultural knowledge and linguistic capability enhancement.

Conclusion: MELLA dataset effectively bridges the gap in low-resource language MLLM performance by addressing both linguistic and cultural dimensions, enabling more culturally-aware and informative multimodal understanding.

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable performance in high-resource languages. However, their effectiveness diminishes significantly in the contexts of low-resource languages. Current multilingual enhancement methods are often limited to text modality or rely solely on machine translation. While such approaches help models acquire basic linguistic capabilities and produce "thin descriptions", they neglect the importance of multimodal informativeness and cultural groundedness, both of which are crucial for serving low-resource language users effectively. To bridge this gap, in this study, we identify two significant objectives for a truly effective MLLM in low-resource language settings, namely 1) linguistic capability and 2) cultural groundedness, placing special emphasis on cultural awareness. To achieve these dual objectives, we propose a dual-source strategy that guides the collection of data tailored to each goal, sourcing native web alt-text for culture and MLLM-generated captions for linguistics. As a concrete implementation, we introduce MELLA, a multimodal, multilingual dataset. Experiment results show that after fine-tuning on MELLA, there is a general performance improvement for the eight languages on various MLLM backbones, with models producing "thick descriptions". We verify that the performance gains are from both cultural knowledge enhancement and linguistic capability enhancement. Our dataset can be found at https://opendatalab.com/applyMultilingualCorpus.

</details>


### [39] [PointDico: Contrastive 3D Representation Learning Guided by Diffusion Models](https://arxiv.org/abs/2512.08330)
*Pengbo Li,Yiding Sun,Haozhe Cheng*

Main category: cs.CV

TL;DR: PointDico is a novel 3D representation learning model that integrates diffusion and contrastive learning through knowledge distillation to overcome limitations of existing methods in handling unordered point clouds.


<details>
  <summary>Details</summary>
Motivation: Existing self-supervised methods struggle with 3D data due to its unordered and uneven density properties. Contrastive models suffer from overfitting while 3D Mask Autoencoders have difficulty handling unordered point clouds.

Method: PointDico integrates diffusion and contrastive learning through knowledge distillation, using diffusion as a guide for contrastive learning. It features a hierarchical pyramid conditional generator for multi-scale geometric feature extraction and a dual-channel design to integrate local and global contextual information.

Result: Achieves state-of-the-art performance: 94.32% accuracy on ScanObjectNN and 86.5% instance mIoU on ShapeNetPart.

Conclusion: PointDico successfully combines diffusion and contrastive learning to overcome limitations of existing 3D representation learning methods, achieving superior performance on benchmark datasets.

Abstract: Self-supervised representation learning has shown significant improvement in Natural Language Processing and 2D Computer Vision. However, existing methods face difficulties in representing 3D data because of its unordered and uneven density. Through an in-depth analysis of mainstream contrastive and generative approaches, we find that contrastive models tend to suffer from overfitting, while 3D Mask Autoencoders struggle to handle unordered point clouds. This motivates us to learn 3D representations by sharing the merits of diffusion and contrast models, which is non-trivial due to the pattern difference between the two paradigms. In this paper, we propose \textit{PointDico}, a novel model that seamlessly integrates these methods. \textit{PointDico} learns from both denoising generative modeling and cross-modal contrastive learning through knowledge distillation, where the diffusion model serves as a guide for the contrastive model. We introduce a hierarchical pyramid conditional generator for multi-scale geometric feature extraction and employ a dual-channel design to effectively integrate local and global contextual information. \textit{PointDico} achieves a new state-of-the-art in 3D representation learning, \textit{e.g.}, \textbf{94.32\%} accuracy on ScanObjectNN, \textbf{86.5\%} Inst. mIoU on ShapeNetPart.

</details>


### [40] [Bi^2MAC: Bimodal Bi-Adaptive Mask-Aware Convolution for Remote Sensing Pansharpening](https://arxiv.org/abs/2512.08331)
*Xianghong Xiao,Zeyu Xia,Zhou Fei,Jinliang Xiao,Haorui Chen,Liangjian Deng*

Main category: cs.CV

TL;DR: BiÂ²MAC: A bimodal bi-adaptive mask-aware convolution method for pansharpening that uses soft/hard masks to route heterogeneous and redundant features to separate processing branches, achieving SOTA performance with lower computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning pansharpening methods struggle with regional heterogeneity in feature representations. Adaptive convolution methods address this but suffer from high computational costs and limited effectiveness in capturing heterogeneous regions in remote sensing images.

Method: Proposes Bimodal Bi-Adaptive Mask-Aware Convolution (BiÂ²MAC) with a lightweight mask generation module that creates both soft and hard masks. Soft masks modulate input features preliminarily, while hard masks guide different region types into separate branches: redundant features go to a compact branch for low-cost global processing, and heterogeneous features go to a focused branch for fine-grained modeling with more computational resources.

Result: Extensive experiments on multiple benchmark datasets show BiÂ²MAC achieves state-of-the-art performance while requiring substantially lower training time and parameter counts, with minimal computational cost among adaptive convolution models.

Conclusion: BiÂ²MAC effectively exploits information from different region types while intelligently allocating computational resources, overcoming limitations of conventional adaptive convolution methods for pansharpening tasks.

Abstract: Pansharpening aims to fuse a high-resolution panchromatic (PAN) image with a low-resolution multispectral (LRMS) image to generate a high-resolution multispectral image (HRMS). Conventional deep learning-based methods are inherently limited in their ability to adapt to regional heterogeneity within feature representations. Although various adaptive convolution methods have been proposed to address this limitation, they often suffer from excessive computational costs and a limited ability to capture heterogeneous regions in remote sensing images effectively. To overcome these challenges, we propose Bimodal Bi-Adaptive Mask-Aware Convolution (Bi^2MAC), which effectively exploits information from different types of regions while intelligently allocating computational resources. Specifically, we design a lightweight module to generate both soft and hard masks, which are used to modulate the input features preliminarily and to guide different types of regions into separate processing branches, respectively. Redundant features are directed to a compact branch for low-cost global processing. In contrast, heterogeneous features are routed to a focused branch that invests more computational resources for fine-grained modeling. Extensive experiments on multiple benchmark datasets demonstrate that Bi^2MAC achieves state-of-the-art (SOTA) performance while requiring substantially lower training time and parameter counts, and the minimal computational cost among adaptive convolution models.

</details>


### [41] [HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting](https://arxiv.org/abs/2512.08334)
*Chang Liu,Hongliang Yuan,Lianghao Zhang,Sichao Wang,Jianwei Guo,Shi-Sheng Huang*

Main category: cs.CV

TL;DR: HybridSplat: A hybrid splatting method that accelerates rendering of complex reflective scenes using reflection-baked Gaussian tracing and tile-based splatting, achieving 7x speedup with 4x fewer primitives.


<details>
  <summary>Details</summary>
Motivation: Current 3D Gaussian splatting methods for rendering complex reflections face bottlenecks in rendering speed and memory storage, especially for photorealistic novel view synthesis of reflective scenes.

Method: Proposes HybridSplat with reflection-baked Gaussian tracing that bakes view-dependent reflection within each Gaussian primitive, uses tile-based Gaussian splatting for reflection rendering, integrates reflective and base Gaussian primitives in unified framework, and employs pipeline-level acceleration with reflection-sensitive Gaussian pruning.

Result: Achieves ~7x rendering speed acceleration across complex reflective scenes from Ref-NeRF and NeRF-Casting with 4x fewer Gaussian primitives compared to similar ray-tracing based Gaussian splatting baselines.

Conclusion: HybridSplat serves as a new state-of-the-art method for complex reflective scenes, offering faster rendering speed, lower memory storage while preserving reflection rendering quality.

Abstract: Rendering complex reflection of real-world scenes using 3D Gaussian splatting has been a quite promising solution for photorealistic novel view synthesis, but still faces bottlenecks especially in rendering speed and memory storage. This paper proposes a new Hybrid Splatting(HybridSplat) mechanism for Gaussian primitives. Our key idea is a new reflection-baked Gaussian tracing, which bakes the view-dependent reflection within each Gaussian primitive while rendering the reflection using tile-based Gaussian splatting. Then we integrate the reflective Gaussian primitives with base Gaussian primitives using a unified hybrid splatting framework for high-fidelity scene reconstruction. Moreover, we further introduce a pipeline-level acceleration for the hybrid splatting, and reflection-sensitive Gaussian pruning to reduce the model size, thus achieving much faster rendering speed and lower memory storage while preserving the reflection rendering quality. By extensive evaluation, our HybridSplat accelerates about 7x rendering speed across complex reflective scenes from Ref-NeRF, NeRF-Casting with 4x fewer Gaussian primitives than similar ray-tracing based Gaussian splatting baselines, serving as a new state-of-the-art method especially for complex reflective scenes.

</details>


### [42] [DINO-BOLDNet: A DINOv3-Guided Multi-Slice Attention Network for T1-to-BOLD Generation](https://arxiv.org/abs/2512.08337)
*Jianwei Wang,Qing Wang,Menglan Ruan,Rongjun Ge,Chunfeng Yang,Yang Chen,Chunming Xie*

Main category: cs.CV

TL;DR: DINO-BOLDNet generates BOLD fMRI images from T1w structural MRI using DINOv3-guided multi-slice attention, achieving better quality than GAN baselines.


<details>
  <summary>Details</summary>
Motivation: To recover missing BOLD fMRI information when BOLD images are corrupted or unavailable, enabling downstream analysis tasks that require functional imaging data.

Method: Uses frozen self-supervised DINOv3 encoder for structural representations, slice-attention module for cross-slice context, multi-scale decoder for functional contrast restoration, and DINO-based perceptual loss for structural/textural consistency.

Result: Outperforms conditional GAN baseline in both PSNR and MS-SSIM metrics on clinical dataset of 248 subjects. First framework capable of generating mean BOLD images directly from T1w images.

Conclusion: Demonstrates the potential of self-supervised transformer guidance for structural-to-functional mapping in neuroimaging, offering a solution for BOLD image recovery when functional data is unavailable.

Abstract: Generating BOLD images from T1w images offers a promising solution for recovering missing BOLD information and enabling downstream tasks when BOLD images are corrupted or unavailable. Motivated by this, we propose DINO-BOLDNet, a DINOv3-guided multi-slice attention framework that integrates a frozen self-supervised DINOv3 encoder with a lightweight trainable decoder. The model uses DINOv3 to extract within-slice structural representations, and a separate slice-attention module to fuse contextual information across neighboring slices. A multi-scale generation decoder then restores fine-grained functional contrast, while a DINO-based perceptual loss encourages structural and textural consistency between predictions and ground-truth BOLD in the transformer feature space. Experiments on a clinical dataset of 248 subjects show that DINO-BOLDNet surpasses a conditional GAN baseline in both PSNR and MS-SSIM. To our knowledge, this is the first framework capable of generating mean BOLD images directly from T1w images, highlighting the potential of self-supervised transformer guidance for structural-to-functional mapping.

</details>


### [43] [TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels](https://arxiv.org/abs/2512.08358)
*Jiahao Lu,Weitao Xiong,Jiacheng Deng,Peng Li,Tianyu Huang,Zhiyang Dou,Cheng Lin,Sai-Kit Yeung,Yuan Liu*

Main category: cs.CV

TL;DR: TrackingWorld is a novel pipeline for dense 3D tracking of almost all pixels in world-centric coordinates from monocular video, addressing limitations in separating camera motion and tracking newly emerging dynamic subjects.


<details>
  <summary>Details</summary>
Motivation: Existing monocular 3D tracking methods fail to properly separate camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in videos. These limitations hinder accurate world-centric 3D tracking.

Method: Three-stage pipeline: 1) Tracking upsampler lifts sparse 2D tracks to dense 2D tracks, 2) Generalization to newly emerging objects by applying upsampler to all frames and eliminating redundant tracks in overlapped regions, 3) Optimization-based framework back-projects dense 2D tracks into world-centric 3D trajectories by estimating camera poses and 3D coordinates.

Result: Extensive evaluations on both synthetic and real-world datasets demonstrate that the system achieves accurate and dense 3D tracking in a world-centric coordinate frame.

Conclusion: TrackingWorld successfully addresses the two key limitations of existing methods by enabling dense 3D tracking of almost all pixels within a world-centric coordinate system, separating camera motion from dynamic motion and handling newly emerging subjects.

Abstract: Monocular 3D tracking aims to capture the long-term motion of pixels in 3D space from a single monocular video and has witnessed rapid progress in recent years. However, we argue that the existing monocular 3D tracking methods still fall short in separating the camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in the videos. To address these two limitations, we propose TrackingWorld, a novel pipeline for dense 3D tracking of almost all pixels within a world-centric 3D coordinate system. First, we introduce a tracking upsampler that efficiently lifts the arbitrary sparse 2D tracks into dense 2D tracks. Then, to generalize the current tracking methods to newly emerging objects, we apply the upsampler to all frames and reduce the redundancy of 2D tracks by eliminating the tracks in overlapped regions. Finally, we present an efficient optimization-based framework to back-project dense 2D tracks into world-centric 3D trajectories by estimating the camera poses and the 3D coordinates of these 2D tracks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our system achieves accurate and dense 3D tracking in a world-centric coordinate frame.

</details>


### [44] [SCU-CGAN: Enhancing Fire Detection through Synthetic Fire Image Generation and Dataset Augmentation](https://arxiv.org/abs/2512.08362)
*Ju-Young Kim,Ji-Hong Park,Gun-Woo Kim*

Main category: cs.CV

TL;DR: SCU-CGAN model generates realistic fire images from non-fire images to augment limited fire datasets, improving fire detection model performance by 56.5% in mAP@0.5:0.95 for YOLOv5 nano.


<details>
  <summary>Details</summary>
Motivation: Fire detection is crucial for safety, but limited fire datasets constrain detection model performance. Home IoT systems need better fire detection capabilities, requiring more diverse training data.

Method: Proposed SCU-CGAN model integrates U-Net, CBAM (Channel and Spatial Attention Module), and an additional discriminator to generate realistic fire images from non-fire images, creating augmented datasets for training.

Result: SCU-CGAN outperforms existing models with 41.5% improvement in KID score over CycleGAN. Augmented dataset improves YOLOv5 nano's mAP@0.5:0.95 by 56.5% without model structure changes.

Conclusion: SCU-CGAN effectively generates high-quality fire images to overcome dataset limitations, significantly enhancing fire detection model performance for IoT applications.

Abstract: Fire has long been linked to human life, causing severe disasters and losses. Early detection is crucial, and with the rise of home IoT technologies, household fire detection systems have emerged. However, the lack of sufficient fire datasets limits the performance of detection models. We propose the SCU-CGAN model, which integrates U-Net, CBAM, and an additional discriminator to generate realistic fire images from nonfire images. We evaluate the image quality and confirm that SCU-CGAN outperforms existing models. Specifically, SCU-CGAN achieved a 41.5% improvement in KID score compared to CycleGAN, demonstrating the superior quality of the generated fire images. Furthermore, experiments demonstrate that the augmented dataset significantly improves the accuracy of fire detection models without altering their structure. For the YOLOv5 nano model, the most notable improvement was observed in the mAP@0.5:0.95 metric, which increased by 56.5%, highlighting the effectiveness of the proposed approach.

</details>


### [45] [The Unseen Bias: How Norm Discrepancy in Pre-Norm MLLMs Leads to Visual Information Loss](https://arxiv.org/abs/2512.08374)
*Bozhou Li,Xinda Xue,Sihan Yang,Yang Shi,Xinlong Chen,Yushuo Guan,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: The paper identifies a norm disparity issue in MLLMs where visual tokens have much higher norms than text tokens, causing asymmetric update dynamics that impair cross-modal fusion. The authors propose adding a single LayerNorm after the visual projector to align norms, which improves both multimodal and text-only performance.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs using Pre-Norm architecture suffer from a severe norm disparity between high-norm visual tokens and low-norm text tokens. This imbalance isn't static but creates asymmetric update dynamics where visual tokens update much slower than text tokens, fundamentally impairing effective cross-modal feature fusion.

Method: The authors first provide formal theoretical analysis showing the norm disparity induces "representational inertia" in visual tokens. They then propose a simple solution: inserting a single, carefully initialized LayerNorm layer after the visual projector to enforce norm alignment between visual and text tokens.

Result: Empirical validation across mainstream MLLMs confirms the prevalence of norm disparity and asymmetric update rates. Experiments on LLaVA-1.5 show significant performance gains on multimodal benchmarks and notably also on text-only evaluations like MMLU, indicating more holistic model capability.

Conclusion: The norm disparity in MLLMs is a critical architectural flaw that impairs cross-modal fusion. A simple LayerNorm intervention effectively resolves this imbalance, leading to better multimodal understanding and surprisingly improved text-only performance, suggesting the solution creates more balanced and capable models.

Abstract: Multimodal Large Language Models (MLLMs), which couple pre-trained vision encoders and language models, have shown remarkable capabilities. However, their reliance on the ubiquitous Pre-Norm architecture introduces a subtle yet critical flaw: a severe norm disparity between the high-norm visual tokens and the low-norm text tokens. In this work, we present a formal theoretical analysis demonstrating that this imbalance is not a static issue. Instead, it induces an ``asymmetric update dynamic,'' where high-norm visual tokens exhibit a ``representational inertia,'' causing them to transform semantically much slower than their textual counterparts. This fundamentally impairs effective cross-modal feature fusion. Our empirical validation across a range of mainstream MLLMs confirms that this theoretical dynamic -- the persistence of norm disparity and the resulting asymmetric update rates -- is a prevalent phenomenon. Based on this insight, we propose a remarkably simple yet effective solution: inserting a single, carefully initialized LayerNorm layer after the visual projector to enforce norm alignment. Experiments conducted on the LLaVA-1.5 architecture show that this intervention yields significant performance gains not only on a wide suite of multimodal benchmarks but also, notably, on text-only evaluations such as MMLU, suggesting that resolving the architectural imbalance leads to a more holistically capable model.

</details>


### [46] [Simultaneous Enhancement and Noise Suppression under Complex Illumination Conditions](https://arxiv.org/abs/2512.08378)
*Jing Tao,You Li,Banglei Guan,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: A novel framework for simultaneous image enhancement and noise suppression under complex illumination conditions using gradient-domain weighted guided filtering, Retinex decomposition, and multi-exposure fusion.


<details>
  <summary>Details</summary>
Motivation: Existing image enhancement methods either amplify noise excessively or only work well under specific lighting conditions, limiting their effectiveness in challenging illumination scenarios common in real-world applications.

Method: 1. Uses gradient-domain weighted guided filter (GDWGIF) for accurate illumination estimation; 2. Applies Retinex model to decompose image into illumination and reflection layers; 3. Processes layers in parallel - correcting illumination layer and enhancing reflection layer; 4. Optimizes dynamic range through multi-exposure fusion and linear stretching.

Result: Experimental evaluation on real-world datasets shows the proposed method achieves better performance than state-of-the-art methods in both contrast enhancement and noise suppression.

Conclusion: The proposed framework effectively addresses the limitations of existing methods by simultaneously handling image enhancement and noise suppression under complex illumination conditions, making it suitable for practical vision-based applications.

Abstract: Under challenging light conditions, captured images often suffer from various degradations, leading to a decline in the performance of vision-based applications. Although numerous methods have been proposed to enhance image quality, they either significantly amplify inherent noise or are only effective under specific illumination conditions. To address these issues, we propose a novel framework for simultaneous enhancement and noise suppression under complex illumination conditions. Firstly, a gradient-domain weighted guided filter (GDWGIF) is employed to accurately estimate illumination and improve image quality. Next, the Retinex model is applied to decompose the captured image into separate illumination and reflection layers. These layers undergo parallel processing, with the illumination layer being corrected to optimize lighting conditions and the reflection layer enhanced to improve image quality. Finally, the dynamic range of the image is optimized through multi-exposure fusion and a linear stretching strategy. The proposed method is evaluated on real-world datasets obtained from practical applications. Experimental results demonstrate that our proposed method achieves better performance compared to state-of-the-art methods in both contrast enhancement and noise suppression.

</details>


### [47] [Detection of Digital Facial Retouching utilizing Face Beauty Information](https://arxiv.org/abs/2512.08397)
*Philipp Srock,Juan E. Tapia,Christoph Busch*

Main category: cs.CV

TL;DR: This paper studies facial retouching detection in biometric systems, analyzing beauty assessment algorithms and AI-based feature extraction to improve detection rates, achieving 1.1% D-EER on single image detection.


<details>
  <summary>Details</summary>
Motivation: Facial retouching is widely used for beautification but becomes problematic when retouched images are used as biometric samples, as it challenges face recognition systems. There's a growing need to detect facial retouching to maintain biometric system integrity.

Method: The study analyzes changes in beauty assessment algorithms on retouched images, assesses different AI-based feature extraction methods for retouching detection, and evaluates whether face beauty metrics can enhance detection rates. The approach works in scenarios where the attacking retouching algorithm is unknown.

Result: The proposed method achieved 1.1% D-EER (Detection Equal Error Rate) on single image detection, demonstrating effective performance in detecting facial retouching even when the specific retouching algorithm is unknown.

Conclusion: Facial retouching detection is crucial for biometric system security, and leveraging beauty assessment algorithms and AI-based feature extraction can effectively detect retouched images, with promising results shown by the 1.1% D-EER performance.

Abstract: Facial retouching to beautify images is widely spread in social media, advertisements, and it is even applied in professional photo studios to let individuals appear younger, remove wrinkles and skin impurities. Generally speaking, this is done to enhance beauty. This is not a problem itself, but when retouched images are used as biometric samples and enrolled in a biometric system, it is one. Since previous work has proven facial retouching to be a challenge for face recognition systems,the detection of facial retouching becomes increasingly necessary. This work proposes to study and analyze changes in beauty assessment algorithms of retouched images, assesses different feature extraction methods based on artificial intelligence in order to improve retouching detection, and evaluates whether face beauty can be exploited to enhance the detection rate. In a scenario where the attacking retouching algorithm is unknown, this work achieved 1.1% D-EER on single image detection.

</details>


### [48] [Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries](https://arxiv.org/abs/2512.08400)
*Samitha Nuwan Thilakarathna,Ercan Avsar,Martin Mathias Nielsen,Malte Pedersen*

Main category: cs.CV

TL;DR: The paper develops an optimized deep learning pipeline for automated fish re-identification using the AutoFish dataset, achieving 90.43% Rank-1 accuracy with Vision Transformers outperforming CNNs through hard triplet mining and custom image transformations.


<details>
  <summary>Details</summary>
Motivation: Electronic Monitoring systems in fisheries generate more video data than can be manually reviewed, creating a need for automated fish re-identification to support sustainable marine resource management.

Method: Developed an optimized deep learning pipeline using the AutoFish dataset (simulating EM systems with conveyor belts). Employed hard triplet mining with custom image transformation pipeline including dataset-specific normalization. Compared Vision Transformer-based Swin-T architecture against CNN-based ResNet-50.

Result: Swin-T consistently outperformed ResNet-50, achieving peak performance of 41.65% mAP@k and 90.43% Rank-1 accuracy. Analysis revealed intra-species errors (distinguishing visually similar individuals of same species) as primary challenge, with viewpoint inconsistency more detrimental than partial occlusion.

Conclusion: The optimized pipeline with hard triplet mining and custom transformations enables effective automated fish re-identification for EM systems, with Vision Transformers showing superior performance over CNNs. The main challenge remains distinguishing similar-looking individuals within the same species.

Abstract: Accurate fisheries data are crucial for effective and sustainable marine resource management. With the recent adoption of Electronic Monitoring (EM) systems, more video data is now being collected than can be feasibly reviewed manually. This paper addresses this challenge by developing an optimized deep learning pipeline for automated fish re-identification (Re-ID) using the novel AutoFish dataset, which simulates EM systems with conveyor belts with six similarly looking fish species. We demonstrate that key Re-ID metrics (R1 and mAP@k) are substantially improved by using hard triplet mining in conjunction with a custom image transformation pipeline that includes dataset-specific normalization. By employing these strategies, we demonstrate that the Vision Transformer-based Swin-T architecture consistently outperforms the Convolutional Neural Network-based ResNet-50, achieving peak performance of 41.65% mAP@k and 90.43% Rank-1 accuracy. An in-depth analysis reveals that the primary challenge is distinguishing visually similar individuals of the same species (Intra-species errors), where viewpoint inconsistency proves significantly more detrimental than partial occlusion. The source code and documentation are available at: https://github.com/msamdk/Fish_Re_Identification.git

</details>


### [49] [SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos](https://arxiv.org/abs/2512.08406)
*Mingqi Gao,Yunqi Miao,Jungong Han*

Main category: cs.CV

TL;DR: SAM-Body4D is a training-free framework for temporally consistent and occlusion-robust human mesh recovery from videos, leveraging video segmentation and occlusion-aware refinement without extra training.


<details>
  <summary>Details</summary>
Motivation: Current image-based HMR methods like SAM 3D Body work well on single images but suffer from temporal inconsistency and degraded performance under occlusions when applied to videos due to per-frame inference.

Method: 1) Generate identity-consistent masklets using promptable video segmentation, 2) Refine them with Occlusion-Aware module to recover missing regions, 3) Use refined masklets to guide SAM 3D Body for consistent mesh trajectories, 4) Padding-based parallel strategy for efficient multi-human inference.

Result: Achieves improved temporal stability and robustness in challenging in-the-wild videos without any retraining, demonstrating effectiveness in handling occlusions and maintaining consistency.

Conclusion: SAM-Body4D provides a training-free solution for temporally consistent and occlusion-robust human mesh recovery from videos by leveraging inherent human continuity and video segmentation techniques.

Abstract: Human Mesh Recovery (HMR) aims to reconstruct 3D human pose and shape from 2D observations and is fundamental to human-centric understanding in real-world scenarios. While recent image-based HMR methods such as SAM 3D Body achieve strong robustness on in-the-wild images, they rely on per-frame inference when applied to videos, leading to temporal inconsistency and degraded performance under occlusions. We address these issues without extra training by leveraging the inherent human continuity in videos. We propose SAM-Body4D, a training-free framework for temporally consistent and occlusion-robust HMR from videos. We first generate identity-consistent masklets using a promptable video segmentation model, then refine them with an Occlusion-Aware module to recover missing regions. The refined masklets guide SAM 3D Body to produce consistent full-body mesh trajectories, while a padding-based parallel strategy enables efficient multi-human inference. Experimental results demonstrate that SAM-Body4D achieves improved temporal stability and robustness in challenging in-the-wild videos, without any retraining. Our code and demo are available at: https://github.com/gaomingqi/sam-body4d.

</details>


### [50] [Towards Effective and Efficient Long Video Understanding of Multimodal Large Language Models via One-shot Clip Retrieval](https://arxiv.org/abs/2512.08410)
*Tao Chen,Shaobo Ju,Qiong Wu,Chenxin Fang,Kun Zhang,Jun Peng,Hui Li,Yiyi Zhou,Rongrong Ji*

Main category: cs.CV

TL;DR: OneClip-RAG: A novel video retrieval augmentation method that enables MLLMs to process long videos efficiently by using one-shot video clips for retrieval, with query-guided chunking and progressive training.


<details>
  <summary>Details</summary>
Motivation: Current Multimodal Large Language Models (MLLMs) struggle with long videos due to excessive memory overhead, limiting them to processing only limited frames. There's a need for an efficient method to handle long video understanding without compromising performance.

Method: Proposes One-shot video-Clip based Retrieval AuGmentation (OneClip-RAG) with: 1) Query-guided video chunking algorithm that unifies clip chunking and cross-modal retrieval in one step, 2) Creation of SynLongVideo dataset for training, 3) Progressive training regime, 4) Plug-and-play integration with existing MLLMs.

Result: Significant performance gains: boosts InternLV2 8B and Qwen2-VL 7B to GPT-4o level on MLVU benchmark. Superior efficiency: enables LLaVA-Video to understand up to 1-hour videos in <2.2 minutes on single 4090 GPU. Outperforms existing video RAG methods in both knowledge integrity and semantic coherence.

Conclusion: OneClip-RAG effectively addresses the memory limitations of MLLMs for long video processing, achieving state-of-the-art performance with remarkable efficiency, making long-video understanding practical on consumer-grade hardware.

Abstract: Due to excessive memory overhead, most Multimodal Large Language Models (MLLMs) can only process videos of limited frames. In this paper, we propose an effective and efficient paradigm to remedy this shortcoming, termed One-shot video-Clip based Retrieval AuGmentation (OneClip-RAG). Compared with existing video RAG methods, OneClip-RAG makes full use of the merits of video clips for augmented video understanding in terms of both knowledge integrity and semantic coherence. Besides, it is also equipped with a novel query-guided video chunking algorithm that can unify clip chunking and cross-modal retrieval in one processing step, avoiding redundant computations. To improve instruction following, we further propose a new dataset called SynLongVideo and design a progressive training regime for OneClip-RAG. OneClip-RAG is plugged into five recent MLLMs and validated on a set of long-video benchmarks. Experimental results not only show the obvious performance gains by OneClip-RAG over MLLMs, e.g., boosting InternLV2 8B and Qwen2-VL 7B to the level of GPT-4o on MLVU, but also show its superior efficiency in handling long videos. e.g., enabling LLaVA-Video understand up to an hour of videos in less than 2.2 minutes on a single 4090 GPU.

</details>


### [51] [SDT-6D: Fully Sparse Depth-Transformer for Staged End-to-End 6D Pose Estimation in Industrial Multi-View Bin Picking](https://arxiv.org/abs/2512.08430)
*Nico Leuze,Maximilian Hoh,Samed DoÄan,Nicolas R. -PeÃ±a,Alfred Schoettl*

Main category: cs.CV

TL;DR: A depth-only 6D pose estimation method for cluttered bin-picking using multi-view depth fusion, staged heatmap attention, and sparse transformers to handle occlusions and textureless objects.


<details>
  <summary>Details</summary>
Motivation: 6D pose estimation in industrial bin-picking is challenging due to occlusions, reflections, and textureless parts in densely packed environments. Existing methods struggle with these conditions, creating a need for robust solutions.

Method: Uses multi-view depth maps fused into fine-grained 3D point clouds or sparse TSDFs. Features staged heatmap mechanism for scene-adaptive attention priors and density-aware sparse transformer blocks to handle self-occlusions and non-uniform 3D data distribution. Operates fully sparse with per-voxel voting strategy for simultaneous pose predictions.

Result: Validated on IPD and MV-YCB datasets, demonstrating competitive performance in heavily cluttered industrial and household bin-picking scenarios. Enables high-resolution volumetric representations while keeping memory requirements feasible.

Conclusion: The proposed holistic depth-only approach effectively addresses 6D pose estimation challenges in cluttered environments through sparse representations, staged attention mechanisms, and novel voting strategies, showing promise for robotic applications.

Abstract: Accurately recovering 6D poses in densely packed industrial bin-picking environments remain a serious challenge, owing to occlusions, reflections, and textureless parts. We introduce a holistic depth-only 6D pose estimation approach that fuses multi-view depth maps into either a fine-grained 3D point cloud in its vanilla version, or a sparse Truncated Signed Distance Field (TSDF). At the core of our framework lies a staged heatmap mechanism that yields scene-adaptive attention priors across different resolutions, steering computation toward foreground regions, thus keeping memory requirements at high resolutions feasible. Along, we propose a density-aware sparse transformer block that dynamically attends to (self-) occlusions and the non-uniform distribution of 3D data. While sparse 3D approaches has proven effective for long-range perception, its potential in close-range robotic applications remains underexplored. Our framework operates fully sparse, enabling high-resolution volumetric representations to capture fine geometric details crucial for accurate pose estimation in clutter. Our method processes the entire scene integrally, predicting the 6D pose via a novel per-voxel voting strategy, allowing simultaneous pose predictions for an arbitrary number of target objects. We validate our method on the recently published IPD and MV-YCB multi-view datasets, demonstrating competitive performance in heavily cluttered industrial and household bin picking scenarios.

</details>


### [52] [LapFM: A Laparoscopic Segmentation Foundation Model via Hierarchical Concept Evolving Pre-training](https://arxiv.org/abs/2512.08439)
*Qing Xu,Kun Yuan,Yuxiang Luo,Yuhao Zhai,Wenting Duan,Nassir Navab,Zhen Chen*

Main category: cs.CV

TL;DR: LapFM is a surgical foundation model for universal laparoscopic segmentation that learns from massive unlabeled images using hierarchical concept evolving pre-training, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current surgical segmentation approaches struggle with annotation scarcity and semantic inconsistency across diverse procedures. Existing methods fine-tune natural foundation models with limited supervision, acting as domain adapters rather than true surgical foundation models, limiting their generalization across variable surgical targets.

Method: LapFM uses Hierarchical Concept Evolving Pre-training: 1) Creates Laparoscopic Concept Hierarchy (LCH) with hierarchical mask decoder and parent-child query embeddings to unify diverse entities (Anatomy, Tissue, Instrument) into scalable knowledge structure; 2) Implements Confidence-driven Evolving Labeling that iteratively generates and filters pseudo-labels based on hierarchical consistency, progressively incorporating reliable samples from unlabeled images.

Result: Creates LapBench-114K benchmark with 114K image-mask pairs. Extensive experiments show LapFM significantly outperforms state-of-the-art methods and establishes new standards for granularity-adaptive generalization in universal laparoscopic segmentation.

Conclusion: LapFM successfully bridges the gap for surgical foundation models by evolving robust segmentation capabilities from massive unlabeled surgical images, demonstrating superior performance and generalization compared to existing approaches.

Abstract: Surgical segmentation is pivotal for scene understanding yet remains hindered by annotation scarcity and semantic inconsistency across diverse procedures. Existing approaches typically fine-tune natural foundation models (e.g., SAM) with limited supervision, functioning merely as domain adapters rather than surgical foundation models. Consequently, they struggle to generalize across the vast variability of surgical targets. To bridge this gap, we present LapFM, a foundation model designed to evolve robust segmentation capabilities from massive unlabeled surgical images. Distinct from medical foundation models relying on inefficient self-supervised proxy tasks, LapFM leverages a Hierarchical Concept Evolving Pre-training paradigm. First, we establish a Laparoscopic Concept Hierarchy (LCH) via a hierarchical mask decoder with parent-child query embeddings, unifying diverse entities (i.e., Anatomy, Tissue, and Instrument) into a scalable knowledge structure with cross-granularity semantic consistency. Second, we propose a Confidence-driven Evolving Labeling that iteratively generates and filters pseudo-labels based on hierarchical consistency, progressively incorporating reliable samples from unlabeled images into training. This process yields LapBench-114K, a large-scale benchmark comprising 114K image-mask pairs. Extensive experiments demonstrate that LapFM significantly outperforms state-of-the-art methods, establishing new standards for granularity-adaptive generalization in universal laparoscopic segmentation. The source code is available at https://github.com/xq141839/LapFM.

</details>


### [53] [Leveraging Multispectral Sensors for Color Correction in Mobile Cameras](https://arxiv.org/abs/2512.08441)
*Luca Cogo,Marco Buzzelli,Simone Bianco,Javier Vazquez-Corral,Raimondo Schettini*

Main category: cs.CV

TL;DR: A unified learning framework for color correction that jointly uses high-res RGB and low-res multispectral sensors in an end-to-end pipeline, achieving 50% error reduction.


<details>
  <summary>Details</summary>
Motivation: Existing color correction methods treat the pipeline in separate stages and discard multispectral data early, failing to fully leverage the richer spectral information from emerging compact MS sensors for consumer devices.

Method: Proposes a unified learning-based framework that performs end-to-end color correction by jointly leveraging data from high-resolution RGB and auxiliary low-resolution MS sensors. The approach integrates the full pipeline within a single model and demonstrates flexibility by refactoring two different state-of-the-art image-to-image architectures.

Result: Extensive experiments show improved color accuracy and stability, reducing error by up to 50% compared to RGB-only and MS-driven baselines. A dedicated dataset was constructed by aggregating and repurposing publicly available spectral datasets with multiple RGB camera sensitivities.

Conclusion: The unified framework effectively leverages multispectral information for superior color correction, demonstrating the value of end-to-end joint processing of RGB and MS data. The approach is flexible, generalizable, and suitable for emerging compact spectral sensors in consumer devices.

Abstract: Recent advances in snapshot multispectral (MS) imaging have enabled compact, low-cost spectral sensors for consumer and mobile devices. By capturing richer spectral information than conventional RGB sensors, these systems can enhance key imaging tasks, including color correction. However, most existing methods treat the color correction pipeline in separate stages, often discarding MS data early in the process. We propose a unified, learning-based framework that (i) performs end-to-end color correction and (ii) jointly leverages data from a high-resolution RGB sensor and an auxiliary low-resolution MS sensor. Our approach integrates the full pipeline within a single model, producing coherent and color-accurate outputs. We demonstrate the flexibility and generality of our framework by refactoring two different state-of-the-art image-to-image architectures. To support training and evaluation, we construct a dedicated dataset by aggregating and repurposing publicly available spectral datasets, rendering under multiple RGB camera sensitivities. Extensive experiments show that our approach improves color accuracy and stability, reducing error by up to 50% compared to RGB-only and MS-driven baselines. Datasets, code, and models will be made available upon acceptance.

</details>


### [54] [Uncertainty-Aware Subset Selection for Robust Visual Explainability under Distribution Shifts](https://arxiv.org/abs/2512.08445)
*Madhav Gupta,Vishak Prasad C,Ganesh Ramakrishnan*

Main category: cs.CV

TL;DR: Subset selection methods for explaining vision models degrade under out-of-distribution (OOD) conditions, producing unreliable explanations. The paper introduces an uncertainty-guided submodular framework that improves robustness in both OOD and in-distribution settings.


<details>
  <summary>Details</summary>
Motivation: Current subset selection methods for explaining deep vision models work well in in-distribution settings but their reliability significantly degrades under out-of-distribution conditions, producing redundant, unstable, and uncertainty-sensitive explanations. There's a need for more robust explanation methods that work reliably across different data distributions.

Method: The paper introduces a framework combining submodular subset selection with layer-wise, gradient-based uncertainty estimation. It uses adaptive weight perturbations to estimate uncertainty and guides submodular optimization with these estimates to ensure diverse and informative subset selection, without requiring additional training or auxiliary models.

Result: Empirical evaluations show the framework mitigates weaknesses of existing methods under OOD scenarios while also yielding improvements in in-distribution settings. The approach produces more robust, diverse, and informative explanations compared to existing subset-based methods.

Conclusion: The findings highlight limitations of current subset-based explanation approaches and demonstrate that uncertainty-driven optimization can enhance attribution and object-level interpretability, paving the way for more transparent and trustworthy AI in real-world vision applications.

Abstract: Subset selection-based methods are widely used to explain deep vision models: they attribute predictions by highlighting the most influential image regions and support object-level explanations. While these methods perform well in in-distribution (ID) settings, their behavior under out-of-distribution (OOD) conditions remains poorly understood. Through extensive experiments across multiple ID-OOD sets, we find that reliability of the existing subset based methods degrades markedly, yielding redundant, unstable, and uncertainty-sensitive explanations. To address these shortcomings, we introduce a framework that combines submodular subset selection with layer-wise, gradient-based uncertainty estimation to improve robustness and fidelity without requiring additional training or auxiliary models. Our approach estimates uncertainty via adaptive weight perturbations and uses these estimates to guide submodular optimization, ensuring diverse and informative subset selection. Empirical evaluations show that, beyond mitigating the weaknesses of existing methods under OOD scenarios, our framework also yields improvements in ID settings. These findings highlight limitations of current subset-based approaches and demonstrate how uncertainty-driven optimization can enhance attribution and object-level interpretability, paving the way for more transparent and trustworthy AI in real-world vision applications.

</details>


### [55] [Team-Aware Football Player Tracking with SAM: An Appearance-Based Approach to Occlusion Recovery](https://arxiv.org/abs/2512.08467)
*Chamath Ranasinghe,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: A lightweight football player tracking method combining SAM segmentation with CSRT trackers and jersey color models achieves 7.6-7.7 FPS with stable memory usage, maintaining high tracking success in most scenarios but struggling with long-term occlusions.


<details>
  <summary>Details</summary>
Motivation: Football player tracking faces challenges from frequent occlusions, similar player appearances, and rapid motion in crowded scenes, requiring robust yet lightweight solutions that can operate under resource constraints.

Method: Team-aware tracking system combining Segment Anything Model (SAM) for precise initialization, CSRT trackers for continuous tracking, and HSV histogram-based appearance models using jersey colors for re-identification after occlusions.

Result: Achieves 7.6-7.7 FPS with ~1880 MB memory usage, 100% tracking success in light occlusions, 90% success in crowded penalty-box scenarios with 5+ players, and 50% recovery from heavy occlusions using appearance-based re-identification.

Conclusion: SAM+CSRT combination provides consistent performance across crowd densities but struggles with long-term occlusions (only 8.66% re-acquisition success). Classical tracker-based methods work well with continuous visibility but need stronger re-identification for extended absences, offering practical guidelines for resource-constrained football tracking deployment.

Abstract: Football player tracking is challenged by frequent occlusions, similar appearances, and rapid motion in crowded scenes. This paper presents a lightweight SAM-based tracking method combining the Segment Anything Model (SAM) with CSRT trackers and jersey color-based appearance models. We propose a team-aware tracking system that uses SAM for precise initialization and HSV histogram-based re-identification to improve occlusion recovery. Our evaluation measures three dimensions: processing speed (FPS and memory), tracking accuracy (success rate and box stability), and robustness (occlusion recovery and identity consistency). Experiments on football video sequences show that the approach achieves 7.6-7.7 FPS with stable memory usage (~1880 MB), maintaining 100 percent tracking success in light occlusions and 90 percent in crowded penalty-box scenarios with 5 or more players. Appearance-based re-identification recovers 50 percent of heavy occlusions, demonstrating the value of domain-specific cues. Analysis reveals key trade-offs: the SAM + CSRT combination provides consistent performance across crowd densities but struggles with long-term occlusions where players leave the frame, achieving only 8.66 percent re-acquisition success. These results offer practical guidelines for deploying football tracking systems under resource constraints, showing that classical tracker-based methods work well with continuous visibility but require stronger re-identification mechanisms for extended absences.

</details>


### [56] [ContextDrag: Precise Drag-Based Image Editing via Context-Preserving Token Injection and Position-Consistent Attention](https://arxiv.org/abs/2512.08477)
*Huiguo He,Pengyu Yan,Ziqi Yi,Weizhi Zhong,Zheng Liu,Yejun Tang,Huan Yang,Kun Gai,Guanbin Li,Lianwen Jin*

Main category: cs.CV

TL;DR: ContextDrag is a new drag-based image editing method that leverages contextual modeling from editing models like FLUX-Kontext to preserve fine-grained texture details and improve coherence without needing finetuning or inversion.


<details>
  <summary>Details</summary>
Motivation: Existing drag-based image editing methods fail to fully exploit contextual information from reference images, particularly fine-grained texture details, resulting in edits with limited coherence and fidelity.

Method: ContextDrag introduces two key innovations: 1) Context-preserving Token Injection (CTI) that injects noise-free reference features via Latent-space Reverse Mapping (LRM) algorithm, and 2) Position-Consistent Attention (PCA) that re-encodes reference tokens with overlap-aware masking to eliminate irrelevant feature interference.

Result: Extensive experiments on DragBench-SR and DragBench-DR benchmarks demonstrate that ContextDrag surpasses all existing state-of-the-art methods.

Conclusion: ContextDrag provides a new paradigm for drag-based editing that effectively leverages contextual modeling capabilities to achieve precise drag control while preserving semantic and texture consistency, without requiring finetuning or inversion.

Abstract: Drag-based image editing aims to modify visual content followed by user-specified drag operations. Despite existing methods having made notable progress, they still fail to fully exploit the contextual information in the reference image, including fine-grained texture details, leading to edits with limited coherence and fidelity. To address this challenge, we introduce ContextDrag, a new paradigm for drag-based editing that leverages the strong contextual modeling capability of editing models, such as FLUX-Kontext. By incorporating VAE-encoded features from the reference image, ContextDrag can leverage rich contextual cues and preserve fine-grained details, without the need for finetuning or inversion. Specifically, ContextDrag introduced a novel Context-preserving Token Injection (CTI) that injects noise-free reference features into their correct destination locations via a Latent-space Reverse Mapping (LRM) algorithm. This strategy enables precise drag control while preserving consistency in both semantics and texture details. Second, ContextDrag adopts a novel Position-Consistent Attention (PCA), which positional re-encodes the reference tokens and applies overlap-aware masking to eliminate interference from irrelevant reference features. Extensive experiments on DragBench-SR and DragBench-DR demonstrate that our approach surpasses all existing SOTA methods. Code will be publicly available.

</details>


### [57] [Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform](https://arxiv.org/abs/2512.08478)
*Yuning Gong,Yifei Liu,Yifan Zhan,Muyao Niu,Xueying Li,Yuanjun Liao,Jiaming Chen,Yuanyuan Gao,Jiaqi Chen,Minming Chen,Li Zhou,Yuning Zhang,Wei Wang,Xiaoqing Hou,Huaxi Huang,Shixiang Tang,Le Ma,Dingwen Zhang,Xue Yang,Junchi Yan,Yanchi Zhang,Yinqiang Zheng,Xiao Sun,Zhihang Zhong*

Main category: cs.CV

TL;DR: Visionary is a web-native platform for real-time 3D Gaussian Splatting rendering with dynamic neural processing, offering plug-and-play algorithms and superior efficiency through WebGPU.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS viewer solutions are fragmented, heavy, and constrained by legacy pipelines, causing high deployment friction and limited support for dynamic content and generative models.

Method: Built on efficient WebGPU renderer with per-frame ONNX inference, introduces standardized Gaussian Generator contract for plug-and-play algorithms, and provides three.js library with TypeScript API.

Result: Achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting, supports multiple variants (MLP-based 3DGS, 4DGS, neural avatars, style transformation).

Conclusion: Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.

Abstract: Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, "click-to-run" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.

</details>


### [58] [Temporal Concept Dynamics in Diffusion Models via Prompt-Conditioned Interventions](https://arxiv.org/abs/2512.08486)
*Ada Gorgun,Fawaz Sammani,Nikos Deligiannis,Bernt Schiele,Jonas Fischer*

Main category: cs.CV

TL;DR: PCI is a training-free framework that analyzes when specific concepts "lock in" during diffusion model generation by measuring Concept Insertion Success across timesteps.


<details>
  <summary>Details</summary>
Motivation: Current diffusion model evaluation focuses only on final outputs, but understanding the dynamic generation process is crucial for analyzing controllability, reliability, and failure modes. The paper aims to answer when noise transforms into specific concepts during denoising.

Method: Proposes PCI (Prompt-Conditioned Intervention) - a training-free, model-agnostic framework that analyzes Concept Insertion Success (CIS). CIS measures the probability that a concept inserted at a given timestep is preserved in the final image, characterizing temporal dynamics of concept formation.

Result: Applied to state-of-the-art text-to-image diffusion models across diverse concepts, PCI reveals varied temporal behaviors where specific phases are more favorable to certain concepts. The framework provides actionable insights for text-driven image editing, yielding quantitatively stronger edits that balance semantic accuracy and content preservation better than baselines.

Conclusion: PCI enables analysis of concept dynamics in diffusion models without requiring model internals or training, offering both diagnostic insights about generation processes and practical benefits for image editing applications.

Abstract: Diffusion models are usually evaluated by their final outputs, gradually denoising random noise into meaningful images. Yet, generation unfolds along a trajectory, and analyzing this dynamic process is crucial for understanding how controllable, reliable, and predictable these models are in terms of their success/failure modes. In this work, we ask the question: when does noise turn into a specific concept (e.g., age) and lock in the denoising trajectory? We propose PCI (Prompt-Conditioned Intervention) to study this question. PCI is a training-free and model-agnostic framework for analyzing concept dynamics through diffusion time. The central idea is the analysis of Concept Insertion Success (CIS), defined as the probability that a concept inserted at a given timestep is preserved and reflected in the final image, offering a way to characterize the temporal dynamics of concept formation. Applied to several state-of-the-art text-to-image diffusion models and a broad taxonomy of concepts, PCI reveals diverse temporal behaviors across diffusion models, in which certain phases of the trajectory are more favorable to specific concepts even within the same concept type. These findings also provide actionable insights for text-driven image editing, highlighting when interventions are most effective without requiring access to model internals or training, and yielding quantitatively stronger edits that achieve a balance of semantic accuracy and content preservation than strong baselines. Code is available at: https://github.com/adagorgun/PCI-Prompt-Controlled-Interventions

</details>


### [59] [On-the-fly Large-scale 3D Reconstruction from Multi-Camera Rigs](https://arxiv.org/abs/2512.08498)
*Yijia Guo,Tong Hu,Zhiwei Li,Liwen Hu,Keming Qian,Xitong Lin,Shengbo Chen,Tiejun Huang,Lei Ma*

Main category: cs.CV

TL;DR: First on-the-fly 3D reconstruction framework for multi-camera rigs using 3D Gaussian Splatting, achieving drift-free trajectory estimation and efficient online reconstruction without calibration.


<details>
  <summary>Details</summary>
Motivation: Existing on-the-fly 3DGS methods for monocular RGB streams fail to achieve complete 3D coverage due to limited field of view. Multi-camera rigs address this limitation but lack real-time reconstruction frameworks.

Method: Hierarchical camera initialization for coarse inter-camera alignment without calibration, lightweight multi-camera bundle adjustment, redundancy-free Gaussian sampling strategy, and frequency-aware optimization scheduler.

Result: Reconstructs hundreds of meters of 3D scenes within 2 minutes using raw multi-camera video streams, demonstrating unprecedented speed, robustness, and fidelity for on-the-fly 3D reconstruction.

Conclusion: The method enables efficient, real-time 3D scene reconstruction from multi-camera rigs with complete 3D coverage, addressing limitations of monocular approaches while maintaining reconstruction quality.

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled efficient free-viewpoint rendering and photorealistic scene reconstruction. While on-the-fly extensions of 3DGS have shown promise for real-time reconstruction from monocular RGB streams, they often fail to achieve complete 3D coverage due to the limited field of view (FOV). Employing a multi-camera rig fundamentally addresses this limitation. In this paper, we present the first on-the-fly 3D reconstruction framework for multi-camera rigs. Our method incrementally fuses dense RGB streams from multiple overlapping cameras into a unified Gaussian representation, achieving drift-free trajectory estimation and efficient online reconstruction. We propose a hierarchical camera initialization scheme that enables coarse inter-camera alignment without calibration, followed by a lightweight multi-camera bundle adjustment that stabilizes trajectories while maintaining real-time performance. Furthermore, we introduce a redundancy-free Gaussian sampling strategy and a frequency-aware optimization scheduler to reduce the number of Gaussian primitives and the required optimization iterations, thereby maintaining both efficiency and reconstruction fidelity. Our method reconstructs hundreds of meters of 3D scenes within just 2 minutes using only raw multi-camera video streams, demonstrating unprecedented speed, robustness, and Fidelity for on-the-fly 3D scene reconstruction.

</details>


### [60] [Disrupting Hierarchical Reasoning: Adversarial Protection for Geographic Privacy in Multimodal Reasoning Models](https://arxiv.org/abs/2512.08503)
*Jiaming Zhang,Che Wang,Yang Cao,Longtao Huang,Wei Yang Bryan Lim*

Main category: cs.CV

TL;DR: ReasonBreak is an adversarial framework that disrupts hierarchical reasoning in multi-modal large reasoning models to protect geographic privacy in personal images, outperforming existing methods by 14.4% at tract-level protection.


<details>
  <summary>Details</summary>
Motivation: Multi-modal large reasoning models (MLRMs) pose significant privacy risks by inferring precise geographic locations from personal images through sophisticated hierarchical reasoning. Existing privacy protection techniques designed for perception-based models are ineffective against MLRMs' multi-step reasoning processes.

Method: ReasonBreak uses concept-aware perturbations aligned with conceptual hierarchies rather than uniform noise. It strategically targets critical conceptual dependencies within reasoning chains, generating perturbations that invalidate specific inference steps and cascade through subsequent reasoning stages. The approach is supported by GeoPrivacy-6K, a dataset of 6,341 ultra-high-resolution images with hierarchical concept annotations.

Result: Extensive evaluation across seven state-of-the-art MLRMs (including GPT-o3, GPT-5, Gemini 2.5 Pro) shows ReasonBreak achieves 14.4% improvement in tract-level protection (33.8% vs 19.4%) and nearly doubles block-level protection (33.5% vs 16.8%).

Conclusion: ReasonBreak establishes a new paradigm for privacy protection against reasoning-based threats by effectively disrupting hierarchical reasoning in MLRMs through concept-aware perturbations, addressing a critical gap in existing privacy protection techniques.

Abstract: Multi-modal large reasoning models (MLRMs) pose significant privacy risks by inferring precise geographic locations from personal images through hierarchical chain-of-thought reasoning. Existing privacy protection techniques, primarily designed for perception-based models, prove ineffective against MLRMs' sophisticated multi-step reasoning processes that analyze environmental cues. We introduce \textbf{ReasonBreak}, a novel adversarial framework specifically designed to disrupt hierarchical reasoning in MLRMs through concept-aware perturbations. Our approach is founded on the key insight that effective disruption of geographic reasoning requires perturbations aligned with conceptual hierarchies rather than uniform noise. ReasonBreak strategically targets critical conceptual dependencies within reasoning chains, generating perturbations that invalidate specific inference steps and cascade through subsequent reasoning stages. To facilitate this approach, we contribute \textbf{GeoPrivacy-6K}, a comprehensive dataset comprising 6,341 ultra-high-resolution images ($\geq$2K) with hierarchical concept annotations. Extensive evaluation across seven state-of-the-art MLRMs (including GPT-o3, GPT-5, Gemini 2.5 Pro) demonstrates ReasonBreak's superior effectiveness, achieving a 14.4\% improvement in tract-level protection (33.8\% vs 19.4\%) and nearly doubling block-level protection (33.5\% vs 16.8\%). This work establishes a new paradigm for privacy protection against reasoning-based threats.

</details>


### [61] [Beyond the Noise: Aligning Prompts with Latent Representations in Diffusion Models](https://arxiv.org/abs/2512.08505)
*Vasco Ramos,Regev Cohen,Idan Szpektor,Joao Magalhaes*

Main category: cs.CV

TL;DR: NoisyCLIP enables real-time detection of text/image misalignment during diffusion model generation by measuring semantic alignment in noisy latent space, reducing computational cost by 50% while maintaining 98% of CLIP performance.


<details>
  <summary>Details</summary>
Motivation: Current conditional diffusion models suffer from misalignment and hallucinations, requiring expensive post-generation alignment assessment. The authors hypothesize that misalignments can be detected early in the denoising process to enable real-time assessment without waiting for complete generation.

Method: Proposes NoisyCLIP, a method that measures semantic alignment in the noisy latent space during the reverse diffusion process. It's the first approach to explore prompt-to-latent misalignment detection using dual encoders during image generation.

Result: NoisyCLIP reduces computational cost by 50% while achieving 98% of CLIP alignment performance in Best-of-N settings. Enables real-time alignment assessment during generation without sacrificing semantic fidelity.

Conclusion: Early detection of text/image misalignments during diffusion model generation is feasible and efficient. NoisyCLIP provides a cost-effective solution for real-time alignment assessment that maintains high semantic fidelity.

Abstract: Conditional diffusion models rely on language-to-image alignment methods to steer the generation towards semantically accurate outputs. Despite the success of this architecture, misalignment and hallucinations remain common issues and require automatic misalignment detection tools to improve quality, for example by applying them in a Best-of-N (BoN) post-generation setting. Unfortunately, measuring the alignment after the generation is an expensive step since we need to wait for the overall generation to finish to determine prompt adherence. In contrast, this work hypothesizes that text/image misalignments can be detected early in the denoising process, enabling real-time alignment assessment without waiting for the complete generation. In particular, we propose NoisyCLIP a method that measures semantic alignment in the noisy latent space. This work is the first to explore and benchmark prompt-to-latent misalignment detection during image generation using dual encoders in the reverse diffusion process. We evaluate NoisyCLIP qualitatively and quantitatively and find it reduces computational cost by 50% while achieving 98% of CLIP alignment performance in BoN settings. This approach enables real-time alignment assessment during generation, reducing costs without sacrificing semantic fidelity.

</details>


### [62] [OCCDiff: Occupancy Diffusion Model for High-Fidelity 3D Building Reconstruction from Noisy Point Clouds](https://arxiv.org/abs/2512.08506)
*Jialu Sui,Rui Liu,Hongsheng Zhang*

Main category: cs.CV

TL;DR: OCCDiff uses latent diffusion in occupancy function space to reconstruct buildings from LiDAR point clouds, handling varying densities and noise through continuous occupancy functions and multi-task training.


<details>
  <summary>Details</summary>
Motivation: Accurate building reconstruction from LiDAR point clouds is challenging due to varying point densities and noise interference, requiring flexible methods to capture high-quality 3D building profiles.

Method: OCCDiff combines latent diffusion with function autoencoder architecture to generate continuous occupancy functions. It includes a point encoder for condition features, occupancy decoder for prediction, and latent encoder for multi-modal features, with multi-task training for robust feature learning.

Result: The method generates physically consistent samples with high fidelity to target distribution and exhibits robustness to noisy data, as shown in empirical results.

Conclusion: OCCDiff effectively addresses building reconstruction challenges from LiDAR data by leveraging latent diffusion in occupancy function space with robust feature learning through multi-task training.

Abstract: A major challenge in reconstructing buildings from LiDAR point clouds lies in accurately capturing building surfaces under varying point densities and noise interference. To flexibly gather high-quality 3D profiles of the building in diverse resolution, we propose OCCDiff applying latent diffusion in the occupancy function space. Our OCCDiff combines a latent diffusion process with a function autoencoder architecture to generate continuous occupancy functions evaluable at arbitrary locations. Moreover, a point encoder is proposed to provide condition features to diffusion learning, constraint the final occupancy prediction for occupancy decoder, and insert multi-modal features for latent generation to latent encoder. To further enhance the model performance, a multi-task training strategy is employed, ensuring that the point encoder learns diverse and robust feature representations. Empirical results show that our method generates physically consistent samples with high fidelity to the target distribution and exhibits robustness to noisy data.

</details>


### [63] [Thinking with Images via Self-Calling Agent](https://arxiv.org/abs/2512.08511)
*Wenxi Yang,Yuzhong Zhao,Fang Wan,Qixiang Ye*

Main category: cs.CV

TL;DR: sCoT reformulates multimodal reasoning as language-only CoT with self-calling virtual subagents, improving performance with 75% fewer GPU hours.


<details>
  <summary>Details</summary>
Motivation: Optimizing interleaved multimodal CoT through reinforcement learning is challenging due to scarce high-quality reasoning data. Current thinking-with-images paradigms integrate visual information dynamically but face training difficulties.

Method: Proposes Self-Calling Chain-of-Thought (sCoT) where a main agent decomposes visual reasoning into atomic subtasks and invokes parameter-sharing virtual subagents to solve them in isolated context. Uses group-relative policy optimization to reinforce effective reasoning behavior.

Result: On HR-Bench 4K, sCoT improves overall reasoning performance by up to 1.9% with ~75% fewer GPU hours compared to strong baselines.

Conclusion: sCoT provides an effective and efficient visual reasoning paradigm that avoids explicit interleaving between modalities while enhancing optimization through self-calling architecture.

Abstract: Thinking-with-images paradigms have showcased remarkable visual reasoning capability by integrating visual information as dynamic elements into the Chain-of-Thought (CoT). However, optimizing interleaved multimodal CoT (iMCoT) through reinforcement learning remains challenging, as it relies on scarce high-quality reasoning data. In this study, we propose Self-Calling Chain-of-Thought (sCoT), a novel visual reasoning paradigm that reformulates iMCoT as a language-only CoT with self-calling. Specifically, a main agent decomposes the complex visual reasoning task to atomic subtasks and invokes its virtual replicas, i.e. parameter-sharing subagents, to solve them in isolated context. sCoT enjoys substantial training effectiveness and efficiency, as it requires no explicit interleaving between modalities. sCoT employs group-relative policy optimization to reinforce effective reasoning behavior to enhance optimization. Experiments on HR-Bench 4K show that sCoT improves the overall reasoning performance by up to $1.9\%$ with $\sim 75\%$ fewer GPU hours compared to strong baseline approaches. Code is available at https://github.com/YWenxi/think-with-images-through-self-calling.

</details>


### [64] [Beyond Real Weights: Hypercomplex Representations for Stable Quantization](https://arxiv.org/abs/2512.08524)
*Jawad Ibn Ahad,Maisha Rahman,Amrijit Biswas,Muhammad Rafsan Kabir,Robin Krambroeckers,Sifat Momen,Nabeel Mohammed,Shafin Rahman*

Main category: cs.CV

TL;DR: Progressive reparameterization using PHM layers compresses multimodal language models by replacing dense feed-forward blocks, achieving significant parameter/FLOP reductions while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Multimodal language models are computationally heavy and difficult to deploy efficiently due to their large parameter capacity needed to align visual features with linguistic representations.

Method: Progressive reparameterization strategy that gradually replaces dense feed-forward network blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers, using residual interpolation schedule with lightweight reconstruction and knowledge distillation losses.

Result: Substantial parameter and FLOP reductions while preserving strong multimodal alignment, enabling faster inference without degrading output quality. Maintains performance comparable to base models while delivering significant reductions in model size and inference latency.

Conclusion: Progressive PHM substitution offers an architecture-compatible path toward more efficient multimodal reasoning and complements existing low-bit quantization techniques.

Abstract: Multimodal language models (MLLMs) require large parameter capacity to align high-dimensional visual features with linguistic representations, making them computationally heavy and difficult to deploy efficiently. We introduce a progressive reparameterization strategy that compresses these models by gradually replacing dense feed-forward network blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers. A residual interpolation schedule, together with lightweight reconstruction and knowledge distillation losses, ensures that the PHM modules inherit the functional behavior of their dense counterparts during training. This transition yields substantial parameter and FLOP reductions while preserving strong multimodal alignment, enabling faster inference without degrading output quality. We evaluate the approach on multiple vision-language models (VLMs). Our method maintains performance comparable to the base models while delivering significant reductions in model size and inference latency. Progressive PHM substitution thus offers an architecture-compatible path toward more efficient multimodal reasoning and complements existing low-bit quantization techniques.

</details>


### [65] [MVP: Multiple View Prediction Improves GUI Grounding](https://arxiv.org/abs/2512.08529)
*Yunzhu Zhang,Zeyu Pan,Zhengwen Zeng,Shuheng Shen,Changhua Meng,Linchao Zhu*

Main category: cs.CV

TL;DR: MVP is a training-free framework that improves GUI grounding stability by aggregating predictions from multiple cropped views to overcome coordinate prediction instability.


<details>
  <summary>Details</summary>
Motivation: Existing GUI grounding models suffer from significant coordinate prediction instability where minor visual perturbations (like cropping a few pixels) can drastically alter predictions, flipping results between correct and incorrect. This instability severely undermines model performance, especially for high-resolution screens with small UI elements.

Method: Multi-View Prediction (MVP) framework with two components: (1) Attention-Guided View Proposal - derives diverse views guided by instruction-to-image attention scores, and (2) Multi-Coordinates Clustering - ensembles predictions by selecting the centroid of the densest spatial cluster. The approach is training-free and enhances grounding through multi-view inference.

Result: Extensive experiments show MVP's effectiveness across various models and benchmarks. On ScreenSpot-Pro, MVP boosts UI-TARS-1.5-7B to 56.1%, GTA1-7B to 61.7%, Qwen3VL-8B-Instruct to 65.3%, and Qwen3VL-32B-Instruct to 74.0%.

Conclusion: MVP effectively addresses GUI grounding instability by aggregating predictions from multiple views, demonstrating significant performance improvements across different models without requiring additional training.

Abstract: GUI grounding, which translates natural language instructions into precise pixel coordinates, is essential for developing practical GUI agents. However, we observe that existing grounding models exhibit significant coordinate prediction instability, minor visual perturbations (e.g. cropping a few pixels) can drastically alter predictions, flipping results between correct and incorrect. This instability severely undermines model performance, especially for samples with high-resolution and small UI elements. To address this issue, we propose Multi-View Prediction (MVP), a training-free framework that enhances grounding performance through multi-view inference. Our key insight is that while single-view predictions may be unstable, aggregating predictions from multiple carefully cropped views can effectively distinguish correct coordinates from outliers. MVP comprises two components: (1) Attention-Guided View Proposal, which derives diverse views guided by instruction-to-image attention scores, and (2) Multi-Coordinates Clustering, which ensembles predictions by selecting the centroid of the densest spatial cluster. Extensive experiments demonstrate MVP's effectiveness across various models and benchmarks. Notably, on ScreenSpot-Pro, MVP boosts UI-TARS-1.5-7B to 56.1%, GTA1-7B to 61.7%, Qwen3VL-8B-Instruct to 65.3%, and Qwen3VL-32B-Instruct to 74.0%. The code is available at https://github.com/ZJUSCL/MVP.

</details>


### [66] [PaintFlow: A Unified Framework for Interactive Oil Paintings Editing and Generation](https://arxiv.org/abs/2512.08534)
*Zhangli Hu,Ye Chen,Jiajun Yao,Bingbing Ni*

Main category: cs.CV

TL;DR: A unified multimodal framework for oil painting generation and editing that combines reference images, hand-drawn sketches, and text prompts while maintaining consistent painting style.


<details>
  <summary>Details</summary>
Motivation: Oil painting is challenging for digital generation/editing due to complex brushstroke dynamics and stylized characteristics. Existing methods are limited by training data distribution and focus mainly on modifying real photos rather than creating artistic oil paintings.

Method: Three key technical advances: 1) Enhanced training with spatial alignment and semantic enhancement conditioning (masks/sketches as spatial constraints, reference images/text as feature constraints); 2) Self-supervised style transfer pipeline using Stroke-Based Rendering to create large-scale paired training data by simulating oil painting restoration; 3) AdaIN operator integration during inference for stylistic consistency.

Result: The interactive system enables fine-grained editing while preserving artistic qualities of oil paintings, achieving unprecedented imagination realization in stylized oil painting generation and editing.

Conclusion: The proposed unified multimodal framework successfully addresses oil painting generation/editing challenges through innovative training strategies, data augmentation via style transfer, and inference techniques for maintaining stylistic consistency.

Abstract: Oil painting, as a high-level medium that blends human abstract thinking with artistic expression, poses substantial challenges for digital generation and editing due to its intricate brushstroke dynamics and stylized characteristics. Existing generation and editing techniques are often constrained by the distribution of training data and primarily focus on modifying real photographs. In this work, we introduce a unified multimodal framework for oil painting generation and editing. The proposed system allows users to incorporate reference images for precise semantic control, hand-drawn sketches for spatial structure alignment, and natural language prompts for high-level semantic guidance, while consistently maintaining a unified painting style across all outputs. Our method achieves interactive oil painting creation through three crucial technical advancements. First, we enhance the training stage with spatial alignment and semantic enhancement conditioning strategy, which map masks and sketches into spatial constraints, and encode contextual embedding from reference images and text into feature constraints, enabling object-level semantic alignment. Second, to overcome data scarcity, we propose a self-supervised style transfer pipeline based on Stroke-Based Rendering (SBR), which simulates the inpainting dynamics of oil painting restoration, converting real images into stylized oil paintings with preserved brushstroke textures to construct a large-scale paired training dataset. Finally, during inference, we integrate features using the AdaIN operator to ensure stylistic consistency. Extensive experiments demonstrate that our interactive system enables fine-grained editing while preserving the artistic qualities of oil paintings, achieving an unprecedented level of imagination realization in stylized oil paintings generation and editing.

</details>


### [67] [Photo3D: Advancing Photorealistic 3D Generation through Structure-Aligned Detail Enhancement](https://arxiv.org/abs/2512.08535)
*Xinyue Liang,Zhinyuan Ma,Lingchen Sun,Yanjun Guo,Lei Zhang*

Main category: cs.CV

TL;DR: Photo3D is a framework for photorealistic 3D generation that uses GPT-4o-Image generated images to enhance texture details while preserving 3D-native geometry structure.


<details>
  <summary>Details</summary>
Motivation: Current 3D-native generators produce reliable geometry but lack realistic appearances due to limited high-quality real-world 3D assets with rich texture details, which are difficult to capture due to diverse scene scales, non-rigid motions, and scanner limitations.

Method: 1) Structure-aligned multi-view synthesis pipeline to address multi-view consistency issues in generated images; 2) Detail-enhanced multi-view dataset construction; 3) Realistic detail enhancement scheme using perceptual feature adaptation and semantic structure matching; 4) Dedicated training strategies for geometry-texture coupled and decoupled 3D-native generation paradigms.

Result: Photo3D generalizes well across diverse 3D-native generation paradigms and achieves state-of-the-art photorealistic 3D generation performance.

Conclusion: The framework successfully bridges the gap between reliable geometry and realistic appearance in 3D generation by leveraging AI-generated image data while maintaining structural consistency with 3D-native geometry.

Abstract: Although recent 3D-native generators have made great progress in synthesizing reliable geometry, they still fall short in achieving realistic appearances. A key obstacle lies in the lack of diverse and high-quality real-world 3D assets with rich texture details, since capturing such data is intrinsically difficult due to the diverse scales of scenes, non-rigid motions of objects, and the limited precision of 3D scanners. We introduce Photo3D, a framework for advancing photorealistic 3D generation, which is driven by the image data generated by the GPT-4o-Image model. Considering that the generated images can distort 3D structures due to their lack of multi-view consistency, we design a structure-aligned multi-view synthesis pipeline and construct a detail-enhanced multi-view dataset paired with 3D geometry. Building on it, we present a realistic detail enhancement scheme that leverages perceptual feature adaptation and semantic structure matching to enforce appearance consistency with realistic details while preserving the structural consistency with the 3D-native geometry. Our scheme is general to different 3D-native generators, and we present dedicated training strategies to facilitate the optimization of geometry-texture coupled and decoupled 3D-native generation paradigms. Experiments demonstrate that Photo3D generalizes well across diverse 3D-native generation paradigms and achieves state-of-the-art photorealistic 3D generation performance.

</details>


### [68] [Fast-ARDiff: An Entropy-informed Acceleration Framework for Continuous Space Autoregressive Generation](https://arxiv.org/abs/2512.08537)
*Zhen Zou,Xiaoxiao Ma,Jie Huang,Zichao Yu,Feng Zhao*

Main category: cs.CV

TL;DR: Fast-ARDiff is a unified AR-diffusion framework that accelerates both AR speculative decoding and diffusion decoding through entropy-informed strategies and joint optimization, achieving 4.3Ã speedup on ImageNet 256Ã256 generation.


<details>
  <summary>Details</summary>
Motivation: AR-diffusion hybrid models suffer from high latency due to sequential AR generation and iterative denoising, creating a bottleneck that needs to be addressed for practical applications.

Method: 1) Entropy-informed speculative strategy aligns draft model's entropy with target model to reduce rejection rates. 2) Dynamic scheduler integrates diffusion into end-to-end framework, prioritizing AR optimization to guide diffusion. 3) Joint distillation combines trajectory and distribution matching for stable training with few steps. 4) Inference uses shallow feature entropy to pre-filter low-entropy drafts.

Result: Achieves state-of-the-art acceleration: 4.3Ã lossless speedup on ImageNet 256Ã256 with TransDiff, and 3Ã acceleration on text-conditioned generation with NextStep-1.

Conclusion: Fast-ARDiff successfully addresses latency bottlenecks in AR-diffusion hybrids through unified optimization, enabling efficient high-quality image generation with significant speed improvements.

Abstract: Autoregressive(AR)-diffusion hybrid paradigms combine AR's structured modeling with diffusion's photorealistic synthesis, yet suffer from high latency due to sequential AR generation and iterative denoising. In this work, we tackle this bottleneck and propose a unified AR-diffusion framework Fast-ARDiff that jointly optimizes both components, accelerating AR speculative decoding while simultaneously facilitating faster diffusion decoding. Specifically: (1) The entropy-informed speculative strategy encourages draft model to produce higher-entropy representations aligned with target model's entropy characteristics, mitigating entropy mismatch and high rejection rates caused by draft overconfidence. (2) For diffusion decoding, rather than treating it as an independent module, we integrate it into the same end-to-end framework using a dynamic scheduler that prioritizes AR optimization to guide the diffusion part in further steps. The diffusion part is optimized through a joint distillation framework combining trajectory and distribution matching, ensuring stable training and high-quality synthesis with extremely few steps. During inference, shallow feature entropy from AR module is used to pre-filter low-entropy drafts, avoiding redundant computation and improving latency. Fast-ARDiff achieves state-of-the-art acceleration across diverse models: on ImageNet 256$\times$256, TransDiff attains 4.3$\times$ lossless speedup, and NextStep-1 achieves 3$\times$ acceleration on text-conditioned generation. Code will be available at https://github.com/aSleepyTree/Fast-ARDiff.

</details>


### [69] [A Novel Wasserstein Quaternion Generative Adversarial Network for Color Image Generation](https://arxiv.org/abs/2512.08542)
*Zhigang Jia,Duan Wang,Hengkai Wang,Yajun Xie,Meixiang Zhao,Xiaoyu Zhao*

Main category: cs.CV

TL;DR: Proposes quaternion Wasserstein distance and Wasserstein quaternion GAN for better color image generation by modeling color channel correlations.


<details>
  <summary>Details</summary>
Motivation: Existing color image generation models ignore correlations among color channels, leading to chromatic aberration problems. Also, there's lack of systematic theory for measuring color image datasets.

Method: Defines new quaternion Wasserstein distance with dual theory, solves quaternion linear programming using convex set separation theorem and Farkas lemma, and proposes Wasserstein quaternion GAN.

Result: The proposed model surpasses both quaternion GANs and Wasserstein GANs in generation efficiency and image quality.

Conclusion: Quaternion Wasserstein distance effectively models color channel correlations, leading to improved color image generation performance.

Abstract: Color image generation has a wide range of applications, but the existing generation models ignore the correlation among color channels, which may lead to chromatic aberration problems. In addition, the data distribution problem of color images has not been systematically elaborated and explained, so that there is still the lack of the theory about measuring different color images datasets. In this paper, we define a new quaternion Wasserstein distance and develop its dual theory. To deal with the quaternion linear programming problem, we derive the strong duality form with helps of quaternion convex set separation theorem and quaternion Farkas lemma. With using quaternion Wasserstein distance, we propose a novel Wasserstein quaternion generative adversarial network. Experiments demonstrate that this novel model surpasses both the (quaternion) generative adversarial networks and the Wasserstein generative adversarial network in terms of generation efficiency and image quality.

</details>


### [70] [An Iteration-Free Fixed-Point Estimator for Diffusion Inversion](https://arxiv.org/abs/2512.08547)
*Yifei Chen,Kaiyu Song,Yan Pan,Jianxing Yu,Jian Yin,Hanjiang Lai*

Main category: cs.CV

TL;DR: Proposes an iteration-free fixed-point estimator for diffusion inversion that avoids the computational cost of iterative methods while maintaining reconstruction accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing fixed-point iteration methods for diffusion inversion suffer from high computational costs due to iterative nature and complex hyperparameter selection, creating a need for more efficient alternatives.

Method: Derives explicit fixed-point expression from ideal inversion step, then introduces error approximation using calculable error from previous step to approximate unknown current error, creating an unbiased low-variance estimator.

Result: Achieves consistent and superior reconstruction performance on NOCAPS and MS-COCO datasets compared to DDIM inversion and other fixed-point iteration methods, without additional iterations or training.

Conclusion: Proposed iteration-free fixed-point estimator provides efficient and accurate diffusion inversion, overcoming computational limitations of iterative approaches while maintaining reconstruction quality.

Abstract: Diffusion inversion aims to recover the initial noise corresponding to a given image such that this noise can reconstruct the original image through the denoising diffusion process. The key component of diffusion inversion is to minimize errors at each inversion step, thereby mitigating cumulative inaccuracies. Recently, fixed-point iteration has emerged as a widely adopted approach to minimize reconstruction errors at each inversion step. However, it suffers from high computational costs due to its iterative nature and the complexity of hyperparameter selection. To address these issues, we propose an iteration-free fixed-point estimator for diffusion inversion. First, we derive an explicit expression of the fixed point from an ideal inversion step. Unfortunately, it inherently contains an unknown data prediction error. Building upon this, we introduce the error approximation, which uses the calculable error from the previous inversion step to approximate the unknown error at the current inversion step. This yields a calculable, approximate expression for the fixed point, which is an unbiased estimator characterized by low variance, as shown by our theoretical analysis. We evaluate reconstruction performance on two text-image datasets, NOCAPS and MS-COCO. Compared to DDIM inversion and other inversion methods based on the fixed-point iteration, our method achieves consistent and superior performance in reconstruction tasks without additional iterations or training.

</details>


### [71] [SSCATeR: Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling for Real-Time 3D Object Detection in LiDAR Point Clouds](https://arxiv.org/abs/2512.08557)
*Alexander Dow,Manduhu Manduhu,Matheus Santos,Ben Bartlett,Gerard Dooly,James Riordan*

Main category: cs.CV

TL;DR: SSCATeR algorithm uses temporal data recycling and scatter-based convolutions to process only changing parts of LiDAR point clouds, achieving 6.61x speedup with identical accuracy to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional LiDAR processing methods waste computation on unchanged regions between frames. There's a need to reduce computational load while maintaining detection accuracy by focusing only on dynamic parts of the scene.

Method: Uses sliding time windows with short strides to track temporal changes. Extends scatter-based convolutions to support data reuse, creating SSCATeR algorithm that processes only changing parts of LiDAR point clouds as a continuous stream.

Result: Achieves up to 6.61-fold reduction in processing time while producing identical feature maps to traditional sparse convolution techniques. Maintains same accuracy with significantly improved computational efficiency.

Conclusion: SSCATeR successfully enables efficient LiDAR processing by exploiting temporal sparsity through data recycling, making real-time object detection more practical without sacrificing performance.

Abstract: This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.

</details>


### [72] [BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain](https://arxiv.org/abs/2512.08560)
*Navve Wasserman,Matias Cosarinsky,Yuval Golbari,Aude Oliva,Antonio Torralba,Tamar Rott Shaham,Michal Irani*

Main category: cs.CV

TL;DR: Large-scale automated framework discovers and explains visual concept representations across human cortex using unsupervised pattern discovery and automated explanation generation.


<details>
  <summary>Details</summary>
Motivation: Understanding how the human brain represents visual concepts and where these representations are encoded remains challenging due to complex brain signals, vast concept space, and limitations of small-scale studies relying on manual inspection.

Method: Two-stage approach: 1) Discover candidate interpretable patterns in fMRI activity through unsupervised data-driven decomposition methods. 2) Explain each pattern by identifying natural images that most strongly elicit it and generating natural-language descriptions. Automated pipeline tests multiple candidate explanations, assigns reliability scores, and selects most consistent descriptions.

Result: Framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.

Conclusion: The automated large-scale framework enables systematic discovery and explanation of visual representations across human cortex, overcoming limitations of traditional small-scale studies.

Abstract: Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.

</details>


### [73] [Modular Neural Image Signal Processing](https://arxiv.org/abs/2512.08564)
*Mahmoud Afifi,Zhongling Wang,Ran Zhang,Michael S. Brown*

Main category: cs.CV

TL;DR: Modular neural ISP framework for raw-to-display image processing with full control over intermediate stages, enabling high-quality rendering, scalability, debuggability, and user-style flexibility.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of prior neural ISP designs by introducing modularity for better control, scalability, debuggability, generalization to unseen cameras, and flexibility to match user preferences.

Method: A modular neural image signal processing framework that processes raw inputs through controllable intermediate stages, with variants ranging from 0.5M to 3.9M parameters, enabling user-interactive editing and unlimited post-editable re-rendering.

Result: Consistently delivers competitive qualitative and quantitative results across multiple test sets, with moderate-sized models achieving high rendering accuracy while supporting diverse editing operations and picture styles.

Conclusion: The modular neural ISP framework provides superior control and flexibility over the image rendering process while maintaining competitive performance, enabling practical applications like interactive photo-editing tools with unlimited re-rendering capabilities.

Abstract: This paper presents a modular neural image signal processing (ISP) framework that processes raw inputs and renders high-quality display-referred images. Unlike prior neural ISP designs, our method introduces a high degree of modularity, providing full control over multiple intermediate stages of the rendering process.~This modular design not only achieves high rendering accuracy but also improves scalability, debuggability, generalization to unseen cameras, and flexibility to match different user-preference styles. To demonstrate the advantages of this design, we built a user-interactive photo-editing tool that leverages our neural ISP to support diverse editing operations and picture styles. The tool is carefully engineered to take advantage of the high-quality rendering of our neural ISP and to enable unlimited post-editable re-rendering. Our method is a fully learning-based framework with variants of different capacities, all of moderate size (ranging from ~0.5 M to ~3.9 M parameters for the entire pipeline), and consistently delivers competitive qualitative and quantitative results across multiple test sets. Watch the supplemental video at: https://youtu.be/ByhQjQSjxVM

</details>


### [74] [Instance-Aware Test-Time Segmentation for Continual Domain Shifts](https://arxiv.org/abs/2512.08569)
*Seunghwan Lee,Inyoung Jung,Hojoon Lee,Eunil Park,Sungeun Hong*

Main category: cs.CV

TL;DR: Proposes adaptive pseudo-label adjustment for continual test-time adaptation in semantic segmentation, using class- and instance-aware thresholds to handle varying difficulty across classes and instances.


<details>
  <summary>Details</summary>
Motivation: Existing CTTA methods use fixed or batch-level thresholds that cannot handle varying difficulty across classes and instances, which is especially problematic in semantic segmentation where each image requires dense, multi-class predictions.

Method: Adaptively adjusts pseudo labels based on confidence distribution within each image and dynamically balances learning toward classes most affected by domain shifts, using fine-grained, class- and instance-aware adaptation.

Result: Extensive experiments across eight CTTA and TTA scenarios (including synthetic-to-real and long-term shifts) show consistent outperformance over state-of-the-art techniques, setting new standard for semantic segmentation under evolving conditions.

Conclusion: The proposed approach produces more reliable supervision and mitigates error accumulation throughout continual adaptation, demonstrating superior performance in handling evolving domains for semantic segmentation tasks.

Abstract: Continual Test-Time Adaptation (CTTA) enables pre-trained models to adapt to continuously evolving domains. Existing methods have improved robustness but typically rely on fixed or batch-level thresholds, which cannot account for varying difficulty across classes and instances. This limitation is especially problematic in semantic segmentation, where each image requires dense, multi-class predictions. We propose an approach that adaptively adjusts pseudo labels to reflect the confidence distribution within each image and dynamically balances learning toward classes most affected by domain shifts. This fine-grained, class- and instance-aware adaptation produces more reliable supervision and mitigates error accumulation throughout continual adaptation. Extensive experiments across eight CTTA and TTA scenarios, including synthetic-to-real and long-term shifts, show that our method consistently outperforms state-of-the-art techniques, setting a new standard for semantic segmentation under evolving conditions.

</details>


### [75] [From Cells to Survival: Hierarchical Analysis of Cell Inter-Relations in Multiplex Microscopy for Lung Cancer Prognosis](https://arxiv.org/abs/2512.08572)
*Olle Edgren SchÃ¼llerqvist,Jens Baumann,Joakim Lindblad,Love Nordling,Artur Mezheyeuski,Patrick Micke,NataÅ¡a Sladoje*

Main category: cs.CV

TL;DR: HiGINE is a hierarchical graph-based model that predicts lung cancer patient survival from multiplex immunofluorescence images by capturing complex cell interactions in the tumor microenvironment.


<details>
  <summary>Details</summary>
Motivation: The tumor microenvironment contains valuable prognostic biomarkers, but existing methods fail to capture complex cellular interactions needed for accurate survival prediction and risk stratification in lung cancer.

Method: Hierarchical graph-based approach that encodes local and global inter-relations in cell neighborhoods using cell type and morphology information from mIF images, with multimodal fusion incorporating cancer stage data.

Result: Validated on two public datasets, showing improved risk stratification, robustness, and generalizability compared to existing methods.

Conclusion: HiGINE effectively leverages TME complexity for survival prediction, demonstrating potential for enhanced clinical risk assessment in lung cancer.

Abstract: The tumor microenvironment (TME) has emerged as a promising source of prognostic biomarkers. To fully leverage its potential, analysis methods must capture complex interactions between different cell types. We propose HiGINE -- a hierarchical graph-based approach to predict patient survival (short vs. long) from TME characterization in multiplex immunofluorescence (mIF) images and enhance risk stratification in lung cancer. Our model encodes both local and global inter-relations in cell neighborhoods, incorporating information about cell types and morphology. Multimodal fusion, aggregating cancer stage with mIF-derived features, further boosts performance. We validate HiGINE on two public datasets, demonstrating improved risk stratification, robustness, and generalizability.

</details>


### [76] [Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery](https://arxiv.org/abs/2512.08577)
*Yuna Kato,Shohei Mori,Hideo Saito,Yoshifumi Takatsume,Hiroki Kajita,Mariko Isogawa*

Main category: cs.CV

TL;DR: Automated system for generating unobstructed surgical videos by realigning multiple camera views and selecting the least occluded perspective.


<details>
  <summary>Details</summary>
Motivation: Surgical videos are valuable for education/research but often obstructed by surgeons blocking the view. Current multi-camera setups require manual post-processing alignment when lighting systems are moved.

Method: Proposed method detects when lighting system moves, automatically realigns camera frames, selects camera with least occlusion, and synthesizes consistent surgical field view from fixed perspective.

Result: Surgeon user study showed videos were superior for confirming surgical area and viewing comfort. Improved video quality over existing techniques. Multiple synthesis options evaluated for surgeon preferences.

Conclusion: Fully automated alignment system successfully generates high-quality, unobstructed surgical videos from multi-camera setups, addressing occlusion and manual alignment challenges.

Abstract: Video recordings of open surgeries are greatly required for education and research purposes. However, capturing unobstructed videos is challenging since surgeons frequently block the camera field of view. To avoid occlusion, the positions and angles of the camera must be frequently adjusted, which is highly labor-intensive. Prior work has addressed this issue by installing multiple cameras on a shadowless lamp and arranging them to fully surround the surgical area. This setup increases the chances of some cameras capturing an unobstructed view. However, manual image alignment is needed in post-processing since camera configurations change every time surgeons move the lamp for optimal lighting. This paper aims to fully automate this alignment task. The proposed method identifies frames in which the lighting system moves, realigns them, and selects the camera with the least occlusion to generate a video that consistently presents the surgical field from a fixed perspective. A user study involving surgeons demonstrated that videos generated by our method were superior to those produced by conventional methods in terms of the ease of confirming the surgical area and the comfort during video viewing. Additionally, our approach showed improvements in video quality over existing techniques. Furthermore, we implemented several synthesis options for the proposed view-synthesis method and conducted a user study to assess surgeons' preferences for each option.

</details>


### [77] [Decoupling Template Bias in CLIP: Harnessing Empty Prompts for Enhanced Few-Shot Learning](https://arxiv.org/abs/2512.08606)
*Zhenyu Zhang,Guangyao Chen,Yixiong Zou,Zhimeng Huang,Yuhua Li*

Main category: cs.CV

TL;DR: CLIP's template-sample similarity (TSS) introduces bias that reduces classification accuracy and robustness. The paper proposes using empty prompts to capture unbiased template features and offset TSS bias through a two-stage framework.


<details>
  <summary>Details</summary>
Motivation: The study identifies that CLIP's template-sample similarity (TSS) creates bias where the model relies on template proximity rather than true sample-to-category alignment, leading to reduced accuracy and robustness in few-shot classification.

Method: A two-stage framework using empty prompts (textual inputs conveying "emptiness" without category information) to capture unbiased template features. During pre-training, empty prompts reduce template-induced bias within CLIP encoder. During few-shot fine-tuning, a bias calibration loss enforces correct alignment between images and categories.

Result: Experiments across multiple benchmarks show the template correction method significantly reduces performance fluctuations caused by TSS, yielding higher classification accuracy and stronger robustness.

Conclusion: The proposed framework effectively decouples template bias in CLIP, improving few-shot learning performance by ensuring the model focuses on relevant visual cues rather than template proximity.

Abstract: The Contrastive Language-Image Pre-Training (CLIP) model excels in few-shot learning by aligning visual and textual representations. Our study shows that template-sample similarity (TSS), defined as the resemblance between a text template and an image sample, introduces bias. This bias leads the model to rely on template proximity rather than true sample-to-category alignment, reducing both accuracy and robustness in classification. We present a framework that uses empty prompts, textual inputs that convey the idea of "emptiness" without category information. These prompts capture unbiased template features and offset TSS bias. The framework employs two stages. During pre-training, empty prompts reveal and reduce template-induced bias within the CLIP encoder. During few-shot fine-tuning, a bias calibration loss enforces correct alignment between images and their categories, ensuring the model focuses on relevant visual cues. Experiments across multiple benchmarks demonstrate that our template correction method significantly reduces performance fluctuations caused by TSS, yielding higher classification accuracy and stronger robustness. The repository of this project is available at https://github.com/zhenyuZ-HUST/Decoupling-Template-Bias-in-CLIP.

</details>


### [78] [Automated Pollen Recognition in Optical and Holographic Microscopy Images](https://arxiv.org/abs/2512.08589)
*Swarn Singh Warshaneyan,Maksims Ivanovs,BlaÅ¾ Cugmas,Inese BÄrziÅa,Laura Goldberga,Mindaugas Tamosiunas,Roberts KadiÄ·is*

Main category: cs.CV

TL;DR: Deep learning models (YOLOv8s + MobileNetV3L) applied to pollen grain detection/classification in optical and holographic microscopy images, achieving 91.3% mAP50 detection and 97% classification accuracy on optical images, with improved holographic performance through dataset expansion techniques.


<details>
  <summary>Details</summary>
Motivation: To improve and automate pollen grain detection and classification for veterinary cytology using both optical and holographic microscopy, addressing the performance gap between these imaging modalities.

Method: Used YOLOv8s for object detection and MobileNetV3L for classification, evaluated across imaging modalities. Applied dataset expansion techniques (automated labeling and bounding box area enlargement) to improve holographic image performance.

Result: Optical images: 91.3% mAP50 for detection, 97% overall accuracy for classification. Holographic images: initial performance low (2.49% mAP50 detection, 42% classification), improved to 13.3% mAP50 detection and 54% classification after dataset expansion.

Conclusion: Deep learning techniques can be effectively paired with cost-effective lensless digital holographic microscopy devices for image classification tasks, though performance on holographic images requires dataset expansion techniques to bridge the gap with optical microscopy.

Abstract: This study explores the application of deep learning to improve and automate pollen grain detection and classification in both optical and holographic microscopy images, with a particular focus on veterinary cytology use cases. We used YOLOv8s for object detection and MobileNetV3L for the classification task, evaluating their performance across imaging modalities. The models achieved 91.3% mAP50 for detection and 97% overall accuracy for classification on optical images, whereas the initial performance on greyscale holographic images was substantially lower. We addressed the performance gap issue through dataset expansion using automated labeling and bounding box area enlargement. These techniques, applied to holographic images, improved detection performance from 2.49% to 13.3% mAP50 and classification performance from 42% to 54%. Our work demonstrates that, at least for image classification tasks, it is possible to pair deep learning techniques with cost-effective lensless digital holographic microscopy devices.

</details>


### [79] [Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning](https://arxiv.org/abs/2512.08639)
*Huilin Xu,Zhuoyang Liu,Yixiang Luomei,Feng Xu*

Main category: cs.CV

TL;DR: A unified aerial VLN framework using only monocular RGB images and language instructions, formulated as next-token prediction with multi-task learning, achieving strong performance without panoramic or depth inputs.


<details>
  <summary>Details</summary>
Motivation: Existing aerial VLN methods rely on panoramic images, depth inputs, or odometry, which increase system cost and complexity, hindering practical deployment on lightweight UAVs. There's a need for simpler, more practical solutions using only monocular RGB observations.

Method: 1) Formulates navigation as next-token prediction problem; 2) Uses prompt-guided multi-task learning to jointly optimize spatial perception, trajectory reasoning, and action prediction; 3) Implements keyframe selection to reduce visual redundancy; 4) Includes action merging and label reweighting to address long-tailed supervision imbalance.

Result: Achieves strong results on Aerial VLN benchmark under monocular RGB-only setting, significantly outperforming existing RGB-only baselines and narrowing performance gap with state-of-the-art panoramic RGB-D methods across both seen and unseen environments.

Conclusion: The proposed unified framework demonstrates that effective aerial VLN can be achieved using only monocular RGB observations and language instructions, offering a more practical solution for lightweight UAV deployment without compromising performance.

Abstract: Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices.

</details>


### [80] [Mitigating Individual Skin Tone Bias in Skin Lesion Classification through Distribution-Aware Reweighting](https://arxiv.org/abs/2512.08733)
*Kuniko Paxton,Zeinab Dehghani,Koorosh Aslansefat,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: This paper introduces a distribution-based framework for individual fairness in skin lesion classification, treating skin tone as continuous rather than categorical, using kernel density estimation and statistical distance metrics to quantify and mitigate disparities.


<details>
  <summary>Details</summary>
Motivation: Current fairness research in medical imaging relies on coarse subgroup categories, overlooking individual-level variations and potentially obscuring biases faced by outliers within subgroups. Skin color has historically been a focal point of discrimination, necessitating more nuanced fairness approaches.

Method: The study treats skin tone as a continuous attribute using kernel density estimation (KDE) to model its distribution. It compares twelve statistical distance metrics to quantify disparities between skin tone distributions and proposes a distance-based reweighting (DRW) loss function to correct underrepresentation in minority tones.

Result: Experiments across CNN and Transformer models show: (1) limitations of categorical reweighting in capturing individual-level disparities, and (2) superior performance of distribution-based reweighting, particularly with Fidelity Similarity (FS), Wasserstein Distance (WD), Hellinger Metric (HM), and Harmonic Mean Similarity (HS).

Conclusion: The distribution-based framework establishes a robust methodology for advancing individual-level fairness in dermatological AI systems, with broader implications for handling sensitive continuous attributes in medical image analysis.

Abstract: Skin color has historically been a focal point of discrimination, yet fairness research in machine learning for medical imaging often relies on coarse subgroup categories, overlooking individual-level variations. Such group-based approaches risk obscuring biases faced by outliers within subgroups. This study introduces a distribution-based framework for evaluating and mitigating individual fairness in skin lesion classification. We treat skin tone as a continuous attribute rather than a categorical label, and employ kernel density estimation (KDE) to model its distribution. We further compare twelve statistical distance metrics to quantify disparities between skin tone distributions and propose a distance-based reweighting (DRW) loss function to correct underrepresentation in minority tones. Experiments across CNN and Transformer models demonstrate: (i) the limitations of categorical reweighting in capturing individual-level disparities, and (ii) the superior performance of distribution-based reweighting, particularly with Fidelity Similarity (FS), Wasserstein Distance (WD), Hellinger Metric (HM), and Harmonic Mean Similarity (HS). These findings establish a robust methodology for advancing fairness at individual level in dermatological AI systems, and highlight broader implications for sensitive continuous attributes in medical image analysis.

</details>


### [81] [OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics](https://arxiv.org/abs/2512.08625)
*Jisang Yoo,Gyeongjin Kang,Hyun-kyu Ko,Hyeonwoo Yu,Eunbyung Park*

Main category: cs.CV

TL;DR: OpenMonoGS-SLAM: First monocular SLAM framework combining 3D Gaussian Splatting with open-set semantic understanding using visual foundation models, requiring no depth sensors or semantic annotations.


<details>
  <summary>Details</summary>
Motivation: Current SLAM systems with semantic understanding often rely on depth sensors or closed-set semantic models, limiting scalability and adaptability in open-world environments. There's a need for monocular SLAM that can handle open-set semantics without specialized hardware or annotations.

Method: Unifies 3D Gaussian Splatting with open-set semantic understanding using Visual Foundation Models (MASt3R for geometry, SAM and CLIP for open-vocabulary semantics). Uses self-supervised learning without depth input or 3D semantic ground truth. Includes a memory mechanism for managing high-dimensional semantic features to construct Gaussian semantic feature maps.

Result: Achieves performance comparable to or surpassing existing baselines in both closed-set and open-set segmentation tasks, without relying on supplementary sensors like depth maps or semantic annotations.

Conclusion: OpenMonoGS-SLAM demonstrates successful integration of monocular SLAM with open-set semantic understanding using visual foundation models, enabling robust spatial AI applications in open-world environments without specialized hardware requirements.

Abstract: Simultaneous Localization and Mapping (SLAM) is a foundational component in robotics, AR/VR, and autonomous systems. With the rising focus on spatial AI in recent years, combining SLAM with semantic understanding has become increasingly important for enabling intelligent perception and interaction. Recent efforts have explored this integration, but they often rely on depth sensors or closed-set semantic models, limiting their scalability and adaptability in open-world environments. In this work, we present OpenMonoGS-SLAM, the first monocular SLAM framework that unifies 3D Gaussian Splatting (3DGS) with open-set semantic understanding. To achieve our goal, we leverage recent advances in Visual Foundation Models (VFMs), including MASt3R for visual geometry and SAM and CLIP for open-vocabulary semantics. These models provide robust generalization across diverse tasks, enabling accurate monocular camera tracking and mapping, as well as a rich understanding of semantics in open-world environments. Our method operates without any depth input or 3D semantic ground truth, relying solely on self-supervised learning objectives. Furthermore, we propose a memory mechanism specifically designed to manage high-dimensional semantic features, which effectively constructs Gaussian semantic feature maps, leading to strong overall performance. Experimental results demonstrate that our approach achieves performance comparable to or surpassing existing baselines in both closed-set and open-set segmentation tasks, all without relying on supplementary sensors such as depth maps or semantic annotations.

</details>


### [82] [Refining Visual Artifacts in Diffusion Models via Explainable AI-based Flaw Activation Maps](https://arxiv.org/abs/2512.08774)
*Seoyeon Lee,Gwangyeol Yu,Chaewon Kim,Jonghyuk Park*

Main category: cs.CV

TL;DR: Self-refining diffusion framework uses XAI-based flaw detection to identify artifacts and unrealistic regions, then improves image quality by focusing denoising on these flawed areas during diffusion process.


<details>
  <summary>Details</summary>
Motivation: Despite remarkable success of diffusion models in image synthesis, addressing artifacts and unrealistic regions remains a critical challenge that needs improvement.

Method: Proposes self-refining diffusion framework with XAI-based flaw highlighter that generates flaw activation maps (FAMs). These maps identify problematic regions, then improve reconstruction by amplifying noise in flawed areas during forward process and focusing on them during reverse denoising process.

Result: Achieves up to 27.3% improvement in FrÃ©chet inception distance across various diffusion models, shows strong performance on diverse datasets, and demonstrates effectiveness across image generation, text-to-image generation, and inpainting tasks.

Conclusion: Explainable AI techniques can extend beyond interpretability to actively contribute to image refinement. The framework offers versatile and effective approach applicable to various diffusion models and tasks, significantly advancing image synthesis field.

Abstract: Diffusion models have achieved remarkable success in image synthesis. However, addressing artifacts and unrealistic regions remains a critical challenge. We propose self-refining diffusion, a novel framework that enhances image generation quality by detecting these flaws. The framework employs an explainable artificial intelligence (XAI)-based flaw highlighter to produce flaw activation maps (FAMs) that identify artifacts and unrealistic regions. These FAMs improve reconstruction quality by amplifying noise in flawed regions during the forward process and by focusing on these regions during the reverse process. The proposed approach achieves up to a 27.3% improvement in FrÃ©chet inception distance across various diffusion-based models, demonstrating consistently strong performance on diverse datasets. It also shows robust effectiveness across different tasks, including image generation, text-to-image generation, and inpainting. These results demonstrate that explainable AI techniques can extend beyond interpretability to actively contribute to image refinement. The proposed framework offers a versatile and effective approach applicable to various diffusion models and tasks, significantly advancing the field of image synthesis.

</details>


### [83] [Trajectory Densification and Depth from Perspective-based Blur](https://arxiv.org/abs/2512.08627)
*Tianchen Qiu,Qirun Zhang,Jiajian He,Zhengyue Zhuge,Jiahui Xu,Yueting Chen*

Main category: cs.CV

TL;DR: A method that estimates metric depth by analyzing perspective-based blur patterns in videos, using vision encoders, point trackers, and vision-language models for dense trajectory reconstruction.


<details>
  <summary>Details</summary>
Motivation: Camera rotational dynamics during handheld shooting cause perspective-based blur that varies with object depth, creating an opportunity to estimate depth from blur patterns in video streams.

Method: Uses off-the-shelf vision encoder and point tracker to extract video information, estimates depth via windowed embedding and multi-window aggregation, and densifies sparse optical flow trajectories using a vision-language model.

Result: Achieves strong performance across multiple depth datasets with good generalization, superior precision in handheld shooting settings compared to real trajectories, and maintains strong accuracy in dense reconstruction.

Conclusion: Perspective-based blur patterns in videos can be effectively leveraged for metric depth estimation and dense trajectory reconstruction, demonstrating practical applications for handheld camera scenarios.

Abstract: In the absence of a mechanical stabilizer, the camera undergoes inevitable rotational dynamics during capturing, which induces perspective-based blur especially under long-exposure scenarios. From an optical standpoint, perspective-based blur is depth-position-dependent: objects residing at distinct spatial locations incur different blur levels even under the same imaging settings. Inspired by this, we propose a novel method that estimate metric depth by examining the blur pattern of a video stream and dense trajectory via joint optical design algorithm. Specifically, we employ off-the-shelf vision encoder and point tracker to extract video information. Then, we estimate depth map via windowed embedding and multi-window aggregation, and densify the sparse trajectory from the optical algorithm using a vision-language model. Evaluations on multiple depth datasets demonstrate that our method attains strong performance over large depth range, while maintaining favorable generalization. Relative to the real trajectory in handheld shooting settings, our optical algorithm achieves superior precision and the dense reconstruction maintains strong accuracy.

</details>


### [84] [MatteViT: High-Frequency-Aware Document Shadow Removal with Shadow Matte Guidance](https://arxiv.org/abs/2512.08789)
*Chaewon Kim,Seoyeon Lee,Jonghyuk Park*

Main category: cs.CV

TL;DR: MatteViT is a novel document shadow removal framework that uses spatial and frequency-domain information with high-frequency amplification and continuous luminance-based shadow matte to preserve text details while removing shadows.


<details>
  <summary>Details</summary>
Motivation: Document shadow removal is essential for digitized document clarity, and preserving high-frequency details like text edges and lines is critical because shadows often obscure or distort these fine structures.

Method: Proposes MatteViT framework with two key strategies: 1) lightweight high-frequency amplification module (HFAM) that decomposes and adaptively amplifies high-frequency components, and 2) continuous luminance-based shadow matte generated using custom dataset and shadow matte generator for precise spatial guidance from early processing stages.

Result: Extensive experiments on public benchmarks (RDD and Kligler) show MatteViT achieves state-of-the-art performance, providing robust practical solution for real-world document shadow removal. Method better preserves text-level details in downstream tasks like OCR, improving recognition performance over prior methods.

Conclusion: MatteViT effectively eliminates shadows while preserving fine-grained structural details through spatial and frequency-domain processing with specialized modules, demonstrating superior performance in document shadow removal and downstream OCR applications.

Abstract: Document shadow removal is essential for enhancing the clarity of digitized documents. Preserving high-frequency details (e.g., text edges and lines) is critical in this process because shadows often obscure or distort fine structures. This paper proposes a matte vision transformer (MatteViT), a novel shadow removal framework that applies spatial and frequency-domain information to eliminate shadows while preserving fine-grained structural details. To effectively retain these details, we employ two preservation strategies. First, our method introduces a lightweight high-frequency amplification module (HFAM) that decomposes and adaptively amplifies high-frequency components. Second, we present a continuous luminance-based shadow matte, generated using a custom-built matte dataset and shadow matte generator, which provides precise spatial guidance from the earliest processing stage. These strategies enable the model to accurately identify fine-grained regions and restore them with high fidelity. Extensive experiments on public benchmarks (RDD and Kligler) demonstrate that MatteViT achieves state-of-the-art performance, providing a robust and practical solution for real-world document shadow removal. Furthermore, the proposed method better preserves text-level details in downstream tasks, such as optical character recognition, improving recognition performance over prior methods.

</details>


### [85] [Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation](https://arxiv.org/abs/2512.08645)
*Young Kyung Kim,Oded Schlesinger,Yuzhou Zhao,J. Matias Di Martino,Guillermo Sapiro*

Main category: cs.CV

TL;DR: CoIG framework reframes image generation as sequential semantic process using LLM to decompose prompts into step-by-step instructions, enabling monitorability and mitigating entity collapse.


<details>
  <summary>Details</summary>
Motivation: Current image generation models are opaque "black boxes" that limit human observation, intervention, and control. Their non-human-like workflows make interpretation difficult, posing barriers to reliability and safety.

Method: Chain-of-Image Generation (CoIG) uses an LLM to decompose complex prompts into sequential simple instructions. The image generation model then executes this plan by progressively generating and editing images, with each step focusing on a single semantic entity.

Result: CoIG substantially enhances quantitative monitorability while achieving competitive compositional robustness compared to baselines. It introduces two novel metrics: CoIG Readability (clarity of intermediate steps) and Causal Relevance (impact of each step on final image).

Conclusion: CoIG brings monitorability and performance benefits to text-to-image generation similar to how Chain-of-Thought improved LLMs. The framework is model-agnostic, mitigates entity collapse, and enables direct observation of the generative process.

Abstract: While state-of-the-art image generation models achieve remarkable visual quality, their internal generative processes remain a "black box." This opacity limits human observation and intervention, and poses a barrier to ensuring model reliability, safety, and control. Furthermore, their non-human-like workflows make them difficult for human observers to interpret. To address this, we introduce the Chain-of-Image Generation (CoIG) framework, which reframes image generation as a sequential, semantic process analogous to how humans create art. Similar to the advantages in monitorability and performance that Chain-of-Thought (CoT) brought to large language models (LLMs), CoIG can produce equivalent benefits in text-to-image generation. CoIG utilizes an LLM to decompose a complex prompt into a sequence of simple, step-by-step instructions. The image generation model then executes this plan by progressively generating and editing the image. Each step focuses on a single semantic entity, enabling direct monitoring. We formally assess this property using two novel metrics: CoIG Readability, which evaluates the clarity of each intermediate step via its corresponding output; and Causal Relevance, which quantifies the impact of each procedural step on the final generated image. We further show that our framework mitigates entity collapse by decomposing the complex generation task into simple subproblems, analogous to the procedural reasoning employed by CoT. Our experimental results indicate that CoIG substantially enhances quantitative monitorability while achieving competitive compositional robustness compared to established baseline models. The framework is model-agnostic and can be integrated with any image generation model.

</details>


### [86] [Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning](https://arxiv.org/abs/2512.08820)
*Yi Zhang,Chun-Wun Cheng,Junyi He,Ke Yu,Yushun Tang,Carola-Bibiane SchÃ¶nlieb,Zhihai He,Angelica I. Aviles-Rivero*

Main category: cs.CV

TL;DR: T-DHA is a training-free adaptation method for vision-language models that uses hyperbolic space embeddings and negative learning to improve few-shot recognition and domain generalization without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models suffer from performance degradation with domain changes or require substantial computational resources for fine-tuning in new domains, creating a need for efficient adaptation methods.

Method: Proposes Training-free Dual Hyperbolic Adapters (T-DHA) that characterizes vision-language relationships in hyperbolic space using the PoincarÃ© ball model, leveraging exponential volume growth for hierarchical data embedding, combined with negative learning for improved classification.

Result: Extensive experiments show T-DHA significantly outperforms state-of-the-art methods in few-shot image recognition and domain generalization tasks across various datasets.

Conclusion: Hyperbolic space embeddings with negative learning provide an effective training-free adaptation approach for vision-language models, offering improved representation power and robustness with fewer feature dimensions.

Abstract: Recent research in Vision-Language Models (VLMs) has significantly advanced our capabilities in cross-modal reasoning. However, existing methods suffer from performance degradation with domain changes or require substantial computational resources for fine-tuning in new domains. To address this issue, we develop a new adaptation method for large vision-language models, called \textit{Training-free Dual Hyperbolic Adapters} (T-DHA). We characterize the vision-language relationship between semantic concepts, which typically has a hierarchical tree structure, in the hyperbolic space instead of the traditional Euclidean space. Hyperbolic spaces exhibit exponential volume growth with radius, unlike the polynomial growth in Euclidean space. We find that this unique property is particularly effective for embedding hierarchical data structures using the PoincarÃ© ball model, achieving significantly improved representation and discrimination power. Coupled with negative learning, it provides more accurate and robust classifications with fewer feature dimensions. Our extensive experimental results on various datasets demonstrate that the T-DHA method significantly outperforms existing state-of-the-art methods in few-shot image recognition and domain generalization tasks.

</details>


### [87] [C-DIRA: Computationally Efficient Dynamic ROI Routing and Domain-Invariant Adversarial Learning for Lightweight Driver Behavior Recognition](https://arxiv.org/abs/2512.08647)
*Keito Inoshita*

Main category: cs.CV

TL;DR: C-DIRA is a lightweight driver behavior recognition framework that combines dynamic ROI routing and domain-invariant adversarial learning to achieve efficient, accurate, and robust performance on edge devices.


<details>
  <summary>Details</summary>
Motivation: Current lightweight models for driver distraction recognition fail to capture fine-grained behavioral cues and suffer from reduced performance on unseen drivers/varying conditions. ROI-based methods increase computational costs, creating a trade-off between efficiency and accuracy.

Method: Proposes C-DIRA framework with: 1) Saliency-driven Top-K ROI pooling and fused classification for local feature extraction, 2) Dynamic ROI routing that applies ROI inference only to high-difficulty samples, and 3) Pseudo-domain labeling and adversarial learning for domain-invariant features.

Result: On State Farm Distracted Driver Detection Dataset, C-DIRA maintains high accuracy with significantly fewer FLOPs and lower latency than prior lightweight models. Shows robustness to visual degradation (blur, low-light) and stable performance across unseen domains.

Conclusion: C-DIRA effectively achieves compactness, efficiency, and generalization for real-time driver behavior recognition on edge devices, balancing computational efficiency with robust performance across varying conditions.

Abstract: Driver distraction behavior recognition using in-vehicle cameras demands real-time inference on edge devices. However, lightweight models often fail to capture fine-grained behavioral cues, resulting in reduced performance on unseen drivers or under varying conditions. ROI-based methods also increase computational cost, making it difficult to balance efficiency and accuracy. This work addresses the need for a lightweight architecture that overcomes these constraints. We propose Computationally efficient Dynamic region of Interest Routing and domain-invariant Adversarial learning for lightweight driver behavior recognition (C-DIRA). The framework combines saliency-driven Top-K ROI pooling and fused classification for local feature extraction and integration. Dynamic ROI routing enables selective computation by applying ROI inference only to high difficulty data samples. Moreover, pseudo-domain labeling and adversarial learning are used to learn domain-invariant features robust to driver and background variation. Experiments on the State Farm Distracted Driver Detection Dataset show that C-DIRA maintains high accuracy with significantly fewer FLOPs and lower latency than prior lightweight models. It also demonstrates robustness under visual degradation such as blur and low-light, and stable performance across unseen domains. These results confirm C-DIRA's effectiveness in achieving compactness, efficiency, and generalization.

</details>


### [88] [InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models](https://arxiv.org/abs/2512.08829)
*Hongyuan Tao,Bencheng Liao,Shaoyu Chen,Haoran Yin,Qian Zhang,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: InfiniteVL is a linear-complexity Vision-Language Model that combines sliding window attention with Gated DeltaNet to overcome limitations of window attention and linear attention, achieving competitive performance with 3.6Ã speedup and constant memory footprint.


<details>
  <summary>Details</summary>
Motivation: Window attention suffers performance degradation when sequence length exceeds window size, while linear attention underperforms on information-intensive tasks like OCR and document understanding. Need for efficient VLM architecture with linear complexity and good performance.

Method: Proposes InfiniteVL architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. Uses three-stage training: distillation pretraining, instruction tuning, and long-sequence SFT. Achieves linear complexity with constant memory footprint.

Result: Using <2% of training data of leading VLMs, InfiniteVL outperforms previous linear-complexity VLMs and matches Transformer-based VLMs. Achieves 3.6Ã inference speedup over FlashAttention-2 accelerated Transformers with constant latency/memory. Maintains 24 FPS real-time prefill in streaming video.

Conclusion: InfiniteVL successfully addresses limitations of both window and linear attention, providing efficient linear-complexity VLM with competitive performance, constant memory footprint, and effective long-term memory retention for practical applications.

Abstract: Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.

</details>


### [89] [Repulsor: Accelerating Generative Modeling with a Contrastive Memory Bank](https://arxiv.org/abs/2512.08648)
*Shaofeng Zhang,Xuanqi Chen,Ning Liao,Haoxiang Zhao,Xiaoxing Wang,Haoru Tan,Sitong Wu,Xiaosong Jia,Qi Fan,Junchi Yan*

Main category: cs.CV

TL;DR: A plug-and-play training framework called {\mname} that uses a memory bank mechanism to provide abundant negative samples for contrastive learning in generative models, eliminating dependency on external encoders while improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Denoising generative models (diffusion, flow-matching) have high training costs and inefficient representation learning. Current approaches using external pre-trained encoders for discriminative representations introduce overhead and domain shift issues.

Method: Proposes {\mname} with a memory bank mechanism that maintains a large, dynamically updated queue of negative samples across training iterations, decoupling negative sample count from batch size. Uses low-dimensional projection head to minimize memory/bandwidth overhead.

Result: Achieves state-of-the-art FID of 2.40 on ImageNet-256 within 400k steps, significantly outperforming comparable methods. Enables substantially faster convergence and superior generative quality more efficiently.

Conclusion: {\mname} provides a self-contained solution that eliminates dependency on pretrained vision foundation models, introduces no additional inference costs, and achieves better performance with faster convergence through efficient negative sampling.

Abstract: The dominance of denoising generative models (e.g., diffusion, flow-matching) in visual synthesis is tempered by their substantial training costs and inefficiencies in representation learning. While injecting discriminative representations via auxiliary alignment has proven effective, this approach still faces key limitations: the reliance on external, pre-trained encoders introduces overhead and domain shift. A dispersed-based strategy that encourages strong separation among in-batch latent representations alleviates this specific dependency. To assess the effect of the number of negative samples in generative modeling, we propose {\mname}, a plug-and-play training framework that requires no external encoders. Our method integrates a memory bank mechanism that maintains a large, dynamically updated queue of negative samples across training iterations. This decouples the number of negatives from the mini-batch size, providing abundant and high-quality negatives for a contrastive objective without a multiplicative increase in computational cost. A low-dimensional projection head is used to further minimize memory and bandwidth overhead. {\mname} offers three principal advantages: (1) it is self-contained, eliminating dependency on pretrained vision foundation models and their associated forward-pass overhead; (2) it introduces no additional parameters or computational cost during inference; and (3) it enables substantially faster convergence, achieving superior generative quality more efficiently. On ImageNet-256, {\mname} achieves a state-of-the-art FID of \textbf{2.40} within 400k steps, significantly outperforming comparable methods.

</details>


### [90] [Dual-Branch Center-Surrounding Contrast: Rethinking Contrastive Learning for 3D Point Clouds](https://arxiv.org/abs/2512.08673)
*Shaofeng Zhang,Xuanqi Chen,Xiangdong Zhang,Sitong Wu,Junchi Yan*

Main category: cs.CV

TL;DR: Proposes CSCon, a dual-branch center-surrounding contrastive learning framework for 3D point clouds that addresses limitations of existing generative SSL methods by better capturing both high-level discriminative features and local geometric details.


<details>
  <summary>Details</summary>
Motivation: Existing 3D SSL is dominated by generative MAE methods which struggle to capture high-level discriminative features, while contrastive methods effective in 2D images are scarce for 3D data. Direct application of 2D CL methods to 3D fails to learn local details effectively.

Method: Dual-branch center-surrounding contrastive framework: applies masking to center and surrounding parts separately to create center-biased and surrounding-biased representations, combined with patch-level contrastive loss to enhance both high-level information and local sensitivity.

Result: Achieves comparable performance to generative methods under FULL/ALL protocols, and SOTA results under MLP-LINEAR, MLP-3, and ONLY-NEW protocols, surpassing cross-modal approaches. Specifically, outperforms Point-MAE baseline by 7.9%, 6.7%, and 10.3% on ScanObjectNN variants under MLP-LINEAR.

Conclusion: CSCon effectively addresses the limitations of existing 3D SSL methods by combining the strengths of contrastive learning with 3D geometric awareness, achieving superior performance on discriminative downstream tasks while maintaining competitive results on generative tasks.

Abstract: Most existing self-supervised learning (SSL) approaches for 3D point clouds are dominated by generative methods based on Masked Autoencoders (MAE). However, these generative methods have been proven to struggle to capture high-level discriminative features effectively, leading to poor performance on linear probing and other downstream tasks. In contrast, contrastive methods excel in discriminative feature representation and generalization ability on image data. Despite this, contrastive learning (CL) in 3D data remains scarce. Besides, simply applying CL methods designed for 2D data to 3D fails to effectively learn 3D local details. To address these challenges, we propose a novel Dual-Branch \textbf{C}enter-\textbf{S}urrounding \textbf{Con}trast (CSCon) framework. Specifically, we apply masking to the center and surrounding parts separately, constructing dual-branch inputs with center-biased and surrounding-biased representations to better capture rich geometric information. Meanwhile, we introduce a patch-level contrastive loss to further enhance both high-level information and local sensitivity. Under the FULL and ALL protocols, CSCon achieves performance comparable to generative methods; under the MLP-LINEAR, MLP-3, and ONLY-NEW protocols, our method attains state-of-the-art results, even surpassing cross-modal approaches. In particular, under the MLP-LINEAR protocol, our method outperforms the baseline (Point-MAE) by \textbf{7.9\%}, \textbf{6.7\%}, and \textbf{10.3\%} on the three variants of ScanObjectNN, respectively. The code will be made publicly available.

</details>


### [91] [What really matters for person re-identification? A Mixture-of-Experts Framework for Semantic Attribute Importance](https://arxiv.org/abs/2512.08697)
*Athena Psalta,Vasileios Tsironis,Konstantinos Karantzalos*

Main category: cs.CV

TL;DR: MoSAIC-ReID is a Mixture-of-Experts framework that systematically quantifies the importance of pedestrian attributes for person re-identification, providing interpretable insights into which semantic attributes models actually rely on.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art person re-identification methods achieve high accuracy but remain opaque, leaving unanswered which high-level semantic attributes these models actually rely on for their decisions.

Method: Proposes MoSAIC-ReID, a Mixture-of-Experts framework using LoRA-based experts (each linked to a single attribute) with an oracle router for controlled attribution analysis. Uses generalized linear models, statistical tests, and feature-importance analyses to quantify attribute importance.

Result: Achieves competitive performance on Market-1501 and DukeMTMC datasets when attribute annotations are available at test time. Provides large-scale quantitative study revealing clothing colors and intrinsic characteristics contribute most strongly, while infrequent cues like accessories have limited effect.

Conclusion: Offers a principled framework for interpretable ReID and highlights requirements for integrating explicit semantic knowledge in practice, advancing transparency in person re-identification models.

Abstract: State-of-the-art person re-identification methods achieve impressive accuracy but remain largely opaque, leaving open the question: which high-level semantic attributes do these models actually rely on? We propose MoSAIC-ReID, a Mixture-of-Experts framework that systematically quantifies the importance of pedestrian attributes for re-identification. Our approach uses LoRA-based experts, each linked to a single attribute, and an oracle router that enables controlled attribution analysis. While MoSAIC-ReID achieves competitive performance on Market-1501 and DukeMTMC under the assumption that attribute annotations are available at test time, its primary value lies in providing a large-scale, quantitative study of attribute importance across intrinsic and extrinsic cues. Using generalized linear models, statistical tests, and feature-importance analyses, we reveal which attributes, such as clothing colors and intrinsic characteristics, contribute most strongly, while infrequent cues (e.g. accessories) have limited effect. This work offers a principled framework for interpretable ReID and highlights the requirements for integrating explicit semantic knowledge in practice. Code is available at https://github.com/psaltaath/MoSAIC-ReID

</details>


### [92] [Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning](https://arxiv.org/abs/2512.08873)
*Jing Jie Tan,Anissa Mokraoui,Ban-Hoe Kwan,Danny Wee-Kiat Ng,Yan-Chai Hum*

Main category: cs.CV

TL;DR: SOLI uses Siamese network architecture to optimize latent embeddings for efficient low-resolution image captioning on resource-constrained devices.


<details>
  <summary>Details</summary>
Motivation: Image captioning is important for assisting visually impaired individuals and improving content management, but existing methods struggle with low-resolution images and require heavy computational resources that make retraining difficult.

Method: SOLI employs a Siamese network architecture with dual-pathway neural network structure to optimize latent embeddings for low-resolution images, minimizing computational overhead while maintaining performance.

Result: The approach enhances efficiency and accuracy of image-to-text translation for low-resolution images while being suitable for resource-constrained scenarios.

Conclusion: SOLI provides an effective lightweight solution for low-resolution image captioning that balances computational efficiency with performance, making it practical for real-world applications with limited resources.

Abstract: Image captioning is essential in many fields including assisting visually impaired individuals, improving content management systems, and enhancing human-computer interaction. However, a recent challenge in this domain is dealing with low-resolution image (LRI). While performance can be improved by using larger models like transformers for encoding, these models are typically heavyweight, demanding significant computational resources and memory, leading to challenges in retraining. To address this, the proposed SOLI (Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning) approach presents a solution specifically designed for lightweight, low-resolution images captioning. It employs a Siamese network architecture to optimize latent embeddings, enhancing the efficiency and accuracy of the image-to-text translation process. By focusing on a dual-pathway neural network structure, SOLI minimizes computational overhead without sacrificing performance, making it an ideal choice for training on resource-constrained scenarios.

</details>


### [93] [Scale-invariant and View-relational Representation Learning for Full Surround Monocular Depth](https://arxiv.org/abs/2512.08700)
*Kyumin Hwang,Wonhyeok Choi,Kiljoon Han,Wonjoon Choi,Minwoo Choi,Yongcheon Na,Minwoo Park,Sunghoon Im*

Main category: cs.CV

TL;DR: Proposes a knowledge distillation method for Full Surround Monocular Depth Estimation that transfers depth knowledge from foundation models to lightweight networks while addressing computational cost and metric-scale depth challenges.


<details>
  <summary>Details</summary>
Motivation: Foundation models have strong generalization for monocular depth estimation but face two challenges when applied to Full Surround Monocular Depth Estimation (FSMDE): 1) high computational cost limiting real-time performance, and 2) difficulty in estimating metric-scale depth since they typically predict only relative depth.

Method: A novel knowledge distillation strategy with hybrid regression framework combining knowledge distillation (traditionally for classification) with depth binning module. Includes: 1) cross-interaction knowledge distillation that distills scale-invariant depth bin probabilities from foundation model to student network while guiding metric-scale depth bin centers from ground truth, and 2) view-relational knowledge distillation that encodes structural relationships among adjacent camera views to enhance cross-view depth consistency.

Result: Experiments on DDAD and nuScenes datasets demonstrate effectiveness compared to conventional supervised methods and existing knowledge distillation approaches. The method achieves favorable trade-off between performance and efficiency, meeting real-time requirements.

Conclusion: The proposed knowledge distillation approach successfully addresses computational cost and metric-scale depth challenges in FSMDE by transferring robust depth knowledge from foundation models to lightweight networks while maintaining real-time performance.

Abstract: Recent foundation models demonstrate strong generalization capabilities in monocular depth estimation. However, directly applying these models to Full Surround Monocular Depth Estimation (FSMDE) presents two major challenges: (1) high computational cost, which limits real-time performance, and (2) difficulty in estimating metric-scale depth, as these models are typically trained to predict only relative depth. To address these limitations, we propose a novel knowledge distillation strategy that transfers robust depth knowledge from a foundation model to a lightweight FSMDE network. Our approach leverages a hybrid regression framework combining the knowledge distillation scheme--traditionally used in classification--with a depth binning module to enhance scale consistency. Specifically, we introduce a cross-interaction knowledge distillation scheme that distills the scale-invariant depth bin probabilities of a foundation model into the student network while guiding it to infer metric-scale depth bin centers from ground-truth depth. Furthermore, we propose view-relational knowledge distillation, which encodes structural relationships among adjacent camera views and transfers them to enhance cross-view depth consistency. Experiments on DDAD and nuScenes demonstrate the effectiveness of our method compared to conventional supervised methods and existing knowledge distillation approaches. Moreover, our method achieves a favorable trade-off between performance and efficiency, meeting real-time requirements.

</details>


### [94] [SegEarth-OV3: Exploring SAM 3 for Open-Vocabulary Semantic Segmentation in Remote Sensing Images](https://arxiv.org/abs/2512.08730)
*Kaiyu Li,Shengqi Zhang,Yupeng Deng,Zhi Wang,Deyu Meng,Xiangyong Cao*

Main category: cs.CV

TL;DR: Simple training-free adaptation of SAM 3 for remote sensing open-vocabulary semantic segmentation using mask fusion and presence score filtering.


<details>
  <summary>Details</summary>
Motivation: Existing CLIP-based methods for OVSS struggle with precise localization and require complex pipelines, especially for remote sensing with dense small targets. SAM 3 offers unified segmentation and recognition but needs adaptation for remote sensing OVSS.

Method: 1) Mask fusion strategy combining SAM 3's semantic segmentation head and Transformer decoder outputs for better land coverage. 2) Presence score filtering to remove non-existent categories, reducing false positives from large vocabularies and patch-level processing.

Result: The simple adaptation achieves promising performance on extensive remote sensing datasets, demonstrating SAM 3's potential for remote sensing OVSS.

Conclusion: SAM 3 shows strong potential for training-free remote sensing open-vocabulary semantic segmentation through simple adaptations like mask fusion and presence filtering.

Abstract: Most existing methods for training-free Open-Vocabulary Semantic Segmentation (OVSS) are based on CLIP. While these approaches have made progress, they often face challenges in precise localization or require complex pipelines to combine separate modules, especially in remote sensing scenarios where numerous dense and small targets are present. Recently, Segment Anything Model 3 (SAM 3) was proposed, unifying segmentation and recognition in a promptable framework. In this paper, we present a preliminary exploration of applying SAM 3 to the remote sensing OVSS task without any training. First, we implement a mask fusion strategy that combines the outputs from SAM 3's semantic segmentation head and the Transformer decoder (instance head). This allows us to leverage the strengths of both heads for better land coverage. Second, we utilize the presence score from the presence head to filter out categories that do not exist in the scene, reducing false positives caused by the vast vocabulary sizes and patch-level processing in geospatial scenes. We evaluate our method on extensive remote sensing datasets. Experiments show that this simple adaptation achieves promising performance, demonstrating the potential of SAM 3 for remote sensing OVSS. Our code is released at https://github.com/earth-insights/SegEarth-OV-3.

</details>


### [95] [No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers](https://arxiv.org/abs/2512.08889)
*Damiano Marsili,Georgia Gkioxari*

Main category: cs.CV

TL;DR: VALOR is an annotation-free training framework that improves visual reasoning and grounding by using AI-powered verifiers - an LLM verifier for reasoning refinement via RL and a VLM verifier for visual grounding via automated hard-negative mining.


<details>
  <summary>Details</summary>
Motivation: Existing visual reasoning methods have limitations: language-only chain-of-thought approaches require large-scale supervised data (image, query, answer), while program-synthesis approaches avoid training but suffer from flawed logic and erroneous grounding. There's a need for a method that improves both reasoning and grounding without requiring expensive annotations.

Method: Proposes an annotation-free training framework using AI-powered verifiers: 1) LLM verifier refines LLM reasoning via reinforcement learning, 2) VLM verifier strengthens visual grounding through automated hard-negative mining. Combines strengths of modern AI systems: language-only reasoning models for decomposing spatial queries and vision specialist models improved via VLM critics.

Result: The method improves visual reasoning across diverse spatial reasoning tasks, surpassing both open-source and proprietary models. With the improved visual grounding model, it further outperforms recent text-only visual reasoning methods.

Conclusion: VALOR demonstrates that annotation-free training with AI-powered verifiers can effectively improve both reasoning and grounding in visual reasoning tasks, combining the strengths of language models and vision models without requiring ground truth labels.

Abstract: Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/

</details>


### [96] [Pose-Based Sign Language Spotting via an End-to-End Encoder Architecture](https://arxiv.org/abs/2512.08738)
*Samuel Ebimobowei Johnny,Blessed Guda,Emmanuel Enejo Aaron,Assane Gueye*

Main category: cs.CV

TL;DR: A novel Sign Language Spotting task is introduced for detecting specific sign queries within continuous sign sequences, using pose keypoints instead of RGB frames for efficiency and noise reduction.


<details>
  <summary>Details</summary>
Motivation: Current ASLR systems don't adequately address sign-to-sign retrieval or detecting specific signs within continuous sequences, creating a gap in practical sign language applications for deaf-hearing communication.

Method: End-to-end model using pose keypoints extracted from sign videos with encoder-only backbone and binary classification head to detect query sign presence in target sequences.

Result: Achieved 61.88% accuracy and 60.00% F1-score on Word Presence Prediction dataset from WSLP 2025 shared task, demonstrating effectiveness of pose-based approach.

Conclusion: The pose-based framework successfully establishes foundation for Sign Language Spotting, offering computational efficiency and noise reduction for future sign language retrieval research.

Abstract: Automatic Sign Language Recognition (ASLR) has emerged as a vital field for bridging the gap between deaf and hearing communities. However, the problem of sign-to-sign retrieval or detecting a specific sign within a sequence of continuous signs remains largely unexplored. We define this novel task as Sign Language Spotting. In this paper, we present a first step toward sign language retrieval by addressing the challenge of detecting the presence or absence of a query sign video within a sentence-level gloss or sign video. Unlike conventional approaches that rely on intermediate gloss recognition or text-based matching, we propose an end-to-end model that directly operates on pose keypoints extracted from sign videos. Our architecture employs an encoder-only backbone with a binary classification head to determine whether the query sign appears within the target sequence. By focusing on pose representations instead of raw RGB frames, our method significantly reduces computational cost and mitigates visual noise. We evaluate our approach on the Word Presence Prediction dataset from the WSLP 2025 shared task, achieving 61.88\% accuracy and 60.00\% F1-score. These results demonstrate the effectiveness of our pose-based framework for Sign Language Spotting, establishing a strong foundation for future research in automatic sign language retrieval and verification. Code is available at https://github.com/EbimoJohnny/Pose-Based-Sign-Language-Spotting

</details>


### [97] [A Scalable Pipeline Combining Procedural 3D Graphics and Guided Diffusion for Photorealistic Synthetic Training Data Generation in White Button Mushroom Segmentation](https://arxiv.org/abs/2512.08747)
*ArtÃºr I. KÃ¡roly,PÃ©ter Galambos*

Main category: cs.CV

TL;DR: A novel workflow combining 3D rendering in Blender with constrained diffusion models generates photorealistic synthetic mushroom images for computer vision training, achieving state-of-the-art segmentation performance with only synthetic data.


<details>
  <summary>Details</summary>
Motivation: Industrial mushroom cultivation needs computer vision for monitoring/harvesting, but creating large annotated datasets is expensive. Synthetic data offers scalability but often lacks realism for real-world generalization.

Method: Integrates 3D rendering in Blender with constrained diffusion model to automatically generate high-quality annotated, photorealistic synthetic images of Agaricus Bisporus mushrooms. Preserves full control over 3D scene configuration and annotations while achieving photorealism without specialized graphics expertise.

Result: Released two synthetic datasets (6,000 images each, over 250k mushroom instances). Mask R-CNN models trained on synthetic data achieved state-of-the-art segmentation performance (F1 = 0.859 on M18K) in zero-shot testing on two independent real-world datasets, including a newly collected benchmark.

Conclusion: The workflow successfully generates photorealistic synthetic data that generalizes well to real-world scenarios. Although demonstrated on Agaricus Bisporus mushrooms, the pipeline can be adapted to other mushroom species or agricultural domains like fruit and leaf detection.

Abstract: Industrial mushroom cultivation increasingly relies on computer vision for monitoring and automated harvesting. However, developing accurate detection and segmentation models requires large, precisely annotated datasets that are costly to produce. Synthetic data provides a scalable alternative, yet often lacks sufficient realism to generalize to real-world scenarios. This paper presents a novel workflow that integrates 3D rendering in Blender with a constrained diffusion model to automatically generate high-quality annotated, photorealistic synthetic images of Agaricus Bisporus mushrooms. This approach preserves full control over 3D scene configuration and annotations while achieving photorealism without the need for specialized computer graphics expertise. We release two synthetic datasets (each containing 6,000 images depicting over 250k mushroom instances) and evaluate Mask R-CNN models trained on them in a zero-shot setting. When tested on two independent real-world datasets (including a newly collected benchmark), our method achieves state-of-the-art segmentation performance (F1 = 0.859 on M18K), despite using only synthetic training data. Although the approach is demonstrated on Agaricus Bisporus mushrooms, the proposed pipeline can be readily adapted to other mushroom species or to other agricultural domains, such as fruit and leaf detection.

</details>


### [98] [Astra: General Interactive World Model with Autoregressive Denoising](https://arxiv.org/abs/2512.08931)
*Yixuan Zhu,Jiaqi Feng,Wenzhao Zheng,Yuan Gao,Xin Tao,Pengfei Wan,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: Astra is an interactive general world model that generates real-world futures for diverse scenarios with precise action interactions, using autoregressive denoising with temporal causal attention and noise-augmented history memory.


<details>
  <summary>Details</summary>
Motivation: While diffusion transformers have advanced video generation, world models capable of predicting long-horizon futures from past observations and actions remain underexplored for general-purpose scenarios and various action forms.

Method: Autoregressive denoising architecture with temporal causal attention to aggregate past observations; noise-augmented history memory to balance responsiveness with coherence; action-aware adapter for precise action control; mixture of action experts for heterogeneous action modalities.

Result: Astra achieves interactive, consistent, and general long-term video prediction supporting various interactions. Experiments across multiple datasets show improvements in fidelity, long-range prediction, and action alignment over state-of-the-art world models.

Conclusion: Astra bridges the gap in general-purpose world models by enabling precise action-controlled long-term video prediction across diverse real-world scenarios like autonomous driving and robot grasping.

Abstract: Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.

</details>


### [99] [Skewness-Guided Pruning of Multimodal Swin Transformers for Federated Skin Lesion Classification on Edge Devices](https://arxiv.org/abs/2512.08751)
*Kuniko Paxton,Koorosh Aslansefat,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: A skewness-guided pruning method for multimodal Swin Transformers achieves 36% model size reduction with no accuracy loss in federated learning for medical imaging.


<details>
  <summary>Details</summary>
Motivation: High-performance medical vision models are too large for edge devices, and privacy constraints prevent centralized data management, necessitating federated learning with compressed models.

Method: Proposes skewness-guided pruning that selectively prunes Multi-Head Self-Attention and Multi-Layer Perceptron layers based on statistical skewness of their output distributions, validated in horizontal FL environment.

Result: Achieved approximately 36% model size reduction with no loss in accuracy on compact Swin Transformer in federated learning setting.

Conclusion: Demonstrates feasibility of efficient model compression and privacy-preserving distributed learning for multimodal medical AI on edge devices through skewness-guided pruning in federated learning.

Abstract: In recent years, high-performance computer vision models have achieved remarkable success in medical imaging, with some skin lesion classification systems even surpassing dermatology specialists in diagnostic accuracy. However, such models are computationally intensive and large in size, making them unsuitable for deployment on edge devices. In addition, strict privacy constraints hinder centralized data management, motivating the adoption of Federated Learning (FL). To address these challenges, this study proposes a skewness-guided pruning method that selectively prunes the Multi-Head Self-Attention and Multi-Layer Perceptron layers of a multimodal Swin Transformer based on the statistical skewness of their output distributions. The proposed method was validated in a horizontal FL environment and shown to maintain performance while substantially reducing model complexity. Experiments on the compact Swin Transformer demonstrate approximately 36\% model size reduction with no loss in accuracy. These findings highlight the feasibility of achieving efficient model compression and privacy-preserving distributed learning for multimodal medical AI on edge devices.

</details>


### [100] [Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance](https://arxiv.org/abs/2512.08765)
*Ruihang Chu,Yefei He,Zhekai Chen,Shiwei Zhang,Xiaogang Xu,Bin Xia,Dingdong Wang,Hongwei Yi,Xihui Liu,Hengshuang Zhao,Yu Liu,Yingya Zhang,Yujiu Yang*

Main category: cs.CV

TL;DR: Wan-Move is a framework that adds precise motion control to video generation models using dense point trajectories as motion guidance, achieving commercial-level quality without architecture changes.


<details>
  <summary>Details</summary>
Motivation: Existing motion-controllable video generation methods suffer from coarse control granularity and limited scalability, making them insufficient for practical applications. There's a need for precise, high-quality motion control that can scale effectively.

Method: Represents object motions with dense point trajectories, projects them into latent space, propagates first frame features along trajectories to create aligned spatiotemporal feature maps, and uses these as motion guidance for existing image-to-video models without architecture changes.

Result: Generates 5-second, 480p videos with motion controllability rivaling commercial systems like Kling 1.5 Pro's Motion Brush. Introduces MoveBench benchmark with diverse content, longer durations, and high-quality annotations for comprehensive evaluation.

Conclusion: Wan-Move provides a simple, scalable solution for precise motion control in video generation, eliminating need for auxiliary motion encoders and enabling easy fine-tuning of base models while achieving commercial-level quality.

Abstract: We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.

</details>


### [101] [LoFA: Learning to Predict Personalized Priors for Fast Adaptation of Visual Generative Models](https://arxiv.org/abs/2512.08785)
*Yiming Hao,Mutian Xu,Chongjie Ye,Jie Qin,Shunlin Lu,Yipeng Qin,Xiaoguang Han*

Main category: cs.CV

TL;DR: LoFA: A hypernetwork framework that predicts personalized LoRA weights in seconds by learning structured distribution patterns in parameter changes, outperforming traditional LoRA optimization.


<details>
  <summary>Details</summary>
Motivation: Current personalization methods like LoRA require task-specific data and lengthy optimization, while existing hypernetwork approaches struggle to map fine-grained user prompts to complex LoRA weight distributions.

Method: Two-stage hypernetwork: first predicts relative distribution patterns that identify key adaptation regions in LoRA parameter changes, then uses these patterns to guide final LoRA weight prediction.

Result: Consistently predicts high-quality personalized priors within seconds across multiple tasks and user prompts, outperforming conventional LoRA that requires hours of processing.

Conclusion: LoFA bridges the gap between efficient personalization and high-quality adaptation by leveraging structured distribution patterns in LoRA parameter changes, enabling practical real-time model personalization.

Abstract: Personalizing visual generative models to meet specific user needs has gained increasing attention, yet current methods like Low-Rank Adaptation (LoRA) remain impractical due to their demand for task-specific data and lengthy optimization. While a few hypernetwork-based approaches attempt to predict adaptation weights directly, they struggle to map fine-grained user prompts to complex LoRA distributions, limiting their practical applicability. To bridge this gap, we propose LoFA, a general framework that efficiently predicts personalized priors for fast model adaptation. We first identify a key property of LoRA: structured distribution patterns emerge in the relative changes between LoRA and base model parameters. Building on this, we design a two-stage hypernetwork: first predicting relative distribution patterns that capture key adaptation regions, then using these to guide final LoRA weight prediction. Extensive experiments demonstrate that our method consistently predicts high-quality personalized priors within seconds, across multiple tasks and user prompts, even outperforming conventional LoRA that requires hours of processing. Project page: https://jaeger416.github.io/lofa/.

</details>


### [102] [Generation is Required for Data-Efficient Perception](https://arxiv.org/abs/2512.08854)
*Jack Brady,Bernhard SchÃ¶lkopf,Thomas Kipf,Simon Buchholz,Wieland Brendel*

Main category: cs.CV

TL;DR: Generative methods with decoder inversion enable compositional generalization in vision, while non-generative encoder methods struggle without large-scale pretraining or extra supervision.


<details>
  <summary>Details</summary>
Motivation: To investigate whether generative approaches (with decoder inversion) are necessary for human-level visual perception, specifically for achieving compositional generalization - a key aspect of human perception that current successful non-generative vision models may lack.

Method: Theoretical analysis formalizing inductive biases needed for compositional generalization in both generative (decoder-based) and non-generative (encoder-based) methods. Empirical evaluation on photorealistic image datasets comparing various generative and non-generative approaches.

Result: Generative methods can enforce necessary inductive biases through decoder constraints and inversion (via gradient-based search or generative replay), enabling compositional generalization. Non-generative methods generally cannot enforce these biases through regularization or architecture, often failing to generalize compositionally without large-scale pretraining or added supervision.

Conclusion: Generative approaches with decoder inversion are crucial for achieving compositional generalization in visual perception, supporting the hypothesis that human-level perception requires generative mechanisms. This provides theoretical and empirical evidence favoring generative methods over purely discriminative approaches for compositional reasoning.

Abstract: It has been hypothesized that human-level visual perception requires a generative approach in which internal representations result from inverting a decoder. Yet today's most successful vision models are non-generative, relying on an encoder that maps images to representations without decoder inversion. This raises the question of whether generation is, in fact, necessary for machines to achieve human-level visual perception. To address this, we study whether generative and non-generative methods can achieve compositional generalization, a hallmark of human perception. Under a compositional data generating process, we formalize the inductive biases required to guarantee compositional generalization in decoder-based (generative) and encoder-based (non-generative) methods. We then show theoretically that enforcing these inductive biases on encoders is generally infeasible using regularization or architectural constraints. In contrast, for generative methods, the inductive biases can be enforced straightforwardly, thereby enabling compositional generalization by constraining a decoder and inverting it. We highlight how this inversion can be performed efficiently, either online through gradient-based search or offline through generative replay. We examine the empirical implications of our theory by training a range of generative and non-generative methods on photorealistic image datasets. We find that, without the necessary inductive biases, non-generative methods often fail to generalize compositionally and require large-scale pretraining or added supervision to improve generalization. By comparison, generative methods yield significant improvements in compositional generalization, without requiring additional data, by leveraging suitable inductive biases on a decoder along with search and replay.

</details>


### [103] [Tri-Bench: Stress-Testing VLM Reliability on Spatial Reasoning under Camera Tilt and Object Interference](https://arxiv.org/abs/2512.08860)
*Amit Bendkhale*

Main category: cs.CV

TL;DR: Tri-Bench benchmark shows VLMs struggle with geometric reasoning, especially with camera tilt and minority triangle types, despite explicit reference frame hints.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models often fail under realistic scene changes, and verifiable geometric reasoning is critical for trustworthy agentic AI. The paper aims to isolate relative geometric reasoning while testing deployment-critical factors.

Method: Created Tri-Bench - a compact benchmark of planar triangle problems testing camera pose (planar vs. tilted) and scene context via object interference. Evaluated four recent VLMs using a single fixed prompt with explicit square border guardrail for homography-based answers.

Result: Overall accuracy modest (~69% average, best ~75%, worst ~64%). Accuracy higher with 2D projections (~72%). VLMs fail completely (~0% accuracy) on minority triangle classes (equilateral, isosceles, right-angled). Camera tilt degrades accuracy by ~4.1%. Object interference has no significant effect.

Conclusion: VLMs fail to utilize explicit frame-of-reference hints, defaulting to 2D image plane cues. Models struggle with geometric reasoning under realistic conditions, highlighting limitations for trustworthy agentic AI applications.

Abstract: Verifiable geometric reasoning is a critical component for trustworthy and controllable agentic AI. Despite impressive capabilities, Vision-Language Models (VLMs) often fail under realistic scene changes. We present Tri-Bench, a compact benchmark of planar triangle problems that isolates relative geometric reasoning while stressing two deployment-critical factors: camera pose (planar vs. tilted) and scene context via object interference (10 everyday objects). To test verifiability and control, we evaluate four recent VLMs using a single, fixed prompt whose guardrail explicitly describes a surrounding square border, enabling correct answers via homography. We evaluate six simple tasks over binary and continuous targets, and observe that the overall accuracy with respect to 3D ground truth is modest, ~69% on average (best ~75%, worst ~64%). The same responses align even more closely with 2D projections in the image plane, where mean accuracy is ~72%. All four VLMs consistently fail, with accuracy falling to ~0%, on recognizing minority shape classes (equilateral, isosceles, right-angled triangles). Additionally, overall VLM accuracy degrades by ~4.1% under camera tilt. This demonstrates that models fail to correctly utilize the explicit frame-of-reference hint provided in the prompt and default to 2D image plane cues. Finally, we find that object interference has no significant effect on VLM accuracy.

</details>


### [104] [SATGround: A Spatially-Aware Approach for Visual Grounding in Remote Sensing](https://arxiv.org/abs/2512.08881)
*Aysim Toker,Andreea-Maria Oncescu,Roy Miles,Ismail Elezi,Jiankang Deng*

Main category: cs.CV

TL;DR: Enhanced VLM-based visual grounding in satellite imagery using structured localization mechanism with specialized control tokens, achieving 24.8% improvement over previous methods.


<details>
  <summary>Details</summary>
Motivation: Vision-language models (VLMs) are becoming powerful tools for remote sensing, but need improved visual grounding capabilities for precise object localization in complex satellite scenes.

Method: Finetune pretrained VLM on diverse instruction-following tasks while interfacing a dedicated grounding module through specialized control tokens for localization, enabling joint reasoning over language and spatial information.

Result: Consistent state-of-the-art improvements on remote sensing benchmarks, including 24.8% relative improvement over previous methods on visual grounding tasks.

Conclusion: Integrating structured spatial reasoning into VLMs enhances precise object localization in satellite imagery, paving the way for more reliable real-world satellite data analysis.

Abstract: Vision-language models (VLMs) are emerging as powerful generalist tools for remote sensing, capable of integrating information across diverse tasks and enabling flexible, instruction-based interactions via a chat interface. In this work, we enhance VLM-based visual grounding in satellite imagery by proposing a novel structured localization mechanism. Our approach involves finetuning a pretrained VLM on a diverse set of instruction-following tasks, while interfacing a dedicated grounding module through specialized control tokens for localization. This method facilitates joint reasoning over both language and spatial information, significantly enhancing the model's ability to precisely localize objects in complex satellite scenes. We evaluate our framework on several remote sensing benchmarks, consistently improving the state-of-the-art, including a 24.8% relative improvement over previous methods on visual grounding. Our results highlight the benefits of integrating structured spatial reasoning into VLMs, paving the way for more reliable real-world satellite data analysis.

</details>


### [105] [Accelerated Rotation-Invariant Convolution for UAV Image Segmentation](https://arxiv.org/abs/2512.08888)
*Manduhu Manduhu,Alexander Dow,Gerard Dooly,James Riordan*

Main category: cs.CV

TL;DR: GPU-optimized rotation-invariant convolution framework for UAV aerial imagery segmentation that eliminates im2col step, reduces memory/computation via structured data sharing among rotated filters, achieving significant speedups and energy savings while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Rotation invariance is crucial for precise object-level segmentation in UAV aerial imagery where targets have arbitrary orientations. Conventional CNNs lack rotation invariance, degrading segmentation accuracy across varying viewpoints. Existing rotation-invariant methods using multiple orientation filters are computationally expensive and memory-intensive.

Method: Proposes a GPU-optimized rotation-invariant convolution framework that eliminates the traditional im2col step. Exploits structured data sharing among symmetrically rotated filters to reduce memory traffic and computational redundancy. Generalizes the approach to accelerate convolution with arbitrary (non-symmetric) rotation angles.

Result: Achieves 20-55% faster training and 15-45% lower energy consumption than CUDNN while maintaining comparable accuracy to state-of-the-art rotation-invariant methods. In 8-orientation setting: up to 45% speedup and 41% energy savings on 256Ã256 inputs, 32% speedup and 23% lower energy on 1024Ã1024 inputs. Integrated into U-Net yields up to 6% accuracy improvement over non-rotation-aware baseline.

Conclusion: The proposed method provides an effective and highly efficient alternative to existing rotation-invariant CNN frameworks, enabling rotation-invariant segmentation with significantly reduced computational cost and energy consumption while maintaining or improving accuracy.

Abstract: Rotation invariance is essential for precise, object-level segmentation in UAV aerial imagery, where targets can have arbitrary orientations and exhibit fine-scale details. Conventional segmentation architectures like U-Net rely on convolution operators that are not rotation-invariant, leading to degraded segmentation accuracy across varying viewpoints. Rotation invariance can be achieved by expanding the filter bank across multiple orientations; however, this will significantly increase computational cost and memory traffic. In this paper, we introduce a GPU-optimized rotation-invariant convolution framework that eliminates the traditional data-lowering (im2col) step required for matrix-multiplication-based convolution. By exploiting structured data sharing among symmetrically rotated filters, our method achieves multi-orientation convolution with greatly reduced memory traffic and computational redundancy. We further generalize the approach to accelerate convolution with arbitrary (non-symmetric) rotation angles.
  Across extensive benchmarks, the proposed convolution achieves 20--55% faster training and 15--45% lower energy consumption than CUDNN, while maintaining accuracy comparable to state-of-the-art rotation-invariant methods. In the eight-orientation setting, our approach achieves up to 45% speedup and 41% energy savings on 256\(\times\)256 inputs, and 32% speedup and 23% lower energy usage on 1024\(\times\)1024 inputs. Integrated into a U-Net segmentation model, the framework yields up to 6% improvement in accuracy over the non-rotation-aware baseline. These results demonstrate that the proposed method provides an effective and highly efficient alternative to existing rotation-invariant CNN frameworks.

</details>


### [106] [UniLayDiff: A Unified Diffusion Transformer for Content-Aware Layout Generation](https://arxiv.org/abs/2512.08897)
*Zeyang Liu,Le Wang,Sanping Zhou,Yuxuan Wu,Xiaolong Sun,Gang Hua,Haoxiang Li*

Main category: cs.CV

TL;DR: UniLayDiff is a unified diffusion transformer model that handles various content-aware layout generation tasks with a single end-to-end trainable model, treating constraints as distinct modalities and achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current methods for content-aware layout generation either handle only subsets of tasks or require separate models for different constraints, lacking a truly unified solution for diverse real-world applications.

Method: Proposes UniLayDiff, a Unified Diffusion Transformer that treats layout constraints as distinct modalities using Multi-Modal Diffusion Transformer framework. Integrates relation constraints through LoRA fine-tuning after pretraining on other tasks.

Result: Achieves state-of-the-art performance across unconditional and various conditional generation tasks, and is the first model to unify the full range of content-aware layout generation tasks.

Conclusion: UniLayDiff successfully addresses the challenge of unifying diverse layout generation tasks with a single model, demonstrating superior performance and offering a comprehensive solution for graphic design automation.

Abstract: Content-aware layout generation is a critical task in graphic design automation, focused on creating visually appealing arrangements of elements that seamlessly blend with a given background image. The variety of real-world applications makes it highly challenging to develop a single model capable of unifying the diverse range of input-constrained generation sub-tasks, such as those conditioned by element types, sizes, or their relationships. Current methods either address only a subset of these tasks or necessitate separate model parameters for different conditions, failing to offer a truly unified solution. In this paper, we propose UniLayDiff: a Unified Diffusion Transformer, that for the first time, addresses various content-aware layout generation tasks with a single, end-to-end trainable model. Specifically, we treat layout constraints as a distinct modality and employ Multi-Modal Diffusion Transformer framework to capture the complex interplay between the background image, layout elements, and diverse constraints. Moreover, we integrate relation constraints through fine-tuning the model with LoRA after pretraining the model on other tasks. Such a schema not only achieves unified conditional generation but also enhances overall layout quality. Extensive experiments demonstrate that UniLayDiff achieves state-of-the-art performance across from unconditional to various conditional generation tasks and, to the best of our knowledge, is the first model to unify the full range of content-aware layout generation tasks.

</details>


### [107] [Self-Evolving 3D Scene Generation from a Single Image](https://arxiv.org/abs/2512.08905)
*Kaizhi Zheng,Yue Fan,Jing Gu,Zishuo Xu,Xuehai He,Xin Eric Wang*

Main category: cs.CV

TL;DR: EvoScene is a training-free framework that progressively reconstructs complete 3D scenes from single images by combining geometric reasoning from 3D generation models with visual knowledge from video generation models through iterative 2D-3D domain alternation.


<details>
  <summary>Details</summary>
Motivation: Existing image-to-3D generators have object-centric training that limits generalization to complex, large-scale scenes with faithful structure and texture. There's a need for methods that can handle complex scenes with both good geometry and texture from single views.

Method: EvoScene uses a self-evolving, training-free framework with three iterative stages: 1) Spatial Prior Initialization, 2) Visual-guided 3D Scene Mesh Generation, and 3) Spatial-guided Novel View Generation. It alternates between 2D and 3D domains, combining strengths from 3D generation models (geometric reasoning) and video generation models (visual knowledge).

Result: Experiments on diverse scenes show EvoScene achieves superior geometric stability, view-consistent textures, and unseen-region completion compared to strong baselines, producing ready-to-use 3D meshes for practical applications.

Conclusion: EvoScene successfully addresses the challenge of generating high-quality textured 3D scenes from single images by progressively improving both structure and appearance through iterative 2D-3D domain alternation, outperforming existing methods on complex scene reconstruction.

Abstract: Generating high-quality, textured 3D scenes from a single image remains a fundamental challenge in vision and graphics. Recent image-to-3D generators recover reasonable geometry from single views, but their object-centric training limits generalization to complex, large-scale scenes with faithful structure and texture. We present EvoScene, a self-evolving, training-free framework that progressively reconstructs complete 3D scenes from single images. The key idea is combining the complementary strengths of existing models: geometric reasoning from 3D generation models and visual knowledge from video generation models. Through three iterative stages--Spatial Prior Initialization, Visual-guided 3D Scene Mesh Generation, and Spatial-guided Novel View Generation--EvoScene alternates between 2D and 3D domains, gradually improving both structure and appearance. Experiments on diverse scenes demonstrate that EvoScene achieves superior geometric stability, view-consistent textures, and unseen-region completion compared to strong baselines, producing ready-to-use 3D meshes for practical applications.

</details>


### [108] [LiDAS: Lighting-driven Dynamic Active Sensing for Nighttime Perception](https://arxiv.org/abs/2512.08912)
*Simon de Moreau,Andrei Bursuc,Hafid El-Idrissi,Fabien Moutarde*

Main category: cs.CV

TL;DR: LiDAS is an active illumination system that dynamically optimizes headlight patterns to improve nighttime perception performance without retraining daytime models.


<details>
  <summary>Details</summary>
Motivation: Nighttime environments challenge camera-based perception as existing methods passively rely on scene lighting. There's a need for active illumination control to enable daytime-trained models to work effectively at night without retraining.

Method: Closed-loop active illumination system combining off-the-shelf perception models with HD headlights. Dynamically predicts optimal illumination field to maximize perception performance, reallocating light from empty areas to object regions. Trained on synthetic data and deployed zero-shot in real-world driving.

Result: +18.7% mAP50 and +5.0% mIoU over standard low-beam at equal power. Maintains performance while reducing energy use by 40%. Enables zero-shot nighttime generalization of daytime-trained models.

Conclusion: LiDAS offers a cost-effective solution to robust nighttime perception by turning headlights into active vision actuators, complementing domain-generalization methods without requiring model retraining.

Abstract: Nighttime environments pose significant challenges for camera-based perception, as existing methods passively rely on the scene lighting. We introduce Lighting-driven Dynamic Active Sensing (LiDAS), a closed-loop active illumination system that combines off-the-shelf visual perception models with high-definition headlights. Rather than uniformly brightening the scene, LiDAS dynamically predicts an optimal illumination field that maximizes downstream perception performance, i.e., decreasing light on empty areas to reallocate it on object regions. LiDAS enables zero-shot nighttime generalization of daytime-trained models through adaptive illumination control. Trained on synthetic data and deployed zero-shot in real-world closed-loop driving scenarios, LiDAS enables +18.7% mAP50 and +5.0% mIoU over standard low-beam at equal power. It maintains performances while reducing energy use by 40%. LiDAS complements domain-generalization methods, further strengthening robustness without retraining. By turning readily available headlights into active vision actuators, LiDAS offers a cost-effective solution to robust nighttime perception.

</details>


### [109] [Unified Diffusion Transformer for High-fidelity Text-Aware Image Restoration](https://arxiv.org/abs/2512.08922)
*Jin Hyeon Kim,Paul Hyunbin Cho,Claire Kim,Jaewon Min,Jaeeun Lee,Jihye Park,Yeji Choi,Seungryong Kim*

Main category: cs.CV

TL;DR: UniT is a unified text restoration framework that combines Diffusion Transformer, Vision-Language Model, and Text Spotting Module to recover high-quality images from degraded text inputs while reducing text hallucinations.


<details>
  <summary>Details</summary>
Motivation: Diffusion models for image restoration often produce text hallucinations in text-centric tasks due to lack of explicit linguistic knowledge, creating a need for text-aware restoration that preserves textual content accurately.

Method: UniT integrates three components iteratively: 1) Diffusion Transformer (DiT) backbone for strong generative priors, 2) Vision-Language Model (VLM) to extract textual content from degraded images, and 3) Text Spotting Module (TSM) trained on diffusion features to generate intermediate OCR predictions at each denoising step, enabling iterative refinement of textual guidance.

Result: Experiments on SA-Text and Real-Text benchmarks show UniT faithfully reconstructs degraded text, substantially reduces hallucinations, and achieves state-of-the-art end-to-end F1-score performance in Text-Aware Image Restoration tasks.

Conclusion: UniT successfully addresses text hallucinations in diffusion-based image restoration by integrating explicit linguistic guidance through VLM and iterative refinement via TSM, demonstrating superior performance in text-aware image restoration.

Abstract: Text-Aware Image Restoration (TAIR) aims to recover high- quality images from low-quality inputs containing degraded textual content. While diffusion models provide strong gen- erative priors for general image restoration, they often pro- duce text hallucinations in text-centric tasks due to the ab- sence of explicit linguistic knowledge. To address this, we propose UniT, a unified text restoration framework that in- tegrates a Diffusion Transformer (DiT), a Vision-Language Model (VLM), and a Text Spotting Module (TSM) in an it- erative fashion for high-fidelity text restoration. In UniT, the VLM extracts textual content from degraded images to provide explicit textual guidance. Simultaneously, the TSM, trained on diffusion features, generates intermedi- ate OCR predictions at each denoising step, enabling the VLM to iteratively refine its guidance during the denoising process. Finally, the DiT backbone, leveraging its strong representational power, exploit these cues to recover fine- grained textual content while effectively suppressing text hallucinations. Experiments on the SA-Text and Real-Text benchmarks demonstrate that UniT faithfully reconstructs degraded text, substantially reduces hallucinations, and achieves state-of-the-art end-to-end F1-score performance in TAIR task.

</details>


### [110] [Efficiently Reconstructing Dynamic Scenes One D4RT at a Time](https://arxiv.org/abs/2512.08924)
*Chuhan Zhang,Guillaume Le Moing,Skanda Koppula,Ignacio Rocco,Liliane Momeni,Junyu Xie,Shuyang Sun,Rahul Sukthankar,JoÃ«lle K Barral,Raia Hadsell,Zoubin Ghahramani,Andrew Zisserman,Junlin Zhang,Mehdi SM Sajjadi*

Main category: cs.CV

TL;DR: D4RT is a feedforward transformer model that jointly infers depth, spatio-temporal correspondence, and camera parameters from single videos for efficient 4D scene reconstruction.


<details>
  <summary>Details</summary>
Motivation: Reconstructing complex geometry and motion of dynamic scenes from video is challenging. Existing methods often involve heavy computation with dense per-frame decoding or complex multi-decoder architectures.

Method: Uses unified transformer architecture with novel querying mechanism that avoids dense per-frame decoding and multiple task-specific decoders. Allows flexible probing of 3D position of any point in space and time through a decoding interface.

Result: Sets new state of the art, outperforming previous methods across wide spectrum of 4D reconstruction tasks. Achieves lightweight, highly scalable design enabling remarkably efficient training and inference.

Conclusion: D4RT provides a simple yet powerful solution for dynamic scene reconstruction from video, offering computational efficiency and strong performance through its unified transformer architecture and novel querying approach.

Abstract: Understanding and reconstructing the complex geometry and motion of dynamic scenes from video remains a formidable challenge in computer vision. This paper introduces D4RT, a simple yet powerful feedforward model designed to efficiently solve this task. D4RT utilizes a unified transformer architecture to jointly infer depth, spatio-temporal correspondence, and full camera parameters from a single video. Its core innovation is a novel querying mechanism that sidesteps the heavy computation of dense, per-frame decoding and the complexity of managing multiple, task-specific decoders. Our decoding interface allows the model to independently and flexibly probe the 3D position of any point in space and time. The result is a lightweight and highly scalable method that enables remarkably efficient training and inference. We demonstrate that our approach sets a new state of the art, outperforming previous methods across a wide spectrum of 4D reconstruction tasks. We refer to the project webpage for animated results: https://d4rt-paper.github.io/.

</details>


### [111] [Selfi: Self Improving Reconstruction Engine via 3D Geometric Feature Alignment](https://arxiv.org/abs/2512.08930)
*Youming Deng,Songyou Peng,Junyi Zhang,Kathryn Heal,Tiancheng Sun,John Flynn,Steve Marschner,Lucy Chai*

Main category: cs.CV

TL;DR: Selfi improves 3D reconstruction by aligning VGGT features for better geometric consistency, achieving SOTA in novel view synthesis and camera pose estimation.


<details>
  <summary>Details</summary>
Motivation: Traditional NVS uses explicit 3D biases with known camera parameters, while VGGT learns 3D implicitly but lacks explicit multi-view geometric consistency. Improving 3D feature consistency benefits both NVS and pose estimation.

Method: Selfi is a self-improving pipeline that transforms VGGT into a 3D reconstruction engine using its own outputs as pseudo-ground-truth. It trains a lightweight feature adapter with reprojection-based consistency loss to distill VGGT outputs into geometrically-aligned features.

Result: Achieves state-of-the-art performance in both novel view synthesis and camera pose estimation.

Conclusion: Feature alignment is a highly beneficial step for downstream 3D reasoning, enabling VGGT to produce geometrically consistent 3D representations.

Abstract: Novel View Synthesis (NVS) has traditionally relied on models with explicit 3D inductive biases combined with known camera parameters from Structure-from-Motion (SfM) beforehand. Recent vision foundation models like VGGT take an orthogonal approach -- 3D knowledge is gained implicitly through training data and loss objectives, enabling feed-forward prediction of both camera parameters and 3D representations directly from a set of uncalibrated images. While flexible, VGGT features lack explicit multi-view geometric consistency, and we find that improving such 3D feature consistency benefits both NVS and pose estimation tasks. We introduce Selfi, a self-improving 3D reconstruction pipeline via feature alignment, transforming a VGGT backbone into a high-fidelity 3D reconstruction engine by leveraging its own outputs as pseudo-ground-truth. Specifically, we train a lightweight feature adapter using a reprojection-based consistency loss, which distills VGGT outputs into a new geometrically-aligned feature space that captures spatial proximity in 3D. This enables state-of-the-art performance in both NVS and camera pose estimation, demonstrating that feature alignment is a highly beneficial step for downstream 3D reasoning.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [112] [Missing Wedge Inpainting and Joint Alignment in Electron Tomography through Implicit Neural Representations](https://arxiv.org/abs/2512.08113)
*Cedric Lim,Corneel Casert,Arthur R. C. McCray,Serin Lee,Andrew Barnum,Jennifer Dionne,Colin Ophus*

Main category: eess.IV

TL;DR: Self-supervised implicit neural representation approach for electron tomography that addresses missing-wedge artifacts, data misalignment, and noise without requiring training data.


<details>
  <summary>Details</summary>
Motivation: Conventional electron tomography reconstruction suffers from missing-wedge artifacts and data misalignment due to experimental constraints. Existing supervised machine learning methods require training data and lack generalization across different materials systems.

Method: Proposes a fully self-supervised implicit neural representation (INR) approach using a neural network as a regularizer. The method enables fast inline alignment through pose optimization, missing wedge inpainting, and denoising of low dose datasets via model regularization using only a single dataset.

Result: The method produces high-quality tomograms from diverse and information-limited datasets. It shows high fidelity reconstructions with minimal user input and preprocessing, and can be readily applied to a wide variety of materials samples and experimental parameters.

Conclusion: INR-based self-supervised reconstructions offer an effective solution for electron tomography that overcomes limitations of conventional algorithms and supervised learning approaches, providing high-quality results with broad applicability across materials systems.

Abstract: Electron tomography is a powerful tool for understanding the morphology of materials in three dimensions, but conventional reconstruction algorithms typically suffer from missing-wedge artifacts and data misalignment imposed by experimental constraints. Recently proposed supervised machine-learning-enabled reconstruction methods to address these challenges rely on training data and are therefore difficult to generalize across materials systems. We propose a fully self-supervised implicit neural representation (INR) approach using a neural network as a regularizer. Our approach enables fast inline alignment through pose optimization, missing wedge inpainting, and denoising of low dose datasets via model regularization using only a single dataset. We apply our method to simulated and experimental data and show that it produces high-quality tomograms from diverse and information limited datasets. Our results show that INR-based self-supervised reconstructions offer high fidelity reconstructions with minimal user input and preprocessing, and can be readily applied to a wide variety of materials samples and experimental parameters.

</details>


### [113] [FlowSteer: Conditioning Flow Field for Consistent Image Restoration](https://arxiv.org/abs/2512.08125)
*Tharindu Wickremasinghe,Chenyang Qi,Harshana Weligampola,Zhengzhong Tu,Stanley H. Chan*

Main category: eess.IV

TL;DR: FlowSteer is a zero-shot conditioning scheme that improves flow-based text-to-image models for image restoration tasks by injecting measurement priors during sampling, without retraining or adapters.


<details>
  <summary>Details</summary>
Motivation: Flow-based T2I models drift from measurement fidelity in image restoration tasks. Existing solutions require task-specific adapters or retraining, which are computationally heavy and not scalable.

Method: FlowSteer injects measurement priors along the sampling path, coupling a frozen flow's implicit guidance with explicit measurement constraints. It's an operator-aware conditioning scheme that leverages flow model sensitivities to noise.

Result: Across super-resolution, deblurring, denoising, and colorization, FlowSteer improves measurement consistency and identity preservation in zero-shot settings without retrained models or adapters.

Conclusion: FlowSteer achieves higher fidelity reconstructed images while leveraging flow models' rich generative priors, offering a simple yet effective solution for image restoration with flow-based T2I models.

Abstract: Flow-based text-to-image (T2I) models excel at prompt-driven image generation, but falter on Image Restoration (IR), often "drifting away" from being faithful to the measurement. Prior work mitigate this drift with data-specific flows or task-specific adapters that are computationally heavy and not scalable across tasks. This raises the question "Can't we efficiently manipulate the existing generative capabilities of a flow model?" To this end, we introduce FlowSteer (FS), an operator-aware conditioning scheme that injects measurement priors along the sampling path,coupling a frozed flow's implicit guidance with explicit measurement constraints. Across super-resolution, deblurring, denoising, and colorization, FS improves measurement consistency and identity preservation in a strictly zero-shot setting-no retrained models, no adapters. We show how the nature of flow models and their sensitivities to noise inform the design of such a scheduler. FlowSteer, although simple, achieves a higher fidelity of reconstructed images, while leveraging the rich generative priors of flow models.

</details>


### [114] [Tumor-anchored deep feature random forests for out-of-distribution detection in lung cancer segmentation](https://arxiv.org/abs/2512.08216)
*Aneesh Rangnekar,Harini Veeraraghavan*

Main category: eess.IV

TL;DR: RF-Deep: A lightweight random forests-based OOD detection framework for tumor segmentation in CT scans that uses deep features from pretrained transformers to detect out-of-distribution inputs with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Current tumor segmentation models using SSL-pretrained transformers are vulnerable to OOD inputs, producing confidently incorrect segmentations that pose clinical risks. Existing OOD detection methods have limitations: logit-based approaches suffer from task-specific biases, while architectural modifications increase computational costs.

Method: Plug-and-play post-hoc framework using random forests with limited outlier exposure. Extracts hierarchical features from pretrained-then-finetuned backbone encoder, focusing on multiple regions of interest anchored to predicted tumor segmentations. Scales to varying fields-of-view and is architecture-agnostic.

Result: Achieved AUROC > 93.50 for challenging near-OOD datasets (pulmonary embolism, negative COVID-19) and near-perfect AUROC > 99.00 for far-OOD datasets (kidney cancer, healthy pancreas). Outperformed logit-based and radiomics approaches. Maintained consistent performance across networks with different depths and pretraining strategies.

Conclusion: RF-Deep provides an effective, lightweight, architecture-agnostic solution for OOD detection in tumor segmentation, enhancing reliability for safe clinical deployment without adding significant computational overhead.

Abstract: Accurate segmentation of cancerous lesions from 3D computed tomography (CT) scans is essential for automated treatment planning and response assessment. However, even state-of-the-art models combining self-supervised learning (SSL) pretrained transformers with convolutional decoders are susceptible to out-of-distribution (OOD) inputs, generating confidently incorrect tumor segmentations, posing risks for safe clinical deployment. Existing logit-based methods suffer from task-specific model biases, while architectural enhancements to explicitly detect OOD increase parameters and computational costs. Hence, we introduce a plug-and-play and lightweight post-hoc random forests-based OOD detection framework called RF-Deep that leverages deep features with limited outlier exposure. RF-Deep enhances generalization to imaging variations by repurposing the hierarchical features from the pretrained-then-finetuned backbone encoder, providing task-relevant OOD detection by extracting the features from multiple regions of interest anchored to the predicted tumor segmentations. Hence, it scales to images of varying fields-of-view. We compared RF-Deep against existing OOD detection methods using 1,916 CT scans across near-OOD (pulmonary embolism, negative COVID-19) and far-OOD (kidney cancer, healthy pancreas) datasets. RF-Deep achieved AUROC > 93.50 for the challenging near-OOD datasets and near-perfect detection (AUROC > 99.00) for the far-OOD datasets, substantially outperforming logit-based and radiomics approaches. RF-Deep maintained similar performance consistency across networks of different depths and pretraining strategies, demonstrating its effectiveness as a lightweight, architecture-agnostic approach to enhance the reliability of tumor segmentation from CT volumes.

</details>


### [115] [Learned iterative networks: An operator learning perspective](https://arxiv.org/abs/2512.08444)
*Andreas Hauptmann,Ozan Ãktem*

Main category: eess.IV

TL;DR: This chapter presents a unified operator view for learned iterative networks in image reconstruction, connecting functional analytic foundations with discrete implementations.


<details>
  <summary>Details</summary>
Motivation: Learned iterative networks have become successful for image reconstruction, but there's a gap between their functional analytic foundations and discrete implementations. The chapter aims to provide a unified operator perspective to bridge this gap.

Method: The authors formulate a learned reconstruction operator with two components: how to compute (algorithmic implementation) and what to compute (learning problem). They present common approaches within this framework and show their core relationships.

Result: The chapter demonstrates that many learned iterative network approaches are closely related in their core structure when viewed through the unified operator framework. It reviews both linear and nonlinear inverse problems in this context.

Conclusion: A unified operator view provides a coherent framework for understanding learned iterative networks, connecting their functional analytic foundations with practical implementations, and revealing fundamental relationships between different approaches.

Abstract: Learned image reconstruction has become a pillar in computational imaging and inverse problems. Among the most successful approaches are learned iterative networks, which are formulated by unrolling classical iterative optimisation algorithms for solving variational problems. While the underlying algorithm is usually formulated in the functional analytic setting, learned approaches are often viewed as purely discrete. In this chapter we present a unified operator view for learned iterative networks. Specifically, we formulate a learned reconstruction operator, defining how to compute, and separately the learning problem, which defines what to compute. In this setting we present common approaches and show that many approaches are closely related in their core. We review linear as well as nonlinear inverse problems in this framework and present a short numerical study to conclude.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [116] [Signal and Noise Classification in Bio-Signals via unsupervised Machine Learning](https://arxiv.org/abs/2512.07851)
*Sansrit Paudel*

Main category: eess.SP

TL;DR: Paper presents a biosignal noise classification system using K-means clustering to distinguish clean segments from noisy ones and categorize different noise types.


<details>
  <summary>Details</summary>
Motivation: Real-world biosignal data is frequently corrupted by various types of noise (motion artifacts, baseline wander, sensor failure), and heavily degraded signals cannot be recovered through traditional digital signal processing techniques alone.

Method: Implemented K-means clustering algorithm to perform two classification tasks: 1) binary classification of noisy vs. clean biosignals, and 2) categorization of various noise types (motion artifacts, sensor failure, etc.).

Result: The algorithm demonstrates strong performance in reliably grouping clean segments from noisy ones, with particularly strong performance in identifying clean data compared to various categories of noise.

Conclusion: This approach enables selection of only high-quality biosignal segments, providing accurate results for feature engineering that may enhance the precision of machine learning models trained on biosignals.

Abstract: Real-world biosignal data is frequently corrupted by various types of noise, such as motion artifacts, and baseline wander. Although digital signal processing techniques exist to process such signals; however, heavily degraded signals cannot be recovered. In this study, we aim to classify two things: first, a binary classification of noisy and clean biosignals, and next, to categorize various kinds of noise such as motion artifacts, sensor failure, etc. We implemented K-means clustering, and our results indicate that the algorithm can most reliably group clean segments from noisy ones, particularly strong performance in identifying clean data compared to various categories of noise. This approach enables the selection of only high-quality bio-signal segments and provides accurate results for feature engineering that may enhance the precision of machine learning models trained on biosignals.

</details>


### [117] [Polarization-Diversity-Based Rotation Sensing Methodology Using COTS UHF RFID Tags](https://arxiv.org/abs/2512.08069)
*Florian Muralter,Fabian Muralter,Hugo Landaluce,Asier Perallos*

Main category: eess.SP

TL;DR: A polarization diversity-based rotation sensing method using COTS UHF RFID tags with SDR reader, analyzing phase changes from backscatter modulation to measure rotation speed.


<details>
  <summary>Details</summary>
Motivation: To develop a practical rotation sensing solution using existing UHF RFID infrastructure, leveraging phase-based sensing capabilities for IoT applications without requiring specialized hardware.

Method: Uses polarization diversity with COTS UHF RFID tags and SDR reader, performs coherent demodulation of tag-to-reader messages, calculates difference signal from backscatter load modulation states, and computes rotation speed from phase change over time.

Result: Experimental results validate the theoretical model and demonstrate the system's performance and limitations for rotation sensing applications.

Conclusion: The proposed methodology enables rotation sensing using standard UHF RFID infrastructure, expanding IoT sensing capabilities through phase-based analysis of backscatter modulation.

Abstract: Phase-based sensing using ultra-high frequency (UHF) radio-frequency identification (RFID) has, in recent years, yielded numerous additions to the Internet of Things (IoT). This work presents a polarization diversity-based rotation sensing methodology using common-off-the-shelf (COTS) UHF RFID tags identified with a software-defined radio (SDR) UHF RFID reader. The proposed methodology uses the tag-to-reader message after fully coherent demodulation to calculate a difference signal of the backscatter load modulation states. This sequence is then used to compute the rotation speed by evaluating its phase change over time. Experimental results are used to validate the theoretical model and to evaluate the performance and limitations of the proposed system.

</details>


### [118] [Millimeter-Wave True-Time Delay Array Beamforming with Robustness to Mobility](https://arxiv.org/abs/2512.08162)
*Benjamin W. Domae,Ibrahim Pehlivan,Danijela Cabric*

Main category: eess.SP

TL;DR: Proposes frequency-dependent slanted beams using true-time delay analog arrays for robust beamforming in wideband multi-user mmW downlink, trading capacity for coverage.


<details>
  <summary>Details</summary>
Motivation: Ultra-reliable low-latency connectivity is needed for real-time applications like AR/VR, but mmW networks struggle with continuous mobile connectivity due to beamforming challenges with fast-moving users and practical analog array architectures.

Method: Uses frequency-dependent slanted beams from true-time delay analog arrays with linear angle-frequency relationships for different users and sub-bands in wideband multi-user downlink scenarios.

Result: Slanted beams provide higher reliability to angle offsets and greater adaptability to varied user movement statistics compared to alternative analog array beamforming designs.

Conclusion: Frequency-dependent slanted beams offer a practical solution for robust beamforming in mobile mmW networks, balancing instantaneous capacity with angular coverage for reliable connectivity.

Abstract: Ultra-reliable and low-latency connectivity is required for real-time and latency-sensitive applications, like wireless augmented and virtual reality streaming. Millimeter-wave (mmW) networks have enabled extremely high data rates through large available bandwidths but struggle to maintain continuous connectivity with mobile users. Achieving the required beamforming gain from large antenna arrays with minimal disruption is particularly challenging with fast-moving users and practical analog mmW array architectures. In this work, we propose frequency-dependent slanted beams from true-time delay (TTD) analog arrays to achieve robust beamforming in wideband, multi-user downlink scenarios. Novel beams with linear angle-frequency relationships for different users and sub-bands provide a trade-off between instantaneous capacity and angular coverage. Compared to alternative analog array beamforming designs, slanted beams provide higher reliability to angle offsets and greater adaptability to varied user movement statistics.

</details>


### [119] [Metasurfaces Enable Active-Like Passive Radar](https://arxiv.org/abs/2512.08208)
*Mingyi Li,Jiawen Xu,Hanting Zhao,Xu Zhao,Yan Jin Chen,Tie Jun Cui,Vincenzo Galdi,Lianlin Li*

Main category: eess.SP

TL;DR: A metasurface-enabled passive radar (MEPR) uses programmable metasurfaces to tag ambient wireless signals, enabling active-like sensing without dedicated transmitters, overcoming limitations of conventional passive radars.


<details>
  <summary>Details</summary>
Motivation: Conventional passive radars have limitations: they require prior knowledge of non-cooperative source waveforms, are vulnerable to strong interference, and rely on Doppler signatures which limit detection of subtle or slow-moving targets. There's a need for low-cost, energy-efficient sensing that overcomes these constraints.

Method: Integrates a space-time-coding programmable metasurface to imprint distinct spatiotemporal tags onto ambient wireless wavefields. This transforms passive radar into an active-like sensing platform without needing source control, enabling interference suppression and signal enhancement.

Result: Proof-of-concept implementation at 5.48 GHz confirms real-time imaging and tracking of unmanned aerial vehicles under interference-rich conditions, with performance comparable to active radar systems.

Conclusion: MEPR establishes a foundation for scalable, adaptive, and energy-efficient next-generation integrated sensing and communication systems, combining the benefits of passive radar (low-cost, energy-efficient) with active-like sensing capabilities.

Abstract: Passive radars (PRs) provide a low-cost and energy-efficient approach to object detection by reusing existing wireless transmissions instead of emitting dedicated probing signals. Yet, conventional passive systems require prior knowledge of non-cooperative source waveforms, are vulnerable to strong interference, and rely on Doppler signatures, limiting their ability to detect subtle or slow-moving targets. Here, we introduce a metasurface-enabled PR (MEPR) concept that integrates a space-time-coding programmable metasurface to imprint distinct spatiotemporal tags onto ambient wireless wavefields. This mechanism transforms a PR into an active-like sensing platform without the need for source control, enabling interference suppression, signal enhancement, and accurate target localization and tracking in cluttered environments. A proof-of-concept implementation operating at 5.48 GHz confirms real-time imaging and tracking of unmanned aerial vehicles under interference-rich conditions, with performance comparable to active radar systems. These results establish MEPR as a solid foundation for scalable, adaptive, and energy-efficient next-generation integrated sensing and communication systems.

</details>


### [120] [1024-Channel 0.8V 23.9-nW/Channel Event-based Compute In-memory Neural Spike Detector](https://arxiv.org/abs/2512.08244)
*Ye Ke,Zhengnan Fu,Junyi Yang,Hongyang Shang,Arindam Basu*

Main category: eess.SP

TL;DR: Proposed event-based spike detection algorithm with in-memory computing architecture for high-density intracortical brain-machine interfaces, achieving 96% accuracy and ultra-low power consumption.


<details>
  <summary>Details</summary>
Motivation: Next-generation iBMIs face data rate challenges due to increasing recording sites, requiring complex wiring and high digitization power. Event-based frontends provide compression but generate false events, while conventional spike detection methods are power-hungry and incompatible with event-based systems.

Method: Developed event-based spike detection algorithm for compressive EBFs, implemented using novel 10-T eDRAM-SRAM hybrid random-access memory in-memory computing bitcell for event processing. Fabricated 1024-channel IMC SPD macro in 65nm process.

Result: Achieved 96.06% spike detection accuracy on synthetic dataset, 95.08% similarity and 0.05 firing pattern MAE on Neuropixel recordings. Energy efficiency of 23.9 nW per channel and area efficiency of 375 umÂ² per channel.

Conclusion: Presented a spike detection scheme compatible with compressive event-based frontends for high-density iBMIs, achieving ultra-low power consumption with IMC architecture while maintaining high accuracy.

Abstract: The increasing data rate has become a major issue confronting next-generation intracortical brain-machine interfaces (iBMIs). The scaling number of recording sites requires complex analog wiring and lead to huge digitization power consumption. Compressive event-based neural frontends have been used in high-density neural implants to support the simultaneous recording of more channels. Event-based frontends (EBF) convert recorded signals into asynchronous digital events via delta modulation and can inherently achieve considerable compression. But EBFs are prone to false events that do not correspond to neural spikes. Spike detection (SPD) is a key process in the iBMI pipeline to detect neural spikes and further reduce the data rate. However, conventional digital SPD suffers from the increasing buffer size and frequent memory access power, and conventional spike emphasizers are not compatible with EBFs. In this work we introduced an event-based spike detection (Ev-SPD) algorithm for scalable compressive EBFs. To implement the algorithm effectively, we proposed a novel low-power 10-T eDRAM-SRAM hybrid random-access memory in-memory computing bitcell for event processing. We fabricated the proposed 1024-channel IMC SPD macro in a 65nm process and tested the macro with both synthetic dataset and Neuropixel recordings. The proposed macro achieved a high spike detection accuracy of 96.06% on a synthetic dataset and 95.08% similarity and 0.05 firing pattern MAE on Neuropixel recordings. Our event-based IMC SPD macro achieved a high per channel spike detection energy efficiency of 23.9 nW per channel and an area efficiency of 375 um^2 per channel. Our work presented a SPD scheme compatible with compressive EBFs for high-density iBMIs, achieving ultra-low power consumption with an IMC architecture while maintaining considerable accuracy.

</details>


### [121] [Geometry-Aligned Differential Privacy for Location-Safe Federated Radio Map Construction](https://arxiv.org/abs/2512.08263)
*Jijia Tian,Wangqian Chen,Junting Chen,Pooi-Yuen Kam*

Main category: eess.SP

TL;DR: Proposes geometry-aligned differential privacy for radio map construction that protects user location privacy while maintaining model accuracy.


<details>
  <summary>Details</summary>
Motivation: Radio maps need location-labeled signal measurements, raising location privacy concerns. Even with local data processing, model updates can reveal user locations through spatial patterns in gradients, while existing privacy methods either fail to hide location leakage or degrade model accuracy too much.

Method: Analyzes location leakage from gradients in virtual-environment radio map models, then proposes a geometry-aligned differential privacy mechanism with heterogeneous noise tailored to both confuse localization and cover gradient spatial patterns.

Result: The approach increases attacker localization error from 30 m to over 180 m, with only 0.2 dB increase in radio map construction error compared to uniform-noise baseline. The method is theoretically supported with convergence guarantee linking privacy strength to learning accuracy.

Conclusion: Geometry-aligned differential privacy effectively protects location privacy in radio map construction while maintaining high model accuracy, addressing the fundamental tension between privacy protection and model utility in distributed wireless systems.

Abstract: Radio maps that describe spatial variations in wireless signal strength are widely used to optimize networks and support aerial platforms. Their construction requires location-labeled signal measurements from distributed users, raising fundamental concerns about location privacy. Even when raw data are kept local, the shared model updates can reveal user locations through their spatial structure, while naive noise injection either fails to hide this leakage or degrades model accuracy. This work analyzes how location leakage arises from gradients in a virtual-environment radio map model and proposes a geometry-aligned differential privacy mechanism with heterogeneous noise tailored to both confuse localization and cover gradient spatial patterns. The approach is theoretically supported with a convergence guarantee linking privacy strength to learning accuracy. Numerical experiments show the approach increases attacker localization error from 30 m to over 180 m, with only 0.2 dB increase in radio map construction error compared to a uniform-noise baseline.

</details>


### [122] [Self-Alignment Resonant Beam Empowers Beamforming without Estimation and Control for 6G IoT](https://arxiv.org/abs/2512.08386)
*Yixuan Guo,Mingliang Xiong,Qingwen Liu*

Main category: eess.SP

TL;DR: RF-RBS uses retro-directive antenna arrays to create self-sustaining electromagnetic loops for 6G IoT, eliminating complex CSI processing and enabling self-aligning beamforming for WPT, communication, and positioning.


<details>
  <summary>Details</summary>
Motivation: Traditional beamforming in 6G IoT requires complex CSI estimation and active beam scanning, creating prohibitive overheads especially in dynamic environments, which RF-RBS aims to overcome.

Method: Deploy retro-directive antenna arrays (RAA) at transceivers to establish self-sustaining cyclic electromagnetic loops that enable self-aligning, high-gain beamforming through positive feedback without digital CSI processing.

Result: RF-RBS can support high-efficiency wireless power transfer, robust communication, and millimeter-level passive positioning while eliminating CSI processing overhead.

Conclusion: RF-RBS presents a strategic solution for latency-sensitive 6G scenarios like unmanned systems and industrial automation, though implementation challenges need to be addressed.

Abstract: The integration of communication, sensing, and wireless power transfer (WPT) is a cornerstone of 6G intelligent IoT. However, relying on traditional beamforming imposes prohibitive overheads due to complex channel state information (CSI) estimation and active beam scanning, particularly in dynamic environments. This paper presents a comprehensive review of the radio frequency resonant beam system (RF-RBS), a native physical-layer paradigm that circumvents these limitations. By deploying retro-directive antenna arrays (RAA) at transceivers, RF-RBS establishes a self-sustaining cyclic electromagnetic loop. This mechanism inherently enables self-aligning, high-gain beamforming through positive feedback, eliminating the reliance on digital CSI processing. We analyze the system's architecture and its capability to support high-efficiency WPT, robust communication, and millimeter-level passive positioning. Finally, we evaluate the implementation challenges and strategic value of RF-RBS in latency-sensitive 6G scenarios, including unmanned systems and industrial automation.

</details>


### [123] [Hybrid Fuzzy Logic and Shading-Aware Particle Swarm Optimization for Dynamic Photovoltaic Shading Faults Mitigation](https://arxiv.org/abs/2512.08419)
*F. Philibert Andriniriniaimalaza,Nour Mohammad Murad,George Balan,Habachi Bilal,Nirilalaina Randriatefison,Abdel Khoodaruth,Charles Bernard Andrianirina,Blaise Ravelo*

Main category: eess.SP

TL;DR: Hybrid optimization combining Fuzzy Logic Control with Shading-Aware Particle Swarm Optimization improves PV system performance under shading conditions, achieving 11.8% power output improvement and 62% faster tracking.


<details>
  <summary>Details</summary>
Motivation: Shading faults significantly reduce PV system efficiency and disrupt maximum power point tracking, creating a need for better solutions to handle partial and complete shading events in real-world conditions.

Method: A hybrid optimization framework combining Fuzzy Logic Control (for rapid decision support based on shading patterns) with Shading-Aware Particle Swarm Optimization (to accelerate search and avoid local minima).

Result: The hybrid model shows 11.8% improvement in power output and 62% reduction in tracking time compared to conventional Perturb and Observe algorithm, with reliable global maximum power point detection under 20%-80% partial shading and complete shading.

Conclusion: Integrating intelligent control with shading-aware optimization significantly enhances PV system resilience and energy yield under complex real-world shading conditions.

Abstract: Shading faults remain one of the most critical challenges affecting photovoltaic (PV) system efficiency, as they not only reduce power generation but also disturb maximum power point tracking (MPPT). To address this issue, this study introduces a hybrid optimization framework that combines Fuzzy Logic Control (FLC) with a Shading-Aware Particle Swarm Optimization (SA-PSO) method. The proposed scheme is designed to adapt dynamically to both partial shading (20%-80%) and complete shading events, ensuring reliable global maximum power point (GMPP) detection. In this approach, the fuzzy controller provides rapid decision support based on shading patterns, while SA-PSO accelerates the search process and prevents the system from becoming trapped in local minima. A comparative performance assessment with the conventional Perturb and Observe (P\&O) algorithm highlights the advantages of the hybrid model, showing up to an 11.8% improvement in power output and a 62% reduction in tracking time. These results indicate that integrating intelligent control with shading-aware optimization can significantly enhance the resilience and energy yield of PV systems operating under complex real-world conditions.

</details>


### [124] [Aliasing in Near-Field Array Ambiguity Functions: a Spatial Frequency-Domain Framework](https://arxiv.org/abs/2512.08469)
*Gilles Monnoyer,JÃ©rÃ´me Louveaux,Laurence Defraigne,Baptiste Sambon,Luc vandendorpe*

Main category: eess.SP

TL;DR: This paper develops a general framework for analyzing grating lobes in near-field ambiguity functions of extremely large-scale arrays (XL-arrays), providing design guidelines for aliasing-safe operation.


<details>
  <summary>Details</summary>
Motivation: XL-arrays operate in the near-field regime where spherical wavefronts matter, but implementing very wide apertures with half-wavelength spacing is costly and complex. Array thinning introduces grating lobes (aliasing structures), and existing approaches use approximations specific to certain geometries rather than a general framework.

Method: The paper develops a general framework using local spatial-frequency analysis of steering signals to model near-field grating lobes as aliasing artifacts. This systematic methodology quantifies grating lobe structure on the ambiguity function and provides design guidelines for aliasing-safe operation in XL-arrays.

Result: The framework reveals fundamental origins and geometric behavior of grating lobes in near-field ambiguity functions, connects to established far-field principles, and derives closed-form expressions for aliasing-free regions in canonical uniform linear arrays and uniform circular arrays.

Conclusion: The paper provides a comprehensive theoretical framework for understanding and mitigating grating lobes in near-field XL-arrays, offering practical design guidelines for array thinning while maintaining aliasing-free operation.

Abstract: Next-generation communication and localization systems increasingly rely on extremely large-scale arrays (XL-arrays), which promise unprecedented spatial resolution and new functionalities. These gains arise from their inherent operation in the near field (NF) regime, where the spherical nature of the wavefront can no longer be ignored; consequently, characterizing the ambiguity function--which amounts to the matched beam pattern-- is considerably more challenging. Implementing very wide apertures with half-wavelength element spacing is costly and complex. This motivates thinning the array (removing elements), which introduces intricate aliasing structures, i.e., grating lobes. Whereas prior work has addressed this challenge using approximations tailored to specific array geometries, this paper develops a general framework that reveals the fundamental origins and geometric behavior of grating lobes in near-field ambiguity functions. Using a local spatial-frequency analysis of steering signals, we derive a systematic methodology to model NF grating lobes as aliasing artifacts, quantifying their structure on the AF, and providing design guidelines for XL-arrays that operate within aliasing-safe regions. We further connect our framework to established far-field principles. Finally, we demonstrate the practical value of the approach by deriving closed-form expressions for aliasing-free regions in canonical uniform linear arrays and uniform circular arrays.

</details>


### [125] [LoS+NLoS Holographic MIMO: Analysis and Application of Wavenumber-Division Multiplexing](https://arxiv.org/abs/2512.08509)
*Ashutosh Prajapati,Prathapasinghe Dharmawansa,Marco Di Renzo,Italo Atzeni*

Main category: eess.SP

TL;DR: Holographic MIMO channel model unifying LoS and NLoS components with wavenumber-division multiplexing, showing NLoS improves degrees of freedom and capacity.


<details>
  <summary>Details</summary>
Motivation: Existing holographic MIMO channel models treat LoS and NLoS components separately or use environment-specific multipath models, lacking a physically consistent unified representation for near-field operation.

Method: Develop unified LoS+NLoS channel representation integrating spatial-sampling and expansion-based formulations. Extend wavenumber-division multiplexing (WDM) framework to LoS+NLoS scenario, applying WDM to NLoS component for angular-domain representation characterized by power spectral factor/density.

Result: Derived closed-form characterizations for isotropic and non-isotropic scattering (with isotropic case recovering Jakes' model). Evaluation shows NLoS component substantially improves degrees of freedom and ergodic capacity compared to purely LoS case.

Conclusion: Unified LoS+NLoS channel model with WDM framework enables direct characterization of NLoS components, demonstrating significant performance improvements in holographic MIMO systems through combined LoS and NLoS exploitation.

Abstract: Holographic multiple-input multiple-output (MIMO) enables electrically large continuous apertures, overcoming the physical scaling limits of conventional MIMO architectures with half-wavelength spacing. Their near-field operating regime requires channel models that jointly capture line-of-sight (LoS) and non-line-of-sight (NLoS) components in a physically consistent manner. Existing studies typically treat these components separately or rely on environment-specific multipath models. In this work, we develop a unified LoS+NLoS channel representation for holographic lines that integrates spatial-sampling-based and expansion-based formulations. Building on this model, we extend the wavenumber-division multiplexing (WDM) framework, originally introduced for purely LoS channels, to the LoS+NLoS scenario. Applying WDM to the NLoS component yields its angular-domain representation, enabling direct characterization through the power spectral factor and power spectral density. We further derive closed-form characterizations for isotropic and non-isotropic scattering, with the former recovering Jakes' isotropic model. Lastly, we evaluate the resulting degrees of freedom and ergodic capacity, showing that incorporating the NLoS component substantially improves the performance relative to the purely LoS case.

</details>


### [126] [Beyond Diagonal RIS-assisted MIMO Transmission: Beamforming Gain and Capacity Optimization](https://arxiv.org/abs/2512.08516)
*Ainna Yue Moreno-Locubiche,Josep Vidal*

Main category: eess.SP

TL;DR: BD-RIS outperforms conventional diagonal RIS in MIMO downlink systems, with gradient-based optimization providing lower complexity than existing solutions.


<details>
  <summary>Details</summary>
Motivation: RIS technology offers unprecedented control over signal propagation, and BD-RIS generalizes conventional diagonal RIS for better performance in MIMO systems.

Method: Gradient-based optimization approach for BD-RIS elements in MIMO downlink systems, comparing TxBF and MIMO capacity transmission with waterfilling in mmWave LOS conditions.

Result: BD-RIS significantly outperforms traditional diagonal RIS in spectral efficiency and coverage, with the gradient method requiring lower complexity than existing solutions.

Conclusion: BD-RIS is a superior alternative to conventional diagonal RIS for MIMO downlink systems, offering improved performance with manageable computational complexity.

Abstract: Reconfigurable Intelligent Surfaces (RIS) have emerged as a transformative technology in wireless communications, offering unprecedented control over signal propagation. This study focuses on passive beyond diagonal reconfigurable intelligent surface (BD-RIS), which has been proposed to generalize conventional diagonal RIS, in Multiple-Input Multiple-Output (MIMO) downlink (DL) communication systems. We compare the performance of transmit beamforming (TxBF) and MIMO capacity transmission with waterfilling power allocation in the millimeter wave (mmWave) band, where propagation primarily occurs under line-of-sight (LOS) conditions. In the lack of closed-form expressions for the optimal RIS elements in either case, our approach adopts a gradient-based optimization approach requiring lower complexity than the solution in arXiv:2406.02170. Numerical results reveal that BD-RIS significantly outperforms traditional diagonal RIS in terms of spectral efficiency and coverage

</details>


### [127] [Contextual Bandits and Reconfigurable Intelligent Surfaces for Predictive LTM Handover Decisions](https://arxiv.org/abs/2512.08556)
*Ainna Yue Moreno-Locubiche,Josep Vidal,Olga MuÃ±oz-Medina,Margarita Cabrera-Bean*

Main category: eess.SP

TL;DR: Integration of RIS, signal prediction, and CMAB learning reduces handovers and improves reliability in next-gen wireless networks.


<details>
  <summary>Details</summary>
Motivation: To optimize handover in next-generation wireless networks by addressing challenges of unnecessary handovers, signaling overhead, and signal blockage in mobile environments.

Method: Combines three techniques: 1) Linear prediction of received signal power to anticipate link degradation, 2) Reconfigurable Intelligent Surfaces (RIS) to mitigate signal blockage and extend coverage, 3) Online-trained non-linear Contextual Multi-Armed Bandit (CMAB) agent for target gNB selection based on context features.

Result: Extensive simulations show CMAB and RSRP prediction consistently reduce number of handovers, ping-pong rate, and cell preparations, while RIS improves link reliability. Eight technique combinations were evaluated under realistic mobility and channel conditions.

Conclusion: The integrated approach combining predictive analytics, RIS technology, and learning-based decision-making effectively optimizes handover performance in next-generation wireless networks, reducing unnecessary operations while maintaining reliable connectivity.

Abstract: This article addresses the challenge of optimizing handover (HO) in next-generation wireless networks by integrating Reconfigurable Intelligent Surfaces (RIS), predicting received signal power, and utilizing learning-based decision-making. A conventional reactive HO mechanism, such as lower-layer triggered mobility (LTM), is enhanced through linear prediction to anticipate link degradation. Additionally, the use of RIS helps to mitigate signal blockage and extend coverage. An online trained non-linear Contextual Multi-Armed Bandit (CMAB) agent selects target gNBs based on context features, which reduces unnecessary HO and signaling overhead. Extensive simulations evaluate eight combinations of these techniques under realistic mobility and channel conditions. Results show that CMAB and RSRP prediction consistently reduce the number of HO, ping-pong rate and cell preparations, while RIS improves link reliability.

</details>


### [128] [Applications of Singular Entropy to Signals and Singular Smoothness to Images](https://arxiv.org/abs/2512.08717)
*Oscar Romero,NÃ©stor Thome*

Main category: eess.SP

TL;DR: This paper proposes refined SVD/GSVD methods for medical signal analysis (fetal/maternal ECG separation using Energy Gap Variation and Singular Energy) and introduces Singular Smoothness for image analysis (detecting natural anomalies like mountain fractures and burned forests).


<details>
  <summary>Details</summary>
Motivation: To improve upon existing SVD-based methods for signal and image analysis by developing more effective threshold selection techniques for separating maternal and fetal ECG components, and creating new approaches for detecting natural anomalies in images.

Method: 1) For ECG analysis: Introduces Energy Gap Variation (EGV) and Singular Energy concepts with refined threshold selection for SVD-based maternal/fetal signal separation, enhanced by GSVD for better discriminative power. 2) For image analysis: Develops Singular Smoothness technique incorporating Singular Entropy and Frobenius norm to evaluate information density for anomaly detection.

Result: Numerical experiments demonstrate effectiveness: improved separation of maternal and fetal ECG signals using the refined SVD/GSVD approach, and successful detection of natural anomalies (mountain fractures, burned forest regions) using the Singular Smoothness method.

Conclusion: The proposed SVD/GSVD refinements provide more effective signal separation capabilities for medical applications, while the novel Singular Smoothness technique offers a powerful approach for image analysis and natural anomaly detection, expanding the utility of singular value decomposition methods.

Abstract: This paper explores signal and image analysis by using the Singular Value Decomposition (SVD) and its extension, the Generalized Singular Value Decomposition (GSVD). A key strength of SVD lies in its ability to separate information into orthogonal subspaces. While SVD is a well-established tool in ECG analysis, particularly for source separation, this work proposes a refined method for selecting a threshold to distinguish between maternal and fetal components more effectively. In the first part of the paper, the focus is onmedical signal analysis,where the concepts of Energy Gap Variation (EGV) and Singular Energy are introduced to isolate fetal and maternal ECG signals, improving the known ones. Furthermore, the approach is significantly enhanced by the application of GSVD, which provides additional discriminative power for more accurate signal separation. The second part introduces a novel technique called Singular Smoothness, developed for image analysis. This method incorporates Singular Entropy and the Frobenius normto evaluate information density, and is applied to the detection of natural anomalies such asmountain fractures and burned forest regions. Numerical experiments are presented to demonstrate the effectiveness of the proposed approaches.

</details>


### [129] [RF sensing with dense IoT network graphs: An EM-informed analysis](https://arxiv.org/abs/2512.08746)
*Federica Fieramosca,Vittorio Rampa,Michele D'Amico,Stefano Savazzi*

Main category: eess.SP

TL;DR: This paper investigates theoretical bounds for RF sensing accuracy in dense IoT networks, proposes a deep graph neural network for human movement detection using RSS data, and validates the approach through indoor case studies.


<details>
  <summary>Details</summary>
Motivation: RF sensing has growing importance in IoT applications for passive localization and environmental monitoring, but there's a need to understand theoretical performance limits and develop practical detection methods for dense network deployments.

Method: The paper uses an EM model to predict body blockage effects, proposes a deep graph neural network trained on RSS samples structured as dense graphs (nodes as antennas, edges as radio links), and analyzes theoretical bounds on distinguishable subjects based on network parameters.

Result: The paper establishes theoretical limits on the number of distinguishable human subjects based on network parameters, demonstrates effective human movement detection using the proposed graph neural network, and validates the model's predictive potential through indoor case studies.

Conclusion: The proposed approach enables performance prediction during network pre-deployment stages, providing valuable insights for RF sensing system design in dense IoT networks, with practical validation confirming the model's effectiveness.

Abstract: Radio Frequency (RF) sensing is attracting interest in research, standardization, and industry, especially for its potential in Internet of Things (IoT) applications. By leveraging the properties of the ElectroMagnetic (EM) waves used in wireless networks, RF sensing captures environmental information such as the presence and movement of people and objects, enabling passive localization and vision applications. This paper investigates the theoretical bounds on accuracy and resolution for RF sensing systems within dense networks. It employs an EM model to predict the effects of body blockage in various scenarios. To detect human movements, the paper proposes a deep graph neural network, trained on Received Signal Strength (RSS) samples generated from the EM model. These samples are structured as dense graphs, with nodes representing antennas and edges as radio links. Focusing on the problem of identifying the number of human subjects co-present in a monitored area over time, the paper analyzes the theoretical limits on the number of distinguishable subjects, exploring how these limits depend on factors such as the number of radio links, the size of the monitored area and the subjects physical dimensions. These bounds enable the prediction of the system performance during network pre-deployment stages. The paper also presents the results of an indoor case study, which demonstrate the effectiveness of the approach and confirm the model's predictive potential in the network design stages.

</details>


### [130] [Evaluating the Deformation Measurement Accuracy Using Low-SNR Radars for Future InSAR Missions](https://arxiv.org/abs/2512.08779)
*Emre Havazli,Shadi Oveisgharan,Michael Denbina,Brian Hawkins*

Main category: eess.SP

TL;DR: Low-SNR conditions degrade InSAR displacement accuracy, but through multilooking techniques, comparable precision to high-SNR systems can be achieved at the cost of spatial resolution.


<details>
  <summary>Details</summary>
Motivation: Low SNR conditions in InSAR (common in low-backscatter regions) degrade phase coherence and displacement accuracy, creating challenges for deformation monitoring in challenging environments.

Method: Used L-band UAVSAR data over San Andreas Fault and Greenland ice sheet, simulated low-SNR conditions by degrading NESZ to -15dB, assessed effects on coherence, phase unwrapping, and time series inversion, and applied 8x8 multilooking.

Result: 4mm displacement accuracy achievable with signal decorrelation of 0.6 and SNR between -9dB to -10dB; 0.5 cm/yr velocity precision possible even under low-SNR conditions; multilooking significantly improves coherence and eliminates bias.

Conclusion: Low-SNR SAR systems can achieve comparable precision to high-SNR systems through multilooking, enabling cost-effective SAR mission design and optimized InSAR processing for challenging environments.

Abstract: Interferometric Synthetic Aperture Radar (InSAR) is a powerful tool for monitoring surface deformation with high precision. However, low Signal-to-Noise Ratio (SNR) conditions, common in regions with low backscatter, can degrade phase coherence and compromise displacement accuracy. In this study, we quantify the impact of low-SNR conditions on InSAR-derived displacement using L-band UAVSAR data collected over the San Andreas Fault and Greenland ice sheet. We simulate low-SNR conditions by degrading the Noise-Equivalent Sigma Zero (NESZ) to $-15~\mathrm{dB}$ and assess the resulting effects on interferometric coherence, phase unwrapping, and time series inversion. The displacement accuracy of 4mm in single interferogram can be achieved by taking looks for the signal decorrelation of 0.6 and SNR between -9dB to -10dB. Our findings indicate that even under low-SNR conditions, a velocity precision of $0.5~\mathrm{cm/yr}$ can be achieved in comparison to high-SNR conditions. By applying multilooking with an 8x8 window, we significantly improve coherence and eliminate this bias, demonstrating that low-SNR systems can achieve comparable precision to high-SNR systems at the expense of spatial resolution. These results have important implications for the design of future cost-effective SAR missions, such as Surface Deformation and Change (SDC), and the optimization of InSAR processing techniques in challenging environments.

</details>


### [131] [Delay-Oriented Distributed Scheduling with TransGNN](https://arxiv.org/abs/2512.08799)
*Boxuan Wen,Junyu Luo*

Main category: eess.SP

TL;DR: Proposes a Transformer-based GNN framework for distributed wireless scheduling that reduces transmission delay by capturing long-range interference dependencies and generating adaptive utility scores for conflict-free link selection.


<details>
  <summary>Details</summary>
Motivation: Traditional scheduling algorithms (max-weight, queue-length-based) optimize throughput but suffer from high latency in heterogeneous/dynamic topologies. Existing GNN approaches are limited by local aggregation and inability to model long-range dependencies in conflict graphs.

Method: Transformer GNN framework with attention-based graph encoder to generate adaptive per-link utility scores reflecting queue backlog and interference intensity, combined with Local Greedy Solver (LGS) for distributed conflict-free scheduling.

Result: Not specified in abstract (requires full paper details).

Conclusion: Proposed framework addresses limitations of traditional and GCN-based approaches by capturing long-range dependencies and enabling distributed delay-optimized scheduling in wireless multi-hop networks.

Abstract: Minimizing transmission delay in wireless multi-hop networks is a fundamental yet challenging task due to the complex coupling among interference, queue dynamics, and distributed control. Traditional scheduling algorithms, such as max-weight or queue-length-based policies, primarily aim to optimize throughput but often suffer from high latency, especially in heterogeneous or dynamically changing topologies. Recent learning-based approaches, particularly those employing Graph Neural Networks (GNNs), have shown promise in capturing spatial interference structures. However, conventional Graph Convolutional Networks (GCNs) remain limited by their local aggregation mechanism and their inability to model long-range dependencies within the conflict graph. To address these challenges, this paper proposes a delay-oriented distributed scheduling framework based on Transformer GNN. The proposed model employs an attention-based graph encoder to generate adaptive per-link utility scores that reflect both queue backlog and interference intensity. A Local Greedy Solver (LGS) then utilizes these utilities to construct a feasible independent set of links for transmission, ensuring distributed and conflict-free scheduling.

</details>


### [132] [A Fast Broadband Beamspace Transformation](https://arxiv.org/abs/2512.08887)
*Nakul Singh,Coleman DeLude,Mark Davenport,Justin Romberg*

Main category: eess.SP

TL;DR: A computationally efficient method for broadband multi-beamforming using fast beamspace transformation with O(M log N + B log N) complexity per sample, outperforming traditional delay-and-sum beamformers with O(MB) complexity.


<details>
  <summary>Details</summary>
Motivation: Traditional broadband beamforming using delay-and-sum methods has high computational complexity of O(MB) operations per sample, which becomes prohibitive when forming many beams (B ~ M). There's a need for more efficient broadband beamforming algorithms that can match the efficiency of narrowband FFT-based approaches.

Method: The method encodes M sensor outputs over N time samples using a special non-uniform spaced Fourier transform. Each beam is then formed by solving a small system of equations with Toeplitz structure. The algorithm processes multiple array snapshots coherently to handle broadband signals.

Result: The algorithm achieves O(M log N + B log N) operations per sample, essentially matching the efficiency of narrowband FFT-based beamforming. Numerical experiments demonstrate favorable computational scaling and high accuracy. The beamspace representation also enables efficient interpolation to off-grid angles and interferer nulling.

Conclusion: The fast broadband beamspace transformation provides a computationally efficient alternative to traditional delay-and-sum beamforming, with complexity scaling that makes broadband multi-beamforming practical for large arrays and many beams. The beamspace representation offers additional computational advantages for common beamforming tasks.

Abstract: We present a new computationally efficient method for multi-beamforming in the broadband setting. Our "fast beamspace transformation" forms $B$ beams from $M$ sensor outputs using a number of operations per sample that scales linearly (to within logarithmic factors) with $M$ when $B\sim M$. While the narrowband version of this transformation can be performed efficiently with a spatial fast Fourier transform, the broadband setting requires coherent processing of multiple array snapshots simultaneously. Our algorithm works by taking $N$ samples off of each of $M$ sensors and encoding the sensor outputs into a set of coefficients using a special non-uniform spaced Fourier transform. From these coefficients, each beam is formed by solving a small system of equations that has Toeplitz structure. The total runtime complexity is $\mathcal{O}(M\log N+B\log N)$ operations per sample, exhibiting essentially the same scaling as in the narrowband case and vastly outperforming broadband beamformers based on delay and sum whose computations scale as $\mathcal{O}(MB)$. Alongside a careful mathematical formulation and analysis of our fast broadband beamspace transform, we provide a host of numerical experiments demonstrating the algorithm's favorable computational scaling and high accuracy. Finally, we demonstrate how tasks such as interpolating to ``off-grid" angles and nulling an interferer are more computationally efficient when performed directly in beamspace.

</details>


### [133] [Timing-Error Optimized Architecture for Current-Steering DACs](https://arxiv.org/abs/2512.08903)
*Ramin Babaee,Shahab Oveis Gharan,Martin Bouchard*

Main category: eess.SP

TL;DR: A novel DAC weighting architecture that statistically minimizes distortion from random timing mismatches, with three decoding algorithms of varying complexity, showing performance improvement over segmented structures.


<details>
  <summary>Details</summary>
Motivation: Random timing mismatches among current sources in DACs cause distortion that degrades dynamic performance. Existing segmented structures have limitations in handling these mismatches effectively.

Method: Proposed a novel DAC weighting architecture that statistically minimizes timing mismatch distortion. Developed three algorithms with varying computational complexities to decode DAC input codewords into corresponding DAC switches. Used high-level Matlab simulations for evaluation.

Result: The proposed architecture demonstrates dynamic performance improvement over traditional segmented structures, as shown through Matlab simulations.

Conclusion: The novel weighting architecture with statistical minimization of timing mismatches offers improved DAC performance, with flexible algorithm choices based on computational complexity requirements.

Abstract: We propose a novel digital-to-analog converter (DAC) weighting architecture that statistically minimizes the distortion caused by random timing mismatches among current sources. To decode the DAC input codewords into corresponding DAC switches, we present three algorithms with varying computational complexities. We perform high-level Matlab simulations to illustrate the dynamic performance improvement over the segmented structure.

</details>


### [134] [Architecture Design for Rise/Fall Asymmetry Glitch Minimization in Current-Steering DACs](https://arxiv.org/abs/2512.08909)
*Ramin Babaee,Shahab Oveis Gharan,Martin Bouchard*

Main category: eess.SP

TL;DR: The paper proposes a novel weighting scheme for current-steering DACs to mitigate output glitches caused by switch asymmetry, potentially outperforming traditional segmented architectures.


<details>
  <summary>Details</summary>
Motivation: Current-steering DACs are widely used in high-speed applications like optical communications, but suffer from input-dependent output glitches that degrade dynamic performance. These glitches arise from asymmetry in the fall/rise response of DAC switches.

Method: The authors investigate DAC glitches from switch asymmetry, formulate a glitch metric to define overall DAC performance, and use this metric to develop a novel DAC weighting scheme.

Result: Numerical simulations demonstrate that the proposed architecture can potentially provide significant performance advantages compared to traditional segmented structures.

Conclusion: The novel weighting scheme based on a formulated glitch metric offers a promising approach to mitigate glitch issues in current-steering DACs, potentially improving dynamic performance in high-speed applications.

Abstract: Current-steering digital-to-analog converter (DAC) is a prominent architecture that is commonly used in high-speed applications such as optical communications. One of the shortcomings of this architecture is the output glitches that are input dependent and degrade the dynamic performance of the DAC. We investigate DAC glitches that arise from asymmetry in the fall/rise response of DAC switches. We formulate a glitch metric that defines the overall DAC performance, which is then used to find a novel DAC weighting scheme. Numerical simulations show that the proposed architecture can potentially provide a significant performance advantage compared to the segmented structure.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [135] [Impact of Data-Oriented and Object-Oriented Design on Performance and Cache Utilization with Artificial Intelligence Algorithms in Multi-Threaded CPUs](https://arxiv.org/abs/2512.07841)
*Gabriel M. Arantes,Richard F. Pinto,Bruno L. Dalmazo,Eduardo N. Borges,Giancarlo Lucca,Viviane L. D. de Mattos,Fabian C. Cardoso,Rafael A. Berri*

Main category: cs.AI

TL;DR: DOD outperforms OOD in multi-threaded environments for cache efficiency and execution speed, though single-threaded versions beat multi-threaded ones for fine-grained A* algorithm tasks.


<details>
  <summary>Details</summary>
Motivation: The performance gap between multi-core CPUs and main memory requires hardware-aware software design paradigms to maximize hardware efficiency in complex computing tasks.

Method: Developed and compared four versions of A* search algorithm: single-threaded OOD, single-threaded DOD, multi-threaded OOD, and multi-threaded DOD. Evaluated using execution time, memory usage, and CPU cache misses.

Result: DOD showed significant performance gains in multi-threaded tests with faster execution times, fewer system calls, and lower cache misses. However, single-threaded versions outperformed multi-threaded ones for fine-grained A* tasks due to thread management overhead.

Conclusion: DOD demonstrates foundational architectural superiority over OOD for maximizing hardware efficiency in complex, large-scale AI and parallel computing tasks, despite subtle differences in simple algorithms.

Abstract: The growing performance gap between multi-core CPUs and main memory necessitates hardware-aware software design paradigms. This study provides a comprehensive performance analysis of Data Oriented Design (DOD) versus the traditional Object-Oriented Design (OOD), focusing on cache utilization and efficiency in multi-threaded environments. We developed and compared four distinct versions of the A* search algorithm: single-threaded OOD (ST-OOD), single-threaded DOD (ST-DOD), multi-threaded OOD (MT-OOD), and multi-threaded DOD (MT-DOD). The evaluation was based on metrics including execution time, memory usage, and CPU cache misses. In multi-threaded tests, the DOD implementation demonstrated considerable performance gains, with faster execution times and a lower number of raw system calls and cache misses. While OOD occasionally showed marginal advantages in memory usage or percentage-based cache miss rates, DOD's efficiency in data-intensive operations was more evident. Furthermore, our findings reveal that for a fine-grained task like the A* algorithm, the overhead associated with thread management led to single-threaded versions significantly outperforming their multi-threaded counterparts in both paradigms. We conclude that even when performance differences appear subtle in simple algorithms, the consistent advantages of DOD in critical metrics highlight its foundational architectural superiority, suggesting it is a more effective approach for maximizing hardware efficiency in complex, large-scale AI and parallel computing tasks.

</details>


### [136] [Can AI autonomously build, operate, and use the entire data stack?](https://arxiv.org/abs/2512.07926)
*Arvind Agarwal,Lisa Amini,Sameep Mehta,Horst Samulowitz,Kavitha Srinivas*

Main category: cs.AI

TL;DR: The paper argues for a paradigm shift from AI-assisted data management to fully autonomous data estates using intelligent agents to handle the entire data lifecycle holistically.


<details>
  <summary>Details</summary>
Motivation: Current AI assistants only help specific personas with limited tasks, falling short of full automation. As AI capabilities advance, there's an imminent opportunity to achieve fully autonomous data management that can handle inherent complexities previously resistant to automation.

Method: The paper explores how intelligent agents can autonomously manage each stage of the modern data stack, examining how agents can streamline the entire data lifecycle from architecture to continuous improvement.

Result: The work identifies opportunities for autonomous data estates and highlights open research questions, aiming to inspire debate and collaborative approaches toward more self-sufficient data systems.

Conclusion: A paradigm shift is needed from AI-assisted component operations to holistic autonomous data lifecycle management, enabling systems that serve both human users and AI itself, requiring further research and collaboration.

Abstract: Enterprise data management is a monumental task. It spans data architecture and systems, integration, quality, governance, and continuous improvement. While AI assistants can help specific persona, such as data engineers and stewards, to navigate and configure the data stack, they fall far short of full automation. However, as AI becomes increasingly capable of tackling tasks that have previously resisted automation due to inherent complexities, we believe there is an imminent opportunity to target fully autonomous data estates. Currently, AI is used in different parts of the data stack, but in this paper, we argue for a paradigm shift from the use of AI in independent data component operations towards a more holistic and autonomous handling of the entire data lifecycle. Towards that end, we explore how each stage of the modern data stack can be autonomously managed by intelligent agents to build self-sufficient systems that can be used not only by human end-users, but also by AI itself. We begin by describing the mounting forces and opportunities that demand this paradigm shift, examine how agents can streamline the data lifecycle, and highlight open questions and areas where additional research is needed. We hope this work will inspire lively debate, stimulate further research, motivate collaborative approaches, and facilitate a more autonomous future for data systems.

</details>


### [137] [SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models](https://arxiv.org/abs/2512.07993)
*Jiayi Tian,Seyedarmin Azizi,Yequan Zhao,Erfan Baghaei Potraghloo,Sean McPherson,Sharath Nittur Sridhar,Zhengyang Wang,Zheng Zhang,Massoud Pedram,Souvik Kundu*

Main category: cs.AI

TL;DR: SkipKV is a training-free KV cache compression method for large reasoning models that uses sentence-level eviction and generation steering to reduce KV cache overhead while maintaining reasoning accuracy.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models suffer from significant KV cache overhead that grows linearly with verbose chain-of-thought reasoning, causing memory and throughput bottlenecks. Existing KV cache eviction methods fail in multi-batch settings due to unstable token-wise scoring and padding token issues, and often generate longer sequences.

Method: SkipKV operates at sentence-level rather than token-level: 1) Uses a sentence-scoring metric to identify and remove highly similar sentences while maintaining semantic coherence, 2) Dynamically adjusts a steering vector to update hidden activation states during inference to suppress redundant generation and enforce concise responses.

Result: SkipKV maintains up to 26.7% improved accuracy compared to alternatives at similar compression budgets, yields up to 1.6Ã fewer generation length than state-of-the-art methods, and improves throughput up to 1.7Ã on multiple reasoning benchmarks.

Conclusion: SkipKV effectively addresses KV cache overhead in large reasoning models through coarse-grained sentence-level compression, maintaining reasoning accuracy while significantly improving efficiency through reduced generation length and increased throughput.

Abstract: Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \textbf{SkipKV}, a \textbf{\textit{training-free}} KV compression method for selective \textit{eviction} and \textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\mathbf{26.7}\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\mathbf{1.6}\times$ fewer generation length while improving throughput up to $\mathbf{1.7}\times$.

</details>


### [138] [Toward an AI Reasoning-Enabled System for Patient-Clinical Trial Matching](https://arxiv.org/abs/2512.08026)
*Caroline N. Leach,Mitchell A. Klusty,Samuel E. Armstrong,Justine C. Pickarski,Kristen L. Hankins,Emily B. Collier,Maya Shah,Aaron D. Mullen,V. K. Cody Bumgardner*

Main category: cs.AI

TL;DR: AI system for patient-trial matching that uses LLMs to generate structured eligibility assessments with reasoning chains, addressing EHR integration, expert review, and security challenges.


<details>
  <summary>Details</summary>
Motivation: Manual clinical trial screening is time-consuming and resource-intensive. Current systems lack integration with heterogeneous EHR data, don't support expert review well, and have security concerns.

Method: Secure, scalable proof-of-concept system using open-source reasoning-enabled LLMs. Moves beyond binary classification to generate structured eligibility assessments with interpretable reasoning chains for human-in-the-loop review.

Result: System represents eligibility as dynamic state rather than fixed determination, identifies matches when available, and offers actionable recommendations for future eligibility. Supports comprehensive auditability of AI outputs.

Conclusion: AI-augmented patient-trial matching system reduces coordinator burden, intelligently broadens trial consideration, and maintains rigorous security standards while supporting expert review.

Abstract: Screening patients for clinical trial eligibility remains a manual, time-consuming, and resource-intensive process. We present a secure, scalable proof-of-concept system for Artificial Intelligence (AI)-augmented patient-trial matching that addresses key implementation challenges: integrating heterogeneous electronic health record (EHR) data, facilitating expert review, and maintaining rigorous security standards. Leveraging open-source, reasoning-enabled large language models (LLMs), the system moves beyond binary classification to generate structured eligibility assessments with interpretable reasoning chains that support human-in-the-loop review. This decision support tool represents eligibility as a dynamic state rather than a fixed determination, identifying matches when available and offering actionable recommendations that could render a patient eligible in the future. The system aims to reduce coordinator burden, intelligently broaden the set of trials considered for each patient and guarantee comprehensive auditability of all AI-generated outputs.

</details>


### [139] [Large Language Models for Education and Research: An Empirical and User Survey-based Analysis](https://arxiv.org/abs/2512.08057)
*Md Mostafizer Rahman,Ariful Islam Shiplu,Md Faizul Ibne Amin,Yutaka Watanobe,Lu Peng*

Main category: cs.AI

TL;DR: Comprehensive evaluation of ChatGPT and DeepSeek LLMs showing ChatGPT excels in language tasks while DeepSeek outperforms in programming, with both performing well in medical and mathematical domains.


<details>
  <summary>Details</summary>
Motivation: To comprehensively evaluate and compare state-of-the-art LLMs (ChatGPT and DeepSeek) in educational and research contexts, examining their trade-offs in accuracy, computational efficiency, and user experience.

Method: Three-pronged approach: 1) Background technology analysis, 2) Empirical experiments benchmarking performance in text generation, programming, and specialized problem-solving, 3) Real-world user survey with students, educators, and researchers.

Result: ChatGPT excels in general language understanding and text generation; DeepSeek demonstrates superior performance in programming tasks due to efficiency-focused design; Both models deliver medically accurate diagnostic outputs and effectively solve complex mathematical problems.

Conclusion: Both ChatGPT and DeepSeek have valuable but distinct strengths in educational and research applications, with practical benefits confirmed by user feedback, offering insights for advancing AI-assisted education and research.

Abstract: Pretrained Large Language Models (LLMs) have achieved remarkable success across diverse domains, with education and research emerging as particularly impactful areas. Among current state-of-the-art LLMs, ChatGPT and DeepSeek exhibit strong capabilities in mathematics, science, medicine, literature, and programming. In this study, we present a comprehensive evaluation of these two LLMs through background technology analysis, empirical experiments, and a real-world user survey. The evaluation explores trade-offs among model accuracy, computational efficiency, and user experience in educational and research affairs. We benchmarked these LLMs performance in text generation, programming, and specialized problem-solving. Experimental results show that ChatGPT excels in general language understanding and text generation, while DeepSeek demonstrates superior performance in programming tasks due to its efficiency- focused design. Moreover, both models deliver medically accurate diagnostic outputs and effectively solve complex mathematical problems. Complementing these quantitative findings, a survey of students, educators, and researchers highlights the practical benefits and limitations of these models, offering deeper insights into their role in advancing education and research.

</details>


### [140] [Scalable Back-End for an AI-Based Diabetes Prediction Application](https://arxiv.org/abs/2512.08147)
*Henry Anand Septian Radityo,Bernardus Willson,Reynard Tanadi,Latifa Dwiyanti,Saiful Akbar*

Main category: cs.AI

TL;DR: Developed scalable back-end system for diabetes prediction app using horizontal scaling, database sharding, and RabbitMQ message queue, achieving 83% feature performance targets and handling 10k concurrent users.


<details>
  <summary>Details</summary>
Motivation: Global diabetes prevalence requires early detection, but AI prediction apps need responsive, scalable back-end architecture to serve large user bases effectively.

Method: Implemented horizontal scaling, database sharding, and asynchronous communication via RabbitMQ message queue to build scalable back-end system for mobile diabetes prediction application.

Result: 83% of system features (20/24) met performance targets (<5% failure rate, <1000ms latency). System handled 10k concurrent users successfully. RabbitMQ minimized error rates for intensive prediction requests by queuing requests and preventing data loss.

Conclusion: The scalable back-end architecture successfully supports diabetes prediction application with reliable performance, demonstrating effective handling of high concurrent user loads through modern scaling and queuing techniques.

Abstract: The rising global prevalence of diabetes necessitates early detection to prevent severe complications. While AI-powered prediction applications offer a promising solution, they require a responsive and scalable back-end architecture to serve a large user base effectively. This paper details the development and evaluation of a scalable back-end system designed for a mobile diabetes prediction application. The primary objective was to maintain a failure rate below 5% and an average latency of under 1000 ms. The architecture leverages horizontal scaling, database sharding, and asynchronous communication via a message queue. Performance evaluation showed that 83% of the system's features (20 out of 24) met the specified performance targets. Key functionalities such as user profile management, activity tracking, and read-intensive prediction operations successfully achieved the desired performance. The system demonstrated the ability to handle up to 10,000 concurrent users without issues, validating its scalability. The implementation of asynchronous communication using RabbitMQ proved crucial in minimizing the error rate for computationally intensive prediction requests, ensuring system reliability by queuing requests and preventing data loss under heavy load.

</details>


### [141] [Enhancing Explainability of Graph Neural Networks Through Conceptual and Structural Analyses and Their Extensions](https://arxiv.org/abs/2512.08344)
*Tien Cuong Bui*

Main category: cs.AI

TL;DR: This thesis proposes a novel XAI framework for GNNs that provides adaptable, efficient explanations by capturing how graph structure influences predictions, addressing limitations of current post-hoc and interpretable methods.


<details>
  <summary>Details</summary>
Motivation: GNNs are powerful for graph-structured data but their complexity makes decision-making opaque. Current XAI methods struggle with graph relationships: post-hoc methods are computationally expensive and unreliable, while interpretable models lack generalizability.

Method: Develops a novel XAI framework specifically tailored for graph-based ML that moves beyond individual feature analysis to capture how graph structure influences predictions, aiming for adaptable and computationally efficient explanations.

Result: The framework is proposed but not yet implemented/results shown - this appears to be a thesis proposal or abstract describing planned research rather than completed work.

Conclusion: A new XAI framework for GNNs is needed to address current limitations, and this thesis will develop such a framework that provides efficient, adaptable explanations by analyzing graph structure's influence on predictions.

Abstract: Graph Neural Networks (GNNs) have become a powerful tool for modeling and analyzing data with graph structures. The wide adoption in numerous applications underscores the value of these models. However, the complexity of these methods often impedes understanding their decision-making processes. Current Explainable AI (XAI) methods struggle to untangle the intricate relationships and interactions within graphs. Several methods have tried to bridge this gap via a post-hoc approach or self-interpretable design. Most of them focus on graph structure analysis to determine essential patterns that correlate with prediction outcomes. While post-hoc explanation methods are adaptable, they require extra computational resources and may be less reliable due to limited access to the model's internal workings. Conversely, Interpretable models can provide immediate explanations, but their generalizability to different scenarios remains a major concern. To address these shortcomings, this thesis seeks to develop a novel XAI framework tailored for graph-based machine learning. The proposed framework aims to offer adaptable, computationally efficient explanations for GNNs, moving beyond individual feature analysis to capture how graph structure influences predictions.

</details>


### [142] [Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions](https://arxiv.org/abs/2512.08230)
*Eunice Yiu,Kelsey Allen,Shiry Ginosar,Alison Gopnik*

Main category: cs.AI

TL;DR: The paper proposes empowerment (mutual information between actions and outcomes) as a bridge between Bayesian causal learning and reinforcement learning, suggesting it can explain human causal learning and enable it in machines.


<details>
  <summary>Details</summary>
Motivation: Current large pretrained models struggle with causal learning, while cognitive science has successfully applied Causal Bayes Net formalism to understand human causal learning. There's a need to bridge classical Bayesian approaches with reinforcement learning to better understand and enable causal learning.

Method: The paper connects empowerment (an intrinsic reward signal from reinforcement learning) with causal learning, proposing that empowerment can serve as a bridge between Bayesian causal learning and reinforcement learning. An empirical study tests how children and adults use empowerment cues to infer causal relations and design interventions.

Result: The paper presents a theoretical framework linking empowerment to causal learning, suggesting that accurate causal models increase empowerment and vice versa. Empirical findings show how children and adults use empowerment cues for causal inference and intervention design.

Conclusion: Empowerment provides a promising bridge between Bayesian causal learning and reinforcement learning, potentially explaining human causal learning and enabling it in machines, while also offering insights into developmental aspects of causal learning.

Abstract: Learning about the causal structure of the world is a fundamental problem for human cognition. Causal models and especially causal learning have proved to be difficult for large pretrained models using standard techniques of deep learning. In contrast, cognitive scientists have applied advances in our formal understanding of causation in computer science, particularly within the Causal Bayes Net formalism, to understand human causal learning. In the very different tradition of reinforcement learning, researchers have described an intrinsic reward signal called "empowerment" which maximizes mutual information between actions and their outcomes. "Empowerment" may be an important bridge between classical Bayesian causal learning and reinforcement learning and may help to characterize causal learning in humans and enable it in machines. If an agent learns an accurate causal world model, they will necessarily increase their empowerment, and increasing empowerment will lead to a more accurate causal world model. Empowerment may also explain distinctive features of childrens causal learning, as well as providing a more tractable computational account of how that learning is possible. In an empirical study, we systematically test how children and adults use cues to empowerment to infer causal relations, and design effective causal interventions.

</details>


### [143] [Beyond Traditional Diagnostics: Transforming Patient-Side Information into Predictive Insights with Knowledge Graphs and Prototypes](https://arxiv.org/abs/2512.08261)
*Yibowen Zhao,Yinan Zhang,Zhixiang Su,Lizhen Cui,Chunyan Miao*

Main category: cs.AI

TL;DR: KPI framework improves disease prediction from patient data using knowledge graphs, prototype learning, and LLM explanations to address imbalance and interpretability issues.


<details>
  <summary>Details</summary>
Motivation: Existing disease prediction methods from patient-side information face challenges with imbalanced disease distributions and lack of interpretability, leading to biased or unreliable predictions that limit clinical utility.

Method: Proposes KPI framework that: 1) integrates structured medical knowledge into a unified disease knowledge graph, 2) constructs clinically meaningful disease prototypes, 3) uses contrastive learning for better accuracy (especially for long-tailed diseases), and 4) employs LLMs to generate patient-specific medical explanations.

Result: KPI outperforms state-of-the-art methods in predictive accuracy on real-world datasets and provides clinically valid explanations that closely align with patient narratives.

Conclusion: KPI framework demonstrates practical value for patient-centered healthcare delivery by addressing key challenges of imbalance and interpretability in disease prediction from patient-side information.

Abstract: Predicting diseases solely from patient-side information, such as demographics and self-reported symptoms, has attracted significant research attention due to its potential to enhance patient awareness, facilitate early healthcare engagement, and improve healthcare system efficiency. However, existing approaches encounter critical challenges, including imbalanced disease distributions and a lack of interpretability, resulting in biased or unreliable predictions. To address these issues, we propose the Knowledge graph-enhanced, Prototype-aware, and Interpretable (KPI) framework. KPI systematically integrates structured and trusted medical knowledge into a unified disease knowledge graph, constructs clinically meaningful disease prototypes, and employs contrastive learning to enhance predictive accuracy, which is particularly important for long-tailed diseases. Additionally, KPI utilizes large language models (LLMs) to generate patient-specific, medically relevant explanations, thereby improving interpretability and reliability. Extensive experiments on real-world datasets demonstrate that KPI outperforms state-of-the-art methods in predictive accuracy and provides clinically valid explanations that closely align with patient narratives, highlighting its practical value for patient-centered healthcare delivery.

</details>


### [144] [Reasoning Models Ace the CFA Exams](https://arxiv.org/abs/2512.08270)
*Jaisal Patel,Yunzhe Chen,Kaiwen He,Keyi Wang,David Li,Kairong Xiao,Xiao-Yang Liu*

Main category: cs.AI

TL;DR: Recent reasoning models achieve strong performance on CFA exams, with most models passing all three levels and Gemini 3.0 Pro setting a record 97.6% on Level I.


<details>
  <summary>Details</summary>
Motivation: Previous research showed LLMs performed poorly on CFA exams, but recent reasoning models have demonstrated strong capabilities on graduate-level professional exams, warranting evaluation on CFA exams.

Method: Evaluated state-of-the-art reasoning models on 980 questions across mock CFA exams (3 Level I, 2 Level II, 3 Level III) using same pass/fail criteria from prior studies.

Result: Most models cleared all three CFA levels. Top performers: Gemini 3.0 Pro (97.6% on Level I), GPT-5 (94.3% on Level II), Gemini 2.5 Pro (86.4% on Level III multiple-choice), Gemini 3.0 Pro (92.0% on Level III constructed-response).

Conclusion: Recent reasoning models demonstrate strong performance on CFA exams, significantly outperforming previous LLMs and achieving passing scores across all three levels.

Abstract: Previous research has reported that large language models (LLMs) demonstrate poor performance on the Chartered Financial Analyst (CFA) exams. However, recent reasoning models have achieved strong results on graduate-level academic and professional examinations across various disciplines. In this paper, we evaluate state-of-the-art reasoning models on a set of mock CFA exams consisting of 980 questions across three Level I exams, two Level II exams, and three Level III exams. Using the same pass/fail criteria from prior studies, we find that most models clear all three levels. The models that pass, ordered by overall performance, are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Specifically, Gemini 3.0 Pro achieves a record score of 97.6% on Level I. Performance is also strong on Level II, led by GPT-5 at 94.3%. On Level III, Gemini 2.5 Pro attains the highest score with 86.4% on multiple-choice questions while Gemini 3.0 Pro achieves 92.0% on constructed-response questions.

</details>


### [145] [AgentEval: Generative Agents as Reliable Proxies for Human Evaluation of AI-Generated Content](https://arxiv.org/abs/2512.08273)
*Thanh Vu,Richi Nayak,Thiru Balasubramaniam*

Main category: cs.AI

TL;DR: Generative Agents enable automated, cost-effective evaluation of AI-generated content by simulating human judgment across multiple quality dimensions, reducing reliance on expensive human evaluations.


<details>
  <summary>Details</summary>
Motivation: Businesses face challenges with time and cost constraints in content creation and evaluation. Human writers have time limitations, and traditional evaluation methods like human surveys are expensive. While LLMs offer content generation potential, concerns about AI-generated content quality persist, creating a need for efficient automated evaluation solutions.

Method: The research introduces Generative Agents that simulate human judgment to evaluate AI-generated content. These agents assess content across multiple dimensions including coherence, interestingness, clarity, fairness, and relevance, providing automated quality assessment.

Result: Generative Agents enable rapid, cost-effective evaluation of AI-generated content, allowing businesses to streamline content generation processes while ensuring consistent, high-quality output. They minimize reliance on costly human evaluations while maintaining quality standards.

Conclusion: The study demonstrates that Generative Agents provide significant advancements in automated content generation and evaluation, offering critical insights for enhancing LLMs to produce business-aligned, high-quality content while reducing operational costs.

Abstract: Modern businesses are increasingly challenged by the time and expense required to generate and assess high-quality content. Human writers face time constraints, and extrinsic evaluations can be costly. While Large Language Models (LLMs) offer potential in content creation, concerns about the quality of AI-generated content persist. Traditional evaluation methods, like human surveys, further add operational costs, highlighting the need for efficient, automated solutions. This research introduces Generative Agents as a means to tackle these challenges. These agents can rapidly and cost-effectively evaluate AI-generated content, simulating human judgment by rating aspects such as coherence, interestingness, clarity, fairness, and relevance. By incorporating these agents, businesses can streamline content generation and ensure consistent, high-quality output while minimizing reliance on costly human evaluations. The study provides critical insights into enhancing LLMs for producing business-aligned, high-quality content, offering significant advancements in automated content generation and evaluation.

</details>


### [146] [Towards a Science of Scaling Agent Systems](https://arxiv.org/abs/2512.08296)
*Yubin Kim,Ken Gu,Chanwoo Park,Chunjong Park,Samuel Schmidgall,A. Ali Heydari,Yao Yan,Zhihan Zhang,Yuchen Zhuang,Mark Malhotra,Paul Pu Liang,Hae Won Park,Yuzhe Yang,Xuhai Xu,Yilun Du,Shwetak Patel,Tim Althoff,Daniel McDuff,Xin Liu*

Main category: cs.AI

TL;DR: Researchers develop quantitative scaling principles for AI agent systems, identifying key trade-offs between tool coordination and computational overhead, with predictive models that can determine optimal agent architectures based on task properties.


<details>
  <summary>Details</summary>
Motivation: Despite widespread adoption of language model-based agents for real-world applications, there's a lack of principled understanding of what determines their performance, forcing practitioners to rely on heuristics rather than systematic design choices.

Method: Evaluated four diverse benchmarks (Finance-Agent, BrowseComp-Plus, PlanCraft, Workbench) using five canonical agent architectures across three LLM families, totaling 180 configurations with standardized tools and token budgets. Derived predictive models using empirical coordination metrics like efficiency, overhead, error amplification, and redundancy.

Result: Identified three dominant effects: 1) tool-coordination trade-off where tool-heavy tasks suffer from multi-agent overhead, 2) capability saturation where coordination yields diminishing returns once single-agent baselines exceed ~45%, and 3) topology-dependent error amplification. Centralized coordination improved performance by 80.9% on parallelizable tasks, while decentralized coordination excelled on dynamic web navigation. The framework predicted optimal coordination strategy for 87% of held-out configurations.

Conclusion: The research provides a predictive principle for agentic scaling based on measurable task properties, offering practitioners a systematic framework for designing optimal agent architectures rather than relying on heuristics.

Abstract: Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.

</details>


### [147] [rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection](https://arxiv.org/abs/2512.08300)
*Sijia Chen,Baochun Li,Di Niu*

Main category: cs.AI

TL;DR: rSIM enables small LLMs to become Reasoning Language Models (RLMs) through a planner-guided strategy injection mechanism trained with multi-agent RL, achieving performance surpassing much larger models.


<details>
  <summary>Details</summary>
Motivation: LLMs can evolve into Reasoning Language Models (RLMs) through reinforcement learning, exhibiting "aha" moments with strategic reasoning like self-reflection and deep thinking in chain of thoughts. The goal is to enable any LLM to become an RLM by injecting reasoning strategies.

Method: Proposes reinforced strategy injection mechanism (rSIM) using a small planner to guide LLM's chain of thoughts through adaptive injection of reasoning strategies. The planner (leader agent) is jointly trained with LLM (follower agent) using multi-agent RL based on leader-follower framework with rule-based rewards.

Result: rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. The planner is generalizable - trained once and applied as plug-in to improve reasoning capabilities of existing LLMs. Supports continual learning across tasks, improving planning abilities and generalization.

Conclusion: rSIM provides an effective approach to transform LLMs into RLMs through strategy injection, demonstrating that small models with proper reasoning guidance can surpass much larger models, with promising generalization and continual learning capabilities.

Abstract: Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.

</details>


### [148] [Predicting California Bearing Ratio with Ensemble and Neural Network Models: A Case Study from TÃ¼rkiye](https://arxiv.org/abs/2512.08340)
*Abdullah Hulusi KÃ¶kÃ§am,UÄur DaÄdeviren,Talas Fikret Kurnaz,Alparslan Serhat Demir,Caner Erden*

Main category: cs.AI

TL;DR: This paper presents a machine learning framework for predicting California Bearing Ratio (CBR) using 382 soil samples from Turkey, with Random Forest achieving the best performance (RÂ²=0.83 on test data).


<details>
  <summary>Details</summary>
Motivation: Traditional CBR laboratory tests are time-consuming, costly, and impractical for large-scale or diverse soil profiles. The study aims to leverage machine learning for faster, more efficient CBR prediction in geotechnical engineering.

Method: Used 382 soil samples from various geoclimatic regions in Turkey with physicochemical properties. Tested 12 ML algorithms including decision tree, random forest, gradient boosting, XGBoost, SVM, neural networks, and ensemble methods. Models were trained, validated, and evaluated for generalization.

Result: Random Forest regressor performed best with RÂ² scores of 0.95 (training), 0.76 (validation), and 0.83 (test). The model demonstrated strong nonlinear mapping capabilities for CBR prediction.

Conclusion: ML models, particularly Random Forest, offer an effective alternative to traditional CBR testing, supporting digital transformation in geotechnical engineering and infrastructure design through intelligent, data-driven approaches.

Abstract: The California Bearing Ratio (CBR) is a key geotechnical indicator used to assess the load-bearing capacity of subgrade soils, especially in transportation infrastructure and foundation design. Traditional CBR determination relies on laboratory penetration tests. Despite their accuracy, these tests are often time-consuming, costly, and can be impractical, particularly for large-scale or diverse soil profiles. Recent progress in artificial intelligence, especially machine learning (ML), has enabled data-driven approaches for modeling complex soil behavior with greater speed and precision. This study introduces a comprehensive ML framework for CBR prediction using a dataset of 382 soil samples collected from various geoclimatic regions in TÃ¼rkiye. The dataset includes physicochemical soil properties relevant to bearing capacity, allowing multidimensional feature representation in a supervised learning context. Twelve ML algorithms were tested, including decision tree, random forest, extra trees, gradient boosting, xgboost, k-nearest neighbors, support vector regression, multi-layer perceptron, adaboost, bagging, voting, and stacking regressors. Each model was trained, validated, and evaluated to assess its generalization and robustness. Among them, the random forest regressor performed the best, achieving strong R2 scores of 0.95 (training), 0.76 (validation), and 0.83 (test). These outcomes highlight the model's powerful nonlinear mapping ability, making it a promising tool for predictive geotechnical tasks. The study supports the integration of intelligent, data-centric models in geotechnical engineering, offering an effective alternative to traditional methods and promoting digital transformation in infrastructure analysis and design.

</details>


### [149] [Soil Compaction Parameters Prediction Based on Automated Machine Learning Approach](https://arxiv.org/abs/2512.08343)
*Caner Erden,Alparslan Serhat Demir,Abdullah Hulusi Kokcam,Talas Fikret Kurnaz,Ugur Dagdeviren*

Main category: cs.AI

TL;DR: AutoML approach using XGBoost achieves high accuracy (RÂ²: 80.4% for MDD, 89.1% for OMC) in predicting soil compaction parameters across diverse soil types.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for determining optimum moisture content (OMC) and maximum dry density (MDD) are labor-intensive, and existing empirical regression models have limited accuracy and generalizability across different soil types. ML models also struggle with accuracy and generalization on heterogeneous datasets.

Method: Proposes an automated machine learning (AutoML) approach that automates algorithm selection and hyperparameter optimization to predict OMC and MDD. The study uses extensive experimentation with heterogeneous datasets representing various soil types.

Result: Extreme Gradient Boosting (XGBoost) algorithm performed best, achieving R-squared values of 80.4% for MDD and 89.1% for OMC on separate test datasets. Results demonstrate AutoML's effectiveness in predicting compaction parameters across different soil types.

Conclusion: AutoML approach enhances prediction accuracy and generalizability of soil compaction parameters, contributing to more efficient and reliable construction practices. The study also highlights the importance of heterogeneous datasets for improving ML model performance and generalization.

Abstract: Soil compaction is critical in construction engineering to ensure the stability of structures like road embankments and earth dams. Traditional methods for determining optimum moisture content (OMC) and maximum dry density (MDD) involve labor-intensive laboratory experiments, and empirical regression models have limited applicability and accuracy across diverse soil types. In recent years, artificial intelligence (AI) and machine learning (ML) techniques have emerged as alternatives for predicting these compaction parameters. However, ML models often struggle with prediction accuracy and generalizability, particularly with heterogeneous datasets representing various soil types. This study proposes an automated machine learning (AutoML) approach to predict OMC and MDD. AutoML automates algorithm selection and hyperparameter optimization, potentially improving accuracy and scalability. Through extensive experimentation, the study found that the Extreme Gradient Boosting (XGBoost) algorithm provided the best performance, achieving R-squared values of 80.4% for MDD and 89.1% for OMC on a separate dataset. These results demonstrate the effectiveness of AutoML in predicting compaction parameters across different soil types. The study also highlights the importance of heterogeneous datasets in improving the generalization and performance of ML models. Ultimately, this research contributes to more efficient and reliable construction practices by enhancing the prediction of soil compaction parameters.

</details>


### [150] [The High Cost of Incivility: Quantifying Interaction Inefficiency via Multi-Agent Monte Carlo Simulations](https://arxiv.org/abs/2512.08345)
*Benedikt Mangold*

Main category: cs.AI

TL;DR: LLM-based multi-agent systems simulate workplace toxicity through adversarial debates, showing toxic participants increase conversation duration by 25%, providing an ethical alternative to human-subject research for measuring social friction's operational costs.


<details>
  <summary>Details</summary>
Motivation: Workplace toxicity harms organizational culture but quantifying its direct impact on operational efficiency is challenging due to ethical and practical difficulties in reproducing conflict with human subjects. There's a need for ethical, reproducible methods to measure social friction's effects.

Method: Used LLM-based Multi-Agent Systems to simulate 1-on-1 adversarial debates as a "sociological sandbox." Employed Monte Carlo method to simulate hundreds of discussions, measuring convergence time (arguments needed to reach conclusion) between baseline control groups and treatment groups with "toxic" system prompts.

Result: Found statistically significant increase of approximately 25% in conversation duration involving toxic participants. This "latency of toxicity" serves as a proxy for financial damage in corporate and academic settings.

Conclusion: Agent-based modeling provides a reproducible, ethical alternative to human-subject research for measuring the mechanics of social friction. The approach demonstrates that workplace toxicity significantly delays decision-making processes, with measurable operational consequences.

Abstract: Workplace toxicity is widely recognized as detrimental to organizational culture, yet quantifying its direct impact on operational efficiency remains methodologically challenging due to the ethical and practical difficulties of reproducing conflict in human subjects. This study leverages Large Language Model (LLM) based Multi-Agent Systems to simulate 1-on-1 adversarial debates, creating a controlled "sociological sandbox". We employ a Monte Carlo method to simulate hundrets of discussions, measuring the convergence time (defined as the number of arguments required to reach a conclusion) between a baseline control group and treatment groups involving agents with "toxic" system prompts. Our results demonstrate a statistically significant increase of approximately 25\% in the duration of conversations involving toxic participants. We propose that this "latency of toxicity" serves as a proxy for financial damage in corporate and academic settings. Furthermore, we demonstrate that agent-based modeling provides a reproducible, ethical alternative to human-subject research for measuring the mechanics of social friction.

</details>


### [151] [Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making](https://arxiv.org/abs/2512.08366)
*Wentao Zhang,Qunbo Wang,Tao Zhang,Junsheng Wu,Hongping Gan,Yang Liu,Ling Dai,Shizhuang Deng,Shuntong Sun*

Main category: cs.AI

TL;DR: DuSAR is a demonstration-free LLM agent framework using dual-strategy reasoning (holistic plan + local policy) with reflection, achieving SOTA performance while reducing computational overhead.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. The authors aim to create a more robust, efficient agent inspired by human problem-solving.

Method: DuSAR uses a single frozen LLM with two complementary strategies: high-level holistic plan and context-grounded local policy. These interact through a lightweight reflection mechanism with Strategy Fitness Score, allowing dynamic revision of global plans when stuck or refinement upon meaningful advancement.

Result: Achieves state-of-the-art performance on ALFWorld (37.1% success with Llama3.1-70B, more than doubling prior best of 13.0%) and Mind2Web (4.02%, also more than doubling strongest baseline). Reduces per-step token consumption by 3-9X while maintaining strong performance.

Conclusion: DuSAR demonstrates that dual-strategy coordination with reflection enables efficient, robust LLM agents without external demonstrations, with optional integration of expert knowledge further boosting performance, showing flexibility and compatibility with external knowledge.

Abstract: Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.

</details>


### [152] [DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals](https://arxiv.org/abs/2512.08379)
*Kaiwei Liu,Yuting He,Bufang Yang,Mu Yuan,Chun Man Victor Wong,Ho Pong Andrew Sze,Zhenyu Yan,Hongkai Chen*

Main category: cs.AI

TL;DR: DeepFeature is an LLM-powered framework that generates context-aware features for wearable biosignals by integrating expert knowledge with task settings, using iterative refinement and robust code translation.


<details>
  <summary>Details</summary>
Motivation: Existing feature extraction methods for wearable biosignals lack task-specific contextual knowledge, struggle with optimal feature selection in high-dimensional spaces, and are prone to code generation errors.

Method: Proposes DeepFeature with: 1) multi-source feature generation integrating expert knowledge and task settings, 2) iterative feature refinement using assessment-based feedback, and 3) robust multi-layer filtering/verification for feature-to-code translation.

Result: Achieves average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baselines, outperforms state-of-the-art on five tasks while maintaining comparable performance on remaining tasks.

Conclusion: DeepFeature successfully addresses limitations of existing feature extraction methods by leveraging LLMs for context-aware feature generation, demonstrating significant performance improvements in wearable biosignal analysis tasks.

Abstract: Biosignals collected from wearable devices are widely utilized in healthcare applications. Machine learning models used in these applications often rely on features extracted from biosignals due to their effectiveness, lower data dimensionality, and wide compatibility across various model architectures. However, existing feature extraction methods often lack task-specific contextual knowledge, struggle to identify optimal feature extraction settings in high-dimensional feature space, and are prone to code generation and automation errors. In this paper, we propose DeepFeature, the first LLM-empowered, context-aware feature generation framework for wearable biosignals. DeepFeature introduces a multi-source feature generation mechanism that integrates expert knowledge with task settings. It also employs an iterative feature refinement process that uses feature assessment-based feedback for feature re-selection. Additionally, DeepFeature utilizes a robust multi-layer filtering and verification approach for robust feature-to-code translation to ensure that the extraction functions run without crashing. Experimental evaluation results show that DeepFeature achieves an average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baseline methods. It outperforms state-of-the-art approaches on five tasks while maintaining comparable performance on the remaining tasks.

</details>


### [153] [Prismatic World Model: Learning Compositional Dynamics for Planning in Hybrid Systems](https://arxiv.org/abs/2512.08411)
*Mingwei Li,Xiaoyuan Zhang,Chengwei Yang,Zilong Zheng,Yaodong Yang*

Main category: cs.AI

TL;DR: PRISM-WM is a structured world model that decomposes hybrid dynamics into composable primitives using Mixture-of-Experts to handle sharp mode transitions, reducing rollout drift for better planning.


<details>
  <summary>Details</summary>
Motivation: Conventional latent world models use monolithic neural networks that enforce global continuity, causing over-smoothing of distinct dynamic modes (e.g., sticking vs. sliding, flight vs. stance). This smoothing leads to catastrophic compounding errors during long-horizon planning, making search unreliable at physical boundaries.

Method: PRISM-WM uses a context-aware Mixture-of-Experts framework where a gating mechanism identifies the current physical mode, and specialized experts predict associated transition dynamics. It includes a latent orthogonalization objective to ensure expert diversity and prevent mode collapse.

Result: PRISM-WM significantly reduces rollout drift by accurately modeling sharp mode transitions. Extensive experiments on challenging continuous control benchmarks (including high-dimensional humanoids and diverse multi-task settings) show it provides superior high-fidelity substrate for trajectory optimization algorithms like TD-MPC.

Conclusion: PRISM-WM proves to be a powerful foundational model for next-generation model-based agents by effectively handling hybrid dynamics through structured decomposition into composable primitives.

Abstract: Model-based planning in robotic domains is fundamentally challenged by the hybrid nature of physical dynamics, where continuous motion is punctuated by discrete events such as contacts and impacts. Conventional latent world models typically employ monolithic neural networks that enforce global continuity, inevitably over-smoothing the distinct dynamic modes (e.g., sticking vs. sliding, flight vs. stance). For a planner, this smoothing results in catastrophic compounding errors during long-horizon lookaheads, rendering the search process unreliable at physical boundaries. To address this, we introduce the Prismatic World Model (PRISM-WM), a structured architecture designed to decompose complex hybrid dynamics into composable primitives. PRISM-WM leverages a context-aware Mixture-of-Experts (MoE) framework where a gating mechanism implicitly identifies the current physical mode, and specialized experts predict the associated transition dynamics. We further introduce a latent orthogonalization objective to ensure expert diversity, effectively preventing mode collapse. By accurately modeling the sharp mode transitions in system dynamics, PRISM-WM significantly reduces rollout drift. Extensive experiments on challenging continuous control benchmarks, including high-dimensional humanoids and diverse multi-task settings, demonstrate that PRISM-WM provides a superior high-fidelity substrate for trajectory optimization algorithms (e.g., TD-MPC), proving its potential as a powerful foundational model for next-generation model-based agents.

</details>


### [154] [From Accuracy to Impact: The Impact-Driven AI Framework (IDAIF) for Aligning Engineering Architecture with Theory of Change](https://arxiv.org/abs/2512.08449)
*Yong-Woon Kim*

Main category: cs.AI

TL;DR: IDAIF is a novel AI framework integrating Theory of Change principles with AI architecture to ensure AI systems align with human values and create positive social impact across high-stakes domains.


<details>
  <summary>Details</summary>
Motivation: Current AI approaches focus too much on technical performance metrics while ignoring sociotechnical dimensions and alignment with human values, especially in high-stakes domains like healthcare, finance, and public policy where AI's impact is significant.

Method: IDAIF maps Theory of Change's five-stage model (Inputs-Activities-Outputs-Outcomes-Impact) to AI architectural layers (Data-Pipeline-Inference-Agentic-Normative), incorporating multi-objective Pareto optimization, hierarchical multi-agent orchestration, causal DAGs for hallucination mitigation, adversarial debiasing with RLHF, and an Assurance Layer for managing assumption failures.

Result: The framework provides formal mathematical formulations for each component and demonstrates successful application through three case studies in healthcare, cybersecurity, and software engineering domains.

Conclusion: IDAIF represents a paradigm shift from model-centric to impact-centric AI development, offering engineers concrete architectural patterns for building ethical, trustworthy, and socially beneficial AI systems that align with human values and intentions.

Abstract: This paper introduces the Impact-Driven AI Framework (IDAIF), a novel architectural methodology that integrates Theory of Change (ToC) principles with modern artificial intelligence system design. As AI systems increasingly influence high-stakes domains including healthcare, finance, and public policy, the alignment problem--ensuring AI behavior corresponds with human values and intentions--has become critical. Current approaches predominantly optimize technical performance metrics while neglecting the sociotechnical dimensions of AI deployment. IDAIF addresses this gap by establishing a systematic mapping between ToC's five-stage model (Inputs-Activities-Outputs-Outcomes-Impact) and corresponding AI architectural layers (Data Layer-Pipeline Layer-Inference Layer-Agentic Layer-Normative Layer). Each layer incorporates rigorous theoretical foundations: multi-objective Pareto optimization for value alignment, hierarchical multi-agent orchestration for outcome achievement, causal directed acyclic graphs (DAGs) for hallucination mitigation, and adversarial debiasing with Reinforcement Learning from Human Feedback (RLHF) for fairness assurance. We provide formal mathematical formulations for each component and introduce an Assurance Layer that manages assumption failures through guardian architectures. Three case studies demonstrate IDAIF application across healthcare, cybersecurity, and software engineering domains. This framework represents a paradigm shift from model-centric to impact-centric AI development, providing engineers with concrete architectural patterns for building ethical, trustworthy, and socially beneficial AI systems.

</details>


### [155] [Using reinforcement learning to probe the role of feedback in skill acquisition](https://arxiv.org/abs/2512.08463)
*Antonio Terpin,Raffaello D'Andrea*

Main category: cs.AI

TL;DR: RL agent learns to control drag on spinning cylinder in water channel using flow feedback, discovering high-performance strategies that work open-loop after training.


<details>
  <summary>Details</summary>
Motivation: To study skill acquisition under controlled conditions, bypassing human subjects and using a physical system with rich dynamics that's difficult to simulate accurately.

Method: Interface reinforcement learning agent with spinning cylinder in tabletop water channel to maximize/minimize drag; use high-dimensional flow feedback during training.

Result: Agent discovers high-performance drag-control strategies with minutes of real-world interaction; learned policies work open-loop without feedback; flow feedback crucial for learning maximization but not for execution.

Conclusion: Learning high-performance skills can require richer information than executing them; learning conditions can be kind or wicked depending on goal, not dynamics complexity.

Abstract: Many high-performance human activities are executed with little or no external feedback: think of a figure skater landing a triple jump, a pitcher throwing a curveball for a strike, or a barista pouring latte art. To study the process of skill acquisition under fully controlled conditions, we bypass human subjects. Instead, we directly interface a generalist reinforcement learning agent with a spinning cylinder in a tabletop circulating water channel to maximize or minimize drag. This setup has several desirable properties. First, it is a physical system, with the rich interactions and complex dynamics that only the physical world has: the flow is highly chaotic and extremely difficult, if not impossible, to model or simulate accurately. Second, the objective -- drag minimization or maximization -- is easy to state and can be captured directly in the reward, yet good strategies are not obvious beforehand. Third, decades-old experimental studies provide recipes for simple, high-performance open-loop policies. Finally, the setup is inexpensive and far easier to reproduce than human studies. In our experiments we find that high-dimensional flow feedback lets the agent discover high-performance drag-control strategies with only minutes of real-world interaction. When we later replay the same action sequences without any feedback, we obtain almost identical performance. This shows that feedback, and in particular flow feedback, is not needed to execute the learned policy. Surprisingly, without flow feedback during training the agent fails to discover any well-performing policy in drag maximization, but still succeeds in drag minimization, albeit more slowly and less reliably. Our studies show that learning a high-performance skill can require richer information than executing it, and learning conditions can be kind or wicked depending solely on the goal, not on dynamics or policy complexity.

</details>


### [156] [Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance](https://arxiv.org/abs/2512.08492)
*Aliaksei Kaliutau*

Main category: cs.AI

TL;DR: The paper proposes a paradigm shift from Code Property Graphs to Data Transformation Graphs for repository-scale Automated Program Repair, introducing a multi-agent framework that resolves semantic traps in RAG systems and achieves 87.1% resolution rate on SWE-Verified benchmark.


<details>
  <summary>Details</summary>
Motivation: Current approaches to repository-scale Automated Program Repair (APR) use control-centric paradigms that force agents to navigate complex directory structures and irrelevant control logic, creating challenges for effective code repair at scale.

Method: Proposes Data Transformation Graph (DTG) that inverts topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage. Introduces multi-agent framework reconciling data integrity navigation with control flow logic, implemented as Autonomous Issue Resolver (AIR) using neuro-symbolic reasoning.

Result: Demonstrates 87.1% resolution rate on SWE-Verified benchmark, resolving the "Semantic Trap" inherent in standard RAG systems in modern coding agents and showing good results on several SWE benchmarks.

Conclusion: The DTG approach addresses core limitations of current AI code-assistant tools and provides a more robust foundation for software-dependent world by enabling scalable logic repair through data lineage tracing rather than control flow navigation.

Abstract: Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory structures and irrelevant control logic. In this paper, we propose a paradigm shift from the standard Code Property Graphs (CPGs) to the concept of Data Transformation Graph (DTG) that inverts the topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage rather than control flow. We introduce a multi-agent framework that reconciles data integrity navigation with control flow logic. Our theoretical analysis and case studies demonstrate that this approach resolves the "Semantic Trap" inherent in standard RAG systems in modern coding agents. We provide a comprehensive implementation in the form of Autonomous Issue Resolver (AIR), a self-improvement system for zero-touch code maintenance that utilizes neuro-symbolic reasoning and uses the DTG structure for scalable logic repair. Our approach has demonstrated good results on several SWE benchmarks, reaching a resolution rate of 87.1% on SWE-Verified benchmark. Our approach directly addresses the core limitations of current AI code-assistant tools and tackles the critical need for a more robust foundation for our increasingly software-dependent world.

</details>


### [157] [A Lightweight Transfer Learning-Based State-of-Health Monitoring with Application to Lithium-ion Batteries in Unmanned Air Vehicles](https://arxiv.org/abs/2512.08512)
*Jiang Liu,Yan Qin,Wei Dai,Chau Yuen*

Main category: cs.AI

TL;DR: Lightweight transfer learning approach for battery SOH monitoring using constructive incremental transfer learning (CITL) with semi-supervised mechanism and convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: Traditional transfer learning for battery SOH monitoring consumes too much computational resources for portable mobile devices, reducing working endurance. Need lightweight approach that works with variable working conditions.

Method: Proposes CITL with semi-supervised TL using unlabeled target data, constructive network node addition, and optimization through structural risk minimization, transfer mismatching minimization, and manifold consistency maximization.

Result: Outperforms SS-TCA, MMD-LSTM-DA, DDAN, BO-CNN-TL, and ASÂ³LSTM by 83.73%, 61.15%, 28.24%, 87.70%, and 57.34% respectively in RMSE for SOH estimation on realistic UAV battery dataset.

Conclusion: CITL provides effective lightweight transfer learning for battery SOH monitoring in portable devices with theoretical convergence guarantees and superior performance over existing methods.

Abstract: Accurate and rapid state-of-health (SOH) monitoring plays an important role in indicating energy information for lithium-ion battery-powered portable mobile devices. To confront their variable working conditions, transfer learning (TL) emerges as a promising technique for leveraging knowledge from data-rich source working conditions, significantly reducing the training data required for SOH monitoring from target working conditions. However, traditional TL-based SOH monitoring is infeasible when applied in portable mobile devices since substantial computational resources are consumed during the TL stage and unexpectedly reduce the working endurance. To address these challenges, this paper proposes a lightweight TL-based SOH monitoring approach with constructive incremental transfer learning (CITL). First, taking advantage of the unlabeled data in the target domain, a semi-supervised TL mechanism is proposed to minimize the monitoring residual in a constructive way, through iteratively adding network nodes in the CITL. Second, the cross-domain learning ability of node parameters for CITL is comprehensively guaranteed through structural risk minimization, transfer mismatching minimization, and manifold consistency maximization. Moreover, the convergence analysis of the CITL is given, theoretically guaranteeing the efficacy of TL performance and network compactness. Finally, the proposed approach is verified through extensive experiments with a realistic unmanned air vehicles (UAV) battery dataset collected from dozens of flight missions. Specifically, the CITL outperforms SS-TCA, MMD-LSTM-DA, DDAN, BO-CNN-TL, and AS$^3$LSTM, in SOH estimation by 83.73%, 61.15%, 28.24%, 87.70%, and 57.34%, respectively, as evaluated using the index root mean square error.

</details>


### [158] [Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans](https://arxiv.org/abs/2512.08536)
*Tammy Zhong,Yang Song,Maurice Pagnucco*

Main category: cs.AI

TL;DR: Principles2Plan: Human-LLM collaboration system for generating context-sensitive ethical rules to guide automated planning in robotics.


<details>
  <summary>Details</summary>
Motivation: Robots need ethical awareness in human environments, but existing planning tools lack ethical support. Manual specification of ethical rules is labor-intensive and context-specific, creating a need for practical ethical planning solutions.

Method: Interactive prototype where domain expert provides planning domain, problem details, and high-level principles (beneficence, privacy). LLM collaborates to generate operationalizable ethical rules consistent with principles. User reviews, prioritizes rules, then supplies to planner for ethically-informed plans.

Result: First system supporting users in generating principle-grounded rules for classical planning contexts. Demonstrates potential of human-LLM collaboration for practical ethical automated planning.

Conclusion: Principles2Plan showcases how human-LLM collaboration can make ethical automated planning more practical and feasible, addressing the gap in existing planning tools for ethical awareness.

Abstract: Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible.

</details>


### [159] [The SMART+ Framework for AI Systems](https://arxiv.org/abs/2512.08592)
*Laxmiraju Kandikatla,Branislav Radeljic*

Main category: cs.AI

TL;DR: The paper introduces the SMART+ Framework for AI governance across industries, addressing safety, accountability, and regulatory challenges in AI systems.


<details>
  <summary>Details</summary>
Motivation: AI systems are now integral across industries (clinical research, finance, manufacturing) but introduce new challenges regarding safety, accountability, and regulatory compliance that need to be addressed.

Method: The authors introduce the SMART+ Framework - a structured model built on Safety, Monitoring, Accountability, Reliability, and Transparency, enhanced with Privacy & Security, Data Governance, Fairness & Bias, and Guardrails.

Result: SMART+ offers a practical, comprehensive approach to evaluating and governing AI systems across industries, demonstrating risk mitigation, trust-building, and compliance readiness.

Conclusion: SMART+ provides a robust foundation for effective AI governance in clinical research by enabling responsible AI adoption and ensuring auditability, aligning with evolving regulatory guidance.

Abstract: Artificial Intelligence (AI) systems are now an integral part of multiple industries. In clinical research, AI supports automated adverse event detection in clinical trials, patient eligibility screening for protocol enrollment, and data quality validation. Beyond healthcare, AI is transforming finance through real-time fraud detection, automated loan risk assessment, and algorithmic decision-making. Similarly, in manufacturing, AI enables predictive maintenance to reduce equipment downtime, enhances quality control through computer-vision inspection, and optimizes production workflows using real-time operational data. While these technologies enhance operational efficiency, they introduce new challenges regarding safety, accountability, and regulatory compliance. To address these concerns, we introduce the SMART+ Framework - a structured model built on the pillars of Safety, Monitoring, Accountability, Reliability, and Transparency, and further enhanced with Privacy & Security, Data Governance, Fairness & Bias, and Guardrails. SMART+ offers a practical, comprehensive approach to evaluating and governing AI systems across industries. This framework aligns with evolving mechanisms and regulatory guidance to integrate operational safeguards, oversight procedures, and strengthened privacy and governance controls. SMART+ demonstrates risk mitigation, trust-building, and compliance readiness. By enabling responsible AI adoption and ensuring auditability, SMART+ provides a robust foundation for effective AI governance in clinical research.

</details>


### [160] [CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models](https://arxiv.org/abs/2512.08609)
*Hui Wang,Yang Liu,Xiaoyu Zhang,Chaoxu Mu*

Main category: cs.AI

TL;DR: CogMCTS: A novel cognitive-guided MCTS framework that integrates LLMs with Monte Carlo Tree Search for automated heuristic design, using multi-round cognitive feedback and dual-track expansion to improve optimization performance.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based evolutionary methods for Automatic Heuristic Design (AHD) suffer from local optima issues, while current LLM-MCTS integrations have limited multi-round cognitive integration and constrained search diversity.

Method: CogMCTS tightly integrates LLM cognitive guidance with MCTS using multi-round cognitive feedback (incorporating historical experience, node information, and negative outcomes), dual-track node expansion with elite heuristic management, and strategic mutation of heuristic forms and parameters.

Result: Experimental results show CogMCTS outperforms existing LLM-based AHD methods in stability, efficiency, and solution quality.

Conclusion: CogMCTS effectively addresses limitations of existing LLM-based AHD approaches by achieving better exploration-exploitation balance and enhanced search diversity through cognitive-guided MCTS integration.

Abstract: Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality.

</details>


### [161] [Protein Secondary Structure Prediction Using Transformers](https://arxiv.org/abs/2512.08613)
*Manzi Kevin Maxime*

Main category: cs.AI

TL;DR: Transformer model predicts protein secondary structures (alpha helices, beta sheets, coils) from amino acid sequences using attention mechanisms and sliding-window data augmentation.


<details>
  <summary>Details</summary>
Motivation: Protein secondary structure prediction is essential for understanding protein function, as structures like alpha helices and beta sheets determine protein behavior and interactions.

Method: Transformer-based model with attention mechanisms applied to protein sequence data, using sliding-window data augmentation on CB513 dataset to expand training samples.

Result: Transformer shows strong generalization across variable-length sequences and effectively captures both local and long-range residue interactions.

Conclusion: Transformer architecture is effective for protein secondary structure prediction, handling variable sequence lengths and capturing complex residue interactions through attention mechanisms.

Abstract: Predicting protein secondary structures such as alpha helices, beta sheets, and coils from amino acid sequences is essential for understanding protein function. This work presents a transformer-based model that applies attention mechanisms to protein sequence data to predict structural motifs. A sliding-window data augmentation technique is used on the CB513 dataset to expand the training samples. The transformer shows strong ability to generalize across variable-length sequences while effectively capturing both local and long-range residue interactions.

</details>


### [162] [See-Control: A Multimodal Agent Framework for Smartphone Interaction with a Robotic Arm](https://arxiv.org/abs/2512.08629)
*Haoyu Zhao,Weizhong Ding,Yuhao Yang,Zheng Tian,Linyi Yang,Kun Shao,Jun Wang*

Main category: cs.AI

TL;DR: See-Control is a framework for smartphone operation using a low-DoF robotic arm instead of ADB, enabling platform-agnostic physical interaction with phones.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM-based smartphone operation methods rely on Android Debug Bridge (ADB), which limits them to Android devices and requires system back-end access. There's a need for a platform-agnostic solution that works through physical interaction rather than software interfaces.

Method: See-Control introduces the Embodied Smartphone Operation (ESO) task and framework with three components: 1) ESO benchmark with 155 tasks and evaluation metrics, 2) MLLM-based embodied agent that generates robotic control commands without ADB or system access, and 3) annotated dataset of operation episodes for research.

Result: The framework enables smartphone operation via direct physical interaction with a low-DoF robotic arm, providing a platform-agnostic solution that bridges digital agents with the physical world.

Conclusion: See-Control represents a concrete step toward enabling home robots to perform smartphone-dependent tasks in realistic environments by moving beyond ADB-dependent approaches to physical interaction.

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled their use as intelligent agents for smartphone operation. However, existing methods depend on the Android Debug Bridge (ADB) for data transmission and action execution, limiting their applicability to Android devices. In this work, we introduce the novel Embodied Smartphone Operation (ESO) task and present See-Control, a framework that enables smartphone operation via direct physical interaction with a low-DoF robotic arm, offering a platform-agnostic solution. See-Control comprises three key components: (1) an ESO benchmark with 155 tasks and corresponding evaluation metrics; (2) an MLLM-based embodied agent that generates robotic control commands without requiring ADB or system back-end access; and (3) a richly annotated dataset of operation episodes, offering valuable resources for future research. By bridging the gap between digital agents and the physical world, See-Control provides a concrete step toward enabling home robots to perform smartphone-dependent tasks in realistic environments.

</details>


### [163] [Multi-Agent Intelligence for Multidisciplinary Decision-Making in Gastrointestinal Oncology](https://arxiv.org/abs/2512.08674)
*Rongzhao Zhang,Junqiao Wang,Shuyun Yang,Mouxiao Bian,Chao Ding,Yuwei Bai,Chihao Zhang,Yuguang Shen,Lei Wang,Lei Zheng,Qiujuan Yan,Yun Zhong,Meiling Liu,Jiwei Yu,Zheng Wang,Jie Xu,Meng Luo*

Main category: cs.AI

TL;DR: A hierarchical Multi-Agent Framework mimicking human MDT collaboration outperforms monolithic MLLMs in GI oncology by improving reasoning logic and medical accuracy, achieving 4.60/5 expert evaluation score.


<details>
  <summary>Details</summary>
Motivation: Multimodal clinical reasoning in GI oncology requires integrating endoscopic, radiological, and biochemical data, but current MLLMs suffer from context dilution and hallucination when processing complex medical histories.

Method: Proposed a hierarchical Multi-Agent Framework that emulates human Multidisciplinary Team (MDT) collaborative workflow to address MLLM limitations.

Result: Achieved composite expert evaluation score of 4.60/5.00, significantly outperforming monolithic baseline, with most substantial improvements in reasoning logic and medical accuracy.

Conclusion: Agent-based collaboration provides scalable, interpretable, and clinically robust paradigm for automated oncology decision support by mimicking human MDT workflows.

Abstract: Multimodal clinical reasoning in the field of gastrointestinal (GI) oncology necessitates the integrated interpretation of endoscopic imagery, radiological data, and biochemical markers. Despite the evident potential exhibited by Multimodal Large Language Models (MLLMs), they frequently encounter challenges such as context dilution and hallucination when confronted with intricate, heterogeneous medical histories. In order to address these limitations, a hierarchical Multi-Agent Framework is proposed, which emulates the collaborative workflow of a human Multidisciplinary Team (MDT). The system attained a composite expert evaluation score of 4.60/5.00, thereby demonstrating a substantial improvement over the monolithic baseline. It is noteworthy that the agent-based architecture yielded the most substantial enhancements in reasoning logic and medical accuracy. The findings indicate that mimetic, agent-based collaboration provides a scalable, interpretable, and clinically robust paradigm for automated decision support in oncology.

</details>


### [164] [Deconstructing the Dual Black Box:A Plug-and-Play Cognitive Framework for Human-AI Collaborative Enhancement and Its Implications for AI Governance](https://arxiv.org/abs/2512.08740)
*Yiming Lu*

Main category: cs.AI

TL;DR: Proposes human-AI collaborative cognitive enhancement to transform dual black boxes (human intuition + AI decisions) into composable, auditable functional white-box systems via structured meta-interaction and plug-and-play cognitive frameworks.


<details>
  <summary>Details</summary>
Motivation: Addresses the fundamental divide between human expert intuition (cognitive black box) and AI decision-making (computational black box) that lacks trustworthiness. Aims to create a new paradigm where AI becomes a thinking partner rather than just a tool, enabling cognitive equity and verifiable AI governance.

Method: Introduces the "plug-and-play cognitive framework" - computable knowledge packages extracted from expert dialogues, loaded into Recursive Adversarial Meta-Thinking Network (RAMTN). Uses structured "meta-interaction" to convert expert thinking (medical diagnostics, teaching intuition) into reusable, scalable public assets.

Result: Provides first engineering proof for "cognitive equity" and opens new path for AI governance through "transparency of interaction protocols" rather than prying into model internals. Framework is open-sourced to promote technology for good and cognitive inclusion.

Conclusion: Enables paradigm shift from "AI as a tool" to "AI as thinking partner" by creating composable, auditable, extensible functional white-box systems that bridge human expertise and AI capabilities through structured meta-interaction and reusable cognitive frameworks.

Abstract: Currently, there exists a fundamental divide between the "cognitive black box" (implicit intuition) of human experts and the "computational black box" (untrustworthy decision-making) of artificial intelligence (AI). This paper proposes a new paradigm of "human-AI collaborative cognitive enhancement," aiming to transform the dual black boxes into a composable, auditable, and extensible "functional white-box" system through structured "meta-interaction." The core breakthrough lies in the "plug-and-play cognitive framework"--a computable knowledge package that can be extracted from expert dialogues and loaded into the Recursive Adversarial Meta-Thinking Network (RAMTN). This enables expert thinking, such as medical diagnostic logic and teaching intuition, to be converted into reusable and scalable public assets, realizing a paradigm shift from "AI as a tool" to "AI as a thinking partner." This work not only provides the first engineering proof for "cognitive equity" but also opens up a new path for AI governance: constructing a verifiable and intervenable governance paradigm through "transparency of interaction protocols" rather than prying into the internal mechanisms of models. The framework is open-sourced to promote technology for good and cognitive inclusion. This paper is an independent exploratory research conducted by the author. All content presented, including the theoretical framework (RAMTN), methodology (meta-interaction), system implementation, and case validation, constitutes the author's individual research achievements.

</details>


### [165] [Towards Foundation Models with Native Multi-Agent Intelligence](https://arxiv.org/abs/2512.08743)
*Shuyue Hu,Haoyang Yan,Yiqun Zhang,Yang Chen,Dongzhan Zhou,Lei Bai*

Main category: cs.AI

TL;DR: Current foundation models excel as single agents but lack robust multi-agent intelligence despite assumptions that single-agent abilities naturally transfer to multi-agent contexts.


<details>
  <summary>Details</summary>
Motivation: Foundation models are becoming the "brain" of AI agents, but while recent work focuses on single-agent capabilities (GUI interaction, tool use), the next frontier is endowing FMs with native multi-agent intelligence for effective collaboration and coordination.

Method: Identified four core capabilities for multi-agent contexts (understanding, planning, efficient communication, adaptation), conducted extensive empirical evaluation across 41 large language models to test whether single-agent performance transfers to multi-agent settings.

Result: Strong single-agent performance does NOT automatically yield robust multi-agent intelligence. The paper provides empirical evidence showing this gap exists across multiple models.

Conclusion: Research directions are needed in dataset construction, evaluation frameworks, training paradigms, and safety considerations to build foundation models with native multi-agent intelligence, as current models lack these capabilities despite strong single-agent performance.

Abstract: Foundation models (FMs) are increasingly assuming the role of the "brain" of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence across 41 large language models showing that strong single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.

</details>


### [166] [Performance Comparison of Aerial RIS and STAR-RIS in 3D Wireless Environments](https://arxiv.org/abs/2512.08755)
*Dongdong Yang,Bin Li,Jiguang He*

Main category: cs.AI

TL;DR: Aerial STAR-RIS outperforms RIS at low altitudes due to full-space coverage, while RIS performs better near base stations at higher altitudes in 6G networks.


<details>
  <summary>Details</summary>
Motivation: RIS and STAR-RIS on UAVs offer flexible deployment for 6G networks, but comprehensive performance comparison between aerial RIS and STAR-RIS architectures has not been thoroughly investigated.

Method: Established accurate channel models with directional radiation patterns, formulated joint optimization problems for both architectures, and proposed efficient solution using weighted minimum mean square error and block coordinate descent algorithms.

Result: STAR-RIS outperforms RIS in low-altitude scenarios due to full-space coverage capability, while RIS delivers better performance near base stations at higher altitudes.

Conclusion: The findings provide practical insights for deployment of aerial intelligent surfaces in future 6G communication systems, with STAR-RIS preferred for low-altitude coverage and RIS for high-altitude near-base-station scenarios.

Abstract: Reconfigurable intelligent surface (RIS) and simultaneously transmitting and reflecting RIS (STAR-RIS) have emerged as key enablers for enhancing wireless coverage and capacity in next-generation networks. When mounted on unmanned aerial vehicles (UAVs), they benefit from flexible deployment and improved line-of-sight conditions. Despite their promising potential, a comprehensive performance comparison between aerial RIS and STAR-RIS architectures has not been thoroughly investigated. This letter presents a detailed performance comparison between aerial RIS and STAR-RIS in three-dimensional wireless environments. Accurate channel models incorporating directional radiation patterns are established, and the influence of deployment altitude and orientation is thoroughly examined. To optimize the system sum-rate, we formulate joint optimization problems for both architectures and propose an efficient solution based on the weighted minimum mean square error and block coordinate descent algorithms. Simulation results reveal that STAR-RIS outperforms RIS in low-altitude scenarios due to its full-space coverage capability, whereas RIS delivers better performance near the base station at higher altitudes. The findings provide practical insights for the deployment of aerial intelligent surfaces in future 6G communication systems.

</details>


### [167] [A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows](https://arxiv.org/abs/2512.08769)
*Eranga Bandara,Ross Gore,Peter Foytik,Sachin Shetty,Ravi Mukkamala,Abdul Rahman,Xueping Liang,Safdar H. Bouk,Amin Hass,Sachini Rajapakse,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.AI

TL;DR: Practical guide for designing, developing, and deploying production-grade agentic AI systems with structured engineering lifecycle and nine core best practices.


<details>
  <summary>Details</summary>
Motivation: As agentic AI adoption accelerates, organizations need practical guidance on building reliable, observable, maintainable, and safe production-grade agentic workflows that integrate multiple specialized agents, tools, and orchestration logic.

Method: Introduces structured engineering lifecycle covering workflow decomposition, multi-agent design patterns, Model Context Protocol (MCP), tool integration, deterministic orchestration, Responsible-AI considerations, and deployment strategies. Presents nine core best practices including tool-first design, pure-function invocation, single-responsibility agents, externalized prompt management, and containerized deployment.

Result: Provides comprehensive case study of a multimodal news-analysis and media-generation workflow demonstrating the principles in practice, offering architectural guidance, operational patterns, and practical implementation insights.

Conclusion: The paper offers foundational reference for building robust, extensible, and production-ready agentic AI workflows by combining structured engineering approaches with practical best practices for real-world deployment.

Abstract: Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.

</details>


### [168] [CARLoS: Retrieval via Concise Assessment Representation of LoRAs at Scale](https://arxiv.org/abs/2512.08826)
*Shahar Sarfaty,Adi Haviv,Uri Hacohen,Niva Elkin-Koren,Roi Livni,Amit H. Bermano*

Main category: cs.AI

TL;DR: CARLoS is a framework for characterizing LoRAs using CLIP embeddings to create a three-part representation (Directions, Strength, Consistency) that enables semantic retrieval and analysis without relying on metadata.


<details>
  <summary>Details</summary>
Motivation: The proliferation of generative components like LoRAs has created an unstructured ecosystem where discovery methods rely on unreliable user descriptions or biased popularity metrics, hindering usability.

Method: Analyzed over 650 LoRAs by using them in image generation across various prompts and seeds. Used CLIP embeddings and their difference to base-model generations to define a three-part representation: Directions (semantic shift), Strength (effect significance), and Consistency (effect stability).

Result: Developed an efficient retrieval framework that semantically matches textual queries to relevant LoRAs while filtering overly strong or unstable ones, outperforming textual baselines in automated and human evaluations. The representation also supports analyses linking Strength and Consistency to legal notions of substantiality and volition in copyright.

Conclusion: CARLoS provides a practical system for LoRA characterization and retrieval without requiring metadata, with broader relevance for LoRA analysis including legal considerations in copyright.

Abstract: The rapid proliferation of generative components, such as LoRAs, has created a vast but unstructured ecosystem. Existing discovery methods depend on unreliable user descriptions or biased popularity metrics, hindering usability. We present CARLoS, a large-scale framework for characterizing LoRAs without requiring additional metadata. Analyzing over 650 LoRAs, we employ them in image generation over a variety of prompts and seeds, as a credible way to assess their behavior. Using CLIP embeddings and their difference to a base-model generation, we concisely define a three-part representation: Directions, defining semantic shift; Strength, quantifying the significance of the effect; and Consistency, quantifying how stable the effect is. Using these representations, we develop an efficient retrieval framework that semantically matches textual queries to relevant LoRAs while filtering overly strong or unstable ones, outperforming textual baselines in automated and human evaluations. While retrieval is our primary focus, the same representation also supports analyses linking Strength and Consistency to legal notions of substantiality and volition, key considerations in copyright, positioning CARLoS as a practical system with broader relevance for LoRA analysis.

</details>


### [169] [Interpolation in Knowledge Representation](https://arxiv.org/abs/2512.08833)
*Jean Christoph Jung,Patrick Koopmann,Matthias Knorr*

Main category: cs.AI

TL;DR: Survey paper examining Craig interpolation and uniform interpolation in description logics and logic programming, covering theoretical results and practical computation methods.


<details>
  <summary>Details</summary>
Motivation: Craig interpolation and uniform interpolation have important applications in knowledge representation (explainability, forgetting, modularization, reuse, learning), but many KR formalisms lack these properties and computing interpolants is challenging.

Method: The paper analyzes two prominent knowledge representation formalisms: description logics and logic programming. It examines theoretical results about interpolation properties and discusses practical methods for computing interpolants in these formalisms.

Result: The paper provides a comprehensive survey of interpolation properties and computational methods in description logics and logic programming, highlighting which formalisms support interpolation and practical approaches for computing interpolants.

Conclusion: Understanding interpolation properties and developing practical methods for computing interpolants is crucial for advancing knowledge representation applications, despite the challenges posed by many formalisms lacking these properties.

Abstract: Craig interpolation and uniform interpolation have many applications in knowledge representation, including explainability, forgetting, modularization and reuse, and even learning. At the same time, many relevant knowledge representation formalisms do in general not have Craig or uniform interpolation, and computing interpolants in practice is challenging. We have a closer look at two prominent knowledge representation formalisms, description logics and logic programming, and discuss theoretical results and practical methods for computing interpolants.

</details>


### [170] [EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce](https://arxiv.org/abs/2512.08868)
*Rui Min,Zile Qiao,Ze Xu,Jiawen Zhai,Wenyu Gao,Xuanzhong Chen,Haozhen Sun,Zhen Zhang,Xinyu Wang,Hong Zhou,Wenbiao Yin,Xuan Zhou,Yong Jiang,Haicheng Liu,Liang Ding,Ling Zou,Yi R.,Fung,Yalong Li,Pengjun Xie*

Main category: cs.AI

TL;DR: EcomBench is a new benchmark for evaluating foundation agents in realistic e-commerce environments, addressing the gap in current benchmarks that focus on academic/artificial scenarios rather than practical real-world applications.


<details>
  <summary>Details</summary>
Motivation: Current agent benchmarks focus too much on academic settings and artificially designed scenarios, overlooking the complex challenges of real-world applications. E-commerce represents a highly practical domain with diverse user interactions, dynamic market conditions, and real decision-making tasks that need proper evaluation frameworks.

Method: EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems, carefully curated and annotated by human experts to ensure clarity, accuracy, and domain relevance. It covers multiple e-commerce task categories with three difficulty levels.

Result: The benchmark evaluates agents on key capabilities including deep information retrieval, multi-step reasoning, and cross-source knowledge integration, providing a rigorous and dynamic testbed for measuring practical agent capabilities in modern e-commerce.

Conclusion: EcomBench addresses the need for realistic evaluation of foundation agents by grounding assessment in real e-commerce contexts, offering a comprehensive framework to measure agent performance in practical, real-world applications rather than artificial academic scenarios.

Abstract: Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.

</details>


### [171] [Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs](https://arxiv.org/abs/2512.08923)
*Angela van Sprang,Laurens Samson,Ana Lucic,Erman Acar,Sennay Ghebreab,Yuki M. Asano*

Main category: cs.AI

TL;DR: REST and REST+ benchmarks evaluate cross-modal inconsistency in MLLMs by testing reasoning consistency across image, text, and mixed modalities with identical semantic content.


<details>
  <summary>Details</summary>
Motivation: MLLMs are trained to represent vision and language in the same embedding space, but they cannot perform the same tasks consistently across different modalities. There's a need for systematic evaluation of cross-modal inconsistency in these models.

Method: Created REST and REST+ benchmarks containing samples with identical semantic information in three modalities (image, text, mixed). Evaluated 15 state-of-the-art MLLMs on their ability to reason consistently across these modalities. Analyzed factors like OCR accuracy, visual characteristics (text color, resolution, font), and number of vision tokens.

Result: State-of-the-art MLLMs show significant cross-modal inconsistency. Neither rendering text as images nor images as text solves the problem. Visual characteristics (text color and resolution) and number of vision tokens impact performance even when OCR is correct. Consistency scores correlate with modality gap between text and images.

Conclusion: MLLMs exhibit substantial cross-modal inconsistency that cannot be solved by simple modality conversion. The REST benchmarks provide systematic evaluation tools, and findings reveal mechanistic insights about modality gaps in MLLMs.

Abstract: We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [172] [Structure Theorems (and Fast Algorithms) for List Recovery of Subspace-Design Codes](https://arxiv.org/abs/2512.08017)
*Rohan Goyal,Venkatesan Guruswami*

Main category: cs.IT

TL;DR: Folded Reed-Solomon codes can be list-recovered efficiently with compact structured output despite exponential list sizes.


<details>
  <summary>Details</summary>
Motivation: Previous work showed that FRS codes require exponential list sizes for list-recovery near capacity (1-R-Îµ), suggesting polynomial-time algorithms in 1/Îµ may be impossible. The authors aim to show that despite large lists, they have structured representations enabling efficient algorithms.

Method: Extends algorithmic advances from list decoding to list-recovery, showing lists have highly structured representations. Outputs a compact description of size â^O((log â)/Îµ) containing the relevant list, running in polynomial time in 1/Îµ.

Result: Achieves significant improvement over previous compact description size (â n^{â/Îµ} from Guruswami and Wang) to â^O((log â)/Îµ). Also improves state-of-the-art algorithmic results for list-recovery task.

Conclusion: Despite theoretical limitations showing exponential list sizes for list-recovery near capacity, the lists have structured representations enabling efficient algorithms with compact output descriptions, overcoming previous computational barriers.

Abstract: List recovery of error-correcting codes has emerged as a fundamental notion with broad applications across coding theory and theoretical computer science. Folded Reed-Solomon (FRS) and univariate multiplicity codes are explicit constructions which can be efficiently list-recovered up to capacity, namely a fraction of errors approaching $1-R$ where $R$ is the code rate.
  Chen and Zhang and related works showed that folded Reed-Solomon codes and linear codes must have list sizes exponential in $1/Îµ$ for list-recovering from an error-fraction $1-R-Îµ$. These results suggest that one cannot list-recover FRS codes in time that is also polynomial in $1/Îµ$. In contrast to such limitations, we show, extending algorithmic advances of Ashvinkumar, Habib, and Srivastava for list decoding, that even if the lists in the case of list-recovery are large, they are highly structured. In particular, we can output a compact description of a set of size only $\ell^{O((\log \ell)/Îµ)}$ which contains the relevant list, while running in time only polynomial in $1/Îµ$ (the previously known compact description due to Guruswami and Wang had size $\approx n^{\ell/Îµ}$). We also improve on the state-of-the-art algorithmic results for the task of list-recovery.

</details>


### [173] [Expectations in Expectation Propagation](https://arxiv.org/abs/2512.08034)
*Zilu Zhao,Fangqing Xiao,Dirk Slock*

Main category: cs.IT

TL;DR: The paper analyzes Expectation Propagation in linear models, focusing on messages with infinite integrals (negative variances) that can block algorithmic progress, and proposes three approaches to prevent or avoid these problematic messages.


<details>
  <summary>Details</summary>
Motivation: Expectation Propagation (EP) allows messages with infinite integrals (negative variances in Gaussian case), which can impede algorithmic progress by blocking the algorithm. The paper aims to address this issue in linear models.

Method: 1) Analyze EP in linear models and relationships between corresponding beliefs; 2) Propose non-persistent and persistent approaches to prevent algorithm blockage by infinite-integral messages; 3) Develop additional approach by examining EP message relationships to avoid infinite-integral messages altogether.

Result: The paper provides theoretical analysis of EP in linear models and proposes three practical approaches: two to prevent algorithm blockage (non-persistent and persistent), and one to avoid the occurrence of infinite-integral messages.

Conclusion: The paper addresses a key limitation of EP in linear models by providing both analytical understanding and practical solutions for handling problematic messages with infinite integrals, improving algorithmic robustness and convergence.

Abstract: Expectation Propagation (EP) is a widely used message-passing algorithm that decomposes a global inference problem into multiple local ones. It approximates marginal distributions (beliefs) using intermediate functions (messages). While beliefs must be proper probability distributions that integrate to one, messages may have infinite integral values. In Gaussian-projected EP, such messages take a Gaussian form and appear as if they have "negative" variances. Although allowed within the EP framework, these negative-variance messages can impede algorithmic progress.
  In this paper, we investigate EP in linear models and analyze the relationship between the corresponding beliefs. Based on the analysis, we propose both non-persistent and persistent approaches that prevent the algorithm from being blocked by messages with infinite integral values.
  Furthermore, by examining the relationship between the EP messages in linear models, we develop an additional approach that avoids the occurrence of messages with infinite integral values.

</details>


### [174] [Adaptive Matched Filtering for Sensing With Communication Signals in Cluttered Environments](https://arxiv.org/abs/2512.08157)
*Lei Xie,Hengtao He,Yifeng Xiong,Fan Liu,Shi Jin*

Main category: cs.IT

TL;DR: The paper proposes shifting from instantaneous to average SCNR optimization for adaptive matched filtering in cluttered environments, using RMT for asymptotic analysis, showing PSK outperforms QAM/Gaussian and OFDM beats SC/AFDM, and developing two pilot design schemes with dedicated optimization algorithms.


<details>
  <summary>Details</summary>
Motivation: Direct optimization of instantaneous SCNR in cluttered environments with superimposed signals is impractical due to prohibitive computational burdens and signaling overhead, requiring a shift to statistical metrics.

Method: Proposes shifting optimization objective from instantaneous to average SCNR, uses random matrix theory for asymptotic approximation, develops two pilot design schemes (DPD and DPI), and creates dedicated optimization algorithms (fractional optimization with KKT for DPD, manifold optimization for DPI).

Result: Theoretical analysis shows PSK achieves superior average SCNR compared to QAM and Gaussian constellations, and OFDM outperforms SC and AFDM. Simulation results validate theoretical accuracy and demonstrate effectiveness of proposed methods.

Conclusion: Optimizing average SCNR rather than instantaneous metrics provides practical advantages, with PSK and OFDM offering performance benefits, and the proposed DPD/DPI schemes provide flexible trade-offs between sensing performance and implementation complexity.

Abstract: This paper investigates the performance of the adaptive matched filtering (AMF) in cluttered environments, particularly when operating with superimposed signals. Since the instantaneous signal-to-clutter-plus-noise ratio (SCNR) is a random variable dependent on the data payload, using it directly as a design objective poses severe practical challenges, such as prohibitive computational burdens and signaling overhead. To address this, we propose shifting the optimization objective from an instantaneous to a statistical metric, which focuses on maximizing the average SCNR over all possible payloads. Due to its analytical intractability, we leverage tools from random matrix theory (RMT) to derive an asymptotic approximation for the average SCNR, which remains accurate even in moderate-dimensional regimes. A key finding from our theoretical analysis is that, for a fixed modulation basis, the PSK achieves a superior average SCNR compared to QAM and the pure Gaussian constellation. Furthermore, for any given constellation, the OFDM achieves a higher average SCNR than SC and AFDM. Then, we propose two pilot design schemes to enhance system performance: a Data-Payload-Dependent (DPD) scheme and a Data-Payload-Independent (DPI) scheme. The DPD approach maximizes the instantaneous SCNR for each transmission. Conversely, the DPI scheme optimizes the average SCNR, offering a flexible trade-off between sensing performance and implementation complexity. Then, we develop two dedicated optimization algorithms for DPD and DPI schemes. In particular, for the DPD problem, we employ fractional optimization and the KKT conditions to derive a closed-form solution. For the DPI problem, we adopt a manifold optimization approach to handle the inherent rank-one constraint efficiently. Simulation results validate the accuracy of our theoretical analysis and demonstrate the effectiveness of the proposed methods.

</details>


### [175] [On the Fundamental Tradeoff of Joint Communication and QCD: The Monostatic Case](https://arxiv.org/abs/2512.08332)
*Sung Hoon Lim,Daewon Seo*

Main category: cs.IT

TL;DR: Proposes JCCS framework for ISAC systems to optimize communication rate vs. change detection delay tradeoff using feedback and adaptive coding.


<details>
  <summary>Details</summary>
Motivation: There's a fundamental tradeoff between communication performance and quickest change detection in integrated sensing and communication systems that needs systematic analysis and optimization.

Method: Introduces Joint Communication and quickest Change subblock coding Strategy (JCCS) using feedback to adapt coding based on real-time state estimation, characterized by state-dependent mutual information and KL divergence.

Result: Characterizes achievable rate-delay region, provides partial converse showing asymptotic optimality of detection algorithm, and analyzes binary and MIMO Gaussian channels for practical insights.

Conclusion: JCCS framework effectively addresses communication-detection tradeoff in ISAC systems, with theoretical optimality and practical applicability demonstrated through channel analyses.

Abstract: This paper investigates the fundamental tradeoff between communication and quickest change detection (QCD) in integrated sensing and communication (ISAC) systems under a monostatic setup. We introduce a novel Joint Communication and quickest Change subblock coding Strategy (JCCS) that leverages feedback to adapt coding dynamically based on real-time state estimation. The achievable rate-delay region is characterized using state-dependent mutual information and KL divergence, providing a comprehensive framework for analyzing the interplay between communication performance and detection delay. Moreover, we provide a partial converse demonstrating the asymptotic optimality of the proposed detection algorithm within the JCCS framework. To illustrate the practical implications, we analyze binary and MIMO Gaussian channels, revealing insights into achieving optimal tradeoffs in ISAC system design.

</details>


### [176] [On Discrete Ambiguity Functions of Random Communication Waveforms](https://arxiv.org/abs/2512.08352)
*Ying Zhang,Fan Liu,Yifeng Xiong,Weijie Yuan,Shuangyang Li,Le Zheng,Tony Xiao Han,Christos Masouros,Shi Jin*

Main category: cs.IT

TL;DR: This paper analyzes discrete ambiguity functions of random communication waveforms for ISAC applications, deriving closed-form expressions for sidelobe levels and revealing fundamental limitations and optimality conditions for different modulation schemes.


<details>
  <summary>Details</summary>
Motivation: To characterize the delay-Doppler sensing performance of random communication waveforms in integrated sensing and communications (ISAC) applications, providing fundamental understanding of ambiguity functions for orthogonal modulation schemes.

Method: Developed a unified analytical framework for two types of discrete ambiguity functions (DP-AF and FST-AF), using matrix representation based on finite Weyl-Heisenberg group and analyzing expectations of squared AFs to derive closed-form expressions for sidelobe metrics.

Result: Derived exact closed-form expressions for expected sidelobe level (ESL) and expected integrated sidelobe level (EISL); proved normalized EISL is identical for all orthogonal waveforms under DP-AF; discovered no-go theorem showing no waveform can achieve minimum ESL over compact 2D regions; revealed constellation-dependent optimality: OFDM optimal for sub-Gaussian, OTFS optimal for super-Gaussian constellations.

Conclusion: The paper provides fundamental characterization of discrete ambiguity functions for ISAC waveforms, revealing structural limitations and optimality conditions that guide waveform selection based on constellation statistics, with implications for future ISAC system design.

Abstract: This paper provides a fundamental characterization of the discrete ambiguity functions (AFs) of random communication waveforms under arbitrary orthonormal modulation with random constellation symbols, which serve as a key metric for evaluating the delay-Doppler sensing performance in future ISAC applications. A unified analytical framework is developed for two types of AFs, namely the discrete periodic AF (DP-AF) and the fast-slow time AF (FST-AF), where the latter may be seen as a small-Doppler approximation of the DP-AF. By analyzing the expectation of squared AFs, we derive exact closed-form expressions for both the expected sidelobe level (ESL) and the expected integrated sidelobe level (EISL) under the DP-AF and FST-AF formulations. For the DP-AF, we prove that the normalized EISL is identical for all orthogonal waveforms. To gain structural insights, we introduce a matrix representation based on the finite Weyl-Heisenberg (WH) group, where each delay-Doppler shift corresponds to a WH operator acting on the ISAC signal. This WH-group viewpoint yields sharp geometric constraints on the lowest sidelobes: The minimum ESL can only occur along a one-dimensional cut or over a set of widely dispersed delay-Doppler bins. Consequently, no waveform can attain the minimum ESL over any compact two-dimensional region, leading to a no-optimality (no-go) result under the DP-AF framework. For the FST-AF, the closed-form ESL and EISL expressions reveal a constellation-dependent regime governed by its kurtosis: The OFDM modulation achieves the minimum ESL for sub-Gaussian constellations, whereas the OTFS waveform becomes optimal for super-Gaussian constellations. Finally, four representative waveforms, namely, SC, OFDM, OTFS, and AFDM, are examined under both frameworks, and all theoretical results are verified through numerical examples.

</details>


### [177] [Skew polynomial representations of matrix algebras and applications to coding theory](https://arxiv.org/abs/2512.08602)
*Alessandro Neri,Paolo Santonastaso*

Main category: cs.IT

TL;DR: The paper extends skew polynomial representations of matrix algebras to construct new families of maximum sum-rank distance (MSRD) codes over various algebraic structures, generalizing existing optimal code constructions.


<details>
  <summary>Details</summary>
Motivation: To develop a unified framework for constructing optimal codes in sum-rank, rank, and Hamming metrics by extending skew polynomial representations of matrix algebras, enabling generalizations of known code families.

Method: Extends existing skew polynomial representations of matrix algebras (direct sums of matrix spaces over division rings). Uses weight functions on associated skew polynomials defined through degrees and greatest common right divisors with the defining polynomial. Exploits this representation to construct new MSRD code families.

Result: New families of maximum sum-rank distance (MSRD) codes over finite/infinite fields and division rings. Generalizes many known MSRD, optimal rank-metric, and Hamming-metric code constructions. For finite fields, obtains new MDS codes linear over subfields with lengths close to field size.

Conclusion: The extended skew polynomial representation provides a powerful unified framework for constructing optimal codes across multiple metrics, yielding new code families that generalize and extend existing constructions while achieving maximum sum-rank distance properties.

Abstract: We extend the existing skew polynomial representations of matrix algebras which are direct sum of matrix spaces over division rings. In this representation, the sum-rank distance between two tuples of matrices is captured by a weight function on their associated skew polynomials, defined through degrees and greatest common right divisors with the polynomial that defines the representation. We exploit this representation to construct new families of maximum sum-rank distance (MSRD) codes over finite and infinite fields, and over division rings. These constructions generalize many of the known existing constructions of MSRD codes as well as of optimal codes in the rank and in the Hamming metric. As a byproduct, in the case of finite fields we obtain new families of MDS codes which are linear over a subfield and whose length is close to the field size.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [178] [ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models](https://arxiv.org/abs/2512.07843)
*Long Lian,Sida Wang,Felix Juefei-Xu,Tsu-Jui Fu,Xiuyu Li,Adam Yala,Trevor Darrell,Alane Suhr,Yuandong Tian,Xi Victoria Lin*

Main category: cs.LG

TL;DR: ThreadWeaver is a framework for adaptive parallel reasoning in LLMs that achieves accuracy comparable to sequential CoT baselines while reducing inference latency through parallel reasoning threads.


<details>
  <summary>Details</summary>
Motivation: Sequential decoding in LLMs causes substantial latency on complex reasoning tasks, while existing parallel reasoning methods either have limited supervised behavior cloning or suffer accuracy drops compared to sequential CoT baselines, often requiring customized inference engines.

Method: Three key innovations: 1) Two-stage parallel trajectory generator for high-quality CoT data with parallel annotations; 2) Trie-based training-inference co-design enabling parallel reasoning on standard autoregressive engines; 3) Parallelization-aware reinforcement learning to balance accuracy with effective parallelization.

Result: On six mathematical reasoning benchmarks, ThreadWeaver trained on Qwen3-8B achieves 71.9% average accuracy (79.9% on AIME24) comparable to state-of-the-art sequential models, with up to 1.53x average token latency speedup.

Conclusion: ThreadWeaver establishes a new Pareto frontier between accuracy and efficiency, enabling parallel reasoning with standard inference engines while maintaining accuracy comparable to sequential baselines.

Abstract: Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.

</details>


### [179] [Geometric-Stochastic Multimodal Deep Learning for Predictive Modeling of SUDEP and Stroke Vulnerability](https://arxiv.org/abs/2512.08257)
*Preksha Girish,Rachana Mysore,Mahanthesha U,Shrey Kumar,Misbah Fatimah Annigeri,Tanish Jain*

Main category: cs.LG

TL;DR: A unified geometric-stochastic multimodal deep learning framework integrating EEG, ECG, respiration, SpO2, EMG, and fMRI signals to model SUDEP and stroke vulnerability, combining Riemannian manifold embeddings, Lie-group invariant features, fractional stochastic dynamics, and cross-modal attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: SUDEP and acute ischemic stroke are life-threatening conditions involving complex interactions across cortical, brainstem, and autonomic systems, requiring advanced multimodal analysis for early detection and risk stratification.

Method: Geometric-stochastic multimodal deep learning framework integrating EEG, ECG, respiration, SpO2, EMG, and fMRI signals; combines Riemannian manifold embeddings, Lie-group invariant feature representations, fractional stochastic dynamics, Hamiltonian energy-flow modeling, cross-modal attention mechanisms, and fractional epidemic diffusion over structural brain graphs for stroke propagation modeling.

Result: Experiments on MULTI-CLARID dataset demonstrate improved predictive accuracy and interpretable biomarkers derived from manifold curvature, fractional memory indices, attention entropy, and diffusion centrality.

Conclusion: The framework provides a mathematically principled foundation for early detection, risk stratification, and interpretable multimodal modeling in neural-autonomic disorders.

Abstract: Sudden Unexpected Death in Epilepsy (SUDEP) and acute ischemic stroke are life-threatening conditions involving complex interactions across cortical, brainstem, and autonomic systems. We present a unified geometric-stochastic multimodal deep learning framework that integrates EEG, ECG, respiration, SpO2, EMG, and fMRI signals to model SUDEP and stroke vulnerability. The approach combines Riemannian manifold embeddings, Lie-group invariant feature representations, fractional stochastic dynamics, Hamiltonian energy-flow modeling, and cross-modal attention mechanisms. Stroke propagation is modeled using fractional epidemic diffusion over structural brain graphs. Experiments on the MULTI-CLARID dataset demonstrate improved predictive accuracy and interpretable biomarkers derived from manifold curvature, fractional memory indices, attention entropy, and diffusion centrality. The proposed framework provides a mathematically principled foundation for early detection, risk stratification, and interpretable multimodal modeling in neural-autonomic disorders.

</details>


### [180] [Space Alignment Matters: The Missing Piece for Inducing Neural Collapse in Long-Tailed Learning](https://arxiv.org/abs/2512.07844)
*Jinping Wang,Zhiqiang Gao,Zhiwu Xie*

Main category: cs.LG

TL;DR: The paper addresses feature-classifier misalignment in long-tailed learning, proposing plug-and-play alignment strategies that improve existing methods without architectural changes.


<details>
  <summary>Details</summary>
Motivation: In long-tailed regimes, severe sample imbalance prevents Neural Collapse (NC) and the emergence of simplex ETF geometry, leading to poor generalization. Current methods focus on recovering ETF geometry but overlook the critical misalignment between feature and classifier weight spaces.

Method: The authors theoretically quantify the harm of feature-classifier misalignment through optimal error exponent analysis, then propose three explicit alignment strategies that can be integrated as plug-and-play modules into existing long-tail methods without requiring architectural changes.

Result: Extensive experiments on CIFAR-10-LT, CIFAR-100-LT, and ImageNet-LT datasets show that the proposed alignment strategies consistently boost examined baselines and achieve state-of-the-art performances.

Conclusion: Addressing the misalignment between feature and classifier weight spaces is crucial for improving long-tailed learning, and the proposed plug-and-play alignment strategies provide an effective solution that enhances existing methods without architectural modifications.

Abstract: Recent studies on Neural Collapse (NC) reveal that, under class-balanced conditions, the class feature means and classifier weights spontaneously align into a simplex equiangular tight frame (ETF). In long-tailed regimes, however, severe sample imbalance tends to prevent the emergence of the NC phenomenon, resulting in poor generalization performance. Current efforts predominantly seek to recover the ETF geometry by imposing constraints on features or classifier weights, yet overlook a critical problem: There is a pronounced misalignment between the feature and the classifier weight spaces. In this paper, we theoretically quantify the harm of such misalignment through an optimal error exponent analysis. Built on this insight, we propose three explicit alignment strategies that plug-and-play into existing long-tail methods without architectural change. Extensive experiments on the CIFAR-10-LT, CIFAR-100-LT, and ImageNet-LT datasets consistently boost examined baselines and achieve the state-of-the-art performances.

</details>


### [181] [CarBench: A Comprehensive Benchmark for Neural Surrogates on High-Fidelity 3D Car Aerodynamics](https://arxiv.org/abs/2512.07847)
*Mohamed Elrefaie,Dule Shu,Matt Klenk,Faez Ahmed*

Main category: cs.LG

TL;DR: CarBench is the first comprehensive benchmark for large-scale 3D car aerodynamics, evaluating 11 state-of-the-art models on the largest public automotive aerodynamics dataset (DrivAerNet++ with 8,000+ simulations).


<details>
  <summary>Details</summary>
Motivation: While benchmarks drive progress in computer vision and NLP, there's no standardized benchmark for large-scale numerical simulations in engineering design, despite growing availability of CFD datasets.

Method: Created CarBench to evaluate 11 architectures including neural operators (Fourier Neural Operator), geometric deep learning (PointNet, RegDGCNN, etc.), transformer-based neural solvers (Transolver, Transolver++, AB-UPT), and implicit field networks (TripNet). Performs both standard interpolation tasks and cross-category experiments where models trained on one car archetype are tested on unseen categories.

Result: Provides comprehensive analysis covering predictive accuracy, physical consistency, computational efficiency, and statistical uncertainty. The benchmark framework is open-sourced with training pipelines, uncertainty estimation routines based on bootstrap resampling, and pretrained model weights.

Conclusion: Establishes the first reproducible foundation for large-scale learning from high-fidelity CFD simulations, accelerating progress in data-driven engineering design by providing standardized evaluation for aerodynamic prediction models.

Abstract: Benchmarking has been the cornerstone of progress in computer vision, natural language processing, and the broader deep learning domain, driving algorithmic innovation through standardized datasets and reproducible evaluation protocols. The growing availability of large-scale Computational Fluid Dynamics (CFD) datasets has opened new opportunities for applying machine learning to aerodynamic and engineering design. Yet, despite this progress, there exists no standardized benchmark for large-scale numerical simulations in engineering design. In this work, we introduce CarBench, the first comprehensive benchmark dedicated to large-scale 3D car aerodynamics, performing a large-scale evaluation of state-of-the-art models on DrivAerNet++, the largest public dataset for automotive aerodynamics, containing over 8,000 high-fidelity car simulations. We assess eleven architectures spanning neural operator methods (e.g., Fourier Neural Operator), geometric deep learning (PointNet, RegDGCNN, PointMAE, PointTransformer), transformer-based neural solvers (Transolver, Transolver++, AB-UPT), and implicit field networks (TripNet). Beyond standard interpolation tasks, we perform cross-category experiments in which transformer-based solvers trained on a single car archetype are evaluated on unseen categories. Our analysis covers predictive accuracy, physical consistency, computational efficiency, and statistical uncertainty. To accelerate progress in data-driven engineering, we open-source the benchmark framework, including training pipelines, uncertainty estimation routines based on bootstrap resampling, and pretrained model weights, establishing the first reproducible foundation for large-scale learning from high-fidelity CFD simulations, available at https://github.com/Mohamedelrefaie/CarBench.

</details>


### [182] [RaX-Crash: A Resource Efficient and Explainable Small Model Pipeline with an Application to City Scale Injury Severity Prediction](https://arxiv.org/abs/2512.07848)
*Di Zhu,Chen Xie,Ziwei Wang,Haoyun Zhang*

Main category: cs.LG

TL;DR: RaX-Crash is an explainable small model pipeline for predicting injury severity in NYC motor vehicle collisions, using tree-based ensembles (XGBoost/Random Forest) that outperform small language models, with SHAP analysis revealing key risk factors.


<details>
  <summary>Details</summary>
Motivation: NYC experiences over 100k motor vehicle collisions annually, creating substantial injury and public health burden, necessitating efficient and interpretable prediction models for injury severity analysis at city scale.

Method: Integrates three linked tables with tens of millions of records, builds unified feature schema in partitioned storage, trains compact tree-based ensembles (Random Forest and XGBoost) on engineered tabular features, compares against small language models (SLMs) prompted with textual summaries, uses SHAP for attribution analysis.

Result: XGBoost achieves 0.7828 accuracy, Random Forest 0.7794, both clearly outperforming SLMs (0.594 and 0.496); class weighting improves fatal recall with modest accuracy trade-offs; SHAP highlights human vulnerability factors, timing, and location as dominant severity drivers.

Conclusion: Interpretable small model ensembles remain strong baselines for city-scale injury analytics, while hybrid pipelines pairing tabular predictors with SLM-generated narratives improve communication without sacrificing scalability.

Abstract: New York City reports over one hundred thousand motor vehicle collisions each year, creating substantial injury and public health burden. We present RaX-Crash, a resource efficient and explainable small model pipeline for structured injury severity prediction on the official NYC Motor Vehicle Collisions dataset. RaX-Crash integrates three linked tables with tens of millions of records, builds a unified feature schema in partitioned storage, and trains compact tree based ensembles (Random Forest and XGBoost) on engineered tabular features, which are compared against locally deployed small language models (SLMs) prompted with textual summaries. On a temporally held out test set, XGBoost and Random Forest achieve accuracies of 0.7828 and 0.7794, clearly outperforming SLMs (0.594 and 0.496); class imbalance analysis shows that simple class weighting improves fatal recall with modest accuracy trade offs, and SHAP attribution highlights human vulnerability factors, timing, and location as dominant drivers of predicted severity. Overall, RaX-Crash indicates that interpretable small model ensembles remain strong baselines for city scale injury analytics, while hybrid pipelines that pair tabular predictors with SLM generated narratives improve communication without sacrificing scalability.

</details>


### [183] [SABER: Small Actions, Big Errors - Safeguarding Mutating Steps in LLM Agents](https://arxiv.org/abs/2512.07850)
*Alejandro Cuadron,Pengfei Yu,Yang Liu,Arpit Gupta*

Main category: cs.LG

TL;DR: LLM agents struggle with long-horizon tool-using tasks, with mutating actions being critical failure points. The paper introduces a safeguard system (CM) that improves performance through mutation-gated verification and targeted reflection.


<details>
  <summary>Details</summary>
Motivation: To understand why LLM agents fail on long-horizon, tool-using tasks and identify which actions contribute most to failure, particularly distinguishing between mutating (environment-changing) and non-mutating actions.

Method: Analyzed execution traces on Ï-Bench and SWE-Bench Verified, decomposed trajectories into mutating vs. non-mutating steps, formalized decisive deviations, and used logistic regression. Introduced CM - a model-agnostic safeguard with mutation-gated verification, targeted reflection before mutating steps, and block-based context cleaning.

Result: Each additional deviation in mutating actions reduces success odds by up to 92-96%, while non-mutating deviations have little effect. CM improved performance significantly: Qwen3-Thinking +28% on Airline, +11% on Retail, +7% on SWE-Bench Verified; Claude +9%/+7%. Also identified ceiling effects in Ï-Bench and released Ï-Bench Verified.

Conclusion: Mutating actions are critical failure points in LLM agents. Action-level analysis, targeted safeguards like CM, and reliable evaluations are essential for robust multi-turn agents. The paper provides both diagnostic insights and practical solutions.

Abstract: Despite rapid progress in LLM agents, performance on long-horizon, tool-using tasks remains fragile. To better understand this fragility, we ask a simple question: \emph{do all actions contribute equally to failure?} Analyzing execution traces on $Ï$-Bench (Airline/Retail) and SWE-Bench Verified, we decompose trajectories into \emph{mutating} (environment-changing) vs.\ non-mutating steps and formalize \emph{decisive deviations}, earliest action, level divergences that flip success to failure. A logistic regression reveals that each additional deviation in a mutating action reduces the odds of success by upto $92\%$ on Airline and upto $96\%$ on Retail for SoTA models. In contrast, deviations in non-mutating actions have little to no effect. Errors also grow with context length as agents drift from role and act on stale constraints. Motivated by these observations, we introduce \cm{}, a model-agnostic, gradient-free, test-time safeguard that (i) adds mutation-gated verification, (ii) injects \emph{Targeted Reflection} before mutating steps, and (iii) performs block-based context cleaning. \cm{} delivers consistent gains, e.g., Qwen3-Thinking: +28\% \emph{relative} on Airline, +11\% on Retail, and +7\% on SWE-Bench Verified; Claude: +9\%/+7\%. We further identify ceiling effects in $Ï$-Bench, where annotation errors and underspecified tasks artificially cap model performance. To address this, we release $Ï$-Bench Verified, which restores benchmark headroom through targeted revisions. Our results argue for action-level analysis, targeted safeguards, and reliable evaluations as prerequisites for robust multi-turn agents.

</details>


### [184] [GPU Memory Prediction for Multimodal Model Training](https://arxiv.org/abs/2512.07853)
*Jinwoo Jeong,Minchul Kang,Younghun Go,Changyong Shin,Hyunho Lee,Junho Yoon,Gyeongsik Yang,Chuck Yoo*

Main category: cs.LG

TL;DR: A framework for predicting GPU memory usage in multimodal models to prevent out-of-memory errors during training.


<details>
  <summary>Details</summary>
Motivation: As agentic AI systems use larger multimodal models, GPU memory requirements often exceed capacity causing out-of-memory errors that waste computational resources. Existing methods only work for unimodal architectures and fail to generalize to multimodal models.

Method: The framework decomposes multimodal models into constituent layers and applies factorization to estimate memory usage of each layer by analyzing model architecture and training behavior.

Result: The framework achieves high prediction accuracy with ~8.7% average MAPE (Mean Absolute Percentage Error).

Conclusion: The proposed framework successfully addresses the limitation of previous unimodal-focused methods by providing accurate GPU memory usage prediction for multimodal models, which is essential for preventing OoM errors in agentic AI systems.

Abstract: As deep learning models in agentic AI systems grow in scale and complexity, GPU memory requirements increase and often exceed the available GPU memory capacity, so that out-of-memory (OoM) errors occur. It is well known that OoM interrupts the whole training itself and wastes substantial computational resources. Therefore, to prevent OoM, accurate prediction of GPU memory usage is essential. However, previous studies focus only on unimodal architectures and fail to generalize to multimodal models, even though the multimodal models are a common choice in agentic AI systems. To address this limitation, we propose a framework that predicts the peak GPU memory usage by analyzing the model architecture and training behavior of multimodal models. Specifically, the framework decomposes the multimodal model into its constituent layers and applies factorization to estimate the memory usage of each layer. Our evaluation shows that our framework achieves high prediction accuracy of ~8.7% average MAPE.

</details>


### [185] [HSTMixer: A Hierarchical MLP-Mixer for Large-Scale Traffic Forecasting](https://arxiv.org/abs/2512.07854)
*Yongyao Wang,Jingyuan Wang,Xie Yu,Jiahao Ji,Chao Li*

Main category: cs.LG

TL;DR: HSTMixer: A hierarchical MLP-based framework for efficient large-scale traffic forecasting with adaptive region mixing and multi-resolution feature extraction.


<details>
  <summary>Details</summary>
Motivation: Existing traffic forecasting models have quadratic computational complexity, making them impractical for large-scale real-world traffic networks that better reflect real-world complexity.

Method: Proposes HSTMixer with all-MLP architecture, hierarchical spatiotemporal mixing blocks for multi-resolution features (bottom-up aggregation + top-down propagation), and adaptive region mixer that generates transformation matrices based on regional semantics.

Result: Extensive experiments on four large-scale real-world datasets show state-of-the-art performance with competitive computational efficiency.

Conclusion: HSTMixer provides an efficient and effective solution for large-scale traffic forecasting by addressing computational complexity limitations while maintaining high performance.

Abstract: Traffic forecasting task is significant to modern urban management. Recently, there is growing attention on large-scale forecasting, as it better reflects the complexity of real-world traffic networks. However, existing models often exhibit quadratic computational complexity, making them impractical for large-scale real-world scenarios. In this paper, we propose a novel framework, Hierarchical Spatio-Temporal Mixer (HSTMixer), which leverages an all-MLP architecture for efficient and effective large-scale traffic forecasting. HSTMixer employs a hierarchical spatiotemporal mixing block to extract multi-resolution features through bottom-up aggregation and top-down propagation. Furthermore, an adaptive region mixer generates transformation matrices based on regional semantics, enabling our model to dynamically capture evolving spatiotemporal patterns for different regions. Extensive experiments conducted on four large-scale real-world datasets demonstrate that the proposed method not only achieves state-of-the-art performance but also exhibits competitive computational efficiency.

</details>


### [186] [LAPA: Log-Domain Prediction-Driven Dynamic Sparsity Accelerator for Transformer Model](https://arxiv.org/abs/2512.07855)
*Huizheng Wang,Hongbin Wang,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.LG

TL;DR: LAPA is a log-domain attention prediction algorithm-architecture co-design for cross-stage sparse Transformer acceleration that achieves 2.79-3.52x higher energy efficiency than SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Transformers have computational bottlenecks that vary dynamically across stages as input sequences change, requiring cross-stage sparse acceleration. Existing sparse Transformer approaches are single-stage based and their sparsity prediction mechanisms cause significant power overhead when applied across multiple stages.

Method: Proposes LAPA with: 1) asymmetric leading one computing (ALOC) to eliminate expensive multiplications, 2) mixed-precision multi-round shifting accumulation (MRSA) to mitigate accumulation overhead, 3) data-feature dependent filter (DDF) strategy working with MRSA, and 4) an elaborate accelerator to translate theoretical enhancement into practical hardware improvement.

Result: LAPA achieves 3.52x, 3.24x and 2.79x higher energy efficiency than state-of-the-art works Spatten, Sanger and FACT, respectively.

Conclusion: The proposed LAPA co-design effectively addresses cross-stage sparse Transformer acceleration with significant energy efficiency improvements over existing approaches.

Abstract: Attention-based Transformers have revolutionized natural language processing (NLP) and shown strong performance in computer vision (CV) tasks. However, as the input sequence varies, the computational bottlenecks in Transformer models exhibit dynamic behavior across stages, which calls for a cross-stage sparse acceleration strategy. Unfortunately, most existing sparse Transformer approaches are single-stage based, and their sparsity prediction mechanisms lead to significant power overhead when applied across multiple stages. To this end, this paper proposes a log-domain attention prediction algorithm-architecture co-design, named LAPA. First, an asymmetric leading one computing (ALOC) scheme is designed to eliminate expensive multiplications. Next, a mixed-precision multi-round shifting accumulation (MRSA) mechanism is further proposed to mitigate the accumulation overhead. A data-feature dependent filter (DDF) strategy is designed to work in concert with the MRSA process. Finally, an elaborate accelerator is designed to translate the theoretical enhancement into practical hardware improvement. Experimental results show that LAPA achieves 3.52x, 3.24x and 2.79x higher energy efficiency than the state-of-the-art (SOTA) works Spatten, Sanger and FACT, respectively.

</details>


### [187] [Medical Test-free Disease Detection Based on Big Data](https://arxiv.org/abs/2512.07856)
*Haokun Zhao,Yingzhe Bai,Qingyang Xu,Lixin Zhou,Jianxin Chen,Jicong Fan*

Main category: cs.LG

TL;DR: CLDD is a graph-based deep learning model that detects multiple diseases simultaneously by leveraging disease associations and patient similarities, reducing reliance on expensive medical tests.


<details>
  <summary>Details</summary>
Motivation: Traditional disease detection requires extensive medical testing which is costly and impractical for screening hundreds/thousands of diseases per patient. There's a need for efficient, comprehensive disease detection methods that minimize testing requirements.

Method: Proposes Collaborative Learning for Disease Detection (CLDD), a graph-based deep learning model that formulates disease detection as collaborative learning. It exploits disease associations and patient similarities adaptively, integrating patient-disease interactions and demographic features from EHRs.

Result: On MIMIC-IV dataset (61,191 patients, 2,000 diseases), CLDD outperforms baselines with 6.33% recall improvement and 7.63% precision improvement. Case studies show it successfully recovers masked diseases in top predictions, demonstrating interpretability and reliability.

Conclusion: CLDD enables large-scale disease screening with minimal testing, reducing diagnostic costs and improving healthcare accessibility, promising for social health security applications.

Abstract: Accurate disease detection is of paramount importance for effective medical treatment and patient care. However, the process of disease detection is often associated with extensive medical testing and considerable costs, making it impractical to perform all possible medical tests on a patient to diagnose or predict hundreds or thousands of diseases. In this work, we propose Collaborative Learning for Disease Detection (CLDD), a novel graph-based deep learning model that formulates disease detection as a collaborative learning task by exploiting associations among diseases and similarities among patients adaptively. CLDD integrates patient-disease interactions and demographic features from electronic health records to detect hundreds or thousands of diseases for every patient, with little to no reliance on the corresponding medical tests. Extensive experiments on a processed version of the MIMIC-IV dataset comprising 61,191 patients and 2,000 diseases demonstrate that CLDD consistently outperforms representative baselines across multiple metrics, achieving a 6.33\% improvement in recall and 7.63\% improvement in precision. Furthermore, case studies on individual patients illustrate that CLDD can successfully recover masked diseases within its top-ranked predictions, demonstrating both interpretability and reliability in disease prediction. By reducing diagnostic costs and improving accessibility, CLDD holds promise for large-scale disease screening and social health security.

</details>


### [188] [SA^2GFM: Enhancing Robust Graph Foundation Models with Structure-Aware Semantic Augmentation](https://arxiv.org/abs/2512.07857)
*Junhua Shi,Qingyun Sun,Haonan Yuan,Xingcheng Fu*

Main category: cs.LG

TL;DR: SA^2GFM is a robust Graph Foundation Model framework that improves domain adaptation through structure-aware semantic augmentation and information bottleneck compression.


<details>
  <summary>Details</summary>
Motivation: Current Graph Foundation Models lack robustness against domain noise, structural perturbations, and adversarial attacks, with insufficient modeling of hierarchical structural semantics crucial for generalization.

Method: 1) Encode hierarchical structural priors using entropy-based encoding trees transformed into structure-aware textual prompts; 2) Self-supervised Information Bottleneck mechanism for robust representation distillation; 3) Expert adaptive routing with mixture-of-experts and null expert design to prevent negative transfer; 4) Fine-tuning module with joint intra- and inter-community structure learning.

Result: SA^2GFM outperforms 9 state-of-the-art baselines in effectiveness and robustness against random noise and adversarial perturbations for both node and graph classification tasks.

Conclusion: The proposed SA^2GFM framework successfully addresses robustness limitations in Graph Foundation Models through structure-aware semantic augmentation and adaptive mechanisms, demonstrating superior performance in noisy and adversarial environments.

Abstract: We present Graph Foundation Models (GFMs) which have made significant progress in various tasks, but their robustness against domain noise, structural perturbations, and adversarial attacks remains underexplored. A key limitation is the insufficient modeling of hierarchical structural semantics, which are crucial for generalization. In this paper, we propose SA^2GFM, a robust GFM framework that improves domain-adaptive representations through Structure-Aware Semantic Augmentation. First, we encode hierarchical structural priors by transforming entropy-based encoding trees into structure-aware textual prompts for feature augmentation. The enhanced inputs are processed by a self-supervised Information Bottleneck mechanism that distills robust, transferable representations via structure-guided compression. To address negative transfer in cross-domain adaptation, we introduce an expert adaptive routing mechanism, combining a mixture-of-experts architecture with a null expert design. For efficient downstream adaptation, we propose a fine-tuning module that optimizes hierarchical structures through joint intra- and inter-community structure learning. Extensive experiments demonstrate that SA^2GFM outperforms 9 state-of-the-art baselines in terms of effectiveness and robustness against random noise and adversarial perturbations for node and graph classification.

</details>


### [189] [FAIM: Frequency-Aware Interactive Mamba for Time Series Classification](https://arxiv.org/abs/2512.07858)
*Da Zhang,Bingyu Li,Zhiyuan Zhao,Yanhan Zhang,Junyu Gao,Feiping Nie,Xuelong Li*

Main category: cs.LG

TL;DR: FAIM is a lightweight Frequency-Aware Interactive Mamba model for time series classification that combines frequency-domain feature extraction with efficient multi-granularity information interaction, achieving state-of-the-art performance with better accuracy-efficiency trade-off.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for time series classification often suffer from high computational costs, sensitivity to noise, and overfitting on small datasets. There's a need for lightweight, robust models that can effectively capture discriminative temporal patterns while being efficient and noise-resistant.

Method: FAIM introduces two key components: 1) Adaptive Filtering Block (AFB) that uses Fourier Transform to extract frequency-domain features with learnable adaptive thresholds for noise suppression and element-wise coupling of global/local semantic filtering; 2) Interactive Mamba Block (IMB) for efficient multi-granularity information interaction to balance fine-grained and global contextual features. The model also incorporates self-supervised pre-training for robustness.

Result: Extensive experiments on multiple benchmarks show FAIM consistently outperforms existing state-of-the-art methods, achieving superior accuracy-efficiency trade-off and demonstrating outstanding performance across various domains and high-noise scenarios.

Conclusion: FAIM provides an effective solution for time series classification by combining frequency-domain analysis with efficient information interaction, addressing key challenges of computational cost, noise sensitivity, and overfitting while achieving state-of-the-art performance.

Abstract: Time series classification (TSC) is crucial in numerous real-world applications, such as environmental monitoring, medical diagnosis, and posture recognition. TSC tasks require models to effectively capture discriminative information for accurate class identification. Although deep learning architectures excel at capturing temporal dependencies, they often suffer from high computational cost, sensitivity to noise perturbations, and susceptibility to overfitting on small-scale datasets. To address these challenges, we propose FAIM, a lightweight Frequency-Aware Interactive Mamba model. Specifically, we introduce an Adaptive Filtering Block (AFB) that leverages Fourier Transform to extract frequency-domain features from time series data. The AFB incorporates learnable adaptive thresholds to dynamically suppress noise and employs element-wise coupling of global and local semantic adaptive filtering, enabling in-depth modeling of the synergy among different frequency components. Furthermore, we design an Interactive Mamba Block (IMB) to facilitate efficient multi-granularity information interaction, balancing the extraction of fine-grained discriminative features and comprehensive global contextual information, thereby endowing FAIM with powerful and expressive representations for TSC tasks. Additionally, we incorporate a self-supervised pre-training mechanism to enhance FAIM's understanding of complex temporal patterns and improve its robustness across various domains and high-noise scenarios. Extensive experiments on multiple benchmarks demonstrate that FAIM consistently outperforms existing state-of-the-art (SOTA) methods, achieving a superior trade-off between accuracy and efficiency and exhibits outstanding performance.

</details>


### [190] [SetAD: Semi-Supervised Anomaly Learning in Contextual Sets](https://arxiv.org/abs/2512.07863)
*Jianling Gao,Chongyang Tao,Xuelian Lin,Junfeng Liu,Shuai Ma*

Main category: cs.LG

TL;DR: SetAD reframes semi-supervised anomaly detection as a set-level task using attention-based set encoding and graded learning, outperforming point/pair-based methods by capturing group-level anomaly patterns.


<details>
  <summary>Details</summary>
Motivation: Existing semi-supervised AD methods focus on individual points or simple pairs, which overlooks the contextual nature of anomalies (defined by deviation from groups) and fails to exploit combinatorial supervisory signals from sets. This limits their ability to capture high-order interactions critical for discriminative representations.

Method: Proposes SetAD framework with: 1) Attention-based set encoder trained via graded learning objective to quantify anomalousness of entire sets, 2) Context-calibrated anomaly scoring mechanism that aggregates normalized deviations from peer behavior across multiple diverse contextual sets for robust score calibration.

Result: Extensive experiments on 10 real-world datasets show SetAD significantly outperforms state-of-the-art models. Performance consistently improves with increasing set size, providing empirical support for set-based formulation.

Conclusion: SetAD successfully addresses limitations of point/pair-centric approaches by modeling group-level interactions that define anomalies, demonstrating superior performance through set-based formulation and context-calibrated scoring.

Abstract: Semi-supervised anomaly detection (AD) has shown great promise by effectively leveraging limited labeled data. However, existing methods are typically structured around scoring individual points or simple pairs. Such {point- or pair-centric} view not only overlooks the contextual nature of anomalies, which are defined by their deviation from a collective group, but also fails to exploit the rich supervisory signals that can be generated from the combinatorial composition of sets. Consequently, such models struggle to exploit the high-order interactions within the data, which are critical for learning discriminative representations. To address these limitations, we propose SetAD, a novel framework that reframes semi-supervised AD as a Set-level Anomaly Detection task. SetAD employs an attention-based set encoder trained via a graded learning objective, where the model learns to quantify the degree of anomalousness within an entire set. This approach directly models the complex group-level interactions that define anomalies. Furthermore, to enhance robustness and score calibration, we propose a context-calibrated anomaly scoring mechanism, which assesses a point's anomaly score by aggregating its normalized deviations from peer behavior across multiple, diverse contextual sets. Extensive experiments on 10 real-world datasets demonstrate that SetAD significantly outperforms state-of-the-art models. Notably, we show that our model's performance consistently improves with increasing set size, providing strong empirical support for the set-based formulation of anomaly detection.

</details>


### [191] [Pattern Recognition of Ozone-Depleting Substance Exports in Global Trade Data](https://arxiv.org/abs/2512.07864)
*Muhammad Sukri Bin Ramli*

Main category: cs.LG

TL;DR: Unsupervised ML framework detects suspicious trade patterns for environmental treaty monitoring using clustering, anomaly detection, and heuristic flagging to prioritize customs review.


<details>
  <summary>Details</summary>
Motivation: Need new methods to monitor environmental treaties like Montreal Protocol by analyzing large, complex customs datasets to detect suspicious trade activities.

Method: Combines unsupervised clustering (K-Means) to find trade archetypes, anomaly detection (Isolation Forest & IQR) for rare mega-trades and price outliers, heuristic flagging for vague descriptions, and integrates these into priority scores with SHAP for explainability.

Result: Identified 1,351 price outliers and 1,288 high-priority shipments; found high-priority commodities have different value-to-weight ratios; validated by detecting real-world mega-trade spike in 2021 correlating with US AIM Act impact.

Conclusion: Presents repeatable unsupervised learning pipeline to transform raw trade data into prioritized intelligence for regulatory groups monitoring environmental treaties.

Abstract: New methods are needed to monitor environmental treaties, like the Montreal Protocol, by reviewing large, complex customs datasets. This paper introduces a framework using unsupervised machine learning to systematically detect suspicious trade patterns and highlight activities for review. Our methodology, applied to 100,000 trade records, combines several ML techniques. Unsupervised Clustering (K-Means) discovers natural trade archetypes based on shipment value and weight. Anomaly Detection (Isolation Forest and IQR) identifies rare "mega-trades" and shipments with commercially unusual price-per-kilogram values. This is supplemented by Heuristic Flagging to find tactics like vague shipment descriptions. These layers are combined into a priority score, which successfully identified 1,351 price outliers and 1,288 high-priority shipments for customs review. A key finding is that high-priority commodities show a different and more valuable value-to-weight ratio than general goods. This was validated using Explainable AI (SHAP), which confirmed vague descriptions and high value as the most significant risk predictors. The model's sensitivity was validated by its detection of a massive spike in "mega-trades" in early 2021, correlating directly with the real-world regulatory impact of the US AIM Act. This work presents a repeatable unsupervised learning pipeline to turn raw trade data into prioritized, usable intelligence for regulatory groups.

</details>


### [192] [Using Text-Based Life Trajectories from Swedish Register Data to Predict Residential Mobility with Pretrained Transformers](https://arxiv.org/abs/2512.07865)
*Philipp Stark,Alexandros Sopasakis,Ola Hall,Markus Grillitsch*

Main category: cs.LG

TL;DR: Swedish register data converted to textual life trajectories to handle categorical variables and coding inconsistencies, predicting residential mobility using NLP models.


<details>
  <summary>Details</summary>
Motivation: Address two challenges in data analysis: high cardinality of categorical variables and inconsistencies in coding schemes over time in longitudinal register data.

Method: Transform Swedish register data (6.9M individuals, 2001-2013) into textual life trajectories combining demographic info with annual changes in residence, work, education, income, and family. Use NLP architectures (LSTM, DistilBERT, BERT, Qwen) to predict residential mobility (2013-2017).

Result: Sequential and transformer-based models capture temporal and semantic structure better than baselines. Textualized register data preserves meaningful information about individual pathways and supports complex, scalable modeling.

Conclusion: Combining semantically rich register data with modern language models can substantially advance longitudinal analysis in social sciences, providing a rigorous testbed for developing sequence-modeling approaches.

Abstract: We transform large-scale Swedish register data into textual life trajectories to address two long-standing challenges in data analysis: high cardinality of categorical variables and inconsistencies in coding schemes over time. Leveraging this uniquely comprehensive population register, we convert register data from 6.9 million individuals (2001-2013) into semantically rich texts and predict individuals' residential mobility in later years (2013-2017). These life trajectories combine demographic information with annual changes in residence, work, education, income, and family circumstances, allowing us to assess how effectively such sequences support longitudinal prediction. We compare multiple NLP architectures (including LSTM, DistilBERT, BERT, and Qwen) and find that sequential and transformer-based models capture temporal and semantic structure more effectively than baseline models. The results show that textualized register data preserves meaningful information about individual pathways and supports complex, scalable modeling. Because few countries maintain longitudinal microdata with comparable coverage and precision, this dataset enables analyses and methodological tests that would be difficult or impossible elsewhere, offering a rigorous testbed for developing and evaluating new sequence-modeling approaches. Overall, our findings demonstrate that combining semantically rich register data with modern language models can substantially advance longitudinal analysis in social sciences.

</details>


### [193] [Command & Control (C2) Traffic Detection Via Algorithm Generated Domain (Dga) Classification Using Deep Learning And Natural Language Processing](https://arxiv.org/abs/2512.07866)
*Maria Milena Araujo Felix*

Main category: cs.LG

TL;DR: A deep learning approach using LSTM neural networks achieves 97.2% accuracy in detecting DGA domains, outperforming traditional entropy analysis for complex malware communication patterns.


<details>
  <summary>Details</summary>
Motivation: Traditional blacklist-based defenses are ineffective against modern malware using Domain Generation Algorithms (DGA) that generate thousands of dynamic addresses daily, bypassing conventional firewalls.

Method: Collected hybrid database with 50,000 legitimate and 50,000 malicious domains, extracted lexical features, and trained a Recurrent Neural Network (LSTM) using NLP techniques.

Result: The neural network approach achieved 97.2% accuracy and reduced false positive rates in ambiguous lawful traffic scenarios, showing superiority over statistical entropy analysis for complex DGA patterns.

Conclusion: Deep learning with NLP techniques provides an effective solution for detecting sophisticated DGA domains, offering better performance than traditional methods for complex malware communication patterns.

Abstract: The sophistication of modern malware, specifically regarding communication with Command and Control (C2) servers, has rendered static blacklist-based defenses obsolete. The use of Domain Generation Algorithms (DGA) allows attackers to generate thousands of dynamic addresses daily, hindering blocking by traditional firewalls. This paper aims to propose and evaluate a method for detecting DGA domains using Deep Learning and Natural Language Processing (NLP) techniques. The methodology consisted of collecting a hybrid database containing 50,000 legitimate and 50,000 malicious domains, followed by the extraction of lexical features and the training of a Recurrent Neural Network (LSTM). Results demonstrated that while statistical entropy analysis is effective for simple DGAs, the Neural Network approach presents superiority in detecting complex patterns, reaching 97.2% accuracy and reducing the false positive rate in ambiguous lawful traffic scenarios.

</details>


### [194] [Bayesian Optimization for Function-Valued Responses under Min-Max Criteria](https://arxiv.org/abs/2512.07868)
*Pouya Ahadi,Reza Marzban,Ali Adibi,Kamran Paynabar*

Main category: cs.LG

TL;DR: MM-FBO is a Bayesian optimization framework for functional responses that minimizes maximum error across the functional domain using functional PCA and Gaussian processes with integrated uncertainty acquisition.


<details>
  <summary>Details</summary>
Motivation: Bayesian optimization typically handles scalar responses, but many scientific/engineering problems have functional responses (varying over time/wavelength). Existing methods minimize integrated error, ignoring worst-case deviations which can be critical in applications.

Method: Uses functional principal component analysis to represent functional responses, constructs Gaussian process surrogates for principal component scores, and introduces an integrated uncertainty acquisition function that balances exploitation of worst-case expected error with exploration across the functional domain.

Result: Provides theoretical guarantees: discretization bound for worst-case objective and consistency result showing acquisition converges to true min-max objective as surrogate becomes accurate. Outperforms existing baselines in synthetic benchmarks and physics case studies (electromagnetic scattering, vapor phase infiltration).

Conclusion: MM-FBO effectively addresses functional response optimization by explicitly minimizing maximum error, demonstrating superior performance and highlighting the importance of modeling functional uncertainty in Bayesian optimization.

Abstract: Bayesian optimization is widely used for optimizing expensive black box functions, but most existing approaches focus on scalar responses. In many scientific and engineering settings the response is functional, varying smoothly over an index such as time or wavelength, which makes classical formulations inadequate. Existing methods often minimize integrated error, which captures average performance but neglects worst case deviations. To address this limitation we propose min-max Functional Bayesian Optimization (MM-FBO), a framework that directly minimizes the maximum error across the functional domain. Functional responses are represented using functional principal component analysis, and Gaussian process surrogates are constructed for the principal component scores. Building on this representation, MM-FBO introduces an integrated uncertainty acquisition function that balances exploitation of worst case expected error with exploration across the functional domain. We provide two theoretical guarantees: a discretization bound for the worst case objective, and a consistency result showing that as the surrogate becomes accurate and uncertainty vanishes, the acquisition converges to the true min-max objective. We validate the method through experiments on synthetic benchmarks and physics inspired case studies involving electromagnetic scattering by metaphotonic devices and vapor phase infiltration. Results show that MM-FBO consistently outperforms existing baselines and highlights the importance of explicitly modeling functional uncertainty in Bayesian optimization.

</details>


### [195] [Advancing physiological time series reconstruction and imputation via mixture of receptive fields and experts fusion](https://arxiv.org/abs/2512.07873)
*Ci Zhang,Huayu Li,Changdi Yang,Jiangnan Xia,Yanzhi Wang,Xiaolong Ma,Jin Lu,Geng Yuan*

Main category: cs.LG

TL;DR: A novel Mixture of Experts (MoE)-based diffusion framework for medical time series reconstruction that uses RFAMoE for adaptive receptive fields and Fusion MoE for parallel noise generation, achieving SOTA performance with single-inference efficiency.


<details>
  <summary>Details</summary>
Motivation: Diffusion models show promise for time series reconstruction but remain unexplored for medical signals. Physiological time series have unique challenges: multivariate, high temporal variability, noisy, and artifact-prone, making deep learning approaches difficult for tasks like imputation.

Method: Proposes a MoE-based noise estimator within a score-based diffusion framework. Two key components: 1) RFAMoE module for adaptive receptive field selection per channel throughout diffusion, 2) Fusion MoE module that generates K noise signals in parallel and fuses them via routing mechanism, enabling single-inference reconstruction instead of multiple inferences.

Result: Extensive results show the framework consistently outperforms diffusion-based SOTA works on different tasks and datasets. The method improves performance while eliminating the computational cost and latency of multiple inference processes.

Conclusion: The proposed MoE-based diffusion framework effectively addresses the challenges of medical time series reconstruction, achieving superior performance with efficient single-inference operation, making it practical for real-world medical applications.

Abstract: Recent studies show that using diffusion models for time series signal reconstruc- tion holds great promise. However, such approaches remain largely unexplored in the domain of medical time series. The unique characteristics of the physiological time series signals, such as multivariate, high temporal variability, highly noisy, and artifact-prone, make deep learning-based approaches still challenging for tasks such as imputation. Hence, we propose a novel Mixture of Experts (MoE)-based noise estimator within a score-based diffusion framework. Specifically, the Receptive Field Adaptive MoE (RFAMoE) module is designed to enable each channel to adap- tively select desired receptive fields throughout the diffusion process. Moreover, recent literature has found that when generating a physiological signal, performing multiple inferences and averaging the reconstructed signals can effectively reduce reconstruction errors, but at the cost of significant computational and latency over- head. We design a Fusion MoE module and innovatively leverage the nature of MoE module to generate K noise signals in parallel, fuse them using a routing mechanism, and complete signal reconstruction in a single inference step. This design not only improves performance over previous methods but also eliminates the substantial computational cost and latency associated with multiple inference processes. Extensive results demonstrate that our proposed framework consistently outperforms diffusion-based SOTA works on different tasks and datasets.

</details>


### [196] [Controllable risk scenario generation from human crash data for autonomous vehicle testing](https://arxiv.org/abs/2512.07874)
*Qiujing Lu,Xuanhan Wang,Runze Yuan,Wei Lu,Xinyi Gong,Shuo Feng*

Main category: cs.LG

TL;DR: CRAG is a framework for generating realistic autonomous vehicle test scenarios with controllable risk behaviors, enabling both normal driving and safety-critical conditions.


<details>
  <summary>Details</summary>
Motivation: Testing autonomous vehicles requires simulating realistic agents that can exhibit both normal driving behaviors and rare safety-critical behaviors, but existing approaches struggle to model both regimes effectively with limited crash data.

Method: CRAG constructs a structured latent space that disentangles normal and risk-related behaviors, combining risk-aware latent representations with optimization-based mode-transition mechanisms to enable smooth, plausible transitions from safe to risk states.

Result: CRAG improves diversity compared to existing baselines and enables controllable generation of risk scenarios for targeted and efficient evaluation of AV robustness.

Conclusion: CRAG provides a unified framework for generating realistic test scenarios that can model both nominal and safety-critical behaviors, addressing the challenge of limited crash data while enabling efficient AV safety evaluation.

Abstract: Ensuring the safety of autonomous vehicles (AV) requires rigorous testing under both everyday driving and rare, safety-critical conditions. A key challenge lies in simulating environment agents, including background vehicles (BVs) and vulnerable road users (VRUs), that behave realistically in nominal traffic while also exhibiting risk-prone behaviors consistent with real-world accidents. We introduce Controllable Risk Agent Generation (CRAG), a framework designed to unify the modeling of dominant nominal behaviors and rare safety-critical behaviors. CRAG constructs a structured latent space that disentangles normal and risk-related behaviors, enabling efficient use of limited crash data. By combining risk-aware latent representations with optimization-based mode-transition mechanisms, the framework allows agents to shift smoothly and plausibly from safe to risk states over extended horizons, while maintaining high fidelity in both regimes. Extensive experiments show that CRAG improves diversity compared to existing baselines, while also enabling controllable generation of risk scenarios for targeted and efficient evaluation of AV robustness.

</details>


### [197] [Softly Symbolifying Kolmogorov-Arnold Networks](https://arxiv.org/abs/2512.07875)
*James Bagrow,Josh Bongard*

Main category: cs.LG

TL;DR: S2KAN integrates symbolic primitives into KAN training with learnable gates and MDL objective, enabling interpretable symbolic forms when possible while gracefully degrading to dense splines when needed.


<details>
  <summary>Details</summary>
Motivation: Standard KANs often learn pathological decompositions without meaningful interpretable forms despite their theoretical promise for interpretable machine learning.

Method: Integrates symbolic primitives directly into training with learnable gates that sparsify representations using differentiable sparsification guided by Minimum Description Length objective.

Result: Achieves competitive or superior accuracy with substantially smaller models across symbolic benchmarks, dynamical systems forecasting, and real-world prediction tasks, showing emergent self-sparsification.

Conclusion: S2KAN successfully bridges symbolic and dense representations in KANs, discovering interpretable forms when possible while maintaining flexibility through graceful degradation to splines.

Abstract: Kolmogorov-Arnold Networks (KANs) offer a promising path toward interpretable machine learning: their learnable activations can be studied individually, while collectively fitting complex data accurately. In practice, however, trained activations often lack symbolic fidelity, learning pathological decompositions with no meaningful correspondence to interpretable forms. We propose Softly Symbolified Kolmogorov-Arnold Networks (S2KAN), which integrate symbolic primitives directly into training. Each activation draws from a dictionary of symbolic and dense terms, with learnable gates that sparsify the representation. Crucially, this sparsification is differentiable, enabling end-to-end optimization, and is guided by a principled Minimum Description Length objective. When symbolic terms suffice, S2KAN discovers interpretable forms; when they do not, it gracefully degrades to dense splines. We demonstrate competitive or superior accuracy with substantially smaller models across symbolic benchmarks, dynamical systems forecasting, and real-world prediction tasks, and observe evidence of emergent self-sparsification even without regularization pressure.

</details>


### [198] [Fourier-Enhanced Recurrent Neural Networks for Electrical Load Time Series Downscaling](https://arxiv.org/abs/2512.07876)
*Qi Chen,Mihai Anitescu*

Main category: cs.LG

TL;DR: Fourier-enhanced RNN with attention for electrical load downscaling outperforms Prophet baselines and RNN ablations across PJM territories.


<details>
  <summary>Details</summary>
Motivation: Need for accurate electrical load downscaling from low-resolution to high-resolution forecasts, improving upon classical methods like Prophet and basic RNNs.

Method: Combines recurrent backbone with Fourier seasonal embeddings fused in latent space and self-attention layer for high-resolution component dependencies.

Result: Achieves lower and flatter horizon-wise RMSE across four PJM territories compared to Prophet baselines (with/without seasonality/LAA) and RNN ablations without attention or Fourier features.

Conclusion: Fourier-enhanced RNN with attention is effective for electrical load downscaling, outperforming existing approaches through explicit seasonal modeling and attention mechanisms.

Abstract: We present a Fourier-enhanced recurrent neural network (RNN) for downscaling electrical loads. The model combines (i) a recurrent backbone driven by low-resolution inputs, (ii) explicit Fourier seasonal embeddings fused in latent space, and (iii) a self-attention layer that captures dependencies among high-resolution components within each period. Across four PJM territories, the approach yields RMSE lower and flatter horizon-wise than classical Prophet baselines (with and without seasonality/LAA) and than RNN ablations without attention or Fourier features.

</details>


### [199] [Artificial Intelligence-Driven Network-on-Chip Design Space Exploration: Neural Network Architectures for Design](https://arxiv.org/abs/2512.07877)
*Amogh Anshu N,Harish BP*

Main category: cs.LG

TL;DR: Machine learning framework automates NoC design space exploration using BookSim simulations and reverse neural networks, with Conditional Diffusion Model achieving best performance.


<details>
  <summary>Details</summary>
Motivation: Traditional NoC design space exploration is slow and struggles with complex parameter interactions, requiring faster automated solutions.

Method: ML-driven framework using BookSim simulations and reverse neural networks (MLP, Conditional Diffusion Model, CVAE) to predict optimal NoC parameters from target performance metrics.

Result: Conditional Diffusion Model achieved highest accuracy (MSE 0.463), framework reduces exploration time by orders of magnitude using 150,000+ simulation data points.

Conclusion: Proposed framework enables rapid, scalable NoC co-design by automating design space exploration with machine learning.

Abstract: Network-on-Chip (NoC) design requires exploring a high-dimensional configuration space to satisfy stringent throughput requirements and latency constraints.Traditional design space exploration techniques are often slow and struggle to handle complex, non-linear parameter interactions.This work presents a machine learning-driven framework that automates NoC design space exploration using BookSim simulations and reverse neural network models.Specifically, we compare three architectures - a Multi-Layer Perceptron (MLP),a Conditional Diffusion Model, and a Conditional Variational Autoencoder (CVAE) to predict optimal NoC parameters given target performance metrics.Our pipeline generates over 150,000 simulation data points across varied mesh topologies.The Conditional Diffusion Model achieved the highest predictive accuracy, attaining a mean squared error (MSE) of 0.463 on unseen data.Furthermore, the proposed framework reduces design exploration time by several orders of magnitude, making it a practical solution for rapid and scalable NoC co-design.

</details>


### [200] [Graph Contrastive Learning via Spectral Graph Alignment](https://arxiv.org/abs/2512.07878)
*Manh Nguyen,Joshua Cape*

Main category: cs.LG

TL;DR: SpecMatch-CL is a novel contrastive learning loss for graph representation learning that aligns view-specific graph-of-graphs by minimizing differences between their normalized Laplacians, improving both unsupervised and semi-supervised performance.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive learning methods for graphs (like InfoNCE) align pairwise embeddings across augmented views but lack control over the global structure of the view-specific graph-of-graphs built from these embeddings.

Method: SpecMatch-CL introduces a novel loss function that aligns view-specific graph-of-graphs by minimizing the difference between their normalized Laplacians, providing theoretical guarantees about alignment quality.

Result: Achieves new state-of-the-art on eight TU benchmarks in unsupervised and semi-supervised learning at low label rates, and yields consistent gains in transfer learning on PPI-306K and ZINC 2M datasets.

Conclusion: SpecMatch-CL effectively addresses the limitation of existing contrastive learning methods by controlling global structure alignment through normalized Laplacian matching, leading to superior performance across multiple graph learning tasks.

Abstract: Given augmented views of each input graph, contrastive learning methods (e.g., InfoNCE) optimize pairwise alignment of graph embeddings across views while providing no mechanism to control the global structure of the view specific graph-of-graphs built from these embeddings. We introduce SpecMatch-CL, a novel loss function that aligns the view specific graph-of-graphs by minimizing the difference between their normalized Laplacians. Theoretically, we show that under certain assumptions, the difference between normalized Laplacians provides an upper bound not only for the difference between the ideal Perfect Alignment contrastive loss and the current loss, but also for the Uniformly loss. Empirically, SpecMatch-CL establishes new state of the art on eight TU benchmarks under unsupervised learning and semi-supervised learning at low label rates, and yields consistent gains in transfer learning on PPI-306K and ZINC 2M datasets.

</details>


### [201] [Nonnegative Matrix Factorization through Cone Collapse](https://arxiv.org/abs/2512.07879)
*Manh Nguyen,Daniel Pimentel-AlarcÃ³n*

Main category: cs.LG

TL;DR: Cone Collapse algorithm for NMF-based clustering that geometrically recovers the minimal generating cone of data, leading to improved clustering performance.


<details>
  <summary>Details</summary>
Motivation: Existing NMF algorithms don't exploit the conic geometry of nonnegative data, where data points lie in a convex cone whose extreme rays encode fundamental topics/directions. The authors aim to leverage this geometric perspective for better clustering.

Method: Proposes Cone Collapse algorithm that starts from full nonnegative orthant and iteratively shrinks it toward the minimal cone generated by the data. Then derives cone-aware orthogonal NMF (CC-NMF) by applying uni-orthogonal NMF to the recovered extreme rays.

Result: Proves that under mild assumptions, Cone Collapse terminates in finitely many steps and recovers the minimal generating cone. On 16 benchmark datasets (gene-expression, text, images), CC-NMF consistently matches or outperforms strong NMF baselines in clustering purity.

Conclusion: Explicitly recovering the data cone yields both theoretically grounded and empirically strong NMF-based clustering methods, demonstrating the value of geometric perspective in NMF.

Abstract: Nonnegative matrix factorization (NMF) is a widely used tool for learning parts-based, low-dimensional representations of nonnegative data, with applications in vision, text, and bioinformatics. In clustering applications, orthogonal NMF (ONMF) variants further impose (approximate) orthogonality on the representation matrix so that its rows behave like soft cluster indicators. Existing algorithms, however, are typically derived from optimization viewpoints and do not explicitly exploit the conic geometry induced by NMF: data points lie in a convex cone whose extreme rays encode fundamental directions or "topics". In this work we revisit NMF from this geometric perspective and propose Cone Collapse, an algorithm that starts from the full nonnegative orthant and iteratively shrinks it toward the minimal cone generated by the data. We prove that, under mild assumptions on the data, Cone Collapse terminates in finitely many steps and recovers the minimal generating cone of $\mathbf{X}^\top$ . Building on this basis, we then derive a cone-aware orthogonal NMF model (CC-NMF) by applying uni-orthogonal NMF to the recovered extreme rays. Across 16 benchmark gene-expression, text, and image datasets, CC-NMF consistently matches or outperforms strong NMF baselines-including multiplicative updates, ANLS, projective NMF, ONMF, and sparse NMF-in terms of clustering purity. These results demonstrate that explicitly recovering the data cone can yield both theoretically grounded and empirically strong NMF-based clustering methods.

</details>


### [202] [Semi-Supervised Contrastive Learning with Orthonormal Prototypes](https://arxiv.org/abs/2512.07880)
*Huanran Li,Manh Nguyen,Daniel Pimentel-AlarcÃ³n*

Main category: cs.LG

TL;DR: CLOP is a novel semi-supervised contrastive learning loss that prevents dimensional collapse by promoting orthogonal subspaces, improving performance and stability across various learning rates and batch sizes.


<details>
  <summary>Details</summary>
Motivation: Contrastive learning suffers from dimensional collapse where embeddings converge into lower-dimensional spaces, especially in semi-supervised and self-supervised setups. This collapse limits representation quality and model performance.

Method: First identified a critical learning-rate threshold beyond which standard contrastive losses collapse. Then proposed CLOP, a novel semi-supervised loss function that prevents dimensional collapse by promoting formation of orthogonal linear subspaces among class embeddings.

Result: Extensive experiments on real and synthetic datasets show CLOP improves performance in image classification and object detection tasks while exhibiting greater stability across different learning rates and batch sizes.

Conclusion: CLOP effectively addresses dimensional collapse in contrastive learning through orthogonal subspace promotion, leading to more robust and performant representations in semi-supervised settings.

Abstract: Contrastive learning has emerged as a powerful method in deep learning, excelling at learning effective representations through contrasting samples from different distributions. However, dimensional collapse, where embeddings converge into a lower-dimensional space, poses a significant challenge, especially in semi-supervised and self-supervised setups. In this paper, we first identify a critical learning-rate threshold, beyond which standard contrastive losses converge to collapsed solutions. Building on these insights, we propose CLOP, a novel semi-supervised loss function designed to prevent dimensional collapse by promoting the formation of orthogonal linear subspaces among class embeddings. Through extensive experiments on real and synthetic datasets, we demonstrate that CLOP improves performance in image classification and object detection tasks while also exhibiting greater stability across different learning rates and batch sizes.

</details>


### [203] [GSPN-2: Efficient Parallel Sequence Modeling](https://arxiv.org/abs/2512.07884)
*Hongjun Wang,Yitong Jiang,Collin McCarthy,David Wehr,Hanrong Ye,Xinhao Li,Ka Chun Cheung,Wonmin Byeon,Jinwei Gu,Ke Chen,Kai Han,Hongxu Yin,Pavlo Molchanov,Jan Kautz,Sifei Liu*

Main category: cs.LG

TL;DR: GSPN-2 is an optimized version of Generalized Spatial Propagation Network that addresses GPU inefficiencies in the original GSPN implementation while introducing a compact channel propagation strategy, achieving transformer-level accuracy with significantly lower computational cost.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational inefficiency of vision transformers for high-resolution images and long videos. While GSPN reduced quadratic self-attention to near-linear cost, its implementation still suffered from GPU overhead issues including repeated kernel launches, excessive memory transfers, and redundant computations.

Method: GSPN-2 combines algorithm and system redesign: (1) System optimization: consolidates thousands of micro-launches into a single 2D kernel, pins one warp per channel slice, and stages activations in shared memory. (2) Algorithm improvement: introduces compact channel propagation strategy that replaces per-channel matrices with aligned affinity maps, reducing parameters.

Result: Experiments show GSPN-2 matches transformer-level accuracy on image classification and text-to-image synthesis tasks while achieving significantly lower computational cost, establishing a new efficiency frontier for modeling global spatial context in vision applications.

Conclusion: GSPN-2 successfully addresses the GPU inefficiencies of the original GSPN while maintaining accuracy, providing an efficient alternative to vision transformers for high-resolution and long-video applications through its optimized algorithm-system co-design.

Abstract: Efficient vision transformer remains a bottleneck for high-resolution images and long-video related real-world applications. Generalized Spatial Propagation Network (GSPN) addresses this by replacing quadratic self-attention with a line-scan propagation scheme, bringing the cost close to linear in the number of rows or columns, while retaining accuracy. Despite this advancement, the existing GSPN implementation still suffers from (i) heavy overhead due to repeatedly launching GPU kernels, (ii) excessive data transfers from global GPU memory, and (iii) redundant computations caused by maintaining separate propagation weights for each channel. We introduce GSPN-2, a joint algorithm-system redesign. In particular, we eliminate thousands of micro-launches from the previous implementation into one single 2D kernel, explicitly pin one warp to each channel slice, and stage the previous column's activations in shared memory. On the model side, we introduce a compact channel propagation strategy that replaces per-channel matrices, trimming parameters, and align naturally with the affinity map used in transformer attention. Experiments demonstrate GSPN-2's effectiveness across image classification and text-to-image synthesis tasks, matching transformer-level accuracy with significantly lower computational cost. GSPN-2 establishes a new efficiency frontier for modeling global spatial context in vision applications through its unique combination of structured matrix transformations and GPU-optimized implementation. Project page: https://whj363636.github.io/GSPN2/

</details>


### [204] [ByteStorm: a multi-step data-driven approach for Tropical Cyclones detection and tracking](https://arxiv.org/abs/2512.07885)
*Davide Donno,Donatello Elia,Gabriele Accarino,Marco De Carlo,Enrico Scoccimarro,Silvio Gualdi*

Main category: cs.LG

TL;DR: ByteStorm is a deep learning framework for tropical cyclone tracking that uses vorticity and pressure data with BYTE algorithm for track linking, outperforming traditional threshold-based methods.


<details>
  <summary>Details</summary>
Motivation: Traditional TC tracking methods rely on subjective thresholds that introduce biases and lack robustness across different geographical regions. There's a need for more accurate, data-driven approaches that don't require manual threshold tuning.

Method: ByteStorm combines deep learning networks for TC center detection (using classification and localization on 850 mb relative vorticity and mean sea-level pressure) with the BYTE algorithm for linking detected centers into complete tracks.

Result: ByteStorm achieves superior performance: Probability of Detection (85.05% ENP, 79.48% WNP), False Alarm Rate (23.26% ENP, 16.14% WNP), and high Inter-Annual Variability correlations (0.75 ENP, 0.69 WNP).

Conclusion: The integration of deep learning and computer vision offers a fast, accurate, and robust alternative to traditional threshold-based TC tracking methods, with strong performance across different Pacific basins.

Abstract: Accurate tropical cyclones (TCs) tracking represents a critical challenge in the context of weather and climate science. Traditional tracking schemes mainly rely on subjective thresholds, which may introduce biases in their skills on the geographical region of application. We present ByteStorm, an efficient data-driven framework for reconstructing TC tracks without threshold tuning. It leverages deep learning networks to detect TC centers (via classification and localization), using only relative vorticity (850 mb) and mean sea-level pressure. Then, detected centers are linked into TC tracks through the BYTE algorithm. ByteStorm is evaluated against state-of-the-art deterministic trackers in the East- and West-North Pacific basins (ENP and WNP). The proposed framework achieves superior performance in terms of Probability of Detection ($85.05\%$ ENP, $79.48\%$ WNP), False Alarm Rate ($23.26\%$ ENP, $16.14\%$ WNP), and high Inter-Annual Variability correlations ($0.75$ ENP and $0.69$ WNP). These results highlight the potential of integrating deep learning and computer vision for fast and accurate TC tracking, offering a robust alternative to traditional approaches.

</details>


### [205] [Towards symbolic regression for interpretable clinical decision scores](https://arxiv.org/abs/2512.07961)
*Guilherme Seidyo Imai Aldeia,Joseph D. Romano,Fabricio Olivetti de Franca,Daniel S. Herman,William G. La Cava*

Main category: cs.LG

TL;DR: Brush is a symbolic regression algorithm that combines decision-tree-like splitting with non-linear optimization to create interpretable clinical risk scores that integrate rule-based logic.


<details>
  <summary>Details</summary>
Motivation: Medical decision-making uses algorithms combining risk equations with rules, but traditional symbolic regression is limited to continuous functions and can't easily model rule-based decision-making. There's a need for data-driven, interpretable clinical risk scores.

Method: Brush combines decision-tree-like splitting algorithms with non-linear constant optimization, allowing seamless integration of rule-based logic into symbolic regression and classification models.

Result: Brush achieves Pareto-optimal performance on SRBench, accurately recapitulates two widely used clinical scoring systems, and produces simpler models with comparable or superior predictive performance compared to decision trees, random forests, and other SR methods.

Conclusion: Brush enables development of data-driven, interpretable clinical risk scores by effectively integrating rule-based logic into symbolic regression, offering a promising approach for medical decision-making algorithms.

Abstract: Medical decision-making makes frequent use of algorithms that combine risk equations with rules, providing clear and standardized treatment pathways. Symbolic regression (SR) traditionally limits its search space to continuous function forms and their parameters, making it difficult to model this decision-making. However, due to its ability to derive data-driven, interpretable models, SR holds promise for developing data-driven clinical risk scores. To that end we introduce Brush, an SR algorithm that combines decision-tree-like splitting algorithms with non-linear constant optimization, allowing for seamless integration of rule-based logic into symbolic regression and classification models. Brush achieves Pareto-optimal performance on SRBench, and was applied to recapitulate two widely used clinical scoring systems, achieving high accuracy and interpretable models. Compared to decision trees, random forests, and other SR methods, Brush achieves comparable or superior predictive performance while producing simpler models.

</details>


### [206] [CIP-Net: Continual Interpretable Prototype-based Network](https://arxiv.org/abs/2512.07981)
*Federico Di Valerio,Michela Proietti,Alessio Ragno,Roberto Capobianco*

Main category: cs.LG

TL;DR: CIP-Net is an exemplar-free self-explainable prototype-based model for continual learning that avoids catastrophic forgetting without storing past examples, achieving state-of-the-art performance with low memory overhead.


<details>
  <summary>Details</summary>
Motivation: Continual learning faces catastrophic forgetting where models lose performance on previous tasks when learning new ones. Existing explainable AI approaches either use post-hoc explanations or require additional memory per task, limiting scalability. There's a need for practical, interpretable solutions that don't store past examples.

Method: CIP-Net is an exemplar-free self-explainable prototype-based model designed for continual learning. It maintains a simple architecture without storing past examples, generates explanations during prediction, and uses prototype-based learning to preserve knowledge across tasks.

Result: CIP-Net achieves state-of-the-art performance compared to previous exemplar-free and self-explainable methods in both task- and class-incremental settings, while bearing significantly lower memory-related overhead.

Conclusion: CIP-Net provides a practical and interpretable solution for continual learning that combines strong performance with useful explanations and minimal memory requirements, making it scalable for real-world applications.

Abstract: Continual learning constrains models to learn new tasks over time without forgetting what they have already learned. A key challenge in this setting is catastrophic forgetting, where learning new information causes the model to lose its performance on previous tasks. Recently, explainable AI has been proposed as a promising way to better understand and reduce forgetting. In particular, self-explainable models are useful because they generate explanations during prediction, which can help preserve knowledge. However, most existing explainable approaches use post-hoc explanations or require additional memory for each new task, resulting in limited scalability. In this work, we introduce CIP-Net, an exemplar-free self-explainable prototype-based model designed for continual learning. CIP-Net avoids storing past examples and maintains a simple architecture, while still providing useful explanations and strong performance. We demonstrate that CIPNet achieves state-of-the-art performances compared to previous exemplar-free and self-explainable methods in both task- and class-incremental settings, while bearing significantly lower memory-related overhead. This makes it a practical and interpretable solution for continual learning.

</details>


### [207] [HOLE: Homological Observation of Latent Embeddings for Neural Network Interpretability](https://arxiv.org/abs/2512.07988)
*Sudhanva Manjunath Athreya,Paul Rosen*

Main category: cs.LG

TL;DR: HOLE uses persistent homology to analyze neural network representations through topological features and visualization tools, revealing patterns in class separation, feature disentanglement, and model robustness.


<details>
  <summary>Details</summary>
Motivation: Deep learning models achieve great success but remain opaque and hard to interpret. There's a need for better methods to understand learned representations and decision-making processes in neural networks.

Method: HOLE (Homological Observation of Latent Embeddings) applies persistent homology to extract topological features from neural activations. It uses visualization techniques including Sankey diagrams, heatmaps, dendrograms, and blob graphs to examine representation structure across layers.

Result: Evaluation on standard datasets with various discriminative models shows topological analysis reveals patterns associated with class separation, feature disentanglement, and model robustness. The method provides insights into representation quality and interpretability across layers, as well as robustness to input perturbations and model compression.

Conclusion: Topological analysis through HOLE offers a complementary perspective for understanding and improving deep learning systems by revealing structural patterns in neural representations that traditional methods might miss.

Abstract: Deep learning models have achieved remarkable success across various domains, yet their learned representations and decision-making processes remain largely opaque and hard to interpret. This work introduces HOLE (Homological Observation of Latent Embeddings), a method for analyzing and interpreting deep neural networks through persistent homology. HOLE extracts topological features from neural activations and presents them using a suite of visualization techniques, including Sankey diagrams, heatmaps, dendrograms, and blob graphs. These tools facilitate the examination of representation structure and quality across layers. We evaluate HOLE on standard datasets using a range of discriminative models, focusing on representation quality, interpretability across layers, and robustness to input perturbations and model compression. The results indicate that topological analysis reveals patterns associated with class separation, feature disentanglement, and model robustness, providing a complementary perspective for understanding and improving deep learning systems.

</details>


### [208] [Bridging the Clinical Expertise Gap: Development of a Web-Based Platform for Accessible Time Series Forecasting and Analysis](https://arxiv.org/abs/2512.07992)
*Aaron D. Mullen,Daniel R. Harris,Svetla Slavova,V. K. Cody Bumgardner*

Main category: cs.LG

TL;DR: A web platform that simplifies time series forecasting for healthcare by providing accessible tools for data analysis, model training, and result interpretation with AI assistance.


<details>
  <summary>Details</summary>
Motivation: Time series forecasting has valuable applications in healthcare, but technical expertise requirements create barriers for researchers and clinicians who want to use these techniques effectively.

Method: Developed a web platform that allows users to upload data, generate visualizations, train customizable forecasting models, and receive AI-powered recommendations and explanations from a large language model to guide parameter selection and result interpretation.

Result: Created an accessible platform that makes time series forecasting available to non-experts, supporting multiple models and training techniques with customization options and AI assistance for parameter selection and result understanding.

Conclusion: The platform successfully democratizes time series forecasting for healthcare applications and is designed for integration into learning health systems to enable continuous data collection and inference from clinical pipelines.

Abstract: Time series forecasting has applications across domains and industries, especially in healthcare, but the technical expertise required to analyze data, build models, and interpret results can be a barrier to using these techniques. This article presents a web platform that makes the process of analyzing and plotting data, training forecasting models, and interpreting and viewing results accessible to researchers and clinicians. Users can upload data and generate plots to showcase their variables and the relationships between them. The platform supports multiple forecasting models and training techniques which are highly customizable according to the user's needs. Additionally, recommendations and explanations can be generated from a large language model that can help the user choose appropriate parameters for their data and understand the results for each model. The goal is to integrate this platform into learning health systems for continuous data collection and inference from clinical pipelines.

</details>


### [209] [Benchmarking Offline Multi-Objective Reinforcement Learning in Critical Care](https://arxiv.org/abs/2512.08012)
*Aryaman Bansal,Divya Sharma*

Main category: cs.LG

TL;DR: Offline Multi-Objective Reinforcement Learning (MORL) outperforms single-objective approaches in ICU settings by learning flexible policies along the Pareto Frontier, with PEDA Decision Transformer showing superior adaptability for balancing patient survival and resource utilization.


<details>
  <summary>Details</summary>
Motivation: In critical care, clinicians need to balance conflicting objectives: maximizing patient survival while minimizing resource use (e.g., length of stay). Single-objective RL uses fixed scalarized rewards, creating rigid policies that can't adapt to varying clinical priorities. MORL can learn flexible policies along the Pareto Frontier for dynamic preference selection.

Method: Benchmarked three offline MORL algorithms (Conditioned Conservative Pareto Q-Learning, Adaptive CPQL, and modified PEDA Decision Transformer) against three scalarized single-objective baselines (BC, CQL, DDQN) on MIMIC-IV dataset. Used Off-Policy Evaluation metrics to compare performance.

Result: PEDA Decision Transformer algorithm demonstrated superior flexibility compared to static scalarized baselines. Sequence modeling architectures (like Decision Transformers) remain robust and effective when scaled to multi-objective conditioned generation. Offline MORL enables personalized, adjustable decision-making without retraining.

Conclusion: Offline MORL is a promising framework for personalized, adjustable decision-making in critical care. It allows clinicians to dynamically select preferences at test time without retraining, addressing the fundamental trade-off between patient survival and resource utilization more effectively than rigid single-objective approaches.

Abstract: In critical care settings such as the Intensive Care Unit, clinicians face the complex challenge of balancing conflicting objectives, primarily maximizing patient survival while minimizing resource utilization (e.g., length of stay). Single-objective Reinforcement Learning approaches typically address this by optimizing a fixed scalarized reward function, resulting in rigid policies that fail to adapt to varying clinical priorities. Multi-objective Reinforcement Learning (MORL) offers a solution by learning a set of optimal policies along the Pareto Frontier, allowing for dynamic preference selection at test time. However, applying MORL in healthcare necessitates strict offline learning from historical data.
  In this paper, we benchmark three offline MORL algorithms, Conditioned Conservative Pareto Q-Learning (CPQL), Adaptive CPQL, and a modified Pareto Efficient Decision Agent (PEDA) Decision Transformer (PEDA DT), against three scalarized single-objective baselines (BC, CQL, and DDQN) on the MIMIC-IV dataset. Using Off-Policy Evaluation (OPE) metrics, we demonstrate that PEDA DT algorithm offers superior flexibility compared to static scalarized baselines. Notably, our results extend previous findings on single-objective Decision Transformers in healthcare, confirming that sequence modeling architectures remain robust and effective when scaled to multi-objective conditioned generation. These findings suggest that offline MORL is a promising framework for enabling personalized, adjustable decision-making in critical care without the need for retraining.

</details>


### [210] [CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space](https://arxiv.org/abs/2512.08029)
*Tianxingjian Ding,Yuanhao Zou,Chen Chen,Mubarak Shah,Yu Tian*

Main category: cs.LG

TL;DR: CLARITY is a medical world model that forecasts disease evolution in structured latent space, integrating temporal and clinical contexts to generate physiologically faithful, individualized treatment plans with transparent recommendations.


<details>
  <summary>Details</summary>
Motivation: Current static AI predictors cannot handle dynamic disease evolution in oncology. Existing medical world models ignore patient-specific temporal/clinical contexts, lack feedback mechanisms linking predictions to treatment decisions, and focus on visual reconstruction rather than causal physiological transitions.

Method: CLARITY forecasts disease evolution directly within structured latent space, explicitly integrating time intervals (temporal context) and patient-specific data (clinical context). It models treatment-conditioned progression as smooth, interpretable trajectories and introduces a novel prediction-to-decision framework translating latent rollouts into actionable recommendations.

Result: CLARITY demonstrates state-of-the-art performance in treatment planning, outperforming recent MeWM by 12% on the MU-Glioma-Post dataset and significantly surpassing all other medical-specific large language models.

Conclusion: CLARITY addresses key limitations in medical world models by providing physiologically faithful, individualized treatment forecasts with transparent recommendations, advancing clinical decision-making in oncology through dynamic disease evolution prediction.

Abstract: Clinical decision-making in oncology requires predicting dynamic disease evolution, a task current static AI predictors cannot perform. While world models (WMs) offer a paradigm for generative prediction, existing medical applications remain limited. Existing methods often rely on stochastic diffusion models, focusing on visual reconstruction rather than causal, physiological transitions. Furthermore, in medical domain, models like MeWM typically ignore patient-specific temporal and clinical contexts and lack a feedback mechanism to link predictions to treatment decisions. To address these gaps, we introduce CLARITY, a medical world model that forecasts disease evolution directly within a structured latent space. It explicitly integrates time intervals (temporal context) and patient-specific data (clinical context) to model treatment-conditioned progression as a smooth, interpretable trajectory, and thus generate physiologically faithful, individualized treatment plans. Finally, CLARITY introduces a novel prediction-to-decision framework, translating latent rollouts into transparent, actionable recommendations. CLARITY demonstrates state-of-the-art performance in treatment planning. On the MU-Glioma-Post dataset, our approach outperforms recent MeWM by 12\%, and significantly surpasses all other medical-specific large language models.

</details>


### [211] [LUNA: Linear Universal Neural Attention with Generalization Guarantees](https://arxiv.org/abs/2512.08061)
*Ashkan Shahbazi,Ping He,Ali Abbasi,Yikun Bai,Xinran Liu,Elaheh Akbari,Darian Salehi,Navid NaderiAlizadeh,Soheil Kolouri*

Main category: cs.LG

TL;DR: LUNA introduces a kernelized linear attention mechanism that learns feature maps instead of using fixed ones, achieving linear computational cost while matching or surpassing quadratic attention accuracy.


<details>
  <summary>Details</summary>
Motivation: Standard softmax attention has O(nÂ²) computational cost that limits long-sequence applications. Existing linear attention methods use fixed random feature maps, creating a trade-off where practitioners must sacrifice model accuracy for computational efficiency.

Method: LUNA learns a parameterized kernel feature map tailored to specific data and tasks, inducing a positive-definite kernel with streaming form. This enables linear time and memory scaling while maintaining expressive power.

Result: On Long Range Arena (LRA), LUNA achieves state-of-the-art average accuracy among efficient Transformers under compute parity. It also excels at post-hoc conversion, replacing softmax in fine-tuned BERT and ViT-B/16 checkpoints while recovering most original performance.

Conclusion: LUNA eliminates the accuracy-efficiency trade-off in attention mechanisms by learning kernel feature maps, enabling linear computational cost while matching or surpassing quadratic attention performance across diverse applications.

Abstract: Scaling attention faces a critical bottleneck: the $\mathcal{O}(n^2)$ quadratic computational cost of softmax attention, which limits its application in long-sequence domains. While linear attention mechanisms reduce this cost to $\mathcal{O}(n)$, they typically rely on fixed random feature maps, such as random Fourier features or hand-crafted functions. This reliance on static, data-agnostic kernels creates a fundamental trade-off, forcing practitioners to sacrifice significant model accuracy for computational efficiency. We introduce \textsc{LUNA}, a kernelized linear attention mechanism that eliminates this trade-off, retaining linear cost while matching and surpassing the accuracy of quadratic attention. \textsc{LUNA} is built on the key insight that the kernel feature map itself should be learned rather than fixed a priori. By parameterizing the kernel, \textsc{LUNA} learns a feature basis tailored to the specific data and task, overcoming the expressive limitations of fixed-feature methods. \textsc{Luna} implements this with a learnable feature map that induces a positive-definite kernel and admits a streaming form, yielding linear time and memory scaling in the sequence length. Empirical evaluations validate our approach across diverse settings. On the Long Range Arena (LRA), \textsc{Luna} achieves state-of-the-art average accuracy among efficient Transformers under compute parity, using the same parameter count, training steps, and approximate FLOPs. \textsc{Luna} also excels at post-hoc conversion: replacing softmax in fine-tuned BERT and ViT-B/16 checkpoints and briefly fine-tuning recovers most of the original performance, substantially outperforming fixed linearizations.

</details>


### [212] [Deep Kernel Aalen-Johansen Estimator: An Interpretable and Flexible Neural Net Framework for Competing Risks](https://arxiv.org/abs/2512.08063)
*Xiaobin Shen,George H. Chen*

Main category: cs.LG

TL;DR: DKAJ is an interpretable deep learning model for competing risks analysis that generalizes the classical Aalen-Johansen estimator using learned kernel functions and cluster-based representations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create an interpretable deep learning model for competing risks analysis that maintains the interpretability of classical nonparametric methods like the Aalen-Johansen estimator while leveraging the power of deep learning for complex data patterns.

Method: DKAJ represents each data point as a weighted combination of clusters, where weights come from an automatically learned kernel function measuring similarity between data points. When a data point has nonzero weight only for one cluster, its predicted cumulative incidence functions correspond to the classical Aalen-Johansen estimator restricted to that cluster.

Result: On four standard competing risks datasets, DKAJ is competitive with state-of-the-art baselines while providing visualizations to assist model interpretation.

Conclusion: DKAJ successfully bridges classical nonparametric survival analysis with modern deep learning, offering competitive performance while maintaining interpretability through cluster-based representations and visualizations.

Abstract: We propose an interpretable deep competing risks model called the Deep Kernel Aalen-Johansen (DKAJ) estimator, which generalizes the classical Aalen-Johansen nonparametric estimate of cumulative incidence functions (CIFs). Each data point (e.g., patient) is represented as a weighted combination of clusters. If a data point has nonzero weight only for one cluster, then its predicted CIFs correspond to those of the classical Aalen-Johansen estimator restricted to data points from that cluster. These weights come from an automatically learned kernel function that measures how similar any two data points are. On four standard competing risks datasets, we show that DKAJ is competitive with state-of-the-art baselines while being able to provide visualizations to assist model interpretation.

</details>


### [213] [CAMO: Causality-Guided Adversarial Multimodal Domain Generalization for Crisis Classification](https://arxiv.org/abs/2512.08071)
*Pingchuan Ma,Chengshuai Zhao,Bohan Jiang,Saketh Vishnubhatla,Ujun Jeong,Alimohammad Beigi,Adrienne Raglin,Huan Liu*

Main category: cs.LG

TL;DR: A causality-guided multimodal domain generalization framework for crisis classification in social media that improves generalization to unseen disaster types by disentangling causal features and aligning multimodal representations.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal crisis classification methods suffer from poor generalization to unseen crisis types due to failure to disentangle spurious vs. causal features and inability to align heterogeneous modality representations, preventing adaptation of single-modality domain generalization techniques.

Method: Proposes a causality-guided multimodal domain generalization (MMDG) framework combining adversarial disentanglement with unified representation learning. Uses adversarial objective to focus on domain-invariant causal features and aligns multimodal features in shared latent space.

Result: The approach achieves best performance in unseen disaster scenarios across different datasets, demonstrating superior generalization compared to existing methods.

Conclusion: The causality-guided MMDG framework effectively addresses generalization challenges in multimodal crisis classification by disentangling causal features and enabling seamless extension of single-modality DG strategies to multimodal settings.

Abstract: Crisis classification in social media aims to extract actionable disaster-related information from multimodal posts, which is a crucial task for enhancing situational awareness and facilitating timely emergency responses. However, the wide variation in crisis types makes achieving generalizable performance across unseen disasters a persistent challenge. Existing approaches primarily leverage deep learning to fuse textual and visual cues for crisis classification, achieving numerically plausible results under in-domain settings. However, they exhibit poor generalization across unseen crisis types because they 1. do not disentangle spurious and causal features, resulting in performance degradation under domain shift, and 2. fail to align heterogeneous modality representations within a shared space, which hinders the direct adaptation of established single-modality domain generalization (DG) techniques to the multimodal setting. To address these issues, we introduce a causality-guided multimodal domain generalization (MMDG) framework that combines adversarial disentanglement with unified representation learning for crisis classification. The adversarial objective encourages the model to disentangle and focus on domain-invariant causal features, leading to more generalizable classifications grounded in stable causal mechanisms. The unified representation aligns features from different modalities within a shared latent space, enabling single-modality DG strategies to be seamlessly extended to multimodal learning. Experiments on the different datasets demonstrate that our approach achieves the best performance in unseen disaster scenarios.

</details>


### [214] [Unveiling Latent Knowledge in Chemistry Language Models through Sparse Autoencoders](https://arxiv.org/abs/2512.08077)
*Jaron Cohen,Alexander G. Hasson,Sara Tanovic*

Main category: cs.LG

TL;DR: Researchers use sparse autoencoders to extract interpretable features from chemistry language models, revealing they encode rich chemical knowledge including structural motifs, properties, and drug classes.


<details>
  <summary>Details</summary>
Motivation: Interpretability remains a critical challenge in machine learning, especially as generative models are used in high-stakes applications like drug and material discovery. While chemistry language models show impressive capabilities, their internal representations of chemical knowledge are poorly understood.

Method: Extend sparse autoencoder techniques to uncover and examine interpretable features within chemistry language models. Apply this methodology to the Foundation Models for Materials (FM4M) SMI-TED chemistry foundation model to extract semantically meaningful latent features and analyze their activation patterns across diverse molecular datasets.

Result: The models encode a rich landscape of chemical concepts. Researchers identify correlations between specific latent features and distinct domains of chemical knowledge, including structural motifs, physicochemical properties, and pharmacological drug classes.

Conclusion: The approach provides a generalizable framework for uncovering latent knowledge in chemistry-focused AI systems, with implications for both foundational understanding and practical deployment, potentially accelerating computational chemistry research.

Abstract: Since the advent of machine learning, interpretability has remained a persistent challenge, becoming increasingly urgent as generative models support high-stakes applications in drug and material discovery. Recent advances in large language model (LLM) architectures have yielded chemistry language models (CLMs) with impressive capabilities in molecular property prediction and molecular generation. However, how these models internally represent chemical knowledge remains poorly understood. In this work, we extend sparse autoencoder techniques to uncover and examine interpretable features within CLMs. Applying our methodology to the Foundation Models for Materials (FM4M) SMI-TED chemistry foundation model, we extract semantically meaningful latent features and analyse their activation patterns across diverse molecular datasets. Our findings reveal that these models encode a rich landscape of chemical concepts. We identify correlations between specific latent features and distinct domains of chemical knowledge, including structural motifs, physicochemical properties, and pharmacological drug classes. Our approach provides a generalisable framework for uncovering latent knowledge in chemistry-focused AI systems. This work has implications for both foundational understanding and practical deployment; with the potential to accelerate computational chemistry research.

</details>


### [215] [Complexity of One-Dimensional ReLU DNNs](https://arxiv.org/abs/2512.08091)
*Jonathan Kogan,Hayden Jananthan,Jeremy Kepner*

Main category: cs.LG

TL;DR: 1D ReLU networks' expressivity measured by linear regions: expected count grows linearly with total neurons in infinite-width limit, plus a function-adaptive sparsity metric.


<details>
  <summary>Details</summary>
Motivation: To understand the expressive power of 1D ReLU neural networks by analyzing their linear regions, which characterize piecewise linear functions, and to develop better metrics for network efficiency.

Method: Analyze randomly initialized fully connected 1D ReLU networks with He scaling and nonzero bias. Derive asymptotic formula for expected number of linear regions in infinite-width limit. Propose function-adaptive sparsity measure comparing network's regions to minimal needed for target approximation.

Result: Expected linear regions grows as sum of neurons across all layers plus small order term plus 1. Provides precise asymptotic characterization of network expressivity through linear region count.

Conclusion: Linear region analysis reveals fundamental expressivity properties of 1D ReLU networks. The proposed sparsity measure offers better assessment of network efficiency relative to approximation tasks.

Abstract: We study the expressivity of one-dimensional (1D) ReLU deep neural networks through the lens of their linear regions. For randomly initialized, fully connected 1D ReLU networks (He scaling with nonzero bias) in the infinite-width limit, we prove that the expected number of linear regions grows as $\sum_{i = 1}^L n_i + \mathop{o}\left(\sum_{i = 1}^L{n_i}\right) + 1$, where $n_\ell$ denotes the number of neurons in the $\ell$-th hidden layer. We also propose a function-adaptive notion of sparsity that compares the expected regions used by the network to the minimal number needed to approximate a target within a fixed tolerance.

</details>


### [216] [Training LLMs for Honesty via Confessions](https://arxiv.org/abs/2512.08093)
*Manas Joglekar,Jeremy Chen,Gabriel Wu,Jason Yosinski,Jasmine Wang,Boaz Barak,Amelia Glaese*

Main category: cs.LG

TL;DR: A method to elicit honest confessions from LLMs about their shortcomings by training them to provide separate confession outputs that are rewarded solely for honesty, enabling better monitoring and intervention.


<details>
  <summary>Details</summary>
Motivation: LLMs can be dishonest about their actions and beliefs due to reinforcement learning issues, potentially covering up misbehavior or overstating confidence. There's a need for methods to get honest self-reports of model shortcomings.

Method: Train models to produce "confessions" - separate outputs after main answers that fully account for compliance with policies. Confession rewards are based solely on honesty, independent of main answer rewards, incentivizing truthful disclosure of misbehavior.

Result: When models lie or omit shortcomings in main answers, they often confess honestly to these behaviors. Confession honesty modestly improves with training. Confessions enable inference-time interventions like monitoring, rejection sampling, and issue surfacing.

Conclusion: Confession-based training provides a viable approach to elicit honest self-reports from LLMs about their shortcomings, especially for egregious misbehavior, enabling better oversight and intervention capabilities.

Abstract: Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.
  In this work we propose a method for eliciting an honest expression of an LLM's shortcomings via a self-reported *confession*. A confession is an output, provided upon request after a model's original answer, that is meant to serve as a full account of the model's compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer's reward. As long as the "path of least resistance" for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Our findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior.
  To demonstrate the viability of our approach, we train GPT-5-Thinking to produce confessions, and we evaluate its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. We find that when the model lies or omits shortcomings in its "main" answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user.

</details>


### [217] [Scalable Offline Model-Based RL with Action Chunks](https://arxiv.org/abs/2512.08108)
*Kwanyoung Park,Seohong Park,Youngwoon Lee,Sergey Levine*

Main category: cs.LG

TL;DR: MAC uses action-chunk models and rejection sampling to improve offline RL for long-horizon tasks by reducing compounding model errors and preventing model exploitation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the scalability challenges of model-based RL for complex, long-horizon tasks in offline settings, where traditional approaches suffer from compounding model errors when using long rollouts for value expansion.

Method: Proposes Model-based RL with Action Chunks (MAC): 1) Uses action-chunk models that predict future states from sequences of actions instead of single actions to reduce compounding errors; 2) Employs rejection sampling from an expressive behavioral action-chunk policy instead of direct policy optimization to prevent model exploitation from out-of-distribution actions.

Result: MAC achieves state-of-the-art performance among offline model-based RL algorithms, particularly excelling on challenging long-horizon tasks, as demonstrated through experiments on large-scale datasets with up to 100M transitions.

Conclusion: Action-chunk modeling combined with rejection sampling provides an effective recipe for scalable offline model-based RL that can handle complex, long-horizon tasks by mitigating compounding model errors and preventing exploitation of model inaccuracies.

Abstract: In this paper, we study whether model-based reinforcement learning (RL), in particular model-based value expansion, can provide a scalable recipe for tackling complex, long-horizon tasks in offline RL. Model-based value expansion fits an on-policy value function using length-n imaginary rollouts generated by the current policy and a learned dynamics model. While larger n reduces bias in value bootstrapping, it amplifies accumulated model errors over long horizons, degrading future predictions. We address this trade-off with an \emph{action-chunk} model that predicts a future state from a sequence of actions (an "action chunk") instead of a single action, which reduces compounding errors. In addition, instead of directly training a policy to maximize rewards, we employ rejection sampling from an expressive behavioral action-chunk policy, which prevents model exploitation from out-of-distribution actions. We call this recipe \textbf{Model-Based RL with Action Chunks (MAC)}. Through experiments on highly challenging tasks with large-scale datasets of up to 100M transitions, we show that MAC achieves the best performance among offline model-based RL algorithms, especially on challenging long-horizon tasks.

</details>


### [218] [Balanced Accuracy: The Right Metric for Evaluating LLM Judges - Explained through Youden's J statistic](https://arxiv.org/abs/2512.08121)
*Stephane Collot,Colin Fraser,Justin Zhao,William F. Shen,Timon Willi,Ilias Leontiadis*

Main category: cs.LG

TL;DR: The paper argues that Balanced Accuracy (or Youden's J statistic) should be used instead of traditional metrics like Accuracy, Precision, and F1 for selecting classifiers in LLM evaluation, as these traditional metrics can distort prevalence estimates when comparing models.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluation relies on classifiers (LLM-as-a-judge or human annotators) to estimate behavior prevalence rates for model comparison. Traditional classifier selection metrics (Accuracy, Precision, F1) are problematic because they're sensitive to class imbalance and arbitrary positive class choices, potentially favoring judges that distort prevalence estimates.

Method: The authors propose using Youden's J statistic (or equivalently, Balanced Accuracy) for classifier selection. They provide theoretical justification showing J is aligned with choosing the best judge for model comparison, and demonstrate through analytical arguments, empirical examples, and simulations that Balanced Accuracy leads to better, more robust classifier selection.

Result: The paper shows that Balanced Accuracy/Youden's J statistic is theoretically superior for selecting classifiers in LLM evaluation contexts, and empirical evidence demonstrates it leads to more reliable prevalence estimates when comparing models.

Conclusion: For trustworthy LLM evaluation and model comparison, classifier selection should use Balanced Accuracy or Youden's J statistic rather than traditional metrics like Accuracy, Precision, and F1, as these traditional metrics can systematically distort prevalence estimates.

Abstract: Rigorous evaluation of large language models (LLMs) relies on comparing models by the prevalence of desirable or undesirable behaviors, such as task pass rates or policy violations. These prevalence estimates are produced by a classifier, either an LLM-as-a-judge or human annotators, making the choice of classifier central to trustworthy evaluation. Common metrics used for this choice, such as Accuracy, Precision, and F1, are sensitive to class imbalance and to arbitrary choices of positive class, and can favor judges that distort prevalence estimates. We show that Youden's $J$ statistic is theoretically aligned with choosing the best judge to compare models, and that Balanced Accuracy is an equivalent linear transformation of $J$. Through both analytical arguments and empirical examples and simulations, we demonstrate how selecting judges using Balanced Accuracy leads to better, more robust classifier selection.

</details>


### [219] [Long-only cryptocurrency portfolio management by ranking the assets: a neural network approach](https://arxiv.org/abs/2512.08124)
*Zijiang Yang*

Main category: cs.LG

TL;DR: Proposes a neural network-based cryptocurrency portfolio management method that predicts relative returns (ranks) across multiple cryptocurrencies rather than individual price movements, achieving 64.26% annualized return with Sharpe ratio of 1.01 over 3.5 years.


<details>
  <summary>Details</summary>
Motivation: Existing cryptocurrency portfolio methods typically focus on predicting individual cryptocurrency price movements independently, ignoring cross-sectional relationships between cryptocurrencies. This paper aims to leverage relative performance relationships across multiple cryptocurrencies for more effective portfolio management.

Method: Uses neural networks to predict the rank of future returns across a group of cryptocurrencies at each time step, then allocates portfolio weights based on these predicted rankings. The approach analyzes cross-sectional relationships rather than treating cryptocurrencies independently.

Result: Backtesting on real daily cryptocurrency data from May 2020 to Nov 2023 shows the method outperforms existing approaches, achieving Sharpe ratio of 1.01 and annualized return of 64.26%. The method remains robust across bullish, bearish, and stagnant market conditions and is resilient to transaction fee increases.

Conclusion: The proposed cross-sectional ranking approach using neural networks provides an effective cryptocurrency portfolio management strategy that leverages relative relationships between cryptocurrencies, delivering strong risk-adjusted returns across diverse market conditions.

Abstract: This paper will propose a novel machine learning based portfolio management method in the context of the cryptocurrency market. Previous researchers mainly focus on the prediction of the movement for specific cryptocurrency such as the bitcoin(BTC) and then trade according to the prediction. In contrast to the previous work that treats the cryptocurrencies independently, this paper manages a group of cryptocurrencies by analyzing the relative relationship. Specifically, in each time step, we utilize the neural network to predict the rank of the future return of the managed cryptocurrencies and place weights accordingly. By incorporating such cross-sectional information, the proposed methods is shown to profitable based on the backtesting experiments on the real daily cryptocurrency market data from May, 2020 to Nov, 2023. During this 3.5 years, the market experiences the full cycle of bullish, bearish and stagnant market conditions. Despite under such complex market conditions, the proposed method outperforms the existing methods and achieves a Sharpe ratio of 1.01 and annualized return of 64.26%. Additionally, the proposed method is shown to be robust to the increase of transaction fee.

</details>


### [220] [Improving the Sensitivity of Backdoor Detectors via Class Subspace Orthogonalization](https://arxiv.org/abs/2512.08129)
*Guangmingmei Yang,David J. Miller,George Kesidis*

Main category: cs.LG

TL;DR: CSO improves backdoor detection by suppressing intrinsic class features to reveal subtle backdoor triggers that traditional outlier-based methods miss.


<details>
  <summary>Details</summary>
Motivation: Existing backdoor detection methods fail when: 1) some non-target classes naturally achieve extreme detection statistics due to being easily discriminable, and 2) backdoors are subtle with weak triggers relative to intrinsic class features.

Method: Proposes Class Subspace Orthogonalization (CSO) - a constrained optimization that suppresses intrinsic features while optimizing detection statistics using clean examples, revealing backdoor contributions that remain after feature suppression.

Result: CSO achieves more sensitive detection against challenging mixed-label and adaptive attacks by distinguishing target classes (with backdoor trigger contributions) from non-target classes (only intrinsic features).

Conclusion: CSO provides a plug-and-play approach that overcomes limitations of outlier-based detection by separating backdoor trigger contributions from intrinsic class features, enabling detection of subtle backdoors.

Abstract: Most post-training backdoor detection methods rely on attacked models exhibiting extreme outlier detection statistics for the target class of an attack, compared to non-target classes. However, these approaches may fail: (1) when some (non-target) classes are easily discriminable from all others, in which case they may naturally achieve extreme detection statistics (e.g., decision confidence); and (2) when the backdoor is subtle, i.e., with its features weak relative to intrinsic class-discriminative features. A key observation is that the backdoor target class has contributions to its detection statistic from both the backdoor trigger and from its intrinsic features, whereas non-target classes only have contributions from their intrinsic features. To achieve more sensitive detectors, we thus propose to suppress intrinsic features while optimizing the detection statistic for a given class. For non-target classes, such suppression will drastically reduce the achievable statistic, whereas for the target class the (significant) contribution from the backdoor trigger remains. In practice, we formulate a constrained optimization problem, leveraging a small set of clean examples from a given class, and optimizing the detection statistic while orthogonalizing with respect to the class's intrinsic features. We dub this plug-and-play approach Class Subspace Orthogonalization (CSO) and assess it against challenging mixed-label and adaptive attacks.

</details>


### [221] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models I: The Task-Query Architecture](https://arxiv.org/abs/2512.08130)
*Gary Ackerman,Brandon Behlendorf,Zachary Kallenborn,Sheriff Almakki,Doug Clifford,Jenna LaTourette,Hayley Peterson,Noah Sheinbaum,Olivia Shoemaker,Anna Wetzel*

Main category: cs.LG

TL;DR: A framework for benchmarking AI models' biosecurity risks, starting with bacterial threats, using a hierarchical task-query architecture to assess both technical and operational risks across different adversary capabilities.


<details>
  <summary>Details</summary>
Motivation: There's a need to quantify and mitigate biosecurity risks from rapidly-evolving frontier AI models, especially LLMs, which could facilitate bioterrorism or biological weapons access. Current benchmarks often overlook key threat aspects like different actor capabilities and operational risk factors.

Method: Developed a Biothreat Benchmark Generation (BBG) Framework with a hierarchical structure of biothreat categories, elements, and tasks. Created a Bacterial Biothreat Schema as the first component, which serves as a task-query architecture for bacterial threats specifically. The approach accounts for different actor capability levels and operational risk factors.

Result: The paper presents the Bacterial Biothreat Schema - a hierarchical structure for evaluating bacterial biological risks from LLMs across multiple aggregation levels. This captures the full scope of technical and operational requirements for biological adversaries and accounts for a wide spectrum of adversary capabilities.

Conclusion: The BBG Framework offers a robust, re-usable structure for evaluating bacterial biological risks from LLMs. Future work will focus on turning queries into model prompts and implementing the benchmarks for model evaluation, with the overall goal of providing reliable measurement of biosecurity risk uplift from AI models.

Abstract: Both model developers and policymakers seek to quantify and mitigate the risk of rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons. An important element of such efforts is the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper describes the first component of a novel Biothreat Benchmark Generation (BBG) Framework. The BBG approach is designed to help model developers and evaluators reliably measure and assess the biosecurity risk uplift and general harm potential of existing and future AI models, while accounting for key aspects of the threat itself that are often overlooked in other benchmarking efforts, including different actor capability levels, and operational (in addition to purely technical) risk factors. As a pilot, the BBG is first being developed to address bacterial biological threats only. The BBG is built upon a hierarchical structure of biothreat categories, elements and tasks, which then serves as the basis for the development of task-aligned queries. This paper outlines the development of this biothreat task-query architecture, which we have named the Bacterial Biothreat Schema, while future papers will describe follow-on efforts to turn queries into model prompts, as well as how the resulting benchmarks can be implemented for model evaluation. Overall, the BBG Framework, including the Bacterial Biothreat Schema, seeks to offer a robust, re-usable structure for evaluating bacterial biological risks arising from LLMs across multiple levels of aggregation, which captures the full scope of technical and operational requirements for biological adversaries, and which accounts for a wide spectrum of biological adversary capabilities.

</details>


### [222] [Robust Agents in Open-Ended Worlds](https://arxiv.org/abs/2512.08139)
*Mikayel Samvelyan*

Main category: cs.LG

TL;DR: This thesis develops methods for training robust AI agents that generalize to novel environments, out-of-distribution inputs, and multi-agent interactions using open-endedness and multi-agent learning approaches.


<details>
  <summary>Details</summary>
Motivation: As AI becomes more prevalent, there's a critical need for agents that can adapt to changing, open-ended environments and generalize beyond their training data to handle unseen scenarios and interactions.

Method: Four main approaches: 1) MiniHack framework for procedural content generation of diverse RL environments, 2) Maestro method for adversarial curriculum generation in two-player zero-sum games, 3) Quality-diversity methods to identify vulnerabilities in pre-trained RL policies in football domain, and 4) Evolutionary search to generate adversarial prompts for testing LLM robustness.

Result: Developed frameworks and methods for creating diverse test environments, generating adversarial training curricula, systematically identifying policy vulnerabilities, and testing LLM robustness against adversarial inputs.

Conclusion: The work provides foundational approaches for advancing AI robustness, enabling development of agents that can adapt to evolving environments and thrive in unforeseen challenges and interactions.

Abstract: The growing prevalence of artificial intelligence (AI) in various applications underscores the need for agents that can successfully navigate and adapt to an ever-changing, open-ended world. A key challenge is ensuring these AI agents are robust, excelling not only in familiar settings observed during training but also effectively generalising to previously unseen and varied scenarios. In this thesis, we harness methodologies from open-endedness and multi-agent learning to train and evaluate robust AI agents capable of generalising to novel environments, out-of-distribution inputs, and interactions with other co-player agents. We begin by introducing MiniHack, a sandbox framework for creating diverse environments through procedural content generation. Based on the game of NetHack, MiniHack enables the construction of new tasks for reinforcement learning (RL) agents with a focus on generalisation. We then present Maestro, a novel approach for generating adversarial curricula that progressively enhance the robustness and generality of RL agents in two-player zero-sum games. We further probe robustness in multi-agent domains, utilising quality-diversity methods to systematically identify vulnerabilities in state-of-the-art, pre-trained RL policies within the complex video game football domain, characterised by intertwined cooperative and competitive dynamics. Finally, we extend our exploration of robustness to the domain of LLMs. Here, our focus is on diagnosing and enhancing the robustness of LLMs against adversarial prompts, employing evolutionary search to generate a diverse range of effective inputs that aim to elicit undesirable outputs from an LLM. This work collectively paves the way for future advancements in AI robustness, enabling the development of agents that not only adapt to an ever-evolving world but also thrive in the face of unforeseen challenges and interactions.

</details>


### [223] [PolyLingua: Margin-based Inter-class Transformer for Robust Cross-domain Language Detection](https://arxiv.org/abs/2512.08143)
*Ali Lotfi Rezaabad,Bikram Khanal,Shashwat Chaurasia,Lu Zeng,Dezhi Hong,Hossein Beshashati,Thomas Butler,Megan Ganji*

Main category: cs.LG

TL;DR: PolyLingua is a lightweight Transformer model for language identification that uses two-level contrastive learning to achieve high accuracy on challenging cases like music requests with code-switching, outperforming larger models while being 10x smaller.


<details>
  <summary>Details</summary>
Motivation: Language identification is critical for multilingual systems but existing tools struggle with key cases like music requests where song titles and user languages differ. Open-source tools are fast but less accurate, while LLMs are accurate but too costly for low-latency or low-resource settings.

Method: PolyLingua uses a lightweight Transformer-based model with a two-level contrastive learning framework combining instance-level separation and class-level alignment with adaptive margins, producing compact and well-separated embeddings even for closely related languages.

Result: On Amazon Massive (multilingual digital assistant utterances) and a Song dataset (music requests with frequent code-switching), PolyLingua achieves 99.25% F1 and 98.15% F1 respectively, surpassing Sonnet 3.5 while using 10x fewer parameters.

Conclusion: PolyLingua provides an ideal solution for compute- and latency-constrained environments, offering high accuracy language identification with significantly reduced model size compared to existing approaches.

Abstract: Language identification is a crucial first step in multilingual systems such as chatbots and virtual assistants, enabling linguistically and culturally accurate user experiences. Errors at this stage can cascade into downstream failures, setting a high bar for accuracy. Yet, existing language identification tools struggle with key cases--such as music requests where the song title and user language differ. Open-source tools like LangDetect, FastText are fast but less accurate, while large language models, though effective, are often too costly for low-latency or low-resource settings. We introduce PolyLingua, a lightweight Transformer-based model for in-domain language detection and fine-grained language classification. It employs a two-level contrastive learning framework combining instance-level separation and class-level alignment with adaptive margins, yielding compact and well-separated embeddings even for closely related languages. Evaluated on two challenging datasets--Amazon Massive (multilingual digital assistant utterances) and a Song dataset (music requests with frequent code-switching)--PolyLingua achieves 99.25% F1 and 98.15% F1, respectively, surpassing Sonnet 3.5 while using 10x fewer parameters, making it ideal for compute- and latency-constrained environments.

</details>


### [224] [TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models](https://arxiv.org/abs/2512.08153)
*Zheng Ding,Weirui Ye*

Main category: cs.LG

TL;DR: TreeGRPO is an efficient RL framework for generative model alignment that recasts denoising as a search tree, achieving 2.4Ã faster training with better sample efficiency and fine-grained credit assignment.


<details>
  <summary>Details</summary>
Motivation: RL post-training is essential for aligning generative models with human preferences, but its high computational cost limits widespread adoption. Current methods suffer from inefficiency in sample usage and coarse credit assignment.

Method: TreeGRPO recasts the denoising process as a search tree, branching from shared initial noise samples to generate multiple candidate trajectories while reusing common prefixes. It enables fine-grained credit assignment via reward backpropagation with step-specific advantages and amortized computation through multi-child branching.

Result: TreeGRPO achieves 2.4Ã faster training while establishing a superior Pareto frontier in efficiency-reward trade-off. It consistently outperforms GRPO baselines across multiple benchmarks and reward models for both diffusion and flow-based models.

Conclusion: TreeGRPO provides a scalable and effective pathway for RL-based visual generative model alignment by dramatically improving training efficiency through tree-structured search and fine-grained credit assignment.

Abstract: Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \emph{High sample efficiency}, achieving better performance under same training samples (2) \emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \textbf{2.4$\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.

</details>


### [225] [LayerPipe2: Multistage Pipelining and Weight Recompute via Improved Exponential Moving Average for Training Neural Networks](https://arxiv.org/abs/2512.08160)
*Nanda K. Unnikrishnan,Keshab K. Parhi*

Main category: cs.LG

TL;DR: LayerPipe2 formally derives gradient delay requirements for pipelined neural network training using variable delayed gradient adaptation and retiming, showing delay patterns depend on network structure and enabling memory-efficient implementation.


<details>
  <summary>Details</summary>
Motivation: Previous work (LayerPipe) empirically accelerated training by overlapping forward/backward computation but lacked principled understanding of gradient delay requirements at each layer for effective pipelining.

Method: Formally derives LayerPipe using variable delayed gradient adaptation and retiming theory; identifies legal delay insertion points; develops pipeline-aware moving average to reconstruct historical weights instead of storing them explicitly.

Result: Shows delay requirements follow directly from network structure: inner layers need fewer delays, outer layers need longer delays; when pipelining every layer, delay depends only on downstream stages; group pipelining shares delay assignments.

Conclusion: Provides principled framework for constructing LayerPipe architectures, predicting delay requirements, and mitigating storage burden, enabling scalable pipelined training with controlled communication-computation tradeoffs.

Abstract: In our prior work, LayerPipe, we had introduced an approach to accelerate training of convolutional, fully connected, and spiking neural networks by overlapping forward and backward computation. However, despite empirical success, a principled understanding of how much gradient delay needs to be introduced at each layer to achieve desired level of pipelining was not addressed. This paper, LayerPipe2, fills that gap by formally deriving LayerPipe using variable delayed gradient adaptation and retiming. We identify where delays may be legally inserted and show that the required amount of delay follows directly from the network structure where inner layers require fewer delays and outer layers require longer delays. When pipelining is applied at every layer, the amount of delay depends only on the number of remaining downstream stages. When layers are pipelined in groups, all layers in the group share the same assignment of delays. These insights not only explain previously observed scheduling patterns but also expose an often overlooked challenge that pipelining implicitly requires storage of historical weights. We overcome this storage bottleneck by developing a pipeline--aware moving average that reconstructs the required past states rather than storing them explicitly. This reduces memory cost without sacrificing the accuracy guarantees that makes pipelined learning viable. The result is a principled framework that illustrates how to construct LayerPipe architectures, predicts their delay requirements, and mitigates their storage burden, thereby enabling scalable pipelined training with controlled communication computation tradeoffs.

</details>


### [226] [MobileFineTuner: A Unified End-to-End Framework for Fine-Tuning LLMs on Mobile Phones](https://arxiv.org/abs/2512.08211)
*Jiaxiang Geng,Lunyu Zhao,Yiyi Lu,Bing Luo*

Main category: cs.LG

TL;DR: MobileFineTuner is an open-source framework that enables end-to-end LLM fine-tuning directly on commodity mobile phones, addressing the gap in practical on-device training solutions.


<details>
  <summary>Details</summary>
Motivation: As high-quality public data for LLMs becomes exhausted, on-device fine-tuning offers a way to leverage private user data while preserving privacy. Existing approaches are simulation-based or rely on IoT/PCs, leaving commodity mobile phones unexplored, with no open-source framework for practical mobile fine-tuning.

Method: MobileFineTuner is a unified framework supporting both full-parameters fine-tuning (Full-FT) and parameter-efficient fine-tuning (PEFT). It introduces system-level optimizations including parameter sharding, gradient accumulation, and energy-aware computation scheduling to address mobile phone memory and energy limitations.

Result: The framework successfully fine-tuned GPT-2, Gemma 3, and Qwen 2.5 on real mobile phones. Extensive experiments and ablation studies validated the effectiveness of the proposed optimizations.

Conclusion: MobileFineTuner establishes a viable foundation for future research on on-device LLM training, demonstrating practical LLM fine-tuning on commodity mobile phones while addressing privacy concerns through local data processing.

Abstract: Mobile phones are the most ubiquitous end devices, generating vast amounts of human-authored data and serving as the primary platform for end-side applications. As high-quality public data for large language models (LLMs) approaches exhaustion, on-device fine-tuning provides an opportunity to leverage private user data while preserving privacy. However, existing approaches are predominantly simulation-based or rely on IoT devices and PCs, leaving commodity mobile phones largely unexplored. A key gap is the absence of an open-source framework that enables practical LLM fine-tuning on mobile phones. We present MobileFineTuner, a unified open-source framework that enables end-to-end LLM fine-tuning directly on commodity mobile phones. MobileFineTuner is designed for efficiency, scalability, and usability, supporting full-parameters fine-tuning (Full-FT) and parameter-efficient fine-tuning (PEFT). To address the memory and energy limitations inherent to mobile phones, we introduce system-level optimizations including parameter sharding, gradient accumulation, and energy-aware computation scheduling. We demonstrate the practicality of MobileFineTuner by fine-tuning GPT-2, Gemma 3, and Qwen 2.5 on real mobile phones. Extensive experiments and ablation studies validate the effectiveness of the proposed optimizations and establish MobileFineTuner as a viable foundation for future research on on-device LLM training.

</details>


### [227] [Correction of Decoupled Weight Decay](https://arxiv.org/abs/2512.08217)
*Jason Chuan-Chih Chou*

Main category: cs.LG

TL;DR: The paper challenges the conventional wisdom that decoupled weight decay should be proportional to learning rate (Î³) and shows that it should instead be proportional to Î³Â² for stable weight norms and better training dynamics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to question the long-standing assumption that decoupled weight decay should be proportional to learning rate (Î³) in optimizers like AdamW, and to provide a theoretical foundation for why it should instead be proportional to Î³Â² for stable training dynamics.

Method: The authors analyze training dynamics by assuming updates become independent of weights at steady state. They derive that decoupled weight decay â Î³Â² results in stable weight norm, and empirically verify that the Total Update Contribution (TUC) under Scion optimizer is better characterized by momentum-dependent effective learning rate.

Result: The paper shows that decoupled weight decay â Î³Â² leads to stable weight and gradient norms, allows better control of training dynamics, and improves model performance compared to the conventional â Î³ setting.

Conclusion: The conventional assumption about decoupled weight decay being proportional to learning rate is incorrect; it should be proportional to Î³Â² for stable weight norms and optimal training dynamics, which improves model performance.

Abstract: Decoupled weight decay, solely responsible for the performance advantage of AdamW over Adam, has long been set to proportional to learning rate $Î³$ without questioning. Some researchers have recently challenged such assumption and argued that decoupled weight decay should be set $\propto Î³^2$ instead based on orthogonality arguments at steady state. To the contrary, we find that eliminating the contribution of the perpendicular component of the update to the weight norm leads to little change to the training dynamics. Instead, we derive that decoupled weight decay $\propto Î³^2$ results in stable weight norm based on the simple assumption that updates become independent of the weights at steady state, regardless of the nature of the optimizer. Based on the same assumption, we derive and empirically verify that the Total Update Contribution (TUC) of a minibatch under the Scion optimizer is better characterized by the momentum-dependent effective learning rate whose optimal value transfers and we show that decoupled weight decay $\propto Î³^2$ leads to stable weight and gradient norms and allows us to better control the training dynamics and improve the model performance.

</details>


### [228] [PR-CapsNet: Pseudo-Riemannian Capsule Network with Adaptive Curvature Routing for Graph Learning](https://arxiv.org/abs/2512.08218)
*Ye Qin,Jingchao Wang,Yang Shi,Haiying Huang,Junxu Li,Weijian Liu,Tinghui Chen,Jinghui Qin*

Main category: cs.LG

TL;DR: PR-CapsNet extends capsule networks to pseudo-Riemannian manifolds with adaptive curvature for better graph representation learning, outperforming SOTA models on node and graph classification tasks.


<details>
  <summary>Details</summary>
Motivation: Capsule Networks have strong graph representation capacity but perform poorly on complex real-world graphs due to fixed-curvature Euclidean space limitations. While pseudo-Riemannian manifolds provide good inductive biases for graph data, their integration with CapsNets remains underexplored.

Method: PR-CapsNet extends Euclidean capsule routing to pseudo-Riemannian manifolds with three key components: 1) Pseudo-Riemannian Tangent Space Routing decomposes capsule states into spherical-temporal and Euclidean-spatial subspaces, 2) Adaptive Curvature Routing fuses features from different curvature spaces using a learnable curvature tensor with geometric attention, and 3) Pseudo-Riemannian Capsule Classifier projects embeddings to tangent spaces with curvature-weighted softmax for classification.

Result: Extensive experiments on node and graph classification benchmarks show PR-CapsNet outperforms state-of-the-art models, validating its strong representation power for complex graph structures.

Conclusion: PR-CapsNet successfully integrates pseudo-Riemannian geometry with capsule networks, enabling adaptive curvature modeling that better captures hierarchical, cluster, and cyclic graph structures, leading to superior graph representation learning performance.

Abstract: Capsule Networks (CapsNets) show exceptional graph representation capacity via dynamic routing and vectorized hierarchical representations, but they model the complex geometries of real\-world graphs poorly by fixed\-curvature space due to the inherent geodesical disconnectedness issues, leading to suboptimal performance. Recent works find that non\-Euclidean pseudo\-Riemannian manifolds provide specific inductive biases for embedding graph data, but how to leverage them to improve CapsNets is still underexplored. Here, we extend the Euclidean capsule routing into geodesically disconnected pseudo\-Riemannian manifolds and derive a Pseudo\-Riemannian Capsule Network (PR\-CapsNet), which models data in pseudo\-Riemannian manifolds of adaptive curvature, for graph representation learning. Specifically, PR\-CapsNet enhances the CapsNet with Adaptive Pseudo\-Riemannian Tangent Space Routing by utilizing pseudo\-Riemannian geometry. Unlike single\-curvature or subspace\-partitioning methods, PR\-CapsNet concurrently models hierarchical and cluster or cyclic graph structures via its versatile pseudo\-Riemannian metric. It first deploys Pseudo\-Riemannian Tangent Space Routing to decompose capsule states into spherical\-temporal and Euclidean\-spatial subspaces with diffeomorphic transformations. Then, an Adaptive Curvature Routing is developed to adaptively fuse features from different curvature spaces for complex graphs via a learnable curvature tensor with geometric attention from local manifold properties. Finally, a geometric properties\-preserved Pseudo\-Riemannian Capsule Classifier is developed to project capsule embeddings to tangent spaces and use curvature\-weighted softmax for classification. Extensive experiments on node and graph classification benchmarks show PR\-CapsNet outperforms SOTA models, validating PR\-CapsNet's strong representation power for complex graph structures.

</details>


### [229] [Persistent Topological Structures and Cohomological Flows as a Mathematical Framework for Brain-Inspired Representation Learning](https://arxiv.org/abs/2512.08241)
*Preksha Girish,Rachana Mysore,Mahanthesha U,Shrey Kumar,Shipra Prashant*

Main category: cs.LG

TL;DR: A mathematical framework for brain-inspired representation learning using topological structures and cohomological flows, achieving better manifold consistency and noise resilience than existing methods.


<details>
  <summary>Details</summary>
Motivation: To establish a mathematically rigorous foundation for brain-inspired representation learning by capturing invariants across temporal, spatial, and functional brain states through topological structures.

Method: Reformulates neural computation as evolution of cochain maps over dynamic simplicial complexes, integrating algebraic topology with differential geometry to construct cohomological operators that generalize gradient-based learning.

Result: The model achieves superior manifold consistency and noise resilience compared to graph neural networks and manifold-based deep architectures, demonstrating stability, continuity, and structural preservation.

Conclusion: Establishes a coherent mathematical foundation for topology-driven representation learning by combining persistent homology, sheaf cohomology, and spectral Laplacians to analyze both synthetic and real neural data.

Abstract: This paper presents a mathematically rigorous framework for brain-inspired representation learning founded on the interplay between persistent topological structures and cohomological flows. Neural computation is reformulated as the evolution of cochain maps over dynamic simplicial complexes, enabling representations that capture invariants across temporal, spatial, and functional brain states. The proposed architecture integrates algebraic topology with differential geometry to construct cohomological operators that generalize gradient-based learning within a homological landscape. Synthetic data with controlled topological signatures and real neural datasets are jointly analyzed using persistent homology, sheaf cohomology, and spectral Laplacians to quantify stability, continuity, and structural preservation. Empirical results demonstrate that the model achieves superior manifold consistency and noise resilience compared to graph neural and manifold-based deep architectures, establishing a coherent mathematical foundation for topology-driven representation learning.

</details>


### [230] [SPROCKET: Extending ROCKET to Distance-Based Time-Series Transformations With Prototypes](https://arxiv.org/abs/2512.08246)
*Nicholas Harner*

Main category: cs.LG

TL;DR: SPROCKET introduces a prototype-based feature engineering strategy for time series classification that achieves comparable performance to existing convolutional algorithms and improves ensemble accuracy.


<details>
  <summary>Details</summary>
Motivation: Current time series classification is dominated by feature engineering approaches like ROCKET, which uses random kernel features. The authors aim to develop a more effective feature transformation strategy based on prototypes to enhance classification performance.

Method: SPROCKET implements a new feature engineering strategy based on prototypes rather than random kernels. It's combined with existing methods in the MR-HY-SP ensemble (MultiROCKET-HYDRA-SPROCKET).

Result: SPROCKET achieves comparable performance to existing convolutional algorithms on most UCR and UEA archives. The MR-HY-SP ensemble's average accuracy ranking exceeds HYDRA-MR, the previous best convolutional ensemble.

Conclusion: Prototype-based feature transformation can enhance both accuracy and robustness in time series classification, offering a promising alternative to random kernel approaches.

Abstract: Classical Time Series Classification algorithms are dominated by feature engineering strategies. One of the most prominent of these transforms is ROCKET, which achieves strong performance through random kernel features. We introduce SPROCKET (Selected Prototype Random Convolutional Kernel Transform), which implements a new feature engineering strategy based on prototypes. On a majority of the UCR and UEA Time Series Classification archives, SPROCKET achieves performance comparable to existing convolutional algorithms and the new MR-HY-SP ( MultiROCKET-HYDRA-SPROCKET) ensemble's average accuracy ranking exceeds HYDRA-MR, the previous best convolutional ensemble's performance. These experimental results demonstrate that prototype-based feature transformation can enhance both accuracy and robustness in time series classification.

</details>


### [231] [Wavelet-Accelerated Physics-Informed Quantum Neural Network for Multiscale Partial Differential Equations](https://arxiv.org/abs/2512.08256)
*Deepak Gupta,Himanshu Pandey,Ratikanta Behera*

Main category: cs.LG

TL;DR: Wavelet-based physics-informed quantum neural network framework for multiscale PDEs with sharp gradients and oscillatory behavior, eliminating automatic differentiation for faster training.


<details>
  <summary>Details</summary>
Motivation: Traditional PINNs and quantum-PINNs struggle with multiscale features and computational overhead from automatic differentiation, requiring improved methods for efficient multiscale PDE solving.

Method: Wavelet-accelerated physics-informed quantum neural network that incorporates wavelet multiresolution properties into quantum architecture, eliminating automatic differentiation to reduce computational complexity.

Result: Achieves superior accuracy with <5% trainable parameters vs classical wavelet-based PINNs, 3-5x speedup vs existing quantum PINNs, and faster convergence for multiscale problems.

Conclusion: Proposed framework effectively captures multiscale features with reduced computational cost, demonstrating potential for solving challenging oscillatory and multiscale PDE problems efficiently.

Abstract: This work proposes a wavelet-based physics-informed quantum neural network framework to efficiently address multiscale partial differential equations that involve sharp gradients, stiffness, rapid local variations, and highly oscillatory behavior. Traditional physics-informed neural networks (PINNs) have demonstrated substantial potential in solving differential equations, and their quantum counterparts, quantum-PINNs, exhibit enhanced representational capacity with fewer trainable parameters. However, both approaches face notable challenges in accurately solving multiscale features. Furthermore, their reliance on automatic differentiation for constructing loss functions introduces considerable computational overhead, resulting in longer training times. To overcome these challenges, we developed a wavelet-accelerated physics-informed quantum neural network that eliminates the need for automatic differentiation, significantly reducing computational complexity. The proposed framework incorporates the multiresolution property of wavelets within the quantum neural network architecture, thereby enhancing the network's ability to effectively capture both local and global features of multiscale problems. Numerical experiments demonstrate that our proposed method achieves superior accuracy while requiring less than five percent of the trainable parameters compared to classical wavelet-based PINNs, resulting in faster convergence. Moreover, it offers a speedup of three to five times compared to existing quantum PINNs, highlighting the potential of the proposed approach for efficiently solving challenging multiscale and oscillatory problems.

</details>


### [232] [Mathematical Foundations of Neural Tangents and Infinite-Width Networks](https://arxiv.org/abs/2512.08264)
*Rachana Mysore,Preksha Girish,Kavitha Jayaram,Shrey Kumar,Preksha Girish,Shravan Sanjeev Bagal,Kavitha Jayaram,Shreya Aravind Shastry*

Main category: cs.LG

TL;DR: NTK-ECRN architecture integrates Fourier features, residual connections, and stochastic depth to enable rigorous analysis of NTK dynamics in infinite-width neural networks, with theoretical bounds on kernel evolution and empirical validation.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between infinite-width neural network theory (via Neural Tangent Kernel) and practical deep learning architectures, enabling rigorous mathematical analysis of kernel evolution during training.

Method: Proposes NTK-ECRN architecture combining Fourier feature embeddings, residual connections with layerwise scaling, and stochastic depth. Theoretically analyzes NTK dynamics, derives bounds on kernel evolution, characterizes eigenvalue evolution, and links spectral properties to generalization and optimization stability.

Result: Theoretical bounds on NTK dynamics and eigenvalue evolution are derived. Empirical validation on synthetic and benchmark datasets confirms predicted kernel behavior and demonstrates improved training stability and generalization compared to baseline approaches.

Conclusion: Provides a comprehensive framework connecting infinite-width neural network theory with practical architectures, offering tools for rigorous analysis of training dynamics and establishing links between spectral properties, generalization, and optimization stability.

Abstract: We investigate the mathematical foundations of neural networks in the infinite-width regime through the Neural Tangent Kernel (NTK). We propose the NTK-Eigenvalue-Controlled Residual Network (NTK-ECRN), an architecture integrating Fourier feature embeddings, residual connections with layerwise scaling, and stochastic depth to enable rigorous analysis of kernel evolution during training. Our theoretical contributions include deriving bounds on NTK dynamics, characterizing eigenvalue evolution, and linking spectral properties to generalization and optimization stability. Empirical results on synthetic and benchmark datasets validate the predicted kernel behavior and demonstrate improved training stability and generalization. This work provides a comprehensive framework bridging infinite-width theory and practical deep-learning architectures.

</details>


### [233] [SOFA-FL: Self-Organizing Hierarchical Federated Learning with Adaptive Clustered Data Sharing](https://arxiv.org/abs/2512.08267)
*Yi Ni,Xinkun Wang,Han Zhang*

Main category: cs.LG

TL;DR: SOFA-FL is a self-organizing hierarchical federated learning framework that dynamically adapts to evolving environments through clustering, topology restructuring, and controlled data sharing.


<details>
  <summary>Details</summary>
Motivation: FL faces challenges with data heterogeneity and rigid fixed network topologies in evolving environments. Current approaches lack adaptability to changing data distributions and dynamic client relationships.

Method: Three core mechanisms: 1) DMAC for initial hierarchical clustering, 2) SHAPE for dynamic topology restructuring via grafting/pruning/consolidation/purification operations, 3) Adaptive Clustered Data Sharing for controlled partial data exchange.

Result: The framework enables hierarchical federated systems to self-organize and adapt over time, capturing dynamic client relationships and enhancing personalization without predetermined cluster structures.

Conclusion: SOFA-FL effectively addresses FL challenges in evolving environments by providing adaptive hierarchical structures that respond to data distribution changes while mitigating data heterogeneity.

Abstract: Federated Learning (FL) faces significant challenges in evolving environments, particularly regarding data heterogeneity and the rigidity of fixed network topologies. To address these issues, this paper proposes \textbf{SOFA-FL} (Self-Organizing Hierarchical Federated Learning with Adaptive Clustered Data Sharing), a novel framework that enables hierarchical federated systems to self-organize and adapt over time.
  The framework is built upon three core mechanisms: (1) \textbf{Dynamic Multi-branch Agglomerative Clustering (DMAC)}, which constructs an initial efficient hierarchical structure; (2) \textbf{Self-organizing Hierarchical Adaptive Propagation and Evolution (SHAPE)}, which allows the system to dynamically restructure its topology through atomic operations -- grafting, pruning, consolidation, and purification -- to adapt to changes in data distribution; and (3) \textbf{Adaptive Clustered Data Sharing}, which mitigates data heterogeneity by enabling controlled partial data exchange between clients and cluster nodes.
  By integrating these mechanisms, SOFA-FL effectively captures dynamic relationships among clients and enhances personalization capabilities without relying on predetermined cluster structures.

</details>


### [234] [gHAWK: Local and Global Structure Encoding for Scalable Training of Graph Neural Networks on Knowledge Graphs](https://arxiv.org/abs/2512.08274)
*Humera Sabir,Fatima Farooq,Ashraf Aboulnaga*

Main category: cs.LG

TL;DR: gHAWK is a scalable GNN framework for large knowledge graphs that precomputes structural features (Bloom filters for local structure and TransE embeddings for global position) to improve efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing message-passing GNNs struggle to scale to large knowledge graphs due to inefficient iterative message passing, especially in mini-batch training where nodes only see partial neighborhood views.

Method: Precompute structural features before GNN training: (a) Bloom filters to compactly encode local neighborhood structure, (b) TransE embeddings to represent each node's global position. Fuse these with domain-specific features to create node feature vectors that can be incorporated into any GNN technique.

Result: gHAWK achieves state-of-the-art accuracy and lower training time on both node property prediction and link prediction tasks, topping the OGB leaderboard for three graphs. It significantly reduces memory usage, accelerates convergence, and improves model accuracy.

Conclusion: By augmenting message-passing training with structural priors, gHAWK provides a scalable solution for large knowledge graphs that addresses the limitations of traditional GNN approaches while maintaining or improving performance.

Abstract: Knowledge Graphs (KGs) are a rich source of structured, heterogeneous data, powering a wide range of applications. A common approach to leverage this data is to train a graph neural network (GNN) on the KG. However, existing message-passing GNNs struggle to scale to large KGs because they rely on the iterative message passing process to learn the graph structure, which is inefficient, especially under mini-batch training, where a node sees only a partial view of its neighborhood. In this paper, we address this problem and present gHAWK, a novel and scalable GNN training framework for large KGs. The key idea is to precompute structural features for each node that capture its local and global structure before GNN training even begins. Specifically, gHAWK introduces a preprocessing step that computes: (a)~Bloom filters to compactly encode local neighborhood structure, and (b)~TransE embeddings to represent each node's global position in the graph. These features are then fused with any domain-specific features (e.g., text embeddings), producing a node feature vector that can be incorporated into any GNN technique. By augmenting message-passing training with structural priors, gHAWK significantly reduces memory usage, accelerates convergence, and improves model accuracy. Extensive experiments on large datasets from the Open Graph Benchmark (OGB) demonstrate that gHAWK achieves state-of-the-art accuracy and lower training time on both node property prediction and link prediction tasks, topping the OGB leaderboard for three graphs.

</details>


### [235] [Jacobian Aligned Random Forests](https://arxiv.org/abs/2512.08306)
*Sarwesh Rauniyar*

Main category: cs.LG

TL;DR: JARF (Jacobian-Aligned Random Forests) uses gradient information from an initial axis-aligned forest to compute a global linear preconditioner that rotates the feature space, enabling axis-aligned trees to capture complex decision boundaries without the computational cost of oblique forests.


<details>
  <summary>Details</summary>
Motivation: Axis-aligned decision trees are computationally efficient but struggle with rotated or interaction-dependent decision boundaries. Oblique forests can handle these but are computationally expensive and complex to implement. There's a need for a method that combines the simplicity of axis-aligned trees with the boundary-capturing ability of oblique methods.

Method: 1. Fit an initial axis-aligned forest to estimate class probabilities/regression outputs. 2. Compute finite-difference gradients of predictions with respect to each feature. 3. Aggregate gradients into an expected Jacobian outer product (generalizing EGOP). 4. Use this as a single global linear preconditioner to rotate the feature space. 5. Apply transformed data to a standard axis-aligned forest, preserving existing training pipelines.

Result: On tabular classification and regression benchmarks, JARF consistently improves axis-aligned forests and often matches or surpasses oblique baselines while improving training time. The method recovers much of the accuracy of oblique forests while maintaining the simplicity and robustness of axis-aligned trees.

Conclusion: Supervised preconditioning through gradient-based feature space rotation enables axis-aligned forests to capture complex decision boundaries without the computational overhead of oblique methods, offering a practical compromise between efficiency and accuracy for tabular data problems.

Abstract: Axis-aligned decision trees are fast and stable but struggle on datasets with rotated or interaction-dependent decision boundaries, where informative splits require linear combinations of features rather than single-feature thresholds. Oblique forests address this with per-node hyperplane splits, but at added computational cost and implementation complexity. We propose a simple alternative: JARF, Jacobian-Aligned Random Forests. Concretely, we first fit an axis-aligned forest to estimate class probabilities or regression outputs, compute finite-difference gradients of these predictions with respect to each feature, aggregate them into an expected Jacobian outer product that generalizes the expected gradient outer product (EGOP), and use it as a single global linear preconditioner for all inputs. This supervised preconditioner applies a single global rotation of the feature space, then hands the transformed data back to a standard axis-aligned forest, preserving off-the-shelf training pipelines while capturing oblique boundaries and feature interactions that would otherwise require many axis-aligned splits to approximate. The same construction applies to any model that provides gradients, though we focus on random forests and gradient-boosted trees in this work. On tabular classification and regression benchmarks, this preconditioning consistently improves axis-aligned forests and often matches or surpasses oblique baselines while improving training time. Our experimental results and theoretical analysis together indicate that supervised preconditioning can recover much of the accuracy of oblique forests while retaining the simplicity and robustness of axis-aligned trees.

</details>


### [236] [Minimizing Layerwise Activation Norm Improves Generalization in Federated Learning](https://arxiv.org/abs/2512.08314)
*M Yashwanth,Gaurav Kumar Nayak,Harsh Rangwani,Arya Singh,R. Venkatesh Babu,Anirban Chakraborty*

Main category: cs.LG

TL;DR: Proposes MAN regularization for Federated Learning to improve generalization by minimizing activation norms to converge to flat minima instead of sharp minima.


<details>
  <summary>Details</summary>
Motivation: Federated Learning often converges to 'sharp minima' which hurts model generalization. Need to improve generalization performance in FL setup by ensuring convergence to flat minima.

Method: Introduces flatness-constrained FL optimization by minimizing top eigenvalue of Hessian. Proposes MAN (Minimizes Activation's Norm) regularization technique that minimizes activation norm of each layer in client-side models to reduce Hessian eigenvalues.

Result: Theoretical proof that minimizing activation norm reduces top eigenvalue of layer-wise Hessian, decreasing overall Hessian's top eigenvalue. Application to existing FL techniques yields significant improvements and establishes new state-of-the-art.

Conclusion: MAN regularization effectively improves FL model generalization by ensuring convergence to flat minima through activation norm minimization, providing both theoretical guarantees and practical performance gains.

Abstract: Federated Learning (FL) is an emerging machine learning framework that enables multiple clients (coordinated by a server) to collaboratively train a global model by aggregating the locally trained models without sharing any client's training data. It has been observed in recent works that learning in a federated manner may lead the aggregated global model to converge to a 'sharp minimum' thereby adversely affecting the generalizability of this FL-trained model. Therefore, in this work, we aim to improve the generalization performance of models trained in a federated setup by introducing a 'flatness' constrained FL optimization problem. This flatness constraint is imposed on the top eigenvalue of the Hessian computed from the training loss. As each client trains a model on its local data, we further re-formulate this complex problem utilizing the client loss functions and propose a new computationally efficient regularization technique, dubbed 'MAN,' which Minimizes Activation's Norm of each layer on client-side models. We also theoretically show that minimizing the activation norm reduces the top eigenvalue of the layer-wise Hessian of the client's loss, which in turn decreases the overall Hessian's top eigenvalue, ensuring convergence to a flat minimum. We apply our proposed flatness-constrained optimization to the existing FL techniques and obtain significant improvements, thereby establishing new state-of-the-art.

</details>


### [237] [A Multivariate Bernoulli-Based Sampling Method for Multi-Label Data with Application to Meta-Research](https://arxiv.org/abs/2512.08371)
*Simon Chung,Colby J. Vorland,Donna L. Maney,Andrew W. Brown*

Main category: cs.LG

TL;DR: Novel weighted sampling algorithm for multi-label datasets that accounts for label dependencies to create balanced samples while preserving population characteristics.


<details>
  <summary>Details</summary>
Motivation: Multi-label datasets with non-mutually exclusive labels and varying frequencies create challenges for sampling - scarce labels need sufficient representation while maintaining known deviation from population frequencies.

Method: Uses multivariate Bernoulli distribution to model multi-label problems, estimates parameters from observed label frequencies, calculates weights for each label combination, and performs weighted sampling that accounts for label dependencies.

Result: Applied to Web of Science biomedical articles with 64 topic categories, the approach produced a more balanced sub-sample that preserved category frequency order, reduced frequency differences between most/least common categories, and enhanced representation of minority categories.

Conclusion: The proposed sampling algorithm effectively addresses multi-label sampling challenges by incorporating label dependencies, enabling creation of balanced samples while maintaining important population characteristics.

Abstract: Datasets may contain observations with multiple labels. If the labels are not mutually exclusive, and if the labels vary greatly in frequency, obtaining a sample that includes sufficient observations with scarcer labels to make inferences about those labels, and which deviates from the population frequencies in a known manner, creates challenges. In this paper, we consider a multivariate Bernoulli distribution as our underlying distribution of a multi-label problem. We present a novel sampling algorithm that takes label dependencies into account. It uses observed label frequencies to estimate multivariate Bernoulli distribution parameters and calculate weights for each label combination. This approach ensures the weighted sampling acquires target distribution characteristics while accounting for label dependencies. We applied this approach to a sample of research articles from Web of Science labeled with 64 biomedical topic categories. We aimed to preserve category frequency order, reduce frequency differences between most and least common categories, and account for category dependencies. This approach produced a more balanced sub-sample, enhancing the representation of minority categories.

</details>


### [238] [Fully Decentralized Certified Unlearning](https://arxiv.org/abs/2512.08443)
*Hithem Lamri,Michail Maniatakos*

Main category: cs.LG

TL;DR: RR-DU: A certified machine unlearning method for decentralized networks using random walks with gradient updates, noise injection, and trust region projection to remove specific data while maintaining privacy guarantees.


<details>
  <summary>Details</summary>
Motivation: Machine unlearning needs certified guarantees for privacy/data removal requests, but existing work focuses on centralized/federated settings. Decentralized networks (peer-to-peer without coordinator) remain underexplored for certified unlearning.

Method: RR-DU: Random-walk procedure where unlearning client performs projected gradient ascent on forget set, other clients perform geometrically distributed projected descent steps on retained data, combined with subsampled Gaussian noise and projection onto trust region around original model.

Result: Provides convergence guarantees (convex case), stationarity guarantees (nonconvex case), (Îµ,Î´) network-unlearning certificates via subsampled Gaussian RÃ©nyi DP, deletion-capacity bounds scaling with forget-to-local data ratio. Empirically matches privacy guarantees while achieving higher test accuracy than decentralized DP baselines and reducing forget accuracy to random guessing (~10%).

Conclusion: RR-DU enables certified machine unlearning in decentralized networks with strong privacy guarantees, better utility than DP baselines, and effective data removal, addressing the gap in decentralized unlearning research.

Abstract: Machine unlearning (MU) seeks to remove the influence of specified data from a trained model in response to privacy requests or data poisoning. While certified unlearning has been analyzed in centralized and server-orchestrated federated settings (via guarantees analogous to differential privacy, DP), the decentralized setting -- where peers communicate without a coordinator remains underexplored. We study certified unlearning in decentralized networks with fixed topologies and propose RR-DU, a random-walk procedure that performs one projected gradient ascent step on the forget set at the unlearning client and a geometrically distributed number of projected descent steps on the retained data elsewhere, combined with subsampled Gaussian noise and projection onto a trust region around the original model. We provide (i) convergence guarantees in the convex case and stationarity guarantees in the nonconvex case, (ii) $(\varepsilon,Î´)$ network-unlearning certificates on client views via subsampled Gaussian $RÃ©nyi$ DP (RDP) with segment-level subsampling, and (iii) deletion-capacity bounds that scale with the forget-to-local data ratio and quantify the effect of decentralization (network mixing and randomized subsampling) on the privacy--utility trade-off. Empirically, on image benchmarks (MNIST, CIFAR-10), RR-DU matches a given $(\varepsilon,Î´)$ while achieving higher test accuracy than decentralized DP baselines and reducing forget accuracy to random guessing ($\approx 10\%$).

</details>


### [239] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models II: Benchmark Generation Process](https://arxiv.org/abs/2512.08451)
*Gary Ackerman,Zachary Kallenborn,Anna Wetzel,Hayley Peterson,Jenna LaTourette,Olivia Shoemaker,Brandon Behlendorf,Sheriff Almakki,Doug Clifford,Noah Sheinbaum*

Main category: cs.LG

TL;DR: This paper presents the Bacterial Biothreat Benchmark (B3) dataset, a key component of a framework to assess AI models' potential to facilitate bioterrorism risks.


<details>
  <summary>Details</summary>
Motivation: There's growing concern that advanced AI models, particularly LLMs, could be misused for bioterrorism or biological weapons development. Both developers and policymakers need reliable benchmarks to quantify and mitigate these biosecurity risks.

Method: Used three complementary approaches: 1) web-based prompt generation, 2) red teaming, and 3) mining existing benchmark corpora to generate over 7,000 potential benchmarks. Applied de-duplication, uplift diagnosticity assessment, and quality control to refine to 1,010 final benchmarks aligned with the Task-Query Architecture from the project's first component.

Result: Created the B3 dataset containing 1,010 carefully curated benchmarks that are diagnostic (provide uplift), directly relevant to biosecurity threats, and aligned with a larger biosecurity architecture for nuanced analysis.

Conclusion: The B3 dataset provides a robust benchmark for assessing AI models' potential to facilitate biological threats, addressing critical biosecurity concerns through systematic development and validation processes.

Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper, the second in a series of three, describes the second component of a novel Biothreat Benchmark Generation (BBG) framework: the generation of the Bacterial Biothreat Benchmark (B3) dataset. The development process involved three complementary approaches: 1) web-based prompt generation, 2) red teaming, and 3) mining existing benchmark corpora, to generate over 7,000 potential benchmarks linked to the Task-Query Architecture that was developed during the first component of the project. A process of de-duplication, followed by an assessment of uplift diagnosticity, and general quality control measures, reduced the candidates to a set of 1,010 final benchmarks. This procedure ensured that these benchmarks are a) diagnostic in terms of providing uplift; b) directly relevant to biosecurity threats; and c) are aligned with a larger biosecurity architecture permitting nuanced analysis at different levels of analysis.

</details>


### [240] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset](https://arxiv.org/abs/2512.08459)
*Gary Ackerman,Theodore Wilson,Zachary Kallenborn,Olivia Shoemaker,Anna Wetzel,Hayley Peterson,Abigail Danfora,Jenna LaTourette,Brandon Behlendorf,Douglas Clifford*

Main category: cs.LG

TL;DR: Researchers pilot-tested a Bacterial Biothreat Benchmark (B3) dataset to assess biosecurity risks from frontier AI models, finding it provides a viable method for rapid risk assessment and mitigation guidance.


<details>
  <summary>Details</summary>
Motivation: Growing concern about frontier AI models (especially LLMs) potentially facilitating bioterrorism or biological weapons access, creating need for benchmarks to quantify and mitigate biosecurity risks.

Method: Pilot implementation of Bacterial Biothreat Benchmark (B3) dataset - part of Biothreat Benchmark Generation (BBG) framework. Involved running benchmarks through sample frontier AI model, human evaluation of responses, and applied risk analysis across multiple dimensions.

Result: B3 dataset demonstrated viability as nuanced method for rapidly assessing biosecurity risk from LLMs. Successfully identified key risk sources and provided guidance for priority mitigation areas.

Conclusion: The benchmark offers practical tool for model developers and policymakers to evaluate and address biosecurity risks posed by frontier AI models, particularly in identifying and prioritizing mitigation strategies.

Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper discusses the pilot implementation of the Bacterial Biothreat Benchmark (B3) dataset. It is the third in a series of three papers describing an overall Biothreat Benchmark Generation (BBG) framework, with previous papers detailing the development of the B3 dataset. The pilot involved running the benchmarks through a sample frontier AI model, followed by human evaluation of model responses, and an applied risk analysis of the results along several dimensions. Overall, the pilot demonstrated that the B3 dataset offers a viable, nuanced method for rapidly assessing the biosecurity risk posed by a LLM, identifying the key sources of that risk and providing guidance for priority areas of mitigation priority.

</details>


### [241] [Transformers for Multimodal Brain State Decoding: Integrating Functional Magnetic Resonance Imaging Data and Medical Metadata](https://arxiv.org/abs/2512.08462)
*Danial Jafarzadeh Jazi,Maryam Hajiesmaeili*

Main category: cs.LG

TL;DR: Transformer-based framework integrates fMRI data with DICOM metadata using attention mechanisms to improve brain state decoding accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional ML and deep learning approaches for fMRI brain state decoding fail to utilize the rich contextual information available in DICOM metadata, limiting their effectiveness.

Method: Novel transformer-based architecture that processes multimodal inputs including fMRI data and DICOM metadata, using attention mechanisms to capture spatial-temporal patterns and contextual relationships.

Result: Enhanced model accuracy, interpretability, and robustness for brain state decoding from fMRI data, with applications in clinical diagnostics, cognitive neuroscience, and personalized medicine.

Conclusion: The framework successfully integrates DICOM metadata with fMRI data using transformers, addressing previous limitations while acknowledging challenges like metadata variability and computational demands, with future work needed for scalability optimization.

Abstract: Decoding brain states from functional magnetic resonance imaging (fMRI) data is vital for advancing neuroscience and clinical applications. While traditional machine learning and deep learning approaches have made strides in leveraging the high-dimensional and complex nature of fMRI data, they often fail to utilize the contextual richness provided by Digital Imaging and Communications in Medicine (DICOM) metadata. This paper presents a novel framework integrating transformer-based architectures with multimodal inputs, including fMRI data and DICOM metadata. By employing attention mechanisms, the proposed method captures intricate spatial-temporal patterns and contextual relationships, enhancing model accuracy, interpretability, and robustness. The potential of this framework spans applications in clinical diagnostics, cognitive neuroscience, and personalized medicine. Limitations, such as metadata variability and computational demands, are addressed, and future directions for optimizing scalability and generalizability are discussed.

</details>


### [242] [Solving Over-Smoothing in GNNs via Nonlocal Message Passing: Algebraic Smoothing and Depth Scalability](https://arxiv.org/abs/2512.08475)
*Weiqi Guan,Junlin He*

Main category: cs.LG

TL;DR: Proposes a Post-LN based method that induces algebraic smoothing to resolve the dilemma between avoiding over-smoothing (Pre-LN) and avoiding the curse of depth (Post-LN) in deep networks.


<details>
  <summary>Details</summary>
Motivation: The relationship between Layer Normalization placement and over-smoothing is underexplored. There's a critical dilemma: Pre-LN avoids over-smoothing but suffers from curse of depth, while Post-LN avoids curse of depth but experiences over-smoothing.

Method: Proposes a new method based on Post-LN that induces algebraic smoothing to prevent over-smoothing without suffering from the curse of depth. The approach is parameter-efficient and requires no additional parameters.

Result: Empirical results across five benchmarks demonstrate that the approach supports deeper networks (up to 256 layers) and improves performance. The method effectively resolves the over-smoothing vs. curse of depth dilemma.

Conclusion: The proposed Post-LN based method with algebraic smoothing successfully resolves the critical dilemma between over-smoothing and curse of depth, enabling deeper and more effective networks without additional parameters.

Abstract: The relationship between Layer Normalization (LN) placement and the over-smoothing phenomenon remains underexplored. We identify a critical dilemma: Pre-LN architectures avoid over-smoothing but suffer from the curse of depth, while Post-LN architectures bypass the curse of depth but experience over-smoothing.
  To resolve this, we propose a new method based on Post-LN that induces algebraic smoothing, preventing over-smoothing without the curse of depth. Empirical results across five benchmarks demonstrate that our approach supports deeper networks (up to 256 layers) and improves performance, requiring no additional parameters.
  Key contributions:
  Theoretical Characterization: Analysis of LN dynamics and their impact on over-smoothing and the curse of depth.
  A Principled Solution: A parameter-efficient method that induces algebraic smoothing and avoids over-smoothing and the curse of depth.
  Empirical Validation: Extensive experiments showing the effectiveness of the method in deeper GNNs.

</details>


### [243] [Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning](https://arxiv.org/abs/2512.08485)
*Junnan Qiu,Jie Li*

Main category: cs.LG

TL;DR: Proposes Global Budget Allocation attack for offline RL that allocates poisoning perturbations proportionally to TD-error sensitivity, outperforming uniform attacks with better efficiency and stealth.


<details>
  <summary>Details</summary>
Motivation: Existing offline RL poisoning attacks use uniform perturbations across all samples, which is inefficient (wastes budget on low-impact samples) and lacks stealth (causes detectable statistical deviations).

Method: Formulates attack as global resource allocation problem based on theoretical insight that sample influence on value function convergence is proportional to TD error. Derives closed-form solution where perturbation magnitudes are assigned proportional to TD-error sensitivity under global L2 constraint.

Result: Empirical results on D4RL benchmarks show method significantly outperforms baseline strategies, achieving up to 80% performance degradation with minimal perturbations that evade detection by state-of-the-art statistical and spectral defenses.

Conclusion: The proposed Global Budget Allocation attack strategy is more efficient and stealthy than existing uniform perturbation approaches, effectively poisoning offline RL while avoiding detection.

Abstract: Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to data poisoning attacks. Existing attack strategies typically rely on locally uniform perturbations, which treat all samples indiscriminately. This approach is inefficient, as it wastes the perturbation budget on low-impact samples, and lacks stealthiness due to significant statistical deviations. In this paper, we propose a novel Global Budget Allocation attack strategy. Leveraging the theoretical insight that a sample's influence on value function convergence is proportional to its Temporal Difference (TD) error, we formulate the attack as a global resource allocation problem. We derive a closed-form solution where perturbation magnitudes are assigned proportional to the TD-error sensitivity under a global L2 constraint. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms baseline strategies, achieving up to 80% performance degradation with minimal perturbations that evade detection by state-of-the-art statistical and spectral defenses.

</details>


### [244] [Developing Distance-Aware Uncertainty Quantification Methods in Physics-Guided Neural Networks for Reliable Bearing Health Prediction](https://arxiv.org/abs/2512.08499)
*Waleed Razzaq,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: Two distance-aware uncertainty methods (PG-SNGP and PG-SNER) for physics-guided neural networks improve degradation prediction accuracy and uncertainty calibration in rotating machinery, outperforming Monte Carlo and Deep Ensemble methods.


<details>
  <summary>Details</summary>
Motivation: Existing uncertainty methods for predictive maintenance lack confidence calibration, are computationally expensive, not distance-aware, and fail to generalize under out-of-distribution data, which is critical for safety-critical systems like rotating machinery with rolling-element bearings.

Method: Two distance-aware uncertainty methods: PG-SNGP (Spectral Normalization Gaussian Process) replaces final dense layer with Gaussian Process layer; PG-SNER (Deep Evidential Regression) outputs Normal Inverse Gamma parameters. Both use spectral normalization to preserve input-to-latent space distances, plus dynamic weighting in loss to balance data fidelity and physical consistency.

Result: PG-SNGP and PG-SNER improve prediction accuracy, generalize reliably under OOD conditions, and remain robust to adversarial attacks and noise when tested on PRONOSTIA dataset for rolling-element bearing degradation, outperforming Monte Carlo and Deep Ensemble PGNNs.

Conclusion: The proposed distance-aware uncertainty methods provide accurate, calibrated uncertainty estimation for predictive maintenance in safety-critical systems, addressing limitations of existing approaches through distance preservation and coherent probabilistic modeling.

Abstract: Accurate and uncertainty-aware degradation estimation is essential for predictive maintenance in safety-critical systems like rotating machinery with rolling-element bearings. Many existing uncertainty methods lack confidence calibration, are costly to run, are not distance-aware, and fail to generalize under out-of-distribution data. We introduce two distance-aware uncertainty methods for deterministic physics-guided neural networks: PG-SNGP, based on Spectral Normalization Gaussian Process, and PG-SNER, based on Deep Evidential Regression. We apply spectral normalization to the hidden layers so the network preserves distances from input to latent space. PG-SNGP replaces the final dense layer with a Gaussian Process layer for distance-sensitive uncertainty, while PG-SNER outputs Normal Inverse Gamma parameters to model uncertainty in a coherent probabilistic form. We assess performance using standard accuracy metrics and a new distance-aware metric based on the Pearson Correlation Coefficient, which measures how well predicted uncertainty tracks the distance between test and training samples. We also design a dynamic weighting scheme in the loss to balance data fidelity and physical consistency. We test our methods on rolling-element bearing degradation using the PRONOSTIA dataset and compare them with Monte Carlo and Deep Ensemble PGNNs. Results show that PG-SNGP and PG-SNER improve prediction accuracy, generalize reliably under OOD conditions, and remain robust to adversarial attacks and noise.

</details>


### [245] [A Hybrid Model for Stock Market Forecasting: Integrating News Sentiment and Time Series Data with Graph Neural Networks](https://arxiv.org/abs/2512.08567)
*Nader Sadek,Mirette Moawad,Christina Naguib,Mariam Elzahaby*

Main category: cs.LG

TL;DR: Multimodal GNN combining news articles and historical stock data outperforms LSTM baseline for stock prediction, with news headlines showing stronger signals than full articles.


<details>
  <summary>Details</summary>
Motivation: Stock market prediction traditionally relies on historical prices, but financial news provides valuable external signals that could improve accuracy. There's a need to integrate multimodal data (news + historical data) for better prediction performance.

Method: Proposes a multimodal approach using Graph Neural Networks (GNN). Historical stock data encoded with LSTM, news titles embedded with language model. Creates heterogeneous graph with nodes for articles, companies, and industries, using GraphSAGE to capture interactions. Compares against LSTM baseline.

Result: GNN outperforms LSTM baseline: achieves 53% accuracy on binary direction-of-change prediction and 4% precision gain on significance-based labeling. Companies with more news coverage yield higher accuracy. Headlines contain stronger predictive signals than full articles.

Conclusion: Multimodal integration of news and historical data via GNN improves stock prediction. Concise news summaries (headlines) play crucial role in short-term market reactions, and news volume correlates with prediction accuracy.

Abstract: Stock market prediction is a long-standing challenge in finance, as accurate forecasts support informed investment decisions. Traditional models rely mainly on historical prices, but recent work shows that financial news can provide useful external signals. This paper investigates a multimodal approach that integrates companies' news articles with their historical stock data to improve prediction performance. We compare a Graph Neural Network (GNN) model with a baseline LSTM model. Historical data for each company is encoded using an LSTM, while news titles are embedded with a language model. These embeddings form nodes in a heterogeneous graph, and GraphSAGE is used to capture interactions between articles, companies, and industries. We evaluate two targets: a binary direction-of-change label and a significance-based label. Experiments on the US equities and Bloomberg datasets show that the GNN outperforms the LSTM baseline, achieving 53% accuracy on the first target and a 4% precision gain on the second. Results also indicate that companies with more associated news yield higher prediction accuracy. Moreover, headlines contain stronger predictive signals than full articles, suggesting that concise news summaries play an important role in short-term market reactions.

</details>


### [246] [Long-Sequence LSTM Modeling for NBA Game Outcome Prediction Using a Novel Multi-Season Dataset](https://arxiv.org/abs/2512.08591)
*Charles Rios,Longzhen Han,Almas Baimagambetov,Nikolaos Polatidis*

Main category: cs.LG

TL;DR: LSTM model with 8-season sequence length achieves best performance (72.35% accuracy) for NBA game prediction using new multi-season dataset.


<details>
  <summary>Details</summary>
Motivation: Existing NBA prediction models struggle with concept drift, limited temporal context, and instability across seasons, creating need for better forecasting systems for coaching strategy, fan engagement, and sports betting.

Method: Created new longitudinal NBA dataset (2004-05 to 2024-25 seasons) and developed LSTM architecture with extended sequence length of 9,840 games (8 full seasons) to capture long-term performance trends and season dependencies.

Result: LSTM outperformed all baselines (Logistic Regression, Random Forest, MLP, CNN) with 72.35% accuracy, 73.15% precision, and 76.13% AUC-ROC, demonstrating superiority of long-sequence temporal modeling.

Conclusion: Long-sequence temporal modeling is crucial for basketball outcome prediction, and the new multi-season dataset enables development of robust, generalizable NBA forecasting systems that address concept drift and season instability.

Abstract: Predicting the outcomes of professional basketball games, particularly in the National Basketball Association (NBA), has become increasingly important for coaching strategy, fan engagement, and sports betting. However, many existing prediction models struggle with concept drift, limited temporal context, and instability across seasons. To advance forecasting in this domain, we introduce a newly constructed longitudinal NBA dataset covering the 2004-05 to 2024-25 seasons and present a deep learning framework designed to model long-term performance trends. Our primary contribution is a Long Short-Term Memory (LSTM) architecture that leverages an extended sequence length of 9,840 games equivalent to eight full NBA seasons to capture evolving team dynamics and season-over-season dependencies. We compare this model against several traditional Machine Learning (ML) and Deep Learning (DL) baselines, including Logistic Regression, Random Forest, Multi-Layer Perceptron (MLP), and Convolutional Neural Network (CNN). The LSTM achieves the best performance across all metrics, with 72.35 accuracy, 73.15 precision and 76.13 AUC-ROC. These results demonstrate the importance of long-sequence temporal modeling in basketball outcome prediction and highlight the value of our new multi-season dataset for developing robust, generalizable NBA forecasting systems.

</details>


### [247] [DS FedProxGrad: Asymptotic Stationarity Without Noise Floor in Fair Federated Learning](https://arxiv.org/abs/2512.08671)
*Huzaifa Arif*

Main category: cs.LG

TL;DR: Improved asymptotic convergence analysis for FedProxGrad-type algorithms in group fair federated learning, proving convergence to exact stationarity without noise floor dependence.


<details>
  <summary>Details</summary>
Motivation: Previous FedProxGrad analysis only showed convergence to a noise-dominated neighborhood with explicit variance-induced noise floor dependence, which is suboptimal for non-convex composite optimization in group fair federated learning.

Method: Extended FedProxGrad framework called DS-FedProxGrad with decay step sizes (Robbins-Monro schedule), inexact local proximal solutions, explicit fairness regularization, and mild decay condition on local inexactness.

Result: Proved liminf_{rââ} ð¼[ââF(x^r)âÂ²] = 0, meaning algorithm is asymptotically stationary with convergence rate independent of variance-induced noise floor.

Conclusion: DS-FedProxGrad provides stronger convergence guarantees than original FedProxGrad, achieving exact asymptotic stationarity for non-convex composite optimization in group fair federated learning.

Abstract: Recent work \cite{arifgroup} introduced Federated Proximal Gradient \textbf{(\texttt{FedProxGrad})} for solving non-convex composite optimization problems in group fair federated learning. However, the original analysis established convergence only to a \textit{noise-dominated neighborhood of stationarity}, with explicit dependence on a variance-induced noise floor. In this work, we provide an improved asymptotic convergence analysis for a generalized \texttt{FedProxGrad}-type analytical framework with inexact local proximal solutions and explicit fairness regularization. We call this extended analytical framework \textbf{DS \texttt{FedProxGrad}} (Decay Step Size \texttt{FedProxGrad}). Under a Robbins-Monro step-size schedule \cite{robbins1951stochastic} and a mild decay condition on local inexactness, we prove that $\liminf_{r\to\infty} \mathbb{E}[\|\nabla F(\mathbf{x}^r)\|^2] = 0$, i.e., the algorithm is asymptotically stationary and the convergence rate does not depend on a variance-induced noise floor.

</details>


### [248] [An Additive Manufacturing Part Qualification Framework: Transferring Knowledge of Stress-strain Behaviors from Additively Manufactured Polymers to Metals](https://arxiv.org/abs/2512.08699)
*Chenglong Duan,Dazhong Wu*

Main category: cs.LG

TL;DR: DTW-Transfer Learning framework transfers stress-strain behavior knowledge from polymers to metals for additive manufacturing part qualification, achieving better prediction accuracy than baseline models.


<details>
  <summary>Details</summary>
Motivation: Part qualification is essential in additive manufacturing to ensure consistent production and reliability in critical applications. Predicting complex stress-strain behaviors of AM parts is crucial for verifying performance requirements, but this is challenging due to material and process variations.

Method: Developed a Dynamic Time Warping (DTW)-Transfer Learning framework that transfers knowledge from polymer stress-strain behaviors to metals. Uses DTW to select the most relevant polymer dataset as source domain, then employs Long Short-Term Memory (LSTM) models. Tested with four source polymers (Nylon, PLA, CF-ABS, Resin) and three target metals (AlSi10Mg, Ti6Al4V, carbon steel) fabricated by different AM techniques.

Result: DTW-TL framework successfully identifies closest matches between polymers and metals to select optimal source domain. Achieves lowest mean absolute percentage error of 12.41% and highest coefficient of determination of 0.96 when three metals are used as target domain, outperforming vanilla LSTM without TL and TL models pre-trained on all four polymer datasets.

Conclusion: The DTW-Transfer Learning framework provides an effective approach for AM part qualification by enabling knowledge transfer from low-cost polymers to metals, improving prediction accuracy of stress-strain behaviors and supporting reliable part qualification processes.

Abstract: Part qualification is crucial in additive manufacturing (AM) because it ensures that additively manufactured parts can be consistently produced and reliably used in critical applications. Part qualification aims at verifying that an additively manufactured part meets performance requirements; therefore, predicting the complex stress-strain behaviors of additively manufactured parts is critical. We develop a dynamic time warping (DTW)-transfer learning (TL) framework for additive manufacturing part qualification by transferring knowledge of the stress-strain behaviors of additively manufactured low-cost polymers to metals. Specifically, the framework employs DTW to select a polymer dataset as the source domain that is the most relevant to the target metal dataset. Using a long short-term memory (LSTM) model, four source polymers (i.e., Nylon, PLA, CF-ABS, and Resin) and three target metals (i.e., AlSi10Mg, Ti6Al4V, and carbon steel) that are fabricated by different AM techniques are utilized to demonstrate the effectiveness of the DTW-TL framework. Experimental results show that the DTW-TL framework identifies the closest match between polymers and metals to select one single polymer dataset as the source domain. The DTW-TL model achieves the lowest mean absolute percentage error of 12.41% and highest coefficient of determination of 0.96 when three metals are used as the target domain, respectively, outperforming the vanilla LSTM model without TL as well as the TL model pre-trained on four polymer datasets as the source domain.

</details>


### [249] [Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search](https://arxiv.org/abs/2512.08724)
*Manos Plitsis,Giorgos Bouritsas,Vassilis Katsouros,Yannis Panagakis*

Main category: cs.LG

TL;DR: BGPS is an automated framework that discovers subtle biases in text-to-image models by generating prompts that maximize biased image generation, revealing previously undocumented fairness issues.


<details>
  <summary>Details</summary>
Motivation: Current bias mitigation approaches rely on curated prompt datasets which are costly and risk missing unanticipated, subtle prompts that trigger biased generation even in debiased models.

Method: BGPS uses an LLM to generate attribute-neutral prompts combined with attribute classifiers that steer the LLM's decoding toward prompts that amplify specific image attributes, creating an automated bias search framework.

Result: The method discovered subtle, previously undocumented biases in Stable Diffusion 1.5 and state-of-the-art debiased models that severely deteriorate fairness metrics, with interpretable prompts that could be entered by typical users.

Conclusion: BGPS uncovers TTI vulnerabilities, expands the bias search space, and can serve as a new evaluation tool for bias mitigation by automatically generating prompts that reveal subtle fairness issues.

Abstract: Text-to-image (TTI) diffusion models have achieved remarkable visual quality, yet they have been repeatedly shown to exhibit social biases across sensitive attributes such as gender, race and age. To mitigate these biases, existing approaches frequently depend on curated prompt datasets - either manually constructed or generated with large language models (LLMs) - as part of their training and/or evaluation procedures. Beside the curation cost, this also risks overlooking unanticipated, less obvious prompts that trigger biased generation, even in models that have undergone debiasing. In this work, we introduce Bias-Guided Prompt Search (BGPS), a framework that automatically generates prompts that aim to maximize the presence of biases in the resulting images. BGPS comprises two components: (1) an LLM instructed to produce attribute-neutral prompts and (2) attribute classifiers acting on the TTI's internal representations that steer the decoding process of the LLM toward regions of the prompt space that amplify the image attributes of interest. We conduct extensive experiments on Stable Diffusion 1.5 and a state-of-the-art debiased model and discover an array of subtle and previously undocumented biases that severely deteriorate fairness metrics. Crucially, the discovered prompts are interpretable, i.e they may be entered by a typical user, quantitatively improving the perplexity metric compared to a prominent hard prompt optimization counterpart. Our findings uncover TTI vulnerabilities, while BGPS expands the bias search space and can act as a new evaluation tool for bias mitigation.

</details>


### [250] [Neural Ordinary Differential Equations for Simulating Metabolic Pathway Dynamics from Time-Series Multiomics Data](https://arxiv.org/abs/2512.08732)
*Udesh Habaraduwa,Andrei Lixandru*

Main category: cs.LG

TL;DR: NODEs outperform traditional ML for predicting metabolic dynamics, achieving >90% RMSE improvement and 1000x faster inference on engineered E. coli pathways.


<details>
  <summary>Details</summary>
Motivation: There's a bottleneck in converting abundant multiomics data into actionable predictive models for human healthspan and bioengineering. Current mechanistic models are limited by prior knowledge, while data-driven approaches are needed to infer latent interactions and simulate temporal trajectories for personalized medicine and synthetic biology.

Method: Introduced Neural Ordinary Differential Equations (NODEs) as a dynamic framework to learn complex interplay between proteome and metabolome. Applied to time-series data from engineered E. coli strains to model continuous dynamics of metabolic pathways.

Result: NODE architecture demonstrated superior performance: >90% improvement in RMSE over baselines (94.38% for Limonene, 97.65% for Isopentenol pathways). Also achieved 1000x acceleration in inference time.

Conclusion: NODEs establish a scalable, high-fidelity tool for next-generation metabolic engineering and biological discovery, enabling better prediction of complex biological system dynamics.

Abstract: The advancement of human healthspan and bioengineering relies heavily on predicting the behavior of complex biological systems. While high-throughput multiomics data is becoming increasingly abundant, converting this data into actionable predictive models remains a bottleneck. High-capacity, datadriven simulation systems are critical in this landscape; unlike classical mechanistic models restricted by prior knowledge, these architectures can infer latent interactions directly from observational data, allowing for the simulation of temporal trajectories and the anticipation of downstream intervention effects in personalized medicine and synthetic biology. To address this challenge, we introduce Neural Ordinary Differential Equations (NODEs) as a dynamic framework for learning the complex interplay between the proteome and metabolome. We applied this framework to time-series data derived from engineered Escherichia coli strains, modeling the continuous dynamics of metabolic pathways. The proposed NODE architecture demonstrates superior performance in capturing system dynamics compared to traditional machine learning pipelines. Our results show a greater than 90% improvement in root mean squared error over baselines across both Limonene (up to 94.38% improvement) and Isopentenol (up to 97.65% improvement) pathway datasets. Furthermore, the NODE models demonstrated a 1000x acceleration in inference time, establishing them as a scalable, high-fidelity tool for the next generation of metabolic engineering and biological discovery.

</details>


### [251] [Learning and Editing Universal Graph Prompt Tuning via Reinforcement Learning](https://arxiv.org/abs/2512.08763)
*Jinfeng Xu,Zheyu Chen,Shuo Yang,Jinze Li,Hewei Wang,Yijie Li,Edith C. H. Ngai*

Main category: cs.LG

TL;DR: LEAP strengthens universal graph prompt tuning theory by proving prompts must be added to all nodes for universality, then uses actor-critic RL to selectively edit prompts while preserving theoretical foundation.


<details>
  <summary>Details</summary>
Motivation: Existing selective node-based graph prompt tuning approaches compromise the theoretical foundation of universal graph prompt tuning, which claims prompts can achieve equivalent effect of any prompting function. The authors aim to strengthen this theoretical foundation while still pursuing more ideal prompts.

Method: LEAP (Learning and Editing Universal GrAph Prompt Tuning) first builds basic universal graph prompts to preserve theoretical foundation (adding prompts to all nodes), then uses actor-critic reinforcement learning to select nodes and edit prompts for optimization.

Result: Extensive experiments on graph- and node-level tasks across various pre-training strategies in both full-shot and few-shot scenarios show LEAP consistently outperforms fine-tuning and other prompt-based approaches.

Conclusion: Adding prompts to all nodes is necessary for achieving universality of graph prompts, and LEAP successfully preserves this theoretical foundation while achieving superior performance through selective prompt editing via reinforcement learning.

Abstract: Early graph prompt tuning approaches relied on task-specific designs for Graph Neural Networks (GNNs), limiting their adaptability across diverse pre-training strategies. In contrast, another promising line of research has investigated universal graph prompt tuning, which operates directly in the input graph's feature space and builds a theoretical foundation that universal graph prompt tuning can theoretically achieve an equivalent effect of any prompting function, eliminating dependence on specific pre-training strategies. Recent works propose selective node-based graph prompt tuning to pursue more ideal prompts. However, we argue that selective node-based graph prompt tuning inevitably compromises the theoretical foundation of universal graph prompt tuning. In this paper, we strengthen the theoretical foundation of universal graph prompt tuning by introducing stricter constraints, demonstrating that adding prompts to all nodes is a necessary condition for achieving the universality of graph prompts. To this end, we propose a novel model and paradigm, Learning and Editing Universal GrAph Prompt Tuning (LEAP), which preserves the theoretical foundation of universal graph prompt tuning while pursuing more ideal prompts. Specifically, we first build the basic universal graph prompts to preserve the theoretical foundation and then employ actor-critic reinforcement learning to select nodes and edit prompts. Extensive experiments on graph- and node-level tasks across various pre-training strategies in both full-shot and few-shot scenarios show that LEAP consistently outperforms fine-tuning and other prompt-based approaches.

</details>


### [252] [De novo generation of functional terpene synthases using TpsGPT](https://arxiv.org/abs/2512.08772)
*Hamsini Ramanathan,Roman Bushuiev,MatouÅ¡ SoldÃ¡t,JirÃ­ Kohout,TÃ©o Hebra,Joshua David Smith,Josef Sivic,TomÃ¡Å¡ Pluskal*

Main category: cs.LG

TL;DR: TpsGPT is a generative AI model that designs novel terpene synthase enzymes by fine-tuning ProtGPT2 on TPS sequences, generating functional enzymes validated experimentally.


<details>
  <summary>Details</summary>
Motivation: Terpene synthases (TPS) are crucial for producing valuable natural products like anticancer drugs, but traditional directed evolution methods for TPS design are expensive and slow.

Method: Fine-tuned ProtGPT2 protein language model on 79k TPS sequences from UniProt to create TpsGPT, then generated sequences and filtered them using multiple validation metrics including EnzymeExplorer classification, ESMFold structural confidence, sequence diversity, CLEAN classification, InterPro domain detection, and Foldseek structure alignment.

Result: From 28k generated sequences, identified seven putative TPS enzymes meeting all validation criteria, with at least two experimentally confirmed to have TPS enzymatic activity.

Conclusion: Fine-tuning protein language models on enzyme-class-specific datasets with rigorous filtering enables de novo generation of functional, evolutionarily distant enzymes, offering a scalable alternative to traditional directed evolution.

Abstract: Terpene synthases (TPS) are a key family of enzymes responsible for generating the diverse terpene scaffolds that underpin many natural products, including front-line anticancer drugs such as Taxol. However, de novo TPS design through directed evolution is costly and slow. We introduce TpsGPT, a generative model for scalable TPS protein design, built by fine-tuning the protein language model ProtGPT2 on 79k TPS sequences mined from UniProt. TpsGPT generated de novo enzyme candidates in silico and we evaluated them using multiple validation metrics, including EnzymeExplorer classification, ESMFold structural confidence (pLDDT), sequence diversity, CLEAN classification, InterPro domain detection, and Foldseek structure alignment. From an initial pool of 28k generated sequences, we identified seven putative TPS enzymes that satisfied all validation criteria. Experimental validation confirmed TPS enzymatic activity in at least two of these sequences. Our results show that fine-tuning of a protein language model on a carefully curated, enzyme-class-specific dataset, combined with rigorous filtering, can enable the de novo generation of functional, evolutionarily distant enzymes.

</details>


### [253] [Can TabPFN Compete with GNNs for Node Classification via Graph Tabularization?](https://arxiv.org/abs/2512.08798)
*Jeongwhan Choi,Woosung Kang,Minseo Kim,Jongwoo Kim,Noseong Park*

Main category: cs.LG

TL;DR: TabPFN-GN transforms graph node classification into a tabular learning problem by extracting various node features, enabling competitive performance with GNNs without graph-specific training.


<details>
  <summary>Details</summary>
Motivation: To investigate whether graph node classification can be effectively reformulated as a tabular learning problem, building on the success of TabPFN for tabular data and its extension to time series.

Method: TabPFN-GN transforms graph data into tabular features by extracting node attributes, structural properties, positional encodings, and optionally smoothed neighborhood features, enabling TabPFN to perform direct node classification without graph-specific training.

Result: Experiments on 12 benchmark datasets show TabPFN-GN achieves competitive performance with GNNs on homophilous graphs and consistently outperforms them on heterophilous graphs.

Conclusion: Principled feature engineering can bridge the gap between tabular and graph domains, providing a practical alternative to task-specific GNN training and LLM-dependent graph foundation models.

Abstract: Foundation models pretrained on large data have demonstrated remarkable zero-shot generalization capabilities across domains. Building on the success of TabPFN for tabular data and its recent extension to time series, we investigate whether graph node classification can be effectively reformulated as a tabular learning problem. We introduce TabPFN-GN, which transforms graph data into tabular features by extracting node attributes, structural properties, positional encodings, and optionally smoothed neighborhood features. This enables TabPFN to perform direct node classification without any graph-specific training or language model dependencies. Our experiments on 12 benchmark datasets reveal that TabPFN-GN achieves competitive performance with GNNs on homophilous graphs and consistently outperforms them on heterophilous graphs. These results demonstrate that principled feature engineering can bridge the gap between tabular and graph domains, providing a practical alternative to task-specific GNN training and LLM-dependent graph foundation models.

</details>


### [254] [Identifying counterfactual probabilities using bivariate distributions and uplift modeling](https://arxiv.org/abs/2512.08805)
*ThÃ©o Verhelst,Gianluca Bontempi*

Main category: cs.LG

TL;DR: Proposes using uplift modeling to estimate joint counterfactual distributions by fitting bivariate beta distributions to predicted uplift scores, enabling richer causal insights than uplift alone.


<details>
  <summary>Details</summary>
Motivation: While uplift modeling estimates average treatment effects, counterfactual identification aims to recover the full joint distribution of potential outcomes, providing richer information about individual-level causal effects. However, counterfactual estimation is more challenging than uplift modeling, and the paper aims to leverage the synergy between these approaches.

Method: Proposes a counterfactual estimator that fits a bivariate beta distribution to predicted uplift scores from uplift models. This yields posterior distributions over counterfactual outcomes without requiring additional causal assumptions beyond those already made in uplift modeling.

Result: Simulations demonstrate the efficacy of the approach. The method can be applied to real-world problems like telecom customer churn, revealing insights that are unavailable to standard machine learning or uplift models alone.

Conclusion: Uplift models can be effectively leveraged for counterfactual estimation through the proposed bivariate beta distribution approach, providing richer causal insights while maintaining the same causal assumptions as uplift modeling.

Abstract: Uplift modeling estimates the causal effect of an intervention as the difference between potential outcomes under treatment and control, whereas counterfactual identification aims to recover the joint distribution of these potential outcomes (e.g., "Would this customer still have churned had we given them a marketing offer?"). This joint counterfactual distribution provides richer information than the uplift but is harder to estimate. However, the two approaches are synergistic: uplift models can be leveraged for counterfactual estimation. We propose a counterfactual estimator that fits a bivariate beta distribution to predicted uplift scores, yielding posterior distributions over counterfactual outcomes. Our approach requires no causal assumptions beyond those of uplift modeling. Simulations show the efficacy of the approach, which can be applied, for example, to the problem of customer churn in telecom, where it reveals insights unavailable to standard ML or uplift models alone.

</details>


### [255] [Forecasting Fails: Unveiling Evasion Attacks in Weather Prediction Models](https://arxiv.org/abs/2512.08832)
*Huzaifa Arif,Pin-Yu Chen,Alex Gittens,James Diffenderfer,Bhavya Kailkhura*

Main category: cs.LG

TL;DR: WAAPO is a framework for generating targeted adversarial perturbations against AI weather forecasting models that are effective yet stealthy by incorporating physical realism constraints.


<details>
  <summary>Details</summary>
Motivation: As AI models become increasingly relied upon for weather forecasting, there's a critical need to evaluate their vulnerability to adversarial attacks that could manipulate forecasts while remaining undetected.

Method: WAAPO (Weather Adaptive Adversarial Perturbation Optimization) generates adversarial perturbations with constraints for channel sparsity, spatial localization, and smoothness to ensure physical realism and imperceptibility. Tested using ERA5 dataset and FourCastNet model.

Result: WAAPO successfully generates adversarial trajectories that closely align with predefined targets even under constrained conditions, demonstrating that small perturbations to initial conditions can cause significant deviations in predicted weather patterns.

Conclusion: The research reveals critical vulnerabilities in AI-driven weather forecasting models and underscores the urgent need for robust safeguards against adversarial exploitation in operational forecasting systems.

Abstract: With the increasing reliance on AI models for weather forecasting, it is imperative to evaluate their vulnerability to adversarial perturbations. This work introduces Weather Adaptive Adversarial Perturbation Optimization (WAAPO), a novel framework for generating targeted adversarial perturbations that are both effective in manipulating forecasts and stealthy to avoid detection. WAAPO achieves this by incorporating constraints for channel sparsity, spatial localization, and smoothness, ensuring that perturbations remain physically realistic and imperceptible. Using the ERA5 dataset and FourCastNet (Pathak et al. 2022), we demonstrate WAAPO's ability to generate adversarial trajectories that align closely with predefined targets, even under constrained conditions. Our experiments highlight critical vulnerabilities in AI-driven forecasting models, where small perturbations to initial conditions can result in significant deviations in predicted weather patterns. These findings underscore the need for robust safeguards to protect against adversarial exploitation in operational forecasting systems.

</details>


### [256] [Reinforcement Learning From State and Temporal Differences](https://arxiv.org/abs/2512.08855)
*Lex Weaver,Jonathan Baxter*

Main category: cs.LG

TL;DR: STD(Î») modifies TD(Î») to focus on relative state value ordering rather than absolute value errors, preventing convergence to suboptimal policies in certain cases.


<details>
  <summary>Details</summary>
Motivation: TD(Î») with function approximation minimizes squared error between approximate and true state values, but for policy optimization, the relative ordering of states is more critical than absolute values. The authors show TD(Î») can converge to suboptimal policies even starting from optimal ones.

Method: The authors propose STD(Î»), a modified form of TD(Î») where function approximators are trained with respect to relative state values on binary decision problems. They provide theoretical analysis including proof of monotonic policy improvement for STD(Î») in a two-state system.

Result: STD(Î») successfully demonstrates improved performance on the two-state system and a variation of the acrobot problem, showing it can avoid the suboptimal policy convergence issues of TD(Î»).

Conclusion: STD(Î») addresses a fundamental limitation of TD(Î») by focusing on relative state value ordering rather than absolute value accuracy, leading to better policy optimization in reinforcement learning with function approximation.

Abstract: TD($Î»$) with function approximation has proved empirically successful for some complex reinforcement learning problems. For linear approximation, TD($Î»$) has been shown to minimise the squared error between the approximate value of each state and the true value. However, as far as policy is concerned, it is error in the relative ordering of states that is critical, rather than error in the state values. We illustrate this point, both in simple two-state and three-state systems in which TD($Î»$)--starting from an optimal policy--converges to a sub-optimal policy, and also in backgammon. We then present a modified form of TD($Î»$), called STD($Î»$), in which function approximators are trained with respect to relative state values on binary decision problems. A theoretical analysis, including a proof of monotonic policy improvement for STD($Î»$) in the context of the two-state system, is presented, along with a comparison with Bertsekas' differential training method [1]. This is followed by successful demonstrations of STD($Î»$) on the two-state system and a variation on the well known acrobot problem.

</details>


### [257] [Refining Diffusion Models for Motion Synthesis with an Acceleration Loss to Generate Realistic IMU Data](https://arxiv.org/abs/2512.08859)
*Lars Ole HÃ¤usler,Lena Uhlenberg,GÃ¶ran KÃ¶ber,Diyora Salimova,Oliver Amft*

Main category: cs.LG

TL;DR: Text-to-IMU motion synthesis framework using diffusion model fine-tuning with acceleration-based loss (L_acc) to generate realistic IMU data, improving Human Activity Recognition performance.


<details>
  <summary>Details</summary>
Motivation: To generate realistic IMU data from text descriptions by aligning diffusion-based motion generation with IMU-specific acceleration patterns, addressing the gap between generic motion synthesis and sensor-specific data requirements.

Method: Fine-tune pretrained diffusion model with acceleration-based second-order loss (L_acc) that enforces consistency in discrete second-order temporal differences, integrate into existing text-to-IMU framework with surface modeling and virtual sensor simulation.

Result: L_acc decreased by 12.7% relative to original model, with larger improvements in high-dynamic activities; synthetic IMU data distribution shifted closer to real recordings; HAR classification improved by 8.7% over earlier diffusion model.

Conclusion: Acceleration-aware diffusion refinement effectively aligns motion generation with IMU synthesis, demonstrating flexible deep learning pipelines for specializing generic text-to-motion priors to sensor-specific tasks.

Abstract: We propose a text-to-IMU (inertial measurement unit) motion-synthesis framework to obtain realistic IMU data by fine-tuning a pretrained diffusion model with an acceleration-based second-order loss (L_acc). L_acc enforces consistency in the discrete second-order temporal differences of the generated motion, thereby aligning the diffusion prior with IMU-specific acceleration patterns. We integrate L_acc into the training objective of an existing diffusion model, finetune the model to obtain an IMU-specific motion prior, and evaluate the model with an existing text-to-IMU framework that comprises surface modelling and virtual sensor simulation. We analysed acceleration signal fidelity and differences between synthetic motion representation and actual IMU recordings. As a downstream application, we evaluated Human Activity Recognition (HAR) and compared the classification performance using data of our method with the earlier diffusion model and two additional diffusion model baselines. When we augmented the earlier diffusion model objective with L_acc and continued training, L_acc decreased by 12.7% relative to the original model. The improvements were considerably larger in high-dynamic activities (i.e., running, jumping) compared to low-dynamic activities~(i.e., sitting, standing). In a low-dimensional embedding, the synthetic IMU data produced by our refined model shifts closer to the distribution of real IMU recordings. HAR classification trained exclusively on our refined synthetic IMU data improved performance by 8.7% compared to the earlier diffusion model and by 7.6% over the best-performing comparison diffusion model. We conclude that acceleration-aware diffusion refinement provides an effective approach to align motion generation and IMU synthesis and highlights how flexible deep learning pipelines are for specialising generic text-to-motion priors to sensor-specific tasks.

</details>


### [258] [Differentially Private Synthetic Data Generation Using Context-Aware GANs](https://arxiv.org/abs/2512.08869)
*Anantaa Kotal,Anupam Joshi*

Main category: cs.LG

TL;DR: ContextGAN is a privacy-preserving synthetic data generation method that incorporates domain-specific implicit rules through constraint matrices, ensuring realistic data while maintaining differential privacy.


<details>
  <summary>Details</summary>
Motivation: Traditional synthetic data methods fail to capture complex implicit domain rules (like medical prescription guidelines) while balancing privacy requirements from regulations like GDPR and HIPAA.

Method: Context-Aware Differentially Private Generative Adversarial Network (ContextGAN) that uses a constraint matrix to encode explicit and implicit domain knowledge, with a constraint-aware discriminator to evaluate synthetic data against these rules while maintaining differential privacy.

Result: ContextGAN produces high-quality synthetic data that respects domain rules and preserves privacy across healthcare, security, and finance domains, improving realism and utility compared to traditional methods.

Conclusion: ContextGAN successfully addresses the gap between privacy preservation and domain-specific realism by enforcing both explicit patterns and implicit rules, making it suitable for applications requiring strict privacy guarantees while maintaining data utility.

Abstract: The widespread use of big data across sectors has raised major privacy concerns, especially when sensitive information is shared or analyzed. Regulations such as GDPR and HIPAA impose strict controls on data handling, making it difficult to balance the need for insights with privacy requirements. Synthetic data offers a promising solution by creating artificial datasets that reflect real patterns without exposing sensitive information. However, traditional synthetic data methods often fail to capture complex, implicit rules that link different elements of the data and are essential in domains like healthcare. They may reproduce explicit patterns but overlook domain-specific constraints that are not directly stated yet crucial for realism and utility. For example, prescription guidelines that restrict certain medications for specific conditions or prevent harmful drug interactions may not appear explicitly in the original data. Synthetic data generated without these implicit rules can lead to medically inappropriate or unrealistic profiles. To address this gap, we propose ContextGAN, a Context-Aware Differentially Private Generative Adversarial Network that integrates domain-specific rules through a constraint matrix encoding both explicit and implicit knowledge. The constraint-aware discriminator evaluates synthetic data against these rules to ensure adherence to domain constraints, while differential privacy protects sensitive details from the original data. We validate ContextGAN across healthcare, security, and finance, showing that it produces high-quality synthetic data that respects domain rules and preserves privacy. Our results demonstrate that ContextGAN improves realism and utility by enforcing domain constraints, making it suitable for applications that require compliance with both explicit patterns and implicit rules under strict privacy guarantees.

</details>


### [259] [Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents](https://arxiv.org/abs/2512.08870)
*Xiang Chen,Yuling Shi,Qizhen Lan,Yuchao Qiu,Xiaodong Gu*

Main category: cs.LG

TL;DR: Fed-SE: A federated learning framework for LLM agent self-evolution that addresses gradient conflicts in heterogeneous environments through local trajectory filtering and global low-rank subspace aggregation.


<details>
  <summary>Details</summary>
Motivation: Privacy constraints prevent centralized optimization of LLM agents across dynamic environments, and standard FL fails due to heterogeneous tasks and sparse trajectory-level rewards causing gradient conflicts that destabilize global optimization.

Method: Fed-SE uses local evolution-global aggregation: locally, agents perform parameter-efficient fine-tuning on filtered high-return trajectories; globally, updates are aggregated within a low-rank subspace that disentangles environment-specific dynamics to reduce negative transfer.

Result: Experiments across five heterogeneous environments show Fed-SE improves average task success rates by approximately 18% over federated baselines, demonstrating effective cross-environment knowledge transfer in privacy-constrained settings.

Conclusion: Fed-SE successfully bridges the gap between federated learning and open-ended agent self-evolution, enabling robust knowledge transfer across heterogeneous environments while maintaining privacy constraints.

Abstract: LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.

</details>


### [260] [When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation](https://arxiv.org/abs/2512.08875)
*Joshua Ward,Bochao Gu,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: LLM-based tabular data generation methods (fine-tuning and prompting) leak privacy by reproducing memorized numeric patterns from training data, exposed by a simple No-box MIA called LevAtt. Proposed defense: strategic digit perturbation during generation.


<details>
  <summary>Details</summary>
Motivation: LLMs show promise for high-quality tabular synthetic data generation, but existing approaches (fine-tuning smaller models or prompting larger ones) may compromise privacy by memorizing and reproducing numeric patterns from training data. Need to systematically analyze this privacy risk.

Method: 1) Introduce LevAtt - a simple No-box Membership Inference Attack that targets numeric digit sequences in synthetic data, requiring only access to generated data. 2) Evaluate attack across various models and datasets. 3) Propose two defense methods, including a novel sampling strategy that strategically perturbs digits during generation.

Result: LevAtt exposes substantial privacy leakage across wide range of models and datasets, sometimes achieving perfect membership classification on state-of-the-art models. The proposed digit perturbation defense effectively defeats attacks with minimal loss of synthetic data fidelity and utility.

Conclusion: LLM-based synthetic data generation has unique privacy vulnerabilities through numeric digit memorization. Simple attacks like LevAtt can successfully infer membership, highlighting need for effective defenses. Strategic digit perturbation during generation offers promising defense with minimal utility trade-off.

Abstract: Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data.

</details>


### [261] [DAO-GP Drift Aware Online Non-Linear Regression Gaussian-Process](https://arxiv.org/abs/2512.08879)
*Mohammad Abu-Shaira,Ajita Rattani,Weishi Shi*

Main category: cs.LG

TL;DR: DAO-GP is a drift-aware online Gaussian Process model that dynamically adapts to evolving data distributions without fixed hyperparameters, using built-in drift detection and sparse memory management.


<details>
  <summary>Details</summary>
Motivation: Real-world datasets exhibit temporal dynamics (concept drift) that degrade model accuracy. Existing online GP methods lack drift awareness, have fixed hyperparameters, suffer from data snooping, lack principled decay mechanisms, and are memory inefficient.

Method: Proposes DAO-GP: a fully adaptive, hyperparameter-free, decayed, and sparse non-linear regression model with built-in drift detection and adaptation mechanism that dynamically adjusts model behavior based on drift severity.

Result: DAO-GP demonstrates robustness across stationary conditions and diverse drift types (abrupt, incremental, gradual). It shows dynamic adaptation, efficient memory management, and evolving inducing points, achieving superior or competitive performance compared to state-of-the-art models.

Conclusion: DAO-GP establishes itself as a drift-resilient solution for online non-linear regression, addressing critical limitations of conventional online GP methods through its adaptive, hyperparameter-free design with built-in drift awareness.

Abstract: Real-world datasets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. Gaussian Process (GP) models offer powerful non-parametric regression capabilities with uncertainty quantification, making them ideal for modeling complex data relationships in an online setting. However, conventional online GP methods face several critical limitations, including a lack of drift-awareness, reliance on fixed hyperparameters, vulnerability to data snooping, absence of a principled decay mechanism, and memory inefficiencies. In response, we propose DAO-GP (Drift-Aware Online Gaussian Process), a novel, fully adaptive, hyperparameter-free, decayed, and sparse non-linear regression model. DAO-GP features a built-in drift detection and adaptation mechanism that dynamically adjusts model behavior based on the severity of drift. Extensive empirical evaluations confirm DAO-GP's robustness across stationary conditions, diverse drift types (abrupt, incremental, gradual), and varied data characteristics. Analyses demonstrate its dynamic adaptation, efficient in-memory and decay-based management, and evolving inducing points. Compared with state-of-the-art parametric and non-parametric models, DAO-GP consistently achieves superior or competitive performance, establishing it as a drift-resilient solution for online non-linear regression.

</details>


### [262] [Explainable Anomaly Detection for Industrial IoT Data Streams](https://arxiv.org/abs/2512.08885)
*Ana Rita PaupÃ©rio,Diogo Risca,Afonso LourenÃ§o,Goreti Marreiros,Ricardo Martins*

Main category: cs.LG

TL;DR: A collaborative data stream mining framework combining unsupervised anomaly detection with human-in-the-loop learning for industrial maintenance, using online Isolation Forest with interpretability features for real-time fault detection.


<details>
  <summary>Details</summary>
Motivation: Industrial maintenance generates continuous data streams requiring real-time adaptive decision-making, but most data stream mining methods assume fully supervised settings while ground-truth labels are often delayed or unavailable in practice.

Method: Collaborative DSM framework integrating unsupervised anomaly detection with interactive human-in-the-loop learning. Uses online Isolation Forest enhanced with interpretability features: incremental Partial Dependence Plots and a feature importance score derived from deviations of Individual Conditional Expectation curves from a fading average.

Result: Real-time implementation and initial results for fault detection in a Jacquard loom unit. Ongoing work targets continuous monitoring to predict and explain imminent bearing failures.

Conclusion: The framework enables users to dynamically reassess feature relevance and adjust anomaly thresholds, supporting maintenance decisions in real-world industrial settings where labels are scarce or delayed.

Abstract: Industrial maintenance is being transformed by the Internet of Things and edge computing, generating continuous data streams that demand real-time, adaptive decision-making under limited computational resources. While data stream mining (DSM) addresses this challenge, most methods assume fully supervised settings, yet in practice, ground-truth labels are often delayed or unavailable. This paper presents a collaborative DSM framework that integrates unsupervised anomaly detection with interactive, human-in-the-loop learning to support maintenance decisions. We employ an online Isolation Forest and enhance interpretability using incremental Partial Dependence Plots and a feature importance score, derived from deviations of Individual Conditional Expectation curves from a fading average, enabling users to dynamically reassess feature relevance and adjust anomaly thresholds. We describe the real-time implementation and provide initial results for fault detection in a Jacquard loom unit. Ongoing work targets continuous monitoring to predict and explain imminent bearing failures.

</details>


### [263] [Revisiting the Scaling Properties of Downstream Metrics in Large Language Model Training](https://arxiv.org/abs/2512.08894)
*Jakub Krajewski,Amitis Shidani,Dan Busbridge,Sam Wiseman,Jason Ramapuram*

Main category: cs.LG

TL;DR: Direct framework for predicting downstream task performance from training budget using simple power laws, outperforming previous two-stage approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional scaling laws focus on proxy metrics like pretraining loss, but predicting downstream task performance has been considered unreliable. This paper challenges that view by proposing a direct approach.

Method: Proposes a direct framework to model scaling of benchmark performance from training budget using simple power laws for fixed token-to-parameter ratios, with functional forms that predict accuracy across ratios and account for inference compute under repeated sampling.

Result: The direct approach extrapolates better than previously proposed two-stage procedures (which are prone to compounding errors), validated on models up to 17B parameters trained on up to 350B tokens across two dataset mixtures.

Conclusion: Downstream task performance can be reliably predicted from training budget using direct power law scaling, with the complete set of pretraining losses and downstream evaluation results released for reproducibility and future research.

Abstract: While scaling laws for Large Language Models (LLMs) traditionally focus on proxy metrics like pretraining loss, predicting downstream task performance has been considered unreliable. This paper challenges that view by proposing a direct framework to model the scaling of benchmark performance from the training budget. We find that for a fixed token-to-parameter ratio, a simple power law can accurately describe the scaling behavior of log accuracy on multiple popular downstream tasks. Our results show that the direct approach extrapolates better than the previously proposed two-stage procedure, which is prone to compounding errors. Furthermore, we introduce functional forms that predict accuracy across token-to-parameter ratios and account for inference compute under repeated sampling. We validate our findings on models with up to 17B parameters trained on up to 350B tokens across two dataset mixtures. To support reproducibility and encourage future research, we release the complete set of pretraining losses and downstream evaluation results.

</details>


### [264] [Unsupervised Learning of Density Estimates with Topological Optimization](https://arxiv.org/abs/2512.08895)
*Suina Tanweer,Firas A. Khasawneh*

Main category: cs.LG

TL;DR: Unsupervised bandwidth selection for kernel density estimation using topological data analysis loss function.


<details>
  <summary>Details</summary>
Motivation: Kernel density estimation requires bandwidth tuning, which is critical for bias-variance trade-off but challenging to select automatically, especially in high dimensions where visualization is impossible.

Method: Propose an unsupervised learning approach using a topology-based loss function for automated bandwidth selection, leveraging topological data analysis to quantify topological characteristics.

Result: Benchmarked against classical techniques and demonstrated potential across different dimensions.

Conclusion: Topology-based loss function provides an effective unsupervised method for optimal bandwidth selection in kernel density estimation.

Abstract: Kernel density estimation is a key component of a wide variety of algorithms in machine learning, Bayesian inference, stochastic dynamics and signal processing. However, the unsupervised density estimation technique requires tuning a crucial hyperparameter: the kernel bandwidth. The choice of bandwidth is critical as it controls the bias-variance trade-off by over- or under-smoothing the topological features. Topological data analysis provides methods to mathematically quantify topological characteristics, such as connected components, loops, voids et cetera, even in high dimensions where visualization of density estimates is impossible. In this paper, we propose an unsupervised learning approach using a topology-based loss function for the automated and unsupervised selection of the optimal bandwidth and benchmark it against classical techniques -- demonstrating its potential across different dimensions.

</details>


### [265] [Open Polymer Challenge: Post-Competition Report](https://arxiv.org/abs/2512.08896)
*Gang Liu,Sobin Alosious,Subhamoy Mahajan,Eric Inae,Yihan Zhu,Yuhan Liu,Renzheng Zhang,Jiaxin Xu,Addison Howard,Ying Li,Tengfei Luo,Meng Jiang*

Main category: cs.LG

TL;DR: The Open Polymer Challenge (OPC) releases the first community-developed benchmark for polymer informatics with 10K polymers and 5 properties, focusing on multi-task prediction under realistic constraints to accelerate sustainable materials discovery.


<details>
  <summary>Details</summary>
Motivation: Machine learning progress in sustainable polymer materials has been limited by the lack of large, high-quality, openly accessible polymer datasets, creating a significant gap in polymer informatics.

Method: The challenge released a benchmark dataset with 10K polymers and 5 properties, requiring participants to develop models under realistic constraints including small data, label imbalance, and heterogeneous simulation sources. Techniques used included feature-based augmentation, transfer learning, self-supervised pretraining, and targeted ensemble strategies.

Result: The competition revealed important lessons about data preparation, distribution shifts, and cross-group simulation consistency, informing best practices for future large-scale polymer datasets. The resulting models, analysis, and released data create a new foundation for molecular AI in polymer science.

Conclusion: The Open Polymer Challenge establishes a crucial benchmark for polymer informatics, accelerating the development of sustainable and energy-efficient materials through improved data accessibility and modeling approaches.

Abstract: Machine learning (ML) offers a powerful path toward discovering sustainable polymer materials, but progress has been limited by the lack of large, high-quality, and openly accessible polymer datasets. The Open Polymer Challenge (OPC) addresses this gap by releasing the first community-developed benchmark for polymer informatics, featuring a dataset with 10K polymers and 5 properties: thermal conductivity, radius of gyration, density, fractional free volume, and glass transition temperature. The challenge centers on multi-task polymer property prediction, a core step in virtual screening pipelines for materials discovery. Participants developed models under realistic constraints that include small data, label imbalance, and heterogeneous simulation sources, using techniques such as feature-based augmentation, transfer learning, self-supervised pretraining, and targeted ensemble strategies. The competition also revealed important lessons about data preparation, distribution shifts, and cross-group simulation consistency, informing best practices for future large-scale polymer datasets. The resulting models, analysis, and released data create a new foundation for molecular AI in polymer science and are expected to accelerate the development of sustainable and energy-efficient materials. Along with the competition, we release the test dataset at https://www.kaggle.com/datasets/alexliu99/neurips-open-polymer-prediction-2025-test-data. We also release the data generation pipeline at https://github.com/sobinalosious/ADEPT, which simulates more than 25 properties, including thermal conductivity, radius of gyration, and density.

</details>
